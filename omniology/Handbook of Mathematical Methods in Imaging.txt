
Handbook of Mathematical Methods in Imaging


Otmar Scherzer (Ed.)
Handbook of
Mathematical Methods
in Imaging
with Figures and Tables
123

Editor:
Otmar Scherzer
Computational Science Center
University of Vienna
Nordbergstrasse 
Vienna
Austria
and
RICAM
Austrian Academy of Sciences
Linz
Austria
Library of Congress Control Number: 
ISBN ----
This publication is available also as:
Electronic publication under ISBN ----
Print and electronic bundle under ISBN ----
DOI ./----
© Springer Science+Business Media, LLC 
All rights reserved. This work may not be translated or copied in whole or in part without the written per-
mission of the publisher (Springer Science+Business Media, LLC, Spring Street, New York, NY , USA),
except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form
of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not
identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are subject to
proprietary rights.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Springer is part of Springer Science+Business Media
www.springer.com
Printed on acid-free paper
SPIN: SPi–

Preface
Today, computer imaging covers various aspects of data filtering, pattern recognition, feature
extraction, computer aided design, and computer aided inspection and diagnosis.
Pillars of the field of computer imaging are advanced, stable, and reliable algorithms. In
addition, feasibility analysis is required to evaluate practical relevance of the methods. To
put these pillars on solid grounds, a significant amount of mathematical tools are required.
This handbook makes a humble attempt to provide a survey of such mathematical tools.
We had the vision that this imaging handbook should contain individual chapters
which can serve as toolboxes, which, when aligned, form background material for complete
applied imaging problems. Therefore it should also give an impression on the broad mathe-
matical knowledge required to solve industrial and applied research applications: The
image formation process, very frequently, is assigned to the inverse problems community,
which is prominently represented in this handbook. The subsequent step is image analysis.
Nowadays, advanced Image Analysis, and Image Processing in general, uses sophisticated
methods from Geometry, Differential Geometry, Convex Analysis, Numerical Analysis, to
mention just a few. In fact, by the rapid advance of Imaging, the mathematical areas have
been pushed forward heavily, and raised their impact in application sciences.
My special thanks go to all individual authors for their valuable contributions and the
referees for their help in improving the contributions and making detailed comments. My
sincere thanks go to the Springer’s editors and staff, Vaishali Damle, Jennifer Carlson, and
Saskia Ellis for their patience, and their constant support and encouragement over the last
two years.
Finally, I would like to encourage the readers to submit suggestions regarding the hand-
book’s content. For a project of this size, it is likely that essential topics are missed. In a
rapidly evolving area like Imaging it is likely that new areas will appear in a very short time
and should be added to the handbook, as well as recent development enforce modifica-
tions of existing contributions. We are committed to issuing periodic updates and we look
forward to the feedback from the community.
Otmar Scherzer
Computational Science Center
University of Vienna, Austria
and
RICAM
Austrian Academy of Sciences
Linz, Austria


Table of Contents
Preface...................................................................................................
v
About the Editor......................................................................................
xi
List of Contributors ..................................................................................
xiii
Volume 
Inverse Problems – Methods

Linear Inverse Problems............................................................

Charles Groetsch

Large-Scale Inverse Problems in Imaging.....................................

Julianne Chung ⋅Sarah Knepper ⋅James G. Nagy

Regularization Methods for Ill-Posed Problems ............................

Jin Cheng ⋅Bernd Hofmann

Distance Measures and Applications to Multi-Modal
Variational Imaging .................................................................

Christiane Pöschl ⋅Otmar Scherzer

Energy Minimization Methods................................................... 
Mila Nikolova

Compressive Sensing ...............................................................

Massimo Fornasier ⋅Holger Rauhut

Duality and Convex Programming.............................................. 
Jonathan M. Borwein ⋅D. Russell Luke

EM Algorithms ........................................................................

Charles Byrne ⋅Paul P. B. Eggermont

Iterative Solution Methods........................................................ 
Martin Burger ⋅Barbara Kaltenbacher ⋅Andreas Neubauer

Level Set Methods for Structural Inversion and Image
Reconstruction........................................................................ 
Oliver Dorn ⋅Dominique Lesselier

viii
Table of Contents
Volume 
Inverse Problems – Case Examples

Expansion Methods.................................................................. 
Habib Ammari ⋅Hyeonbae Kang

Sampling Methods................................................................... 
Martin Hanke ⋅Andreas Kirsch

Inverse Scattering.................................................................... 
David Colton ⋅Rainer Kress

Electrical Impedance Tomography.............................................. 
Andy Adler ⋅Romina Gaburro ·William Lionheart

Synthetic Aperture Radar Imaging.............................................. 
Margaret Cheney ⋅Brett Borden

Tomography ........................................................................... 
Gabor T. Herman

Optical Imaging....................................................................... 
Simon R. Arridge ⋅Jari P. Kaipio ·Ville Kolehmainen ⋅Tanja Tarvainen

Photoacoustic and Thermoacoustic Tomography:
Image Formation Principles....................................................... 
Kun Wang ⋅Mark A. Anastasio

Mathematics of Photoacoustic and Thermoacoustic Tomography.... 
Peter Kuchment ⋅Leonid Kunyansky

Wave Phenomena.................................................................... 
Matti Lassas ⋅Mikko Salo ⋅Gunther Uhlmann
Volume 
Image Restoration and Analysis

Statistical Methods in Imaging................................................... 
Daniela Calvetti ⋅Erkki Somersalo

Supervised Learning by Support Vector Machines ........................ 
Gabriele Steidl

Table of Contents
ix

Total Variation in Imaging......................................................... 
V. Caselles ⋅A. Chambolle ⋅M. Novaga

Numerical Methods and Applications in Total Variation Image
Restoration............................................................................. 
Raymond Chan ⋅Tony Chan ⋅Andy Yip

Mumford and Shah Model and its Applications to Image
Segmentation and Image Restoration......................................... 
Leah Bar ⋅Tony F. Chan ⋅Ginmo Chung ⋅Miyoun Jung ⋅Nahum Kiryati ⋅
Rami Mohieddine ⋅Nir Sochen ⋅Luminita A. Vese

Local Smoothing Neighborhood Filters....................................... 
Jean-Michel Morel ⋅Antoni Buades ⋅Tomeu Coll

Neighborhood Filters and the Recovery of D Information ............. 
Julie Digne ⋅Mariella Dimiccoli ⋅Philippe Salembier ⋅Neus Sabater

Splines and Multiresolution Analysis .......................................... 
Brigitte Forster

Gabor Analysis for Imaging ....................................................... 
Ole Christensen ⋅Hans G. Feichtinger ⋅Stephan Paukner

Shape Spaces.......................................................................... 
Alain Trouvé ⋅Laurent Younes

Variational Methods in Shape Analysis........................................ 
Martin Rumpf ⋅Benedikt Wirth

Manifold Intrinsic Similarity ...................................................... 
Alexander M. Bronstein ⋅Michael M. Bronstein

Image Segmentation with Shape Priors: Explicit Versus Implicit
Representations...................................................................... 
Daniel Cremers

Starlet Transform in Astronomical Data Processing ....................... 
Jean-Luc Starck ⋅Fionn Murtagh ⋅Mario Bertero

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis..... 
Werner Benger ⋅René Heinzl ⋅Dietmar Hildenbrand ⋅Tino Weinkauf ⋅
Holger Theisel ⋅David Tschumperlé
Index ..................................................................................................... 


About the Editor
Otmar Scherzer  was born on June , in Vöcklabruck, Austria. He studied technical
mathematics at the University of Linz, Austria and received his Diploma in Mathematics
in . He received his PhD in and his habilitation in from the same university.
During and , he visited Texas A&M University and the University of Delaware
in USA. From –, he held professorships at the Ludwig Maximilian University,
Munich, and the University of Bayreuth, Germany.  Otmar then joined the University of
Innsbruck, Austria where he served as full professor “Applied and Computer Oriented
Mathematics,” from to mid-. In mid-, he accepted a position at the Uni-
versity of Vienna, where he currently heads the Computational Science Center which
was created upon his appointment. More information about the center can be found at:
http://www.csc.univie.ac.at/.
 
Otmar’s research interests include Inverse Problems, in particular photoacoustic imag-
ing, Regularization, Image Processing and PDEs. He is a prolific researcher, with more than
a research articles published in several well-respected journals. He is the co-author of
two monographs and had co-edited volumes, including this handbook, and has served
on the editorial board of many prominent journals.
 
Otmar was elected member of “Junge Kurie” of the Austrian Academy of Sciences in
.


List of Contributors
Andy Adler
Carleton University
Ottawa, ON
Canada
Habib Ammari
École Normale Supérieure
Paris
France
Mark A. Anastasio
Illinois Institute of Technology
Chicago, IL
USA
Simon R. Arridge
University College London
London
UK
Leah Bar
University of Minnesota
Minneapolis, MN
USA
Werner Benger
Center for Computation and Technology
at Lousiana State University
Baton Rouge, LA
USA
Mario Bertero
Università di Genova
Genova
Italy
Brett Borden
Naval Postgraduate School of
Engineering
Monterey, CA
USA
Jonathan M. Borwein
University of Newcastle
Newcastle, NSW
Australia
Alexander M. Bronstein
Technion-Israel Institute of Technology
Haifa
Israel
Michael M. Bronstein
Technion-Israel Institute of Technology
Haifa
Israel
Antoni Buades
Universite Rene Descartes
Paris
France
Martin Burger
University of Münster
Münster
Germany
Charles Byrne
University of Massachusetts Lowell
Lowell, MA
USA

xiv
List of Contributors
Daniela Calvetti
Case Western Reserve University
Cleveland, OH
USA
Vicent Caselles
Universitat Pompeu-Fabra
Barcelona
Spain
Antonin Chambolle
Ecole Polytechnique
Palaiseau
France
Raymond Chan
The Chinese University of Hong Kong
Shatin
Hong Kong
Tony F. Chan
The Hong Kong University of Science and
Technology
Clear Water Bay
Hong Kong
and
University of California Los Angeles
Los Angeles, CA
USA
Margaret Cheney
Rensselaer Polytechnic Institute
Troy, NY
USA
Jin Cheng
Fudan University
Shanghai
China
Ole Christensen
Technical University of Denmark
Lyngby
Denmark
Ginmo Chung
Nanyang Technological University
Singapore
Singapore
Julianne Chung
University of Maryland
College Park, MD
USA
Tomeu Coll
Universitat de les Illes Balears
Palma-Illes Balears
Spain
David Colton
University of Delaware
Newark, DE
USA
Daniel Cremers
TU München
München
Germany
Julie Digne
École Normale Supérieure de Cachan
Cachan
France
Mariella Dimiccoli
Collège de France
Paris
France

List of Contributors
xv
Oliver Dorn
The University of Manchester
Manchester
UK
and
Universidad Carlos III de Madrid
Madrid
Spain
Paul P. B. Eggermont
University of Delaware
Newark, DE
USA
Hans G. Feichtinger
University of Vienna
Vienna
Austria
Massimo Fornasier
Austrian Academy of Sciences
Linz
Austria
Brigitte Forster
Technische Universität München
Garching
Germany
and
Helmholtz Zentrum München
Neuherberg
Germany
Romina Gaburro
University of Limerick
Limerick
Ireland
Charles Groetsch
The Citadel
Charleston, SC
USA
Martin Hanke
University of Mainz
Mainz
Germany
René Heinzl
Shenteq s.r.o
Bratislava
Slovak Republic
Gabor T. Herman
The Graduate Center of the City
University of New York
New York, NY
USA
Dietmar Hildenbrand
University of Technology Darmstadt
Darmstadt
Germany
Bernd Hofmann
Chemnitz University of Technology
Chemnitz
Germany
Miyoun Jung
University of California Los Angeles
Los Angeles, CA
USA
Jari P. Kaipio
University of Auckland
Auckland
New Zealand
Barbara Kaltenbacher
University of Graz
Graz
Austria

xvi
List of Contributors
Hyeonbae Kang
Inha University
Incheon
Korea
Andreas Kirsch
Karlsruhe Institute of Technology (KIT)
Karlsruhe
Germany
Nahum Kiryati
Tel Aviv University
Tel Aviv
Israel
Sarah Knepper
Emory University
Atlanta, GA
USA
Ville Kolehmainen
University of Eastern Finland
Kuopio
Finland
Rainer Kress
Universität Göttingen
Göttingen
Germany
Peter Kuchment
Texas A & M University
College Station, TX
USA
Leonid Kunyansky
University of Arizona
Tucson, AZ
USA
Matti Lassas
University of Helsinki
Helsinki
Finland
Dominique Lesselier
Laboratoire des Signaux et Systèmes
Gif-sur-Yvette
France
William Lionheart
The University of Manchester
Manchester
UK
Russell D. Luke
Universität Göttingen
Göttingen
Germany
Rami Mohieddine
University of California Los Angeles
Los Angeles, CA
USA
Jean-Michel Morel
École Normale Supérieure de Cachan
Cachan
France
Fionn Murtagh
Science Foundation Ireland
Dublin
Ireland
and
Royal Holloway University of London
Egham
UK
James G. Nagy
Emory University
Atlanta, GA
USA
Andreas Neubauer
University of Linz
Linz
Austria

List of Contributors
xvii
Mila Nikolova
ENS Cachan, CNRS UniversSud
Cachan Cedex
France
Matteo Novaga
Università di Padova
Padova
Italy
Stephan Paukner
Applied Research Center
Communication Systems GmbH
Vienna
Austria
Christiane Pöschl
Universitat Pompeu Fabra
Barcelona
Spain
Holger Rauhut
University of Bonn
Bonn
Germany
Martin Rumpf
Bonn University
Bonn
Germany
Neus Sabater
École Normale Supérieure de Cachan
Cachan
France
Philippe Salembier
Technical University of Catalonia (UPC)
Barcelona
Spain
Mikko Salo
University of Helsinki
Helsinki
Finland
Otmar Scherzer
University of Vienna
Vienna
Austria
and
RICAM
Austrian Academy of Sciences
Linz
Austria
Nir Sochen
Tel Aviv University
Tel Aviv
Israel
Erkki Somersalo
Case Western Reserve University
Cleveland, OH
USA
Jean-Luc Starck
CEA/Saclay
Gif-sur-Yvette Cedex
France
Gabriele Steidl
Universität Mannheim
Mannheim
Germany
Tanja Tarvainen
University of Eastern Finland
Kuopio
Finland

xviii
List of Contributors
Holger Theisel
Institut fur Simulation und Graphik AG
Visual Computing
Magdeburg
Germany
Alain Trouvé
École Normale Supérieure de Cachan
Cachan
France
David Tschumperlé
GREYC (UMR-CNRS )
CAEN Cedex
France
Gunther Uhlmann
University of Washington
Seattle, WA
USA
Luminita A. Vese
University of California Los Angeles
Los Angeles, CA
USA
Kun Wang
Illinois Institute of Technology
Chicago, IL
USA
Tino Weinkauf
New York University
New York, NY
USA
Benedikt Wirth
Bonn University
Bonn
Germany
Andy Yip
National University of Singapore
Singapore
Singapore
Laurent Younes
John Hopkins University
Baltimore, MD
USA


Linear Inverse Problems
Charles Groetsch
.
Introduction........................................................................
.
Background.........................................................................
.
Mathematical Modeling and Analysis............................................
..
A Platonic Inverse Problem..............................................................
..
Cormack’s Inverse Problem.............................................................
..
Forward and Reverse Diffusion.........................................................
..
Deblurring as an Inverse Problem......................................................
..
Extrapolation of Band-Limited Signals................................................
..
PET..........................................................................................
..
Some Mathematics for Inverse Problems.............................................
...
Weak Convergence.......................................................................
...
Linear Operators..........................................................................
...
Compact Operators and the SVD......................................................
...
The Moore–Penrose Inverse............................................................
...
Alternating Projection Theorem.......................................................
.
Numerical Methods...............................................................
..
Tikhonov Regularization................................................................
..
Iterative Regularization..................................................................
..
Discretization..............................................................................
.
Conclusion.........................................................................
.
Cross-References..................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Linear Inverse Problems
Abstract: This introductory treatment of linear inverse problems is aimed at students and
neophytes. An historical survey of inverse problems and some examples of model inverse
problems related to imaging are discussed to furnish context and texture to the mathemat-
ical theory that follows. The development takes place within the sphere of the theory of
compact linear operators on Hilbert space and the singular value decomposition plays an
essential role. The primary concern is regularization theory: the construction of conver-
gent well-posed approximations to ill-posed problems. For the most part, the discussion is
limited to the familiar regularization method devised by Tikhonov and Phillips.
.
Introduction
▷…although nature begins with the cause and ends with the
experience we must follow the opposite course, namely
…begin with the experience and by means of it
end with the cause.
Leonardo da Vinci
An inverse problem is the flip side of some direct problem. Direct problems treat the
transformation of known causes into effects that are determined by some specified model
of a natural process. They tend to be future directed and outward looking, and are often
concerned with forecasting or with determining external effects of internal causes. Direct
problems have solutions (causes have effects), and the process of transforming causes into
effects is a mathematical function: a given cause determines, via the model, a unique effect.
In direct problems the operator that maps causes into effects is typically continuous in
natural metrics: close causes have close effects. These features of direct problems make
them well posed.
The idea of a well-posed problem has its origins in Jacques Hadamard’s short paper []
published in . Hadamard held the opinion that an important physical problem must
have three attributes:
. (Existence) It has a solution.
. (Uniqueness) It has only one solution.
. (Stability) The solution depends continuously on the data of the problem.
A problem satisfying these three conditions is called well posed. In his paper,
Hadamard called a problem bien posé if it has properties () and (). Again in his 
lectures [], he called a problem “correctly set” if it satisfies () and (). Condition () was
not named as a specific requirement of a well-posed problem, but his explicit notice of
the lack of continuous dependence on boundary data of the solution of Cauchy’s problem
for Laplace’s equation led to () becoming part of the accepted definition of a well-posed
problem.

Linear Inverse Problems 

A problem is ill posed if it lacks these qualities. Hadamard’s suggestion that ill-posed
problems are devoid of physical significance (déprourvu de signification physique) was
unfortunate, as almost all inverse problems in the physical and biological sciences are ill
posed. To be fair, it should be noted that Hadamard was speaking about a specific prob-
lem, the Cauchy problem for Laplace’s equation in a strip. On the other hand, Courant []
insisted more generally that “a mathematical problem cannot be considered as realistically
corresponding to physical phenomena unless …” it satisfies condition (). The problems
of existence and uniqueness in inverse problems can often be ameliorated by generalizing
the notion of solution and constraining the generalized solution, but the key attribute of
stability often is a feature that is inherently absent in inverse problems. This essential lack
of stability usually has dire consequences when numerical methods, using measured or
uncertain data, are applied to inverse problems.
Inverse problems are as old as science itself. In fact, a reasonable working definition of
science is the explanation of natural phenomena by the construction of conceptual models
for interpreting imperfect observational representations of “true” natural objects or pro-
cesses. This definition encompasses the three essential ingredients of mathematical inverse
problems: a “true” solution, a model or operator that transforms this true solution into an
imperfect representation that is amenable to observations or measurements. One could say
that inverse theory embraces an operating principle that is essentially Platonic: true natural
objects exist, but it is only through models and imperfectly perceived images that we expe-
rience them. The challenge is to “invert” the model to recover a useful estimate of the true
object from the observed image. In this sense, all of inverse theory deals with “imaging.”
A mathematical framework for the study of inverse problems must provide sufficient
scope for each of the three elements: true solutions, model, and observations. In this
chapter the solution space and the space of observations are both taken to be Hilbert spaces,
but not necessarily the same Hilbert space, as one naturally desires more of the solution
than one demands from the observations. The model is a transformation or operator that
carries a possible solution to an observed effect. We consider only linear inverse problems,
so our models are linear operators.
Any practical model suppresses some information. If a model represents every bit of
information in the objects themselves (i.e., the model operator is the identity operator),
then nothing is gained in conceptual economy. In this case one is in the absurd position of
Mein Herr in Lewis Carroll’s Sylvie and Bruno Concluded:
▷
“We actually made a map of the country, on a scale of a mile to the mile!”…“It has never been
spread out yet,” said Mein Herr: “the farmers objected; they said it would cover the whole
country, and shut out the sunlight! So now we use the country itself, as its own map, and I
assure you it does nearly as well.”
Finite linear models lead to linear algebra problems. Idealized limiting versions of finite
models typically lead to compact linear operators, that is, limits of finite rank operators. A
compact operator may have a nontrivial null-space, a non-closed range, or an unbounded
(generalized) inverse. Therefore these operators, which occur widely in models of lin-
ear inverse problems, lack all the virtues of well-posedness. In this chapter, we provide a


Linear Inverse Problems
somewhat slanted survey of linear inverse problems, mainly involving compact operators,
with special attention to concepts underlying methods for constructing stable approximate
solutions.
Before draping these ideas on a mathematical framework, we discuss a half dozen
examples of model inverse problems that have played significant roles in the development
of the physical sciences.
.
Background
▷Our science is from the watching of shadows;
Ezra Pound
This brief and incomplete historical survey of physical inverse problems is meant to
give some perspective on certain inverse problems closely related to imaging in the broad
sense. Our viewpoint involves both the very large scale, treating inverse problems loosely
associated with assessing the cosmos, and the human scale, dealing with evaluation of the
inaccessible interior of bodies (human or otherwise).
Inverse theory, as a distinct field of inquiry, is a relatively recent development, however
inverse problems are as old as science itself. A desire to know causes of perceived effects
is ingrained in the human intellectual makeup. The earliest attempts at explanations, as
for example in the creation myths of various cultures, were supernatural – grounded in
mysticism and mythology. When humankind embarked on a program of rationalization
of natural phenomena, inverse problems emerged naturally and inevitably. An early exam-
ple is Plato’s allegory of the cave (ca. B.C.). In the seventh book of his Republic, Plato
describes the situation. A group of people have been imprisoned since their birth in a cave
where they are chained in such a manner that allows them to view only a wall at the back of
the cave. Outside the cave life goes on, illuminated by a fire blazing in the distance. The cap-
tives in the cave must reconstruct this external reality on the basis of shadows cast on the
rear wall of the cave. This is the classic inverse imaging problem: real objects are perceived
only as two-dimensional images in the form of shadows on the cave wall. This annihilation
of a dimension immediately implies that the reconstruction problem has multiple solutions
and that solutions are unstable in that highly disparate objects may have virtually identical
images.
Aristotle adapted his teacher Plato’s story of the cave to address a scientific inverse prob-
lem: the shape of the earth. This shape could not be directly assessed in Aristotle’s time, so
he suggested an indirect approach (See Book II of On the Heavens.):
▷
As it is, the shapes which the moon itself each month shows are of every kind – straight,
gibbous, and concave – but in eclipses the outline is always curved; and since it is the inter-
position of the earth that makes the eclipse, the form of the line will be caused by the form
of the earth’s surface, which is therefore spherical.

Linear Inverse Problems 

Aristotle’s reasoning provided an indirect argument for the sphericity of the earth based on
the shapes of shadows cast on the moon.
Inverse imaging has been a technical challenge for centuries. The difficulties that early
investigators encountered were vividly captured by Albrecht Dürer’s woodcut Man drawing
a lute (). We can see the doubts and angst brought on by the inverse imaging problem
etched on the face of the crouching technician (> Fig. -):
The character on the left, standing bolt upright in a confident attitude, has the com-
paratively easy direct problem. He has complete knowledge of the object, and he knows
exactly how the projection model will produce the image. On the other hand, the crouch-
ing man on the right, with the furrowed brow, faces the more difficult inverse problem of
assessing whether the image captures the essential features of the object. Dürer’s woodcut
is a striking representation of the comparative difficulties of direct and inverse assessment.
Modern imaging science has its roots in Galileo Galilei’s lunar observations carried out
during the winter of . Prior to Galileo’s study, the moon was thought to belong to the
realm of the Pythagorean fifth essence, consisting of perfectly uniform material in perfect
spherical form. Galileo’s eyes, empowered by his improved telescope, the earliest scientific
imaging device, showed him otherwise []:
▷
…we certainly see the surface of the Moon to be not smooth, even, and perfectly spherical,
as the great crowd of philosophers has believed about this and other heavenly bodies, but,
on the contrary, to be uneven, rough, and crowded with depressions and bulges. And it is
like the Earth itself, which is marked here and there with chains of mountains and depth of
valleys.
But Galileo was not satisfied with qualitative evidence. He famously stated that the book
of nature is written in the language of mathematics, and he used mathematics, in the form
⊡Fig. -
A renaissance inverse problem


Linear Inverse Problems
of the Pythagorean theorem, along with some shrewd estimates, to assess indirectly the
heights of lunar mountains. The process of indirect assessment is a hallmark of inverse
problems in the natural sciences. (See [] for an account of inverse problems of indirect
assessment.)
Non-uniqueness is a feature of many inverse problems that was slow to gain accep-
tance. An early instance of this phenomenon in a physical inverse problem occurred in
the kinematic studies of ballistics carried out by Niccolò Tartaglia in the sixteenth century.
Tartaglia claimed to be the inventor of the gunner’s square, a device for measuring the angle
of inclination of a cannon. Using his square Tartaglia carried out ranging trials and pub-
lished some of the earliest firing tables. He studied not only the direct problem of finding
ranges for a given firing angle, but also the inverse problem of determining the firing angle
that results in a given range. Although Tartaglia’s treatment was conceptually flawed and
lacked rigor, his work contains glimmers of a number of basic principles of mathemati-
cal analysis that took several centuries to mature []. Tartaglia was particularly struck by
non-uniqueness of solutions of the inverse problem. As he put it proudly in the dedication
of his book Nova Scientia (Venice, ):
▷
I knew that a cannon could strike in the same place with two diﬀerent elevations or aimings, I
found a way of bringing about this event, a thing not heard of and not thought by any other,
ancient or modern.
With this boast Tartaglia was one of the first to call attention to this common feature of
non-uniqueness in inverse problems.
Tartaglia found that for a given fixed charge each range (other than the maximum
range) is achieved by two distinct aimings placed symmetrically above and below the ○
inclination. A century and a half later, Edmond Halley [] took up the more general prob-
lem of allowing both the charge and the firing angle to vary while firing on a fixed target
situated on an inclined plane. In this case the inverse problem of determining charge-angle
pairs that result in a strike on the target has infinitely many solutions. (Of course, Halley
did not address air resistance; his results are extended to the case of first order resistance
in [].) Halley restored uniqueness to the inverse aiming problem by restricting consid-
eration to the solution which minimizes what we would now call the kinetic energy of
the emergent cannon ball. The idea of producing uniqueness by seeking the solution that
minimizes a quadratic functional would in due course become a key feature of inverse
theory.
The model for a modern scientific society was laid out in Francis Bacon’s utopian novel
The New Atlantis (). Bacon describes a voyage to the mythical land of Bensalem, which
was inhabited by wise men inclined to inverse thinking. Solomon’s House, a research insti-
tute in Bensalem was dedicated to the “knowledge of causes, and secret motions of things;
and the enlarging of the bounds of human empire, to the effecting of all things possible.”
(my italics). The Royal Society of London, founded in and modeled on Baconian
principles, was similarly dedicated. The first great triumph (due largely to Halley’s efforts)
of the Royal Society was the publication of Newton’s magisterial Principia Mathematica
(). In the Principia, Newton’s laws relating force, mass, and acceleration, combined

Linear Inverse Problems 

with his inverse square law of gravity, were marshaled to solve the direct problem of two-
body dynamics, confirming the curious form of Kepler’s planetary orbits: an inverse square
centrally directed force leads to an orbit, which is a conic section. But Newton was not sat-
isfied with this. He also treated the inverse problem of determining what gravitational law
(cause) can give rise to a given geometrical orbit (effect).
In the history of science literature the problem of determining the orbit, given the law of
attraction, is sometimes called the inverse problem; this practice inverts the terminology
currently common in the scientific community. The reverse terminology in the history
community is evidently a consequence of the fact that Newton took up the determination
of force law first and then treated the orbit determination problem. Indeed, Newton treated
the inverse problem of orbits before he took up the direct problem. After all, his primary
goal was to discover the laws of nature, the causes, rather than the effects. As Newton put it
in the preface to the first edition of his Principia: “ …the whole burden of philosophy seems
to consist of this – from the phenomena of motions to investigate the forces of nature, and
then from these forces to demonstrate the other phenomena”;
In , mathematical inverse theory produced a spectacular scientific triumph – the
discovery of another world. The seeds of the discovery lay in the observed irregularities
in the orbit of Uranus, the most distant of the planets known at the time. The orbit of
Uranus did not fit with predictions based on Newton’s theories of gravity and motion. In
particular an orbit calculated to fit contemporary observations did not fit observations
made in the previous century, and an orbit that fit to the older sightings did not match
the contemporary data. This suggested two possibilities: either Newton’s theory had to be
modified at great distances, or perhaps the anomalies in the orbit of Uranus were the effect
of an as yet undiscovered planet (the cause) operating on Uranus via Newton’s laws.
During the summer vacation of , John Couch Adams, an undergraduate of St. John’s
College, Cambridge, was intrigued by the second possibility. He recorded this diary entry:
▷
, July . Formed a design in the beginning of the week, of investigating, as soon as
possible after taking my degree, the irregularities in the motion of Uranus, which are yet
unaccounted for; in order to ﬁnd whether they may be attributed to the action of an undis-
covered planet beyond it; and if possible thence to determine the elements of its orbit, etc.
approximately, which would probably lead to its discovery.
Adams solved the inverse problem of determining the characteristics of the orbit of the
undiscovered planet, now known as Neptune, that perturbs Uranus. However, a sequence
of lamentable missteps, involving his own timidity, bureaucratic inertia, and other human
factors, resulted in the honor of “discovering” the new planet on the basis of mathematics
going to Urbain LeVerrier of France, who solved the inverse problem independently of
Adams. This of course led to disappointment in England over the botched opportunity to
claim the discovery and to a good deal of hauteur in France over the perceived attempt by
the English to grab credit deserved by a Frenchman. The fascinating story of the unseemly
squabble is well told in []. See also [] for a recent update in which old wounds are
reopened.


Linear Inverse Problems
Newton’s discussion of inverse orbit problems in his Principia, and vague doubts about
the form of the gravitational force law raised prior to the discovery of Neptune, may have
inspired other inverse problems. An early interesting “toy” inverse problem in this vein
was published by Ferdinand Joachimstahl in []. The problem Joachimstahl posed,
and solved by an Abel transform, was to determine the law of gravitational attraction if the
total force at any distance from a line of known mass density is given.
Johann Radon laid the foundation of mathematical imaging science, without knowing
it, in his memoir []. (An English translation of Radon’s paper may be found in [].)
Radon was concerned with the purely mathematical problem of determining a real-valued
function of two variables from knowledge of the values of its line integrals over all lines
intersecting its domain. Although Radon evidently had no application in mind, his treat-
ment was to become, after its rediscovery a half century later, the basis for the mathematics
of computed tomography. (See [] for more on the history of computed tomography.)
Essentially the same result was obtained independently by Viktor Ambarzumian [] who
was interested in a specific inverse problem in astronomy. Proper motions of stars are diffi-
cult to determine, but radial velocities (relative to the earth) are obtainable from chromatic
Doppler shift measurements. Ambarzumian used a mathematical model essentially equiv-
alent to that of Radon to deduce the true three-dimensional distribution of stellar velocities
from the distribution of the radial velocities.
In the mid-fifties of the last century, Allan Cormack, a young physics lecturer at the
University of Cape Town, who was moonlighting in the radiology department of Groote
Schuur Hospital, had a bright idea. In Cormack’s words:
▷
It occurred to me that in order to improve treatment planning one had to know the distribu-
tion of the attenuation coeﬃcient of tissues in the body, and that this distribution had to be
found by measurements made external to the body. It soon occurred to me that this infor-
mation would be useful for diagnostic purposes and would constitute a tomogram or series
of tomograms, though I did not learn the word “tomogram”for many years.
This was the birth of the mathematical theory of medical imaging. Cormack would not
learn of Radon’s work for another two decades, but he developed the basic results for
radially symmetric attenuation coefficient distributions and tested the theory with good
results on a simple manufactured specimen in the form of a cylinder of aluminum encased
in an annular prism of wood. The reconstructed piecewise constant attenuation function
matched that of the known specimen well enough to show the promise of this revolutionary
new imaging technology.
In the s, inverse thinking and indirect assessment led to another spectacular
advance in astronomy: the discovery of extrasolar planets. Philosophers had speculated
on the reality of planets linked to the stars at least since classical Greek times, and few in
modern times doubted the existence of extrasolar planets. But convincing evidence of their
existence had to await the development of sufficiently sensitive telescope-mounted spec-
trometers and the application of simple inverse theory. The indirect evidence of extrasolar
planets consisted of spectral shift data extracted from optical observations of a star.

Linear Inverse Problems 

In a single star-planet system determining the variable radial velocity (relative to the
earth) of a star wobbling under the gravitational influence of an orbiting planet of known
mass and orbital radius is a simple direct problem – just equate the gravitational acceler-
ation of the planet to its centripetal acceleration. (Consider only the simple case in which
the planet, star, and earth are coplanar and the orbit is circular; an orbit oblique to the line
of sight from earth introduces an additional unknown quantity. As a consequence of this
obliquity, the relative mass estimated from the inverse problem is actually a lower bound
for this quantity.) Using Doppler shift data a simple inverse problem model may be devel-
oped for determining approximations to the relative planetary mass and orbital radius. The
solution of the inverse problem enabled astronomers to announce in the existence of
the first confirmed extrasolar planet orbiting the star Pegasi. The millennia-old question
of the existence of extrasolar worlds finally had a convincing positive answer.
We bring this historical survey of inverse problems up to the present day with the great-
est challenge in contemporary cosmology: the search for dark matter. Such matter, being
“dark,” is by definition inaccessible to direct measurement. But recently an imaging model
on the largest scale in the history of science has come to be used in attempts to assay this
dark matter. The process of gravitational lensing, which is based on Einstein’s theory of
curved space-time, presents the possibility of inverting the imaging model to estimate a
dark mass (the gravitational lens) that intervenes between the observer on earth and an
immensely distant light source. The dark mass warps space in its vicinity resulting, under
appropriate conditions, in focusing onto the earth light rays, that in flat space would not
intersect the earth. In an extreme case in which the light source (e.g., a galaxy), the inter-
vening gravitational lens (dark matter), and the collected image are collinear, this results
in a phenomenon called an Einstein ring (first observed in , see []). If the distances
from earth to the lens, and from the lens to source can be estimated, then the solution of
an inverse problem gives an estimate of the dark mass (see []).
.
Mathematical Modeling and Analysis
▷…we have to remember that what we observe is not nature in itself
but nature exposed to our method of questioning.
Werner Heisenberg
..
A Platonic Inverse Problem
Plato’s discussion of captives struggling to discern the real cause of shadows cast on the
back wall of a cave is a primeval exemplar of inverse imaging problems. Here we present a
toy imaging problem inspired by Plato’s allegory. While the problem is very elementary, it
usefully illustrates some important aspects of imaging problems and inverse problems in
general.


Linear Inverse Problems
Imagine a two-dimensional convex object in the xy-plane, which is bounded by the
positive coordinate axes and the graph of a function y = f (x),≤x ≤that is positive on
[,), strictly decreasing and concave-down, and satisfies f () = = f ′(). The object is
illuminated by parallel light rays from the left that form angles θ with the negative ray of
the horizontal axis, as illustrated in > Fig. -.
The goal is to reconstruct the shape of the object from observations of the extent s(θ)
of the shadow cast by the object. This is accomplished by fashioning a parameterization
(x(θ), f (x(θ))) of the boundary curve of the object. As a further simplification we assume
that f ′() = −. These assumptions guarantee that for each s > there is a unique point
(t, f (t)) on the graph of f at which the tangent line intersects the x-axis at s. What is
required to see this is the existence of a unique t ∈(,) such that the tangent line to the
graph at the point (t, f (t)) intersects the x-axis at s. That is,
(s −t)f ′(t) + f (t) = .
For each fixed s > the expression on the left is strictly decreasing for t ∈(,), positive
at t = and negative at t = , so the existence of a unique such t = x(θ) is assured. At the
point of tangency
−tan θ = f ′(x(θ)).
Also,
f (x(θ)) = (tan θ)(s(θ) −x(θ)),
and hence determining x(θ) also gives (x(θ), f (x(θ))), which solves the inverse imaging
problem. Combining these results we have
−(tan θ)x′(θ) = f ′(x(θ))x′(θ) = (s(θ) −x(θ))secθ + (s′(θ) −x′(θ))tan θ.
A bit of simplification yields
x(θ) = s(θ) + 
sin (θ)s′(θ),
(.)
(x(θ), f(x(θ)))
s(θ)
θ
⊡Fig. -
A model shadow problem

Linear Inverse Problems 

which explicitly solves the inverse problem of determining the shape (x(θ), f (x(θ)))
from knowledge of the extent of the shadows s(θ).
The explicit formula (> .) would seem to completely solve the inverse problem. In
a theoretical sense this is certainly true. However, the formulation (> .) shelters a sub-
versive factor (the derivative) that should alert us to potential challenges involved in the
practical solution of the inverse problem. Observations are always subject to measurement
errors. The differentiation process, even if performed exactly, may amplify these errors as
differentiation is a notoriously unstable process. For example, if a shadow function s(θ) is
perturbed by low-amplitude high-frequency noise of the form ηn(θ) =

n sin nθ giving
observed data
sn(θ) = s(θ) + ηn(θ),
then the corresponding shape abscissas provided by (> .) satisfy
xn(θ) = x(θ) + ηn(θ) + sin θ

η′
n(θ).
But ηn converges uniformly to as n →∞, while max ∣η′
n∣→∞, giving a convincing illus-
tration of the instability of the solution of the inverse problem provided by (> .). For more
examples of model inverse problems with explicit solutions that involve differentiation
see [].
It is instructive to view the inverse problem from another perspective. Note that by
(> .), s(θ) is the solution of the linear differential equation
ds
dθ +

sin (θ)s =

sin (θ)x(θ)
satisfying s(π/) = . This differential equation may be solved by elementary means
yielding
s(θ) = + cos θ
sin θ
+ ∫
θ
π/
(+ cos θ)
(+ cos φ)sin θ x(φ) dφ.
(.)
In this formulation, the “hidden” solution x(φ) of the inverse problem is seen to be trans-
formed by a linear integral operator into observations of the shadows s(θ). The goal now is
to uncover x(φ) from (> .) using knowledge of s(θ), that is, one must solve an integral
equation.
The solution of the integral formulation (> .) suffers from the same instability as the
explicit solution (> .). Indeed, one may write (> .) as
s = ψ + ψTx
where ψ(θ) = (+ cos θ)/sin θ and
(Tx)(θ) = ∫
θ
π/

+ cos φ x(φ) dφ.
If we let
n(φ) = n
(+ cos φ)sin nφ and set
sn = ψ + ψT(x +
n)


Linear Inverse Problems
then one finds that sn →s uniformly, while max ∣n∣→∞. That is, arbitrarily small per-
turbations in the data s may correspond to arbitrarily large deviations in the solution x.
This story has a moral: instability is intrinsic to the inverse problem itself and not a
manifestation of a particular representation of the solution.
..
Cormack’s Inverse Problem
As noted in the previous section, the earliest tomographic test problem explicitly moti-
vated by medical imaging was Cormack’s experiment [] with a fabricated sample having
a simple radially symmetric absorption coefficient. The absorption coefficient is a scalar
field whose support may be assumed to be contained within the body to be imaged. This
coefficient f supplies a measure of the attenuation rate of radiation as it passes through a
given body point and is characterized by Bouguer’s law
dI
ds = −f I,
where I is the intensity of the radiation and s is arclength. The integral of f along a line L
intersecting the body then satisfies
g = ∫L f ds.
Here g = ln(I/Ie), where Iis the incident intensity, and Ie the emergent intensity, of the
beam. The observable quantity g is then a measure of the total attenuation effect that the
body has on the beam traversing the line L.
To be more specific, for a given t ∈R and a given unit vector ⃗φ = (cos φ,sin φ) let Lt,φ
represent the line
Lt,φ = {⃗x ∈R: ⟨⃗x, ⃗φ⟩= t}
where ⟨⋅,⋅⟩is the Euclidean inner product. We will denote the integral of f over Lt,φ by
R(f )(t, φ) = ∫Lt,φ
f ds = ∫
∞
−∞f (t cos φ −s sin φ, t sin φ + s cos φ) ds.
If f is radial, that is, independent of φ, then
R(f )(t, φ) = R(f )(t,) = ∫
∞
−∞f (t, s) ds.
(.)
Furthermore, if f vanishes exterior to the disk of radius R, then on setting r =
√
t+ s
and f (r) = f (t, s), one finds
g(t) = ∫
R
t
r f (r)
√
r−tdr,
(.)
where g(t) = R(f )(t,). The mapping defined by (> .), which for a given line Lt,φ
transforms the radial attenuation coefficient into the function g, is an Abel transform of f .
It represents, as a direct problem, Cormack’s early experiment with a radially symmetric

Linear Inverse Problems 

test body. Determining the distribution of the attenuation coefficient requires solving the
inverse problem. The Abel transform may be formally inverted by elementary means to
furnish a solution of the inverse problem of determining the attenuation coefficient f from
knowledge of the loss data g. Indeed, by (> .) and a reversal of order of integration,
∫
R
r
tg(t)
√
t−rdt = ∫
R
r
f (s)s ∫
s
r
t
√
s−t√
t−rdt ds = π ∫
R
r
f (s)s ds,
since
∫
s
r
t
√
s−t√
t−rdt = π
(change the variable of integration to w =
√
s−t/
√
s−r). However,
∫
R
r
tg(t)
√
t−rdt = −∫
R
r
(t−r)/g′(t) dt
and hence on differentiating, we have
∫
R
r
rg′(t)
√
t−rdt = −πr f (r).
Therefore,
f (r) = −
π ∫
R
r
g′(t)
√
t−rdt.
The derivative lurking within this inversion formula is again a harbinger of instability in
the solution of the inverse problem.
..
Forward and Reverse Diﬀusion
Imagine a bar, identified with the interval [, π] of the x-axis, the lateral surface of which
is thermally insulated while its ends are kept always at temperature zero. The diffusion of
heat in the bar is governed by the one-dimensional heat equation
∂u
∂t = κ ∂u
∂x,
< x < π
where u(x, t) is the temperature at position x and time t, and κ is the thermal diffusivity.
If the initial temperature distribution in the bar is a function f (x), then the boundary and
initial conditions associated with this model are
u(, t) = ,
u(π, t) = ,
u(x,) = f (x).
In the forward diffusion problem the goal is to find, for a given future time T > , the
temperature distribution g(x) = u(x, T). Formal separation of variable techniques lead to
a solution of the form
u(x, t) =
∞
∑
n=
ane−κnt sin nx,


Linear Inverse Problems
where an are the Fourier coefficients of the initial temperature distribution
an = 
π ∫
π

f (s)sin ns ds.
The future temperature distribution is then seen to be, after some rearranging,
g(x) = ∫
π

k(x, s)f (s) ds,
where
k(x, s) = 
π
∞
∑
n=
e−κnT sin nx sin ns.
A high degree of smoothing is a notable feature of the forward diffusion process. Specif-
ically, the factors e−κnT in the transformation have the effect of severely damping
high-frequency components in the initial temperature distribution f .
A corresponding reverse diffusion process is immediately suggested, namely the retro-
diction of the initial temperature distribution f , from knowledge of the later temperature
distribution g. In this inverse problem one finds that
f (x) = 
π
∞
∑
n=
eκnT ∫
π

g(s)sin ns ds.
(.)
The contrast with the forward problem is striking: now high-frequency components in g
are amplified by the huge factors eκnT. Also note that the inverse problem is soluble only
for a restricted class of functions g – those for which the series (> .) converges in L[, π].
As will be seen in the next section, the reverse diffusion process is a useful metaphor in the
discussion of deblurring.
..
Deblurring as an Inverse Problem
Cameras and other optical imagers capture a scene, or object, and convert it into an imper-
fect image. The object may be represented mathematically by a function f : R→R, that
codes, for example, gray scale or intensity. The image produced by the device is a function
g : R→R, and the process may be phrased abstractly as g = K f , where K is an operator
modeling the operation of the imager. In a perfect imager, K = I the identity operator (recall
Mein Herr’s map!). The perfect model may be expressed in terms of the two-dimensional
delta distribution as
f (⃗x) = ∫∫
Rδ(⃗x −⃗ξ)f (ξ) dξ.
However, any physical imaging device blurs the object f into an image g, which in many
cases can be represented by
g(⃗x) = ∫∫
Rk(⃗x −⃗ξ)f (ξ)d⃗ξ
(.)
where k(⋅), the point spread function of the device, is some approximation of the delta
function centered at the origin. Theoretical examples of such approximations include the

Linear Inverse Problems 

tin-can function χR/(πR), where χR is the indicator function of the disk of radius R
centered at the origin, and the sinc and sombrero functions given in polar coordinates by
sinc(r, θ) = sin πr
πr
and
somb(r, θ) = J(πr)
πr
,
respectively, where Jis the Bessel function of first kind and order . A frequently occurring
model uses the Gaussian point spread function
k(r, θ) =

πσ e−r/σ .
The problem of deblurring consists of solving (> .) for the object f , given the blurred
image g. For a good introduction to deblurring, see [].
Reverse diffusion in two dimensions is a close cousin of deblurring. A basic tool in the
analysis is the D–Fourier transform defined for f : R→R and ⃗x, ⃗ω ∈Rby
̂f (⃗ω) = F{f }(⃗ω) = ∫
∞
−∞∫
∞
−∞e−i⟨⃗x, ⃗ω⟩f (⃗x) dxdx
with the inversion formula
f (⃗x) =

(π)∫
∞
−∞∫
∞
−∞ei⟨⃗x, ⃗ω⟩̂f (⃗ω) dωdω.
On integrating by parts one sees that
F{Δf }(⃗ω) = −∥⃗ω∥̂f (⃗ω),
where Δ is the Laplacian operator: Δ =
∂
∂x
+ ∂
∂x
. Consider now the initial value problem
for the D–heat equation
∂u
∂t = κΔu
⃗x ∈R,
t > ,
u(⃗x,) = f (⃗x).
Applying the Fourier transform yields the initial value problem
dU
dt = −κ∥⃗ω∥U,
U() = ̂f
where U(t) = ̂u(⋅, t) and hence U(t) = ̂f e−∥ω∥κt. The convolution theorem then gives
u(⃗x, t) = F−{e−∥ω∥κt ̂f } = ∫
∞
−∞∫
∞
−∞k(⃗x −⃗ξ)f (⃗ξ) dξdξ
where (using the integral result of [], .A)
k(⃗x)
= F−{e−ω
κte−ω
κt} =

π∫
∞
−∞∫
∞
−∞ei⟨⃗x, ⃗ω⟩e−(ω
+ω
)κt dωdω
=

π∫
∞
−∞eixω−ω
κtdω∫
∞
−∞eixω−ω
κtdω=

πκt e−(x
+x
)/(κt).


Linear Inverse Problems
The inverse problem of determining the initial distribution f (⃗x) = u(⃗x,), given the dis-
tribution g(⃗x) = u(⃗x, T) at a later time T > , is equivalent to solving the integral equation
of the first kind
g(⃗x) =

πκT ∫
∞
−∞∫
∞
−∞e−((x−ξ)+(x−ξ))/(κT) f (ξ, ξ) dξdξ,
which is in turn equivalent to the deblurring problem with Gaussian point spread function
Γσ(⃗x) =

πσ e−∥⃗x∥/σ 
having variance σ = κT. The idea of deblurring by reverse diffusion is developed in [].
..
Extrapolation of Band-Limited Signals
Extrapolation is a basic challenge in signal analysis. The Fourier transform, F, is the ana-
lytical workhorse in this field. It transforms a time signal f (t),−∞< t < ∞, into a
complex-frequency distribution ̂f (ω) via the formula
̂f (ω) = F{f }(ω) = ∫
∞
−∞f (t)e−iωt dt.
In a suitable setting, the time-to-frequency transformation may be inverted by the formula
(e.g., []):
f (t) = F−{̂f }(t) = 
π ∫
∞
−∞
̂f (ω)eiωt dt.
Any physically realizable detector is capable of picking up frequencies only in a limited
range, say ∣ω∣≤Ω. A signal f whose Fourier transform vanishes for ∣ω∣> Ω, for some
given Ω > , is called a band-limited signal. A detector that operates in the frequency band
−Ω ≤ω ≤Ω band-limits signals it collects, that is, it treats only χ[−Ω,Ω] ̂f , where
χ[−Ω,Ω](ω) = { ,
ω ∈[−Ω, Ω]
,
ω ∉[−Ω, Ω].
is the indicator function of the interval [−Ω, Ω]. Multiplication by χ[−Ω,Ω] in the frequency
domain is called a low-pass filter as only components with frequency ∣ω∣≤Ω survive the
filtering process.
Reconstruction of the full signal f is generally not possible as information in compo-
nents with frequency greater than Ω is unavailable. What is available is the signal
g = F−{χ[−Ω,Ω] ̂f }.
By the convolution theorem for Fourier transforms one then has
g = F−{χ[−Ω,Ω]} ∗f .
However,
F−{χ[−Ω,Ω]}(t) = 
π ∫
Ω
−Ω eiωtdω = sin Ωt
πt
.

Linear Inverse Problems 

The reconstruction (or extrapolation) of the full signal f given the detected signal g the
requires the solution of the convolution equation
g(t) = ∫
∞
−∞
sin (Ω(t −τ))
π(t −τ)
f (τ) dτ.
The problem of extrapolating a band-limited signal is then seen to be mathematically the
same as deblurring the effect of an instrument with the one-dimensional point spread
function
kΩ(t) = sin (Ωt)
πt
.
..
PET
CT-scanning with X-rays is an instance of transmission tomography. A decade and a half
prior to Cormack’s publications on transmission tomography, an emission tomography
technique, now known as PET (positron transmission tomography) was proposed []. In
PET, a metabolically active tracer in the form of a positron-emitting isotope is injected
into an area for which it has an affinity and taken up (metabolized) by an organ. The
isotope emits positrons that immediately combine with free electrons in so-called annihi-
lation events, which result in the ejection of two photons (γ-rays) along oppositely directed
collinear rays. When a pair of detectors located on an array surrounding the body pick up
the simultaneous arrival of two photons, one at each detector, respectively, an annihila-
tion event is assumed to have taken place on the segment connecting the two detectors.
In PET, the data collected from a very large number of such events is used to construct a
two-dimensional tomographic slice of the isotope distribution. Because the uptake of the
isotope is metabolically driven, PET is an effective tool for studying metabolism giving
it a diagnostic advantage over X-ray CT-scanning. A combination of an X-ray CT-scan
with a PET scan provides the diagnostician anatomical information (distribution of atten-
uation coefficient) and physiological information (density of metabolized tracer isotope),
respectively.
If f : R→R (we consider only a simplified version of D-PET) is the density
of the metabolized tracer isotope, then the number of annihilations occurring along the
coincidence line L connecting two detectors is proportional to the line integral
∫L f ds.
That is, the observed counts of annihilation events is measured by the Radon transform
of the density f . However, this does not take account of attenuation effects and can under
represent features of deep-seated tissue. If the attenuation distribution is μ(⋅,⋅), and the
pair of photons resulting from an annihilation event on the coincidence line L traverse


Linear Inverse Problems
oppositely directed rays L+ and L−of L, emanating from the annihilation site, then the
detected attenuated signal takes the form
g
= ∫L e−∫L+ μdue−∫L−μdu f ds
= e−∫L μdu∫L f ds.
The model operator may now be viewed as a bivariate operator K(μ, f ) = g, in which the
operator K(⋅, f ) is nonlinear and the operator K(μ,⋅) is linear. In soft tissue the attenuation
coefficient is essentially zero and therefore the solution of the inverse problem is accom-
plished by a Radon inversion of K(,⋅). PET scans may be performed in combination with
X-ray CT-scans; the CT-scan provides the attenuation coefficient, which may then be used
in the model above to find the isotope density. See [] for an extensive survey of emission
tomography.
..
Some Mathematics for Inverse Problems
▷Philosophy is written in that great book which ever lies before our
gaze – I mean the universe …. The book is written in the
mathematical language …without which one wanders
in vain through a dark labyrinth.
Galileo Galilei
Hilbert space is a familiar environment that is rich enough for a discussion of the chief
mathematical issues that are important in the theory of inverse problems. For the most part
we restrict our attention to real Hilbert spaces. The inner product and associated norm will
be symbolized by ⟨⋅,⋅⟩and ∥⋅∥, respectively:
∥x∥=
√
⟨x, x⟩.
We assume the reader is familiar with the basic properties of inner product spaces (see,
e.g., [ [], Chap. I]), including the Cauchy–Schwarz inequality
∣⟨x, y⟩∣≤∥x∥∥y∥.
A Hilbert space H is complete, that is, Cauchy sequences in H converge:
if
lim
n,m→∞∥xn −xm∥= ,
then
∥xn −x∥→,
for some x ∈H. The smallest (in the sense of inclusion) Hilbert space that contains a given
inner product space is known as the completion of the inner product space. (Every inner
product space has a unique completion.)

Linear Inverse Problems 

The space, denoted L[a, b], of measurable functions on an interval [a, b] whose
squares are Lebesgue integrable, with inner product
⟨f , g⟩= ∫
b
a
f (t)g(t) dt,
is the prototypical example of a Hilbert space. The Sobolev space of order m, H(m)[a, b], is
the completion with respect to the norm
∥f ∥= (
m
∑
k=
∥f (k)∥
)
/
,
associated with the inner product
⟨f , g⟩m =
m
∑
k=
⟨f (k), g(k)⟩,
of the space of functions having m continuous derivatives on [a, b]. Here ⟨⋅,⋅⟩and ∥⋅∥
are the L[a, b] norm and inner product; of course, H()[a, b] = L[a, b].
Two vectors x and y in a Hilbert space H are called orthogonal, denoted x ⊥y, if
⟨x, y⟩= . The Pythagorean Theorem
x ⊥y ⇐⇒∥x + y∥= ∥x∥+ ∥y∥,
is a key notion that suggests the transfer of familiar geometrical ideas from Euclidean space
to Hilbert space. The orthogonal complement of a set S is the closed subspace
S⊥= {x ∈H : x ⊥s,
for all
s ∈S}.
It is not difficult to show that if S is a subspace, then S⊥⊥= S, where ¯S is the closure of S, that
is, the smallest closed subspace that contains S. A closed subspace S of a Hilbert space H
engenders a Cartesian decomposition of H, symbolized by H = S ⊕S⊥, meaning that each
x ∈H has a unique representation of the form x = x+ x, where x∈S is the projection
of x onto S:
∥x −x∥= inf {∥x −y∥: y ∈S},
and similarly xis the projection of x onto S⊥. The projection of a vector x onto a closed
subspace S is denoted by PSx.
A set of mutually orthogonal vectors each of which has unit norm is called an orthonor-
mal set. An orthonormal set S is complete if S⊥= {}. A complete orthonormal system
for a Hilbert space is a sequence of vectors in H, which is complete and orthonormal.
For example, {sin nπt : n = ,,, . . . } is a complete orthonormal system for the Hilbert
space L[,]. Each vector x ∈H has a convergent Fourier expansion in terms of a complete
orthonormal system {φn}∞
n=for H:
x =
∞
∑
n=
⟨x, φn⟩φn,


Linear Inverse Problems
of which Parseval’s identity is an immediate consequence
∥x∥=
∞
∑
n=
∣⟨x, φn⟩∣.
...
Weak Convergence
“Weak” notions are crucial to our development of mathematics for inverse problems. Sup-
pose the class of functions of interest forms a real Hilbert space H with an inner product
⟨⋅,⋅⟩and associated norm ∥⋅∥. A functional is a mapping from H to R. It is helpful to think
of a functional as a measurement on elements of H. Proportionality and additivity are nat-
ural features of most measuring processes. A functional F : H →R with these features,
that is, satisfying
F(αx + βy) = αF(x) + βF(y)
where α and β are scalars and x, y ∈H, is called a linear functional. Another common, and
highly desirable feature of a measurement process is continuity: elements of H, which are
nearly the same should result in measurements that are nearly the same. In mathematical
terms, a functional F is continuous if, as n →∞,
∥xn −x∥→
implies
∣F(xn) −F(x)∣→.
For example, if the Hilbert space is L[, T], the space of square integrable functions on
[, T], then the average value functional,
F(x) = 
T ∫
T

x(τ) dτ,
is a continuous linear functional. (This is an immediate consequence of the Cauchy–
Schwarz inequality.)
The Riesz Representation Theorem characterizes continuous linear functionals on a
Hilbert space:
▷
A continuous linear functional F on H has a unique representation of the form
F(x) = ⟨x, φ⟩
for some φ ∈H.
This result is so fundamental that it is worthwhile to sketch a micro-proof. We may assume
that F is not identically zero (otherwise, take φ = ) and hence there is a z ∈H, with
F(z) = , which is orthogonal to the closed subspace
N = {x ∈H : F(x) = }.
Then x −F(x)z ∈N for all x ∈H, and hence φ = z/∥z∥fits the bill
= ⟨x −F(x)z, z/∥z∥⟩= ⟨x, φ⟩−F(x).

Linear Inverse Problems 

Any two distinct vectors in H are distinguishable by some measurement in the form
of a continuous linear functional. Indeed, if ⟨x −y, φ⟩= for all φ ∈H, then x = y
(set φ = x −y). However, it is possible for a sequence of vectors {xn}, which does not
converge in H to any vector, nevertheless to be ultimately indistinguishable from some
vector x by bounded linear functionals. This is the idea of weak convergence. We say that
{xn} converges weakly to x, symbolized xn ⇀x, if ⟨xn, φ⟩→⟨x, φ⟩for every φ ∈H. The
simple identity
∥x −xn∥= ⟨x −xn, x −xn⟩= ∥x∥+ ∥xn∥−⟨xn, x⟩
shows that if xn ⇀x and ∥xn∥→∥x∥, then xn converges strongly to x, that is, ∥xn −x∥→.
In a Hilbert space, every sequence of vectors whose norms are uniformly bounded has
a subsequence that is weakly convergent (e.g., [], p. ). We note that any complete
orthonormal system {φn} converges weakly to zero, for by Parseval’s identity
∑
n
∣⟨x, φn⟩∣= ∥x∥,
and hence ⟨x, φn⟩→as n →∞for each x ∈H.
A set is called weakly closed if it contains the weak limit of every weakly convergent
sequence of vectors in the set. Hilbert spaces have the following feature (see e.g., []),
which is fundamental in the theory of optimization:
▷
Suppose C is a weakly closed convex subset of a Hilbert space H. For each x ∈H there is a unique
vector PC(x) ∈C such that
∥x −PC(x)∥= inf{∥x −u∥: u ∈C}.
PC(x) is called the metric projection of x onto C. It can be shown that a closed convex set
is also weakly closed.
...
Linear Operators
A bounded linear operator from a Hilbert space Hinto a Hilbert space His a mapping
K : H→H, which is linear, K(αx + βy) = αKx + βKy, and for which the number
∥K∥= sup {∥Kx∥/∥x∥: x ≠}
is finite. Note that we have used the same symbol for the norm in each of the spaces; this
generally will be our practice in the sequel. If K is a bounded linear operator, then K is
(uniformly) continuous since
∥Kx −Ky∥= ∥K(x −y)∥≤∥K∥∥x −y∥.
For our purposes, the most cogent example of a bounded linear operator is an integral
operator K : L[a, b] →L[c, d] of the form
K f (t) = ∫
b
a
k(t, s)f (s) ds,
c ≤t ≤d,
(.)


Linear Inverse Problems
where k(⋅,⋅) ∈L([c, d] × [a, b]) is called the kernel of the integral operator. The kernel is
called degenerate if it has the form
k(t, s) =
m
∑
j=
Tj(t)S j(s)
where the Tj and the S j are each linearly independent sets of functions of a single variable.
In this case the range, R(K), of the operator K is the finite-dimensional subspace
R(K) = span{Tj : j = , . . . , m}
and
K f (t) =
m
∑
j=
⟨k(t,⋅), f ⟩Tj(t),
where ⟨⋅,⋅⟩is the L[a, b] inner product.
The adjoint of a bounded linear operator K : H→His the bounded linear operator
K∗: H→H, which satisfies
⟨Kx, y⟩= ⟨x, K∗y⟩
for all x ∈Hand y ∈H. For example, by changing the order of integration, one can see
that the adjoint of the integral operator (> .) is
K∗g(s) = ∫
d
c
k(u, s)g(u) du.
A bounded linear operator K : H →H is called self-adjoint if K∗= K. The null-space of a
bounded linear operator K : H→His the closed subspace
N(K) = {x ∈H: Kx = }.
Note that N(K∗K) = N(K) for if x ∈N(K∗K), then
= ⟨K∗Kx, x⟩= ⟨Kx, Kx⟩= ∥Kx∥.
There are fundamental relationships between the null-space and the range
R(K) = {Kx : x ∈H},
of a linear operator and its adjoint. In fact, y ∈R(K)⊥if and only if
= ⟨Kx, y⟩= ⟨x, K∗y⟩
for all x ∈H, and hence R(K)⊥= N(K∗). It follows that R(K) = R(K)⊥⊥= N(K∗)⊥.
Replacing K by K∗in these relations (noting that K∗∗= K), we obtain the four fundamental
relationships:
R(K)⊥= N(K∗),
R(K) = N(K∗)⊥
R(K∗)⊥= N(K),
R(K∗) = N(K)⊥.
Examples in a previous section have highlighted the unstable nature of solutions of
inverse problems. This instability is conveniently phrased in terms of linear operators

Linear Inverse Problems 

that are unbounded. Unbounded linear operators are typically defined only on restricted
subspaces of the Hilbert space. For example, L[, π] contains discontinuous, and hence
nondifferentiable, functions. But the differentiation operator may be defined on the proper
subspace of L[, π] consisting of differentiable functions with derivatives in L[, π]. This
differentiation operator is unbounded since
∥
n sin nt∥

=
π
n→
as
n →∞,
while
∥d
dt ( 
n sin nt)∥

= π
n→∞
as
n →∞.
The reverse diffusion process is also governed by an unbounded operator. As seen in
(> .), the operator L, which maps the temperature distribution g(x) = u(x, T) to the
initial temperature distribution f (x) = u(x,) is defined on the subspace
D(L) = {g ∈L[, π] :
∞
∑
n=
eknT∣bn∣< ∞},
where
bn = 
π ∫
π

g(s)sin ns ds.
L is unbounded because the functions φm(s) = sin ms reside in D(L) and satisfy ∥φm∥=
π/, but by the orthogonality relationships,
Lφm = ekmTφm,
and hence ∥Lφm∥= ekmTπ/→∞as m →∞.
...
Compact Operators and the SVD
A bounded linear operator K : H→Hof the form
Kx =
r
∑
j=
⟨x,v j⟩u j,
(.)
where {u j}r
j=is a linearly independent set of vectors in Hand {v j}r
j=is a set of vectors
in H, is called an operator of finite rank (with rank = r). For example, an integral operator
on L[a, b] with a degenerate kernel is an operator of finite rank. Finite rank operators
transform weakly convergent sequences into strongly convergent sequences: if xn ⇀x,
then
Kxn =
r
∑
j=
⟨xn,v j⟩u j →
r
∑
j=
⟨x,v j⟩u j = Kx.
More generally, a linear operator is called compact if it enjoys this weak-to-strong conti-
nuity, that is, if xn ⇀x implies that Kxn →Kx. In terms of our metaphor of bounded


Linear Inverse Problems
linear functionals as measurements, one could say that if the linear operator K model-
ing an inverse problem is compact, and if all measurements on a sequence of functions
{xn} are ultimately indistinguishable from the corresponding measurements on x, then
the model values Kxn are ultimately indistinguishable from Kx. That is, causes, which
are ultimately indistinguishable by linear measurement processes result in effects that are
ultimately indistinguishable. It is therefore not surprising that compact operators occur
frequently in models of linear inverse problems.
Erhard Schmidt’s theory of singular functions, now called the singular value decom-
position (SVD), is the most versatile and effective tool for the analysis of compact linear
operators. (The SVD has been rediscovered several times in various contexts; for the
curious history of the SVD see [].) The SVD extends the representation (> .) in a
particularly useful way. A singular system {v j,u j; μj}∞
j=for a compact linear operator K
bundles a complete orthonormal system {v j}∞
j=for N(K)⊥, consisting of eigenvectors of
K∗K; a complete orthonormal system {uj}∞
j=for N(K∗)⊥= R(K), consisting of eigen-
vectors of KK∗; and a sequence of positive numbers μj, called singular values of K. The
singular values and singular vectors are tied together by the relationships
Kv j = μju j,
K∗u j = μjv j,
j = ,,, . . . .
Every compact linear operator has a singular system and the action of the operator may be
expressed in terms of the SVD as
Kx =
∞
∑
j=
μj⟨x,v j⟩u j
(.)
In case K has finite rank r, this sum terminates at j = r, and otherwise
μj →,
as
j →∞.
We shall see that this fact is singularly important in the analysis of inverse problems.
As an example of the SVD of a non-self-adjoint compact operator, consider the integral
operator K : L[, π] →L[, π] defined by
(K f )(t) = ∫
π

h(t,u)f (u) du
where
h(t,u) = { ,
≤u ≤t
,
t < u ≤π.
One can verify that a singular system {v j,u j; μj}∞
j=for this operator is
v j(t) =
√

π cos (j + 

t),
u j(s) =
√

π sin (j + 

s),
μj =

j + .
A compact operator has closed range if and only if it has finite rank. This follows from
the SVD and the open mapping theorem (e.g., [], p. ). Indeed, if K is compact and
R(K) is closed, then the restricted operator K : N(K)⊥→R(K) is one-to-one and onto,

Linear Inverse Problems 

and hence has a bounded inverse. That is, there is a positive number m such that ∥Kx∥≥
m∥x∥for all x ∈N(K)⊥. But then, by (> .),
μj = μj∥u j∥= ∥Kv j∥≥m∥v j∥= m > ,
and hence K has only finitely many singular values for otherwise μj →. This result is
highly significant in inverse theory for it says that finite rank linear models, when pushed
too far toward the limiting case of an operator of infinite rank, will inevitably result in
instability.
In , Emil Picard [] established a criterion that characterizes the existence of
solutions of an equation of the first kind
Kx = y,
(.)
where K is a compact linear operator. Picard’s criterion plays a role in inverse theory anal-
ogous to that which the Fredholm alternative plays for integral equations of the second
kind.
Since {v j} is a complete orthonormal system for N(K)⊥, the series
∞
∑
j=
∣⟨x,v j⟩∣
is convergent (and equals ∥PN(K)⊥x∥). However, if y = Kx ∈R(K), then
⟨x,v j⟩= μ−
j ⟨x, K∗u j⟩= μ−
j ⟨Kx,u j⟩= μ−
j ⟨y,u j⟩,
and so
∞
∑
j=
μ−
j ∣⟨y,u j⟩∣< ∞
is a necessary condition for y ∈R(K).
On the other hand, this condition guarantees that the series
x =
∞
∑
j=
μ−
j ⟨y,u j⟩v j
(.)
is convergent in R(K)⊥and the singular value relations show that Kx = PN(K∗)⊥y. Taken
together these results establish the Picard Criterion:
y ∈R(K) ⇔y ∈N(K∗)⊥
and
∞
∑
j=
μ−
j ∣⟨y,u j⟩∣< ∞.
(.)
If y satisfies Picard’s criterion, then y = Kx where x is given by (> .).
...
The Moore–Penrose Inverse
If y ∉R(K), then the equation Kx = y has no solution, but this should not prevent one
from doing the best one can to try to solve the problem. Perhaps the best that can be done


Linear Inverse Problems
is to seek a vector u that is as near as possible to serving as a solution. A vector u ∈Hthat
minimizes the quadratic functional
F(x) = ∥Kx −y∥
is called a least squares solution of Kx = y. It is not hard to see that a least squares solution
exists if and only if y belongs to the dense subspace R(T) + R(T)⊥of H. Also, as the
geometry suggests, u is a least squares solution if and only if y −Ku ∈R(K)⊥= N(K∗),
and hence least squares solutions are vector v that satisfy the so-called normal equation
K∗Kv = K∗y.
(.)
Furthermore, the solution set of (> .) is closed and convex, and therefore contains a
unique vector nearest to the origin (i.e., of smallest norm), say v†. This smallest norm
least squares solution v† lies in N(K)⊥, for otherwise Pv† ≠, where P is the orthogonal
projector onto N(K). The Pythagorean theorem then gives
∥v†∥= ∥v† −Pv†∥+ ∥Pv†∥.
But, since K∗KPv† = , this implies that v† −Pv† is a least squares solution with norm
smaller than that of v†. This contradiction ensures that v† ∈N(K)⊥.
The operator K† : D(K†) →N(K)⊥, which associates with each y in the dense sub-
space D(K†) = R(K) + R(K)⊥of Hthe unique minimum norm least squares solution
K†y ∈N(K)⊥of the equation Kx = y is called the Moore–Penrose generalized inverse
of K. (E.H. Moore died when Roger (now Sir Roger) Penrose was an infant; [] tells the
story of how the names of both men came to be associated with the generalized inverse.)
If K is a compact linear operator with SVD {v j,u j; μj} and y ∈D(K†), then the vector
∞
∑
j=

μj
⟨y,u j⟩v j
is well defined by Picard’s criterion, resides in N(K)⊥, and is a least squares solution.
Therefore,
K†y =
∞
∑
j=

μj
⟨y,u j⟩v j.
(.)
The operator K† so-defined is linear, but it is unbounded (unless K has finite rank) since
∥un∥= ,
but
∥K†un∥= /μn →∞
as
n →∞.
(.)
There is an immense literature on generalized inverses; see [] for a start.
...
Alternating Projection Theorem
Draw a pair of intersecting lines. Take a point at random in the plane spanned by the lines
and project it onto the first line. Then project that point onto the second line, and continue
in this manner projecting alternately onto each line in turn. It soon becomes apparent that

Linear Inverse Problems 

the sequence of points so generated zigzags and converges to the point that is common to
both lines. In , von Neumann showed the same behavior for two closed subspaces S
and Sof a Hilbert space H. Namely, for each x ∈H,
(PSPS)nx →PS∩Sx
as
n →∞,
where PW stands for the orthogonal projector onto the closed subspace W. This result
extends easily to the case where Sand Sare translates of closed subspaces, that is, closed
affine sets (see e.g., [,] for proofs). In fact, a modification of the method, due to Boyle
and Dykstra (see [], p. ), provides a sequence that converges to the metric projection
onto the intersection of a finite collection of closed convex sets.
Stefan Kaczmarz [] developed an alternating projection algorithm, independently of
von Neumann, for approximating solutions of underdetermined systems of linear algebraic
equations. (See [] concerning Kaczmarz’s early and tragic demise.) A solution ⃗x ∈Rn of
a system of m linear equations in n unknowns with coefficient matrix A and right-hand
side ⃗b lies in the intersection of the hyperplanes
πi = {⃗x ∈Rn : ⟨⃗ai, ⃗x⟩= bi},
i = ,, . . ., m,
where ⃗ai is the ith row vector of A. Kaczmarz’s algorithm, which consists of successively
and cyclically projecting onto these hyperplanes, produces a sequence of vectors that con-
verges to that vector in the intersection of the hyperplanes, which is nearest to the initial
approximation (see [] for a complete treatment of the method). Kaczmarz’s method has
come to be known as ART (the algebraic reconstruction technique) in the tomography
community.
.
Numerical Methods
▷All of exact science is dominated by
the idea of approximation.
Bertrand Russell
..
Tikhonov Regularization
The unboundedness of the operator K†, displayed in (> .), is a fundamental chal-
lenge when solving linear inverse problems of the form Kx = y. This unboundedness is
manifested as instability when the data vector y contains errors, which is always the case
in practical circumstances as the data result from observation and measurement. Small
errors in high-order singular components ⟨y,un⟩(n large), will be magnified by the factor
/μn in the representation (> .), resulting in large deviations in the computed solu-
tion. Such instabilities in numerical solutions were noticed from the very beginning of the


Linear Inverse Problems
use of digital computers to solve linear inverse problems (see [] for examples and ref-
erences). The development of theoretical strategies to mitigate this instability is known as
regularization theory.
One way to stabilize the solution process is to restrict the notion of solution. Tikhonov’s
classic result [] [] of is an instance of this idea. In that paper Tikhonov treated
the inverse problem of determining the spatial distribution of a uniform star-shaped mass
lying below the horizontal surface from measurements of the gravitational potential on
the surface. He showed that the inverse problem becomes well posed if the forward oper-
ator is restricted to a certain compact set. Another approach is to modify the forward
operator itself without a restriction on its domain. In what has come to be known as
Tikhonov regularization the notion of solution is generalized to the minimum norm least
squares solution, which is unstable, but a stable approximation to this generalized solution,
depending on a regularization parameter, is constructed.
The idea of Tikhonov regularization may be introduced from either an algebraic or a
variational viewpoint. Algebraically, the method, in its simplest form, consists in replacing
the normal > equation (.) with the second kind equation
K∗Kv + αv = K∗y,
(.)
where α is a positive parameter. The key point is that the problem of solving (> .) is well
posed. Indeed,
∥x∥∥K∗K + αI∥≥⟨(K∗K + αI)x, x⟩= ∥Kx∥+ α∥x∥≥α∥x∥,
and hence (K∗K + αI)−is a bounded linear operator, in fact, ∥(K∗K + αI)−∥≤/α. The
significance of this fact is that for fixed α > , the approximation
xα = (K∗K + αI)−K∗y
(.)
depends continuously on y. Specifically, suppose yδ is an observed version of y satisfying
∥y −yδ∥≤δ, and let xδ
α be the approximation formed using this approximate data, that is,
xδ
α = (K∗K + αI)−K∗yδ.
From the SVD we have
xα −xδ
α =
∞
∑
j=
μj
μ
j + α ⟨y −yδ,u j⟩v j,
and hence
∥xα −xδ
α∥
=
∞
∑
j=
μ
j
μ
j + α

μ
j + α ∣⟨y −yδ,u j⟩∣
≤
α ∑∞
j=∣⟨y −yδ,u j⟩∣≤δ/α.
(.)
If the minimum norm least squares solution K†y satisfies the source condition K†y =
K∗Kw, for some w ∈H, then one can show that
K†y −xα = α(K∗K + αI)−K∗Kw

Linear Inverse Problems 

and hence
∥K†y −xα∥= α
∞
∑
j=
⎛
⎝
μ
j
μ
j + α
⎞
⎠

∣⟨w,v j⟩∣≤α∥w∥.
(.)
Combining this with (> .), we see that if K†y ∈R(K∗K), then
∥xδ
α −K†y∥≤δ/√α + O(α).
Therefore, an a priori choice of the regularization parameter of the form
α = α(δ) = Cδ/,
(.)
yields a convergence rate of the form
∥xδ
α(δ) −K†y∥= O(δ/).
(.)
This two thirds power rate is an asymptotic “brick wall” for Tikhonov regularization in
the sense that it is impossible to uniformly improve it to a o(δ/) rate unless the compact
operator K has finite rank (see []). Roughly speaking, this says that the best one can hope
for is m-digit accuracy in the solution if there is m-digit accuracy in the data.
The Tikhonov approximation (> .) has a variational characterization that is useful
in both theoretical analysis and computational implementation. The > equation (.) that
characterizes the Tikhonov approximation is the Euler equation for the functional
Fα(⋅; y) = ∥K ⋅−y∥+ α∥⋅∥,
(.)
and hence the approximation (> .) is a global minimizer of (> .). This opens the
possibility of applying standard optimization techniques for calculating the Tikhonov
approximation. Next we illustrate the usefulness of the variational characterization in a
convergence analysis for an a posteriori selection technique for the regularization param-
eter known as Morozov’s discrepancy principle.
The a priori parameter selection criterion (> .) is of theoretical interest as it
gives information on the order of magnitude of the regularization parameter that can be
expected to result in a convergent procedure. However, a posteriori methods of choos-
ing the regularization parameter that depend on the actual progress of the computations
would be expected to lead to more satisfactory results. Morozov’s discrepancy principle []
is the earliest parameter choice strategy of this type. Morozov’s idea (which was presaged
by Phillips [], []) is to choose the regularization parameter in such a way that the size
of the residual ∥Kxδ
α −gδ∥is equal to error level in the data:
∥Kxδ
α −yδ∥= δ.
(.)
It should be recognized that this condition contains some “slack” as δ, the bound for the
data error, might not be tight. Nevertheless, this choice is not only possible, but it leads to
a convergent procedure, as we now show.


Linear Inverse Problems
If ∥yδ∥> δ, that is, there is more signal than noise in the data, and if y ∈R(K), then
there is a unique positive parameter α satisfying (> .). To see this, we use the SVD
∥Kxδ
α −yδ∥
=
∞
∑
j=
⎛
⎝
α
μ
j + α
⎞
⎠

∣⟨yδ,u j⟩∣+ ∥Pyδ∥
(.)
where P is the orthogonal projector of Honto R(K)⊥. From this we see that the real
function
ψ(α) = ∥Kxδ
α −yδ∥
is a continuous, strictly increasing function of α satisfying (since Py = )
lim
α→+ ψ(α) = ∥Pyδ∥= ∥Pgδ −Pg∥≤∥yδ −y∥≤δ
and
lim
α→∞ψ(α) = ∥yδ∥> δ.
The intermediate value theorem, then guarantees a unique α = α(δ) satisfying (> .).
We now show that the choice α(δ) as given by the discrepancy method (> .) leads
to a regular scheme for approximating K†y:
xδ
α(δ) →K†y as δ →.
To this end it is sufficient to show that for any sequence δn →there is a subsequence,
which we will denote by {δk}, that satisfies xδk
α(δk) →K†y. The argument relies on the fol-
lowing previously discussed facts: norm-bounded sequences contain a weakly convergent
subsequence, and weak convergence along with convergence of the norms implies strong
convergence.
We assume that y ∈R(K), that K : H→His a compact linear operator, and we let
x = K†y. That is, x is the unique vector in N(K)⊥satisfying Kx = y.
The variational characterization of the Tikhonov approximation xδ
α(δ) as the global
minimizer of the quadratic functional Fα(⋅; yδ) (see (> .)) implies that
Fα(δ) (xδ
α(δ); yδ) ≤Fα(δ)(x; yδ),
that is,
δ+ α(δ)∥xδ
α(δ)∥

= ∥Kxδ
α(δ) −yδ∥

+ α(δ)∥xδ
α(δ)∥

≤Fα(δ)(x) = ∥y −yδ∥+ α(δ)∥x∥
≤δ+ α(δ)∥x∥
and hence ∥xδ
α(δ)∥≤∥x∥. Therefore, for any sequence δn →there is a subsequence δk →
with xδk
α(δk) ⇀w, for some w. But
xδ
α(δ) = K∗(KK∗+ α(δ)I)−yδ ∈R(K∗) ⊆N(K)⊥

Linear Inverse Problems 

and N(K)⊥is weakly closed, and so w ∈N(K)⊥. Furthermore,
∥Kxδk
α(δk) −yδk∥→
and hence Kxδk
α(δk) →y. But as K is compact, Kxδk
α(δk) →Kw and it follows that Kw = y
and w ∈N(K)⊥, that is, w = x. Since ∥xδk
α(δk)∥≤∥x∥, we then have
∥x∥= lim
k→∞⟨xδk
α(δk), x⟩≤lim
k→∞
∥xδk
α(δk)∥⋅∥x∥
and therefore
∥x∥≤lim
k→∞
∥xδk
α(δk)∥≤limk→∞∥xδk
α(δk)∥≤∥x∥.
So we have shown that xδk
α(δk) ⇀x and ∥xδk
α(δk)∥→∥x∥, and hence xδk
α(δk) →x,
completing the proof.
It can be shown that, under the source condition x ∈R(K∗), Tikhonov’s method with
parameter choice by the discrepancy principle (> .) achieves an asymptotic order of
accuracy O(
√
δ), however a swifter rate of o(
√
δ) is generally impossible except in the
case when K has finite rank []. Engl and Gfrerer (see [, Chap. ]) have developed a
modification of the discrepancy principle that achieves the optimal order of convergence.
Our sketch of the basic theory of Tikhonov regularization has assumed that the regu-
larization functional, which augments the least square objective functional ∥K⋅−y∥is (the
square of) a norm. (Note however that while the same symbol is used for the norm in each
of the spaces Hand H, these norms may be distinct. In his original paper [] Tikhonov
used a Sobolev norm on the solution space and an Lnorm on the data space.) Phillips [],
in a paper that barely predates that of Tikhonov, used a regularizing semi-norm – the L
norm of the second derivative. In all of these cases the equation characterizing the regu-
larized approximation is linear. However, certain non-quadratic regularizing functionals,
leading to nonlinear problems for determining the regularized approximation, are found to
be effective in imaging science. Of particular note is the total variation, or TV-functional:
TV(u) = ∫Ω ∣∇u∣,
where u : Ω ⊆Rn →R. Regularization now consists of minimizing the augmented least
squares functional
Fα(u) = ∥Ku −y∥
L(Ω) + αTV(u),
where K is the identity operator for denoising problems, while for deblurring problems K
is the blurring operator associated with a known point spread function. A full exposition
may be found in [].
..
Iterative Regularization
Ordinary Tikhonov regularization consists in minimizing the functional
Fα(z) = ∥Kz −y∥+ α∥z∥


Linear Inverse Problems
for a range of positive regularization parameters α. In iterated Tikhonov regularization,
α > is fixed, an initial approximation xis selected (we take x= for simplicity; a
general initial approximation requires only small modifications to the arguments), and
successive approximations are updated by a multi-stage optimization scheme in which the
nth approximation is chosen to minimize the functional
Fn(z) = ∥Kz −y∥+ α∥z −xn−∥,
n = ,,, . . .
(.)
This results in the iterative method
xn = (K∗K + αI)−(αxn−+ K∗y),
n = ,,, . . . .
The conventional proof of the convergence of iterated Tikhonov regularization uses spec-
tral theory. (See [] for the more general case of nonstationary iterated Tikhonov regu-
larization.) However, the convergence of the method is also an immediate consequence of
the alternating projection theorem, as we now show.
Let H be the product Hilbert space H× Hwith norm ∣⋅∣given by
∣(x, y)∣= ∥y∥+ α∥x∥,
where α is a fixed positive constant. Note that the graph
G = {(x, Kx) : x ∈H}
is a closed subspace of H. For a given y ∈H, let
Ly = {u ∈H: Ku = Py},
where P is the orthogonal projector of Honto R(K), be the set of least squares solutions of
Kx = y. One sees that y ∈D(K†) if and only if Ly is nonempty. If Ly = H×{Py}, then Ly
is a closed affine set in the Hilbert space H, and y ∈D(K†) ⇔Ly ∩G ≠ø. Furthermore,
PLy∩G(, y) = (K†y, Py),
where PW stands for the metric projector of H onto a closed convex set W ⊆H.
From the variational characterization (> .), one sees that
PLy(x, Kx) = PLy(,) = (, Py)
and PG(, Py) = (x, Kx), therefore (x, Kx) = PGPLy(x, Kx), and generally
(xn, Kxn) = PGPLy(xn−, Kxn−) = ⋅⋅⋅= (PGPLy)n(, y).
This process of projecting in alternate fashion in the space H is illustrated in > Fig. -:
The alternating projection theorem then gives
(xn, Kxn) →PLy∩G(, y) = (K†y, Py),
as
n →∞.
If xδ
n are defined as in (> .), with y replaced by yδ, then it is not difficult to see that
∥xn −xδ
n∥≤√n∥y −yδ∥,

Linear Inverse Problems 

⊡Fig. -
The geometry of iterated regularization
and hence if ∥y−yδ∥≤δ and n = n(δ) →∞as δ →, in such as manner that
√
n(δ)δ →,
then xδ
n(δ) →K†x.
There are many other iterative methods for regularization of ill-posed problems (see
[,]). Perhaps the simplest is based on the observation that for any λ > , the subspace
N(K)⊥is invariant under the mapping
F(z) = (I −λK∗K)x + λK∗y,
and K†y is the unique fixed point of F in N(K)⊥. Furthermore, if < λ < /∥K∗K∥, then
F is a contraction and hence the iterative method
xn+= (I −λK∗K)xn + λK∗y
(.)
converges to K†y for any x∈N(K)⊥. This method was studied for Fredholm integral
equations of the first kind by Landweber and independently by Fridman. It has since
become known as Landweber iteration []. For this method one can show easily that if
∥y −yδ∥≤δ, and xδ
n represents the approximation obtained by (> .) with y replaced by
yδ, then xδ
n(δ) →K†y, if
√
n(δ)δ →.
The theory of Landweber iteration has been developed for nonlinear operators, includ-
ing a stopping criterion based on the discrepancy principle, by Hanke et al. [] (see
also []). See [] for a very useful survey of iterative regularization methods.
..
Discretization
▷… numerical precision is the very soul of science …
D’Arcy Wentworth Thompson
The preceding discussion of regularization methods took place in the context of gen-
eral (infinite-dimensional) Hilbert space. However, practical numerical computations are


Linear Inverse Problems
necessarily finitary. Passing from general elements in an infinite-dimensional space to
finitely represented approximations involves a process of discretization. Discretization of
an ill-posed problem can lead to a well-posed finite-dimensional problem; however, this
discretized version generally will be ill-conditioned. Regularization methods are meant to
address this problem. There are two approaches to constructing computable regulariza-
tions of linear inverse problems; one could handle the ill posedness by first regularizing
the infinite-dimensional problem and then discretizing the result, or one could discretize
the original problem and then regularize the resulting ill-conditioned finite-dimensional
problem. We give a couple of examples of discretized regularizations of the former type.
(For results on discretized versions of general regularization methods see [].)
A key point in the theoretical convergence analysis of regularization methods is the
interplay between the regularization parameter and the error properties of the data. For
example, assuming a source condition of the form K†y ∈R(K∗K), the balancing of the
rate O(α) for the infinite-dimensional Tikhonov approximation xα using “clean” data with
the stability bound δ/√α for the approximation using noisy data leads to the optimal rate
O(δ/) found in (> .). To obtain an overall convergence rate with respect to the error
in data for discretized approximations it is necessary to juggle three balls: a theoretical
convergence rate, a measure of the quality of the discretization, and a stability bound. In
both of the cases we consider, the measure of quality of the discretization will be denoted
by γm. In our first example, γm measures how well a given finite-dimensional subspace
Vm ⊆Hsupports the operator K, specifically,
γm = ∥K(I −Pm)∥,
where Pm is the orthogonal projector of Honto Vm. The smaller γm is, the better the
subspace Vm supports the operator K. In the second example, the discretization of a regu-
larized version of a Fredholm integral equation of the first kind is accomplished by applying
a quadrature method to the iterated kernel that generates the operator K∗K. In this case,
γm measures the quality of this quadrature. In both examples it is shown that it is theoret-
ically possible to match the optimal rate O(δ/) established for the infinite-dimensional
approximation in (> .).
The variational characterization (> .) immediately suggests a Ritz approach to dis-
cretization, namely minimization of the Tikhonov functional over a finite-dimensional
subspace. Note that the global minimum xα of the functional Fα(⋅; y) on Hmay be
characterized by the condition
⟨Kxα −Kx, Kv⟩+ α⟨xα,v⟩= ,
for all v ∈H,
(.)
where x = K†y. The bilinear form defined on Hby
q(u,v) = ⟨Ku, Kv⟩+ α⟨u,v⟩
is an inner product on H, and (> .) may be succinctly expressed in terms of this inner
product as
q(xα −x,v) = 
for all v ∈H.

Linear Inverse Problems 

Suppose that {Vm}∞
m=is a sequence of finite-dimensional subspaces of Hsatisfying
V⊆V⊆V⊆⋅⋅⋅⊆H
and
∪∞
m=Vm = H.
The minimizer xα,m of Fα(⋅; y) over the finite-dimensional subspace Vm satisfies
q(xα,m −x,vm) = 
for all vm ∈Vm,
and hence
q(xα −xα,m,vm) = 
for all vm ∈Vm.
In other words, xα,m = Pmxα, where Pm is the projector of Honto Vm, which is orthogonal
in the sense of the inner product q(⋅,⋅).
If ∣⋅∣q denotes the norm on Hassociated with the inner product q(⋅,⋅), that is,
∣z∣
q = ∥Kz∥+ α∥z∥,
then, by the characteristic property of projectors,
∣xα −xα,m∣
q = ∣xα −Pmxα∣
q ≤∣xα −Pmxα∣
q,
where Pm is the projector of Honto Vm associated with the (original) inner product on H.
But then (since projectors are idempotent),
α∥xα −xα,m∥≤∣xα −xα,m∣
q
≤∥Kxα −KPmxα∥+ α∥(I −Pm)xα∥
= ∥K(I −Pm)xα∥+ α∥(I −Pm)xα∥
≤(γm + α)∥(I −Pm)xα∥,
where
γm = ∥K(I −Pm)∥.
Therefore,
∥xα −xα,m∥≤
√
+ γm/α
∥(I −Pm)xα∥.
If K†y satisfies the source condition x = K†y ∈R(K∗K), say, x = K∗Kw, then
(I −Pm)xα = (I −Pm)K∗(KK∗+ αI)−KK∗Kw,
and hence
∥(I −Pm)xα∥≤γm∥Kw∥.
If γm = O(αm), then we find from (> .),
∥K†y −xα,m∥= O(αm).
In the case of approximate data yδ satisfying ∥y −yδ∥≤δ, one can show, using arguments
of the same type as above, that a stability bound of the same form as (> .) holds for the
finite-dimensional approximations:
∥xα,m −xδ
α,m∥≤δ/√α.


Linear Inverse Problems
Taking these results together, we see that if K†y ∈R(K∗K) and αm = αm(δ) is chosen in
such a way that αm = Cδ/and γm = O(αm), then the finite-dimensional approximations
achieve the optimal order of convergence:
∥K†y −xδ
α(m),m∥= O(δ/).
Quadrature is another common discretization technique. If a linear inverse problem is
expressed as a Fredholm integral equation of the first kind
y(s) = ∫
b
a
k(s, t)x(t)dt,
c ≤s ≤d,
mapping functions x ∈L[a, b] to function y ∈L[c, d], then the Tikhonov approximation
xα is the solution of the well-posed Fredholm integral equation of the second kind
∫
d
c
k(u, s)y(u)du = αxα(s) + ∫
b
a
̃k(s, t)xα(t)dt,
a ≤s ≤b,
where the iterated kernel ̃k(⋅,⋅) is given by
̃k(s, t) = ∫
d
c
k(u, s)k(u, t)du,
a ≤s, t ≤b.
If a convergent quadrature scheme with positive weights {w(m)
j
}
m
j=, and nodes {u(m)
j
}
m
j=,
is applied to the iterated kernel, a degenerate kernel
̃km(s, t) =
m
∑
j=
w(m)
j
k (u(m)
j
, s) k (u(m)
j
, t)
results, converting the infinite-dimensional Tikhonov problem into the finite rank problem
αxα,m + ̃Kmxα,m = K∗y
(.)
where
̃Kmz =
m
∑
j=
w(m)
j
⟨k j, z⟩k j
and
k j(s) = k (u(m)
j
, s).
The problem (> .) is equivalent to an m × m linear algebraic system with a unique
solution. The convergence of the approximations resulting from this finite system to the
infinite-dimensional Tikhonov approximation xα ∈L[a, b] depends on the number
γm = ∥̃Km −K∗K∥.
If α = α(m) →as m →∞and γm = O(α(m)), then it can be shown that xα(m),m →K†y.
Furthermore, a stability bound of the form O(δ/√α) holds under appropriate conditions,
and one can show that the optimal rate O(δ/) is achievable if the parameters governing
the finite-dimensional approximations are appropriately related []. A much more exten-
sive analysis along these lines is carried out in []. For more on numerical methods for
discrete inverse problems see [,].

Linear Inverse Problems 

.
Conclusion
▷Eventually, we reach the dim boundary …
There, we measure shadows …
Edwin Hubble
The first book devoted exclusively to the mathematical theory of inverse and ill-posed
problems was that of Tikhonov and Arsenin []. Kirsch [] is a fine treatment of the gen-
eral theory of inverse problems, and Engl et al. [] is the best comprehensive presentation
of the theory of regularization for inverse and ill-posed problems. Other useful books on
the general topic are [] and []. A number of books and survey articles treat inverse
theory in a specific context. Some of the areas treated include astronomy []; engineer-
ing []; geophysics []; imaging [, , , , , , , ]; mathematical physics [];
oceanography [, ]; parameter estimation []; indirect measurement []; and vibration
analysis [].
.
Cross-References
> Expansion Methods
> Inverse Scattering
> Iterative Solution Methods
> Large Scale Inverse Problems
> Numerical Methods for Variational Approach in Image Analysis
> Regularization Methods for Ill-Posed Problems
> Tomography
> Total Variation in Imaging
> Variational Approach in Image Analysis
References and Further Reading
. Ambarzumian V () On the derivation of the
frequency function of space velocities of the stars
from the observed radial velocities. Mon Not R
Astron Soc Lond :–
. Anderssen RS () Inverse problems: a prag-
matist’s approach to the recovery of information
from indirect measurements. Aust NZ Ind Appl
Math J :–
. Aster R, Borchers B, Thurber C () Param-
eter estimation and inverse problems. Elsevier,
Boston
. Bennett A () Inverse modeling of the ocean
and atmosphere. Cambridge University Press,
Cambridge
. Ben-Israel A () The Moore of the Moore
penrose
inverse.
Electron
J
Linear
Algebr
:–
. Bertero M, Boccacci P () Introduction to
inverse problems in imaging. IOP, London
. Bonilla L (ed) () Inverse problems and imag-
ing, LNM. Springer, Berlin
. Carasso A, Sanderson J, Hyman J () Digital
removal of random media image degradations by
solving the diffusion equation backwards in time.
SIAM J Numer Anal :–
. Chalmond B () Modeling and inverse prob-
lems in image analysis. Springer, New York


Linear Inverse Problems
. Chan TF, Shen J () Image processing and
analysis. SIAM, Philadelphia
. Chen Z, Xu Y, Yang H () Fast collocation
methods for solving ill-posed integral equations
of the first kind, Inverse Probl :()
. Cormack A () Representation of a function
by its line integrals, with some radiological appli-
cations I. J Appl Phys :–
. Cormack A () Representation of a function
by its line integrals, with some radiological appli-
cations II. J Appl Phys :–
. Cormack A. Computed tomography: some his-
tory and recent developments, in [], pp –
. Courant R, Hilbert D () Methods of mathe-
matical physics, vol . Partial Differential Equa-
tions, Interscience, New York
. Craig I, Brown J () Inverse problems in
astronomy. Adam Hilger, Bristol
. Deans SR () The radon transform and some
of its applications. Wiley, New York
. Deutsch F () Best approximation in inner
product spaces. Springer, New York
. Engl HW, Hanke M, Neubauer A () Regular-
ization of inverse problems. Kluwer, Dordrecht
. Epstein CL () Introduction to the mathe-
matics of medical imaging. Pearson Education,
Upper Saddle River
. Galilei G () Sidereus Nuncius (trans: Albert
van Helden). University
of Chicago
Press,
Chicago, 
. Gates E () Einstein’s telescope. W.W. Norton,
New York
. Gladwell GML () Inverse problems in vibra-
tion. Martinus Nijhoff, Dordrecht
. Glasko V () Inverse problems of mathemati-
cal physics (trans: Bincer A (Russian)), American
Institute of Physics, New York
. Goldberg RR () Fourier transforms. Cam-
bridge University Press, Cambridge
. Groetsch CW () Comments on Morozov’s
discrepancy principle. In: Hämmerlin G, Hoff-
mann K-H (eds) Improperly posed problems
and their numerical treatment. Birkhäuser, Basel,
pp –
. Groetsch CW () On the asymptotic order of
convergence of Tikhonov regularization. J Optim
Theory Appl :–
. Groetsch CW () The theory of Tikhonov reg-
ularization for Fredholm equations of the first
kind. Pitman, Boston
. Groetsch CW () Convergence analysis of a
regularized degenerate kernel method for Fred-
holm integral equations of the first kind. Integr
Equ Oper Theory :–
. Groetsch CW () Inverse problems in the
mathematical sciences. Vieweg, Braunschweig
. Groetsch CW () The delayed emergence of
regularization theory. Bollettino di Storia delle
Scienze Matematiche :–
. Groetsch CW () Nascent function concepts
in Nova Scientia. Int J Math Educ Sci Tech
:–
. Groetsch CW () Extending Halley’s prob-
lem. Math Sci :–
. Groetsch CW, Neubauer A () Regulariza-
tion of ill-posed problems: optimal parameter
choice in finite dimensions. J Approx Theory :
–
. Groetsch CW () Stable approximate eval-
uation of unbounded operators, LNM .
Springer, New York
. Grosser M () The discovery of neptune. Har-
vard University Press, Cambridge
. HadamardJ () Sur lesproblèmesauxdériveès
partielles et leur signification physique, Prince-
ton University Bulletin. Princeton University Bull
No. :–
. Hadamard J () Lectures on Cauchy’s prob-
lems in linear partial differential equations. Yale
University Press, New Haven (Reprinted by
Dover, New York, .)
. Halley E () A discourse concerning gravity,
and its properties, wherein the descent of heavy
bodies, and the motion of projects is briey, but
fully handled: together with the solution of a
problem of great use in gunnery. Philos Trans R
Soc Lond :–
. Hanke M () Iterative regularization tech-
niques in image reconstruction. In: Colton D et al
(eds) Surveys on solution methods for inverse
problems. Springer, Vienna, pp –
. Hanke M, Groetsch CW () Nonstationary
iterated Tikhonov regularization. J Optim Theory
Appl :–
. Hanke M, Neubauer A, Scherzer O () A con-
vergence analysis of Landweber iteration for non-
linear ill-posed problems. Numer Math : –
. Hansen PC, Nagy J, O’Leary D () Deblurring
images: matrices, spectra, and filtering. SIAM,
Philadelphia

Linear Inverse Problems 

. Hansen PC () Rank deficient and discrete
ill-posed problems. SIAM, Philadelphia
. Hensel E () Inverse theory and applications
for engineers. Prentice-Hall, Englewood Cliffs
. Hofmann B () Regularization for applied
inverse and ill-posed problems. Teubner, Leipzig
. Joachimstahl F () Über ein attractionsprob-
lem. J für die reine und angewandte Mathematik
:–
. Kaczmarz S (), Angenäherte Auflösung von
Systemen linearer Gleichungen, Bulletin Interna-
tional de l’Academie Polonaise des Sciences, Cl. d.
Sc. Mathém. A, pp –
. Kaltenbacher B, Neubauer A, Scherzer O ()
Iterative regularization methods for nonlinear Ill-
posed problems. Walter de Gruyter, Berlin
. Kirsch A () An introduction to the math-
ematical theory of inverse problems. Springer,
New York
. Landweber L () An iteration formula for
Fredholm integral equations of the first kind. Am
J Math :–
. Lewitt RM, Matej S () Overview of meth-
ods for image reconstruction from projections in
emission computed tomography. Proc IEEE :
–
. Morozov VA () On the solution of functional
equations by the method of regularization. Sov
Math Doklady :–
. Nashed MZ (ed) () Generalized inverses and
applications. Academic, New York
. Natterer F, Wübberling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Newbury P, Spiteri R () Inverting gravita-
tional lenses. SIAM Rev :–
. Parks PC, Kaczmarz S () –. Int J
Control :–
. Parker RL () Geophysical inverse theory.
Princeton University Press, Princeton
. Phillips DL () A technique for the numerical
solution of certain integral equations of the first
kind. J Assoc Comput Mach :–
. Picard E () Sur un théorème général relatif
aux équations intégrales de premiére espéce et sur
quelques probl_emes de physique mathématique.
Rendiconti del Cicolo Matematico di Palermo
:–
. Radon J () Über die Bestimmung von Funk-
tionen durch ihre Integralwerte längs gewisser
Mannigfaltigkeiten. Berichte über die Verhand-
lungen der Königlich Sächsischen Gesellshaft der
Wissenschaften zur Leipzig :–
. Scherzer
O,
Grasmair
M,
Grossauer
H,
Haltmeier M, Lenzen F () Variational
methods in imaging. Springer, New York
. Sheehan W, Kollerstrom N, Waff C () The
case of the pilfered planet: did the British steal
Neptune? Scient Am, pp –
. Shepp LA (ed) () Computed tomography,
proceedings of symposia in applied mathematics,
vol . American Mathematical Society, Provi-
dence
. StewartGW() Ontheearlyhistoryof thesin-
gular value decomposition. SIAM Rev :–
. Tikhonov AN () On the stability of inverse
problems. Dokl Akad Nau SSSR :–
. Tihonov (Tikhonov) AN() Solutionof incor-
rectly formulated problems and the regulariza-
tion method, Sov Math Doklady :-
. Tikhonov AN, Arsenin VY () Solutions of Ill-
posed Problems. Winston & Sons, Washington
. Uhlmann G (ed) () Inside out: inverse prob-
lems and applications. Cambridge University
Press, New York
. Vogel CR () Computational methods for
inverse problems. SIAM, Philadelphia
. Wing GM () A primer on integral equations
of the first kind: the problem of deconvolution
and unfolding. SIAM, Philadelphia
. Wrenn FR, Good ML, Handler P () The use
of positron-emitting radioisotopes for the local-
ization of brain tumors. Science :–
. Wunsch C () The ocean circulation inverse
problem, Cambridge University Press, Cam-
bridge



Large-Scale Inverse
Problems in Imaging
Julianne Chung ⋅Sarah Knepper ⋅James G. Nagy
.
Introduction.......................................................................
.
Background.......................................................................
..
Model Problems..........................................................................
..
Imaging Applications....................................................................
...
Image Deblurring and Deconvolution................................................
...
Multi-Frame Blind Deconvolution.....................................................
...
Tomosynthesis............................................................................
.
Mathematical Modelling and Analysis...........................................
..
Linear Problems...........................................................................
...
SVD Analysis..............................................................................
...
Regularization by SVD Filtering........................................................
...
Variational Regularization and Constraints..........................................
...
Iterative Regularization..................................................................
...
Hybrid Iterative-Direct Regularization................................................
...
Choosing Regularization Parameters..................................................
..
Separable Inverse Problems.............................................................
...
Fully Coupled Problem..................................................................
...
Decoupled Problem......................................................................
...
Variable Projection Method.............................................................
..
Nonlinear Inverse Problems............................................................
.
Numerical Methods and Case Examples........................................
..
Linear Example: Deconvolution........................................................
..
Separable Example: Multi-Frame Blind Deconvolution............................
..
Nonlinear Example: Tomosynthesis...................................................
.
Conclusion........................................................................
.
Cross-References...................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Large-Scale Inverse Problems in Imaging
Abstract: Large-scale inverse problems arise in a variety of significant applications in
image processing, and efficient regularization methods are needed to compute meaning-
ful solutions. This chapter surveys three common mathematical models including a linear,
a separable nonlinear, and a general nonlinear model. Techniques for regularization and
large-scale implementations are considered, with particular focus on algorithms and com-
putations that can exploit structure in the problem. Examples from image deconvolution,
multi-frame blind deconvolution, and tomosynthesis illustrate the potential of these algo-
rithms. Much progress has been made in the field of large-scale inverse problems, but many
challenges still remain for future research.
.
Introduction
Powerful imaging technologies, including very large telescopes, synthetic aperture radar,
medical imaging scanners, and modern microscopes, typically combine a device that col-
lects electromagnetic energy (e.g., photons) with a computer that assembles the collected
data into images that can be viewed by practitioners, such as scientists and doctors. The
“assembling” process typically involves solving an inverse problem; that is, the image is
reconstructed from indirect measurements of the corresponding object. Many inverse
problems are also ill-posed, meaning that small changes in the measured data can lead
to large changes in the solution, and special tools or techniques are needed to deal with
this instability. In fact, because real data will not be exact (it will contain at least some
small amount of noise or other errors from the data collection device), it is not possible to
find the exact solution. Instead, a physically realistic approximation is sought. This is done
by formulating an appropriate regularized (i.e., stabilized) problem, from which a good
approximate solution can be computed.
Inverse problems are ubiquitous in imaging applications, including deconvolution (or,
more generally, deblurring) [, ], super-resolution (or image fusion) [, ], image reg-
istration [], image reconstruction [, ], seismic imaging [], inverse scattering [],
and radar imaging []. These problems are referred to as large-scale because they typically
require processing a large amount of data (the number of pixels or voxels in the discretized
image) and systems with a large (e.g., for a D image reconstruction problem) number
of equations. Mathematicians began to rigorously study inverse problems in the s, and
this interest has continued to grow over the past few decades due to applications in fields
such as biomedical, seismic, and radar imaging; see, for example, [, , , , ] and
the references therein.
We remark that the discussion in this chapter does not address some very important
issues that can arise in PDE-based inverse problems, such as adjoints and proper meshing.
Inverse problems such as these arise in important applications, including PDE parameter
identification, seismic imaging, and inverse scattering; we refer those interested in these
topics and applications to the associated chapters in this handbook and the references
therein.

Large-Scale Inverse Problems in Imaging 

This chapter discusses computational approaches to compute approximate solutions of
large-scale inverse problems. Mathematical models and some applications are presented in
> Sect. .. Three basic models are considered: a general nonlinear model, a linear model,
and a mixed linear/nonlinear model. Several regularization approaches are described in
> Sect. .. Numerical methods that can be used to compute approximate solutions for the
three basic models, along with illustrative examples from specific imaging applications, are
described in > Sect. .. Concluding remarks, including a partial list of open questions, are
provided in > Sect. ..
.
Background
A mathematical framework for inverse problems is presented in this chapter, including
model problems and imaging applications. Although only a limited number of imaging
applications are considered, the model problems, which range from linear to nonlinear,
are fairly general and can be used to describe many other applications. For more complete
treatments of inverse problems and regularization, see [, , , , , ].
..
Model Problems
An inverse problem involves the estimation of certain quantities using information
obtained from indirect measurements. A general mathematical model to describe this
process is given by
bexact = F(xexact),
(.)
where xexact denotes the exact (or ideal) quantities that need to be estimated, and bexact
is used to represent perfectly measured (error free) data. The function F is defined by the
data collection process and is assumed known. Typically, it is assumed that F is defined on
Hilbert spaces, and that it is continuous and weakly sequentially closed [].
Unfortunately, in any real application, it is impossible to collect error-free data, so a
more realistic model of the data collection process is given by
b = F(xexact) + η,
(.)
where η represents noise and other errors in the measured data. The precise form of F
depends on the application; the following three general problems are considered in this
chapter:
•
For linear problems F(x) = Ax, where A is a linear operator. In this case, the data
collection process is modeled as
b = Axexact + η,
and the inverse problem is: given b and A, compute an approximation of xexact.


Large-Scale Inverse Problems in Imaging
•
In some cases, x can be separated into two distinct components, x(ℓ) and x(nℓ), with
F(x) = F (x(ℓ), x(nℓ)) = A(x(nℓ)) x(ℓ), where A is a linear operator defined by x(nℓ).
That is, the data b depends linearly on x(ℓ) and nonlinearly on x(nℓ). In this case, the
data collection process is modeled as
b = A(x(nℓ)
exact) x(ℓ)
exact + η,
and the inverse problem is: given b and the parametric form of A, compute approxima-
tions of x(nℓ)
exact and x(ℓ)
exact.
•
If the problem is not linear or separable, as described above, then the general nonlinear
model,
b = F(xexact) + η,
will be considered. In this case, the inverse problem is: given b and F, compute an
approximation of xexact.
In most of what follows, it is assumed that the problem has been discretized, so x, b, and
η are vectors, and A is a matrix. Depending on the constraints assumed and the complexity
of the model used, problems may range from linear to fully nonlinear. This is true of the
applications described in the next subsection.
..
Imaging Applications
Three applications in image processing that lead to inverse problems are discussed in this
subsection. For each application, the underlying mathematical model is described and
some background for the problem is presented. The formulation of each of these problems
results in a linear, separable, and nonlinear inverse problem, respectively.
...
Image Deblurring and Deconvolution
In many important applications, such as when ground-based telescopes are used to observe
objects in space, the observed image is degraded by blurring and noise. Although the
blurring can be partially avoided by using sophisticated and expensive imaging devices,
computational post processing techniques are also often needed to further improve the
resolution of the image. This post processing is known as image deblurring. To give a pre-
cise mathematical model of image deblurring, suppose x(t), t ∈Rd, is a scalar function
describing the true d-dimensional (e.g., for a plane image containing pixels, d = ) image.
Then the observed, blurred, and noisy image is given by
b(s) = ∫Ω k(s, t)x(t)dt + η(s),
(.)
where s ∈Rd, and η(s) represents additive noise. The kernel k(s, t) is a function that
specifies how the points in the image are distorted, and is therefore called the point spread

Large-Scale Inverse Problems in Imaging 

function (PSF). The inverse problem of image deblurring is: given k and b, compute an
approximation of x. If the kernel has the property that k(s, t) = k(s−t),then the PSF is said
to be spatially invariant; otherwise, it is said to be spatially variant. In the spatially invariant
case, the blurring operation, ∫k(s−t)x(t)dt,is convolution, and thus the corresponding
inverse problem is called deconvolution.
In a realistic problem, images are collected only at discrete points (pixels or voxels) and
are only available in a finite bounded region. Therefore, one must usually work directly
either with a semi-discrete model
b(s j) = ∫Ω k(s j, t)x(t)dt + η j
j = , . . . , N
where N is the number of pixels or voxels in the observed image, or with the fully discrete
model
b = Axexact + η,
where xexact, b, and η are vectors obtained by discretizing functions x, b, and η, and A
is a matrix that arises when approximating the integration operation with, for example, a
quadrature rule. Moreover, a precise kernel representation of the PSF may not be known,
but instead must be constructed experimentally from the imaging system by generating
images of “point sources.” What constitutes a point source depends on the application.
For example, in atmospheric imaging, the point source can be a single bright star [].
In microscopy, the point source is typically a fluorescent microsphere having a diameter
that is about half the diffraction limit of the lens []. For general motion blurs, the PSF is
described by the direction (e.g., angle) and speed at which objects are moving [].
For spatially invariant blurs, one point source image and appropriate boundary condi-
tions are enough to describe the matrix A. This situation has been well studied; algorithms
to compute approximations of x can be implemented efficiently with fast Fourier trans-
forms (FFT) or other trigonometric transforms [, ]. More recently, an approach has been
proposed where the data can be transformed to the Radon domain so that computations
can be done efficiently with, for example, wavelet filtering techniques [].
Spatially variant blurs also occur in a variety of important applications. For example, in
positron emission tomography (PET), patient motion during the relatively long scan times
causes reconstructed images to be corrupted by nonlinear, nonuniform spatially variant
motion blur [, ]. Spatially variant blurs also occur when the object and image coor-
dinates are tilted relative to each other, as well as in X-ray projection imaging [], lens
distortions [], and wave aberrations []. Moreover, it is unlikely that the blur is truly
spatially invariant in any realistic application, especially over large image planes.
Various techniques have been proposed to approximately model spatially variant blurs.
For example, in the case of patient motion in PET brain imaging, a motion detection device
is used to monitor the position of the patient’s head during the scan time. This information
can then be used to construct a large sparse matrix A that models the motion blur. Other,
more general techniques include coordinate transformation [], image partitioning [],
and PSF interpolation [, ].


Large-Scale Inverse Problems in Imaging
...
Multi-Frame Blind Deconvolution
The image deblurring problem described in the previous subsection assumes that the blur-
ring operator, or PSF, is known. However, in most cases, only an approximation of the
operator, or an approximation of parameters that define the operator, is known. For exam-
ple, as previously mentioned, the PSF is often constructed experimentally from the imaging
system by generating images of point sources. In many cases, such approximations are fairly
good and are used to construct the matrix A in the linear model. However, there are situa-
tions where it is not possible to obtain good approximations of the blurring operator, and
it is necessary to include this knowledge in the mathematical model. Specifically, consider
the general image formation model
b = A(x(nℓ)
exact) x(ℓ)
exact + η
(.)
where b is a vector representing the observed, blurred, and noisy image, and x(ℓ)
exact is
a vector representing the unknown true image to be reconstructed. A(x(nℓ)
exact) is an ill-
conditioned matrix defining the blurring operator. For example, in the case of spatially
invariant blurs, x(nℓ)
exact could simply be the pixel (image space) values of the PSF. Or
x(nℓ)
exact could be a small set of parameters that define the PSF, such as with a Zernike
polynomial-based representation []. In general, the number of parameters defining x(nℓ)
exact
is significantly smaller than the number of pixels in the observed image. As in the pre-
vious subsection, η is a vector that represents unknown additive noise in the measured
data. The term blind deconvolution is used for algorithms that attempt to jointly compute
approximations of x(nℓ)
exact and x(ℓ)
exact from the separable inverse problem given by > Eq. (.).
Blind deconvolution problems are highly underdetermined, which present many chal-
lenges to optimization algorithms that can easily become trapped in local minima. This
difficulty has been well documented; see, for example, [, ]. To address challenges of
nonuniqueness, it may be necessary to include additional constraints, such as nonnegativ-
ity and bounds on the computed approximations x(nℓ) and x(ℓ).
Multi-frame blind deconvolution (MFBD) [, ] reduces some of the nonuniqueness
problems by collecting multiple images of the same object, but with different blurring
operators. Specifically, suppose a set of (e.g., m) observed images of the same object
are modeled as
bi = A(x(nℓ)
i
) x(ℓ)
exact + ηi,
i = ,, . . . , m.
(.)
Then, a general separable inverse problem of the form given by > Eq. (.) can be obtained
by setting
b =
⎡⎢⎢⎢⎢⎢⎣
b
⋮
bm
⎤⎥⎥⎥⎥⎥⎦
,
x(nℓ)
exact =
⎡⎢⎢⎢⎢⎢⎢⎣
x(nℓ)

⋮
x(nℓ)
m
⎤⎥⎥⎥⎥⎥⎥⎦
,
η =
⎡⎢⎢⎢⎢⎢⎣
η
⋮
ηm
⎤⎥⎥⎥⎥⎥⎦
.
Although multiple frames reduce, to some extent, the nonuniqueness problem, they do not
completely eliminate it. In addition, compared to single frame blind deconvolution, there
is a significant increase in the computational complexity of processing the large, multiple
data sets.

Large-Scale Inverse Problems in Imaging 

There are many approaches to solving the blind and multi-frame blind deconvolu-
tion problem; see, for example []. In addition, many other imaging applications require
solving separable inverse problems, including super-resolution (which is an example of
image data fusion) [, , , ], the reconstruction of D macromolecular struc-
tures from D electron microscopy images of cryogenically frozen samples (Cryo-EM)
[, , , , , ], and seismic imaging applications [].
...
Tomosynthesis
Modern conventional X-ray systems that use digital technology have many benefits to the
classical film X-ray systems, including the ability to obtain high quality images with lower
dosage X-rays. The term “conventional” is used to refer to a system that produces a D
projection image of a D object, as opposed to computed tomography (CT), which pro-
duces D images. Because of the inexpensive cost, low X-ray dosage, and ease of use, digital
X-ray systems are widely used in medicine, from emergency rooms, to mammography, to
dentistry.
Tomosynthesis is a technique that can produce D image information of an object using
conventional X-ray systems []. The basic idea underlying tomosynthesis is that multiple
D image projections of the object are taken at varying incident angles, and each D image
provides different information about the D object. See
> Fig. -for an illustration of a
typical geometry for breast tomosynthesis imaging. The relationship between the multiple
D image projections and the D object can be modeled as a nonlinear inverse problem.
Reconstruction algorithms that solve this inverse problem should be able toreconstruct any
Detector
Compressed breast
Detector
Front view
Side view with X-ray tube at 0°
Center of
rotation
X-ray tube
Compression plate
Support plate
X-ray tube
Chest wall
⊡Fig. -
Breast tomosynthesis example. Typical geometry of the imaging device used in breast
imaging


Large-Scale Inverse Problems in Imaging
number of slices of the D object. Sophisticated approaches used for D CT reconstruction
cannot be applied here because projections are only taken from a limited angular range,
leaving entire regions of the frequency space unsampled. Thus, alternative approaches need
to be considered.
The mathematical model described in this section is specifically designed for breast
imaging, and assumes a polyenergetic (i.e., multiple energy) X-ray source. It is first nec-
essary to determine what quantity will be reconstructed. Although most X-ray projection
models are derived in terms of the values of the attenuation coefficients for the voxels, it
is common in breast imaging to interpret the voxels as a composition of adipose tissue,
glandular tissue, or a combination of both []. Thus, each voxel of the object can be rep-
resented using the percentage glandular fraction, that is, the percentage of glandular tissue
present in that voxel. If density or attenuation coefficient values are desired, then these can
be obtained from the glandular fraction through a simple algebraic transformation.
Now assume that the D object is discretized into a regular grid of voxels and that each
of the D projection images is discretized into a regular grid of pixels. Specifically, let N
represent the number of voxels in the discretized D object and let M be the number of
pixels in a discretized D projection image. In practice, N is on the order of a few billion
and M is the order of a few million, depending on the size of the imaging detector. The
energy-dependent linear attenuation coefficient for voxel j = ,, . . . , N in the breast can
be represented as
μ(e)(j) = s(e)x(j)
exact + z(e),
where x(j)
exact represents the percentage glandular fraction in voxel j of the “true” object, and
s(e) and z(e) are known energy-dependent linear fit coefficients. This type of decompo-
sition to reduce the number of degrees of freedom, which is described in more detail in
[], is similar to an approach used by De Man et al. [] for CT, in which they express the
energy dependent linear attenuation coefficient in terms of its photoelectric component
and Compton scatter component.
The projections are taken from various angles in a predetermined angular range, and
the photon energies can be discretized into a fixed number of levels. Let there be nθ angu-
lar projections and assume the incident X-ray has been discretized into ne photon energy
levels. In practice, a typical scan may have nθ = and ne = . For a particular projec-
tion angle, compute a monochromatic ray trace for one energy level and then sum over all
energies. Let a(i j) represent the length of the ray that passes through voxel j, contributing
to pixel i. Then, the discrete monochromatic ray trace for pixel i can be represented by
N
∑
j=
μ(e)(j)a(i j) = s(e)
N
∑
j=
x(j)
exacta(i j) + z(e)
N
∑
j=
a(i j).
(.)
Using the standard mathematical model for transmission radiography, the ith pixel
value for the θth noise-free projection image, incorporating all photon energies present
in the incident X-ray spectrum, can be written as

Large-Scale Inverse Problems in Imaging 

b(i)
θ
=
ne
∑
e=
ϱ(e)exp ⎛
⎝−
N
∑
j=
μ(e)(j)a(i j)⎞
⎠,
(.)
where ϱ(e) is a product of the current energy with the number of incident photons at that
energy.
To simplify notation, define Aθ to be an M × N matrix with entries a(i j). Then
> Eq. (.) gives the ith entry of the vector
s(e)Aθxexact + z(e)Aθ,
where xexact is a vector whose jth entry is x(j)
exact and is a vector of all ones. Furthermore,
the θth noise-free projection image in vector form can be written as
bθ =
ne
∑
e=
ϱ(e)exp (−[s(e)Aθxexact + z(e)Aθ]),
(.)
where the exponential function is applied component-wise.
Tomosynthesis reconstruction is a nonlinear inverse problem where the goal is to
approximate the volume, xexact, given the set of projection images from various angles,
bθ, θ = ,, . . . ., nθ. This can be put in the general nonlinear model
b = F(xexact) + η,
where
b =
⎡⎢⎢⎢⎢⎢⎣
b
⋮
bnθ
⎤⎥⎥⎥⎥⎥⎦
and
F(x) =
⎡⎢⎢⎢⎢⎢⎣
∑ne
e=ϱ(e)exp (−[s(e)Ax + z(e)A])
⋮
∑ne
e=ϱ(e)exp (−[s(e)Anθ x + z(e)Anθ])
⎤⎥⎥⎥⎥⎥⎦
.
.
Mathematical Modelling and Analysis
A significant challenge when attempting to compute approximate solutions of inverse
problems is that they are typically ill-posed. To be precise, in Hadamard defined a
well-posed problem as one that satisfies the following requirements:
. The solution is unique;
. The solution exists for arbitrary data; and
. The solution depends continuously on the data.
Ill-posed problems, and hence most inverse problems, typically fail to satisfy at least one of
these criteria. It is worth mentioning that this definition of an ill-posed problem applies to
continuous mathematical models, and not precisely to the discrete approximations used
in computational methods. However, the properties of the continuous ill-posed prob-
lem are often carried over to the discrete problem in the form of a particular kind of


Large-Scale Inverse Problems in Imaging
ill-conditioning, making certain (usually high frequency) components of the solution very
sensitive to errors in the measured data; this property is discussed in more detail for lin-
ear problems in > Sect. ... Of course, this may depend on the level of discretization;
a coarsely discretized problem may not be very ill-conditioned, but it also may not bear
much similarity to the underlying continuous problem.
Regularization is a term used to refer to various techniques that modify the inverse
problem in an attempt to overcome the instability caused by ill-posedness. Regularization
seeks to incorporate a priori knowledge into the solution process. Such knowledge may
include information about the amount or type of noise, the smoothness or sparsity of the
solution, or restrictions on the values the solution may obtain. Each regularization method
also requires choosing one or more regularization parameters. A variety of approaches are
discussed in this section.
The theory for regularizing linear problems is much more developed than it is for
nonlinear problems. This is due, in large part, to the fact that the numerical treatment
of nonlinear inverse problems is often highly dependent on the particular application.
However, good intuition can be gained by first studying linear inverse problems.
..
Linear Problems
Consider the linear inverse problem
b = Axexact + η,
where b and A are known, and the aim is to compute an approximation of xexact. The linear
problem is a good place to illustrate the challenges that arise when attempting to solve
large-scale inverse problems. In addition, some of the regularization methods and iterative
algorithms discussed here can be used in, or generalized for, nonlinear inverse problems.
...
SVD Analysis
A useful tool in studying linear inverse problems is the singular value decomposition
(SVD). Any m × n matrix A can be written as
A = UΣVT
(.)
where U is an m × m orthogonal matrix, V is an n × n orthogonal matrix, and Σ is an
m × n diagonal matrix containing the singular values σ≥σ≥. . . ≥σmin(m,n) ≥. If A is
nonsingular, then an approximation of xexact is given by the inverse solution
xinv = A−b =
n
∑
i=
uT
i b
σi
vi =
n
∑
i=
uT
i bexact
σi
vi

xexact
+
n
∑
i=
uT
i η
σi
vi

error
where ui and vi are the singular vectors of A (i.e., the columns of U and V, respectively).
As indicated above, the inverse solution is comprised of two components: xexact and an
error term. Before discussing algorithms to compute approximations of xexact, it is useful
to study the error term.

Large-Scale Inverse Problems in Imaging 

For matrices arising from ill-posed inverse problems, the following properties hold:
P.
The matrix A is severely ill conditioned, with the singular values σi decaying to zero
without a significant gap to indicate numerical rank.
P.
The singular vectors corresponding to the small singular values tend to oscillate more
(i.e., have higher frequency) than singular vectors corresponding to large singular
values.
P.
The components ∣uT
i bexact∣decay on average faster than the singular values σi. This
is referred to as the discrete Picard condition [].
The first two properties imply that the high frequency components of the error term
are highly magnified by division of small singular values. The computed inverse solu-
tion is dominated by these high frequency components and is in general a very poor
approximation of xexact. However, the third property suggests that there is hope of recon-
structing some information about xexact; that is, an approximate solution can be obtained
by reconstructing components corresponding to the large singular values and filtering out
components corresponding to small singular values.
...
Regularization by SVD Filtering
The SVD filtering approach to regularization is motivated by observations made in the pre-
vious subsection. That is, by filtering out components of the solution corresponding to the
small singular values, a reasonable approximation of xexact can be computed. Specifically,
an SVD filtered solution is given by
xfilt =
n
∑
i=
ϕi
uT
i b
σi
vi,
(.)
where the filter factors, ϕi, satisfy ϕi ≈for large σi, and ϕi ≈for small σi. That is, the
large singular value components of the solution are reconstructed, while the components
corresponding to the small singular values are filtered out. Different choices of filter factors
lead to different methods. Some examples include:
Truncated SVD Filter
Tikhonov Filter
Exponential Filter
ϕi = {
ifσi > τ

ifσi ≤τ
ϕi =
σ 
i
σ 
i + α
ϕi = −e−σ 
i /α
Note that using a Taylor series expansion of the exponential term in the exponential filter,
it is not difficult to see that the Tikhonov filter is a truncated approximation of the expo-
nential filter. Moreover, the Tikhonov filter has an equivalent variational form, which is
described in > Sect. ....
Observe that each of the filtering methods has a parameter (e.g., in the above examples,
τ and α) that needs to be chosen to specify how much filtering is done. Appropriate values
depend on properties of the matrix A (i.e., on its singular values and singular vectors)


Large-Scale Inverse Problems in Imaging
as well as on the data, b. Some techniques to help guide the choice of the regularization
parameter are discussed in > Sect. ....
Because the SVD can be very expensive to compute for large matrices, this explicit
filtering approach is generally not used for large-scale inverse problems. There are some
exceptions, though, if A is highly structured. For example, suppose A can be decomposed
as a Kronecker product,
A = Ar ⊗Ac =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a(r)
Ac
a(r)
Ac
⋯
a(r)
n Ac
a(r)
Ac
a(r)
Ac
⋯
a(r)
n Ac
⋮
⋮
⋮
a(r)
nAc
a(r)
nAc
⋯
a(r)
nn Ac
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
where Ac is an m × m matrix, and Ar is an n × n matrix with entries denoted by a(r)
i j . Then
this block structure can be exploited when computing the SVD and when implementing
filtering algorithms [].
It is also sometimes possible to use an alternative factorization. Specifically, suppose
that
A = QΛQ∗,
whereΛisadiagonalmatrix,andQ∗isthecomplexconjugatetransposeofQ,withQ∗Q = I.
This is called a spectral factorization, where the columns of Q are eigenvectors and the
diagonal elements of Λ are the eigenvalues of A. Although every matrix has an SVD, only
normal matrices (i.e., matrices that satisify A∗A = AA∗) have a spectral decomposition.
However, if Ahas a spectral factorization, then it can be used, in place of the SVD, to imple-
ment the filtering methods described in this section. The advantage is that it is sometimes
more computationally convenient to compute a spectral decomposition than an SVD; an
example of this is given in > Sect. ...
...
Variational Regularization and Constraints
Variational regularization methods have the form
min
x
{∥b −Ax∥
+ αJ (x)},
(.)
where the regularization operator J and the regularization parameter α must be chosen.
The variational form provides a lot of flexibility. For example, one could include additional
constraints on the solution, such as nonnegativity, or it may be preferable to replace the
least squares criterion with the Poisson log likelihood function [–]. As with filtering,
there are many choices for the regularization operator, J , such as Tikhonov, total variation
[, , ], and sparsity constraints [, , ]:
Tikhonov
Total Variation
Sparsity
J (x) = ∥Lx∥

J (x) = ∥
√
(Dhx)+ (Dvx)∥

J (x) = ∥Φx∥

Large-Scale Inverse Problems in Imaging 

Tikhonov regularization, which was first proposed and studied extensively in the early
s [, , –], is perhaps the most well-known approach to regularizing ill-posed
problems. L is typically chosen to be the identity matrix, or a discrete approximation to a
derivative operator, such as the Laplacian. If L = I, then it is not difficult to show that the
resulting variational form of Tikhonov regularization, namely,
min
x
{∥b −Ax∥
+ α∥x∥
},
(.)
can be written in an equivalent filtering framework by replacing A with its SVD [].
For total variation, Dh and Dv denote discrete approximations of horizontal and ver-
tical derivatives of the D image x, and the approach extends to D images in an obvious
way. Efficient and stable implementation of total variation regularization is a nontrivial
problem; see [, ] and the references therein for further details.
In the case of sparse reconstructions, the matrix Φ represents a basis in which the
image, x, is sparse. For example, for astronomical images that contain a few bright objects
surrounded by a significant amount of black background, an appropriate choice for Φ
might be the identity matrix. Clearly, the choice of Φ is highly dependent on the structure
of the image x. The usage of sparsity constraints for regularization is currently a very active
field of research, with many open problems. We refer interested readers to the chapter in
this handbook on compressive sensing, and the references therein.
We also mention that when the majority of the elements in the image x are zero or
near zero, as may be the case for astronomical or medical images, it may be wise to enforce
nonnegativity constraints on the solution [, , ]. This requires that each element of the
computed solution x is not negative, which is often written as x ≥. Though these con-
straints add a level of difficulty when solving, they can produce results that are more feasible
than when nonnegativity is ignored.
Finally, it should be noted that depending on the structure of matrix A, the type of
regularization, and the additional constraints, a variety of optimization algorithms can be
used to solve (> .). In some cases, it is possible to use a very efficient filtering approach,
but typically it is necessary to use an iterative method.
...
Iterative Regularization
As mentioned in > Sect. ..., iterative methods are often needed to solve the variational
form of the regularized problem. An alternate approach to using variational regularization
is to simply apply the iterative method to the least squares problem,
min
x
∥b −Ax∥
.
Note that if an iterative method applied to this unregularized problem is allowed to “con-
verge,” it will converge to an inverse solution, xinv, which is corrupted by noise (recall the


Large-Scale Inverse Problems in Imaging
discussion in > Sect. ...). However, many iterative methods have the property (provided
the problem on which it is applied satisfies the discrete Picard condition) that the early iter-
ations reconstruct components of the solution corresponding to large singular values, while
components corresponding to small singular values are reconstructed at later iterations.
Thus, there is an observed “semi-convergence” behavior in the quality of the reconstruc-
tion, whereby the approximate solution improves at early iterations and then degrades at
later iterations (a more detailed discussion of this behavior is given in > Sect. ...in
the context of the iterative method LSQR). If the iteration is terminated at an appropriate
point, a regularized approximation of the solution is computed. Thus, the iteration index
acts as the regularization parameter, and the associated scheme is referred to as an iterative
regularization method.
Many algorithms can be used as iterative regularization methods, including Landweber
[], steepest descent, and the conjugate gradient method (e.g., for nonsymmetric problems
the CGLS implementation [] or the LSQR implementation [, ], and for symmetric
indefinite problems, the MR-II implementation []). Most iterative regularization meth-
ods can be put into a general framework associated with solving the minimization problem
min f (x) = 
xTATAx −xTATb
(.)
with a general iterative method of the form
xk+= xk + ρkMk (ATb −ATAxk) = xk + ρkMkrk,
(.)
where rk = ATb −ATAxk. With specific choices of ρk and Mk, one can obtain a variety of
well-known iterative methods:
•
The Landweber method is obtained by taking ρk = ρ (i.e., ρ remains constant for each
iteration), and Mk = I (the identity matrix). Due to its very slow convergence, this
classic approach is not often used for linear inverse problems. However, it is very easy
to analyze the regularization properties of the Landweber iteration, and it can be useful
for certain large-scale nonlinear ill-posed inverse problems.
•
The steepest descent method is produced if Mk = I is again fixed as the identity, but
now ρk is chosen to minimize the residual at each iteration. That is, ρk is chosen as
ρk = arg min
ρ>
f (xk + ρrk).
Again, this method typically has very slow convergence, but with proper precondition-
ing it may be competitive with other methods.
•
It is also possible to obtain the conjugate gradient method by setting M= I and Mk+=
I −skyT
k
yT
k sk
, where sk = xk+−xk and yk = ATA(xk+−xk). As with the steepest descent
method, ρk is chosen to minimize the residual at each iteration. Generally, the conjugate
gradient method converges much more quickly than Landweber or steepest descent.

Large-Scale Inverse Problems in Imaging 

Other iterative algorithms that can be put into this general framework include the Brakhage
methods [], and Barzilai and Borwein’s lagged steepest descent scheme [].
...
Hybrid Iterative-Direct Regularization
One of the main disadvantages of iterative regularization methods is that it can be very
difficult to determine appropriate stopping criteria. To address this problem, work has
been done to develop hybrid methods that combine variational approaches with iterative
methods. That is, an iterative method, such as the LSQR implementation of the conjugate
gradient method, is applied to the least squares problem min
x
∥Ax −b∥
, and variational
regularization is incorporated within the iteration process. To understand how this can be
done, it is necessary to briefly describe how the LSQR iterates are computed.
LSQR is based on the Golub–Kahan (sometimes referred to as Lanczos) bidiagonaliza-
tion (GKB) process. Given an m×n matrix A and vector b, the kth GKB iteration computes
an m×(k+)matrix Wk, an n×k matrix Yk, an n×vector yk+, and a (k+)×k bidiagonal
matrix Bk such that
ATWk = YkBT
k + γk+yk+eT
k+
(.)
AYk = WkBk,
(.)
where ek+denotes the (k + )st standard unit vector and Bk has the form
Bk =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
γ
β
γ
⋱
⋱
βk
γk
βk+
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(.)
Matrices Wk and Yk have orthonormal columns, and the first column of Wk is b/∥b∥.
Given these relations, an approximate solution xk can be computed from the projected least
squares problem
min
x∈R(Yk)∥Ax −b∥
= min
ˆx
∥Bk ˆx −βe∥

(.)
where β = ∥b∥, and xk = Yk ˆx. An efficient implementation of LSQR does not require
storing the matrices Wk and Yk and uses an efficient updating scheme to compute ˆx at
each iteration; see [] for details.
An important property of GKB is that for small values of k, the singular values of
the matrix Bk approximate very well certain singular values of A, with the quality of the
approximation depending on the relative spread of the singular values; specifically, the
larger the relative spread, the better the approximation [, , ]. For ill-posed inverse
problems, the singular values decay to and cluster at zero, such as σi = O(i−c) where
c > , or σi = O(ci), where < c < and i = ,, . . ., n [, ]. Thus, the relative gap
between large singular values is generally much larger than the relative gap between small


Large-Scale Inverse Problems in Imaging
singular values. Therefore, if the GKB iteration is applied to a linear system arising from
discretization of an ill-posed inverse problem, then the singular values of Bk converge very
quickly to the largest singular values of A. The following example illustrates this situation.
Example
Consider a linear system obtained by discretization of a one-dimensional first
kind Fredholm integral equation of the form (> .), where the kernel k(s, t) is given by
the Green’s function for the second derivative, and which is constructed using deriv2 in
the MATLAB package Regularization Tools []. Although this is not an imaging example,
it is a small scale canonical ill-posed inverse problem that has properties found in imaging
applications. The deriv2 function constructs an n × n matrix A from the kernel
k(s, t) = {s (t −)
if s < t
t (s −)
if s ≥t
defined on [,] × [,]. We use n = . There are also several choices for constructing
vectors xexact and bexact (see []), but we focus only on the matrix A in this example.
> Figure -shows a plot of the singular values of A and their relative spread; that is,
σi(A) −σi+(A)
σi(A)
,
where the notation σi(A) is used to denote the ith largest singular value of A. > Figure -
clearly illustrates the properties of ill-posed inverse problems; the singular values of A
decay to and cluster at . Moreover, it can be observed that in general the relative gap of the
singular values is larger for the large singular values and smaller for the smaller singular
values. Thus, for small values of k, the singular values of Bk converge quickly to the large
singular values of A. This can be seen in > Fig. -, which compares the singular values of
A with those of the bidiagonal matrix Bk for k = ,,.
∎
10−1
10−2
10−3
10−4
10−5
si(A)
10−6
10−7
50
100
150
200
250
i
101
100
10–1
10–2
10–3
Isi(A) − si+1(A)I /si(A)
10–4
50
100
150
200
250
i
⊡Fig. -
This ﬁgure shows plots of the singular values of A, denoted as σi(A) (left plot), and the
relative spread of A’s singular values (right plot)

Large-Scale Inverse Problems in Imaging 

100
σi(B10)
σi(A)
10–2
10–4
10–6
Singular values, σi(A) and σi(B10) 
10–8
0
10
20
30
40
50
i
Singular values, σi(A) and σi(B50) 
100
10−2
10−4
10−6
10−8
0
10
20
30
40
50
i
σi(B50)
σi(A)
100
10−5
10−10
10−15
10−20
0
5 10 15 20 25 30 35 40 45 50 55
i
Iσi(A) - σi(B10)I/σi(A)
100
10−5
10−10
10−15
10−20
0
5 10 15 20 25 30 35 40 45 50 55
i
Iσi(A) - σi(B50)I/σi(A)
100
10−5
10−10
10−15
10−20
0
5 10 15 20 25 30 35 40 45 50 55
i
Iσi(A) − σi(B20)I /σi(A)
Singular values, σi(A) and σi(B20) 
100
10−2
10−4
10−6
10−8
0
10
20
30
40
50
i
σi(B20)
σi(A)
⊡Fig. -
The plots in the left column of this ﬁgure show the singular values of A, denoted as σi(A),
along with the singular values of Bk, denoted as σi(Bk), for k = , , . The plots in the
right column show the relative diﬀerence, ∣σi(A) −σi(Bk)∣
σi(A)


Large-Scale Inverse Problems in Imaging
This example implies that if LSQR is applied to the least squares problem min
x
∥Ax−b∥,
then at early iterations the approximate solutions xk will be in a subspace that approximates
a subspace spanned by the large singular components of A. Thus, for k ≪n, xk is a regu-
larized solution. However, eventually xk should converge to the inverse solution, which is
corrupted with noise (recall the discussion in > Sect. ...). This means that the iteration
index k plays the role of a regularization parameter; if k is too small, then the computed
approximation xk is an over-smoothed solution, while if k is too large, xk is corrupted with
noise. Again, we emphasize that this semi-convergence behavior requires that the problem
satisfies the discrete Picard condition. More extensive theoretical arguments of this semi-
convergence behavior of conjugate gradient methods can be found elsewhere; see [] and
the references therein.
Instead of early termination of the iteration, hybrid approaches enforce regularization
at each iteration of the GKB method. Hybrid methods were first proposed by O’Leary and
Simmons in [], and later by Björck in []. The basic idea is to regularize the pro-
jected least squares problem (> .) involving Bk, which can be done very cheaply because
of the smaller size of Bk. More specifically, because the singular values of Bk approximate
those of A, as the GKB iteration proceeds, the matrix Bk becomes more ill conditioned.
The iteration can be stabilized by including Tikhonov regularization in the projected least
square problem (> .), to obtain
min
ˆx
{∥Bk ˆx −βe∥
+ α∥ˆx∥
}
(.)
where again β = ∥b∥and xk = Yk ˆx. Thus, at each iteration it is necessary to solve a
regularized least squares problem involving a bidiagonal matrix Bk. Notice that since the
dimension of Bk is very small compared to A, it is much easier to solve for ˆx in > Eq. (.)
than it is to solve for x in the full Tikhonov regularized problem (> .). More importantly,
when solving > Eq. (.) one can use sophisticated parameter choice methods to find a
suitable α at each iteration.
To summarize, hybrid methods have the following benefits:
•
Powerful regularization parameter choice methods can be implemented efficiently on
the projected problem.
•
Semi-convergence behavior of the relative errors observed in LSQR is avoided, so an
imprecise (over) estimate of the stopping iteration does not have a deleterious effect on
the computed solution.
Realizing these benefits in practice, though, is nontrivial. Thus, various authors have con-
sidered computational and implementation issues, such as robust approaches to choose
regularization parameters and stopping iterations; see, for example, [, , , , ,
, ]. We also remark that our discussion of hybrid methods focused on the case
of Tikhonov regularization with L = I. Implementation of hybrid methods when L
is not the identity matrix, such as a differentiation operator, can be nontrivial; see, for
example, [, ].

Large-Scale Inverse Problems in Imaging 

...
Choosing Regularization Parameters
Each of the regularization methods discussed in this section requires choosing a regulariza-
tion parameter. It is a nontrivial matter to choose “optimal” regularization parameters, but
there are methods that can be used as guides. Some require a priori information, such as a
bound on the noise or a bound on the solution. Others attempt to estimate an appropriate
regularization parameter directly from the given data.
To describe some of the more popular parameter choice methods, let xreg denote a
solution computed by a particular regularization method.
•
Discrepancy Principle. In this approach, a solution is sought such that
∥b −Axreg∥= τ∥η∥
where τ > is a predetermined number []. This is perhaps the easiest of the
methods to implement, and there are substantial theoretical results establishing its
behavior in the presence of noise. However, it is necessary to have a good estimate
for ∥η∥.
•
Generalized Cross Validation. The idea behind generalized cross validation (GCV) is
that if one data point is removed from the problem, then a good regularized solution
should predict that missing data point well. If α is the regularization parameter used to
obtain xreg, then it can be shown [] that the GCV method chooses α to minimize the
function
G(α) =
∥b −Axreg∥
(trace (I −AA†
reg))
.
where A†
reg is the matrix such that xreg = A†
regb. For example, in the case of Tikhonov
regularization (> .),
A†
reg = (ATA + αI)−AT.
A weighted version of GCV, W-GCV, finds a regularization parameter to minimize
Gω(α) =
∥b −Axreg∥
(trace (I −ωAA†
reg))
.
W-GCV is sometimes more effective at choosing regularization parameters than the
standard GCV function for certain classes of problems. Setting the weight ω = 
gives the standard GCV method, while ω
<
produces less smooth solutions
and ω > produces smoother solutions. Further details about W-GCV can be
found in [].
•
L-Curve. This approach attempts to balance the size of the discrepancy (i.e., residual)
produced by the regularized solution with the size of the solution. In the context of
Tikhonov regularization, this can often be found by a log-log scale plot of ∥b −Axreg∥


Large-Scale Inverse Problems in Imaging
versus ∥xreg∥for all possible regularization parameters. This plot often produces an
L-shaped curve, and the solution corresponding to the corner of the L indicates a
good balance between discrepancy and size of the solution. This observation was
first made by Lawson and Hanson [], and later studied extensively, including effi-
cient numerical schemes to find the corner of the L (i.e., the point of maximum
curvature), by Hansen [, ]. Although the L-curve tends to work well for many
problems, some concerns about its effectiveness have been reported in the literature;
see [, ].
There exist many other parameter choice methods besides the ones discussed above; for
more information, see [, , ] and the references therein.
A proper choice of the regularization parameter is critical. If the parameter is chosen
too small, then too much noise will be introduced in the computed solution. On the other
hand, if the parameter is too large, the regularized solution may become over-smoothed
and may not contain as much information about the true solution as it could. However, it
is important to keep in mind that no parameter choice method is “fool proof,” and it may
be necessary to solve the problem with a variety of parameters and to use knowledge of the
application to help decide which solution is best.
..
Separable Inverse Problems
Separable nonlinear inverse problems,
b = A(x(nℓ)
exact) x(ℓ)
exact + η,
(.)
arise in many imaging applications, such as blind deconvolution (see > Sect. ...), super-
resolution (which is an example of image data fusion) [, , , ], the reconstruction
of D macromolecular structures from D electron microscopy images of cryogenically
frozen samples (Cryo-EM) [, , , , , ], and seismic imaging applications [].
One could consider > Eq. (.) as a general nonlinear inverse problem and use the
approaches discussed in > Sect. ..to compute regularized solutions. However, this sec-
tion considers approaches that exploit the separability of the problem. In particular, some
of the regularization methods described in > Sect. .., such as variational and iterative
regularization, can be adapted to > Eq. (.). To illustrate, consider the general Tikhonov
regularized least squares problem:
min
x(ℓ),x(nℓ) {∥A(x(nℓ)) x(ℓ) −b∥
+ α∥x(ℓ)∥
} =
min
x(ℓ), x(nℓ) ∥[A(x(nℓ))
αI
] x(ℓ) −[b
]∥


.
(.)
Three approaches to solve this nonlinear least squares problem are outlined in this
section.

Large-Scale Inverse Problems in Imaging 

...
Fully Coupled Problem
The nonlinear least squares problem given in > Eq. (.) can be rewritten as
min
x
ϕ(x) = min
x

∥ρ(x)∥
,
(.)
where
ρ(x) = ρ (x(ℓ), x(nℓ)) = [A(x(nℓ))
αI
] x(ℓ) −[b
],
and
x = [ x(ℓ)
x(nℓ)]
Nonlinear least squares problems are solved iteratively, with algorithms having the general
form:
General Iterative Algorithm
choose initial x=
⎡⎢⎢⎢⎣
x(ℓ)

x(nℓ)

⎤⎥⎥⎥⎦
for k = ,,, . . .
●
choose a step direction, dk
●
determine step length, τk
●
update the solution: xk+= xk + τkdk
●
stop when a minimum of the objective is obtained
end
Typically, dk is chosen to approximate the Newton direction,
dk = −(̂ϕ ′′(xk))−ϕ′(xk),
where ̂ϕ ′′ is an approximation of ϕ′′, ϕ′ = JT
ϕ ρ, and Jϕ is the Jacobian matrix
Jϕ = [∂ρ (x(ℓ), x(nℓ))
∂x(ℓ)
∂ρ (x(ℓ), x(nℓ))
∂x(nℓ)
].
In the case of the Gauss–Newton method, which is often recommended for nonlinear least
squares problems, ̂ϕ ′′ = JT
ϕ Jϕ.


Large-Scale Inverse Problems in Imaging
This general Gauss–Newton approach can work well, but constructing and solving
the linear systems required to update dk can be very expensive. Note that the dimension
of the matrix Jϕ corresponds to the number of pixels in the image, x(ℓ), plus the num-
ber of parameters in x(nℓ), and thus Jϕ may be on the order of × . Thus, instead
of using Gauss–Newton, it might be preferable to use a low storage scheme such as the
(nonlinear) conjugate gradient method. But there is a tradeoff – although the cost per
iteration is reduced, the number of iterations needed to attain a minimum can increase
significantly.
Relatively little research has been done on understanding and solving the fully cou-
pled problem. For example, methods are needed for choosing regularization parameters. In
addition, the rate of convergence of the linear and nonlinear terms may be quite different,
and the effect this has on overall convergence rate is not well understood.
...
Decoupled Problem
Probably the simplest idea to solve the nonlinear least squares problem is to decouple it into
two problems, one involving x(ℓ) and the other involving x(nℓ). Specifically, the approach
would have the form:
Block Coordinate Descent Iterative Algorithm
choose initial x(nℓ)

for k = ,,, . . .
●
choose αk and solve the linear problem:
x(ℓ)
k
= arg min
x(ℓ) ∥A(x(nℓ)
k
)x(ℓ) −b∥

+ α
k∥x(ℓ)∥

●
solve the nonlinear problem:
x(nℓ)
k+= arg min
x(nℓ) ∥A(x(nℓ))x(ℓ)
k
−b∥

+ α
k ∥x(ℓ)
k ∥


●
stop when objectives are minimized
end
The advantage of this approach is that any of the approaches discussed in > Sect. ..,
including methods to determine α, can be used for the linear problem. The nonlinear prob-
lem involving x(nℓ) requires using another iterative method, such as the Gauss–Newton
method. However, there are often significantly fewer parameters than in the fully coupled
approach discussed in the previous subsection. Thus, a Gauss–Newton method to update

Large-Scale Inverse Problems in Imaging 

x(nℓ)
k+at each iteration is significantly more computationally tractable. A disadvantage to
this approach, which is known in the optimization literature as block coordinate descent,
is that it is not clear what are the practical convergence properties of the method. As men-
tioned in the previous subsection, the rate of convergence of the linear and nonlinear terms
may be quite different. Moreover, if the method does converge, it will typically be very slow
(linear), especially for problems with tightly coupled variables [].
...
Variable Projection Method
The variable projection method [, , , , ] exploits structure in the nonlinear least
squares problem (> .). The approach exploits the fact that ϕ (x(ℓ), x(nℓ)) is linear in
x(ℓ), and that x(nℓ) contains relatively fewer parameters than x(ℓ). However, rather than
explicitly separating variables x(ℓ) and x(nℓ) as in coordinate descent, variable projection
implicitly eliminates the linear parameters x(ℓ), obtaining a reduced cost functional that
depends only on x(nℓ). Then, a Gauss–Newton method is used to solve the optimization
problem associated with the reduced cost functional. Specifically, consider
ψ (x(nℓ)) ≡ϕ (x(ℓ) (x(nℓ)), x(ℓ))
where x(ℓ)(x(nℓ)) is a solution of
min
x(ℓ) ϕ (x(ℓ), x(nℓ)) = min
x(ℓ) ∥[A(x(nℓ))
αI
] x(ℓ) −[b
]∥


.
(.)
To use the Gauss–Newton algorithm to minimize the reduced cost functional ψ(x(nℓ)), it
is necessary to compute ψ′ (x(nℓ)). Note that because x(ℓ) solves (> .), it follows that
∂ϕ
∂x(ℓ) = , and thus
ψ′(y) = dx
dy
∂ϕ
∂x(ℓ) +
∂ϕ
∂x(nℓ) =
∂ϕ
∂x(nℓ) = JT
ψ ρ,
where the Jacobian of the reduced cost functional is given by
Jψ = ∂(A(x(nℓ)) x(ℓ))
∂x(nℓ)
.


Large-Scale Inverse Problems in Imaging
Thus, a Gauss–Newton method applied to the reduced cost functional has the
basic form:
Variable Projection Gauss–Newton Algorithm
choose initial x(nℓ)

for k = ,,, . . .
choose αk
x(ℓ)
k
= arg min
x(ℓ)

⎡⎢⎢⎢⎢⎣
A(x(nℓ)
k
)
αkI
⎤⎥⎥⎥⎥⎦
x(ℓ) −
⎡⎢⎢⎢⎣
b

⎤⎥⎥⎥⎦

rk = b −A(x(nℓ)
k
) x(ℓ)
k
dk = arg min
d
∥Jψd −rk∥
determine step length τk
x(nℓ)
k+= x(nℓ)
k
+ τkdk
end
Although computing Jψ is nontrivial, it is often much more tractable than con-
structing Jϕ. In addition, the problem of variable convergence rates for the two sets of
parameters, x(ℓ) and x(nℓ), has been eliminated. Another big advantage of the variable
projection method for large-scale inverse problems is that standard approaches, such
as those discussed in > Sect. .., can be used to solve the linear regularized least
squares problem at each iteration, including the schemes for estimating regularization
parameters.
..
Nonlinear Inverse Problems
Developing regularization approaches for general nonlinear inverse problems can be sig-
nificantly more challenging than the linear and separable nonlinear case. Theoretical tools
such as the SVD that are used to analyze ill-posedness in the linear case are not avail-
able here, and previous efforts to extend these tools to the nonlinear case do not always
apply. For example, a spectral analysis of the linearization of a nonlinear problem does
not necessarily determine the degree of ill-posedness for the nonlinear problem []. Fur-
thermore, convergence properties for nonlinear optimization require very strict assump-
tions that are often not realizable in real applications [, ]. Nevertheless, nonlinear
inverse problems arise in many important applications, motivating research on regular-
ization schemes and general computational approaches. This section discusses some of
this work.

Large-Scale Inverse Problems in Imaging 

One approach for nonlinear problems of the form
F(x) = b
(.)
is to reformulate the problem to find a zero of F(x) −b = . Then a Newton-like method,
where the nonlinear function is repeatedly linearized around the current estimate, can be
written as
xk+= xk + ρkpk
(.)
where pk solves the Jacobian system
J(xk)p = b −F(xk).
(.)
Though generally not symmetric, matrix and matrix transpose multiplication with the
Jacobian, whose elements are the first derivatives of F(x), are typically computable. How-
ever, the main disadvantages of using this approach are that the existence and uniqueness
of a solution are not guaranteed and the sensitivity of solutions depends on the condition-
ing of the Jacobian. Furthermore, there is no natural merit function that can be monitored
to help select the step length, ρk.
Another approach to solve (> .) is to incorporate prior assumptions regarding the
statistical distribution of the model and maximize the corresponding likelihood func-
tion. For example, an additive Gaussian noise model assumption under certain conditions
corresponds to solving the following nonlinear least squares problem:
min
x

∥b −F(x)∥
.
(.)
Since this is a standard nonlinear optimization problem, any optimization algorithm such
as a gradient descent or Newton approach can be used here. For problem (> .), the
gradient vector can be written as g(x) = J(x)T(F(x) −b) and Hessian matrix can be
written as H(x) = J(x)TJ(x) + Z(x), where Z(x) includes second derivatives of F(x).
The main advantage of this approach is that a variety of line search methods can be used.
However, the potential disadvantages of this approach are that the derivatives may be too
difficult to compute or that negative eigenvalues introduced in Z(x) may cause problems
in optimization algorithms.
Some algorithms for solving nonlinear optimization problems are direct extensions of
the iterative methods described in > Sect. .... The nonlinear Landweber iteration can
be written as
xk+= xk + J(xk)T(b −F(xk)),
(.)
which reduces to the standard Landweber iteration if F(x) is linear, and it can be easily
extended to other gradient descent methods such as the steepest descent approach. Newton
and Newton-type methods are also viable options for nonlinear optimization, resulting in
iterates (> .) where pk solves
H(xk)p = −g(xk).
(.)


Large-Scale Inverse Problems in Imaging
Oftentimes, an approximation of the Hessian is used. For example, the Gauss–Newton
algorithm, which takes H ≈J(xk)T J(xk), is a preferred choice for large-scale prob-
lems because it ensures positive semi-definiteness, but it is not advisable for large residual
problems or highly nonlinear problems []. Additionally, nonlinear Conjugate Gradient,
Truncated-Newton, or quasi-Newton methods such as LBFGS can be good alternatives
if storage is a concern. It is important to remark that finding a global minimizer for
a nonlinear optimization problem is in general very difficult, especially since convex-
ity of the objective function is typically not guaranteed, as in the linear case. Thus,
it is very likely that a descent algorithm may get stuck in one of many local minima
solutions.
When dealing with ill-posed problems, the general approach to incorporate regular-
ization is to couple an iterative approach with a stopping criteria such as the discrepancy
principle to produce reasonable solutions. In addition, for Newton-type methods it is
common to incorporate additional regularization for the inner system since the Jaco-
bian or Hessian may become ill-conditioned. For example, including linear Tikhonov
regularization in (> .) would result in
(J(xk)T J(xk) + αI)p = J(xk)T(b −F(xk)),
which is equivalent to a Levenberg–Marquardt iterate, where the update, pk, is the solution
of a particular Tikhonov minimization problem:
min
p ∥F(xk) + J(xk)p −b∥
+ α∥p∥
,
where F(x) has been linearized around xk . Other variations for regularizing the update can
be found in [] and the references therein. Regularization for the inner system can also be
achieved by solving the inner system inexactly using an iterative method and terminating
the iterations early. These are called inexact Newton methods, and the early termination of
the inner iterations is a good way not only to make this approach practical for large-scale
problems but also to enforce regularization on the inner system.
The variational approaches discussed in > Sect. ...can be extended for the sec-
ond class of algorithms where a likelihood function results in a nonlinear optimization
problem. For example, after selecting a regularization operator J (x) and regularization
parameter α for (> .), the goal would be to solve a nonlinear optimization problem of
the form
min
x
{∥b −F(x)∥
+ αJ (x)}.
(.)
The flexibility in the choice of the regularization operator is nice, but selecting a good
regularization parameter a priori can be a computationally demanding task, especially
for large-scale problems. Some work on estimating the regularization parameter within
a constrained optimization framework has been done [, ], but the most common
approach for regularization of nonlinear ill-posed inverse problems is to use standard iter-
ative methods to solve (> .), where regularization is obtained via early termination
of the iterations. It cannot be stressed enough that when using any iterative method to
solve a nonlinear inverse problem where the regularization is not already incorporated,

Large-Scale Inverse Problems in Imaging 

a good stopping iteration for the outer iteration that serves as a regularization parameter is
imperative. See also [, , , , , , ] for additional references on nonlinear inverse
problems.
.
Numerical Methods and Case Examples
Given a specific large-scale inverse problem from an imaging application, it can be
nontrivial to implement the algorithms and regularization methods discussed in this
chapter. Efficient computations require exploiting the structure of the problem. More-
over, choosing specific regularization schemes and constraints requires knowledge about
the physical process underlying the data collection process. A few illustrative exam-
ples, using the imaging applications described in
> Sect. .., are given in this
section.
..
Linear Example: Deconvolution
Perhaps the most well known and well studied linear inverse problem is deconvolution. As
discussed in > Sect. ..., this spatially invariant image deblurring problem is modeled as
b = Axexact + η,
where A is a structured matrix that depends on the PSF and imposed boundary conditions.
For example, if periodic boundary conditions are imposed on the blurring operation, then
A has a circulant structure, and, moreover, A has the spectral decomposition
A = F∗ΛF,
where F is a matrix representing a d-dimensional discrete Fourier transform, which satis-
fies F∗F = I. The matrix F does not need to be constructed explicitly. Instead, fast Fourier
transform (FFT) functions can be used to implement matrix vector multiplications with F
and F∗. Specifically, for D images,
Fx
⇔
fft2 (x)
(D forward FFT)
F∗x
⇔
ifft2 (x)
(D inverse FFT).
The main advantages are that FFT-based spectral filtering regularization algorithms
are very easy to implement and extremely efficient; see [] for implementation
details.
To illustrate, consider the image data shown in
> Fig. -, where the simulated
observed image was obtained by convolving the PSF with the true image and adding %
Gaussian white noise. The PSF was constructed from a Gaussian blurring operator,
pi j = exp (−(i −k)s
−(j −l)s
+ (i −k)(j −l)s

s
s
−s

)
(.)


Large-Scale Inverse Problems in Imaging
Truth
PSF
Blurred
Restored
⊡Fig. -
Simulated data for an image deconvolution problem. The restored image was computed
using an FFT-based spectral ﬁltering method, with Tikhonov regularization and GCV-chosen
regularization parameter
centered at (k, l) (location of point source), with s= s= , and s= . An FFT-
based Tikhonov spectral filtering solution was computed, with regularization operator
L = I, and regularization parameter α = ., which was chosen using GCV. (All
computations for this example were done with MATLAB. The implementation of the
FFT-based spectral filter used in this example is described in []. The MATLAB code,
which is called tik_fft.m, can be found at http://www.imm.dtu.dk/~pch/HNO/#.)
The reconstructed image, which was computed in a fraction of a second on a standard
laptop computer, is also shown in > Fig. -.
If there are significant details near the boundary of the image, then the periodic
boundary condition assumption might not be an accurate representation of the details
outside the viewable region. In this case, severe ringing artifacts can appear in the recon-
structed image, and parameter choice methods may perform very poorly in these sit-
uations. Consider, for example, the image data shown in
> Fig. -. The PSF is the
same as in the previous example, but the blurred image contains features at the bound-
aries of the viewable region. The “restored” image in
> Fig. -was again computed
using a Tikhonov spectral filtering solution with regularization operator L = I, and reg-
ularization parameter (α = .× −) was chosen using GCV. This noise-corrupted
reconstructed image indicates that the regularization parameter chosen by GCV is too
small.
It is possible that another parameter choice method would perform better, but
it is also the case that imposing alternative boundary conditions may improve the
situation. For example, reflective boundary conditions assume the image scene out-
side the viewable region is a mirror image of the details inside the viewable region.
With this assumption, and if the PSF is also circularly symmetric, then the matrix
A has a symmetric Toepliz-plus-Hankel structure, and, moreover, A has the spectral
decomposition
A = CTΛC,

Large-Scale Inverse Problems in Imaging 

Truth
PSF
Blurred
Restored
⊡Fig. -
Simulated data for an image deconvolution problem. The restored image was computed
using an FFT-based spectral ﬁltering method, with Tikhonov regularization and GCV-chosen
regularization parameter
where C is a matrix representing a d-dimensional discrete cosine transform, which satisfies
CTC = I. As with FFTs, the matrix C does not need to be constructed explicitly, and very
efficient functions can be used to implement matrix vector multiplications with C and CT,
such as
Cx
⇔
dct2 (x)
(D forward DCT)
CTx
⇔
idct2 (x)
(D inverse DCT).
In addition, DCT-based spectral filtering regularization algorithms are very easy to imple-
ment and are extremely efficient; see [] for implementation details.
> Figure -illustrates the superior performance that can be obtained if the bound-
ary condition and the corresponding basis (in this case, DCT) is used in the spectral
filtering deconvolution algorithms. Specifically, the image on the left in > Fig. -was com-
puted using a DCT-based Tikhonov spectral filtering method, with regularization operator
L = I and a GCV-chosen regularization parameter α = .× −. The image on the
right was computed using the FFT-based Tikhonov filter, but instead of using the GCV-
chosen regularization parameter (which produced the poor reconstruction displayed in
the middle of this figure), α was set to the same value used by the DCT reconstruction.
This example clearly illustrates that the quality of the reconstruction, and the effectiveness
of parameter choice methods, can depend greatly on the imposed boundary conditions
and corresponding spectral basis. (As with previous examples, all computations described
here were done with MATLAB. The implementation of the DCT-based spectral filter is
described in []. The MATLAB code, which is called tik_dct.m, can be found at
http://www.imm.dtu.dk/~pch/HNO/.)
Spectral filtering methods work well for many deconvolution problems, but it may not
always be possible to find a convenient basis that allows for efficient implementation. Con-
sider, for example, the data shown in
> Fig. -. The PSF in this figure was constructed
using > Eq. (.), with s= s= and s= , and results in a nonsymmetric PSF. As
with the previous example, the FFT-based filter does not work well for this deconvolution


Large-Scale Inverse Problems in Imaging
DCT, a = 4.83 × 10−3
FFT, a = 6.30 × 10−5
FFT, a = 4.83 × 10−3
⊡Fig. -
These restored images were computed using DCT and FFT-based spectral ﬁltering methods,
with Tikhonov regularization. For the DCT and the middle FFT reconstructions, the
regularization parameter α was chosen by GCV. The FFT reconstruction on the right was
obtained using the same regularization parameter as was used for the DCT reconstruction
Truth
PSF
Blurred
⊡Fig. -
Simulated deconvolution data with a nonsymmetric Gaussian PSF
problem because of its implicit assumption of periodic boundary conditions. Reflective
boundary conditions are more appropriate, but the lack of circular symmetry in the PSF
means that the DCT basis does not diagonalize the matrix A. The reconstructed image on
the left in > Fig. -illustrates what happens if we attempt to reconstruct the image using
a DCT-based Tikhonov filter.
An iterative method may be the best option for a problem such as this; one can impose
any appropriate boundary condition (which only needs to be implemented in matrix-
vector multiplications with A and AT) without needing to assume any symmetry or further
structure in the PSF. The reconstructed image shown on the right in >Fig. -was obtained
using a hybrid approach described in > Sect. .... Specifically, Tikhonov regularization is
used for the projected subproblem, with regularization parameters chosen by W-GCV. The

Large-Scale Inverse Problems in Imaging 

DCT, a = 4.83 × 10−3
HyBR, a = 7.31 × 10−2
⊡Fig. -
These restored images were computed using a DCT-based spectral ﬁltering method (left)
and an iterative hybrid method (right)
MATLAB software for this, which is called HyBR, is discussed in [] and can be obtained
from http://www.cs.umd.edu/~jmchung/Home/HyBR.html. For this particular example,
HyBR terminated at iteration , with a regularization parameter α = .× −.
The examples in this subsection illustrate that many approaches can be used for the
linear inverse problem deconvolution. It is possible that other methods, such as those that
incorporate nonnegativity constraints, may produce better results than those presented
here, but this is typical of all inverse problems. It would be impossible to give an exhaustive
study and comparison in this chapter.
..
Separable Example: Multi-Frame Blind Deconvolution
In this section, multi-frame blind deconvolution (MFBD) is used to illustrate a numerical
example of a separable (nonlinear) inverse problem,
b = A(x(nℓ)
exact) x(ℓ)
exact + η.
Recall from > Sect. ...that in MFBD, a set of, say, m blurred images of an object are
collected, and the aim is to simultaneously reconstruct an approximation of the true image
as well as the PSFs (or the parameters that define the PSFs) associated with each of the


Large-Scale Inverse Problems in Imaging
observed blurred images. Such an example can be simulated using the Gaussian blurring
kernel given in > Eq.(.), and the true satellite image given in
> Fig. -. Specifically,
suppose using > Eq. (.), three PSFs are constructed using the following values
x(nℓ)
exact =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
.
.
.
.
.
.
.
.
.
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎫⎪⎪⎪⎬⎪⎪⎪⎭
Gaussian PSF parameters s, s, sfor frame 
⎫⎪⎪⎪⎬⎪⎪⎪⎭
Gaussian PSF parameters s, s, sfor frame 
⎫⎪⎪⎪⎬⎪⎪⎪⎭
Gaussian PSF parameters s, s, sfor frame .
(.)
Simulated observed image data can then be generated by convolving the PSFs constructed
from these sets of parameters with the true satellite image, and then adding % white noise.
The resulting simulated observed image frames are shown in > Fig. -.
Image reconstructions can then be computed using the variable projection Gauss–
Newton algorithm described in > Sect. .... The Jacobian Jψ can be constructed analyt-
ically for this problem (see, e.g., []), but a finite difference approach can also work very
well. In the experiments reported here, centered differences were used to approximate the
Jacobian.
⊡Fig. -
Simulated MFBD data. The images were obtained by convolving the true satellite image
from > Fig. -with Gaussian PSFs using parameters given in > Eq. (.), and then adding
% white noise

Large-Scale Inverse Problems in Imaging 

The hybrid method implementation HyBR, described in the previous subsection, was
used to choose αk and to solve the linear subproblem for x(ℓ)
k . The step length τk was
chosen using an Armijo rule []. The initial guess for x(nℓ)

was
aaaa
x(nℓ)

=
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
.
.
.
.
.
.
.
.
.
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎫⎪⎪⎪⎬⎪⎪⎪⎭
initial guess for s, s, sfor frame 
⎫⎪⎪⎪⎬⎪⎪⎪⎭
initial guess for s, s, sfor frame 
⎫⎪⎪⎪⎬⎪⎪⎪⎭
initial guess for s, s, sfor frame .
The results in
> Fig. -show the convergence behavior in terms of relative error at
each iteration of the variable projection Gauss–Newton algorithm for this example. The left
plot shows the convergence history of x(nℓ)
k
, and the right plot shows the convergence his-
tory of x(ℓ)
k . Note that the convergence behavior of both terms is very similar. > Figure -
shows the reconstructed image after the first variable projection Gauss–Newton iteration
(i.e., the initial reconstruction) and the reconstructed image after the last iteration of the
algorithm.
..
Nonlinear Example: Tomosynthesis
Polyenergetic digital tomosynthesis is an example of a nonlinear inverse problem where
the forward problem can be modeled as (> .). The typical approach to compute an
0
5
10
15
20
25
30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
GN iteration
Relative error (p)
0
5
10
15
20
25
30
0.34
0.36
0.38
0.4
0.42
0.44
0.46
GN iteration
Relative error (f )
⊡Fig. -
Convergence results for MFBD. The relative error of the estimated PSF parameters at each
iteration is shown in the left plot, while the relative error of the reconstructed image at each
iteration is shown in the right plot


Large-Scale Inverse Problems in Imaging
⊡Fig. -
On the left is the initial reconstructed image using x(nℓ)

, and on the right is the ﬁnal
reconstructed image. The true image is displayed in the middle for comparison purposes
approximation of xexact is to assume that the observed projection image is a realization
of a Poisson random variable with mean values
¯bθ + ¯η =
ne
∑
e=
ϱ(e)exp (−[s(e)Aθx + z(e)Aθ]) + ¯η,
(.)
where ¯η is the mean of the additive noise. Then the maximum likelihood estimator (MLE)
can be found by minimizing the negative log likelihood function:
−Lθ(x) =
M
∑
i=
(¯b(i)
θ
+ ¯η) −b(i)
θ
log(¯b(i)
θ
+ ¯η) + c,
(.)
where superscripts refer to entries in a vector and c is a constant term. A regularized
estimate can be found by solving the following nonlinear optimization problem
xMLE = min
x
{
nθ
∑
θ=
−Lθ(x)}
(.)
using a gradient descent or Newton-type algorithm and terminating the iterations before
the noise enters the problem. For this example, the gradient of the objective function with
respect to the D volume, x, can be written as
g(xk) = ATvk
where the entries of vector vk are given by
v(i) = (
b(i)
¯b(i) + ¯η(i) −)
ne
∑
e=
ϱ(e)s(e)exp(−[s(e) aT
i xk + z(e) aT
i ]).
The Hessian matrix can be written as
Hk = ATWkA

Large-Scale Inverse Problems in Imaging 

where Wk is a diagonal matrix with vector wk on the diagonal. A mathematical formula
for the values of the diagonal can be quite complicated as they depend on the values of the
second derivatives. Furthermore, the Newton step at iteration k in > Eq. (.) is just the
normal equations formulation of the least squares problem
min
sk ∥W


k Ask −W
−

k vk∥

(.)
where W


k
= diag (w

). For solving the Newton system, CGLS can be used to solve
(> .) inexactly. Furthermore, regularization for the outer problem is achieved by early
termination of the iterative optimization method.
The example illustrated here comes from a true volume of size × × whose
values range between and , representing the percentage of glandular tissue present in
the voxel. Then projection images were taken at equally spaced angles, within an angular
range from −○to ○at ○intervals, using the typical geometry for breast tomosynthesis,
illustrated in > Fig. -. Each D projection image contains ×pixels. Subimages of
three of these projections can be found in > Fig. -.
The original object represented a portion of a patient breast with mean compressed
breast thickness of size .cm × .cm × .cm, and the detector was .cm × cm. The
source to detector distance at ○was set to cm and the distance from the center of rota-
tion to detector was cm. The incident X-ray spectrum was produced by a rhodium target
with a tube voltage of kVp and an added rhodium filter of μm thickness, discretized
to consist of different energy levels, from .keV to keV, in .keV steps.
For the reconstruction algorithms, the ray trace matrix Aθ for each projection angle
was computed using a cone beam model, and an initial guess of the volume was a uni-
form image with all voxel values set to , meaning half glandular and half adipose
tissue. The reconstructed volume consisted of × × voxels with a voxel size of
μm × μm × .mm. Furthermore, additive Poisson noise was included in the pro-
jection images so that there was a relative noise level of approximately %. Some slices of
the true volume can be found in > Fig. -.
⊡Fig. -
Extracted regions of sample projection images


Large-Scale Inverse Problems in Imaging
⊡Fig. -
Sample slices from the original breast volume
⊡Table -
Convergence results for polyenergetic tomosynthesis reconstruction
Gradient descent method
Iteration
Rel. objective
Rel. gradient
Rel. error

.e-
.
.

.e-
.
.

.e-
.
.

.e-
.
.

.e-
.
.

.e-
.
.
Newton-CG
Iteration
Rel. objective
Rel. gradient
Rel. error
CGLS iterations

.e-
.
.
–

.e-
.
.


.e-
.
.


.e-
.
.


.e-
.
.


.e-
.
.

Recall that the goal of digital tomosynthesis is to reconstruct an approximation of the
D volume, x, given the set of projection images bθ, θ = ,, . . ., nθ. Using the above like-
lihood function, the problem has been reformulated as a nonlinear optimization problem
for which standard numerical optimization schemes can be applied. A gradient descent,
Newton-CG, and LBFGS algorithm are investigated as methods to solve this problem, and
early termination of the iterative method produces a regularized solution.
Results presented in
> Table -include the iteration, the relative objective function
value, the relative gradient value, and the relative error for the D volume for two iterative
algorithms. The relative error can be computed as ∥xk−xexact∥
∥xexact∥, where xk is the reconstructed
volume at the kth iteration. For the inexact Newton-CG algorithm, the stopping criterion
used for CGLS on the inner problem (> .) was a residual tolerance of .and a max-
imum number of iterations. The number of CGLS iterations reported for the inner
problem at each Newton-CG iteration can be found in the last column of the table. It is

Large-Scale Inverse Problems in Imaging 

worth mentioning here that many parameters such as the number of inner and outer iter-
ations rely heavily on heuristics that may or may not be provided by the application. In
any case, appropriate parameters should be used in order to ensure that nonlinearity and
ill-posedness of the problem are addressed.
Since each Newton-CG iteration requires the solution of a linear system, it is diffi-
cult to present a fair comparison of reconstruction algorithms. In terms of computational
effort, the most computationally burdensome aspect of the reconstruction is the matrix-
vector and matrix-transpose-vector multiplication with ray trace matrix, A. Each function
and gradient evaluation of the likelihood function requires a total of three “ray trace”
multiplications (two for the function evaluation and one more for the gradient), and a
multiplication operation with the Hessian (or its transpose) only requires two “ray trace”
multiplications. Furthermore, a backtracking line search strategy is used to ensure suffi-
cient descent at each iteration of the optimization scheme. The Cauchy point [] is used
as an initial guess in the line search scheme, thus requiring another multiplication with
the Hessian. Thus, the computational cost and timing for, say, one Newton-CG iteration
with inner CG iterations with the Hessian is not equivalent to gradient descent
iterations.
Another important remark is that although the image errors in
> Table -decrease
in the early iterations, these errors eventually increase. This is illustrated in the later
Newton-CG iterations in
> Fig. -, where plots of the relative errors per iteration
for the three algorithms are presented. From
> Fig. -, it is evident that the gradi-
ent descent method is slow to converge. On the contrary, Newton methods can compute
a good approximation very quickly, but corruption from errors occurs quickly as well.
Although LBFGS is typically used for problems where the Hessian cannot be computed
0
20
40
60
80
100
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Iteration
Relative error
Gradient
Newton−CG
LBFGS
⊡Fig. -
Plot of relative errors


Large-Scale Inverse Problems in Imaging
True
0
20
40
60
80
100
Gradient
Newton−CG
0
20
40
60
80
100
0
20
40
60
80
100
⊡Fig. -
Comparison of slices from the reconstructed volumes computed after three iterations of
Newton-CG algorithm and iterations of gradient descent
directly, this approach seems to offer a good balance between fast convergence and slow
semi-convergence behavior. An important remark is that direct regularization techniques
can also be used to regularize the problem, but appropriate regularization operators and
good regularization parameter selection methods for this problem are still topics of cur-
rent research. Thus, regularization via early termination of the iterations is the approach
followed here.
For a comparison of images, > Fig. -contains corresponding slices from the recon-
structed volumes after three Newton-CG iterations and gradient descent iterations,
each requiring approximately matrix-vector operations. It is evident that the Newton-
CG reconstruction has more fine details and more closely resembles the true image
slice.
Although nonlinear inverse problems can be difficult to analyze, there are a vari-
ety of scientific applications such as polyenergetic digital breast tomosynthesis that
require methods for computing approximate solutions. Iterative methods with reg-
ularization via early termination can be a good choice, but proper precondition-
ing techniques may be needed to accelerate the algorithms and good heuristics are
required.
.
Conclusion
Large-scale inverse problems arise in many imaging applications. The examples in this
chapter illustrate the range of difficulties (from linear to nonlinear) that can be encoun-
tered and the issues that must be addressed when designing algorithms. It is important
to emphasize that the literature in this field is vast, and that this presentation is far from
being a complete survey. However, the techniques discussed in this chapter can be used as
a foundation on which to learn more about the subject.
The study of inverse problems continues to be an extremely active field of research.
Although linear inverse problems have been fairly well studied, some fundamental ques-
tions still need to be addressed and many open problems remain. For example, in

Large-Scale Inverse Problems in Imaging 

hybrid algorithms, simple filtering methods (e.g., truncated SVD or standard Tikhonov
regularization) and standard regularization parameter choice methods (e.g., discrepancy
principle or GCV) are typically used to regularize the projected problem. Some work has
been done to generalize this (see, e.g., []), but extensions to more sophisticated filtering
algorithms and parameter choice methods should be investigated. In addition, the develop-
ment of novel algorithmic implementations and software is necessary for running existing
algorithms on state-of-the-art computing technologies, as is the development of techniques
for uncertainty quantification. Another area of active research for the solution of linear
and nonlinear inverse problems is sparse reconstruction schemes, where regularization
enforces some structure to be sparse in a certain basis, that is, represented with only a few
nonzero coefficients.
As discussed in > Sect. ..and > .., there are many open problems related to
solving nonlinear inverse problems. For example, in the case of the variable projection
Gauss–Newton method, a thorough study of its regularization and convergence proper-
ties remains to be done, especially in the context of an iteration dependent regularization
parameter. For more general nonlinear problems, issues that need to be addressed include
analyzing the sensitivity of the Jacobian and Hessian matrices, as well as determining
appropriate merit functions for selecting step lengths. In nonlinear optimization, difficul-
ties arise because convexity of the objective function cannot be guaranteed, so algorithms
can become trapped in local minima. More work also needs to be done in the area of reg-
ularization parameter choice methods for nonlinear problems and appropriate stopping
criteria for iterative methods. For a further discussion of open problems for nonlinear
inverse problems, see [, ].
Finally, it should be noted that many open problems are given in the context of the
application, such as determining appropriate constraints and regularization operators for
the problem. Future directions are often motivated by the application, and many of these
questions can be found in application specific references; see, for example, []. With
such varied and widespread applications, large-scale inverse problems continue to be a
thriving research interest in the mathematics, computer science, and image processing
communities.
.
Cross-References
Other chapters in this handbook covering material and/or applications that overlap with
this chapter include:
Applications:
> Astronomy
> EIT
> Inverse Scattering


Large-Scale Inverse Problems in Imaging
> Magnetic Resonance and Ultrasound Elastography
> Multimodal Image Processing
> Optical Imaging
> Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
> Radar
> Seismic
> Tomography
Regularization Methods:
> Compressive Sensing
> Linear Inverse Problems
> Neighborhood Filters
> Numerical Methods for Variational Approach in Image Analysis
> Regularization Methods for Ill-Posed Problems
> Statistical Inverse Problems
> Total Variation in Imaging
Numerical Methods:
> Duality and Convex Minimization
> EM Algorithms
> Iterative Solution Methods
Acknowledgements
We would like to thank Eldad Haber, University of British Columbia, and Per Chris-
tian Hansen, Technical University of Denmark, for carefully reading the first draft of this
chapter. Their comments and suggestions helped to greatly improve our presentation. The
research of J. Chung is supported by the United States National Science Foundation (NSF)
under grant DMS-. The research of J. Nagy is supported by the United States
National Science Foundation (NSF) under grant DMS-, and by the United States
Air Force Office of Scientific Research (AFOSR) under grant FA---.
References and Further Reading
. Andrews HC, Hunt BR () Digital image
restoration. Prentice-Hall, Englewood Cliffs
. Bachmayr M, Burger M () Iterative total
variation schemes for nonlinear inverse prob-
lems. Inverse Prob :
. Bardsley JM () An efficient computational
method for total variation-penalized Poisson
likelihood estimation. Inverse Prob Imaging
():–
. Bardsley JM () Stopping rules for a nonneg-
atively constrained iterative method for illposed
Poisson imaging problems. BIT ():–
. Bardsley JM, Vogel CR () A nonnegatively
constrained convex programming method for

Large-Scale Inverse Problems in Imaging 

image reconstruction. SIAM J Sci Comput
():–
. Barzilai J, Borwein JM () Two-point step
size gradient methods. IMA J Numer Anal
():–
. Björck Å () A bidiagonalization algorithm
for solving large and sparse ill-posed systems of
linear equations. BIT, ():–
. Björck Å () Numerical methods for least
squares problems. SIAM, Philadelphia
. Björck Å, Grimme E, van Dooren P () An
implicit shift bidiagonalization algorithm for ill-
posed systems. BIT ():–
. Brakhage H () On ill-posed problems and
the method of conjugate gradients. In: Engl HW,
Groetsch CW (eds) Inverse and ill-posed prob-
lems. Academic, Boston, pp –
. Calvetti D, Reichel L () Tikhonov reg-
ularization
of
large
linear
problems.
BIT
():–
. Calvetti D, Somersalo E () Introduction
to Bayesian scientific computing. Springer,
New York
. Candès EJ, Romberg JK, Tao T () Stable
signal recovery from incomplete and inaccu-
rate measurements. Commun Pure Appl Math
():–
. Carasso AS () Direct blind deconvolution.
SIAM J Appl Math ():–
. Chadan K, Colton D, Päivärinta L, Rundell W
() An introduction to inverse scattering and
inverse spectral problems. SIAM, Philadelphia
. Chan TF, Shen J () Image processing and
analysis: variational, PDE, wavelet, and stochas-
tic methods. SIAM, Philadelphia
. Cheney M, Borden B () Fundamentals of
radar imaging. SIAM, Philadelphia
. Chung J, Haber E, Nagy J () Numerical
methods for coupled super-resolution. Inverse
Prob ():–
. Chung J, Nagy J () An efficient itera-
tive approach for large-scale separable non-
linear inverse problems. SIAM J Sci Comput
():–
. Chung J, Nagy J, Sechopoulos I () Numer-
ical algorithms for polyenergetic digital breast
tomosynthesis reconstruction. SIAM J Imaging
Sci ():–
. Chung J, Nagy JG, O’Leary DP () A
weighted GCV method for Lanczos hybrid
regularization. Elec Trans Numer Anal :
–
. Chung J, Sternberg P, Yang C () High per-
formance -d image reconstruction for molecu-
lar structure determination. Int J High Perform
Comput Appl ():–
. De Man B, Nuyts J, Dupont P, Marchal G,
Suetens P () An iterative maximumlike-
lihood polychromatic algorithm for CT. IEEE
Trans Med Imaging ():–
. Diaspro A, Corosu M, Ramoino P, Robello M
() Two-photon excitation imaging based on
a compact scanning head. IEEE Eng Med Biol
():–
. Dobbins JT III, Godfrey DJ () Digital X-ray
tomosynthesis: current state of the art and clin-
ical potential. Phys Med Biol ():R–R
. Easley GR, Healy DM, Berenstein CA ()
Image deconvolution using a general ridgelet
and curvelet domain. SIAM J Imaging Sci
():–
. Elad M, Feuer A () Restoration of a sin-
gle superresolution image from several blurred,
noisy, and undersampled measured images.
IEEE Trans Image Process ():–
. Engl HW, Hanke M, Neubauer A ()
Regularization of inverse problems. Kluwer,
Dordrecht
. Engl HW, Kügler P () Nonlinear inverse
problems: theoretical aspects and some indus-
trial applications. In: Capasso V, Périaux J
(eds) Multidisciplinary methods for analysis
optimization and control of complex systems.
Springer, Berlin, pp –
. Engl HW, Kunisch K, Neubauer A ()
Convergence rates for Tikhonov regularisation
of nonlinear ill-posed problems. Inverse Prob
():–
. Engl HW, Louis AK, Rundell W (eds) ()
Inverse problems in geophysical applications.
SIAM, Philadelphia
. Eriksson J, Wedin P () Truncated Gauss-
Newton algorithms for ill-conditioned nonlin-
ear least squares problems. Optim Meth Softw
():–
. Faber TL, Raghunath N, Tudorascu D, Votaw JR
() Motion correction of PET brain images
through deconvolution: I. Theoretical develop-
ment and analysis in software simulations. Phys
Med Biol ():–


Large-Scale Inverse Problems in Imaging
. Figueiredo MAT, Nowak RD, Wright SJ ()
Gradient projection for sparse reconstruction:
Application to compressed sensing and other
inverse problems. IEEE J Sel Top Signal Process
():–
. Frank J () Three-dimensional electron
microscopy
of
macromolecular
assemblies.
Oxford University Press, New York
. Golub GH, Heath M, Wahba G () Gener-
alized cross-validation as a method for choos-
ing a good ridge parameter. Technometrics
():–
. Golub GH, Luk FT, Overton ML () A block
Lanczos method for computing the singular val-
ues and corresponding singular vectors of a
matrix. ACM Trans Math Softw (): –
. Golub GH, Pereyra V () The differentiation
of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM J
Numer Anal ():–
. Golub GH, Pereyra V () Separable non-
linear least squares: the variable projection
method and its applications. Inverse Prob :
R–R
. Haber E, Ascher UM, Oldenburg D () On
optimization techniques for solving nonlinear
inverse problems. Inverse Prob ():–
. Haber E, Oldenburg D () A GCV based
method for nonlinear ill-posed problems. Com-
put Geosci ():–
. Hammerstein GR, Miller DW, White DR,
Masterson ME, Woodard HQ, Laughlin JS
() Absorbed radiation dose in mammogra-
phy. Radiology ():–
. Hanke M () Conjugate gradient type meth-
ods for ill-posed problems. Pitman research
notes in mathematics, Longman Scientific &
Technical, Harlow
. Hanke
M
()
Limitations
of
the
L-
curve method in ill-posed problems. BIT
():–
. Hanke M () On Lanczos based methods
for the regularization of discrete ill-posed prob-
lems. BIT ():–
. Hansen PC () Analysis of discrete ill-posed
problems by means of the L-curve. SIAM Rev
():–
. Hansen PC () Numerical tools for analysis
and solution of Fredholm integral equations of
the first kind. Inverse Prob ():–
. Hansen PC () Regularization tools: a MAT-
LAB package for analysis and solution of dis-
crete ill-posed problems. Numer Algorithms
():–
. Hansen PC () Rank-deficient and discrete
ill-posed problems. SIAM, Philadelphia
. Hansen PC () Discrete inverse problems:
insight and algorithms. SIAM, Philadelphia
. Hansen PC, Nagy JG, O’Leary DP ()
Deblurring images: matrices, spectra and filter-
ing. SIAM, Philadelphia
. Hansen PC, O’Leary DP () The use of the L-
curve in the regularization of discrete ill-posed
problems. SIAM J Sci Comput ():–
. Hardy JW () Adapt Opt Sci Am ():
–
. Hofmann B () Regularization of nonlinear
problems and the degree of ill-posedness. In:
Anger G, Gorenflo R, Jochmann H, Moritz H,
Webers W (eds) Inverse problems: principles
and applications in geophysics, technology, and
medicine. Akademie Verlag, Berlin
. Hohn M, Tang G, Goodyear G, Baldwin PR,
Huang Z, Penczek PA, Yang C, Glaeser RM,
Adams PD, Ludtke SJ () SPARX, a new
environment for Cryo-EM image processing. J
Struct Biol ():–
. Jain AK () Fundamentals of digital image
processing. Prentice-Hall, Englewood Cliffs
. Kang MG, Chaudhuri S () Super-resolution
image reconstruction. IEEE Signal Process Mag
():–
. Kaufman L () A variable projection method
for solving separable nonlinear least squares
problems. BIT ():–
. Kilmer ME, Hansen PC, Español MI ()
A projection-based approach to general-form
Tikhonov regularization. SIAM J Sci Comput
():–
. Kilmer ME, O’Leary DP () Choosing reg-
ularization parameters in iterative methods for
ill-posed problems. SIAM J Matrix Anal Appl
():–
. Landweber L () An iteration formula for
Fredholm integral equations of the first kind.
Am J Math ():–
. Larsen RM () Lanczos bidiagonalization
with partial reorthogonalization. PhD thesis,
Department of Computer Science, University of
Aarhus, Denmark

Large-Scale Inverse Problems in Imaging 

. Lawson CL, Hanson RJ () Solving least
squares problems. SIAM, Philadelphia
. Löfdahl MG () Multi-frame blinddeconvo-
lution with linear equality constraints. In: Bones
PJ, Fiddy MA, Millane RP (eds) Image recon-
struction from incomplete data II, vol -.
SPIE, pp –
. Lohmann AW, Paris DP () Space-variant
image formation. J Opt Soc Am ():–
. Marabini R, Herman GT, Carazo JM ()
D reconstruction
in electron
microscopy
using ART with smooth spherically symmetric
volume
elements
(blobs).
Ultramicroscopy
(–):–
. Matson CL, Borelli K, Jefferies S, Beckner CC Jr,
Hege EK, Lloyd-Hart M () Fast and optimal
multiframe blind deconvolution algorithm for
high-resolution groundbased imaging of space
objects. Appl Opt ():A–A
. McNown SR, Hunt BR () Approximate
shift-invariance by warping shift-variant sys-
tems. In: Hanisch RJ, White RL (eds) The
restoration of HST images and spectra II. Space
Telescope Science Institute, Baltimore, MD,
pp –
. Miller K () Least squares methods for ill-
posed problems with a prescribed bound. SIAM
J Math Anal ():–
. Modersitzki J () Numerical methods for
image registration. Oxford University Press,
Oxford
. Morozov VA () On the solution of func-
tional equations by the method of regulariza-
tion. Sov Math Dokl :–
. Nagy JG, O’Leary DP () Fast iterative image
restoration with a spatially varying PSF. In:
Luk FT (ed) Advanced signal processing: algo-
rithms, architectures, and implementations VII,
vol . SPIE, pp –
. Nagy JG, O’Leary DP () Restoring images
degraded by spatially-variant blur. SIAM J Sci
Comput ():–
. Natterer F () The mathematics of comput-
erized tomography. SIAM, Philadelphia
. Natterer F, Wübbeling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Nguyen N, Milanfar P, Golub G () Efficient
generalized cross-validation with applications
to parametric image restoration and resolution
enhancement.
IEEE
Trans
Image
Process
():–
. Nocedal J, Wright S () Numerical optimiza-
tion. Springer, New York
. O’Leary DP, Simmons JA () A bidiagonali-
zation-regularization procedure for large scale
discretizations of ill-posed problems. SIAM J Sci
Stat Comput ():–
. Osborne MR () Separable least squares,
variable projection, and the Gauss-Newton
algorithm. Elec Trans Numer Anal :–
. Paige CC, Saunders MA () Algorithm
LSQR: Sparse linear equations and least
squares problems. ACM Trans Math Softw ():
–
. Paige CC, Saunders MA () LSQR: An algo-
rithm for sparse linear equations and sparse
least squares. ACM Trans Math Softw ():
–
. Penczek PA, Radermacher M, Frank J ()
Three-dimensional
reconstruction
of single
particles embedded in ice. Ultramicroscopy
():–
. Phillips DL () A technique for the numer-
ical solution of certain integral equations of
the first kind. J Assoc Comput Mach ():
–
. Raghunath N, Faber TL, Suryanarayanan S,
Votaw JR () Motion correction of PET
brain images through deconvolution: II. Prac-
tical implementation and algorithm optimiza-
tion. Phys Med Biol ():–
. Rudin LI, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D :–
. Ruhe A, Wedin P () Algorithms for separa-
ble nonlinear least squares problems. SIAM Rev
():–
. Saad Y () On the rates of convergence of the
Lanczos and the block-Lanczos methods. SIAM
J Numer Anal ():–
. Saban SD, Silvestry M, Nemerow GR, Stewart
PL () Visualization of α-helices in a -
Ångstrom resolution cryoelectron microscopy
structure of adenovirus allows refinement of
capsid protein assignments. J Virol ():
–
. Tikhonov AN () Regularization of incor-
rectly posed problems. Sov Math Dokl :–



Large-Scale Inverse Problems in Imaging
. Tikhonov AN () Solution of incorrectly
formulated problems and the regularization
method. Sov Math Dokl :–
. Tikhonov AN, Arsenin VY () Solutions of
ill-posed problems. Winston, Washington
. Tikhonov
AN,
Leonov
AS,
Yagola
AG
()
Nonlinear
ill-posed
problems,
vol
–. Chapman and Hall, London
. Trussell HJ, Fogel S () Identification and
restoration of spatially variant motion blurs in
sequential images. IEEE Trans Image Process
():–
. Tsaig Y, Donoho DL () Extensions of
compressed
sensing.
Signal
Process
():
–
. Varah JM () Pitfalls in the numerical solu-
tion of linear ill-posed problems. SIAM J Sci Stat
Comput ():–
. Vogel CR () Optimal choice of a truncation
level for the truncated SVD solution of linear
first kind integral equations when data are noisy.
SIAM J Numer Anal ():–
. Vogel CR () An overview of numerical
methods for nonlinear ill-posed problems. In:
Engl HW, Groetsch CW (eds) Inverse and ill-
posed problems. Academic Press, Boston, pp
–
. Vogel CR () Non-convergence of the
L-curve
regularization
parameter
selection
method. Inverse Prob ():–
. Vogel CR () Computational methods for
inverse problems. SIAM, Philadelphia
. Wagner FC, Macovski A, Nishimura DG ()
A characterization of the scatter pointspread-
function in terms of air gaps. IEEE Trans Med
Imaging ():–


Regularization Methods for
Ill-Posed Problems
Jin Cheng ⋅Bernd Hofmann
.
Introduction.......................................................................
.
Theory of Direct Regularization Methods.......................................
..
Tikhonov Regularization in Hilbert Spaces with Quadratic Misfit
and Penalty Terms........................................................................
..
Variational Regularization in Banach Spaces with Convex Penalty Term........
..
Extended Results for Hilbert Space Situations.......................................
.
Examples..........................................................................
.
Conclusions......................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Regularization Methods for Ill-Posed Problems
Abstract: In this chapter, we outline the mathematical theory of direct regularization
methods for in general nonlinear and ill-posed inverse problems. One focus is on
Tikhonov regularization in Hilbert spaces with quadratic misfit and penalty terms.
Moreover, recent results of an extension of the theory to Banach spaces are presented
concerning the variational regularization with convex penalty term. Five examples of
parameter identification problems in integral and differential equations are given in
order to show how to apply the theory of this chapter to specific inverse and ill-posed
problems.
.
Introduction
This chapter will be devoted to direct regularization methods – theory and examples – for
the solution of inverse problems formulated as nonlinear ill-posed operator equations
F(x) = y,
(.)
where the forward operator F : D(F) ⊆X →Y with domain D(F) maps between
infinite dimensional normed linear spaces X and Y, which are Banach spaces or
Hilbert spaces, with norms ∥⋅∥. The symbols →and ⇀denote the strong conver-
gence and weak convergence, respectively, in such spaces. In the Hilbert space case,
the symbol ⟨⋅,⋅⟩designates inner products. The majority of inverse problems is ill
posed in the sense of Hadamard, that is, at least one of the following difficulties
occurs:
.
The
> equation (.) has no solution in D(F) if the exact right-hand side y is
replaced by a perturbed element yδ (noisy data) satisfying the inequality
∥yδ −y∥≤δ
(.)
with noise level δ > .
.
The solution to (> .) is not uniquely determined in D(F).
.
The solution to (> .) is unstable with respect to perturbations, that is, for xδ ∈D(F)
with Fxδ = yδ and (> .) the norm deviation ∥xδ −x∥may be arbitrarily large. In
other words, the possibly multi-valued inverse operator F−fails to be continuous.
Since for nonlinear equations the local behavior of solutions is of main interest, the
aspect of local ill-posedness according to [] is focused in numerous considerations. An
operator
> equation (.) is called locally ill posed at some solution point x† ∈D(F) if
for any ball Bρ(x†) with arbitrarily small radius ρ > there exists an infinite sequences
{xn} ⊂Bρ(x†) ∩D(F) such that
F(xn) →F(x†)
in Y,
but
xn /→x†
in X
as n →∞.
In case of local ill-posedness x† cannot be identified arbitrarily precise by noisy data yδ
even if the noise level δ is arbitrarily small. The aspect of local ill-posedness involves

Regularization Methods for Ill-Posed Problems 

both the non-injectivity of F around x† corresponding with () and the local instability
of (> .) corresponding with () in Hadamard’s sense. Wide classes of inverse problems
that have smoothing, for example, compact, forward operators F lead to locally ill-posed
situations.
To overcome the ill-posedness and local ill-posedness of (> .), in particular, to com-
pensate the instability of solutions with respect to small changes in the right-hand side
expressing a deficit of information in the data with respect the solution to be determined,
regularization methods have to be used for the stable approximate solution of (> .) when-
ever only noisy data are given. The basic idea of regularization is to replace the ill-posed
original problem by a well-posed and stable neighboring problem. A regularization param-
eter α > controls the trade-off between closeness of the neighboring problem expressed
by small values α and high stability of the auxiliary problem expressed by large values α.
In the former case the approximate solutions are too unstable, whereas in the latter case
approximate solutions are too far from the original one. On the other hand, the loss of
information in the data caused by smoothing properties of the forward operator F can be
diminished when external a priori information is exploited. This can be done by the choice
of appropriate structure in the neighboring problems.
If the forward operator F and hence the operator
> equation (.) is linear, then in
Hilbert spaces a comprehensive and rather complete regularization theory including a gen-
eral regularization schema, a well-established collection of methods, assertions on stability,
convergence, and convergence rates is available for more than years. See [, , , ].
For recent progress of regularization theory for linear ill-posed problems in a Hilbert
space setting and for extensions to Banach spaces we refer, for example, to the papers
[, , , , , ]. It is well known that inverse problems aimed at the identification of
parameter functions in differential equations or boundary conditions from observations
of state variables are in general nonlinear even if the differential equations are linear (see,
e.g., []). The nonlinearity of F, however, makes the construction and the use of regular-
ization methods more complicated and diversified. In this chapter our focus is on direct
regularization methods, where regularized solutions mostly are solutions of variational
problems, where the functional to be minimized over a set of admissible solutions contains
a regularization parameter α > which has to be controlled in an appropriate manner. An
alternative way of regularization is the solution of (> .) for noisy data yδ by an iteration
process, where the stopping criterion, frequently depending on δ, plays the role of the regu-
larization parameter. For iterative solution methods, we refer to the corresponding chapter
of this book and to the monograph [,].
.
Theory of Direct Regularization Methods
In contrast to the treatment of linear ill-posed problems, where stable approximate solu-
tions (regularized solutions) xδ
α = Rα yδ can be directly obtained by applying bounded
linear operators Rα : Y →X for all regularization parameters α > to the data yδ,
such explicit approach fails if in (> .) either F is nonlinear or D(F) is not a linear


Regularization Methods for Ill-Posed Problems
subspace of X. Both sources of nonlinearity make it necessary to define the regularized
solutions in an implicit manner. The preferred approach of direct regularization methods
is variational regularization or Tikhonov-type regularization (see, e.g., the monographs
[,,,,,,,–] and the survey paper []), where regularized solutions xδ
α are
minimizers of the functional
Φ(x) := S(F(x), yδ) + αR(x)
(.)
by assuming that S is a nonnegative misfit functional measuring the discrepancy between
F(x) and the data yδ, α > is the regularization parameter, and R is a nonnegative
stabilizing functional with small values for element x being reliable and large values for
improbable x. Sometimes it makes sense to replace the noise model (> .) by S(yδ, y) ≤
ψ(δ) with some appropriate increasing positive function ψ tending to zero as δ →. With
respect to imaging, for example, deblurring, image reconstruction, image registration,
and partial differential equations occurring there, different chapters of the monographs
[,,,,,,] and the papers [,,] motivate and discuss regularized solutions
xδ
α as well as different choices of functionals S and R. On the other hand, the minimizers
of (> .) play also an important role in the treatment of statistical inverse problems by
Bayesian methods, maximum a posteriori estimation (MAP) and penalized maximum
likelihood estimation (see, e.g., []), where in some cases the penalty term R(x) can even
be determined by a priori information when the solution x is a realization of a randomized
state variable.
A typical property of ill-posed equations is that minimizing S(F(x), yδ) alone is very
sensitive to data changes and yields in most cases highly oscillating minimizers. For exam-
ple, the least-squares approach ∥F(x)−yδ∥→min as preferred method in Hilbert spaces
shows such behavior. Therefore, the regularization parameter α > in the variational prob-
lem Φ(x) →min, subject to x ∈D, controls the trade-off between optimal data fitting
with unstable solutions if α is near zero and a high level of stability and sympathy but
larger misfit for the approximate solution if α is more far from zero. The set D of admis-
sible solutions in the process of minimizing (> .) is a subset of the intersection of the
domains of F and R. For obtaining a regularized solution xδ
α to a nonlinear inverse prob-
lem, a nonlinear and frequently non-convex optimization problem has to be solved, since
either the functional Φ or the set D(F) can be non-convex. As a consequence, for the
numerical treatment of direct regularization methods in combination with discretization
approaches, iterative procedures are also required. In this context, we omit details here and
refer only to the monographs [, , , ] and to the sample [, , , , ] of papers
from a comprehensive set of publications on numerical approaches.
The appropriate choice of α is one of the most serious tasks in regularization, where a
priori choices α = α(δ) and a posteriori choices α = α(δ, yδ) have to be distinguished.
For a priori choices the decay rate of α(δ) →as δ →is prescribed with the goal that
regularized solutions converge to a solution of (> .), that is, xδ
α(δ) →x† as δ →. Such
convergence can be arbitrarily slow depending on smoothness properties of x†. To obtain
convergence rates ∥xδ
α(δ) −x†∥= O(φ(δ)) as δ →, that means a uniform convergence

Regularization Methods for Ill-Posed Problems 

for some nonnegative increasing rate function φ(δ) with φ() = , additional conditions
on x†, so-called source conditions, have to be satisfied. In contrast to a priori choices
and a posteriori choice of the regularization parameter, α takes into account the present
data yδ and tries to equilibrate the noise level δ and the deviation between F (xδ
α(δ,yδ))
and yδ. For the discrepancy method as the most prominent approach α is chosen origi-
nally such that ∥F (xδ
α(δ,yδ) −x†∥= Cδ with some constant C ≥. Various generalizations
and improvements of this method have been developed (see, e.g., [, , ]). If δ is not
known sufficiently well, then heuristic methods for choosing α = α(yδ) can be exploited
as the quasioptimality method, the L-curve method, and others (see, e.g., [,,]). They
have theoretical drawbacks, since convergence fails in worst case situations, but the utility
of those methods for many classes of applications is beyond controversy.
The choices of S, R, and α = α(δ, yδ) should be realized such that the following
questions Q–Qcan be answered in a positive manner:
Q:
Do minimzers xδ
α of (> .) exist for all α > and yδ ∈Y?
Q:
Do the minimizers xδ
α for fixed α > stably depend on the data yδ?
Q:
Is there a convergence xδ
α(δ,yδ) →x† to a solution x† of (> .) if yδ →y, δ →?
Q:
Are there sufficient conditions imposed on x† for obtaining convergence rates
∥xδ
α −x†∥= O(φ(δ)) as δ →?
Sometimes the requirement of norm convergence is too strong. Then a convergence of reg-
ularized solutions with respect to a weak topology can be of interest. On the other hand, it
may be useful to replace the norm as a measure for the error of regularization by alternative,
for example, the Bregman distance if R is a convex functional.
..
Tikhonov Regularization in Hilbert Spaces with
Quadratic Misﬁt and Penalty Terms
In Hilbert spaces X and Y, quadratic Tikhonov regularization with the functional
Φ(x) := ∥F(x) −yδ∥+ α∥x −x∗∥
(.)
to be minimized over D = D(F) is the most prominent variant of variational regular-
ization of nonlinear ill-posed operator equations, for which the complete theory with
respect to questions Q–Qwas elaborated years ago (see [,]). For a comprehensive
presentation including the convergence rates results we refer to [, Chap. ].
For some initial guess or reference element x∗∈X minimizers of (> .) tend to
approximate x∗-minimum norm solutions x† to (> .) for which
∥x† −x∗∥= min{∥x −x∗∥: F(x) = y, x ∈D(F)}.
Note that x∗-minimum norm solutions need not exist. In case of existence, they need not be
uniquely determined. However, under the following assumption, the existence of a solution


Regularization Methods for Ill-Posed Problems
x† ∈D(F) to (> .) implies the existence of an x∗-minimum norm solution (see [,
Lemma .]).
Assumption 
. The operator F : D(F) ⊆X →Y maps between Hilbert spaces X and Y with a non-empty
domain D(F).
. F is weakly sequentially closed, i.e., weak convergence of the sequences xn ⇀xand
F(xn) ⇀ywith xn ∈D(F), x∈X, y∈Y imply x∈D(F) and F(x) = y.
For checking item of Assumption , it is important to know that in case of weakly
closed and convex domains D(F), the weak continuity of F, i.e., xn ⇀ximplies
F(xn) ⇀F(x), is a sufficient condition. Moreover, we have the following proposition
(see [, Sect. .]) answering the questions Q–Qin a positive manner.
Proposition 
Under Assumption the functional (> .) has a minimizer xδ
α ∈D(F) for
all α > and yδ ∈Y. For fixed α > and a sequence yn →yδ every infinite sequence {xn}
of minimizers to the associated functionals
Φn(x) := ∥F(x) −yn∥+ α∥x −x∗∥
(.)
has a convergent subsequence, and all limits of such subsequences are minimzers xδ
α of (> .).
Whenever the a priori parameter choice α = α(δ) > for δ > satisfies
α(δ) →
and
δ
α(δ) →,
as δ →,
and if (> .) has a solution in D(F), δn →is a sequence of noise levels with corresponding
data yn = yδn such that ∥yn −y∥≤δn, then every the associated sequence {xn} of minimiz-
ers to (> .) has a convergent subsequence, and all limit elements are x∗-minimum norm
solutions x† of (> .).
An answer to question Qconcerning convergence rates is given by the following
theorem along the lines of [, Theorem .].
Theorem 
In addition to Assumption let D(F) be convex and x† be an x∗-minimum
norm solution to (> .) such that
∥F(x) −F(x†) −A(x −x†)∥≤L
∥x −x†∥
for all x ∈D(F) ∩Bρ(x†)
(.)
with positive constants ρ, L and some bounded linear operator A : X →Y satisfying a source
condition
x† −x∗= A∗w
(.)
where A∗: Y →X is the adjoint operator to A and w ∈Y is some source element fulfilling
the smallness condition
L ∥w∥< .
(.)

Regularization Methods for Ill-Posed Problems 

Then we obtain the error estimate
∥xδ
α −x†∥≤
δ + α∥w∥
√α
√
−L∥w∥
and for the a priori parameter choice cδ ≤α(δ) ≤cδ with constants < c ≤c < ∞the
convergence rate
∥xδ
α(δ) −x†∥= O(
√
δ)
as δ →.
(.)
The operator A in (> .) must be considered as a linearization of F at the point x† in
the sense of a Gâteaux or Fréchet derivative F′(x†). The condition (> .) characterizes
the structure of nonlinearity of the forward operator F in a neighborhood of x†. If the
Fréchet derivative F′(x) exists and is locally Lipschitz continuous around x† with Lipschitz
constant L > , then (> .) is fulfilled.
For further convergence rates results of Tikhonov regularization in Hilbert spaces with
quadratic penalty term we refer, for example, to [,,,,].
..
Variational Regularization in Banach Spaces
with Convex Penalty Term
In Banach spaces X and Y, the wide variety of variational regularization realized by
minimizing the functional (> .) allows us to establish a priori information about the
noise model and the solution x† to be determined in a more sophisticated manner than
Tikhonov regularization in Hilbert spaces with quadratic misfit and penalty terms. Knowl-
edge of the specific situation motivates the selection of the functionals S and R, where we
consider here norm powers
S(y, y) := ∥y−y∥p,
p > , y, y∈Y
as misfit functional, which, for example, simplifies the numerical treatment of minimiza-
tion problems if Y is a Lebesgue space Y = Lp(Ω) or a Sobolev space Y = W l,p(Ω) with
< p < ∞, Ω ⊂Rk. We refer to the papers [,] for a further discussion of alternative
misfit functionals S. In most cases convex penalty functionals R are preferred. An impor-
tant class of penalties form the norm functionals R(x) := ∥x∥q
˜X, q > , x ∈˜X, where as an
alternative to the case X = ˜X the space ˜X can also be chosen as a dense subspace of X with
stronger norm, for example, X = Lq(Ω), ˜X = W l,q(Ω), l = ,,.... To reconstruct non-
smooth solutions x†, the exponent q can be chosen smaller than two, for example, close
to one. To recover solutions for which the expected smoothness is lower, penalty terms
R(x) = TV(x) can be applied, where TV(x) = ∫
Ω
∣∇x∣expresses the total variation of the
function x (see, e.g., [,,,]). For specific applications in imaging (see [, Chap. ])
and to handle sparsity of solutions (see [, ] and [, Sect. .]) the systematic use of


Regularization Methods for Ill-Posed Problems
non-convex misfit and penalty functionals can be appropriate. In the sequel, however, we
focus in this section on the functional
Φ(x) := ∥F(x) −yδ∥p + αR(x),
< p < ∞,
(.)
with a convex penalty functional R to be minimized over D = D(F) ∩D(R) yielding
minimizers xδ
α .
Assumption 
. The operator F : D(F) ⊆X →Y maps between reflexive Banach spaces X and Y with
duals X∗and Y∗, respectively.
. F is weakly sequentially closed, D(F) is weakly closed, and D := D(F) ∩D(R) is
non-empty.
. The functional R is convex and weakly sequentially lower semi-continuous.
. For every α > , c ≥, and for the exact right-hand side y of (> .), the level sets
Mα(c) := {x ∈D : ∥F(x) −y∥p + αR(x) ≤c}
(.)
are weakly sequentially pre-compact in the following sense: every infinite sequence {xn}
in Mα(c) has a subsequence, which is weakly convergent in X to some element from X.
Under Assumption existence and stability of regularized solutions xδ
α can be shown
(see [, Sect. ]), that is, the questions Qand Qabove get a positive answer. In Banach
spaces, regularization errors are frequently measured, for the convex functional R with
subdifferential ∂R, by means of Bregman distances
Dξ(˜x, x) := R(˜x) −R(x) −⟨ξ, ˜x −x⟩,
˜x ∈D(R) ⊆X,
at x ∈D(R) ⊆X and ξ ∈∂R(x) ⊆X∗, where ⟨ˆx, x⟩denotes the dual pairing with
respect to x ∈X and ˆx ∈X∗. The set DB(R) := {x ∈D(R) : ∂R(x) /= /} is called Bregman
domain. An element x† ∈D is called an R-minimizing solution to (> .) if
R(x†) = min {R(x) : F(x) = y, x ∈D} < ∞.
Such R-minimizing solutions exist under Assumption if (> .) has a solution x ∈D.
Following [, Sect. l] and [] we present in the following some results on the
regularization theory for that setting.
Under an a priori parameter choice α = α(δ) > satisfying
α(δ) →
and
δ p
α(δ) →,
as δ →,
we can answer Qand have weak convergence in analogy to Proposition . For results on
strong convergence we refer, for example, to Proposition .in [].
For given αmax > let x† denote an R-minimizing solution of (> .). If we set
ρ := p−αmax(+ R(x†)),
(.)

Regularization Methods for Ill-Posed Problems 

then we have x† ∈Mαmax(ρ) and there exists some δmax > such that
xδ
α(δ) ∈Mαmax(ρ)
for all ≤δ ≤δmax.
Convergence rates results for the variational regularization of nonlinear problems (see
question Qabove), both the smoothness of R-minimizing solutions x† and the smooth-
ing properties of the nonlinear forward operator F in a neighborhood of x† are essential.
With respect to operator properties, we exploit the concept of a degree of nonlinearity
from [].
Deﬁnition 
Let c, c≥and c+ c> . We define F to be nonlinear of degree (c, c)
for the Bregman distance Dξ(⋅, x†) of R at a solution x† ∈DB(R) ⊆X of (> .) with
ξ ∈∂R(x†) ⊆X∗if there is a constant K > such that
∥F(x) −F(x†) −F′(x†)(x −x†)∥≤K ∥F(x) −F(x†)∥cDξ(x, x†) c
(.)
for all x ∈Mαmax(ρ).
On the other hand, the solution smoothness of x† in combination with a well-defined
degree of nonlinearity can be expressed in an efficient manner by variational inequalities
⟨ξ, x† −x⟩≤βDξ(x, x†) + β∥F(x) −F(x†)∥κ
for all x ∈Mαmax(ρ)
(.)
with some ξ ∈∂R(x†), two multipliers ≤β< , β≥, and an exponent κ > 
for obtaining convergence rates. The subsequent theorem (for a proof see []) shows
the utility of such variational inequalities for ensuring convergence rates in variational
regularization (for more details in the case κ = see also [] and [, Sect. .]).
Theorem 
For variational regularization with regularized solutions xδ
α minimizing
(> .) under Assumption and provided that there is an R-minimizing solution x† ∈
DB(R) we have the convergence rate
Dξ (xδ
α(δ), x†) = O (δκ)
as
δ →
(.)
for an a priori parameter choice α(δ) ≍δ p−κ if there exist an element ξ ∈∂R(x†) and
constants ≤β< , β≥, < κ ≤, such that the variational inequality (> .) holds
with ρ from (> .).
This result which is based on Young’s inequality can immediately be extended to the
situation < κ < p ≤. Moreover, the case κ = p ≤characterizes the so-called exact
penalization, where regularized solutions and x† coincide if noise-free data yδ = y are
used. For noisy data and κ = p ≤we have Dξ (xδ
α, x†) = O (δ p) as δ →for a regular-
ization parameter α = αwhich is fixed but sufficiently small (see []). An extension of
such results to convergence rates of higher order is outlined in [].


Regularization Methods for Ill-Posed Problems
To verify different situations for the exponent κ > we restrict the setting as follows:
Assumption 
. F,R,D, X, and Y satisfy Assumption .
. Let x† ∈D be an R-minimizing solution of (> .).
. The operator F is Gâteaux differentiable in x† with the Gâteaux derivative
F′(x†) ∈L(X, Y), where L(X, Y) denotes the space of bounded linear operators from
X to Y.
. The functional R is Gâteaux differentiable in x† with the Gâteaux derivative
ξ = R′(x†) ∈X∗, i.e., the subdifferential ∂R(x†) = {ξ} is a singleton.
The following proposition (see [, Proposition .]) shows that exponents κ > in the
variational inequality (> .) under Assumption in principle cannot occur.
Proposition 
Under the Assumption the variational inequality (> .) cannot hold
with ξ = R′(x†) /= and multipliers β, β≥whenever κ > .
Now the following proposition will highlight the borderline case κ = and the cross-
connections between variational inequalities and source conditions for the Banach space
setting. Moreover, in
> Sect. ..the interplay with (> .) and generalized source
condition can be discussed.
Proposition 
Under Assumption the following two assertions hold:
. The validity of a variational inequality
⟨ξ, x† −x⟩≤βDξ(x, x†) + β∥F(x) −F(x†)∥
for all
x ∈Mαmax(ρ)
(.)
for ξ = R′(x†) and two multipliers β, β≥implies the source condition
ξ = F′(x†)∗w,
w ∈Y∗.
(.)
. Let F be nonlinear of degree (,) for the Bregman distance Dξ(⋅, x†) of R at x†, i.e., we
have
∥F(x) −F(x†) −F′(x†)(x −x†)∥≤K Dξ(x, x†)
(.)
for a constant K > and all x ∈Mαmax(ρ). Then the source condition (> .) together
with the smallness condition
K ∥w∥Y∗< 
(.)
imply the validity of a variational inequality (> .) with ξ = R′(x†) and multipliers
≤β= K∥w∥Y∗< , β= ∥w∥Y∗≥.
Sufficient conditions for the validity of a variational inequality (> .) with fractional
exponents < κ < are formulated in [] based on the method of approximate source
conditions using appropriate distance functions that measure the degree of violation of the

Regularization Methods for Ill-Posed Problems 

source condition (> .) for the solution x†. Assertions on convergence rates for that case
can be made when the degree of nonlinearity is such that c> as the next proposition
shows.
Proposition 
Under Assumption let F be nonlinear of degree (c, c) with
< c≤, ≤c< , c+ c≤for the Bregman distance Dξ(⋅, x†) of R at x†, i.e.,
we have
∥F(x) −F(x†) −F′(x†)(x −x†)∥≤K ∥F(x) −F(x†)∥cDξ(u, x†)c
(.)
for a constant K > and all x ∈Mαmax(ρ). Then the source condition (> .) immediately
implies the validity of a variational inequality (> .) with
κ =
c
−c
,
(.)
ξ = R′(x†) and multipliers ≤β< , β≥.
..
Extended Results for Hilbert Space Situations
We are going to illustrate now the abstract concepts of
> Sect. ..for the Tikhonov
regularization with quadratic functionals under Assumption , but with Assumption in
Hilbert spaces. For
R(x) := ∥x −x∗∥
the R-minimizing solutions and the classical x∗-minimum norm solutions coincide.
Moreover, we have D = D(F) and for ξ and Dξ(˜x, x) the simple structure
ξ = (x† −x∗)
and
Dξ(˜x, x) = ∥˜x −x∥
with Bregman domain DB(R) = X. Then the source condition (> .) attains the form
(> .) with A = F′(x†)/.
To focus on the distinguished character of the Hilbert space X and Y setting we will
specify the Definition as follows:
Deﬁnition 
Let c, c≥and c+ c> . We define F to be nonlinear of degree (c, c)
at a solution x† ∈D(F) of (> .) if there is a constant K > such that
∥F(x) −F(x†) −F′(x†)(x −x†)∥≤K ∥F(x) −F(x†)∥c∥x −x†∥c
(.)
for all x ∈Mαmax(ρ).
Furthermore, in Hilbert spaces Hölder source conditions
ξ = (F′(x†)∗F′(x†))η/v,
v ∈X,
(.)


Regularization Methods for Ill-Posed Problems
can be formulated that allow us to express a lower level of solution smoothness of x† for
< η < compared to the case η = , where (> .) is equivalent to
ξ = F′(x†)∗w,
w ∈Y
(cf. condition (> .) in Theorem ). For that situation of lower smoothness, the following
theorem (see [, Proposition .]) complements Theorem .
Theorem 
Under the Assumption let the operator F mapping between the Hilbert
spaces X and Y be nonlinear of degree (c, c) at x† with c
>
and let with
R(x) = ∥x −x∗∥the element ξ = (x† −x∗) satisfy the Hölder source condition (> .).
Then we have the variational inequality (> .) with exponent
κ = min {
ηc
+ η(−c) ,
η
+ η },
< η ≤,
(.)
for all x ∈Mαmax(ρ) and multipliers ≤β< , β≥. Consequently, we have for
regularized solutions xδ
α(δ) minimizing (> .) the convergence rate
∥xδ
α(δ) −x†∥= O (δκ/)
as
δ →
(.)
for an a priori parameter choice α(δ) ≍δ p−κ.
For parameter identification problems in partial differential equations (cf., e.g., [,]),
which can be written as nonlinear operator
> equation (.) with implicitly given for-
ward operators F, it is difficult to estimate the Taylor remainder ∥F(x) −F(x†) −F′(x†)
(x −x†)∥and the variational inequality approach may fail. However, if in addition to
the Hilbert space X a densely defined subspace ˜X with stronger norm is considered,
then in many applications for all R > there hold conditional stability estimates of the
form
∥x−x∥≤K ∥F(x) −F(x)∥κ
if
xi ∈D(F) ∩˜X, ∥xi∥˜X ≤R (i = ,)
(.)
with some < κ ≤and a constant K = K(R) > , which may depend on the radius R.
Assumption 
. Let X and Y be Hilbert spaces with norms ∥⋅∥.
. Let B : D(B) ⊂X →X be an unbounded injective, positive definite, self-adjoint linear
operator with domain ˜X = D(B) dense in X. Furthermore let ˜C > be a constant such
that ∥x∥˜X := ∥Bx∥≥˜C ∥x∥(x ∈˜X) such that ˜X becomes a Hilbert space with norm
∥⋅∥˜X stronger than ∥⋅∥.
. Let R(x) := ∥Bx∥= ∥x∥
˜X with D(R) = ˜X.
Then along the lines of the paper [] we can formulate the following theorem.

Regularization Methods for Ill-Posed Problems 

Theorem 
For the nonlinear operator F : D(F) ⊆X →Y mapping between the Hilbert
spaces X and Y we consider under Assumption regularized solutions xδ
α as minimizers over
D = D(F) ∩˜X of the functional
Φ(x) := ∥F(x) −yδ∥+ α ∥x∥
˜X.
(.)
Moreover, for all R > let hold a conditional stability estimate of the form (> .) with some
< κ ≤and a constant K = K(R) > . Then for a solution x† ∈D of
> equation (.) we
obtain the convergence rate
∥xδ
α(δ) −x†∥= O (δκ)
as
δ →
(.)
with an a priori parameter choice cδ≤α(δ) ≤cδfor constants < c ≤c < ∞.
.
Examples
In this section, we will present several examples of parameter identification problems in
integral and differential equations in order to show how to apply the regularization the-
ory outlined above to specific ill-posed and inverse problems. The examples refer either to
nonlinear inverse problems, which can be formulated as operator
> equations (.) with
forward operator F mapping from a Hilbert space X to a Hilbert space Y or to linearizations
of such problems, which then appear as linear operator equations. All discussed examples
originally represent ill-posed problems in the sense that small data changes may lead to
arbitrarily large errors in the solution. If the forward operator F is linear, then this phe-
nomenon can be characterized by the fact that the range of the operator F is a non-closed
subspace in Y. For nonlinear F such simple characterization fails, but a local version of ill-
posedness (see []) takes place in general. In order to make clear the cross-connections to
the theory, as in the previous sections we denote the unknown parameter functions by x, in
particular the exact solution to (> .) by x†, and we denote the exact and noisy data by y
and yδ, respectively. For conciseness, we restrict ourselves to five examples. More examples
can be found in the corresponding references of this book.
Example (Identification of coefficients in wave equations)
Let Ω ⊂Rn, n = ,,, be a
bounded domain with C-boundary ∂Ω. We consider
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
∂u
∂t(ζ, t) = Δu(ζ, t) + x(ζ)u(ζ, t),
ζ ∈Ω, < t < T,
u(ζ,) = a(ζ),
∂u
∂t (ζ,) = b(ζ),
ζ ∈Ω,
∂u
∂(ζ, t) = ,
ζ ∈∂Ω,
< t < T.
(.)
Here and henceforth
∂
∂
denotes the normal derivative. We fix initial values a and b
such that
a ∈H(Ω),
b ∈H(Ω),
∂a
∂
∣∂Ω×(,T) = ∂b
∂∣
∂Ω×(,T)
= .
(.)


Regularization Methods for Ill-Posed Problems
Then for any function x ∈W,∞(Ω), there exists a unique solution
u(x) = u(x)(ζ, t) ∈C([, T]; H(Ω)) ∩C([, T]; H(Ω)) ∩C([, T]; H(Ω))
to (> .) (see, e.g., []).
Our inverse problem here consists in the identification of the parameter function
x = x(ζ), ζ ∈Ω, occurring in the hyperbolic partial differential equation based on noisy
observations yδ of time derivatives y of the state variable [u(x)](ζ, t) on the boundary
(ζ, t) ∈∂Ω × (, T). In addition to (> .), let us assume
T > min
ζ′∈Ω
max
ζ∈Ω
∣ζ −ζ′∣
(.)
and
∣a(ζ)∣> ,
ζ ∈Ω.
(.)
Moreover, we set
UM = {x ∈W,∞(Ω) : ∥x∥W,∞(R) ≤M}
(.)
for M > .
In [], it is proved that there exists a constant C = C(Ω, T, a, b, M) > such that
∥x−x∥L(Ω) ≤C ∥∂
∂t(u(x) −u(x))∥
H(∂Ω×(,T))
(.)
for all x, x∈UM.
We define the forward operator F from the space X = L(Ω) to the space Y = H(∂Ω×
(, T)) according to
[F(x)](ζ, t) := ∂u(x)
∂t
∣∂Ω×(,T),
(ζ, t) ∈∂Ω × (, T).
This is a nonlinear operator mapping between the Hilbert spaces X and Y and ill-posedness
of the corresponding operator equation can be indicated. However, the estimate (> .)
shows that this inverse problem possesses good stability properties if we restrict the set
of admissible solutions suitably or if we choose the regularization term in an appropriate
manner.
A Tikhonov regularization approach as outlined in the previous sections is useful. If
we choose the functional Φ as
Φ(x) = ∥F(x) −yδ∥
Y + α∥x −x∗∥
X,
the theory applies. Alternatively, we can also choose the penalty term by using a stronger
norm. In this case, the functional Φ is chosen as
Φ(x) = ∥F(x) −yδ∥
Y + α∥x∥
̃X,
where ̃X = W,∞(Ω).

Regularization Methods for Ill-Posed Problems 

Then by the conditional stability estimation (> .), we obtain here the convergence
rate
∥xδ
α −x†∥
L(Ω) = O(δ)
as
δ →
with the choice α = δ.
Example (Determination of shapes of boundaries)
Using polar coordinates (r, θ) we
are going here to identify the shape of a boundary in R. For M > and < m< m< ,
we set
Um,M = {x = x(θ) ∈C[,π] : dkx
dθk () = dkx
dθk (π), k = ,,,
∥x∥C[,π] ≤M,
∥x∥C[,π] ≤m}
and
Qm= {x ∈C[,π] : x(θ) ≥m, ≤θ ≤π}.
Now with a function x ∈Um,M let Ω(x) ⊂Rdenote a domain being a subset of the
unit circle, which is bounded by the curve γ(x) = {ζ = (r, θ) : r = x(θ), ≤θ ≤π}. We
consider the Laplacian field in Ω(x):
⎧⎪⎪⎨⎪⎪⎩
Δu = 
in
Ω(x),
u ∣γ(x) = ,
u ∣Γ = ψ,
(.)
where ψ ∈C(Γ) is fixed and ψ ≥does not vanish identically on Γ. Then, there exists a
unique classical solution u(x) = u(x)(ζ) to (> .).
Our inverse problem in this example is aimed at the identification of the interior sub-
boundary γ(x) from noisy data yδ of y := ∂u(x)
∂
∣Γ′, where Γ′ is an arbitrary relatively open
subset of Γ.
In the paper [], a uniqueness assertion was proved, namely that we can conclude, for
x, x∈Um,M ∩Qm, from the equality of two potential flux functions
∂u(x)
∂
= ∂u(x)
∂
on Γ′
that
x(θ) = x(θ),
≤θ ≤π.
Moreover, there exists a constant C = C(m, m, M,ψ) > such that
∥x−x∥C[,π] ≤
C
∣log∥∂u(x)
∂
−∂u(x)
∂
∥
C(Γ′)∣
(.)
for all x, x∈Um,M ∩Qm.
We fix the Banach spaces X and Y here as
X = C[,π],
Y = C(Γ′),


Regularization Methods for Ill-Posed Problems
and introduce the forward operator by the assignment
F(x) := ∂u(x)
∂
∣
Γ′
.
Taking into account the intrinsic ill-posedness of this inverse problem we nevertheless
see that the estimate (> .) shows some weak, that is, logarithmic, stability. This allows
us to overcome the ill-posedness here again if we choose the admissible set suitably or if
we apply Tikhonov regularization in an appropriate way.
The theory of the preceding sections applies if we choose the functional Φ as
Φ(x) = ∥F(x) −yδ∥
Y + αR(x)
with R(x), a convex penalty term or if we choose the penalty term with some stronger
norm leading to
Φ(x) = ∥F(x) −yδ∥
Y + α∥x∥
̃X,
where ̃X = Qm∩Z and
Z = {x ∈C[,π] :
dkx
dθk () = dkx
dθk (π), k = ,,}.
The conditional stability estimation (> .) gives the convergence rate
∥xδ
α −x†∥
C[,π] = O (

∣log δ∣)
as
δ →
for the parameter choice α = δ.
Similar inverse problems are discussed in the papers [,]. The regularization methods
outlined above can be used to treat those inverse problems, too.
Example (Integral equation of the first kind with analytic kernel)
Let D and Dbe
simple connected bounded domains in Rsuch that D ∩D= /. We consider an integral
equation of the first kind:
[F(x)](η) := ∫D
x(ζ)
∣η −ζ∣dζ = y(η),
η ∈D.
(.)
This type of integral equation is derived in the context of models for nondestructive
testing (see []). The original inverse problem there is a nonlinear one. The integral equa-
tion (> .), however, can be considered as a linearization of the original problem. It was
shown in [] that the linearized problem (> .) is close to the original problem under
some assumptions on the size of domain D.
By D ∩D= /, the kernel

∣η−ζ∣is analytic in η ∈Dand ζ ∈D, so that (> .) appears
as a severely ill-posed linear operator equation.
In the paper [], it was proved that if there are two functions x, x∈L(D) such that
the corresponding y, ysatisfy
y(η) = y(η),
η ∈D,

Regularization Methods for Ill-Posed Problems 

then we have
x(ζ) = x(ζ),
ζ ∈D.
Moreover, the following conditional stability is proved: Let us fix q > and
UM = {x ∈W,q
(D) : ∥x∥W,q

(D) ≤M}.
Then, there exists a constant C = C(q, M, D, D) > such that
∥x∥L(D) ≤
C
∣log∥y∥H(D)∣
(.)
for all x ∈UM.
The linear forward operator F maps here from the space X = L(D) to the space
Y = H(D). In spite of the original ill-posedness of the operator equation the esti-
mate (> .) shows again logarithmic stability after appropriate restriction of the set of
admissible solutions.
Variational regularization with the Tikhonov functional
Φ(x) = ∥F(x) −yδ∥
Y + α∥x −x∗∥
X
or alternatively with
Φ(x) = ∥F(x) −yδ∥
Y + α∥x∥
̃X
for ̃X = W,q
(D) allows the application of the general theory to that example. In particular,
the conditional stability estimation (> .) provides us with the convergence rate
∥xδ
α −x†∥
L(D) = O (

∣log δ∣)
as
δ →
whenever the a priori choice α = δof the regularization parameter is used.
Example (Identification of wave sources)
Let Ω ⊂Rn be a bounded domain with
C-boundary ∂Ω. We consider
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∂u
∂t(ζ, t) = Δu(ζ, t) + λ(t)x(ζ),
x ∈Ω, < t < T,
u(ζ,) = ∂u
∂t (ζ,) = ,
ζ ∈Ω,
u(ζ, t) = ,
ζ ∈∂Ω,
< t < T.
(.)
We assume that
λ ∈C∞[,∞),
λ() ≠,
(.)
and we fix such λ. Then for any function x ∈L(Ω), there exists a unique solution
u(x) ∈C ([, T]; H(Ω) ∩H
(Ω)) ∩C([, T]; H
(Ω)) ∩C([, T]; L(Ω)).
Our inverse problem is the identification of x = x(ζ), ζ ∈Ω from observations yδ
of y := ∂u(x)
∂
∣∂Ω×(,T). Corresponding uniqueness and conditional stability results can be
found in [].


Regularization Methods for Ill-Posed Problems
Let
κ ≠
, 
,
≤κ ≤
and let M > be arbitrarily given. We set
Xκ =
⎧⎪⎪⎨⎪⎪⎩
Hκ(Ω),
≤κ < 
,
Hκ
(Ω),

< κ ≤, κ ≠
,
where Hκ(Ω), Hκ
(Ω) denote the Sobolev spaces, and
UM,κ = {x ∈Xκ : ∥x∥Hκ(Ω) ≤M}.
Furthermore, we assume
T > diam Ω ≡sup
x,x′∈Ω
∣ζ −ζ′∣.
(.)
Then, it is proved that there exists a constant C = C(Ω, T, λ, κ) > such that
∥x−x∥L(Ω) ≤CM

κ+∥∂u(x)
∂
−∂u(x)
∂
∥
κ
κ+
L(∂Ω×(,T))
(.)
for all x, x∈UM,κ.
The definition
F(x) = y := ∂u(x)
∂
∣
∂Ω×(,T)
of the forward operator F : X →Y is well defined for the Hilbert spaces X = L(Ω) and
Y = L(∂Ω×(, T)).In contrast to Example , where also a wave equation is under consid-
eration, F appears here as a linear operator with nonclosed range. However, the estimate
(> .) shows that this inverse problem possesses even Hölder type stability if we choose
the admissible set suitably.
With respect to the regularization methods from the previous sections, we can choose
the functional Φ as
Φ(x) = ∥F(x) −yδ∥
Y + α∥x −x∗∥
X.
or as
Φ(x) = ∥F(x) −yδ∥
Y + α∥x∥
̃X,
where ̃X = Xκ. Then the conditional stability estimation (> .) gives here the Hölder
convergence rate
∥xδ
α −x†∥
L(Ω) = O (δ
κ
κ+)
as
δ →
with the choice α = δ.
Example (Identification of potential in an elliptic equation)
Let Ω be a simply con-
nected domain in Rwith the C-boundary ∂Ω. We consider the following problem
⎧⎪⎪⎨⎪⎪⎩
Δu + x ⋅u = ,
in Ω
u = f ,
on ∂Ω
(.)
with functions x ∈L(Ω) and f ∈H

(∂Ω).

Regularization Methods for Ill-Posed Problems 

Assume that zero is not the Dirichlet eigenvalue of the Schrödinger operator Δ + x on
the domain Ω, we know that there exists unique solution u ∈H(Ω) for this problem. Then
we can define the Dirichlet-to-Neumann map Λx : H

(∂Ω) →H−
(∂Ω) as
Λx f = ∂u
∂∣
∂Ω
,
(.)
where
is the unit outer normal with respect to ∂Ω.
The inverse problem under consideration here addresses the recovery of the not directly
observable potential function x(ζ), for ζ ∈Ω, from data y delivered by Λx. We will specify
this as follows: We consider an infinite sequence {VN}∞
N=of N-dimensional subspaces
VN = span(f, f,..., fN) generated by a basis {fj}∞
j=in H(∂Ω), that is, we have
VN ⊂VN+⊂H(∂Ω)
and
∞
⋃
N=
VN is dense in H(∂Ω).
In this context, we assume that the finite dimensional spaces VN, N = ,, . . . , have the
following properties:
. For any g ∈H(∂Ω), there exists a gN ∈VN and a function β(N), which satisfies
limN→∞β(N) = , such that
∥g −gN∥H

(∂Ω) ≤β(N)∥g∥H(∂Ω).
(.)
. There exists a constant C > , which is independent of g, such that
∥gN∥H

(∂Ω) ≤C∥g∥H

(∂Ω).
(.)
The following result is proved in []: Suppose that xj ∈Hs(Ω), j = ,, with s > 
,
satisfy
∥x j∥Hs(Ω) ≤M
for some constant M > . Then, there exists a constant C > , which depends on M, such
that
∥x−x∥L(Ω) ≤Cω (∥Λx−Λx∥VN + β(N))
(.)
for N large enough and ∥Λx−Λx∥VN small enough. Precisely, we have here ω(t) = (

log 
t )
γ
with some < γ ≤taking into account that
∥Λx−Λx∥VN =
sup
ϕ∈VN ,∥ϕ∥
H

(∂Ω)
=
∣⟨(Λx−Λx)ϕ, ϕ⟩∣,
where ⟨⋅,⋅⟩is the dual pairing between H−
(∂Ω) to H

(∂Ω).
Here, we define the forward operator F as
F(q) := Λ∣YN.
This is a nonlinear operator mapping from the space X = L(Ω) into the space Y ∈
L(L(∂Ω), H

(∂Ω)), which represents the space of bounded linear operators mapping


Regularization Methods for Ill-Posed Problems
between L(∂Ω) and H

(∂Ω). Moreover, we consider the restriction YN = Y ∣VN of Y
generated by the subspace VN. The original problem of finding x from Λx data is ill posed,
but even without the uniqueness of the inverse problem the estimate (> .) shows some
stability behavior of logarithmic type under the associated restrictions on the expected
solution. Again for the Tikhonov regularization with functionals
Φ(x) = ∥F(x) −yδ∥
YN + α∥x −x∗∥
X
or
Φ(x) = ∥F(x) −yδ∥
YN + α∥x∥
̃X,
where ̃X = Hs with s > 
, the theory of > Sects. ..and
> ..is applicable. From the
latter section, we derive with the conditional stability estimation (> .) the convergence
rate
∥xδ
α −x†∥
L(Ω) = O ((log(/δ))−γ)
as
δ →
for the parameter choice α = δ.
.
Conclusions
In this chapter, we have presented some theoretic results including convergence and con-
vergence rates assertions on direct regularization methods for nonlinear inverse problems
formulated in the setting of infinite dimensional Hilbert or Banach spaces. The inverse
problems can be written as ill-posed nonlinear operator equations with the consequence
that their solutions tend to be unstable with respect to data perturbations. To overcome
that drawback, regularization methods use stable auxiliary problems, which are close to
the original inverse problem. A regularization parameter controls the trade-off between
approximation and stability. For direct regularization methods, the auxiliary problems
are mostly minimization problems in abstract spaces, where a weighted sum of a resid-
ual term that expresses the data misfit and a penalty term expressing expected solution
properties has to be minimized. In this context, the regularization parameter controls the
relative weight of both terms. Furthermore, five examples are given that show the wide
range of applicability for such regularization methods in the light of specific inverse prob-
lems. Eighty references at the end of this chapter survey the relevant literature in this
field.
.
Cross-References
> Iterative Solution Methods
> Linear Inverse Problems
> Numerical Methods for Variational Approach in Image Analysis

Regularization Methods for Ill-Posed Problems 

> Statistical Inverse Problems
> Total Variation in Imaging
> Variational Approach in Image Analysis
References and Further Reading
. Acar R, Vogel CR () Analysis of bounded
variation penalty methods for ill-posed prob-
lems. Inverse Probl ():–
. Ammari H () An introduction to mathemat-
ics of emerging biomedical imaging. Springer,
Berlin
. Bakushinsky A, Goncharsky A () Ill-posed
problems: theory
and
applications.
Kluwer,
Dordrecht
. Bakushinsky AB, Kokurin MYu () Itera-
tive methods for approximate solution of inverse
problems. Springer, Dordrecht
. Banks HT, Kunisch K () Estimation tech-
niques
for
distributed
parameter
systems.
Birkhäuser, Boston
. Baumeister J () Stable solution of inverse
problems. Vieweg, Braunschweig
. Beretta E, Vessella S () Stable determination
of boundaries from Cauchy data. SIAM J Math
Anal :–
. Bertero M, Boccacci P () Introduction to
inverse problems in imaging. Institute of Physics
Publishing, Bristol
. Bonesky T, Kazimierski K, Maass P, Schöpfe F,
Schuster T () Minimization of Tikhonov
functionals in Banach spaces. Abstr Appl Anal
Art :–
. Bredies K, Lorenz DA () Regularization with
non-convex separable constraints. Inverse Probl
():–, 
. Bukhgeim AL, Cheng J, Yamamoto M ()
Stability for an inverse boundary problem of
determining a part of a boundary. Inverse Probl
:–
. Burger M, Osher S () Convergence rates of
convex variational regularization. Inverse Probl
():–
. Burger M, Resmerita E, He L () Error esti-
mation for Bregman iterations and inverse scale
space methods in image restoration. Computing
(–):–
. Chavent G () Nonlinear least squares for
inverse problems. Springer, Dordrecht
. Chavent G, Kunisch K () On weakly nonlin-
ear inverse problems. SIAM J Appl Math ():
–
. Chavent G, Kunisch K () State space reg-
ularization: geometric theory. Appl Math Opt
():–
. Cheng J, Nakamura G () Stability for the
inverse potential problem by finite measurements
on the boundary. Inverse Probl :–
. Cheng J, Yamamoto M () Conditional stabi-
lizing estimation for an integral equation of first
kind with analytic kernel. J Integral Equat Appl
:–
. Cheng J, Yamamoto M () One new strat-
egy for a priori choice of regularizing parame-
ters in Tikhonov’s regularization. Inverse Probl
():L–L
. Colton D, Kress R () Inverse acoustic
and electromagnetic scattering theory. Springer,
Berlin
. Engl HW, Hanke M, Neubauer A (/)
Regularization of inverse problems. Kluwer,
Dordrecht
. Engl HW, Isakov V () On the identifiabil-
ity of steel reinforcement bars in concrete from
magnetostatic measurements. Eur J Appl Math
:–
. Engl HW, Kunisch K, Neubauer A () Con-
vergence rates for Tikhonov regularisation of
non-linear ill-posedproblems.InverseProbl ():
–
. Engl HW, Zou J () A new approach to con-
vergence rate analysis of Tikhonov regularization
for parameter identification in heat conduction.
Inverse Probl ():–
. Favaro P, Soatto S () -D shape estimation
and image restoration. Exploiting defocus and
motion blur. Springer, London


Regularization Methods for Ill-Posed Problems
. Fischer
B,
Modersitzki
J
()
Ill-posed
medicine – an introduction to image registration.
Inverse Probl (): –, 
. Flemming J, Hofmann B () A new approach
to source conditions in regularization with gen-
eral residual term. Numer Funct Anal Optimiz
():–
. Grasmair M, Haltmeier M, Scherzer O ()
Sparse regularization with ℓq penalty term.
Inverse Probl ():–, 
. Gorenflo R, Hofmann B () On autoconvo-
lution and regularization. Inverse Probl ():
–
. Groetsch CW () The theory of Tikhonov reg-
ularization for Fredholm integral equations of the
first kind. Pitman, Boston
. Hansen PC () Rank-deficient and discrete
ill-posed poblems. SIAM, Philadelphia
. Hein T () Tikhonov regularization in
Banach spaces – improved convergence rates
results. Inverse Probl ():–, 
. Hein T, Hofmann B () Approximate source
conditions for nonlinear ill-posed problems –
chances and limitations. Inverse Probl ():–,

. Hofmann
B,
Kaltenbacher
B,
Pöschl
C,
Scherzer O () A convergence rates result
for Tikhonov regularization in Banach spaces
with
non-smooth
operators.
Inverse
Probl
():–
. Hofmann B, Mathé P () Analysis of profile
functions for general linear regularization meth-
ods. SIAM J Num Anal ():–
. Hofmann B, Mathé P, Pereverzev SV () Reg-
ularization by projection: approximation theo-
retic aspects and distance functions. J Inv Ill-
Posed Problems ():–
. Hofmann B, Scherzer O () Factors influ-
encing the ill-posedness of nonlinear problems.
Inverse Probl ():–
. Hofmann B, Yamamoto M () On the inter-
play of source conditions and variational inequal-
ities for nonlinear ill-posed problems. Appl Anal
:doi ./
. Imanuvilov OYu, Yamamoto M () Global
uniqueness and stability in determining coef-
ficients of wave equations. Comm Partial Diff
Equat :–
. Isakov V () Inverse problems for partial dif-
ferential equations. Springer, New York
. Ito K, Kunisch K () On the choice of the reg-
ularization parameter in nonlinear inverse prob-
lems. SIAM J Optimiz ():–
. Janno J, Wolfersdorf Lv () A general class
of autoconvolution equations of the third kind.
Z Anal Anwend ():–
. Jin B, Zou J () Aug mented Tikhonov regu-
larization. Inverse Probl ():–, 
. Kaipio J, Somersalo E () Statistical and
computational
inverse
problems.
Springer,
New York
. Kaltenbacher B () A note on logarithmic
convergence rates for nonlinear Tikhonov regu-
larization. J Inv Ill-Posed Probl ():–
. Kaltenbacher B, Neubauer A, Scherzer O ()
Iterative regularization methods for nonlinear ill-
posed problems. Walter de Gruyter, Berlin
. Kirsch A () An introduction to the math-
ematical theory of inverse problems. Springer,
New York
. Klann E, Kuhn M, Lorenz DA, Maass P, Thiele H
() Shrinkage versus deconvolution. Inverse
Probl ():–
. Kress
R
()
Linear
integral
equations.
Springer, Berlin
. Lattès R, Lions J-L () The method of quasi-
reversibility. applications to partial differential
equations. Modern analytic and computational
methods in science and mathematics, No. .
American Elsevier Publishing, New York
. Louis AK () Inverse und schlecht gestellte
Probleme. Teubner, Stuttgart
. Liu F, Nashed MZ () Regularization of non-
linear ill-posed variational inequalities and con-
vergence rates. Set-Valued Anal ():–
. Lu S, Pereverzev SV, Ramlau R () An anal-
ysis of Tikhonov regularization for nonlinear
ill-posed problems under a general smoothness
assumption. Inverse Probl ():–
. Mahale P, Nair MT () Tikhonov regulariza-
tion of nonlinear ill-posed equations under gen-
eral source conditions. J Inv Ill-Posed Probl ():
–
. Mathé P, Pereverzev SV () Geometry of lin-
ear ill-posed problems in variable Hilbert scales.
Inverse Probl ():–
. Modersitzki J () FAIR. Flexible Algorithms
for Image Registration. SIAM, Philadelphia
. Morozov VA () Methods for solving incor-
rectly posed problems. Springer, New York

Regularization Methods for Ill-Posed Problems 

. Natterer F () Imaging and inverse problems
of partial differential equations. Jahresber Dtsch
Math-Ver ():–
. Natterer F, Wübbeling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Neubauer A () Tikhonov regularization for
nonlinear ill-posed problems: optimal conver-
gence rate and finite dimensional approximation.
Inverse Probl ():–
. Neubauer A () On enhanced convergence
rates for Tikhonov regularization of nonlinear ill-
posed problems in Banach spaces. Inverse Probl
():–, 
. Neubauer A, Hein T, Hofmann B, Kindermann S,
Tautenhahn U () Improved and extended
results
for
enhanced
convergence
rates
of
Tikkhonov regularization in Banach spaces.
Appl Anal (DOI: ./)
. Pöschl C () Tikhonov regularization with
general residual term. Ph.D thesis, University of
Innsbruck, Austria
. Ramlau R () TIGRA – an iterative algorithm
for regularizing nonlinear ill-posed problems.
Inverse Probl ():–
. Resmerita E, Scherzer O () Error estimates
for non-quadratic regularization and the relation
to enhancement. Inverse Probl ():–
. Rondi L () Uniqueness and stability for the
determination of boundary defects by electro-
static measurements. Proc Roy Soc Edinburgh
Sect A :–
. Scherzer O, Engl HW, Kunisch K () Optimal
a posteriori parameter choice for Tikhonov reg-
ularization for solving nonlinear ill-posed prob-
lems. SIAM J Numer Anal ():–
. Scherzer O (ed) () Mathematical models for
registration and applications to medical imaging.
Mathematics in industry . The European Con-
sortium for Mathematics in Industry. Springer,
Berlin
. Scherzer
O,
Grasmair
M,
Grossauer
H,
Haltmeiner M, Lenzen F () Variational
methods in imaging. Springer, New York
. Seidman TI, Vogel CR () Well posedness
and convergence of some regularization methods
for nonlinear ill posed problems. Inverse Probl
():–
. Tautenhahn U () On a general regularization
scheme for nonlinear ill-posed problems. Inverse
Probl ():–
. Tautenhahn U () On the method of Lavren-
tiev regularization for nonlinear ill-posed prob-
lems. Inverse Probl ():–
. Tautenhahn U, Jin Q () Tikhonov regular-
ization and a posteriori rules for solving non-
linear ill-posed problems. Inverse Probl ():
–
. Tikhonov AN, Arsenin VY () Solutions of
ill-posed problems. Wiley, New York
. Tikhonov AN, Goncharsky AV, Stepanov VV,
Yagola
AG
()
Numerical
methods
for
the solution of ill-posed problems. Kluwer,
Dordrecht
. Tikhonov AN, Leonov AS, Yagola AG ()
Nonlinear ill-posed problems, vols and . Series
Applied mathematics and mathematical compu-
tation, vol . Chapman & Hall, London
. Vasin VV () Some tendencies in the
Tikhonov regularization of ill-posed problems.
J Inv Ill-Posed Problems ():–
. Vogel C () Computational methods for
inverse problems. SIAM, Philadelphia
. Yamamoto M () On ill-posedness and a
Tikhonov regularization for a multidimensional
inverse hyperbolic problem. J Math Kyoto Univ
:–
. Zarzer CA () On Tikhonov regularization
with non-convex sparsity constraints. Inverse
Probl ():–, 


Distance Measures and
Applications to
Multi-Modal Variational
Imaging
Christiane Pöschl ⋅Otmar Scherzer
.
Introduction......................................................................
.
Distance Measures................................................................
..
Deterministic Pixel Measure...........................................................
..
Morphological Measures................................................................
..
Statistical Distance Measures...........................................................
..
Statistical Distance Measures (Density Based)......................................
...
Density Estimation......................................................................
...
Csiszár-Divergences (f -Divergences)................................................
...
f -Information............................................................................
..
Distance Measures Including Statistical Prior Information.......................
.
Mathematical Models for Variational Imaging.................................
.
Registration......................................................................
.
Recommended Reading..........................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Distance Measures and Applications to Multi-Modal Variational Imaging
.
Introduction
Today imaging is rapidly improving by increased specificity and sensitivity of measurement
devices. However, even more diagnostic information can be gained by combination of data
recorded with different imaging systems.
In particular in medicine, information of different modalities is used for diagnosis.
From the various imaging technologies used in medicine, we mention exemplary positron
emission tomography (PET), single photon emission computed tomography (SPECT), mag-
netic resonance imaging (MRI), magnetic resonance spectroscopy (MRS), X-ray, and ultra-
sound. Soft tissue can be well visualized in magnetic resonance scans, while bone structures
are more easily discernible by X-ray imaging.
Image registration is an appropriate tool to align the information gained from differ-
ent modalities. Thereby it is necessary to use similarity measures that are able to compare
images of different modalities, such that in a post processing step the data can be fused and
relevant information can be aligned.
The main challenge for computer assisted comparison of images from different modal-
ities is to define an appropriate distance measure between the images from different
modalities.
Similarity measures of images can be categorized as follows:
. Pixel-wise comparison of intensities.
. A morphological measure defines the distance between images by the distance between
their level sets.
. Measures based on the image’s gray value distributions.
In the following, we review distance measures for images according to the above
catalog.
We use the notation Ω for the squared domain (,). Images are simultaneously
considered as matrices or functions on Ω: A discrete image is an N × N-matrix U ∈
{, . . . ,}N×N. Each of the entries of the matrix represents an intensity value at a pixel.
Therewith is associated a piecewise constant function
uN(x) =
N
∑
i=
N
∑
j=
U i j χΩi j(x),
(.)
where
Ωi j := ( i −
N , i
N ) × ( j −
N , j
N ) for ≤i, j ≤N ,
and χΩi j is the characteristic function of Ωi j. In the context of image processing U i j denotes
the pixel intensity at the pixel χΩi j. A continuous image is a function u : Ω →R .

Distance Measures and Applications to Multi-Modal Variational Imaging 

We emphasize that the measures for comparing images, presented below, can be applied
in a straightforward way to higher dimensional domains, for example, voxel data. However,
here, for the sake of simplicity of notation and readability we restrict attention to a two-
dimensional squared domain Ω. Even more, we restrict attention to intensity data, and do
not consider vector-valued data, such as color images or tensor data. By this restriction we
exclude for instance feature based intensity measures.
.
Distance Measures
In the following, we review distance measures for comparing discrete and continuous
images. We review the standard and a morphological distance measure, both of them are
deterministic. Moreover, based on the idea to consider images as random variable, we
consider in the last two subsections two statistical approaches.
..
Deterministic Pixel Measure
The most widely used distance measures for discrete and continuous images are the l p, Lp
distance measures, respectively, in particular p = , see for instance the > Chapter “Linear
Inverse Problems” in this handbook. There, two discrete images Uand Uare similar, if
∥U−U∥p := ⎛
⎝

p
N
∑
i=
N
∑
j=
∣U i j
−U i j
∣
p ⎞
⎠

p
,
≤p < ∞,
∥U−U∥∞:=
sup
i,j=,...,N
∣U i j
−U i j
∣,
p = ∞,
respectively, is small. Two continuous images u,u: Ω →R are similar if
∥u−u∥p := ( 
p ∫Ω ∣u(x) −u(x)∣p , dx)

p
≤p < ∞,
∥u−u∥∞:= ess supx,y ∣u(x) −u(x)∣,
p = ∞,
is small. Here ess sup denotes the essential supremum.


Distance Measures and Applications to Multi-Modal Variational Imaging
Ω1.2(u1)
Ω0.5(u1)
u1
Ω0.5(u2)
Ω1.2(u2)
u2
morphology of u1,u2
⊡Fig. -
The gray values of the images are completely diﬀerent, but the images u, uhave the same
morphology
..
Morphological Measures
In this subsection, we consider continuous images ui : Ω →[,], i = ,. uand
uare morphologically equivalent ( > Fig. -), if there exists a one-to-one gray value
transformation β : [,] →[,], such that
β ○u= u.
Level sets of a continuous function u are defined as
Ωt(u) := {x ∈Ω : u(x) = t} .
The level sets Ω R (u) := {Ωt(u) : t ∈[,]} form the objects of an image that
remain invariant under gray value transforms. The normal field (Gauss map) is given by
the normals to the level lines, and can be written as
n(u) :
Ω
→
R d
x
↦
⎧⎪⎪⎨⎪⎪⎩

if ∇u(x) = 
∇u(x)
∥∇u(x)∥
else.
Droske and Rumpf [] consider images as similar, if intensity changes occur at the same
locations. Therefore, the compare the normal fields of the images with the similarity
measure
Sg(u,u) = ∫Ω g(n(u)(x),n(u)(x)) dx ,
(.)
where they choose the function g : R × R →R≥appropriately. The vectors n(u)(x),
n(u)(x) form an angle that is minimal if the images are morphologically equivalent.
Therefore, an appropriate choice of the function g is an increasing function of the minimal

Distance Measures and Applications to Multi-Modal Variational Imaging 

angle between v,v, and v,−v. For instance setting g to be the cross or the negative dot
product, we obtain:
S×(u,u) = 
∫Ω ∣n(u)(x) × n(u)(x)∣dx
S○(u,u) = 
∫Ω (−n(u)(x) ⋅n(u)(x))dx .
(The vectors n have to be embedded in R in order to calculate the crossproduct.)
Example 
Consider the following scaled images ui : [,]→[,],
u(x) = xx,
u(x) = −xx,
u(x) = (−x)x,
with gradients
∇u(x) = (x
x)
∇u(x) = (−x
−x)
∇u(x) = ( −x
−x) .
With g(u,v) :=

∣uv−uv∣, the functional Sg defined in (> .) attains the following
values for the particular images:
Sg(u,u) = 
∫Ω ∣−xx+ xx∣dx = 
Sg(u,u) = 
∫Ω ∣xx+ xx∣dx = 

Sg(u,u) = 
∫Ω ∣−xx−(−x)x∣dx = 
.
The similarity measure indicates that uand uare morphologically identical.
The normalized gradient field is set valued in regions where the function is constant.
Therefore, the numerical evaluation of the gradient field is highly unstable. To overcome
this drawback, Haber and Modersitzki [] suggested to use regularized normal gradient
fields:
nє(u) :
Ω
→
R d
x
↦
∇u(x)
∥∇u(x)∥є
where, ∥v∥є :=
√
vTv + єfor every v ∈R d. The parameter є is connected to the estimated
noise level in the image. In regions whereє is much larger than the gradient, the regularized
normalized fields nє(u) are almost zero and therefore do not have a significant effect of
the measures S× or S○, respectively. However, in regions where є is much smaller than the
gradients, the regularized normal fields are close to the non-regularized ones ( > Fig. -).
..
Statistical Distance Measures
Several distance measures for pairs of images can be motivated from statistics by consid-
ering the images as random variables. In the following, we analyze discrete images from a


Distance Measures and Applications to Multi-Modal Variational Imaging
⊡Fig. -
Top: images u, u, u. Bottom: n(u), n(u), n(u)
statistical point of view. For this purpose we need some elementary statistical definitions.
Applications of the following measures are mentioned in > Sect. ..
Correlation Coefficient:
U :=

N
N
∑
i,j=
U i j
and
Var(U) =
N
∑
i,j=
(U i j −U)
denote the mean intensity and variance of the discrete image U.
Cov(U,U) =
N
∑
i=
N
∑
j=
(U i j
−U)(U i j
−U)
denotes the covariance of two images Uand U, and the correlation coefficient is defined by
ρ(U,U) =
Cov(U,U)
√
Var(U)Var(U)
.
The correlation coefficient is a measure of linear dependence of two images. The range of
the correlation coefficient is [−,], and if ∣ρ(U,U)∣is close to one then it indicates that
Uand Uare linearly dependent.
Correlation Ratio:In statistics, the correlation ratio is used to measure the relationship
between the statistical dispersion within individual categories and the dispersion across
the whole population. The correlation ratio is defined by
η(U∣U) = Var(E(U∣U))
Var(U)
,
where E(U∣U) is the conditional expectation of Usubject to U.

Distance Measures and Applications to Multi-Modal Variational Imaging 

To put this into the context of image comparison let
Ωt(U) := {(i, j) ∣U i j
= t}
be the discrete level set of intensity t ∈{, . . . ,}. Then the expected value of Uon the
t-th level set of Uis given by
E(U∣U= t) :=

#(Ωt(U))
∑
Ωt(U)
U i j
,
where #(Ωt (U)) denotes the number of pixels in Uwith gray-value t. Moreover, the
according conditional variance is defined by
V(U∣U= t) =

#(Ωt (U)))
∑
Ωt(U)
(U i j
−E(U∣U= t))

.
The function
H(U) :
{, . . . ,}
→
N
t
↦
#(Ωt (U))
is called the discrete histogram of U.
The correlation ratio is nonsymmetric, that is η(Y ∣X) /= η(X ∣Y), and takes values
between [,]. It is a measure of (non)linear dependence between two images. If U= U,
then the correlation ratio is maximal.
Variance of Intensity Ratio, Ratio Image Uniformity:This measure is based on the defi-
nition of similarity that two images are similar, if the factor Ri j(U,U) = U i j
/U i j
has a
small variance. The ratio image uniformity (or normalized variance of the intensity ratio)
can be calculated by
RIU(U,U) = Var(R)
R
.
It is not symmetric.
Example 
Consider the discrete images U,U, and Uin > Fig. -. > Table -shows a
comparison of the different similarity measures. The variance of the intensity ratio is insignif-
icant and therefore cannot be used to determine similarities. The correlation ratio is maximal
for the pairing U,Uand in fact there is a functional dependence of the intensity values of
Uand U. However, the dependence of the intensity values of Uand Uis nonlinear, hence
the absolute value of the correlation coefficient (measure of linear dependence) is close to one,
but not identical to one.
..
Statistical Distance Measures (Density Based)
In general, two images of the same object but of different modalities have a large Lp, I p
distance. Hence the idea is to apply statistical tools that consider images as similar if there is
some statistical dependence. Statistical similarity measures are able to compare probability


Distance Measures and Applications to Multi-Modal Variational Imaging
9
9
8
1
2
1
8
10
3
8
10
9
10
3
5
9
8
6
6
8
2
3
9
4
1
6
10
3
3
4
6
10
4
10
5
9
U3
10
10
10
9
8
7
10
10
9
9
8
7
10
9
9
8
7
5
9
9
8
7
7
3
8
8
7
7
5
3
7
7
5
3
3
1
U2
1
1
1
2
3
4
1
1
2
2
3
4
1
2
2
3
4
5
2
2
3
4
4
6
3
3
4
4
5
6
4
4
5
6
6
7
U1
⊡Fig. -
Images for Examples and . Note that there is a dependence between Uand U:
U∼−(U)
⊡Table -
Comparison of the diﬀerent pixel-based similarity measures. The images U, Uare related in
a nonlinear way, this is reﬂected in a correlation ratio of . We see that the variance of intensity
ratio is not symmetric and not signiﬁcant to make a statement on a correlation between the
images
U, U
U, U
U, U
U, U
U, U
U, U
Correlation Coeﬃcient
−.
−.
.
.
−.
−.
Correlation Ratio
.
.
.
.
.
.
Variance of Intensity Ratio
.
.
.
.
.
.
density functions. Hence we first need to relate images to density function. Therefore we
consider an image as a random variable. The basic terminology of random variables is as
follows:
Deﬁnition 
A continuous random variable is a real valued function X : ΩS →R defined
on the sample space ΩS. For a sample x, X(x) is called observation.
Remark (Images as Random Variables)
When we consider an image u : Ω →R as a
continuous random variable, the sample space is Ω. For a sample x ∈Ω the observation u(x)
is the intensity of u at x.
Regarding the intensity values of an image as an observation of a random process
allows us to compare images via their intrinsic probability densities. Since the density can-
not be calculated directly, it has to be estimated. This is outlined in > Sect. ..., below.
There exists a variety of distance measures for probability densities (see for instance []).
In particular, we review f -divergences in > Sect. ...and explain how to use the
f -information as an image similarity measure in > Sect. ....

Distance Measures and Applications to Multi-Modal Variational Imaging 

...
Density Estimation
This section reviews the problem of density estimation, which is the construction of an
estimate of the density function from the observed data.
Deﬁnition 
Let X : ΩS →R be a random variable, that is, a function mapping the
(measurable) sample space ΩS of a random process to the real numbers.
The cumulated probability density function of X is defined by
P(t) :=

meas(ΩS)meas {x : X(x) ≤t}
t ∈R .
The probability density function p is the derivative of P.
The joint cumulated probability density function of two random variables X, Xis
defined by
ˆP(t, t) :=

meas(ΩS)meas{(x, x) : X(x) ≤t, X(x) ≤t}
t, t∈R .
The joint probability density function ˆp satisfies
ˆP(t, t) = ∫
t

∫
t

ˆp(s, s)dsds.
Remark 
When we consider an image u : Ω →R a random variable with sample space
Ω, we write p(u)(t) for the probability density function of the image u. For the joint prob-
ability of two images uand uwe write ˆp(u,u)(t, t) to emphasize, as above, that the
images are considered as random variables.
The terminology of Definition is clarified by the following one-dimensional example:
Example 
Let Ω := [,] and
u :Ω →[,] .
x →x
The cumulated probability density function P : [,] →[,] is obtained by integration:
P(t) := meas{x : x≤t} = meas
⎧⎪⎪⎨⎪⎪⎩
x : x ≤
√
t

⎫⎪⎪⎬⎪⎪⎭
= ∫
√
t


dx =
√
t
.
The probability density function of u is given by the derivative of P, which is
p(u)(t) =


√

√t .
In image processing, it is common to view the discrete image U (or uN as in (> .))
as an approximation of an image u. We aim for the probability density function of u,
which is approximated via kernel density estimation using the available information of


Distance Measures and Applications to Multi-Modal Variational Imaging
u, which is U. A kernel histogram is the normalized probability density function accord-
ing to the discretized image U, where for each pixel a kernel function (see (> .) below) is
superimposed. Kernel functions depend on a parameter, which can be used to control the
smoothness of the kernel histogram.
We first give a general definition of kernel density estimation:
Deﬁnition(Kernel Density Estimation)
Let t, t, . . . , tM be a sample of M independent
observations from a measurable real random variable X with probability density function p.
A kernel density approximation at t is given by
pσ(t) = 
M
M
∑
i=
kσ (t −ti) ,
t ∈[,]
where kσ is a kernel function with bandwidth σ. pσ is called kernel density approximation
with parameter σ.
Let t, t, . . . , tM and s, s, . . . , sM be samples of M independent observations from mea-
surable real random variables X, Xwith joint probability density function ˆp, then a joint
kernel density approximation of ˆp is given by
ˆpσ(s, t) = 
M
M
∑
i=
Kσ(s −si, t −ti),
where Kσ(s, t)is a two-dimensional kernel function.
Remark (Kernel Density Estimation of an Image,
> Fig. .)
Let u be a continuous
image, which is identified with a random variable. Moreover, let U be N × N samples of u. In
analogy to Definition , we denote the kernel density estimation based on the discrete image
U, by
pσ(t) =

N
N
∑
i,j=
kσ (t −U i j)
and remark that for uN as in (> .)
pσ(uN)(t) := ∫Ω kσ (t −uN(x)) dx =

N
N
∑
i,j=
kσ (t −U i j) .
(.)
The joint kernel density of two images u,uwith observations Uand Uis given by
ˆpσ(s, t) =

N
N
∑
i,j=
Kσ (s −U i j
, t −U i j
) ,
where Kσ(s, t) = kσ(s)kσ(t) is the two-dimensional kernel function. Moreover, we remark
that for u,N,u,N
ˆpσ(u,N, u,N)(s, t) := ∫Ω Kσ (s −u,N(x), t −u,N(x)) dx =

N
N
∑
i, j=
Kσ (s −U i j
, t −U i j
) .

Distance Measures and Applications to Multi-Modal Variational Imaging 

ps (u)(t)
ps (u)(t)
~
ti
ks (t − ti)
~
ks (t − ti)
ti
⊡Fig. -
Density estimate for diﬀerent parameters σ
In the following, we review particular kernel functions and show that standard his-
tograms are kernel density estimations.
Example 
Assume that ui : Ω →[,], i = ,are continuous images, with discrete
approximations ui,N as in (> .).
•
We use the joint density kernel Kσ(s, t) := kσ(s)kσ(t), where kσ is the normalized
Gaussian kernel of variance σ. Then for i = ,, the estimates for the marginal densities
are given by
pσ(ui,N)(t) := ∫Ω kσ(ui,N(x) −t) dx =

√
πσ ∫Ω exp (−(ui,N(x) −t)
σ 
)dx ,
and the joint density approximation reads as follows
ˆpσ(s, t) : = ∫Ω Kσ((u(x),u(x)) −(s, t)) dx
=

πσ ∫Ω exp (−(u,N(x) −s)
σ 
)exp (−(u,N(x) −t)
σ 
) dx .


Distance Measures and Applications to Multi-Modal Variational Imaging
•
Histograms: Assume that U only takes values in {,, . . . ,}. When we choose the
characteristic χ[−σ,σ), with σ = 
as kernel function, we obtain the density estimate
pχ,σ(t) = ∫Ω χ[−σ,σ[(u(x) −t) dx
= meas {x : t −σ ≤u(x) < t + σ}
= size of pixel × number of pixels with value ⌊t + σ⌋.
Hence pχ,σ corresponds with the histogram of the discrete image.
Example 
We return to Example . The domain Ω = [,] is partitioned into N
equidistant pieces. Let
uN :=
N
∑
i=
(∫
i
N
i−
N
u(x)dx) χ[ i−
N , i
N ] .
Moreover, we consider the piecewise function uT
N represented in
> Fig. -. The density
according to u, denoted by p(u) and the kernel density estimates of uN and uT
N are represented
in > Fig. -. They resemble the actual density very well.
x
uN(x)
x
uT
N(x)
x
u(x)
⊡Fig. -
Original u, and discretized versions uN and uT
N
(b)
(c)
(d)
(a)
⊡Fig. -
(a) Density from the original image u, (b) Density estimation with Gaussian-kernel based on
uN (N = ), with σ = ., (c, d) normalized histogram, based on uT
N, with σ = ., .

Distance Measures and Applications to Multi-Modal Variational Imaging 

...
Csiszár-Divergences (f-Divergences)
The concept of f -divergences has been introduced by Csiszár in [] as a generalization
of Kullback’s I-divergence and Rényi’s I-divergence, and at the same time by Ali and
Silvey []. In probability calculus f -divergences are used to measure the distances between
probability densities.
Deﬁnition
Set F:= {f : [,∞) →R ∪{∞} : f is convex in[,∞),continuous at ,
and satisfies f () = } and
Vpdf := {p ∈L(R ) : p ≥,∫R p(t) dt = } .
Let g, g∈Vpdf be probability densities functions. The f -divergence between g, gis given by
D f :
Vpdf × Vpdf
→
[,∞)
(g, g)
→
∫R g(t) f ( g(t)
g(t)) dt .
(.)
Remark 
•
In (> .), the integrand at positions t where g(t) = is understood in the following
sense:
f ( g(t)

) := lim
¯t↘(¯t f ( g(t)
¯t
)),
t ∈R .
•
In general f -divergences are not symmetric, unless there exists some number c such that
the generating f satisfies f (x) = x f ( 
x ) + c(x −).
Examples for f -Divergences We list several f -divergences that have been used in
literature (see [, ] and references therein).
Kullback–Leibler Divergence is the f -divergence with f (x) = x log(x)
D f (g, g) = ∫R g(t)log( g(t)
g(t)) dt .
Jensen–Shannon Divergence is the symmetric Kullback–Leibler divergence:
D f (g, g)= ∫R (g(t)log( g(t)
g(t)) + g(t)log( g(t)
g(t) )) dt .


Distance Measures and Applications to Multi-Modal Variational Imaging
f (x)
x
χs-Divergences: These divergences are generated by
f s(x) = ∣x −∣s ,
s ∈[,∞)
and have the form
D f (g, g) =
∫R g(t)∣g(t)
g(t) −∣
s
= ∫g−s
(t)∣g(t) −g(t)∣s dt .
The χ-divergence is a metric. The most widely used out of this family of χs divergences is
the χ-divergence (Pearson).
f (x)
x
Dichotomy Class Divergences: The generating function of this class is given by
f (x) =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
x −−ln(x)
for s = ,

s(−s) (sx + −s −xs)
for s ∈R/{,},
−x + x ln(x)
for s = .
The parameter s = 
provides a distance namely the Hellinger metric
D f (g, g) = ∫R (
√
g(t) −
√
g(t))

dt .

Distance Measures and Applications to Multi-Modal Variational Imaging 

f (x)
x
Matsushita’s Divergences: The elements of this class, which is generated by the function
f (x) = ∣−xs∣

s ,
< s ≤,
are prototypes of metric divergences. The distance is given by
d(g, g) = (D f (g, g))
s
where
D f (g, g) = ∫R g(t)∣−( g(t)
g(t))
s
∣

s
dt .
f (x)
x
Puri–Vincze Divergences: This class is generated by the functions
f (x) =
∣−x∣s
(x + )s−,
s ∈[,∞) .
For s = we obtain the triangular divergence
D f (g, g) = ∫R
(g(t) −g(t))
g(t) + g(t)
dt .
f (x)
x


Distance Measures and Applications to Multi-Modal Variational Imaging
Divergences of Arimoto Type: Generated by the functions
f (x) =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
s
s−((+ xs)

s −

s −(+ x))
for s ∈(,∞)/{}
(+ x)ln() + x ln(x) −(+ x)ln(+ x)
for s = 

∣−x∣
for s = ∞.
For s = ∞the divergence is proportional to the χdivergence. For s ∈(,∞)/{} we
obtain
D f (g, g) = ∫R
s
s −(
s√
gs
(t) + gs
(t) −
−s
s (g(t) + g(t))) dt
Moreover, this class provides the distances
d(g, g) = (D f (g, g))
min{s, 
s }
for s ∈(,∞) .
f (x)
x
...
f-Information
In the following, we review the f -information for measuring the distance between proba-
bility densities. The most important f -information measure is the mutual information.
The notion of information gain induced by simultaneously observing two probability
measures compared to their separate observations is tightly related to divergences. It results
from quantifying the information content of the joint measure in comparison with the
product measure.
This motivation leads to the following definition.
Deﬁnition (f-information for images)
For f ∈F(see Definition ) we define the
f -information of u,u∈L∞(Ω) by
I f (u,u) := D f (p(u) p(u), p(u,u)) ,
where the p(ui) is the probability density of ui, as introduced in the > Sect. ....

Distance Measures and Applications to Multi-Modal Variational Imaging 

Additionally, we define the f -entropy of an image uby
H f (u) := I f (u,u) .
In analogy to independent probability densities, we call two images u,uindependent
if there is no information gain, that is
p(u,u) = p(u)p(u) .
Remark 
The f -information has the following properties
•
Symmetry: I f (u,u) = I f (u,u).
•
Bounds: ≤I f (u,u) ≤min{H f (u), H f (u)}.
•
I f (u,u) = if and only if u,uare mutually independent.
The definition of f -information does not make assumptions on the relationship
between the image intensities (see [] for discussion). It does neither assume a linear,
nor a functional correlation but only a predictable relationship. For more information on
f -information see [].
Example 
The most famous examples of f -informations are the following
Mutual/Shannon Information: For f (x) = x ln x we obtain
I f (u,u) = ∫R ∫R p(u,u)(t, t)ln(
p(u,u)(t, t)
p(u)(t)p(u)(t)) dtdt,
with Shannon entropy
H f (u) = ∫R p(u)(t)ln(

p(u)(t)) dt
joint entropy
H f (u,u) = −∫R ∫R p(u,u)(t, t)ln (p(u,u)(t, t)) dtdt,
conditional entropy
H f (u∣u) = ∫R p(u)(t)H f (u∣u= t) dt
and relative entropy (Kullback–Leibler divergence)
H f (u∣u) = ∫R p(u)(t)ln( p(u)
p(u)) .
The relative entropy is not symmetric. Maes et al. [] and Studholme et al. [] both sug-
gested the use of joint entropy for multimodal image registration. Maes et al. demonstrate


Distance Measures and Applications to Multi-Modal Variational Imaging
the robustness of registration, using mutual information with respect to partial overlap and
image degradation, such as noise and intensity inhomogeneities.
Hellinger Information: For f (x)
=
x −−√x (see also Dichotomy Class in
> Sect. ...) we obtain
I f (u,u) = ∫R ∫R (
√
p(u,u)(t, t) −
√
p(u)(t)p(u)(t))

dtdt,
with Hellinger entropy
H f (u) = (−∫R (p(u)(t))

dt) .
Both are bounded from above by .
For measuring the distance between discrete images Uand U, it is common to map
the images via kernel estimation to continuous estimates of their intensity value densities
pσ(ui,N), where pσ(ui,N) is as defined in (> .). The difference between images is then
measured via the distance between the associated estimated probability densities.
Example 
For Ui, i = , . . .,as in
> Fig. -, let ui,N be the corresponding piece-
wise constant functions. Note that Uand Uare somehow related. In other words, they
highly dependent on each other, so we can expect a low information value. Comparing the
images point-wise with least squares, shows a higher similarity value for Uand Uthan for
Uand U.
For the ease of presentation we work with histograms. Recallthat the estimated probability
function pσ(ui,N) is equal to the normalized histogram of Ui. The histograms (connected to
the marginal density densities) are given by










H(U)










H(U)










H(U)










In order to calculate the information measures, we calculate the joint histograms of
U,U,U, that is, JH(U,U) : (s, t) →number of pixels such that U i j
= s and U i j
= t (see
> Tables -and > -).
The entries in the joint histogram of U,Uare located along a diagonal, whereas the
entries of the other two joint histograms are spread all over. Hence, we can observe the depen-
dence of pσ(u,N), pσ(u,N) already by looking at the joint histogram. Next we calculate the
Hellinger and the Mutual information.
For the f -entropies we obtain
U
U
U
Mutual entropy
.
.
.
Hellinger entropy
.
.
.

Distance Measures and Applications to Multi-Modal Variational Imaging 

⊡Table -
Joint histogram of Uand U. Note that the entries are put in an order, due to the dependence
between Uand U. This will be reﬂected in a high f-information value
1
2
3
4
5
6
7
8
9
10
1
6
6
2
7
7
3
6
6
4
9
9
5
3
3
6
4
4
7
1
1
8
0
9
0
10
0
1
0
4
0
3
0
9
6
7
6
JH (U1,U2)
H (U1)
H (U2)
⊡Table -
Joint histograms of U, Uand U, U. The entries are disperse, this will be reﬂected in a lower
f-information as in the case for U, U
1
2
3
4
5
6
7
8
9
10
1
1
1
2
0
3
2
1
1
4
4
0
5
1
1
1
3
6
0
7
1
2
1
1
2
2
9
8
1
2
1
1
1
6
9
1
2
1
1
2
7
10
2
2
2
6
3
2
5
3
2
4
0
5
6
6
JH (U3,U1)
1
2
3
4
5
6
7
8
9
10
1
1
1
1
3
2
2
2
3
2
2
1
5
4
1
2
3
5
1
1
2
6
1
1
1
1
4
7
0
8
2
2
1
5
9
2
1
2
1
6
10
2
1
2
1
6
6
7
6
9
3
4
1
0
0
0
JH (U2,U3)
H (U2)
H (U3)
H (U1)
H (U3)
and for the f -information measures:
(U,U)
(U,U)
(U,U)
Mutual information
.
.
.
Hellinger information
.
.
.
Sum of least squares
.
.
.
Indeed, in both cases (Hellinger and Mutual information), U,U(high f -information
value) can be considered as more similar than Uand U. Whereas the least squares value
between U,Uis the highest, meaning that they differ at most.
We can observe in Example that the values of f -information differ a lot by different
choices of the function f . Moreover, it is not easy to interpret the values, hence one is
interested in calculating normalized values.


Distance Measures and Applications to Multi-Modal Variational Imaging
⊡Table -
Comparison of measures composed by f-information and f-entropies.
Mutual Information
(u, u)
(u, u)
(u, u)
Normalizeda
.
.
.
Entropy Correlation Coeﬃcient
.
.
.
Exclusivea
.
.
.
Hellinger Information
(u, u)
(u, u)
(u, u)
Normalizeda
.
.
.
Entropy Correlation Coeﬃcient
.
.
.
Exclusive a
.
.
.
aNormalized and exclusive informations are minimal if the images are equal, whereas the entropy correla-
tion coeﬃcient is maximal.
Normalized Mutual Information: Studholme [] proposed a normalized measure of
mutual information. Normalized f -information is defined by
NI f (u,u) := H f (u) + H f (u)
I f (u,u)
.
If u= u, then the normalized f -information is minimal with value .
Entropy Correlation Coefficient: Collignon and Maes [] suggested the use of the
entropy correlation coefficient, another form of normalized f -information (> Table -):
H f CC(u,u) =
I f (u,u)
H f (u) + H f (u) = −

NIf (u,u) .
The entropy correlation coefficient is one if u= uand zero if uand uare completely
independent.
Exclusive f -Information is defined by
EI f (u,u) := H f (u) + H f (u) −I f (u,u)
Note that the exclusive f -information is minimal for u= u.
..
Distance Measures Including Statistical Prior
Information
Most multimodal measures used in literature do not consider the underlying image con-
text or other statistical prior information on the image modalities. Recently several groups
developed similarity measures that incooperate such information:
•
Leventon and Grimson [] proposed to learn prior information from training data
(registered multimodal images) by estimating the joint intensity distributions of
the training images. Based on this paper, Chung et al. [] proposed a to use the
Kullback–Leiber distance to compare the learned joint intensity distribution, with the

Distance Measures and Applications to Multi-Modal Variational Imaging 

joint intensity distribution of the images, in order to compare multimodal images.
This idea was extended by Guetter et al. [], who combines mutual information with
the incorporation of learned prior knowledge with a Kullback–Leibler term.
As a follow-up of their ideas we suggest the following type of generalized similarity
measures: Let pl
σ be the learned joint intensity density (learned from the training data
set) and α ∈[,]. For f ∈Fdefine
Sα,σ(u,u) := αD f (pl
σ, pσ(u,u)) + (−α)D f (pσ(u,u), pσ(u)pσ(u))
6777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777787777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777779
I f (u,u)
.
•
Instead of using a universal, but a-priori fixed similarity measure one can learn a sim-
ilarity measure in a discriminative manner. The methodology proposed by Lee et al.
[] uses a learning algorithm, that constructs a similarity measure based on a set of
preregistered images.
.
Mathematical Models for Variational Imaging
In the following we proceed with an abstract setting. We are given a physical model F,
which in mathematical terms is an operator between spaces U and V. For given data v ∈V
we aim for solving the operator equation
F(Φ) = v .
In general the solution is not unique and we aim for finding the solution with minimal
energy, that is, we aim for a minimizer of the constraint optimization problem
R(Φ) →min subject to F(Φ) = v .
In practice a complication of this problem is that only approximate (noisy) data vδ ∈V of
v is available. To take into account uncertainty of the data, it is then intuitive to consider
the following constrained optimization problem instead
R(Φ) →min subject to ∥F(Φ) −vδ∥
≤δ ,
(.)
where δ is an upper bound for the approximation error ∥v −vδ∥. It is known that solving
(.) is equivalent to minimizing the Tikhonov functional,
Φ →
∥F(Φ) −vδ∥
+ αR(Φ),
(.)
where α > is chosen according to Morozov’s discrepancy principle [].
For the formulation of the constrained optimization problem, Tikhonov method,
respectively, it is essential that F(Φ) and v, vδ, respectively, represent data of the same
modality. If F(Φ) and the data, which we denote now by w, are of different kind, then it is
intuitive to use a multimodal similarity measure Sr, instead of the least squares distance,


Distance Measures and Applications to Multi-Modal Variational Imaging
which allows for comparison of F(Φ) and w. Consequently, we consider the multimodal
variational method, which consists in minimization of
Φ →Tα,wδ (Φ) := Sr(F(Φ),wδ) + αR(Φ),
α > .
In the limiting case, that is, for δ →, one aims for recovering an RSr -minimizing solution
Φ† if
R(Φ†) = min {R(Φ) : Φ ∈A} where A = {Φ : Φ = argmin {Sr (F(⋅),w)}} .
To take into account priors in Tikhonov regularization, the standard way is again by a
least squares approach. In this case, for regularization the least squares functional
Φ →R(Φ) = 
∥Φ −Φ∥
is added to 
∥F(Φ) −vδ∥
(see, e.g., []). In analogy, we consider the regularization func-
tional (.) and incorporate priors by adding generalization of the functional R(Φ).
Taking into account prior information Ψ, that might come, for instance, from another
modality, this leads to the following class of generalized Tikhonov functionals
T wδ,Ψ
α,β
(Φ) := Sr(F(Φ),wδ) + αR(Φ) + βS p(Φ, Ψ) .
Here S p is an appropriate multimodal similarity measure. In the limiting case, that is, for
δ →, one aims for recovering an γ −RSrS p-minimizing solution Φ† if
R(Φ†) + γS p(Φ†, Ψ) = min {R(Φ) + γS p(Φ†, Ψ) : Φ ∈A} where
A = {Φ : Φ = argmin {Sr (F(⋅),w)}} .
The γ-parameter balances between the amount of prior information and regularization and
satisfies γ = limα,β→
β
α . For theoretical results on existence of minimizing elements of the
functionals and convergence we refer to [, ].
.
Registration
In this section we review variational methods for image registration. This problem consists
in determining a spatial transformation (vector field) Φ that aligns pixels of two images
uR and uT in an optimal way (> Fig. .). We use the terminology reference image for
uR and template image for uT, where both are assumed to be compactly supported func-
tions in Ω. That is, we consider the problem of determining the optimal transformation,
which minimizes the functional
u →S(uT ○(id + Φ),uR) .
(.)
To establish the context to the inverse problems setting we use the setting F(Φ) =
uT(id + Φ) and wδ = uR. In general the problem of minimizing (> .) is ill posed.

Distance Measures and Applications to Multi-Modal Variational Imaging 

(x, y)
(x,y) + Φ(x, y)
(x, y)
⊡Fig. -
Left: images uR, uT, right: deformation ﬁeld Φ
Tikhonov type variational regularization for registration then consists in minimization of
the functional
Φ →Sr (uT ○(id + Φ),uR) + αR(Φ)
(.)
(we do not consider constrained registration here, but concentrate on Tikhonov regular-
ization).
Image registration (also of voxel (D) data) is widely used in medical imaging, for
instance for monitoring and evaluating tumor growth, disease development, and therapy
evaluation.
Variational methods for registration differ by the choice of the regularization func-
tional R and the similarity measure Sr. There exists a variety of similarity measures that
are used in practice. For some surveys we refer to [, , ].
The regularization functional R typically involves differential operators. In particular,
for nonrigid registration energy functionals from elasticity theory and fluid dynamics are
used for regularization.
The optimality condition for a minimizer of (> .) reads as follows:
αDΦ (R(Φ), Ψ) + DΦ (Sr (uT ○(id + Φ),uR), Ψ) = 
for all Ψ ∈U ,
(.)
where DΦ(T , Ψ) denotes the directional derivative of a functional T in direction Ψ. The
left hand side of the equation is the steepest descent functional of the energy functional
(> .). In the following we highlight some steepest descent functionals according to
variational registration methods.
Example (Elastic Registration with L-norm-Based Distance Function)
Set α = ,
Sr(v,v) = 
∥v−v∥
L. We consider an elastic regularization functional of the form
R(Φ) = ∫Ω

∑
i=

∑
j=
⎛
⎝
λ

∂
∂xi
Φi ∂
∂x j
Φj + μ
( ∂
∂x j
Φi + ∂
∂xi
Φj)
⎞
⎠,
where λ, μ ≥are Lamé parameters and Φ = (Φ, Φ). λ is adjusted to control the rate of
growth or shrinkage of local regions within the deforming template and μ is adjusted to control


Distance Measures and Applications to Multi-Modal Variational Imaging
shearing between adjacent regions of the template []. In this case, the optimality condition
for minimizing αR(Φ) + Sr(uT ○(id + Φ)), given by (> .), is satisfied if Φ solves the
following PDE
μΔΦ(x) + (μ + λ)∇(∇⋅Φ(x)) = −
α (uT (x + Φ(x)) −uR(x))∇uT (x + Φ(x))
67777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777778777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777779
∂
∂Φ S r
.
Here ΔΦ = (ΔΦ, ΔΦ) and DΦ (Sr (uT ○(id + Φ),uR), Ψ) = ∫Ω
∂
∂Φ Sr ⋅Ψ. This partial
differential equation is known as linear elastic equation and is derived assuming small angles
of rotation and small linear deformations. When large displacements are inherent it is not
applicable [, , , , , ].
Example (Elastic Registration with f-Information)
Assume that kσ ∈C(R, R) is
some kernel density function. Moreover, let Kσ(s, t) = kσ(s)kσ(t). We pose the similarity
measure as the f -information between the template and the reference image:
Sr (uT ○(id + Φ),uR) = H f (uR) −I f (uT ○(id + Φ),uR) ,
and set α and R as in the previous example. In order to write the derivative of I f in a compact
way, we use the abbreviations ˜Φ := id + Φ. The derivative of pσ(uT ○˜Φ) with respect to ˜Φ,
in direction Ψ is given by
D ˜Φ (pσ(uT ○˜Φ), Ψ)(t) = ∫Ω k′
σ (t −uT( ˜Φ(x))) ∇uT( ˜Φ(x)) ⋅Ψ(x) dx
and
D ˜Φ (ˆpσ(uT ○˜Φ, uR), Ψ) (s, t) = ∫Ω k′
σ (s −uT( ˜Φ(x))) kσ(t −uR(x))(∇uT ( ˜Φ(x)) ⋅Ψ(x)) dx .
We use the following abbreviations:
g(s, t) := pσ(uT ○˜Φ)(s)pσ(uR)(t)
pσ(uT ○˜Φ,uR)(s, t)
,
g(s, t) :=
pσ(uR)(t)
(ˆpσ(uT ○˜Φ,uR)(s, t))
,
and
g(s, t) := D ˜Φ(pσ(uT ○˜Φ), Ψ)(s)ˆpσ(uT ○˜Φ,uR)(s, t) +
pσ(uT ○˜Φ)(s)D ˜Φ (ˆpσ(uT ○˜Φ,uR), Ψ)(s, t) .
With this we can calculate the derivative of the f -information to be
D ˜Φ (I f (uT ○˜Φ,uR), Ψ) = ∫R ∫R D ˜Φ (pσ(uT ○˜Φ), Ψ)(s)pσ(uR)(t)f (g(s, t))
+ pσ(uT ○˜Φ)(s)pσ(uR)(t)f ′ (g(s, t)) g(s, t)g(s, t) dt ds .

Distance Measures and Applications to Multi-Modal Variational Imaging 

For mutual information this simplifies to
D ˜Φ (MI(uT ○˜Φ,uR), Ψ) = ∫R ∫R (D ˜Φ (ˆpσ(uT ○˜Φ,uR), Ψ)(s, t)ln(

g(s, t))
+ D ˜Φ (ˆpσ(uT ○˜Φ,uR), Ψ)(s, t)
pσ(uT ○˜Φ)(s)pσ(uR)(t)
⎞
⎠ds dt .
A detailed exposition on elastic registration with mutual information can be found in
[, , ].
In this section we have presented a general framework on variational-based techniques
for nonconstrained multimodal image registration. Below we give a short overview on
relevant literature on this topic.
Kim and Fessler [] describe an intensity-based image registration technique that uses
a robust correlation coefficient as a similarity measure for images. It is less sensitive to
outliers, that are present in one image, but not in the other. Kaneko [] proposed the selec-
tive correlation coefficient, as an extension of the correlation coefficient. Van Elsen et al.
investigated similarity measures for MR and CT images. She proposed to calculate the cor-
relation coefficient of geometrical features []. Alternatively to the correlation coefficient,
one could calculate Spearman’s rank correlation coefficient (also known as Spearman’s ρ),
which is a nonparametric measure of correlation [], but not very popular in multimodal
imaging. Roche et al. [, ] tested the correlation ratio to align MR, CT and PET images.
Woods et al. [] developed an algorithm based on this measure for automated aligning
and re-slicing PET images. Independently, several groups realized that the problem of reg-
istering two different images modalities can be cast in an information theoretic framework.
Collignon et al. [] and Studholme et al. [] both suggested using the joint entropy of the
combined images as a registration potential. Pluim et al. [] investigated in more general
f -informations. For MR-CT registrations, the learned similarity measure by Lee et al. out-
performs all standard measures. Experimental results for learning similarity measures for
multimodal images can be found in [].
.
Recommended Reading
For recent results on divergences and information measures, we refer to Computational
Information Geometry. Website: http://blog.informationgeometry.org/.
Comparison and evaluation of different similarity measures for CT, MR, PET brain
images can be found in [].
It is worth mentioning two databases:
•
The Retrospective Image RegistrationEvaluation Project is designed to compare dif-
ferent multimodal registration techniques. It involves the use of a database of image
volumes, commonly known as the "Vanderbilt Database," on which the registrations


Distance Measures and Applications to Multi-Modal Variational Imaging
are to be performed. Moreover it provides a training data set for multimodal image
registration. Link: http://www.insight-journal.org/rire/
•
Validation of Medical Image Registration. This is a database with references (inter-
national publications) on medical image registration including a validation study of
different similarity measures. Link: http://idm.univ-rennes.fr/VMIP/model/index.
html
A number of image registration software tools have been developed in the last decade.
The following support multimodal image comparison:
ITK is an open-source, cross-platform system that provides developers with an exten-
sive suite of software tools for image analysis. It supports the following similarity
measures: mean squares metric, normalized cross correlation metric, mean recip-
rocal square differences, mutual information (different implementations [, ]),
Kullback–Leibler distance, normalized mutual information, correlation coefficient,
kappa statistics (for binary images), and gradient difference metric. Website: http:
//www.itk.org/.
FLIRT is a robust and accurate automated linear (affine) registration tool based
around a multi-start, multi-resolution global optimization method. It can be
used for inter- and intra-modal registration with D or D images. Websites:
http://www.fmrib.ox.ac.uk/analysis/research/flirt.
FAIR, FLIRT are toolboxes for fast and flexible image registration. Both have
been developed by the SAFIR-research group in Lübeck. They include sum of
squared differences, mutual information, and normalized gradient fields. Websites:
http://www.math.uni-luebeck.de/safir/software.shtml.
AIR stands for automated image registration. It supports standard deviation of ratio
images, least squares, and least squares with global intensity rescaling. Website:
http://bishopw.loni.ucla.edu/AIR/.
RView This software integrates a number of D/D data display and fusion routines
together with three-dimentional rigid volume registration using normalized mutual
information. It also contains many interactive volume segmentation and painting func-
tions for structural data analysis. Website: http://www.colin-studholme.net/software/
software.html.
Acknowledgment
The work of OS has been supported by the Austrian Science Fund (FWF) within the
national research networks Industrial Geometry, project -N, and Photoacoustic
Imaging in Biology and Medicine, project S-N.
The work of CP has been supported by the Austrian Science Fund (FWF) via the Erwin
Schrödinger scholarship J.

Distance Measures and Applications to Multi-Modal Variational Imaging 

References and Further Reading
. Ali SM, Silvey SD () A general class of coef-
ficients of divergence of one distribution from
another. J Roy Stat Soc B :–
. Bajcsy R, Kovaˇciˇc S () Multiresolution elastic
matching. Comput Vision Graph :–
. Christensen G () Deformable shape models
for anatomy. PhD thesis, Washington University,
Department of Electrical Engineering
. Chung ACS, Wells WM, Norbash A, Grimson
WEL () Multi modal image registration by
minimizing Kullback-Leibler distance. In: MIC-
CAI’: proceedings of the th international
conference on medical image computing and
computer-assisted intervention-part II. Springer,
London, pages –
. Csiszár I () Eine informationstheoretische
Ungleichung und ihre Anwendung auf den
Beweis der Ergodizität von Markoffschen Ket-
ten. Magyar Tud Akad Mat Kutató Int Közl :
–
. Dragomir SS () Some general divergence
measures for probability distributions. Acta Math
Hung ():–
. Droske M, Rumpf M (/) A variational
approach to nonrigid morphological image reg-
istration. SIAM J Appl Math ():–(elec-
tronic)
. Engl HW, Hanke M, Neubauer A () Regular-
ization of inverse problems, volume of Mathe-
matics and its Applications. Kluwer, Dordrecht
. Ens K, Schumacher H, Franz A, Fischer B ()
Improved elastic medical image registration using
mutual information. In: Pluim JPW, Reinhardt JM
(eds) Medical imaging : image processing,
vol . SPIE, p C
. Fahrmeir L, Künstler R, Pigeot I, Tutz G ()
Statistik, th edn. Springer, Berlin
. Faugeras O, Hermosillo G () Well-posedness
of two nonrigid multi modal image registrational
methods. SIAM J Appl Math ():–
(electronic)
. Feldman D, Österreicher F () A note on
f -divergences. Stud Sci Math Hung (–):
–
. Gee J, Haynor D, Le Briquer L, Bajcsyand R
() Advances in elastic matching theory and its
implementation. In: CVRMed, pp –
. Guetter C, Xu C, Sauer F, Hornegger J ()
Learning based non-rigid multi modal image reg-
istration using Kullback-Leibler divergence. In:
Medical image computing and computer-assisted
intervention – MICCAI , vol of Lec-
ture Notes in Computer Science. Springer, pp
–
. Haber E, Modersitzki J () Intensity gradi-
ent based registration and fusion of multi modal
images. In: Methods of information in medicine,
Schattauer Verlag, Stuttgart, pp –
. Henn S, Witsch K () multi modal image reg-
istration using a variational approach. SIAM J Sci
Comput ():–
. Hermosillo G () Variational methods for
multi modal image matching. PhD thesis, Univer-
sité de Nice, France
. Hömke L () A multigrid method for elastic
image registration with additional structural con-
straints. PhD thesis, Heinrich-Heine Universität,
Düsseldorf
. Ivanov VK, Vasin VV, Tanana VP () Theory
of linear ill-posed problems and its applications
(inverse and ill-posed problems series), nd edn.
VSP, Utrecht, Translated and revised from the
Russian original
. Kaneko S, Satoh Y, Igarashi S () Using selec-
tive correlation coefficient for robust image regis-
tration. Pattern Recognit ():–
. Kim J, Fessler JA () Intensity-based image
registration using robust correlation coefficients.
IEEE Trans Med Imaging ():–
. Lee D, Hofmann M, Steinke F, Altun Y, Cahill ND,
Scholkopf B () Learning similarity measure
for multi modal d image registration. In: Pro-
ceedings of the IEEE computer society conference
on computer vision and pattern recognition. IEEE
Service Center, Piscataway, pp –
. Leventon ME, Grimson WEL () Multi modal
volume registration using joint intensity dis-
tributions. In: MICCAI’: proceedings of the
first international conference on medical image
computing and computer-assisted intervention.
Springer, London, pp –
. Likar B, Pernus F () A hierarchical approach
to elastic registration based on Mutual Informa-
tion. Image Vis Comput :–


Distance Measures and Applications to Multi-Modal Variational Imaging
. Maes F, Collignon A, Vandermeulen D, Mar-
chal G, Suetens P () multi modality image
registration by maximization of Mutual Informa-
tion. IEEE Trans Med Imaging ():–
. Mattes D, Haynor DR, Vesselle H, Lewellen TK,
Eubank W () PET-CT image registration
in the chest using free-form deformations. IEEE
Trans Med Imaging ():–
. Modersitzki J () Numerical methods for
image registration. Numerical mathematics and
scientific computation. Oxford University Press,
Oxford
. Pluim JPW, Maintz JBA, Viergever MA ()
f-information measures in medical image regis-
tration. IEEE Transactions on Medical Imaging,
():–
. Peckar W, Schnörr C, Rohr K, Stiehl HS ()
Non-rigid image registration using a parameter-
free elastic model. In: British machine vision con-
ference
. Pöschl C () Tikhonov regularization with
gerneral residual term. PhD thesis, Leopold
Franzens Universität, Innsbruck
. Rachev ST () Probability metrics and the
stability of stochastic models. Wiley Series in
probability and mathematical statistics: applied
probability and statistics. Wiley, Chichester
. Roche A () Recalage d’images médicales par
inférence statistique. PhD thesis, Université de
Nice, Sophia-Antipolis, France
. Roche A, Malandain G, Pennec X, Ayache N
() The correlation ratio as a new similar-
ity measure for multi modal image registration.
In: Lecture notes in computer science, vol .
Springer, pp –
. Roche A, Pennec X, Ayache N () The correla-
tion ratio as a new similarity measure for multi
modal image registration. In: Medical image
computing and computer-assisted interventation
MICCAI, LNCS . Springer, pp –
. Studholme C () Measures of D medi-
cal image alignment. PhD thesis, University of
London, London
. Vajda I () Theory of statistical inference and
information. Kluwer, Dordrecht
. van den Elsen P, Maintz JBA, Viergever MA
() Automatic registration of CT and MR
brain images using correlation of geometri-
cal features. IEEE Trans Med Imaging ():
–
. Viola PA () Alignment by maximization of
mutual information. PhD thesis, Massachusetts
Institute of Technology, Massachusetts
. Viola PA, Wells WM () Alignment by max-
imization of mutual information. In: ICCV’:
proceedings
of
the
fifth international
con-
ference on computer vision, IEEE Computer
Society, p 
. Wang XY, Feng DD () Automatic elastic
medical image registration based on image inten-
sity. Int J Image Graph ():–
. West J, Fitzpatrick JM, Wang MY, Dawant BM,
Maurer CR, Kessler RM, Maciunas RJ, Barillot C,
Lemoine D, Collignon A, Maes F, Suetens P,
Vandermeulen D, van den Elsen P, Napel S,
Sumanaweera TS, Harkness B, Hemler PF, Hill
DLG, Hawkes DJ, Studholme C, Maintz JBA,
Viergever MA, Malandain G, Pennec X, Noz ME,
Maguire GQ, Pollack M, Pelizzari CA, Robb RA,
Hanson D, Woods RP () Comparison and
evaluation of retrospective intermodality brain
image registration techniques. J Comput Assist
Tomogr :–
. Woods RP, Cherry SR, Mazziotta JC () Rapid
automated algorithm for aligning and reslicing
PET images. J Comput Assist Tomogr :–


Energy Minimization
Methods
Mila Nikolova
.
Introduction......................................................................
..
Background...............................................................................
..
The Main Features of the Minimizers as a Function of the Energy..............
..
Organization of the Chapter...........................................................
.
Preliminaries.....................................................................
..
Notations.................................................................................
..
Reminders and Definitions............................................................
.
Regularity Results................................................................
..
Some General Results...................................................................
..
Stability of the Minimizers of Energies with Possibly Nonconvex Priors........
...
Local Minimizers........................................................................
...
Global Minimizers of Energies with for Possibly Nonconvex Priors.............
..
Nonasymptotic Bounds on Minimizers..............................................
.
Nonconvex Regularization......................................................
..
Motivation................................................................................
..
Assumptions on Potential Functions ϕ...............................................
..
How It Works on R......................................................................
..
Either Smoothing or Edge Enhancement............................................
..
Selection for the Global Minimum...................................................
.
Minimizers Under Nonsmooth Regularization................................
..
Main Theoretical Result................................................................
..
The D TV Regularization..............................................................
..
An Application to Computed Tomography..........................................
.
Minimizers Relevant to Nonsmooth Data-fidelity.............................
..
General Theory...........................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Energy Minimization Methods
..
Applications..............................................................................
..
The L-TV Case..........................................................................
.
Conclusion........................................................................
.
Cross-References.................................................................

Energy Minimization Methods 

Abstract: Energy minimization methods are a very popular tool in image and signal
processing. This chapter deals with images defined on a discrete finite set. Energy min-
imization methods are presented from a nonclassical standpoint: we provide analytical
results on their minimizers that reveal salient features of the images recovered in this way,
as a function of the shape of the energy itself. The energies under consideration can be
differentiable or not, convex or not. Examples and illustrations corroborate the presented
results. Applications that take benefit from these results are presented as well.
.
Introduction
In numerous applications, an unknown image or a signal uo ∈Rp is represented by data
v ∈Rq according to an observation model, called also forward model
v = A(uo) ⊙n,
(.)
where A : Rp →Rq is a (linear or nonlinear) transform and n represents perturbations
acting via an operation ⊙. When u is an m × n image, it is supposed that its pixels are
arranged into a p-length real vector,where p = mn. Some typical applications are for
instance, denoising, deblurring, segmentation, zooming and super-resolution, reconstruc-
tion in inverse problems, coding and compression. In all these cases, recovering a good
estimate ˆu for uo needs to combine the observation along with a prior and desiderata on
the unknown uo. A common way to define such an estimate is
Find ˆu such that
F(ˆu,v) = min
u∈U F(u,v),
(.)
F(u,v) = Ψ(u,v) + βΦ(u),
(.)
where F : Rp × Rq →R is called an energy, U ⊂Rp is a set of constraints, Ψ is a data-
fidelity term, Φ brings prior information on uo and β > is a parameter which controls
the trade-off between Ψ and Φ.
The term Ψ ensures that ˆu satisfies (> .) quite faithfully according to an appropriate
measure. The noise n is random and a natural way to derive Ψ from (> .) is to use prob-
abilities; see, e.g., [, , , ]. More precisely, if π(v∣u) is the likelihood of data v, the
usual choice is
Ψ(u,v) = −log π(v∣u).
(.)
Consider an m × n image u. For instance, its columns can be concatenated, which can be seen as
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
u[]
u[m + ]
⋯
⋯
u[(n −)m + ]
u[]
u[m + ]
⋯
⋯
u[(n −)m + ]
⋯
⋮
⋮
⋯
u[i −]
⋯
⋮
⋮
⋯
u[i −m]
u[i]
⋯
⋯
u[m]
u[m]
⋯
⋯
u[p]
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(.)
In this case, the original u[i, j] is identified with u[(i −)m + j] in (> .).


Energy Minimization Methods
For instance, if A is a linear operator and v = Au + n where n is additive independent and
identically distributed (i. i. d.) zero-mean Gaussian noise one finds that
Ψ(u,v) ∝∥Au −v∥
.
(.)
This remains quite a common choice partly because it simplifies calculations.
The role of Φ in (> .) is to push the solution to exhibit some a priori known or desired
features. It is called prior term, or regularization, or penalty term, and so on. In many image
processing applications, Φ is of the form
Φ(u) =
r
∑
i=
ϕ(∥Diu∥),
(.)
where for any i ∈{, . . ., r}, Di : Rp →Rs, for s an integer s ⩾, are linear operators. For
instance, the family {Di} ≡{Di :∈{, . . ., r}} can represent the discrete approximation of
the gradient or the Laplacian operator on u, or finite differences of various orders, or the
combination of any of these with the synthesis operator of a frame transform. Note that
s = if {Di} are finite differences or a discrete Laplacian; then
s = 
⇒
ϕ(∥Diu∥) = ϕ(∣Diu∣).
In (> .), ϕ : R+ ↦R is quite a “general” function, often called a potential function (PF).
A very standard assumption is that
H
ϕ : R+ →R is increasing and nonconstant on R+, lower semi-continuous and for
simplicity, ϕ() = .
Several typical examples for ϕ are given in > Table -and their plots are seen in
> Fig. -.
⊡Table -
Commonly used PFs ϕ : R+ →R where α > is a parameter. Note that among the
nonconvex PFs, (f), (f), and (f) are coercive while the remaining PFs, namely (f),
(f), (f), (f), and (f), are bounded
Convex PFs
ϕ′(+) = 
ϕ′(+) > 
(f)
ϕ(t) = tα, < α ⩽
(f)
ϕ(t) = t
(f)
ϕ(t) =
√
α + t−√α
(f)
ϕ(t) = log(cosh(αt))
(f)
ϕ(t) = t/α −log (+ t/α)
Nonconvex PFs
ϕ′(+) = 
ϕ′(+) > 
(f)
ϕ(t) = min{αt,}
(f)
ϕ(t) = tα, < α < 
(f)
ϕ(t) =
αt
+ αt
(f)
ϕ(t) =
αt
+ αt
(f)
ϕ(t) = log (αt+ )
(f)
ϕ(t) = log (αt + )
(f)
ϕ(t) = −exp (−αt)
(f)
ϕ() = , ϕ(t) = if t ≠

Energy Minimization Methods 

0
1
0
1
0
1
0
1
Nonconvex PFs
Convex PFs
⊡Fig. -
Plots of the PFs given in > Table -. PFs with ϕ′(+) = (- - -), PFs with ϕ′(+) > (—)
Remark 
Note that if ϕ′(+) > the function t →ϕ(∣t∣) is nonsmooth at zero in which
case Φ is nonsmooth on ∪r
i=[w ∈Rp : Diw = ]. Conversely, ϕ′(+) = leads to a smooth
at zero t →ϕ(∣t∣).
According to the rules of human vision, an important requirement is that the
prior, i.e., Φ should promote smoothing inside homogeneous regions but preserve sharp
edges.
..
Background
Energy minimization methods, as described here, are at the crossroad of several well-
established methodologies that are briefly sketched below.
•
Bayesian maximum a posteriori (MAP) estimation using Markov random field
(MRF) priors. Such an estimation is based on the maximization of the posterior
distribution
π(u∣v) = π(v∣u)π(u)/Z,
where π(u) is the prior model for uo and Z = π(v) can be seen as a constant.
Equivalently, it minimizes with respect to u the energy
F(u,v) = −ln π(v∣u) −ln π(u).
Identifying the first term above with Ψ(⋅,v) and the second one with Φ shows the
fundamentals of the equivalence. Key papers on MAP energies involving MRF priors
are [–, , , ]. Since the pioneering work of Geman and Geman [], vari-
ous nonconvex PFs ϕ were explored in order to produce images involving neat edges,


Energy Minimization Methods
see, e.g., [, , ]. MAP energies involving MRF priors are also considered in a large
amount of books, such as [, , , ]. A recent pedagogical account is found in [].
•
Regularization for ill posed inverse problems was initiated in the book of Tikhonov
and Arsenin [] in . The background idea can be stated in terms of the stabiliza-
tion of this kind of problems. Useful textbooks in this direction are, e.g., [, , ]
and especially the very recent []. This methodology and its most recent achieve-
ments are nicely discussed from quite a general point of view in > Chap. in this
handbook.
•
Variational methods are originally related to PDE restoration methods and are naturally
developed for signals and images defined on a continuous subset Ω ⊂Rd, d = ,, . . .;
for images d = . Originally, the data-fidelity term is of the form (> .) for A = Id and
Φ(u) = ∫Ω ϕ(∥Du∥)dx, where ϕ is a convex function as those given in
> Table -.
Since the beginning of the s, a remarkable effort was done to find heuristics on ϕ
that enable to recover edges and breakpoints in restored images and signals while smooth-
ing the regions between them (see [, , , , , ] to name just a few from a huge
literature) with a particular emphasis on convex PFs. Up to now, the most success-
ful seems to be the Total Variation (TV) regularization corresponding to ϕ(t) = t,
which was proposed by Rudin, Osher and Fatemi in []. Variational methods were
rapidly applied along with various linear operators A and more generally, with various
data-fidelity terms Ψ. Among all important papers we evoke [, , , , , ].
The use of differential operators Dk of various orders k ⩾in the prior Φ has
been rarely investigated; see [, ]. Let us remind that whenever D is a differen-
tial operator and Φ is nonconvex, the minimization problem on u : Ω ⊂Rd →R
does not admit a solution and no convergence result can be exhibited. More details
on variational methods for image processing can be found in several textbooks like
[, , ].
For numerical implementation, the variational functional is discretized. Using a
rearrangement of a discretized finite u into a p-length vector, Φ takes the formof
(> .) where r = p and Di ∈Rs×p for s = , ⩽i ⩽p.
The equivalence between these approaches is considered in several seminal papers,
see, e.g., [, ] as well as the numerous references therein. The state of the art and
the relationship among all these methodologies is nicely outlined in the recent book
of Scherzer et al. []. This book gives a brief historical overview of these method-
ologies and attaches a great importance to the functional analysis of the presented
results.
By a commonly used discretization (see, e.g., []), using the representation of an image as a vector according
to (> .), we have
∥Diu∥=
√
(u[i] −u[i −])+ (u[i] −u[i −p]),
(.)
along with appropriate boundary conditions. Other discretization approaches have also been considered, see,
e.g., [, ].

Energy Minimization Methods 

..
The Main Features of the Minimizers as a Function of
the Energy
Pushing curiosity ahead leads to various additional questions. One observes that usually
data-fidelity and priors are modeled in a separate way. It is hence necessary to check if the
minimizer ˆu of F(⋅,v) meets properly all information contained in the data production
model Ψ as well as in the prior Φ. Hence the question: how the prior Φ and the data-
fidelity Ψ are effectively involved in ˆu – a minimizer of F(⋅,v). This leads to formulate the
following backward modeling problem:
Analyze the mutual relationship between the salient features exhibited by
the minimizers ˆu of an energy F(⋅,v) and the shape of the energy itself.
(.)
This problem was posed in a systematic way and knowingly studied for the first time in
[, ]. The point of view provided by (> .) is actually adopted by many authors. Problem
(> .) is totally general and involves crucial stakes:
•
It yields rigorous and strong results on the minimizers ˆu.
•
Such a knowledge enables a real control on the solution – the reconstructed image or
signal ˆu.
•
Conversely, it opens new perspectives for modeling.
•
It enables the conception of specialized energies F that meet the requirements in
various applications.
•
This kind of results can help to derive numerical schemes that use what is known about
the solution.
Problem (> .) remains open and is intrinsically tortuous (which properties to look
for, how to conduct the analysis …). The results presented here concern images, signals,
and data living on finite grids. In this practical framework, the results in this chapter are
quite general since they hold for energies F which can be convex or nonconvex, smooth or
nonsmooth, and results address local and global minimizers.
..
Organization of the Chapter
Some preliminary notions and results that help the reading of the chapter are sketched
in > Sect. .. > Section .is devoted to the regularity of the (local) minimizers of
F(⋅,v) with a special focus on nonconvex regularization. > Section .shows how edges
are enhanced using nonconvex regularization. In > Sect. .it is shown that nons-
mooth regularization leads typically to minimizers involving constant patches. Conversely,
> Sect. .exhibits that the minimizers relevant to nonsmooth data-fidelity achieve
an exact fit for numerous data samples. In all cases, illustrations and applications are
presented.


Energy Minimization Methods
.
Preliminaries
In this section we set the notations and recall some classical definitions and results on
minimization problems.
..
Notations
We systematically denote by ˆu a (local) minimizer of F(⋅,v). It is explicitly specified when
ˆu is a global minimizer. Below n is an integer bigger than one.
•
Dn
j – The differential operator of order n with respect to the jth component of a
function.
•
v[i] – The ith entry of vector v.
•
A[i, j] – The component located at row i and column j of matrix A.
•
#J – The cardinality of the set J.
•
Jc = I/J – The complement of J ⊂I in I where I is a set.
•
K – The orthogonal complement of a sub vector space K ⊂Rn.
•
A∗– The transposed of a matrix (or a vector) where A is real valued.
•
A ≻(A ⪰) – The matrix A is positive definite (positive semi-definite)
•
χΣ(x) = {
si
x ∈Σ,

si
x /∈Σ. – The characteristic function of a set Σ.
•
ln ∈Rn with ln[i] = , ⩽i ⩽n.
•
Ln – The Lebesgue measure on Rn.
•
Id – The identity operator.
•
∥.∥ρ – A vector or a matrix ρ-norm.
•
R+
def= {t ∈R : t ⩾} and R∗
+
def= {t ∈R : t > }.
•
TV – Total Variation.
•
{e, . . ., en} – The canonical basis of Rn, i.e., ei[i] = and ei[j] = if i ≠j.
..
Reminders and Deﬁnitions
Deﬁnition 
A function F : Rp →R is coercive if
lim
∥u∥→∞F(u) = +∞.
Deﬁnition 
A function F on Rp is proper if F : Rp →(−∞,+∞] and if it is not
identically equal to +∞.
A special attention being dedicated to nonsmooth functions, we recall some basic facts.
Deﬁnition 
Given v ∈Rq, the function F(⋅,v) : Rp →R admits at ˆu ∈Rp a one-sided
derivative in a direction w ∈Rp, denoted δF(ˆu,v)(w), if the following limit exists:

Energy Minimization Methods 

δF(ˆu,v)(w) = lim
t↘
F(ˆu + tw,v) −F(ˆu,v)
t
,
where the index in δspecifies that we address derivatives with respect to the first variable
of F.
In fact, δF(ˆu,v)(w) is a right-side derivative; the relevant left-side derivative is
−δF(ˆu,v)(−w). If F(⋅,v) is differentiable at ˆu, then δF(ˆu,v)(w) = DF(ˆu,v)w. In
particular, for ϕ : R+ →R we have
ϕ′(θ+) def= δϕ(θ)() = lim
t↘
ϕ(θ + t) −(θ)
t
, θ ⩾and ϕ′(θ−) def= −δϕ(θ)(−).
Next we recall the classical necessary condition for a local minimum of a possibly
nonsmooth function [, ].
Theorem 
If F(⋅,v) has a local minimum at ˆu ∈Rp, then δF(ˆu,v)(w) ⩾, for every
w ∈Rp.
If F(⋅,v) is Fréchet differentiable at ˆu, one can easily deduce that DF(ˆu,v) = at a local
minimizer ˆu.
Rademacher’s theorem states that if F is proper and Lipschitz continuous on Rp,
then the set of points in Rp at which F is not Fréchet differentiable forms a set of
Lebesgue measure zero [, ]. Hence F is differentiable at almost every u. However, when
F(⋅,v) is nondifferentiable, its minimizers are typically located at points where F(⋅,v) is
nondifferentiable. See, e.g., Example and > Fig. -below.
Example 
Consider F(u,v) = 
∥u −v∥+ β∣u∣for β > and u,v ∈R. The minimizer ˆu
of F(⋅,v) reads
ˆu =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
v + β
if
v < −β

if
∣v∣⩽β
v −β
if
v > β
(ˆu is shrunk w.r.t. v.)
F(⋅,v) and ˆu are plotted in > Fig. -for several values of v.
The next corollary tells us what can happen if the necessary condition in Theorem 
does not hold.
Corollary 
Let F be differentiable on (Rp × Rq)/Θwhere
Θ
def= {(u,v) ∈Rp × Rq : ∃w ∈Rp, −δF(u,v)(−w) > δF(u,v)(w)}.
(.)
Given v ∈Rq, if ˆu is a (local) minimizer of F(⋅,v) then
(ˆu,v) /∈Θ.


Energy Minimization Methods
−1
0
1
b = 1,v = –0.9
−1
0
1
−1
0
1
−1
0
1
b = 1,v  = –0.2
b = 1,v  = 0.5
b = 1,v  = 0.95
⊡Fig. -
For the set of values for v given above, F(⋅, v) is plotted with “—”while its minimizer
∧u is
marked with “o.”In all these cases
∧u lies at a point where F(⋅, v) is nondiﬀerentiable
Proof
If ˆu is a local minimizer, then by Theorem , δF(ˆu,v)(−w) ⩾, hence
−δF(ˆu,v)(−w) ⩽⩽δF(ˆu,v)(w), ∀w ∈Rp.
(.)
If (ˆu,v) ∈Θ, the necessary condition (> .) cannot hold.
∎
Example 
Suppose that Ψ in (> .) is a differentiable function for any v ∈Rq. Let the
PF ϕ be such that for some positive numbers, say θ, . . . , θk, for k finite, its left-hand-side
derivative is strictly higher than its right-hand side derivative, i.e., ϕ′ (θ−
j ) > ϕ′ (θ+
j ), ⩽j ⩽
k, and that ϕ is differentiable beyond this set of numbers. Given a (local) minimizer ˆu, denote
I = {, . . . , r} and I ˆu = {i ∈I : ∥Di ˆu∥= θ j,⩽j ⩽k}.
Define F(ˆu,v) = Ψ(ˆu,v) + β ∑
i∈I/I ˆu
ϕ(∥Di ˆu∥). Note that F(⋅,v) is differentiable at ˆu. The
energy F(⋅,v) at ˆu reads F(ˆu,v) = F(ˆu,v) + β ∑
i∈I ˆu
ϕ(∥Di ˆu∥). Applying the necessary
condition (> .) for w = ˆu yields
β ∑
i∈I ˆu
ϕ′ (∥Di ˆu∥−
) ⩽−DF(ˆu,v)(ˆu) ⩽β ∑
i∈I ˆu
ϕ′ (∥Di ˆu∥+
).
In particular, we must have ∑i∈I ˆu ϕ′ (∥Di ˆu∥−
) ⩽∑i∈I ˆu ϕ′ (∥Di ˆu∥+
), which contradicts the
assumption on ϕ. It follows that if ˆu is a (local) minimizer of F(⋅,v), then I ˆu = ∅and
hence ∀i ∈I
∥Di ˆu∥≠θ j, ⩽j ⩽k.
A typical case is the PF (f) in > Table -, namely ϕ(t) = min{αt,}. Then k = and
θ = θ=

√α . Remind that (f) is the discrete equivalent of the Mumford-Shah prior [].
The following existence theorem can be found, e.g., in [].
Theorem 
For v ∈Rq, let U ⊂Rp be a nonempty and closed subset and F(⋅,v) : U →R
a lower semi-continuous (l.s.c.) proper function. If U is unbounded (with possibly U = Rp),
suppose that F(⋅,v) is coercive. Then there exists ˆu ∈U such that F(ˆu,v) = inf
u∈U F(u,v).

Energy Minimization Methods 

We should emphasize that this theorem gives only sufficient conditions for the existence of
a minimizer. They are not necessary, as seen in the example below.
Example 
Let F : R× R→R involve (f) in > Table -and read
F(u,v) = (u[] −v[])+ βϕ(∣u[] −u[]∣) for ϕ(t) = max{αt,},
< β < ∞.
For any v, it is obvious that F(⋅,v) is not coercive since it is bounded by β in the direction
span {(,u[])}. Nevertheless, its global minimum is strict and is reached for ˆu[] = ˆu[] =
v[]. At the global minimum, F(⋅,v) gets its minimal value, namely F(ˆu,v) = .
Most of the results summarized in this chapter exhibit the behavior of the mini-
mizer points ˆu of F(⋅,v) under variations of v. In words, they deal with local minimizer
functions.
Deﬁnition 
Let F : Rp × Rq →R and O ⊆Rq. We say that U : O →Rp is a
local minimizer function for the family of functions F(⋅, O) = {F(⋅,v) : v ∈O} if for any
v ∈O, the function F(⋅,v) reaches a strict local minimum at U(v).
When F(⋅,v) is proper, l.s.c. and convex, the standard results below can be evoked, see
[, ].
Theorem 
Let F(⋅,v) : Rp →R be proper, convex, l.s.c. and coercive for every v ∈Rq.
() Then F(⋅,v) has a unique (global) minimum which is reached for a convex and closed
set of minimizers {̂U(v)} = {ˆu ∈Rp : F(ˆu,v) = inf
u∈U F(u,v)}.
() If in addition F(⋅,v) is strictly convex, then the minimizer ˆu = U(v) is unique.
Moreover, the minimizer function v ↦U(v) is unique (hence it is global) and it is
continuous if F is continuous [, Lemmas and , p. ].
The next lemma, which can be found, e.g., in [], addresses the regularity of the local
minimizer functions when F is smooth. It can be seen as a variant of the Implicit functions
theorem.
Lemma 
Let F be Cm, m ⩾, on a neighborhood of (ˆu,v) ∈Rp×Rq. Suppose that F(⋅,v)
reaches at ˆu a local minimum such that D
F(ˆu,v) ≻. Then there are a neighborhood
O ⊂Rq containing v and a unique Cm−local minimizer function U : O →Rp, such that
D
F(U( ), ) ≻for every
∈O and U(v) = ˆu.
This lemma is extended in several directions in this chapter.
According to a fine analysis conducted in the s and nicely summarized in [],
ϕ preserves edges if Hholds as if H, stated below, holds true as well:
H
lim
t→∞
ϕ′(t)
t
= .


Energy Minimization Methods
This assumption is satisfied by all PFs in
> Table -except for (f) in case if α = . We
do not evoke the numerous other heuristics for edge preservation as far as they will not be
used explicitly in this chapter.
Deﬁnition 
Let ϕ : [,+∞) →R and m ⩾an integer. We say that ϕ is Cm on R+, or
equivalently that ϕ ∈Cm(R+) if and only if the following conditions hold:
()
ϕ is Cm on (,+∞).
()
The function t ↦ϕ (∣t∣) is Cm at zero.
Using this definition, for the PF (f) in
> Table -we see that ϕ is Con [,+∞), that
ϕ ∈C(R+) for (f), while for the other differentiable functions satisfying ϕ′(+) = we
find ϕ ∈C∞(R+).
.
Regularity Results
Here, we focus on the regularity of the minimizers of F : Rp × Rq →R of the form
F(u,v) = ∥Au −v∥
+ β ∑
i∈I
ϕ(∥Diu∥),
(.)
I def= {, . . . , r},
where A ∈Rq×p, and for any i ∈I we have Di ∈Rs×p for s ⩾. Let us denote by D the
following rs × p matrix:
D def=
⎡⎢⎢⎢⎢⎢⎣
D
. . .
Dr
⎤⎥⎥⎥⎥⎥⎦
.
When Ain (> .) is not injective, a standard assumption in order to have regularization is
H
ker(A) ∩ker(D) = {}.
Notice that Htrivially holds when rank A = p. In typical cases ker(D) = span(lp),
whereas usually Alp ≠, so Hholds again. Examples for ϕ are seen in > Table -.
..
Some General Results
We first check the conditions on F(⋅,v) in (> .) that enable Theorems and to be
applied. It is useful to remind that since Hholds, F(⋅,v) in (> .) is l.s.c. and proper.

Energy Minimization Methods 

. Note that F(⋅,v) in (> .) is coercive for any v ∈Rq at least in one of the following
cases:
•
Rank(A) = p and ϕ : R+ ↦R+ is nondecreasing.
•
Hand Hhold and ϕ is coercive in addition (e.g., as (f)-(f),(f), (f), and (f)
in > Table -).
In these cases, Theorem can be applied and shows that F(⋅,v) does admit minimizers.
. For any v ∈Rq, the energy F(⋅,v) in (> .) is convex and coercive if Hand Hhold
for a convex ϕ. Then statement (i) of Theorem holds true.
. Furthermore, F(⋅,v) in (> .) is strictly convex and coercive for any v ∈Rq if ϕ satisfies
Hand if one of the following assumptions holds true
•
Rank(A) = p and ϕ is convex.
•
Hholds and ϕ is strictly convex.
Then statement () of Theorem can be applied. In particular, for any v ∈Rq, F(⋅,v) has
a unique strict minimizer and there is a unique local minimizer function U : Rq →Rp
which is continuous (remind Definition ).
However, the PFs involved in (> .) used for signal and image processing are often
nonconvex or nondifferentiable. An extension of the standard results given above is hence
necessary. This is the goal of the subsequent > Sect. ...
..
Stability of the Minimizers of Energies with Possibly
Nonconvex Priors
In this section the assumptions stated below are considered.
H
The operator A in (> .) satisfies rank A = p, i.e., A∗A is invertible.
H 
The PF ϕ in (> .) is C(R+) and Cm, m ⩾, on R∗
+ with ⩽ϕ′(+) < ∞;
if ϕ′(+) = it is required also that ϕ is Cm on R+ (see Definition ).
Under H, H, H, and H, the prior (and hence F(⋅,v)) in (> .) can be nonconvex
and in addition nonsmooth. Thanks to Hand H, Theorem ensures that for any v ∈Rq,
F(⋅,v) admits a global minimum. However, it can present numerous local minima.
▷
Energies F with nonconvex and possiblynondiﬀerentiable PFs ϕ are frequently used in engi-
neering problems since they were observed to give rise to high-quality solutions
∧u. It is hence
critically important to have good knowledge on the stability of the obtained solutions.
Even though established under restrictions on A, the results summarized in this section
provide the state of the art on this subject. Further research is desirable to assess the stability
of broader classes of energies.


Energy Minimization Methods
...
Local Minimizers
The stability of local minimizers is a matter of critical importance in its own right for
several reasons. In many applications, the estimation of the original signal or image uo
is performed by only locally minimizing a nonconvex energy in the vicinity of some initial
guess. Second, it is worth recalling that minimization schemes that guarantee the finding
of the global minimum of a nonconvex objective function are exceptional. The practically
obtained solutions are usually only local minimizers, hence the importance of knowing
their behavior.
The theorem below is a simplified version of the results established in [].
Theorem 
Let F(⋅,v) in (> .) satisfy H, H, H, and H. Then there exists a closed
subset Θ ⊂Rq whose Lebesgue measure is Lq(Θ) = such that for any v ∈Rq/Θ, there
exists an open subset O ⊂Rq with v ∈O and a local minimizer function (see Definition )
U : O →Rp which is Cm−on O and meets ˆu = U(v).
Related questions have been considered in critical point theory, sometimes in semi-
definite programming; the well-posedness of some classes of smooth optimization prob-
lems was addressed in []. A lot of results have been established on the stability of the
local minimizers of general smooth energies []. It worths noting that these results are
quite abstract to be applied directly to our energy in (> .).
Commentary on the assumptions.
All assumptions H, H, and Hbearing on the PF
ϕ are nonrestrictive at all since they address all nonconvex PFs in
> Table -except for
(f) which is discontinuous at zero. The assumption Hmay, or may not, be satisfied –
it depends on the application in mind. This assumption is difficult to avoid, as seen in
Example .
Example 
Consider F : R× R →R given by
F(u,v) = (u[] −u[] −v)+ ∣u[]∣+ ∣u[]∣,
where v ≡v[]. The minimum is obtained after a simple computation.
v > 

{̂U(v)} = (c, c −v + 
) for any c ∈[,v −
]
(nonstrict minimizer).
∣v∣⩽

ˆu = (unique minimizer)
v < −

{̂U(v)} = (c, c −v −
) for any c ∈[v + 
,]
(nonstrict minimizer).
In this case, assumption His violated and there is a local minimizer function only for
v ∈[−
, 
].

Energy Minimization Methods 

Intermediate results.
The derivations in [] reveal a series of important intermediate
results.
. If ϕ′⟨+⟩= and is C(R+), (remember Definition ) then ∀v ∈Rq/Θ, every local
minimizer ˆu of F(u,v) is strict and D
F(ˆu,v) ≻. Consequently, Lemma is extended
since the statement holds true ∀v ∈Rq/Θ.
▷
For real data v – a random sample of Rq – whenever F(⋅,v) is diﬀerentiable and satisﬁes
the assumptions of Theorem , it as almost sure that local minimizers
∧u are strict and their
Hessians D
F(
∧u,v) are positive deﬁnite.
. Using Corollary , the statement of Theorem holds true if ϕ′(+) = and there is
τ > such that ϕ′(τ−) > ϕ′(τ+). This is the case of the PF (f) in > Table -, which is
the discrete version of the Mumford–Shah regularization.
. If ϕ′(+) > , define
ˆJ def= {i ∈I : Di ˆu = } and
KˆJ
def= {w ∈Rp : Diw = , ∀i ∈ˆJ}.
(.)
Then ∀v ∈Rq/Θ, every local minimizer ˆu of F(u,v) is strict and
(a) DF∣KˆJ(ˆu,v) = and D
F∣KˆJ(ˆu,v) ≻– a sufficient condition for a strict
minimum on KˆJ.
(b) δF(ˆu,v)(w) > , ∀w ∈K
ˆJ /{} – a sufficient condition for a strict minimum
on K
ˆJ .
▷
Let us emphasize that (a) and (b) provide a suﬃcient condition for a strict (local) mini-
mum of F(⋅,v) at
∧u (a straightforward consequence of [, Theorem ] and Lemma ).
Hence, these conditions are satisﬁed at the (local) minimizers
∧u of F(⋅,v) for almost
every v ∈Rq.
We can interpret all these results as follows:
▷
Under the assumptions H, H, H, and H, given real data v ∈Rq, the chance to get
a nonstrict (local) minimizer or a (local) minimizer of the energy in (> .) that does
not result from a Cm−local minimizer function, is null.
...
Global Minimizers of Energies with for Possibly Nonconvex
Priors
An overview of the results on global minimizers for several classes of functions can be
found in []. The setting being quite abstract, the results presented there are difficult to
apply to the energy in (> .). The results on the global minimizers of (> .) presented
next are extracted from [].


Energy Minimization Methods
Theorem 
Assume that F(⋅,v) in (> .) satisfy H, H, H, and H. Then there exists
a subset ˆΘ ⊂Rq such that Lq( ˆΘ) = and the interior of Rq/ ˆΘ is dense in Rq, and for
any v ∈Rq/ ˆΘ the energy F(⋅,v) has a unique global minimizer. Furthermore, the global
minimizer function ˆU : Rq/ ˆΘ →Rp is Cm−on an open subset of Rq/ ˆΘ which is dense in Rq.
▷
This means that in a real-world problem there is no chance of getting data v such that the
energy F(⋅,v) (> .) has more than one global minimizer.
We anticipate mentioning that even though ˆΘ is negligible, it plays a crucial role for the
recovery of edges; this issue is developed in > Sect. ..
..
Nonasymptotic Bounds on Minimizers
The aim here is to give nonasymptotic analytical bounds on the local and the global mini-
mizers ˆu of F(⋅,v) in (> .) that hold for all PFs ϕ in > Table -. Related questions have
mainly been considered in particular situations, such as A = Id, for some particular ϕ,
or when v is a special noise-free function, or in the context of the separable regulariza-
tion of wavelet coefficients, or in asymptotic conditions when one of the terms in (> .)
vanishes – let us cite among others [, , ]. An outstanding paper [] explores the mean
and the variance of the minimizers ˆu for strictly convex and differentiable functions ϕ. The
bounds provided below are of practical interest for the initialization and the convergence
analysis of numerical schemes.
H
ϕ is Con R+ (cf. Definition ) with ϕ() = and ϕ(t) > for any t > .
H
There are two alternative assumptions:
•
ϕ′(+) = and ϕ is Con R∗
+/Θwhere the set Θ= {t > : ϕ′(t−) > ϕ′(t+)} is at
most finite.
•
ϕ′(+) > and ϕ is Con R∗
+.
The conditions on Θin this assumption allows us to address the PF given in (f). Let us
emphasize that under H, H, and Hthe PF ϕ can be convex or nonconvex.
The statements given below were derived in [] where one can find additional bounds
and details.
Theorem 
Consider F of the form (> .), and let H, H, and Hhold.
() Let one of the following assumptions hold:
(a) rank (A) = p;
(b) ϕ is strictly increasing on R+ and Hholds.

Energy Minimization Methods 

For every v ∈Rq, if F(⋅,v) reaches a (local) minimum at ˆu, then
∥Aˆu∥⩽∥v∥.
() Assume that rank (A) = p ⩾, ker(D) = span(lp) and ϕ is strictly increasing on R+.
There is a closed set N ⊂Rq with Lq(N) = such that ∀v ∈Rq/N, if F(⋅,v) reaches a
(local) minimum at ˆu, then
∥Aˆu∥< ∥v∥.
A full description of the set N can be found in [].
Comments on the results.
If A is orthonormal (e.g., A = Id), the obtained results yield
() ⇒∥ˆu∥⩽∥v∥;
() ⇒∥ˆu∥< ∥v∥.
These provide sharper bounds than the one available in [].
When the least eigenvalue λ
min of A∗A is positive, it is obvious that
() ⇒∥ˆu∥⩽∣λ−
min∣∥v∥;
() ⇒∥ˆu∥< ∣λ−
min∣∥v∥.
In the case of noise-free data and rank (A) = p, one naturally wishes to recover the
original (unknown) uo. It is hence necessary, that ∥Aˆu∥= ∥v∥. Comparing the results
obtained in () and () show that such a goal is unreachable if ϕ is strictly increasing on R+.
▷
It follows that exact recovery needs that ϕ is constant for t ⩾τ, for a constant τ ⩾.
The mean of restored data.
In many applications, the noise corrupting the data can
be supposed to have a mean equal to zero. When A = Id, it is well known that
mean(ˆu) =mean(v), see, e.g., []. It is shown in [, Proposition ] that for a general A
Alp ∝lq
(.)
⇒mean(ˆu) =mean(v).
(.)
However, (> .) is quite a restrictive requirement. In the simple case when ϕ(t) = t,
ker(D) = lrs and A is square and invertible, it is easy to see that the restrictive requirement
(> .) is also sufficient [, Remark ]. It turns out that if A is no longer equal to Id,
the natural requirement (> .) is generally false. A way to remedy for this situation is to
minimize F(⋅,v) under the explicit constraint derived from (> .).
The residuals for edge-preserving regularization.
The goal here is to provide bounds
that characterize the data-fidelity term at a (local) minimizer ˆu of F(⋅,v). More precisely,
the focus is on edge-preserving PFs satisfying
H
∥ϕ′∥∞
def= max {sup
t⩾
∣ϕ′(t+)∣, sup
t>
∣ϕ′(t−)∣} < ∞.
Comparing with Hshows that ϕ under His edge-preserving.


Energy Minimization Methods
Observe that except for (f) and (f), all PFs given in > Table -satisfy H. Even though
(f) is edge-preserving, ϕ′ is not well defined at zero. Note that when ϕ′(+) > , ϕ′(+)
is finite we usually have ∥ϕ′∥∞= ϕ′(+).
The statement given below is established in [, Theorem .].
Theorem 
Consider F of the form (> .) where rank (A) = q ⩽p and let H, H, H,
and Hhold. Suppose that ∥ϕ′∥∞= . Then for every v ∈Rq, if F(⋅,v) has a (local)minimum
at ˆu, we have
∥Aˆu −v∥∞⩽β
∥ϕ′∥∞∥(AA∗)−A∥∞∥D∥.
(.)
In particular, if A = Id and D corresponds to the discrete gradient operator, on a two-
dimensional image ∥D∥= and we find
∥v −ˆu∥∞⩽β∥ϕ′∥∞.
The result of this theorem may seem surprising. In a statistical setting, the quadratic
data-fidelity term ∥Au −v∥
in (> .) corresponds to white Gaussian noise on the data,
which is unbounded. However, if ϕ is edge preserving with ∥ϕ′∥∞bounded, the (local)
minimizers ˆu of F(⋅,v) give rise to noise estimates (v −Aˆu)[i], ⩽i ⩽q that are tightly
bounded as stated in (> .).
▷
Hence the assumption for Gaussian noise on the data v is distorted by the solution
∧u.
The proof of the theorem reveals that this behavior is due to the boundedness of
the gradient of the regularization term. Let us emphasize that the bound in (> .)
is independent of data v and that it is satisfied for any local or global minimizer ˆu of
F(⋅,v).
.
Nonconvex Regularization
..
Motivation
A permanent requirement is that the energy F favors the recovery of neat edges. Since the
pioneering work of Geman and Geman [], various nonconvex Φ in (> .) have been
proposed [, , , , , , ]. Indeed, the relevant minimizers exhibit neat edges
between homogeneous regions. However, they are tiresome to control and to reach (only
a few algorithms are proved to find the global minimizer of particular energies within
an acceptable time). In order to avoid the numerical intricacies arising with nonconvex
Let us remind that for any m × n real matrix C with components C[i, j], ⩽m, ⩽j ⩽n, we have
∥C∥= max
j
m
∑
i=
∣c[i, j]∣and ∥C∥∞= max
i
n
∑
j=
∣c[i, j]∣, see, e.g., [].

Energy Minimization Methods 

1
100
0
4
1
100
0
4
1
100
0
4
Data  v = u + n (—)
Original uo (dashed line)
Convex regularization f(t) = t (TV)
Original uo (- - -), minimizer û (—)
Nonconvex regularization
f(t) = αt /(1 + αt)
⊡Fig. -
Minimizers of F(u, v) = ∥u −v∥
+ β ∑
p−
i=ϕ (∣u[i] −u[i + ]∣)
regularization, since [, , ] in the s, an important effort was done to derive con-
vex edge-preserving PFs, see, e.g., [, , , ] and [] for an excellent account. The
most popular convex edge-preserving PF was derived by Rudin, Osher and Fatemi []: it
amounts to ϕ = t, for {Di} yielding the discrete gradient operator (see (> .) and (> .))
and the relevant Φ is called the Total Variation (TV) regularization.
> Figure -nicely shows that the height of the edges is much more faithful when
ϕ is nonconvex, compared to the convex TV regularization. The same effect can also be
observed, e.g., in > Figs. -, > -, and > -.
▷
This section is devoted to explain why edges are nicely recovered using a nonconvex ϕ.
..
Assumptions on Potential Functions ϕ
Consider F(⋅,v) of the form (> .) where Di : Rp →R, i ∈I = {, . . ., r}, i.e.,
F(u,v) = ∥Au −v∥
+ β ∑
i∈I
ϕ (∣Diu∣),
(.)
where ϕ : R+ →R+ satisfies H(see
> Sect. .), H(see
> Sect. ..), and Hgiven
below
H
ϕ is Con R∗
+, ϕ′(+) ⩾∀t ⩾, inf
t∈R∗
+
ϕ′′(t) < and lim
t→∞ϕ′′(t) = ;
as well as one of the following assumptions:
H
If ϕ′(+) = , then ∃τ > and ∃T ∈(τ,∞) such that ϕ′′(t+) ⩾, ∀t ∈[, τ], while
ϕ′′(t) ⩽, ∀t > τ, ϕ′′ strictly decreases on (τ,T ] and increases on [T ,∞).
H
If ϕ′(+) > then lim
t→ϕ′′(t) < is well defined and ϕ′′(t) ⩽is increasing on (,∞).


Energy Minimization Methods
0
0
f′(0+) > 0 (Φ nondifferentiable)
(H1, H6, H9 and H11)
0
increases, < 0
f¢¢(0) < 0
f′′(t)
1
1
1
f′(0+) = 0 (Φ differentiable)
(H1, H6, H9 and H10)
f(t) =
at 2
1 + at 2
f(t ) =
at 
1 + at 
1
0
0
0
0
<0
increases, < 0
f¢¢(t)
1
1
> 0
⊡Fig. -
Illustration of the assumptions in two typical cases
These assumptions are illustrated in > Fig. -. Even though they might seem tricky,
they hold true for all nonconvex PFs in > Table -, except for (f) and (f). The “irregular
cases” (f) and f() are considered separately.
The results presented below are published in [].
..
How It Works on R
▷
This example shows the main phenomena underlying the theory on edge enhancement
using nonconvex ϕ satisfying H, H, and Halong with either Hor H.
Let F : R × R →R read (see [, Sect. , p. ]).
F(u,v) = 
(u−v)+βϕ(u) for
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
β > −/ϕ′′(T )
if ϕ′(+) = (H, H, Hand H)
β > −/lim
t↘ϕ′′(t) if ϕ′(+) > (H, H, Hand H)
The (local) minimality conditions for ˆu of F(⋅,v) read
•
If ϕ′(+) = or [ϕ′(+) > and ˆu ≠] : ˆu + βϕ′(ˆu) = v and + βϕ′′(ˆu) ⩾.
•
If ϕ′(+) > and ˆu = : ∣v∣⩽βϕ′(+).

Energy Minimization Methods 

(u, v) for f(t) =
(u, v) for f(t) = 
f (0+) = 0
f (0+) > 0
t
t
t
t
2
α+
2
α+
x1
x0
x1
x0
u
q0
q1
q0 q1
q
q
u + bf (u)
u + bf (u)
bf
bf
bf
bf
v
u
v
(
=
0 = 0)
Minimizer if : ˆu +
(ˆu) = v
Minimizer at 0 : |v|
(0+)
and 1 +
bf
(ˆu) > 0
Else : ˆu +
(ˆu) = v and 1 +
(ˆu) > 0
⊡Fig. -
The curve of u ↦(DF(u, v) −v) on R/{}. Note that all assumptions mentioned before do
hold
To simplify, we assume that v ⩾. Define
θ= inf Cβ and θ= sup Cβ,
for Cβ = {u ∈R∗
+ : D
F(u,v) < } = {u ∈R∗
+ : ϕ′′(u) < −/β}.
We have θ= if ϕ′(+) > and < θ< T < θif ϕ′(+) = . After some calculations
one finds:
. For every v ∈R+ no minimizer lives in (θ, θ) (cf. > Fig. -).
. One computes < ξ< ξsuch that (cf. > Fig. -)
(a) If ⩽v ⩽ξ, F(⋅,v) has a (local) minimizer ˆu∈[, θ], hence ˆuis subject to a
strong smoothing.
(b) If v ⩾ξ, F(⋅,v) has a (local) minimizer ˆu⩾θ, hence ˆuis subject to a weak
smoothing.
(c) If v ∈[ξ, ξ] then F(⋅,v) has two local minimizers, ˆuand ˆu.
. There is ξ ∈(ξ, ξ) such that F(⋅, ξ) has two global minimizers, F(ˆu, ξ) = F(ˆu, ξ),
as seen in > Fig. -;
(a) If < v < ξ the unique global minimizer is ˆu = ˆu.
(b) If v > ξ the unique global minimizer is ˆu = ˆu.
. The global minimizer function v ↦U(v) is discontinuous at ξ and C-smooth on
R+/{ξ}.
Item is the key for the recovery either of homogeneous regions or of high edges. The
minimizer ˆu(see Items a, a) corresponds to the restoration of homogeneous regions,
while ˆu(see Items b, b) corresponds to edges. Item corresponds to a decision for the


Energy Minimization Methods
(u, v)
u
q0 > 0
q1
u
q0 = 0
q1
f(u) =
αu 2
(1+ αu 2)
f(u ) =
αu
1+ αu
(u, v)
⊡Fig. -
Each curve represents F(u, v) = 
(u −v)+ βϕ(u) for an increasing sequence v ∈[, ξ). The
global minimizer of each F(⋅, v) is emphasized with “●”. Observe that no (local) minimizer
lives in (θ, θ)
presence of an edge at the global minimizer. Since {ξ} is closed and L{ξ} = , Item 
confirms the results of > Sect. .... The detailed calculations are outlined in [, Sect. ].
The theory presented next is a generalization of these facts.
..
Either Smoothing or Edge Enhancement
We adopt the hypotheses formulated in > Sect. ... Given v, let ˆu be a local (or global)
minimizer of F(⋅,v). The results presented here are extracted essentially from [, Sect. ].
(A) Case ϕ′(+) = .
The theorem below as well as Proposition are established in
[, Sect. .].
Theorem 
Let H, H, Hand Hhold. Let {Di : i ∈I} be linearly independent and
μ def= max
⩽i⩽r ∥D∗(DD∗)−ei∥.
If β > μ∥A∗A∥
∣ϕ′′(T )∣
, there are θ∈(τ,T ) and θ∈(T ,∞) such that ∀v ∈Rq, if ˆu is a (local)
minimizer of F(⋅,v), then
either ∣Di ˆu∣⩽θ,
or
∣Di ˆu∣⩾θ,
∀i ∈I.
(.)
In imaging problems, {Di} are generally not linearly independents. Note that if {Di} are
linearly dependent, the result (> .) holds true for all (local) minimizers ˆu that are locally
homogeneous on connected regions.However, if this is not the case, one recovers both
high edges and smooth transitions, as seen in > Fig. -a. When ϕ is convex, all edges are
smoothed, as one can observe in > Fig. -a.
More precisely, connected with respect to {Di}.

Energy Minimization Methods 

The PF ϕ(t) = min{αt,} (the discrete version of Mumford–Shah functional), (f) in
> Table -, does not satisfy assumptions Hand H. In particular,
√α = ϕ′ ( 
√α
−
) > ϕ′ ( 
√α
+
) = .
A straightforward consequence of Corollary is that for any (local) minimizer ˆu of F(⋅,v)
we have
∣Di ˆu∣≠

√α ,
∀i ∈I.
Propositions and below address only global minimizers under specific conditions on
{Di}.
Proposition 
Let ϕ(t) = min{αt,}, the set {Di : i ∈I} be linearly independent and
rank (A) ⩾p −r ⩾. Assume that F(⋅,v) has a global minimizer at ˆu. Then
either ∣Di ˆu∣⩽

√α Γi,
or ∣Di ˆu∣⩾

√α Γi
,
for Γi =
B
D
D
E
∥Bei∥

∥Bei∥
+ αβ < ,
∀i ∈I,
(.)
where B is a matrixdepending only on A and D. Moreover, the inequalities in (> .) are
strict if the global minimizer ˆu of F(⋅,v) is unique.
In the case when u is a one-dimensional signal, the following result is exhibited
in [].
Proposition 
Let ϕ(t) = min{αt,}, Diu = u[i] −u[i + ], ⩽i ⩽p −and Alp ≠
with rank (A) ⩾. Then for any global minimizer ˆu of F(⋅,v) we have
either ∣Di ˆu∣⩽

√α Γi,
or
∣Di ˆu∣⩾

√α Γi
, ∀i ∈I,
(.)
where Γi =
B
D
D
D
D
D
E
∥B ∑
p
j=i+e j∥


∥B ∑
p
j=i+e j∥

+ αβ
< ,
and B is a matrixdepending only on A. Moreover, the inequalities in (> .) are strict if
the global minimizer ˆu of F(⋅,v) is unique.
From the assumptions, r ⩽p in all cases. If r = p, we have B = Id. If r < p, we choose matrices H ∈Rr×p,
Ha ∈Rp×p−r and Da ∈Rp−r×p such that for any u ∈Rp we have u = HDu + HaDau and rank (AHa) = p −r.
Denote Ma = AHa ∈Rq×p−r. Then B = Id −Ma (M∗
a Ma)−M∗
a .
In this case, B reads
A∗⎛
⎝Id −
Alpl∗
pA∗
∥Alp∥

⎞
⎠A.


Energy Minimization Methods
In both Propositions and , set θ=
γ
√α
and
θ=

√αγ for γ def= max
i∈I Γi < .
Let us define the following subsets:
ˆJ
def= {i ∈I : ∣Di ˆu∣⩽θ}
and
ˆJ
def= I/ˆJ= {i ∈I : ∣Di ˆu∣⩾θ}.
(.)
Using these notations, one can unify the interpretation of the results of Theorem and
Propositions and .
▷
Since θ< θ,
a natural interpretation of Theorem , and Propositions and , is that
[∣Di
∧u ∣: i ∈
∧
J] are homogeneous regions with respect to {Di} while {∣Di
∧u ∣: i ∈
∧
J} are
break points in Di
∧u.
In particular, if {Di} correspond to ﬁrst-order diﬀerences,
∧
Jaddresses smoothly varying
regions while
∧
Jcorresponds to edges higher than θ−θ.
(B) Case ϕ′(+) > .
Here the results are stronger without any assumption on {Di}. The
next Theorem and Proposition are proven in [, > Sect. ..].
Theorem 
Let H, H, Hand Hhold. Let β >
μ∥A∗A∥
∣limt↘ϕ′′(t)∣, where μ > is a
constant depending only on {Di}. Then ∃θ> such that ∀v ∈Rq, every (local) minimizer
ˆu of F(⋅,v) satisfies
either
∣Di ˆu∣= ,
or
∣Di ˆu∣⩾θ,
∀i ∈I.
(.)
The “-” PF (f) in
> Table -, ϕ() = , ϕ(t) = if t > does not satisfy
assumptions H, H, and Hsince it is discontinuous at .
Proposition 
Let ϕ be the “-” PF, i.e., (f) in
> Table -, the set {Di : i ∈I} be
linearly independent and rank A ⩾p −r ⩾. If F(⋅,v) has a global minimum at ˆu, then
either ∣Di ˆu∣= 
or ∣Di ˆu∣⩾
√
β
∥Bei∥
,
∀i ∈I,
(.)
where B is the same as in Proposition . The inequality in (> .) is strict if F(⋅,v) has a
unique global minimizer.
Note that (> .) holds true if we set θ= min
i∈I
√
β
∥Bei∥. Let
ˆJ
def= {i : ∣Di ˆu∣= } and ˆJ
def= I/ˆJ= {i : ∣Di ˆu∣⩾θ} .
With the help of these notations, the results established in Theorem and Proposition 
allow the relevant solutions ˆu to be characterized as stated below.

Energy Minimization Methods 

▷
By Theorem and Proposition , the set
∧
Jaddresses regions in
∧u that can be called strongly
homogeneous as far as [ ∣Di
∧u ∣= ⇔i ∈
∧
J] while
∧
Jaddresses break-points in ∣Di
∧u ∣larger
than θsince [ ∣Di
∧u∣> θ⇔i ∈
∧
J].
If D corresponds to ﬁrst-order diﬀerences or discrete gradients,
∧u is neatly segmented with
respect to {Di}:
∧
Jcorresponds to constant regions while
∧
Jdescribes all edges and the latter
are higher than θ.
Let us remind that direct segmentation of an image from data transformed via a general
(nondiagonal) operator A remains a tortuous task using standard methods. The result in
(), Theorem , tells us that such a segmentation is naturally involved in the minimizer
ˆu of F(⋅,v) in the context of this theorem. Let us emphasize that this segmentation effect
holds for any operator A. This can be observed, e.g., on > Figs. –b and > –d.
(C) Illustration: Deblurring of an image from noisy data.
The original image uo in
> Fig. -a presents smoothly varying regions, constant regions and sharp edges. Data
in > Fig. -b correspond to v = a ∗uo + n, where a is a blur with entries ai,j =
exp (−(i+ j)/.) for −⩽i, j ⩽, and n is white Gaussian noise yielding dB of
SNR. The amplitudes of the original image are in the range of [,.] and those of the
data in [−,]. In all restored images, {Di} correspond to the first-order differences of
each pixel with its eight nearest neighbors. In all figures, the obtained minimizers are dis-
played on the top. Just below, the sections corresponding to rows and of the restored
images are compared with the same rows of the original image. Note that these rows cross
the delicate locations of the eyes and the mouth in the image.
The restorations in > Fig. -are obtained using convex PFs ϕ while those in > Fig. -
using nonconvex PFs ϕ. According to the theory presented in paragraphs (A) and (B) here
above, edges are sharp and high in > Fig. -where ϕ is nonconvex, while they are under-
estimated in > Fig. -where ϕ is convex. In > Fig. -b, d ϕ is nonconvex and ϕ′(+) > 
in addition. As stated in Theorem , in spite of the fact that A is nondiagonal (and ill
Original image
a
Data v = blur + noise
b
⊡Fig. -
Data v = a ⋆uo + n, where a is a blur and n is white Gaussian noise, dB of SNR


Energy Minimization Methods
f (0+) = 0
f (0+) > 0
Row 54
Row 54
Row 90
Row 90
f(t) = t   for a = 1.4, b = 40
f(t) = t  for b = 100
a
b
a
⊡Fig. -
Restoration using convex PFs
conditionned), the restored images are fully segmented and the edges between constant
pieces are high. Even though Proposition assumes that {Di} are linearly independent,
the segmentation effect using linearly dependent {Di} as described above is often neat.
..
Selection for the Global Minimum
Let the original image uo be of the form
uo = ηχΣ,
η > ,
(.)
where the sets Σ ⊂{, . . . , p} and Σc are nonempty and χΣ ∈Rp is the characteristic
function of Σ, i.e.,
χΣ[i] = {
if i ∈Σ,

if i ∈Σc.
Let data read
v = Auo = A ηχΣ.
Consider that F(⋅, A ηχΣ) is of the form (> .) and focus on its global minimizer ˆuη.
The question discussed here is:
▷
How to characterize the global minimizer
∧uη of F(⋅,A ηχΣ) according to the value of η > ?
The results sketched below were established in [, Sect. ]. In order to answer the
question formulated above, two additional assumptions may be taken into account.

Energy Minimization Methods 

Row 54
Row 90
Row 90
Row 54
Row 54
Row 54
Row 90
Row 90
d
a
b
f(t ) =
at 2
1 + at 2 for a = 25, b = 35
f(t) =
at
1 + at
for a = 20, b = 100
f(t ) = min {at 2,1} for a =60, b =10
f(0) = 0, f(t) = 1, t > 0 for b = 25
f (0+) = 0
f (0+) > 0
c
⊡Fig. -
Restoration using nonconvex PFs


Energy Minimization Methods
H 
For any i ∈I, Di yields (possibly weighted) pairwise differences and ker(D) =
span(lp).
H
For any t ∈R+, there is a constant< c < +∞such that ϕ(t) ⩽c; for simplicity, fix
c = .
Moreover, it is assumed that A satisfies H. Remind that by Hand H, F(⋅,v) admits a
global minimum and that the latter is reached.
From now on, we denote
J= {i ∈{, . . ., r} : ∣Diuo∣= ∣DiηχΣ∣> }
and Jc
= I/J.
(.)
Note that Jaddresses the edges in uo = ηχΣ.
Proposition (ϕ′(+) = , F(⋅, v) is C(R+).)
Assume H, H, H, H, H, Hand
H. Let every (local) minimizer of F(⋅, AηχΣ) satisfies the property stated in (> .). Then
there are two constants η> and η> ηsuch that
η ∈(, η) ⇒∣Di ˆuη∣⩽θ, ∀i ∈I
(ˆuη is fully smooth)
(.)
whereas
η ⩾η⇒∣Di ˆuη∣⩽θ,
∀i ∈Jc
,
∣Di ˆuη∣⩾θ,
∀i ∈J,
(the edges in ˆuη are correct).
This result corroborates the interpretation of θand θas thresholds for the detection of
smooth differences and edges, respectively – see (> .) and the comments following this
equation.
Proposition (Truncated quadratic PF)
Let ϕ(t)
=
min{αt,} – see (f) in
> Table -. Assume that Hand Hare satisfied. Define ωΣ ∈Rp by
ωΣ = (A∗A + βαD∗D)−A∗AχΣ.
(.)
Then there are η> and η> ηsuch that
η ∈[, η)
⇒
ˆuη = η ωΣ,
(ˆuη is fully smooth)
(.)
η ⩾η
⇒
ˆuη = η χΣ,
(exact recovery, ˆuη = uo)
(.)
Moreover, ˆuη in (> .) and (> .) is the unique global minimizer of the relevant
F(⋅, ηAχΣ).
Observe that ηωΣ in (> .) is the regularized least-squares solution, hence it does not
involve edges. For η ⩾ηthe global minimizer ˆuη is equal to the original uo.
Note that Hand Hentail lim
t→∞ϕ′(t) = . Then the edge-preservation necessary condition His trivially
satisfied.

Energy Minimization Methods 

Proposition (< ϕ′(+) < +∞)
Assume H, H, H(p. ), H, H, Hand
H. Let every (local) minimizer of F(⋅, AηχΣ) satisfies the property stated in (> .) (see
Theorem ). Then there exist η> and η> ηsuch that
η ∈[, η) ⇒
ˆuη = ηζ lp, where ζ = (Alp)∗AχΣ
∥Alp∥

(ˆuη is constant)
(.)
whereas
η > η⇒
Di ˆuη = ,
∀i ∈Jc
,
∣Di ˆuη∣⩾θ,
∀i ∈J.
(ˆuη is piecewise constant with correct edges)
If η is small, the global solution ˆuη is constant, while for η large enough, ˆuη has the
same edges and the same constant regions as the original uo = ηχΣ. Moreover, if Σ and Σc
are connected with respect to {Di : i ∈I}, there are ˆsη ∈(, η] and ˆcη ∈R such that
ˆuη = ˆsη χΣ + ˆcηlp,
(asymptotically exact recovery, ˆuη
η→∞
→uo)
(.)
and ˆsη →η and ˆcη →as η →∞. Hence ˆuη provides a faithful restoration of the original
uo = ηχΣ.
Proposition (“-”PF)
Let ϕ be given by (f) in > Table -. Assume that Hand H
are satisfied. Then there are η> and η> ηsuch that
η ∈[, η)
⇒
ˆuη = ηζ lp
(ˆuη is constant),
(.)
η > η
⇒
ˆuη = η χΣ,
(exact recovery, ˆuη = uo)
(.)
where ζ is given in (> .). Moreover, ˆuη in (> .) and (> .) is the unique global
minimizer of F(⋅, AηχΣ).
▷
By way of conclusion, non convexity and boundedness of ϕ can ensure correct edge recovery
as well as (possibly asymptotically) correct recovery of uo.
The results presented here can be extended to other forms of finite differences. At this
stage, the assumption H(i.e., that A∗A is invertible) seems difficult to avoid. A further
development is necessary to characterize the global minimizer ˆuη when data are corrupted
with some perturbations.
.
Minimizers Under Nonsmooth Regularization
▷
Observe that the minimizers corresponding to ϕ′(+) > (nonsmooth regularization) in
> Figs. 5-3b, c, > 5-8b, > 5-9b, d, > 5-11a–c, > 5-12d are constant on numerous regions. This
section is aimed to explain and to generalize this phenomenon.


Energy Minimization Methods
Consider
F(u,v) = Ψ(u,v) + βΦ(u)
(.)
Φ(u) =
r
∑
i=
ϕ (∥Diu∥),
(.)
where Ψ : Rp × Rq →R is any explicit or implicit Cm-smooth function for m ⩾and
Di : Rp ↦Rs, ∀i ∈I = {,⋯, r}, are general linear operators for any integer s ⩾. It is
assumed that ϕ satisfies Hand Halong with
H
ϕ is C-smooth on R∗
+ and ϕ′(+) > .
It worths emphasizing that Ψ and ϕ can be convex or nonconvex. Let us define the set-valued
function J on Rp by
J (u) = {i ∈I : ∥Diu∥= }.
(.)
Given u ∈Rp, J (u) indicates all regions where Diu = . Such regions are calledstrongly
homogeneous with respect to {Di}. In particular, if {Di} correspond to first-order dif-
ferences between neighboring samples of u or to discrete gradients, J (u) indicates all
constant regions in u.
..
Main Theoretical Result
The results presented below are extracted from [].
Theorem 
Given v ∈Rq, assume that F(⋅,v) in (> .–.) is such that Ψ is Cm,
m ⩾on Rp × Rq, and that ϕ satisfies H, H, and H. Let ˆu ∈Rp be a (local) minimizer
of F(⋅,v). For ˆJ def= J (ˆu), let KˆJ be the vector subspace
KˆJ = {u ∈Rp : Diu = ,∀i ∈ˆJ}.
(.)
Suppose also that
(a) δF(ˆu,v)(w) > , for every w ∈K
ˆJ /{}.
(b) There is an open subset O′
ˆJ ⊂Rq such that F∣KˆJ (., O′
ˆJ) has a local minimizer function
UˆJ : O′
ˆJ →KˆJ which is Cm−continuous at v and ˆu = UˆJ(v).
Then there is an open neighborhood OˆJ ⊂O′
ˆJ of v such that F(⋅, OˆJ) admits a Cm−local
minimizer function U : OˆJ →Rp which satisfies U(v) = ˆu, U∣KˆJ = UˆJ and
∈OˆJ ⇒DiU( ) = , for all i ∈ˆJ.
(.)
The adverb “strongly” is used in order to emphasize the difference with just “homogeneous regions” that are
characterized by ∥Diu∥≈.

Energy Minimization Methods 

It can be shown that the result stated in (> .) holds true also for irregular functions ϕ
of the form (f) in > Table -. Remind that ˆJ and KˆJ are the same as those introduced in
(> .).
Commentary on the assumptions.
Since F(⋅,v) has a local minimum at ˆu, Theorem 
tells us that δF(ˆu,v)(w) ⩾, for all w ∈K
ˆJ and this inequality cannot be strict unless F
is nonsmooth. When Φ is nonsmooth as specified above, it is easy to see that (a) is not a
strong requirement. By Lemma , condition (b) holds if F∣KˆJ is Cm on a neighborhood of
(ˆu,v) belonging to KˆJ × Rq, and if D(F∣KˆJ)(ˆu,v) = and D
(F∣KˆJ)(ˆu,v) ≻, which is
the classical sufficient condition for a strict (local) minimizer.
If F is (possibly nonconvex) of the form (> .) and assumption H(> Sect. ..)
holds as well, the intermediate results given in item next to Theorem (> Sect. ..)
show that (a) and (b) are satisfied for any v ∈Rq/Θ where Θ is closed and Lq(Θ) = . In
these conditions, real-world data have no real chanceto belong to Θ so they lead to (local)
minimizers that satisfy conditions (a) and (b).
Signiﬁcance of the results.
Using the definition of J in (> .), the conclusion of the
theorem can be reformulated as
v ∈OˆJ ⇒J (U(v)) ⊇ˆJ ⇔U(v) ∈KˆJ.
(.)
Minimizers involving large subsets ˆJ are observed in > Figs. -b, c, > -b, > -b, d,
> -a–c, > -d. It was seen in Examples and , as well as in > Sect. ..(case
ϕ′(+) > ), that ˆJ is nonempty for data v living in an open OˆJ. An analytical example
will be presented in > Sect. ... So consider that #ˆJ ⩾. Then (> .) is a severe restric-
tion since KˆJ is a closed and negligible subset of Rp whereas data v vary on open subsets
OˆJ of Rq. (The converse situation where a local minimizer ˆu of F(⋅,v) satisfies Di ˆu ≠, for
all i seems quite natural, especially for noisy data.) Note also that there is an open subset
˜OˆJ ⊂OˆJ such that J (U(v)) = ˆJ for all v ∈˜OˆJ.
Focus on a (local) minimizer function U : O →Rp for F(⋅, O) and put ˆJ = J (U(v))
for some v ∈O. By Theorem , the sets OˆJ and ˜OˆJ are of positive measure in Rq. The
chance that random points
(namely noisy data) come across OˆJ, or ˜OˆJ, is real.When
data
range over O, the set-valued function (J ○U) generally takes several distinct values,
say {J j}. Thus, with a (local) minimizer function U, defined on an open set O, there is
associated a family of subsets { ˜OJ j} which form a covering of O. When
∈˜OJ j, we find a
minimizer ˆu = U( ) satisfying J (ˆu) = J j. This underlies the conclusion stated next.
▷
Energies with nonsmooth regularization terms as those considered here, exhibit local mini-
mizers which generically satisfy constraints of the form J (
∧u) ≠∅.
In particular, if {Di} are discrete gradients or ﬁrst-order diﬀerence operators, minimizers
∧u
are typically constant on many regions. For example, if ϕ(t) = t, we have Φ(u) =TV(u) and
More precisely, the probability that real data (a random sample of Rq) do belong to Θ is null.
The reason for this claim is that probability that
∈OˆJ, or that
∈˜OˆJ, is strictly positive.


Energy Minimization Methods
1
100
0
4
1
100
0
4
⊡Fig. -
Data v = uo + n (—) corresponding to the original uo (-.-.) contaminated with two diﬀerent
noise samples n on the left and on the right
this explains the stair-casing eﬀect observed in TV methods on discrete images and signals
[, ].
Restoration of a noisy signal.
In > Figs. -and > -we consider the restoration
of a piecewise constant signal uo from noisy data v = uo + n by minimizing F(u,v) =
∥u −v∥+ β ∑
p−
i=ϕ(∣u[i] −u[i + ]∣). In order to evaluate the ability of different func-
tions ϕ to recover, and to conserve, the locally constant zones yielded by minimizing the
relevant F(⋅,v), we process in the same numerical conditions two data sets, contaminated
by two very different noise realizations plotted in > Fig. -. The minimizers shown in
> Figs. -a–c correspond to functions ϕ such that ϕ′(+) > . In accordance with the
above theoretical results, they are constant on large segments. In each one of these figures,
the reader is invited to compare the subsets where the minimizers corresponding to the
two data sets (> Fig. -) are constant. In contrast, the function ϕ in > Fig. -d satisfies
ϕ′(+) = and the resultant minimizers are nowhere constant.
..
The D TV Regularization
The example below describes the sets ˜OJ, for every J ⊂{, . . . , r}, in the context of the
one-dimensional discrete TV regularization. It provides a rich geometric interpretation of
Theorem . Let F : Rp × Rp →R be given by
F(u,v) = ∥Au −v∥
+ β
p−
∑
i=
∣u[i] −u[i + ]∣,
(.)
where A ∈Rp×p is invertible and β > . It is easy to see that there is a unique minimizer
function U for F(⋅,Rp). We have two striking phenomena (see [] for details):
. For every point ˆu ∈Rp, there is a polyhedron Q ˆu ⊂Rp of dimension #J (ˆu), such that
for every v ∈Q ˆu, the same point U(v) = ˆu is the unique minimizer of F(⋅,v).

Energy Minimization Methods 

1
100
0
4
0
0
0
4
1
100
1
100
1
100
1
100
0
4
0
0
0
4
1
100
1
100
1
100
f(t) = t (here f (0+) = 1)
f(t ) = (t + a)2 (here f (0+) = 2a)
f
a
f
f
(t) =
at
1 + at (here
(0+) =
)
(t) =
2
2
if t
αt
t
−α
α
α
2
2
if t >
(here f (0+)=0)
a
c
d
b
⊡Fig. -
Restoration using diﬀerent functions ϕ. Original uo (-.-.), minimizer
∧u (—). Each ﬁgure from
(a) to (d) shows the two minimizers
∧u corresponding to the two data sets in > Fig. -(left
and right), while the shape of ϕ is plotted in the middle
. For every J ⊂{, . . . , p−}, there is a subset ˜OJ ⊂Rp, composed of p−#J−unbounded
polyhedra (of dimension p) of Rp such that for every v ∈˜OJ, the minimizer ˆu of
F(⋅,v) satisfies ˆui = ˆui+for all i ∈J and ˆui ≠ˆui+for all i ∈Jc. A description of these
polyhedra is given in the appendix of []. Moreover, their closure forms a covering
of Rp.
Remark 
The energy in (> .) has a straightforward Bayesian interpretation in terms
of maximum a posteriori (MAP) estimation (see > Sec. .., first item). The quadratic data-
fidelity term corresponds to a forward model of the form v = Auo + n where n is independent
identically distributed (i.i.d.) Gaussian noise with mean zero and variance denoted by σ .
The likelihood reads π(v∣u) = exp (−
σ ∥Au −v∥
). The regularization term corresponds
to an i.i.d. Laplacian prior on each difference u[i] −u[i + ], ⩽i ⩽p −. More precisely,
each difference has a distribution of the form exp (−λ∣t∣) for λ =
β
σ . Since this density is
continuous on R, the probability to get a null sample t = u[i] −u[i + ] = , is equal to zero.
However, the results presented above show that for the minimizer ˆu of F(⋅,v), the probability
to have ˆu[i] −ˆu[i + ] = for a certain amount of indexes i is strictly positive. This means
that the Laplacian prior on the differences u[i] −u[i + ] is far from being incorporated in
the MAP solution ˆu. On the other hand, given that ∥ϕ′∥∞= and that ∥D∥= , Theorem 
tells us that ∥Aˆu −v∥∞⩽β∥(AA∗)−A∥∞, hence the recovered noise (Aˆu −v)[i], ⩽i ⩽q


Energy Minimization Methods
is bounded. However, the noise n in the forward model is unbounded. The distribution of the
original noise n is not incorporated in the MAP estimate ˆu neither.
..
An Application to Computed Tomography
The concentration of an isotope in a part of the body provides an image characterizing
metabolic functions and local blood flow [, , ]. In Emission Computed Tomography
(ECT), a radioactive drug is introduced in a region of the body and the emitted photons
are recorded around it. Data are formed by the number of photons v[i] ⩾reaching each
detector, i = , . . . , q. The observed photon counts v have a Poissonian distribution [, ].
Their mean is determined using projection operators {ai, i = ,, . . ., q} and a constant
ρ > . The data-fidelity Ψ derived from the log-likelihood function is nonstrictly convex
and reads:
Ψ(u,v) = ρ ⟨
q
∑
i=
ai, u⟩−
q
∑
i=
v[i]ln(⟨ai,u⟩).
(.)
> Figure -presents image reconstruction from simulated ECT data by minimizing and
energy of the form (> .) and (> .) where Ψ is given by (> .) and {Di} yield the
first-order differences between each pixel and its eight nearest neighbors. One observes,
yet again, that a PF ϕ which is nonconvex with ϕ′(+) > leads to a nicely segmented
piecewise constant reconstruction.
Original phantom
ECT simulated data
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
f (0) = 0, edge  preserving
f(t) = t / (α + t) (f (0+) > 0, nonconvex)
a
b
d
c
⊡Fig. -
ECT. F(u, v) = Ψ(u, v) + β ∑i∈I ϕ(∣Diu∣)

Energy Minimization Methods 

.
Minimizers Relevant to Nonsmooth Data-ﬁdelity
▷
> Figure 5-13 shows that there is a striking distinction in the behavior of the minimizers rele-
vant to nonsmooth data-ﬁdelity terms (b) with respect to nonsmooth regularization (a). More
precisely, many data samples are ﬁttedexactly when the data-ﬁdelity term is nonsmooth. This
particular behavior is explained and generalized in the present section.
Consider
F(u,v) = Ψ(u,v) + βΦ(u),
(.)
Ψ(u,v) =
q
∑
i=
ψ (∣⟨ai,u⟩−v[i]∣),
(.)
where ai ∈Rp for all i ∈{, . . . , q} and ψ : R+ →R+ is a function satisfying
H
ψ() = , ψ is increasing and not identically null on R+, and ψ ∈C(R+).
Remind that the latter condition means that t ↦ψ(∣t∣) is continuous on R (cf. Defi-
nition ). Let A ∈Rq×p denote the matrix such that for any i = , . . . , q, its ith row
reads a∗
i .
Recall that many papers are dedicated to the minimization of Ψ(u,v) = ∥Au −v∥ρ
ρ
alone, i.e., F = Ψ, mainly for ρ = (least-squares problems) [], often for ρ = (least
absolute deviations) [], but also for ρ ∈(,∞] [, ]. Nonsmooth data-fidelity terms
Ψ in energies of the form (> .) and (> .) were introduced in image processing in
[].
= u −v 2
2 +
Du 1
= u −v 1 +
Du 2
2
= u −v
2
2 +
Du 2
2
a
b
c
Stair - casing
Exact data - ﬁt
(u,v )
(u,v )
(u,v)
⊡Fig. -
D is a ﬁrst-order diﬀerence operator, i.e., Diu = u[i] −u[i + ], ⩽i ⩽p −. Data (- - -), restored
signal (—). Constant pieces in (a) are emphasized using “∗,”while data samples that are
equal to the relevant samples of the minimizer in (b) are emphasized using “○.”


Energy Minimization Methods
..
General Theory
Here we present some results on the minimizers ˆu of F as given in (> .) and (> .),
where Ψ is nondifferentiable, obtained in [, ]. Additional assumptions are that
H
ψ is Cm, m ⩾on R∗
+ and ψ′(+) > is finite.
H
The regularization term Φ : Rp →R in (> .) is Cm, m ⩾.
Note that Φ in (> .) can be convex or nonconvex.
To analyze the phenomenon observed in > Fig. -b, the following set-valued function
J will be useful:
(u,v) ∈(Rp × Rq) ↦J (u,v) = {i ∈{, . . . , q} : ⟨ai,u⟩= v[i]}.
(.)
Given v and a (local) minimizer ˆu of F(⋅,v), the set of all data entries v[i] that are fitted
exactly by (Aˆu)[i] reads ˆJ = J (ˆu,v). Its complement is ˆJc = {, . . . , q}/ˆJ.
Theorem 
Let F be of the form (> .–.) where assumptions H, H, and H
hold true. Given v ∈Rq let ˆu ∈Rp be a (local) minimizer of F(⋅,v). For ˆJ = J (ˆu, y), where
J is defined according to (> .), let
KˆJ(v) = {u ∈Rp : ⟨ai,u⟩= v[i] ∀i ∈ˆJ and ⟨ai,u⟩≠v[i] ∀i ∈ˆJc},
and let KˆJ be its tangent. Suppose the following:
(a) The set {ai : i ∈ˆJ} is linearly independent.
(b) ∀w ∈KˆJ/{} we have D(F∣KˆJ(v))(ˆu,v)w = and D
(F∣KˆJ(v))(ˆu,v)(w,w) > .
(c) ∀w ∈K
ˆJ /{} we have δF(ˆu,v)(w) > .
Then there are a neighborhood OˆJ ⊂Rq containing v and a Cm−local minimizer function
U : OˆJ →Rp relevant to F(⋅, OˆJ), yielding in particular ˆu = U(v), and
∈OˆJ ⇒
{⟨ai,U( )⟩= [i]
if
i ∈ˆJ,
⟨ai,U( )⟩≠[i]
if
i ∈ˆJc.
(.)
The latter means that J (U( ), ) = ˆJ is constant on OˆJ.
Note that for every v and J ≠∅, the set KJ(v) is a finite union of connected components,
whereas its closure KJ(v) is an affine subspace. Its tangent KˆJ reads
KˆJ = {u ∈Rp : ⟨ai,u⟩= ∀i ∈ˆJ}.
A comparison with KˆJ in (> .) may be instructive. Compare also (b) and (c) in Theo-
rem with (a) and (b) in Theorem . By the way, conditions (b) and (c) in Theorem 

Energy Minimization Methods 

ensure that F(⋅,v) reaches a strict minimum at ˆu [, Proposition ]. Observe that this suf-
ficient condition for strict minimum involves the behavior of F(⋅,v) on two orthogonal
subspaces separately. This occurs because of the nonsmoothness of t ↦ψ(∣t∣) at zero. It
can be useful to note that at a minimizer ˆu,
δF(ˆu,v)(w) = ϕ′(+)∑
i∈ˆJ
∣⟨ai,w⟩∣+ ∑
i∈ˆJc
ψ′(⟨ai, ˆu⟩−v[i])⟨ai,w⟩+ βDΦ(ˆu)w ⩾,
for any w ∈Rp
(.)
Commentary on the assumptions.
Assumption (a) does not require the independence
of the whole set {ai : i ∈{, . . . , q}}. It is shown [, Remark ] that this assumption fails to
hold only for some v is included in a subspace of dimension strictly smaller than q. Hence,
assumption (a) is satisfied for almost all v ∈Rq and the theorem addresses any matrix A,
whether it be singular or invertible.
Assumption (b) is the classical sufficient condition for a strict local minimum of a
smooth function over an affine subspace. If an arbitrary function F(⋅,v) : Rp →R has
a minimum at ˆu, then necessarily δF(ˆu,v)(w) ⩾for all w ∈K
ˆJ , see Theorem . In
comparison, (c) requires only that the latter inequality be strict.
It will be interesting to characterize the sets of data v for which (b) and (c) may fail at
some (local) minimizers. Some ideas from > Sect. ...might provide a starting point.
Corollary 
Let F be of the form (> .) and (> .) where p = q, and H, H,
and Hhold true. Given v ∈Rq let ˆu ∈Rp be a (local) minimizer of F(⋅,v). Suppose the
following:
(a) The set {ai : ⩽i ⩽q} is linearly independent.
(b) ∀w ∈Rq satisfying ∥w∥= we have β ∣DΦ(ˆu)w∣< ψ′(+)
q
∑
i=
∣⟨ai,w⟩∣.
Then
ˆJ = {, . . . , q}
and there are a neighborhood OˆJ ⊂Rq containing v and a Cm−local minimizer function
U : OˆJ →Rp relevant to F(⋅, OˆJ), yielding in particular ˆu = U(v), and
∈OˆJ ⇒⟨ai,U( )⟩= [i] ∀i ∈ˆJ = {, . . ., q}.
(.)
More precisely, U( ) = A−for any
∈OˆJ.
Note that in the context of Corollary , A is invertible. Combining this with (> .)
and (b) shows that
KˆJ(v) = {u ∈Rp : Au = v} = A−v,
KˆJ = ker(A) = {}.


Energy Minimization Methods
Then
{v ∈Rq : β ∣DΦ(A−v)w∣< ψ′(+)
q
∑
i=
∣⟨ai,w⟩∣, ∀w ∈Rq/{},∥w∥= } ⊂OˆJ ≡O{,..., q}.
The subset on the left contains an open subset of Rq by the continuity of v ↦DΦ(A−v)
combined with (b).
Signiﬁcance of the results.
Consider that #J ⩾. The result in (> .) means that the
set-valued function v →J (U(v),v) is constant on OˆJ, i.e., that J is constant under small
perturbations of v. Equivalently, all residuals ⟨ai,U(v)⟩−v[i] for i ∈ˆJ are null on OˆJ.
Intuitively, this may seem unlikely, especially for noisy data.
Theorem shows that Rq contains volumes of positive measure composed of data that
lead to local minimizers which fit exactly the data entries belonging to the same set. In
general, there are volumes corresponding to various ˆJ so that noisy data come across them.
That is why nonsmooth data-fidelity terms generically yield minimizers fitting exactly a cer-
tain number of the data entries. The resultant numerical effect is observed in > Fig. -b
as well as in > Figs. -and > -.
Remark (stability of minimizers)
The fact that there is a Cm−local minimizer func-
tion shows that, in spite of the nonsmoothness of Ψ (and hence of F), for any v, all the
local minimizers of F(⋅,v) which satisfy the conditions of the theorem are stable under weak
perturbations of data v. This result extends Lemma .
Example.
Let F read
F(u,v) =
q
∑
i=
∣u[i] −v[i]∣+ β

q
∑
i=
(u[i]), β > .
It is easy to compute (see [, p. ]) that there is a local minimizer function U whose
entries read
U(v)[i] = 
βsign(v[i])
if
∣v[i]∣> 
β,
U(v)[i] = v[i]
if
∣v[i]∣⩽
β.
Condition (c) in Theorem fails to hold only for {v ∈Rq : ∣v[i]∣= 
β, ∀i ∈ˆJ}. The latter
set is of Lebesgue measure zero in Rq
For any J ∈{, . . . , q} put
OJ = {v ∈Rq : ∣v[i]∣⩽
β, ∀i ∈J
and
∣v[i]∣> 
β , ∀i ∈Jc}.
Obviously, every v ∈OJ gives rise to a minimizer ˆu satisfying
ˆu[i] = v[i], ∀i ∈J and ˆu[i] ≠v[i], ∀i ∈Jc.

Energy Minimization Methods 

Note that for every J ⊂{, . . ., q}, the set OJ has a positive Lebesque measure in Rq. More-
over, the union of all OJ when J ranges on all subsets J ⊂{, . . . , q} (including the empty set)
forms a partition of Rq.
Numerical experiment.
The original image uo in > Fig. -a can be supposed to be a
noisy version of an ideal piecewise constant image. Data v in > Fig. -b are obtained by
replacing some pixels of uo, whose locations are seen in > Fig. -left, by aberrant impul-
sions, called outliers. In all > Figs. -, > -, > -, and > -, {Di} correspond to
the first-order differences between each pixel and its four nearest neighbors. The image in
> Fig. -a corresponds to an ℓdata-fidelity term for β = .. The outliers are well visible
although their amplitudes are clearly reduced. The image of the residuals v −ˆu, shown in
> Fig. -b, is null everywhere except at the positions of the outliers in v. The pixels cor-
responding to nonzero residuals (i.e., the elements of ˆJc) provide a faithful estimate of the
locations of the outliers in v, as seen in > Fig. -middle. Next, in > Fig. -a we show
a minimizer ˆu of the same F(⋅,v) obtained for β = .. This minimizer does not contain
visible outliers and is very close to the original image uo. The image of the residuals v −ˆu
in > Fig. -b is null only on restricted areas, but has a very small magnitude everywhere
beyond the outliers. However, applying the above detection rule now leads to numerous
false detections, as seen in > Fig. -right.
The minimizers of two different cost-function F involving a smooth data-fidelity term
Ψ, shown in > Figs. -and > -, do not fit any data entry. In particular, the restoration
in > Fig. -corresponds to a nonsmooth regularization and it is constant over large
regions; this effect was explained in > Sect. ..
More details and information can be found in [].
Original u0
a
b
Data v = u0 + outliers
⊡Fig. -
Original uo and data v degraded by outliers
Restoration u for b = 0.14
Residual v − uˆ
ˆ
a
b
⊡Fig. -
Restoration using F(u, v) = ∑i ∣u[i] −v[i]∣+ β ∑i∈I ∣Diu∣α α = .and β = .


Energy Minimization Methods
Residual v − uˆ
Restoration u for b = 0.25
ˆ
a
b
⊡Fig. -
Restoration using F(u, v) = ∑i ∣u[i] −v[i]∣+ β ∑i∈I ∣Diu∣α for α = .and β = .
⊡Fig. -
Left: the locations of the outliers in v. Next – the locations of the pixels of a minimizer
∧u at which
∧u[i] ≠v[i]. Middle: these locations for the minimizer obtained for β = .,
> Fig. -. Right: the same locations for the minimizer relevant to β = ., see > Fig. -
Restoration u for b = 0.2
Residual v − uˆ
ˆ
a
b
⊡Fig. -
Restoration using a smooth cost function, F(u, v) = ∑i (u[i] −v[i])+ β ∑i(∣Diu∣), β = .
a
b
Residual v − uˆ
Restoration u for b = 0.2
ˆ
⊡Fig. -
Restoration using nonsmooth regularization F(u, v) = ∑
i
(u[i] −v[i])+ β ∑
i
∣Diu∣, β = .

Energy Minimization Methods 

..
Applications
The possibility to keep some data samples unchanged by using nonsmooth data-fidelity is a
precious property in various application fields. Nonsmooth data-fidelities are good to detect
and smooth outliers. This was applied to impulse noise removal in [] and to separate
impulse from Gaussian noise in []. This property was extensively exploited for deblurring
under impulse noise contamination, see, e.g., [–].
Denoising of frame coeﬃcients.
Consider the recovery of an original (unknown) uo ∈
Rp– a signal or an image containing smooth zones and edges – from noisy data
v = uo + n,
where n represents a perturbation. As discussed in > Sect. ., a systematic default of the
images and signals restored using convex edge-preserving PFs ϕ is that the amplitude of
edges is underestimated.
Shrinkage estimators operate on a decomposition of data v into a frame of ℓ, say
{wi : i ∈J} where J is a set of indexes. Let W be the corresponding frame operator, i.e.,
(Wv)[i] = ⟨v,wi⟩, ∀i ∈J, and ̃
W be a left inverse of W, giving rise to the dual frame
{̃wi : i ∈J}. The frame coefficients of v read y = Wv and are contaminated with noise
Wn. The inaugural work of Donoho and Johnstone [] considers two different shrinkage
estimators: given T > , hard thresholding corresponds to
yT[i] = {y[i]
if
i ∈J,

if
i ∈J,
where
{J= {i ∈J : ∣y[i]∣⩽T};
J= J/J,
(.)
while in soft thresholding one takes yT[i] = y[i] −Tsign(y[i]) if i ∈Jand yT[i] = if
i ∈J. Both soft and hard thresholding are asymptotically optimal in the minimax sense if
n is white Gaussian noise of standard deviation σ and
T = σ
√
loge p.
(.)
This threshold is difficult to use in practice because it increases with the size of u. Numerous
other drawbacks were found and important improvements were realized, see, e.g., [, , ,
, , , , ]. In all these cases, the main problem is that smoothing large coefficients
oversmooths edges while thresholding small coefficients can generate Gibs-like oscillations
near edges, see > Figs. -c, d. If shrinkage is weak, noisy coefficients (outliers) remain
almost unchanged and produce artifacts having the shape of {̃wi}, see > Figs. -c–e.
In order to alleviate these difficulties, several authors proposed hybrid methods where
the information contained in important coefficients y[i] is combined with priors in the
domain of the sought-after signal or image [, , , , , ]. A critical analysis of
these preexisting methods is presented in [].
The key idea in [] is to construct a specialized hybrid method involving ℓdata-fidelity
on frame coefficients. More precisely, data are initially hard thresholded – see (> .) –
using a suboptimal threshold T in order to keep as much as possible information. (The use


Energy Minimization Methods
of another more sophisticated shrinkage estimator would alter all coefficients, that is why
a hard thresholding is preferred.) Then
. Jis composed of:
•
Large coefficients bearing the main features of uo that one wishes to preserve
intact
•
Aberrant coefficients (outliers) that must be restored using the regularization
term
. Jis composed of:
•
Noise coefficients that must be kept null.
•
Coefficients y[i] corresponding to edges and other details in uo – these need to be
restored in accordance with the prior incorporated in the regularization term.
The theory in [] is developed for signals and images defined on a subset of Rd where
d = or d = , respectively, and for frames of L. To ensure coherence of the chapter,
the approach is presented in the discrete setting. In order to reach the goals formulated in
and above, denoised coefficients ˆx are defined as a minimizer of the hybrid energy
F(., y) given below:
F(x, y) = λ∑
i∈J
∣x[i] −y[i]∣+ λ∑
i∈J
∣x[i]∣+ ∑
i∈I
ϕ (∥Di ̃
Wx∥), λ,> ,
(.)
where ϕ is convex and edge preserving. Then the sought-after denoised image or
signal is
ˆu = ̃
W ˆx = ∑
i∈J
̃wi ˆx[i].
Several important properties relevant to the minimizers of F in (> .), the parameters
λi, i ∈{,} and the solution ˆu are outlined in [].
Noisy data v are shown along with the original uo in > Fig. -a. The restoration in
> Fig. -b minimizes F(u) = ∥Au −v∥
+ β ∑i ϕ(∥Diu∥) where ϕ(t) =
√
α + tfor
α = ., β = – homogeneous regions remain noisy, edges are smoothed and spikes are
eroded. > Fig. -c is obtained using the sure-shrink method [] form the toolbox Wave-
Lab. The other restorations use thresholded Daubechies wavelet coefficients with eight
vanishing moments. The optimal value for the hard thresholding obtained using (> .) is
T = . The relevant restoration – > Fig. -d – exhibits important Gibbs-like oscillations
as well as wavelet-shaped artifacts.
The threshold chosen in [] is T
= . The corresponding coefficients have a
richer information content but ̃
WyT, shown in > Fig. -e manifests Gibbs artifacts and
many wavelet-shaped artifacts. Introducing the thresholded coefficients of
> Fig. -e
in (> .) leads to > Fig. -f : edges are clean and piecewise polynomial parts are well
recovered.

Energy Minimization Methods 

1
250
500
1
250
500
1
250
500
1
250
500
0
100
0
100
1
250
500
0
100
0
100
0
100
1
250
500
0
100
Original and data
TV regularization
Sure - shrink
a
b
c
T
T
T
T
T
T
=35 optimal, ˆu =
i y [i]wi
y , T = 23, ˆu
=
i y [i]wi
⇒
The proposed method
f
e
d
⊡Fig. -
Methods to restore the noisy signal in (a). Restored signal (—), original signal (- -)
..
The L-TV Case
For discrete signals of finite length, energies of the form F(u,v) = ∥u −v∥+ β ∑
p−
i=∣u[i +
] −u[i]∣were considered by Alliney in []. These were exhibited to provide a
variational formulation to digital filtering problems.
Following [, , ], S. Esedoglu and T. Chan explored in [] the minimizers of the
L-TV functional given below
F(u,v) = ∫Rd ∣u(x) −v(x)∣dx + β ∫Rd ∣∇u(x)∣dx,
(.)
where the sought-after minimizer ˆu belongs to the space of bounded variation functions
on Rd. The main focus is on images, i.e., d = . The analysis in [] is based on a rep-
resentation of F in (> .) in terms of the level sets of u and v. Most of the results are
established for data v given by the characteristic function χΣ of a bounded domain Σ ⊂Rd.
Theorem .in [] says that if v = χΣ, where Σ ⊂Rd is bounded, then F(⋅,v) admits a
minimizer of the form ˆu = χ ˆΣ (with possibly ˆΣ ≠Σ). Furthermore, Corollary .. in []
states that if in addition Σ is convex, then for almost every β ⩾, F(⋅,v) admits a unique
minimizer and ˆu = χ ˆΣ with ˆΣ ⊆Σ. Moreover, it is shown that small features in the image
maintain their contrast intact up to some value of β while for a larger β they suddenly
disappear.
Recently, L-TV energies were revealed very successful in image decomposition, see
e.g., [, ].


Energy Minimization Methods
.
Conclusion
In this chapter we provided some theoretical results relating the shape of the energy
F to minimize and the salient features of its minimizers ˆu (see (> .)). These results
can serve as a kind of backward modeling: given an inverse problem along with our
requirements (priors) on its solution, they guide us how to construct an energy func-
tional whose minimizers properly incorporate all this information. The theoretical
results are illustrated using numerical examples. Various application fields can take a
benefit from these results. The problem of such a backward modeling remains open
because of the infinite diversity of the inverse problems to solve and the possible energy
functionals.
.
Cross-References
> Inverse Scattering
> Large-Scale Inverse Problems
> Learning, Classification, Data Mining
> Linear Inverse Problems
> Numerical Methods for Variational Approach in Image Analysis
> Regularization Methods for Ill-Posed Problems
> Segmentation with Priors
> Tomography
> Total Variation in Imaging
References and Further Reading
. Alliney S () Digital filters as absolute norm
regularizers. IEEE Trans Signal Process SP-
:–
. Alter F, Durand S, Forment J () Adapted
total
variation
for artifact free decompres-
sion of JPEG images. J Math Imaging Vis :
–
. Ambrosio L, Fusco N, Pallara D () Functions
of bounded variation and free discontinuity Prob-
lems. Oxford Mathematical Monographs, Oxford
University Press
. Antoniadis A, Fan J () Regularization of
wavelet approximations. J Acoust Soc Am :
–
. Aubert G, Kornprobst P () Mathematical
problems in image processing, nd edn. Springer,
Berlin
. Aujol J-F, Gilboa G, Chan T, Osher S ()
Structure-texture
image
decomposition
-
modeling, algorithms, and parameter selection.
Int J Comput Vis :–
. Bar L, Brook A, Sochen N, Kiryati N ()
Deblurring of color images corrupted by impul-
sive noise. IEEE Trans Image Process :–
. Bar L, Kiryati N, Sochen N () Image deblur-
ring in the presence of impulsive noise, Interna-
tional. J Comput Vision :–
. Bar L, Sochen N, Kiryati N () Image deblur-
ring in the presence of salt-and-pepper noise.
In Proceeding of th international conference on
scale space and PDE methods in computer vision,
ser LNCS, vol , pp –
. Belge M, Kilmer M, Miller E () Wavelet
domain
image
restoration
with
adaptive

Energy Minimization Methods 

edge-preserving
regularization.
IEEE
Trans
Image Process :–
. Besag JE () Spatial interaction and the statis-
tical analysis of lattice systems (with discussion).
J Roy Stat Soc B :–
. Besag JE () Digital image processing : towards
Bayesian image analysis. J Appl Stat :–
. Black M, Rangarajan A () On the unifica-
tion of line processes, outlier rejection, and robust
statistics with applications to early vision. Int J
Comput Vis :–
. Blake A, Zisserman A () Visual reconstruc-
tion. MIT Press, Cambridge
. Bloomfield B, Steiger WL () Least absolute
deviations: theory, applications and algorithms.
Birkhäuser, Boston
. Bobichon Y, Bijaoui A () Regularized mul-
tiresolution methods for astronomical image
enhancement. Exp Astron :–
. Bouman C, Sauer K () A generalized
Gaussian image model for edge-preserving map
estimation. IEEE Trans Image Process :–
. Bouman C, Sauer K () A unified approach to
statistical tomography using coordinate descent
optimization. IEEE Trans Image Process :
–
. Bredies K, Kunich K, Pock T () Total gener-
alized variation. SIAM J Imaging Sci (to appear)
. Candès EJ, Donoho D, Ying L () Fast discrete
curvelet transforms. SIAM Multiscale Model
Simul :–
. Candès EJ, Guo F () New multiscale
transforms, minimum total variation synthe-
sis.
Applications
to
edge-preserving
image
reconstruction. Signal Process :–
. Catte F, Coll T, Lions PL, Morel JM () Image
selective smoothing and edge detection by non-
linear diffusion (I). SIAM J Num Anal :–
. Chambolle A () An algorithm for total varia-
tion minimization and application. J Math Imag-
ing Vis :–
. Chambolle A, Lions P-L () Image recovery
via total variation minimization and related prob-
lems. Numer Math :–
. Chan T, Esedoglu S () Aspects of total varia-
tion regularized lfunction approximation. SIAM
J Appl Math :–
. Chan TF, Wong CK () Total variation blind
deconvolution. IEEE Trans Image Process :
–
. Charbonnier P, Blanc-Féraud L, Aubert G,
Barlaud M () Deterministic edge-preserving
regularization in computed imaging. IEEE Trans
Image Process :–
. Chellapa R, Jain A () Markov random fields:
theory and application. Academic, Boston
. Chesneau C, Fadili J, Starck J-L () Stein
block thresholdingfor imagedenoising.Technical
report
. Ciarlet PG () Introduction to numerical lin-
ear algebra and optimization. Cambridge Univer-
sity Press, Cambridge
. Coifman RR, Sowa A () Combining the
calculus of variations and wavelets for image
enhancement. Appl Comput Harmon Anal :
–
. Demoment G () Image reconstruction and
restoration: overview of common estimation
structure and problems. IEEE Trans Acoust
Speech Signal Process ASSP-:–
. Do MN, Vetterli M () The contourlet trans-
form: an efficient directional multiresolution
image representation. IEEE Trans Image Process
:–
. Dobson D, Santosa F () Recovery of blocky
images from noisy and blurred data. SIAM J Appl
Math :–
. Donoho DL, Johnstone IM () Ideal spa-
tial adaptation by wavelet shrinkage. Biometrika
:–
. Donoho DL, Johnstone IM () Adapting
to unknown smoothness via wavelet shrinkage.
J Acoust Soc Am :–
. Dontchev AL, Zollezi T () Well-posed opti-
mization problems. Springer, New York
. Durand S, Froment J () Reconstruction of
wavelet coefficients using total variation mini-
mization. SIAM J Sci Comput :–
. Durand S, Nikolova M () Stability of mini-
mizers of regularized least squares objective func-
tions I: study of the local behavior. Appl Math
Optim (Springer, New York) :–
. Durand S, Nikolova M () Stability of mini-
mizers of regularized least squares objective func-
tions II: study of the global behaviour. Appl Math
Optim (Springer, New York) :–
. Durand S, Nikolova M () Denoising of
frame coefficients using ℓdata-fidelity term and
edge-preserving regularization. SIAM J Multi-
scale Model Simulat :–


Energy Minimization Methods
. Duval V, Aujol J-F, Gousseau Y () The TVL
model: a geometric point of view. SIAM J Multi-
scale Model Simulat :–
. Ekeland I, Temam R () Convex analysis
and variational problems. North-Holland/SIAM,
Amsterdam
. Fessler F () Mean and variance of implicitly
defined biased estimators (such as penalized max-
imum likelihood): applications to tomography.
IEEE Trans Image Process :–
. Fiacco A, McCormic G () Nonlinear pro-
gramming. Classics in applied mathematics.
SIAM, Philadelphia
. Froment J, Durand S () Artifact free sig-
nal denoising with wavelets. In Proceedings of
the IEEE international conference on acoustics,
speech and signal processing, vol 
. Geman D () Random fields and inverse prob-
lems in imaging, vol , École d’Été de Probabil-
ités de. Saint-Flour XVIII - , Springer, lecture
notes in mathematics ed., pp –
. Geman D, Reynolds G ()
Constrained
restoration
and
recovery
of
discontinuities.
IEEE Trans Pattern Anal Mach Intell PAMI-:
–
. Geman D, Yang C () Nonlinear image
recovery with half-quadratic regularization. IEEE
Trans Image Process IP-:–
. Geman S, Geman D () Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration
of images. IEEE Trans Pattern Anal Mach Intell
PAMI-:–
. Golub G, Van Loan C () Matrix computa-
tions, rd edn. Johns Hopkins University Press,
Baltimore
. Green PJ () Bayesian reconstructions from
emission tomography data using a modified em
algorithm. IEEE Trans Med Imaging MI-:–
. Haddad A, Meyer Y () Variational methods
in image processing, in “Perspective in Nonlinear
Partial Differential equations in Honor of Haïm
Brezis,” Contemp Math (AMS) :–
. Herman G () Image reconstruction from
projections. The fundamentals of computerized
tomography. Academic, New York
. Hiriart-Urruty J-B, Lemaréchal C () Convex
analysis and minimization algorithms, vols I, II.
Springer, Berlin
. Hofmann B () Regularization for applied
inverse and ill posed problems. Teubner, Leipzig
. Kailath T () A view of three decades of lin-
ear filtering theory. IEEE Trans Inf Theory IT-:
–
. Kak A, Slaney M () Principles of computer-
ized tomographic imaging. IEEE Press, New York
. Katsaggelos AKE () Digital image restoration.
Springer, New York
. Keren D, Werman M () Probabilistic analysis
of regularization. IEEE Trans Pattern Anal Mach
Intell PAMI-:–
. Lange K () Convergence of EM image recon-
struction algorithms with Gibbs priors. IEEE
Trans Med Imaging :–
. Li S () Markov random field modeling in
computer vision, st edn. Springer, New York
. Li SZ () On discontinuity-adaptive smooth-
ness priors in computer vision. IEEE Trans Pat-
tern Anal Mach Intell PAMI-:–
. Luisier F, Blu T () SURE-LET multichannel
image denoising: interscale orthonormal wavelet
thresholding. IEEE Trans Image Process :
–
. Malgouyres F () Minimizing the total vari-
ation under a general convex constraint for
image restoration. IEEE Trans Image Process :
–
. Morel J-M, Solimini S () Variational methods
in image segmentation. Birkhäuser, Basel
. Morozov
VA
()
Regularization
meth-
ods for ill posed problems. CRC Press, Boca
Raton
. Moulin P, Liu J () Analysis of multiresolution
imagedenoisingschemesusinggeneralizedGaus-
sian and complexity priors. IEEE Trans Image
Process :–
. Moulin P, Liu J () Statistical imaging and
complexity regularization. IEEE Trans Inf Theory
:–
. Mumford D, Shah J () Optimal approxima-
tions by piecewise smooth functions and associ-
ated variational problems. Commun Pure Appl
Math :–
. Nashed M, Scherzer O () Least squares and
bounded variation regularization with nondiffer-
entiable functional. Numer Funct Anal Optim
:–
. Nikolova M () Regularisation functions and
estimators. In Proceedings of the IEEE interna-
tional conference on image processing, vol ,
pp –

Energy Minimization Methods 

. Nikolova M () Estimées localement forte-
ment homogènes. Comptes-Rendus de l’Acad
émie des Sciences (série ):–
. NikolovaM() Thresholdingimpliedbytrun-
cated quadratic regularization. IEEE Trans Image
Process :–
. Nikolova M () Image restoration by minimiz-
ing objective functions with non-smooth data-
fidelity terms. In IEEE international conference
on computer vision/workshop on variational and
level-set methods, pp –
. Nikolova M () Minimizers of cost-functions
involving nonsmooth data-fidelity terms. Appli-
cation to the processing of outliers. SIAM J Num
Anal :–
. Nikolova M () A variational approach to
remove outliers and impulse noise. J Math Imag-
ing Vis :–
. Nikolova M () Weakly constrained mini-
mization. Application to the estimation of images
and signals involving constant regions. J Math
Imaging Vis :–
. Nikolova M () Analysis of the recovery of
edges in images and signals by minimizing non-
convex regularized least-squares. SIAM J Multi-
scale Model Simulat :–
. Nikolova M () Analytical bounds on the
minimizers of (nonconvex) regularized least-
squares. AIMS J Inverse Probl Imaging :
–
. Nikolova M () Semi-explicit solution and
fast minimization scheme for an energy with ℓ-
fitting and Tikhonov-like regularization. J Math
Imaging Vis :–
. Perona P, Malik J () Scale-space and edge
detection using anisotropic diffusion. IEEE Trans
Pattern Anal Mach Intell PAMI-:–
. Pham TT, De Figueiredo RJP () Maximum
likelihood estimation of a class of non-Gaussian
densities with application to lp deconvolution.
IEEE Trans Signal Process :–
. Rice JR, White JS () Norms for smoothing
and estimation. SIAM Rev :–
. Rockafellar RT, Wets JB () Variational analy-
sis. Springer, New York
. Rudin L, Osher S, Fatemi C () Nonlinear total
variation based noise removal algorithm. Physica
D:–
. Sauer K, Bouman C () A local update strat-
egy for iterative reconstruction from projections.
IEEE Trans Signal Process SP-:–
. Scherzer O, Grasmair M, Grossauer H, Halt-
meier M, Lenzen F () Variational problems
in imaging. Springer, New York
. Simoncelli EP, Adelson EH (Sept ) Noise
removal via Bayesian wavelet coding. In Pro-
ceedings of the IEEE international conference
on image processing, Lausanne, Switzerland,
pp –
. Stevenson R, Delp E () Fitting curves with
discontinuities. In Proceedings of the st inter-
national workshop on robust computer vision,
Seattle, WA, pp –
. Tautenhahn U () Error estimates for regular-
ized solutions of non-linear ill posed problems.
Inverse Probl :–
. Teboul S, Blanc-Féraud L, Aubert G, Barlaud M
() Variational approach for edge-preserving
regularization using coupled PDE’s. IEEE Trans
Image Process :–
. Tikhonov A, Arsenin V () Solutions of ill
posed problems, Winston, Washington
. Vogel C () Computational methods for
inverse problems. Frontiers in applied mathemat-
ics series, vol . SIAM, New York
. Welk M, Steidl G, Weickert J () Locally ana-
lytic schemes: a link between diffusion filtering
and wavelet shrinkage. Appl Comput Harmon
Anal :–
. Winkler G () Image analysis, random fields
and Markov chain Monte Carlo methods. A
mathematical
introduction.
Applications
of
mathematics, nd edn, vol . Stochastic models
and applied probability. Springer, Berlin


Compressive Sensing
Massimo Fornasier⋅Holger Rauhut
.
Introduction......................................................................
.
Background......................................................................
..
Early Developments in Applications..................................................
..
Sparse Approximation..................................................................
..
Information-Based Complexity and Gelfand Widths..............................
..
Compressive Sensing....................................................................
..
Developments in Computer Science..................................................
.
Mathematical Modeling and Analysis..........................................
..
Preliminaries and Notation............................................................
..
Sparsity and Compression..............................................................
..
Compressive Sensing....................................................................
..
The Null Space Property................................................................
..
The Restricted Isometry Property....................................................
..
Coherence................................................................................
..
RIP for Gaussian and Bernoulli Random Matrices................................
..
Random Partial Fourier Matrices.....................................................
..
Compressive Sensing and Gelfand Widths..........................................
..
Applications..............................................................................
.
Numerical Methods.............................................................
..
The Homotopy Method.................................................................
..
Iteratively Reweighted Least Squares.................................................
...
Weighted ℓ-Minimization.............................................................
...
An Iteratively Reweighted Least Squares Algorithm (IRLS)......................
...
Convergence Properties................................................................
...
Local Linear Rate of Convergence....................................................
...
Superlinear Convergence Promoting ℓτ-Minimization for τ < .................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Compressive Sensing
..
Numerical Experiments................................................................
.
Open Questions..................................................................
..
Deterministic Compressed Sensing Matrices.......................................
..
Removing Log-Factors in the Fourier-RIP Estimate...............................
.
Conclusions......................................................................
.
Cross-References.................................................................

Compressive Sensing 

Abstract: Compressive sensing is a new type of sampling theory, which predicts that
sparse signals and images can be reconstructed from what was previously believed to be
incomplete information. As a main feature, efficient algorithms such as ℓ-minimization
can be used for recovery. The theory has many potential applications in signal process-
ing and imaging. This chapter gives an introduction and overview on both theoretical and
numerical aspects of compressive sensing.
.
Introduction
The traditional approach of reconstructing signals or images from measured data follows
the well-known Shannon sampling theorem [], which states that the sampling rate must
be twice the highest frequency. Similarly, the fundamental theorem of linear algebra sug-
gests that the number of collected samples (measurements) of a discrete finite-dimensional
signal should be at least as large as its length (its dimension) in order to ensure reconstruc-
tion. This principle underlies most devices of current technology, such as analog-to-digital
conversion, medical imaging, or audio and video electronics. The novel theory of compres-
sive sensing (CS) – also known under the terminology of compressed sensing, compressive
sampling, or sparse recovery – provides a fundamentally new approach to data acquisi-
tion, which overcomes this common wisdom. It predicts that certain signals or images can
be recovered from what was previously believed to be highly incomplete measurements
(information). This chapter gives an introduction to this new field. Both fundamental the-
oretical and algorithmic aspects are presented, with the awareness that it is impossible to
retrace in a few pages all the current developments of this field, which was growing very
rapidly in the past few years and undergoes significant advances on an almost daily basis.
CS relies on the empirical observation that many types of signals or images can be well
approximated by a sparse expansion in terms of a suitable basis, that is, by only a small
number of nonzero coefficients. This is the key to the efficiency of many lossy compres-
sion techniques such as JPEG, MP, etc. A compression is obtained by simply storing only
the largest basis coefficients. When reconstructing the signal the non-stored coefficients
are simply set to zero. This is certainly a reasonable strategy when full information of the
signal is available. However, when the signal first has to be acquired by a somewhat costly,
lengthy, or otherwise difficult measurement (sensing) procedure, this seems to be a waste
of resources: First, large efforts are spent in order to obtain full information on the signal,
and afterward most of the information is thrown away at the compression stage. One might
ask whether there is a clever way of obtaining the compressed version of the signal more
directly, by taking only a small number of measurements of the signal. It is not obvious
whether this is possible since measuring directly the large coefficients requires to know a
priori their location. Quite surprisingly, compressive sensing provides nevertheless a way
of reconstructing a compressed version of the original signal by taking only a small amount
of linear and nonadaptive measurements. The precise number of required measurements
is comparable to the compressed size of the signal. Clearly, the measurements have to be
suitably designed. It is a remarkable fact that all provably good measurement matrices


Compressive Sensing
designed so far are random matrices. It is for this reason that the theory of compressive
sensing uses a lot of tools from probability theory.
The first naive approach to a reconstruction algorithm consists in searching for the
sparsest vector that is consistent with the linear measurements. This leads to the combi-
natorial ℓ-problem, see (>.) below, which unfortunately is NP-hard in general. There
are essentially two approaches for tractable alternative algorithms. The first is convex
relaxation leading to ℓ-minimization – also known as basis pursuit, see (>.) – while
the second constructs greedy algorithms. This overview focuses on ℓ-minimization. By
now, basic properties of the measurement matrix, which ensure sparse recovery by ℓ-
minimization are known: the null space property (NSP) and the restricted isometry property
(RIP). The latter requires that all column submatrices of a certain size of the measurement
matrix are well conditioned. This is where probabilistic methods come into play because it
is quite hard to analyze these properties for deterministic matrices with minimal amount of
measurements. Among the provably good measurement matrices are Gaussian, Bernoulli
random matrices, and partial random Fourier matrices.
> Figure -serves as a first illustration of the power of compressive sensing. It shows
an example for recovery of a ten-sparse signal x ∈Cfrom only samples (indicated
0
1
0
0.2
0.1
0
–0.1
–0.2
–1
1
0
–1
100
200
300
0
100
200
300
0
100
200
300
0
5
0
–5
100
200
300
Frequency
Frequency
Frequency
Time
Amplitude
Amplitude
Amplitude
Intensity
a
c
d
b
⊡Fig. -
(a) Ten-sparse Fourier spectrum, (b) time-domain signal of length with samples, (c)
reconstruction via ℓ-minimization, (d) exact reconstruction via ℓ-minimization

Compressive Sensing 

Sampling domain in the frequency plane
a
b
26 iterations
c
126 iterations
d
⊡Fig. -
(a) Sampling data of the nuclear magnetic resonance (NMR) image in the Fourier domain,
which corresponds to only .% of all samples. (b) Reconstruction by backprojection. (c)
Intermediate iteration of an eﬃcient algorithm for large-scale total-variation minimization.
(d) The ﬁnal reconstruction is exact
by the red dots in
> Fig. -b). From a first look at the time-domain signal, one would
rather believe that reconstruction should be impossible from only samples. Indeed, the
spectrum reconstructed by traditional ℓ-minimization is very different from the true spec-
trum. Quite surprisingly, ℓ-minimization performs nevertheless an exact reconstruction,
that is, with no recovery error at all!
An example from NMR imaging serves as a second illustration. Here, the device
scans a patient by taking D or D frequency measurements within a radial geometry.
> Figure -a describes such a sampling set of a D Fourier transform. Since a lengthy
scanning procedure is very uncomfortable for the patient it is desired to take only a min-
imal amount of measurements. Total-variation minimization, which is closely related to
ℓ-minimization, is then considered as recovery method. For comparison,
> Fig. -b
shows the recovery by a traditional backprojection algorithm. > Figure -c and d display
iterations of an algorithm, which was proposed and analyzed in [] to perform effi-
cient large-scale total-variation minimization. The reconstruction in
> Fig. -d is again
exact!


Compressive Sensing
.
Background
Although the term compressed sensing (compressive sensing) was coined only recently
with the paper by Donoho [], followed by a huge research activity, such a development
did not start out of thin air. There were certain roots and predecessors in application areas
such as image processing, geophysics, medical imaging, computer science as well as in pure
mathematics. An attempt is made to put such roots and current developments into context
below, although only a partial overview can be given due to the numerous and diverse
connections and developments.
..
Early Developments in Applications
Presumably the first algorithm, which can be connected to sparse recovery is due to
the French mathematician de Prony []. The so-called Prony method, which has found
numerous applications [], estimates nonzero amplitudes and corresponding frequencies
of a sparse trigonometric polynomial from a small number of equispaced samples by solv-
ing an eigenvalue problem. The use of ℓ-minimization appears already in the Ph.D. thesis
of Logan [] in connection with sparse frequency estimation, where he observed that
L-minimization may recover exactly a frequency-sparse signal from undersampled data
provided the sparsity is small enough. The paper by Donoho and Logan [] is perhaps the
earliest theoretical work on sparse recovery using L-minimization. In NMR spectroscopy
the idea to recover sparse Fourier spectra from undersampled non-equispaced samples
was first introduced in the s [] and has seen a significant development since then.
In image processing the use of total-variation minimization, which is closely connected to
ℓ-minimization and compressive sensing, first appears in the s in the work of Rudin
et al. [], and was widely applied later on. In statistics where the corresponding area is
usually called model selection the use of ℓ-minimization and related methods was greatly
popularized with the work of Tibshirani [] on the so-called least absolute shrinkage and
selection operator (LASSO).
..
Sparse Approximation
Many lossy compression techniques such as JPEG, JPEG-, MPEG, or MPrely on
the empirical observation that audio signals and digital images have a sparse representa-
tion in terms of a suitable basis. Roughly speaking, one compresses the signal by simply
keeping only the largest coefficients. In certain scenarios such as audio signal processing
one considers the generalized situation where sparsity appears in terms of a redundant

Compressive Sensing 

system – a so-called dictionary or frame [] – rather than a basis. The problem of finding
the sparsest representation/approximation in terms of the given dictionary turns out to be
significantly harder than in the case of sparsity with respect to a basis where the expansion
coefficients are unique. Indeed, in [,] it was shown that the general ℓ-problem of find-
ing the sparsest solution of an underdetermined system is NP-hard. Greedy strategies such
as matching pursuit algorithms [], FOCal Underdetermined System Solver (FOCUSS)
[], and ℓ-minimization [] were subsequently introduced as tractable alternatives. The
theoretical understanding under which conditions greedy methods and ℓ-minimization
recover the sparsest solutions began to develop with the work in [,,,,,,,].
..
Information-Based Complexity and Gelfand Widths
Information-based complexity (IBC) considers the general question of how well a function
f belonging to a certain class F can be recovered from n sample values, or more generally,
the evaluation of n linear or nonlinear functionals applied to f []. The optimal recov-
ery error, which is defined as the maximal reconstruction error for the “best” sampling
method and “best” recovery method (within a specified class of methods) over all func-
tions in the class F is closely related to the so-called Gelfand width of F [, , ]. Of
particular interest for compressive sensing is F = BN
, the ℓ-ball in RN since its elements
can be well approximated by sparse ones. A famous result due to Kashin [], and Gluskin
and Garnaev [,] sharply bounds the Gelfand widths of BN
(as well as their duals, the
Kolmogorov widths) from above and below, see also []. While the original interest of
Kashin was in the estimate of n-widths of Sobolev classes, these results give precise per-
formance bounds in compressive sensing on how well any method may recover (approx-
imately) sparse vectors from linear measurements [,]. The upper bounds on Gelfand
widths were derived in [] and [] using (Bernoulli and Gaussian) random matrices, see
also [], and in fact such type of matrices have become very useful also in compressive
sensing [,].
..
Compressive Sensing
The numerous developments in compressive sensing began with the seminal work []
and []. Although key ingredients were already in the air at that time, as mentioned
above, the major contribution of these papers was to realize that one can combine
the power of ℓ-minimization and random matrices in order to show optimal results
on the ability of ℓ-minimization of recovering (approximately) sparse vectors. More-
over, the authors made very clear that such ideas have strong potential for numer-
ous application areas. In their work [, ] Candès, Romberg, and Tao introduced the
restricted isometry property (which they initially called the uniform uncertainty principle),


Compressive Sensing
which is a key property of compressive sensing matrices. It was shown that Gaussian,
Bernoulli, and partial random Fourier matrices [, , ] possess this important prop-
erty. These results require many tools from probability theory and finite-dimensional
Banach space geometry, which have been developed for a rather long time now, see,
for example, [,].
Donoho [] developed a different path and approached the problem of characterizing
sparse recovery by ℓ-minimization via polytope geometry, more precisely, via the notion of
k-neighborliness. In several papers, sharp phase transition curves were shown for Gaussian
random matrices separating regions where recovery fails or succeeds with high probability
[,,]. These results build on previous work in pure mathematics by Affentranger and
Schneider [] on randomly projected polytopes.
..
Developments in Computer Science
In computer science, the related area is usually addressed as the heavy hitters detection or
sketching. Here one is interested not only in recovering signals (such as huge data streams
on the Internet) from vastly undersampled data, but one requires sublinear runtime in the
signal length N of the recovery algorithm. This is no impossibility as one only has to report
the locations and values of the nonzero (most significant) coefficients of the sparse vector.
Quite remarkably sublinear algorithms are available for sparse Fourier recovery []. Such
algorithms use ideas from group testing, which date back to World War II, when Dorfman
[] invented an efficient method for detecting draftees with syphilis.
In sketching algorithms from computer science one actually designs the matrix and the
fast algorithm simultaneously [,]. More recently, bipartite expander graphs have been
successfully used in order to construct good compressed sensing matrices together with
associated fast reconstruction algorithms [].
.
Mathematical Modeling and Analysis
This section introduces the concept of sparsity and the recovery of sparse vectors
from incomplete linear and nonadaptive measurements. In particular, an analysis of
ℓ-minimization as a recovery method is provided. The null-space property and the
restricted isometry property are introduced and it is shown that they ensure robust sparse
recovery. It is actually difficult to show these properties for deterministic matrices and the
optimal number m of measurements, and the major breakthrough in compressive sensing
results is obtained for random matrices. Examples of several types of random matrices that
ensure sparse recovery are given, such as Gaussian, Bernoulli, and partial random Fourier
matrices.

Compressive Sensing 

..
Preliminaries and Notation
This exposition mostly treats complex vectors in CN although sometimes the consid-
erations will be restricted to the real-case RN. The ℓp-norm of a vector x ∈CN is
defined as
∥x∥p := ⎛
⎝
N
∑
j=
∣x j∣p⎞
⎠
/p
,
< p < ∞,
∥x∥∞:= max
j=,⋯,N ∣x j∣.
(.)
For ≤p ≤∞, it is indeed a norm while for < p < it is only a quasi-norm. When
emphasizing the norm the term ℓN
p is used instead of CN or RN. The unit ball in ℓN
p is
BN
p = {x ∈CN,∥x∥p ≤}. The operator norm of a matrix A ∈Cm×N from ℓN
p to ℓm
p is
denoted
∥A∥p→p = max
∥x∥p=∥Ax∥p.
(.)
In the important special case p = , the operator norm is the maximal singular value
σmax(A) of A.
For a subset T ⊂{,⋯, N} we denote by xT ∈CN the vector, which coincides with x ∈
CN on the entries in T and is zero outside T. Similarly, AT denotes the column submatrix
of A corresponding to the columns indexed by T. Further, Tc = {,⋯, N} ∖T denotes the
complement of T and #T or ∣T∣indicate the cardinality of T. The kernel of a matrix A is
denoted by ker A = {x, Ax = }.
..
Sparsity and Compression
Compressive sensing is based on the empirical observation that many types of real-world
signals and images have a sparse expansion in terms of a suitable basis or frame, for instance
a wavelet expansion. This means that the expansion has only a small number of significant
terms, or in other words, that the coefficient vector can be well approximated with one
having only a small number of nonvanishing entries.
The support of a vector x is denoted supp(x) = {j : x j ≠}, and
∥x∥:= ∣supp(x)∣.
It has become common to call ∥⋅∥the ℓ-norm, although it is not even a quasi-norm. A
vector x is called k-sparse if ∥x∥≤k. For k ∈{,,⋯, N},
Σk := {x ∈CN : ∥x∥≤k}


Compressive Sensing
denotes the set of k-sparse vectors. Furthermore, the best k-term approximation error of a
vector x ∈CN in ℓp is defined as
σk(x)p = inf
z∈Σk ∥x −z∥p.
If σk(x) decays quickly in k then x is called compressible. Indeed, in order to compress x
one may simply store only the k largest entries. When reconstructing x from its compressed
version the non-stored entries are simply set to zero, and the reconstruction error is σk (x)p.
It is emphasized at this point that the procedure of obtaining the compressed version of x
is adaptive and nonlinear since it requires the search of the largest entries of x in absolute
value. In particular, the location of the nonzeros is a nonlinear type of information.
The best k-term approximation of x can be obtained using the nonincreasing rearrange-
ment r(x) = (∣xi∣, . . . ,∣xiN ∣)T, where i j denotes a permutation of the indices such that
∣xi j∣≥∣xi j+∣for j = , . . . , N −. Then it is straightforward to check that
σk(x)p := ⎛
⎝
N
∑
j=k+
rj(x)p⎞
⎠
/p
,
< p < ∞.
and the vector x[k] derived from x by setting to zero all the N−k smallest entries in absolute
value is the best k-term approximation,
x[k] = arg min
z∈Σk ∥x −z∥p,
for any < p ≤∞.
The next lemma states essentially that ℓq-balls with small q (ideally q ≤) are good
models for compressible vectors.
Lemma 
Let < q < p ≤∞and set r = 
q −
p. Then
σk(x)p ≤k−r,
k = ,, . . . , N
for all x ∈BN
q .
Proof
Let T be the set of indices of the k-largest entries of x in absolute value. The
nonincreasing rearrangement satisfies ∣rk(x)∣≤∣x j∣for all j ∈T, and therefore
krk(x)q ≤∑
j∈T
∣x j∣q ≤∥x∥q
q ≤.
Hence, rk(x) ≤k−
q . Therefore
σk(x)p
p = ∑
j∉T
∣x j∣p ≤∑
j∉T
rk(x)p−q∣x j∣q ≤k−p−q
q ∥x∥q
q ≤k−p−q
q ,
which implies σk(x)p ≤k−r.
∎

Compressive Sensing 

..
Compressive Sensing
The above outlined adaptive strategy of compressing a signal x by only keeping its largest
coefficients is certainly valid when full information on x is available. If, however, the signal
first has to be acquired or measured by a somewhat costly or lengthy procedure then this
seems to be a waste of resources: At first, large efforts are made to acquire the full signal and
then most of the information is thrown away when compressing it. One may ask whether it
is possible to obtain more directly a compressed version of the signal by taking only a small
amount of linear and nonadaptive measurements. Since one does not know a priori the
large coefficients, this seems a daunting task at first sight. Quite surprisingly, compressive
sensing nevertheless predicts that reconstruction from vastly undersampled nonadaptive
measurements is possible – even by using efficient recovery algorithms.
Taking m linear measurements of a signal x ∈CN corresponds to applying a matrix
A ∈Cm×N – the measurement matrix –
y = Ax.
(.)
The vector y ∈Cm is called the measurement vector. The main interest is in the vastly
undersampled case m
≪
N. Without further information, it is, of course, impos-
sible to recover x from y since the linear system (>.) is highly underdetermined,
and has therefore infinitely many solutions. However, if the additional assumption that
the vector x is k-sparse is imposed, then the situation dramatically changes as will be
outlined.
The approach for a recovery procedure that probably comes first to mind is to search
for the sparsest vector x, which is consistent with the measurement vector y = Ax. This
leads to solving the ℓ-miminization problem
min ∥z∥
subject to Az = y.
(.)
Unfortunately, this combinatorial minimization problem is NP-hard in general [,]. In
other words, an algorithm that solves (>.) for any matrix A and any right-hand side y is
necessarily computationally intractable. Therefore, essentially two practical and tractable
alternatives to (>.) have been proposed in the literature: convex relaxation leading to
ℓ-minimization – also called basis pursuit [] – and greedy algorithms, such as various
matching pursuits [,]. Quite surprisingly for both types of approaches various recovery
results are available, which provide conditions on the matrix A and on the sparsity ∥x∥
such that the recovered solution coincides with the original x, and consequently also with
the solution of (>.). This is no contradiction to the NP-hardness of (>.) since these
results apply only to a subclass of matrices A and right-hand sides y.
The ℓ-minimization approach considers the solution of
min ∥z∥
subject to Az = y,
(.)
which is a convex optimization problem and can be seen as a convex relaxation of (>.).
Various efficient convex optimization techniques apply for its solution []. In the real-
valued case, (>.) is equivalent to a linear program and in the complex-valued case


Compressive Sensing
Az = y
1-ball
0
⊡Fig. -
The ℓ-minimizer within the aﬃne space of solutions of the linear system Az = y coincides
with a sparsest solution
it is equivalent to a second-order cone program (SOCP). Therefore, standard software
applies for its solution – although algorithms that are specialized to (>.) outperform
such standard software, see > Sect. ..
The hope is, of course, that the solution of (>.) coincides with the solution of (>.)
and with the original sparse vector x.
> Figure -provides an intuitive explanation
why ℓ-minimization promotes sparse solutions. Here, N = and m = , so one deals
with a line of solutions F(y) = {z : Az = y} in R. Except for pathological situa-
tions where ker A is parallel to one of the faces of the polytope B
, there is a unique
solution of the ℓ-minimization problem, which has minimal sparsity, that is, only one
nonzero entry.
Recovery results in the next sections make rigorous the intuition that ℓ-minimization
indeed promotes sparsity.
For sparse recovery via greedy algorithms we refer the reader to the literature
[,].
..
The Null Space Property
The null space property is fundamental in the analysis of ℓ-minimization.

Compressive Sensing 

Deﬁnition 
A matrix A ∈Cm×N is said to satisfy the null space property (NSP) of order
k with constant γ ∈(,) if
∥ηT∥≤γ∥ηTc∥,
for all sets T ⊂{, . . ., N}, #T ≤k and for all η ∈ker A.
The following sparse recovery result is based on this notion.
Theorem 
Let A ∈Cm×N be a matrix that satisfies the NSP of order k with constant
γ ∈(,). Let x ∈CN and y = Ax and let x∗be a solution of the ℓ-minimization problem
(>.). Then
∥x −x∗∥≤(+ γ)
−γ
σk(x).
(.)
In particular, if x is k-sparse then x∗= x.
Proof
Let η = x∗−x. Then η ∈ker A and
∥x∗∥≤∥x∥
because x∗is a solution of the ℓ-minimization problem (>.). Let T be the set of the
k-largest entries of x in absolute value. One has
∥x∗
T∥+ ∥x∗
Tc∥≤∥xT∥+ ∥xTc∥.
It follows immediately from the triangle inequality that
∥xT∥−∥ηT∥+ ∥ηTc∥−∥xTc∥≤∥xT∥+ ∥xTc∥.
Hence,
∥ηTc∥≤∥ηT∥+ ∥xTc∥≤γ∥ηTc∥+ σk(x),
or, equivalently,
∥ηTc∥≤

−γ σk(x).
(.)
Finally,
∥x −x∗∥= ∥ηT∥+ ∥ηTc∥≤(γ + )∥ηTc∥≤(+ γ)
−γ
σk(x)
and the proof is completed.
∎
One can also show that if all k-sparse x can be recovered from y = Ax using
ℓ-minimization then necessarily A satisfies the NSP of order k with some constant γ ∈
(,) [,]. Therefore, the NSP is actually equivalent to sparse ℓ-recovery.


Compressive Sensing
..
The Restricted Isometry Property
The NSP is somewhat difficult to show directly. The restricted isometry property (RIP) is
easier to handle and it also implies stability under noise as stated below.
Deﬁnition 
The restricted isometry constant δk of a matrix A ∈Cm×N is the smallest
number such that
(−δk)∥z∥
≤∥Az∥
≤(+ δk)∥z∥
,
(.)
for all z ∈Σk.
A matrix A is said to satisfy the restricted isometry property of order k with constant δk
if δk ∈(,). It is easily seen that δk can be equivalently defined as
δk =
max
T⊂{,⋯,N},#T≤k ∥A∗
TAT −Id∥→,
which means that all column submatrices of A with at most k columns are required to be
well conditioned. The RIP implies the NSP as shown in the following lemma.
Lemma 
Assume that A ∈Cm×N satisfies the RIP of order K = k + h with constant
δK ∈(,). Then A has the NSP of order k with constant γ =
√
k
h
+δK
−δK .
Proof
Let η ∈N = ker A and T ⊂{, . . . , N}, #T ≤k. Define T= T and T, T, . . . , Ts to
be disjoint sets of indices of size at most h, associated to a nonincreasing rearrangement of
the entries of η ∈N, that is,
∣η j∣≤∣ηi∣
for all j ∈Tℓ, i ∈Tℓ′, ℓ≥ℓ′ ≥.
(.)
Note that Aη = implies AηT∪T= −∑s
j=AηTj. Then, from the Cauchy–Schwarz
inequality, the RIP, and the triangle inequality, the following sequence of inequalities is
deduced:
∥ηT∥≤
√
k∥ηT∥≤
√
k∥ηT∪T∥
≤
√
k
−δK
∥AηT∪T∥=
√
k
−δK
∥AηT∪T∪⋅⋅⋅∪Ts∥
≤
√
k
−δK
s
∑
j=
∥AηTj∥≤
√
+ δK
−δK
√
k
s
∑
j=
∥ηTj∥.
(.)
It follows from (>.) that ∣ηi∣≤∣ηℓ∣for all i ∈Tj+and ℓ∈Tj. Taking the sum over ℓ∈Tj
first and then the ℓ-norm over i ∈Tj+yields
∣ηi∣≤h−∥ηTj∥
and
∥ηTj+∥≤h−/∥ηTj∥.

Compressive Sensing 

Using the latter estimates in (>.) gives
∥ηT∥≤
√
+ δK
−δK
k
h
s−
∑
j=
∥ηTj∥≤
√
+ δK
−δK
k
h ∥ηTc∥,
(.)
and the proof is finished.
∎
Taking h = k above shows that δk < /implies γ < . By Theorem , recovery of
all k-sparse vectors by ℓ-minimization is then guaranteed. Additionally, stability in ℓis
also ensured. The next theorem shows that RIP implies also a bound on the reconstruction
error in ℓ.
Theorem 
Assume A ∈Cm×N satisfies the RIP of order k with δk < /. For x ∈CN, let
y = Ax and x∗be the solution of the ℓ-minimization problem (>.). Then
∥x −x∗∥≤C σk(x)
√
k
with C =

−γ ( γ+
√
+ γ), γ =
√
+δk
(−δk).
Proof
Similarly as in the proof of Lemma , denote η = x∗−x ∈N = ker A, T= T the
set of the k-largest entries of η in absolute value, and Tjs of size at most k corresponding
to the nonincreasing rearrangement of η. Then, using (>.) and (>.) with h = k of
the previous proof,
∥ηT∥≤
√
+ δk
(−δk) k−/∥ηTc∥.
From the assumption δk < /it follows that γ :=
√
+δk
(−δk) < . Lemma and Lemma 
yield
∥ηTc∥= σk(η)≤(k)−
∥η∥= (k)−/(∥ηT∥+ ∥ηTc∥)
≤(k)−/(γ∥ηTc∥+ ∥ηTc∥) ≤γ + 
√

k−/∥ηTc∥.
Since T is the set of k-largest entries of η in absolute value, it holds
∥ηTc∥≤∥η(supp x[k])c∥≤∥η(supp x[k])c∥,
(.)
where x[k] is the best k-term approximation to x. The use of this latter estimate, combined
with inequality (>.), finally gives
∥x −x∗∥≤∥ηT∥+ ∥ηTc∥
≤(γ + 
√

+ γ) k−/∥ηTc∥
≤

−γ (γ + 
√

+ γ) k−/σk(x).
This concludes the proof.
∎


Compressive Sensing
The restricted isometry property implies also robustness under noise on the measure-
ments. This fact was first noted in [,]. We present the so far best known result [,]
concerning recovery using a noise aware variant of ℓ-minimization without proof.
Theorem 
Assume that the restricted isometry constant δk of the matrix A ∈Cm×N
satisfies
δk <

+
√
/
≈..
(.)
Then the following holds for all x ∈CN. Let noisy measurements y = Ax + e be given with
∥e∥≤η. Let x∗be the solution of
min ∥z∥
subject to ∥Az −y∥≤η.
(.)
Then
∥x −x∗∥≤Cη + C
σk(x)
√
k
for some constants C, C> that depend only on δk.
..
Coherence
The coherence is a by-now classical way of analyzing the recovery abilities of a measure-
ment matrix [, ]. For a matrix A = (a∣a∣⋯∣aN) ∈Cm×N with normalized columns,
∥aℓ∥= , it is defined as
μ := max
ℓ≠k ∣⟨aℓ, ak⟩∣.
Applying Gershgorin’s disc theorem [] to A∗
TAT −I with #T = k shows that
δk ≤(k −)μ.
(.)
Several explicit examples of matrices are known, which have small coherence μ
=
O(/√m). A simple one is the concatenation A = (I∣F) ∈Cm×m of the identity matrix and
the unitary Fourier matrix F ∈Cm×m with entries Fj,k = m−/eπi jk/m. It is easily seen that
μ = /√m in this case. Furthermore, [] gives several matrices A ∈Cm×mwith coherence
μ = /√m. In all these cases, δk ≤C
k
√m . Combining this estimate with the recovery results
for ℓ-minimization above shows that all k-sparse vectors x can be (stably) recovered from
y = Ax via ℓ-minimization provided
m ≥C′k.
(.)
At first sight one might be satisfied with this condition since if k is very small com-
pared to N then still m might be chosen smaller than N and all k-sparse vectors can
be recovered from the undersampled measurements y = Ax. Although this is great

Compressive Sensing 

news for a start, one might nevertheless hope that (>.) can be improved. In par-
ticular, one may expect that actually a linear scaling of m in k should be enough
to guarantee sparse recovery by ℓ-minimization. The existence of matrices, which
indeed provide recovery conditions of the form m ≥Ck logα(N) (or similar) with
some α
≥
, is shown in
> Sect. ... Unfortunately, such results cannot be
shown using simply the coherence because of the general lower bound []
μ ≥
√
N −m
m(N −) ∼

√m
(N sufficiently large).
In particular, it is not possible to overcome the “quadratic bottleneck” in (>.) by using
Gershgorin’s theorem or Riesz–Thorin interpolation between ∥⋅∥→and ∥⋅∥∞→∞, see also
[,]. In order to improve on (>.) one has to take into account also cancellations in the
Gramian A∗
TAT −I, and this task seems to be quite difficult using deterministic methods.
Therefore, it will not come as a surprise that the major breakthrough in compressive sensing
was obtained with random matrices. It is indeed easier to deal with cancellations in the
Gramian using probabilistic techniques.
..
RIP for Gaussian and Bernoulli Random Matrices
Optimal estimates for the RIP constants in terms of the number m of measurement
matrices can be obtained for Gaussian, Bernoulli, or more general sub-Gaussian random
matrices.
Let X be a random variable. Then one defines a random matrix A = A(ω), ω ∈Ω,
as the matrix whose entries are independent realizations of X, where (Ω, Σ,P) is their
common probability space. One assumes further that for any x ∈RN we have the identity
E∥Ax∥
= ∥x∥
, E denoting expectation.
The starting point for the simple approach in [] is a concentration inequality of the
form
P(∣∥Ax∥
−∥x∥
∣≥δ∥x∥
) ≤e−cδm,
< δ < ,
(.)
where c> is some constant.
The two most relevant examples of random matrices, which satisfy the above concen-
tration are the following:
. Gaussian matrices: Here the entries of A are chosen as independent and identically
distributed. Gaussian random variables with expectation and variance /m. As shown
in [] Gaussian matrices satisfy (>.).
. Bernoulli matrices: The entries of a Bernoulli matrices are independent realizations
of ±/√m Bernoulli random variables, that is, each entry takes the value +/√m
or −/√m with equal probability. Bernoulli matrices also satisfy the concentration
inequality (>.) [].


Compressive Sensing
Based on the concentration inequality (>.) the following estimate on RIP constants
can be shown [,,].
Theorem 
Assume A ∈Rm×N to be a random matrix satisfying the concentration prop-
erty (>.). Then there exists a constant C depending only on csuch that the restricted
isometry constant of A satisfies δk ≤δ with probability exceeding −ε provided
m ≥Cδ−(k log(N/m) + log(ε−)).
Combining this RIP estimate with the recovery results for ℓ-minimization shows that
all k-sparse vectors x ∈CN can be stably recovered from a random draw of A satisfying
(>.) with high probability provided
m ≥Ck log(N/m).
(.)
Up to the log-factor this provides the desired linear scaling of the number m of measure-
ments with respect to the sparsity k. Furthermore, as shown in
> Sect. .., condition
(>.) cannot be further improved; in particular, the log-factor cannot be removed.
It is useful to observe that the concentration inequality is invariant under unitary trans-
forms. Indeed, suppose that z is not sparse with respect to the canonical basis but with
respect to a different orthonormal basis. Then z = Ux for a sparse x and a unitary matrix
U ∈CN×N. Applying the measurement matrix A yields
Az = AUx,
so that this situation is equivalent to working with the new measurement matrix A′ = AU
and again sparsity with respect to the canonical basis. The crucial point is that A′ satisfies
again the concentration inequality (>.) once A does. Indeed, choosing x = U−x′ and
using unitarity gives
P(∣∥AUx∥
−∥x∥
∣≥δ∥x∥
ℓN
) = P(∣∥Ax′∥
−∥U−x′∥
∣≥δ∥U−x′∥
ℓN
)
= P(∣∥Ax′∥
−∥x′∥
∣≥δ∥x′∥
ℓN
) ≤e−cδ−m.
Hence, Theorem also applies to A′ = AU. This fact is sometimes referred to as the uni-
versality of the Gaussian or Bernoulli random matrices. It does not matter in which basis
the signal x is actually sparse. At the coding stage, where one takes random measurements
y = Az, knowledge of this basis is not even required. Only the decoding procedure needs
to know U.
..
Random Partial Fourier Matrices
While Gaussian and Bernoulli matrices provide optimal conditions for the minimal num-
ber of required samples for sparse recovery, they are of somewhat limited use for practical
applications for several reasons. Often the application imposes physical or other constraints

Compressive Sensing 

on the measurement matrix, so that assuming A to be Gaussian may not be justifiable in
practice. One usually has only limited freedom to inject randomness in the measurements.
Furthermore, Gaussian or Bernoulli matrices are not structured so there is no fast matrix–
vector multiplication available, which may speed up recovery algorithms, such as the ones
described in > Sect. .. Thus, Gaussian random matrices are not applicable in large-scale
problems.
A very important class of structured random matrices that overcomes these drawbacks
are random partial Fourier matrices, which were also the object of study in the very first
papers on compressive sensing [,,,]. A random partial Fourier matrix A ∈Cm×N
is derived from the discrete Fourier matrix F ∈CN×N with entries
Fj,k =

√
N
eπ jk/N,
by selecting m rows uniformly at random among all N rows. Taking measurements of a
sparse x ∈CN corresponds then to observing m of the entries of its discrete Fourier trans-
form ˆx = Fx. It is important to note that the fast Fourier transform may be used to compute
matrix–vector multiplications with A and A∗with complexity O(N log(N)). The follow-
ing theorem concerning the RIP constant was proven in [], and improves slightly on the
results in [,,].
Theorem 
Let A ∈Cm×N be the random partial Fourier matrix as just described. Then
the restricted isometry constant of the rescaled matrix
√
N
m A satisfy δk ≤δ with probability
at least −N−γ log(N) provided
m ≥Cδ−k log(N).
(.)
The constants C,γ > are universal.
Combining this estimate with the ℓ-minimization results above shows that recovery
with high probability can be ensured for all k-sparse x provided
m ≥Ck log(N).
The plots in
> Fig. -illustrate an example of successful recovery from partial Fourier
measurements.
The proof of the above theorem is not straightforward and involves Dudley’s inequality
as a main tool [,]. Compared to the recovery condition (>.) for Gaussian matrices,
we suffer a higher exponent at the log-factor, but the linear scaling of m in k is preserved.
Also a nonuniform recovery result for ℓ-minimization is available [,,], which states
that each k-sparse x can be recovered using a random draw of the random partial Fourier
matrix A with probability at least −ε provided m ≥Ck log(N/ε). The difference to the
statement in Theorem is that, for each sparse x, recovery is ensured with high probability
for a new random draw of A. It does not imply the existence of a matrix, which allows
recovery of all k-sparse x simultaneously. The proof of such recovery results do not make
use of the restricted isometry property or the null space property.


Compressive Sensing
One may generalize the above results to a much broader class of structured ran-
dom matrices, which arise from random sampling in bounded orthonormal systems. The
interested reader is referred to [,,].
Another class of structured random matrices, for which recovery results are known,
consists of partial random circulant and Toeplitz matrices. These correspond to subsam-
pling the convolution of x with a random vector b at m fixed (deterministic) entries. The
reader is referred to [,] for detailed information. It is only noted that a good estimate
for the RIP constants for such types of random matrices is still an open problem. Further
types of random measurement matrices are discussed in [,].
..
Compressive Sensing and Gelfand Widths
In this section a quite general viewpoint is taken. The question is investigated how well
any measurement matrix and any reconstruction method – in this context usually called
the decoder – may perform. This leads to the study of Gelfand widths, already mentioned
in
> Sect. ... The corresponding analysis allows to draw the conclusion that Gaus-
sian random matrices in connection with ℓ-minimization provide optimal performance
guarantees.
Following the tradition of the literature in this context, only the real-valued case will
be treated. The complex-valued case is easily deduced from the real case by identifying CN
with RN and by corresponding norm equivalences of ℓp-norms.
The measurement matrix A ∈Rm×N is here also referred to as the encoder. The set Am,N
denotes all possible encoder/decoder pairs (A, Δ) where A ∈Rm×N and Δ : Rm →RN is
any (nonlinear) function. Then, for ≤k ≤N, the reconstruction errors over subsets
K ⊂RN, where RN is endowed with a norm ∥⋅∥X, are defined as
σk(K)X := sup
x∈K
σk(x)X,
Em(K, X) :=
inf
(A,Δ)∈Am,N sup
x∈K
∥x −Δ(Ax)∥X.
In words, Em(K, X) is the worst reconstruction error for the best pair of encoder/decoder.
The goal is to find the largest k such that
Em(K, X) ≤Cσk(K)X.
Of particular interest for compressive sensing are the unit balls K = BN
p for < p ≤and
X = ℓN
because the elements of BN
p are well approximated by sparse vectors due to Lemma
. The proper estimate of Em(K, X) turns out to be linked to the geometrical concept of
Gelfand width.

Compressive Sensing 

Deﬁnition 
Let K be a compact set in a normed space X. Then the Gelfand width of K of
order m is
dm(K, X) :=
inf
Y ⊂X
codim(Y) ≤m
sup{∥x∥X : x ∈K ∩Y},
where the infimum is over all linear subspaces Y of X of codimension less or equal to m.
The following fundamental relationship between Em(K, X) and the Gelfand widths
holds.
Proposition 
Let K ⊂RN be a closed compact set such that K = −K and K + K ⊂CK
for some constant C. Let X = (RN,∥⋅∥X) be a normed space. Then
dm(K, X) ≤Em(K, X) ≤Cdm(K, X).
Proof
For a matrix A ∈Rm×N, the subspace Y = ker A has codimension less or equal
to m. Conversely, to any subspace Y ⊂RN of codimension less or equal to m, a matrix
A ∈Rm×N can be associated, the rows of which form a basis for Y⊥. This identification
yields
dm(K, X) =
inf
A∈Rm×N sup{∥η∥X : η ∈ker A ∩K}.
Let (A, Δ) be an encoder/decoder pair in Am,N and z = Δ(). Denote Y = ker(A). Then
with η ∈Y also −η ∈Y, and either ∥η −z∥X ≥∥η∥X or ∥−η −z∥X ≥∥η∥X. Indeed, if both
inequalities were false then
∥η∥X = ∥η −z + z + η∥X ≤∥η −z∥X + ∥−η −z∥X < ∥η∥X,
a contradiction. Since K = −K it follows that
dm(K, X) =
inf
A∈Rm×N sup{∥η∥X : η ∈Y ∩K} ≤sup
η∈Y∩K
∥η −z∥X
= sup
η∈Y∩K
∥η −Δ(Aη)∥X ≤sup
x∈K
∥x −Δ(Ax)∥X.
Taking the infimum over all (A, Δ) ∈Am,N yields
dm(K, X) ≤Em(K, X).
To prove the converse inequality, choose an optimal Y such that
dm(K, X) = sup{∥x∥X : x ∈Y ∩K}.
(An optimal subspace Y always exists [].) Let A be a matrix whose rows form a basis for
Y⊥. Denote the affine solution space F(y) := {x : Ax = y}. One defines then a decoder


Compressive Sensing
as follows. If F(y) ∩K ≠∅then choose some ¯x(y) ∈F(y) and set Δ(y) = ¯x(y). If
F(y) ∩K = ∅then Δ(y) ∈F(y). The following chain of inequalities is then deduced:
Em(K, X) ≤sup
y
sup
x,x′∈F(y)∩K
∥x −x′∥X
≤
sup
η∈C(Y∩K)
∥η∥X ≤Cdm(K, X),
which concludes the proof.
∎
The assumption K + K ⊂CK clearly holds for norm balls with C= and for quasi-
norm balls with some C≥. The next theorem provides a two-sided estimate of the
Gelfand widths dm (BN
p , ℓN
) [, , ]. Note that the case p = was considered much
earlier in [,,].
Theorem 
Let < p ≤. There exist universal constants Cp, Dp > such that the Gelfand
widths dm (BN
p , ℓN
) satisfy
Cp min {, ln(N/m)
m
}
/p−/
≤dm (BN
p , ℓN
)
≤Dp min {, ln(N/m)
m
}
/p−/
(.)
Combining Proposition and Theorem gives in particular, for large m,
˜C
√
log(N/m)
m
≤Em (BN
, ℓN
) ≤˜D
√
log(N/m)
m
.
(.)
This estimate implies a lower estimate for the minimal number of required samples, which
allows for approximate sparse recovery using any measurement matrix and any recovery
method whatsoever. The reader should compare the next statement with Theorem .
Corollary 
Suppose that A ∈Rm×N and Δ : Rm →RN such that
∥x −Δ(Ax)∥≤C σk(x)
√
k
for all x ∈BN
and some constant C > . Then necessarily
m ≥C′k log(N/m).
(.)
Proof
Since σk(x)≤∥x∥≤, the assumption implies Em (BN
, ℓN
) ≤Ck−/. The lower
bound in (>.) combined with Proposition yields
˜C
√
log(N/m)
m
≤Em (BN
, ℓN
) ≤Ck−/.
Consequently, m ≥C′k log(eN/m) as claimed.
∎

Compressive Sensing 

In particular, the above lemma applies to ℓ-minimization and consequently δk ≤.
(say) for a matrix A ∈Rm×N implies m ≥Ck log(N/m). Therefore, the recovery results for
Gaussian or Bernoulli random matrices with ℓ-minimization stated above are optimal.
It can also be shown that a stability estimate in the ℓ-norm of the form ∥x −Δ(Ax)∥≤
Cσk(x)for all x ∈RN implies (>.) as well [,].
..
Applications
Compressive sensing can be potentially used in all applications where the task is the recon-
struction of a signal or an image from linear measurements, while taking many of those
measurements – in particular, a complete set of measurements – is a costly, lengthy, diffi-
cult, dangerous, impossible, or otherwise undesired procedure. Additionally, there should
be reasons to believe that the signal is sparse in a suitable basis (or frame). Empirically, the
latter applies to most types of signals.
In computerized tomography, for instance, one would like to obtain an image of the
inside of a human body by taking X-ray images from different angles. Taking an almost
complete set of images would expose the patient to a large and dangerous dose of radi-
ation, so the amount of measurements should be as small as possible, and nevertheless
guarantee a good enough image quality. Such images are usually nearly piecewise constant
and therefore nearly sparse in the gradient, so there is a good reason to believe that com-
pressive sensing is well applicable. And indeed, it is precisely this application that started
the investigations on compressive sensing in the seminal paper [].
Also radar imaging seems to be a very promising application of compressive sensing
techniques [,]. One is usually monitoring only a small number of targets, so that spar-
sity is a very realistic assumption. Standard methods for radar imaging actually also use
the sparsity assumption, but only at the very end of the signal processing procedure in
order to clean up the noise in the resulting image. Using sparsity systematically from the
very beginning by exploiting compressive sensing methods is therefore a natural approach.
First numerical experiments in [,] are very promising.
Further potential applications include wireless communication [], astronomical sig-
nal and image processing [], analog-to-digital conversion [], camera design [], and
imaging [].
.
Numerical Methods
The previous sections showed that ℓ-minimization performs very well in recovering sparse
or approximately sparse vectors from undersampled measurements. In applications it is


Compressive Sensing
important to have fast methods for actually solving ℓ-minimization problems. Two such
methods – the homotopy method introduced in [,] and iteratively reweighted least
squares (IRLS) [] – will be explained in more detail below.
As a first remark, the ℓ-minimization problem
min∥x∥
subject to Ax = y
(.)
is in the real case equivalent to the linear program
min
N
∑
j=
v j
subject to
v ≥, (A∣−A)v = y.
(.)
The solution x∗to (>.) is obtained from the solution v∗of (>.) via x∗= (Id ∣−Id)v∗.
Any linear programming method may therefore be used for solving (>.). The simplex
method as well as interior point methods applies in particular [], and standard soft-
ware may be used. (In the complex case, (>.) is equivalent to a second-order cone
program (SOCP) and can also be solved with interior point methods.) However, such
methods and software are of general purpose and one may expect that methods special-
ized to (>.) outperform such existing standard methods. Moreover, standard software
often has the drawback that one has to provide the full matrix rather than fast routines for
matrix–vector multiplication, which are available for instance in the case of partial Fourier
matrices. In order to obtain the full performance of such methods one would therefore
need to reimplement them, which is a daunting task because interior point methods usually
require much fine-tuning. On the contrary, the two specialized methods described below
are rather simple to implement and very efficient. Many more methods are available nowa-
days, including greedy methods, such as orthogonal matching pursuit [], CoSaMP [],
and iterative hard thresholding [, ], which may offer better complexity than standard
interior point methods. Due to space limitations, however, only the two methods below
are explained in detail.
..
The Homotopy Method
The homotopy method – or modified LARS – [, , , ] solves (>.) in the real-
valued case. One considers the ℓ-regularized least squares functionals
Fλ(x) = 
∥Ax −y∥
+ λ∥x∥,
x ∈RN, λ > ,
(.)
and its minimizer xλ. When λ = ˆλ is large enough then xˆλ = , and furthermore,
limλ→xλ = x∗, where x∗is the solution to (>.). The idea of the homotopy method
is to trace the solution xλ from xˆλ = to x∗. The crucial observation is that the solution
path λ ↦xλ is piecewise linear, and it is enough to trace the endpoints of the linear pieces.
The minimizer of (>.) can be characterized using the subdifferential, which is
defined for a general convex function F : RN →R at a point x ∈RN by
∂F(x) = {v ∈RN, F(y) −F(x) ≥⟨v, y −x⟩for all y ∈RN}.

Compressive Sensing 

Clearly, x is a minimizer of F if and only if ∈∂F(x). The subdifferential of Fλ is given by
∂Fλ(x) = A∗(Ax −y) + λ∂∥x∥
where the subdifferential of the ℓ-norm is given by
∂∥x∥= {v ∈RN : vℓ∈∂∣xℓ∣, ℓ= ,⋯, N}
with the subdifferential of the absolute value being
∂∣z∣= { {sgn(z)}
if z ≠,
[−,]
if z = .
The inclusion ∈∂Fλ(x) is equivalent to
(A∗(Ax −y))ℓ= λ sgn(xℓ)
if xℓ≠,
(.)
∣(A∗(Ax −y)ℓ∣≤λ
if xℓ= ,
(.)
for all ℓ= ,⋯, N.
As already mentioned above the homotopy method starts with x() = xλ = . By condi-
tions (>.) and (>.) the corresponding λ can be chosen as λ = λ() = ∥A∗y∥∞. In the
further steps j = ,,⋯, the algorithm computes minimizers x(), x(),⋯, and maintains
an active (support) set Tj. Denote by
c(j) = A∗(Ax(j−) −y)
the current residual vector.
Step : Let
ℓ() := arg max
ℓ=,⋯,N ∣(A∗y)ℓ∣= arg max
ℓ=,⋯,N ∣c()
ℓ∣.
One assumes here and also in the further steps that the maximum is attained at only one
index ℓ. The case that the maximum is attained simultaneously at two or more indices ℓ
(which almost never happens) requires more complications that will not be covered here.
The reader is referred to [] for such details.
Now set T= {ℓ()}. The vector d ∈RN describing the direction of the solution
(homotopy) path has components
d()
ℓ() = ∥aℓ()∥−
sgn((Ay)ℓ())
and
d()
ℓ
= ,
ℓ≠ℓ().
The first linear piece of the solution path then takes the form
x = x(γ) = x() + γd() = γd(),
γ ∈[,γ()].
One verifies with the definition of d() that (>.) is always satisfied for x = x(γ) and
λ = λ(γ) = λ()−γ, γ ∈[, λ()]. The next breakpoint is found by determining the maximal
γ = γ() > for which (>.) is still satisfied, which is
γ() = min
ℓ≠ℓ()
⎧⎪⎪⎨⎪⎪⎩
λ() −c()
ℓ
−(A∗Ad())ℓ
,
λ() + c()
ℓ
+ (A∗Ad())ℓ
⎫⎪⎪⎬⎪⎪⎭
.
(.)


Compressive Sensing
Here, the minimum is taken only over positive arguments. Then x() = x(γ()) = γ()d()
is the next minimizer of Fλ for λ = λ() := λ() −γ(). This λ() satisfies λ() = ∥c()∥∞. Let
ℓ() be the index where the minimum in (>.) is attained (where we again assume that
the minimum is attained only at one index) and put T= {ℓ(), ℓ()}.
Step j: Determine the new direction d(j) of the homotopy path by solving
A∗
TjATjd(j)
Tj = sgn (c(j)
Tj ),
(.)
which is a linear system of equations of size ∣Tj∣× ∣Tj∣, ∣Tj∣≤j. Outside the components in
Tj one sets d(j)
ℓ
= , ℓ∉Tj. The next piece of the path is then given by
x(γ) = x(j−) + γd(j),
γ ∈[,γ(j)].
The maximal γ such that x(γ) satisfies (>.) is
γ(j)
+
= min
ℓ∉Tj
⎧⎪⎪⎨⎪⎪⎩
λ(j−) −c(j)
ℓ
−(A∗Ad(j))ℓ
,
λ(j−) + c(j)
ℓ
+ (A∗Ad(j))ℓ
⎫⎪⎪⎬⎪⎪⎭
.
(.)
The maximal γ such that x(γ) satisfies (>.) is determined as
γ(j)
−
= min
ℓ∈Tj {−x(j−)
ℓ
/d(j)
ℓ}.
(.)
In both (>.) and (>.) the minimum is taken only over positive arguments. The next
breakpoint is given by x(j+) = x(γ(j)) with γ(j) = min {γ(j)
+ ,γ(j)
−}. If γ(j)
+
determines
the minimum then the index ℓ(j)
+
∉Tj providing the minimum in (>.) is added to the
active set, Tj+= Tj ∪{ℓ(j)
+ }. If γ(j) = γ(j)
−
then the index ℓ(j)
−
∈Tj is removed from the
active set, Tj+= Tj ∖{ℓ(j)
−}. Further, one updates λ(j) = λ(j−) −γ(j). By construction
λ(j) = ∥c(j)∥∞.
The algorithm stops when λ(j) = ∥c(j)∥∞= , that is, when the residual vanishes, and
outputs x∗= x(j). Indeed, this happens after a finite number of steps. In [] the following
result was shown.
Theorem 
If in each step the minimum in (>.) and (>.) is attained in only
one index ℓ, then the homotopy algorithm as described yields the minimizer of the ℓ-
minimization problem (>.).
If the algorithm is stopped earlier at some iteration j then obviously it yields the min-
imizer of Fλ = Fλ(j). In particular, obvious stopping rules may also be used to solve the
problems
min ∥x∥
subject to ∥Ax −y∥≤є
(.)
or
min ∥Ax −y∥
subject to ∥x∥≤δ.
(.)

Compressive Sensing 

The first of these appears in (>.), and the second is called the LASSO (least absolute
shrinkage and selection operator) [].
The least angle regression (LARS) algorithm is a simple modification of the homo-
topy method, which only adds elements to the active set in each step. So γ(j)
−
in
(>.) is not considered. (Sometimes the homotopy method is therefore also called
modified LARS.) Clearly, LARS is not guaranteed any more to yield the solution of
(>.). However, it is observed empirically – and can be proven rigorously in cer-
tain cases [] – that often in sparse recovery problems, the homotopy method does
never remove elements from the active set, so that in this case LARS and homo-
topy perform the same steps. It is a crucial point that if the solution of (>.) is
k-sparse and the homotopy method never removes elements then the solution is obtained
after precisely k-steps. Furthermore, the most demanding computational part at step j
is then the solution of the j × j linear system of equations (>.). In conclusion, the
homotopy and LARS methods are very efficient for sparse recovery problems.
..
Iteratively Reweighted Least Squares
This section is concerned with an iterative algorithm which, under the condition that A
satisfies the NSP (see Definition ), is guaranteed to reconstruct vectors with the same
error estimate (>.) as ℓ-minimization. Again we restrict the following discussion to
the real case. This algorithm has a guaranteed linear rate of convergence, which can even
be improved to a superlinear rate with a small modification. First a brief introduction aims
at shedding light on the basic principles of this algorithm and their interplay with sparse
recovery and ℓ-minimization.
Denote F(y) = {x : Ax = y} and N = ker A. The starting point is the trivial obser-
vation that ∣t∣= t
∣t∣for t ≠. Hence, an ℓ-minimization can be recasted into a weighted
ℓ-minimization, with the hope that
arg min
x∈F(y)
N
∑
j=
∣x j∣≈arg min
x∈F(y)
N
∑
j=
x
j ∣x∗
j ∣
−,
as soon as x∗is the desired ℓ-norm minimizer. The advantage of the reformulation con-
sists in the fact that minimizing the smooth quadratic function tis an easier task than
the minimization of the nonsmooth function ∣t∣. However, the obvious drawbacks are that
neither one disposes of x∗a priori (this is the vector one is interested to compute!) nor one
can expect that x∗
j ≠for all j = , . . . , N, since one hopes for k-sparse solutions.
Suppose one has a good approximation wn
j of ∣(x∗
j )+є
n∣−/≈∣x∗
j ∣−, for some єn > .
One computes
xn+= arg min
x∈F(y)
N
∑
j=
x
j wn
j ,
(.)


Compressive Sensing
and then updates єn+≤єn by some rule to be specified later. Further, one sets
wn+
j
= ∣(xn+
j
)
+ є
n+∣
−/
,
(.)
and iterates the process. The hope is that a proper choice of єn →allows the iterative com-
putation of an ℓ-minimizer. The next sections investigate convergence of this algorithm
and properties of the limit.
...
Weighted ℓ-Minimization
Suppose that the weight w is strictly positive, which means that wj > for all j ∈{, . . ., N}.
Then ℓ(w) is a Hilbert space with the inner product
⟨u,v⟩w :=
N
∑
j=
wju jv j.
(.)
Define
xw := arg min
z∈F(y)∥z∥,w,
(.)
where ∥z∥,w = ⟨z, z⟩/
w . Because the ∥⋅∥,w-norm is strictly convex, the minimizer xw is
necessarily unique; it is characterized by the orthogonality conditions
⟨xw, η⟩w = ,
for all η ∈N.
(.)
...
An Iteratively Reweighted Least Squares Algorithm (IRLS)
An IRLS algorithm appears for the first time in the Ph.D. thesis of Lawson in [],
in the form of an algorithm for solving uniform approximation problems. This iterative
algorithm is now well known in classical approximation theory as Lawson’s algorithm. In
[] it is proved that it obeys a linear convergence rate. In the s, extensions of Lawson’s
algorithm for ℓp-minimization, and in particular ℓ-minimization, were introduced. In
signal analysis, IRLS was proposed as a technique to build algorithms for sparse signal
reconstruction in []. The interplay of the NSP, ℓ-minimization, and a reweighted least
square algorithm has been clarified only recently in the work [].
The analysis of the algorithm (>.) and (>.) starts from the observation that
∣t∣= min
w>

(wt+ w−),
the minimum being attained for w =

∣t∣. Inspired by this simple relationship, given a real
number є > and a weight vector w ∈RN, with wj > , j = , . . . , N, one introduces the
functional
J (z,w,є) := 

N
∑
j=
(z
jwj + єwj + w−
j ),
z ∈RN.
(.)

Compressive Sensing 

The algorithm roughly described in (>.) and (>.) can be recast as an alternat-
ing method for choosing minimizers and weights based on the functional J . To describe
this more rigorously, recall that r(z) denotes the nonincreasing rearrangement of a vector
z ∈RN.
Algorithm IRLS.
Initialize by taking w:= (, . . . ,). Set є:= . Then recursively deﬁne, for n = ,, . . . ,
xn+:= arg min
z∈F(y) J (z,wn,єn) = arg min
z∈F(y)∥z∥,wn
(.)
and
єn+:= min {єn, rK+(xn+)
N
},
(.)
where K is a ﬁxed integer that will be speciﬁed later. Set
wn+:= arg min
w>J (xn+,w,єn+).
(.)
The algorithm stops if єn = ; in this case, deﬁne xj := xn for j > n. In general, the algorithm
generates an inﬁnite sequence (xn)n∈N of vectors.
Each step of the algorithm requires the solution of a weighted least squares problem.
In matrix form
xn+= D−
n A∗(AD−
n A∗)
−y,
(.)
where Dn is the N × N diagonal matrix the jth diagonal entry of which is wn
j . Once xn+
is found, the weight wn+is given by
wn+
j
= [(xn+
j
)
+ є
n+]
−/
,
j = , . . . , N.
(.)
...
Convergence Properties
Lemma 
Set L := J (x,w,є). Then
∥xn −xn+∥
≤L [J (xn,wn,єn) −J (xn+,wn+,єn+)].
Hence (J(xn,wn,єn))n∈N is a monotonically decreasing sequence and
lim
n→∞∥xn −xn+∥
= .
Proof
Note that J (xn,wn,єn) ≥J (xn+,wn+,єn+) for each n = ,, . . ., and
L = J (x,w,є) ≥J (xn,wn,єn) ≥(wn
j )
−,
j = , . . . , N.


Compressive Sensing
Hence, for each n = ,, . . . , the following estimates hold:
[J (xn,wn,єn) −J (xn+,wn+,єn+)]
≥[J (xn,wn,єn) −J (xn+,wn,єn)] = ⟨xn, xn⟩wn −⟨xn+, xn+⟩wn
= ⟨xn + xn+, xn −xn+⟩wn = ⟨xn −xn+, xn −xn+⟩wn
=
N
∑
j=
wn
j (xn
j −xn+
j
)
≥L−∥xn −xn+∥
,
In the third line it is used that ⟨xn+, xn −xn+⟩wn = due to (>.) since xn −xn+is
contained in N.
∎
Moreover, if one assumes that xn →¯x and єn →, then, formally,
J (xn,wn,єn) →∥¯x∥.
Hence, one expects that this algorithm performs similar to ℓ-minimization. Indeed, the
following convergence result holds.
Theorem 
Suppose A ∈Rm×N satisfies the NSP of order K with constant γ < . Use K in
the update rule (>.). Then, for each y ∈Rm, the sequence xn produced by the algorithm
converges to a vector ¯x, with rK+(¯x) = N limn→∞єn and the following holds:
. If є = limn→∞єn = , then ¯x is K-sparse; in this case there is therefore a unique ℓ-
minimizer x∗, and ¯x = x∗; moreover, we have, for k ≤K, and any z ∈F(y),
∥z −¯x∥≤(+ γ)
−γ
σk(z).
(.)
. If є = limn→∞єn > , then ¯x = xє := arg minz∈F(y) ∑N
j=(z
j + є)
/
.
. In this last case, if γ satisfies the stricter bound γ < −

K+(or, equivalently, if γ
−γ < K),
then we have, for all z ∈F(y) and any k < K −γ
−γ , that
∥z −¯x∥≤˜cσk(z),
with ˜c := (+ γ)
−γ
⎡⎢⎢⎢⎢⎣
K −k + 

K −k −γ
−γ
⎤⎥⎥⎥⎥⎦
(.)
As a consequence, this case is excluded if F(y) contains a vector of sparsity k < K −γ
−γ .
Note that the approximation properties (>.) and (>.) are exactly of the same
order as the one (>.) provided by ℓ-minimization. However, in general, ¯x is not
necessarily an ℓ-minimizer, unless it coincides with a sparse solution.
The proof of this result is not included and the interested reader is referred to [,]
for the details.

Compressive Sensing 

...
Local Linear Rate of Convergence
It is instructive to show a further result concerning the local rate of convergence of
this algorithm, which again uses the NSP as well as the optimality conditions we intro-
duced above. One assumes here that F(y) contains the k-sparse vector x∗. The algorithm
produces a sequence xn, which converges to x∗, as established above. One denotes the
(unknown) support of the k-sparse vector x∗by T.
For now, one introduces an auxiliary sequence of error vectors ηn ∈N via ηn := xn −
x∗and
En := ∥ηn∥= ∥x∗−xn∥.
Theorem guarantees that En →for n →∞. A useful technical result is reported next.
Lemma 
For any z, z′ ∈RN, and for any j,
∣σj(z)−σj(z′)∣≤∥z −z′∥,
(.)
while for any J > j,
(J −j)rJ(z) ≤∥z −z′∥+ σj(z′).
(.)
Proof
To prove (>.), approximate z by a best j-term approximation z′
[j] ∈Σ j of z′ in
ℓ.
Then
σj(z)≤∥z −z′
[j]∥
≤∥z −z′∥+ σj(z′),
and the result follows from symmetry. To prove (>.), it suffices to note that (J −
j) rJ(z) ≤σj(z).
∎
The following theorem gives a bound on the rate of convergence of En to zero.
Theorem 
Assume A satisfies the NSP of order K with constant γ. Suppose that k <
K −γ
−γ , < ρ < , and < γ < −

K+are such that
μ := γ(+ γ)
−ρ
(+

K + −k) < .
Assume that F(y) contains a k-sparse vector x∗and let T = supp(x∗). Let nbe such that
En≤R∗:= ρ min
i∈T ∣x∗
i ∣.
(.)
Then, for all n ≥n, we have
En+≤μ En.
(.)
Consequently, xn converges to x∗exponentially.


Compressive Sensing
Proof
The relation (>.) with w = wn, xw = xn+= x∗+ηn+, and η = xn+−x∗= ηn+,
gives
N
∑
i=
(x∗
i + ηn+
i
) ηn+
i
wn
i = .
Rearranging the terms and using the fact that x∗is supported on T, one obtains
N
∑
i=
∣ηn+
i
∣
wn
i = −∑
i∈T
x∗
i ηn+
i
wn
i = −∑
i∈T
x∗
i
[(xn
i )
+ єn]
/ηn+
i
.
(.)
The proof of the theorem is by induction. Assume that En ≤R∗has already been
established. Then, for all i ∈T,
∣ηn
i ∣≤∥ηn∥= En ≤ρ ∣x∗
i ∣,
so that
∣x∗
i ∣
[(xn
i )
+ єn]
/≤∣x∗
i ∣
∣xn
i ∣=
∣x∗
i ∣
∣x∗
i + ηn
i ∣≤

−ρ,
(.)
and hence (>.) combined with (>.) and the NSP gives
N
∑
i=
∣ηn+
i
∣
wn
i ≤

−ρ ∥ηn+
T ∥≤
γ
−ρ ∥ηn+
Tc ∥
The Cauchy–Schwarz inequality combined with the above estimate yields
∥ηn+
Tc ∥

≤( ∑
i∈Tc ∣ηn+
i
∣
wn
i )( ∑
i∈Tc [(xn
i )+ є
n]
/
)
= (
N
∑
i=
∣ηn+
i
∣
wn
i )( ∑
i∈Tc [(ηn
i )+ є
n]
/
)
≤
γ
−ρ ∥ηn+
Tc ∥(∥ηn∥+ Nєn).
(.)
If ηn+
Tc = , then xn+
Tc
= . In this case xn+is k-sparse and the algorithm has stopped by
definition; since xn+−x∗is in the null space N, which contains no k-sparse elements
other than , one has already obtained the solution xn+= x∗. If ηn+
Tc ≠, then cancelling
the factor ∥ηn+
Tc ∥in (>.) yields
∥ηn+
Tc ∥≤
γ
−ρ (∥ηn∥+ Nєn),
and thus
∥ηn+∥= ∥ηn+
T ∥+ ∥ηn+
Tc ∥≤(+ γ)∥ηn+
Tc ∥≤γ(+ γ)
−ρ
(∥ηn∥+ Nєn).
(.)
Now, by (>.) and (>.) it follows
Nєn ≤rK+(xn) ≤

K + −k(∥xn −x∗∥+ σk(x∗)) =
∥ηn∥
K + −k ,
(.)

Compressive Sensing 

since by assumption σk(x∗)= . Together with (>.) this yields the desired bound,
En+= ∥ηn+∥≤γ(+ γ)
−ρ
(+

K + −k )∥ηn∥= μEn.
In particular, since μ < , one has En+≤R∗, which completes the induction step. It follows
that En+≤μEn for all n ≥n.
∎
...
Superlinear Convergence Promoting ℓτ-Minimization for
τ < 
The linear rate (>.) can be improved significantly, by a very simple modification of the
rule of updating the weight:
wn+
j
= ((xn+
j
)
+ є
n+)
−−τ
,
j = , . . . , N, for any < τ < .
This corresponds to the substitution of the function J with
Jτ(z,w,є) := τ

N
∑
j=
⎛
⎜
⎝
z
jwj + єwj + −τ
τ

w
τ
−τ
j
⎞
⎟
⎠
,
where z ∈RN,w ∈RN
+ ,є ∈R+. With this new update rule for the weight, which depends
on < τ < , we have formally, for xn →¯x and єn →,
Jτ(xn,wn,єn) →∥¯x∥τ
τ .
Hence, such an iterative optimization tends to promote the ℓτ-quasi-norm minimization.
Surprisingly, the rate of local convergence of this modified algorithm is superlinear; the
rate is larger for smaller τ, and approaches a quadratic rate as τ →. More precisely, the
local error En := ∥xn −x∗∥τ
τ satisfies
En+≤μ(γ, τ)E−τ
n
,
(.)
where μ(γ, τ) < for γ > sufficiently small. The validity of (>.) is restricted to xn in a
(small) ball centered at x∗. In particular, if xis close enough to x∗then (>.) ensures
the convergence of the algorithm to the k-sparse solution x∗, see > Fig. -.
..
Numerical Experiments
> Figure -shows a typical phase transition diagram related to the (experimentally
determined) probability of successful recovery of sparse vectors by means of the itera-
tively reweighted least squares algorithm. For each point of this diagram with coordinates
(m/N, k/m) ∈[,], we indicate the empirical success probability of recovery of a k-
sparse vector x ∈RN from m measurements y = Ax. The brightness level corresponds to


Compressive Sensing
0
5
10
10–15
15
20
25
30
35
40
45
100
105
10–10
10–5
Number of iterations
Logarithmic error
Comparison of the rate of convergence for different t 
t = 1
t = 0.8
t = 0.6
t = 0.56
t = 0.5, initial iter. with t=1 
⊡Fig. -
The decay of logarithmic error is shown, as a function of the number of iterations of
iteratively reweighted least squares (IRLS) for diﬀerent values of τ (, ., ., .). We show
also the results of an experiment in which the initial ten iterations are performed with τ = 
and the remaining iterations with τ = .
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
⊡Fig. -
Empirical success probability of recovery of k-sparse vectors x ∈RN from measurements
y = Ax, where A ∈Rm×N is a real random Fourier matrix. The dimension N = of the vectors
is ﬁxed. Each point of this diagram with coordinates (m/N, k/m) ∈[, ]indicates the
empirical success probability of recovery, which is computed by running experiments
with randomly generated k-sparse vectors x and randomly generated matrix. The algorithm
used for the recovery is the iteratively reweighted least squares method tuned to promote
ℓ-minimization

Compressive Sensing 

the probability. As measurement matrix a real random Fourier type matrix A was used,
with entries given by
Ak,j = cos(πjξk),
j = ,⋯, N,
(.)
and the ξk, k = ,..., m, are sampled independently and uniformly at random from [,].
(Theorem does not apply directly to real random Fourier matrices, but an analogous
result concerning the RIP for such matrices can be found in [].)
> Figure -shows a section of a phase transition diagram related to the (exper-
imentally determined) probability of successful recovery of sparse vectors from linear
measurements y = Ax, where the matrix A has independent and identically distributed
Gaussian entries. Here both m and N are fixed and only k is variable. This diagram
establishes the transition from a situation of exact reconstruction for sparse vectors
with high probability to very unlikely recovery for vectors with many nonzero entries.
These numerical experiments used the iteratively re-weighted least squares algorithm
with different parameters < τ ≤. It is of interest to emphasize the enhanced suc-
cess rate when using the algorithm for τ < . Similarly, many other algorithms are
0
5
10
15
20
25
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
k.sparsity
Probability of exact reconstruction
Comparison of iterative reweighted least squares for l1 and l1→lτ
minimization in compressed sensing
l1 minimization
l1 → lτ minimization
⊡Fig. -
Empirical success probability of recovery of a k-sparse vector x ∈Rfrom measurements
y = Ax, where A ∈R×is Gaussian. The matrix is generated once; then, for each sparsity
value k shown in the plot, attempts were made, for randomly generated k-sparse
vectors x. Two diﬀerent IRLS algorithms were compared: one with weights inspired by
ℓ-minimization, and the IRLS with weights that gradually moved during the iterations from
an ℓ- to an ℓτ-minimization goal, with ﬁnal τ = .


Compressive Sensing
⊡Fig. -
Iterations of the recolorization methods proposed in [,] via ℓand total-variation
minimization, for the virtual restoration of the frescoes of A. Mantegna (), which were
destroyed by a bombing during World War II. Only a few colored fragments of the images
were saved from the disaster, together with good quality gray level pictures dated to 
tested by showing the corresponding phase transition diagrams and comparing them,
see [] for a detailed account of phase transitions for greedy algorithms and [, ] for
ℓ-minimization.
This section is concluded by showing applications of ℓ-minimization methods to a
real-life image recolorization problem [, ] in
> Fig. -. The image is known com-
pletely only on very few colored portions, while on the remaining areas only gray levels
are provided. With this partial information, the use of ℓ-minimization with respect
to wavelet or curvelets coefficients allows for high fidelity recolorization of the whole
images.
.
Open Questions
The field of compressed sensing is rather young so there remain many directions to be
explored and it is questionable whether one can assign certain problems in the field already
at this point the status of an “open problem.” Anyhow, below we list two problems that
remained unsolved until the time of writing of this chapter.
..
Deterministic Compressed Sensing Matrices
So far only several types of random matrices A ∈Cm×N are known to satisfy the RIP δs ≤
δ ≤.(say) for
m = Cδs logα(N)
(.)

Compressive Sensing 

for some constant Cδ and some exponent α (with high probability). This is a strong form
of existence statement. It is open, however, to provide deterministic and explicit m × N
matrices that satisfy the RIP δs ≤δ ≤.(say) in the desired range (>.).
In order to show RIP estimates in the regime (>.) one has to take into account
cancellations of positive and negative (or more generally complex) entries in the matrix, see
also > Sect. ... This is done “automatically” with probabilistic methods but seems to be
much more difficult to exploit when the given matrix is deterministic. It may be conjectured
that certain equiangular tight frames or the “Alltop matrix” in [, ] do satisfy the RIP
under (>.). This is supported by numerical experiments in []. It is expected, however,
that a proof is very hard and requires a good amount of analytic number theory.
The best deterministic construction of CS matrices known so far uses deterministic
expander graphs []. Instead of the usual RIP, one shows that the adjacency matrix of
such an expander graph has the -RIP, where the ℓ-norm is replaced by the ℓ-norm
at each occurrence in (>.). The -RIP also implies recovery by ℓ-minimization. The
best known deterministic expanders [] yield sparse recovery under the condition m ≥
Cs(log N)c log(N). Although the scaling in s is linear as desired, the term (log N)c log(N)
grows faster than any polynomial in log N. Another drawback is that the deterministic
expander graph is the output of a polynomial time algorithm, and it is questionable whether
the resulting matrix can be regarded as explicit.
..
Removing Log-Factors in the Fourier-RIP Estimate
It is known [,,,] that a random partial Fourier matrix A ∈Cm×N satisfies the RIP
with high probability provided
m
log(m) ≥Cδs log(s)log(N).
(The condition stated in (>.) implies this one.) It is conjectured that one can remove
some of the log-factors. It must be hard, however, to improve this to a better estimate than
m ≥Cδ,єs log(N)log(log N). Indeed, this would imply an open conjecture of Talagrand
[] concerning the equivalence of the ℓand ℓnorm of a linear combination of a subset
of characters (complex exponentials).
.
Conclusions
Compressive sensing established itself by now as a new sampling theory, which exhibits
fundamental and intriguing connections with several mathematical fields, such as prob-
ability, geometry of Banach spaces, harmonic analysis, theory of computability, and
information-based complexity. The link to convex optimization and the development of
very efficient and robust numerical methods make compressive sensing a concept useful


Compressive Sensing
for a broad spectrum of natural science and engineering applications, in particular, in sig-
nal and image processing and acquisition. It can be expected that compressive sensing will
enter various branches of science and technology to notable effect.
Recent developments, for instance the work [, ] on low-rank matrix recovery via
nuclear norm minimization, suggest new possible extensions of compressive sensing to
more complex structures. Moreover, new challenges are now emerging in numerical anal-
ysis and simulation where high-dimensional problems (e.g., stochastic partial differential
equations in finance and electron structure calculations in chemistry and biochemistry)
became the frontier. In this context, besides other forms of efficient approximation, such as
sparse grid and tensor product methods [], compressive sensing is a promising concept,
which is likely to cope with the “curse of dimensionality.” In particular, further system-
atic developments of adaptivity in the presence of different scales, randomized algorithms,
an increasing role for combinatorial aspects of the underlying algorithms, are examples of
possible future developments, which are inspired by the successful history of compressive
sensing [].
.
Cross-References
> Astronomy
> Duality and Convex Programming
> Iterative Solution Methods
> Learning, Classification, Data Mining
> Linear Inverse Problems
> Mumford Shah, Phase Field Models
> Numerical Methods for Variational Approach in Image Analysis
> Radar
> Regularization Methods for Ill-Posed Problems
> Sampling Methods
> Variational Approach in Image Analysis
References and Further Reading
. Achlioptas D () Database-friendly random
projections. In: Proceedings of the th annual
ACM SIGACT-SIGMOD-SIGART symposium
on principles of database systems, Santa Barbara,
–May , pp –
. Affentranger F, Schneider R () Random pro-
jections of regular simplices. Discrete Comput
Geom ():–
. Baraniuk R () Compressive sensing. IEEE
Signal Process Mag ():–
. Baraniuk RG, Davenport M, DeVore RA, Wakin
M () A simple proof of the restricted
isometry property for random matrices. Constr
Approx ():–
. Berinde R, Gilbert A, Indyk P, Karloff H,
Strauss M () Combining geometry and

Compressive Sensing 

combinatorics: a unified approach to sparse
signal recovery. In: Proceedings th Annual
Allerton
Conference
on
Communication,
Control, and Computing, pp –
. Blanchard JD, Cartis C, Tanner J, Thompson A
() Phase transitions
for greedy sparse
approximation algorithms. Preprint
. Blumensath T, Davies M () Iterative hard
thresholding
for
compressed
sensing.
Appl
Comput Harmon Anal ():–
. Bobin J, Starck J-L, Ottensamer R () Com-
pressed sensing in astronomy. IEEE J Sel Top
Signal Process ():–
. Boyd S, Vandenberghe L () Convex Optim-
ization. Cambridge University Press, Cambridge
. Bungartz H-J, Griebel M () Sparse grids.
Acta Numerica :–
. Candès E, Wakin M () An introduction to
compressive sampling. IEEE Signal Process Mag
():–
. Candès EJ () Compressive sampling. In:
Proceedings of the International congress of
mathematicians, Madrid
. Candès EJ, Romberg J, Tao T () Robust
uncertainty principles: exact signal reconstruc-
tion from highly incomplete frequency informa-
tion. IEEE Trans Inform Theory ():–
. Candès EJ, Recht B () Exact matrix com-
pletion via convex optimization. Found Comput
Math :–
. Candès EJ, Romberg J, Tao T () Stable
signal recovery from incomplete and inaccu-
rate measurements. Commun Pure Appl Math
():–
. Candès EJ, Tao T () Near optimal signal
recovery from random projections: universal
encoding strategies? IEEE Trans Inform Theory
():–
. Capalbo M, Reingold O, Vadhan S, Wigderson A
() Randomness conductors and constant-
degree lossless expanders. In: Proceedings of the
thirty-fourth annual ACM, ACM, Montreal, pp
–(electronic)
. Chen SS, Donoho DL, Saunders MA ()
Atomic decomposition by basis pursuit. SIAM J
Sci Comput ():–
. Christensen O () An introduction to frames
and Riesz bases: applied and numerical harmonic
analysis. Birkhäuser, Boston
. Cline AK () Rate of convergence of Lawson’s
algorithm. Math Comput :–
. Cohen A, Dahmen W, DeVore RA () Com-
pressed sensing and best k-term approximation.
J Am Math Soc ():–
. Cormode G, Muthukrishnan S () Com-
binatorial algorithms for compressed sensing,
SIROCCO, Springer, Heidelberg
. Daubechies I, DeVore R, Fornasier M, Güntürk C
() Iteratively re-weighted least squares min-
imization for sparse recovery. Commun Pure
Appl Math ():–
. Do Ba K, Indyk P, Price E, Woodruff D ()
Lower bounds for sparse recovery. In: Proceed-
ings of ACM-SIAM Symposium on Discrete
Algorithms (SODA), pp –.
. Donoho D, Logan B () Signal recovery
and the large sieve. SIAM J Appl Math ():
–
. Donoho DL () Compressed sensing. IEEE
Trans Inform Theory ():–
. Donoho DL () For most large underdeter-
mined systems of linear equations the minimal
l solution is also the sparsest solution. Commun
Pure Appl Anal ():–
. Donoho DL () High-dimensional centrally
symmetric polytopes with neighborliness pro-
portional to dimension. Discrete Comput Geom
():–
. Donoho DL, Elad M () Optimally sparse
representation in general (nonorthogonal) dic-
tionaries via ℓminimization. Proc Natl Acad Sci
USA ():–
. Donoho DL, Huo X () Uncertainty prin-
ciples and ideal atomic decompositions. IEEE
Trans Inform Theory ():–
. Donoho
DL,
Tanner
J
()
Neighborli-
ness of randomly projected simplices in high
dimensions. Proc Natl Acad Sci USA ():
–
. Donoho DL, Tanner J () Counting faces
of
randomly-projected
polytopes
when
the
projection radically lowers dimension. J Am
Math Soc ():–
. Donoho DL, Tsaig Y () Fast solution of
l-norm minimization problems when the solu-
tion may be sparse. IEEE Trans Inform Theory
():–


Compressive Sensing
. Dorfman R () The detection of defective
members of large populations. Ann Statist
:–
. Duarte M, Davenport M, Takhar D, Laska J,
Ting S, Kelly K, Baraniuk R () Single-pixel
imaging via compressive sampling. IEEE Signal
Process Mag ():–
. Efron B, Hastie T, Johnstone I, Tibshirani
R () Least angle regression. Ann Statist
():–
. Elad M, Bruckstein AM () A generalized
uncertainty principle and sparse representation
in pairs of bases. IEEE Trans Inform Theory
():–
. Fannjiang A, Yan P, Strohmer T (in press),
Compressed remote sensing of sparse object.
SIAM J Imaging Sci
. Fornasier M () Numerical methods for
sparse recovery. In: Fornasier M (ed) Theoret-
ical foundations and numerical methods for
sparse recovery. Radon Series on Computational
and Applied Mathematics, de Gruyter, Berlin,
pp –
. Fornasier M, Langer A, Schönlieb CB (to appear,
A convergent overlapping domain decomposi-
tion method for total variation minimization.
Numer Math
. Fornasier M, March R () Restoration of color
images by vector valued BV functions and varia-
tional calculus. SIAM J Appl Math ():–
. Fornasier M, Ramlau R, Teschke G ()
The application of joint sparsity and total vari-
ation minimization algorithms to a real-life
art restoration problem. Adv Comput Math
(–):–
. Foucart S (to appear) A note on guaranteed
sparse
recovery
via
ℓ-minimization.
Appl
Comput Harmon Anal
. Foucart S, Pajor A, Rauhut H, Ullrich T (to
appear) The Gelfand widths of ℓp-balls for <
p ≤. J Complexity. doi:./j.jco...
. Foucart S, Rauhut H (in preparation) A math-
ematical introduction to compressive sensing.
Applied and Numerical Harmonic Analysis,
Birkhäuser, Boston
. Fuchs JJ () On sparse representations in
arbitrary redundant bases. IEEE Trans Inform
Theory ():–
. Garnaev
A,
Gluskin
E
()
On
widths
of the Euclidean ball. Sov Math Dokl :
–
. Gilbert
AC,
Muthukrishnan
S,
Guha
S,
Indyk
P,
Strauss
M
()
Near-optimal
sparse Fourier representations via sampling.
In:
Proceedings
of
the
STOC’, Associa-
tion for Computing Machinery, New York,
pp –
. Gilbert AC, Muthukrishnan S, Strauss MJ ()
Approximation of functions over redundant
dictionaries using coherence. In: Proceedings of
the fourteenth annual ACM-SIAM symposium
on discrete algorithms, SIAM and Association
for Computing Machinery, Baltimore, –
January , pp –
. Gilbert AC, Strauss M, Tropp JA, Vershynin R
() One sketch for all: fast algorithms for
compressed sensing. In: Proceedings of the
ACM Symposium on the Theory of Computing
(STOC), San Diego
. Gluskin E () Norms of random matrices and
widths of finite-dimensional sets. Math USSR-Sb
:–
. Gorodnitsky I, Rao B () Sparse signal
reconstruction
from
limited
data
using
FOCUSS:
a
re-weighted
minimum
norm
algorithm. IEEE Trans Signal Process ():
–
. Gribonval R, Nielsen M () Sparse represen-
tations in unions of bases. IEEE Trans Inform
Theory ():–
. Horn R, Johnson C () Matrix analysis.
Cambridge University Press, Cambridge
. Johnson WB, Lindenstrauss J (eds) () Hand-
book of the geometry of Banach spaces, vol .
North-Holland, Amsterdam
. Kashin B () Diameters of some finite-
dimensional sets and classes of smooth functions.
Math USSR, Izv :–
. Lawson C () Contributions to the the-
ory of linear least maximum approximation.
PhD
thesis,
University
of
California,
Los
Angeles
. Ledoux M, Talagrand M () Probability in
Banach
spaces.
Springer,
Berlin/Heidelberg/
New York
. Logan B () Properties of high-pass signals.
PhD thesis, Columbia University, New York

Compressive Sensing 

. Lorentz GG, von Golitschek M, Makovoz Y
() Constructive approximation: advanced
problems. Springer, Berlin
. Mallat SG, Zhang Z () Matching pursuits
with time-frequency dictionaries. IEEE Trans
Signal Process ():–
. Marple S () Digital spectral analysis with
applications. Prentice-Hall, Englewood Cliffs
. Mendelson S, Pajor A, Tomczak Jaegermann N
()
Uniform
uncertainty
principle
for
Bernoulli and subgaussian ensembles. Constr
Approx ():–
. Natarajan
BK
()
Sparse
approximate
solutions to linear systems. SIAM J Comput
:–
. Nesterov Y, Nemirovskii A () Interior-point
polynomial algorithms
in convex program-
ming. In: Volume of SIAM studies in applied
mathematics, Society for Industrial and Applied
Mathematics (SIAM), Philadelphia
. Novak E () Optimal recovery and n-widths
for convex classes of functions. J Approx Theory
():–
. Osborne M, Presnell B, Turlach B () A
new approach to variable selection in least
squares problems. IMA J Numer Anal ():
–
. Osborne M, Presnell B, Turlach B () On
the LASSO and its dual. J Comput Graph Stat
():–
. Pfander GE, Rauhut H () Sparsity in time-
frequency representations. J Fourier Anal Appl
():–
. Pfander
GE,
Rauhut
H,
Tanner
J
()
Identification of matrices having a sparse rep-
resentation. IEEE Trans Signal Process ():
–
. Prony R () Essai expérimental et analytique
sur les lois de la Dilatabilité des uides élastique
et sur celles de la Force expansive de la vapeur
de leau et de la vapeur de lalkool, à différentes
températures. J École Polytech :–
. Rauhut H () Random sampling of sparse
trigonometric
polynomials.
Appl
Comput
Harmon Anal ():–
. Rauhut H () Stability results for random
sampling
of
sparse
trigonometric
polyno-
mials.
IEEE
Trans
Inform
Theory
():
–
. Rauhut H () Circulant and Toeplitz matrices
in compressed sensing. In: Proceedings of the
SPARS’, Saint-Malo
. Rauhut H () Compressive sensing and struc-
tured random matrices. In: Fornasier M (ed)
Theoretical foundations and numerical methods
for sparse recovery. Radon Series on Compu-
tational and Applied Mathematics, de Gruyter,
Berlin
. Recht B, Fazel M, Parillo P (to appear) Guaran-
teed minimum rank solutions to linear matrix
equations via nuclear norm minimization. SIAM
Rev
. Romberg J () Imaging via compressive
sampling.
IEEE Signal Process Mag ():
–
. Rudelson M, Vershynin R () On sparse
reconstruction
from
Fourier
and
Gaussian
measurements.
Commun
Pure
Appl
Math
:–
. Rudin L, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D (–):–
. Santosa F, Symes W () Linear inversion of
band-limited reflection seismograms. SIAM J Sci
Stat Comput ():–
. Schnass K, Vandergheynst P () Dictionary
preconditioning for greedy algorithms. IEEE
Trans Signal Process ():–
. Strohmer T, Heath RW Jr () Grassman-
nian frames with applications to coding and
communication. Appl Comput Harmon Anal
():–
. Strohmer T, Hermann M () Compressed
sensing radar. In: IEEE proceedings of the
international conference on acoustic, speech,
and signal processing, pp –
. Tadmor E () Numerical methods for non-
linear partial differential equations. In: Meyers
R (ed) Encyclopedia of complexity and systems
science, Springer
. Talagrand M () Selecting a proportion of
characters. Israel J Math :–
. Tauboeck G, Eiwen D, Hlawatsch F, Rauhut H
() Compressive estimation of doubly selec-
tive channels: exploiting channel sparsity to
improve
spectral
efficiency
in
multicarrier
transmissions. J Sel Topics Signal Process ():
–


Compressive Sensing
. Taylor H, Banks S, McCoy J () Deconvolu-
tion with the ℓ-norm. Geophys J Int ():–
. Tibshirani
R
()
Regression
shrinkage
and selection via the lasso. J R Stat Soc Ser B
():–
. Traub J, Wasilkowski G, Woniakowski H ()
Information-based complexity. Computer Sci-
ence
and
Scientific
Computing.
Academic,
New York
. Tropp JA Needell D () CoSaMP: Iterative
signal recovery from incomplete and inaccu-
rate samples. Appl Comput Harmon Anal :
–
. Tropp JA () Greed is good: algorithmic
results for sparse approximation. IEEE Trans
Inform Theory ():–
. Tropp JA () Just relax: convex program-
ming methods for identifying sparse signals
in noise. IEEE Trans Inform Theory ():
–
. Tropp JA, Laska JN, Duarte MF, Romberg JK,
Baraniuk RG () Beyond Nyquist: efficient
sampling of sparse bandlimited signals. IEEE
Trans Inform Theory ():–
. Unser M () Sampling – years after
shannon. Proc IEEE ():–
. Vybiral J () Widths of embeddings in
function spaces. J Complexity ():–
. Wagner G, Schmieder P, Stern A, Hoch J ()
Application of nonlinear sampling schemes to
cosy-type spectra. J Biomol NMR ():


Duality and Convex
Programming
Jonathan M. Borwein ⋅D. Russell Luke
.
Introduction.....................................................................
..
Linear Inverse Problems with Convex Constraints................................
..
Imaging with Missing Data............................................................
..
Image Denoising and Deconvolution................................................
..
Inverse Scattering.......................................................................
..
Fredholm Integral Equations..........................................................
.
Background......................................................................
..
Lipschitzian Properties.................................................................
..
Subdifferentials..........................................................................
.
Duality and Convex Analysis...................................................
..
Fenchel Conjugation....................................................................
..
Fenchel Duality..........................................................................
..
Applications..............................................................................
..
Optimality and Lagrange Multipliers................................................
..
Variational Principles...................................................................
..
Fixed Point Theory and Monotone Operators......................................
.
Case Studies......................................................................
..
Linear Inverse Problems with Convex Constraints................................
..
Imaging with Missing Data............................................................
..
Inverse Scattering.......................................................................
..
Fredholm Integral Equations..........................................................
.
Open Questions..................................................................
.
Conclusion.......................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Duality and Convex Programming
Abstract: This chapter surveys key concepts in convex duality theory and their application
to the analysis and numerical solution of problem archetypes in imaging.
Keywords: Convex analysis ⋅variational analysis ⋅duality
AMS subject classification: M⋅M⋅K⋅C⋅C
.
Introduction
An image is worth a thousand words, the joke goes, but takes up a million times the mem-
ory – all the more reason to be efficient when computing with images. Whether one is
determining a “best” image according to some criteria or applying a “fast” iterative algo-
rithm for processing images, the theory of optimization and variational analysis lies at the
heart of achieving these goals. Success or failure hinges on the abstract structure of the task
at hand. For many practitioners, these details are secondary: if the picture looks good, it is
good. For the specialist in variational analysis and optimization, however, it is what is went
into constructing the image that matters: if it is convex, it is good.
This chapter surveys more than a half-a-century of work in convex analysis that has
played a fundamental role in the development of computational imaging and to bring to
light as many of the contributors to this field as possible. There is no shortage of good books
on convex and variational analysis; we point interested readers to the modern references
[, , , , , , , , , , , , , , , , , ]. References focused more on
imaging and signal processing, but with a decidedly variational flavor, include [, , ].
For general references on numerical optimization, see [,,,,,].
For many years, the dominant distinction in applied mathematics between problem
types has rested upon linearity, or lack thereof. This categorization still holds sway today
with nonlinear problems largely regarded as “hard,” while linear problems are generally
considered “easy.” But since the advent of the interior point revolution [], at least in linear
optimization, it is more or less agreed that nonconvexity, not nonlinearity, more accurately
delineates hard from easy. The goal of this chapter is to make this case more broad. Indeed,
for convex sets topological, algebraic and geometric notions often coincide, and so the tools
of convex analysis provide not only for a tremendous synthesis of ideas but also for key
insights, whose dividends are efficient algorithms for solving large (infinite dimensional)
problems, and indeed even large nonlinear problems.
We consider different instances of a single optimization model. This model accounts
for the vast majority of variational problems appearing in imaging science:
minimize
x∈C⊂X
Iφ(x)
subject to
Ax ∈D.
(.)
Here, X and Y are real Banach spaces with continuous duals X∗and Y∗, C and D are
closed and convex, A : X →Y is a continuous linear operator, and the integral functional
Iφ(x) := ∫T φ(x(t))μ(dt) is defined on some vector subspace Lp(T, μ) of X for μ, a com-
plete totally finite measure on some measure space T. The integral operator Iφ is an entropy

Duality and Convex Programming 

with integrand φ : R →[−∞,+∞] a closed convex function. This provides an extremely
flexible framework that specializes to most of the instances of interest and is general enough
to extend results to non-Hilbert space settings. The most common examples are
Burg entropy: φ(x) := −ln(x)
(.)
Shannon–Boltzmann entropy: φ(x) := x ln(x)
(.)
Fermi–Dirac entropy : φ(x) := x ln(x) + (−x)ln(−x)
(.)
Lp norm φ(x) := ∥x∥p
p
(.)
Lp entropy φ(x) :=
⎧⎪⎪⎨⎪⎪⎩
x p
p
x ≥
+∞
else
(.)
Total variation φ(x) := ∣∇x∣.
(.)
See [,,,,,,,,,] for these and other entropies.
There is a rich correspondence between the algorithmic approach to applications
implicit in the variational formulation (>.) and the prevalent feasibility approach to prob-
lems. Here, one considers the problem of finding the point x that lies in the intersection of
the constraint sets:
find x ∈C ∩S
where
S := {x ∈X ∣Ax ∈D}.
In the case where the intersection is quite large, one might wish to find the point in the
intersection in some sense closest to a reference point x(frequently the origin). It is the
job of the objective in (>.) to pick the element of C ∩S that has the desired properties,
that is, to pick the best approximation. The feasibility formulation suggests very naturally
projection algorithms for finding the intersection whereby one applies the constraints one
at a time in some fashion, e.g., cyclically, or at random [,,,]. This is quite a powerful
framework as it provides a great deal of flexibility and is amenable to parallelization for
large-scale problems. Many of the algorithms for feasibility problems have counterparts for
the more general best approximation problems [,,,]. For studies of these algorithms
in nonconvex settings, see [, , , , , , –]. The projection algorithms that are
central to convex feasibility and best approximation problems play a key role in algorithms
for solving the problems we will consider here.
Before detailing specific applications, we state a general duality result for problem (>.)
that motivates many of the tools we use. One of the more central tools we make use of is the
Fenchel conjugate [] of a mapping f : X →[−∞,+∞], denoted f ∗: X∗→[−∞,+∞]
and defined by
f ∗(x∗) = sup
x∈X
{⟨x∗, x⟩−f (x)}.
The conjugate is always convex (as a supremum of affine functions), while f = f ∗∗exactly
if f is convex, proper (not everywhere infinite), and lower semi-continuous (lsc) [, ].


Duality and Convex Programming
Here and below, unless otherwise specified, X is a normed space with dual X∗. The fol-
lowing theorem uses constraint qualifications involving the concept of the core of a set, the
effective domain of a function (dom f ), and the points of continuity of a function (cont f ).
Deﬁnition (core)
The core of a set F ⊂X is defined by x ∈core F if for each h ∈
{x ∈X ∣∥x∥= }, there exists δ > so that x + th ∈F for all ≤t ≤δ.
It is clear from the definition that int F ⊂core F. If F is a convex subset of a Euclidean
space, or if F is closed, then the core and the interior are identical [, Theorem ..].
Theorem (Fenchel duality [, Theorems ..and ..])
Let X and Y be Banach
spaces, let f : X →(−∞,+∞] and g : Y →(−∞,+∞] and let A : X →Y be a bounded
linear map. Define the primal and dual values p, d ∈[−∞,+∞] by the Fenchel problems
p = inf
x∈X{f (x) + g(Ax)}
d = sup
y∗∈Y∗{−f ∗(A∗y∗) −g∗(−y∗)}.
(.)
Then these values satisfy the weak duality inequality p ≥d.
If f , g are convex and satisfy either
∈core (dom g −Adom f )
with f and g lsc,
(.)
or
Adom f ∩cont g ≠Ø,
(.)
then p = d, and the supremum to the dual problem is attained if finite.
Applying Theorem to problem (>.) we have f (x) = Iφ(x)+ιC(x) and g(y) = ιD(y)
where ιF is the indicator function of the set F:
ιF(x) :=
⎧⎪⎪⎨⎪⎪⎩

if x ∈F
+∞
else.
(.)
The tools of convex analysis and the phenomenon of duality are central to formulating,
analyzing, and solving application problems. Already apparent from the general applica-
tion above is the necessity for a calculus of Fenchel conjugation in order to compute the
conjugate of sums of functions. In some specific cases, one can arrive at the same con-
clusion with less theoretical overhead, but this is at the cost of missing out more general
structures that are not necessarily automatic in other settings.
Duality has a long-established place in economics where primal and dual problems
have direct interpretations in the context of the theory of zero-sum games, or where
Lagrange multipliers and dual variables are understood, for instance, as shadow prices.
In imaging, there is not as often an easy interplay between the physical interpretation of
primal and dual problems. Duality has been used toward a variety of ends in contem-
porary image and signal processing, the majority of them, however, having to do with

Duality and Convex Programming 

algorithms [, , , –, , , , , , , ]. Nevertheless, the dual perspective
yields new statistical or information theoretic insight into image processing problems,
in addition to faster algorithms. Five main applications illustrate the variational ana-
lytical approach to problem solving: linear inverse problems with convex constraints,
compressive imaging, image denoising and deconvolution, nonlinear inverse scattering,
and finally Fredholm integral equations. We briefly review these applications below. In
subsequent sections, we develop the tools for their analysis. At the end of the chap-
ter, we revisit these applications in light of the convex analytical tools collected along
the way.
..
Linear Inverse Problems with Convex Constraints
Let X be a Hilbert space and φ(x) := 
∥x∥. The integral functional Iφ is the usual Lnorm
and the solution is the closest feasible point to the origin:
minimize
x∈C⊂X

∥x∥
subject to
Ax = b.
(.)
Levi, for instance, used this variational formulation to determine the minimum energy
band-limited signal that matched N measurements b ∈Rn with the model A : X →Rn
[]. Note that the signal space is infinite dimensional while the measurement space is
finite dimensional, a common situation in practice. Potter and Arun [] recognized a
much broader applicability of this variational formulation to remote sensing and medical
imaging and applied duality theory to characterize solutions to (>.) by x = PCA∗(y),
where y ∈Y satisfies b = APCA∗y [, Theorem ]. Particularly attractive is the feature
that when Y is finite dimensional, these formulas yield a finite dimensional approach to
an infinite dimensional problem. The numerical algorithm suggested by Potter and Arun
is an iterative procedure in the dual variables:
y j+= y j + γ(b −APCA∗y j)
j = ,,, . . .
(.)
The optimality condition and numerical algorithms are explored at the end of this chapter.
As satisfying as this theory is, there is a crucial assumption in the theorem of Potter and
Arun about the existence of y ∈Y such that b = APCA∗y; one needs to only consider linear
least squares, for an example, where the primal problem is well posed but no such y exists
[]. To facilitate the argument we specialize Theorem to the case of linear constraints.
The next corollary is a specialization of Theorem , where g is the indicator function of the
point b in the linear constraint.
Corollary (Fenchel duality for linear constraints)
Given any f : X →(−∞,∞], any
bounded linear map A : X →Y , and any element b ∈Y, the following weak duality
inequality holds:
inf
x∈X {f (x) ∣Ax = b} ≥sup
y∗∈Y∗{⟨b, y∗⟩−f ∗(A∗y∗)}.


Duality and Convex Programming
If f is lsc and convex and b ∈core(Adom f ), then equality holds and the supremum is
attained if finite.
Suppose that C = X, a Hilbert space and A : X →X . The Fenchel dual to (>.) is
maximize
y∈X
⟨y, b⟩−
∥A∗y∥.
(.)
(The Lnorm is self-dual.) Suppose that the primal problem (>.) is feasible, that is,
b ∈range(A). The objective in (>.) is convex and differentiable, so elementary calculus
(Fermat’s rule) yields the optimal solution y with AA∗y = b, assuming y exists. If the range
of A is strictly larger than that of AA∗, however, it is possible to have b ∈range(A) but
b ∉range(AA∗), in which case the optimal solution x to (>.) is not equal to A∗y, since
y is not attained. For a concrete example see [, Example .].
..Imaging with Missing Data
Let X = Rn and φ(x) := ∥x∥p for p = or p = . The case p = is the ℓnorm, and by ∥x∥
we mean the function
∥x∥:= ∑
j
∣sign(x j)∣,
where sign() := . One then has the optimization problem
minimize
x∈Rn
∥x∥p
subject to
Ax = b.
(.)
This model has received a great deal of attention recently in applications of compressive
sensing where the number of measurements is much smaller than the dimension of the
signal space, that is, b ∈Rm for m ≪n. This problem is well known in statistics as the
missing data problem.
For ℓoptimization (p = ), the seminal work of Candés and Tao establishes probabilis-
tic criteria for when the solution to (>.) is unique and exactly matches the true signal
x∗[]. Sparsity of the original signal x∗and the algebraic structure of the matrix A are
key requirements. Convex analysis easily yields a geometric interpretation of these facts.
We develop the tools to show that the dual to this problem is the linear program
maximize
y∈Rm
bT y
subject to
(A∗y)j ∈[−,]
j = ,, . . . , n.
(.)
Elementary facts from linear programming guarantee that the solution includes a vertex
of the polyhedron described by the constraints, and hence, assuming A is full rank, there
can be at most m active constraints. The number of active constraints in the dual problem
provides an upper bound on the number of nonzero elements in the primal variable –
the signal to be recovered. Unless the number of nonzero elements of x∗is less than the
number of measurements m, there is no hope of uniquely recovering x∗. The uniqueness

Duality and Convex Programming 

of solutions to the primal problem is easily understood in terms of the geometry of the
dual problem, that is, whether or not solutions to the dual problem reside along the edges
or faces of the polyhedron. More refined details about how sparse x∗needs to be in order
to have a reasonable hope of exact recovery require more work, but elementary convex
analysis already provides the essential intuition.
For the function ∥x∥(p = in (>.) the equivalence of the primal and dual problems
is lost due to the nonconvexity of the objective. The theory of Fenchel duality still yields
weak duality, but this is of limited use in this instance. The Fenchel dual to (>.) is
maximize
y∈Rm
bT y
subject to
(A∗y)j = 
j = ,, . . . , n.
(.)
If we denote the values of the primal (>.) and dual problems (>.) by p and d respec-
tively, then these values satisfy the weak duality inequality p ≥d. The primal problem is a
combinatorial optimization problem, and hence NP-hard; the dual problem, however, is
a linear program, which is finitely terminating. Relatively elementary variational analysis
provides a lower bound on the sparsity of signals x that satisfy the measurements. In this
instance, however, the lower bound only reconfirms what we already know. Indeed, if A is
full rank, then the only solution to the dual problem is y = . In other words, the minimal
sparsity of the solution to the primal problem is zero, which is obvious. The loss of infor-
mation in passing from primal to dual formulations of nonconvex problems is a common
phenomenon and underscores the importance of convexity.
The Fenchel conjugates of the ℓnorm and the function ∥⋅∥are given respectively by
φ∗
(y) :=
⎧⎪⎪⎨⎪⎪⎩

∥y∥∞≤
+∞
else
(φ(x) := ∥x∥)
(.)
φ∗
(y) :=
⎧⎪⎪⎨⎪⎪⎩

y = 
+∞
else
(φ(x) := ∥x∥)
(.)
It is not uncommon to consider the function ∥⋅∥as the limit of (∑j ∣x j∣p)
/p as p →.
We present an alternative approach based on the regularization of the conjugates. for L and
є > define
φє,L(y) :=
⎧⎪⎪⎨⎪⎪⎩
є ( (L+y) ln(L+y)+(L−y) ln(L−y)
L ln()
−ln(L)
ln() )
(y ∈[−L, L])
+∞
for ∣y∣> L.
(.)
This is a scaled and shifted Fermi–Dirac entropy (>.). It is also a smooth convex function
on the interior of its domain and so elementary calculus can be used to calculate the Fenchel
conjugate,
φ∗
є,L(x) =
є
ln() ln (xL/є + ) −xL −є.
(.)


Duality and Convex Programming
For L > fixed, in the limit as є →we have
lim
є→φє,L(y) =
⎧⎪⎪⎨⎪⎪⎩

y ∈[−L, L]
+∞
else
and
lim
є→φ∗
є,L(x) = L∣x∣.
For є > fixed we have
lim
L→φє,L(x) =
⎧⎪⎪⎨⎪⎪⎩

y = 
+∞
else
and
lim
L→φ∗
є,L(x) := .
Note that ∥⋅∥and φ∗
є,:= have the same conjugate, but unlike ∥⋅∥the biconjugate of
φ∗
є,is itself. Also note that φє,L and φ∗
є,L are convex and smooth on the interior of their
domains for all є, L > . This is in contrast to metrics of the form (∑j ∣x j∣p) which are
nonconvex for p < . We therefore propose solving
minimize
x∈Rn
Iφ∗
є,L(x)
subject to
Ax = b
(.)
as a smooth convex relaxation of the conventional ℓp optimization for ≤p ≤.
..
Image Denoising and Deconvolution
We consider next problems of the form
minimize
x∈X
Iφ(x) + 
λ∥Ax −y∥,
(.)
where X is a Hilbert space, Iφ : X →(−∞,+∞] is a semi-norm on X, and A : X →Y ,
is a bounded linear operator. This problem is explored in [] as a general framework that
includes total variation minimization [], wavelet shrinkage [], and basis pursuit [].
When A is the identity, problem (>.) amounts to a technique for denoising; here y is the
received, noisy signal, and the solution x is an approximation with the desired statistical
properties promoted by the objective Iφ. When the linear mapping A is not the identity
(for instance, A models convolution against the point spread function of an optical system)
problem (>.) is a variational formulation of deconvolution, that is, recovering the true
signal from the image y. The focus here is on total variation minimization.
Total variation was first introduced by Rudin et al. [] as a regularization technique
for denoising images while preserving edges and, more precisely, the statistics of the noisy
image. The total variation of an image x ∈X = L(T) – for T and open subset of R– is
defined by
ITV(x) := sup {∫T x(t)div ξ(t)dt ∣ξ ∈C
c(T,R),∣ξ(t)∣≤∀t ∈T }.
The integral functional ITV is finite if and only if the distributional derivative Dx of x
is a finite Radon measure in T, in which case we have ITV(x) = ∣Dx∣(T). If, moreover,
x has a gradient ∇x ∈L(T,R), then ITV(x) = ∫∣∇x(t)∣dt, or, in the context of the

Duality and Convex Programming 

general framework established at the beginning of this chapter, ITV(x) = Iφ(x) where
φ(x(t)) := ∣∇x(t)∣. The goal of the original total variation denoising problem proposed
in [] is then to
minimize
x∈X
ITV(x)
subject to
∫T Ax = ∫T x
and
∫T ∣Ax −x∣= σ .
(.)
The first constraint corresponds to the assumption that the noise has zero mean and the
second assumption requires the denoised image to have a predetermined standard devi-
ation σ. Under reasonable assumptions [], this problem is equivalent to the convex
optimization problem
minimize
x∈X
ITV(x)
subject to
∥Ax −x∥≤σ .
(.)
Several authors have exploited duality in total variation minimization for efficient algo-
rithms to solve the above problem [,,,]. One can “compute” the Fenchel conjugate
of ITV indirectly by using the already mentioned property that the biconjugate of a proper,
convex lsc function is the function itself: f ∗∗(x) = f (x) if (and only if) f is proper, convex
and lsc at x. Rewriting ITV as the Fenchel conjugate of some function, we have
ITV(x) = sup
v
⟨x, v⟩−ιK(v),
where
K := {div ξ ∣ξ ∈Cc(T,R)
and
∣ξ(t)∣≤∀t ∈T }.
From this, it is then clear that the Fenchel conjugate of ITV is the indicator function of the
convex set K, ιK.
In [], duality is used to develop an algorithm, with proof of convergence, for the
problem
minimize
x∈X
ITV(x) + 
λ∥x −x∥
(.)
with X a Hilbert space. First-order optimality conditions for this unconstrained problem
are
∈x −x+ λ∂ITV(x),
(.)
where ∂ITV(x) is the subdifferential of ITV at x defined by
v ∈∂ITV(x) ⇐⇒ITV(y) ≥ITV(x) + ⟨v, y −x⟩
∀y.
The optimality condition (>.) is equivalent to [, Proposition ..]
x ∈∂I∗
TV ((x−x)/λ)
(.)
or, since I∗
TV = ιK,
x
λ ∈(I + 
λ ∂ιK)(z)


Duality and Convex Programming
where z = (x−x)/λ. (For the finite dimensional statement, see [, Proposition I...].)
Since K is convex, standard facts from convex analysis determine that ∂ιK(z) is the normal
cone mapping to K at z, denoted NK(z) and defined by
NK(z) :=
⎧⎪⎪⎨⎪⎪⎩
{v ∈X ∣⟨v, x −z⟩≤
for all x ∈K }
z ∈K
Ø
z ∉K.
Note that this is a set-valued mapping. The resolvent (I + 
λ ∂ιK)
−evaluated at x/λ is the
orthogonal projection of x/λ onto K. That is, the solution to (>.) is
x∗= x−PK(x/λ) = x−PλK(x).
The inclusions disappear from the formulation due to convexity of K: the resolvent of the
normal cone mapping of a convex set is single valued. The numerical algorithm for solving
(>.) then amounts to an algorithm for computing the projection PλK . We develop below
the tools from convex analysis used in this derivation.
..
Inverse Scattering
An important problem in applications involving scattering is the determination of the
shape and location of scatterers from measurements of the scattered field at a distance.
Modern techniques for solving this problem use indicator functions to detect the inconsis-
tency or insolubility of an Fredholm integral equation of the first kind, parameterized by
points in space. The shape and location of the object is determined by those points where
the auxiliary problem is solvable. Equivalently, the technique determines the shape and
location of the scatterer by determining whether a sampling function, parameterized by
points in space, is in the range of a compact linear operator constructed from the scattering
data.
These methods have enjoyed great practical success since their introduction in the latter
half of the s. Recently Kirsch and Grinberg [] established a variational interpretation
of these ideas. They observe that the range of a linear operator G : X →Y (X and Y are
reflexive Banach spaces) can be characterized by the infimum of the mapping
h(ψ) : Y∗→R ∪{−∞,+∞} := ∣⟨ψ, Fψ⟩∣,
where F := GSG∗for S : X∗→X , a coercive bounded linear operator. Specifically, they
establish the following.
Theorem ([, Thoerem .])
Let X, Y be reflexive Banach spaces with duals X∗and
Y∗. Let F : Y∗→Y and G : X →Y be bounded linear operators with F = GSG∗for
S : X∗→X a bounded linear operator satisfying the coercivity condition
∣⟨φ, Sφ⟩∣≥c∥φ∥
X∗
for some c > and all φ ∈range(G∗) ⊂X∗.

Duality and Convex Programming 

Then for any ϕ ∈Y/{} ϕ ∈range(G) if and only if
inf{h(ψ) ∣ψ ∈Y∗,⟨ϕ, ψ⟩= } > .
It is shown below that the infimal characterization above is equivalent to the computa-
tion of the effective domain of the Fenchel conjugate of h,
h∗(ϕ) := sup
ψ∈Y∗{⟨ϕ, ψ⟩−h(ψ)}.
(.)
In the case of scattering, the operator F above is an integral operator whose kernel is
made up of the “measured” field on a surface surrounding the scatterer. When the mea-
surement surface is a sphere at infinity, the corresponding operator is known as the far field
operator. The factor G maps the boundary condition of the governing PDE (the Helmholtz
equation) to the far field pattern, that is, the kernel of the far field operator. Given the right
choice of spaces, the mapping G is compact, one-to-one, and dense. There are two keys to
using the above facts for determining the shape and location of scatterers: first, the con-
struction of the test function ϕ and, second, the connection of the range of G to that of
some operator easily computed from the far field operator F. The secret behind the success
of these methods in inverse scattering is, first, that the construction of ϕ is trivial and, sec-
ond, that there is (usually) a simpler object to work with than the infimum in Theorem 
that depends only on the far field operator (usually the only thing that is known). Indeed,
the test functions ϕ are simply far field patterns due to point sources: ϕz := e−ik̂x⋅z, where ̂x
is a point on the unit sphere (the direction of the incident field), k is a nonnegative integer
(the wave number of the incident field), and z is some point in space.
The crucial observation of Kirsch is that ϕz is in the range of G if and only if z is a point
inside the scatterer. If one does not know where the scatter is, let alone its shape, then one
does not know G, however, the Fenchel conjugate depends not on G but on the operator
F which is constructed from measured data. In general, the Fenchel conjugate, and hence
the Kirsch–Grinberg infimal characterization, is difficult to compute, but depending on
the physical setting, there is a functional U of F under which the ranges of U(F) and G
coincide. In the case where F is a normal operator, U(F) = (F∗F)/; for non-normal
F, the functional U depends more delicately on the physical problem at hand and is only
known in a handful of cases. So the algorithm for determining the shape and location of a
scatterer amounts to determining those points z, where e−ik̂x⋅z is in the range of U(F) and
where U and F are known and easily computed.
..
Fredholm Integral Equations
In the scattering application of
> Sect. .., the prevailing numerical technique is not to
calculate the Fenchel conjugate of h(ψ) but rather to explore the range of some functional


Duality and Convex Programming
of F. Ultimately, the computation involves solving a Fredholm integral equation of the first
kind. This brings us back to the more general setting with which we began. Let
(Ax)(s) = ∫T a(s, t)μ(dt) = b(s)
for reasonable kernels and operators. If A is compact, for instance, as in most deconvolu-
tion problems of interest, the problem is ill posed in the sense of Hadamard. Some sort of
regularization technique is therefore required for numerical solutions [,,,,]. We
explore regularization in relation to the constraint qualifications (>.) or (>.).
Formulating the integral equation as an entropy minimization problem we have
minimize
x∈X
Iφ(x)
subject to
Ax = b.
(.)
Following [, Example .], let T and S be the interval [,] with Lebesgue measures μ and
, and let a(s, t) be a continuous kernel of the Fredholm operator Amapping X := C([,])
to Y := C([,]), both equipped with the supremum norm. The adjoint operator is given
by A∗y = {∫S a(s, t)λ(ds)}μ(dt), where the dual spaces are the spaces of Borel measures,
X∗= M([,]) and Y∗= M([,]). Every element of the range is therefore μ-absolutely
continuous and A∗can be viewed as having its range in L([,], μ). It follows from []
that the Fenchel dual of (>.) for the operator A is therefore
max
y∗∈Y∗⟨b, y∗⟩−Iφ∗(A∗y∗).
(.)
Note that the dual problem, unlike the primal, is unconstrained. Suppose that A is injec-
tive and that b ∈range(A). Assume also that φ∗is everywhere finite and differentiable.
Assuming the solution y to the dual is attained, the naive application of calculus provides
that
b = A(∂φ∗
∂r (A∗y))
and
xφ = (∂φ∗
∂r (A∗y)).
(.)
Similar to the counterexample explored in
> Sect. .., it is quite likely that
A( ∂φ∗
∂r (range(A∗))) is smaller than the range of A, hence it is possible to have b ∈range(A)
but not in A( ∂φ∗
∂r (range(A∗))). Thus the assumption that the solution to the dual problem
is attained cannot hold and the primal–dual relationship is broken.
For a specific example, following [, Example .], consider the Laplace transform
restricted to [,]: a(s, t) := e−st (s ∈[,]), and let φ be either the Boltzmann–Shannon
entropy, Fermi–Dirac entropy, or an Lp norm with p ∈(,), (>.)–(>.) respectively.
Take b(s) := ∫[,] e−stx(t)dt for x := α ∣t −
∣+ β, a solution to (>.). It can be shown
that the restricted Laplace operator defines an injective linear operator from C([,])
to C([,]). However, xφ given by (>.) is continuously differentiable and thus can-
not match the known solution x which is not differentiable. Indeed, in the case of the
Boltzmann–Shannon entropy, the conjugate function and A∗y are entire hence the osten-
sible solution xφ must be infinitely differentiable on [,]. One could guarantee that the
solution to the primal problem (>.) is attained by replacing C([,]) with Lp([,]),
but this does not resolve the problem of attainment in the dual problem.

Duality and Convex Programming 

To recapture the correspondence between primal and dual problems it is necessary
to regularize or, alternatively, relax the problem, or to require the constraint qualification
b ∈core (Adom φ). Such conditions usually require A to be surjective, or at least to have
closed range.
.
Background
As this is meant to be a survey of some of the more useful milestones in convex analysis,
the focus is more on the connections between ideas than their proofs. For the proofs, we
point the reader to a variety of sources for the sake of diversity. The presentation is by
default in a normed space X with dual X∗, though if statements become too technical we
will specialize to Euclidean space. E denotes a finite-dimensional real vector space Rn for
some n ∈N endowed with the usual norm. Typically, X will be reserved for a real infinite-
dimensional Banach space. A common convention in convex analysis is to include one or
both of −∞and +∞in the range of functions (typically only +∞). This is denoted by the
(semi-) closed interval (−∞,+∞] or [−∞,+∞].
A set C ⊂X is said to be convex if it contains all line segments between any two
points in C: λx + (−λ)y ∈C for all λ ∈[,] and x, y ∈C. Much of the theory
of convexity is centered on the analysis of convex sets, however, sets and functions are
treated interchangeably through the use of level sets, epigraphs, and indicator functions.
The lower level sets of a function f : X →[−∞,+∞] are denoted lev ≤α f and defined by
lev α f := {x ∈X ∣f (x) ≤α } where α ∈R. The epigraph of a function f : X →[−∞,+∞]
is defined by
epi f := {(x, t) ∈E × R ∣f (x) ≤t }.
This leads to the very natural definition of a convex function as one whose epigraph is a
convex set. More directly, a convex function is defined as a mapping f : X →[−∞,+∞]
with convex domain and
f (λx + (−λ)y) ≤λ f (x) + (−λ)f (y)
for any x, y ∈dom f and λ ∈[,].
A proper convex function f : X →[−∞,+∞] is strictly convex if the above inequality is
strict for all distinct x and y in the domain of f and all < λ < . A function is said to
be closed if its epigraph is closed; whereas a lower semi-continuous (lsc) function f satisfies
lim inf x→x f (x) ≥f (x) for all x ∈X. These properties are in fact equivalent:
Proposition 
The following properties of a function f : X →[−∞,+∞] are equivalent:
(i)
f is lsc.
(ii) epi f is closed in X × R.
(iii) The level sets lev ≤α f are closed on X for each α ∈R.


Duality and Convex Programming
Guide. For Euclidean spaces, this is shown in [, Theorem .]. In the Banach space
setting this is [, Proposition ..]. This is left as an exercise for the Hilbert space setting
in [, Exercise .].
∎
Our principal focus is on proper functions, that is, f : E →[−∞,+∞] with nonempty
domain. One passes from sets to functions through the indicator function
ιC(x) :=
⎧⎪⎪⎨⎪⎪⎩

x ∈C
+∞
else.
For C ⊂X convex, we may refer to f : C →[−∞,+∞] as a convex function if the extended
function
f (x) :=
⎧⎪⎪⎨⎪⎪⎩
f (x)
x ∈C
+∞
else
is convex.
..
Lipschitzian Properties
Convex functions have the remarkable, yet elementary, property that local bounded-
ness and local Lipschitz properties are equivalent without any additional assumptions on
the function. In the following statement of this fact, we denote the unit ball by BX :=
{x ∈X ∣∥x∥≤}.
Lemma 
Let f : X →(−∞,+∞] be a convex function and suppose that C ⊂X is a
bounded convex set. If f is bounded on C + δBX for some δ > , then f is Lipschitz on C.
Guide. See [, Lemma ..].
∎
With this fact, one can easily establish the following.
Proposition (Convexity and continuity in normed spaces)
Let f : X →(−∞,+∞] be
proper and convex, and let x ∈dom f . The following are equivalent:
(i)
f is Lipschitz on some neighborhood of x.
(ii)
f is continuous at x.
(iii)
f is bounded on a neighborhood of x.
(iv)
f is bounded above on a neighborhood of x.
Guide. See [, Proposition ..] or [, Sect. ..].
∎
In finite dimensions, convexity and continuity are much more tightly connected.
Proposition (Convexity and continuity in Euclidean spaces)
Let f : E →(−∞,+∞]
be convex. Then f is locally Lipschitz, and hence continuous, on the interior of its domain.
Guide. See [, Theorem ..] or [, Theorem ..]
∎

Duality and Convex Programming 

Unlike finite dimensions, in infinite dimensions a convex function need not be con-
tinuous. A Hamel basis, for instance, an algebraic basis for the vector space can be used
to define discontinuous linear functionals [, Exercise ..]. For lsc convex functions,
however, the correspondence follows through. The following statement uses the notion of
the core of a set given by Definition .
Example (A discontinuous linear functional)
Let cdenote the normed subspace of
all finitely supported sequences in c, the vector space of sequences in X converging to ;
obviously cis open. Define Λ : c→R by Λ(x) = ∑x j where x = (x j) ∈c. This is
clearly a linear functional and discontinuous at . Now extend Λ to a functional ̂Λ on the
Banach space cby taking a basis for cconsidered as a vector space over c. In particular,
C := ̂Λ−([−,]) is a convex set with empty interior for which is a core point. Moreover,
C = cand ̂Λ is certainly discontinuous.
∎
Proposition (Convexity and continuity in Banach spaces)
Suppose X is a Banach space
and f : X →(−∞,+∞] is lsc, proper and convex. Then the following are equivalent:
(i)
f is continuous at x.
(ii) x ∈int dom f .
(iii) x ∈core dom f .
Guide. This is [, Theorem ..]. See also [, Theorem ..].
∎
The above result is helpful since it is often easier to verify that a point is in the core of the
domain of a convex function than in the interior.
..
Subdiﬀerentials
The analog to the linear function in classical analysis is the sublinear function in convex
analysis. A function f : X →[−∞,+∞] is said to be sublinear if
f (λx + γy) ≤λ f (x) + γ f (y)
for all x, y ∈X and λ,γ ≥.
For this we use the convention that ⋅(+∞) = . Sometimes sublinearity is defined as a
function f that is positively homogeneous (of degree ) – i.e., ∈dom f and f (λx) = λ f (x)
for all x and all λ > – and is subadditive
f (x + y) ≤f (x) + f (y)
for all x and y.
Example (Norms)
A norm on a vector space is a sublinear function. Recall that a
nonnegative function ∥⋅∥on a vector space X is a norm if


Duality and Convex Programming
(i) ∥x∥≥for each x ∈X.
(ii) ∥x∥= if and only if x = .
(iii) ∥λx∥= ∣λ∥x∥for every x ∈X and scalar λ.
(iv) ∥x + y∥≤∥x∥+ ∥y∥for every x, y ∈X.
A normed space is a vector space endowed with such a norm and is called a Banach space
if it is complete which is to say that all Cauchy sequences converge.
∎
Another important sublinear function is the directional derivative of the function f at
x in the direction d defined by
f ′(x; d) := lim
t↘
f (x + td) −f (x)
t
whenever this limit exists.
Proposition (Sublinearity of the directional derivative)
Let X be a Banach space and
let f : X →(−∞,+∞] be a convex function. Suppose that x ∈core(dom f ). Then the
directional derivative f ′(x;⋅) is everywhere finite and sublinear.
Guide. See [, Proposition ..]. For the finite dimensional analog, see [, Proposi-
tion D...] or [, Proposition ..].
∎
Another important instance of sublinear functions are support functions of convex
sets, which, in turn permit local first-order approximations to convex functions. A sup-
port function of a nonempty subset S of the dual space X∗, usually denoted σS, is defined
by σS(x) := sup {⟨s, x⟩∣s ∈S }. The support function is convex, proper (not everywhere
infinite), and ∈dom σS.
Example (Support functions and Fenchel conjugation)
From the definition of the
support function it follows immediately that, for a closed convex set C,
ι∗
C = σC
and
ι∗∗
C = ιC.
∎
A powerful observation is that any closed sublinear function can be viewed as a support
function. To see this, we represent closed convex functions via affine minorants. This is the
content of the Hahn–Banach theorem, which we state in infinite dimensions as we will
need this below.
Theorem (Hahn–Banach: analytic form)
Let X be a normed space and σ : X →R be a
continuous sublinear function with dom σ = X. Suppose that L is a linear subspace of X and
that the linear function h : L →R is dominated by σ on L, that is σ ≥h on L. Then there is

Duality and Convex Programming 

a linear function minorizing σ on X, that is, there exists a x∗∈X∗dominated by σ such that
h(x) = ⟨x∗, x⟩≤σ(x) for all x ∈L.
Guide. The proof can be carried out in finite dimensions with elementary tools,
constructing x∗from h sequentially by one–dimensional extensions from L. See [,
Theorem C...] and [, Proposition ..]. The technique can be extended to Banach
spaces using Zorn’s lemma and a verification that the linear functionals so constructed
are continuous (guaranteed by the domination property) [, Theorem ..]. See also
[, Theorem .].
∎
An important point in the Hahn–Banach extension theorem is the existence of a
minorizing linear function, and hence the existence of the set of linear minorants. In fact,
σ is the supremum of the linear functions minorizing it. In other words, σ is the support
function of the nonempty set
Sσ := {s ∈X∗∣⟨s, x⟩≤σ(x)
for all x ∈X }.
A number of facts follow from Theorem , in particular the nonemptiness of the subdiffer-
ential, a sandwich theorem and, thence, Fenchel Duality (respectively Theorems , , and
). It turns out that the converse also holds, and thus these facts are actually equivalent
to nonemptiness of the subdifferential. This is the so-called Hahn–Banach/Fenchel duality
circle.
As stated in Proposition , the directional derivative is everywhere finite and sublinear
for a convex function f at points in the core of its domain. In light of the Hahn–Banach
theorem, we then can express f ′(x,⋅) for all d ∈X in terms of its minorizing function:
f ′(x, d) = σS(d) = max
v∈S {⟨v, d⟩}.
The set S for which f ′(x, d) is the support function has a special name: the subdifferential of
f at x. It is tempting to define the subdifferential this way, however there is a more elemental
definition that does not rely on directional derivatives or support functions, or indeed even
the convexity of f . We prove the correspondence between directional derivatives of convex
functions and the subdifferential below as a consequence of the Hahn–Banach theorem.
Deﬁnition (Subdifferential)
For a function f : X →(−∞,+∞] and a point x ∈dom f ,
the subdifferential of f at x, denoted ∂f (x) is defined by
∂f (x) := {v ∈X∗∣v(x) −v(x) ≤f (x) −f (x) for all x ∈X }.
When x ∉dom f we define ∂f (x) = Ø.
In Euclidean space the subdifferential is just
∂f (x) = {v ∈E ∣⟨v, x⟩−⟨v, x⟩≤f (x) −f (x) for all x ∈E }.
An element of ∂f (x) is called a subgradient of f at x. See [, , ] for more in-depth
discussion of the regular, or limiting subdifferential we have defined here, in addition to


Duality and Convex Programming
other useful varieties. This is a generalization of the classical gradient. Just as the gradient
need not exist, the subdifferential of a lsc convex function may be empty at some points in
its domain. Take, for example, f (x) = −
√
−xfor −≤x ≤. Then ∂f (x) = Ø for x = ±.
Example (Common subdifferentials)
(i) Gradients. A function f : X →R is said to be strictly differentiable at x if
lim
x→x,u→x
f (x) −f (u) −∇f (x)(x −u)
∥x −u∥
= .
This is a stronger differentiability property than Fréchet differentiability since it
requires uniformity in pairs of points converging to x. Luckily for convex functions
the two notions agree. If f is convex and strictly differentiable at x, then the subdiffer-
ential is exactly the gradient. (This follows from the equivalence of the subdifferential
in Definition and the basic limiting subdifferential defined in [, Definition .]
for convex functions and [, Corollary .].) In finite dimensions, at a point x ∈
dom f for f convex, Fréchet and Gâteaux differentiability coincide, and the subdif-
ferential is a singleton [, Theorem ..]. In infinite dimensions, a convex function
f that is continuous at x is Gâteaux differentiable at x if and only if the ∂f (x) is a
singleton [, Corollary ..].
(ii) The subdifferential of the indicator function.
∂ιC(x) = NC(x),
where C ⊂X is closed and convex, X is a Banach, and NC(x) ⊂X∗is the normal
cone mapping to C at x defined by
NC(x) :=
⎧⎪⎪⎨⎪⎪⎩
{v ∈X∗∣⟨v, x −x⟩≤
for all x ∈C }
x ∈C
Ø
x ∉C.
(.)
See ( > .) for alternative definitions and further discussion of this important
mapping.
(iii) Absolute value. For x ∈R,
∂∣⋅∣(x) =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
−
x < 
[−,]
x = 

x > .
∎
The following elementary observation suggests the fundamental significance of sub-
differential in optimization.

Duality and Convex Programming 

Theorem (Subdifferential at optimality: Fermat’s rule)
Let X be a normed space, and
let f : X →(−∞,+∞] be proper and convex. Then f has a (global) minimum at x if and
only if ∈∂f (x).
Guide. The first implication of the global result follows from a more general local result
[, Proposition .] by convexity; the converse statement follows from the definition of
the subdifferential and convexity.
∎
Returning now to the correspondence between the subdifferential and the directional
derivative of a convex function f ′(x; d) has the following fundamental result.
Theorem (Max formula – existence of ∂f )
Let X be a normed space, d ∈X and let
f : X →(−∞,+∞] be convex. Suppose that x ∈cont f . Then ∂f (x) ≠Ø and
f ′(x, d) = max {⟨x∗, d⟩∣x∗∈∂f (x)}.
Proof
The tools are in place for a simple proof that synthesizes many of the facts tabulated
so far. By Proposition f ′(x;⋅) is finite; so, for fixed d ∈{x ∈X ∣∥x∥= }, let α = f ′(x; d) <
∞. The stronger assumption that x ∈cont f and the convexity of f ′(x;⋅) yield that the
directional derivative is Lipschitz continuous with constant K. Let S := {td ∣t ∈R} and
define the linear function Λ : S →R by Λ(td) := tα for t ∈R. Then Λ(⋅) ≤f ′(x;⋅) on S.
The Hahn–Banach theorem then guarantees the existence of ϕ ∈X∗such that
ϕ = Λ on S,
ϕ(⋅) ≤f ′(x;⋅) on X.
Then ϕ ∈∂f (x) and ϕ(sd) = f ′(x; sd) for all s ≥.
∎
A simple example on R illustrates the importance of the qualification x ∈cont f . Let
f (x) : R →(−∞,+∞] :=
⎧⎪⎪⎨⎪⎪⎩
−√x,
x ≥
+∞
otherwise.
For this example, ∂f () = Ø.
An important application of the Max formula in finite dimensions is the mean value
theorem for convex functions.
Theorem (Convex mean value theorem)
Let f : E →(−∞,+∞] be convex and
continuous. For u,v ∈E there exists a point z ∈E interior to the line segment [u,v] with
f (u) −f (v) ≤⟨w, u −v⟩
for all w ∈∂f (z).
Guide. See [,] for extensions of this result and detailed historical background.
∎
The next theorem is a key tool in developing a subdifferential calculus. It relies on
assumptions that are used frequently enough that we present them separately.


Duality and Convex ProgrammingAssumption
Let X and Y be Banach spaces and let T : X →Y be a bounded linear
mapping. Let f : X →(−∞,+∞] and g : Y →(−∞,+∞] satisfy one of
∈core (dom g −T dom f ) and both f and g are lsc,
(.)
or
T dom f ∩cont g ≠Ø.
(.)
The later assumption can be used in incomplete normed spaces as well.
Theorem (Sandwich theorem)
Let X and Y be Banach spaces and let T : X →Y be a
bounded linear mapping. Suppose that f : X →(−∞,+∞] and g : Y →(−∞,+∞] are
proper convex functions with f ≥−g ○T and which satisfy Assumption . Then there is an
affine function A : X →R defined by Ax := ⟨T∗y∗, x⟩+ r satisfying f ≥A ≥−g ○T.
Moreover, for any x satisfying f (x) = (−g ○T)(x), we have −y∗∈∂g(Tx).
Guide. By our development to this point, we would use the Max formula [, Theorem
..] to prove the result. For a vector space version see [, Corollary .]. Another route
is via Fenchel duality which we explore in the next section. A third approach closely related
to the Fenchel duality approach [, Theorem ..] is based on a decoupling lemma which
is also presented in the next section (Lemma ).
∎
Corollary (Basic separation)
Let C ⊂X be a nonempty convex set with nonempty
interior in a normed space, and suppose x∉int C. Then there exists ϕ ∈X∗/{} such
that
sup
C
ϕ ≤ϕ(x)
and
ϕ(x) < ϕ(x) for all x ∈int C.
If x∉C then we may assume supC ϕ < ϕ(x).
Proof
Assume without loss of generality that ∈int C and apply the sandwich theorem
with f = ι{x}, T the identity mapping on X and g(x) = inf {r > ∣x ∈rC } −. See [,
Theorem ..] and [, Corollary ..].
∎
The Hahn–Banach theorem can be seen as an easy consequence of the sandwich
theorem , which completes part of the circle. > Figure -illustrates these ideas.
In the next section we will add Fenchel duality to this cycle. Before doing so, we
finish with a calculus of subdifferentials and a few fundamental results connecting the
subdifferential to classical derivatives and monotone operators.
Theorem (Subdifferential sum rule)
Let X and Y be Banach spaces, T : X →Y a
bounded linear mapping and let f : X →(−∞,+∞] and g : Y →(−∞,+∞] be convex
functions. Then at any point x ∈X we have
∂(f + g ○T) (x) ⊃∂f (x) + T∗(∂g(Tx)),
with equality if Assumption holds.

Duality and Convex Programming 

Success
Failure
(a)
(b)
epi f
hyp g
⊡Fig. -
Hahn–Banach sandwich theorem and its failure
Proof sketch. The inclusion is clear. Proving equality permits an elegant proof using the
sandwich theorem [, Theorem ..], which we sketch here. Take ϕ ∈∂(f + g ○T)(x)
and assume without loss of generality that
x ↦f (x) + g(Tx) −ϕ(x)
attains a minimum of at x. By Theorem there is an affine function A := ⟨T∗y∗, ⋅⟩+ r
with −y∗∈∂g(Tx) such that
f (x) −ϕ(x) ≥Ax ≥−g(Ax).
Equality is attained at x = x. It remains to check that ϕ + T∗y∗∈∂f (x).
∎
The next result is a useful extension to Proposition .
Theorem (Convexity and regularity in normed spaces)
Let f : X →(−∞,+∞] be
proper and convex, and let x ∈dom f . The following are equivalent:
(i)
f is Lipschitz on some neighborhood of x.
(ii)
f is continuous at x.
(iii)
f is bounded on a neighborhood of x.
(iv)
f is bounded above on a neighborhood of x.
(v) ∂f maps bounded subsets of X into bounded nonempty subsets of X∗.
Guide. See [, Theorem ..].
∎
The next results relate to Example and provide additional tools for verifying differen-
tiability of convex functions. The notation →w∗denotes weak∗convergence.


Duality and Convex Programming
Theorem (Šmulian)
Let the convex function f be continuous at x.
(i) The following are equivalent:
(a)
f is Fréchet differentiable at x.
(b) For each sequence xn →x and ϕ ∈∂f (x), there exist n ∈N and ϕn ∈∂f (xn) for
n ≥n such that ϕn →ϕ.
(c) ϕn →ϕ whenever ϕn ∈∂f (xn), ϕ ∈∂f (x).
(ii) The following are equivalent:
(a)
f is Gâteaux differentiable at x.
(b) For each sequence xn →x and ϕ ∈∂f (x), there exist n ∈N and ϕn ∈∂f (xn) for
n ≥n such that ϕn →w∗ϕ.
(c) ϕn →w∗ϕ whenever ϕn ∈∂f (xn), ϕ ∈∂f (x).
A more complete statement of these facts and their provenance can be found in [, The-
orems ..and ..]. In particular, in every infinite dimensional normed space, there is a
continuous convex function which is Gâteaux but not Fréchet differentiable at the origin.
An elementary but powerful observation about the subdifferential viewed as a multi-
valued mapping will conclude this section. A multi-valued mapping T from X to X∗is
denoted with double arrows, T : X ⇉X∗. Then T is monotone if
⟨v−v, x−x⟩≥
whenever v∈T(x), v∈T(x).
Proposition (Monotonicity and convexity)
Let f : X →(−∞,+∞] be proper and
convex on a normed space. Then the subdifferential mapping ∂f : X ⇉X∗is monotone.
Proof. Add the subdifferential inequalities in the Definition applied to f (x) and
f (x) for v∈∂f (x) and v∈∂(f (x).
∎
.
Duality and Convex Analysis
The Fenchel conjugate is to convex analysis what the Fourier transform is to harmonic
analysis. We begin by collecting some basic facts about this fundamental tool.
..
Fenchel Conjugation
The Fenchel conjugate, introduced in [], of a mapping f : X →[−∞,+∞], as mentioned
above is denoted f ∗: X∗→[−∞,+∞] and defined by
f ∗(x∗) = sup
x∈X
{⟨x∗, x⟩−f (x)}.

Duality and Convex Programming 

The conjugate is always convex (as a supremum of affine functions). If the domain of f is
nonempty, then f ∗never takes the value −∞.
Example (Important Fenchel conjugates)
(i) Absolute value.
f (x) = ∣x∣
(x ∈R),
f ∗(y) =
⎧⎪⎪⎨⎪⎪⎩

y ∈[−,]
+∞
else.
(ii) Lp norms (p > ).
f (x) = 
p∥x∥p
(p > ),
f ∗(y) = 
q∥y∥q
( 
p + 
q = ).
In particular, note that the two-norm conjugate is “self-conjugate.”
(iii) Indicator functions.
f = ιC,
f ∗= σC,
where σC is the support function of the set C. Note that if C is not closed and convex,
then the conjugate of σC, that is the biconjugate of ιC, is the closed convex hull of C.
(See Proposition (ii).)
(iv) Boltzmann–Shannon entropy.
f (x) =
⎧⎪⎪⎨⎪⎪⎩
x ln x −x
(x > )

(x = )
,
f ∗(y) = e y
(y ∈R).
(v) Fermi–Dirac entropy.
f (x) =
⎧⎪⎪⎨⎪⎪⎩
x ln x + (−x)ln(−x)
(x ∈(,))

(x = ,)
,
f ∗(y) = ln(+ e y)
(y ∈R).
∎
Some useful properties of conjugate functions are tabulated below.
Proposition (Fenchel–Young inequality)
Let X be a normed space and let f : X →
[−∞,+∞]. Suppose that x∗∈X∗and x ∈dom f . Then
f (x) + f ∗(x∗) ≥⟨x∗, x⟩.
(.)
Equality holds if and only if x∗∈∂f (x).


Duality and Convex Programming
Proof sketch. The proof follows by an elementary application of the definitions of the
Fenchel conjugate and the subdifferential. See [] for the finite dimensional case. The same
proof works in the normed space setting.
∎
The conjugate, as the supremum of affine functions, is convex. In the following, we
denote the closure of a function f by f , and we let conv f be the function whose epigraph
is the closed convex hull of the epigraph of f .
Proposition 
Let X be a normed space and let f : X →[−∞,+∞].
(i) If f ≥g then g∗≥f ∗.
(ii)
f ∗= (f )
∗= (conv f )∗.
Proof
The definition of the conjugate immediately implies (i). This immediately yields
f ∗≤(f )
∗≤(conv f )∗. To show (ii) it remains to show that f ∗≥(conv f )∗. Choose any
ϕ ∈X∗. If f ∗(ϕ) = +∞the conclusion is clear, so assume f ∗(ϕ) = α for some α ∈R. Then
ϕ(x)−f (x) ≤α for all x ∈X. Define g := ϕ−f . Then g ≤conv f and, by (i) (conv f )∗≤g∗.
But g∗= α, so (conv f )∗≤α = f ∗(ϕ).
∎
Application of Fenchel conjugation twice, or biconjugation denoted by f ∗∗, is a func-
tion on X∗∗. In certain instances, biconjugation is the identity – in this way, the Fenchel
conjugate resembles the Fourier transform. Indeed, Fenchel conjugation plays a role in
the convex analysis similar to the Fourier transform in harmonic analysis and has a
contemporaneous provenance dating back to Legendre.
Proposition (Biconjugation)
Let f : X →(−∞,+∞], x ∈X and x∗∈X∗.
(i)
f ∗∗∣X ≤f .
(ii) If f is convex and proper, then f ∗∗(x) = f (x) at x if and only if f is lsc at x. In
particular, f is lsc if and only if f ∗∗
X
= f .
(iii)
f ∗∗∣X = conv f if conv f is proper.
Guide. (i) follows from Fenchel–Young, Proposition , and the definition of the conju-
gate. (ii) follows from (i) and an epi-separation property [, Proposition ..]. (iii) follows
from (ii) of this proposition and (ii).
∎
The next results highlight the relationship between the Fenchel conjugate and the
subdifferential that we have already made use of in (>.).
Proposition 
Let f : X →(−∞,+∞] be a function and x ∈dom f . If ϕ ∈∂f (x)
then x ∈∂f ∗(ψ). If, additionally, f is convex and lsc at x, then the converse holds, namely
x ∈∂f ∗(ϕ) implies ϕ ∈∂f (x).

Duality and Convex Programming 

Guide. See [, Corollary ..] for the finite dimensional version of this fact that, with
some modification, can be extended to normed spaces.
∎
To close this subsection we introduce infimal convolutions. Among their many applica-
tions are smoothing and approximation – just as is the case for integral convolutions.
Deﬁnition (Infimal convolution)
Let f and g be proper extended real-valued functions
on a normed space X. The infimal convolution of f and g is defined by
(f ◻g)(x) := inf
y∈X f (y) + g(x −y).
The infimal convolution of f and g is the largest extended real-valued function whose
epigraph contains the sum of epigraphs of f and g; consequently, it is a convex function
when f and g are convex.
The next lemma follows directly from the definitions and careful application of the
properties of suprema and infima.
Lemma 
Let X be a normed space and let f and g be proper functions on X, then
(f ◻g)∗= f ∗+ g∗.
An important example of infimal convolution is Yosida approximation.
Theorem (Yosida approximation)
Let f : X →R be convex and bounded on bounded
sets. Then both f ◻n∥⋅∥and f ◻n∥⋅∥converge uniformly to f on bounded sets.
Guide. This follows from the above lemma and basic approximation facts.
∎
In the inverse problems literature (f ◻n∥⋅∥)() is often referred to as Tikhonov regu-
larization; elsewhere, f ◻n∥⋅∥is referred to as Moreau–Yosida regularization because
f ◻
∥⋅∥, the Moreau envelope, was studied in depth by Moreau [, ]. The argmin
mapping corresponding to the Moreau envelope – that is the mapping of x ∈X to the
point y ∈X at which the value of f ◻
∥⋅∥is attained – is called the proximal mapping
[,,]
proxλ, f (x) := argmin y∈X f (y) + 
λ∥x −y∥.
(.)
When f is the indicator function of a closed convex set C, the proximal mapping is just the
metric projection onto C, denoted by PC(x): proxλ,ιC (x) = PC(x).
..
Fenchel Duality
Fenchel duality can be proved by Theorem and the sandwich theorem [, Theorem
..]. According to our development, this places Fenchel duality as a consequence of


Duality and Convex Programming
the Hahn–Banach theorem. In order to close the Fenchel duality/Hahn–Banach circle of
ideas, however, following [] we prove the main duality result of this section using the
Fenchel–Young inequality and the next important lemma.
Lemma (Decoupling)
Let X and Y be Banach spaces and let T : X →Y be a bounded
linear mapping. Suppose that f : X →(−∞,+∞] and g : Y →(−∞,+∞] are proper
convex functions which satisfy Assumption . Then there is a y∗∈Y∗such that for any x ∈X
and y ∈Y,
p ≤(f (x) −⟨y∗, Tx⟩) + (g(y) + ⟨y∗, y⟩),
where p := inf X{f (x) + g(Tx)}.
Guide. Define the perturbed function h : Y →[−∞,+∞] by
h(u) := inf
x∈X{f (x) + g(Tx + u)},
which has the property that h is convex, dom h = dom g −T dom f and (the most tech-
nical part of the proof) ∈int (dom h). This can be proved by assuming the first of the
constraint qualifications (>.). The second condition (>.) implies (>.). Then by
Theorem , we have ∂h() ≠Ø, which guarantees the attainment of a minimum of the per-
turbed function. The decoupling is achieved through a particular choice of the perturbation
u. See [, Lemma ..].
∎
One can now provide an elegant proof of Theorem , which is restated here for
convenience.
Theorem (Fenchel duality)
Let X and Y be normed spaces, consider the functions f :
X →(−∞,+∞] and g : Y →(−∞,+∞] and let T : X →Y be a bounded linear map.
Define the primal and dual values p, d ∈[−∞,+∞] by the Fenchel problems
p = inf
x∈X{f (x) + g(Tx)}
(.)
d = sup
y∗∈Y∗{−f ∗(T∗y∗) −g∗(−y∗)}.
(.)
These values satisfy the weak duality inequality p ≥d.
If X, Y are Banach, f , g are convex and satisfy Assumption then p = d, and the
supremum to the dual problem is attained if finite.
Proof
Weak duality follows directly from the Fenchel–Young inequality.
For equality assume that p ≠−∞(this case is clear). Then Assumption guarantees
that p < +∞, and by the decoupling lemma (Lemma ), there is a ϕ ∈Y∗such that for all
x ∈X and y ∈Y
p ≤(f (x) −⟨ϕ, Tx⟩) + (g(y) −⟨−ϕ, y⟩).
Taking the infimum over all x and then over all y yields
p ≤−f ∗(T∗, ϕ) −g∗(−ϕ) ≤d ≤p.

Duality and Convex Programming 

Hence, ϕ attains the supremum in (>.), and p = d.
∎
Fenchel duality for linear constraints, Corollary , follows immediately by taking g :=
ι{b}.
..
Applications
Calculus. Fenchel duality is, in some sense, the dual space representation of the sand-
wich theorem. It is a straightforward exercise to derive Fenchel duality from Theorem
. Conversely, the existence of a point of attainment in Theorem yields an explicit
construction of the linear mapping in Theorem : A := ⟨T∗ϕ, ⋅⟩+ r, where ϕ is the
point of attainment in (>.) and r ∈[a, b] where a := inf x∈X f (x) −⟨T∗ϕ, x⟩and
b := supz∈X −g(Tz) −⟨T∗ϕ, z⟩. One could then derive all the theorems using the sand-
wich theorem, in particular the Hahn–Banach theorem and the subdifferential sum
rule, Theorem , as consequences of Fenchel duality instead. This establishes the Hahn–
Banach/Fenchel duality circle: Each of these facts is equivalent and easily interderivable
with the nonemptiness of the subgradient of a function at a point of continuity.
An immediate consequence of Fenchel duality is a calculus of polar cones. Define the
negative polar cone of a set K in a Banach space X by
K−= {x∗∈X∗∣⟨x∗, x⟩≤∀x ∈K }.
(.)
An important example of a polar cone that we have seen in the applications is the normal
cone of a convex set K at a point x ∈K, defined by (>.). Note that
NK(x) := (K −x)−.
(.)
Corollary (Polar cone calculus)
Let X and Y be Banach spaces and K ⊂X and H ⊂Y
be cones, and let A : X →Y be a bounded linear map. Then
K−+ A∗H−⊂(K + A−H)
−
where equality holds if K and H are closed convex cones which satisfy H −AK = Y.
This can be used to easily establish the normal cone calculus for closed convex sets Cand
Cat a point x ∈C∩C
NC∩C(x) ⊃NC(x) + NC(x)
with equality holding if, in addition, ∈core(C−C) or C∩int C≠Ø.
Optimalityconditions. Another important consequence of these ideas is the Pshenichnyi–
Rockafellar [,] condition for optimality for nonsmooth constrained optimization.
Theorem (Pshenichnyi–Rockafellar conditions)
Let X be a Banach space, let C ⊂X
be closed and convex, and let f : X →(−∞,+∞] be a convex function. Suppose that either


Duality and Convex Programming
int C ∩dom f ≠Ø and f is bounded below on C, or C ∩cont f ≠Ø. Then there is an affine
function α ≤f with infC f = inf C α. Moreover, x is a solution to
(P)
minimize
x∈X
f (x)
subject to
x ∈C
if and only if
∈∂f (x) + NC(x).
Guide. Apply the subdifferential sum rule to f + ιC at x.
∎
A slight generalization extends this to linear constraints
(Plin)
minimize
x∈X
f (x)
subject to
Tx ∈D
Theorem (First-order necessary and sufficient)
Let X and Y be Banach spaces with
D ⊂Y convex, and let f : X →(−∞,+∞] be convex and T : X →Y a bounded linear
mapping. Suppose further that one of the following holds:
∈core(D −T dom f ), D is closed and f is lsc,
(.)
or
T dom f ∩int (D) ≠Ø.
(.)
Then the feasible set C := {x ∈X ∣Tx ∈D} satisfies
∂(f + ιC)(x) = ∂f (x) + T∗(ND(Tx))
(.)
and x is a solution to (Plin) if and only if
∈∂f (x) + T∗(ND(Tx)).
(.)
A point y∗∈Y∗satisfying T∗y∗∈−∂f (x) in Theorem is a Lagrange multiplier.
Lagrangian duality. We limit the setting to Euclidean space and consider the general
convex program
(Pcvx)
minimize
x∈E
f(x)
subject to
fj(x) ≤
(j = ,, . . ., m)
where the functions fj for j = ,,, . . . , m are convex and satisfy
m
⋂
j=
dom fj ≠Ø.
(.)
Define the Lagrangian L : E × Rm+ →(−∞,+∞] by
L(x, λ) := f(x) + λTF(x),
where F := (f, f, . . . , fm)T. A Lagrange multiplier in this context is a vector λ ∈Rm+ for
a feasible solution x if x minimizes the function L(⋅, λ) over E and λ satisfies the so-called

Duality and Convex Programming 

complimentary slackness conditions: λj = whenever fj(x) < . On the other hand, if x is
feasible for the convex program (Pcvx) and there is a Lagrange multiplier, then x is opti-
mal. Existence of the Lagrange multiplier is guaranteed by the following Slater constraint
qualification first introduced in the s.
Assumption (Slater constraint qualification)
There exists an ̂x ∈dom fwith fj(̂x) < 
for j = ,, . . . , m.
Theorem (Lagrangian necessary conditions)
Suppose that x ∈dom fis optimal for
the convex program (Pcvx) and that Assumption holds. Then there is a Lagrange multiplier
vector for x.
Guide. See [, Theorem ..].
∎
Denote the optimal value of (Pcvx) by p. Note that, since
sup
λ∈Rm +
L(x, λ) =
⎧⎪⎪⎨⎪⎪⎩
f (x)
if x ∈dom f
+∞
otherwise,
then
p = inf
x∈E sup
λ∈Rm +
L(x, λ).
(.)
It is natural, then to consider the problem
d = sup
λ∈Rm +
inf
x∈E L(x, λ)
(.)
where d is the dual value. It follows immediately that p ≥d. The difference between d and
p is called the duality gap. The interesting problem is to determine when the gap is zero,
that is, when d = p.
Theorem (Dual attainment)
If Assumption holds for the convex programming prob-
lem (Pcvx), then the primal and dual values are equal and the dual value is attained if
finite.
Guide. For a more detailed treatment of the theory of Lagrangian duality see [,
Sect. .].
∎
..
Optimality and Lagrange Multipliers
In the previous sections, we introduced duality theory via the Hahn–Banach/Fenchel dual-
ity circle of ideas to provide many entry points to the theory of convex and variational
analysis. For our purposes, however, the real significance of duality lies with its power to
illuminate duality in convex optimization, not only as a theoretical phenomenon but also
as an algorithmic strategy.


Duality and Convex Programming
In order to get to optimality criteria and the existence of solutions to convex optimiza-
tion problems, we turn our focus to the approximation of minima, or more generally the
regularity and well-posedness of convex optimization problems. Due to its reliance on the
Slater constraint qualification ( > .), Theorem is not adequate for problems with
equality constraints:
(Peq)
minimize
x∈S
f(x)
subject to
F(x) = 
for S ⊂E closed and F : E →Y a Fréchet differentiable mapping between the Euclidean
spaces E and Y.
More generally, we consider problems of the form
(PE)
minimize
x∈S
f(x)
subject to
F(x) ∈D
(.)
for E and Y Euclidean spaces, and S ⊂E and D ⊂Y are convex but not necessarily with
nonempty interior.
Example (Simple Karush Kuhn–Tucker)
For linear optimization problems, relatively
elementary linear algebra is all that is needed to assure the existence of Lagrange multipli-
ers. Consider
(PE)
minimize
x∈S
f(x)
subject to
fj(x) ∈D j,
j = ,, . . . , m
for fj : Rn →R (j = ,,, . . . , s) continuously differentiable, fj : Rn →R (j = s +
, . . . , m) linear. Suppose S ⊂E is closed and convex, while Di := (−∞,] for j = ,, . . . , s
and D j := {} for j = s + , . . . , m.
Theorem 
Denote by f ′
J(x) the submatrix of the Jacobian of (f, . . . , fs)T (assuming this
is defined at x) consisting only of those f ′
j for which fj(x) = . In other words, f ′
J(x) is the
Jacobian of the active inequality constraints at x. Let x be a local minimizer for (PE) at
which fj are continuously differentiable (j = ,, . . . , s) and the matrix
( f ′
J(x)
A
)
(.)
is full-rank where A := (∇fs+, . . . ,∇fm)T. Then there are λ ∈Rs and μ ∈Rm satisfying
λ ≥.
(.a)
(f(x), . . . , fs(x))λ = .
(.b)
f ′
(x) +
s
∑
j=
λj f ′
j (x) + μTA = .
(.c)
Guide. An elegant and elementary proof is given in [].
∎

Duality and Convex Programming 

For more general constraint structure, regularity of the feasible region is essential for
the normal cone calculus which plays a key role in the requisite optimality criteria. More
specifically, we consider the following constraint qualification.
Assumption (Basic constraint qualification)
y = (, . . .,) is the only solution in ND(F(x)) to ∈∇FT(x)y + NS(x).
Theorem (Optimality on sets with constraint structure)
Let
C = {x ∈S ∣F(x) ∈D}
for F = (f, f, . . . , fm) : E →Rm with fj continuously differentiable (j = ,,⋯, m), S ⊂E
closed, and for D = D× D× ⋯Dm ⊂Rm with D j closed intervals (j = ,,⋯, m). Then for
any x ∈C at which Assumption is satisfied one has
NC(x) = ∇FT(x)ND(F(x)) + NS(x).
(.)
If, in addition, fis continuously differentiable and x is a locally optimal solution to (PE)
then there is a vector y ∈ND(F(x)), called a Lagrange multiplier such that ∈∇f(x) +
∇FT(x)y + NS(x).
Guide. See [, Theorems .and .].
∎
..
Variational Principles
The Slater condition ( > .) is an interiority condition on the solutions to optimization
problems. Interiority is just one type of regularity required of the solutions, wherein one
is concerned with the behavior of solutions under perturbations. The next classical result
lays the foundation for many modern notions of regularity of solutions.
Theorem (Ekeland’s variational principle)
Let (X, d) be a complete metric space and
let f : X →(−∞,+∞] be a lsc function bounded from below. Suppose that є > and z ∈X
satisfy
f (z) < inf
X f + є.
For a given fixed λ > , there exists y ∈X such that
(i) d(z, y) ≤λ.
(ii)
f (y) + є
λ d(z, y) ≤f (z).
(iii)
f (x) + є
λ d(x, y) > f (y), for all x ∈X/{y}.
Guide. For a proof see [].
∎


Duality and Convex Programming
An important application of Ekeland’s variational principle is to the theory of subd-
ifferentials. Given a function f : X →(−∞,+∞], a point x∈dom f and є ≥, the
є-subdifferential of f at xis defined by
∂є f (x) = {ϕ ∈X∗∣⟨ϕ, x −x⟩≤f (x) −f (x) + є, ∀x ∈X }.
If x∉dom f then by convention ∂є f (x) := Ø. When є = we have ∂є f (x) = ∂f (x). For
є > the domain of the є-subdifferential coincides with dom f when f is a proper convex
lsc function.
Theorem (Brønsted–Rockafellar)
Suppose f is a proper lsc convex function on a
Banach space X. Then given any x∈dom f ,є > , λ > and w∈∂є f (x) there exist
x ∈dom f and w ∈X∗such that
w ∈∂f (x),
∥x −x∥≤є/λ
and
∥w −w∥≤λ.
In particular, the domain of ∂f is dense in dom f .
Guide. Define g(x) := f (x)−⟨w, x⟩on X, a proper lsc convex function with the same
domain as f . Then g(x) ≤inf X g(x) + є. Apply Theorem to yield a nearby point y that
is the minimum of a slightly perturbed function, g(x)+ λ∥x −y∥. Define the new function
h(x) := λ∥x −y∥−g(y),so that h(x) ≤g(x) for all X. The sandwich theorem (Theorem )
establishes the existence of an affine separator α + ϕ which is used to construct the desired
element of ∂f (x).
∎
A nice application of Ekeland’s variational principle provides an elegant proof of Klee’s
problem in Euclidean spaces []: Is every ˘Ceby˘cev set C convex? Here, a ˘Ceby˘cev set is
one with the property that every point in the space has a unique best approximation in C.
A famous result is as follows.
Theorem 
Every ˘Ceby˘cev set in a Euclidean space is closed and convex.
Guide. Since, for every finite dimensional Banach space with smooth norm, approxi-
mately convex sets are convex, it suffices to show that C is approximately convex, that is, for
every closed ball disjoint from C there is another closed ball disjoint from C of arbitrarily
large radius containing the first. This follows from the mean value theorem and Theorem
. See [, Theorem ..]. It is not known whether the same holds for Hilbert space.
∎
..
Fixed Point Theory and Monotone Operators
Another application of Theorem is Banach’s fixed point theorem.

Duality and Convex Programming 

Theorem 
Let (X, d) be a complete metric space and let ϕ : X →X . Suppose there is a
γ ∈(,) such that d(ϕ(x), ϕ(y)) ≤γd(x, y) for all x, y ∈X. Then there is a unique fixed
point x ∈X such that ϕ(x) = x.
Guide. Define f (x) := d(x, ϕ(x)). Apply Theorem to f with λ = and є = −γ. The
fixed point x satisfies f (x) + єd(x, x) ≥f (x) for all x ∈X.
∎
The next theorem is a celebrated result in convex analysis concerning the maximality
of lsc proper convex functions. A monotone operator T on X is maximal if gph T cannot
be enlarged in X × X without destroying the monotonicity of T.
Theorem (Maximal monotonicity of subdifferentials)
Let f : X →(−∞,+∞] be a
lsc proper convex function on a Banach space. Then ∂f is maximal monotone.
Guide. The result was first shown by Moreau for Hilbert spaces [, Proposition .b],
and shortly thereafter extended to Banach spaces by Rockafellar [, ]. For a modern
infinite dimensional proof see [, ]. This result fails badly in incomplete normed spaces
[].
∎
Maximal monotonicity of subdifferentials of convex functions lies at the heart of the
success of algorithms as this is equivalent to firm nonexpansiveness of the resolvent of the
subdifferential (I + ∂f )−[]. An operator T is firmly nonexpansive on a closed convex
subset C ⊂X when
∥Tx −Ty∥≤⟨x −y, Tx −Ty⟩
for all x, y ∈X.
(.)
T is just nonexpansive on the closed convex subset C ⊂X if
∥Tx −Ty∥≤∥x −y∥
for all x, y ∈C.
(.)
Clearly, all firmly nonexpansive operators are nonexpansive. One of the most longstanding
questions in geometric fixed point theory is whether a nonexpansive self-map T of a closed
bounded convex subset C of a reflexive space X must have a fixed point. This is known to
hold in Hilbert space.
.
Case Studies
One can now collect the dividends from the analysis outlined above for problems of the
form
minimize
x∈C⊂X
Iφ(x)
subject to
Ax ∈D
(.)
where X and Y are real Banach spaces with continuous duals X∗and Y∗, C and D are
closed and convex, A : X →Y is a continuous linear operator, and the integral functional
Iφ(x) := ∫T φ(x(t))μ(dt) is defined on some vector subspace Lp(T, μ) of X.


Duality and Convex Programming
..
Linear Inverse Problems with Convex Constraints
Suppose X is a Hilbert space, D = {b} ∈Rm and φ(x) := 
∥x∥. To apply Fenchel duality,
we rewrite (>.) using the indicator function
minimize
x∈X

∥x∥+ ιC(x)
subject to
Ax = b.
(.)
Note that the problem is posed on an infinite dimensional space, while the con-
straints (the measurements) are finite dimensional. Here we use of Fenchel duality to
transform an infinite dimensional problem into a finite dimensional problem. Let F :=
{x ∈C ⊂E ∣Ax = b } and let G denote the extensible set in E consisting of all measurement
vectors b for which F is nonempty. Potter and Arun show that the existence of y ∈Rm such
that b = APCA∗y is guaranteed by the constraint qualification b ∈ri G, where ri denotes
the relative interior [, Corollary ]. This is a special case of Assumption , which here
reduces to b ∈int A(C). Though at first glance the latter condition is more restrictive, it is
no real loss of generality since, if it fails, we restrict ourselves to range(A) which is closed.
Then it turns out that b ∈Aqri C, the image of the quasi-relative interior of C [, Exercise
..]. Assuming this holds Fenchel duality, Theorem , yields the dual problem
sup
y∈Rm ⟨b, y⟩−( 
∥⋅∥+ ιC)
∗(A∗y),
(.)
whose value is equivalent to the value of the primal problem. This is a finite dimen-
sional unconstrained convex optimization problem whose solution is characterized by the
inclusion (Theorem )
∈∂( 
∥⋅∥+ ιC)
∗(A∗y) −b.
(.)
Now from Lemma , Examples (ii) and (iii), and (>.),
( 
∥⋅∥+ ιC)
∗(x) = (σC ◻
∥⋅∥)(x) = inf
z∈X σC(z) + 
∥x −z∥.
The argmin of the Yosida approximation above (see Theorem ) is the proximal operator
(>.). Applying the sum rule for differentials, Theorem and Proposition yield
prox,σC(x) = argmin z∈X {σC(z) + 
∥z −x∥} = x −PC(x),
(.)
where PC is the orthogonal projection onto the set C. This together with (>.) yields the
optimal solution y to (>.):
b = APC(A∗y).
(.)
Note that the existence of a solution to (>.) is guaranteed by Assumption . This yields
the solution to the primal problem as x = PC(A∗y).
With the help of (>.), the iteration proposed in [] can be seen as a subgradient
descent algorithm for solving
inf
y∈Rm h(y) := σC(A∗y −PC(A∗y)) + 
∥PC(A∗y)∥−⟨b, y⟩.

Duality and Convex Programming 

The proposed algorithm is, given y∈Rm generates the sequence {yn}∞
n=by
yn+= yn −λ∂h(yn) = yn + λ (b −APCA∗yn).
For convergence results of this algorithm in a much larger context see [].
..
Imaging with Missing Data
This application is formally simpler than the previous example since there is no abstract
constraint set. As discussed in
> Sect. ..we consider relaxations to the conventional
problem
minimize
x∈Rn
Iφ∗
є,L(x)
subject to
Ax = b,
(.)
where
φ∗
є,L(x) =
є
ln() ln (xL/є + ) −xL −є.
(.)
Using Fenchel duality, the dual to this problem is the concave optimization problem
sup
y∈Rm yTb −Iφє,L(A∗y),
where
φє,L(x) := є ((L + x)ln(L + x) + (L −x)ln(L −x)
L ln()
−ln(L)
ln() )
L,є > x ∈[−L, L].
If there exists a point y satisfying b = AA∗y, then the optimal value in the dual problem
is attained and the primal solution is given by A∗y. The objective in the dual problem is
smooth and convex, so we could apply any number of efficient unconstrained optimization
algorithms. Also, for this relaxation, the same numerical techniques can be used for all
L →.
..
Inverse Scattering
Theorem 
Let X, Y be reflexive Banach spaces with duals X∗and Y∗. Let F : Y∗→Y
and G : X →Y be bounded linear operators with F = GSG∗for S : X∗→X a bounded
linear operator satisfying the coercivity condition
∣⟨φ, Sφ⟩∣≥c∥φ∥
X∗
for some c > and all φ ∈range(G∗) ⊂X∗.
Define h(ψ) : Y∗→(−∞,+∞] := ∣⟨ψ, Fψ⟩∣, and let h∗denote the Fenchel conjugate of h.
Then range(G) = dom h∗.


Duality and Convex Programming
Proof
Following [, Theorem .], we show that h∗(ϕ) = ∞for ϕ ∉range(G). To do
this we work with a dense subset of range G: G∗(C) for C := {ψ ∈Y∗∣⟨ψ, ϕ⟩= }. It was
shown in [, Theorem .] that G∗(C) is dense in range(G).
Now by the Hahn-Banach theorem there is a ̂ϕ ∈Y∗such that ⟨̂ϕ, ϕ⟩= . Since
G∗(C) is dense in range(G∗), there is a sequence {ψn}∞
n=⊂C with
G∗ψn →−G∗̂ϕ,
n →∞.
Now set ψn := ̂ψn + ̂ϕ. Then ⟨ϕ, αψn⟩= α and G∗(αψn) = αG∗ψn →for any α ∈R.
Using the factorization of F we have
∣⟨ψn, Fψn⟩∣= ∣⟨G∗ψn, SG∗ψn⟩∣≤∥S∥∥G∗ψn∥
X∗
hence α⟨ψn, Fψn⟩→as n →∞for all α, but ⟨ϕ, αψn⟩= α, that is, ⟨ϕ, αψn⟩−h(αψn) →
α and h∗(ϕ) = ∞.
∎
In the scattering application, we have a scatterer supported on a domain D ⊂Rm (m = 
or ) that is illuminated by an incident field. The Helmholtz equation models the behavior
of the fields on the exterior of the domain and the boundary data belongs to X = H/(Γ).
On the sphere at infinity the leading-order behavior of the fields, the so-called far field
pattern, lies in Y = L(S). The operator mapping the boundary condition to the far field
pattern – the data-to-pattern operator – is G : H/(Γ) →L(S). Assume that the far field
operator F : L(S) →L(S) has the factorization F = GS∗G∗, where S : H−/(Γ) →
H/(Γ) is a single layer boundary operator defined by
(Sφ)(x) := ∫Γ Φ(x, y)φ(y)ds(y), x ∈Γ,
for Φ(x, y) the fundamental solution to the Helmholtz equation. With a few results about
the denseness of G and the coercivity of S, which, though standard, we will not go into
here, we have the following application to inverse scattering.
Corollary (Application to inverse scattering)
Let D ⊂Rm (m = or ) be an open
bounded domain with connected exterior and boundary Γ. Let G : H/(Γ) →L(S), be the
data-to-pattern operator, S : H−/(Γ) →H/(Γ), the single layer boundary operator and
let the far field pattern F : L(S) →L(S) have the factorization F = GS∗G∗. Assume k
is not a Dirichlet eigenvalue of −△on D. Then range G = dom h∗where h(ψ) : L(S) →
(−∞,+∞] := ∣⟨ψ, Fψ⟩∣.
..
Fredholm Integral Equations
We showed in the introduction the failure of Fenchel duality for Fredholm integral equa-
tions. Here we briefly sketch a result on regularizations, or relaxations, that recovers duality
relationships. The result will show that by introducing a relaxation, we can recover the solu-
tion to ill-posed integral equations as the norm limit of solutions computable from a dual
problem of maximum entropy type.

Duality and Convex Programming 

Theorem ([], Theorem .)
Let X = L(T, μ) on a complete measure finite measure
space and let (Y,∥⋅∥) be a normed space. The infimum inf x∈X {Iφ(x) ∣Ax = b } is attained
when finite. In the case where it is finite, consider the relaxed problem for є > 
(Pє
MEP)
minimize
x∈X
Iφ(x)
subject to
∥Ax −b∥≤є.
Let pє denote the value of (Pє
MEP). The value of pє equals dє, the value of the dual
problem
(Pє
DEP)
maximize
y∗∈Y∗
⟨b, y∗⟩−є∥y∗∥∗−Iφ∗(A∗y∗),
and the unique optimal solution of (Pє
MEP) is given by
xφ,є := ∂φ∗
∂r (A∗y∗
є ),
where y∗
є is any solution to (Pє
DEP). Moreover, as є →+, xφ,є converges in mean to the
unique solution of (P
MEP) and pє →p.
Guide. Attainment of the infimum in inf x∈X {Iφ(x) ∣Ax = b } follows from strong
convexity of Iφ [, ]: strictly convex with weakly compact lower–level sets and with
the Kadec property, i.e., that weak convergence together with convergence of the func-
tion values implies norm convergence. Let g(y) := ιS(y) for S = {y ∈Y ∣b ∈y + єBY }
and rewrite (Pє
MEP) as inf {Iφ(x) + g(Ax) ∣x ∈X }. An elementary calculation shows
that the Fenchel dual to (Pє
MEP) is (Pє
DEP). The relaxed problem (Pє
MEP) has a con-
straint for which a Slater-type constraint qualification holds at any feasible point for
the unrelaxed problem. The value dє is thus attained and equal to pє. Subgradient
arguments following [] show that xφ,є is feasible for (Pє
MEP) and is the unique solu-
tion to (Pє
MEP). Convergence follows from weak compactness of the lower level set
L(p) := {x ∣Iφ(x) ≤p}, which contains the sequence (xφ,є)є>. Weak convergence
of xφ,є to the unique solution to the unrelaxed problem follows from strict convexity
of Iφ. Convergence of the function values and strong convexity of Iφ then yields norm
convergence.
∎
Notice that the dual in Theorem is unconstrained and easier to compute with, espe-
cially when there are finitely many constraints. This theorem remains valid for objectives
of the form Iφ(x) + ⟨x∗, x⟩for x∗in L∞(T). This enables one to apply them to many
Bregman distances, that is, integrands of the form ϕ(x) −ϕ(x) −⟨ϕ′(x), x −x⟩, where
ϕ is closed and convex on R.


Duality and Convex Programming
.
Open Questions
Regrettably, due to space constraints, we have omitted fixed point theory and many facts
about monotone operators that are useful in proving convergence of algorithms. How-
ever, it is worthwhile noting two long-standing problems that impinge on fixed point and
monotone operator theory.
. Klee’s problem: is every ˘Ceby˘cev set C in a Hilbert space convex?
. Must a nonexpansive self-map T of a closed bounded convex subset C of a reflexive
space X have a fixed point?
.
Conclusion
Duality and convex programming provides powerful techniques for solving a wide range
of imaging problems. While frequently a means toward computational ends, the dual per-
spective can also yield new insight into image processing problems and the information
content of data implicit in certain models. Five main applications illustrate the convex
analytical approach to problem solving and the use of duality: linear inverse problems
with convex constraints, compressive imaging, image denoising and deconvolution, non-
linear inverse scattering, and finally Fredholm integral equations. These are certainly not
exhaustive, but serve as good templates. The Hahn-Banach/Fenchel duality cycle of ideas
developed here not only provides a variety of entry points into convex and variational anal-
ysis, but also underscores duality in convex optimization as both a theoretical phenomenon
and an algorithmic strategy.
As readers of this volume will recognize, not all problems of interest are convex. But just
as nonlinear problems are approached numerically by sequences of linear approximations,
nonconvex problems can be approached by sequences of convex approximations. Con-
vexity is the central organizing principle and has tremendous algorithmic implications,
including not only computable guarantees about solutions, but efficient means toward
that end. In particular, convexity implies the existence of implementable, polynomial-time,
algorithms. This chapter is meant to be a foundation for more sophisticated methodologies
applied to more complicated problems.
.
Cross-References
> Compressive Sensing
> Inverse Scattering
> Iterative Solution Methods
> Numerical Methods for Variational Approach in Image Analysis
> Regularization Methods for III-Posed Problems

Duality and Convex Programming 

> Total Variation in Imaging
> Variational Approach in Image Analysis
> Variational Methods and Shape Spaces.
Acknowledgments
D. Russell Luke’s work was supported in part by NSF grants DMS-and
DMS-.
References and Further Reading
. Alves M, Svaiter BF () A new proof for max-
imal monotonicity of subdifferential operators.
J Convex Anal ():–
. Aubert G, Kornprost P () Mathematical
problems image processing in, volume of
applied mathematical sciences. Springer, New
York
. Auslender A, Teboulle M () Asymptotic
cones and functions in optimization and varia-
tional inequalities. Springer, New York
. Bauschke HH, Borwein JM () On projection
algorithms for solving convex feasibility prob-
lems. SIAM Rev ():–
. Bauschke HH, Combettes PL Convex Analy-
sis and Monotone Operator Theory in Hilbert
spaces. CMS books in mathematics. Springer,
New York, to appear
. Bauschke HH, Combettes PL, Luke DR ()
Phase retrieval, error reduction algorithm and
Fienup variants: a view from convex feasibility.
J Opt Soc Am A ():–
. Bauschke HH, Combettes PL, Luke DR ()
A hybrid projection
reflection
method for
phase retrieval.
J Opt
Soc Am
A
():
–
. Bauschke HH, Combettes PL, Luke DR ()
Finding best approximation pairs relative to two
closed convex sets in Hilbert spaces. J Approx
Theory :–
. Bect J, Blanc-Féraud L, Aubert G, Chambolle A
() A ℓ-unified variational framework for
image restoration. In Pajdla T, Matas J (eds) Pro-
ceedings of the Eighth European Conference on
Computer Vision, Prague, , volume of
Lecture Notes in Computer Science, Springer,
New York, pp –
. Ben-Tal A, Borwein JM, Teboulle M () A
dual approach to multidimensional lp spectral
estimation problems. SIAM J Contr Optim :
–
. Bonnans JF, Gilbert JC, Lemaréchal C, Sagas-
tizábal CA () Numerical optimization, nd
edn. Springer, New York
. Borwein JM () On the failure of maximum
entropy reconstruction for Fredholm equations
and other infinite systems. Math Program :
–
. Borwein JM, Lewis AS () Duality relation-
ships for entropy-like minimization problems.
SIAM J Contr Optim :–
. Borwein JM, Lewis AS () Convergence of best
entropy estimates. SIAM J Optim :–
. Borwein JM, Lewis AS () Convex analysis
and nonlinear optimization: theory and exam-
ples, nd edn. Springer, New York
. Borwein JM, Zhu QJ () Techniques of vari-
ational analysis. CMS books in mathematics.
Springer, New York
. Borwein JM, Lewis AS, Limber MN, Noll D
() Maximum entropy spectral analysis using
first order information. Part : a numerical algo-
rithm for fisher information duality. Numerische
Mathematik :–
. Borwein JM, Lewis AS, Noll D () Maximum
entropy spectral analysis using first order infor-
mation. Part : fisher information and convex
duality. Math Oper Res :–
. Borwein JM, Jon Vanderwerff J () Con-
vex functions: constructions, characterizations
and counterexamples, volume of Encyclope-
diasinmathematics.CambridgeUniversityPress,
New York


Duality and Convex Programming
. Boyd S, Vandenberghe L () Convex opti-
mization. Oxford University Press, New York
. Brezhneva
OA, Tret’yakov
AA, Wright
SE
()
A
simple
and
elementary
proof
of
the
KarushKuhnTucker
theorem
for
inequality-constrained
optimization.
Optim
Lett :–
. Burg JP () Maximum entropy spectral anal-
ysis. Paper presented at the th Meeting of the
Society of Exploration Geophysicists, Oklahoma
City
. Burke JV, Luke DR () Variational analysis
applied to the problem of optical phase retrieval.
SIAM J Contr Optim ():–
. Byrne CL () Signal processing: a mathemat-
ical approach. AK Peters, Natick, MA
. Candes E, Tao T () Near-optimal signal
recovery from random projections: universal
encoding strategies? IEEE Trans Inform Theory
():–
. Censor Y, Zenios SA () Parallel optimiza-
tion: theory algorithms and applications. Oxford
University Press, Oxford
. Chambolle A () An algorithm for total varia-
tionminimizationandapplications.J Math Imag-
ing Vis :–
. Chambolle A, Lions PL () Image recovery
viatotal variationminimizationandrelatedprob-
lems. Numer Math :–
. Chan TF, Golub GH, Mulet P () A non-
linear primal-dual method for total variation
based image restoration. SIAM J Sci Comput
():–
. Chen SS, Donoho DL, Saunders MA ()
Atomic decomposition by basis pursuit. SIAM J
Sci Comput ():–
. Clarke FH () Optimization and nonsmooth
analysis, volume of Classics in applied mathe-
matics. SIAM, Philadelphia
. Clarke FH, Stern RJ, Ledyaev YuS, Wolenski PR
() Nonsmooth analysis and control theory.
Springer, New York
. Combettes PL () The convex feasibility prob-
lem in image recovery. In: Hawkes PW (ed)
Advances in imaging and electron physics, vol .
Academic, New York, pp –
. Combettes PL, D˜ung D, V˜u BC () Dual-
ization of signal recovery problems. Technical
report, arXiv:.v[math.OC]
. Combettes PL, Trussell HJ () Method of suc-
cessive projections for finding a common point
of sets in metric spaces. J Optimiz Theory App
():–
. Combettes PL, Wajs VR () Signal recovery
by proximal forward-backward splitting. SIAM J
Multiscale Model Simul ():–
. Dacunha-Castelle D, Gamboa F () Max-
imum d’entropie et probléme des moments.
l’Institut Henri Poincaré :–
. Destuynder P, Jaoua M, Sellami H () A dual
algorithm for denoising and preserving edges
in image processing. J Inverse Ill-Posed Probl
:–
. Deutsch F () Best approximation in inner
product spaces. CMS books in mathematics.
Springer, New York
. Donoho DL, Johnstone IM () Ideal spa-
tial adaptation by wavelet shrinkage. Biometrika
():–
. Eggermont PPB () Maximum entropy regu-
larization for Fredholm integral equations of the
first kind. SIAM J Math Anal ():–
. Ekeland I, Temam R () Convex analysis and
variational problems. Elsevier, New York
. Fenchel W() Onconjugateconvexfunctions.
Canadian J Math :
. Goodrich RK, Steinhardt A () Lspectral
estimation. SIAM J Appl Math :–
. Groetsch CW () The theory of Tikhonov reg-
ularization for Fredholm integral equations of the
first kind. Pitman, Bostan
. Groetsch CW () Stable approximate eval-
uation of unbounded operators, volume 
of Lecture notes in mathematics. Springer,
New York
. Hintermüller M, Stadler G () An infeasi-
ble primal-dual algorithm for total bounded
variation-based
inf-convolution-type
image
restoration. SIAM J Sci Comput :–
. Hiriart-Urruty J-B, Lemaréchal C () Con-
vex analysis and minimization algorithms, I
and II, volume –of Grundlehren der
mathematischen
Wissenschaften.
Springer,
New York
. Hiriart-Urruty J-B, Lemaréchal C () Fun-
damentals
of convex analysis.
Grundlehren
der mathematischen Wissenschaften. Springer,
New York

Duality and Convex Programming 

. Iusem AN, Teboulle M () A regularized dual-
based iterative method for a class of image recon-
struction problems. Inverse Probl :–
. Kirsch A, Grinberg N () The factorization
method for inverse problems. Number in
Oxford Lecture Series in Mathematics and
its
Applications.
Oxford
University
Press,
New York
. Klee V () Convexity of Cebysev sets. Math
Annalen :–
. KressR() Linear integral equations,nd edn.
volume of Applied mathematical sciences.
Springer, New York
. Levi L () Fitting a bandlimited signal to
given points. IEEE Trans Inform Theory :
–
. Lewis AS, Luke DR, Malick J () Local linear
convergence of alternating and averaged projec-
tions. Found Comput Math ():–
. LewisAS,Malick J () Alternatingprojections
on manifolds. Math Oper Res : –
. Lucchetti R () Convexity and well-posed
problems,volumeof CMSbooksinmathemat-
ics. Springer, New York
. Luenberger DG, Ye Y () Linear and nonlin-
ear programming, rd edn. Springer, New York
. Luke DR () Relaxed averaged alternating
reflections for diffraction imaging. Inverse Probl
:–
. Luke DR () Finding best approximation
pairs relative to a convex and a prox-regular set
in Hilbert space. SIAM J Optim ():–
. Luke DR, Burke JV, Lyon RG () Optical
wavefront reconstruction: theory and numerical
methods. SIAM Rev :–
. Maréchal P, Lannes A () Unification of some
deterministic and probabilistic methods for the
solution of inverse problems via the principle of
maximum entropy on the mean. Inverse Probl
:–
. Minty GJ () Monotone (nonlinear) operators
in Hilbert space. Duke Math J ():–
. Mordukhovich BS () Variational analysis
and generalized differentiation, I: basic theory; II:
applications. Grundlehren der mathematischen
Wissenschaften. Springer, New York
. Moreau JJ () Fonctions convexes duales et
points proximaux dans un espace Hilbertien.
Comptes Rendus de l’Académie des Sciences de
Paris :–
. Moreau JJ () Proximité et dualité dans un
espace Hilbertian. Bull de la Soc math de France
():–
. Nesterov YE, Nemirovskii AS () Interior-
point polynomial algorithms in convex program-
ming. SIAM, Philadelphia
. Nocedal J, Wright S () Numerical optimiza-
tion. Springer, New York
. Phelps RR () Convex functions, monotone
operators and differentiability, nd edn, volume
of Lecture Notes in Mathematics. Springer,
New York
. Potter LC, Arun KS () A dual approach to
linear inverse problems with convex constraints.
SIAM J Contr Opt ():–
. Pshenichnyi BN () Necessary conditions for
an extremum, volume of Pure and applied
mathematics. Marcel Dekker, New York, .
Translated from Russian by Karol Makowski.
Translation edited by Lucien W. Neustadt
. Rockafellar RT () Characterization of the
subdifferentials of convex functions. Pacific J
Math :–
. Rockafellar RT () Convex analysis. Princeton
University Press, Princeton
. Rockafellar RT () On the maximal mono-
tonicity of subdifferential mappings. Pacific J
Math :–
. Rockafellar RT () Integrals which are convex
functionals, II. Pacific J Math :–
. Rockafellar RT () Conjugate duality and opti-
mization. SIAM, Philadelphia
. Rockafellar RT, Wets RJ () Variational anal-
ysis. Grundlehren der mathematischen Wis-
senschaften. Springer, Berlin
. Rudin LI, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D :–
. Scherzer O, Grasmair M, Grossauer H, Halt-
meier M, Lenzen F () Variational methods
in imaging, volume of Applied mathematical
sciences. Springer, New York
. Simons S () From Hahn-Banach to Mono-
tonicity, volume of Lecture notes in mathe-
matics. Springer, New York
. Singer I () Duality for nonconvex approxi-
mation and optimization. Springer, New York
. Teboulle M, Vajda I () Convergence of best
φ-entropy estimates. IEEE Trans Inform Process
:–


Duality and Convex Programming
. Tihonov AN () On the regularization of
ill-posed problems. (Russian). Dokl Akad Nauk
SSSR :–
. Weiss P, Aubert G, Blanc-Féraud L () Effi-
cient schemes for total variation minimization
under constraintsinimageprocessing.SIAMJ Sci
Comput :–
. Wright SJ () Primal-dual interior-point
methods. SIAM, Philadelphia, PA
. Zarantonello EH () Projections on convex
sets in Hilbert space and spectral theory. In:
Zarantonello EH (ed) Contributions to nonlin-
ear functional analysis. Academic, New York,
pp –
. Z˘alinescu C () Convex analysis in gen-
eral
vector
spaces.
World
Scientific,
River
Edge, NJ

EM Algorithms
Charles Byrne ⋅Paul P. B. Eggermont
.
Maximum Likelihood Estimation...............................................
.
The Kullback–Leibler Divergence...............................................
.
The EM Algorithm...............................................................
..
The Maximum Likelihood Problem..................................................
..
The Bare-Bones EM Algorithm.......................................................
..
The Bare-Bones EM Algorithm Fleshed Out.......................................
..
The EM Algorithm Increases the Likelihood........................................
.
The EM Algorithm in Simple Cases.............................................
..
Mixtures of Known Densities.........................................................
..
A Deconvolution Problem.............................................................
..
The Deconvolution Problem with Binning..........................................
..
Finite Mixtures of Unknown Distributions..........................................
..
Empirical Bayes Estimation............................................................
.
Emission Tomography...........................................................
..
Flavors of Emission Tomography.....................................................
..
The Emission Tomography Experiment.............................................
..
The Shepp–Vardi EM Algorithm for PET...........................................
..
Prehistory of the Shepp–Vardi EM Algorithm.....................................
.
Electron Microscopy.............................................................
..
Imaging Macromolecular Assemblies................................................
..
The Maximum Likelihood Problem..................................................
..
The EM Algorithm, up to a Point.....................................................
..
The ill-posed Weighted Least-Squares Problem....................................
.
Regularization in Emission Tomography......................................
..
The Need for Regularization..........................................................
..
Smoothed EM Algorithms.............................................................
..
Good’s Roughness Penalization.......................................................
..
Gibbs Smoothing........................................................................
.
Convergence of EM Algorithms.................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


EM Algorithms
..
The Two Monotonicity Properties....................................................
..
Monotonicity of the Shepp–Vardi EM Algorithm..................................
..
Monotonicity for Mixtures.............................................................
..
Monotonicity of the Smoothed EM Algorithm.....................................
..
Monotonicity for Exact Gibbs Smoothing...........................................
.
EM-Like Algorithms.............................................................
..
Minimum Cross-Entropy Problems..................................................
..
Nonnegative Least Squares.............................................................
..
Multiplicative Iterative Algorithms...................................................
.
Accelerating the EM Algorithm.................................................
..
The Ordered Subset EM Algorithm..................................................
..
The ART and Cimmino–Landweber Methods......................................
..
The MART and SMART Methods....................................................
..
Row-Action and Block-Iterative EM Algorithms...................................

EM Algorithms 

.
Maximum Likelihood Estimation
Expectation-Maximization algorithms, or em algorithms for short, are iterative algorithms
designed to solve maximum likelihood estimation problems. The general setting is that one
observes a random sample Y, Y, . . . , Yn of a random variable Y whose probability density
function (pdf) f (⋅∣xo) with respect to some (known) dominating measure is known up
to an unknown “parameter” xo. The goal is to estimate xo and, one might add, to do it well.
In this chapter that means to solve the maximum likelihood problem
maximize
n
∏
i=
f (Yi ∣x)
over
x,
(.)
and to solve it by means of em algorithms. The solution, assuming it exists and is unique,
is called the maximum likelihood estimator of xo. Here, the estimator is typically denoted
by ̂x.
The notion of em algorithms was coined by [], who unified various earlier instances
of em algorithms and in particular emphasized the notion of “missing” data in maximum
likelihood estimation problems, following Hartley []. Here, the missing data refers to
data that were not observed. Although this seems to imply that these data could have or
should have been observed, it is usually the case that these missing data are inherently
inaccessible. Typical examples of this are deconvolution problems, but it may be instructive
to describe a simplified version in the form of finite mixtures of probability densities.
Let the random variable Y be a mixture of some other continuous random variables
Z, Z, . . . , Zm for some known integer m. For each j, ⩽j ⩽m, denote the pdf of Z j by
f (⋅∣j). The pdf of Y is then
fY (y) =
m
∑
j=
w∗
j f (y ∣j),
(.)
where w∗= (w∗
,w∗
, . . .,w∗
m) is a probability vector. In other words, the w∗
j are nonnega-
tive and add up to . The interpretation is that for each j
Y = Z j
with probability
w∗
j .
(.)
As before, given a random sample Y, Y, . . . , Yn of the random variable Y, the goal is
to estimate the “parameter” w∗. The maximum likelihood problem for doing this is
maximize
m
∏
i=
⎧⎪⎪⎨⎪⎪⎩
m
∑
j=
wj f (Yi ∣j)
⎫⎪⎪⎬⎪⎪⎭
(.)
subject to
w = (w,w, . . . ,wm)
is a probability vector.
Now, what are the “missing” data for this finite mixture problem? In view of the inter-
pretation (> .), it would clearly be useful if for each Yi, it was known which random
variable Z j it was supposed to be a random sample of. Thus, let Ji ∈{,, . . . , m} such that
Yi ∣Ji = ZJi .
(.)


EM Algorithms
Then, J, J, . . ., Jn would be random sample of the random variable J, whose distribution
would then be easy to estimate: for each j
̂wj = #{Ji = j}
n
,
(.)
the fraction of the Ji that were equal to j. Note that the distribution of J is given by
P[J = j] = w∗
j ,
j = ,, . . . , m.
(.)
Of course, unfortunately, the Ji are not known. It is not even apparent that it would be
advantageous to think about the Ji but in fact it is, as this chapter tries to make clear.
From the image processing point of view, the above problem becomes more interesting
if the finite sum in (> .) is interpreted as a discretization of the integral transform
fY (y) = ∫f (y ∣x)w(x) dx
(.)
and the goal is to recover the function or image w from the random sample Y, Y, . . . , Yn.
The maximum likelihood problem of estimating w,
maximize
n
∏
i=
{∫f (Yi ∣x)w(x) dx}
over
w,
(.)
is (formally) a straightforward extension of the mixture problem. Such (one- and two-
dimensional) deconvolution problems abound in practice, e.g., in astronomy and tomog-
raphy. See the suggested reading list.
In the next two sections, the bare essentials of a more-or-less general version of the em
algorithm are presented and it is shown that it increases the likelihood. Without special
conditions, that is all one can say about the convergence of the em algorithm; one cannot
even claim that in the limit, it achieves the maximum value of the likelihood. See [].
For the convex case, where the negative log-likelihood is convex and the constraint set is
convex as well, one can say much more, as will become clear.
Before discussing the two “big” applications of positron emission tomography (PET)
and three-dimensional electron microscopy (d-em. Yes, another instance of em!), it is
prudent to discuss some simple examples of maximum likelihood estimation and to derive
the associated em algorithms. It turns that the prototypical example is that of estimating the
weights in a mixture of known distributions, see (> .). By analogy, this example shows
how one should derive the em algorithm for deconvolution problems with binned data,
which is similar to the situation in positron emission tomography. The general parametric
maximum likelihood estimation is also discussed, as well as the related case of empirical
Bayes estimation. The latter has some similarity with d-em.

EM Algorithms 

All of this naturally leads to the discussion of the maximum likelihood approach
to positron emission tomography (which originated with Rockmore and Macovski [],
but who mistakenly took the road of a least squares treatment) and the em algorithm
of Shepp and Vardi []. This is one of the classic examples of Poisson data. However,
even Poisson data may be interpreted as a random sample of some random variable,
see > Sect. ... For the ubiquitous nature of Poisson data, see [] and references
therein.
The very messy example of the reconstruction of the shapes of macromolecules of
biological interest by way of d-em also passes review.
For the example of mixtures of known distributions as well as for positron emission
tomography, there is a well-rounded theory for the convergence of the em algorithm to
wit the alternating projections approach of Csiszár and Tusnády [] and the majoriz-
ing functional approach of Mülthei and Schorr [] and De Pierro []. This approach
extends to em-like algorithms for some maximum likelihood-like problems. Unfortu-
nately, this ignores the fact that the maximum likelihood problem is ill conditioned
when the number of components in the mixture is large (or that the deconvolution
problem is ill-posed). So, one needs to regularize the maximum likelihood problems,
and then, in this chapter, the issue is whether there are em algorithms for the regu-
larized problems. For the PET problem, this certainly works for Bayesian approaches,
leading to maximum a posteriori (MAP) likelihood problems as well as to arbitrary con-
vex maximum penalized likelihood problems. In this context, mention should be made
of the ems algorithm of Silverman et al. [], the em algorithm with a linear smooth-
ing step added, and the nems algorithm of Eggermont and LaRiccia [] in which an
extra nonlinear smoothing step is added to the ems algorithm to make it a genuine em
algorithm for a smoothed maximum likelihood problem. However, the convergence of reg-
ularization procedures for ill-posed maximum likelihood estimation problems, whether
Tikhonov style penalization or “optimally” stopping the em algorithm will not be discussed.
See, e.g., [].
The final issue under consideration is that em algorithms are painfully slow, so
methods for accelerating em algorithms are discussed as well. The accelerated meth-
ods take the form of block-iterative methods, including the extreme case of row-action
methods.
The selection of topics is driven by applications to image processing. As such, there is
very little overlap with the extensive up-to-date survey of em algorithms of McLachlan and
Krishnan [].
.
The Kullback–Leibler Divergence
Before turning to the issue of em algorithms, emphatic mention must be made of the
pervasiveness of the Kullback–Leibler divergence (also called I-divergence or informa-
tion divergence, see, e.g., [] and references therein) in maximum likelihood estimation.


EM Algorithms
For probability density functions f and g on Rd say, it is defined as
KL(f , g) = ∫Rd (f (y) log { f (y)
g(y)} + g(y) −f (y)) dμ(y),
(.)
with μ denoting Lebesgue measure. Here, log(/) is defined as . Note that the
Kullback–Leibler divergence is not symmetric in its arguments. Also note that the inte-
grand is nonnegative, so that the integral is well defined if the value +∞is admitted.
Moreover, the integrand equals if and only if f (y) = g(y), so that KL(f , g) > unless
f = g almost everywhere, in which case KL(f , g) = .
Now consider the problem of estimating the unknown parameter xo in a probability
density f (⋅∣xo). In view of the above, the ideal way would be to
minimize
KL(f (⋅∣xo), f (⋅∣x))
over
x,
(.)
but of course, this is not a rational problem because the objective function is unknown.
However, note that
KL(f (⋅∣xo), f (⋅∣x)) = −∫Rd f (y∣xo) log f (y∣x) dμ(y)
+ ∫Rd f (y∣xo) log f (y∣xo) dμ(y),
(.)
and that the second term does not depend on x. So, the problem (> .) is equivalent to
(has the same solutions as)
minimize
−∫Rd f (y∣xo) log f (y∣x) dμ(y)
over
x.
(.)
Of course, this is still not a rational problem, but the objective function equals E[Ln(x)],
where Ln(x) is the scaled negative log-likelihood
Ln(x) = −
n
n
∑
i=
log f (Yi∣x),
(.)
if Y, Y, . . . , Yn is a random sample of the random variable Y with probability density
function f (⋅∣xo). So, solving the maximum likelihood problem (> .) may be viewed as
approximately solving the minimum Kullback–Leibler divergence problem (> .). This
is the basic reason for the pervasiveness of the Kullback–Leibler divergence in the analysis
of maximum likelihood estimation and em algorithms.
The above illustrates two additional points. First, in maximum likelihood estimation
one attempts to solve the minimum Kullback–Leibler problem (> .) by first estimating
the objective function. So, if the estimator is “optimal” at all, it has to be in a sense related
to the Kullback–Leibler divergence. Second, one may well argue that one is not estimating
the parameter xo but rather the density f (⋅∣xo). This becomes especially clear if f (⋅∣x)
is reparametrized as φ(⋅∣z) = f ( ⋅∣T(z)) for some transformation T. This would have
an effect on the possible unbiasedness of the estimators ̂x and ̂z of xo and zo. However,
under reasonable conditions on T, the maximum likelihood density estimators f (⋅∣̂x)
and φ(⋅∣̂z) of f (⋅∣xo) will be the same.

EM Algorithms 

.
The EM Algorithm
..
The Maximum Likelihood Problem
Let (Y,BY,P) be a statistical space, i.e., (Y,BY ) is a measurable space and P is a collection
of probability measures on BY, represented as a family indexed by some index set X as
follows,
P = {P(⋅∣x) : x ∈X}.
(.)
Assume that there is a measure P∞that dominates P in the sense that every P(⋅∣x) is
absolutely continuous with respect to P∞. Then, the Radon–Nikodym derivative of P(⋅∣x)
with respect to P∞exists and is BY-measurable for all x ∈X. It may be written as
fY (y∣x) = [ dP(⋅∣x)
dP∞
](y)
(.)
and is referred to as the density of P(⋅∣x) with respect to P∞. It should be observed that
fY (⋅∣xo) = fY (⋅) is the density of the random variable Y with respect to P∞. For arbitrary
x, fY (⋅∣x) is a density but since it is not known of what random variable the subscript Y
is used here.
Let Y be a random variable with values in Y, and assume that it is distributed as P(⋅∣xo)
for some (unknown) xo ∈X. The objective is to estimate xo based on a random sample
Y, Y, . . . , Yn of the random variable Y. Note that estimating xo amounts to constructing
a measurable function of the data, which may then be denoted as ̂x = ̂x(Y, Y, . . . , Yn).
The maximum-likelihood problem for estimating xo is then, written in the form of
minimizing the scaled negative log-likelihood,
minimize
Ln(x)
def= −
n
n
∑
i=
log fY (Yi∣x)
(.)
subject to
x ∈X.
In this formulation, the parameter x is deemed important. The alternative formulation in
which the densities are deemed important is
minimize
̃Ln(f )
def= −
n
n
∑
i=
log f (Yi)
subject to
f ∈P.
(.)
In this formulation there are two ingredients: the likelihood function and the (parametric)
family of densities under consideration.
It is not obvious that solutions should exist, especially if the index set X is large, but
in applications of image processing type, this turns out to be of lesser importance than
one might think. See > Sect. .. Regardless, closed form solutions are generally not avail-
able, and one must employ iterative methods for the solution of the maximum likelihood
problem. In this chapter, that means em algorithms.


EM Algorithms
..
The Bare-Bones EM Algorithm
Here the bare essentials of the EM algorithm are presented. The basic premise in the deriva-
tion of the em algorithm is that there is “missing” data that would make estimating xo a lot
easier had they been observed. So, assume that the missing data refers to data in a space
Z, with (Z,BZ,Q) another statistical space, where the collection of probability measures
is again indexed by X,
Q = {Q(⋅∣x) : x ∈X} .
(.)
Assume that Q is dominated by some measure Q∞, and denote the associated Radon–
Nikodym derivatives as
fZ (z∣x) = [ dQ(⋅∣x)
dQ∞
](z),
z ∈Z.
(.)
Let Z be a random variable with values in Z and with distribution Q(⋅∣xo), with the
same xo as for the random variable Y. Note that the pair (Y, Z) takes on values in Y×Z. The
relevant statistical space is then (Y × Z, BY×Z,R), where BY×Z is the smallest σ-algebra
that contains all sets A×B with A ∈BY and B ∈BZ. Again, assume that R may be indexed
by X as
R = {R(⋅∣x) : x ∈X},
(.)
and that R is dominated by some measure R∞. Write
fY,Z (y, z∣x) = [ dR(⋅∣x)
dR∞
](y, z)
(.)
for the associated Radon–Nikodym derivatives.
Now, if the “complete” data (Y, Z),(Y, Z), . . . ,(Yn, Zn), a random sample of the
random variable (Y, Z), is available then the maximum-likelihood problem for estimating
xo is
minimize
−
n
n
∑
i=
log fY,Z (Yi, Zi∣x)
subject to
x ∈X.
(.)
Of course, this is not a rational problem, since the Zi went unobserved. In other words,
the objective function is not known (and not knowable). However, one may attempt to
estimate it by the conditional expectation
E[ −
n
n
∑
i=
log fY,Z (Yi, Zi∣x)∣Yn] = −
n
n
∑
i=
E[ log fY,Z (Yi, Zi∣x)∣Yi],
where Yn = (Y, Y, . . . , Yn). The fly in the ointment is that computing this expectation
involves the distribution of Z conditioned on Y, which surely will involve the unknown xo
one wishes to estimate! So, at this point, assume that some initial guess xfor xo is available;
then denote the resulting (approximate) conditional expectation by E[. . . ∣Yn, x].

EM Algorithms 

Determining this conditional expectation constitutes the E-step of the first itera-
tion of the em algorithm. The M-step of the first iteration then amounts to solving the
minimization problem
minimize
Λn(x∣x)
def= −
n
n
∑
i=
E[log fY,Z (Yi, Zi∣x)∣x, Yi]
subject to
x ∈X.
(.)
Denote the solution by x(assuming it exists). Suppressing the presence of the Yi in the
notation, one may define the iteration operator by x= R(x), and then the em algorithm
may be stated as
xk+= R(xk),
k = ,, . . . ,
(.)
provided xhas been chosen appropriately. This is the bare-bones version of the em algo-
rithm. Note that it may not be necessary to solve the problem (> .) exactly, e.g., one
may consider (over)relaxation ideas or so-called stochastic em algorithms. See, e.g., []
and references therein. This will not be considered further.
Remarks 
(a) It may be inappropriate to speak of the em algorithm, since the introduc-
tion of different missing data may lead to a different algorithm. However, usually there is
not much choice in the missing data.
(b) There is a different approach to the complete data, by assuming that Y = T(Z) for some
many-to-one map T : Z →Y. Then Z is the complete data and Y is the incomplete data,
but one does not identify missing data as such.
..
The Bare-Bones EM Algorithm Fleshed Out
Here, some of the details of the bare-bones em algorithm are filled in by using explicit
expressions for the conditional expectations. To that end, assume that one may take the
dominating measure R∞to be R∞= P∞⋅Q∞, in the sense that
R∞(A × B) = P∞(A) ⋅Q∞(B)
for all A ∈BY and B ∈BZ.
(.)
Let (Y, Z) have density fY,Z (y, z∣xo) with respect to the product measure P∞× Q∞.
Then, for all A ∈BY×Z and all measurable functions h on Y × Z with finite expectation
E[h(Y, Z)], one may write
E[h(Y, Z)] = ∫A h(y, z) fY,Z(y, z∣x) dP∞(y) dQ∞(z)
= ∫Y {∫Z h(y, z) fY,Z(y, z∣x) dQ∞(z)} dP∞(y)
(Fubini)
= ∫Y
⎧⎪⎪⎨⎪⎪⎩
∫Z h(y, z)
fY,Z (y, z∣x)
fY (y∣x)
dQ∞(z)
⎫⎪⎪⎬⎪⎪⎭
fY (y∣x) dP∞(y).


EM Algorithms
It is clear that this may be interpreted as the expected value of
∫Z h(Y, z)
fY,Z (Y, z∣x)
fY (Y∣x)
dQ∞(z),
and then interpret this in turn as E[h(Y, Z)∣Y], the expected value of h(Y, Z) condi-
tioned on Y.
Now define the density of Z conditional on Y by
fZ∣Y (z∣y, x) =
fY,Z (y, z∣x)
fY (y∣x)
(.)
for those y for which fY (y∣x) > (and arbitrarily if fY (y∣x) = ). Similarly, one defines
fY∣Z (y∣z, x) =
fY,Z (y, z∣x)
fZ (z∣x)
(.)
for those z for which fZ (z∣x) > (and arbitrarily if fZ (z∣x) = ). So then Bayes’ rule yields
fZ∣Y (z∣y, x) =
fY∣Z (y∣z, x) fZ(z∣x)
fY (y∣x)
.
(.)
The conditional expectation of a measurable function h(Y, Z) given Y is then
E[h(Y, Z)∣Y, x] = ∫Z h(Y, z) fZ∣Y(z∣Y, x) dQ∞(z).
(.)
Probabilists force us to add “almost surely” here.
Now apply this to the conditional expectation of log fY,Z (Y, Z∣x), with a guess xof
the true x. Then
E[log fY,Z (Y, Z∣x)∣Y, x]
= E[log fZ (Z∣x) + log fY∣Z (Y∣Z, x)∣Y, x]
= ∫Z
fY∣Z (Y∣z, x) fZ (z∣x)
fY (y∣x)
log fZ (z∣x) dQ∞(z)
(.)
+ ∫Z
fY∣Z (Y∣z, x) fZ (z∣x)
fY (Yi∣x)
log fY∣Z (Y∣z, x) dQ∞(z).
For Λn(x∣x) this gives
Λn(x∣x) = ∫Z φZ(z∣x) log fZ (z∣x) dQ∞(z) +
−
n
n
∑
i=∫Z
fY∣Z (Yi∣z, x) fZ (z∣x)
fY (Yi∣x)
log fY∣Z (Yi∣z, x) dQ∞(z), (.)
with
φZ(z) = fZ (z∣x) ⋅
n
n
∑
i=
fY∣Z (Yi∣z, x)
fY (Yi∣x)
.
(.)

EM Algorithms 

For the M-step of the algorithm, one has to minimize this over x, which is in general
not trivial. The problem is simplified somewhat in the important case where fY∣Z (y∣z, x)
is known and does not depend on x. Then the problem reduces to solving
minimize
Ln(x∣x)
def= −∫Z φZ(z∣x) log fZ (z∣x) dQ∞(z)
subject to
x ∈X.
(.)
Note that
Ln(x∣x) = KL(φZ(⋅∣x), fZ (⋅, x)) + (terms not depending on x),
(.)
where KL(f , g) is the Kullback–Leibler divergence between the density f and g with
respect to the same measure Q∞, defined as
KL(f , g) = ∫Z {f (z)log{ f (z)
g(z) } + f (z) −g(z)} dQ∞(z).
(.)
Compare with (> .).
So, solving (> .) amounts to computing what one may call the Kullback–Leibler
projection of φZ(⋅∣x) onto the parametric family P. If P is such that φZ(⋅∣x) ∈P, then
the projection is fZ (⋅∣x) = φZ(⋅∣x).
..
The EM Algorithm Increases the Likelihood
The expression for Λn(x∣x), see (> .), may be reworked as follows. Using
fY,Z (y, z,∣x) = fZ∣Y (z∣y, x) fY (y∣x),
see (> .), one gets
E[log fY,Z (Y, Z∣x) ∣x, Y] = log fY (Y∣x) + E[log fZ∣Y (Z∣Y, x)∣Y, x],
and so,
Λ(x∣x) = Ln(x) + en(x∣x),
(.)
where Ln(x) is given by (> .) and
en(u∣w)
def= −
n
∑
i=∫Z fZ∣Y (z∣Yi,u) log fZ∣Y (z∣Yi,w) dQ∞(z).
(.)
It is now obvious that the em algorithm decreases Ln(x): Let xbe the minimizer of
Λ(x∣x) over x ∈X. Then Λn(x∣x) ⩽Λn(x∣x), and so Ln(x) + en(x∣x) ⩽Ln(x) +
en(x∣x), or
Ln(x) −Ln(x) ⩾en(x∣x) −en(x∣x) = Kn(x∣x),
(.)
where
Kn(u∣w) =
n
∑
i=
KL(fZ∣Y (⋅∣Yi,u), fZ∣Y (⋅∣Yi,w))
(.)


EM Algorithms
is a sum of Kullback–Leibler “distances,” see (> .). Then, Kn(x∣x) ⩾unless x= x,
assuming that the conditional densities fZ∣Y (⋅∣Y,u) and fZ∣Y (⋅∣Y,w) are equal Q∞almost
everywhere only if u = w. Then the conclusion
Ln(x) > Ln(x)
unless
x= x
(.)
is justified. Thus, the em algorithm decreases the likelihood.
Unfortunately, x= xbeing a fixed point of the em iteration does not guarantee that
then xis a maximum likelihood estimator of xo. Equally unfortunately, even if one
gets an infinite sequence of estimators, this does not imply that the sequence of estima-
tors converges, nor that the likelihood converges to its maximum. Later on the conver-
gence of em algorithms for special, convex maximum likelihood problems is discussed in
detail.
.
The EM Algorithm in Simple Cases
In this section, some simple cases of the em algorithm are discussed, capturing some of the
essential features of more complicated “real” examples of maximum likelihood estimation
to be discussed later on. It turns out that the em algorithms are the “same” in all but the
last example (regarding a finite mixture of unknown densities), even though the settings
appear to be quite different. However, even in the last case the “same” em algorithm arises
via the empirical Bayes approach.
A word on notation: The scaled negative log-likelihood for each problem is always
denoted as Ln; Ln(x) in the discrete case, Ln(f ) in the continuous case. The negative
log-likelihood in the M-step of the em algorithm is denoted by Λ(x∣x) or variations
thereof.
..
Mixtures of Known Densities
Let d ⩾be an integer and consider the statistical space (Y,B,P), with Y = Rd, B
the σ-algebra of Borel subsets of Y and P the collection of probability measures that are
absolutely continuous with respect to Lebesgue measure. Consider the random variable Y
with values in Y, with density
fY (y) =
m
∑
j=
xo(j) a j(y),
y ∈Y,
(.)
where a, a, . . . , am are known densities and xo = (xo,, xo,, . . . , xo,m) T is a probability
vector, i.e., xo ∈Vm,
Vm =
⎧⎪⎪⎨⎪⎪⎩
x ∈Rm ∣x ⩾(componentwise),
m
∑
j=
x(j) = 
⎫⎪⎪⎬⎪⎪⎭
.
(.)

EM Algorithms 

Suppose that one has a random sample Y, Y, . . . , Yn of the random variable Y. The
maximum likelihood problem for estimating fY (or estimating the probability vector xo)
is then
minimize
Ln(x)
def= −
n
n
∑
i=
log⎛
⎝
m
∑
j=
x(j) a j(Yi)⎞
⎠
subject to
x ∈Vm.
(.)
To derive an em algorithm, missing data must be introduced. To see what could be
missing, it is helpful to think of how one would simulate the random variable Y. First,
draw the random variable J from the distribution
fJ(j) = P[J = j] = xo(j) ,
j = ,, . . . , m.
(.)
Then, conditional on J = j, draw Y from the distribution with density a j. So, the missing
data is J, a random variable with values in M = {,, . . ., m}. The associated statistical
space is
(M,M, Vm),
(.)
the σ-algebra is the collection of all subsets of M, and the collection of all probability
measures on M may be represented by Vm. Let α denote counting measure on M, i.e., for
any A ∈M,
α(A) = ∣A∣
(the number of elements in A).
(.)
Then it is easy to see that the distribution of (Y, J) is absolutely continuous with respect to
the product measure μ × α, with density
fY,J (y, j) = fJ(j) fY∣J (y∣j) = xo(j) a j(y),
y ∈Y, j ∈M.
(.)
Now, the complete data is (Y, J),(Y, J), . . . ,(Yn, Jn) and the complete maximum
likelihood problem for estimating xo is
minimize
−
n
n
∑
i=
log {x(Ji) aJi (Yi)}
subject to
x ∈Vm.
(.)
Of course, the Ji went unobserved, so one must compute the conditional expectations
E[log {x(J) aJ(Y)} ∣Y ] . Now,
fJ∣Y (j∣y) =
fY,J(y, j)
fY (y)
=
xo(j) a j(y)
m
∑
p=
ap(y) xo(j)
,
but of course, xo is unknown; approximate it by some initial guess x[] ∈Vm, e.g., x[]
j
= /m
for all j. Then, the conditional expectation in question is approximated by
E[log {x(J) aJ(Y)} ∣Y, x[] ] = −∫M log{x(j) a j(Y)} x[](j, Y) dα(j)
= −∑
j∈M
x[](j, Y) log {x(j) a j(Y)},


EM Algorithms
with
x[](j, Y) =
x[](j) a j(Y)
m
∑
p=
ap(Y) x[](p)
,
j ∈M.
Then, the E-step of the em algorithm leads to the problem
minimize
−∑
j∈M
x[](j) log{x(j) ai j}
subject to
x ∈Vℓ,
(.)
where ai j = a j(Yi) for all i, j, and x[](j) = 
n ∑n
i=x[](j, Yi), or
x[](j) = x[](j) ⋅
n
n
∑
i=
ai j
⎛
⎝
m
∑
p=
aip x[](p)⎞
⎠
.
(.)
Taking into account that
log{x(j) a j(Y)} = log x(j) + (a term not depending on x),
one then arrives at the problem
minimize
Λn (x∣x[])
def= −
m
∑
j=
x[](j)log x(j)
subject to
x ∈Vm.
(.)
This is the E-step of the first iteration of the em algorithm. Now consider the identity
Λ (x∣x[]) −Λ (x[]∣x[]) = KL(x[], x),
(.)
where for u, w nonnegative vectors in Rm,
KL(u,w)
def=
m
∑
j=
{u(j) log u(j)
w(j) + w(j) −u(j)}.
(.)
This is the finite dimensional Kullback–Leibler divergence between from the nonnegative
vectors u and w. Note that the summand is nonnegative and so is minimal when u = w. So,
the solution of (> .) is precisely x = x[]. This would be the M-step of the first iteration
of the em algorithm. The EM-step is then (> .).
..
A Deconvolution Problem
The setting is the statistical space (Rd,B,P), where B is the σ-algebra of Borel subsets
of Rd and P is the collection of all probability density functions on Rd (with respect to
Lebesgue measure). Denote Lebesgue measure by μ. Let Y be a random variable with values
in Rd and with density fY (with respect to Lebesgue measure); the interest is in estimating

EM Algorithms 

fY . Now, assume that one is unable to observe Y directly but that instead one only observes
a corrupted version, viz. W = Y + Z, where Z is another Rd-valued random variable.
Assume that the distribution of Z is completely known; denote its density by k. The density
of W is then KfY , where the integral operator K : L(Rd, dμ) →L(Rd, dμ) is defined as
[K f ](w) = ∫Rd k(w −y) f (y) dμ(y),
w ∈Rd.
(.)
Note that k(⋅−y) is the density of W conditioned on Y = y, i.e., fW∣Y (w∣y) = k(w −y)
for all w and y, and then
fW,Y (w, y) = k(w −y) fY (y) ,
w, y ∈Rd.
(.)
So, assume that one has a random sample W, W, . . . , Wn of the random variable W. The
maximum likelihood problem for estimating fY is then
minimize
Ln(f )
def= −
n
n
∑
i=
log[Kf ](Wi)
subject to
f ∈P.
(.)
Recall that P is the collection of all pdfs in L(Rd, dμ). It is impossible to guarantee that this
problem has a solution. In fact, regularization is required, see
> Sect. ... Nevertheless,
one can use em algorithms to construct useful approximations to the density fY by the
expedient of stopping the iteration “early.”
Note that one could view (> .) as a continuous mixture problem, since Kf is a
continuous mixture of known densities to wit the known densities ky(w) = k(w −y),
y ∈Rd, and the continuous weights are the unknown fY(y), y ∈Rd. However, the present
approach is somewhat different.
To derive an em algorithm, one must decide on the missing data. It seems obvious that
the missing data is Y itself or Z (or both), but the choice Y seems the most convenient.
Thus, assume that one has available the random sample (W, Y),(W, Y), . . .,(Wn, Yn)
of the random variable (W, Y). In view of (> .), the maximum likelihood problem for
estimating fY is then
minimize
Λn(f )
def= −
n
n
∑
i=
log {k(Wi −Yi) f (Wi)}
subject to
f ∈P.
(.)
Since the Yi are not really observed, one must compute or approximate the conditional
expectation E[log {k(W −Y) f (Y)} ∣W]. Since the density of Y conditioned on W may
be written as
fY∣W (y∣w) = k(w −y) fY (y)
[K fY ](w)
,


EM Algorithms
then, approximating fY by some initial guess f, the conditional expectation is approxi-
mated by
∫Rd
k(W −y) f(y)
[K f](w)
log f (y) dμ(y),
apart from a term not depending on f . So then, the problem (> .) is approximated by
minimize
−∫Rd f(y) log f (y) dμ(y)
subject to
f ∈P,
(.)
where
f(y) = f(y) ⋅
n
n
∑
i=
k(Wi −y)
[K f](Wi) ,
y ∈Rd.
(.)
This is the E-step of the em algorithm
For the M-step, i.e., actually solving (> .), note that
Λn(f ∣f) −Λn(f∣f) = KL(f, f ),
(.)
which is minimal for f. Thus the solution of (> .) is f = fas well. Thus, the em
algorithm takes the form (> .) iteratively applied.
The discretized EM algorithm:
The em algorithm (> .) cannot be implemented
as is, but it certainly may be discretized. However, it is more straightforward to discretize
the maximum likelihood problem (> .).
A reasonable way to discretize the maximum likelihood problem (> .) is to restrict
the minimization to step functions on a suitable partition of the space. Suppose that the
compact set Co ⊂Rd contains the support of fY , and let {C j}m
j=be a partition of Co. Define
the (step) functions a j by
a j(y) = ∣C j ∣−(y ∈C j) ,
i = ,, ⋅, m,
(.)
where for any set A, the indicator function (y ∈A) is defined as
(y ∈A) = 
if
y ∈A
and
= 
otherwise.
(.)
Then, define Pm to be the set of pdfs in the linear span of the a j,
Pm =
⎧⎪⎪⎨⎪⎪⎩
m
∑
j=
x j a j(⋅)∣x ∈Vm
⎫⎪⎪⎬⎪⎪⎭
.
(.)
Note that the a j are pdfs, and in fact, one could take the a j, j = ,, . . . , m, to be any
collection of pdfs.
Now, consider the restricted maximum likelihood problem
minimize
−
n
n
∑
i=
log[Kf ](Wi)
subject to
f ∈Pm
(.)

EM Algorithms 

and observe that it may obviously be rewritten as
minimize
−
n
n
∑
i=
log⎛
⎝
m
∑
j=
ai j x j
⎞
⎠
subject to
x ∈Vm,
(.)
where for i = ,, . . . , n and j = ,, . . . , m,
ai j = ∫Rd k(Wi −y) a j(y) dμ(y).
And this is all there is to it: The problem (> .) is just a finite mixture problem with
known distributions! Thus, the em algorithm is as in > Sect. ..,
x[k+]
j
= x[k]
j
⋅
n
n
∑
i=
ai j
⎛
⎝
m
∑
p=
aip x[k]
p
⎞
⎠
,
j = ,, . . ., m,
(.)
with the estimator for fY induced by the representation of (> .).
Another EM algorithm?
There is of course another way to derive an em algorithm
for the problem (> .), viz. by introducing the missing data Yi as before. As for the
unrestricted maximum likelihood problem (> .), the E-step of the first iteration of the
em algorithm leads to the problem, analogous to (> .),
minimize
−∫Rd f(y) log f (y) dμ(y)
subject to
f ∈Pm,
(.)
with fgiven by (> .). Now, using the representations
f (y) =
m
∑
j=
x j a j(y),
fk(y) =
m
∑
j=
x[k]
j
a j(y)
with the step functions a j, for k = ,, the objective function in (> .) may be written as
−
m
∑
j=
(log x j) ∫C j
f(y) dμ(y),
and of course
∫C j
f(y) dμ(y) = x[]
j
⋅
n
n
∑
i=
ai j
[K f](Wi)
def= x[]
j ,
(.)
where
ai j = ∫Rd k(Wi −y) a j(y) dμ(y) = ∣C j ∣−∫C j
k(Wi −y) dμ(y).
Note that
[K f](Wi) =
m
∑
j=
ai j x[]
j
,
i = ,, . . . , n.


EM Algorithms
Thus, the problem (> .) is equivalent to
minimize
−
m
∑
j=
x[]
j
log x j
subject to
x ⩾,
m
∑
j=
x j = .
(.)
But it was already shown in
> Sect. ..that the solution is x = x[]. So, the iterative
step is exactly the same as in (> .). As an aside, this is a case where the introduction of
“different” missing data leads to the same em algorithm.
..
The Deconvolution Problem with Binning
Consider again the deconvolution problem of
> Sect. .., but now with the extra twist
that the data is binned.
Recall that the random variable of interest is Y which lives in the statistical space
(Y,BY,P) with Y = Rd, BY the σ-algebra of Borel subsets of Y, and P the collection
of probability measures on BY that are absolutely continuous with respect to Lebesgue
measure. The density of Y is denoted by fY . The random variable Y was not observable.
Instead, one can observe the random variable
W = Y + Z,
where Z is another random variable living in (Y,BY,P), with known density denoted by
k and independent of Y. Actually, with binned data, W is not observed either. Let ℓ∈N
and let {B j}ℓ
j=⊂BY be a partition of Y (or of a set containing the support of W). What
one does observe is which “bin” B j the observation W belongs to. That is, one observes the
random variable J, with J = j if
(W ∈B j) = ,
(.)
cf. (> .). Then the statistical space of interest is (M,M, Vm) see (> .). Of course,
Vm is dominated by the counting measure, denoted by α; see (> .). The density of J
then satisfies
fJ(j) = [KfY](B j) = ∫B j
[KfY](w) dμ(w),
j ∈M.
(.)
So, now one observes the random sample J, J, . . . , Jn of the random variable J and the
goal is to estimate fY . The maximum likelihood problem is then
minimize
−
n
n
∑
i=
log[Kf ](BJi)
subject to
f ∈P,
(.)
which is equivalent to
minimize
−
n
ℓ
∑
j=
N j log[Kf ](B j)
subject to
f ∈P.
(.)

EM Algorithms 

Here, the N j are the bin counts
N j =
n
∑
i=
(Ji = j),
j ∈M.
(.)
Remark 
Later, for any function h : M →R the more general identity
n
∑
i=
h(Ji) =
m
∑
j=
N j h(j)
will be useful.
So, the real data are the bin counts, but it is advantageous to keep the Ji. It has to be seen
whether one can get away it, though. So, the starting point is (> .) and not (> .).
To derive an em algorithm, the missing data must be considered. It seems obvious that
the Wi are missing, and the treatment in
> Sect. ..suggests that the Yi are missing
as well. So, the complete data is the random sample (Ji, Wi, Yi) of the random variable
(J, W, Y). This random variable lives in the statistical space (M × Y × Y,B,Q), with B
the σ-algebra generated by the sets A× B ×C with A ⊂M, and B, C ∈BY. Finally, Q is the
collection of probability measures on B that are absolutely continuous with respect to the
product measure α × μ × μ. The density of (J, W, Y) is then
fJ,W,Y (j,w, y) = k(w −y) fY (y)(w ∈B j),
j ∈M , w, y ∈Y.
(.)
The complete maximum likelihood problem for estimating fY is now
minimize
−
n
n
∑
i=
log {k(Wi −Yi) f (Yi)}
subject to
f ∈P.
(.)
This is the same as the problem (> .). However, here one conditions differently. At issue
is the conditional expectation E[log{k(W −Y) f (Y)}∣J]. Observe that
fW,Y∣J (w, y∣j) =
fJ,W,Y (j,w, y)
fJ(j)
=
k(w −y) fY (y)(w ∈B j)
[KfY ](B j)
,
(.)
so that, replacing fY by some initial guess f, one finds that
E[log{k(W −Y) f (Y)}∣J , f]
= ∫Y×Y log{k(w −y) f (y)} k(w −y) f(y)(w ∈B j)
[Kf](B j)
dμ(w) dμ(y).
Now, using
log{k(w −y) f (y)} = log{f (y)} + (a term not depending on f ),


EM Algorithms
one arrives at
E[log{k(W −Y) f (Y)}∣J , f]
= ∫Y
f(y) k j(y)
[Kf](B j) log{f (y)} dμ(y) + rem,
(.)
where “rem” involves terms not depending on f , and
k j(y) = ∫B j
k(w −y) dμ(w),
j ∈M.
(.)
Note that then
[Kf](B j) = ∫Y k j(y) f(y) dμ(y).
(.)
So, the E-step of the em algorithm leads to the problem
minimize
−∫Y f(y) log f (y) dμ(y)
subject to
f ∈P,
(.)
where (one would say: as always)
f(y) = f(y) ⋅
n
n
∑
i=
kJi (y)
[Kf](BJi ) ,
y ∈Y.
(.)
Since
∫Y f(y) dμ(y) = 
n
n
∑
i=
{∫Y kJi (y) f (y) dμ(y)/[Kf](B j)} = ,
the solution of (> .) is f. So, the iterative step of the em algorithm is given by (> .).
Actually, the situation is a little sticky, since (> .) involves the Ji. However, one may
collect the Ji with equal values, so that then
N j =
n
∑
i=
(j = Ji),
and so an equivalent definition of fis
f(y) = f(y) ⋅
n
m
∑
j=
N j k j(y)
[Kf](B j)
y ∈Y.
(.)
The em algorithm is then obtained by iterative applying of the em-step (> .).
Discretizing the EM algorithm:
Note that a discretized em algorithm may be derived
by restricting the minimization in (> .) to step functions on a partition {C j}ℓ
j=of a set
containing the support of Y. With Pm as in (> .), the restricted maximum likelihood
problem with binned data is
minimize
−
n
ℓ
∑
j=
N j log[Kf ](C j)
subject to
f ∈Pm.
(.)

EM Algorithms 

One then derives the em algorithm as in > Sect. .., leading to
x[k+]
j
= x[k]
j
⋅
n
ℓ
∑
p=
Np kip
⎛
⎝
ℓ
∑
q=
aiq x[k]
q
⎞
⎠
,
j = ,, . . . , ℓ,
(.)
where for p = ,, . . . , m and q = ,, . . . , ℓ,
apq = ∫Cp
{∫Bq
k(w −y) dμ(y)} dμ(w).
The estimators for fY are then
fk(y) =
ℓ
∑
j=
x[k]
j
∣C j ∣−(y ∈C j),
y ∈Y.
(.)
..
Finite Mixtures of Unknown Distributions
The final simple case to be discussed is that of a mixture with a small number of densities
belonging to some parametric family.
Consider a random variable Y in a statistical space (Y,BY,P), with
P = {f (⋅∣x) : x ∈X},
(.)
a family of probability measures indexed by the (low-dimensional) parameter x ∈X.
Assume that P is dominated by some measure P∞and that
[ dP(⋅∣x)
dP∞
](y) = fY (y∣x),
y ∈Y.
(.)
So, let Y be a random variable with density
fY (y) =
m
∑
j=
wo(j)fY (y∣xo(j)),
y ∈Y.
(.)
Here, wo = (wo,wo, . . . ,wom) ∈Vm, the space of probability vectors, see (> .), and
xo = (xo,, xo,, . . . , xo,m) ∈Xm, thus defining Xm. (The notations xo,j and xo(j) are used
interchangeably.)
Given a random sample Y, Y, . . ., Ym, the maximum likelihood problem for estimat-
ing wo and xo is then
minimize
−
n
n
∑
i=
log⎛
⎝
m
∑
j=
wj fY (Yi∣x j)⎞
⎠
subject to
w ∈Vm , x ∈Xm.
(.)


EM Algorithms
To derive an em algorithm, one must introduce missing data. As in
> Sect. .., the
random index J ∈M = {,, . . ., m} would be useful information because fY∣J (y∣j) =
fY (y∣xo,j).
So considering (Y, J),(Y, J), . . . ,(Yn, Jn) to be the complete data, the maximum
likelihood problem is
minimize
−
n
n
∑
i=
log{w(Ji) fY (Yi∣x(Ji))}
subject to
W ∈Vm, x ∈Xm.
(.)
Similar to the development in
> Sect. .., now with initial guesses w[] and x[], one
obtains that
E[log{w(J) fY (Y∣x(j))}∣Y,w[], x[] ]
= ∑
j∈M
w[](j) fY (Y∣x[](j))
⎛
⎝
m
∑
p=
w[](j) fY (Y∣x[](j))⎞
⎠
log{w(j) fY (Y∣x[](j))}.
It follows that
E[−
n
n
∑
i=
log{w(Ji) fY (Yi∣x(Ji),)}∣Y, Y, . . . , Yn,w[], x[] ]
= −∑
j∈M
w[](j) logw(j) + Ln(x∣x[],w[]),
(.)
where
w[](j) = w[](j) ⋅
n
n
∑
i=
fY (Yi∣x[](j))
⎛
⎝∑
p∈M
w[](p) fY (Yi∣x[](p))⎞
⎠
,
j ∈M,
(.)
and
Ln(x∣x[],w[]) = −
n
n
∑
i=
fY (Yi∣x[](j)) log fY (Yi∣x(j))
⎛
⎝∑
p∈M
w[](p) fY (Yi∣x[](p))⎞
⎠
.
(.)
This is essentially the E-step of the em algorithm. Note that the definition of w[] is in the
by-now-familiar form. For the M-step one must solve
minimize
−∑
j∈M
w[](j) log w(j) + Ln (x∣x[],w[])
subject to
w ∈Vm, x ∈Xm,
(.)
and this nicely separates. The minimization over w gives w = w[] as always, and x = x[]
is the solution of
minimize
Ln (x∣x[],w[])
subject to
x ∈Xm.
(.)

EM Algorithms 

Unfortunately, in general, there is no closed form solution of this problem.
There are numerous examples of this type of mixture problems. See, e.g., [,].
..
Empirical Bayes Estimation
There is another way of deriving em algorithms for the mixtures problem under consider-
ation. Of course, that means one must introduce a different collection of missing data. The
following development is from Eggermont and LaRiccia [].
Starting from the beginning, consider the random variable Y with density fY(⋅∣xo)
with respect to P∞. Given a random sample Y, Y, . . . , Yn of the random variable Y, one
wishes to estimate xo. The maximum likelihood problem is
minimize
−
n
n
∑
i=
log fY (Yi∣x)
subject to
x ∈X.
(.)
So, what is the missing data in this case ? It was already alluded to: the missing informa-
tion is xo ! In the so-called empirical Bayes approach, one considers xo to be a random
variable in the statistical space (X,BX ,T ), with T a collection of probability mea-
sures on BX , dominated by some measure T∞. The complete data is the random sample
(Y, X),(Y, X), . . . ,(Yn, Xn) of the random variable (Y, X), with density
fY,X (y, x) = fX(x) fY∣X (y∣x) = fX(x) fY (y∣x).
(.)
So, fX is the marginal density of X, but instead of prescribing a prior distribution on X,
one’s task is to estimate this distribution without using prior information. The estimator of
fX will tell us whether one parameter X = xo suffices for all Yi, viz. if the estimator of fX
has most of its mass near x = xo, or if one has (mostly) a mixture with a few components,
or indeed a continuous mixture.
Note that
fY (y) = ∫X fY (y∣x) fX(x) dT∞(x).
(.)
Defining the integral operator K by
[Kf ](y) = ∫X fY (y∣x) f (x) dT∞(x),
y ∈Y,
(.)
the maximum likelihood problem for estimating fX is
minimize
−
n
n
∑
i=
log[Kf ](Yi)
subject to
f ∈T .
(.)
Note that the problem (> .) is very much like the problem (> .). Indeed, in very
much the same way as in > Sect. .., one derives the em algorithm,
fk+(x) = fk(x) ⋅
n
n
∑
i=
fY (Yi∣x)
[Kfk](Yi) ,
x ∈X.
(.)


EM Algorithms
This pretty much exhausts the simple examples that lead to this “same old” em
algorithm.
.
Emission Tomography
..
Flavors of Emission Tomography
There are at least three flavors of emission tomography, viz. single photon emission tomog-
raphy or SPECT (the c stands for “computerized”), positron emission tomography or
PET, and time-of-flight PET (TOFPET). In all of these cases, given measurements of the
emissions, the objective is to reconstruct the three-dimensional distribution of a radio-
pharmaceutical compound in the brain, giving insight into the metabolism in general and
blood flow in particular in the brain.
In the single photon version, single photons are emitted in random locations in the
brain and are detected (or not, as the case may be) by detectors situated around the head.
In the positron version, single positrons are emitted in random locations. The positrons
travel short distances until they are annihilated by single electrons, at which instances pairs
of photons are created which fly off in nearly opposite random directions. The pairs of
photons may then be detected by pairs of detectors. Thus, positron emission tomography
amounts to double photon emission tomography. In the time-of-flight version of PET, the
arrival times of the pairs of photons are recorded as well, which gives some information
on the location of the emission. The time-of-flight version will not be considered further.
Although the specifics are different, in their idealized form, the reconstruction problems
for SPECT and PET are just about the same. For some of the details of the not-so-ideal
circumstances in actual practice, see, e.g., [], [].
..
The Emission Tomography Experiment
The data collection experiment for emission tomography may be described as follows. Con-
sider a three-dimensional Poisson random field living in an open ball Ω ∈Rd (with d = ).
Here, “events” (viz. the creation of a single or double photon) happen with spatial intensity
per unit time denoted by fZ(z), z ∈Ω. This means that for any Borel subset C of Ω, the
number N(C, t , δt) of events that happen inside C during a time interval ( t , t +δt) does
not depend on t and is a Poisson random variable with mean
E[N(C, t , δt)] = δt ∫C fZ (z) dμ(z).
(.)
Moreover, if C, C, . . . , Cm are disjoint Borel subsets of Ω and (ti, ti + δti),
i
=
,, . . ., m, denote arbitrary (deterministic) time intervals, then the counts
N(C, t, δt), N(C, t, δt), . . . , N(Cm, tm, δtm), are independent. One may choose the
unit of time ΔT in such a way that fZ is the density of a probability measure with

EM Algorithms 

respect to Lebesgue measure on Ω. Then, in a time interval ( t , t + λΔT), the number
N = N(Ω, t , λΔT) of events that occur throughout Ω is a Poisson random variable with
mean
λ∫Ω fZ (z) dμ(z) = λ.
This may be written succinctly as
N ∼Poisson(λ).
(.)
For more on spatial Poisson processes, see, e.g., [], > Sect. ...
Returning to the experiment, conditional on N = n, during the time interval (, λΔT)
one collects a random sample Z, Z, . . . , Zn (of sample size n) of the random variable Z,
the random location of an event, with density fZ with respect to Lebesgue measure. The
random variable Z lives in the statistical space (Ω, BΩ,PΩ) with BΩ the σ-algebra of Borel
subsets of Ω and PΩ the collection of probability measures that are absolutely continuous
with respect to Lebesgue measure. The events themselves are detected by detectors or pairs
of detectors, denoted by B, B, . . . , Bm, which one may view as disjoint subsets (or antipo-
dal subsets) of a sphere surrounding Ω. For each event at a location Zi, there is a random
index J ∈M = {,, . . . , m} such that the event is detected by the detector (pair) BJ. Thus,
J lives in the statistical space (M,M, Vm), see (> .). The random variable (Z, J) is
absolutely continuous with respect to the product measure μ × α, and its density is
fZ,J(z, j) = fJ∣Z (j∣z) fZ (z),
z ∈Ω , j ∈M,
(.)
with fJ∣Z determined by geometric considerations. See, e.g., []. Assume that this is
known. Assuming that every event is detected, then fJ∣Z is a conditional density, so
m
∑
j=
fJ∣Z(j∣z) = 
for all z.
(.)
Note that then
fJ(j) = ∫Ω fJ∣Z (j∣z) fZ (z) dμ(z),
j ∈M.
(.)
So, conditional on N = n, one may pretend to have a random sample (Z, J),
(Z, J), . . .,(Zn, Jn) of the random variable (Z, J). This gives rise to the usual form of
the actual data to wit the bin counts
N j =
n
∑
i=
(Ji = j).
(.)
Continuing (and no longer conditioning on N = n), then N, N, . . . , Nm are indepen-
dent Poisson random variables with E[N j] = [KfZ](j),
N j ∼Poisson (λ [KfZ ](j)),
j ∈M,
(.)
where K : L(Ω, dμ) 8→L(M, dα) is defined by
[Kφ](j) = ∫Ω fJ∣Z(j∣z) φ(z) dμ(z),
j ∈M.
(.)


EM Algorithms
This concludes the description of the ideal emission tomography experiment. In reality,
quite a few extra things need to be taken into account, such as the attenuation of photons
by tissue and bone in the head, see, e.g., []. For the treatment of background noise, see,
e.g., [] and references therein.
..
The Shepp–Vardi EM Algorithm for PET
After these preparations, the maximum likelihood problem for estimating fZ may be for-
mulated. The observed data are the count data N, N, . . . , Nm, which leads to the problem
minimize
−
N
m
∑
j=
N j log[Kf ](j)
subject to
f ∈PΩ.
(.)
Alternatively, and this is actually more convenient, one may view the total number of
detected events N and J, J, . . . , JN as the actual data, which gives
minimize
−
N
N
∑
i=
log[Kf ](Ji)
subject to
f ∈PΩ.
(.)
In view of Remark following Remark (> .), these two problems are equivalent.
So far, nothing has been said about approximating/representing the estimators in terms
of pixels or voxels. Let {Cp}ℓ
p=be a partition of Ω, and let Pℓ⊂PΩ be the space of step
functions that are constant on each Cp. Thus, with Vℓdefined as in (> .),
Pℓ=
⎧⎪⎪⎨⎪⎪⎩
ℓ
∑
p=
xp bp(⋅)∣x ∈Vℓ
⎫⎪⎪⎬⎪⎪⎭
,
(.)
where
bp(z) = ∣Cp ∣−(z ∈Cp),
z ∈Ω.
(Note however, that one may take other basis functions.) The discretized maximum
likelihood problem is then obtained by restriction
minimize
−
N
N
∑
i=
log[Kf ](Ji)
subject to
f ∈Pℓ.
(.)
From the description of the experiment, it is clear that the missing data are the Zi , so the
complete data set is (Z, J),(Z, J), . . .,(ZN, JN), a random sample (of random sample
size N) of the random variable (Z, J). The joint density of N,(Z, J),(Z, J), . . . ,(ZN, JN)
is then
λn
n ! e−λ
n
∏
i=
{fJ∣Z (ji∣zi) fZ (zi)},
(.)
so that the complete maximum likelihood problem is
minimize
−
N
N
∑
i=
log{fJ∣Z(Ji∣Zi) f (Zi)}
subject to
f ∈Pℓ.
(.)

EM Algorithms 

In the objective function, the terms corresponding to the Poisson distribution of N have
been omitted, and the scaling /N was applied.
For the E-step of the em algorithm, consider the computation of
E[−log {fJ∣Z (j∣z) f (Z)}∣J, f] ,
(.)
assuming the approximation fto fZ. Since
fZ∣J(z∣j) =
fJ∣Z (j∣z) fZ(z)
fJ(j)
,
(.)
this gives for the conditional expectation (> .) the expression
−∫Ω
fJ∣Z (j∣z) fZ(z)
fJ(j)
log f (z) dμ(z),
(.)
where the contribution involving the known fJ∣Z may be ignored, since it does not depend
on f .
Consequently, the E-step leads to the problem
minimize
−∫Ω f(z) log f (z) dμ(z)
subject to
f ∈Pℓ,
(.)
where
f(z) = f(z) ⋅
N
N
∑
i=
fJ∣Z (Ji∣z)
(∫Ω fJ∣Z (Ji∣s) f(s) dμ(s))
.
(.)
Note that fis a density.
In terms of the representation of elements in Pℓ,
f (z) =
ℓ
∑
p=
xp ap(z),
(.)
with ap(z) = ∣Cp ∣−(z ∈Cp) as in (> .), and likewise for fand f, this leads to
∫Ω f(z) log f (z) dμ(z) =
ℓ
∑
p=
x[]
p
log {xp ∣Cp ∣−}
=
ℓ
∑
p=
x[]
p
log xp −log ∣Cp ∣
ℓ
∑
p=
x[]
p
(.)
=
ℓ
∑
p=
x[]
p
log xp −log ∣Cp ∣,
where
x[]
p
= x[]
p
⋅
N
N
∑
i=
a(Ji, p)
⎛
⎝
ℓ
∑
q=
a(Ji, p) x[]
p
⎞
⎠
,
(.)


EM Algorithms
with
a(j, p) = ∫Rd fJ∣Z (j∣z) ap(z) dμ(z) = ∫Cp
fJ∣Z(j∣z) dμ(z).
(.)
Note that
ℓ
∑
p=
x[]
p
= , and by (> .) that
m
∑
j=
a(j, p) = 
for all p.
(.)
So, the E-step gives
minimize
−
ℓ
∑
p=
x[]
p
log xp
subject to
x ∈Vℓ.
(.)
In > Sect. .., it was already shown that the solution of the problem (> .) is given by
x = x[]. So (> .) is the iterative step of the em algorithm. Of course, using Remark ,
the iterative step for x[] may be rewritten in terms of the bin counts as
x[]
p
= x[]
p
⋅
N
m
∑
j=
N j a(j, p)
⎛
⎝
ℓ
∑
q=
a(j, p) x[]
p
⎞
⎠
.
(.)
Observe again the similarity with the em algorithms for the simple examples in > Sect. ..
Remark 
The problem (> .) is not really discretized. The actual discretized problem
is
minimize
−
N
m
∑
j=
N j log[Ax]j +
ℓ
∑
p=
xp
subject to
x ∈Vℓ,
(.)
with Vℓgiven by (> .) and A ∈Rm×ℓhas components a(j, p) given by (> .). This
uses Remark .
Remark 
To finish, note that the original derivation by Shepp and Vardi [] involved
the missing data M(j, p), the number of events in each “cell” of Ω that contribute to the
counts N j,
M(j, p) =
m
∑
i=
(Ji = j)(Zi ∈Cp),
p = ,, . . . , ℓ.
This calls for a rather complicated relation between the M(j, p) and the N j. In particular,
one does not have random samples of the appropriate random variables. It gets much sim-
plified if one introduces the random variables I, I, . . . , Im which indicate to what “cell”
the event Zi belongs to. So, Ii = p if
(Zi ∈Cp) = .
Then for the complete data one gets back to considering random samples of random vari-
ables, viz. (J, I). This would provide for an alternative approach to discretization but would

EM Algorithms 

lead to the same em algorithm. This is essentially the “list mode” approach of Parra and
Barrett []. See also [].
..
Prehistory of the Shepp–Vardi EM Algorithm
The earliest reference to maximum likelihood estimation in emission tomography is the
afore-mentioned paper by Rockmore and Macovski []. In astronomy, an early reference
is Lucy []. The em algorithm for these maximum likelihood problems was introduced
by Shepp and Vardi [] and independently by Lange and Carson []. See also [] for
a completely different setting. The em algorithm for SPECT is essentially the same, see,
e.g., [] and references therein.
The algorithm itself may be viewed as a method for approximately solving the integral
equation with moment discretization
[Kf ](j) = N j
N ,
j = ,, . . . , m,
(.)
with K as in (> .). In particular, this may be applied to Fredholm integral equations
of the first kind. As such it was independently discovered in various settings many times
over, by Tarasko [] and Kondor [] in Physics, by Richardson [] and Lucy [] in
Astronomy, and perhaps other authors. It is interesting to note that both Richardson []
and Lucy [] derive the algorithm based on probabilistic considerations involving Bayes’
theorem, as in (> .). For more on the integral equations aspect see also [].
.
Electron Microscopy
In this section a recent application of em algorithms at the bleeding edge of science is
considered. As far as the em algorithm is concerned, the foundation is far from complete,
whether it be practical or theoretical.
..
Imaging Macromolecular Assemblies
Structural biologists are interested in the shape of biological objects at the macromolecu-
lar level. The tail of the Tbacteriophage is a famous example. Such objects are referred
to as macromolecular assemblies. To view objects that are this tiny, electron microscopy
seems to be the only tool available. Its use in structural biology goes back to DeRosier
and Klug, see []. Ideally, one would like to take a single tail of the Tbacteriophage
say, obtain electron micrographs (projections) from many directions, and reconstruct the
three-dimensional structure of the tail. Unfortunately, the bombardment with electrons
destroys the object, so that only one projection can be taken. The biologists have found
a way around this, but it comes at a price. Very roughly speaking, many tails are isolated


EM Algorithms
and suspended in a thin layer of water, which is then rapidly cooled to below freezing.
This results in vitreous water, with the tails suspended in it but randomly located and ori-
ented. A single electron micrograph of this layer is then taken. This is equivalent to taking
projections of a single tail in many different directions, corresponding to the random ori-
entations. For a precise description and analysis of the procedure, see []. Now, the price
one pays is that one has many projections of the tail but in random unknown directions.
Since these random directions may be viewed as missing data, it is clear that em algorithms
may be used. This was first realized by Scheres et al. []. A complication is that the objects
can appear in conformational states, which means that one has a mixture of finitely many
(different) objects. Another complication is that the signal-to-noise ratio is typically quite
small, in the % range.
..
The Maximum Likelihood Problem
Mathematically, following Scheres et al. [, ], the set-up may be described as follows.
Each object in the thin layer may be considered as being randomly chosen from a finite col-
lection xo
, xo
, . . . , xo
κ of κ objects. Its position and orientation in the thin layer is described
by five real-valued parameters: two location parameters and three Eulerian angles describ-
ing its orientation. Denote them by Θ, and the set of all possible Θ by Ξ. The problem
of finding the location parameters is referred to as the problem of alignment. For low
signal-to-noise ratios, maximum likelihood methods seem to be preferable [].
So, for a random object, one observes the projection in the form of a discretized
image Y,
Y = C ∗RΘ xo
K + ε.
(.)
Here, K is the random index into the collection of possible objects, RΘxK is the projection
data of the object in the “direction” Θ, C is the (known) contrast transfer function (due
to the experimental set-up), and ∗denotes the two-dimensional discretized convolution
operation. Finally, ε is the noise, assumed to be normal and isotropic, i.e., the components
of ε are jointly normal and the components of the variance–covariance matrix Vo satisfy
E[εp,q εr,s] = Σp−r,q−s,
(.)
where Σp−r,q−s is a function of (p −r)+ (q −s), the (squared) Euclidean distance, only.
In terms of the two-dimensional discrete Fourier transform, this means that (ignoring
boundary effects)
E[̂εP,Q ̂εR,S ] = [σ 
o]
P,Q
for
P = R
and
Q = S,
(.)
and = otherwise. Moreover, σ 
P,Q is rotationally symmetric, i.e., it is a function of P+Q.
Unfortunately, σ 
P,Q is unknown; it must be estimated. Moreover, σ varies with Y.

EM Algorithms 

So, the distribution of Y conditional on K = k and Θ = θ is given by
fY∣Θ,K (y∣θ, xo
k) =
exp (−
∥C ∗Rθ xo
k −y ∥
Vo )
(π)N/det(Vo)
,
(.)
where N is the size of Y (or y) and ∥z ∥
V = zTV−z . In terms of Fourier transforms this
reads as
∥C ∗Rθ xo
k −y ∥
Vo = ∑
P,Q
[σ−
o ]
P,Q ∣[̂C]
P,Q [Rθ xo
k]∧
P,Q −[̂y]P,Q ∣

,
(.)
log det(Vo) = ∑
P,Q
log [σ 
o ]
P,Q .
Now introduce the state of the system Y one wishes to estimate and the initial state of
one’s understanding of the system,
S = {
o, fΘ∣K , xo, σo}
and
S[] = { bar
[], φ[], x[], σ[]},
(.)
where
o
k = P[K = k], fΘ∣K is the density of Θ conditional on K, and xo and σo are as
before. The current understanding of the system is comprised of one’s best guesses so far
for the true system.
The distribution of Y may be expressed as
fY (y) =
κ
∑
k=
o
k ∫Ξ fY∣Θ,K (y∣θ, xk) fΘ∣K (θ∣k) d (θ),
(.)
where
= μ × ω, with μ – Lebesgue measure on R, and ω – the surface measure on the
sphere in R. Since it is reasonable to assume that the random location parameters are
independent of the random orientation, then
fΘ(θ, θ, θ, θ, θ) = g(θ, θ) h(θ, θ, θ),
(.)
for appropriate densities g and h. This reduces the actual dimension of the problem but for
notational ease, such a specialization will not be made.
Given a random sample Y, Y, . . . , Yn, of Y, the maximum likelihood problem for
estimating the unknown objects x, x, . . . , xκ may then be formulated:
minimize −
n
n
∑
i=
log (
κ
∑
k=
k ∫Ξ φ(Θi∣k) fY∣K,Θ (Yi∣xk, θ) d (θ)).
(.)
The unknowns are the probability vector
, the densities φ(θ∣k) (keep (> .) in mind),
the unknown objects x, x, . . . , xκ, and the variances [σ 
i ]P,Q. Note that there is some
similarity with the empirical Bayes problem of > Sect. ...


EM Algorithms
..
The EM Algorithm, up to a Point
Obviously, the goal is to derive an em algorithm for the solution of (> .), but the final
algorithm is not quite the real thing. It is clear that the missing data for each observed
projection Yi consists of the orientation, denoted by Θi, and which kind of object one is
looking at, encoded in the index Ki. The σ 
i and the objects xk are considered as param-
eters. So, the complete data set is (Yi, Θi, Ki), i = ,, . . . , n, and the complete maximum
likelihood problem is then to minimize
Λn(S)
def= −
n
n
∑
i=
log (
Ki φ(Θi∣Ki) fY∣Θ,K (Yi∣Θi, xKi , σi))
(.)
over all probability vectors
, all densities φ(⋅∣k), all variance matrices σ 
i , and all
x, x, . . . , xκ. However, recall that the φ(⋅∣k) have a simple structure.
For the E-step, the conditional expectation E[Λn(S)∣Y] is needed. By Bayes’ rule, the
distribution of (K, Θ) conditional on Y is described by
P[K= k∣Y= y] fΘ∣Y,K (θ∣y, xk) =
o
k fΘ∣K (θ∣k) fY∣Θ,K (y∣θ, xk, σ)
fY (y)
.
So, with the current state S[], and setting Yn = {Y, Y, . . . , Yn}, one gets
E[Λn(S)∣Yn, S[]] =
κ
∑
k=∫Ξ g[]
k
(θ, Yi) log (
k φ(θ∣k) fY∣Θ,K (Yi∣θ, x[]
k , σ[]
i )) d (θ),
(.)
where
g[]
k (θ, Yi) =
[]
k φ[](θ∣k) fY∣Θ,K (Yi∣θ, x[]
k , σ[]
i )
f []
Y (Yi)
,
with
(.)
f []
Y (y) =
κ
∑
k=
[]
k
∫Ξ φ[](θ∣k) fY∣Θ,K (y∣θ[], x[]
k ) d (θ).
(.)
This completes the E-step.
The M-step deals with the minimization of E[Λn(S)∣Yn, S[]] over S. This separates
into three problems. First, estimating
may be done by solving
minimize
−
κ
∑
k=
(log
k) ∫Ξ h[]
k (θ) d (θ)
subject to
is a probability vector,
(.)
where
h[]
k (θ) = 
n
n
∑
i=
g[]
k (θ∣Yi).
(.)

EM Algorithms 

One verifies that the solution is
=
[],
[]
k
= ∫Ξ h[]
k (θ) d (θ),
k = ,, . . . , κ.
(.)
Second, estimating the φ(⋅∣k) may be done by solving
minimize
−
κ
∑
k=∫Ξ h[]
k (θ, Yi) log φ(θ∣k) d (θ)
subject to
φ(⋅∣k) is a pdf, k = ,, . . . , κ.
(.)
This separates into κ minimization problems for the φ(⋅∣k). One verifies that the solutions
are, for k = ,, . . . , κ,
φ[](θ∣k) = φ[](θ∣k) ⋅
n
n
∑
i=
fY∣Θ,K (Yi∣θ, x[]
k , σ[]
i )
f []
Y∣K (Yi∣k)
,
(.)
where
f []
Y∣K (Yi∣k) = ∫Ξ φ[](θ∣k) fY∣Θ,K (Yi∣θ, x[]
k , σ[]
i ) d (θ).
(.)
Note that (> .) and (> .) are again multiplicative algorithms.
Third and last, one must estimate fY∣Θ,K, which boils down to estimating the xk and σi.
The problem is to minimize
−
κ
∑
k=∫Ξ

n
n
∑
i=
g[]
k (θ, Yi) log fY∣Θ,K (Yi∣θ, xk, σi) d (θ)
(.)
over xk and σi. Since log fY∣Θ,K (Yi∣θ, xk, σi) equals
∑
P,Q
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
∣̂CP,Q ⋅[(Rθ xk)∧]P,Q −[̂Yi]P,Q ∣

[σ 
i ]
P,Q
+ log [σ 
i ]
P,Q
⎫⎪⎪⎪⎪⎬⎪⎪⎪⎪⎭
,
here too the minimization problems separate. This may be solved for each xk by
minimizing
WLSk(xk) = ∫Ξ

n
n
∑
i=
g[]
k (θ, Yi)
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∣̂CP,Q ⋅[(Rθ xk)∧]P,Q −[̂Yi]
P,Q ∣

[σ 
i ]
P,Q
⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭
d (θ).
Denoting the minimizing xk by x[]
k , then the new σi is σi = σ[]
i
with
[σ[]
i
]
P,Q
=
κ
∑
k=∫Ξ

n
n
∑
i=
g[]
k (θ, Yi) ×
⎧⎪⎪⎨⎪⎪⎩
@@@@@@@@@@@
̂CP,Q ⋅[(Rθ x[]
k )
∧
]
P,Q
−[̂Yi]
P,Q
@@@@@@@@@@@
⎫⎪⎪⎬⎪⎪⎭
d (θ).


EM Algorithms
..
The ill-posed Weighted Least-Squares Problem
Up to this point, the M-step has been carried out exactly. The last part is to minimize
WLSk(xk). This is a weighted least-squares problem, which is nice, but it is ill-posed (or
ill conditioned after discretization), which implies that one cannot and should not solve it
exactly. In fact, Scheres et al. [] employ a Wiener filter to stably implement the “exact”
deconvolution procedure
[(Rθ x[]
k )
∧
]
P,Q
=
[̂Yi]P,Q
̂CP,Q
for all P, Q,
as in Penczek et al. [], compare with Byrne and Fiddy [], after which a weighted least-
squares version of ART (wlsart) is applied.
It should be observed that these problems are very large and are computationally very
expensive. It is clear that there is much room for algorithmic development, but the present
discussion ends here.
.
Regularization in Emission Tomography
..
The Need for Regularization
It is clear that the simple deconvolution problem (> .) and the more complicated PET
problem (> .) are ill-posed, let alone the electron microscopy problem of > Sect. ..
As already observed, in the PET problem (> .) one is trying to solve the compact
operator equation with moment discretization
[Kf ](j) = bj ,
j = ,, . . . , m,
(.)
where bj = N j/N. The possible nonexistence of solutions is dealt with by considering the
maximum likelihood problem, which may be reformulated as
minimize
KL(b,Kf )
subject to
f ∈P,
(.)
where KL is the discrete Kullback–Leibler divergence, see (> .), and
Kf = ([Kf ](),[Kf ](), . . . ,[Kf ](m))T .
The problem (> .) is similar to a least-squares problem. However, as in the case of the
least-squares approach to compact operator equations, this still does not take care of the
ill-posedness of the problem. So, the problem (> .) must be regularized.
The standard and practically the most often used method is to use the em algorithm
and stop the algorithm at some appropriate point in the iteration. See, e.g., [] for prac-
tical aspects and [] and [] for some theoretical results. The alternative is essentially
Tikhonov regularization of the negative log-likelihood, which also comes in the guises of
Bayesian or maximum a posteriori (MAP) likelihood estimation, Gibbs smoothing, or just

EM Algorithms 

roughness penalization. See [, , , , ]. A new twist is the use of total-variation
regularization and nonlinear diffusion filtering in connection with maximum likelihood
estimation and em algorithms, see, e.g., [,,,,], but unfortunately, this will not be
discussed further.
In many cases, the E-step of the em algorithm may be carried out explicitly, but not
so for the M-step. Here, some obvious modifications of the em algorithm or extraneous
iterative methods must be introduced. However, a few examples of explicit honest-to-
goodness em algorithms for regularized maximum likelihood problems are discussed: the
nems modification of the ems method of Silverman et al. [] and two em algorithms for
Good’s roughness penalization.
..
Smoothed EM Algorithms
In this section, the discussion centers on the ems algorithm of Silverman et al. [] and
the nonlinearly smoothed nems variant of Eggermont and LaRiccia [] in the context of
the deconvolution problem of
> Sect. ... Silverman et al. [] realized the necessity
for regularization of the maximum likelihood problem in that the em algorithm produces
increasingly rougher estimators. Initially, this is good since one typically starts out with
a uniform estimator and more features of the signal appear. However, as the iteration
progresses, the estimator becomes increasingly nonsmooth, giving rise to spurious fea-
tures. But Silverman et al. [] figured they knew how to fix the nonsmoothness: Add a
smoothing step to the em algorithm.
So, let Sh be a smoothing operator in the form
[Sh f ](y) = ∫Rd Sh(y −z) f (z) dμ(z),
y ∈Rd,
(.)
where Sh(z) = h−d S(h−z) for some bounded, continuous, symmetric pdf S ∈L(Rd),
possibly with compact support. The ems algorithm then takes the form
fk+/(z) = fk(z) ⋅
n
n
∑
i=
k(Yi −z)
[Kfk](Yi) ,
fk+= Sh fk+/.
(.)
So the general step of the ems algorithm is one step of the em algorithm followed by one
smoothing step.
Silverman et al. [] apply this algorithm to the simple problem of stereology (a
integral equation on a compact interval) and to positron emission tomography. In both
cases it seems to work quite well. Of course, the question is whether the algorithm
(> .) converges, and if so, what it converges to. Regarding the first question, see
[, ]. Characterizing the limit is not so easy, e.g., if one has a fixed point of the itera-
tion, does one then have a point where the gradient of some log-likelihood-like function
vanishes?


EM Algorithms
In retrospect, it is clear that adding a smoothing step to the em algorithm is a fun-
damentally sound idea, but the way it is implemented is not “right.” Indeed, in view of
the multiplicative character of the em algorithm, it seems that multiplicative smoothing
is called for. So, with Sh as before but with Sh(z) ⩾everywhere, define the nonlinear
smoothing operator N on nonnegative functions f by
[N(f )](y) = exp ([Sh(log f )](y)),
y ∈Rd.
(.)
Note that by convexity [N(f )](y) ⩽[Sh f ](y), so that N(f ) is always well defined. One
verifies that N performs multiplicative smoothing, i.e.,
N(f ⋅g) = N(f ) ⋅N(g),
(.)
where the dot means pointwise multiplication: [f ⋅g](y) = f (y)g(y) for all y. It now turns
out that the smoothed maximum likelihood problem
minimize
−
n
∑
i=
log[KN(f )](Yi)
subject to
f ∈P
(.)
admits the em algorithm
fk+/= N(fk),
fk+/(z) = fk+/(z) ⋅
n
n
∑
i=
k(Yi −z)
[Kfk+/](Yi) ,
(.)
fk+= Sh fk+/,
see []. In addition, the problem (> .) has a solution and it is unique, and the algo-
rithm (> .) converges to this solution in the Kullback–Leibler sense. See > Sect. ...
The algorithm (> .) is referred to as the nems algorithm: The general step consists
of a nonlinear smoothing step, one step of the original em algorithm, and a final (linear)
smoothing step. The practical performance on the toy stereology problem is just about
indistinguishable from the nems algorithm except that with the same smoothing operator
Sh, the nems algorithm does about twice the smoothing of the ems algorithm. Note that
the question about the proper choice of the smoothing operator (or smoothing matrix in
the discrete case) arises. This is in effect a question about the selection of the regularization
parameter in ill-posed problems. Unfortunately, this is not addressed in this chapter.
..
Good’s Roughness Penalization
Good’s roughness penalization of the deconvolution problem is a particular form of
Tikhonov regularization. The roughness penalty function of Good [] is
Φ(f ) = 
∫Rd
∣∇f (z)∣
f (z)
dμ(z).
(.)

EM Algorithms 

(The factor 
is for convenience only.) The maximum penalized likelihood problem is then
minimize
−
n
n
∑
i=
log[Kf ](Yi) + ∫Rd f (z) dμ(z) + hΦ(f )
subject to
f ∈P.
(.)
One can now perform the E-step as in > Sect. ..to arrive at the problem
minimize
−∫Rd f(y) log f (y) dμ(y) + ∫Rd f (y) dy + hΦ(f )
subject to
f ∈P,
(.)
where
f(y) = f(y) ⋅
n
n
∑
i=
k(Wi −y)
[K f](Wi) ,
y ∈Rd.
(.)
At this stage, the change of variable u =
√
f is obviously(?) useful. The problem then
becomes
minimize
−∫Rd f(y) logu(y) dμ(y) + ∥u ∥+ h∥∇u ∥
subject to
u ∈L(Rd), u ⩾,
(.)
where ∥⋅∥denotes the L(R) norm. Here it is convenient to drop the constraint ∥u ∥= .
Note that (> .) is a convex minimization problem. The Euler equations are given by
the boundary value problem
−hΔu + u = f
u
in
Rd,
∇u(y) 8→
for
∣y ∣8→∞,
(.)
where u is nonnegative. The M-step amounts to solving the boundary value problem.
The resulting algorithm converges, by arguments similar to those for the related discrete
case of the next section. See > Sect. ...
For the positron emission tomography problem, Miller and Roysam [] arrived at the
analog of this equation and solved the boundary value problem by finite differences, using
Jacobi’s method on a massively parallel computer. Of course, other methods come to mind.
Another EM algorithm:
There is another way to proceed. With the change of
variable u =
√
f as before, the objective function in (> .) becomes
−
n
n
∑
i=
log[K(u)](Yi) + ∥u ∥+ h∥∇u ∥.
(.)
Now introduce the convolution operator Sh with kernel Sh(z) = h−S(h−z), defined via
its Fourier transform as
̂S(ω) = ∫Rd S(z)e−πi ⟨z , ω ⟩dμ(z) = {+ ∣πω ∣}
−/,
(.)


EM Algorithms
for ω ∈Rd. Here and below, ∣ω ∣denotes the Euclidean norm of ω, and ⟨ω , z ⟩denotes the
inner product on Rd. In fact, then
S(z) = −(d−)/π−(d+)/∣z ∣−(d−)/K(d−)/(∣z ∣),
z ∈Rd,
(.)
where K
is the modified Bessel function of the second kind of order . Aronszajn and
Smith [] turns out to be the ideal reference for this.
The convolution operator is defined as
[Sh f ](z) = ∫Rd Sh(z −s) f (s) dμ(s),
z ∈Rd,
(.)
and satisfies (Sh f )∧(ω) = {+ (πh ∣ω ∣)}
−/̂f (ω)
for ω ∈Rd.
The net effect is that v = Shu satisfies
∥u ∥+ h∥∇u ∥= ∥v ∥,
(.)
so that the final change of variable f = M(w), where
[M(w)](y) = {[Sh
√w ] (y)}
,
y ∈Rd,
(.)
transforms the original maximum likelihood problem (> .) into
minimize
−
n
n
∑
i=
log[KM(w)](Yi) + ∫Rd w(y) dμ(y)
subject to
w ∈P.
(.)
(Actually, the pdf constraints are treated a bit cavalierly. Obviously f and w cannot both
be pdfs, but let it pass.)
It now turns out that there is an em algorithm for the smoothed maximum likelihood
problem (> .) to wit
wk+/= M(wk ),
wk+/(z) = {wk+/(y)}
/
⋅
n
n
∑
i=
k(Yi −z)
[Kwk+/](Yj) ,
(.)
wk+(z) = {wk(z)}
/[Shwk+/](z).
It has the same monotonicity properties as the nems algorithm, see > Sect. ... The orig-
inal method of Miller and Roysam [] satisfies similar monotonicity properties (assuming
that (> .) is solved exactly). See > Sect. ...
..
Gibbs Smoothing
Whereas Good’s roughness penalization was essentially aimed at the continuous setting,
attention now turns to a purely discrete point of view. So, let us consider the discrete

EM Algorithms 

maximum penalized likelihood problem
minimize
−
m
∑
j=
bj log[Ax]j +
ℓ
∑
p=
xp + λ G(x)
subject to
x ∈Vℓ,
(.)
with Vℓgiven by (> .) and A ∈Rm×ℓhas components a(j, p) given by (> .) and
λ > is the regularization parameter. The typical form of the penalization associated with
the name of Gibbs smoothing is
G(x) = ∑
p,q
wpq ϕ (σ−(xp −xq)),
(.)
for some convex function ϕ and nonnegative weights wpq and positive σ. Some typical
examples for ϕ are ϕ( t ) = log cosh( t ) and ϕ( t ) = ∣t ∣for t ∈R. The nonzero weights
wpq determine a neighborhood system. The neighborhood of the p-th component of x is
given by {q ∣wpq > }. Although x was encoded as a column vector, one should think of x
as a two-dimensional image or three-dimensional structure, so that neighboring image ele-
ments may have widely differing indices p and q. See [,,]. The approach of (> .)
originated with Green [].
The role of the penalty term is to penalize differences in neighboring components of x,
but large differences are not penalized much more. In fact, this is an argument for choosing
ϕ( t ) = min(∣t ∣, δ) for some δ.
To solve the problem (> .), again proceed iteratively, and perform the E-step of the
em algorithm. As before, this gives
minimize
−
ℓ
∑
p=
̃x[k]
p
log xp +
ℓ
∑
p=
xp + λ G(x)
subject to
x ∈Vℓ,
(.)
with ̃x[] given by (> .). For convenience, the constraint that x ∈Vℓis now dropped.
To solve the resulting problem, set the gradient equal to ,
−
̃x[]
p
xp
+ + λ ∇G(x) = .
(.)
Now, the one-step-late idea of Green [] is to approximately solve this equation by
x[]
p
=
̃x[]
p
+ λ [∇G(x[])]p
,
p = ,, . . . , ℓ.
(.)
This is referred to as osl-em. Green [] reports that this works well for small λ.
Regarding its convergence under appropriate conditions, see []. If (> .) is solved
exactly, then the resulting algorithm has the usual nice monotonicity properties, see
> Sect. ...


EM Algorithms
Hebert and Leahy [] observed that (> .) is similar in spirit to Jacobi’s method
for solving systems of linear equations, and they noticed that the Gauss–Seidel analog of
sequentially solving (> .) speeds up the computations. See also []. For other ways to
accelerate em algorithms, see > Sect. ..
.
Convergence of EM Algorithms
The convergence of the Shepp–Vardi em algorithm is based on two rather remarkable
monotonicity properties of the em algorithm, established using analytical methods by
Mülthei and Schorr []. Unfortunately, the geometric approach of Csiszár and Tusnády
[], that seems to explain why the Mülthei–Schorr approach works, is not discussed.
See []. However, the methods generalize in different ways. See
> Sect. ..
..
The Two Monotonicity Properties
Consider the discretized maximum likelihood problem of positron emission tomography,
repeated here for convenience:
minimize
L(x)
def= −
m
∑
j=
bj log[Ax]j +
ℓ
∑
p=
xp
subject to
x ∈Vℓ,
(.)
where bj = N j/N.Here, Vℓis given by (> .) and A ∈Rm×ℓhas nonnegative components
a(j, p) given by (> .), with unit column sums
m
∑
j=
a(j, p) = ,
p = ,, . . . , ℓ.
(.)
It is clear that the problem (> .) is convex, and that solutions exist. The uniqueness is
guaranteed only if A has full column rank. Regardless, the set of minimizers, denoted by
C, is convex.
Recall that the em algorithm for solving (> .) is, for k = ,, . . .,
x[k+]
p
= x[k]
p
⋅[ATr[k]]p ,
p = ,, . . . , ℓ,
r[k]
p
=
bj
[Ax[k]]j
,
j = ,, . . . , m.
(.)
starting from some initial strictly positive probability vector x[].
The two monotonicity properties are as follows:
L(x[k]) −L(x[k+]) ⩾KL(x[k+], x[k]) ⩾,
(.)

EM Algorithms 

and, if x∗is any solution of (> .),
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾L(x[k]) −L(x∗) ⩾.
(.)
The meaning of the first monotonicity property is clear: It says that the likelihood decreases
if successive iterates are different. The second one says that the iterates get closer to every
minimizer as measured by the Kullback–Leibler “distance.” The everyday image is that if
one thinks of the set of minimizers as an airport, then the iterates land like a helicopter,
not like an airplane. This kind of monotonicity is called Féjèr monotonicity.
The two monotonicity properties imply that the em algorithm converges.
Theorem 
If x[] is strictly positive, then the sequence {x[k]}k generated by the em
algorithm (> .) converges to a solution, say x∗∗, of the maximum likelihood problem
(> .). In particular,
lim
k→∞KL(x∗∗, x[k]) = .
Proof
The first inequality says that the negative log-likelihood is strictly decreasing,
unless x[k] = x[k+]. If x[k] = x[k+] does indeed hold, then the second inequality says
that L(x[k]) = L(x∗), so that x[k] is a solution of (> .). In general, the second inequal-
ity implies that {KL(x∗, x[k])}k is a decreasing sequence. Since the sequence is bounded
from below (by ), it must have a limit, but then KL(x∗, x[k])−KL(x∗, x[k+]) 8→, which
implies that
L(x[k]) 8→L(x∗).
(.)
So, the negative log-likelihood converges. Finally, since ∑x[k]
p
= , the sequence {x[k]}p
is bounded, and hence has a convergent subsequence, say with limit x∗∗. By (> .),
then L(x∗∗) = L(x∗), so that x∗∗is a minimizer also. Now, in the second monotonicity
property, one may replace x∗by x∗∗, and then {KL(x∗∗, x[k])}k is decreasing. Since a
subsequence converges to , then the whole sequence converges to .
∎
It should be observed that the theorem is actually not very useful: When using the
algorithm (> .), one will always stop the algorithm well short of convergence. See,
e.g., [,]. Thus, the existence of maximum likelihood estimators is moot. One may think
of this as an unfortunate side effect of discretization. For the continuous version, say for the
deconvolution problem, one does indeed have the analogs of the above two monotonicity
properties, but the second one is vacuous, since the continuous maximum likelihood prob-
lem has no solutions. For the nems algorithm, one can show the existence of solutions as
well as its convergence by way of the two monotonicity properties. See > Sect. ...
In the following subsections, the two monotonicity properties are proved for the stan-
dard discrete Shepp–Vardi em algorithm, for the continuous version of the nems algorithm,
and for the exact version of Gibbs smoothing (but not for the one-step-late version). The
basic tool is the analytical proof of Mülthei and Schorr [], which is actually quite versatile,
as demonstrated in > Sect. ..


EM Algorithms
..
Monotonicity of the Shepp–Vardi EM Algorithm
Here, the two monotonicity properties of the em algorithm are exhibited, following the
proof of Mülthei and Schorr []. The first monotonicity property (> .) follows from
the derivation of the E-step of the em algorithm. However, here a purely analytical proof
is explained. Vardi et al. [] prove the two monotonicity properties using the geometric
results of Csiszár and Tusnády ().
It is useful to define the operator R on nonnegative vectors x ∈Rℓby
[R x]p = xp [AT (b/Ax)]p ,
p = ,, . . . , ℓ,
(.)
where b/Ax denotes the vector of componentwise quotients.
Lemma 
For all nonnegative x and y, with y strictly positive
L(x) ⩽L(y) + KL(R y, x) −KL(R y, y).
Proof
Note that for all nonnegative vectors x and y,
L(x) −L(y) = −
m
∑
j=
bj log [Ax]j
[Ay]j
+
ℓ
∑
p=
xp −yp.
Now, for strictly positive y one may write
[Ax]j
[Ay]j
=
ℓ
∑
p=
a(j, p) yp
[Ay]j
⋅xp
yp
.
For each j, this is a convex combination of the points xp/yp, p = ,, . . . , ℓ. Since t O→
−log t is convex, then by Jensen’s inequality
L(x) −L(y) ⩽−
m
∑
j=
bj
ℓ
∑
p=
a(j, p) yp
[Ay]j
log xp
yp
+
ℓ
∑
p=
xp −yp
⩽
ℓ
∑
p=
−yp [ATr]p log xp
yp
+ xp −yp,
where in the last step the order of summation was interchanged. The lemma follows.
∎
Proof of the first monotonicity property (> .)
In the inequality of the lemma above,
take y = x[k] and x = R y = R x[k] = x[k+]. Then L(x[k+]) −L(x[k]) ⩽−KL(x[k+], x[k]).
∎
Proof of the second monotonicity property (> .)
Start with
KL(x∗, x[k]) −KL(x∗, x[k+]) =
ℓ
∑
p=
x∗
p log
x[k+]
p
x[k]
p
=
ℓ
∑
p=
x∗
p log [AT {
b
Ax[k] }]
p
.

EM Algorithms 

Now, if x∗solves (> .), then it must satisfy the necessary and sufficient conditions for
a minimum
x∗⩾,
∇L(x∗) ⩾,
x∗
p [∇L(x∗)]p = 
for all
p.
The last condition says that x∗
p (−[ATr∗]p + ) = , where r∗
j = bj/[Ax∗]j for all j, so that
if x∗
p > , then [ATr∗]p = . So, for x∗
p > , write
[AT {
b
x[k] }]
p
=
m
∑
j=
a(j, p) bj
[Ax∗]j
⋅[Ax∗]j
[Ax[k]]j
,
which is a convex combination of the points [Ax∗]j/[Ax[k]]j , so by the concavity of the
logarithm,
ℓ
∑
p=
x∗
p log [AT {
b
Ax[k] }]
p
⩾
ℓ
∑
p=
x∗
p
m
∑
j=
a(j, p) bj
[Ax∗]j
⋅log [Ax∗]j
[Ax[k]]j
⩾
m
∑
j=
bj log [Ax∗]j
[Ax[k]]j
= KL(b, Ax[k]) −KL(b, Ax[k+]),
where the last equality follows from ∑x[k]
p
= ∑x[k+]
p
= ∑bj.
∎
..
Monotonicity for Mixtures
Here, the two monotonicity properties of the em algorithm for mixtures of known densities
are discussed. The difference with the Shepp–Vardi em algorithm is that the system matrix
is not normalized to have unit column sums. It will transpire that this does not make any
difference.
Recall that the problem is to estimate the pdf
fY (y) =
m
∑
j=
xo,j a j(y),
y ∈Rd,
(.)
where the a j are known pdfs, and xo is an unknown probability vector, given a random
sample Y, Y, . . . , Yn of the random variable Y with density fY . Define the matrix A ∈Rn×m
by
Ai j = ai j = a j(Yi)
for all i and j.
(.)
The em algorithm for estimating xo is, starting from the uniform vector x[],
x[k+] = Mx[k],
(.)
where the iteration operator M is defined as
[Mx]j = x j ⋅
n
n
∑
i=
ai j
[Ax]i
,
j = ,, . . . , m.
(.)


EM Algorithms
One begins again with deriving the majorizing function inequality. However, first
replace the maximum likelihood problem (> .) by the equivalent
minimize
Ln(x)
def= −
n
n
∑
i=
log ⎛
⎝
m
∑
j=
x j ai j
⎞
⎠+
m
∑
j=
x j
subject to
x ⩾.
(.)
Note that the constraint that x be a probability vector was traded for the added sum in the
objective function.
The majorizing function inequality is the same as before, as is its proof. Then the first
monotonicity property follows.
Lemma 
If x and y are nonnegative probability vectors, with y strictly positive, then
Ln(x) ⩽Ln(y) + KL(M y, x) −KL(M y, y).
Note that the minimizer of the right-hand side (over x) is x = My.
Lemma 
Starting from a strictly positive x[], the iterates of the em algorithm (> .)
satisfy
Ln(x[k]) −Ln(x[k+]) ⩾KL(x[k+], x[k]) ⩾.
The second monotonicity property is the same also, but there is a slight change in its
proof.
Lemma
Let x∗be a solution of (> .). Starting from a strictly positive x[], the iterates
of the em algorithm (> .) satisfy
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾Ln(x[k]) −Ln(x∗) ⩾.
Proof
Since ∑x[k]
j
= ∑x[k+]
j
= , one has as usual
KL(x∗, x[k]) −KL(x∗, x[k+]) =
m
∑
j=
x∗
p log x[k+]
p
x[k]
p
=
ℓ
∑
j=
x∗
j log [AT { (/n)
Ax[k] }]
j
.
Now, if x∗solves (> .), then it must satisfy the necessary and sufficient conditions for
a minimum
x∗⩾,
∇Ln(x∗) ⩾,
x∗
j [∇Ln(x∗)]j = 
for all
j.
The last condition says that x∗
j (−[ATr∗]j + ) = , where r∗
i = (/n)/[Ax∗]i for all i, so
that if x∗
j > , then [ATr∗]j = . So, for x∗
j > , write
[AT { (/n)
Ax[k] }]
j
=
n
∑
i=
(/n) ai j
[Ax∗]i
⋅[Ax∗]i
[Ax[k]]i
,

EM Algorithms 

which is a convex combination of the points [Ax∗]i/[Ax[k]]i , so by the concavity of the
logarithm,
m
∑
j=
x∗
j log[AT { (/n)
Ax[k] }]
j
⩾
m
∑
j=
x∗
j
n
∑
i=
(/n) ai j
[Ax∗]i
⋅log [Ax∗]i
[Ax[k]]i
⩾
n
∑
i=
(/n) log [Ax∗]i
[Ax[k]]i
+
m
∑
j=
x[k]
j
−x∗
j = Ln(x[k]) −Ln(x∗),
where the last equality follows from ∑x[k]
j
= ∑x∗
j = .
∎
The convergence of the iterates of the em algorithm follows.
..
Monotonicity of the Smoothed EM Algorithm
Here, the monotonicity properties of the nems algorithm for the smoothed maximum
likelihood problem (> .) are proved. The problem is
minimize
Ln(f )
def= −
n
n
∑
i=
log[K N(f )](Wi) + ∫Rd f (y) dμ(y)
subject to
f ∈L(Rd),
f ⩾.
(.)
Since this is an infinite dimensional problem, showing that the iterates of the nems algo-
rithm converges to a solution of (> .) is a bit more involved. In particular, it requires
us to show the existence of solutions. The only remarkable thing about the proofs of the
two monotonicity properties for the nems algorithm is that apart from a few cosmetic
changes, they are exactly the same as for the Shepp–Vardi em algorithm. The argument
follows Eggermont [].
The nems algorithm (> .) may be represented as
gk+= Th fk,
fk+= Sh gk+,
(.)
starting from a strictly positive initial guess f, assumed to be a pdf. Here, the map Th is
defined as
[Th f ](z) = [N(f )](z) ⋅
n
n
∑
i=
k(Wi −z)
[KN(f )](Wi) .
(.)
The claim is now that the iterates of the nems algorithm satisfy the same two monotonicity
properties (> .) and (> .). The crux is again an analytical proof of what amounts
to the E-step of the em algorithm.
Lemma 
For all densities φ and ψ,
Ln(φ) ⩽Ln(ψ) + KL(ShThψ, φ) −KL(ShThψ,ψ).


EM Algorithms
Proof
Similar to the proof of Lemma , one gets that
Ln(φ) −Ln(ψ) ⩽−∫Rd [Thψ](z) log [N(φ)](z)
[N(ψ)](z) dμ(z).
Since
log ([N(φ)](z)/[N(ψ)](z)) = [Sh log (φ/ψ)] (z),
and Sh is a symmetric operator, then
−∫Rd [Thψ](z) log [N(φ)](z)
[N(ψ)](z) dμ(z) = −∫Rd [ShThψ](y) log φ(y)
ψ(y) dμ(y),
and the lemma follows.
∎
Lemma 
The iterates fk generated by the nems algorithm (> .) satisfy
Ln(fk) −Ln(fk+) ⩾KL(fk+, fk) ⩾.
Proof
In Lemma take φ = fk+and ψ = fk. Then ShThψ = fk+.
∎
The second monotonicity property is actually a little bit stronger than the one for the
Shepp–Vardi em algorithm. Note that by the convexity of the KL function jointly in both
its arguments, KL(Shφ,Shψ) ⩽KL(φ,ψ).
Lemma 
Let f ∗be a solution of (> .), with KL(f ∗, f) < ∞. Then, the iterates fk
generated by the nems algorithm (> .) satisfy
KL(f ∗, fk) −KL(f ∗, fk+) ⩾KL(f ∗, fk) −KL(Th f ∗,Th fk) ⩾Ln(fk) −Ln(f ∗) ⩾.
Proof
Start in the usual fashion and obtain
Ln(fk) −Ln(f ∗) = 
n
n
∑
i=
log [KN f ∗](Wi)
[KN f∗](Wi)
= 
n
n
∑
i=
[KN f ∗](Wi)
[KN f ∗](Wi) log [KN f ∗](Wi)
[KN fk](Wi)
= ∫Rd [N f ∗](z) ⋅
n
n
∑
i=
k(Wi −z)
[KN f ∗](z) log [KN f ∗](Wi)
[KN fk](Wi) dμ(z).
Now, one would like to get a convex combination, so multiply and divide by the sum of the
weights k(Wi −z)/[KN f ∗](Wi). Then, the concavity of the logarithm gives that the last
expression is dominated by
∫Rd [N f ∗](z) ⋅
n
n
∑
i=
k(Wi −z)
[KN f ∗](z) log
⎛
⎜⎜⎜⎜
⎝

n
n
∑
i=
k(Wi −z)
[KN fk](z)

n
n
∑
i=
k(Wi −z)
[KN f ∗](z)
⎞
⎟⎟⎟⎟
⎠
dμ(z).

EM Algorithms 

Now, this expression may be cleaned up as
∫Rd [Th f ∗](z) log( [Th fk](z)
[N fk](z) ⋅[N f ∗](z)
[Th f ∗](z) ) dμ(z).
After splitting up the logarithm, note that
∫Rd [Th f ∗](z) log ( [Th fk](z)
[Th f ∗](z) ) dμ(z) = −KL(Th f ∗,Th fk)
because Thφ is a pdf if φ is one, and also
∫Rd [Th f ∗](z) log ( [N f ∗](z)
[N fk](z) ) dμ(z) = ∫Rd [Thg∗](z) [Sh log f ∗
fk
](z) dμ(z)
= ∫Rd [ShTh f ∗](y) log f ∗(y)
fk(y) dμ(y) = KL(f ∗, fk),
since ShTh f ∗= f ∗. Putting all of this together shows that
Ln(fk) −Ln(f ∗) ⩽KL(f ∗, fk) −KL(Th f ∗,Th fk),
and the lemma follows.
∎
The two monotonicity properties imply that the nems algorithm converges to a solution
of the smoothed maximum likelihood problem (> .), and that this problem actually
has a solution.
Theorem 
The smoothed maximum likelihood problem (> .) has a solution f ∗∈
L(Rd).
Proof
One first shows that Ln(f ) is bounded from below. Let f be a pdf on Rd such that
[KN f ](Wi) > for all i. Let ∈Rn be the vector of all ones, and let vi = [KN f ](Wi).
Then,
Ln(f ) = KL( 
n ,v) −
n
n
∑
i=
log[KN f ](Wi) + ∫Rd f (y) dμ(y).
Now, by convexity [N f ](z) ⩽[Sh f ](z) for all z, so that
[KN f ](z) = ∫Rd k(Wi −z)[N f ](z) dμ(z)
⩽∫Rd k(Wi −z)[Sh f ](z) dμ(z)
= ∫Rd f (y) ∫Rd k(Wi −z) Sh(z −y) dμ(y) dμ(z)
⩽μh ∫Rd f (y) dμ(y) = μh,
where
μh = sup
y∈Rd ∫Rd k(Wi −z) Sh(z −y) dμ(y) dμ(z) ⩽sup
y∈Rd Sh(z),


EM Algorithms
since k is a pdf. Since Sh(z) = h−dS(h−z), the boundedness of S then gives that μh < ∞
for fixed h > . It follows that Ln(f ) is bounded from below.
Now, let {φk}k be a minimizing sequence for Ln(f ). Apply one step of the nems algo-
rithm to each φk, so ψk = ShThφk , k = ,, . . . . By the first monotonicity property, then
{ψk}k is a minimizing sequence also. Since each Thφk is a pdf, then the ψk are uniformly
continuous on Rd, and so it has a subsequence which converges in the strict topology,
i.e., uniformly on every compact subset of Rd, say with limit ψ∗. This is the Arzelà–Ascoli
theorem for the strict topology, see []. Then, along this subsequence
[KN(ψk)](Wi) 8→[KN(ψ∗)](Wi),
and it follows that again along this same subsequence
Ln(ψk) 8→Ln(ψ∗).
Since the whole sequence {ψk}k was a minimizing sequence, this shows that ψ∗solves the
problem (> .).
∎
Theorem 
For f [] strictly positive with KL(f ∗, f []) < ∞, the nems algorithm converges
to a solution of (> .).
Proof
The proof is just about the same as for the discrete em algorithm. Thus, the first
monotonicity property shows that {Ln(fk)} is decreasing. The second monotonicity prop-
erty shows that {KL(f ∗, fk)}k is decreasing as well, and so has a nonnegative limit. But then
KL(f ∗, fk) −KL(f ∗, fk+) converges to , so that again the second monotonicity prop-
erty gives that the nems sequence {fk}k is a minimizing sequence. All one has to do is
extract a convergent subsequence, but that follows from the argument in the proof of the
existence of convergent subsequences. Thus, there exists a subsequence which converges
to some element f ∗∗in the strict topology. Then [KN(fk)](Wi) 8→[KN(f ∗∗)](Wi)
for all i, and then Ln(fk) 8→Ln(f ∗∗) initially only along the subsequence, but since
{Ln(fk)}k is decreasing, then along the whole sequence. Now, if KL(f ∗∗, f) < ∞, then
apply the second monotonicity property with the solution f ∗∗, and then one finds that
KL(f ∗∗, fk) 8→along the subsequence, but since {KL(f ∗∗, fk)}k is decreasing, then
along the whole sequence.
If KL(f ∗∗, f) = ∞, then for < ε < but arbitrary, apply the second monotonicity
property with the solution f ∗
ε = ε f ∗+ (−ε)f ∗∗. Then KL(f ∗
ε , f) < ∞and KL(f ∗
ε , fk)
converges, and it is easy to see that
lim
k→∞KL(f ∗
ε , fk) = KL(f ∗
ε , f ∗∗) = o()
for
ε 8→.
It follows that KL(f ∗∗, fk) 8→.
∎

EM Algorithms 

..
Monotonicity for Exact Gibbs Smoothing
The two monotonicity properties also hold for penalized maximum likelihood estimation
with Gibbs smoothing, at least if the M-step of the em algorithm is performed exactly,
see (> .) below. This is at least approximately the case in the approach of Miller and
Roysam [] but not so for the one-step-late approach of Green [].
Here, the monotonicity properties are proved for “arbitrary” Gibbs functionals. So,
consider the maximum penalized likelihood problem for emission tomography
minimize
Λ(x)
def= −
m
∑
j=
bj log[Ax]j +
ℓ
∑
p=
xp + G(x)
subject to
x ⩾,
(.)
where G(x) is convex and differentiable and satisfies
lim
∥x ∥→∞G(x) = +∞.
(.)
Assume that b is strictly positive, and that A satisfies the usual conditions (> .). Note
that typically, the roughness prior will be of the form λG(x) for some small positive
parameter λ. In the present context, one may as well take λ = .
The goal is again to derive the two monotonicity properties. The majorizing functional
inequality is just about the same as for the Shepp–Vardi em algorithm, see (> .). Recall
the definition of the operator R from (> .).
Lemma 
For all probability vectors x and y,
Λ(x) ⩽Λ(y) + KL(Ry, x) −KL(Ry, y) + G(x) −G(y),
where rj = bj/[Ay]j for j = ,, . . . , m.
Now, to minimize the right-hand side, set the gradient with respect to x equal to .
With y = x[k] this gives the next iterate implicitly as
x[k+]
p
=
x[k]
p
[ATr[k]]p
+ [∇G(x[k+])]p
,
p = ,, . . ., ℓ,
(.)
with r[k]
j
= bj/[Ax[k]]j for all j. Note that + [∇G (x[k+])]p > for all p, since the
equations
x[k+]
p
(+ [∇G (x[k+])]p) = x[k]
p ,
p = ,, . . . , ℓ,
have a solution (the minimization problem has a solution), and x[k] is strictly positive (by
induction).


EM Algorithms
The first monotonicity property is almost immediate. The second one takes more work.
For nonnegative vectors x, y, and w, define
KL(x, y∣w) =
ℓ
∑
p=
wp {xp log xp
yp
+ yp −xp}.
(.)
Lemma 
Starting with a strictly positive initial vector x[],
Λ(x[k]) −Λ(x[k+]) ⩾KL(x[k+], x[k] ∣w[k+]) ⩾,
where w[k+] = + ∇G(x[k+]).
Proof
From Lemma , one gets
Λ (x[k+]) −Λ (x[k])
⩽
ℓ
∑
p=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
⎛
⎜
⎝
w[k+]
p
x[k+]
p
log x[k]
p
x
[k+]
p
⎞
⎟
⎠
+ x[k+]
p
−x[k]
p
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+ G (x[k+]) −G (x[k]).
Now use that G(x[k+]) −G(x[k]) ⩽⟨∇G(x[k+]), x[k+] −x[k] ⟩.
∎
Lemma 
Let x∗be a solution of (> .). Then, starting from a strictly positive initial
guess x[],
KL(w∗⋅x∗,w[k] ⋅x[k]) −KL(w∗⋅x∗,w[k+] ⋅x[k+]) ⩾Λ(x[k]) −Λ(x∗) ⩾,
where w∗= + ∇G(x∗).
Proof
Write Λ(x) = L(x) + G(x). So L(x) is the unpenalized negative log-likelihood.
Now, as in > Sect. .., one has
L(x[k]) −L(x∗) =
m
∑
j=
{bj log [Ax∗]j
[Ax[k]]j
+ [Ax[k]]j −[Ax∗]j}
=
ℓ
∑
p=
{x∗
p [AT {r∗log Ax∗
Ax[k] }]
p
+ x[k]
p
−x∗
p},
with r∗
j = bj/[Ax∗]j for all j. Now, x∗solves the problem (> .), so by the previous
lemma, it must be a fixed point of the algorithm. So,
x∗
p =
x∗
p [ATr∗]p
+ [∇G(x∗)]p
,
p = ,, . . ., ℓ.

EM Algorithms 

Then, if x∗
p > , one must have [ATr∗]p/(+ [∇G(x∗)]p) = . Consequently, by convexity
[AT {r∗log Ax∗
Ax[k] }]
p
+ [∇G(x∗)]p
⩽log
[AT {r∗Ax∗
Ax[k] }]
p
+ [∇G(x∗)]p
,
which equals
log ⎛
⎝
x[k+]
p
x[k]
p
⋅
w[k+]
p
w∗p
⎞
⎠= log⎛
⎝
w[k+]
p
x[k+]
p
w[k]
p
x[k]
p
⎞
⎠+ log
w[k]
p
w∗p
.
Now, substitute this in the upper bound for L(x[k]) −L(x∗). This yields
L(x[k]) −L(x∗) ⩽
ℓ
∑
p=
w∗
p x∗
p log
w[k+]
p
x[k+]
p
w[k]
p
x[k]
p
+
ℓ
∑
p=
⎧⎪⎪⎨⎪⎪⎩
w∗
p x∗
p log
w[k]
p
w∗p
+ x[k]
p
−x∗
p
⎫⎪⎪⎬⎪⎪⎭
.
Now, after some bookkeeping, the first sum is seen to be equal to
KL(w∗⋅x∗,w[k] ⋅x[k]) −KL(w∗⋅x∗,w[k+] ⋅x[k+]) +
ℓ
∑
p=
{w[k+]
p
x[k+]
p
−w[k]
p
x[k]
p }.
Using the inequality log t ⩽t −, one gets that
L(x[k]) −L(x∗) ⩽KL(w∗⋅x∗,w[k] ⋅x[k]) −KL(w∗⋅x∗,w[k+] ⋅x[k+]) + rem
(.)
with the remainder
rem =
ℓ
∑
p=
{(w[k] −) (x∗
p −x[k]
p ) −w∗
p x∗
p + w[k+]
p
x[k+]
p
}.
Now, since w[k+]
p
x[k+]
p
= x[k]
p
[ATr[k]]p and likewise for w∗
p x∗
p, then
ℓ
∑
p=
w∗
p x∗
p =
m
∑
j=
bj =
ℓ
∑
p=
w[k+]
p
x[k+]
p
.
The remaining terms add up to ⟨∇G(x[k]), x∗−x[k] ⟩which is bounded by G(x∗) −
G(x[k]) (by convexity). Moving this to the left-hand side of the resulting inequality proves
the lemma.
∎
The convergence of the exact em algorithm with Gibbs smoothing now follows. Lange
[] proves the convergence of the one-step-late version of the algorithm by essentially
“soft” methods. It would be nice to see under what conditions the two monotonicity
properties carry over to this version.


EM Algorithms
.
EM-Like Algorithms
The analytical proofs of the inequalities of Lemmas (> ) and (> ) may be extended to
other interesting minimization problems. Rather surprisingly, some of these algorithms
enjoy the “same” two monotonicity properties as the em and nems algorithms (and the
proofs appear to be simpler). The problems under consideration are “positive” least-squares
problems and minimum cross-entropy problems. The main idea is that of majorizing func-
tions, as originally exploited by De Pierro [] in the maximum penalized likelihood
approach for emission tomography.
Again, it would have been nice to also outline the geometric approach of Csiszár
and Tusnády [], which just like the analytical approach of Mülthei and Schorr [],
is applicable to the minimum cross-entropy problems. However, it is not clear that the
Csiszár–Tusnády approach works for the “positive” least-squares problem: The Kullback–
Leibler distance shows up in the monotonicity properties. This is in effect due to the
multiplicative nature of the algorithms, as explained in the last section.
..
Minimum Cross-Entropy Problems
Consider again the system of equations
Ax = b,
(.)
in the emission tomography set-up, see (> .), with b a nonnegative vector. The interest
is in the following minimization problem:
minimize
CE(x)
def= KL(Ax, b)
subject to
x ∈Rℓ, x ⩾.
(.)
Here, “CE” stands for cross-entropy. (Why it makes sense to consider KL(Ax, b) instead of
KL(b, Ax) or even ∥Ax −b ∥is not the issue here.)
The objective is to obtain a majorizing function for CE(x) that would result in a nice
algorithm satisfying the “two” monotonicity properties similar to the em algorithm.
Prejudicing the proceedings somewhat, it is useful to define the operator R on nonneg-
ative vectors by
[Ry]p = yp exp ⎛
⎝[AT log b
Ay ]
p
⎞
⎠,
p = ,, . . ., ℓ.
(.)
Here, and elsewhere AT log(Ay/b) = ATv, with v j = log([Ay]j/bj) for all j.
Lemma 
For all nonnegative x, y ∈Rℓ,
CE(x) ⩽CE(y) + KL(x, Ry) −KL(y, Ry).
Proof
The starting point is the straightforward identity
CE(x) = CE(y) + KL(Ax, Ay) + ⟨x −y , ATr ⟩.
with rj = log([Ay]j/bj) for all j.

EM Algorithms 

Now, by convexity of the KL function jointly in both its arguments, the conditions
(> .) and (> .) on A imply that
KL(Ax, Ay) ⩽KL(x, y).
(.)
Finally, since [ATr]p = −log([Ry]p/yp), for all p, then
KL(x, y) + ⟨x −y , ATr ⟩= KL(x, Ry) −KL(y, Ry).
This completes the proof of the lemma.
∎
The inequality of the lemma immediately suggests an iterative algorithm for the mini-
mization of CE(x). Minimizing the right-hand side gives the optimal x as x = Ry with R
given by (> .). Thus, the iterative algorithm is, starting from a strictly positive x[] ∈Rℓ,
x[k+]
p
= [Rx[k]]p,
p = ,, . . . .
(.)
This is the simultaneous multiplicative algebraic reconstruction technique (smart algo-
rithm). It first appeared in [], see (also) Holte et al. [], and in Darroch and Ratcliff [],
who called it the iterative rescaling algorithm. The row-action version (mart) originated
with Gordon et al. []. Byrne [] developed block-iterative versions, see
> Sect. ...
The starting point of Censor and Segman [] was entropy maximization subject to the
linear constraints Ax = b, and arrived at various versions of mart including simultaneous
and block-iterative versions.
On to the two monotonicity properties. The first one is immediate.
Lemma 
If x[] is strictly positive, then the iterates of the smart algorithm (> .)
satisfy
CE (x[k]) −CE (x[k+]) ⩾KL(x[k], x[k+]).
Note the difference with the first monotonicity property (> .) for the Shepp–Vardi
em algorithm. The second monotonicity property is equally simple, but the precise form
must be guessed. (Actually, it follows from the proof.)
Lemma 
If x∗is a solution of the nonnegatively constrained least-squares problem
(> .), then, with x[] strictly positive,
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾CE(x[k+]) −CE(x∗) ⩾.
Proof
Observe that
KL(x∗, x[k]) −KL(x∗, x[k+])
=
ℓ
∑
p=
⎧⎪⎪⎨⎪⎪⎩
x∗
p log
x[k+]
p
x[k]
p
+ x[k]
p
−x[k+]
p
⎫⎪⎪⎬⎪⎪⎭
=
ℓ
∑
p=
(x[k]
p
−x∗
p) log
x[k]
p
x[k+]
p
−
ℓ
∑
p=
⎧⎪⎪⎨⎪⎪⎩
x[k]
p
log
x[k]
p
x[k+]
p
+ x[k+]
p
−x[k]
p
⎫⎪⎪⎬⎪⎪⎭
.
(.)


EM Algorithms
The last sum equals KL(x[k], x[k+]), and by Lemma , then
−KL(x[k], x[k+]) ⩾CE(x[k+]) −CE (x[k]).
The first sum equals
ℓ
∑
p=
(x[k]
p
−x∗
p) [AT log Ax[k]
b
]
p
= ⟨x[k] −x∗, ∇CE(x[k])⟩,
where ⟨⋅, ⋅⟩denotes the inner product on Rℓ, and ∇CE is the gradient of CE(x). By the
convexity of CE, then
⟨x[k] −x∗, ∇CE(x[k])⟩⩾CE(x[k]) −CE(x∗).
Summarizing, the above shows that
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾CE (x[k]) −CE(x∗) + CE(x[k+]) −CE(x[k])
= CE (x[k+]) −CE (x∗) .
This is the lemma.
∎
As before, the convergence of the smart algorithm follows starting from any strictly
positive vector x[].
Minimizing Burg’s entropy:
Compared with the minimum cross-entropy problem from
the previous section, the case of Burg’s entropy is problematic. For the positive system
Ax = b
with the normalization (> .), the minimum Burg entropy problem is
minimize
BE(x)
def=
m
∑
j=
{−log
bj
[Ax]j
+
bj
[Ax]j
}
subject to
x ⩾.
(.)
The first thing one notices is that it is not a convex problem. Then it is conceivable that the
solution set is not convex, and so a “second” monotonicity property is not likely to hold.
However, there is a majorizing function, which suggests a multiplicative algorithm, and
there is a “first” monotonicity property.
Lemma 
For all nonnegative x and y,
BE(x) ⩽BE(y) +
ℓ
∑
p=
{[ATq]p −[ATr]p
yp
xp
} (xp −yp),
where
rj =
bj
([Ay]j)
,
q j =

[Ay]j
,
j = ,, . . . , m.

EM Algorithms 

The algorithm suggested by the lemma comes about by minimizing the upper bound
on BE(x) with y = x[k], the current guess for the solution. This gives the minimizer as
x = x[k+],
x[k+]
p
= x[k]
p
⋅{ [ATr[k]]p
[ATq[k]]p
}
/
,
(.)
where
r[k]
j
=
bj
([Ax[k]]j)
,
q[k]
j
=

[Ax[k]]j
,
j = ,, . . . , m.
The “first” monotonicity property reads as follows.
Lemma 
Starting with a strictly positive initial guess x[], the iterates generated by
(> .) satisfy
BE(x[k]) −BE(x[k+]) ⩾
ℓ
∑
p=
[ATq[k]]p
∣x[k+]
p
−x[k]
p
∣

x[k]
p
.
It follows that the objective function decreases as the iteration proceeds, unless one
has a fixed point of the iteration. It would seem reasonable to conjecture that one then gets
convergence of the iterates to a local minimum, but in the absence of a second monotonicity
property, this is where it ends.
Some reconstructions from simulated and real data are shown in []. The proofs of the
above two lemmas are shown there as well.
..
Nonnegative Least Squares
The absence of em algorithms for least-squares problems sooner or later had to be
addressed. Here, consider positive least-squares problems, and as in
> Sect. .., one
may as well consider them for the discrete emission tomography case. Thus, the interest is
in solving the problem
minimize
LS(x)
def= ∥Ax −b ∥
subject to
x ⩾.
(.)
Recall the properties (> .) and (> .) of the nonnegative matrix A ∈Rm×ℓ, and that
b is a nonnegative vector. It is useful to define the operator T on nonnegative vectors by
[Ty]p = yp
[ATb]p
[ATAy]p
,
p = ,, . . . , ℓ.
(.)
The following discussion of the convergence of this algorithm follows De Pierro [] and
Eggermont []. The first item on the agenda is to prove an analog of Lemma (> ).


EM Algorithms
Lemma 
For all nonnegative x, y ∈Rℓ, with y strictly positive
LS(x) ⩽LS(y) +
ℓ
∑
p=
[ATb]p { (xp −[Ty]p)
[Ty]p
−(yp −[Ty]p)
[Ty]p
}.
Proof
Observe that
LS(x) = LS(y) + ⟨x −y , AT(Ay −b)⟩+ ∥A(x −y)∥.
Let z = x −y. Write Az = A{y/(z/y/)} (with componentwise vector operations) and
use Cauchy–Schwarz. Then,
∥Az ∥⩽
m
∑
j=
[Ay]j [A(z/y)]j =
ℓ
∑
p=
z
p
[ATAy]p
yp
.
Now, consider ∥Az ∥+ ⟨z , AT(Ay −b)⟩. Completing the square gives
∥Az ∥+ ⟨z , AT(Ay −b)⟩⩽
ℓ
∑
p=
[ATAy]p
yp
(zp + yp
[AT(Ay −b)]p
[ATAy]p
)

−
ℓ
∑
p=
yp
[ATAy]p
[AT(Ay −b)]
p.
(.)
For the first sum, note that the expression inside the parentheses equals xp −[Ty]p. Also,
[ATAy]p/yp = [ATb]p/[Ty]p, so that takes care of the first sum. For the second sum, note
that
yp
[ATAy]p
[AT(Ay −b)]
p = [ATAy]p
yp
(yp −yp
[ATb]
[ATAy]p
)

,
and the expression for the second sum follows.
∎
It is now clear how one may construct an algorithm. Take y = x[], a strictly positive
vector, and minimize the upper bound on LS(x) given by the lemma. Setting the gradient
equal to gives x[k+] = Tx[k] or
x[k+]
p
= x[k]
p
⋅
[ATb]p
[ATAx[k]]p
,
p = ,, . . ., ℓ.
(.)
Note that then all x[k] are strictly positive because A is nonnegative and has unit column
sums.
This algorithm is due to Daube-Witherspoon and Muehllehner [] for emission
tomography with the acronym ISRA.
Onward to the monotonicity properties of the algorithm. The following lemma is
immediate.

EM Algorithms 

Lemma
If x[] is strictly positive, then the iterates of the ISRA algorithm (> .) satisfy
LS(x[k]) −LS(x[k+]) ⩾
ℓ
∑
p=
[ATb]p
(x[k]
p
−x[k+]
p
)

x[k+]
p
.
The “second” monotonicity property is a bit more involved than for the em algorithm
but still involves Kullback–Leibler distances. Let
KLS(x, y) = KL(c ⋅x, c ⋅y) + LS(y) −LS(y∗),
(.)
where x = y∗is any minimizer of ∥Ax −b ∥over x ⩾. Here, c = ATb and the dot means
componentwise multiplication.
Lemma 
If x∗is a solution of the nonnegatively constrained least-squares problem
(> .), then
KLS(c ⋅x∗, c ⋅x[k]) −KLS(c ⋅x∗, c ⋅x[k+]) ⩾
LS(x[k]) −
LS(x∗) ⩾.
Proof
As before, one has
KL(c ⋅x∗, c ⋅x[k]) −KL(c ⋅x∗, c ⋅x[k+])
=
ℓ
∑
p=
cp x∗
p log x[k+]
p
x[k]
p
+ cp (x[k]
p
−x[k+]
p
)
⩾
m
∑
p=
cp x∗
p
⎛
⎝−
x[k]
p
x[k+]
p
⎞
⎠+ cp (x[k]
p
−x[k+]
p
)
⩾
ℓ
∑
p=
cp (x[k+]
p
−x∗
p) (x[k]
p
−x[k+]
p
)
x[k+]
p
.
(.)
Here, in the second line, the inequality log t = −log( t −) ⩾−t −was used.
Now, let C ∈Rℓ×ℓbe the diagonal matrix with diagonal components
Cp,p = [ATb]p
x[k+]
p
,
p = ,, . . . , ℓ.
Then the least expression equals
⟨x[k+] −x∗, C (x[k] −x[k+])⟩=
⟨x[k+] −x[k] , C (x[k] −x[k+])⟩+ ⟨x[k] −x∗, C (x[k] −x[k+])⟩.


EM Algorithms
Since C (x[k] −x[k+]) = AT(Ax[k] −b), then
⟨x[k] −x∗, C (x[k] −x[k+])⟩= ⟨x[k] −x∗, AT(Ax[k] −b)⟩
⩾
LS(x[k]) −
LS(x∗),
the last inequality by convexity. Finally,
⟨x[k+] −x[k] , C (x[k] −x[k+])⟩= −
ℓ
∑
p=
[ATb]p
x[k+]
p
(x[k]
p
−x[k+]
p
)

.
which by Lemma dominates LS(x[k+]) −LS(x[k]). This shows that
KL(c⋅x∗, c⋅x[k])−KL(c⋅x∗, c⋅x[k+]) ⩾
LS(x[k])−
LS(x∗)−LS(x[k])+LS(x[k+]).
and the lemma follows.
∎
The convergence of the algorithm now follows similar to the em case.
..
Multiplicative Iterative Algorithms
This final section concerns the observation that multiplicative iterative algorithms may
be constructed by way of proximal point algorithms as in [] and that Kullback–Leibler
distances naturally appear in this context. For arbitrary convex functions F on Rℓ, one may
solve the problem with nonnegativity constraints
minimize
F(x)
subject to
x ⩾
(.)
by computing a sequence {x[k]}k, with x[k+] the solution of
minimize
F(x) + (ωk)−KL(x[k], x)
subject to
x ⩾,
(.)
starting from some x[] with strictly positive components. Here, ωk > . One verifies that
x[k+] satisfies
x[k+]
p
=

+ ωk [∇F(x[k+])]p
,
p = ,, . . . , ℓ.
(.)
This is an implicit equation for x[k+], but explicit versions suggest themselves. Note
that the objective function in (> .) is strictly convex, so that the solution is unique,
assuming solutions exist. Of course, other proximal functions suggest themselves, such as
KL(x, x[k]). See, e.g., []. The classical one is ∥x −x[k] ∥, the squared Euclidean distance,
due to Rockafellar. See, e.g., [].
It is interesting to note that the implicit algorithm (> .) satisfies the two mono-
tonicity properties. The first one is obvious,
F(x[k]) −F(x[k+]) ⩾(ωk)−KL(x[k], x[k+]),
(.)
since x[k+] is the minimizer of (> .).

EM Algorithms 

For the second monotonicity property, assume that x∗is a solution of (> .). Note
that
KL(x∗, x[k]) −KL(x∗, x[k+]) =
ℓ
∑
p=
x∗
p log x[k+]
p
x[k]
p
+ x[k]
p
−x[k+]
p
=
ℓ
∑
p=
x∗log

+ ωk [∇F(x[k+])]p
+ ωk x[k+] [∇F(x[k+])]p
⩾
ℓ
∑
p=
−x∗
p ωk [∇F(x[k+])]p + ωk x[k+] [∇F(x[k+])]p
= ωk⟨x[k+] −x∗,∇F(x[k+])⟩⩾ωk (F(x[k+]) −F(x∗)).
To summarize
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾ωk (F(x[k+]) −F(x∗)).
(.)
The convergence of the algorithm follows. Practically speaking, one has to devise
explicit versions of the algorithm and see how they behave.
.
Accelerating the EM Algorithm
..
The Ordered Subset EM Algorithm
It is well known that em algorithms converge very slowly, even if one wants to stop the
iteration “early.” For the general em algorithm of > Sect. ..some attempts at acceleration
have been made along the lines of coordinate descent methods or more generally descent
along groups of coordinates. In particular, the M-step (> .) is replaced by a sequence
of M-steps, for j = ,, . . . , m, with x= x[k],
minimize
L(x∣x j−)
def= −∫Z φZ(z∣xj−) log fZ (z∣x) dμ(z)
subject to
x ∈Xj,
(.)
and then x[k+] = xm. Here {Xj}m
j=is a not-necessarily-disjoint division of the parameter
space X. See [] and references therein. In the context of emission tomography, the more
generally accepted route to accelerating the em algorithm has been via the ordered subset
approach of Hudson and Larkin []. Without putting too fine a point to it, this amounts to
partitioning the data space rather than the parameter space. The acceleration achieved by
these methods seems to be twofold. The ordered subset approach allows for more efficient
computer implementations and the convergence itself is speeded up. See, e.g., [].
The ordered subset em algorithm (osem) of Hudson and Larkin [] deals with the
maximum likelihood problem of emission tomography (> .). The starting point is to


EM Algorithms
divide the data into blocks, characterized by the sets of indices Ω(), Ω(), . . ., Ω(s) such
that
Ω() ∪Ω() ∪. . . ∪Ω(s) = {,, . . ., m}.
(.)
However, the sets need not be disjoint. Define the partial negative Kullback–Leibler
functionals
Lr(x) =
∑
j∈Ω(r)
{bj log
bj
[Ax]j
+ [Ax]j −bj},
r = ,, . . . , s.
(.)
Note that for all r
∑
j∈Ω(r)
[Ax]j =
ℓ
∑
p=
αr p xp
with
αr p =
∑
j∈Ω(r)
a(j, p).
(.)
The osem algorithm now consists of successively applying one step of the Shepp–Vardi em
algorithm to each of the problems
minimize
Lr(x)
subject to
x ⩾.
(.)
To spell out the osem iteration exactly, it is useful to introduce the data vectors Br and the
matrices Ar by
Br = (bj : j ∈Ω(r))
and
Arx = ([Ax]j : j ∈Ω(r)).
(.)
Then, Lr(x) = KL(Br, Arx), and the osem algorithm takes the form
x[k+]
p
= x[k]
p
⋅α−
r p [AT
r ϱ[k]]p,
p = ,, . . ., ℓ,
(.)
where r = k mod s (in the range ⩽r ⩽s) and ϱ[k]
q
= Brq/[Arx[k]]q for all q. The slight
complication of the αr p arises because the matrices Ar do not have unit column sums. This
is fixed by defining the matrices Ar by
[Ar]qp = α−
r p [Ar]qp
for all
q and p.
(.)
Now, define Lr(y) = Lr(x) = KL(Br,Ar y), where yq = αrqxq for all q. A convenient short-
hand notation for this is y = αr ⋅x. Now, if x minimizes Lr(x), then y = αr ⋅x minimizes
Lr(y) and vice versa. Since the matrices Ar have unit column sums, the em algorithm for
minimizing Lr(y) is
y[k+]
p
= y[k]
p
⋅[AT
r ϱ[k]]p ,
p = ,, . . . , ℓ,
(.)
with ϱ[k]
q
= Brq/[Ar y[k]]q for all q. Transforming back gives (> .).
Regarding the convergence of the osem algorithm, the best one can hope for is cyclic
convergence, i.e., each of the subsequences {x[k+r s]} r⩾converges. Proving this would be
a daunting task. However, as observed by Byrne [], it is useful to consider what happens
if the system of equations Ax = b is consistent in the sense that
∃x∗⩾: Ax∗= b,
(.)

EM Algorithms 

when one should expect convergence of the whole sequence to a nonnegative solution of
Ax = b. Hudson and Larkin [] prove that this is so under the so-called subset-balancing
condition
αr p = αo,p,
p = ,, . . ., ℓ
and
r = ,, . . . , s.
(.)
That is, the column sums are the same for all blocks. This is a strong condition, even if one
allows for overlapping blocks of data. Haltmeier et al. [] make the same assumption in
the continuous setting. Byrne [] observed that the condition may be relaxed to that of
subset-separability: There exist coefficients βr and γp such that
αr p = βrγp,
r = ,, . . . , s
and
p = ,, . . . , ℓ.
(.)
The convergence proof of the osem algorithm (> .) under the subset-separability con-
dition (> .) relies on the two monotonicity properties of the em algorithm (> .),
see
> Sect. ... After translation, one gets the following monotonicity properties for
(> .).
Lemma 
Let x∗be a nonnegative solution of Ax = b. Starting from a strictly positive
x[] ∈Vℓ, then with r = k mod s,
Lr(x[k]) −Lr(x[k+]) ⩾KL(αr⋅x[k+], αr⋅x[k]) ⩾
and
KL(αr⋅x∗, αr ⋅x[k]) −KL(αr⋅x∗, αr⋅x[k+]) ⩾Lr(x[k]) −Lr(x∗) ⩾.
Now, if the αr p change with r, then one cannot conclude much from the lemma. However,
under the subset-separability condition (> .), one obtains for all nonnegative x and y,
KL(αr ⋅x, αr ⋅y) = βr KL(γ ⋅x,γ ⋅y),
and the inequalities of the lemma translate as follows.
Corollary 
Under the conditions of Lemma and the subset-separability condition
(> .),
β−
r
{Lr(x[k]) −Lr(x[k+])} ⩾KL(γ⋅x[k+],γ⋅x[k]) ⩾
and
KL(γ⋅x∗,γ⋅x[k]) −KL(γ⋅x∗,γ⋅x[k+]) ⩾β−
r
{Lr(x[k]) −Lr(x∗)} ⩾.
As in Theorem (> ), one may conclude that the sequence {γ ⋅x[k]}k converges to a
nonnegative solution x∗∗of Ax = b. Note that there is another way of looking at this, see
Remark at the end of this chapter.
As remarked, the subset-separability condition (> .) is very strong. It fails dramat-
ically in the extreme case of the osem algorithm when each block consist of a single row.
In that case, the osem algorithm (> .) reduces to
x[k+]
p
= x[k]
p
⋅
bj
[Ax[k]]j
,
p = ,, . . ., ℓ,
(.)


EM Algorithms
where j = k mod m. So, x[k+] is a multiple of x[k], and the osem algorithm produces only
multiples of the initial guess x[]. So, certainly in this case the algorithm does not converge,
but more seriously, it does not do anything useful.
So, what is one to do? Following Byrne [], see also [], the next section turns to row-
action methods (where the blocks consist of a single datum), that is the (additive) algebraic
reconstruction technique (art) and the multiplicative version (mart) of Gordon et al. [],
as well as block-iterative variants. (The simultaneous version (smart) of the multiplicative
version was already discussed in > Sect. ...) This points into the direction of relaxation
and scaling. After that, the table is set for block-iterative versions of the em algorithm.
..
The ART and Cimmino–Landweber Methods
It is useful to discuss the situation in regards to the so-called algebraic reconstruction tech-
nique (art) of Gordon et al. [], and the Cimmino–Landweber iteration, a reasonable
version of the original sirt method. Herman [] is the authoritative source, but see []
for a comparison with conjugate gradient method. The art and Cimmino–Landweber
algorithms were designed to solve systems of linear equations of the form
Ax = b
(.)
with b the measured nonnegative data and A ∈Rm×ℓ, with nonnegative components
a(j, p) but not necessarily unit column sums. Of course for inconsistent systems of
equations, this must be replaced by the least-squares problem
minimize
∥Ax −b ∥
subject to
x ∈Rm,
(.)
but in fact, the art method solves the weighted least-squares problem
minimize
m
∑
j=
∣⟨a(j,⋅), x ⟩−b j ∣

∥a(j,⋅)∥
subject to
x ∈Rm,
(.)
A standard method for the solution of (> .) is the Cimmino–Landweber iteration
x[k+] = x[k] + ωk AT(b −Ax[k]),
(.)
for suitably small but not too small positive relaxation parameters ωk. It is mentioned here
for its analogy with the em algorithm. The Cimmino–Landweber iteration is a well-studied
method for the regularization of the least-squares problem (> .) and is itself subject
to acceleration, see, e.g., Hanke [] and references therein.
At the other end of the spectrum is Kaczmarz’ method, which consists of sequential
orthogonal projections onto the hyperplanes
H j = {x ∈Rℓ∣⟨a(j, ⋅), x ⟩= bj}.

EM Algorithms 

Formally, this is achieved by computing the new iterate x[k+] from the previous one x[k]
by solving
minimize
∥x −x[k] ∥
subject to
⟨a(j, ⋅), x ⟩= b j .
(.)
The iteration then takes the form, with j = k mod m,
x[k+] = x[k] + ωk
bj −⟨a(j, ⋅), x[k] ⟩
∥a(j, ⋅)∥
a(j, ⋅)
(.)
for ωk = . The relaxation parameter ωk is included to see whether choices other than
ωk = might be advantageous. Geometrically, a requirement is < ωk < . The choice
ωk = would not do anything; the choice ωk = implements reflection with respect to the
hyperplane H j. The algorithm (> .) with relaxation originated with Gordon et al. [].
Typically, one takes the hyperplanes in cyclic order j = k mod m , but Herman and
Meyer [] show experimentally that carefully reordering the hyperplanes has a big effect
on the quality of the reconstruction when the number of iterations is fixed before hand.
The choice of ω(= ωk for all k) also matters greatly, but the optimal one seems to depend
on everything (the experimental set-up leading to the matrix A, the noise level, etc.), so
that the optimal ω can be very close to or close to or in between.
Byrne [, ] observes that the scaling of the art algorithm is just about optimal, as
follows. Actually, it is difficult to say much for inconsistent systems, other than experimen-
tally, see [], but for consistent systems one has the following two monotonicity properties,
which are reminiscent of the monotonicity properties for the Shepp–Vardi em algorithm.
However, they are much less impressive since they only hold for consistent systems. Define
LSj(x) =
∣⟨a(j,⋅), x ⟩−b j ∣

∥a(j,⋅)∥
,
j = ,, . . . , m.
(.)
Lemma 
If x∗satisfies Ax∗= b then
LSj(x[k]) −LSj(x[k+]) = ωk(−ωk)LSj(x[k]),
∥x[k] −x∗∥−∥x[k+] −x∗∥= ωk(−ωk)LSj(x[k]).
The proofs involve only (exact) quadratic Taylor expansions and are omitted. The con-
clusion is that art converges in the consistent case if ωk = ω is constant and < ω < .
Following Byrne [], one notes that the second monotonicity property suggests that
ωk(−ωk) should be as large as possible. This is achieved by ωk = . In other words,
the original Kaczmarz procedure (> .) with ωk = is optimally scaled. However, as
already remarked above, a choice other than ωk = may speed things up initially.
Despite the good news that art is much faster than the Cimmino–Landweber type
methods, it is still “slow.” Now, in transmission tomography as in emission tomography,
the system of equations Ax = b naturally decomposes into a number of blocks
Arx = Br,
r = ,, . . . , s,
(.)


EM Algorithms
see (> .), and then one has the block version of (> .)
minimize
∥x −x[k]∥
subject to
Arx = Br,
(.)
with the solution
x[k+] = x[k] + ωk AT
r (ArAT
r )† (Br −Arx[k]),
(.)
where † denotes the Moore–Penrose inverse. Now, computing (AT
r Ar)† w (for any vector
w) would be expensive, but it seems reasonable that AT
r Ar should be close to diagonal, in
which case one may just replace it with its diagonal. This leads to the algorithm
x[k+] = x[k] + ωk AT
r D−
r (Br −Arx[k]),
(.)
where Dr is a diagonal matrix with [Dr]qq = [ArAT
r ]
qq .
Now, it turns out that computing Arx is not much more expensive than computing
a single ⟨a(j, ⋅), x ⟩and that the matrices AT
r Ar are very close to diagonal, so that one
step of the block method (> .) practically achieves as much as the combined art steps
for all the equations in one block. So, methods that process naturally ordered blocks are
appreciably faster than the two extreme methods. See [].
It is not clear how to choose the optimal relaxation parameters. Regarding (> .),
it is known that the algorithm converges cyclically provided the blocks and the relaxation
parameters are chosen cyclically, i.e., if r = k mod s and ωk ≡ωr, and
max
⩽r⩽s ∥I −ωrArD−
r AT
r ∥
< ,
(.)
then {x[r+ks]}k converges for each r = ,, . . . , s, see []. Moreover, if the relaxation
parameter is kept fixed, say
ωk = ω
for all
k,
(.)
and denoting the iterates by x[i+kI](ω) to show the dependence on ω, then
lim
ω→lim
k→∞x[i+kI](ω) = x∗,
(.)
the minimum norm solution of (> .), provided the initial guess belongs to the range
of AT. See []. At about the same time, Trummer [] showed for the relaxed art method
(> .) that
lim
k→∞x[k](ωk) = x∗,
(.)
provided
ωk > ,
∞
∑
k=
ω
k < ∞
and
∞
∑
k=
ωk = +∞.
(.)
Note the difference between (> .) and (> .).

EM Algorithms 

..
The MART and SMART Methods
Consider again the system of equations Ax = b as it arises in the PET setting, with A and
b having nonnegative components and A having unit column sums. In
> Sect. ..the
smart algorithm was discussed for the solution of
minimize
KL(Ax, b)
subject to
x ⩾,
(.)
i.e.,
x[k+]
p
= x[k+]
p
⋅exp ([AT log
b
Ax[k] ]
p
),
p = ,, . . . , ℓ.
(.)
The multiplicative art algorithm (mart) of Gordon et al. [] formally arises as the
multiplicative version of the additive art algorithm, to wit
x[k+]
p
= x[k]
p
⋅⎛
⎝
bj
⟨a(j,⋅), x[k] ⟩
⎞
⎠
a(j,p)
,
p = ,, . . . , ℓ
or equivalently for p = ,, . . . , ℓ
x[k+]
p
= x[k]
p
⋅exp ⎛
⎝ωk a(j, p)log
bj
⟨a(j,⋅), x[k] ⟩
⎞
⎠
(.)
with ωk = . Again, the relaxation parameter ωk was included to explore whether choices
other than ωk = would be advantageous. Byrne [] observes that the mart algorithm
typically does not enjoy the same speed-up compared to the simultaneous smart version
that art has over Cimmino-Landweber. To get some insight into this, it is useful to con-
sider a projection method analogous to the Kaczmarz method of orthogonal projections
onto hyperplanes. The method in question is well-known, see, e.g., [],
minimize
KL(x, x[k])
subject to
⟨a(j,⋅), x[k] ⟩= bj.
(.)
One may approximately solve this as follows. With the unrestricted Lagrange multiplier λ
one gets the equations log (xp/x[k]
p ) + λa(j, p) = for all p, so that
xp = x[k]
p
exp (λ a(j, p)),
p = ,, . . . , ℓ.
To enforce the constraint take inner products with a(j,⋅). This results in
bj = ⟨a(j,⋅), x ⟩=
ℓ
∑
p=
a(j, p) x[k]
p
exp (λ a(j, p)),
(.)
and one would like to solve this for λ. That does not appear manageable, but it can be done
approximately as follows. Since the a(j, p) and x[k]
p
are nonnegative, by the mean value
theorem there exists a θ, with
< θ < max {a(j, p) : ⩽p ⩽ℓ},
(.)


EM Algorithms
such that the right hand side of (> .) equals exp(λ θ)⟨a(j,⋅), x[k] ⟩. Then, solving
(> .) for λ gives the iteration
x[k+]
p
= x[k]
p
exp ⎛
⎝ω a(j, p) log
bj
⟨a(j,⋅), x[k] ⟩
⎞
⎠,
(.)
with ω = /θ. The conservative choice, the one that changes x[k] the least, is to choose ω
as small as possible. In view of (> .) this gives ω = ω j,
ω j =

max
⩽p⩽ℓa(j, p) .
(.)
Note that if A has unit column sums, then one may expect ω to be quite large. This may
explain why the original mart algorithm is not greatly faster than the smart version.
In defense of Gordon et al. [], one should mention that they considered matrices with
components or , in which case ω = !
Following Byrne [], the block-iterative version of (> .) is as follows. In the
partitioned data set-up of (> .), the bi-mart algorithm is
x[k+]
p
= x[k]
p
⋅exp ⎛
⎝
ωk
αr
[AT
r log {
Br
Arx[k] }]
p
⎞
⎠
(.)
for p = ,, . . . , ℓ, where
αr = max {αr p : ⩽p ⩽ℓ}
(.)
is the maximal column sum of Ar. One would expect that ωk = should be the opti-
mal choice. This is the rescaled bi-mart (or rbi-mart) algorithm of Byrne []. Following
the template of
> Sect. .., one proves the following majorizing inequality and the two
monotonicity properties. For nonnegative vectors y and ω > define Rωy by
[Rωy]p = yp exp ⎛
⎝
ω
αr
[AT
r log{ Br
Ar y }]
p
⎞
⎠
(.)
for p = ,, . . . , ℓ.
Lemma 
For all nonnegative x and y
KL(Arx, Br) ⩽KL(Ar y, Br) + αr
ω {KL(x, Rωy) −KL(y, Rωy)}.
Note that the minimizer of the right hand side is x = Rωy. This would give rise to the
algorithm (> .).
Proof
Recall the identity from > Sect. ..,
KL(Arx, Br) = KL(Ar y, Br) + KL(Arx, Ar y) + ⟨x −y, AT
r ϱ ⟩,
with ϱj = log([Ar y]j/[Br]j). Now, a convexity argument gives that
KL(Arx, Ar y) ⩽αr KL(x, y),

EM Algorithms 

so that one gets the inequality
KL(Arx, Br) ⩽KL(Ar y, Br) + αr KL(x, y) + ⟨x −y, AT
r ϱ ⟩.
The definition of the operator Rω gives that
log [Rωy]p
yp
= −ω
αr
AT
r ϱ,
so then, with θ ≡/ω,
αr KL(x, y) + ⟨x −y, AT
r ϱ ⟩=
= αr {KL(x, y) −θ ⟨x −y,log Rωy
y
⟩}
= αr {(−θ)KL(x, y) + θ (KL(x, Rωy) −KL(y, Rωy))}.
The last line follows after some lengthy bookkeeping. So, for θ ⩽or ω ⩾, the conclusion
follows.
∎
The first monotonicity property then follows.
Lemma 
For ω ⩾and r = k mod s,
KL(Arx[k], Br) −KL(Arx[k+], Br) ⩾αr
ω KL(x[k], x[k+]) ⩾.
The second monotonicity property follows after some work (omitted).
Lemma 
If x∗⩾satisfies Ax∗= b, then for all k, and r = k mod s,
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾ω
αr
KL(Arx[k+], Br) ⩾.
(.)
Now, regardless of whether ω = maximizes the right hand side of the inequality
(> .), the presence of the factor αr, which should be small if the original matrix A has
unit column sums, suggests that the choice ω = in (> .) is a tremendous improve-
ment over the case ω = αr, which would arise if one ignored the non-unit column sums
of Ar.
..
Row-Action and Block-Iterative EM Algorithms
Attention now turns to the construction of the row-action version of the em algorithm, and
the associated block-iterative versions. Recall the formulation (> .) of the maximum
likelihood problem for the PET problem as
minimize
KL(b, Ax)
subject to
x ∈Vℓ,
(.)
with b ∈Rm and A ∈Rm×ℓnonnegative, with A having unit column sums.


EM Algorithms
Now construct a row-action version by considering the following iterative projection
method, where the new iterate x[k+] is obtained by projecting the previous iterate x[k]
onto the hyperplane ⟨a(j,⋅), x ⟩= b j. The particular projection is obtained by
minimize
KL(x[k], x)
subject to
⟨a(j,⋅), x ⟩= b j.
(.)
Again, with λ an unrestricted Lagrange multiplier, one must solve the equations −x[k]
p /xp+
+ λ a(j, p) = , or
xp = x[k]
p
−λ a(j, p) xp,
p = ,, . . . , ℓ.
(.)
At this point, one must make the simplification where xp on the right hand side is replaced
by x[k]
p . This gives the equation
xp = x[k]
p
−λ a(j, p) x[k]
p ,
p = ,, . . . , ℓ.
To enforce the constraint, multiply by a(j, p) and sum over p. Then,
bj = ⟨a(j,⋅), x[k] ⟩−λ
ℓ
∑
p=
a(j, p)x[k]
p
= (−λ θ)⟨a(j,⋅), x[k] ⟩.
(.)
where in the last line the mean value theorem was used, for some θ satisfying
< θ < max {a(j, p) : ⩽p ⩽ℓ}.
(.)
Solving for λ gives the iterative step
x[k+] = (−ω a(j, p)) x[k]
p
+ ω x[k]
p
a(j, p) bj
⟨a(j,⋅), x[k] ⟩,
(.)
for p = ,, . . . , ℓ, where ω ≡/θ. In the notation of (> .), with some imagination the
block-iterative version is then
x[k+] = Rω,r x[k],
(.)
where the operators Rω,r are defined by
[Rω,r x]p = (−ω αr p) xp + ω xp [AT
r (Br/Arx)]
p ,
(.)
for p = ,, . . . , ℓ. So now, ω is considered to be a relaxation parameter.
The algorithm (> .) was obtained by Byrne [, ] after carefully examining the
analogy with mart vs. rbi-smart. His choice for the relaxation parameter ω is to take it
depending on the block, so ω = ωr with
ωr =

max
⩽p⩽ℓαr p
,
(.)
which he obtained by deriving the two monotonicity properties discussed below. Byrne
[, ] designated the resulting algorithm (> .)–(> .) as rescaled block-iterative
em for maximum likelihood algorithm (rbi-emml). At about the same time Browne and
De Pierro [] discovered the algorithm (> .)–(> .). They named (> .) the
ramla (row-action maximum likelihood algorithm). For the latest on this, see [].

EM Algorithms 

The above considerations strongly suggest that algorithm (> .)–(> .) is the
correct one. This is corroborated by practical experience. The following monotonicity
properties lend even more weight to it. A slight drawback is that they require that the
system Ax = b has a nonnegative solution. The first item is again a majorizing inequal-
ity. Note that the majorizing inequality is suggested by the algorithm, not the other way
around. Define
BIr(x, y)
def= ω Lr(x) +
ℓ
∑
p=
(−ω αr p) {xp log
xp
yp
+ yp −xp}.
(.)
Lemma 
For nonnegative x, y ∈Rℓ
BIr(x, y) ⩽BIr(y, y) + KL(Rω y, x) −KL(Rω y, y),
provided ω ⩽/max{αr p : ⩽p ⩽ℓ}.
Proof
Apply Lemma (> ) to ω {KL(Br, Arx) −KL(Br, Ar y)}. Also observe that
∑
q
[Arx]q =
ℓ
∑
p=
αr p xp,
and likewise for [Ar y]q. This gives
BIr(x, y) ⩽BIr(y, y) +
ℓ
∑
p=
[Rωy]p log
yp
xp
+ xp −yp,
and the lemma follows. Note that the condition on ω is used implicitly to assure that Rω,r y
is nonnegative.
∎
The first monotonicity property is an easy consequence.
Lemma 
For r = k mod s and ω ⩽/max{αr p : ⩽p ⩽ℓ}
Lr(x[k]) −Lr (x[k+]) ⩾ω−KL(x[k+], x[k]) ⩾.
Proof
Take y = x[k] and x = Rωy = Rωx[k] = x[k+]. Then one gets BI(x[k+], x[k]) −
BI (x[k], x[k]) ⩽−KL(x[k+], x[k]) , so that
ω (Lr(x[k]) −Lr (x[k+])) ⩾
ℓ
∑
p=
(−ω αr p) {xp log xp
yp
+ yp −xp}.
Since ω αr p ⩽, the conclusion follows.
∎
The second monotonicity property reads as follows.
Lemma 
If x∗⩾satisfies Ax∗= b, then with r = k mod s,
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾ω {Lr (x[k]) −Lr (x∗)},


EM Algorithms
provided ω ⩽/max{αr p : ⩽p ⩽ℓ}.
The lemma suggests that one should take ω as large as possible. This is how Byrne []
arrived at the choice (> .).
Proof of Lemma (> )
Since x∗satisfies Ax∗= b, so Arx∗= Br for all r, the proof is
actually simpler than for the original proof for the em algorithm, see > Sect. ... By the
concavity of the logarithm (twice), one obtains
log x[k+]
p
x[k]
p
= log⎛
⎝(−ωαr p) + ω [Ar {
Br
Arx[k] }]
p
⎞
⎠
⩾ω αr p log⎛
⎝α−
r p [Ar {
Br
Arx[k] }]
p
⎞
⎠⩾ω [AT
r log {
Br
Arx[k] }]
p
,
so that
KL(x∗, x[k]) −KL(x∗, x[k+]) ⩾ω
ℓ
∑
p=
x∗
p [AT
r log(Br/Arx[k])]p +
ℓ
∑
p=
x[k]
p
−x[k+]
p
.
Now, the first sum equals
∑
q
[Arx∗]q log([Br]q/[Arx[k]]q ) = ∑
q
[Br]q log([Br]q/[Arx[k]]q ).
For the remaining sums, note that
ℓ
∑
p=
x[k+]
p
=
ℓ
∑
p=
(−ω αr p) x[k]
p
+ ω x[k]
p
[AT
r (Br/Arx[k])]p
=
ℓ
∑
p=
x[k]
p
−ω ∑
q
[Arx[k]]q + ω ∑
q
[Br]q.
Putting the two together proves the lemma.
∎
Remark 
To wrap things up, note that Byrne [] shows the convergence (in the
consistent case) of a somewhat different version of (> .), which under the subset-
separability condition (> .), reduces to the osem algorithm (> .), thus proving
the convergence of osem under subset-separability (in the consistent case). See also
Corollary .
References and Further Reading
. Aronszajn N, Smith KT () Theory of Bessel
potentials. I. Ann Inst Fourier (Grenoble) :–
, www.numdam.org
. Atkinson KE () The numerical solution of
integral equations on the half line. SIAM J Numer
Anal :–

EM Algorithms 

. Bardsley JM, Luttman A () Total variation-
penalized Poisson likelihood estimation for ill-
posed problems. Adv Comput Math :–
. Bertero M, Bocacci P, Desiderá G, Vicidomini G
() Imagede-blurringwithPoissondata:from
cells to galaxies. Inverse Probl ():
. Browne J, De Pierro AR () A row-action
alternative to the EM algorithm for maximizing
likelihoods in emission tomography. IEEE Trans
Med Imag :–
. Brune C, Sawatzky A, Burger M () Bregman-
EM-TV methods with application to optical
nanoscopy, scale space and variational methods
in computer vision, Lecture Notes in Computer
Science . Springer, Berlin, pp –
. Byrne CL () Iterative image reconstruction
algorithmsbasedoncross-entropyminimization.
IEEE Trans Image Process :–
. Byrne CL () Block-iterative methods for
image reconstruction from projections. IEEE
Trans Image Process :–
. Byrne CL () Accelerating the EMML algo-
rithm and related iterative algorithms by rescaled
block-iterative methods. IEEE Trans Image Pro-
cess :–
. Byrne CL () Likelihood maximization for
list-mode emission tomographic image recon-
struction. IEEE Trans Med Imag :–
. Byrne
CL
()
Choosing
parameters
in
block-iterative or ordered subset reconstruc-
tion algorithms. IEEE Trans Image Process :
–
. Byrne CL () Signal processing: a mathemat-
ical approach. AK Peters, Wellesley
. Byrne CL () Applied iterative methods. AK
Peters, Wellesley
. Byrne CL, Fiddy MA () Images as power
spectra; reconstruction as a Wiener filter approx-
imation. Inverse Probl :–
. Cao Yu, Eggermont PPB, Terebey S () Cross
Burg entropy maximization and its application
to ringing suppression in image reconstruction.
IEEE Trans Image Process :–
. Censor Y, Eggermont PPB, Gordon D ()
Strong under relaxation in Kaczmarz’s method
for inconsistent systems. Numer Math :–
. Censor Y, Lent AH () Optimization of “log x”
entropy over linear equality constraints. SIAM J
Control Optim :–
. Censor Y, Segman J () On block-iterative
entropy maximization. J Inform Optim Sci :
–
. Censor Y, Zenios SA () Proximal minimiza-
tion algorithm with D-functions. J Optim Theory
Appl :–
. Cover TM () An algorithm for maximiz-
ing expected log investment return. IEEE Trans
Inform Theory :–
. Crowther RA, DeRosier DJ, Klug A () The
reconstruction of three-dimensional structure
from projections and its application to electron
microscopy. Proc R Soc Lond A Math Phys Sci
():–
. Csiszár I () I-divergence geometry of prob-
ability distributions and minimization problems.
Ann Probab :–
. Csiszár I, Tusnády G () Information geome-
tryandalternatingminimizationprocedures. Stat
Decisions (Supplement ):–
. DaleyDJ,Vere-JonesD () Anintroductionto
the theory of point processes. Springer, New York
. Darroch JN, Ratcliff D () Generalized itera-
tive scaling for log-linear models. Ann Math Stat
:–
. Daube-Witherspoon ME, Muehllehner G ()
An iterative space reconstruction algorithm
suitable for volume ECT. IEEE Trans Med Imag
: –
. Dempster AP, Laird NM, Rubin DB () Maxi-
mum likelihoodfrom incomplete data via the EM
algorithm. J R Stat Soc B :–
. De Pierro AR () On the convergence of
the iterative image space reconstruction algo-
rithm for volume ECT. IEEE Trans Med Imag :
–
. De Pierro AR () A modified expectation
maximization algorithm for penalized likelihood
estimation in emission tomography. IEEE Trans
Med Imag :–
. De Pierro A, Yamaguchi M () Fast EM-like
methods for maximum a posteriori estimates
in emission tomography. Trans Med Imag :
–
. Dey N, Blanc-Ferraud L, Zimmer Ch, Roux P,
Kam Z, Olivo-Martin J-Ch, Zerubia J ()
Richardson-Lucy algorithm with total varia-
tion regularization for D confocal microscope
deconvolution. Microsc Res Tech :–


EM Algorithms
. Duijster A, Scheunders P, De Backer S ()
Wavelet-based EM algorithm for multispectral-
image
restoration.
IEEE
Trans
Geoscience
Remote Sensing :–
. Eggermont PPB () Multiplicative iterative
algorithms for convex programming. Linear
Algebra Appl :–
. Eggermont PPB () Nonlinear smoothing and
the EM algorithm for positive integral equa-
tions of the first kind. Appl Math Optimiz :
–
. Eggermont PPB, Herman GT, Lent AH ()
Iterative algorithms for large partitioned linear
systems with applications to image reconstruc-
tion. Linear Algebra Appl :–
. Eggermont PPB, LaRiccia VN () Smoothed
maximum likelihood density estimation for
inverse problems. Ann Stat :–
. Eggermont PPB, LaRiccia VN () Maximum
penalized likelihood estimation and smoothed
EM algorithms for positive integral equations of
the firstkind. Numer Funct Anal Optimiz :–

. Eggermont PPB, LaRiccia VN () On EM-
like algorithms for minimum distance estima-
tion. Manuscript, University of Delaware
. Eggermont PPB, LaRiccia VN () Maximum
penalized likelihood estimation, I: Density esti-
mation. Springer, New York
. Elfving T () On some methods for entropy
maximization and matrix scaling. Linear Algebra
Appl :–
. Fessler JA, Ficaro EP, Clinthorne NH, Lange
K () Grouped coordinate ascent algorithms
for penalized log-likelihood transmission image
reconstruction. IEEE Trans Med Imag :–
. Fessler JA, Hero AO () Penalized maximum-
likelihood image reconstruction using space-
alternating generalized EM algorithms. IEEE
Trans Image Process :–
. Figueiredo MAT, Nowak RD () An EM algo-
rithm for wavelet-based image restoration. IEEE
Trans Image Process :–
. Frank
J
() Three-dimensional
electron
microscopy of macromolecular assemblies, nd
edn. Oxford University Press, New York
. Geman S, Geman D () Stochastic relaxation,
Gibbs distributions and the Bayesian restoration
of images. IEEE Trans Pattern Anal Mach Intell
:–
. Geman S, McClure DE () Bayesian image
analysis, an application to single photon emission
tomography, Statistical Computing Section. Proc
Am Stat Assoc –
. Good IJ () A nonparametric roughness
penalty for probability densities. Nature :
–
. Gordon R, Bender R, Herman GT ()
Algebraic reconstruction techniques (ART) for
three-dimensional
electron
microscopy
and
X-ray photography. J Theor Biol :–
. Green PJ () Bayesian reconstructions from
emission tomography data using a modified EM
algorithm. IEEE Trans Med Imag :–
. Guillaume M, Melon P, Réfrégier P ()
Maximum-likelihoodestimationof anastronom-
ical image from a sequence at low photon levels.
J Opt Soc Am A :–
. Haltmeier M, Leitão A, Resmerita E () On
regularization methods of EM-Kaczmarz type.
Inverse Probl ():
. Hanke M () Accelerated Landweber iter-
ations for the solution of ill-posed problems.
Numer Math :–
. Hartley HO () Maximum likelihood esti-
mation from incomplete data. Biometrics :
–
. Hebert T, Leahy R () A generalized EM algo-
rithm for -D Bayesian reconstruction fromPois-
son data using Gibbs priors. IEEE Trans Med
Imag :–
. Herman GT () Fundamentals of comput-
erized tomography: image reconstruction from
projections. Springer, New York
. Herman GT, Meyer LB () Algebraic recon-
struction techniques can be made computa-
tionally efficient. IEEE Trans Med Imag :
–
. Holte S, Schmidlin P, Lindén A, Rosenqvist G,
Eriksson L () Iterative image reconstruc-
tion for positron emission tomography: a study
of convergence and quantitation problems. IEEE
Trans Nuclear Sci :–
. Horváth I, Bagoly Z, Balász LG, de Ugarte Postigo
A, Veres P, Mészáros A () Detailed classifi-
cation of Swift’s Gamma-ray bursts. J Astrophys
:–
. Hudson HM, Larkin RS () Accelerated image
reconstruction using ordered subsets of projec-
tion data. IEEE Trans Med Imag :–

EM Algorithms 

. Kamphuis C, Beekman FJ, Viergever MA ()
Evaluation of OS-EM vs. EM-ML for D, D and
fully D SPECT reconstruction. IEEE Trans Nucl
Sci :–
. Kondor A () Method of convergent weights –
an iterative procedure for solving Fredholm’s
integral equations of the first kind. Nucl Instrum
Methods :–
. Lange K () Convergence of EM image
reconstruction algorithms with Gibbs smooth-
ing. IEEE Trans Med Imag :–
. Lange K, Bahn M, Little R () A theoretical
study of some maximum likelihood algorithms
for emission and transmission tomography. IEEE
Trans Med Imag :–
. Lange K, Carson R () EM reconstruction
algorithms for emission and transmission tomog-
raphy. J Comput Assisted Tomography :–
. Latham GA () Existence of EMS solutions
and a priori estimates. SIAM J Matrix Anal Appl
:–
. Levitan E, Chan M, Herman GT () Image-
modeling Gibbs priors. Graph Models Image
Process :–
. Lewitt RM, Muehllehner G () Accelerated
iterative reconstruction in PET and TOFPET.
IEEE Trans Med Imag :–
. Liu C, Rubin H () The ECME algorithm: a
simple extension of EM and ECM with faster
monotone convergence. Biometrika :–
. Llacer J, Veklerov E () Feasible images and
practical stopping rules for iterative algorithms
in emission tomography. IEEE Trans Med Imag
:–
. Lucy LB () An iterative technique for the rec-
tificationof observeddistributions.Astronomical
J :–
. McLachlan GJ, Krishnan T () The EM algo-
rithm and its extensions. Wiley, Hoboken
. Meidunas E () Re-scaled block iterative
expectation maximization maximum likelihood
(RBI-EMML) abundance estimation and sub-
pixel material identification in hyperspectral
imagery. MS thesis, Department of Electrical
Engineering, University of Massachusetts Lowell
. Miller MI, Roysam B () Bayesian image
reconstruction for emission tomography incor-
porating Good’s roughness prior on massively
parallel processors. Proc Natl Acad Sci USA
:–
. Mülthei HN, Schorr B () On an iterative
method for a class of integral equations of the first
kind. Math Meth Appl Sci :–
. Mülthei HN, Schorr B () On properties
of the iterative maximum likelihood recon-
struction method. Math Meth Appl Sci :
–
. Nielsen SF () The stochastic EM algorithm:
estimation and asymptotic results. Bernoulli
:–
. Parra L, Barrett H () List-mode likelihood:
EM algorithm and image quality estimation
demonstrated on -D PET. IEEE Trans Med Imag
:–
. Penczek P, Zhu J, Schroeder R, Frank J ()
Three-dimensional
reconstruction
with con-
trast transfer function compensation. Scanning
Microscopy :–
. Redner RA, Walker HF () Mixture densi-
ties, maximum likelihood and the EM algorithm.
SIAM Rev :–
. Resmerita E, Engl HW, Iusem AN () The
expectation-maximization
algorithm
for
ill-
posed integral equations: a convergence analysis.
Inverse Probl :–
. Richardson WH () Bayesian based iterative
method of image restoration. J Opt Soc Am
:–
. Rockmore A, Macovski A () A maximum
likelihood approach to emission image recon-
struction from projections. IEEE Trans Nucl Sci
:–
. Scheres SHW, Valle M, Núñez R, Sorzano COS,
Marabini R, Herman GT, Carazo J-M ()
Maximum-likelihood
multi-reference
refine-
ment for electron microscopy images. J Mol Biol
:–
. Scheres SHW, Gao HX, Valle M, Herman GT,
Eggermont PPB, Frank J, Carazo J-M (a)
Disentangling conformational states of macro-
molecules in D-EM through likelihood opti-
mization. Nat Methods :–
. Scheres
SHW,
Núñez-Ramírez
R,
Gómez-
Llorente Y, San Martín C, Eggermont PPB,
Carazo J-M (b) Modeling experimental
image
formation
for
likelihood-based
clas-
sification
of
electron
microscopy.
Structure
:–
. Schmidlin P () Iterative separation of tomo-
graphic scintigrams. Nuklearmedizin :–


EM Algorithms
. Setzer S, Steidl G, Teuber T () Deblurring
Poissonian images by split Bregman techniques.
J Vis Commun Image Repr :–
. Shepp LA, Vardi Y () Maximum likelihood
reconstruction in emission tomography. IEEE
Trans Med Imag :–
. Sigworth
FJ
()
A
maximum-likelihood
approach to single-particle image refinement.
J Struct Biol :–
. Silverman BW, Jones MC, Wilson JD, Nychka
DW () A smoothed EM algorithm approach
to indirect estimation problems, with particular
reference to stereology and emission tomography
(with discussion). J R Stat Soc B :–
. Sun Y, Walker JG () Maximum likeli-
hood data inversion for photon correlation spec-
troscopy. Meas Sci Technol ():
. Tanaka E, Kudo H () Optimal relaxation
parameters of DRAMA (dynamic RAMLA) aim-
ing at one-pass image reconstruction for D-PET.
Phys Med Biol :–
. Tarasko MZ () On a method for solution
of the linear system with stochastic matrices (in
Russian), Report Physics and Energetics Institute,
Obninsk PEI-
. Trummer MR () A note on the ART of relax-
ation. Computing :–
. van der Sluis A, van der Vorst HA ()
SIRT- and CG-type methods for the iterative
solution of sparse linear least-squares prob-
lems. Linear algebra in image reconstruction
from projections. Linear Algebra Appl :
–
. Vardi Y, Shepp LA, Kaufman L () A statistical
model for positron emission tomography (with
discussion). J Am Stat Assoc :–
. Wernick M, Aarsvold J () Emission tomog-
raphy: the fundamentals of PET and SPECT. Else-
vier Academic Press, San Diego
. Wu CFJ () On the convergence properties of
the EM algorithm. Ann Stat :–
. Yu S, Latham GA, Anderssen RS () Stabi-
lizing properties of maximum penalized likeli-
hood estimation for additive Poisson regression.
Inverse Probl :–
. Yuan Jianhua, Yu Jun () Median-prior
tomography reconstruction combined with non-
linear anisotropic diffusion filtering. J Opt Soc
Am A : –

Iterative Solution Methods
Martin Burger ⋅Barbara Kaltenbacher ⋅Andreas Neubauer
.
Introduction.....................................................................
..
Conditions on F.........................................................................
..
Source Conditions......................................................................
..
Stopping Rules...........................................................................
.
Gradient Methods...............................................................
..
Nonlinear Landweber Iteration.......................................................
..
Landweber Iteration in Hilbert Scales................................................
..
Steepest Descent and Minimal Error Method......................................
..
Further Literature on Gradient Methods............................................
...
Iteratively Regularized Landweber Iteration........................................
...
A Derivative Free Approach...........................................................
...
Generalization to Banach Spaces.....................................................
.
Newton Type Methods...........................................................
..
Levenberg-Marquardt and Inexact Newton Methods.............................
..
Further Literature on Inexact Newton Methods....................................
..
Iteratively Regularized Gauss–Newton Method....................................
..
Generalizations of the IRGNM.......................................................
...
Examples of Methods Rα...............................................................
..
Further Literature on Gauss–Newton Type Methods..............................
...
Generalized Source Conditions.......................................................
...
Other A Posteriori Stopping Rules....................................................
...
Stochastic Noise Models................................................................
...
Generalization to Banach Space.......................................................
...
Preconditioning.........................................................................
.
Nonstandard Iterative Methods.................................................
..
Kaczmarz and Splitting Methods.....................................................
..
EM Algorithms..........................................................................
..
Bregman Iterations......................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Iterative Solution Methods
Abstract: This chapter deals with iterative methods for nonlinear ill-posed problems.
We present gradient and Newton type methods as well as nonstandard iterative algorithms
such as Kaczmarz, expectation maximization, and Bregman iterations.
Our intention here is to cite convergence results in the sense of regularization and to
provide further references to the literature.
.
Introduction
This chapter will be devoted to the iterative solution of inverse problems formulated as
nonlinear operator equations
F(x) = y,
(.)
where F : D(F) →Y with domain D(F) ⊆X. The exposition will be mainly restricted to
the case of X and Y being Hilbert spaces with inner products ⟨⋅,⋅⟩and norms ∥⋅∥. Some
references for the Banach space case will be given.
We will assume attainability of the exact data y in a ball Bρ(x), i.e., the equation
F(x) = y is solvable in Bρ(x). The element xis an initial guess which may incorporate a
priori knowledge of an exact solution.
The actually available data yδ will in practice usually be contaminated with noise for
which we here use a deterministic model, i.e.,
∥yδ −y∥≤δ,
(.)
where the noise level δ is assumed to be known. For a convergence analysis with stochastic
noise, see the references in > Sect. ...
..
Conditions on F
For the proofs of well-definedness and local convergence of the iterative methods consid-
ered here we need several conditions on the operator F. Basically, we inductively show that
the iterates remain in a neighborhood of the initial guess. Hence, to guarantee applicability
of the forward operator to these iterates, we assume that
Bρ(x) ⊆D(F)
(.)
for some ρ > .
Moreover, we need that F is continuously Fréchet-differentiable, that ∥F′(x)∥is uni-
formly bounded with respect to x ∈Bρ(x) , and that problem (> .) is properly scaled,
i.e., certain parameters occurring in the iterative methods have to be chosen appropriately
in dependence of this uniform bound.
The assumption that F′ is Lipschitz continuous,
∥F′(˜x) −F′(x)∥≤L ∥˜x −x∥,
x, ˜x ∈Bρ(x),
(.)

Iterative Solution Methods 

that is often used to show convergence of iterative methods for well-posed problems,
implies that
∥F(˜x) −F(x) −F′(x)(˜x −x)∥≤c ∥˜x −x∥,
x, ˜x ∈Bρ(x).
(.)
However, this Taylor remainder estimate is too weak for the ill-posed situation unless the
solution is sufficiently smooth (see, e.g., case (ii) in Theorem below). An assumption on
F that can often be found in the literature on nonlinear ill-posed problems is the tangential
cone condition
∥F(x) −F(˜x) −F′(x)(x −˜x)∥≤η ∥F(x) −F(˜x)∥,
η < 
,
x, ˜x ∈Bρ(x) ⊆D(F),
(.)
which implies that

+ η ∥F′(x)(˜x −x)∥≤∥F(˜x) −F(x)∥≤

−η ∥F′(x)(˜x −x)∥
for all x, ˜x ∈Bρ(x). One can even prove (see [, Proposition .]).
Proposition 
Let ρ, ε > be such that
∥F(x) −F(˜x) −F′(x)(x −˜x)∥≤c(x, ˜x) ∥F(x) −F(˜x)∥,
x, ˜x ∈Bρ(x) ⊆D(F),
for some c(x, ˜x) ≥, where c(x, ˜x) < if ∥x −˜x∥≤ε.
(i) Then for all x ∈Bρ(x)
Mx := {˜x ∈Bρ(x) : F(˜x) = F(x)} = x + N(F′(x)) ∩Bρ(x)
and N(F′(x)) = N(F′(˜x)) for all ˜x ∈Mx. Moreover,
N(F′(x)) ⊇{t(˜x −x) : ˜x ∈Mx, t ∈R},
where instead of ⊇equality holds if x ∈
○
Bρ(x).
(ii) If F(x) = y is solvable in Bρ(x), then a unique x-minimum-norm solution exists. It
is characterized as the solution x† of F(x) = y in Bρ(x) satisfying the condition
x† −x∈N(F′(x†)) ⊆X.
(.)
If F(x) = y is solvable in Bρ(x) but a condition like (> .) is not satisfied, then at least
existence (but no uniqueness) of an x-minimum-norm solution is guaranteed provided
that F is weakly sequentially closed (see [, Chap. ]).
For the proofs of convergence rates, one even needs stronger conditions on F′ than
condition (> .).


Iterative Solution Methods
..
Source Conditions
It is well-known by now that the convergence of regularized solutions can be arbitrar-
ily slow. Rates can only be proven if the exact solution x† satisfies some regularity
assumptions, so-called source conditions. They are usually of Hölder-type, i.e.,
x† −x= (F′(x†)∗F′(x†))μv,
v ∈N(F′(x†))
(.)
for some exponent μ > . Due to typical smoothing properties of the linearized forward
operator F′(x†), they can be interpreted as smoothness assumptions on the initial error
x† −x.
Logarithmic source conditions, i.e.,
x† −x= f L
μ (F′(x†)∗F′(x†))v,
μ > ,
v ∈N(F′(x†)),
f L
μ (λ) := (−ln (λc−
L ))
−μ ,
cL > c
s ,
(.)
have been considered by Hohage [] for severely ill-posed problems (cf. [, Theorem .]
for Landweber iteration, [] for the IRGNM, and [] for generalized IRGNM).
..
Stopping Rules
In the context of ill-posed problems, it is essential to stop iterative solution methods accord-
ing to an appropriate rule to avoid an unbounded growth of the propagated noise. There are
two possibilities, either a priori rules or a posteriori rules. A priori rules (see, e.g., (> .)
and (> .) are computationally very effective. However, the disadvantage is that one has
to know the smoothness index μ in (> .) or (> .) explicitely.
This is avoided in a posteriori stopping rules. The most well-known a posteriori crite-
rion is the so-called discrepancy principle, i.e., the iteration is stopped after k∗= k∗(δ, yδ)
steps with
∥yδ −F (xδ
k∗)∥≤τδ < ∥yδ −F (xδ
k)∥,
≤k < k∗,
(.)
where τ > .
.
Gradient Methods
One way to derive iterative regularization methods is to apply gradient methods to the
minimization problem
min 
∥F(x) −y∥
over D(F).
Since the negative gradient of this functional is given by F′(x)∗(y −F(x)) and taking into
account that only noisy data yδ are available, this yields methods of the form
xδ
k+= xδ
k + ωδ
kF′ (xδ
k)
∗(yδ −F (xδ
k)),
(.)

Iterative Solution Methods 

where xδ
= xis an initial guess of the exact solution. Choosing the factor ωδ
k in a special
way we obtain well-known methods like Landweber iteration, the steepest descent method,
and the minimal error method.
..
Nonlinear Landweber Iteration
If one chooses ωδ
k = ω to be constant, one obtains Landweber iteration. As already men-
tioned in the introduction of this chapter, well-definedness and convergence can only be
proven if problem (> .) is properly scaled. Without loss of generality we may assume that
ωδ
k ≡and that
∥F′(x)∥≤,
x ∈Bρ(x) ⊂D(F).
(.)
The nonlinear Landweber iteration is then given as the method
xδ
k+= xδ
k + F′ (xδ
k)
∗(yδ −F (xδ
k)),
k ∈N.
(.)
We want to emphasize that for fixed iteration index k the iterate xδ
k depends continuously
on the data yδ, since xδ
k is the result of a combination of continuous operations.
The results on convergence and convergence rates for this method presented here were
established in [] (see also []). To begin with, we formulate the following monotonicity
property that gives us a clue on how to choose the number τ in the stopping rule (> .)
(see [, Proposition .]).
Proposition
Assume that the conditions (> .) and (>.) hold and that the equation
F(x) = y has a solution x∗∈Bρ(x). If xδ
k ∈Bρ(x∗), a sufficient condition for xδ
k+to be a
better approximation of x∗than xδ
k is that
∥yδ −F (xδ
k)∥> + η
−η δ.
Moreover, it then holds that xδ
k, xδ
k+∈Bρ(x∗) ⊂Bρ(x).
In view of this proposition, the number τ in the stopping rule (> .) should be chosen
as
τ = + η
−η,
with η as in (> .). To be able to prove that the stopping index k∗in (> .) is finite and
hence well defined it turns out that τ has to be chosen slightly larger (see [, Corollary
.]), i.e.,
τ > + η
−η > .
(.)


Iterative Solution Methods
Corollary 
Let the assumptions of Proposition hold and let k∗be chosen according to the
stopping rule (> .), (> .). Then
k∗(τδ)<
k∗−
∑
k=
∥yδ −F (xδ
k)∥≤
τ
(−η)τ −(+ η) ∥x−x∗∥.
In particular, if yδ = y (i.e., if δ = ), then
∞
∑
k=
∥y −F(xk)∥< ∞.
(.)
Note that (> .) implies that, if Landweber iteration is run with precise data y = yδ,
then the residual norms of the iterates tend to zero as k →∞. That is, if the iteration
converges, then the limit is necessarily a solution of F(x) = y. The following convergence
result holds (see [, Theorem .]):
Theorem 
Assume that the conditions (> .) and (> .) hold and that the equation
F(x) = y is solvable in Bρ(x). Then the nonlinear Landweber iteration applied to exact data
y converges to a solution of F(x) = y. If N(F′(x†)) ⊂N(F′(x)) for all x ∈Bρ(x†), then
xk converges to x† as k →∞.
We emphasize that, in general, the limit of the Landweber iterates is no x-minimum-
norm solution. However, since the monotonicity result of Proposition holds for every
solution, the limit of xk has to be at least close to x†. As can be seen in > Fig. -, it has to
be the closer the larger ρ can be chosen.
It is well known that if yδ does not belong to the range of F, then the iterates xδ
k of
(> .) cannot converge but still allow a stable approximation of a solution of F(x) = y
provided the iteration is stopped after k∗steps. The next result shows that the stopping
r
2r
x0
x† +    (F ′(x†))
x†
⊡Fig. -
The sketch above shows the initial element x, the x-minimum-norm solution x†, the
subset x† + N(F′(x†)), and, in bold, the region where the limit of the iterates xk can be

Iterative Solution Methods 

rules (> .), (> .) render the Landweber iteration a regularization method (see [,
Theorem .]):
Theorem 
Let the assumptions of Theorem hold and let k∗= k∗(δ, yδ) be chosen
according to the stopping rule (> .), (> .). Then the Landweber iterates xδ
k∗converge
to a solution of F(x) = y. If N(F′(x†)) ⊂N(F′(x)) for all x ∈Bρ(x†), then xδ
k∗converges
to x† as δ →.
To obtain convergence rates the exact solution has to satisfy some source conditions.
Moreover, one has to guarantee that the iterates remain in R(F′(x†)∗). In [] rates were
proven under the additional assumption that F satisfies
F′(x) = RxF′(x†)
and
∥Rx −I∥≤c ∥x −x†∥,
x ∈Bρ(x),
where {Rx : x ∈Bρ(x)} is a family of bounded linear operators Rx : Y →Y and c is a
positive constant.
Unfortunately, these conditions are not always satisfied (see [, Example .]). There-
fore, we consider instead of (> .) the following slightly modified iteration method,
xδ
k+= xδ
k + ωGδ (xδ
k)
∗(yδ −F (xδ
k)),
k ∈N,
(.)
where, as above, xδ
= xis an initial guess, Gδ(x) := G(x, yδ), and G is a continuous
operator mapping D(F) × Y into L(X,Y). The iteration will again be stopped according
to the discrepancy principle (> .).
To obtain local convergence and convergence rates for this modification, we need the
following assumptions:
Assumption 
Let ρ be a positive number such that Bρ(x) ⊂D(F).
(i) The equation F(x) = y has an x-minimum-norm solution x† in Bρ(x).
(ii) There exist positive constants c, c, cand linear operators Rδ
x such that for all x ∈
Bρ(x†) the following estimates hold:
∥F(x) −F(x†) −F′(x†)(x −x†)∥≤c∥F(x) −F(x†)∥∥x −x†∥,
(.)
Gδ(x) = Rδ
xGδ(x†),
(.)
∥Rδ
x −I∥≤c∥x −x†∥,
(.)
∥F′(x†) −Gδ(x†)∥≤cδ.
(.)
(iii) The scaling parameter ω in (> .) satisfies the condition
ω ∥F′(x†)∥
≤.


Iterative Solution Methods
Note that, if instead of (> .) the slightly stronger condition
∥F(x) −F(˜x) −F′(x)(x −˜x)∥≤c ∥x −˜x∥∥F(x) −F(˜x)∥,
x, ˜x ∈Bρ(x) ⊆D(F),
(.)
holds in Bρ(x) for some c > , then the unique existence of the x-minimum-norm
solution x† follows from Proposition if F(x) = y is solvable in Bρ(x).
Convergence and convergence rates for the modification above are obtained as follows
(see [, Theorem .and Theorem .]):
Theorem 
Let Assumption hold and let k∗= k∗(δ, yδ) be chosen according to the
stopping rule (> .).
(i) If ∥x−x†∥is so small and if the parameter τ in (> .) is so large that
η+ η
η
< 
and
τ >
(+ η+ cη∥x−x†∥)
−η−η
η

,
where
η:= ∥x−x†∥(c+ c(+ c∥x−x†∥,
η:= + c∥x−x†∥,
η:= + c∥x−x†∥,
then the modified Landweber iterates xδ
k∗converge to x† as δ →.
(ii) If τ > and if x† −xsatisfies (> .) with some < μ ≤/and ∥v∥sufficiently small,
then it holds that
k∗= O (∥v∥

μ+δ−

μ+)
and
∥xδ
k∗−x†∥=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
o (∥v∥

μ+δ
μ
μ+),
μ < 
,
O (
√
∥v∥δ),
μ = 
.
Note that for the modified Landweber iteration we obtain the same convergence rates
and the same asymptotical estimate for k∗as for linear ill-posed problems (compare
[, Theorem .]) if μ ≤/in (> .).
Under the Assumption and according to the theorem above the best possible conver-
gence rate is
∥xδ
k∗−x†∥= O (
√
δ)

Iterative Solution Methods 

provided that μ = /. Even if μ > /we cannot improve this rate without an additional
restriction of the nonlinearity of F.
We will show for the following parameter estimation problem that the conditions of
Assumption are satisfied if F′(x) is replaced by a certain operator Gδ(x).
Example 
We treat the problem of estimating the diffusion coefficient a in
−(a(s)u(s)s)s = f (s),
s ∈(,),
u() = = u(),
(.)
where f ∈L; the subscript s denotes derivative with respect to s.
In this example, F is defined as the parameter-to-solution mapping
F : D(F) := {a ∈H[,] : a(s) ≥a > } →L[,]
a ↦F(a) := u(a),
where u(a) is the solution of (> .). One can prove that F is Fréchet-differentiable
(see, e.g., []) with
F′(a)h = A(a)−[(hus(a))s],
F′(a)∗w = −B−[us(a)(A(a)−w)s],
where
A(a) : H[,] ∩H
[,] →L[,]
u ↦A(a)u := −(aus)s
and
B : D(B) := {ψ ∈H[,] : ψ′() = ψ′() = } →L[,]
ψ ↦Bψ := −ψ′′ + ψ ;
note that B−is the adjoint of the embedding operator from H[,] in L[,].
First of all, we show that F satisfies condition (> .): let F(a) = u, F(˜a) = ˜u, and
w ∈L. Noting that (˜u−u) ∈H∩H
and that A(a) is one-to-one and onto for a, ˜a ∈D(F),
we obtain that
⟨F(˜a) −F(a) −F′(a)(˜a −a),w)⟩L
= ⟨(˜u −u) −A(a)−[((˜a −a)us)s],w ⟩L
= ⟨A(a)(˜u −u) −((˜a −a)us)s, A(a)−w ⟩L
= ⟨((˜a −a)(˜us −us))s, A(a)−w ⟩L
= −⟨(˜a −a)(˜u −u)s,(A(a)−w)s ⟩L
= ⟨F(˜a) −F(a),((˜a −a)(A(a)−w)s)s ⟩L.


Iterative Solution Methods
This together with the fact that ∥g∥L∞≤
√
∥g∥Hand that ∥g∥L∞≤∥g′∥Lif g ∈His
such that g(ξ) = for some ξ ∈[,], yields the estimate
∥F(˜a) −F(a) −F′(a)(˜a −a)∥L
≤
sup
∥w∥L=
⟨F(˜a) −F(a),((˜a −a)(A(a)−w)s)s ⟩L
≤∥F(˜a) −F(a)∥L
sup
∥w∥L=
[∥( ˜a −a
a
)
s
∥
L∥a(A(a)−w)s∥L∞
+ ∥˜a −a
a
∥
L∞∥w∥L]
≤a−(+
√
+ a−√
∥a∥H) ∥F(˜a) −F(a)∥L∥˜a −a∥H.
(.)
This implies (> .).
The conditions (> .) and (> .) are not fulfilled with Gδ(x) = F′(x). Noting that
F′(a)∗w is the unique solution of the variational problem: for all v ∈H
⟨(F′(a)∗w)s,vs ⟩L+ ⟨F′(a)∗w,v ⟩L= ⟨u(a),((A(a)−w)sv)s ⟩L,
(.)
we propose to choose Gδ in (> .) as follows: Gδ(a)∗w = G(a,uδ)∗w is the unique
solution g of the variational problem
⟨gs,vs ⟩L+ ⟨g,v ⟩L= ⟨uδ,((A(a)−w)sv)s ⟩L,
v ∈H.
(.)
This operator Gδ obviously satisfies (> .), since
G(˜a,uδ)∗= G(a,uδ)∗R(˜a, a)∗
with
R(˜a, a)∗= A(a)A(˜a)−.
The condition (> .) is satisfied, since one can estimate as in (> .) that
∥R(˜a, a)∗−I∥= ∥A(a)A(˜a)−−I∥≤a−(+
√
+ a−√
∥˜a∥H) ∥˜a −a∥H.
Note that a constant cindependent from ˜a can be found, since it is assumed that ˜a ∈
Bρ(a). Now we turn to condition (> .): using (> .) and (> .) we obtain similarly
to (> .) the estimate
∥(F′(a)∗−G(a,uδ)∗)w∥H=
sup
∥v∥H=
⟨u(a) −uδ,((A(a)−w)sv)s ⟩L
≤a−(+
√
+ a−√
∥a∥H) ∥u(a) −uδ∥L∥w∥L.
This together with F(a†) = u(a†) and ∥uδ −u(a†)∥L≤δ implies that
∥F′(a†) −G(a†,uδ)∥≤a−(+
√
+ a−√
∥a†∥H) δ
and hence (> .) holds.

Iterative Solution Methods 

Thus, Theorem is applicable, i.e., if ω and τ are chosen appropriately, then the modified
Landweber iterates aδ
k∗(cf. (> .)) where k∗is chosen according to the stopping rule
(> .) converge to the exact solution a† with the rate O (
√
δ) provided that
a† −a= −B−[us(a†)(A(a†)−w)s]
with ∥w∥sufficiently small. Note that this means that
a† −ao ∈H,
(a† −a)s() = = (a† −a)s(),
z := (a† −a)ss −(a† −a)
us(a)
∈H,
∫

z(s) ds = .
Basically this means that one has to know all rough parts of a† up to H. But without this
knowledge one cannot expect to get the rate O (
√
δ).
In [] two other nonlinear problems were treated where conditions (> .) and
(> .) are satisfied with Gδ(x) = F′(x).
..
Landweber Iteration in Hilbert Scales
We have mentioned in the last subsection that for classical Landweber iteration the rates
can not be better than O (
√
δ) under the given assumptions. However, better rates may be
obtained for solutions that satisfy stronger smoothness conditions if the iteration is per-
formed in a subspace of X with a stronger norm. This leads us directly to regularization
in Hilbert scales. On the other hand, for solutions with poor smoothness properties, the
number of iterations may be reduced if the iteration is performed in a space with a weaker
norm.
First of all, we shortly repeat the definition of a Hilbert scale: let L be a densely defined
unbounded selfadjoint strictly positive operator in X. Then (Xs)s∈R denotes the Hilbert
scale induced by L if Xs is the completion of ⋂∞
k=D(Lk) with respect to the Hilbert space
norm ∥x∥s := ∥Lsx∥X ; obviously ∥x∥= ∥x∥X (see [] or [, Sect. .] for details).
The operator F′ (xδ
k)
∗in (> .) will now be replaced by the adjoint of F′ (xδ
k) con-
sidered as an operator from Xs into Y. Usually s ≥, but we will see below that there are
special cases where a negative choice of s can be advantageous. Since by definition of Xs
this adjoint is given by L−sF′ (xδ
k)
∗, (> .) is replaced by the iteration process
xδ
k+= xδ
k + L−sF′ (xδ
k)
∗(yδ −F (xδ
k)),
k ∈N.
(.)
As in the previous chapter, the iteration process is stopped according to the discrepancy
principle (> .).
Proofs of convergence and convergence rates for this method can be found in [, ,
]. For an approach, where the Hilbert scale is chosen in the space Y, see [].


Iterative Solution Methods
The following basic conditions are needed.
Assumption 
(i) F : D(F)(⊂X) →Y is continuous and Fréchet-differentiable in X.
(ii) F(x) = y has a solution x†.
(iii)
∥F′(x†)x∥≤m ∥x∥−a for all x ∈X and some a > , m > . Moreover, the extension
of F′(x†) to X−a is injective.
(iv) B := F′(x†)L−s is such that ∥B∥X ,Y ≤, where −a < s. If s < , F′(x†) has to be
replaced by its extension to Xs.
Usually, for the analysis of regularization methods in Hilbert scales, a stronger condi-
tion than (iii) is used, namely (cf., e.g., [])
∥F′(x†)x∥∼∥x∥−a
for all x ∈X,
(.)
where the number a can be interpreted as a degree of ill-posedness of the linearized prob-
lem in x†. However, this condition is not always fulfilled. Sometimes one can only prove
that condition (iii) in Assumption holds. It might also be possible that one can prove an
estimate from below in a slightly weaker norm (see examples in []), i.e.,
∥F′(x†)x∥≥m ∥x∥−˜a
for all x ∈X and some ˜a ≥a, m > .
(.)
The next proposition sheds more light onto condition (iii) in Assumption and
(> .). The proof follows the lines of [, Corollary .] noting that the results there
not only hold for s ≥but also for s > −a.
Proposition 
Let Assumption hold. Then for all
∈[,] it holds that
D((B∗B)−) = R((B∗B) ) ⊂X (a+s),
∥(B∗B) x∥≤m
∥x∥−(a+s)
for all x ∈X,
∥(B∗B)−x∥≥m−∥x∥
(a+s)
for all x ∈D((B∗B)−).
Note that condition (iii) is equivalent to
R(F′(x†)∗) ⊂Xa
and
∥F′(x†)∗w∥a ≤m ∥w∥
for all w ∈Y.
If in addition condition (> .) holds, then for all
∈[,] it holds that
X ( ˜a+s) ⊂R((B∗B) ) = D((B∗B)−),
∥(B∗B) x∥≥m
∥x∥−( ˜a+s)
for all x ∈X,
∥(B∗B)−x∥≤m−∥x∥
( ˜a+s)
for all x ∈X ( ˜a+s).

Iterative Solution Methods 

Note that condition (> .) is equivalent to
X ˜a ⊂R(F′(x†)∗)
and ∥F′(x†)∗w∥˜a ≥m ∥w∥
for all
w ∈N(F′(x†)∗) with F′(x†)∗w ∈X ˜a.
In our convergence analysis, the following shifted Hilbert scale will play an important
role
̃
Xr := D((B∗B)
s−r
(a+s) Ls)
equipped with the norm
∣∣∣x∣∣∣r := ∥(B∗B)
s−r
(a+s) Lsx∥X ,
where a, s, and B are as in Assumption . Some properties of this shifted Hilbert scale can
be found in [, Proposition .].
For the convergence rates analysis, we need the following smoothness conditions on
the solution x† and the Fréchet-derivative of F.
Assumption 
(i) x∈̃Bρ(x†) := {x ∈X : x −x† ∈̃
X∧∣∣∣x −x†∣∣∣≤ρ} ⊂D(F) for some ρ > .
(ii)
∥F′(x†) −F′(x)∥̃
X−b,Y ≤c∣∣∣x† −x∣∣∣β
for all x ∈̃Bρ(x†) and some b ∈[, a], β ∈
(,], and c > .
(iii) x† −x∈̃
Xu for some (a −b)/β < u ≤b + s, i.e., there is an element v ∈X so that
Ls(x† −x) = (B∗B)
u−s
(a+s) v
and
∥v∥= ∣∣∣x−x†∣∣∣u.
Condition (iii) is a smoothness condition for the exact solution comparable to (> .).
Usually Xu is used instead of ̃
Xu. However, these conditions are equivalent if (> .)
holds.
For the proof of the next convergence rates result see [, Theorem .].
Theorem 
Let Assumptions and hold. Moreover, let k∗= k∗(δ, yδ) be chosen accord-
ing to the stopping rule (> .) with τ > and let ∣∣∣x−x†∣∣∣u be sufficiently small. Then the
following estimates are valid for δ > and some positive constants cr:
k∗≤( τ
τ−∣∣∣x−x†∣∣∣u δ−)
(a+s)
a+u
(.)
and for −a ≤r < u
∣∣∣xδ
k∗−x†∣∣∣r ≤cr ∣∣∣x−x†∣∣∣
a+r
a+u
u
δ
u−r
a+u .
As usual for regularization in Hilbert scales, we are interested in obtaining convergence
rates with respect to the norm in X = X.
Corollary 
Under the assumptions of Theorem the following estimates hold:
∥xδ
k∗−x†∥= O(δ
u
a+u )
if s ≤,
(.)


Iterative Solution Methods
∥xδ
k∗−x†∥= O (∥xδ
k∗−x†∥s) = O(δ
u−s
a+u )
if < s < u.
If in addition (> .) holds, then for s > the rate can be improved to
∥xδ
k∗−x†∥= O (∣∣∣xδ
k∗−x†∣∣∣r) = O(δ
u−r
a+u )
if
r := s( ˜a−a)
˜a+s
≤u.
Note that (> .) implies that k∗is finite for δ > and hence xδ
k∗is a stable
approximation of x†.
Moreover, it can be seen from (> .) that the larger the s, the faster the k∗possibly
grows if δ →. As a consequence, s should be kept as small as possible to reduce the
number of iterations and hence to reduce the numerical effort. If u is close to , it might
be possible to choose a negative s. According to (> .), we would still get the optimal rate,
but, due to (> .), k∗would not grow so fast. Choosing a negative s could be interpreted
as a preconditioned Landweber method (cf. []).
We will now comment on the rates in Corollary : if only Assumption (iii) is satisfied,
i.e., if ∥F′(x†)x∥may be estimated through the norm in X−a only from above, convergence
rates in X can only be given if s < u, i.e., only for the case of undersmoothing. If s > the
rates will not be optimal in general. To obtain rates also for s > u, i.e., for the case of
oversmoothing, condition (> .) has to be additionally satisfied. From what we said on
the choice of s above, the case of oversmoothing is not desirable. However, note that the
rates for ∥xδ
k∗−x†∥can be improved if (> .) holds also for < s < u. Moreover, if
˜a = a, i.e., if the usual equivalence condition (> .) is satisfied, then we always obtain
the usual optimal rates O (δ
u
a+u ) (see []).
For numerical computations one has to approximate the infinite-dimensional spaces by
finte-dimensional ones. Also the operators F and F′(x)∗have to be approximated by suit-
able finite-dimensional realizations. An appropriate convergence rates analysis has been
carried out in []. This analysis also shows that a modification, where F′ (xδ
k)
∗in (> .)
is replaced by Gδ (xδ
k) similar as in (> .), is possible.
..
Steepest Descent and Minimal Error Method
These two methods are again of the form (> .), where the coefficients ωδ
k are chosen as
ωδ
k :=
∥sδ
k∥
∥F′ (xδ
k) sδ
k∥
and
ωδ
k :=
∥yδ −F (xδ
k)∥
∥sδ
k∥
for the steepest descent method and for the minimal error method, respectively.
In [] it has been shown that even for the solution of linear ill-posed problems, the
steepest descent method is only a regularization method when stopped via a discrepancy
principle and not via an a priori parameter choice strategy. Therefore, we will use (> .)
and (> .) as stopping rule.
Again one can show the monotonicity of the errors and well-definedness of the steepest
descent and minimal error method (see [, Proposition .]). Convergence can be shown

Iterative Solution Methods 

for perturbed data (see, e.g., [, Theorem .]). However, so far, convergence rates were
proved only in the case of exact data (see []).
..
Further Literature on Gradient Methods
...
Iteratively Regularized Landweber Iteration
By adding an additional penalty term to the iteration scheme of classical Landweber
iteration, i.e.,
xδ
k+= xδ
k + F′ (xδ
k)
∗(yδ −F (xδ
k)) + βk (x−xδ
k)
with < βk ≤βmax < 
,
one can obtain convergence rates results under weaker restrictions on the nonlinearity of
F (see [, Sect. .],[]). The additional term is motivated by the iteratively regularized
Gauss–Newton method, see > Sect. ...
...
A Derivative Free Approach
Based on an idea by Engl and Zou [], Kügler, in his thesis [] (see also []), developed
a modification of Landweber iteration for parameter identification problems where it is not
needed that F is Fréchet-differentiable.
...
Generalization to Banach Spaces
A generalization of Landweber iteration to the case where X and Y are Banach spaces was
considered in the papers [, , ].
.
Newton Type Methods
Newton’s method for the nonlinear operator equation (> .) reads as
F′ (xδ
k)(xδ
k+−xδ
k) = yδ −F (xδ
k).
(.)
Since ill-posedness of the nonlinear problem (> .) is usually inherited by its linearization
(> .), regularization has to be applied in each Newton step. Formulating (> .) as a
least squares problem,
min
x∈D(F) ∥yδ −F (xδ
k) −F′ (xδ
k)(x −xδ
k)∥,
and applying Tikhonov regularization leads to either the Levenberg–Marquardt method
min
x∈D(F) ∥yδ −F (xδ
k) −F′ (xδ
k)(x −xδ
k)∥+ αk ∥x −xδ
k∥,
(.)


Iterative Solution Methods
where the regularization term ∥x −xδ
k∥is updated in each Newton step, or the iteratively
regularized Gauss–Newton method (IRGNM)
min
x∈D(F) ∥yδ −F (xδ
k) −F′ (xδ
k)(x −xδ
k)∥+ αk ∥x −x∥
(.)
with a fixed a priori guess x∈X. The choice of the sequence of regularization parameters
αk as well as the main ideas of the convergence analysis are quite different for both methods
as will be outlined in the following subsections.
..
Levenberg-Marquardt and Inexact Newton Methods
In the Hilbert space setting with an open set D(F), the minimizer of the quadratic func-
tional in (> .) can be written as the solution of a linear system which leads to the
formulation,
xδ
k+= xδ
k + (F′ (xδ
k)
∗F′ (xδ
k) + αkI)
−
F′ (xδ
k)
∗(yδ −F (xδ
k)),
(.)
of the Levenberg–Marquardt method that can as well be motivated by a trust region
approach.
Our exposition follows the seminal paper by Hanke [], in which the first conver-
gence analysis for this class of Newton type methods was given. According to this paper,
αk should be chosen such that
∥yδ −F (xδ
k) −F′ (xδ
k)(xδ
k+(αk) −xδ
k)∥= q ∥yδ −F (xδ
k)∥
(.)
for some q ∈(,), where xδ
k+(α) is defined as in (> .) with αk replaced by α. This
means that the Newton equation (> .) is only solved up to a residual of magnitude
q ∥yδ −F (xδ
k)∥which corresponds to the concept of inexact Newton methods as they
were first considered for well-posed problems in []. It can be shown (see []) that
(> .) has a unique solution αk provided that
∥yδ −F (xδ
k) −F′ (xδ
k)(x† −xδ
k)∥≤q
γ ∥yδ −F (xδ
k)∥
(.)
for some γ > , which, in its turn, can be guaranteed by (> .).
The key step in the convergence analysis of the Levenberg–Marquardt method is to
show monotonicity of the error norms ∥xδ
k −x†∥. To sketch this monotonicity proof we
assume that (> .) holds and therewith the parameter choice (> .) is feasible. Using
the notation Kk = F′ (xδ
k) as well as Cauchy–Schwarz inequality and the identity
αk(KkK∗
k + αkI)−(yδ −F (xδ
k)) = yδ −F (xδ
k) −Kk (xδ
k+−xδ
k),

Iterative Solution Methods 

we get
∥xδ
k+−x†∥−∥xδ
k −x†∥
= ⟨xδ
k+−xδ
k, xδ
k −x† ⟩+ ∥xδ
k+−xδ
k∥
= ⟨(KkK∗
k + αkI)−(yδ −F (xδ
k)),
Kk (xδ
k −x†) + (KkK∗
k + αkI)−KkK∗
k (yδ −F (xδ
k))⟩
= −αk ∥(KkK∗
k + αkI)−(yδ −F (xδ
k))∥
−∥(K∗
k Kk + αkI)−K∗
k (yδ −F (xδ
k))∥
+ ⟨(KkK∗
k + αkI)−(yδ −F (xδ
k)), yδ −F (xδ
k) −Kk (x† −xδ
k)⟩
≤−∥xδ
k+−xδ
k∥−α−
k ∥yδ −F (xδ
k) −Kk (xδ
k+−xδ
k)∥⋅
(.)
(∥yδ −F (xδ
k) −Kk (xδ
k+−xδ
k)∥−∥yδ −F (xδ
k) −Kk (x† −xδ
k)∥) .
By (> .) and the parameter choice (> .), we have
∥yδ −F (xδ
k) −Kk (x† −xδ
k)∥≤γ−∥yδ −F (xδ
k) −Kk (xδ
k+−xδ
k)∥.
Thus, (> .) and γ > imply estimates (> .) and (> .) in the following
proposition (see [, Proposition .]):
Proposition 
Let < q < < γ and assume that (> .) has a solution and that (> .)
holds so that αk can be defined via (> .). Then, the following estimates hold:
∥xδ
k −x†∥−∥xδ
k+−x†∥≥∥xδ
k+−xδ
k∥,
(.)
∥xδ
k −x†∥−∥xδ
k+−x†∥
≥(γ −)
γαk
∥yδ −F (xδ
k) −F′ (xδ
k)(xδ
k+−xδ
k)∥
(.)
≥(γ −)(−q)q
γ ∥F′ (xδ
k)∥
∥yδ −F (xδ
k)∥.
(.)
Based on the resulting weak convergence of a subsequence of xδ
k as well as on quadratic
summability of the (linearized) residuals, which can be easily obtained by summing up
both sides of (> .) and (> .), one obtains convergence as k →∞in case of exact
data ([, Theorem .]):
Theorem
Let < q < and assume that (> .) is solvable in Bρ(x), that F′ is uniformly
bounded in Bρ(x†), and that the Taylor remainder of F satisfies (> .) for some c > .


Iterative Solution Methods
Then the Levenberg–Marquardt method with exact data yδ = y, ∥x−x†∥< q/c and αk
determined from (> .), converges to a solution of F(x) = y as k →∞.
In case of noisy data, Hanke [] proposes to stop the iteration according to the dis-
crepancy principle (> .) and proves convergence as δ →(see, e.g., [, Theorem .]):
Theorem 
Let the assumptions of Theorem hold. Additionally let k∗= k∗(δ, yδ) be
chosen according to the stopping rule (> .) with τ > /q. Then for ∥x−x†∥sufficiently
small, the discrepancy principle (> .) terminates the Levenberg–Marquardt method with
αk determined from (> .) after finitely many iterations k∗, and
k∗(δ, yδ) = O(+ ∣ln δ∣).
Moreover, the Levenberg–Marquardt iterates xδ
k∗converge to a solution of F(x) = y as δ →.
Convergence rates seem to be much harder to prove for the Levenberg–Marquardt
method than for the iteratively regularized Gauss–Newton method (see
> Sect. ..).
Suboptimal rates under source conditions (> .) have been proven by Rieder [, ]
under the nonlinearity assumption
F′(x) = RxF′(x†)
and
∥I −Rx∥≤cR ∥x −x†∥,
x ∈Bρ(x) ⊆D(F),
(.)
where cR is a positive constant. Only very recently, Hanke [, Theorem .] proved the
following optimal rates result:
Theorem 
Let a solution x† of (> .) exist and let (> .) as well as (> .) hold with
some < μ ≤/and ∥v∥sufficiently small. Moreover, let αk and k∗be chosen according
to (> .) and (> .), respectively, with τ > and > q > /τ. Then the Levenberg–
Marquardt iterates defined by (> .) remain in Bρ(x) and converge with the rate
∥xδ
k∗−x†∥= O(δ
μ
μ+).
Finally, we quote the rates result [, Theorem .] that is almost optimal and instead
of the a posteriori choices of αk and k∗, presume a geometrically decreasing sequence of
regularization parameters, i.e.,
αk = αqk,
for some
α> , q ∈(,),
(.)
and the following a priori stopping rule
ηk∗α
μ+ 

k∗
≤δ < ηkα
μ+ 

k
,
≤k < k∗,
ηk := η(k + )−(+ε),
for some η > ,
ε > .
(.)
Theorem 
Let a solution x† of (> .) exist and let (> .) as well as (> .) hold with
some < μ ≤/and ∥v∥sufficiently small. Moreover, let αk and k∗be chosen according to

Iterative Solution Methods 

(> .) and (> .) with η sufficiently small, respectively. Then the Levenberg–Marquardt
iterates defined by (> .) remain in Bρ(x) and converge with the rate
∥xδ
k∗−x†∥= O((δ (+ ∣ln δ∣)(+ε))
μ
μ+).
Moreover,
∥F (xδ
k∗) −y∥= O(δ (+ ∣ln δ∣)(+ε))
and
k∗= O(+ ∣ln δ∣).
For the noise free case (δ = , η = ) we obtain that
∥xk −x†∥= O (αμ
k),
and that
∥F(xk) −y∥= O (α
μ+ 

k
).
..
Further Literature on Inexact Newton Methods
Hanke [] and Rieder [–] have extended the Levenberg–Marquardt method by
proposing regularization methods other than Tikhonov in the inexact solution of the
Newton equation
xδ
k+= xδ
k + Φ (F′ (xδ
k), yδ −F (xδ
k)),
with Φ (F′ (xδ
k), yδ −F (xδ
k)), e.g., defined by the conjugate gradient method.
Recently, Hochbrück et al. [] proposed the application of an exponential Euler
scheme to the Showalter differential equation
x′(t) = F′(x(t))∗yδ −F (xδ
k),
which leads to a Newton type iterative method of the form
xδ
k+= xδ
k + hkϕ (−hk F′ (xδ
k)
∗F′ (xδ
k)) F′ (xδ
k)
∗(yδ −F (xδ
k)),
with
ϕ(z) = ez −
z
.
In [] they show convergence using the discrepancy principle (> .) as a stopping rule
under condition (> .), as well as optimal convergence rates under the condition that
F′(x) = RxF′(x†)
and
∥I −Rx∥≤cR,
x ∈Bρ(x†) ⊆D(F),
(.)
for some cR ∈(,), and under the source condition (> .) with μ ≤/for an appropriate
choice of the pseudo time step size hk.


Iterative Solution Methods
..
Iteratively Regularized Gauss–Newton Method
In the Hilbert space setting, the variational formulation (> .) of the iteratively regular-
ized Gauss-Newton method can be equivalently written as
xδ
k+= xδ
k + (F′ (xδ
k)
∗F′ (xδ
k) + αkI)
−
(F′ (xδ
k)
∗(yδ −F (xδ
k)) + αk (x−xδ
k)). (.)
Here the sequence of regularization parameters is a priori chosen such that
αk > ,
≤αk
αk+
≤r,
lim
k→∞αk = ,
(.)
for some r > .
This method was first proposed and analyzed by Bakushinskii [], see also [] and the
references therein, as well as [, , –]. The results presented here and in > Sect. ..
together with proofs and further details can be found in [].
The key point in the convergence analysis of the iteratively regularized Gauss–Newton
method is the fact that under a source condition (> .) the error ∥xδ
k+−x†∥is up to some
small additional terms equal to αμ
k wk(μ) with wk(s) defined as in the following lemma that
is easy to prove.
Lemma 
Let K ∈L(X,Y), s ∈[,], and let {αk} be a sequence satisfying αk > and
αk →as k →∞. Then it holds that
wk(s) := α−s
k
∥(K∗K + αkI)−(K∗K)sv∥≤ss(−s)−s ∥v∥≤∥v∥
(.)
and that
lim
k→∞wk(s) = {,
≤s < ,
∥v∥,
s = ,
for any v ∈N(A).
Indeed, in the linear and noiseless case (F(x) = Kx, δ = ) we get from (> .) using
Kx† = y and (> .)
xk+−x† = xk −x† + (K∗K + αkI)−(K∗K(x† −xk) + αk(x−x† + x† −xk))
= −αk(K∗K + αkI)−(K∗K)μv.
To take into account noisy data and nonlinearity, we rewrite (> .) as
xδ
k+−x† = −αk(K∗K + αkI)−(K∗K)μv
−αk(K∗
k Kk + αkI)−(K∗K −K∗
k Kk)
(.)
(K∗K + αkI)−(K∗K)μv
+ (K∗
k Kk + αkI)−K∗
k (yδ −F (xδ
k) + Kk (xδ
k −x†)),
where we set Kk := F′ (xδ
k), K := F′(x†).

Iterative Solution Methods 

Let us consider the case that ≤μ < /in (> .) and assume that the nonlinearity
condition (> .) as well as xδ
k ∈Bρ(x†) ⊆Bρ(x) hold. Therewith, for the Taylor
remainder we obtain that
∥F (xδ
k) −F(x†) −Kk (xδ
k −x†)∥≤cR ∥K (xδ
k −x†)∥.
(.)
The estimates (see (> .))
∥(K∗
k Kk + αkI)−∥≤α−
k ,
∥(K∗
k Kk + αkI)−K∗
k ∥≤
α
−

k ,
and the identity
K∗K −K∗
k Kk = K∗
k (R−
xδ
k
∗−Rxδ
k ) K
imply that
∥αk(K∗
k Kk + αkI)−(K∗K −K∗
k Kk)(K∗K + αkI)−(K∗K)μv∥
≤
α
−

k
∥R−
xδ
k
∗−Rxδ
k ∥∥K(K∗K + αkI)−(K∗K)μv∥.
This together with (> .), (> .), (> .), and F(x†) = y yields the estimate (> .)
in Lemma below. Inserting the identity K = R−
xδ
k Kk into (> .) we obtain
Keδ
k+= −αkK(K∗K + αkI)−(K∗K)μv
−αkR−
xδ
k Kk(K∗
k Kk + αkI)−K∗
k (R−
xδ
k
∗−Rxδ
k ) K
(K∗K + αkI)−(K∗K)μv
−R−
xδ
k Kk(K∗
k Kk + αkI)−K∗
k
(F (xδ
k) −F(x†) −Kk (xδ
k −x†) + y −yδ).
Now the estimate (> .) in Lemma follows together with (> .), (> .), (> .),
and (> .).
Similarly one can derive estimates (> .) and (> .) in case of /≤μ ≤under
the Lipschitz condition (> .) by using (> .) and the decomposition
K∗K −K∗
k Kk = K∗
k (K −Kk) + (K∗−K∗
k ) K.
Lemma 
Let (> .), (> .), (> .) hold and assume that xδ
k ∈Bρ(x†). Moreover, set
K := F′(x†), eδ
k := xδ
k −x†, and let wk(⋅) be defined as in (> .).
(i) If ≤μ < /and (> .) holds, we obtain the estimates
∥eδ
k+∥≤αμ
k wk(μ) + cRαμ
kwk (μ + 
) + α
−

k
(cR ∥Keδ
k∥+ 
δ),
(.)
∥Keδ
k+∥≤(+ cR(+ cR))α
μ+ 

k
wk (μ + 
)
+ (+ cR) (cR ∥Keδ
k∥+ δ).
(.)


Iterative Solution Methods
(ii) If /≤μ ≤and (> .) holds, we obtain the estimates
∥eδ
k+∥≤αμ
kwk(μ) + L ∥eδ
k∥( 
α
μ−

k
wk(μ) + ∥(K∗K)μ−
v∥)
(.)
+ 
α
−

k
( 
L ∥eδ
k∥+ δ),
∥Keδ
k+∥≤αk ∥(K∗K)μ−
v∥+ L∥eδ
k∥( 
α
μ−

k
wk(μ) + ∥(K∗K)μ−
v∥)
+ Lα


k ∥eδ
k∥(α
μ−

k
wk(μ) + 
∥(K∗K)μ−
v∥)
(.)
+ ( 
Lα
−

k
∥eδ
k∥+ )( 
L ∥eδ
k∥+ δ).
It is readily checked that the nonlinearity condition (> .) used in Lemma can be
extended to
F′(˜x) = R(˜x, x)F′(x) + Q(˜x, x)
(.)
∥I −R(˜x, x)∥≤cR
(.)
∥Q(˜x, x)∥≤cQ ∥F′(x†)(˜x −x)∥
(.)
for x, ˜x ∈Bρ(x), where cR and cQ are nonnegative constants.
With the a priori stopping rule
k∗→∞
and
η ≥δα
−

k∗→as δ →.
(.)
For μ = and
ηα
μ+ 

k∗
≤δ < ηα
μ+ 

k
,
≤k < k∗,
(.)
for < μ ≤, one obtains optimal convergence rates as follows (see [, Theorem .]):
Theorem 
Let (> .), (> .), (> .) hold and let k∗= k∗(δ) be chosen according to
(> .) for μ = and (> .) for < μ ≤, respectively.
(i) If ≤μ < /, we assume that (> .)–(> .) hold and that ∥x−x†∥, ∥v∥, η, ρ,
cR are sufficiently small.
(ii) If /≤μ ≤, we assume that (> .) and ∥x−x†∥, ∥v∥, η, ρ are sufficiently small.
Then we obtain that
∥xδ
k∗−x†∥=
⎧⎪⎪⎨⎪⎪⎩
o(),
μ = ,
O(δ
μ
μ+),
< μ ≤.
For the noise free case (δ = , η = ) we obtain that
∥xk −x†∥= {o (αμ
k),
≤μ < ,
O(αk),
μ = ,

Iterative Solution Methods 

and that
∥F(xk) −y∥=
⎧⎪⎪⎨⎪⎪⎩
o(α
μ+ 

k
),
≤μ < 
,
O(αk),

≤μ ≤.
With the discrepancy principle (> .) as an a-posteriori stopping rule in place of the
a priori stopping rule (> .) and (> .), optimal rates can be obtained under a Hölder
type source condition (> .) with μ ≤
(see [, Theorem .]):
Theorem 
Let (> .), (> .), (> .), and (> .)–(> .) hold for some ≤μ ≤
/, and let k∗= k∗(δ) be chosen according to (> .) with τ > . Moreover, we assume that
∥x−x†∥, ∥v∥, /τ, ρ, and cR are sufficiently small. Then we obtain the rates
∥xδ
k∗−x†∥=
⎧⎪⎪⎨⎪⎪⎩
o(δ
μ
μ+),
≤μ < 
,
O(
√
δ),
μ = 
.
In case μ = , and with an a posteriori choice of αk similar to (> .), the nonlinearity
condition can be relaxed to (> .), see [].
..
Generalizations of the IRGNM
Already Bakushinskii in [] proposed to replace Tikonov regularization in (> .) by a
more general method defined via functional calculus by a filter function g with g(λ) ≈
λ :
xδ
k+= x+ g (F′ (xδ
k)
∗F′ (xδ
k)) F′ (xδ
k)
∗(yδ −F (xδ
k) −F′ (xδ
k)(x−xδ
k)),
(.)
with αk ↘.
Still more general, one can replace the operator g (F′ (xδ
k)
∗F′ (xδ
k)) F′ (xδ
k)
∗by some
regularization operator Rα(F′(x)) with
Rα(F′(x)) ≈F′(x)†,
satisfying certain structural conditions so that the convergence analysis for the resulting
Newton type method,
xδ
k+= x+ Rαk (F′ (xδ
k))(yδ −F (xδ
k) −F′ (xδ
k)(x−xδ
k)),
(.)
(see [, , , , , ]) applies not only to methods defined via functional calculus such
as iterated Tikhonov regularization, Landweber iteration, and Lardy’s method, but also to
regularization by discretization. For the convergence proof in this more general setting,
the nonlinearity conditions (> .)–(> .) have to be slightly strengthened to
F′(˜x) = R(˜x, x)F′(x) + Q(˜x, x),
∥R(˜x, x)F′(x)∥≤cs,
∥I −R(˜x, x)∥≤cR ∥˜x −x∥,
and
∥Q(˜x, x)∥≤cQ ∥F′(x†)(˜x −x)∥
(.)


Iterative Solution Methods
for x, ˜x ∈Bρ(x), where cR and cQ are nonnegative constants. The constant cs is such that
∥F′(x)∥≤cs
for all
x ∈Bρ(x) ⊆D(F).
(.)
An additional augmentation of the analysis concerns the type of nonlinearity condition.
Alternatively to range invariance of the adjoint of F′(x) (> .), which is closely related
to (> .), one can consider range invariance of F′(x) itself
F′(˜x) = F′(x)R(˜x, x)
and
∥I −R(˜x, x)∥≤cR ∥˜x −x∥
(.)
for x, ˜x ∈Bρ(x) and some positive constant cR.
Moreover, it is known that Hölder type source conditions are, in general, too strong
for severely ill-posed problems, since they usually imply that solutions are infinitely many
times differentiable. Therefore, we also present results under logarithmic source conditions
(> .).
More precisely, the required approximation and stability properties of Rα are
Rα(K)y →K†y
as α →
for all y ∈R(K),
∥Rα(K)∥≤Φ(α),
and ∥Rα(K)K∥≤cK,
for all K ∈L(X,Y)
with ∥K∥≤cs,
(.)
with cs as in (> .), for some positive function Φ(α) (which by an appropriate scaling
of the regularization parameter without loss of generality can be set to
Φ(α) = cΦα−

(.)
for some positive constant cΦ), and some positive constant cK. Consider the above
mentioned class of methods defined by filter functions gα : [, λ] →R, λ > , satisfying
gα(λ) →λ−
as α →
for all λ ∈(, λ],
sup
λ∈[,λ]
∣λgα(λ)∣≤cg,
and
sup
λ∈[,λ]
∣gα(λ)∣≤c(α),
for some positive constant cg and some positive function c(α), and by setting
Rα(K) := gα(K∗K)K∗.
(.)
Then Rα satisfies (> .) with Φ(α) = (cgc(α))

, cK = cg, and cs = λ

. Further structural
conditions on Rα are
∥(I −Rα(K)K)(K∗K)μ∥≤c,μαμ
and
(.)
∥K(I −Rα(K)K)(K∗K)μ∥≤c,μαμ+ 

(.)
for all K ∈L(X,Y)
with ∥K∥≤cs

Iterative Solution Methods 

in the Hölder type case (> .) and
∥(I −Rα(K)K)f L
μ (K∗K)∥≤c,μ∣ln(α)∣−μ
and
(.)
∥K(I −Rα(K)K)f L
μ (K∗K)∥≤c,μα

∣ln(α)∣−μ
(.)
for all K ∈L(X,Y)
with ∥K∥≤cs
in the logarithmic case (> .).
Conditions on the interaction of K and Rα(K) are
(i) in the context of nonlinearity condition (> .):
∥Rα(KR)KR −Rα(K)K∥≤c∥I −R∥
(.)
for all operators K ∈L(X,Y) and R ∈L(X,X) with ∥K∥, ∥KR∥≤cs and
∥I −R∥≤cI < .
(ii) in the context of nonlinearity condition (> .):
∥KRα(K)∥≤c,
(.)
and either cQ = or
∥Rα( ˜K) ˜K −Rα(K)K∥≤c∥˜K −K∥α−

(.)
∥K(Rα( ˜K) ˜K −Rα(K)K)∥≤c∥˜K −K∥
(.)
for all K, ˜K ∈L(X,Y) with ∥K∥, ∥˜K∥≤cs, as well as
∥Rα(RK)RK −Rα(K)K∥≤c∥I −R∥
∥K(Rα(RK)RK −Rα(K)K)∥≤cα

∥I −R∥
(.)
for all operators K ∈L(X,Y) and R ∈L(Y,Y) with ∥K∥, ∥KR∥≤cs and
∥I −R∥≤cI < .
(iii) in the context of the Lipschitz condition (> .):
∥(Rα( ˜K) ˜K −Rα(K)K)(K∗K)

∥≤c∥˜K −K∥
(.)
for all K, ˜K ∈L(X,Y) with ∥K∥, ∥˜K∥≤cs. In case of spectral methods (> .),
this property can be concluded from the Riesz–Dunford formula, see Sect. .in [].
Here c,μ, c,μ, c,μ, c,μ, c, c, and cI are some positive constants, α ≤α for some α < 
in (> .), and cs is as in (> .) and (> .).
The following convergence rates result is proved in [, Theorem .]. Note that in the
logarithmic case, i.e., when (> .) holds, the following stopping rule is used:
ηα


k∗∣ln(αk∗)∣−μ ≤δ < ηα


k ∣ln(αk)∣−μ,
≤k < k∗.
(.)


Iterative Solution Methods
Theorem 
Let (> .), (> .), (> .), (> .), (> .), (> .) hold and let xδ
k be
defined by the sequence (> .). Moreover, let η and ∥x−x†∥be sufficiently small, and
let one of the following three conditions hold:
(i) The nonlinearity condition (> .) holds together with (> .) for all operators K ∈
L(X,Y) and R ∈L(X,X) with ∥K∥, ∥KR∥≤cs, and ∥I −R∥≤cI < .
If the source condition (> .) or (> .) holds for some μ > , we also assume that
Rα satisfies (> .) or (> .), respectively.
(ii) The nonlinearity condition (> .) holds and Rα satisfies (> .) and (> .) for all
≤μ ≤μfor some μ≥/. (Note that μis called the qualification of the regular-
ization method, cf. []). Moreover, we assume that (> .) and that either (> .)
holds, or (> .), (> .) hold for all K, ˜K ∈L(X,Y) with ∥K∥, ∥˜K∥≤cs.
If the source condition (> .) holds, we assume that < μ ≤/. In the logarithmic
case, i.e., when (> .) holds, we assume that Rα satisfies (> .) and (> .), and
the conditions (> .) for all operators K ∈L(X,Y) and R ∈L(Y,Y) with ∥K∥,
∥KR∥≤cs and ∥I −R∥≤cI < .
(iii) The Lipschitz condition (> .) holds. The solution x† and the regularization method
Rα satisfy (> .) and (> .) for some μ ≥/, respectively. In addition, Rα fulfills
the condition (> .) for all K, ˜K ∈L(X,Y) with ∥K∥, ∥˜K∥≤cs.
Here c, c, and cI are some positive constants and cs is as in (> .).
Then, in the noise free case (δ = , η = ), the sequence xk converges to x† as k →∞. In
case of noisy data and with the choice (> .), xδ
k∗converges to x† as δ →.
If in addition the source condition (> .) or (> .) holds with ∥v∥sufficiently small
and if k∗= k∗(δ) is chosen according to the stopping rule (> .) and (> .), respectively,
then we obtain the convergence rates
∥xδ
k∗−x†∥=
⎧⎪⎪⎨⎪⎪⎩
O (δ
μ
μ+),
in the Hölder type case,
O((+ ∣ln(δ)∣−μ),
in the logarithmic case.
In the noise free case, we obtain the rates
∥xk −x†∥= {O (αμ
k ),
in the Hölder type case,
O(∣ln(αk)∣−μ),
in the logarithmic case.
Corresponding results with a posteriori chosen stopping index can be found in []
with the discrepancy principle (> .) for cases (i) and (iii) and in [] with the modified
discrepancy principle
max {∥F (xδ
k∗−) −yδ∥, σk∗} ≤τδ < max {∥F (xδ
k−) −yδ∥, σk},
≤k < k∗,
where
σk := ∥F (xδ
k−) + F′ (xδ
k−)(xδ
k −xδ
k−) −yδ∥,
for case (ii) with the Hölder type source condition (> .). Moreover, it was shown that
k∗(δ, yδ) satisfies the logarithmic bound O(+ ∣ln δ∣) if αk ∼qk.

Iterative Solution Methods 

...
Examples of Methods Rα
It has been shown in [, ], and in Hohage’s thesis [], see also [], that the following
methods satisfy the assumptions of Theorem :
Tikhonov regularization is defined via gα(λ) := (λ + α)−yielding
Rα(K) = (K∗K + αI)−K∗,
I −Rα(K)K = α(K∗K + αI)−.
Iterated Tikhonov regularization is defined via
gα(λ) :=
n
∑
j=
β−
j
n
∏
l=j
βl(λ + βl)−,
where {βj} is a bounded sequence in R+ such that also β−
j+βj is bounded. An important
special choice is
βj := βq j,
with q ∈(,] and some positive constant β. If q = , the choice is stationary and becomes
Lardy’s method. The effective regularization parameter α is given by
α = αk := ⎛
⎝
nk
∑
j=
β−
j
⎞
⎠
−
.
αk = β(nk + )−if q = 
and
αk ∼qnk if q < .
This yields
Rα(K)
=
n
∑
j=
β−
j
⎛
⎝
n
∏
l=j
βl(K∗K + βl I)−⎞
⎠K∗,
I −Rα(K)K
=
n
∏
j=
βj(K∗K + βjI)−.
(.)
The calculation of wn := Rα(K)z is done iteratively via
wn = (K∗K + βnI)−(K∗z + βnwn−),
w−:= .
Landweber iteration is defined via
gα(λ) :=
n−
∑
j=
(−λ)j
yielding
Rα(K) =
n−
∑
j=
(I −K∗K)jK∗,
I −Rα(K)K = (I −K∗K)n,
with the effective regularization parameter
α = αk := (nk + )−,
where, as for Lardy’s method, nk should grow exponentially.


Iterative Solution Methods
Regularization by discretization is defined by projection onto a sequence of finite
dimensional subspaces
Y⊆Y⊆Y⊆. . . ⊆R(K),
⋃
l∈N
Yl = R(K),
Rα(K) = (Ql K)†Ql = (QlK)† = Pl K†,
(.)
where Ql and Pl are the orthogonal projectors onto Yl and Xl
:=
K∗Yl, respec-
tively. Note that ∥Rα(K)K∥
=
∥Pl∥
=
and that Pl K†y
→
K†y as l
→
∞
(cf. [, Theorem .]). It can be shown (cf. [, Lemma .]) that
∥(I −Rα(K)K)(K∗K)μ∥= ∥(I −Pl)(K∗K)μ∥≤
π ∥(I −Ql)K∥μ
for μ ∈(,] and hence also that
∥K(I −Rα(K)K)(K∗K)μ∥≤∥K(I −Pl)∥∥(I −Pl)(K∗K)μ∥
≤
π ∥(I −Ql)K∥μ+,
where we have used that Ql K(I −Pl) = . Approximation properties of the spaces Yl can
be formulated in terms of the discretization mesh size hl
∥(I −Ql)y∥≤˜chp
l ∥y∥Yp
for all y ∈Yp ⊆Y, where p, ˜c> , see, e.g., Ciarlet [] for the case of discretization by
finite elements. Assuming R(K) ⊆Yp, we obtain
∥(I −Ql)K∥≤˜c∥K∥X ,Yp hp
l .
On the other hand, stability can be guaranteed by so-called inverse inequalities (cf. [])
∥(Ql K)†∥≤˜ch−˜p
l
(.)
for some ˜p, ˜c> and all linear operators K satisfying
K ∈L(X,Yp),
∥K∥X ,Yp ≤cs,
R(K) = Y.
The effective regularization parameter is given by
α = αk := hp
lk .
(.)
If we assume that
p = ˜p
holds, then (> .) and (> .) imply that Rα defined by (> .) satisfies (> .).

Iterative Solution Methods 

..
Further Literature on Gauss–Newton Type Methods
...
Generalized Source Conditions
Convergence and optimal rates for the iteratively regularized Gauss–Newton method were
established in [] under a general source condition of the form
x† −x= f (F′(x†)∗F′(x†))v,
v ∈N(F′(x†)),
with an index function f : [, ∥F′(x†)∥] →[,∞] that is increasing and continuous
with f () = . For this purpose, it is assumed that conditions (> .)–(> .) hold and
the iteration is stopped according to the discrepancy principle (> .).
...
Other A Posteriori Stopping Rules
Bauer and Hohage in [] carry out a convergence analysis with the Lepskii balancing
principle, i.e.,
k∗= min {k ∈{, . . . , kmax} : ∥xδ
k −xδ
m∥≤cΦα−/
m
δ
∀m ∈{k + , . . . , kmax}}
(.)
(with kmax = kmax(δ) an a priori determined index up to which the iterates are well-
defined), in place of the discrepancy principle as an a posteriori stopping rule. Optimal
convergence rates are shown for (> .) with Rα defined by Landweber iteration or (iter-
ated) Tikhonov regularization under condition (> .) if (> .) with μ ≤/or (> .)
holds, and under condition (> .) if μ ≥/in (> .). The advantage of this stopping
rule is that saturation at μ = /is avoided.
...
Stochastic Noise Models
In many practical applications (like, e.g., in weather forcast), the data noise is not only of
deterministic nature as assumed in our exposition, but also random noise has to be taken
into account. In [], Bauer et al. consider the noise model
yδ,σ = F(x†) + δη + σ ξ,
where η ∈Y, ∥η∥≤describes the deterministic part of the noise with noise level δ, ξ
is a normalized Hilbert space process in Y (see, e.g., []), and σ is the variance of the
stochastic noise. Under a Hölder source condition (> .) with μ > /and assuming
a Lipschitz condition (> .), they show almost optimal convergence rates (i.e., with an


Iterative Solution Methods
additional factor that is logarithmic in σ) of (> .) with Rα defined by iterated Tikhonov
regularization and with the balancing principle (> .) as a stopping rule.
...
Generalization to Banach Space
Bakushinski and Kokurin in [] consider the setting Y = X with X Banach space. Using the
Riesz–Dunford formula, they prove optimal convergence rates for the generalized New-
ton method (> .) under the Lipschitz condition (> .), provided a sufficiently strong
source condition, namely (> .) with μ ≥/holds.
In [], based on the variational formulation of the iteratively regularized
Gauss–Newton method, we prove convergence in the general situation of possibly different
Banach spaces X, Y without source condition under the nonlinearity assumption (> .).
Convergence rates are provided in the paper [].
...
Preconditioning
To speed up convergence and save computational effort, it is essential to use precondition-
ing when applying an iterative regularization method Rα in (> .).
Egger in [] defines preconditioners for these iterations (Landweber iteration, CG, or
the -methods, see, e.g., []) via Hilbert scales (see, e.g., []), which leads to an iterative
scheme of the form
xδ
k+= x+ g (L−sF′ (xδ
k)
∗F′ (xδ
k))L−sF′ (xδ
k)
∗(yδ −F (xδ
k) −F′ (xδ
k)(x−xδ
k)),
where L is typically a differential operator and s an appropriately chosen exponent. It is
shown in [] that this leads to a reduction of the number of iterations to about the square
root.
In his thesis [], Langer makes use of the close connection between the CG itera-
tion and Lanczos’ method in order to construct a spectral preconditioner that is especially
effective for severely ill-posed problems.
Further strategies for saving computational effort are, e.g., multigrid and quasi Newton
methods, see [, , , , , , ].
.
Nonstandard Iterative Methods
The methods presented above were based on the standard ideas of minimizing a least-
squares functional, namely, gradient descent and Newton methods. In the following
we shall discuss further iterative methods, either not based on descent of the objective
functional or based on descent for a different functional than least-squares.

Iterative Solution Methods 

..
Kaczmarz and Splitting Methods
Kaczmarz-type methods are used as splitting algorithms for large operators. They are
usually applied if Y and F can be split into
Y = Y× Y× ⋯YM
and
F = (F, F, . . . , FM),
with continuous operators Fj : X →Yj. The corresponding least-squares problem is the
minimization of the functional
J(x) = 

M
∑
j=
∥Fj(x) −yδ
j ∥
Yj.
The basic idea of a Kaczmarz-type method is to apply an iterative scheme to each of
the least-squares terms 
∥Fj(x) −yδ
j ∥
Yj separately in substeps of the iteration. The three
most commonly used approaches are the Landweber–Kaczmarz method (cf. [])
xδ
k+j/M = xδ
k+(j−)/M −ωkF′
j (xδ
k+(j−)/M)
∗
(Fj (xδ
k+(j−)/M) −yδ
j ),
j = , . . ., M,
the nonlinear Kaczmarz method
xδ
k+j/M = xδ
k+(j−)/M −ωkF′
j (xδ
k+j/M)
∗
(Fj (xδ
k+j/M) −yδ
j ) ,
j = , . . . , M,
and the Gauss–Newton–Kaczmarz method
xδ
k+j/M = xδ
k+(j−)/M −(F′
j (xδ
k+(j−)/M)
∗
F′
j (xδ
k+(j−)/M) + αk,jI)
−
F′
j (xδ
k+(j−)/M)
∗
(Fj (xδ
k+(j−)/M) −yδ
j ).
Further Newton–Kaczmarz methods can be constructed in the same way as iteratively
regularized and inexact Newton methods (cf. []).
The Landweber–Kaczmarz and the nonlinear Kaczmarz method can be interpreted as
time discretization by operator splitting for the minimizing flow
x′(t) = −
M
∑
j=
F′
j(x(t))∗(Fj(x(t) −yδ
j ),
with forward respectively backward Euler operator splitting (cf. [, ]). The nonlinear
Kaczmarz method is actually a special case of the Douglas-Rachford splitting algorithm
applied to the above least-squares problem, the iterate xδ
k+j/M can be computed as a
minimizer of the Tikhonov-type functional
Jk,j(x) = 
∥Fj(x) −yδ
j ∥
Yj + 
τ ∥x −xδ
k+(j−)/M∥.
The convergence analysis of Kaczmarz methods is very similar to the analysis of the
iterative methods mentioned above, if nonlinearity conditions on each single operator Fj


Iterative Solution Methods
are posed (cf. [, , ] for the Landweber–Kaczmarz [, ], for Newton–Kaczmarz,
[, ], for nonlinear Kaczmarz, and further variants). The verification of those conditions
is usually an even harder task than for the collection of operators F = (F, . . . , FM), also
due to the usually large nullspace of their linearizations. The analysis can however provide
at least a good idea on the convergence behavior of the algorithms. A nontrivial point in
Kaczmarz methods is an a posteriori stopping criterion, since in general the overall residual
is not decreasing, which rules out standard approaches such as the discrepancy principles.
Some discussions of this issue can be found in [], where criteria based on the sequence
of residuals (∥F (xδ
k+j/M −yδ
j ∥)
j=,...,M have been introduced, supplemented by additional
loping strategies.
Kaczmarz methods have particular advantages in inverse problems for partial differen-
tial equations, when many state equations for different parameters (e.g., different boundary
values or different sources) need to be solved. Then the operators Fj can be set up such that
a problem for one state equation can be solved after the other, which is memory efficient.
We mention that in this case also the Landweber iteration can be carried out in the same
memory-efficient way, since
F′(x)∗(F(x) −yδ) =
M
∑
j=
F′
j(x)∗(Fj(x) −yδ
j ).
But in most cases one observes faster convergence for the Kaczmarz-type variant, which is
similar as comparing classical Jacobi and Gauss–Seidel methods.
Splitting methods are frequently used for the iterative solution of problems with
variational regularization of the form
xδ
α ∈arg min
x
[ 
∥F(x) −yδ∥+ αR(x)],
where R : X →R ∪{+∞} is an appropriate convex regularization functional. It is then
natural to apply operator splitting to the least-squares part and the regularization part, if
R is not quadratic. The most important approaches are the Douglas-Rachford splitting (in
particular for linear operators F, cf. [])
xδ
k+/∈arg min
x
[ 
∥F(x) −y∥+

ωk ∥x −xδ
k∥]
xδ
k+∈arg min
x
[R(x) +

ωk ∥x −xδ
k+/∥]
and the forward–backward splitting algorithm (cf. [])
xδ
k+/= xδ
k −ωkF′ (xδ
k)
∗(F (xδ
k) −yδ)
xδ
k+∈arg min
x
[R(x) +

ωk
∥x −xδ
k+/∥].
Such algorithms are particularly popular for nonsmooth regularization (cf. []). In the
case of sparsity enforcing penalties (ℓ-norms), the second step in both algorithms can

Iterative Solution Methods 

be computed explicitly via shrinkage (thresholding) formulas; such schemes are hence also
called iterative shrinkage (cf. []).
..
EM Algorithms
A very popular algorithm in the case of image reconstruction with nonnegativity con-
straints is the expectation maximization (EM) method, also called Richardson–Lucy algo-
rithm (cf. [, ]). In the case of F : L(Ω) →L(Σ) being a linear operator, it is given by
the multiplicative fixed-point scheme
xδ
k+= xδ
kF∗( yδ
Fxδ
k
).
(.)
For F and F∗being positivity preserving operators (such as the Radon transform or con-
volutions with positive kernels), the algorithm preserves the positivity of an initial value xδ

if the data yδ are positive, too. Positivity of data is a too strong restriction in the case of an
additive noise model, like stochastic Gaussian models or bounds in squared L-distances.
It is however well-suited for multiplicative models such as Poisson models used for imaging
techniques based on counting emitted particles (photons or positrons). The log-likelihood
functional of a Poisson model, respectively its asymptotic for large count rates, can also be
used to derive a variational interpretation of the EM-algorithm. More precisely (> .) is
a descent method for the functional
J(x) := ∫Σ [yδ log( yδ
Fx ) −yδ + Fx]dσ,
which corresponds to the Kullback-Leibler divergence (relative entropy) between the out-
put Fx and the data yδ. Minimizing J over nonnegative functions leads to the optimality
condition
x (−F∗( yδ
Fx ) + F∗) = .
With appropriate operator scaling F∗= , this yields the fixed-point equation
x = xF∗( yδ
Fx ),
which is the basis of the EM-algorithm
xδ
k+= xδ
k
F∗F∗( yδ
Fxδ
k
).
The EM-algorithm or Richardson-Lucy algorithm (cf. []) is a special case of the general
EM framework by Dempster et al. (cf. []).


Iterative Solution Methods
Performing an analogous analysis for a nonlinear operator F : L(Ω) →L(Σ) we are
led to the fixed-point equation
xF′(x)∗= xF′(x)∗( yδ
Fx ).
Since it seems unrealistic to scale F′(x)∗for arbitrary x, it is more suitable to keep the
term and divide by F′(x)∗. The corresponding fixed-point iteration is the nonlinear EM
algorithm
xδ
k+=
xδ
k
F′ (xδ
k)
∗
F′ (xδ
k)
∗⎛
⎝
yδ
F (xδ
k)
⎞
⎠.
The convergence analysis in the nonlinear case is still widely open. Therefore, we only
comment on the case of linear operators F here (cf. also ([, ] for details). In order to
avoid technical difficulties, the scaling condition F∗= will be assumed in the follow-
ing. The major ingredients are reminiscent of the convergence analysis of the Landweber
iteration, but they replace the norm distance by the Kullback–Leibler divergence
KL(x, ˜x) = ∫Ω [x log x
˜x −x + ˜x] ,
which is a nonnegative distance, but obviously not a metric. First of all, xδ
k can be
characterized as a minimizer of the (convex) functional
Jk(x) := −J(x) + KL (xδ
k+, x)
over all nonnegative L-functions, given xδ
k+. Thus, comparing with the functional value at
xδ
k+we find that the likelihood functional J decreases during the iteration, more precisely
J (xδ
k+) ≤J (xδ
k) + KL (xδ
k+, xδ
k).
This part of the analysis holds also in the case of exact data. The second inequality directly
concerns the dissipation of the Kullback–Leibler divergence between the iterates and a
solution and, hence, assumes the existence of x† with Fx† = y. Using convexity arguments
one obtains
KL(x†, xk+) + J(xk) ≤KL(x†, xk).
Hence, together with the monotonicity of J
KL(x†, xk) + kJ(xk) ≤KL(x†, xk) +
k−
∑
j=
J(x j) ≤KL(x†, x),
which implies boundedness of the Kullback–Leibler divergence (hence a weak compactness
of xk in L) and convergence J(xk) →analogous to the arguments for the Landweber
iteration.
The noisy case is less clear, apparently also due to the difficulties in defining a reasonable
noise level for Poisson noise. An analysis defining the noise level in terms of the likelihood
of the noisy data has been given in []. Further analysis in the case of noisy data seems to

Iterative Solution Methods 

be necessary, however. This also concerns stopping rules for noisy data, which are usually
based on the noise level. A promising multiscale stopping criterion based on the stochastic
modelling of Poisson noise has been introduced and tested recently (cf. []).
Iterative methods are also used for Penalized EM-Reconstruction (equivalently
Bayesian MAP estimation), i.e., for minimizing
J(x) + αR(x)
over nonnegative L-functions, where α > is a regularization parameter and R is an
appropriate regularization functional, e.g., total variation or negative entropy.
A frequently used modification of the EM algorithm in this case is Green’s One-Step-
Late (OSL) method (see [, ])
xδ
k+=
xδ
k
F∗+ αR′(xk) F∗( yδ
Fxδ
k
),
which seems efficient if the pointwise sign of R′(xk) can be controlled, such as, e.g., for
entropy-type regularization functionals
R(x) = ∫Ω E(x)
with convex E : Ω →R+ and E′() = . The additional effort compared to EM is neg-
ligible and the method converges reasonably fast to a minimizer of J + αR. For other
important variational regularization methods, in particular gradient-based functionals,
the OSL method is less successful, since R′(xk) does not necessarily have the same sign
as xk, thus F∗+ αR′(xk) can be negative or zero, in which case the iteration has to be
stopped or some ad-hoc fixes have to be introduced. Another obvious disadvantage of
the OSL method is the fact that it cannot handle nonsmooth regularizations such as total
variation and ℓ-norms, which are often used to incorporate structural prior knowledge.
As a more robust alternative, splitting methods have been introduced also in this case. In
[] a positivity-preserving forward-backward splitting algorithm with particular focus on
total variation regularization has been introduced. The two-step algorithm alternates the
classical EM-step with a weighted denoising problem
xδ
k+/= xδ
kF∗( yδ
Fxδ
k
)
xδ
k+∈arg min
x
⎡⎢⎢⎢⎢⎢⎣
∫Ω
(x −xδ
k+/)

xδ
k
+ αR(x)
⎤⎥⎥⎥⎥⎥⎦
.
Convergence can be ensured with further damping, i.e., if the second half step is replaced by
xδ
k+∈arg min
x
⎡⎢⎢⎢⎢⎢⎣
∫Ω
(x −ωkxδ
k+/−(−ωk)xδ
k)

xδ
k
+ αR(x)
⎤⎥⎥⎥⎥⎥⎦


Iterative Solution Methods
with ωk ∈(,) sufficiently small. This algorithm is a semi-implicit approximation of the
optimality condition
−F∗( yδ
Fx ) + + αp = ,
p ∈∂R(x),
where the operator-dependent first part is approximated explicitely and the regularization
part p implicitely. What seems surprising is that the constant is approximated by xδ
k+/xδ
k,
which however turns out to be crucial for preserving positivity.
..
Bregman Iterations
A very general way of constructing iterative methods in Banach spaces are iterations using
so-called Bregman distances. For a convex functional R, the Bregman distance is defined by
Dp
R(˜x, x) = R(˜x) −R(x) −⟨p, ˜x −x ⟩,
p ∈∂R(x).
Note that for nonsmooth R the subgradient is not single-valued, hence the distance
depends on the choice of the specific subgradient. Bregman distances are a very general
class of distances in general, the main properties are Dp
R(˜x, x) ≥and Dp
R(x, x) = .
Particular cases are
Dp
R(˜x, x) = 
∥˜x −x∥
for
R(x) = 
∥x∥
and the Kullback–Leibler divergence for R being a logarithmic entropy functional.
If some data similarity measure H(F(x), yδ) and a regularization functional R is given,
the Bregman iteration (cf. [, ] in its original, different context) consists of
xδ
k+∈arg min
x
[H(F(x), yδ) + Dpk
R (x, xδ
k)]
with the dual update
pk+= pk −∂xH (F (xδ
k+), yδ) ∈∂R (xδ
k+).
The Bregman iteration is a primal-dual method in the sense that it computes an update
for the primal variable x as well as for the dual variable p ∈∂R(x). Consequently one also
needs to specify an initial value for the subgradient p∈∂R (xδ
).
Most investigations of the Bregman iteration have been carried out for H being a
squared norm, i.e., the least-squares case discussed above
H(F(x), yδ) = 
∥F(x) −yδ∥.
Under appropriate nonlinearity conditions, a full convergence analysis can be carried
out (cf. []), in general only leading to some weak convergence and convergence in the
Bregman distance. If F is a nonlinear operator, further approximations in the Bregman

Iterative Solution Methods 

iterations by linearization are possible, leading to the Landweber-type method (also called
linearized Bregman iteration)
xδ
k+∈arg min
x
[⟨F′ (xδ
k)(x −xδ
k), F (xδ
k) −yδ ⟩+ Dpk
R (x, xδ
k)]
and Levenberg–Marquardt type method
xδ
k+∈arg min
x
[ 
∥F (xδ
k) + F′ (xδ
k)(x −xδ
k) −yδ∥+ Dpk
R (x, xδ
k)].
Both schemes have been analyzed in [], see also [–] for the linearized Bregman itera-
tion in compressed sensing. We mention that in particular the linearized Bregman method
does not work with an arbitrary convex regularization functional. In order to guarantee that
the functional to be minimized in each step of the iteration is bounded from below so that
the iterates are well-defined, a quadratic part in the regularization term is needed.
A discussion of Bregman iterations in the case of nonquadratic term H can be found
in [, , ] with particular focus on F being a linear operator. In this case also a dual
Bregman iteration can be constructed, which coincides with the original one in the case of
quadratic H, but differs in general. For this dual Bregman iterations also convergence rates
under appropriate source conditions can be shown (cf. []), which seems out of reach for
the original Bregman iteration for general H.
References and Further Reading
. Akcelik V, Biros G, Draganescu A, Hill J,
Ghattas O, Van Bloemen Waanders B ()
Dynamic data-driven inversion for terascale sim-
ulations: Real-time identification of airborne con-
taminants. In: Proceedings of SC. IEEE/ACM,
Seattle
. Bachmayr M, Burger M () Iterative total vari-
ation schemes for nonlinear inverse problems.
Inverse Prob , 
. Bakushinsky AB () The problem of the con-
vergence of the iteratively regularized Gauss-
Newton method. Comput Math Math Phys
:–
. Bakushinsky AB () Iterative methods with-
outdegenerationfor solvingdegeneratenonlinear
operator equations. Dokl Akad Nauk :–
. Bakushinsky AB, Kokurin MY () Iterative
methods for approximate solution of inverse
problems. Vol. of Mathematics and its appli-
cations. Springer, Dordrecht
. Bauer F, Hohage T () A Lepskij-type stopping
rule for regularized Newton methods. Inverse
Prob :–
. Bauer F, Hohage T, Munk A () Iteratively
regularized Gauss-Newton method for nonlin-
ear inverse problems with random noise. SIAM J
Numer Anal :–
. Baumeister J, De Cezaro A, Leitao A ()
On iterated Tikhonov-Kaczmarz regularization
methods for ill-posed problems, ICJV (), doi:
./s---
. Baumeister J, Kaltenbacher B, Leitao A ()
On
Levenberg-Marquardt-Kaczmarz
iterative
methods for solving systems of nonlinear ill-
posed equations. Inverse Problems and Imaging,
(to appear)
. Bertero M, Boccacci P () Introduction to
inverse problems in imaging. Institute of Physics,
Bristol
. Bissantz N, Hohage T, Munk A, Ruymgaart F
() Convergence rates of general regulariza-
tion methods for statistical inverse problems and
applications. SIAM J Numer Anal :–
. Bissantz N, Mair B, Munk A () A statisti-
cal stopping rule for mlem reconstructions in pet.
IEEE Nucl Sci Symp Conf Rec :–


Iterative Solution Methods
. Blaschke B, Neubauer A, Scherzer O () On
convergence rates for the iteratively regularized
Gauss–Newton method. IMA J Numer Anal
:–
. Bregman LM () The relaxation method of
finding the common point of convex sets and its
application to the solution of problems in con-
vex programming. USSR Comp Math Math Phys
:–
. Brune C, Sawatzky A, Burger M () Bregman-
EM-TV methods with application to optical
nanoscopy. In: Tai X-C et al (ed) Proceedings of
the nd International Conference on Scale Space
and Variational Methods in Computer Vision.
Vol. of LNCS, Springer, pp. –
. Brune C, Sawatzky A, Burger M () Primal
and dual Bregman methods with application to
optical nanoscopy. Preprint, Submitted to IJCV:
Special Issue SSVM, Institute of Computational
and Applied Mathematics
. Burger M, Kaltenbacher B () Regularizing
Newton–Kaczmarz methods for nonlinear ill-
posed problems. SIAM J Numer Anal :–
. Cai JF, Osher S, Shen Z () Convergence of
the linearized Bregman iteration for l-norm min-
imization. Math Comp :–
. Cai JF, OSher S, Shen Z () Linearized
Bregman iterations for compressed sensing. Math
Comp :–
. Cai JF, OSher S, Shen Z () Linearized
Bregman
iterations
for
frame-based
image
deblurring. SIAM J Imaging Sci :–
. Ciarlet PG () The finite element method for
elliptic problems. North Holland, Amsterdam
. Colonius F, Kunisch K () Output least squares
stability in elliptic systems. Appl Math Optim
:–
. Combettes PL, Pesquet J-C () A proximal
decomposition method for solving convex varia-
tional inverse problems, Inverse Prob , 
(pp)
. Daubechies I, Defrise M, De Mol C () An
iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Comm Pure
Appl Math :–
. De Cezaro A, Haltmeier M, Leitao A, Scherzer O
() On steepest-descent-Kaczmarz methods
for regularizing systems of nonlinear ill-posed
equations. Appl Math Comp :–
. Dembo RS, Eisenstat SC, Steihaug T ()
Inexact Newton’s method. SIAM J Numer Anal
:–
. Dempster AP, Laird NM, Rubin DB () Maxi-
mum likelihood from incomplete data via the EM
algorithm. J R Stat Soc Ser B 
. Deuflhard P, Engl HW, Scherzer O ()
A convergence analysis of iterative methods for
the solution of nonlinear ill-posed problems
under affinely invariant conditions. Inverse Prob
:–
. Douglas J, Rachford HH () On the numeri-
cal solution of heat conduction problems in two
and three space variables. Trans Am Math Soc
:–
. Egger H () Fast fully iterative Newton-type
methods for inverse problems. J Inverse Ill-Posed
Prob :–
. Egger H () Y-Scale regularization. SIAM J
Numer Anal :–
. Egger H, Neubauer A () Preconditioning
Landweber iteration in Hilbert scales. Numer
Math :–
. Eicke B, Louis AK, Plato R () The instability
of some gradient methods for ill-posed problems.
Numer Math :–
. Engl HW, Hanke M, Neubauer A () Regular-
ization of inverse problems. Kluwer, Dordrecht
. Engl HW, Zou J () A new approach to con-
vergence rate analysis of Tiknonov regularization
for parameter identification in heat conduction.
Inverse Prob :–
. Glowinski R, Le Tallec P () Augmented
lagrangian and operator splitting methods in
nonlinear mechanics. SIAM, Philadelphia
. Green PJ () Bayesian reconstructions from
emission tomography data using a modified EM
algorithm. IEEE Trans Med Imaging :–
. Green PJ () On use of the EM algorithm for
penalized likelihood estimation. J R Stat Soc Ser B
(Methodological) :–
. Haber E () Quasi-Newton methods for large-
scale electromagnetic inverse problems. Inverse
Prob :–
. Haber E, Ascher U () A multigrid method
for distributed parameter estimation problems.
Inverse Prob :–

Iterative Solution Methods 

. Haltmeier M, Kowar R, Leitao A, Scherzer O
() Kaczmarz methods for regularizing non-
linear ill-posed equations II: applications. Inverse
Prob Imaging :–
. Haltmeier M, Leitao A, Scherzer O ()
Kaczmarz methods for regularizing nonlinear ill-
posed equations I: convergence analysis. Inverse
Prob Imaging :–
. Hanke M () A regularization Levenberg–
arquardt scheme, with applications to inverse
groundwater filtration problems. Inverse Prob
:–
. Hanke M () Regularizing properties of a
truncated Newton-CG algorithm for nonlinear
inverse problems. Numer Funct Anal Optim
:–
. Hanke M () The regularizing Levenberg-
Marquardt scheme is of optimal order, J. Integral
Equations Appl. , (), –
. Hanke M, Neubauer A, Scherzer O () A con-
vergence analysis of the Landweber iteration
for nonlinear ill-posed problems. Numer Math
:–
. He L, Burger M, Osher S () Iterative
total variation regularization with non-quadratic
fidelity. J Math Imaging Vision :–
. Hochbruck M, H¨onig M, Ostermann A ()
A convergence analysis of the exponential Euler
iteration
for
nonlinear
ill-posed
problems.
Inverse Prob :(pp)
. Hochbruck M, H¨onig M, Ostermann A ()
Regularization of nonlinear ill-posed problems by
exponential integrators. Math Mod Numer Anal
:–
. Hohage
T
()
Logarithmic
convergence
rates
of
the
iteratively
regularized
Gauß-
Newton method for an inverse potential and
an inverse scattering problem. Inverse Prob :
–
. Hohage T () Iterative methods in inverse
obstacle scattering: regularization theory of linear
and nonlinear exponentially ill-posed problems.
PhD thesis, University of Linz
. Hohage T () Regularization of exponentially
ill-posed problems. Numer Funct Anal Optim
:–
. Jin
Q,
Tautenhahn
U
()
On
the
discrepancy principle for some Newton type
methods for solving nonlinear ill-posed prob-
lems. Numer Math :–
. Kaltenbacher B () Some Newton type meth-
ods for the regularization of nonlinear ill-posed
problems. Inverse Prob :–
. Kaltenbacher B () On Broyden’s method for
ill-posed problems. Numer Funct Anal Optim
:–
. Kaltenbacher B () A posteriori parameter
choice strategies for some Newton type meth-
ods for the regularization of nonlinear ill-posed
problems. Numer Math :–
. Kaltenbacher B () A projection-regularized
Newton method for nonlinear ill-posed prob-
lems and its application to parameter identifica-
tion problems with finite element discretization.
SIAM J Numer Anal :–
. Kaltenbacher B () On the regularizing prop-
erties of a full multigrid method for ill-posed
problems. Inverse Prob :–
. Kaltenbacher B, Hofmann B () Convergence
rates for the iteratively regularized Gauss-Newton
method in Banach spaces, Inverse Problems 
(), 
. Kaltenbacher B, Neubauer A () Convergence
of projected iterative regularization methods
for nonlinear problems with smooth solutions.
Inverse Prob :–
. Kaltenbacher B, Neubauer A, Ramm AG ()
Convergence rates of the continuous regularized
Gauss–Newton method. J Inverse Ill-Posed Prob
:–
. Kaltenbacher B, Neubauer A, Scherzer O ()
Iterative regularization methods for nonlinear ill-
posed problems. Radon Series on Computational
and Applied Mathematics, de Gruyter, Berlin
. Kaltenbacher B, Schicho J () A multi-grid
method with a priori and a posteriori level choice
for the regularization of nonlinear ill-posed prob-
lems. Numer Math :–
. Kaltenbacher B, Schöpfer F, Schuster T ()
Iterative methods for nonlinear ill-posed prob-
lems in Banach spaces: convergence and appli-
cations to parameter identification problems.
Inverse Prob :(pp)
. King JT () Multilevel algorithms for ill-posed
problems. Numer Math :–


Iterative Solution Methods
. Kowar R, Scherzer O () Convergence analy-
sis of a Landweber-Kaczmarz method for solving
nonlinear ill-posed problems. In: Romanov VG,
Kabanikhin SI, Anikonov YuE, Bukhgeim AL
(eds) Ill-posed and inverse problems. Zeist, VSP,
pp –
. Krein SG, Petunin JI () Scales of Banach
spaces. Russian Math Surveys :–
. Kügler P () A derivative free Landweber iter-
ation for parameter identification in certain ellip-
tic PDEs. Inverse Prob :–
. Kügler P () A derivative free landweber
method for parameter identification in elliptic
partial differential equations with application to
the manufacture of car wind-shields. PhD thesis,
Johannes Kepler University, Linz, Austria
. Langer S () Preconditioned Newton methods
for ill-posed problems. PhD thesis, University of
Göttingen
. Langer S, Hohage T () Convergence anal-
ysis of an inexact iteratively regularized Gauss-
Newton method under general source conditions.
J Inverse Ill-Posed Prob :–
. Lions P-L, Mercier B () Splitting algorithms
for the sum of two nonlinear operators. SIAM J
Numer Anal :–
. Mülthei HN, Schorr B () On properties of
the iterative maximum likelihood reconstruction
method. Math Methods Appl Sci :–
. Natterer F, Wübbeling F () Mathematical
methods in image reconstruction. Society for
Industrial and Applied Mathematics (SIAM),
Philadelphia
. Neubauer A () Tikhonov regularization of
nonlinear ill-posed problems in Hilbert scales.
Appl Anal :–
. Neubauer A () On Landweber iteration for
nonlinear ill-posed problems in Hilbert scales.
Numer Math :–
. Neubauer A, Scherzer O () A convergent
rate result for a steepest descent method and a
minimal error method for the solution of nonlin-
ear ill-posed problems. ZAA :–
. Osher S, Burger M, Goldfarb D, Xu J, Yin W
() An iterative regularization method for
total variation based image restoration. SIAM
Multiscale Mod Simul :–
. Press
WH,
Teukolsky
SA,
Vetterling
WT,
Flannery BP () Numerical recipes: the art
of scientific computing, rd edn. Cambridge
University Press, Cambridge
. Resmerita E, Engl HW, Iusem AN () The
expectation-maximization
algorithm
for
ill-
posed integral equations: a convergence analysis.
Inverse Prob :–
. Rieder A () On the regularization of nonlin-
ear ill-posed problems via inexact Newton itera-
tions. Inverse Prob :–
. Rieder A () On convergence rates of inex-
act Newton regularizations. Numer Math :
–
. Rieder A () Inexact Newton regulariza-
tion using conjugate gradients as inner iteration.
SIAM J Numer Anal :–
. Sawatzky A, Brune C, Wübbeling F, Kösters T,
Schäfers K, Burger M () Accurate EM-TV
algorithm in PET with low SNR. Nuclear Science
Symposium Conference Record. NSS’. IEEE,
pp –
. Scherzer O () A modified Landweber itera-
tion for solving parameter estimation problems.
Appl Math Optim :–
. Schöpfer F, Louis AK, Schuster T () Non-
linear iterative methods for linear ill-posed prob-
lems in Banach spaces. Inverse Prob :–
. Schöpfer F, Schuster T, Louis AK () An itera-
tive regularization method for the solution of the
split feasibility problem in banach spaces. Inverse
Prob :(pp)
. Vardi Y, Shepp LA, Kaufman L () A statisti-
cal model for positron emission tomography with
discussion. J Am Stat Assoc :–

Level Set Methods for
Structural Inversion and
Image Reconstruction
Oliver Dorn ⋅Dominique Lesselier
.
Introduction.....................................................................
..
Level Set Methods for Inverse Problems and Image Reconstruction............
..
Images and Inverse Problems.........................................................
..
The Forward and the Inverse Problem...............................................
.
Examples and Case Studies.....................................................
..
Example : Microwave Breast Screening.............................................
..
Example : History Matching in Petroleum Engineering.........................
..
Example : Crack Detection...........................................................
.
Level Set Representation of Images with Interfaces............................
..
The Basic Level Set Formulation for Binary Media................................
..
Level Set Formulations for Multivalued and Structured Media..................
...Different Levels of a Single Smooth Level Set Function...........................
...Piecewise Constant Level Set Function..............................................
...Vector Level Set..........................................................................
...Color Level Set...........................................................................
...Binary Color Level Set..................................................................
..
Level Set Formulations for Specific Applications...................................
...A Modification of Color Level Set for Tumor Detection..........................
...A Modification of Color Level Set for Reservoir Characterization. .............
...A Modification of the Classical Level Set Technique for Describing Cracks
or Thin Shapes...........................................................................
.
Cost Functionals and Shape Evolution.........................................
..
General Considerations. ...............................................................
..
Cost Functionals........................................................................
..
Transformations and Velocity Flows.................................................
..
Eulerian Derivatives of Shape Functionals..........................................
..
The Material Derivative Method......................................................
..
Some Useful Shape Functionals.......................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Level Set Methods for Structural Inversion and Image Reconstruction
..
The Level Set Framework for Shape Evolution. ....................................
.
Shape Evolution Driven by Geometric Constraints............................
..
Penalizing Total Length of Boundaries...............................................
..
Penalizing Volume or Area of Shape..................................................
.
Shape Evolution Driven by Data Misfit.........................................
..
Shape Deformation by Calculus of Variations. .....................................
...
Least Squares Cost Functionals and Gradient Directions.........................
...Change of b due to Shape Deformations. ............................................
...Variation of Cost due to Velocity Field v(x)........................................
...Example: Shape Variation for TM-Waves............................................
...Example: Evolution of Thin Shapes (Cracks)........................................
..
Shape Sensitivity Analysis and the Speed Method .................................
...Example: Shape Sensitivity Analysis for TM-Waves................................
...Shape Derivatives by a Min-Max Principle..........................................
..
Formal Shape Evolution Using the Heaviside Function...........................
...Example: Breast Screening–Smoothly Varying Internal Profiles.................
...Example: Reservoir Characterization–Parameterized Internal Profiles.........
.
Regularization Techniques for Shape Evolution Driven by Data Misfit......
..
Regularization by Smoothed Level Set Updates....................................
..
Regularization by Explicitly Penalizing Rough Level Set Functions.............
..
Regularization by Smooth Velocity Fields...........................................
..
Simple Shapes and Parameterized Velocities........................................
.
Miscellaneous On-Shape Evolution............................................
..
Shape Evolution and Shape Optimization...........................................
..
Some Remarks on Numerical Shape Evolution with Level Sets..................
..
Speed of Convergence and Local Minima...........................................
..
Topological Derivatives. ...............................................................
.
Case Studies......................................................................
..
Case Study: Microwave Breast Screening............................................
..
Case Study: History Matching in Petroleum Engineering. .......................
..
Case Study: Reconstruction of Thin Shapes (Cracks).............................
.
Cross-References.................................................................

Level Set Methods for Structural Inversion and Image Reconstruction 

Abstract: In this chapter, an introduction is given into the use of level set techniques for
inverse problems and image reconstruction. Several approaches are presented which have
been developed and proposed in the literature since the publication of the original (and
seminal) paper by F. Santosa in on this topic. The emphasis of this chapter, however,
is not so much on providing an exhaustive overview of all ideas developed so far, but on
the goal of outlining the general idea of structural inversion by level sets, which means the
reconstruction of complicated images with interfaces from indirectly measured data. As
case studies, recent results (in D) from microwave breast screening, history matching in
reservoir engineering, and crack detection are presented in order to demonstrate the gen-
eral ideas outlined in this chapter on practically relevant and instructive examples. Various
references and suggestions for further research are given as well.
.
Introduction
..
Level Set Methods for Inverse Problems and Image
Reconstruction
The level set technique has been introduced for the solution of inverse problems in the
seminal paper of Santosa []. Since then, it has developed significantly, and appears to
become now a standard technique for solving inverse problems with interfaces. However,
there are still a large number of unresolved problems and open questions related to this
method, which keeps fuelling active research on it worldwide. This chapter can only give
a rough overview of some techniques which have been discussed so far in the literature.
For more details which go beyond the material covered here the reader is referred to the
recent review articles [, –, ], each of them providing a slightly different view on the
topic and making available a rich set of additional references which the interested reader
can follow for further consultation.
..
Images and Inverse Problems
An image, as referred to in this chapter, is a (possibly vector-valued) function which assigns
to each point of a given domain in D or in D one or more physical parameter values
which are characteristic for that point. An image often contains interfaces, across which
one or more of these physical parameters change value in a discontinuous manner. In many
applications, these interfaces coincide with physical interfaces between different materials
or regions. These interfaces divide the domain Ω in subdomains Ωk, k = , . . . , K of differ-
ent region-specific internal parameter profiles. Often, due to the different physical structure
of each of these regions, quite different mathematical models might be most appropriate
for describing them in the given context.
Since the image represents physical parameters, it can be tested by physical inspection.
Here, the physical parameters typically appear in partial differential equations (PDEs) or in


Level Set Methods for Structural Inversion and Image Reconstruction
integral equations (IEs) as space-dependent coefficients, and various probing fields are cre-
ated for measuring the response of the image to these inputs. Due to physical restrictions,
these measurements are typically only possible at few discrete locations, often situated at
the boundary of the domain Ω, but sometimes also at a small number of points inside Ω. If
the underlying PDE is time-dependent, then these measurements can be time-dependent
functions. The corresponding measured data give information on the spatial distribution
of the subdomains and on the corresponding internal model parameters.
Sometimes the physical interpretation of the image is that of a source distribution rather
than a parameter distribution. Then, the image itself creates the probing field and needs to
be determined from just one set of measured data. Also combinations are possible where
some components of the (vector-valued) image describe source distributions and other
components describe parameter distributions. Initial conditions or boundary conditions
can also often be interpreted as images in this spirit, which need to be determined from
indirect data. It is clear that this concept of an image can be generalized even further, which
leads to interesting mathematical problems and concepts.
There is often a large variety of additional prior information available for determin-
ing the image, whose character depends on the given application. For example, it might
be known or assumed that all parameter profiles inside the individual subregions of a
domain Ω are constant with known or unknown region-specific values. In this particular
case, only the interfaces between the different regions, and possibly the unknown param-
eter values, need to be reconstructed from the gathered data, which, as a mathematical
problem, is much better posed [, ] than the task of estimating independent values at
each individual pixel or voxel from the same data set without additional prior informa-
tion on the image. However, in many realistic applications the image to be found is more
complicated, and even the combined available information is not sufficient or adequate
for completely and uniquely determining the underlying image. This becomes even worse
due to typically noisy or incomplete data, or due to model inaccuracies. Then, it needs
to be determined which information on the image is desired and which information can
reasonably be expected from the data, taking into account the available additional prior
information. Depending on the specific application, different viewpoints are typically taken
which yield different strategies for obtaining images which agree (in an application-specific
sense) with the given information. We will give some instructive examples further below.
Determining an image (or a set of possible images) from the measured data, in the
above described sense and by taking into account the available additional prior infor-
mation, is called here imaging or image reconstruction. In practice, images are often
represented in a computer and thereby need to be discretized somehow. The most popular
discretization model uses D pixels or D voxels for representing an image, even though
alternative models are possible. Often the underlying PDE also needs to be discretized on
some grid, which could be done by finite differences, finite volumes, finite elements, and
other techniques. The discretization for the image does not necessarily need to be identical
to the discretization used for solving the PDE, and sometimes different models are used
for discretizing the image and the PDE. However, in these cases some method needs to be
provided to map from one representation to the other. In a level set representation of an

Level Set Methods for Structural Inversion and Image Reconstruction 

image also the level set functions need to be discretized for being represented in a com-
puter. The above said then holds true also for the discretizations of the level set functions,
which could either follow the same model as the PDE and/or a pixel model for the image,
or follow a different pattern.
..
The Forward and the Inverse Problem
In this chapter it is supposed that data ˜g are given in the form
˜g = M ˜u,
(.)
where M denotes a linear measurement operator, and ˜u are the physical states created by
the sources q for probing the image. It is assumed that a physical model Λ(b) is given,
which incorporates the (possibly vector-valued) model parameter b and which is able
to (roughly) predict the probing physical states when being plugged into an appropri-
ate numerical simulator, provided the correct sources and physical parameters during the
measurement process were known. The forward operator A is defined as
A (b,q) = M Λ(b)−q.
(.)
As mentioned, Λ(b) is often described in form of some partial differential equation (PDE),
or alternatively, an integral equation (IE), and the individual coefficients of the model
parameter b appear at one or several places in this model as space-dependent coefficients.
In most applications, measurements are taken only at few locations of the domain, for
example at the boundary of the area of interest, from which the physical parameters b
or the source q (or both) need to be inferred in the whole domain. It is said that, with
respect to these unknowns, the measurements are indirect: They are taken not at the loca-
tions where the unknowns need to be determined, but indirectly by their overall impact on
the states (modeled by the underlying PDE or IE) probing the image, which are measured
only at few locations. The behavior of the states is modeled by the operator A in (> .).
If in A only b (but not q) is unknown, then the problem is an inverse parameter or inverse
scattering problem. If in A only q (but not b) is unknown, then the problem is an inverse
source problem. Given measured data ˜g, the “residual operators” R are correspondingly
given by
R(b,q) = A (b,q) −˜g.
(.)
Given the above definitions, an image is defined here as as a mapping
a : Ω →Rn,
where Ω is a bounded or unbounded region in Ror in Rand n is the number of compo-
nents of the (vector-valued) image. Each component function ak, k = , . . . , n, represents a
space-dependent physical characteristic of the domain Ω which can be probed by phys-
ical inspection. If it appears as a coefficient of a PDE (or IE), it is denoted ak = bk,
and if it appears as a source, it is denoted ak = qk. The exposition given in this chapter


Level Set Methods for Structural Inversion and Image Reconstruction
mainly focuses on the recovery of parameter distributions ak = bk, and addresses several
peculiarities related to those cases. However, the main concepts carry over without major
changes to inverse source problems, and also to some related formulations as for example
the reconstruction of boundary or initial conditions of PDEs.
.
Examples and Case Studies
Some illustrative examples and case studies are presented in the following, which will be
used further on in this chapter for demonstrating basic ideas and concepts on realistic and
practical situations.
..
Example : Microwave Breast Screening
> Figure -shows two-dimensional images from the application of microwave breast
screening. The images of size × pixels have been constructed synthetically based
on MRI images of the female breast. Three representative breast structures are displayed in
the three images of the left column, where the value at each pixel of the images represents
the physical parameter “static relative permittivity.”
A commonly accepted model for breast tissue is to roughly distinguish between skin,
fatty tissue, and fibroglandular tissue. In the images also a matching liquid is shown in
which the breast is immersed. Inside the breast, regions can be identified easily which cor-
respond to fibroglandular tissue (high static relative permittivity values) and fatty tissue
(low static relative permittivity values), separated by a more or less complicated interface.
On the right column, histograms are shown for the distributions of static relative permit-
tivity values inside the breast. In these histograms it becomes apparent that values for fatty
and fibroglandular tissue are clustered around two typical values, but with a broader range
of distribution. However, a clear identification of fatty and fibroglandular tissue cannot be
made easily for each pixel of the image based on just these values.
Nevertheless, during a reconstruction, and from anatomical reasoning, it does make
sense to assume a model where fatty and fibroglandular tissue occupy some subregions of
the breast where a sharp interface exists between these subregions. Finding these subre-
gions provides valuable information for the physician. Furthermore, it might be sufficient
for an overall evaluation of the situation to have a smoothly varying profile of tissue param-
eters reconstructed inside each of these subregions, allowing for the choice of a smoothly
varying profile of static relative permittivity values inside each region. In the same spirit,
from anatomical reasoning, it makes sense to assume a sharp interface (now of less compli-
cated behavior) separating the skin region from the fatty/fibroglandular tissue on the one
side and from the matching liquid on the other side. It might also be reasonable to assume
that the skin and the matching liquid have constant static permittivity values, which might
be known or not. If a tumor in its early stage of development is sought in this breast model,
it will occupy an additional region of small size (and either simple or complicated shape and

Level Set Methods for Structural Inversion and Image Reconstruction 

x [cm]
y [cm]
0
5
10
15
0
5
10
15
5
10
15
20
25
x [cm]
y [cm]
0
5
10
15
0
5
10
15
5
10
15
20
25
x [cm]
y [cm]
0
5
10
15
0
5
10
15
5
10
15
20
25
5
15
25
0
50
100
150
200
250
300
Permittivity value
Number of pixels
5
15
25
0
200
400
600
800
Permittivity value
Number of pixels
5
15
25
0
50
100
150
200
250
300
350
Permittivity value
Number of pixels
⊡Fig. -
Three images from microwave breast screening. The three images are synthetically
generated from MRI breast models. Left column: two-dimensional maps of the distribution
of the static permittivity єst inside the three breast models. Right column: the corresponding
histograms of values of єst in each map
topology) and might have constant but unknown static relative permittivity value inside
this region.
During a reconstruction for breast screening, this set of plausible assumptions provides
us with a complex mathematical breast model which incorporates this prior information


Level Set Methods for Structural Inversion and Image Reconstruction
and might yield an improved and more realistic image for the reconstructed breast (includ-
ing a better estimate of the tumor characteristics) than a regular pixel-based inversion
would be able to provide. This is so because it is assumed that the real breast follows roughly
the complicated model constructed above, and that this additional information is taken
into account in the inversion.
In this application, the underlying PDE is the system of time-harmonic Maxwell’s equa-
tions, or its D representative (describing so-called TM-waves), a Helmholtz equation. The
“static relative permittivity,” as mapped in
> Fig. -, represents one parameter entering
in the wavenumber of the Debye dispersion model. The electromagnetic fields are created
by specifically developed microwave antennas surrounding the breast, and the data are
gathered at different microwave antennas also located around the breast. For more details,
see [].
..
Example : History Matching in Petroleum
Engineering
> Figure -shows a synthetically created D image of a hydrocarbon reservoir during
the production process. Circles indicate injection wells, and crosses indicate production
wells. The physical parameter displayed in the image is the permeability, which affects fluid
flow in the reservoir. Physically, two lithofacies can be distinguished in this image, namely,
sandstone and shaly sandstone (further on simply called “shale”). The sandstone region has
permeability values roughly in the range –mDarcy, whereas shale has permeability
values more in the range –mDarcy. In petroleum engineering applications, the
parameters inside a given lithofacie sometimes follow an overall linear trend, which is the
case here inside the sandstone region. This information is often available from geological
evaluation of the terrain. As a rough approximation, inside this region, the permeability
distribution can be modeled mathematically as a smooth perturbation of a bilinear model.
Inside the shale region, no trend is observed or expected, and therefore the permeability
distribution is described as a smooth perturbation of a constant distribution (i.e., an overall
smoothly varying profile).
During a reconstruction, a possible model would be to reconstruct a reservoir image
from production data which consists of three different quantities: () the interface between
the sandstone and shale lithofacies, () the smooth perturbation of the constant profile
inside the shale region, and () the overall trend (i.e., the bilinear profile) inside the sand-
stone region, assuming that inside this sandstone region the smooth perturbation is small
compared to this dominant overall trend. In this application, the PDE is a system of equa-
tions modeling two-phase or three-phase fluid flow in a porous medium, of which the
relative permeability is one model parameter.
The “fields” (in a slightly generalized sense) are represented in this application by
pressure values and water-/oil-saturation values at each point inside the reservoir during
production, and are generated by injecting (under high pressure) water in the injection
wells and extracting (imposing lower pressure) water and oil from the production wells.

Level Set Methods for Structural Inversion and Image Reconstruction 

prod
prod
prod
prod
prod
prod
prod
prod
prod
inject
inject
inject
inject
0
100
200
300
400
500
600
0
100
200
300
400
500
600
200
300
400
500
600
700
800
900
1000
1100
1200
⊡Fig. -
An image from reservoir engineering. Shown is the permeability distribution of a ﬂuid ﬂow
model in a reservoir which consists of a sandstone lithofacie (values in the range of –
mDarcy) and a shaly sandstone lithofacie (values in the range of –mDarcy),
separated by a sharp interface. The sandstone region shows an overall linear trend in the
permeability distribution, whereas the shaly sandstone region does not show any clear
trend
The data are the injection and production rates of water and oil, respectively, and some-
times pressure values measured at injection and production wells over production time.
For more details, see [].
..
Example : Crack Detection
> Figure -shows an image of a disconnected crack embedded in a homogeneous
material. The cracks are represented in this simplified model as very thin regions of fixed
thickness. The physical parameter represented by the image is the conductivity distribu-
tion in the domain. Only two values can be assumed by this conductivity, one inside the
thin region (crack) and another one in the background. The background value is typically
known, and the value inside the crack might either be approximately known or it might
be an unknown of the inverse problem. The same holds true for the thickness of the crack,
which is assumed constant along the cracks, even though the correct thickness (the con-
stant) might become an unknown of the inverse problem as well. Here insulating cracks
are considered, where the conductivity is significantly lower than in the background. The
probing fields inside the domain are the electrostatic potentials which are produced by


Level Set Methods for Structural Inversion and Image Reconstruction
20
40
60
80
100
120
140
160
180
200
20
40
60
80
100
120
140
160
180
200
⊡Fig. -
An image from the application of crack detection. Three disconnected crack components
are embedded in a homogeneous background medium and need to be reconstructed from
electrostatic measurements at the region boundary. In the considered case of insulating
cracks, these components are modeled as thin shapes of ﬁxed thickness with a conductivity
value much lower than the background conductivity
applying voltages at various locations along the boundary of the domain, and the data are
the corresponding currents across the boundary at discrete positions.
This model can be considered as a special case of a binary medium where volumet-
ric inclusions are embedded in a homogeneous background. However, the fact that these
structures are very thin with fixed thickness requires some special treatment during the
shape evolution, which will be commented on further below. In this application, the under-
lying PDE is a second order elliptic equation modeling the distribution of electric potentials
in the domain for a set of given applied voltage patterns. For more details, see [].
.
Level Set Representation of Images with Interfaces
A complex image in the above sense needs a convenient mathematical representation
in order to be dealt with in a computational and mathematical framework. In this sec-
tion, several different approaches are listed which have been proposed in the literature for
describing images with interfaces by a level set technique. First, the most basic representa-
tion is given, which only considers binary media. Afterwards, various representations are
described which represent more complicated situations.

Level Set Methods for Structural Inversion and Image Reconstruction 

..
The Basic Level Set Formulation for Binary Media
In the shape inverse problem in its simplest form, the parameter distribution is described by
b(x) = {b(i)(x)
in
D
b(e)(x)
in
Ω/D
,
(.)
where D ⊂Ω is a subregion of Ω and where usually discontinuities in the parameters b
occur at the interface ∂D. In the basic level set representation for the shape D, a (sufficiently
smooth, i.e., for example Lipschitz continuous) level set function ϕ : Ω →R is introduced
and the shape D is described by
{ϕ(x) ≤
for all
x ∈D,
ϕ(x) > 
for all
x ∈Ω/D.
(.)
In other words, the parameter function b has the form
b(x) = {b(i)(x)
where
ϕ(x) ≤
b(e)(x)
where
ϕ(x) > .
(.)
Certainly, a unique representation of the image is possible by just knowing those points
where ϕ(x) has a change of sign (the so-called zero level set), and additionally knowing
the two interior profiles b(i)(x) and b(e)(x) inside those areas of Ω where they are active
(which are D and Ω/D, respectively). Often, however, it is more convenient to assume
that these functions are defined on larger sets which include the minimal sets mentioned
above. In this chapter, it is assumed that all functions are defined on the entire domain Ω,
by employing any convenient extensions from the above mentioned sets to the rest of Ω.
Again, it is clear that the above extensions are not unique, and that many possible repre-
sentations can then be found for a given image. Which one to choose depends on details of
the algorithm for constructing the image, on the available prior information, and possibly
on other criteria.
W\D
f (x,y)
x
y
z
D
D
⊡Fig. -
The basic level set representation of a shape D. Those points of the domain where the
describing level set function assumes negative values are “inside”the shape D described by
the level set function ϕ, those with positive values are “outside”it. The zero level set where
ϕ = represents the shape boundary


Level Set Methods for Structural Inversion and Image Reconstruction
For a sufficiently smooth level set function, the boundary of the shape D permits the
characterization
∂D = {x ∈Ω,
ϕ(x) = }.
(.)
This representation motivates the name zero level set for the boundary of the shape. In some
representations listed further below, however, level set functions are preferred which are
discontinuous across those sets where they change sign. Then, the boundary of the different
regions can be defined alternatively as
∂D = {x ∈Ω : for all ρ > we can find x,x∈Bρ(x)
(.)
with ϕ(x) > and ϕ(x) ≤}
where Bρ(x) = {x ∈Ω : ∣x −x∣< ρ}.
..
Level Set Formulations for Multivalued and
Structured Media
As mentioned already above, in many applications the binary model described in > Sect.
..is not sufficient and more complex image models need to be employed. Several means
have been discussed in the literature for generalizing the basic model to more complex
situations, some of them being listed in the following.
...
Diﬀerent Levels of a Single Smooth Level Set Function
A straightforward generalization of the technique described in > Sect. ..consists inl
using, in addition to the level set zero, additional level sets of a given smooth (e.g., Lipschitz
continuous) level set function in order to describe different regions of a given domain [].
For example, define
Γi = {x ∈Ω,
ϕ(x) = ci}
(.)
Di = {x ∈Ω,
ci+< ϕ(x) < ci},
(.)
where ci are prespecified values with ci+> ci for i = , . . ., i −, and with c= +∞,
ci = −∞. Then,
Ω =
i
⋃
i=
Di,
with
Di ∩Di′ = /
for i ≠i′.
(.)
A level set representation for the image b is then given as a tupel (b, . . . , bi, ϕ) which
satisfies
b(x) = bi(x) for ci+< ϕ(x) < ci.
(.)
It is clear that certain topological restrictions are imposed on the distribution of the regions
Di by this formulation. In particular, it favors certain nested structures. For more details,
see [].

Level Set Methods for Structural Inversion and Image Reconstruction 

...
Piecewise Constant Level Set Function
This model describes piecewise constant multiple phases of a domain by only one level set
function and has its origins in the application of image segmentation. A single level set
function is used which is only allowed to take a small number of different values, e.g.,
ϕ(x) = i
in Di,
for i = , . . ., i,
(.)
Ω =
i
⋃
i=
Di,
with
Di ∩Di′ = /
for
i ≠i′.
Introducing the set of basis functions γi
γi =

αi
i
∏
j=
j≠i
(ϕ −j)
with
αi =
i
∏
j=
j≠i
(i −j),
(.)
the parameter distribution b(x) is defined as
b =
i
∑
i=
biγi.
(.)
A level set representation for the image b is then given as a tupel (b, . . . , bi, ϕ) with
b(x) = bi
where ϕ(x) = i.
(.)
Numerical results using this model can be found, amongst others, in [, , , ].
...
Vector Level Set
In [] multiple phases are described by using one individual level set function for each of
these phases, i.e.,
Γi = {x ∈Ω,
ϕi(x) = }
(.)
Di = {x ∈Ω,
ϕi(x) ≤},
(.)
for sufficiently smooth level set functions ϕi, i = , . . . , i. In this model, the level set
representation for the image b is given by a tupel (b, . . . , bi, ϕ, . . ., ϕi) which satisfies
b(x) = bk(x) where ϕk(x) ≤.
(.)
Care needs to be taken here that different phases do not overlap, which is not automatically
incorporated in the model. For more details on how to address this and other related issues
see [].


Level Set Methods for Structural Inversion and Image Reconstruction
...
Color Level Set
An alternative way of describing different phases by more than one level set functions has
been introduced in [] in the framework of image segmentation and further investigated
by [, , , , ] in the framework of inverse problems. In this model (which also is
known as the Chan-Vese model), up to n different phases can be represented by n different
level set functions by distinguishing all possible sign combinations for these functions. For
example, a level set representation for an image b containing up to four different phases is
given by the tupel (b, b, b, b, ϕ, ϕ) which satisfies
b(x) = b(−H(ϕ))(−H(ϕ)) + b(−H(ϕ))H(ϕ)
(.)
+ bH(ϕ)(−H(ϕ)) + bH(ϕ)H(ϕ).
Also here, the contrast values b ,
= , . . . ,are allowed to be smoothly varying functions
inside each region. The four different regions are then given by
D= {x,
ϕ≤
and
ϕ≤}
(.)
D= {x,
ϕ≤
and
ϕ> }
D= {x,
ϕ> 
and
ϕ≤}
D= {x,
ϕ> 
and
ϕ> }.
This yields a complete covering of the domain Ω by the four regions, each point x ∈Ω
being part of exactly one of the four shapes D , see > Fig. -.
D4
φ2 > 0
φ1 > 0
φ2 > 0
φ1 £ 0
φ2 £ 0
φ1 > 0
φ2 £ 0
φ1 £ 0
D3
D1
D2
⊡Fig. -
Color level set representation of multiple shapes. Each region is characterized by a diﬀerent
sign combination of the two describing level set functions

Level Set Methods for Structural Inversion and Image Reconstruction 

...
Binary Color Level Set
An alternative technique for using more than one level set function for describing multiple
phases, which is, in a certain sense, a combination of the piecewise constant level set model
described in > Sect. ...and the color level set technique described in > Sect. ...,
has been proposed in [] for the application of Mumford-Shah image segmentation. For
the description of up to four phases by two (now piecewise constant) level set functions ϕ
and ϕ, in this binary level set model the two level set functions are required to satisfy
ϕi ∈{−, },
or
ϕ
i = ,
i ∈{,}.
(.)
The parameter function b(x) is given by
b(x) = 
(b(ϕ−)(ϕ−) −b(ϕ−)(ϕ+ )
(.)
−b(ϕ+ ))(ϕ−) + b(ϕ+ )(ϕ+ )),
and the four different regions are encoded as
D= {x,
ϕ= −
and
ϕ= −}
(.)
D= {x,
ϕ= −
and
ϕ= +}
D= {x,
ϕ= +
and
ϕ= −}
D= {x,
ϕ= +
and
ϕ= +}.
A level set representation for an image b containing up to four different phases is given by
the tupel (b, b, b, b, ϕ, ϕ) which satisfies (> .). For more details we refer to [].
..
Level Set Formulations for Speciﬁc Applications
Often, for specific applications it is convenient to develop particular modifications or gen-
eralizations of the above described general approaches for describing multiple regions
by taking into account assumptions and prior information which are very specific to the
particular application. A few examples are given below.
...
A Modiﬁcation of Color Level Set for Tumor Detection
In the application of tumor detection from microwave data for breast screening (see
> Sect. ..), the following situation needs to be modeled. The breast consists of four
possible tissue types, namely, skin, fibroglandular tissue, fatty tissue, and a possible tumor.
Each of these tissue types might have an internal structure, which is (together with the
mutual interfaces) one unknown of the inverse problem. In principle, the color level set
description using two level set functions for describing four different phases would be
sufficient for modeling this situation. However, the reconstruction algorithm as presented
in [] requires some flexibility with handling these four regions separately, which is dif-
ficult in this minimal representation of four regions. Therefore, in [], the following


Level Set Methods for Structural Inversion and Image Reconstruction
modified version of the general representation of color level sets is proposed for model-
ing this situation. In this modified version, m different phases (here m = ) are described
by n = m −level set functions in the following form
b(x) = b(−H(ϕ)) + H(ϕ)[b(−H(ϕ))
(.)
+ H(ϕ){b(−H(ϕ)) + bH(ϕ)}]
or
D= {x,
ϕ≤}
(.)
D= {x,
ϕ> 
and
ϕ≤}
D= {x,
ϕ> 
and
ϕ> 
and
ϕ≤}
D= {x,
ϕ> 
and
ϕ> 
and
ϕ> },
where b, b, b, and bdenote the dielectric parameters of skin, tumorous, fibroglandu-
lar, and fatty tissue, respectively. In (> .), ϕ, ϕ, and ϕare the three different level
set functions indicating the regions filled with skin, tumorous, and fibroglandular tissue,
respectively, and the contrast values b ,
= , . . . ,are generally allowed to be smoothlyh
varying functions inside each region. This combination of m −level set functions for
describing m different phases has certain advantages with respect to the standard color
level set formulation during the reconstruction process, as it is pointed out in []. On the
other hand, it is obvious that (> .) can be considered a special case of the color level set
technique (> Sect. ...) where the theoretically possible = different values of the
color level set description are enforced to fall into m = different groups of characteristic
values, see the central image of > Fig. -.
...
A Modiﬁcation of Color Level Set for Reservoir
Characterization
Another modification of the color level set technique has been used in [] for the applica-
tion of history matching in reservoir engineering, see > Sect. ... Given, as an example,
n = level set functions ϕ, . . . , ϕ, we define the parameter (permeability) distribution
inside the reservoir by
b = b(−H(ϕ))H(ϕ)H(ϕ) + bH(ϕ)(−H(ϕ))H(ϕ)
+ bH(ϕ)H(ϕ)(−H(ϕ)) + bH(ϕ)H(ϕ)H(ϕ)
+ b+ b

H(ϕ)(−H(ϕ))(−H(ϕ))
+ b+ b

(−H(ϕ))H(ϕ)(−H(ϕ))
+ b+ b

(−H(ϕ))(−H(ϕ))H(ϕ)
+ b+ b+ b

(−H(ϕ))(−H(ϕ))(−H(ϕ)),
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

<_0
φ1
<_0
φ2
0
φ3
D1
D2
D3
D4
D5
D6
D7
D8
<_0
φ1
<_0
φ2
<_0
φ3
D1
D1
D1
D1
D2
D2
D3
D4
<_0
φ1
<_0
φ2
<_0
φ3
D1
D2
D3
D12
D13
D23
D123
D4
⊡Fig. -
Multiple level set representation for modeling multiphase inverse problems. Left: original
color level set technique for describing eight diﬀerent phases by the diﬀerent sign
combinations of three level set functions. Center: Modiﬁed color level set technique used in
the model for early detection of breast cancer from microwave data. The possible eight
regions of the color level set presentation are ﬁlled with four diﬀerent materials in a
tailor-made fashion for this application. Right: Modiﬁed color level set technique for
modeling the history matching problem of a water-ﬂooding process in a petroleum
reservoir. Also here the eight diﬀerent regions are ﬁlled by a speciﬁc combination of
materials characteristic for the reconstruction scheme used in this application. Regions with
more than one subindex correspond to “characteristic regions”with averaged parameter
values
where the permeability values b ,
= , . . . ,are assumed constant inside each region.
The four lithofacies are represented as
D= {x,
ϕ≤
and
ϕ> 
and
ϕ> }
(.)
D= {x,
ϕ≤
and
ϕ> 
and
ϕ> }
D= {x,
ϕ≤
and
ϕ> 
and
ϕ> }
D= {x,
ϕ> 
and
ϕ> 
and
ϕ> }.
Let in the following n = be the number of lithofacies. In this model, a point in the
reservoir corresponds to the lithofacie Dl, (l = , . . ., n −) if ϕl has negative sign and all
the other level set functions have positive sign. In addition, one lithofacie (which here is
referred to as the “background” lithofacie with index l = n) corresponds to those points
where none of the level set functions has a negative sign. Notice that typically this defini-
tion does not yield a complete covering of the whole domain Ω by the four (n) lithofacies,
see the right image of > Fig. -. Those regions inside the domain where more than one
level set function are negative are recognized as so-called “critical regions” and are intro-
duced for providing a smooth evolution from the initial guess to the final reconstruction.
Inside these critical regions the permeability assumes values which are calculated as cer-
tain averages over values of the neighboring non-critical regions. They are indicated in
the right image of
> Fig. -by using multiple subindices indicating which non-critical


Level Set Methods for Structural Inversion and Image Reconstruction
regions contribute to this averaging procedure. For more details regarding this model, and
numerical experiments for the application of reservoir characterization, see [].
...
A Modiﬁcation of the Classical Level Set Technique for
Describing Cracks or Thin Shapes
Cracks of finite thickness can be modeled by using two level set functions in a setup which
amounts to a modification of the classical level set technique for binary media. For simplic-
ity, assume that a crack or thin region of finite thickness is embedded in a homogeneous
background. The classical level set technique described in > Sect. ..captures this sit-
uation in principle, since the crack can be interpreted as a simple shape (with possibly
complicated topology) embedded in a homogeneous background. However, when it comes
to shape evolution for such a crack-like structure, it is difficult to maintain a fixed thickness
of the thin shape following the classical shape evolution scheme. This is so since the clas-
sical shape evolution applies an individually calculated velocity field value in the normal
direction at each point of the entire shape boundary, such that the thickness of the thin
region will not be maintained. For crack evolution, the deformations of adjacent boundary
points need to be coordinated in order to maintain the thickness of the crack during the
entire shape evolution, see > Fig. -.
<_ 0
φ2
<_ 0
φ2
<_ 0,
φ1
0
φ2 >
0
φ1 >
0
φ2 >
crack
crack
⊡Fig. -
Multiple level set representation for modeling a disconnected crack. The zero level set of the
ﬁrst level set function deﬁnes the potential outline of the crack, of which the second level
set function selects those parts that are actually occupied by the crack. The “ideal”crack has
vanishing thickness, whereas the “real”crack modeled by the level set technique has a small
ﬁnite thickness and is practically obtained by a narrow band technique

Level Set Methods for Structural Inversion and Image Reconstruction 

A modified version of the classical level set technique has been proposed in [, ]
which uses two level set functions for modeling crack propagation and crack reconstruc-
tion in this sense. Here, a small neighborhood (narrowband) of the zero level set of the
first level set function defines the general outline of the crack, whereas the second level set
function selects those parts of this band structure which in fact contribute to the possibly
disconnected crack topology.
In more details, given a continuously differentiable level set function ϕand its zero
level set
Γϕ= {x ∈Ω : ϕ(x) = }.
(.)
The normal n to Γϕis given by (> .) and is pointing into the direction where ϕ(x) ≥.
An (connected or disconnected) “ideal” (i.e., of thickness zero) crack with finite length
completely contained inside Ω is constructed by introducing a second level set function
ϕwhich selects one or more parts from the line Γϕ, see > Fig. -. This second level set
function defines the region
B = {x ∈Ω : ϕ(x) ≤}.
(.)
The ‘ideal’ crack is then defined as a collection of finite sub-intervals of Γϕ
S[ϕ, ϕ] = Γϕ∩B.
(.)
An ideal “insulating” crack S (of thickness zero) is then supplemented with a vanishing
electrical current condition across this set S[ϕ, ϕ]. However, in the simplified level set
model, not the ideal crack is considered, but cracks of finite thickness δ > with known
conductivity bi inside the crack and be outside it. Moreover, in the insulating case, it is
assumed that bi << be. In this model, a small neighborhood of Γϕis introduced as
Γδ
ϕ= {y ∈Ω : y = x −τn(x), ∣τ∣< δ, x ∈Γϕ},
(.)
and the above defined “ideal crack” S is associated now with a “real crack” counterpart
Sδ = Γδ
ϕ∩B.
(.)
The conductivity distribution is
b(x) = {bi
for
x ∈Sδ
be
otherwise
(.)
in the domain Ω. Certainly, the real crack can also alternatively be defined by
˜Sδ = {y ∈Ω : y = x −τn(x), ∣τ∣< δ, x ∈S},
(.)
which would slightly change the shape of the crack at the crack tips. Here, the form
(> .) is preferred. For the numerical treatment see [, ].


Level Set Methods for Structural Inversion and Image Reconstruction
.
Cost Functionals and Shape Evolution
One important technique for creating images with interfaces satisfying certain criteria is
shape evolution, more specifically, interface and profile evolution. The general goal is to start
with a set of shapes and profiles as initial guess, and then let both, shapes and profiles, evolve
due to some appropriate evolution laws in order to improve the initial guess with increas-
ing artificial evolution time. The focus in the following will be on shape evolution, since
evolution laws for interior profiles fairly much follow classical and well-known concepts.
Evolution of a shape or an interface can be achieved either by defining a velocity field on
the domain Ω which deforms the boundaries of this shape, or by defining evolution laws
directly for the level set functions representing the shape. Some of these techniques will be
presented next.
..
General Considerations
In many applications, images need to be evaluated for verifying their usefulness or merit for
a given situation. This evaluation is usually based on a number of criteria, amongst them
being the ability of the image (in its correct interpretation) to reproduce the physically
measured data (its data fitness). Other criteria include the consistence with any additionally
available prior knowledge on the given situation, or the closeness of the image to a set of
reference images. In many cases, some form of merit function (often in terms of a properly
defined cost functional) is defined whose value is intended to indicate the usefulness of the
image in a given application. However, sometimes this decision is done based on visual
inspection only.
In general, during this evaluation process, a family of images is created and the merit
of each of these images is assessed. Then, one or more of these images are selected. Let
(b(), . . ., b(i), ϕ(), . . . , ϕ(j)) be a level set representation for the class of images to be
considered. Then, creating this family of images can be described either in a continuous
way by an artificial time evolution
(b()(t), . . ., b(i)(t), ϕ()(t), . . ., ϕ(j)(t)),
t ∈[, tmax],
with an artificial evolution time t, or in a discrete way
(b()
k , . . . , b(i)
k , ϕ()
k , . . . , ϕ
(j)
k ),
k = , . . ., k,
with a counting index k. Usually these images are created in a sequential manner, using
evolution laws
d
dt (b()(t), . . ., b(i)(t), ϕ()(t), . . ., ϕ(j)(t)) = f (t),
with a multi-component forcing term f (t), or update formulas
(b()
k+, . . ., b(i)
k+, ϕ()
k+, . . . , ϕ
(j)
k+) = Fk (b()
k , . . . , b(i)
k , ϕ()
k , . . . , ϕ
(j)
k )

Level Set Methods for Structural Inversion and Image Reconstruction 

with update operators Fk. These evolution laws and update operators can also be defined
on ensembles of images, which allows for statistical evaluation of each ensemble during
the evaluation process. Any arbitrarily defined evolution law and set of update operators
yield a family of images which can be evaluated, but typically those are preferred which
point into a descent direction of some pre-defined cost functional. Some choices of such
cost functionals will be discussed in the following.
..
Cost Functionals
In general, a cost functional can consist of various components, typically combined in
an additive or multiplicative manner. Popular components for an image model b =
(b(), . . . , b(i)) and ϕ = (ϕ(), . . . , ϕ(j)) are:
()
Data misfit terms Jdata(b, ϕ)
()
Terms measuring closeness to a prior model inside each subdomain Jprior(b, ϕ)
()
Terms enforcing geometric constraints on the interfaces Jgeom(b, ϕ)
In (), the by far most popular data misfit term is the least squares misfit cost functional
which, in general, is given as an expression of the form
Jdata(b, ϕ) = 
∥A (b, ϕ) −˜g∥

= 
∥uM[b, ϕ] −˜g∥

,
(.)
where A (b, ϕ) is the forward operator defined in (> .) and uM[b, ϕ] indicates the sim-
ulated data at the set of probing locations M for this guess. Other choices can be considered
as well, see for example [].
() corresponds to classical regularization techniques, applied to each subdomain, and
is treated in many textbooks, such as [, ]. Therefore, it is not discussed in this chapter.
() has a long history in the shape optimization literature and in image processing
applications. See, for example, [, ]. A few concepts are presented in Sect. ..
..
Transformations and Velocity Flows
The first technique discussed here is shape evolution by transformations and velocity flows.
This concept has been inspired by applications in continuum mechanics. Given a (possibly
bounded) domain Ω ⊂Rn and a shape D ⊂Ω with boundary ∂D which, as usual, is
denoted as Γ. Let a smooth vector field v : Rn →Rn be given with v⋅n = on ∂Ω. A family
of transformations St proceeds by
St(X) = X + tv(X)
(.)
for all X ∈Ω. In short, St = I + tv where I stands for the identity map. This defines for
each point (‘particle’) X in the domain, a propagation law prescribed by the ordinary


Level Set Methods for Structural Inversion and Image Reconstruction
differential equation
˙x(t,X) = V(t,x(t,X)),
(.)
x(,X) = X
(.)
with the specific velocity choice
V(t,x(t,X)) = v(X).
(.)
Physically, it corresponds to the situation where each point X of the domain travels with
constant speed along a straight line which is defined by its initial velocity vector v(X).
Notice that the definition (> .) can with (> .) also be written in a slightly more
abstract fashion as
V(t,x) = ∂
∂t St(X) = ( ∂
∂t St) ○S−(x).
(.)
In fact, it turns out that the ideas in the above example can be considerably generalized
from the specific case (> .) to quite arbitrary smooth vector fields V(t,x) describ-
ing smooth families of transformations Tt(X). The generating vector field V(t,x) is often
called ‘velocity field’. It can be done as follows.
Let us given an arbitrary smooth family of transformations Tt(X) which maps every
point X of the domain to the point x(t,X) = Tt(X) at time t. The propagation of the point
X over time t is again described by the ordinary differential > Eqs. (.) and (> .)
where the velocity V is defined by
V(t,x) = ( ∂
∂t Tt) ○T−(x).
(.)
Now the propagation of points is not restricted anymore to straight lines, but can be
quite arbitrary. Vice versa, given a smooth vector field V(t,x), it gives rise to a family of
transformations Tt(X) via the differential > Eqs. (.) and (> .) where every point
X ∈Ω is mapped by Tt(X) to the solution x(t,X) of (> .), (> .) at time t, i.e.,
Tt(X)(x) = x(t,X). For more details on this duality of transformations and velocity flows
see the well-known monographs [, ].
Notice that the numerical treatment of such a velocity flow in the level set framework
leads to a Hamilton–Jacobi-type equation. Some remarks regarding this link are given in
> Sect. ...
..
Eulerian Derivatives of Shape Functionals
Given the framework defined in > Sect. .., the goal is now to define transformations
and velocity flows which point into a descent direction for a given cost functional. Some
useful concepts on how to obtain such descent directions are discussed here.
Let D = Dbe a shape embedded in the domain at time t = . When the points in
the said domain start moving under the propagation laws discussed above, the interior
points of the shape, the boundary points, as well as the exterior points will move as well,

Level Set Methods for Structural Inversion and Image Reconstruction 

and therefore the shape will deform. Denote the shape at time t by Dt = Tt(D) where
as before Tt is the family of transformations which correspond to a given velocity field
V(t,x). Assume furthermore that a cost functional J (x, t, Dt, . . .) is given which depends
(amongst others) upon the current shape Dt. Deformation of shape will entail change of
this cost. The so-called shape sensitivity analysis of structural optimization aims at quanti-
fying these changes in the cost due to a given velocity flow (or family of tranformations)
in order to determine suitable descent flows.
Given a vector field V(t,x), the Eulerian derivative of the cost functional J (Dt) at
time t = in the direction V is defined as the limit
dJ (D,V) = lim
t↓
J (Dt) −J (D)
t
,
(.)
if this limit exists. The functional J (Dt) is shape differentiable (or simply differentiable) if
the Eulerian derivative dJ (D,V) exists for all directions V and furthermore the mapping
V →dJ (D,V) is linear and continuous (in appropriate function spaces). It is shown in
[, ] that, if J (D) is shape-differentiable, there exists a distribution G(D) which is
concentrated (supported) on Γ = ∂D such that
dJ (D,V) = ⟨G(D), V()⟩.
(.)
This distribution G is the shape gradient of J in D, which is a vector distribution. More
specifically, let ϖΓ denote the trace (or restriction) operator on the boundary Γ. Then, the
Hadamard-Zolésio structure theorem states that (under certain conditions) there exists a
scalar distribution g such that the shape gradient G writes in the form G = ϖ∗
Γ(gn), where
ϖ∗is the transpose of the trace operator at Γ and where n is the normal to Γ. For more
details see again [, ].
..
The Material Derivative Method
A useful concept for calculating Eulerian derivatives for cost functionals is the so-called
material and shape derivative of states u. In the application of inverse problems, these states
u typically are the solutions of the PDEs (IEs) which model the probing fields and which
depend one way or another on the shape D.
Let as before V be a smooth vector field with ⟨V,n⟩= on ∂Ω, and let Tt(V) denote
the corresponding family of transformations. Moreover, let u = u[Dt] be a state function
(of some Sobolev space) which depends on the shape Dt ⊂Ω (denote as before D= D).
The material derivative ˙u[D,V] of u in the direction V is defined as
˙u[D,V] = lim
t↓
u[Dt] ○Tt(V) −u[D]
t
,
(.)
or
˙u[D,V](X) = lim
t↓
u[Dt](Tt(X)) −u[D](X)
t
for X ∈Ω,
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
where the square brackets in the notation indicate the dependence of the states and deriva-
tives on the shape Dt and/or on the vector field V. The material derivative corresponds to
a Lagrangean point of view describing the evolution of the points in a moving coordinate
system, e.g., located in the point x(t,X) = Tt(X).
The shape derivative u′[D,V] of u in the direction V in contrast corresponds to an
Eulerian point of view observing the evolution from a fixed coordinate system, e.g., located
in the point X. It is defined as
u′[D,V] = lim
t↓
u[Dt] −u[D]
t
,
(.)
or
u′[D,V](X) = lim
t↓
u[Dt](X) −u[D](X)
t
for X ∈Ω .
(.)
The shape derivative and the material derivative are closely related to each other. It can be
shown that
u′[D,V] = ˙u[D,V] −∇(u[D]) ⋅V()
(.)
provided that these quantities exist and are well-defined. Subtracting ∇(u[D]) ⋅V()
in (> .) from the material derivative makes sure that the shape derivative actually
becomes zero in the special case that the states u do not depend on the shape D. The
material derivative usually does not vanish in these situations.
..
Some Useful Shape Functionals
To become more specific, some useful examples for shape functionals which have been
applied to shape inverse problems are provided herein.
. Define for a given function ζ the shape integral
J(D) = ∫Ω χD(x)ζ(x)dx = ∫D ζ(x)dx
(.)
where χD is the characteristic function for the domain D. Then the Eulerian derivative
is given by
dJ(D,V) = ∫D div (ζV())dx = ∫Γ ζ⟨V(),n⟩RndΓ.
(.)
. Consider the shape functional
J(D) = ∫Γ ζ(x)dΓ
(.)
for a sufficiently smooth function ζ defined on Ω such that the traces on Γ exist and
are integrable. The tangential divergence divΓV of the vector field V at the boundary Γ
is defined as
divΓV = (divV −⟨DV ⋅n, n⟩)∣Γ
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

where DV denotes the Jacobian of V. Then,
dJ(D,V) = ∫Γ (⟨∇ζ , V()⟩+ ζdivΓV()) dΓ
(.)
Be N an extension of the normal vector field n on Γ to a local neighborhood of Γ. Then,
the mean curvature κ of Γ is defined as κ = divΓN ∣Γ. With that, dJ(D,V) admits the
alternative representation
dJ(D,V) = ∫Γ ( ∂ζ
∂n + ζκ) ⟨V(),n⟩dΓ
(.)
. A useful link between the shape derivative and the Eulerian derivative of the cost
functional is
J(D) = ∫D u[D]dx
(.)
which depends via the states u[D] on the shape D. Furthermore [, ]
dJ(D,V) = ∫D u′[D,V]dx + ∫Γ u[D]⟨V(), n⟩Rn dΓ .
(.)
. Consider a cost functional
J(D) = ∫Γ ζ(Γ)dΓ
(.)
where ζ is only defined at the shape boundary Γ. Then we cannot use the characteri-
zation (> .) directly, since ∇(ζ) ⋅V() is not well-defined. In that case, the shape
derivative is defined as
ζ′[Γ,V] = ˙ζ[Γ,V] −∇Γ(ζ[Γ]) ⋅V(),
(.)
∇Γ being the gradient along the boundary Γ of the shape (chosen such that ∇ζ = ∇Γζ +
∂ζ
∂n n whenever all these quantities are well-defined). Then, the Eulerian derivative of the
cost functional J(D) can be characterized as
dJ(D,V) = ∫Γ ζ′[Γ,V]dΓ + ∫Γ κζ⟨V(), n⟩Rn dΓ
(.)
where again κ denotes the mean curvature on Γ.
..
The Level Set Framework for Shape Evolution
So far, shape evolution has been discussed independently of its representation by a level
set technique. Any of the above mentioned shape evolutions can practically be described
by employing a level set representation of the shapes.
First, some convenient representations of geometric quantities in the level set frame-
work are listed:
. The outward normal direction [, ] is given by
n(x) = ∇ϕ
∣∇ϕ∣.
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
. The local curvature κ(x) of ∂D, being the divergence of the normal field n(x), is
κ(x) = ∇⋅n(x) = ∇⋅( ∇ϕ
∣∇ϕ∣).
(.)
. The following relation is often useful
δ(ϕ) = δ∂D(x)
∣∇ϕ(x)∣
(.)
where δ∂D is the n-dimensional Dirac delta distribution concentrated on ∂D.
Notice that the right hand sides of (> .) and (> .) make sense at every point of
the domain Ω where the level set function ϕ is sufficiently smooth, giving rise to a natural
extension of these quantities from the boundary ∂D to a local neighborhood.
Assume now that a sufficiently smooth flow field V(x, t) is given, and that a shape D
is represented by the continuously differentiable level set function ϕ with ∣∇ϕ∣≠at the
boundary of the shape. Then, the deformation of the shape due to the flow field V(x, t) in
the level set framework can be obtained as follows.
Since the velocity fields are assumed to be sufficiently smooth, a boundary point x
remains at the boundary of ∂D(t) during the evolution of the shape. Let ϕ(x, t) be the set
of level set functions describing the shape at every time of the evolution. Differentiating
ϕ(x, t) = with respect to t yields
∂ϕ
∂t + ∇ϕ ⋅dx
dt = .
(.)
Identifying V(x, t) to dx
dt and using (> .), one arrives at
∂ϕ
∂t + ∣∇ϕ∣V(x, t) ⋅n(x, t) = .
(.)
Defining the normal velocity as
F(x, t) = V(x, t) ⋅n(x, t)
(.)
the Hamilton–Jacobi-type equation for describing the evolution of the level set function
follows as
∂ϕ
∂t + F(x, t) ⋅∣∇ϕ∣= .
(.)
.
Shape Evolution Driven by Geometric Constraints
It is possible to define a shape evolution without any data misfit functional being involved.
This type of shape evolution often occurs in applications of image processing or compu-
tational physics. For example, starting from an initial shape, the goal might be to define a
shape evolution which aims at reducing the cost of the image with respect to one or more
geometric quantities, typically encoded in some geometric cost functional. Based on the

Level Set Methods for Structural Inversion and Image Reconstruction 

theory developed in Sect. ., some useful expressions will be derived here for calculat-
ing such descent directions. The then obtained geometrically driven shape evolutions can
also be used for adding additional constraints or regularization during the shape evolution
driven by data misfit, if desired. This is achieved practically by adding appropriate geomet-
rical cost terms to the data misfit term and calculating descent directions for this combined
cost.
..
Penalizing Total Length of Boundaries
Assume that Γ = ∂D is a smooth submanifold in Ω. The total length (or surface) of Γ is
defined as
JlenΓ(D) = ∫Γ dΓ = ∫Ω δ∂D(x) dx.
(.)
Applying a flow by a smooth vector field V(x, t), > Eq. (.) yields with ζ = an
expression for the corresponding change in the cost (> .) which is
dJlenΓ(D,V) = ∫Γ κ ⟨V(),n⟩dΓ.
(.)
If the shape D is represented by a continuously differentiable level set function ϕ, an
alternative derivation can be given. First, using (> .), write (> .) in the form
JlenΓ(D(ϕ)) = ∫Ω δ(ϕ)∣∇ϕ(x)∣dx.
(.)
Perturbing now ϕ →ϕ+ψ, formal calculation (see, e.g., []) yields that the cost functional
is perturbed by
⟨∂JlenΓ
∂ϕ
, ψ⟩= ∫Ω δ(ϕ)ψ(x)∇⋅∇ϕ
∣∇ϕ∣dx.
(.)
Therefore, using (> .), it can be identified
∂JlenΓ
∂ϕ
= δ(ϕ)∇⋅∇ϕ
∣∇ϕ∣= δ(ϕ)κ
(.)
where κ is now an extension (defined, e.g., by (> .)) of the local curvature to a small
neighborhood of Γ. For both representations (> .) and (> .), minimizing the cost
by a gradient method leads to curvature driven flow equations, which is V() = −κn.
This curvature-dependent velocity has been widely used to regularize the computation of
motion of fronts via the level set method [], as well in the field of image processing
[], and has been introduced also recently for regularizing inverse problems, see, e.g.,
[, , ].
Two popular concepts related to the above shape evolution are the Mumford-Shah
and the Total-Variation functionals, which are frequently employed in image segmentation
applications. This relationship is briefly described in the following.
The popular Mumford-Shah functional for image segmentation [] contains, in addi-
tion to a fidelity term inside each region of the segmented image, a term which encourages


Level Set Methods for Structural Inversion and Image Reconstruction
to shorten total curve-length of the interfaces. This latter term can be written for piecewise
constant binary media (see > Sect. ..with constant profiles in each region) as
JMS = ∫Ω ∣∇H(ϕ)∣dx.
(.)
Taking into account that ∇H(ϕ) = H
′(ϕ)∇ϕ = δ(ϕ)∣∇ϕ∣n it is seen that JMS =
JlenΓ(D(ϕ)) as given in (> .), which again yields the curvature driven flow
> Eq. (.). For more details see [, , ].
The total variation (TV) functional, on the other hand, can be written, again for the
situation of piecewise constant binary media, as
JTV = ∫Ω ∣∇b(ϕ)∣dx = ∣be −bi∣∫Ω ∣∇H(ϕ)∣dx.
(.)
Therefore, it coincides with the Mumford-Shah functional JMS up to the factor ∣be −bi∣.
Roughly it can be said that the TV functional (> .) penalizes the product of the jump
between different regions and the arc length of their interfaces, whereas the Mumford-Shah
functional (> .) penalizes only this arc length. Refer for more information to [, ].
..
Penalizing Volume or Area of Shape
It is again assumed that Γ = ∂D is a smooth submanifold in Ω. Define the total area
(volume) of D as
JvolD(D) = ∫D dx = ∫Ω χD(x) dx,
(.)
where the characteristic function χD : Ω →{,} for a given shape D is defined as
χD(x) = { ,
x ∈D
,
x ∈Ω/D.
(.)
Applying a flow by a smooth vector field V(x, t), > Eqs. (.) and (> .) yield with
ζ = 
dJvolD(D,V) = ∫D divV()dx = ∫Γ⟨V(),n⟩dΓ.
(.)
Again, if the shape D is represented by a continuously differentiable level set function ϕ,
an alternative derivation can be given. First, using the Heaviside function H, let us write
(> .) in the form
JvolD(D) = ∫Ω H(ϕ) dx.
(.)
Perturbing as before ϕ →ϕ + ψ it follows
⟨∂JvolD
∂ϕ
, ψ⟩= ∫Ω δ(ϕ)ψ(x) dx
(.)
such that
∂JvolD
∂ϕ
= δ(ϕ).
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

In both formulations, a descent flow is given by a motion with constant speed in the
negative direction of the normal n to the boundary Γ, which is V() = −n.
.
Shape Evolution Driven by Data Misﬁt
An essential goal in the solution of inverse problems is to find an image which is able to
reproduce the measured data in a certain sense. As far as interfaces are concerned, this
gives rise to the need of finding descent directions for shape evolution with respect to
the data misfit functional. In the following, some concepts are presented which aim at
providing these descent directions during the shape evolution. These concepts can be com-
bined arbitrarily with the above discussed concepts for shape evolution driven by geometric
terms.
..
Shape Deformation by Calculus of Variations
Historically, the first approach for applying a level set technique for solving an inverse
problem in [] has used concepts from the calculus of variations for calculating descent
directions for the data misfit functional. In many applications, this approach is still a very
convenient way of deriving evolution laws for shapes. In the following, the main ideas of
this approach are briefly reviewed, following []. The goal is to obtain expressions for the
deformation of already existing shapes according to a normal velocity field defined at the
boundary of these shapes. Topological changes are not formally included in the considera-
tion at this stage (even though they occur automatically when implementing the discussed
schemes in a level set based numerical framework). The formal treatment of topological
changes is a topic of active current research and will be discussed briefly in > Sect. ...
...
Least Squares Cost Functionals and Gradient Directions
Typically, appropriate function spaces are needed for defining and calculating appropriate
descent directions with respect to the data misfit cost functional. Without being very spe-
cific, in the following, the general notation P is used for denoting the space of parameters
b, and, if not otherwise specified, Z for denoting the space of measurements ˜g. For sim-
plicity, these function spaces are considered being appropriately chosen Hilbert or vector
spaces. Certainly, other types of spaces can be used as well, which might lead to interesting
variants of the described concepts.
Consider now the least squares cost functional
J (b) = 
∥R(b)∥
Z = 
⟨R(b), R(b)⟩Z ,
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
where ⟨, ⟩Z denotes the canonical inner product in data space Z. Assume that R(b) admits
the expansion
R(b + δb) = R(b) + R′(b)δb + O (∥δb∥
P),
(.)
letting ∥∥P be the canonical norm in parameter space P, for a sufficiently small perturba-
tion (variation) δb ∈P. The linear operator R′(b) (if it exists) is often called the Fréchet
derivative of R. Plugging (> .) into (> .) yields the relationship
J (b + δb) = J (b) + Re⟨R′(b)∗R(b), δb⟩P + O (∥δb∥
P)
(.)
where the symbol Re indicates the real part of the corresponding quantity. The operator
R′(b)∗is the formal adjoint operator of R′(b) with respect to spaces Z and P:
⟨R′(b)∗g , ˆb⟩P = ⟨g , R′(b)ˆb⟩Z
for all ˆb ∈P, g ∈Z.
(.)
The quantity
gradbJ = R′(b)∗R(b)
(.)
is called the gradient direction of J in b.
It is assumed that the operators R′(b) and R′(b)∗take into account the correct inter-
face conditions at ∂D, which is important when actually evaluating these derivatives in a
“direct” or in an “adjoint” fashion. In many practical applications the situation can occur
that (formally) the fields need to be evaluated at interfaces where jumps occur. In these sit-
uations, appropriate limits can be considered. Alternatively, the tools developed in > Sect.
..can be applied there. The existence and special form of Fréchet derivatives R′(b)
(and the corresponding shape derivatives) for parameter distributions b with discontinu-
ities along interfaces are problem specific and beyond the scope of this chapter. Refer to the
cited literature, for example [, , , , , , ]. In many practical implementations,
the interface ∂D is de facto replaced by a narrow transition zone with smoothly varying
parameters, in which case the interface conditions disappear.
...
Change of b due to Shape Deformations
Assume that every point x moves in the domain Ω a small distance y(x), and that the
mapping x →y(x) is sufficiently smooth, such that the basic structure of the shape D
remains preserved. Then, the points located on the boundary Γ = ∂D will move to the
new locations x′ = x + y(x), and the boundary Γ will be deformed into the new boundary
Γ′ = ∂D′. Assume furthermore that the parameter distribution in Ω has the special form
(> .), such that it will change as well. In the following, the first goal is to quantify this
change in the parameter distribution b(x) due to an infinitesimal deformation as described
above.
Consider the inner product of δb with a test function f
⟨δb , f ⟩Ω = ∫Ω δb(x)f (x)dx = ∫symdiff(D,D′) δb(x)f (x)dx,
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

dD
dD¢
x
x′ = x + y(x)
y(x)
b = be
b = bi
⊡Fig. -
Deformation of shapes using calculation of small variations
where the overline means “complex conjugate” and symdiff(D, D′) = (D ∪D′)/(D ∩D′)
is the symmetric difference of the sets D and D′ (see
> Fig. -). Since the difference in
D and D′ is infinitesimal, the area integral reduces to a line integral. Let n(x) denote the
outward normal to x. Then, the integral in (> .) becomes
⟨δb , f ⟩∂D = ∫δD (bi(x) −be(x))y(x) ⋅n(x)f (x) ds(x),
(.)
where ds(x) is the incremental arclength. Here it has been used that in the limit δb(x) =
bi(x) −be(x) at the boundary point x ∈∂D due to (> .). It follows the result
δb(x) = ϖ∂D((bi(x) −be(x))y(x) ⋅n(x))
(.)
where ϖ∂D is the n-dimensional restriction operator which restricts functions defined in
Ω to the boundary ∂D of the shape D (n = or , usually). Therefore, δb(x) is interpreted
now as a surface measure on ∂D. Using the n-dimensional Dirac delta distribution δ∂D
concentrated on the boundary ∂D of the shape D, (> .) can be written in the form
δb(x) = (bi −be)y(x) ⋅n(x) δ∂D(x)
(.)
which is a distribution defined on the entire domain Ω but concentrated on ∂D where it
has the same strength as the corresponding surface measure. Although, strictly speaking,
they are different mathematical objects, they are identified in the following for simplicity.
Compare (> .) also to the classical shape or domain derivative as, for example calcu-
lated in [], focusing there on the effect of the infinitesimal change in the boundary of a
scatterer on the far field pattern of a scattering experiment.
...
Variation of Cost due to Velocity Field v(x)
A popular approach for generating small displacements y(x) (as discussed in
> Sect. ...) for moving the boundary ∂D is to assign to each point in the domain


Level Set Methods for Structural Inversion and Image Reconstruction
a velocity field v(x) and to let the points x ∈Ω move a small artificial evolution time [, τ]
with constant velocity v(x). Then
y(x) = v(x)τ.
(.)
Plugging this into (> .) for t ∈[, τ], the corresponding change in the parameters
follows as
δb(x; t) = (bi −be)v(x) ⋅n(x)t δ∂D(x).
(.)
Plugging expression (> .) into (> .) and neglecting terms of higher than linear
order yields
J (b(t)) −J (b()) = Re⟨gradbJ , δb(x; t)⟩P
(.)
= Re⟨gradbJ , (bi −be)v(x) ⋅n(x)t δ∂D(x)⟩P
or, in the limit t →, evaluating the Dirac delta distribution,
∂J (b)
∂t
∣
t=
= Re∫∂D gradbJ (bi −be)v(x) ⋅n(x)ds(x),
(.)
where the overline means “complex conjugate” and gradbJ is defined in (> .).
Similar expressions will be derived further below using formal shape sensitivity
analysis. (Compare, e.g., for the situation of TM-waves the expression (> .)
calculated by using (> .) with the analogous expressions (> .) and
(> .) calculated by using formal shape sensitivity analysis.)
If a velocity field v(x) can be found such that ∂J (b)
∂t
∣
t=< , then it is expected (for
continuity reasons) that this inequality holds in a sufficiently small time interval [, τ] and
that therefore the total cost during the artificial flow will be reduced. This will be the general
strategy in most optimization type approaches for solving the underlying inverse problem.
See the brief discussion in > Sect. ...
Notice that only the normal component of the velocity field
F(x) = v(x) ⋅n(x)
(.)
at the boundary ∂D of the shape D is of relevance for the change in the cost, compare
the remarks made already in > Sect. ... This is because tangential components of v
do not contribute to shape deformations. In a parameterized way of thinking, they only
“re-parameterize” the existing boundary.
...
Example: Shape Variation for TM-Waves
An instructive example is given here in order to demonstrate the above concepts for a prac-
tical application. Consider TM-waves in a typical imaging situation of subsurface imaging,
or of microwave breast screening as in the case study of > Sect. ... Assume for sim-
plicity that the basic level set model of > Sect. ..is applied here. The cost functional

Level Set Methods for Structural Inversion and Image Reconstruction 

measuring the mismatch between calculated data uM[D] corresponding to the shape D
and physically measured data ˜g is defined as
J (D) = 
∥uM[D] −˜g∥
L(M) ,
(.)
where the calculated measurements uM are given as the electric field values u(x) at the set
of receiver locations M. Using a properly defined adjoint state z(x) (see, e.g., [, , ] for
details on adjoint states), it can be shown by straightforward calculation that R′(b)∗R(b)
takes the form
(gradbJ )(x) = u(x)z(x),
(.)
where u(x) denotes the solution of the forward problem and gradbJ is defined as in
(> .). Therefore, it follows that
∂J (b)
∂t
∣
t=
= Re∫∂D u(x)z(x)(bi −be)v(x) ⋅n(x)ds(x),
(.)
where it is used that the real part of a complex number and its complex conjugate are
identical. Similar expressions based on adjoint field calculations can be derived for a large
variety of applications, see for the example [, , , , , , ].
...
Example: Evolution of Thin Shapes (Cracks)
Another application of this technique has been presented in [, ] for finding cracks in a
homogeneous material from boundary data, see > Sect. ... The evolution of cracks as
defined in > Sect. ...requires the simultaneous consideration of two level set func-
tions. Evolution of the first level set function amounts to displacement of the thin region
(crack) in the transversal direction, whereas evolution of the second level set function
describes the process of crack growth or shrinkage in the longitudinal direction, which
comes with the option of crack splitting and merging. Descent directions for both level set
functions can be calculated by following arguments presented above. It needs to be taken
into account, however, that, due to the specific construction of a crack with finite thickness,
deformation of the zero level set of the first level set function is associated with a displace-
ment of the crack boundary at two adjacent locations, which both contribute to a small
variation in the cost. See > Fig. -.
Assume that a small displacement is applied to the zero level set of the first level set
function defining S in the notation of > Sect. .... This is reflected by two contributions
to the least squares data misfit cost, one from the deformation of S−in
> Fig. -, and
the other one from the deformation of S+ in > Fig. -. It follows that a descent velocity
is now given as v(x) = Fφ(x)n(x) with
Fφ(x) = −(bi −be)[gradbJ ∣S+ −gradbJ ∣S−]
on S,
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
S–
S+
bi
be
be
⊡Fig. -
Deformation of a thin shape (crack) using calculus of small variations
with gradbJ being defined in (> .). In (> .), for each x ∈Γϕ, two adjacent points
of gradbJ ∣S+ and gradbJ ∣S−contribute to the value of Fφ(x) which can be found in the
normal direction to Γϕin x.
In a similar way, a descent direction with respect to the second level set function ϕ
can be obtained. Its detailed derivation depends slightly on the way how the crack tips are
constructed in > Sect. ..., where two alternative choices are given. Overall, a descent
velocity can be calculated following classical rules for those points x of ∂B (i.e., for those
points of the zero level set of ϕ) which satisfy x ∈∂B ∩Γδ
ϕor, alternatively, x ∈∂B ∩Γϕ.
Then, the obtained velocity field needs to be extended first to the remaining parts of ∂B, and
then to the rest of Ω. Notice that the specific form of gradbJ might be slightly different
here from the one given in (> .) due to the slightly different PDE which might be
involved here (depending on the application). For more details refer to [, ].
..
Shape Sensitivity Analysis and the Speed Method
In this section an alternative technique is presented for formally defining shape derivatives
and modeling shape deformations driven by cost functionals. This theory, called shape
sensitivity analysis, is quite general and powerful, such that it is used heavily in various
applications. Only very few concepts of it can be mentioned here which are employed when
calculating descent directions with respect to data misfit cost functionals.
The tools, as presented here, have been used and advanced significantly during the last
twenty years in the framework of optimal shape design [, ]. Having this powerful the-
ory readily available, it is therefore quite natural that these methods have been applied very
early already to the applications of shape-based inverse problems with level sets. The theory
of this section is again mainly concentrated upon modeling in a formally accurate way the

Level Set Methods for Structural Inversion and Image Reconstruction 

deformation of already existing shapes. It does not incorporate topological changes. These
will be discussed briefly in > Sect. ...
...
Example: Shape Sensitivity Analysis for TM-Waves
Again the situation of inverse scattering by TM-waves is considered here. The below discus-
sion closely follows the results presented in []. The main tools used here are the material
and shape derivative defined in > Sect. ...
The cost functional measuring the mismatch between calculated data uM[D] corre-
sponding to the shape D and physically measured data ˜g is defined by (> .). When
perturbing the shape by a velocity field V(t,x), the electric field at the (fixed) probing line
changes according to u →u +u′, where u′ is the shape derivative defined in > Sect. ...
Plugging this into (> .) and neglecting terms of higher than linear order, it is verified
that
dJ (D,V) = Re∫M u′(x)(uM −˜g)(x)dx.
(.)
Now, the shape derivative u′ can be calculated by first computing the material derivative
(also defined in > Sect. ..), and then using one of the relationships between the material
derivative and the shape derivative (see
> Sects. ..and
> ..). Using also here an
adjoint state z, the Eulerian derivative can be characterized and calculated as
dJ (D,V) = Re∫Γ(bint −bext)u(x)z(x)V(,x) ⋅n dΓ.
(.)
Notice that this is exactly the same result as we arrived at in (> .). For more details
refer to [].
...
Shape Derivatives by a Min-Max Principle
In order to avoid the explicit calculation of material and shape derivatives of the states with
respect to the flow fields, an alternative technique can be used as reported in [, , ].
It is based on a reformulation of the derivative of a shape functional J (D) with respect
to time as the partial derivative of a saddle point (or a “min-max”) of a suitably defined
Lagrangian. In the following, the basic features of this approach will be outlined, focusing
in particular on the shape derivative for TM-waves.
Let again the cost functional J (D(t)) be defined as in (> .) by
J (D(t)) = 
∥uM[D(t)] −˜g∥L(M) .
(.)
The goal is to write J (D(t)) in the form
J (D(t)) = min
u max
z
L (t,u, z)
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
for some suitably defined Lagrangian L (t,u, z). Here and in the following, the complex
nature of the forward fields u and the adjoint fields z is (partly) neglected in order to sim-
plify notation (more rigorous expressions can be found in []). The Lagrangian L (t,u, z)
takes the form
L (t,u, z) = 
∫M ∣∫Ω η (x′)G(x,x′) u (x′) dx′ −˜g(x)∣

dx
(.)
+ Re ∫Ω (u (x) −uinc (x) −∫Ω η (x′)G(x,x′)u (x′) dx′) z(x) dx .
Next, it can be shown that this Lagrangian has a unique saddle point denoted by (u∗, z∗),
which is characterized by an optimality condition with respect to u and z. In fact, the
uniqueness follows from the well-posedness and uniqueness of the solutions of the direct
and adjoint state equations, see [, ]. The key observation is now that
dJ
dt
= ∂
∂t (min
u max
z
L (t,u, z)) = ∂
∂tL (t,u∗, z∗)
(.)
which says that the time-derivative of the original cost functional can be replaced by the
partial time-derivative of a saddle point. Following these ideas, the result is derived
dJ
dt
= Re∫Γ(bint −bext)u(x)z(x)V() ⋅n dΓ
(.)
which holds for TM-waves and which is identical to the previously derived expressions
(> .) and (> .).
Similar expressions (now involving expressions of the form ∇u(x)∇z(x) rather than
u(x)z(x) at the interfaces) can be derived also for so-called TE-waves, see []. The above
outlined Min-Max approach is in fact wide ranging and can be extended to D vector scat-
tering, geometrical regularizations, simultaneous searches of shape and contrast, etc. It de
facto applies as soon as one has well-posedness of the direct and adjoint problems. For
more details refer to [, , , ].
..
Formal Shape Evolution Using the Heaviside
Function
A third possibility for describing and modeling shape deformations driven by data misfit
(in addition to using calculus of variation as in > Sect. ..or shape sensitivity analysis
as in > Sect. ..) is the use of the characteristic function and formal (basic) distribution
theory. In contrast to the previous two techniques which first calculate velocity fields in the
normal direction to the interfaces, and then move the interfaces accordingly using a level
set technique (or any other computational front propagation technique), typically leading
to a Hamilton–Jacobi-type formalism (compare the remarks in > Sect. ..), the method
presented in the following does not explicitly use the concept of velocity vector fields, but
instead tries to design evolution laws directly for the describing level set functions (thereby
not necessarily leading to Hamilton–Jacobi-type evolution laws).

Level Set Methods for Structural Inversion and Image Reconstruction 

Notice that many of the level set formulations presented in Sect. .give rise to similar
concepts as discussed in the following. On the other hand, typically also the concepts dis-
cussed in > Sects. ..and > ..can be translated, once suitable velocity fields have
been determined, into level set evolutions using the various representations of Sect. ..
Details on how these evolution laws can be established can be found in the literature cited
in Sect. ..
The formalism discussed in the following is in fact very flexible and quite easy to han-
dle if standard rules for calculations with distributions are taken into account. Moreover,
it often leads to very robust and powerful reconstruction algorithms. Certainly, it also
has some limitations: in the form presented here, it is mainly applicable for “penetrable
objects” with finite jumps in the coefficients between different regions. This means that it
does not generally handle inverse scattering from impenetrable obstacles if very specific
and maybe quite complicated boundary conditions need to be taken into account at the
scatterer surfaces. For those applications the theory based on shape sensitivity analysis is
more appropriate. Nevertheless, since many inverse scattering problems can be described
in the form presented in the following, possibly incorporating some finite jump condi-
tions of the forward and adjoint fields or their normal components across the interfaces
(which can be handled by slightly “smearing out” these interfaces over a small transition
zone), this theory based on formal distribution theory provides an interesting alterna-
tive when deriving level set based shape evolution equations for solving inverse scattering
problems.
The main idea of this technique is demonstrated by giving two examples related to the
test cases from > Sects. ..and > ...
...
Example: Breast Screening–Smoothly Varying Internal
Proﬁles
In the example of breast screening as discussed in > Sect. .., three level set functions
and four different interior parameter profiles need to be reconstructed from the given data
simultaneously. Some of the interior profiles are assumed to be constant, whereas others are
smoothly varying. In the following, the theory is developed under the assumption that all
interior parameter profiles are smoothly varying. The case of constant parameter profiles
in some region is captured in the next section where a similar case is discussed for the
application of reservoir engineering.
Let R (b(ϕ, ϕ, ϕ, b, b, b, b)) denote the difference between measured data and
data corresponding to the latest best guess (ϕ, ϕ, ϕ, b, b, b, b) of level set functions
and interior profiles in the image model discussed in > Sect. .... Then, the least squares
data misfit for tumor detection is given by
J (b(ϕ, ϕ, ϕ, b, b, b, b)) = 
∥R (b(ϕ, ϕ, ϕ, b, b, b, b))∥.
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
Introducing an artificial evolution time t for the above specified unknowns of the
inverse problem, the goal is to find evolution laws
dϕ
dt = f (x, t),
= , . . . ,,
(.)
db
dt = g (x, t),
= , . . . ,,
(.)
such that the cost J decreases with increasing evolution time. With level set functions
and interior profiles evolving, also the cost will change, J = J (t), such that formally its
time-derivative can be calculated by using the chain rule
dJ
dt
= dJ
db [

∑
=
∂b
∂ϕ
dϕ
dt
+

∑
=
∂b
∂b
db
dt ]
(.)
= Re ⟨gradbJ ,

∑
=
∂b
∂ϕ
f
+

∑
=
∂b
∂b g ⟩
P
.
Here, Re indicates to take the real part of the following complex quantity and ⟨, ⟩P denotes
a suitable inner product in parameter space P, and gradbJ is defined in (> .). It is
verified easily that in the situation of > Sect. ...
∂b
∂ϕ
= δ(ϕ)( −b+ b(−H(ϕ))
(.)
+ H(ϕ){b(−H(ϕ)) + bH(ϕ)}),
∂b
∂ϕ
= H(ϕ)δ(ϕ)[−b+ b,(−H(ϕ)) + bH(ϕ)],
(.)
∂b
∂ϕ
= H(ϕ)H(ϕ)δ(ϕ){−b+ b},
(.)
and
∂b
∂b
= −H(ϕ),
(.)
∂b
∂b
= H(ϕ)(−H(ϕ)),
(.)
∂b
∂b
= H(ϕ)H(ϕ)(−H(ϕ)),
(.)
∂b
∂b
= H(ϕ)H(ϕ)H(ϕ).
(.)
Descent directions are therefore given by
f (t) = −C (t)Re[gradbJ ∂b
∂ϕ ],
= , . . . ,,
(.)
g (t) = −ˆC (t)Re[gradbJ ∂b
∂b ],
= , . . . ,,
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

with some appropriately chosen positive valued speed factors C (t) and ˆC (t). An effi-
cient way to compute gradbJ is again to use the adjoint formulation, see (> .), and
for more details [, , ].
Notice that is might be convenient to approximate the Dirac delta on the right hand
side of (> .)–(> .) in the formulation of the level set evolution by either a nor-
rowband scheme or by a positive constant which allows for topological changes in the
entire computational domain driven by the least squares data misfit. For more details, see
the brief discussion in > Sect. ..and the slightly more detailed discussions held in
[, ]. Following the latter scheme, one possible numerical discretization of the expres-
sions (> .) and (> .) in time t = t(n), n = ,,, . . ., then yields the update
rules
ϕ(n+) = ϕ(n) −δt(n)C (t(n))Re[gradbJ ∂b
∂ϕ ]
(n)
,
= , . . . ,,
(.)
b(n+) = b(n) −δt(n) ˆC (t(n))Re[gradbJ ∂b
∂b ]
(n)
,
= , . . . ,.
(.)
...
Example: Reservoir Characterization–Parameterized
Internal Proﬁles
In the example of history matching in reservoir engineering as discussed in > Sect. ..,
one level set function and two interior parameter profiles need to be reconstructed from
the given data, where one interior parameter profile is assumed to be smoothly varying,
and the other one is assumed to overall follow a bilinear pattern. The case of smoothly
varying interior parameter profiles is completely analogous to the situation discussed in
the previous section for microwave breast screening, such that it is not considered here. In
the following, the situation is treated where both interior profiles follow a parameterized
model with a certain set of given basis functions. In the history matching application as well
as in the microwave breast screening application, the mixed cases of partly parameterized
(e.g., with a constant or a bilinear profile) and partly smooth profiles are straightforward
to implement as combinations of these two general approaches.
In this approach, it is assumed that the two internal profiles can be written in the
parameterized form
bi(x) =
Ni
∑
j=
αja j(x),
be(x) =
Ne
∑
k=
βkbk(x),
(.)
where a j and bk are the selected basis functions for each of the two domains D and Ω −D,
respectively. See the model discussed in > Sect. .... In the inverse problem, the level set
function ϕ and the weights αj and βk need to be estimated with the goal to reproduce the
measured data in some sense. In order to obtain an (artificial) evolution of the unknown


Level Set Methods for Structural Inversion and Image Reconstruction
quantities ϕ, αj, and βk, the following three general evolution equations for the level set
function and for the weight parameters are formulated
dϕ
dt = f (x, t, ϕ,R),
(.)
dαj
dt
= g j(t, ϕ,R),
dβk
dt
= hk(t, ϕ,R).
(.)
In the same way as before, the goal is to define the unknown terms f , g j, and hk such that
the mismatch in the production data decreases during the evolution. For this purpose, we
reformulate the cost functional now as
J (b(ϕ, αj, βk)) = 
∥R(b(ϕ, αj, βk))∥,
(.)
where αj denotes the weight parameters for region D, and βk denotes the weight param-
eters for region Ω −D. Formal differentiation of this cost functional with respect to the
artificial time variable t yields, in a similar way as before, the descent directions []
fSD(x) = −CχNB(ϕ)(be −bi)gradbJ ,
(.)
g jSD(t) = −C(αj)∫Ω a j(−H(ϕ))gradbJ dx ,
(.)
hkSD(t) = −C(βk)∫Ω bkH(ϕ)gradbJ dx ,
(.)
where C, C(αj), and C(βk) are again positive valued speed factors which are used for
steering the speed of evolution for each of the unknowns ϕ, αj, and βk individually. The
narrowband function χNB(ϕ) is introduced for computational convenience, and can be
omitted if desired. For details on this narrowband formulation, see the brief discussion
held in > Sect. ...
.
Regularization Techniques for Shape Evolution
Driven by Data Misﬁt
Regularization of shape evolution can be achieved by additional additive or multiplica-
tive terms in the cost functional which control geometric terms, as discussed in Sect. ..
Alternatively, some form of regularization can be obtained by restricting the velocity fields,
level set updates or level set functions to certain classes, often without the need to intro-
duce additional terms into the cost functional. Some of these techniques are presented in
the following.
..
Regularization by Smoothed Level Set Updates
In the binary case (see > Sect. ..), a properly chosen level set function ϕ uniquely spec-
ifies a shape D[ϕ]. This can be described by a nonlinear operator Π mapping level set

Level Set Methods for Structural Inversion and Image Reconstruction 

functions to parameter distributions
Π(ϕ)(x) = {bi(x) ,
ϕ(x) ≤,
be(x),
ϕ(x) > .
(.)
We obviously have the equivalent characterization
Π(ϕ)(x) = bi(x)χD(x) + be(x)(−χD(x))
(.)
where χD is the characteristic function of the shape D. The “level-set-based residual
operator” T (ϕ) follows as
T (ϕ) = R(Π(ϕ)).
(.)
Formal differentiation by the chain rule yields
T ′(ϕ) = R′(Π(ϕ))Π′(ϕ).
(.)
The (formal) gradient direction of the least square cost functional
ˆ
J (ϕ) = 
∥R(b(ϕ))∥
Z
(.)
is then given by
grad
ˆ
J (ϕ) = T ′(ϕ)∗T (ϕ),
(.)
where T ′(ϕ)∗is the L-adjoint of T ′(ϕ). Moreover, formally it is calculated by standard
differentiation rules that
Π′(ϕ) = (bi −be)δ(ϕ).
(.)
Notice that, strictly speaking, the right hand side of (> .) is not an L-function due to
the Delta distribution which is seen in (> .). Nevertheless, in order to obtain practi-
cally useful expressions in a straightforward way, it is convenient to proceed with the formal
considerations and, whenever necessary, to approximate the Dirac delta distribution δ(ϕ)
by a suitable L-function, see the brief discussion on this topic held in > Sect. ... For
example, the narrowband function χϕ,d(x) as defined in (> .) can be used for that
purpose. Then,
T ′(ϕ)∗= Π′(ϕ)∗R′(Π(ϕ))∗.
(.)
Assuming now that ϕ ∈W(Ω) with
W(Ω) = {ϕ : ϕ ∈L(Ω), ∇ϕ ∈L(Ω), ∂ϕ
∂
= at ∂Ω} ,
(.)
the adjoint operator T ′(ϕ)∗needs to be replaced by a new adjoint operator T ′(ϕ)○which
maps back from the data space into this Sobolev space W(Ω). Using the weighted inner
product
⟨v,w⟩W(Ω) = α⟨v,w⟩L(Ω) + β⟨∇v,∇w⟩L(Ω)
(.)
with α ≥and β > being carefully chosen regularization parameters, it follows
T ′(ϕ)○= (αI −βΔ)−T ′(ϕ)∗.
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
The positive definite operator (αI −βΔ)−has the effect of mapping the Lgradient
T ′(ϕ)∗T (ϕ) from L(Ω) towards the smoother Sobolev space W(Ω). In fact, differ-
ent choices of the weighting parameters α and β visually have the effect of “smearing out”
the unregularized updates to a different degree. In particular, high frequency oscillations
or discontinuities of the updates for the level set function are removed, which yields shapes
with more regular boundaries. Notice that for ϕ ∈W(Ω) the trace ϕ∣Γ (which is the zero
level set) is only within the intermediate Sobolev space W/(Γ) due to the trace theo-
rem. Therefore, the “degree of smoothness” of the reconstructed shape boundaries Γ lies
somewhere in between L(Γ) and W(Γ).
Sometimes it is difficult or inconvenient to apply the mapping (> .) to the calcu-
lated updates T ′(ϕ)∗T (ϕ). Then, an approximate version can be applied instead which
is derived next. Denote fr = T ′(ϕ)○T (ϕ) and fd = T ′(ϕ)∗T (ϕ). fr can formally be
interpreted as the minimizer of the cost functional
ˆ
J (f ) = α −

∥f ∥
L+ β
∥∇f ∥
L+ 
∥f −fd∥
L.
(.)
In particular, the minimization process of (> .) can be attempted by applying a gradi-
ent method instead of explicitly applying (αI −βΔ)−to fd. The gradient flow of (> .)
yields a modified heat (or diffusion) equation of the form
vt −βΔv = fd −αv
for
t ∈[, τ]
(.)
v() = fd,
with time-dependent heating term fd −αv, where ˆv = v(τ) evolves towards the minimizer
fr of (> .) for τ →∞. Practically, it turns out that a satisfactory regularization effect is
achieved if instead of (> .) the simplified heat equation is solved for a few time steps
only:
vt −βΔv = 
for
t ∈[, τ]
(.)
v() = fd,
for τ small, using ˆv = v(τ) instead of fr as update. For more details see [].
The above described regularization schemes only operate on the updates (or forcing
terms f in a time-dependent setting), but not on the level set function itself. In particu-
lar, in the case that a satisfactory solution of the shape reconstruction problem has already
been achieved such that the data residuals become zero, the evolution will stop (which
sometimes is desirable). In the following subsection we will mention some alternative reg-
ularization methods where the evolution in the above described situation would continue
until an extended cost functional combining data misfit with additional geometric terms
or with additional constraints on the final level set functions is minimized.

Level Set Methods for Structural Inversion and Image Reconstruction 

..
Regularization by Explicitly Penalizing Rough Level
Set Functions
Instead of smoothing the updates to the level set functions, additional terms can be added
to the data misfit cost functional which have the effect of penalizing certain characteristics
of the level set function. For example, a Tikhonov-Philips term for the level set function
can be added to (> .), which will yield the minimization problem
min
ϕ J (ϕ) = 
∥R(b(ϕ))∥
Z + ρ(ϕ),
(.)
where ∥∥Z denotes the canonical norm in the data space Z and where ρ denotes some
additional regularization term, typically involving the norm or semi-norm in the space of
level set functions, for example ρ(ϕ) = ∥∇ϕ∥
L. A discussion of different choices for ρ(ϕ)
is provided in []. Alternative functionals could be applied to the level set function ϕ,
as for example Mumford-Shah, total variation, etc., which would allow for jumps in the
representing level set functions.
..
Regularization by Smooth Velocity Fields
In the previous two subsections, regularization tools have been discussed, which are
directly linked to the level set formulation of shape evolution. In > Sect. .., smooth-
ing operators have been applied to the updates of the level set functions (or forcing terms)
which are considered as being defined on the whole domain Ω. The additional terms dis-
cussed in > Sect. .., on the other hand, will yield additional evolutions terms which
typically have to be applied directly to the describing level set functions during the shape
evolution.
An alternative concept of regularizing shape evolution, which does not directly refer to
an underlying level set representation of the shapes, consists in choosing function spaces
for the normal velocity fields which drive the shape evolution. These velocity fields are, as
such, only defined on the zero level set, i.e., on the boundaries of the given shapes (unless
extension velocities are defined for a certain reason). For example, the velocity field could
be taken as an element of a Sobolev space W(Γ) equipped with the inner product
⟨v,w⟩W(Γ) = ∫Γ (∂v
∂s
∂w
∂s + vw) ds,
(.)
where ds is the surface increment at the boundary. This leads to a postprocessing operator
applied to the gradient directions which are restricted to the boundary Γ. The action of
this postprocessing operator can be interpreted as mapping the given velocity field from
L(Γ) towards the smoother subspace W(Γ), much as it was described in > Sect. ..for
the spaces L(Ω) and W(Ω). For the above given norm (> .) this is modeled by a
Laplace-Beltrami operator
−∂v
∂s+ v = fd∣Γ.
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
Weighted versions of (> .), (> .), with parameters α and β as in (> .), can
be defined as well. These operators have the effect of smoothing the velocity fields along the
boundary Γ and therefore lead to regularized level set evolutions if suitable extension veloc-
ities are chosen. Alternatively, diffusion processes along the boundary can be employed for
achieving a similar effect of smoothing velocity fields. For a more detailed description of
various norms and the corresponding surface flows, see [, , ].
..
Simple Shapes and Parameterized Velocities
An even stronger way of regularizing shape evolution is to restrict the describing level set
functions or the driving velocities to be members of finite-dimensional function spaces
spanned by certain sets of basis functions. As basis functions, for example polynomials,
sinusoidal or exponential functions, or any other set of linearly independent functions tai-
lored to the specific inverse problem can be used. Closely related to this approach is also
the strategy of restricting the shapes (and thereby the shape evolution) to a small set of geo-
metric objects, as for example ellipsoids. See the discussion in [] where evolution laws for
a small sample of basic shapes are derived. In a related manner, [] considers a multiscale
multiregion level set technique which adaptively adjusts the support and number of basis
functions for the level set representation during the shape evolution. Also related to this
approach is the projection mapping strategy for shape velocities as proposed in [].
.
Miscellaneous On-Shape Evolution
..
Shape Evolution and Shape Optimization
Shape evolution and shape optimization are closely related. Assume given any veloc-
ity function F(x) = v(x) ⋅n(x) pointing into a descent direction of the cost J , such
that
∂J (b)
∂t
∣
t=<. Then, the cost will decrease in the artificial time-evolution during a
sufficiently small time-interval [, τ]. On the practical level, the corresponding Hamilton–
Jacobi-type evolution equation for the representing level set function to be solved during
the time interval [, τ] reads
∂ϕ
∂t + F ∣∇ϕ∣= ,
(.)
where the variables (x, t) have been dropped in the notation. Using, for example, a
straighforward time-discretization scheme with finite differences yields
ϕ(τ) −ϕ()
τ
+ F∣∇ϕ∣= .
(.)
Interpreting ϕ(n+) = ϕ(τ) and ϕ(n) = ϕ(), yields the iteration
ϕ(n+) = ϕ(n) + τδϕ(n),
ϕ() = ϕ,
(.)

Level Set Methods for Structural Inversion and Image Reconstruction 

where τ plays the role of the step-size (which might be determined by a line-search strategy)
and where
δϕ(n) = F∣∇ϕ(n)∣
(.)
for x ∈∂D.
In the level set optimization approach, on the other hand, updates v for a level set func-
tion ϕ →ϕ + v are sought which reduce a given cost. Take for example the situation of
the basic level set formulation described in > Sect. ... Analogously to > Sect. ..., a
small perturbation v then has the effect on the cost
dJ
dϕ v = dJ
db
db
dϕv = Re⟨gradbJ , db
dϕv ⟩
P
(.)
= Re⟨gradbJ , (be(x) −bi(x))δ(ϕ)v ⟩P ,
with gradbJ defined in (> .). Apart from the term δ(ϕ), this yields similar expres-
sions for the discrete updates as (> .) if choosing F∣∇ϕ(n)∣
=
−Re(be(x) −
bi(x))gradbJ .
In fact, the term δ(ϕ) is the one which causes the biggest conceptual problem when
interpreting the above scheme in an optimization framework. Notice that, strictly speak-
ing, the mapping from the level set function to the data (or to the corresponding least
square cost) is not differentiable in standard (for example L) function spaces. This is indi-
cated by the appearance of this Dirac delta distribution δ(ϕ) in (> .), which is not an
Lfunction.
There are several ways to circumvent these difficulties, mainly aiming at replacing this
troublesome Delta distribution by a better behaved approximation of it. First, in the nar-
rowband approach, the Dirac delta is replaced by a narrow band function χϕ,d(x) which
yields
Fd∣∇ϕ(n)∣(x) = −Re ((be −bi) χϕ,d(x)gradbJ )
for all
x ∈Ω.
(.)
Here, χϕ,d(x) is an arbitrary positive-valued approximation to δ(ϕ) where the subscript d
indicates the degree of approximation. For example, it can be chosen as
χϕ,d(x) = { ,
there exists x∈Ω with ∣x −x∣< d and ϕ(x) = 
,
otherwise
(.)
which has the form of a “narrowband” function. Other approximations with certain addi-
tional properties (e.g., on smoothness) are possible as well. This search direction obviously
also provides a descent flow for J . In fact, the term ∣∇ϕ(n)∣can also be neglected in
(> .), without losing the descent property of the resulting flow, since formally it can
be assumed that ∣∇ϕ(n)∣> (repeated recalculation of a signed distance function would
even enforce ∣∇ϕ(n)∣= ).
The Dirac delta could as well be replaced by a positive constant, say , which yields
another choice for a descent direction
Ftop(x) = −Re(be −bi)gradbJ
for all
x ∈Ω.
(.)


Level Set Methods for Structural Inversion and Image Reconstruction
This new direction Ftop(x) has the property that it applies updates driven by data sensitiv-
ities on the entire domain, and thereby enables the creation of objects far away from the
actual zero level set by lowering a positive level set function until its values arrive at zero.
Certainly, at this moment when the level set function changes at some points far away from
the zero level set from positive to negative values, a new object is created, and the descent
property with respect to the cost needs to be evaluated by introducing some concept eval-
uating the effect of object creation on the data misfit. A formal way of doing so is briefly
discussed in > Sect. ..further below. The opposite effect that inside a given shape, with
some distance from the zero level set, the values of the level set function switch from nega-
tive to positive values, can also occur, in which case a hole is created inside the shape. Also
here, justification of this hole creation with respect to its effect on the data misfit cost is
needed, and can be treated as well by the tools discussed in > Sect. ...
Notice that, in the more classical level set framework, these replacements of the Dirac
delta by functions with extended support can be interpreted as different ways of defining
extension velocities for the numerical level set evolution scheme. Refer to [, , , , ]
and the further references given there for numerical approaches which are focusing on
incorporating topology changes during the shape reconstruction.
Once optimization schemes are considered for level set based shape reconstruction, a
rich set of classical optimization schemes can be adapted and applied to this novel applica-
tion. For example, Newton-type optimization techniques and second order shape deriva-
tives can be defined and calculated. Strategies for doing so are available in the literature,
see for example []. Also quasi-Newton-, Gauss–Newton-, or Levenberg–Marquardt-type
schemes look promising in this framework. Some related approaches can be found, for
example, in [, , , , ]. There exists a large amount of literature concerned with
shape optimization problems in various applications. One important application is for
example the structural optimal shape design problem, where the shape of a given object
(a tool, bridge, telegraph pole, airplane wing, etc.) needs to be optimized subject to cer-
tain application-specific constraints [, , ]. Another example is the optimization of a
band-gap structure or of maximal eigenvalues [, , ]. Some techniques from nonlin-
ear optimization which have been successful in those applications consequently have also
found their way into the treatment of shape inverse problems. For brevity, we simply refer
here to the discussions presented in [, , , , , , , ] and the many further ref-
erences therein. Alternative nonlinear algebraic reconstruction techniques are employed
in [], and fixed point techniques in [, ].
..
Some Remarks on Numerical Shape Evolution with
Level Sets
Not much is said here regarding numerical schemes for solving Hamilton–Jacobi equa-
tions numerically, or for solving the related optimality systems for shape inverse problems
numerically. In the framework of imaging science, various schemes have been devel-
oped and discussed extensively in the vast literature on numerical level set evolution, see

Level Set Methods for Structural Inversion and Image Reconstruction 

for example the books and reviews [, , ], to mention just a few examples. These
schemes include CFL conditions, re-initialization of level set functions, signed distance
functions, the fast marching method, higher-order upwind schemes like ENO (essentially
non-oscillating) and WENO (weighted essentially non-oscillating), artificial viscosity solu-
tions, numerical discretizations of mean curvature terms in the level set framework, etc.
All these techniques can be applied when working on the treatment of inverse problems
by a level set formulation.
It is emphasized here, however, that the application of image reconstruction from indi-
rect data comes with a number of additional problems and complications which are due
to the ill-posedness of the inverse problem and to the often high complexity of the PDE
(or IE) involved in the simulation of the data. Therefore, each particular image recon-
struction problem from indirect data requires a careful study of numerical schemes which
typically are tailor-made for the specific application. Overall, a careful choice of numerical
discretization schemes and regularization parameters is indeed essential for a stable and
efficient solution of the shape reconstruction problem. Moreover, also design parameters
of the experimental setup (as for example source and receiver locations) during the data
collection have a significant impact on the shape evolution later on in the reconstruction
process. Judicious choices here pay out in form of faster and more reliable reconstructions.
..
Speed of Convergence and Local Minima
Level set methods for shape reconstruction in inverse problems have initially been claimed
to suffer from slow convergence due to inherent time-discretization constraints (the CFL
condition) for the Hamilton–Jacobi equation and due to the (so far) exclusive use of first-
order shape derivatives. Also, it had been observed that the shape evolution sometimes gets
trapped in local minima, such that, for example, some topological components are missed
by the shape evolution when starting with an inappropriate intitial guess.
However, these initial problems seem to have been resolved by now, and it appears that
level set methods have in fact become quite efficient and stable when following certain
straightforward guidelines, and often even clearly outperform many classical pixel-based
reconstruction schemes when additional prior information is available.
Firstly, the search for a good starting guess for the shape evolution can usually be done
by either specific pre-processing steps (as for example in []) or by employing more tra-
ditional search routines for only a few iteration steps. This helps avoiding “long-distance
evolutions” during the succeeding shape reconstruction process.
A similar effect is achieved by the incorporation of some form of “topological deriva-
tive” in the shape evolution algorithm, see the brief discussion of this topic in the following
> Sect. ... With this topological derivative technique, “seed” objects occur during the
evolution just at the correct locations to be deformed in only few more iterations to their
final shapes.
The topological derivative (or an appropriately designed extension velocity which has
a similar effect) can also help in avoiding the shape evolution to become trapped in local


Level Set Methods for Structural Inversion and Image Reconstruction
minima due to barriers of low sensitivity where velocity fields become very small. Again
by the effect of the creation of “seed” objects in areas of higher sensitivity, the shape evo-
lution can jump over these barriers and quickly arrive at the final reconstruction. When
an object is extended over an area of low sensitivity, then, certainly, any reconstruction
scheme has difficulties with its reconstruction inside this zone, such that additional prior
information might be needed for arriving at a satisfactory result inside this zone of low
sensitivity (regardless which reconstruction technique is used).
In addition, also higher-order shape derivatives have been developed in the literature
(see, e.g., []) which can be used for deriving higher-order shape-based reconstruction
schemes. So far, however, their usefulness as part of a level-set-based shape inversion
technique has been investigated only to a very limited extent.
Finally, in an optimization framework, line-search techniques can replace the CFL con-
dition for marching toward the sought minimum of a cost functional. This can speed up
convergence significantly.
Keeping these simple strategies in mind, level set based reconstruction techniques can
in fact be much faster than more traditional schemes, in particular when the contrast value
of the parameters is assumed to be known and does not need to be recovered simultane-
ously with the shape. For very ill-posed inverse problems, traditional techniques need a
large number of iterations to converge to the right balance between correct volume and
contrast value of the sought objects.
..
Topological Derivatives
Even though the level set formulation allows for automatic topology changes during the
shape evolution, the concepts on calculating descent directions derived so far do not really
apply at the moment when a topological change occurs. This is typically no problem for
the case of splitting and merging of shapes, since descent directions are only calculated for
discrete time steps, such that practically always never the need arises to calculate a descent
direction just when such a topological change occurs. Still, from a theoretical perspective, it
would be interesting to calculate expressions also for topological derivatives which capture
the splitting and merging of shapes.
Another situation where topological changes occur in shape evolution is the creation
and annihilation of shape components. These situations also occur automatically in the
level set framework when a suitable extension velocity is chosen. However, for these two sit-
uations, explicit expressions have been derived in the literature which describe the impact
of an infinitesimal topological change on the least squares data misfit cost. These are
generally known as topological derivatives.
The technique of topological derivatives has received much attention lately as a direct
way of image reconstruction. The idea in these approaches is usually to calculate a value
of the topological derivative (or topological sensitivity) at each location of the imaging
domain, and then adding small geometric objects at those places where this topological
sensitivity is the most negative.

Level Set Methods for Structural Inversion and Image Reconstruction 

Certain issues arise here, as for example the question on how large these new objects
should be, how close to each other or to the domain boundary they can be, and which con-
trast value should be applied for the so created small object. So far, in most cases, just one
update in line with the above said is done, and thereafter the image reconstruction is either
stopped or continued by a shape evolution of the so constructed set of objects. Nevertheless,
the possibility of iterative topological reconstruction techniques remains an interesting
challenge. Furthermore, the combination of simultaneous topological and shape evolution
seems to be a very promising approach which combines the flexibility of level set evolu-
tion with the sensitivity driven creation and annihilation of shapes. This effect occurs in
practice automatically if appropriate extension velocities are chosen in the regular level set
shape evolution technique.
In the following, a more formal approach to topological changes is presented which has
the advantage of providing a stronger mathematical justification of topological changes
in the goal of data misfit reduction. The discussion will be based on the general ideas
described in the references [, , , , , , , , ]. The topological derivative
as described here aims at introducing either a small hole (let us call it Bρ) into an existing
shape D, or at adding a new object (let us call it Dρ) into the background material at some
distance away from an already existing shape D (see > Fig. -). We will concentrate in
the following on the first process, namely, adding a small hole into an existing shape. The
complementary situation of creating a new shape component follows the same guidelines.
Denote ˜Dρ = D/Bρ, where the index ρ indicates the “size” of the hole Bρ, and where
it is assumed that the family of new holes defined by this index is “centered” at a given
point ˆx. (In other words one has ˆx ∈Bρ ⊂Bρ′ for any < ρ < ρ′ < .) It is assumed that all
boundaries are sufficiently smooth. Consider then a cost functional J (D) which depends
on the shape D. The topological derivative DT is defined as
DT(ˆx) = lim
ρ↓
J ( ˜Dρ) −J (D)
f (ρ)
,
(.)
W\D
x^
Br
Br + dr
D
⊡Fig. -
Creating a hole Bρ inside the shape D


Level Set Methods for Structural Inversion and Image Reconstruction
where f (ρ) is a function which approaches zero monotonically, i.e., f (ρ) →for ρ →.
With this definition, the asymptotic expansion follows
J ( ˜Dρ) = J (D) + f (ρ)DT(ˆx) + o(f (ρ)).
(.)
Early applications of this technique (going back to [, , ]) were focusing on introduc-
ing ball-shaped holes into a given domain in connection to Dirichlet or Neumann problems
for a Laplace equation. Here, the function f (ρ) is mainly determined by geometrical fac-
tors of the created shape, and the topological derivative J ( ˜Dρ) can be determined by
solving one forward and one adjoint problem for the underlying Laplace equation. In fact,
for the Neumann problem for the Laplace equation using ball-shaped holes the relationship
(> .) takes the original form introduced in [, , ] where f (ρ) is just the negative
of the volume measure of the ball, i.e., f (ρ) = −πρin D and f (ρ) = −πρ/in D. For
more details and examples see []. In general, the details of the behavior of the limit in
(> .), as well as of the function f (ρ) if the limit exists, depend strongly on the shape
of the hole, on the boundary condition at the hole interface, and on the underlying PDE.
An attempt has been made recently to find alternative definitions for the topological
derivative. One such approach has been presented in [, , ]. Instead of taking the
point of view that a hole is “created”, the topological derivative is modeled via a limiting
process where an already existing hole gradually shrinks until it disappears. For example,
perturb the parameter ρ of an existing hole by a small amount δρ. Then, the cost J ( ˜Dρ)
is perturbed to J ( ˜Dρ+δρ), and the following limit appears,
D∗
T(ˆx) = lim
ρ→{ lim
δρ→
J ( ˜Dρ+δρ) −J ( ˜Dρ)
f (ρ + δρ) −f (ρ)
}.
(.)
In [, ] the authors show a relationship between (> .) and (> .), which reads
as
DT(ˆx) = D∗
T(ˆx) = lim
ρ→

f ′(ρ)∣Vn∣DVn(ρ),
(.)
where DVn(ρ) is a specific form of a shape derivative related to a velocity flow Vn in the
inward normal direction of the boundary ∂Bρ with speed ∣Vn∣. For more details refer
to [, ]. A related link between shape derivative and topological derivative has been
demonstrated also in []. Recently published related work on this topic is briefly reviewed
in [].
.
Case Studies
..
Case Study: Microwave Breast Screening
In > Sect. .., a complex breast model is presented for tackling the problem of early
breast cancer detection from microwave data. Due to the high complexity of the model, also
the reconstruction algorithm is likely to show some complexity. In [] a reconstruction

Level Set Methods for Structural Inversion and Image Reconstruction 

technique is proposed which uses five consecutive stages for the reconstruction. In the
first stage, a pixel-by-pixel reconstruction is performed for the interior fatty-fibroglandular
region, with the skin region being (at this stage of the algorithm typically still incorrectly)
estimated and fixed. Once a pixel-based reconstruction has been achieved, an initial shape
for the fibroglandular region (the background being then fatty tissue) is extracted from it,
which then, in the succeeding stages, is evolved jointly with the interior profiles, the skin
region, and a possible tumor region until the final reconstruction is achieved. An important
feature of the algorithm is that in different stages of the algorithm different combinations
of the unknowns (level set functions and interior parameter profiles) are evolved. For more
details regarding the reconstruction algorithm, refer to [].
Here, the pixel-by-pixel reconstructions of stage I of the algorithm and the final recon-
structions using the complex breast model and a level set evolution are presented for the
three breast models introduced in
> Fig. -and compared with each other in the cases
x [cm]
0
4
8
12
16
0
20
40
60
x [cm]
Permittivity
y [cm]
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
10
20
30
40
⊡Fig. -
First breast model of > Fig. -with a disc-shaped tumor of diameter mm situated deeply
inside the breast. Top left: reference permittivity proﬁle (true tumor permittivity value
єtum
s
= ). Top center: the result at the end of stage I (pixel-by-pixel reconstruction). Top right:
ﬁnal reconstruction of level set based structural inversion scheme (reconstructed
permittivity value є
reconst
st
= ). Bottom: cross section through the correct tumor for constant
y coordinate (the dashed line represents the true permittivity proﬁle, the solid line the
pixel-by-pixel result, and the dash-dotted line the structural inversion result). For more
details see []


Level Set Methods for Structural Inversion and Image Reconstruction
x [cm]
0
4
8
12
16
0
20
40
60
x [cm]
Permittivity
y [cm]
0
10
0
5
10
15
0
10
0
10
10
20
30
⊡Fig. -
Second breast model of > Fig. -with a large ﬁbroglandular tissue and a disc shaped
tumor of mm diametre. The images are arranged as in > Fig. -. The real static
permittivity of the tumor is єtumor
st
= and the reconstructed one is є
reconst
st
= . See also the
animated movie provided in [] which shows the shape evolution for this example
where a small tumor is present. See
> Figs. -–-. The upper left image of each fig-
ure shows the real breast, the central upper image shows the pixel-by-pixel reconstruction
with our basic reconstruction scheme, and the upper right image shows the level set based
reconstruction using the complex breast model explained in > Sect. ... The bottom
images show cross-sections through a horizontal line indicated in the upper row images
and passing through the tumor locations for the three images.
The data are created on a different grid than the one used for the reconstruction. The
corresponding signal-to-noise ratio is dB. Forty antennas are used as sources and as
receivers, which are situated equidistantly around the breast. Microwave frequencies of
,,,, and GHz are used for the illumination of the breast.
Even though the pixel-by-pixel reconstruction scheme is not optimized here, a general
problem of pixel-based reconstruction can be identified immediately from the presented
examples. The reconstructions tend to be oversmoothed, and the small tumor can hardly
be identified from the pixel-based reconstruction. By no means it is possible to give any
reliable estimate from these pixel-based reconstructions for the contrast of the interior
tumor values to the fibroglandular or fatty tissue values of static relative permittivity. True,
the level set reconstruction scheme takes advantage of the fact that it closely follows the

Level Set Methods for Structural Inversion and Image Reconstruction 

x [cm]
0
4
8
12
16
0
10
20
30
40
50
x [cm]
Permittivity
y [cm]
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
10
20
30
40
⊡Fig. -
Third breast model of > Fig. -with a region of ﬁbroglandular tissue intermixed with
adipose tissue. The hidden tumor is an ellipsoid of × mm (lengths of principle axes). The
images are displayed as in > Fig. -. The real static permittivity value of the tumor is
єtumor
st
= and the reconstructed one is є
reconst
st
= . For more details see []
correct model for breast tissue. On the other hand, this information is typically available
(at least approximately) in breast screening applications, such that better estimates of the
tumor characteristics can be expected when using such a level set based complex breast
model. This is confirmed in the three reconstructions shown in the upper right images of
> Figs. -–-. For more details on this reconstruction scheme in microwave breast
screening, and for an animated movie showing the image evolution, see [].
..
Case Study: History Matching in Petroleum
Engineering
> Figure -shows the situation described in > Sect. ..of history matching from
production data. The image is composed of one zone of overall (approximately) bilinear
behavior (a trend), and another zone where the permeability is smoothly varying without
any clearly identifiable trend. The reconstruction follows this model and evolves simulta-
neously the region boundaries (i.e., the describing level set function), the three expansion
parameters of the bilinear profile in the sandstone lithofacie, as well as the smoothly vary-
ing interior permeability profile in the shale region. The initial guess (upper right image
of the figure) is obtained from well-log measurements. The true image is displayed in the


Level Set Methods for Structural Inversion and Image Reconstruction
prod
prod
prod
prod
prod
prod
prod
prod
prod
inject
inject
inject
inject
0
100
200
300
400
500
600
0
100
200
300
400
500
600
200
300
400
500
600
700
800
900
1000
1100
1200
prod
prod
prod
prod
prod
prod
prod
prod
prod
inject
inject
inject
inject
0
100
200
300
400
500
600
0
100
200
300
400
500
600
200
300
400
500
600
700
800
900
1000
1100
1200
prod
prod
prod
prod
prod
prod
prod
prod
prod
inject
inject
inject
inject
0
100
200
300
400
500
600
0
100
200
300
400
500
600
200
300
400
500
600
700
800
900
1000
1100
1200
0
100
200
0
1
2
3
0
100
200
3.9
4.1
4.3
4.5
4.7
215
216
0
40
80
120
0
0.4
0.8
1.2
1.6
2
t [days]
Water prod. rate
⊡Fig. -
Case study: history matching in reservoir engineering from production data. Left column
from top to bottom: reference model, ﬁnal reconstruction, and evolution of parameter values
β, β, βof the bilinear trend model; right column from top to bottom: initial guess, evolution
of the least squares data misﬁt and the initial (red solid), ﬁnal (black dashed) and reference
(black solid) total water production rate in m/s (i.e., the true and estimated measurements).
The complete evolution as an animated ﬁle and more details on the reconstruction scheme
can be found in []

Level Set Methods for Structural Inversion and Image Reconstruction 

upper left image of the figure, and the final reconstruction in the center left image. The cen-
ter right image shows the evolution of the data misfit cost during the joint evolution of all
model unknowns; the lower left image shows the evolution of the three model parameters
for the bilinear model in one of the regions; and the lower right image shows the initial,
true, and final production rate profile over production time averaged over all boreholes (the
data). A classical pixel-based reconstruction scheme typically is not able to use different
models for the parameter profiles in the different regions and simultaneously reconstruct
the sharp region interfaces. For more details on this reconstruction scheme for history
matching in reservoir engineering, including animated movies for additional numerical
examples, see [].
20
40
60
80
100
20
40
60
80
100
20
40
60
80 100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
50
100
150
200
50
100
150
200
5
10
15
20
0
0.05
0.1
0.15
0.2
⊡Fig. -
Case study: crack reconstruction by an evolution of a thin shape. Top row from left to right:
initial guess, reconstruction after and iterations. Middle row: after , , and iterations.
Bottom row: ﬁnal reconstruction after iterations, real crack distribution, and evolution of
least squares data misﬁt with iteration index. The noise level is indicated in the bottom right
image by a horizontal dotted line. One iteration amounts to successive application of the
updates corresponding to the data of each source position in a single-step fashion


Level Set Methods for Structural Inversion and Image Reconstruction
..
Case Study: Reconstruction of Thin Shapes (Cracks)
> Figure -shows a situation of shape evolution for the reconstruction of thin shapes
(cracks) as described in > Sect. ... The numerical model presented in > Sect. ...
is used here, where both level set functions describing the crack are evolved simultane-
ously driven by the least squares data misfit term. It is seen clearly that also in this specific
model topological changes occur automatically, when marching from a single initital (and
somewhat arbitrary) crack candidate (upper left image of the figure) towards the final
reconstruction showing three different crack components (bottom left image of the fig-
ure). The true situation is displayed in the bottom middle image of the figure, which shows
as well three crack components which roughly are at the same location and of similar shape
as the reconstructed ones. The evolution of the data misfit cost over artificial evolution time
is displayed in the bottom right image of the figure. For more details on this reconstruction
scheme and additional numerical experiments see [].
.
Cross-References
Readers interested in the material presented in this chapter will also find interesting and
relevant additional material in many other chapters of this handbook. Some additional
numerical results using level set techniques can be found, for example, in the chapter on
EIT. Many concepts relevant to specific implementations of level set techniques can be
found, amongst others, in the following chapters.
> Inverse Scattering
> Iterative Solution Methods
> Large Scale Inverse Problems
> EIT
> Regularization Methods for Ill-Posed Problems
> Shape Spaces
> Tomography
> Total Variation in Imaging
> Linear Inverse Problems
> Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
> Optical Imaging
Acknowledgments
OD thanks Diego Álvarez, Natalia Irishina, Miguel Moscoso and Rossmary Villegas for
their collaboration on the exciting topic of level set methods in image reconstruction,
and for providing figures which have been included in this chapter. He thanks the Span-
ish Ministerio de Educacion y Ciencia (Grants FIS--E and FIS-),

Level Set Methods for Structural Inversion and Image Reconstruction 

the European Union (Grant FP-), the French CNRS and Univ. Paris Sud , and
the Research Councils UK for their support of some of the work which has been pre-
sented in this chapter. DL thanks Jean Cea for having introduced him to the fascinating
world of shape optimal design, Fadil Santosa for his contribution to his understand-
ing of the linkage between shape optimal design and level set evolutions, and Jean-Paul
Zolésio for his precious help on both topics, plus his many insights on topological
derivatives.
References and Further Reading
. Abascal JFPJ, Lambert M, Lesselier D, Dorn O
() -D eddy-currentimagingof metal tubes
by gradient-based, controlled evolution of level
sets. IEEE Trans Magn :–
. Alexandrov O, Santosa F () A topology pre-
serving level set method for shape optimization.
J Comput Phys :–
. Allaire G, Jouve F, Toader A-M () Struc-
tural optimization using sensitivity analysis
and a level-set method. J Comput Phys :
–
. Alvarez D, Dorn O, Irishina N, Moscoso M
() Crack detection using a level set strategy.
J Comput Phys :–
. Ammari H, Calmon P, Iakovleva E ()
Direct elastic imaging of a small inclusion.
SIAM J Imaging Sci :–
. Ammari H, Kang H () Reconstruction
of
small
inhomogeneities
from
boundary
measurements. Lecture notes in mathematics,
vol . Springer, Berlin
. Amstutz S, Andrä H () A new algorithm for
topologyoptimization using a level-set method.
J Comput Phys :–
. Ascher UM, Huang H, van den Doel K ()
Artificial time integration. BIT Numer Math
:–
. Bal G, Ren K () Reconstruction of singu-
lar surfaces by shape sensitivity analysis and
level set method. Math Model Meth Appl Sci
:–
. Ben Hadj Miled MK, Miller EL () A
projection-based level-set approach to enhance
conductivity anomaly reconstruction in elec-
trical resistance tomography. Inverse Prob
:–
. Ben Ameur H, Burger M, Hackl B ()
Level set methods for geometric inverse prob-
lems in linear elasticity. Inverse Prob :
–
. Benedetti M, Lesselier D, Lambert M, Massa A
() Multiple-shape reconstruction by means
of mutliregion level sets. IEEE Trans Geosci
Remote Sens :–
. Berg JM, Holmstrom K () On parame-
ter estimation using level sets. SIAM J Control
Optim :–
. Berre I, Lien M, Mannseth T () A level
set corrector to an adaptive multiscale per-
meability
prediction.
Comput
Geosci
:
–
. Bonnet M, Guzina BB () Sounding of finite
solid bodies by way of topological derivative. Int
J Numer Methods Eng :–
. Burger M () A level set method for inverse
problems. Inverse Prob :–
. Burger M, Osher S () A survey on level
set methods for inverse problems and optimal
design. Eur J Appl Math :–
. Burger M () A framework for the construc-
tion of level set methods for shape optimiza-
tion and reconstruction. Inter Free Bound :
–
. Burger M () Levenberg-Marquardt level
set methods for inverse obstacle problems.
Inverse Prob :–
. Burger M, Hackl B, Ring W () Incorporat-
ing topological derivatives into level set meth-
ods. J Comput Phys :–
. Carpio A, Rapún M-L () Solving inho-
mogeneous inverse problems by topological
derivative methods. Inverse Prob :


Level Set Methods for Structural Inversion and Image Reconstruction
. Céa J, Gioan A, Michel J () Quelques résul-
tats sur l’identification de domains. Calcolo
(–):–
. Céa J, Haug EJ (eds) Optimization of dis-
tributed parameter structures. Sijhoff & Noord-
hoff, Alphen aan den Rijn
. Céa J, Garreau S, Guillaume P, Masmoudi M
() The shape and topological optimiza-
tions connection. Comput Meth Appl Mech
Eng :–
. Chan TF, Vese LA () Active contours with-
out edges. IEEE Trans Image Process :–

. Chan TF, Tai X-C () Level set and total vari-
ation regularization for elliptic inverse prob-
lems with discontinuous coefficients. J Comput
Phys :–
. Chung ET, Chan TF, Tai XC () Electrical
impedance tomography using level set repre-
sentation and total variational regularization. J
Comput Phys :–
. DeCezaro A, Leitão A, Tai X-C () On
multiple level-set regularization methods for
inverse problems. Inverse Prob :
. Delfour MC, Zolésio J-P () Shape sensitiv-
ity analysis via min max differentiability. SIAM
J Control Optim :–
. Delfour MC, Zolésio J-P () Shapes and
geometries: analysis, differential calculus and
optimization (SIAM advances in design and
control). SIAM, Philadelphia
. Dorn O, Lesselier D () Level set meth-
ods for inverse scattering. Inverse Prob :R–
R. doi:./-///R
. Dorn O, Lesselier D () Level set meth-
ods for inverse scattering - some recent
developments.
Inverse
Prob
:.
doi:./-///
. Dorn O, Lesselier D Level set techniques
for structural inversion in medical imaging.
In: Deformable models. Springer, New York,
pp –
. Dorn O, Villegas R () History matching of
petroleum reservoirs using a level set technique.
Inverse Prob :
. Dorn
O,
Miller
E,
Rappaport
C
()
A
shape
reconstruction
method
for electromagnetic tomography using adjoint
fields and level sets. Inverse Prob :–
. Duflot M () A study of the representation
of cracks with level sets. Int J Numer Methods
Eng :–
. Engl HW, Hanke M, Neubauer A () Reg-
ularization of inverse problems (mathemat-
ics and its applications), vol . Kluwer,
Dordrecht
. Fang W () Multi-phase permittivity recon-
struction in electrical capacitance tomography
by level set methods. Inverse Prob Sci Eng
:–
. Feijóo RA, Novotny AA, Taroco E, Padra C
() The topological derivative for the Pois-
son problem. Math Model Meth Appl Sci :
–
. Feijóo GR () A new method in inverse
scattering based on the topological derivative.
Inverse Prob :–
. Feng H, Karl WC, Castanon DA () A
curve
evolution
approach to
object-based
tomographic reconstruction. IEEE Trans Image
Process :–
. Ferrayé R, Dauvignac JY, Pichot C () An
inverse scattering method based on contour
deformations by means of a level set method
using frequency hopping technique. IEEE Trans
Antennas Propagat :–
. Frühauf F, Scherzer O, Leitao A () Anal-
ysis of regularization methods for the solution
of ill-posed problems involving discontinuous
operators. SIAM J Numer Anal :–
. González-Rodriguez P, Kindelan M, Moscoso
M, Dorn O () History matching prob-
lem in reservoir engineering using the propa-
gation back-propagation method. Inverse Prob
:–
. Guzina BB, Bonnet M () Small-inclusion
asymptotic for inverse problems in acoustics.
Inverse Prob :
. Haber E () A multilevel level-set method
for optimizing eigenvalues in shape design
problems. J Comput Phys :–
. Hackl B () Methods for reliable topol-
ogy changes for perimeter-regularized geomet-
ric inverse problems. SIAM J Numer Anal :
–

Level Set Methods for Structural Inversion and Image Reconstruction 

. Harabetian E, Osher S () Regularization of
ill-posed problems via the level set approach.
SIAM J Appl Math :–
. Hettlich F () Fréchet derivatives in inverse
obstacle scattering. Inverse Prob :–
. Hintermüller M, Ring W () A second order
shape optimization approach for image seg-
mentation. SIAM J Appl Math :–
. Hou S, Solna K, Zhao H () Imaging of loca-
tion and geometry for extended targets using
the response matrix. J Comput Phys :–
. Irishina N, Alvarez D, Dorn O, Moscoso
M () Structural level set inversion for
microwave breast screening.
Inverse Prob
:
. Ito K, Kunisch K, Li Z () Level-setapproach
to an inverse interface problem. Inverse Prob
:–
. Ito K () Level set methods for variational
problems and application. In: Desch W, Kap-
pel F, Kunisch K (eds) Control and estimation
of distributed parameter systems. Birkhäuser,
Basel, pp –
. Jacob M, Bresler Y, Toronov V, Zhang X, Webb
A () Level set algorithm for the reconstruc-
tion of functional activation in near-infrared
spectroscopic imaging. J Biomed Opt :
. Kao CY, Osher S, Yablonovitch E () Maxi-
mizing band gaps in two-dimentional photonic
crystals by using level set methods. Appl Phys B
:–
. Klann E, Ramlau R, Ring W () A
Mumford-Shah
level-set approach for
the
inversion and segmentation of SPECT/CT data.
J Comput Phys :–
. Kortschak B, Brandstätter B () A FEM-
BEM approach using level-sets in electri-
cal capacitance tomography. COMPEL :
–
. Leitão A, Alves MM () On level set type
methods for elliptic Cauchy problems. Inverse
Prob :–
. Leitao A, Scherzer O () On the relation
between constraint regularization, level sets and
shape optimization. Inverse Prob :L–L
. Lie J, Lysaker M, Tai X () A variant of
the level set method and applications to image
segmentation. Math Comput :–
. Lie J, Lysaker M, Tai X () A binary level set
method and some applications for Mumford-
Shah image segmentation. IEEE Trans Image
Process :–
. Litman A, Lesselier D, Santosa D () Recon-
struction of a two-dimensional binary obstacle
by controlled evolution of a level-set. Inverse
Prob :–
. Litman A () Reconstruction by level sets of
n-ary scattering obstacles. Inverse Prob :S–
S
. Liu K, Yang X, Liu D et al () Spec-
trally resolved three-dimensional biolumines-
cence tomography with a level-set strategy. J
Opt Soc Am A :–
. Lu Z, Robinson BA () Parameter identifi-
cation using the level set method. Geophys Res
Lett :L
. Luo Z, Tong LY, Luo JZ et al () Design of
piezoelectric actuators using a multiphase level
set method of piecewise constants. J Comput
Phys :–
. Lysaker M, Chan TF, Li H, Tai X-C ()
Level
set
method
for
positron
emission
tomography. Int J Biomed Imaging :.
doi:.//
. Masmoudi M, Pommier J, Samet B ()
The topological asymptotic expansion for the
Maxwell equations and some applications.
Inverse Prob :–
. Mumford D, Shah J () Optimal approxima-
tion by piecewise smooth functions and associ-
ated variational problems. Commun Pure Appl
Math :–
. Natterer F, Wübbeling F () Mathematical
methods in image reconstruction (monographs
on mathematical modeling and computation),
vol . SIAM, Philadelphia
. Nielsen LK, Li H, Tai XC, Aanonsen SI, Espedal
M () Reservoir description using a binary
level set model. Comput Visual Sci ():–
. Novotny AA, Feijóo RA, Taroco E, Padra C
() Topological sensitivity analysis. Comput
Meth Appl Mech Eng :–
. Osher S, Sethian JA () Fronts propagating
with curvature-dependent speed: algorithms
basedonHamilton-Jacobi formulations.J Com-
put Phys :–


Level Set Methods for Structural Inversion and Image Reconstruction
. Osher S, Santosa F () Level set methods
for optimisation problems involving geometry
and constraints I. Frequencies of a two-density
inhomogeneous drum. J Comput Phys :
–
. Osher S, Fedkiw R () Level set meth-
ods and dynamic implicit surfaces. Springer,
New York
. Park WK, Lesselier D () Reconstruction of
thin electromagnetic inclusions by a level set
method. Inverse Prob :
. Ramananjaona C, Lambert M, Lesselier D,
Zolésio J-P () Shape reconstruction of
buried obstacles by controlled evolution of
a level set: from a min-max formulation
to numerical experimentation. Inverse Prob
:–
. Ramananjaona C, Lambert M, Lesselier D,
Zolésio J-P () On novel developments of
controlled evolution of level sets in the field of
inverse shape problems. Radio Sci :
. Ramlau R, Ring W () A Mumford-Shah
level-set approach for the inversion and seg-
mentation of X-ray tomography data. J Comput
Phys :–
. Rocha de Faria J, Novotny AA, Feijóo RA,
Taroco E () First- and second-order
topological sensitivity analysis for inclusions.
Inverse Prob Sci Eng :–
. Santosa F () A level set approach for inverse
problems involving obstacles. ESAIM Contr
Optim Calc Var :–
. Schumacher A, Kobolev VV, Eschenauer HA
() Bubble method for topology and shape
optimization of structures. J Struct Optim
:–
. Schweiger
M,
Arridge
SR,
Dorn
O,
Zacharopoulos A, Kolehmainen V ()
Reconstructing absorption and diffusion shape
profiles in optical tomography using a level set
technique. Opt Lett :–
. Sethian JA () Level set methods and fast
marching methods, nd edn. Cambridge Uni-
versity Press, Cambridge
. Sokolowski J, Zochowski A () On topolog-
ical derivative in shape optimization. SIAM J
Control Optim :–
. Sokolowski J, Zolésio J-P () Introduction
to shape optimization: shape sensitivity analy-
sis (springer series in computational mathemat-
ics), vol . Springer, Berlin
. Soleimani M () Level-set method applied
to
magnetic
induction
tomography
using
experimental data. Res Nondestr Eval ():
–
. Soleimani M, Lionheart WRB, Dorn O ()
Level set reconstruction of conductivity and
permittivity from boundary electrical measure-
ments using experimental data. Inverse Prob Sci
Eng :–
. Soleimani M,DornO,LionheartWRB() A
narrowband level set method applied to EIT in
brain for cryosurgery monitoring. IEEE Trans
Biomed Eng :–
. Suri JS, Liu K, Singh S, Laxminarayan SN, Zeng
X, Reden L () Shape recovery algorithms
using level sets in D/D medical imagery: a
state-of-the-art review. IEEE Trans Inf Technol
Biomed :–
. Tai X-C, Chan TF () A survey on multiple
level set methods with applications for identify-
ing piecewise constant functions. Int J Numer
Anal Model :–
. van den Doel K et al () Dynamic level
set regularization for large distributed param-
eter estimation problems. Inverse Prob :
–
. Van den Doel K, Ascher UM () On level set
regularization for highly ill-posed distributed
parameter estimation problems. J Comput Phys
:–
. Vese LA, Chan TF () A multiphase level
set framework for image segmentation using
the Mumford-Shah model. Int J Comput Vision
:–
. Wang M, Wang X () Color level sets:
a multi-phase method for structural topology
optimization with multiple materials. Comput
Meth Appl Mech Eng :–
. Wei P, Wang MY () Piecewise constant
level set method for structural topology opti-
mization. Int J Numer Methods Eng ():
–
. Ye JC, Bresler Y, Moulin P () A self-
referencing level-set method for image recon-
struction from sparse Fourier samples. Int J
Comput Vision :–
. Zhao H-K, Chan T, Merriman B, Osher S ()
A variational level set approach to multiphase
motion. J Comput Phys :–

Expansion Methods
Habib Ammari ⋅Hyeonbae Kang
.
Introduction.....................................................................
.
Electrical Impedance Tomography for Anomaly Detection...................
..
Physical Principles......................................................................
..
Mathematical Model....................................................................
..
Asymptotic Analysis of the Voltage Perturbations.................................
..
Numerical Methods for Anomaly Detection.......................................
...
Detection of a Single Anomaly: A Projection-Type Algorithm..................
...Detection of Multiple Anomalies: A MUSIC-Type Algorithm...................
..
Bibliography and Open Questions...................................................
.
Ultrasound Imaging for Anomaly Detection...................................
..
Physical Principles......................................................................
..
Asymptotic Formulas in the Frequency Domain...................................
..
Asymptotic Formulas in the Time Domain.........................................
..
Numerical Methods.....................................................................
...
MUSIC-Type Imaging at a Single Frequency........................................
...Backpropagation-Type Imaging at a Single Frequency............................
...Kirchhoff-Type Imaging Using a Broad Range of Frequencies...................
...Time-Reversal Imaging................................................................
..
Bibliography and Open Questions...................................................
.
Infrared Thermal Imaging......................................................
..
Physical Principles......................................................................
..
Asymptotic Analysis of Temperature Perturbations...............................
..
Numerical Methods....................................................................
...
Detection of a Single Anomaly........................................................
...Detection of Multiple Anomalies: A MUSIC-Type Algorithm...................
..
Bibliography and Open Questions....................................................
.
Impediography...................................................................
..
Physical Principles......................................................................
..
Mathematical Model....................................................................
..
Substitution Algorithm.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Expansion Methods
..
Bibliography and Open Questions...................................................
.
Magneto-Acoustic Imaging.....................................................
..
Magneto-Acousto-Electrical Tomography..........................................
...
Physical Principles......................................................................
...
Mathematical Model....................................................................
...
Substitution Algorithm.................................................................
..
Magneto-Acoustic Imaging with Magnetic Induction.............................
...
Physical Principles.......................................................................
...Mathematical Model....................................................................
...Reconstruction Algorithm.............................................................
..
Bibliography and Open Questions...................................................
.
Magnetic Resonance Elastography.............................................
..
Physical Principles......................................................................
..
Mathematical Model...................................................................
..
Asymptotic Analysis of Displacement Fields.......................................
..
Numerical Methods....................................................................
..
Bibliography and Open Questions...................................................
.
Photo-Acoustic Imaging of Small Absorbers...................................
..
Physical Principles......................................................................
..
Mathematical Model....................................................................
..
Reconstruction Algorithms............................................................
...
Determination of Location.............................................................
...Estimation of Absorbing Energy......................................................
...
Reconstruction of the Absorption Coefficient......................................
..
Bibliography and Open Questions...................................................
.
Conclusion.......................................................................
.
Cross-References.................................................................

Expansion Methods 

Abstract: The aim of this chapter is to review recent developments in the mathemati-
cal and numerical modeling of anomaly detection and multi-physics biomedical imaging.
Expansion methods are designed for anomaly detection. They provide robust and accu-
rate reconstruction of the location and of some geometric features of the anomalies, even
with moderately noisy data. Asymptotic analysis of the measured data in terms of the size
of the unknown anomalies plays a key role in characterizing all the information about
the anomaly that can be stably reconstructed from the measured data. In multi-physics
imaging approaches, different physical types of waves are combined into one tomographic
process to alleviate deficiencies of each separate type of waves, while combining their
strengths. Muti-physics systems are capable of high-resolution and high-contrast imaging.
Asymptotic analysis plays a key role in multi-physics modalities as well.
.
Introduction
Inverse problems in medical imaging are in their most general form ill-posed. They lit-
erally have no solution [, ]. If, however, in advance one has additional structural
information or can supply missing information, then one may be able to determine spe-
cific features about what one wishes to image with a satisfactory resolution and accuracy.
One such type of information can be that the imaging problem is to find unknown small
anomalies with significantly different parameters from those of the surrounding medium.
These anomalies may represent potential tumors at an early stage.
Over the last few years, an expansion technique has been developed for the imaging
of such anomalies. It has proven useful in dealing with many medical imaging problems.
The method relies on deriving asymptotics. Such asymptotics have been investigated in
the case of the conductivity equation, the elasticity equation, the Helmholtz equation, the
Maxwell system, the wave equation, the heat equation, and the (modified) Stokes system.
A remarkable feature of this method is that it allows a stable and accurate reconstruction of
the location and of some geometric features of the anomalies, even with moderately noisy
data. This is because the method reduces the set of admissible solutions and the number
of unknowns. It can be seen as a kind of regularization in comparison with (nonlinear)
iterative approaches.
Another promising technique for efficient imaging is to combine into one tomographic
process different physical types of waves. Doing so, one alleviates deficiencies of each sep-
arate type of waves, while combining their strengths. Again, asymptotic analysis plays a
key role in the design of robust and efficient imaging techniques based on this concept of
multi-waves. In the last decade or so, work on multi-physics imaging in biomedical applica-
tions has come a long way. The motivation is to achieve high-resolution and high-contrast
imaging.


Expansion Methods
The objective of this chapter is threefold: () to provide asymptotic expansions for both
internal and boundary perturbations that are due to the presence of small anomalies, ()
to apply those asymptotic formulas for the purpose of identifying the location and certain
properties of the shape of the anomalies, () to design efficient inversion algorithms in
multi-physics modalities.
Applications of the anomaly detection and multi-physics approaches in medical imag-
ing are described in some detail. In particular, the use of asymptotic analysis to improve
a multitude of emerging imaging techniques is highlighted. These imaging modalities
include electrical impedance tomography, ultrasound imaging, infrared thermography,
magnetic resonance elastography, impediography, magneto-acousto-electrical tomogra-
phy, magneto-acoustic tomography with magnetic induction, and photo-acoustic imaging.
They can be divided into three groups: () those using boundary or scattering measure-
ments such as electrical impedance tomography, ultrasound, and infrared tomographies;
() those using internal measurements such as magnetic resonance elastography; () those
using boundary measurements obtained from internal perturbations of the medium such
as impediography and magneto-acoustic imaging.
As it will be shown in this chapter, modalities from group () can only be used for
anomaly detection, while those from groups () and () can provide a stable reconstruction
of a distribution of physical parameters.
.
Electrical Impedance Tomography for Anomaly
Detection
..
Physical Principles
Electrical impedance tomography uses low-frequency electrical current to probe a body;
the method is sensitive to changes in electrical conductivity. By injecting known amounts
of current and measuring the resulting electrical potential field at points on the boundary
of the body, it is possible to “invert” such data to determine the conductivity or resistivity
of the region of the body probed by the currents. This method can also be used in principle
to image changes in dielectric constant at higher frequencies, which is why the method is
often called “impedance” tomography rather than “conductivity” or “resistivity” tomog-
raphy. However, the aspect of the method that is most fully developed to date is the
imaging of conductivity/resistivity. Potential applications of electrical impedance tomog-
raphy include determination of cardiac output, monitoring for pulmonary edema, and in
particular screening for breast cancer.
Recently, a commercial system called TS(Mirabel Medical Systems Inc., Austin,
TX) has been released for adjunctive clinical uses with X-ray mammography in the diag-
nostic of breast cancer. The mathematical model of the TransScan can be viewed as a

Expansion Methods 

realistic or practical version of the general electrical impedance system. In the TransScan,
a patient holds a metallic cylindrical reference electrode, through which a constant volt-
age of –.V, with frequencies spanning Hz–KHz, is applied. A scanning probe
with a planar array of electrodes, kept at ground potential, is placed on the breast. The
voltage difference between the hand and the probe induces a current flow through the
breast, from which information about the impedance distribution in the breast can be
extracted.
The use of asymptotic analysis yields a rigorous mathematical framework for the
TransScan. See [, ] for a detailed study of this electrical impedance tomography
system.
..
Mathematical Model
Let Ω be a smooth bounded domain in Rd, d = or and let
x denote the outward
normal to ∂Ω at x. Suppose that the conductivity of Ω is equal to . Let D denote a smooth
anomaly inside Ω with conductivity < k ≠< +∞. The voltage potential in the presence
of the set D of conductivity anomalies is denoted by u. It is the solution to the conductivity
problem
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
∇⋅(χ(Ω / D) + kχ(D))∇u = 
in Ω,
∂u
∂∣
∂Ω
= g
(g ∈L(∂Ω),∫∂Ω g dσ = ),
∫∂Ω u dσ = ,
(.)
where χ(D) is the characteristic function of D.
The background voltage potential U in the absence of any anomaly satisfies
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
ΔU = 
in Ω,
∂U
∂∣
∂Ω
= g,
∫∂Ω U dσ = .
(.)
Let N(x, z) be the Neumann function for −Δ in Ω corresponding to a Dirac mass at z.
That is, N is the solution to
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
−ΔxN(x, z) = δz
in Ω,
∂N
∂x
∣
∂Ω = −

∣∂Ω∣,∫∂Ω N(x, z) dσ(x) = 
for z ∈Ω.
(.)


Expansion Methods
Note that the Neumann function N(x, z) is defined as a function of x ∈Ω for each
fixed z ∈Ω.
For B a smooth bounded domain in Rd and < k ≠< +∞a conductivity parameter,
let ˆv = ˆv(B, k) be the solution to
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Δˆv = 
in Rd / B,
Δˆv = 
in B,
ˆv∣−−ˆv∣+ = 
on ∂B,
k ∂ˆv
∂∣
−
−∂ˆv
∂∣
+
= 
on ∂B,
ˆv(ξ) −ξ →
as ∣ξ∣→+∞.
(.)
Here one denotes
v∣±(ξ) := lim
t→+ v(ξ ± t
ξ),
ξ ∈∂B,
and
∂v
∂ξ
∣
±
(ξ) := lim
t→+⟨∇v(ξ ± t
ξ),
ξ⟩,
ξ ∈∂B,
if the limits exist, where
ξ is the outward unit normal to ∂B at ξ, and ⟨,⟩is the scalar
product in Rd. For ease of notation the dot will be sometimes used for the scalar product
in Rd.
Recall that ˆv plays the role of the first-order corrector in the theory of homogeniza-
tion [].
..
Asymptotic Analysis of the Voltage Perturbations
In this section, an asymptotic expansion of the voltage potentials in the presence of a dia-
metrically small anomaly with conductivity different from the background conductivity is
provided.
The following theorem gives asymptotic formulas for both boundary and internal per-
turbations of the voltage potential that are due to the presence of a conductivity anomaly.
Theorem (Voltage perturbations)
Suppose that D = δB + z, δ being the characteristic
size of D, and let u be the solution of (> .), where < k ≠< +∞. Denote by U the
background solution, that is, the solution of (> .).
(i) The following asymptotic expansion of the voltage potential on ∂Ω holds for d = ,:
u(x) ≈U(x) −δd∇U(z)M(k, B)∇zN(x, z).
(.)

Expansion Methods 

Here N(x, z) is the Neumann function, that is, the solution to (> .), and M(k, B) =
(mpq)d
p,q=is the polarization tensor given by
M(k, B) := (k −)∫B ∇ˆv(ξ) dξ,
(.)
where ˆv is the solution to (> .).
(ii) Let w be a smooth harmonic function in Ω. The weighted boundary measurements
Iw[U] satisfies
Iw[U] := ∫∂Ω(u −U)(x)∂w
∂(x) dσ(x) ≈−δd∇U(z) ⋅M(k, B)∇w(z).
(.)
(iii) The following inner asymptotic formula holds:
u(x) ≈U(z) + δˆv (x −z
δ
) ⋅∇U(z)
for x near z.
(.)
The inner asymptotic expansion (> .) uniquely characterizes the shape and the con-
ductivity of the anomaly. In fact, suppose for two Lipschitz domains B and B′ and two
conductivities k and k′ that ˆv(B, k) = ˆv(B′, k′) in a domain englobing B and B′, then using
the jump conditions satisfied by ˆv(B, k) and ˆv(B′, k′) one can easily prove that B = B′ and
k = k′.
The asymptotic expansion (> .) expresses the fact that the conductivity anomaly
can be modeled by a dipole far away from z. It does not hold uniformly in Ω. It shows
that, from an imaging point of view, the location z and the polarization tensor M of the
anomaly are the only quantities that can be determined from boundary measurements of
the voltage potential, assuming that the noise level is of order δd+. It is then important
to precisely characterize the polarization tensor and derive some of its properties, such as
symmetry, positivity, and isoperimetric inequalities satisfied by its elements, in order to
develop efficient algorithms for reconstructing conductivity anomalies of small volume.
Some important properties of the polarization tensor are listed in the next theorem.
Theorem (Properties of the polarization tensor)
For < k ≠< +∞, let M =
M(k, B) = (mpq)d
p,q=be the polarization tensor associated with the bounded domain B
in Rd and the conductivity k. Then
(i) M is symmetric.
(ii) If k > , then M is positive definite, and it is negative definite if < k < .
(iii) The following isoperimetric inequalities for the polarization tensor
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩

k −trace(M) ≤(d −+ 
k )∣B∣,
(k −)trace(M−) ≤d −+ k
∣B∣
,
(.)
hold, where trace denotes the trace of a matrix and ∣B∣is the volume of B.


Expansion Methods
The polarization tensor M can be explicitly computed for disks and ellipses in the plane
and balls and ellipsoids in three-dimensional space. See [, pp. –]. The formula of the
polarization tensor for ellipses will be useful here. Let B be an ellipse whose semi-axes are
on the x- and x-axes and of length a and b, respectively. Then, M(k, B) takes the form
M(k, B) = (k −)∣B∣
⎛
⎜⎜⎜
⎝
a + b
a + kb


a + b
b + ka
⎞
⎟⎟⎟
⎠
.
(.)
Formula (> .) shows that from boundary measurements one can always represent and
visualize an arbitrary-shaped anomaly by means of an equivalent ellipse of center z with
the same polarization tensor. Further, it is impossible to extract the conductivity from the
polarization tensor. The information contained in the polarization tensor is a mixture of
the conductivity and the volume. A small anomaly with high conductivity and a larger
anomaly with lower conductivity can have the same polarization tensor.
The bounds (> .) are known as the Hashin–Shtrikman bounds. By making use of
these bounds, size and thickness estimations of B can be obtained. An inclusion whose
trace of the associated polarization tensor is close to the upper bound must be infinitely
thin [].
..
Numerical Methods for Anomaly Detection
In this section, one applies the asymptotic formula (> .) for the purpose of identify-
ing the location and certain properties of the shape of the conductivity anomalies. Two
simple fundamental algorithms that take advantage of the smallness of the anomalies are
singled out: projection-type algorithms and multiple signal classification (MUSIC)-type
algorithms. These algorithms are fast, stable, and efficient.
...
Detection of a Single Anomaly: A Projection-Type Algorithm
One briefly discusses a simple algorithm for detecting a single anomaly. The reader can
refer to [, ] for further details. The projection-type location search algorithm makes
use of constant current sources. One wants to apply a special type of current that makes
∇U constant in D. The injection current g = a ⋅
for a fixed unit vector a ∈Rd yields
∇U = a in Ω.
Assume for the sake of simplicity that d = and D is a disk. Set
w(y) = −(/π)log ∣x −y∣
for x ∈R/Ω, y ∈Ω .
Since w is harmonic in Ω, then from (> .) to (> .), it follows that
Iw[U] ≈(k −)∣D∣
π(k + )
(x −z) ⋅a
∣x −z∣
,
x ∈R/Ω .
(.)

Expansion Methods 

The first step for the reconstruction procedure is to locate the anomaly. The location
search algorithm is as follows. Take two observation lines Σand Σcontained in R/Ω
given by
Σ:= a line parallel to a,
Σ:= a line normal to a.
Find two points Pi ∈Σi, i = ,, so that
Iw[U](P) = ,
Iw[U](P) = max
x∈Σ∣Iw[U](x)∣.
From (> .), one can see that the intersecting point P of the two lines
Π(P) := {x ∣a ⋅(x −P) = },
(.)
Π(P) := {x ∣(x −P) is parallel to a}
(.)
is close to the center z of the anomaly D: ∣P −z∣= O(δ).
Once one locates the anomaly, the factor ∣D∣(k −)/(k + ) can be estimated. As it has
been said before, this information is a mixture of the conductivity and the volume. A small
anomaly with high conductivity and larger anomaly with lower conductivity can have the
same polarization tensor.
An arbitrary-shaped anomaly can be represented and visualized by means of an ellipse
or an ellipsoid with the same polarization tensor. See > Fig. -.
We refer the reader to [] for a discussion on the limits of the applicability of the
projection-type location search algorithm and the derivation of a second efficient method,
called the effective dipole method.
−10
−5
0
5
10
−10
−5
0
5
10
→
2
4
−6
−4
⊡Fig. -
Detection of the location and the polarization tensor of a small arbitrary-shaped anomaly
by a projection-type algorithm. The shape of the anomaly is approximated by an ellipse
with the same polarization tensor


Expansion Methods
...
Detection of Multiple Anomalies: A MUSIC-Type Algorithm
Consider m well-separated anomalies Ds = δBs +zs (these are a fixed distance apart), with
conductivities ks, s = , . . . , m. Suppose for the sake of simplicity that all the domains Bs
are disks. Let yl ∈R/Ω for l = , . . . , n denote the source points. Set
Uyl = wyl := −(/π)log ∣x −yl∣
for x ∈Ω,
l = , . . . , n.
The MUSIC-type location search algorithm for detecting multiple anomalies is as follows.
For n ∈N sufficiently large, define the response matrix A = (All′)n
l,l′=by
All′ = Iwyl [Uyl′] := ∫∂Ω(u −Uyl′)(x)∂wyl
∂
(x) dσ(x) .
Expansion (> .) yields
All′ ≈−
m
∑
s=
(ks −)∣Ds∣
ks + 
∇Uyl′(zs)∇Uyl (zs) .
Introduce
g(x) = (Uy(x), . . .,Uyn(x))
∗,
where v∗denotes the transpose of the vector v.
Lemma (MUSIC characterization of the range of the response matrix)
There exists
n> dm such that for any n > nthe following characterization of the location of the
anomalies in terms of the range of the matrix A holds:
g(x) ∈Range(A) if and only if x ∈{z, . . . , zm} .
(.)
The MUSIC-type algorithm to determine the location of the anomalies is as follows.
Let Pnoise = I −P, where P is the orthogonal projection onto the range of A. Given any
point x ∈Ω, form the vector g(x). The point x coincides with the location of an anomaly
if and only if Pnoiseg(x) = . Thus one can form an image of the anomalies by plotting, at
each point x, the cost function
WMU(x) =

∣∣Pnoiseg(x)∣∣.
The resulting plot will have large peaks at the locations of the anomalies.
Once one locates the anomalies, the factors ∣Ds∣(ks −)/(ks + ) can be estimated from
the significant singular values of A.
..
Bibliography and Open Questions
Part (i) in Theorem was proven in [, , ]. The proof in [] is based on a
decomposition formula of the solution into a harmonic part and a refraction part first

Expansion Methods 

derived in []. Part (iii) is from []. The Hashin–Shtrikman bounds for the polarization
tensor were proved in [, ]. The projection algorithm was introduced in [, ]. The
MUSIC algorithm was originally developed for source separation in signal theory [].
The MUSIC-type algorithm for locating small conductivity anomalies from the response
matrix was first developed in []. The strong relation between MUSIC and linear sampling
methods was clarified in []. The results of this section can be generalized to the detection
of anisotropic anomalies [].
As it has been said before, it is impossible to extract separately from the detected
polarization tensor information about the material property and the size of the anomaly.
However, if the measurement system is very sensitive, then making use of higher-order
polarization tensors yields such information. See [] for the notion of the higher-order
polarization tensors.
One of the most challenging problems in anomaly detection using electrical impedance
tomography is that in practical measurements, one usually lacks exact knowledge of the
boundary of the background domain. Because of this, the numerical reconstruction from
the measured data is done using a model domain that represents the best guess for the true
domain. However, it has been noticed that an inaccurate model of the boundary causes
severe errors for the reconstructions. An elegant and original solution toward eliminating
the error caused by an incorrectly modeled boundary has been proposed and imple-
mented numerically in []. As nicely shown in [], another promising approach is to use
multifrequency data. The anomaly can be detected from a weighted frequency difference of
the measured boundary voltage perturbations. Moreover, this method eliminates the need
for numerically simulated background measurements at the absence of the conductivity
anomaly. See [,].
.
Ultrasound Imaging for Anomaly Detection
..
Physical Principles
Ultrasound imaging is a noninvasive, easily portable, and relatively inexpensive diagnostic
modality which finds extensive clinical use. The major applications of ultrasound include
many aspects of obstetrics and gynecology involving the assessment of fetal health, intra-
abdominal imaging of the liver, kidney, and the detection of compromised blood flow in
veins and arteries.
Operating typically at frequencies between and MHz, ultrasound imaging pro-
duces images via the backscattering of mechanical energy from interfaces between tissues
and small structures within tissue. It has high spatial resolution, particularly at high fre-
quencies, and involves no ionizing radiation. The weakness of the technique includes
the relatively poor soft-tissue contrast and the fact that gas and bone impede the pas-
sage of ultrasound waves, meaning that certain organs cannot easily be imaged. However,


Expansion Methods
ultrasound imaging is a valuable technique for anomaly detection. It can be done in the
time domain and the frequency domain.
Mathematical models for acoustical soundings of biological media involve the
Helmholtz equation in the frequency domain and the scalar wave equation in the time
domain.
..
Asymptotic Formulas in the Frequency Domain
Let k and ρ be positive constants. With the notation of > .., ρ is the compressibility of
the anomaly D and k is its volumetric mass density. The scalar acoustic pressure u generated
by the Neumann data g in the presence of the anomaly D is the solution to the Helmholtz
equation:
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∇⋅(χ(Ω/D) + kχ(D))∇u + ω(χ(Ω/D) + ρχ(D))u = 
in Ω,
∂u
∂
= g
on ∂Ω,
(.)
while the background solution U satisfies
⎧⎪⎪⎪⎨⎪⎪⎪⎩
ΔU + ωU = 
in Ω,
∂U
∂
= g
on ∂Ω.
(.)
Here, ω is the operating frequency. A relevant boundary data g is the normal derivative
of a plane wave eiωx⋅θ, with the wavelength λ := π/ω, traveling in the direction of the unit
vector θ.
Introduce the Neumann function for −(Δ + ω) in Ω corresponding to a Dirac mass
at z. That is, Nω is the solution to
⎧⎪⎪⎪⎨⎪⎪⎪⎩
−(Δx + ω)Nω(x, z) = δz
in Ω,
∂N
∂x
∣∂Ω = 
on ∂Ω.
(.)
Assuming that ωis not an eigenvalue for the operator −Δ in L(Ω) with homo-
geneous Neumann boundary conditions, one can prove, using the theory of relatively
compact operators, existence and uniqueness of a solution to (> .) at least for δ small
enough []. Moreover, the following asymptotic formula holds.
Theorem (Pressure perturbations)
Let u be the solution of (> .) and let U be the
background solution. Suppose that D = δB + z, with < (k, ρ) ≠(,) < +∞. Suppose that
ωδ ≪.

Expansion Methods 

(i) For any x ∈∂Ω,
u(x) ≈U(x) −δd (∇U(z) ⋅M(k, B)∇zNω(x, z)
+ω(ρ −)∣B∣U(z)Nω(x, z)),
(.)
where M(k, B) is the polarization tensor associated with B and k.
(ii) Let w be a smooth function such that (Δ + ω)w = in Ω. The weighted boundary
measurements Iw[U, ω] satisfies
Iw[U, ω] := ∫∂Ω(u −U)(x)∂w
∂(x) dσ(x)
≈−δd (∇U(z) ⋅M(k, B)∇w(z) + ω(ρ −)∣B∣U(z)w(z)).
(.)
(iii) The following inner asymptotic formula holds:
u(x) ≈U(z) + δˆv (x −z
δ
) ⋅∇U(z)
for x near z,
(.)
where ˆv is the solution to (> .).
Compared to the conductivity equation, the only extra difficulty in establishing asymp-
totic formulas for the Helmholtz equation (> .) as the size of the acoustic anomaly goes
to zero is that the equations inside and outside the anomaly are not the same.
..
Asymptotic Formulas in the Time Domain
Suppose that ρ = . Consider the initial boundary value problem for the (scalar) wave
equation
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∂
tu −∇⋅(χ(Ω/D) + kχ(D))∇u = 
in ΩT,
u(x,) = u(x),
∂tu(x,) = u(x)
for x ∈Ω,
∂u
∂
= g
on ∂ΩT,
(.)
where T < +∞is a final observation time, ΩT = Ω×], T[, and ∂ΩT = ∂Ω×], T[. The
initial data u,u∈C∞(Ω), and the Neumann boundary data g ∈C∞(, T;C∞(∂Ω)) are
subject to compatibility conditions.
Define the background solution U to be the solution of the wave equation in the absence
of any anomalies. Thus U satisfies
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∂
tU −ΔU = 
in ΩT,
U(x,) = u(x),
∂tU(x,) = u(x)
for x ∈Ω,
∂U
∂
= g
on ∂ΩT.


Expansion Methods
For ρ > , define the operator Pρ on tempered distributions by
Pρ[ψ](x, t) = ∫∣ω∣≤ρ e−
√
−ωt ˆψ(x, ω) dω,
(.)
where ˆψ(x, ω) denotes the Fourier transform of ψ(x, t) in the t-variable. Clearly, the
operator Pρ truncates the high-frequency component of ψ.
The following asymptotic expansion holds as δ →.
Theorem (Perturbations of weighted boundary measurements)
Let w ∈C∞(ΩT)
satisfy (∂
t −Δ)w(x, t) = in ΩT with ∂tw(x, T) = w(x, T) = for x ∈Ω. Suppose that
ρ ≪/δ. Define the weighted boundary measurements
Iw[U, T] := ∫∂ΩT
Pρ[u −U](x, t)∂w
∂(x, t) dσ(x) dt.
Then, for any fixed T > diam(Ω), the following asymptotic expansion for Iw[U, T] holds as
δ →:
Iw[U, T] ≈δd ∫
T

∇Pρ[U](z, t)M(k, B)∇w(z, t) dt,
(.)
where M(k, B) is defined by (> .).
Expansion (> .) is a weighted expansion. Pointwise expansions similar to those in
Theorem which is for the steady-state model can also be obtained.
Let y ∈Rbe such that ∣y −z∣≫δ. Choose
U(x, t) := Uy(x, t) := δt=∣x−y∣
π∣x −y∣
for x ≠y.
(.)
It is easy to check that Uy is the outgoing Green function to the wave equation:
(∂
t −Δ)Uy(x, t) = δx=yδt=
in R×],+∞[ .
Moreover, Uy satisfies the initial conditions: Uy(x,) = ∂tUy(x,) = for x ≠y. Consider
now for the sake of simplicity the wave equation in the whole three-dimensional space with
appropriate initial conditions:
⎧⎪⎪⎨⎪⎪⎩
∂
tu −∇⋅(χ(R/D) + kχ(D))∇u = δx=yδt=
in R×],+∞[ ,
u(x,) = ,
∂tu(x,) = 
for x ∈R, x ≠y.
(.)
The following theorem holds.
Theorem (Pointwise perturbations)
Let u be the solution to (> .). Set Uy to be the
background solution. Suppose that ρ ≪/δ.
(i) The following outer expansion holds
Pρ[u −Uy](x, t) ≈−δ∫R ∇Pρ[Uz](x, t −τ) ⋅M(k, B)∇Pρ[Uy](z, τ) dτ ,
(.)
for x away from z, where M(k, B) is defined by (> .) and Uy and Uz by (> .).

Expansion Methods 

(ii) The following inner approximation holds:
Pρ[u −Uy](x, t) ≈δˆv (x −z
δ
) ⋅∇Pρ[Uy](x, t)
for x near z,
(.)
where ˆv is given by (> .) and Uy by (> .).
Formula (> .) shows that the perturbation due to the anomaly is in the time-
domain a wavefront emitted by a dipolar source located at the point z.
Taking the Fourier transform of (> .) in the time variable yields the expansions
given in Theorem for the perturbations resulting from the presence of a small anomaly
for solutions to the Helmholtz equation at low frequencies (at wavelengths large compared
to the size of the anomaly).
..
Numerical Methods
...
MUSIC-Type Imaging at a Single Frequency
Consider m well-separated anomalies Ds = δBs + zs, s = , . . . , m. The compressibility and
volumetric mass density of Ds are denoted by ρs and ks, respectively. Suppose as before
that all the domains Bs are disks. Let (θ, . . . , θn) be n unit vectors in Rd. For arbitrary
θ ∈{θ, . . . , θn}, one assumes that one is in the possession of the boundary data u when
the object Ω is illuminated with the plane wave U(x) = eiωθ⋅x. Therefore, taking w(x) =
e−iωθ′⋅x for θ′ ∈{θ, . . . , θn}, shows that one is in possession of
m
∑
s=
∣Ds∣((ks −)
ks + θ ⋅θ′ + (ρs −)) eiω(θ−θ′)⋅zs,
for θ, θ′ ∈{θ, . . ., θn}. Define the response matrix A = (All′)n
l,l′=∈Cn×n by
All′ =
m
∑
s=
∣Ds∣((ks −)
ks + θl ⋅θl′ + (ρs −))) eiω(θ l−θ l′)⋅zs,
l, l′ = , . . . , n .
Introduce
g(x) = ((, θ)∗eiωθ⋅x, . . .,(, θn)∗eiωθ n⋅x)
∗.
Analogously to Lemma , one has the following characterization of the location of the
anomalies in terms of the range of the matrix A.
Lemma (MUSIC characterization of the range of the response matrix)
There exists
n∈N, n> (d + )m, such that for any n ≥nthe following statement holds:
g j(x) ∈Range(A) if and onl y i f x ∈{z, . . . , zm}
for j = , . . . , d + ,
where g j(x) is the jth column of g(x).


Expansion Methods
X axis
Y axis
X axis
Y axis
−2 −1.5
30
25
20
15
10
5
0
2
1
0
–1
–2 –2
–1
0
1
2
−1 −0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
5
10
15
20
25
⊡Fig. -
MUSIC-type reconstruction from the singular value decomposition of A represented in
> Fig. -
The MUSIC algorithm can now be used as before to determine the location of the
anomalies. Let Pnoise = I −P, where P is the orthogonal projection onto the range of A.
The imaging functional
WMU(x) :=

∑d+
j=∣∣Pnoiseg j(x)∣∣
has large peaks only at the locations of the anomalies. See > Fig. -.
The significant singular vectors of A can be computed by the singular value decompo-
sition. The number of significant singular values determines the number of anomalies. If,
for example, ks ≠and ρs ≠for all s = , . . . , m, then there are exactly (d +)m significant
singular values of A and the rest are zero or close to zero. See
> Fig. -. The significant
singular values of A can be used to estimate (ks−)
ks+∣Ds∣and (ρs −)∣Ds∣.
...
Backpropagation-Type Imaging at a Single Frequency
A backpropagation imaging functional at a single frequency ω is given by
WBP(x) := 
n
n
∑
l=
e−iωθ l ⋅xIwl[Ul],
where Ul(x) = wl(x) = eiωθ l ⋅x for θl ∈{θ, . . . , θn} . Suppose that (θ, . . . , θn) are
equidistant points on the unit sphere Sd−. For sufficiently large n, since

n
n
∑
l=
eiωθ l ⋅x ≈
⎧⎪⎪⎨⎪⎪⎩
j(ω∣x∣)
for d = ,
J(ω∣x∣)
for d = ,

Expansion Methods 

0
2
4
6
8
10
12
14
16
18
20
10−20
10−15
10−10
10−5
100
Singular values of A (20×20)
Singular value number, sj
Iog10(sj)
⊡Fig. -
Singular value decomposition of the response matrix A corresponding to two
well-separated anomalies of general shape for n = , using a standard log scale. Six
singular values emerge from the others in the noise subspace
where jis the spherical Bessel function of order zero and Jis the Bessel function of the
first kind and of order zero, it follows that
WBP(x) ≈
m
∑
s=
∣Ds∣((ks −)
ks + + (ρs −))) ×
⎧⎪⎪⎨⎪⎪⎩
j(ω∣x −zs∣)
for d = ,
J(ω∣x −zs∣)
for d = .
An analogy between the backpropagation and MUSIC-type imaging can be established.
Suppose that ks = for s = , . . . , m. One can see that
WMU(x) ∝

∣Ds∣(ρs −) −WBP(x)
for x near zs [].
...
Kirchhoﬀ-Type Imaging Using a Broad Range of Frequencies
Let yl ∈R/Ω for l = , . . . , n denote an array of source points. Set
wyl (x) = i
H()
(ω∣x −yl∣)
and
Uyl′(x) = i
H()
(ω∣x −yl′∣),
where H()

is the Hankel function of first kind and order zero. Using the asymptotic form
of the Hankel function one finds that for ω∣x −y∣≫:
i
H()
(ω∣x −y∣) ≈


√
π
ei π/
√
ω∣x −y∣
eiω∣x−y∣,


Expansion Methods
and
i
∇H()
(ω∣x −y∣) ≈


√
π
( iω(x −y)
∣x −y∣
)
ei π/
√
ω∣x −y∣
eiω∣x−y∣.
Assume a high frequency regime with ωL ≫for L the distance from the array center
point to the locations zs, s = , . . ., m. It follows that
Iwl [Ul′, ω] ∝
m
∑
s=
∣Ds∣(−ks −
ks + 
(zs −yl) ⋅(zs −yl′)
∣zs −yl∣∣zs −yl′∣
+ (ρs −)) eiω(∣zs−yl∣+∣zs−yl′∣).
Introduce the response matrix A(ω) = (All′(ω)) by
All′(ω) := Iwl[Ul′, ω]
and the illumination vector
g(x, ω) := ((, x −y
∣x −y∣)
∗
eiω∣x−y∣, . . . ,(, x −yn
∣x −yn∣)
∗
eiω∣x−yn ∣)
∗
.
In the case of measurements at multiple frequencies (ω j), we construct the weighted
Kirchhoff imaging functional as
WKI(x) = 
J
∑
ω j,j=,...,J
∑
l
(g(x, ω j),ul(ω j))(g(x, ω j),vl(ω j)),
where (a, b) = a ⋅b, J is the number of frequencies and ul and vl are respectively the left
and right singular vectors of A. As for WMU, WKI is written in terms of the singular value
decompositions of the response matrices A(ω j).
...
Time-Reversal Imaging
Unlike the three previous imaging methods, the one in this section is in time domain. It is
based on time reversal.
The main idea of time reversal is to take advantage of the reversibility of the wave
equation in a non-dissipative unknown medium in order to back-propagate signals to the
sources that emitted them. In the context of anomaly detection, one measures the per-
turbation of the wave on a closed surface surrounding the anomaly and retransmits it
through the background medium in a time-reversed chronology. Then the perturbation
will travel back to the location of the anomaly. One can show that the time-reversal pertur-
bation focuses on the location z of the anomaly with a focal spot size limited to one-half
the wavelength which is in agreement with the Rayleigh resolution limit.
In mathematical terms, suppose that one is able to measure the perturbation u −Uy
and its normal derivative at any point x on a sphere S englobing the anomaly D and for a
large time t. The time-reversal operation is described by the transform t ↦t−t. Both the
perturbation and its normal derivative on S are time reversed and emitted from S. Then a
time-reversed perturbation propagates inside the volume surrounded by S.

Expansion Methods 

To detect the anomaly from measurements of the wavefield u −Uy away from the
anomaly one can use a time-reversal technique. Taking into account the definition of the
outgoing fundamental solution (> .) to the wave equation, spatial reciprocity and time
reversal invariance of the wave equation, one defines the time-reversal imaging functional
WTR by
WTR(x, t) = ∫R ∫S [Ux(x′, t −s)∂Pρ[u −Uy]
∂
(x′, t−s)
−∂Ux
∂
(x′, t −s)Pρ[u −Uy](x′, t−s)] dσ(x′) ds,
(.)
where
Ux(x′, t −τ) = δ(t −τ −∣x −x′∣)
π∣x −x′∣
.
The imaging functional WTR corresponds to propagating inside the volume surrounded
by S, the time-reversed perturbation Pρ[u −Uy], and its normal derivative on S. Theorem
shows that
Pρ[u −Uy](x, t) ≈−δ∫R ∇Pρ[Uz](x, t −τ) ⋅m(z, τ) dτ,
where
m(z, τ) = M(k, B)∇Pρ[Uy](z, τ) .
(.)
Therefore, since
∫R ∫S [Ux(x′, t −s)∂Pρ[Uz]
∂
(x′, t−s −τ)
−∂Ux
∂
(x′, t −s)Pρ[Uz](x′, t−s −τ)] dσ(x′) ds
= Pρ[Uz](x, t−τ −t) −Pρ[Uz](x, t −t+ τ),
(.)
one obtains the approximation
WTR(x, t) ≈−δ∫R m(z, τ) ⋅∇z [Pρ[Uz](x, t−τ −t) −Pρ[Uz](x, t −t+ τ)]dτ,
which can be interpreted as the superposition of incoming and outgoing waves, centered
on the location z of the anomaly. Since
Pρ[Uy](x, τ) =
sin ρ(τ −∣x −y∣)
π(τ −∣x −y∣)∣x −y∣,
m(z, τ) is concentrated at the travel time τ = T = ∣z −y∣. It then follows that
WTR(x, t) ≈−δm(z, T) ⋅∇z [Pρ[Uz](x, t−T −t) −Pρ[Uz](x, t −t+ T)].
(.)
The imaging functional WTR is clearly the sum of incoming and outgoing polarized
spherical waves.


Expansion Methods
Approximation (> .) has an important physical interpretation. By changing the ori-
gin of time, T can be set to without loss of generality. Then by taking a Fourier transform
of (> .) over the time variable t, one obtains that
̂
WTR(x, ω) ∝δm(z, T) ⋅∇j(ω∣x −z∣),
where ω is the wave number. This shows that the time-reversal perturbation WTR focuses
on the location z of the anomaly with a focal spot size limited to one-half the wavelength.
An identity parallel to (> .) can be derived in the frequency domain. In fact, one
has
∫S
⎡⎢⎢⎢⎢⎣
̂Ux(x′)∂̂
Uz
∂
(x′) −∂̂Ux
∂
(x′)̂
Uz(x′)
⎤⎥⎥⎥⎥⎦
dσ(x′) = iℑm ̂
Uz(x)
∝j(ω∣x −z∣),
(.)
which shows that in the frequency domain WTR coincides with WBP.
..
Bibliography and Open Questions
The initial boundary-value problems for the wave equation in the presence of anomalies of
small volume have been considered in [,]. Theorem is from []. In [], a time-reversal
approach was also designed for locating the anomaly from the outer expansion (> .).
The physics literature on time reversal is quite rich. One refers, for instance, to [] and
the references therein. See [] for clinical applications of time reversal. Many interesting
mathematical works have dealt with different aspects of time-reversal phenomena: see; for
instance, [] for time reversal in the time domain, [–, ] for time reversal in the
frequency domain, and [,] for time reversal in random media.
The MUSIC-type algorithm for locating small acoustic or electromagnetic anomalies
from the multi-static response matrix at a fixed frequency was developed in []. See also
[–], where a variety of numerical results was presented to highlight its potential and
its limitation. It is worth mentioning that the MUSIC-type algorithm is related to time
reversal [,].
MUSIC and Kirchhoff imaging functionals can be extended to the time domain in
order to detect the anomaly and its polarization tensor from (dynamical) boundary
measurements [].
The inner expansion in Theorem can be used to design an efficient optimization algo-
rithm for reconstructing the shape and the physical parameter of an anomaly from the
near-field perturbations of the wavefield, which can be used in radiation force imaging.
In radiation force imaging one uses the acoustic radiation force of an ultrasonic focused
beam to remotely generate mechanical vibrations in organs. A spatiotemporal sequence of
the propagation of the induced transient wave can be acquired, leading to a quantitative
estimation of the physical parameters of the anomaly. See, for instance, [,].

Expansion Methods 

The proposed location search algorithms using transient wave or broad range multifre-
quency boundary measurements can be extended to the case with limited-view measure-
ments. Using the geometrical control method [], one can still exploit those algorithms
and perform imaging with essentially the same resolution using partial data as using
complete data, provided that the geometric optics condition holds.
An identity similar to (> .) can be derived in an inhomogeneous medium, which
shows that the sharper the behavior of the imaginary part of the Green function around
the location of the anomaly is, the higher is the resolution. It would be quite challenging to
explicitly see how this behavior depends on the heterogeneity of the surrounding medium.
This would yield super-resolved ultrasound imaging systems.
.
Infrared Thermal Imaging
..
Physical Principles
Infrared thermal imaging is becoming a common screening modality in the area of breast
cancer. By carefully examining the aspects of temperature and blood vessels of the breasts
in thermal images, signs of possible cancer or precancerous cell growth may be detected up
to years prior to being discovered using any other procedure. This provides the earliest
detection of cancer possible.
Because of thermal imaging’s extreme sensitivity, these temperature variations and vas-
cular changes may be among the earliest signs of breast cancer and/or a precancerous state
of the breast. An abnormal infrared image of the breast is an important marker of high risk
for developing breast cancer. See [,].
..
Asymptotic Analysis of Temperature Perturbations
Suppose that the background Ω is homogeneous with thermal conductivity and that the
anomaly D = δB+z has thermal conductivity < k ≠< +∞. In this section one considers
the following transmission problem for the heat equation:
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∂tu −∇⋅(χ(Ω/D) + kχ(D))∇u = 
in ΩT,
u(x,) = u(x)
for x ∈Ω,
∂u
∂
= g
on ∂ΩT,
(.)
where the Neumann boundary data g and the initial data uare subject to a compatibility
condition. Let U be the background solution defined as the solution of


Expansion Methods
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
∂tU −ΔU = 
in ΩT,
U(x,) = u(x)
for x ∈Ω,
∂U
∂
= g
on ∂ΩT.
The following asymptotic expansion holds as δ →.
Theorem (Perturbations of weighted boundary measurements)
Let w ∈C∞(ΩT)
be a solution to the adjoint problem, namely, satisfy (∂t + Δ)w(x, t) = in ΩT with
w(x, T) = for x ∈Ω. Define the weighted boundary measurements
Iw[U, T] := ∫∂ΩT
(u −U)(x, t)∂w
∂(x, t) dσ(x) dt.
Then, for any fixed T > , the following asymptotic expansion for Iw[U, T] holds as δ →:
Iw[U, T] ≈−δd ∫
T

∇U(z, t) ⋅M(k, B)∇w(z, t) dt,
(.)
where M(k, B) is defined by (> .).
Note that (> .) holds for any fixed positive final time T, while (> .) holds only
for T > diam(Ω). This difference comes from the finite speed propagation property for
the wave equation compared to the infinite one for the heat equation.
Consider now the background solution to be the Green function of the heat
equation at y:
U(x, t) := Uy(x, t) :=
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
e−∣x−y∣
t
(πt)d/
for t > ,

for t < .
(.)
Let u be the solution to the following heat equation with an appropriate initial condition:
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∂tu −∇⋅(χ(Rd/D) + kχ(D))∇u = 
in Rd×],+∞[ ,
u(x,) = Uy(x,)
for x ∈Rd.
(.)
Proceeding as in the derivation of (> .), one can prove that δu(x, t) := u −U is
approximated by
−(k −)∫
t


(π(t −τ))d/∫∂D e−∣x−x′∣
(t−τ) ∂ˆv
∂∣
−
(x′ −z
δ
) ⋅∇Uy(x′, τ) dσ(x′) dτ, (.)
for x near z. Therefore, analogously to Theorem , the following pointwise expansion
follows from the approximation (> .).
Theorem (Pointwise perturbations)
Let y ∈Rd be such that ∣y −z∣≫δ. Let u be the
solution to (> .). The following expansion holds

Expansion Methods 

(u−U)(x, t) ≈−δd ∫
t
∇Uz(x, t−τ)M(k, B)∇Uy(z, τ) dτ
for ∣x−z∣≫O(δ), (.)
where M(k, B) is defined by (> .) and Uy and Uz by (> .).
When comparing (> .) and (> .), one should point out that for the heat
equation the perturbation due to the anomaly is accumulated over time.
An asymptotic formalism for the realistic half-space model for thermal imaging, well
suited for the design of anomaly reconstruction algorithms, has been developed in [].
..
Numerical Methods
In this section, the formula (> .) is applied (with an appropriate choice of test functions
w and background solutions U) for the purpose of identifying the location of the anomaly
D. The first algorithm makes use of constant heat flux and, not surprisingly, it is limited in
its ability to effectively locate multiple anomalies.
Using many heat sources, one then describes an efficient method to locate multiple
anomalies and illustrate its feasibility. For the sake of simplicity only the two-dimensional
case will be considered.
...
Detection of a Single Anomaly
For y ∈R/Ω, let
w(x, t) = wy(x, t) :=

π(T −t)e−∣x−y∣
(T−t) .
(.)
The function w satisfies (∂t + Δ)w = in ΩT and the final condition w∣t=T = in Ω.
Suppose that there is only one anomaly D = z+δB with thermal conductivity k. For sim-
plicity assume that B is a disk. Choose the background solution U(x, t) to be a harmonic
(time-independent) function in ΩT. One computes
∇wy(z, t) =
y −z
π(T −t)e−∣z−y∣
(T−t) ,
M(k, B)∇wy(z, t) = (k −)∣B∣
k + 
y −z
π(T −t)e−∣z−y∣
(T−t) ,
and
∫
T

M(k, B)∇wy(z, t) dt = (k −)∣B∣
k + 
y −z
π ∫
T

e−∣z−y∣
(T−t)
(T −t)dt.
But
d
dt e−∣z−y∣
(T−t) = −∣z −y∣

e−∣z−y∣
(T−t)
(T −t)


Expansion Methods
and therefore
∫
T

M(k, B)∇wy(z, t) dt = (k −)∣B∣
k + 
y −z
π∣z −y∣e−∣z−y∣
(T−t).
Then the asymptotic expansion (> .) yields
Iw[U, T](y) ≈δk −
k + ∣B∣∇U(z) ⋅(y −z)
π∣y −z∣
e−∣y−z∣
T .
(.)
Now, one is in a position to present the projection-type location search algorithm for
detecting a single anomaly. Prescribe the initial condition u(x) = a ⋅x for some fixed
unit constant vector a and choose g = a ⋅
as an applied time-independent heat flux on
∂ΩT, where a is taken to be a coordinate unit vector. Take two observation lines Σand Σ
contained in R/Ω such that
Σ:= a line parallel to a,
Σ:= a line normal to a .
Next find two points Pi ∈Σi (i = ,) so that Iw(T)(P) = and
Iw(T)(P) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
min
x∈ΣIw(T)(x)
if k −< ,
max
x∈ΣIw(T)(x)
if k −> .
Finally, draw the corresponding lines Π(P) and Π(P) given by (> .). Then the inter-
secting point P of Π(P) ∩Π(P) is close to the anomaly D: ∣P −z∣= O(δ ∣log δ∣) for δ
small enough.
...
Detection of Multiple Anomalies: A MUSIC-Type Algorithm
Consider m well-separated anomalies Ds = δBs + zs, s = , . . . , m, whose heat conductivity
is ks. Choose
U(x, t) = Uy′(x, t) :=

πt e−∣x−y′∣
t
for y′ ∈R/Ω
or, equivalently, g to be the heat flux corresponding to a heat source placed at the point
source y′ and the initial condition u(x) = in Ω, to obtain that
Iw[U, T] ≈−δ
m
∑
s=
(−ks)
π(y′ −zs)M(s)(y −zs)
× ∫
T


t(T −t)exp (−∣y −zs∣
(T −t) −∣y′ −zs∣
t
) dt,
where w is given by (> .) and M(s) is the polarization tensor of Ds.
Suppose for the sake of simplicity that all the domains Bs are disks. Then it follows from
(> .) that M(s) = m(s)I, where m(s) = (ks −)∣Bs∣/(ks +) and Iis the ×identity
matrix. Let yl ∈R/Ω for l ∈N be the source points. One assumes that the countable
set {yl}l∈N has the property that any analytic function which vanishes in {yl}l∈N vanishes
identically.

Expansion Methods 

The MUSIC-type location search algorithm for detecting multiple anomalies is as
follows. For n ∈N sufficiently large, define the matrix A = [All′]n
l,l′=by
All′ := −δ
m
∑
s=
(−ks)
πm(s)(yl′ −zs) ⋅(yl −zs)
× ∫
T


t(T −t)exp (−∣yl −zs∣
(T −t) −∣yl′ −zs∣
t
) dt .
For z ∈Ω, one decomposes the symmetric real matrix C defined by
C := [∫
T


t(T −t)exp (−∣yl −z∣
(T −t) −∣yl′ −z∣
t
) dt]
l,l′=,...,n
as follows:
C =
p
∑
l=
vl(z)vl(z)∗
(.)
for some p ≤n, where vl ∈Rn and v∗
l denotes the transpose of vl. Define the vector
g(l)
z
∈Rn×for z ∈Ω by
g(l)
z
= ((y−z)vl(z), . . .,(yn −z)vln(z))∗,
l = , . . . , p .
(.)
Here vl, . . . ,vln are the components of the vector vl, l = , . . . , p. Let yl = (ylx, yl y) for
l = , . . . , n, z = (zx, zy), and zs = (zsx, zsy). One also introduces
g(l)
zx = ((yx −zx)vl(z), . . .,(ynx −zx)vln(z))∗
and
g(l)
zy = ((yy −zy)vl(z), . . .,(yny −zy)vln(z))
∗.
Lemma (MUSIC characterization of the range of the response matrix)
The following
characterization of the location of the anomalies in terms of the range of the matrix A holds:
g(l)
zx and g(l)
zy ∈Range(A) ∀l ∈{, . . ., p}
if and only if
z ∈{z, . . ., zm} .
(.)
Note that the smallest number n which is sufficient to efficiently recover the anomalies
depends on the (unknown) number m. This is the main reason for taking n sufficiently
large. As for the electrical impedance imaging, the MUSIC-type algorithm for the ther-
mal imaging is as follows. Compute Pnoise, the projection onto the noise space, by the
singular value decomposition of the matrix A. Compute the vectors vl by (> .).
Form an image of the locations, z, . . . , zm, by plotting, at each point z, the quantity
∥g(l)
z
⋅a∥/∥Pnoise (g(l)
z
⋅a)∥for l = , . . . , p, where g(l)
z
is given by (> .) and a
is a unit constant vector. The resulting plot will have large peaks at the locations of zs,
s = , . . . , m.
The next two figures (> Figs. -and
> -) show MUSIC-type reconstructions of
two anomalies without and with noise.
In > Fig. -, one sees clearly the presence of two anomalies. However, the one on the
right, which is also deeper, is not as well rendered as the one on the left.


Expansion Methods
0.5
0
–0.5
–1
–1.5
–2
–2.5
–3
–3.5
–4
–4.5
–5–5
–4
–3
–2
–1
0
1
2
3
4
5
⊡Fig. -
Detection of anomalies using n = heat sources equi-placed on the top
0.5
0
–0.5
–1
–1.5
–2
–2.5
–3
–3.5
–4
–4.5
–5–5 –4 –3 –2
–1
0
1
2
3
4
5
0.5
0
–0.5
–1
–1.5
–2
–2.5
–3
–3.5
–4
–4.5
–5–5
–4 –3 –2
–1
0
1
2
3
4
5
⊡Fig. -
Detection in the presence of % (on the left) and % (on the right) of measurement noise
..
Bibliography and Open Questions
Thermal imaging of small anomalies has been considered in []. See also [], where a
realistic half-space model for thermal imaging was considered and accurate and robust
reconstruction algorithms are designed.
It is worth mentioning that the inner expansions derived for the heat equation can be
used to improve reconstruction in ultrasonic temperature imaging. The idea behind ultra-
sonic temperature imaging hinges on measuring local temperature near anomalies. The
aim is to reconstruct anomalies with higher spatial and contrast resolution as compared to

Expansion Methods 

those obtained from boundary measurements alone. Further numerical investigations on
this emerging topic are required.
.
Impediography
..
Physical Principles
Since all the present electrical impedance tomography technologies are only practically
applicable in feature extraction of anomalies, improving electrical impedance tomogra-
phy calls for innovative measurement techniques that incorporate structural information.
A very promising direction of research is the recent magnetic resonance imaging tech-
nique, called current density imaging, which measures the internal current density dis-
tribution. See the breakthrough work by Seo and his group described, for instance, in
[,,]. However, this technique has a number of disadvantages, among which the lack
of portability and a potentially long imaging time. Moreover, it uses an expensive magnetic
resonance imaging scanner.
Impediography is another mathematical direction for future electrical impedance
tomography research in view of biomedical applications. It keeps the most important
merits of electrical impedance tomography (real-time imaging, low cost, portability).
It is based on the simultaneous measurement of an electric current and of acoustic
vibrations induced by ultrasound waves. Its intrinsic resolution depends on the size of
the focal spot of the acoustic perturbation, and thus it may provide high-resolution
images.
The core idea of impediography is to couple electric measurements to localized elastic
perturbations. A body (a domain Ω ⊂R) is electrically probed: one or several currents are
imposed on the surface and the induced potentials are measured on the boundary. At the
same time, a circular region of a few millimeters in the interior of Ω is mechanically excited
by ultrasonic waves, which dilate this region. The measurements are made as the focus of
the ultrasounds scans the entire domain. Several sets of measurements can be obtained by
varying amplitudes of the ultrasound waves and the applied currents.
Within each disk of (small) volume, the conductivity is assumed to be constant per
volume unit. At a point x ∈Ω, within a disk D of volume VD, the electric conductivity γ is
defined in terms of a density ρ as γ(x) = ρ(x)VD.
The ultrasonic waves induce a small elastic deformation of the disk D. If this defor-
mation is isotropic, the material points of D occupy a volume V p
D in the perturbed
configuration, which at first order is equal to
V p
D = VD (+ Δr
r ),
where r is the radius of the disk D and Δr is the variation of the radius due to the elastic
perturbation. As Δr is proportional to the amplitude of the ultrasonic wave, one obtains
a proportional change of the deformation. Using two different ultrasonic waves with dif-


Expansion Methods
ferent amplitudes but with the same spot, it is therefore easy to compute the ratio V p
D/VD.
As a consequence, the perturbed electrical conductivity γp satisfies
∀x ∈Ω,
γp(x) = ρ(x)V p
D = γ(x)η(x),
where η(x) = V p
D/VD is a known function. One makes the following realistic assump-
tions: () the ultrasonic wave expands the zone it impacts and changes its conductivity:
∀x ∈Ω, η(x) > and () the perturbation is not too small: η(x) −≫VD.
..
Mathematical Model
Let u be the voltage potential induced by a current g, in the absence of ultrasonic
perturbations. It is given by
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∇⋅(γ(x)∇u) = 
in Ω,
γ ∂u
∂
= g
on ∂Ω,
(.)
with the convention that∫∂Ω u = . One supposes that the conductivity γ of the region close
to the boundary of the domain is known, so that ultrasonic probing is limited to interior
points. One denotes the region (open set) by Ω.
Let uδ be the voltage potential induced by a current g, in the presence of ultrasonic
perturbations localized in a disk-shaped domain D := z + δB of volume ∣D∣= O(δ). The
voltage potential uδ is a solution to
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∇⋅(γδ(x)∇uδ(x)) = 
in Ω,
γ ∂uδ
∂
= g
on ∂Ω,
(.)
with the notation
γδ(x) = γ(x)[+ χ(D)(x)(η(x) −)] ,
where χ(D) is the characteristic function of the domain D.
As the zone deformed by the ultrasound wave is small, one can view it as a small-volume
perturbation of the background conductivity γ and seek an asymptotic expansion of the
boundary values of uδ −u. The method of small-volume expansions shows that comparing
uδ and u on ∂Ω provides information about the conductivity. Indeed, one can prove that
∫∂Ω(uδ −u)g dσ = ∫D γ(x)(η(x) −)
η(x) + ∇u ⋅∇u dx + o(∣D∣)
= γ(z)∣∇u(z)∣∫D
(η(x) −)
η(x) + 
dx + o(∣D∣).
Note that because of assumption () at the end of the previous section, it follows that
∫D
(η(x) −)
η(x) + 
dx ≥C∣D∣

Expansion Methods 

for some positive constant C. Therefore, one has
γ(z)∣∇u(z)∣= E(z) + o(),
(.)
where the function E(z) is defined by
E(z) = (∫D
(η(x) −)
η(x) + dx)
−
∫∂Ω(uδ −u)g dσ.
(.)
By scanning the interior of the body with ultrasound waves, given an applied current g,
one then obtains data from which one can compute the electrical energy
E(z) := γ(z)∣∇u(z)∣
in an interior subregion of Ω. The new inverse problem is now to reconstruct γ,
knowing E.
..
Substitution Algorithm
The use of E leads one to transform (> .), having two unknowns γ and u with highly
nonlinear dependency on γ, into the following nonlinear PDE (the –Laplacian)
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∇x ⋅(
E
∣∇u∣∇u) = 
in Ω,
E
∣∇u∣
∂u
∂
= g
on ∂Ω.
(.)
It is worth emphasizing that E is a known function, constructed from the measured
data (> .). Consequently, all the parameters entering in (> .) are known. Thus,
the ill-posed inverse problem in electrical impedance tomography is converted into a
less-complicated direct problem (> .).
The E-substitution algorithm, which will be explained below, uses two currents g
and g. One chooses this pair of current patterns to have ∇u× ∇u≠for all x ∈Ω,
where ui, i = ,, is the solution to (> .). One refers to [] and the references therein
for an evidence of the possibility of such a choice. The E-substitution algorithm is based
on an approximation of a linearized version of problem (> .).
Suppose that γ is a small perturbation of conductivity profile γ: γ = γ+ δγ. Let u
and u = u+ δu denote the potentials corresponding to γand γ with the same Neumann
boundary data g. It is easily seen that δu satisfies ∇⋅(γ∇δu) = −∇⋅(δγ∇u) in Ω with
the homogeneous Dirichlet boundary condition. Moreover, from
E = (γ+ δγ)∣∇(u+ δu)∣≈γ∣∇u∣+ δγ∣∇u∣+ γ∇u⋅∇δu,
after neglecting the terms δγ∇u⋅∇δu and δγ∣∇δu∣, it follows that
δγ ≈
E
∣∇u∣−γ−γ
∇δu ⋅∇u
∣∇u∣
.


Expansion Methods
The E-substitution algorithm is as follows. One starts from an initial guess for the
conductivity γ and solves the corresponding Dirichlet conductivity problem
{∇⋅(γ∇u) = 
in Ω,
u= ψ
on ∂Ω.
The data ψ is the Dirichlet data measured as a response to the current g (say g = g) in
the absence of elastic deformation. The discrepancy between the data and the guessed
solution is
є:=
E
∣∇u∣−γ.
(.)
One then introduces a corrector, δu, computed as the solution to
{∇⋅(γ∇δu) = −∇⋅(ε∇u)
in Ω,
δu = 
on ∂Ω,
and updates the conductivity
γ := E −γ∇δu ⋅∇u
∣∇u∣
.
One iteratively updates the conductivity, alternating directions of currents (i.e.,
with g = g).
Consider a disk-shaped domain Ω, which contains three anomalies, an ellipse, an
L-shaped domain, and a triangle. See > Fig. -.
> Figure -shows the result of the reconstruction when measurements with very
accurate precision for two directions of currents are available.
In the case of incomplete data, that is, if E is only known on a subset Ω′ of the domain,
one can follow an optimal control approach. See [].
⊡Fig. -
Conductivity distribution

Expansion Methods 

⊡Fig. -
Reconstruction test. From left to right, the initial guess, the collected data E for two
directions of currents and the reconstructed conductivity
..
Bibliography and Open Questions
Impediography was proposed in [] and the substitution algorithm proposed there. An
optimal control approach for solving the inverse problem in impediography has been
described in []. The inversion was considered as a minimization problem, and it was
performed in two or three dimensions.
As pointed out in [], the success of impediography depends on the feasibility of focus-
ing ultrasound waves at an arbitrary point inside the body. Such a focusing, however, is
quite tricky to achieve in practice. See, for instance, []. A method to extract the mea-
surements corresponding to well-focused beams from the data obtained with unfocused
waves has been proposed in [].
An interesting problem is to study the sensitivity of the inversion methods to lim-
itations on the intensities of the applied voltages, as electrical safety regulations limit
the amount of the total current that patients can sustain. Another interesting prob-
lem is to reconstruct anisotropic conductivity distributions and to see whether or not
impediography allows one to remove the obstruction to unique identifiability of the
conductivity by electrical impedance tomography. In electrical impedance tomography,
it is known that any change of variables of the background conductor that leaves the
boundary fixed gives rise to a new anisotropic conductivity with the same boundary
measurements [].
.
Magneto-Acoustic Imaging
In magneto-acoustic imaging, a probe signal such as an acoustic wave or an electric cur-
rent (or voltage) is applied to a biological tissue placed in a magnetic field. The probe
signal produces, by the Lorentz force, an induced signal that is a function of the local
electrical conductivity of the biological tissue. If the probe signal is an acoustic wave,
then the induced signal is an electric current and the Lorentz force causes a local current
density.


Expansion Methods
Induced boundary currents or pressure which are proportional to the local electrical
conductivity can be measured to reconstruct the conductivity distribution with the spatial
resolution of the ultrasound. The induced signal is detected and an image of the local elec-
trical conductivity of the specimen is generated based on the detected induced signal. The
first method is referred as magneto-acousto-electrical tomography and the second one as
magneto-acoustic tomography with magnetic induction.
..
Magneto-Acousto-Electrical Tomography
...
Physical Principles
In magneto-acousto-electrical imaging, an acoustic wave is applied to a biological tissue
placed in a magnetic field. The probe signal produces by the Lorentz force an electric cur-
rent that is a function of the local electrical conductivity of the biological tissue []. The
mathematical basis for this magneto-acoustic imaging approach is provided and an effi-
cient algorithm for solving the inverse problem is proposed which is quite similar to the
one designed for impediography.
...
Mathematical Model
Denote by γ(x) the unknown conductivity and let the voltage potential v be the solution
to the conductivity problem
⎧⎪⎪⎨⎪⎪⎩
∇⋅γ∇v = 
in Ω,
v = g
on ∂Ω.
(.)
Suppose that the conductivity γ is a known constant on a neighborhood of the boundary
∂Ω and let γ∗denote γ∣∂Ω.
In magneto-acoustic imaging, ultrasonic waves are focused on regions of small dia-
meter inside a body placed on a static magnetic field. The oscillation of each small region
results in frictional forces being applied to the ions, making them move. In the presence of
a magnetic field, the ions experience a Lorentz force. This gives rise to a localized current
density within the medium. The current density is proportional to the local electrical con-
ductivity []. In practice, the ultrasounds impact a spherical or ellipsoidal zone, of a few
millimeters in diameter. The induced current density should thus be sensitive to conduc-
tivity variations at the millimeter scale, which is the precision required for breast cancer
diagnostics.
Let z ∈Ω and D be a small impact zone around the point z. The created current by the
Lorentz force density is given by
Jz(x) = cχ(D)(x)γ(x)e,
(.)

Expansion Methods 

for some constant c and a constant unit vector e both of which are independent of z. With
the induced current Jz the new voltage potential, denoted by uz, satisfies
⎧⎪⎪⎨⎪⎪⎩
∇⋅(γ∇uz + Jz) = 
in Ω,
uz = g
on ∂Ω.
According to (> .), the induced electrical potential wz := v−uz satisfies the conductivity
equation:
⎧⎪⎪⎨⎪⎪⎩
∇⋅γ∇wz = c∇⋅(χ(D)γe)
for x ∈Ω,
wz(x) = 
for x ∈∂Ω.
(.)
The inverse problem for the magneto-acousto-electrical imaging is to reconstruct the
conductivity profile γ from boundary measurements of ∂uz
∂∣∂Ω or equivalently
∂wz
∂
∣∂Ω
for z ∈Ω.
...
Substitution Algorithm
Since γ is assumed to be constant in D and ∣D∣is small, one obtains using Green’s identity
∫∂Ω γ∗
∂wz
∂
gdσ ≈−c∣D∣∇(γv)(z) ⋅e .
(.)
The relation (> .) shows that, by scanning the interior of the body with ultrasound
waves, c∇(γv)(z) ⋅e can be computed from the boundary measurements ∂wz
∂
∣∂Ω in Ω. If
one can rotate the subject, then c∇(γv)(z)for any z in Ω can be reconstructed. In practice,
the constant c is not known. But, since γv and ∂(γv)/∂
on the boundary of Ω are known,
one can recover c and γv from c∇(γv) in a constructive way [].
The new inverse problem is now to reconstruct the contrast profile γ, knowing
E(z) := γ(z)v(z)
(.)
for a given boundary potential g, where v is the solution to (> .).
In view of (> .), v satisfies
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∇⋅E
v ∇v = 
in Ω,
v = g
on ∂Ω.
(.)
If one solves (> .) for v, then (> .) yields the conductivity contrast γ. Note that to
be able to solve (> .) one needs to know the coefficient E(z) for all z, which amounts
to scanning all the points z ∈Ω by the ultrasonic beam.
Observe that solving (> .) is quite easy mathematically: if one puts w = lnv, then
w is the solution to
⎧⎪⎪⎨⎪⎪⎩
∇⋅E∇w = 
in Ω,
w = ln g
on ∂Ω,
(.)


Expansion Methods
as long as g > . Thus if one solves (> .) for w, then v := ew is the solution to (> .).
However, taking an exponent may amplify the error which already exists in the computed
data E. In order to avoid this numerical instability, one solves (> .) iteratively. To do
so, one can adopt an iterative scheme similar to the one proposed in the previous section.
Start with γand let vbe the solution of
⎧⎪⎪⎨⎪⎪⎩
∇⋅γ∇v= 
in Ω,
v= g
on ∂Ω.
(.)
According to (> .), the updates, γ+ δγ and v+ δv, should satisfy
γ+ δγ =
E
v+ δv ,
(.)
where
⎧⎪⎪⎨⎪⎪⎩
∇⋅(γ+ δγ)∇(v+ δv) = 
in Ω,
δv = 
on ∂Ω,
or equivalently
⎧⎪⎪⎨⎪⎪⎩
∇⋅γ∇δv + ∇⋅δγ∇v= 
in Ω,
δv = 
on ∂Ω.
(.)
One then linearizes (> .) to have
γ+ δγ =
E
v(+ δv/v) ≈E
v
(−δv
v
).
(.)
Thus
δγ = −Eδv
v

−δ,
δ = −E
v
+ γ.
(.)
One then finds δv by solving
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∇⋅γ∇δv −∇⋅( E∇v
v
δv) = ∇⋅δ∇v
in Ω,
δv = 
on ∂Ω.
(.)
> Figure -shows the result of the reconstruction when very accurate measurements
for two Dirichlet boundary conditions, g = g, g, are available.

Expansion Methods 

⊡Fig. -
Reconstruction test. From left to right, the conductivity distribution, the initial guess, the
reconstructed conductivity after three iterations
In the case of incomplete data, that is, if E is only known on a subset ω of the domain,
one can follow an optimal control approach. See [].
..
Magneto-Acoustic Imaging with Magnetic Induction
...
Physical Principles
In the magneto-acoustic tomography with magnetic induction, pulsed magnetic stimu-
lation by the ultrasound beam is imposed on an object placed in a static magnetic field.
The magnetic stimulation can be considered as an ideal pulsed distribution over time.
The magnetically induced eddy current is then subject to a Lorentz force. This in turn
creates a pressure wave that can be detected using an ultrasound hydrophone []. The
magneto-acoustic tomography with magnetic induction uses this acoustic pressure wave
to reconstruct the conductivity distribution of the sample as the focus of the ultrasound
beam scans the entire domain.
...
Mathematical Model
Let γ be the conductivity distribution of the object as before. Denoting the constant mag-
netic field as Band the magnetically induced current density distribution as Jz(x) with z
indicating the location of the magnetic stimulation, the Lorentz force is given by
Jz(x) × Bδt== cχ(D)(x)γ(x)eδt=,
where D is the impact zone which is a small neighborhood of z as before, and c is a constant
independent of z and x. Then the wave equation governing the pressure distribution pz can
be written as
∂pz
∂t−c
s Δpz = ,
x ∈Ω,
t ∈], T[,
(.)


Expansion Methods
for some final observation time T, where cs is the acoustic speed in Ω. The pressure satisfies
the Dirichlet boundary condition
pz = 
on ∂Ω×], T[
(.)
and the initial conditions
pz∣t== 
and
∂pz
∂t ∣
t== −c∇⋅(χ(D)γe)
in Ω.
(.)
The inverse problem for the magneto-acoustic tomography with magnetic induction is
to determine the conductivity distribution γ in Ω from boundary measurements of ∂pz
∂
on
∂Ω×], T[ for all z ∈Ω. Suppose that T is large enough so that
T > diam(Ω)
cs
,
(.)
which says that the observation time is long enough for the wave initiated at z to reach the
boundary ∂Ω.
...
Reconstruction Algorithm
The algorithms for the magneto-acoustic tomography with magnetic induction available
in the literature are limited to unbounded media. They use the spherical Radon transform
inversion. However, the pressure field is significantly affected by the acoustic boundary
conditions at the tissue–air interface, where the pressure must vanish. Thus, one can-
not base magneto-acoustic imaging on pressure measurements made over a free surface.
Instead, one can use the following algorithm.
Let w satisfy
∂w
∂t−c
s Δw = 
in Ω×], T[,
(.)
with the final conditions
w∣t=T = ∂w
∂t ∣
t=T = 
in Ω.
(.)
Since γ is constant on D one can prove that the following identity holds:
∫
T

∫∂Ω
∂pz
∂
(x, t)w(x, t) dσ(x) dt = c
cs
γ(z)∫D e ⋅∇w(x,)dx.
(.)
Suppose that d = . For y ∈R/Ω, define the probe function
wy(x, t) :=
δ (t + τ −∣x−y∣
cs )
π∣x −y∣
in Ω×], T[,
(.)

Expansion Methods 

where τ :=
∣y−z∣
cs . The function wy is a Green’s function corresponding to retarded
potentials. Choosing wy as a test function in (> .) yields the new identity
cγ(z) =
c
s
∫D e ⋅∇wy(x,)dx ∫
T

∫∂Ω
∂pz
∂
(x, t)wy(x, t) dσ(x) dt.
(.)
The quantity ∫D e ⋅∇wy(x,)dx can be explicitly computed. In particular, if the source
point y is such that z −y is parallel to e and D is a sphere of radius r (and center
z), then
cγ(z) = −
cs
r
∣z−y∣−
r
∣z−y∣∫
T

∫∂Ω
∂pz
∂
(x, t)wy(x, t) dσ(x) dt,
(.)
provided that γ is constant on D. But since r is sufficiently small one obtains
cγ(z) ≈−cs∣z −y∣
r
∫
T

∫∂Ω
∂pz
∂
(x, t)wy(x, t) dσ(x) dt.
(.)
Formula (> .) can be used to effectively compute the conductivity contrast in Ω
with a resolution of order the size of the ultrasound beam. It is worth emphasizing that
unlike magneto-acousto-electrical imaging, in magneto-acoustic tomography with mag-
netic induction it suffices to excite the local spot at z in order to obtain the value cγ(z), as
clearly shown by (> .).
..
Bibliography and Open Questions
The feasibility of magneto-acoustic imaging has been demonstrated in [, , ]. The
mathematical and numerical modelling described in this section is from []. As it will be
shown in > Sect. ., the approach for the magneto-acoustic tomography with magnetic
induction can be used in photo-acoustic imaging.
It would be interesting to prove the convergence of the proposed iterative scheme
for magneto-acousto-electrical tomography. Another important problem is to design an
efficient inversion algorithm for magneto-acoustic tomography with magnetic induction
when the acoustic speed fluctuates randomly.
.
Magnetic Resonance Elastography
..
Physical Principles
Extensive work has been carried out in the past decade to image, by inducing motion,
the elastic properties of human soft tissues. This wide application field, called elastic-
ity imaging or elastography, is based on the initial idea that shear elasticity can be
correlated with the pathological state of tissues. Several techniques arose according


Expansion Methods
to the type of mechanical excitation chosen (static compression, monochromatic, or
transient vibration) and the way these excitations are generated (externally or inter-
nally). Different imaging modalities can be used to estimate the resulting tissue
displacements.
Magnetic resonance elastography (MRE) is a new way of realizing the idea of elas-
tography. It can directly visualize and quantitatively measure the displacement field in
tissues subject to harmonic mechanical excitation at low-frequencies. A phase-contrast
magnetic resonance imaging technique is used to spatially map and measure the com-
plete three-dimensional displacement patterns. From this data, local quantitative values of
shear modulus can be calculated and images that depict tissue elasticity or stiffness can
be generated. The inverse problem for magnetic resonance elastography is to determine
the shape and the elastic parameters of an elastic anomaly from internal measurements of
the displacement field. In most cases, the most significant elastic parameter is the stiffness
coefficient.
In biological media, the compression modulus is four to six orders higher than
the shear modulus. One can prove that, as the compression modulus goes to +∞,
the Lamé system converges to the modified Stokes system. By reducing the elastic-
ity system to a modified Stokes system, one removes the compression modulus from
consideration.
..
Mathematical Model
Consider the modified Stokes system, i.e., the problem of determining v and q in a domain
Ω from the conditions:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
(Δ + κ)v −∇q = ,
∇⋅v = ,
v∣∂Ω = g .
(.)
Problem (>.) governs elastic wave propagation in nearly incompressible homogeneous
media.
Let (Gil)d
i,l=be the Dirichlet Green function for the operator in (> .), i.e., for y ∈Ω,
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(Δx + κ)Gil(x, y) −∂Fi(x −y)
∂xl
= δil δy(x)
in Ω,
d
∑
l=
∂
∂xl
Gil(x, y) = 
in Ω,
Gil(x, y) = 
on ∂Ω.
(.)

Expansion Methods 

Denote by (e, . . . ,ed) an orthonormal basis of Rd. Let d(ξ) := (/d) ∑k ξkek and ˆvpq,
for p, q = , . . . , d, be the solution to
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
μΔˆvpq + ∇ˆp = 
in Rd/B,
̃μΔˆvpq + ∇ˆp = 
in B,
ˆvpq∣−−ˆvpq∣+ = 
on ∂B,
(ˆpN + ̃μ ∂ˆvpq
∂N )∣−−(ˆpN + μ ∂ˆvpq
∂N )∣+ = 
on ∂B,
∇⋅ˆvpq = 
in Rd,
ˆvpq(ξ) →ξpeq −δpqd(ξ)
as ∣ξ∣→∞,
ˆp(ξ) →
as ∣ξ∣→+∞.
(.)
Here ∂v/∂N = (∇v + (∇v)∗) ⋅N and (∇v)∗denotes the transpose of the matrix ∇v.
Define the viscous moment tensor (Vi jpq)i,j,p,q=,... ,d by
Vi jpq := (̃μ −μ)∫B ∇ˆvpq ⋅(∇(ξiej) + ∇(ξiej)∗) dξ.
(.)
Consider an elastic anomaly D inside a nearly compressible medium Ω. The anomaly
D has a shear modulus ̃μ different from that of Ω, μ. The displacement field u solves the
following transmission problem for the modified Stokes problem:
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(μΔ + ω)u + ∇p = 
in Ω/D,
(̃μΔ + ω)u + ∇p = 
in D,
u∣−= u∣+
on ∂D,
(p∣+ −p∣−)N + μ ∂u
∂N∣
+
−̃μ ∂u
∂N∣
−
= 
on ∂D,
∇⋅u = 
in Ω,
u = g
on ∂Ω,
∫Ω p = ,
(.)
where g ∈L(∂Ω) satisfies the compatibility condition ∫∂Ω g ⋅N = .
The inverse problem consists of reconstructing ˜μ and the shape of the inclusion D from
internal measurements of u.


Expansion Methods
..
Asymptotic Analysis of Displacement Fields
Let (U, q) denote the background solution to the modified Stokes system in the absence of
any anomalies, that is, the solution to
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(μΔ + ω)U + ∇q = 
in Ω,
∇⋅U = 
in Ω,
U = g
on ∂Ω,
∫Ω q = .
(.)
The following asymptotic expansions hold.
Theorem (Expansions of the displacement ﬁeld)
Suppose that D = δB + z, and let u
be the solution of (> .), where < ̃μ ≠μ < +∞.
(i) The following inner expansion holds:
u(x) ≈U(z) + δ
d
∑
p,q=
∂qU(z)pˆvpq (x −z
δ
)
for x near z,
(.)
where ˆvpq is defined by (> .)
(ii) Let (Vi jpq) be the viscous moment tensor defined by (> .). The following outer
expansion holds uniformly for x ∈∂Ω:
(u −U)(x) ≈δd
⎡⎢⎢⎢⎢⎣
d
∑
i,j,p,q,ℓ=
eℓ∂jGℓi(x, z)∂qU(z)pVi jpq
⎤⎥⎥⎥⎥⎦
,
(.)
where Vi jpq is given by (> .), and the Green function (Gil)d
i,l=is defined by
(> .) with κ= ω/μ, μ being the shear modulus of the background medium.
The notion of a viscous moment tensor extends the notion of a polarization tensor to
quasi-incompressible elasticity. The viscous moment tensor, V, characterizes all the infor-
mation about the elastic anomaly that can be learned from the leading-order term of the
outer expansion (> .). It can be explicitly computed for disks and ellipses in the plane
and balls and ellipsoids in three-dimensional space. If B is a two-dimensional disk, then
V = ∣B∣μ ( ˜μ −μ)
˜μ + μ P,
where P = (Pi jpq) is the orthogonal projection from the space of symmetric matrices onto
the space of symmetric matrices of trace zero, i.e.,
Pi jpq = 
(δipδ jq + δiqδ jp) −
d δi jδpq.

Expansion Methods 

If B is an ellipse of the form
x

a+ x

b= ,
a ≥b > ,
(.)
then the viscous moment tensor for B is given by
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
V= V= −V= −V= ∣B∣
μ( ˜μ −μ)
μ + ˜μ −( ˜μ −μ)m,
V= V= V= V= ∣B∣
μ( ˜μ −μ)
μ + ˜μ + ( ˜μ −μ)m,
the remaining terms are zero,
(.)
where m = (a −b)/(a + b).
If B is a ball in three dimensions, the viscous moment tensor associated with B and an
arbitrary ̃μ is given by
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Viiii = μ∣B∣

̃μ −μ
̃μ + μ ,
Vii j j = −μ∣B∣

̃μ −μ
̃μ + μ (i ≠j),
Vi ji j = Vi j ji = μ∣B∣̃μ −μ
̃μ + μ, (i ≠j),
the remaining terms are zero.
(.)
Theorem (Properties of the viscous moment tensor)
For < ̃μ ≠μ < +∞, let V =
(Vi jpq)d
i,p,q=be the viscous moment tensor associated with the bounded domain B in Rd and
the pair of shear modulus (̃μ, μ). Then
(i) For i, j, p, q = , . . . , d,
Vi jpq = Vjipq,
Vi jpq = Vi jqp,
Vi jpq = Vpqi j.
(.)
(ii) One has
∑
p
Vi jpp = 
for all i, j
and
∑
i
Viipq = 
for all p, q,
or equivalently, V = PVP.
(iii) The tensor V is positive (negative, resp.) definite on the space of symmetric matrices of
trace zero if ̃μ > μ (̃μ < μ, resp.).
(iv) The tensor (/(μ)) V satisfies the following bounds:
Tr ( 
μ V) ≤∣B∣( ˜μ
μ −)((d −) μ
̃μ + d(d −)

),
(.)
Tr( 
μ V)
−
≤

∣B∣( ˜μ
μ −)
((d −) ̃μ
μ + d(d −)

),
(.)
where for C = (Ci jpq), Tr(C) := ∑d
i,j=Ci ji j.


Expansion Methods
Note that the viscous moment tensor, V, is a four tensor and can be regarded, because
of its symmetry, as a linear transformation on the space of symmetric matrices. Note also
that, in view of Theorem , the right-hand sides of (> .) and (> .) are exactly in the
two-dimensional case (d = ) the Hashin–Shtrikman bounds (> .) for the polarization
tensor associated with the same domain B and the conductivity contrast k = ̃μ/μ.
..
Numerical Methods
Let u be the solution to the modified Stokes system (> .). The inverse problem in the
magnetic resonance elastography is to reconstruct the shape and the shear modulus of the
anomaly D from internal measurements of u.
Based on the inner asymptotic expansion (> .) of δu (:= u−U) of the perturbations
in the displacement field that are due to the presence of the anomaly, a reconstruction
method of binary level set type can be designed.
The first step for the reconstruction procedure is to locate the anomaly. This can be
done using the outer expansion of δu, i.e., an expansion far away from the elastic anomaly.
Suppose that z is reconstructed. Since the representation D = z + δB is not unique, one
can fix δ. One uses a binary level set representation f of the scaled domain B:
f (x) = { ,
x ∈B ,
−,
x ∈R/B.
(.)
Let
h(x) = ̃μ (f (x −z
δ
) + ) −μ (f (x −z
δ
) −)
(.)
and let β be a regularization parameter. Then the second step is to fix a window W
(containing z) and solve the following constrained minimization problem
min
̃μ, f L(f , ̃μ) = 

JJJJJJJJJJJ
δu(x) −δ
d
∑
p,q=
∂qU(z)pˆvpq (x −z
δ
) + ∇U(z)(x −z)
JJJJJJJJJJJ

L(W)
+ β ∫W ∣∇h(x)∣dx,
(.)
subject to (> .). Here, ∫W ∣∇h∣dx is the total variation of the shear modulus and ∣∇h∣
is understood as a measure:
∫W ∣∇h∣= sup {∫W h∇⋅v dx, v ∈C
(W) and ∣v∣≤in W}.
This regularization indirectly controls both the length of the level curves and the jumps in
the coefficients.
The local character of the method is due to the decay of
δ
d
∑
p,q=
∂qU(z)pˆvpq (⋅−z
δ ) −∇U(z)(⋅−z)

Expansion Methods 

⊡Fig. -
Reconstruction using the data on the whole domain on the left, a zoom on the anomaly in
the middle, and on the right the reconstruction limited on the subregion deﬁned by the
boxed region on the left
away from z. This is one of the main features of the method. In the presence of noise,
because of a trade-off between accuracy and stability, one has to choose carefully the size
of W. As it has been shown in [], the size of W should not be too small in order to preserve
some stability and not too big so that one can gain some accuracy. See > Fig. -.
The minimization problem (> .) corresponds to a minimization with respect to ̃μ
followed by a step of minimization with respect to f . The minimization steps are over the
set of ̃μ and f and can be performed using a gradient-based method with a line search. Of
importance are the optimal bounds satisfied by the viscous moment tensor V. One should
check at each step whether the bounds (> .) and (> .) on V are satisfied or not.
In the case where they are not, one has to restate the value of ̃μ. Another way to deal with
(> .) and (> .) is to introduce them into the minimization problem (> .) as a
constraint. Set α = Tr(V) and β = Tr(V−) and suppose for simplicity that ̃μ > μ. Then,
(> .) and (> .) can be rewritten (when d = ) as follows
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
α ≤(̃μ −μ)(+ μ
̃μ )∣D∣,
μ(̃μ −μ)
μ + ̃μ ∣D∣≤β−.
(.)
..
Bibliography and Open Questions
Magnetic resonance elastography was first proposed in []. The results provided on this
technique are from []. Theorem and the results on the viscous moment tensor in
Theorem are from [].
In general, the elastic parameters of biological tissues show anisotropic properties, that
is, the local value of elasticity is different in the different spatial directions [] and also
viscous properties. It would be very interesting to extend the algorithm described in this


Expansion Methods
section for detecting the shape of an elastic anomaly and the viscosity and the anisotropy
in its shear modulus. The study of the dependence of the shear modulus as a function of
the frequency is also important [].
.
Photo-Acoustic Imaging of Small Absorbers
..
Physical Principles
In photo-acoustic imaging, optical energy absorption causes thermo-elastic expansion of
the tissue, which in turn leads to propagation of a pressure wave. This signal is mea-
sured by transducers distributed on the boundary of the object, which is in turn used for
imaging optical properties of the object. The significance of photo-acoustic imaging is to
provide images of optical contrasts (based on the optical absorption) with the resolution
of ultrasound.
In pure optical imaging, optical scattering in soft tissues degrades spatial resolution
significantly with depth. As for electrical impedance tomography, even though pure opti-
cal imaging is very sensitive to optical absorption, it can only provide a spatial resolution
of the order of cm at centimeter depths. As discussed before, pure conventional ultra-
sound imaging is based on the detection of the mechanical properties (acoustic impedance)
in biological soft tissues. It can provide good spatial resolution because of its millimetric
wavelength and weak scattering at megahertz frequencies.
If the medium is acoustically homogeneous and has the same acoustic properties as
the free space, then the boundary of the object plays no role and the optical properties of
the medium can be extracted from the measurements of the pressure wave by inverting a
spherical Radon transform.
In the more realistic situation, where a boundary condition has to be imposed on the
pressure field, such an inversion formula does not hold. Using asymptotic analysis one can
develop an efficient approach for reconstructing absorbing regions and absorbing energy
density inside a bounded domain from boundary data. One can also reconstruct the opti-
cal absorption coefficient. In general, it is not possible to infer physiological parameters
from the absorbing energy density. It is the optical absorption coefficient distribution
that directly correlates with tissue structural and functional information such as blood
oxygenation.
..
Mathematical Model
Let Dl, l = , . . . , m, be m absorbing domains inside the non-absorbing background-
bounded medium Ω ⊂Rd, d = or . In an acoustically homogeneous medium, the
photo-acoustic effect is described by the following equation:
∂p
∂t(x, t) −c
s Δp(x, t) = γ ∂H
∂t (x, t),
x ∈Ω,
t ∈R,
(.)

Expansion Methods 

where cs is the acoustic speed in Ω, γ – the dimensionless Grüneisen coefficient in Ω, and
H(x, t) – a heat source function (absorbed energy per unit time per unit volume).
Assuming the stress-confinement condition, the source term can be modeled as
γH(x, t) = δ(t)A(x), where the absorbed optical energy density times the Grüneisen coef-
ficient A = ∑m
l=Al χ(Dl) and Al are constants. Under this assumption, the pressure in an
acoustically homogeneous medium obeys the following wave equation:
∂p
∂t(x, t) −c
s Δp(x, t) = ,
x ∈Ω,
t ∈], T[,
for some final observation time T. The pressure satisfies the Dirichlet boundary condition
p = 
on ∂Ω×], T[
and the initial conditions
p∣t==
m
∑
l=
χ(Dl)Al
and
∂p
∂t ∣
t== 
in Ω.
Suppose that T satisfies (> .). The inverse problem in photo-acoustic imaging is to
determine the supports of nonzero optical absorption (Dl, l = , . . . , m) in Ω and A(x)
from boundary measurements of ∂p
∂
on ∂Ω×], T[.
..
Reconstruction Algorithms
Analogously to (> .), the following identity holds:

cs
m
∑
l=
Al ∫D ∂twy(x,; τ)dx = ∫
T

∫∂Ω
∂p
∂(x, t)wy(x, t; τ) dσ(x) dt,
(.)
where the probe function wy is given by (> .).
...
Determination of Location
Suppose for simplicity that there is only one absorbing object (m = ) which is denoted by
D (= z + δB). Identity (> .) shows that the imaging functional
W(τ, y) := ∫
T

∫∂Ω
∂p
∂(x, t)wy(x, t; τ) dσ(x) dt
(.)
is nonzero only on the interval ]τa, τe[, where τa = dist(y, D)/cs is the first τ for which
the sphere of center y and radius τ hits D and τe is the last τ for which such sphere hits D.
This gives a simple way to detect the location (by changing the source point y and taking
intersection of spheres). The functional W(τ, y) can be used to probe the medium as a
function of τ and y. For fixed y, it is a one-dimensional function and is related to time
reversal in the sense that it is a convolution with a reversed wave.


Expansion Methods
−20
−15
−10
−5
0
5
10
15
−20
6
5
4
3
2
1
0
−15
−10
−5
0
5
10
15
⊡Fig. -
Real conﬁguration of the medium on the left – there are seven optical anomalies of various
size and absorption. Reconstructed conﬁguration on the right – anomalies and are
reconstructed as a single anomaly
A result of numerical simulation to validate the location search algorithm is given in
> Fig. -.
...
Estimation of Absorbing Energy
Consider first the three-dimensional case. If D is a sphere with A(x) = Aχ(D), then
one has
δA ≈cs∣z −y∣∫
τe
τa
∣∫
T

∫∂Ω
∂p
∂(x, t)wy(x, t; τ) dσ(x) dt ∣dτ,
(.)
which gives an approximation of δA.
In two dimensions, one should rather consider the probe wave given by
wθ(x, t; τ) = δ (t + τ −⟨x, θ⟩
c
),
(.)
where θ is a unit vector and τ is a parameter satisfying
τ > max
x∈Ω (⟨x, θ⟩
c
).
One can still use the function
τ ↦∫
T

∫∂Ω
∂p
∂(x, t)wθ(x, t; τ) dσ(x) dt

Expansion Methods 

to probe the medium as a function of τ. This quantity is nonzero on the interval ]τa, τe[,
where τa and τe are defined such that planes ⟨x, θ⟩= cτ for τ = τa and τe hit D. Changing
the direction θ and intersecting stripes gives an efficient way to reconstruct the anomalies.
By exactly the same arguments as in three dimensions, one can show that
δA ≈cs
∫
τe
τa
∣∫
T

∫∂Ω
∂p
∂(x, t)wθ(x, t; τ) dσ(x) dt∣dτ.
(.)
The above formula can be used to estimate δA.
In the case when there are m inclusions, one first computes for each l the quantity
θl,best = argmax
θ∈[,π]
(min
j≠l ∣⟨z j −zl, θ⟩∣)
and then, since along the direction θl,best, the inclusion Dl is well separated from all the
other inclusions, one can use formula (> .) to estimate its δAl.
...
Reconstruction of the Absorption Coeﬃcient
The density A(x) is related to the optical absorption coefficient distribution μa(x) =
μa χ(D) by the equation A(x) = μa(x)Φ(x), where Φ is the fluence rate. The function Φ
depends on the distribution of scattering and absorption within Ω, as well as the light
sources. Based on the diffusion approximation to the transport equation, Φ satisfies
( iω
c + μa(x) −
∇⋅

μa(x) + μs(x)∇) Φ(x) = 
in Ω,
(.)
with the boundary condition

μs
∂Φ
∂
= g
on ∂Ω.
(.)
Here, g denotes the light source, ω a given frequency, c the speed of light, and μs the
scattering coefficient. The diffusion approximation holds when μs ≫μa.
Suppose that d = and μs is known a priori. Define Φby
( iω
c −
∇⋅

μs(x)∇) Φ(x) = 
in Ω,
subject to the boundary condition

μs
∂Φ
∂
= g
on ∂Ω.
Introduce ˆNB to be the Newton potential given by
ˆNB(ξ) := ∫B Γ(ξ −y) dy,
where Γ := −/(π∣x∣) is a fundamental solution of the Laplacian in three dimensions.


Expansion Methods
Let α := δμaΦ(z) = δA. As shown before, α can be reconstructed from ∂p
∂. To extract
δμa from α one uses the following theorem.
Theorem (Fluence rate perturbations)
If B is the unit sphere, then the following
expansion holds:
(Φ −Φ)(z) ≈αμs(z) ˆNB(),
(.)
from which it follows that the (normalized) absorption coefficient can be approximated by
δμa ≈
α
αμs(z) ˆNB() + Φ(z)
.
(.)
Separating δ from μa requires boundary measurements of Φ on ∂Ω. One can use
∫∂Ω g(Φ −Φ) dσ ≈μaΦ
(z)∣D∣
(.)
to separately recover δ from μa.
In the case where μs is unknown, an algorithm to extract the absorption coefficient μa
from absorbed energies obtained at multiple wavelengths was developed in []. It assumes
that the wavelength dependence of the scattering and absorption coefficients are known.
In biological tissues, the wavelength-dependence of the scattering often approximates to a
power law.
..
Bibliography and Open Questions
Basic physical principles of the photo-acoustic effect have been described for instance in
[,]. The results of this section are from [,]. The location search algorithm described
in this section can be extended to the case with limited-view measurements. The half-
space problem has been considered []. In free space, one refers to [–,,,,] for
uniqueness of the reconstruction and inversion procedures based on the spherical Radon
transform. Reconstruction methods with incomplete data have been developed in [].
Sensitivity analysis of a photo-acoustic wave to the presence of small absorbing objects has
been provided in [].
In connection with photo-acoustic imaging, it is worth mentioning the multi-physics
imaging technique proposed in [], which combines electrical impedance tomography
with acoustic tomography. This method makes use of the fact that the absorbed electri-
cal energy causes thermo-elastic expansion of the tissue, which leads to propagation of a
pressure wave. With the notation of > Sect. ., the induced signal is measured on the
boundary of the object and can be used for calculating the absorbed electrical energy,
E = γ∣∇u∣, inside the body, from which the electrical conductivity γ can be reconstructed
using, for instance, the substitution algorithm.
As for magneto-acoustic imaging with magnetic induction, it would be very interesting
to design a robust inversion algorithm when the acoustic speed fluctuates randomly.

Expansion Methods 

.
Conclusion
In this chapter, applications of asymptotic analysis in emerging medical imaging are
outlined. This method leads to very effective and robust reconstruction algorithms in
many imaging problems. Of particular interest are emerging multi-physics or hybrid-
imaging approaches. These approaches allow one to overcome the severe ill-posedness
character of image reconstruction. It would be very interesting to analytically investi-
gate their robustness, with respect to incomplete data, measurement, and medium noises.
Another important problem is to take into account the effect of anisotropy, dissipation, or
attenuation in biological tissues.
.
Cross-References
> EIT
> Magnetic Resonance and Ultrasound Elastography
> Optical Imaging
> Photoacoustic and Thermoacoustic Tomography: Image Formation and Principles
> Thermoacoustic Tomography
> Tomography
> Wave Phenomena
References and Further Reading
. Agranovsky M, Kuchment P () Uniqueness
of reconstruction and an inversion procedure
for thermoacoustic and photoacoustic tomog-
raphy with variable sound speed. Inverse Prob
:–
. Agranovsky M, Kuchment P, Kunyansky L ()
On reconstruction formulas and algorithms for
the thermoacoustic and photoacoustic tomogra-
phy. In: Wang LH (ed) Photoacoustic imaging
and spectroscopy. CRC Press, Boca Raton, pp
–
. AmbartsoumianG,PatchS() Thermoacous-
tic tomography–Implementation of exact back-
projection formulas, math.NA/
. Ammari H () An inverse initial boundary
value problem for the wave equation in the pres-
ence of imperfections of small volume. SIAM
J Contr Optim :–
. Ammari H () An introduction to mathemat-
ics of emerging biomedical imaging. Mathéma-
tiques and applications, vol . Springer, Berlin
. Ammari H, Asch M, Guadarrama Bustos L,
Jugnon V, Kang H Transient wave imaging with
limited-view data, submitted
. Ammari
H,
Bonnetier
E,
Capdeboscq
Y,
Tanter M, Fink M () Electrical impedance
tomography by elastic deformation. SIAM J Appl
Math :–
. Ammari H,BossyE,JugnonV,andKangH Math-
ematical modelling in photo-acoustic imaging of
small absorbers, SIAM Rev., to appear
. Ammari H, Bossy E, Jugnon V, and Kang H
Quantitative photoacoustic imaging of small
absorbers. submitted
. Ammari
H,
Capdeboscq
Y,
Kang
H,
Kozhemyak A () Mathematical models


Expansion Methods
andreconstructionmethodsinmagneto-acoustic
imaging. Euro J Appl Math :–
. Ammari H, Garapon P, Guadarrama Bustos L,
Kang H Transient anomaly imaging by the acous-
tic radiation force. J Diff Equat, to appear
. Ammari H, Garapon P, Jouve F () Separation
of scales in elasticity imaging: a numerical study,
J Comput Math :–.
. Ammari H, Garapon P, Kang H, Lee H ()
A method of biological tissues elasticity recon-
struction using magnetic resonance elastog-
raphy measurements. Quart Appl Math :
–
. Ammari H, Garapon P, Kang H, Lee H Effec-
tive viscosity properties of dilute suspensions of
arbitrarily shaped particles. submitted
. Ammari H, Griesmaier R, Hanke M ()
Identification of small inhomogeneities: asymp-
totic factorization. Math Comp :–
. Ammari H, Iakovleva E, Kang H, Kim K ()
Direct algorithms for thermal imaging of small
inclusions. SIAM Multiscale Model Simul :
–
. Ammari H, Iakovleva E, Lesselier D () Two
numerical methods for recovering small electro-
magnetic inclusions from scattering amplitude at
a fixed frequency. SIAM J Sci Comput :–
. Ammari H, Iakovleva E, Lesselier D () A
MUSIC algorithm for locating small inclusions
buried in a half-space from the scattering ampli-
tude at a fixed frequency. SIAM Multiscale Model
Simul :–
. Ammari H, Iakovleva E, Lesselier D, Perrusson G
() A MUSIC-type electromagnetic imaging
of a collection of small three-dimensional inclu-
sions. SIAM J Sci Comput :–
. Ammari H, Kang H () High-order terms
in the asymptotic expansions of the steady-state
voltage potentials in the presence of conductivity
inhomogeneities of small diameter. SIAM J Math
Anal :–
. Ammari H, Kang H () Reconstruction of
small inhomogeneities from boundary measure-
ments. Lecture Notes in Mathematics, vol .
Springer, Berlin
. Ammari H, Kang H () Boundary layer tech-
niques for solving the Helmholtz equation in the
presence of small inhomogeneities. J Math Anal
Appl :–
. Ammari H, Kang H () Reconstruction of
elastic inclusions of small volume via dynamic
measurements. Appl Math Opt :–
. Ammari H, Kang H () Polarization and
moment tensors: with applications to inverse
problems and effective medium theory. Applied
mathematical
sciences,
vol
.
Springer,
New York
. Ammari H, Kang H, Lee H () A boundary
integral method for computing elastic moment
tensors for ellipses and ellipsoids. J Comp Math
:–
. Ammari H, Kang H, Nakamura G, Tanuma K
() Complete asymptotic expansions of solu-
tions of the system of elastostatics in the presence
of an inclusion of small diameter and detection of
an inclusion. J Elasticity :–
. Ammari H, Khelifi A () Electromagnetic
scattering by small dielectric inhomogeneities.
J Math Pures Appl :–
. Ammari H, Kozhemyak A, Volkov D ()
Asymptotic formulas for thermography based
recoveryofanomalies.NumerMathTMA:–
. Ammari H, Kwon O, Seo JK, Woo EJ ()
Anomaly detection in Tscan trans-admittance
imaging system. SIAM J Appl Math :–
. Ammari H, Seo JK () An accurate formula
for the reconstruction of conductivity inhomo-
geneities. Adv Appl Math :–
. Amalu WC, Hobbins WB, Elliot RL ()
Infrared imaging of the breast – an overview. In:
Bronzino JD (ed) Medical devices and systems,
the biomedical engineering handbook, rd edn.,
chap . CRC Press, Baton Rouge
. Assenheimer M, Laver-Moskovitz O, Malonek D,
Manor D, Nahliel U, Nitzan R, Saad A ()
The T-scan technology: electrical impedance as
a diagnostic tool for breast cancer detection.
Physiol Meas :–
. Bardos C () A mathematical and deter-
ministic analysis of the time-reversal mirror in
Inside out: inverse problems and applications.
Mathematical Science Research Institute Publi-
cation, vol . Cambridge University of Press,
Cambridge, pp –
. Bardos C, Lebeau G, Rauch J () Sharp suffi-
cient conditions for the observation, control, and
stabilization of waves from the boundary. SIAM J
Contr Optim :–

Expansion Methods 

. Bercoff J, Tanter M, Fink M () Supersonic
shear imaging: a new technique for soft tissue
elasticity mapping. IEEE Trans Ultrasonics Ferro
Freq Contr :–
. Bercoff J, Tanter M, Fink M () The role of
viscosity in the impulse diffraction field of elas-
tic waves induced by the acoustic radiation force.
IEEE Trans Ultrasonics Ferro Freq Contr :
–
. Borcea L, Papanicolaou GC, Tsogka C, Berry-
mann JG () Imaging and time reversal in
random media. Inverse Problems :–
. Brühl M, Hanke M, Vogelius MS () A
direct impedance tomography algorithm for
locating small inhomogeneities. Numer Math :
–
. Capdeboscq Y, De Gournay F, Fehrenbach J,
Kavian O () An optimal control approach
to imaging by modification. SIAM J Imaging Sci
:–
. Capdeboscq Y, Kang H () Improved bounds
on the polarization tensor for thick domains.
In: Inverse problems, multi-scale analysis and
effective medium theory. Contemporary Mathe-
matics, vol . American Mathematical Society,
RI, pp –
. Capdeboscq Y, Kang H () Improved Hashin-
Shtrikman bounds for elastic moment ten-
sors and an application. Appl Math Opt :
–
. Capdeboscq Y, Vogelius MS () A general
representation formula for the boundary volt-
age perturbations caused by internal conductivity
inhomogeneities of low volume fraction. Math
Model Num Anal :–
. Capdeboscq Y, Vogelius MS () Optimal
asymptotic estimates for the volume of inter-
nal inhomogeneities in terms of multiple bound-
ary measurements. Math Model Num Anal :
–
. Cedio-Fengya DJ, Moskow S, Vogelius MS ()
Identification of conductivity imperfections of
small diameter by boundary measurements: con-
tinuous dependence and computational recon-
struction. Inverse Prob :–
. Chambers DH, Berryman JG () Analysis of
the time-reversal operator for a small spherical
scatterer in an electromagnetic field. IEEE Trans
Antennas Propag :–
. Chambers DH, Berryman JG () Time-
reversal analysis for scatterer characterization.
Phys Rev Lett :–
. Devaney AJ () Time reversal imaging of
obscured targets from multistatic data. IEEE
Trans Antennas Propagat :–
. Fink M () Time-reversal acoustics. Contemp
Math :–
. Fisher AR, Schissler AJ, Schotland JC () Pho-
toacoustic effect of multiply scattered light. Phys
Rev E :
. Fouque JP, Garnier J, Papanicolaou G. Solna K
() Wave propagation and time reversal in
randomly layered media. Springer, New York
. Friedman A, Vogelius MS () Identification
of small inhomogeneities of extreme conductiv-
ity by boundary measurements: a theorem on
continuous dependence. Arch Rat Mech Anal
:–
. GebauerB,ScherzerO()Impedance-acoustic
tomography. SIAM J Appl Math :–
. Greenleaf
JF, Fatemi M, Insana M ()
Selected methods for imaging elastic properties
of biological tissues. Annu Rev Biomed Eng :
–
. Haider S, Hrbek A, Xu Y () Magneto-
acousto-electrical
tomography:
a
potential
method
for
imaging
current
density
and
electrical impedance. Physiol Meas :–
. Haltmeier M, Schuster T, Scherzer O () Fil-
tered backprojection for thermoacoustic com-
puted tomography in spherical geometry. Math
Meth Appl Sci :-
. Haltmeier M, Scherzer O, Burgholzer P, Nuster R,
Paltauf G () Thermoacoustic tomography
and the circular Radon transform: exact inver-
sion formula. Math Model Meth Appl Sci
():–
. Hanke M () On real-time algorithms for
the location search of discontinuous conductiv-
ities with one measurement. Inverse Prob :
.
. Harrach B, Seo JK () Detecting inclusions
in electrical impedance tomography without ref-
erence measurements. SIAM J Appl Math :
–
. Isakov V () Inverse problems for partial
differential equations, applied mathematical sci-
ences, vol . Springer, New York


Expansion Methods
. Kang H, Kim E, Kim K () Anisotropic
polarization
tensors
and
determination
of
an anisotropic inclusion. SIAM J Appl Math
:–
. Kang H, Seo JK () Layer potential technique
for the inverse conductivity problem. Inverse
Prob :–
. Kang H, Seo JK () Identification of domains
with near-extreme
conductivity:
global sta-
bility and error estimates. Inverse Prob :
–
. Kang H, Seo JK () Inverse conductivity prob-
lem with one measurement: uniqueness of balls
in R. SIAM J Appl Math :–
. Kang H, Seo JK () Recent progress in the
inverse conductivity problem with single mea-
surement. Inverse problems and related fields.
CRC Press, Boca Raton, pp –
. Kim S, Kwon O, Seo JK, Yoon JR () On a
nonlinear partial differential equation arising in
magnetic resonance electrical impedance imag-
ing. SIAM J Math Anal :–
. Kim YJ, Kwon O, Seo JK, Woo EJ () Unique-
ness and convergence of conductivity image
reconstruction in magnetic resonance electri-
cal impedance tomography. Inverse Prob :
–
. Kim S, Lee J, Seo JK, Woo EJ, Zribi H ()
Multifrequency transadmittance scanner: math-
ematical framework and feasibility. SIAM J Appl
Math :–
. Kohn R, Vogelius M () Identification of
an unknown conductivity by means of mea-
surements at the boundary. In: McLaughlin D
(ed) Inverse problems. SIAM-AMS Proc. No. ,
American Mathmetical Society, Providence, pp
–
. Kolehmainen V, Lassas M, Ola P () The
inverse conductivity problem with an imper-
fectly known boundary. SIAM J Appl Math :
–
. Kuchment P, Kunyansky L () Mathematics of
thermoacoustic tomography. Euro J Appl Math
:–
. Kuchment P, Kunyansky L Synthetic focusing in
ultrasound modulated tomography. Inverse Prob
Imag to appear
. Kwon O, Seo JK () Total size estimation and
identification of multiple anomalies in the inverse
electrical impedance tomography. Inverse Prob
:–
. Kwon O, Seo JK, Yoon JR () A real-time algo-
rithm for the location search of discontinuous
conductivities with one measurement. Comm
Pure Appl Math :–
. Kwon O, Yoon JR, Seo JK, Woo EJ, Cho YG ()
Estimation of anomaly location and size using
impedance tomography. IEEE Trans Biomed Eng
:–
. Li X, Xu Y, He B () Magnetoacoustic tomog-
raphy with magnetic induction for imaging elec-
trical impedance of biological tissue. J Appl Phys
, Art. No. 
. Li X, Xu Y, He B () Imaging electrical
impedance from acoustic measurements by
means of magnetoacoustic tomography with
magnetic induction (MAT-MI). IEEE Trans
Biomed Eng :–
. Lipton R () Inequalities for electric and elas-
tic polarization tensors with applications to ran-
dom composites. J Mech Phys Solids :–
. Manduca
A,
Oliphant
TE,
Dresner
MA,
Mahowald
JL,
Kruse
SA,
Amromin
E,
Felmlee JP, Greenleaf JF, Ehman RL ()
Magnetic resonance elastography: non-invasive
mapping of tissue elasticity. Med Image Anal
:–
. Milton GW () The theory of compos-
ites. Cambridge University Press, Cambridge,
Cambridge monographs on applied and compu-
tational mathematics
. Montalibet A, Jossinet J, Matias A, Cathignol D
() Electric current generated by ultrasoni-
cally induced Lorentz force in biological media.
Med Biol Eng Comput :–
. Muthupillai
R,
Lomas
DJ,
Rossman
PJ,
Greenleaf JF, Manduca A, Ehman RL ()
Magnetic resonance elastography by direct visu-
alization of propagating acoustic strain waves.
Science :–
. Mast TD, Nachman A, Waag RC () Focus-
ing and imagining using eigenfunctions of the
scattering operator. J Acoust Soc Am : –
. Parisky YR, Sardi A, Hamm R, Hughes K, Esser-
man L, Rust S, Callahan K () Efficacy of
computerized infrared imaging analysis to eval-
uate mammographically suspicious lesions. Am J
Radiol :–

Expansion Methods 

. Patch SK, Scherzer O () Guest editors’ intro-
duction: photo- and thermo-acoustic imaging.
Inverse Prob :S–
. Pernot M, Montaldo G, Tanter M, Fink M
() “Ultrasonic stars” for time-reversal focus-
ing using induced cavitation bubbles. Appl Phys
Lett :
. Pinker S () How the mind works. Penguin
Science, Harmondsworth
. Prada C, Thomas J-L, Fink M () The iterative
timereversal process:analysisof theconvergence.
J Acoust Soc Am :–
. Seo JK, Kwon O, Ammari H, Woo EJ ()
Mathematical framework and anomaly estima-
tion algorithm for breast cancer detection using
TSconfiguration. IEEE Trans Biomed Eng
:–
. Seo JK, Woo EJ () Multi-frequency electri-
cal impedance tomography and magnetic res-
onance electrical impedance tomography in
mathematical modeling in biomedical imaging
I, Lecture Notes in Mathematics: Mathemat-
ical Biosciences Subseries, vol . Springer,
Berlin
. Sinkus R, Tanter M, Catheline S, Lorenzen J,
Kuhl C, Sondermann E, Fink M () Imag-
ing anisotropic and viscous properties of breast
tissue by magnetic resonance-elastography. Mag
Res Med :–
. Sinkus R, Siegmann K, Xydeas T, Tanter M,
Claussen C, Fink M () MR elastography
of breast lesions: understanding the solid/liquid
duality can improve the specificity of contrast-
enhanced MR mammography. Mag Res Med
:–
. Sinkus R, Tanter M, Xydeas T, Catheline S,
Bercoff J, Fink M () Viscoelastic shear
properties
of
in
vivo breast
lesions
mea-
sured by MR elastography. Mag Res Imag :
–
. Tanter M, Fink M () Time reversing waves
for biomedical applications in mathematical
modeling in biomedical imaging I, Lecture Notes
in Mathematics: Mathematical Biosciences Sub-
series, vol . Springer, Berlin
. Therrien CW () Discrete random signals
and statistical signal processing. Prentice-Hall,
Englewood Cliffs
. Vogelius MS, Volkov D () Asymptotic for-
mulas for perturbations in the electromagnetic
fields due to the presence of inhomogeneities.
Math Model Numer Anal :–
. Wang LV, Yang X () Boundary conditions
in photoacoustic tomography and image recon-
struction. J Biomed Opt :
. Xu M, Wang LV () Photoacoustic imag-
ing
in
biomedicine.
Rev
Sci
Instrum
:

. Xu Y, Wang LV, Ambartsoumian G, Kuch-
ment P () Reconstructions in limited view
thermoacoustic
tomography.
Med
Phys
:
–


Sampling Methods
Martin Hanke ⋅Andreas Kirsch
.
Introduction and Historical Background......................................
.
The Factorization Method in Impedance Tomography........................
..
Impedance Tomography in the Presence of Insulating Inclusions...............
..
Conducting Obstacles...................................................................
..
Local Data................................................................................
..
Other Generalizations..................................................................
...The Half Space Problem................................................................
...The Crack Problem......................................................................
.
The Factorization Method in Inverse Scattering Theory.......................
..
Inverse Acoustic Scattering by a Sound-Soft Obstacle.............................
..
Inverse Electromagnetic Scattering by an Inhomogeneous Medium............
..
Historical Remarks and Open Questions............................................
.
Related Sampling Methods......................................................
..
The Linear Sampling Method.........................................................
..
MUSIC....................................................................................
..
The Singular Sources Method.........................................................
..
The Probe Method......................................................................
.
Appendix.........................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Sampling Methods
.
Introduction and Historical Background
The topic of this chapter is devoted to shape identification problems, i.e., problems where
the shape of an object has to be determined from indirect measurements. Such a situation
typically occurs in problems of tomography, in particular electrical impedance tomogra-
phy or optical tomography. For example, a current through a homogeneous object will
in general induce a different potential than the same current through the same object
containing an enclosed cavity. In impedance tomography, the task is to determine the
shape of the cavity from measurements of the potential on the boundary of the object.
For survey articles on this subject we refer to [], [], and to
> Chap. in this
volume.
As a second of these fields we mention inverse scattering problems where one wants to
detect – and identify – unknown objects through the use of acoustic, electromagnetic, or
elastic waves. Similar to above, one of the important problems in inverse scattering theory
is to determine the shape of the scattering obstacle from field measurements. Applications
of inverse scattering problems occur in such diverse areas as medical imaging, material
science, nondestructive testing, radar, remote sensing, or seismic exploration. A survey on
the state of the art of the mathematical theory and numerical approaches for solving inverse
time harmonic scattering problems until can be found in the standard monograph
[], see also
> Chap. or [] for an introduction and survey on inverse scattering
problems.
Shape identification problems are intrinsically nonlinear, i.e., the measured quantities
do not depend linearly on the shape. Even the notion of linearity does not make sense since,
in general, the set of admissible shapes does not carry a linear structure. Traditional (and
still very successful) approaches describe the objects by appropriate parameterizations and
compute the parameters by iterative schemes as, e.g., Newton-type methods. Newton-type
methods are attractive because of their fast convergence, although they require a good ini-
tial guess to converge. Still, these methods are widely used – partly because techniques from
shape optimization theory can be used to characterize the required first or second order
derivatives. We refer to [, ] for general references, and to [, , ] for applications
in inverse scattering theory.
While classical iterative algorithms use explicit parameterizations of the object, new
shape optimization methods have been developed since around which completely
avoid the use of parameterizations and replace the classical Fréchet derivative by a geomet-
rically motivated topological derivative, see, e.g., [] for the application of these methods
in the inverse scattering context. Yet these methods have the shortcoming that they are not
able to change the number of connectivity components during the algorithm. This has led
to the development of level set methods which are based on implicit representations of the
unknown object involving an “evolution parameter” t. We refer to [] or
> Chap. for
recent surveys.
While very successful in many cases, iterative methods for shape identification prob-
lems – may they use classical tools as the Fréchet derivative or more recent techniques
such as domain derivatives, level curves, or topological derivatives – are computation-
ally very expensive since they require the solution of a direct problem in every step.

Sampling Methods 

Furthermore, for many important cases, the convergence theory is still missing. This is due
to the fact that these problems are not only nonlinear but also because their linearizations
are improperly posed. Although there exist many results on the convergence of (regular-
ized) iterative methods for solving nonlinear improperly posed problems (see, e.g., [, ]
or > Chap. ), the assumptions for convergence are not met in the applications to shape
identification problems. (Or, at least, it is unknown whether these assumptions are fulfilled
or not.)
These difficulties and disadvantages of iterative schemes gave rise to the development
of different classes of non-iterative methods which avoid the solution of a sequence of
direct problems. We briefly mention decomposition methods (according to the notion of
[]) which consist of an analytic continuation step (which is linear but highly improperly
posed) and a nonlinear step of finding the boundary of the unknown domain by forcing
the boundary condition to hold. We refer to > Sect. ...
This chapter will focus on a different class of non-iterative methods, the so-called sam-
pling methods. The common idea of these methods is the construction of criteria on the
known data to decide whether a given test object (a point or a curve or a set) is inside or
outside the unknown domain D. Then, a grid of “sampling” points is chosen in a region
that is known to contain the unknown domain D, in order to compute the (approximate)
characteristic function of D. The different kinds of sampling methods differ in the way of
defining the criteria and in the type of test objects.
One of the first methods which falls into this class has been developed by David Colton
and one of the authors (A.K.) in ([]), now known as the Linear Sampling Method.
Its origin goes back to the Dual Space Method developed between and (see, e.g.,
[]). The numerical implementation of the Linear Sampling Method is extremely simple
and fast because sampling is done by points z only. For every sampling point z one has to
compute the field of a point source in z with respect to the background medium (Essen-
tially, one has to compute the fundamental solution of the underlying differential operator;
if the background is constant the response is given analytically) and evaluate a series, i.e.,
a finite sum in practice.
A problem with the Linear Sampling Method from the mathematical point of view
is that the computable criterion is only a sufficient condition which is, in general, not
necessary. The Factorization Method overcomes this drawback and yields a criterion for
z which is both necessary and sufficient. Therefore, this method succeeds to provide a sim-
ple formula for the characteristic function of D which can easily be used for numerical
computations.
The Factorization Method consists of three components. First, a “measurement opera-
tor” M is factorized in three factors of the form
M = AGA∗,
(.)
where A∗is the dual operator of A with respect to the Ltopology. Second, the range of A
is characterized by the obstacle D, and vice versa. Third, if the operator G satisfies a certain
coercivity condition, then the range of A can be determined by the given operator M. This
requires some functional analytic results on range identities which we have collected in an
> appendix.


Sampling Methods
Combining these three steps yields an explicit characterization of the unknown obstacle
D by the measurement operator M.
The outline of this chapter is as follows. First, in Sect. ., we present the Factoriza-
tion Method for two different settings in the impedance tomography context. In the very
first setting we deal with insulating inclusions, and this allows for a very elementary pre-
sentation of the method. Afterwards, in Sect. ., we turn to applications from inverse
acoustic and (full D) electromagnetic scattering. Finally, we give a brief overview of other
sampling type methods in Sect. ., including the original Linear Sampling Method and
MUSIC type methods.
.
The Factorization Method in Impedance Tomography
We start with the impedance tomography problem. Consider an object, that fills a simply
connected domain Ω ⊂Rn with Lipschitz continuous boundary, where n = or n = ,
respectively. We assume that the object is a homogeneous and isotropic conductor, except
for a finite number m of so-called inclusions, given by domains Di ⊂Ω, i = , . . . , m, with
Lipschitz continuous boundaries ∂Di. We assume that these domains are well separated,
i.e., Di ∩Dj = / when i ≠j, and that the complement of the closure D of D = ⋃m
i=Di is
connected. In impedance tomography, currents are imposed through the boundary of the
object and the resulting boundary potentials are measured. Linear independent bound-
ary currents yield independent pieces of information, which can be used as input data to
determine the unknown shapes and positions of the inclusions.
In practice, at least in most medical applications, the boundary currents have a fre-
quency in the kHz range (–kHz), and the dc approximation with a positive real
conductivity σ (or possibly a positive definite tensor) serves as a suitable physical model.
Without loss of generality, we can always assume that the homogeneous conductivity of
the object equals σ = , whereas σ ≠within the inclusions.
Below we will consider two specific scenarios. In the first one, we assume that the inclu-
sions are insulating, formally corresponding to the case where σ = . Our analysis of the
Factorization Method for the corresponding inverse problem will be somewhat nonstan-
dard; in particular, we employ a factorization in only two factors instead of three as in
(> .), but this allows for a most elementary treatment of the method.
Subsequently, we show how to deal with conducting obstacles with a conductivity ten-
sor σ. Of particular interest is the setting where the object under consideration can be
modeled as a half space: examples of this sort arise in geophysics, cf. [], and in medicine,
e.g., when a planar device is used for mammography examinations, cf. []. Another inter-
esting application for the half space problem has recently been considered in []. We
therefore briefly describe the differences that arise in this context (mainly in the theoretical
justification of the method).
We conclude our case studies with a setting where the inclusion degenerates to a crack,
i.e., an n −dimensional smooth manifold within Ω. This application requires some care
in the appropriate implementation of the Factorization Method.

Sampling Methods 

..
Impedance Tomography in the Presence of Insulating
Inclusions
To begin with, we take up the case where Ω is a bounded domain, and the domains Di ⊂Ω,
i = , . . . , m, correspond to insulating inclusions. Within the dc model the potential u
induced by a boundary current f is given by
Δu= 
in Ω/D,
∂
∂u= 
on ∂D,
∂
∂u= f
on ∂Ω,
∫∂Ω uds = ,
(.)
where the normal vectors
on ∂Ω and ∂D are pointing into the exterior of Ω and D,
respectively. In order to make the forward problem (> .) well-posed, we restrict f to
be square integrable with vanishing mean on ∂Ω. The corresponding set of admissible
boundary currents is
L
◇(∂Ω) = {f ∈L(∂Ω) : ∫∂Ω f ds = }.
(.)
Under these assumptions problem (> .) has a unique (weak) solution
u∈H
◇(Ω/D) = {u ∈H(Ω/D) : ∫∂Ω uds = }.
The last condition in (> .) normalizes this boundary potential to have vanishing mean;
without this condition, the solution would only be unique up to additive constants, reflect-
ing the fact that only the voltage, i.e., the difference between the potential at two different
points, is a well-defined physical quantity.
Therefore, the direct problem is to determine the field uwhen f and D are given.
The quantity that is measured in impedance tomography is the trace g= u∣∂Ω, i.e.,
the boundary potential. The corresponding measurement operator
Λ: {L
◇(∂Ω)
→
L
◇(∂Ω),
f
↦
g= u∣∂Ω,
(.)
i.e., the so-called Neumann–Dirichlet operator, is usually referred to as absolute data in
impedance tomography.
The inverse problem is to determine the shape of D from the measurement operator
Λ.
For the Factorization Method we employ relative data, that is, the difference between
the above Neumann–Dirichlet operator and the corresponding one for a completely homo-
geneous object in Ω. To be precise, let u1 be the reference solution for the homogeneous
object, given the same boundary current f ∈L
◇(∂Ω),
Δu1 = 
in Ω,
∂
∂u1 = f
on ∂Ω,
∫∂Ω u1ds = ,
(.)


Sampling Methods
and denote by Λ1 : f ↦g1 = u1∣∂Ω the Neumann–Dirichlet map associated with (> .).
It is the relative data M = Λ−Λ1 that later enters in (> .) to lay the grounds for the
setting of the Factorization Method.
We refer to
> Chap. for a more elaborate treatment of the impedance tomography
problem, but we will see below that Λ−Λ1 is a bounded and positive self adjoint operator.
We also do not discuss practical issues such as electrode models that should be incorpo-
rated into a realistic problem setting. For the same reason we do not comment on how to
obtain relative data in practice; the generation of accurate reference data is indeed a diffi-
cult subject, and some workarounds have therefore been suggested for this purpose. (We
like to highlight one recent approach from [], where different frequencies are used in
the experimental setup to obtain relative data. This approach, however, leads to a different
variant of the Factorization Method than the one that is described here.) Our specification
of the impedance tomography problem is thus a purely mathematical one, although it can
be shown to be a pretty reasonable approximation of the real case, cf., e.g., [, ].
Before we continue, we pause to comment on the nature of the relative data introduced
above. Any function h in the range R(Λ−Λ1) of Λ−Λ1 corresponds to a suitable input
current f ∈L
◇(∂Ω), such that h is the trace of w = u−u1 : Ω/D →R, where uand u1 are
the solutions of (> .) and (> .), respectively. As uand u1 are both harmonic in Ω/D,
the same holds true for w; on top of that, like uand u1, w has finite Hnorm on Ω/D,
as well as vanishing mean on ∂Ω. Moreover, w has homogeneous Neumann boundary
conditions on ∂Ω, as uand u1 both satisfy the same Neumann boundary condition. And
finally, on ∂Di, i = , . . . , m, we have
∫∂Di
∂
∂wds = −∫∂Di
∂
∂u1ds = 
by virtue of Green’s formula. Accordingly, the range of Λ−Λ1 consists of traces of
potentials w from
W = {w ∈H
◇(Ω/D) : Δw = , ∂
∂w = on ∂Ω,∫∂Di
∂
∂wds = , i = , . . . , m}.
(.)
It is well known that harmonic functions have infinite smoothness. Moreover, as the ele-
ments of W have a vanishing Neumann derivative on ∂Ω, the “variation” of w on ∂Ω can
only be caused by their behavior near the boundary of D – unless the boundary of Ω is
non-smooth. In other words, the (local) variation of the trace of some function w ∈W is
an indicator for the (local) width of the domain Ω/D. In fact, as we will show next, it is
possible to characterize D completely, if the set of all traces of W on ∂Ω were known. (For
one insulating inclusion it is even known that the trace of one single potential w ∈W is
enough to identify D, cf., e.g., []. For conducting obstacles, with known conductivity, the
corresponding uniqueness problem is still open).
To this end, we introduce the Neumann function N(⋅, z) associated with the Laplacian
in the domain Ω, which is given as the (distributional) solution of the problem

Sampling Methods 

−ΔN(x, z) = δ(x −z)
in Ω,
∂
∂N(x, z) = −

∣∂Ω∣
on ∂Ω,
∫∂Ω N(x, z)ds(x) = ,
(.)
where z ∈Ω is kept fixed, and the differential operators act on the x-variable only. To
achieve a unique solution we have normalized N(⋅, z) to have vanishing mean on ∂Ω. The
directional derivative
Uz(x) = p ⋅gradz N(x, z)
(.)
with respect to z of N in direction p (of unit length) yields the potential of a dipole source
in z with moment p in the presence of an insulated boundary ∂Ω: We refer to Uz as the
dipole potential, tacitly assuming the dipole moment to be fixed. (All subsequent results
hold true for an arbitrary choice of p ∈Rn with ∣p∣= , and it appears that there is still space
to improve the numerical performance of the method, especially in three space dimensions,
provided that this property is exploited in an optimal way.) We remark that Uz behaves like
Uz(x) ∼
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩

π
(x −z) ⋅p
∣x −z∣,
n = ,

π
(x −z) ⋅p
∣x −z∣
,
n = ,
as x →z,
(.)
and, in fact, Uz agrees with the right-hand side of (> .) up to a harmonic function. This
statement holds true for every fixed z ∈Ω.
Now we are ready to formulate the characterization of the inclusion D as it has been
established by Brühl in his dissertation [] (see also []), and which constitutes the basis
for the Factorization Method.
Theorem 
A point z ∈Ω belongs to D, if and only if the trace ϕz = Uz∣∂Ω coincides with
the trace of some potential w ∈W.
Proof
First, let z ∈D. Then the dipole potential Uz is harmonic in Ω/{z}, i.e., in Ω/D
and in a neighborhood of ∂D. Accordingly Uz belongs to H
◇(Ω/D). As N(x, z) has the
same Neumann boundary data for any z ∈Rn, its directional derivative with respect to z
has vanishing Neumann data on ∂Ω. Moreover, according to Green’s formula,
∫∂Di
∂
∂Uzds = 
(.)
for every component Di of D which does not contain z; however, as the total flux of Uz
across ∂(Ω/D) vanishes as well, (> .) must also hold true for that component Di of D
which does contain z. Therefore, Uz ∈W, and its trace belongs to the corresponding trace
space.
Now, let z ∉D, and assume that the trace ϕz of the dipole potential Uz is the trace of
a potential w ∈W. As we have seen in the first part of this proof, Uz and w thus have the
same Cauchy data on ∂Ω, and it follows from the uniqueness of solutions of the Cauchy
problem for the Poisson equation that Uz and w coincide in Ω/(D ∪{z}), where both are


Sampling Methods
harmonic. (It is here where the assumption on the connectedness of Ω/D is needed.) Now,
w extends as a harmonic function into the point z, and, hence, is bounded near z whereas
Uz is not, cf. (> .). This provides the desired contradiction.
In the last case, where z sits on the boundary of D, we can use the same argument as
before to show that w and Uz coincide in Ω/D. According to (> .), Uz must therefore
have a finite H-norm on Ω/D, which contradicts the asymptotic behaviour (> .) near
z ∈∂D. (This argument requires the Lipschitz continuity of ∂D, because this assumption
makes sure that we can find an open cone C ⊂Ω/D with vertex in z, and hence, that the
integral ∫C ∣grad Uz∣dx is unbounded.)
∎
It turns out that the potentials w = u−u1, which provide the given relative data, have
additional features that are not captured by the description of the set W of (> .). For
example, if the boundaries of the domains Di are smooth, then the potential uof (> .)
can be extended by reflection to a certain subset of D, showing that w has a harmonic exten-
sion to a larger domain than just Ω/D (see [] Appendix). Therefore, the space spanned
by the relative data is smaller than the trace space of W in general. Still, there is a means
to deduce this trace space from the given relative data – and the appropriate tool is the
Factorization Method.
At this point we deviate from the usual presentation of the Factorization Method to opt
for a more elementary derivation of the main results: Instead of the usual factorization of
the data map in three factors as in (> .) we follow the approach in [], and factor the
relative data in only two parts, namely,
Λ−Λ1 = K∗K,
(.)
where K∗is an appropriate adjoint of the operator K given by
K : f ↦
⎧⎪⎪⎨⎪⎪⎩
u−u1
in Ω/D,
ci −u1
in Di, i = , . . . , m,
(.)
and the real numbers ci in (> .) are the means of the potential uat the boundaries of
the insulating inclusions, i.e.,
ci =

∣∂Di∣∫∂Di
uds,
i = , . . . , m.
(.)
We claim (see Theorem below for a proof) that K is a continuous operator from L
◇(∂Ω)
to X, where
X = {v : Ω →R : v ∣Ω/D ∈H
◇(Ω/D),v∣
D ∈H(D),∫∂Di
[v]ds = , i = , . . . , m}. (.)
In this definition, again, the subscript ◇indicates that any v ∈X is required to have
vanishing mean on ∂Ω, and
[v] = v+∣∂D −v−∣∂D

Sampling Methods 

denotes the jump of v across the boundary of the inclusion(s), defined in the appropriate
trace spaces. Here and below we denote by v+ and v−the restriction of a generic element
v ∈X to Ω/D and D, respectively. We equip X with the inner product
(v,w)X = ∫Ω/∂D grad v ⋅grad wdx = ∫D gradv−⋅grad w−dx + ∫Ω/D grad v+ ⋅gradw+dx,
(.)
which turns X into a Hilbert space. Take note that H
◇(Ω), i.e., the set of all functions from
H(Ω) with vanishing mean on ∂Ω, is a subset of X.
Lemma 
Let K ⊂X be the set of all elements w ∈X that are harmonic in Ω/∂D and
satisfy
∂
∂w = 
on ∂Ω
and
[ ∂
∂w] = 
on ∂D.
Then K is the orthogonal complement of H
◇(Ω) in X.
Proof
Using Green’s formula for any v ∈H
◇(Ω) and any w ∈X that is harmonic in Ω/∂D
we obtain
∫Ω/∂D grad v ⋅grad wdx = ∫∂Ω v ∂w
∂ds −∫∂D v ∂w+
∂
ds + ∫∂D v ∂w−
∂
ds
= ∫∂Ω v ∂w
∂ds −∫∂D v [ ∂w
∂] ds,
(.)
as v has a well-defined unique trace on ∂D. Now, if we choose w ∈K then both integrals
vanish, and hence wv with respect to the scalar product in X.
Vice versa, pick w ∈X from the orthogonal complement of H
◇(Ω), and let v be a C∞
function with compact support in Ω/D, then Green’s formula yields
∫Ω/D wΔvdx = ∫∂Ω w ∂v
∂ds −∫∂D w ∂v
∂ds −∫Ω/D grad w ⋅grad vdx
= ∫∂Ω w ∂v
∂ds −∫∂D w ∂v
∂ds −∫Ω/∂D grad w ⋅grad vdx,
and all three integrals in the bottom line are zero by construction. Thus, w is harmonic in
Ω/D according to Weyl’s Lemma. The same kind of argument also shows that w is har-
monic in D. Accordingly, as above, (> .) holds true for any v ∈H
◇(Ω), where now
the left hand side of (> .) is zero because of the orthogonality. A standard variational
argument then shows that the normal derivative of w on ∂Ω and the flux of w across ∂D
must vanish.
∎
We briefly mention that every potential w from W of (> .) has a unique continua-
tion to a potential w ∈K, and the restriction of a nontrivial element from K to Ω/D is a
nonzero element from W. Accordingly, the set of traces on ∂Ω of potentials from W and
K, respectively, are the same.


Sampling Methods
Theorem 
The operator K : L
◇(∂Ω) →X defined in (> .) is bounded, injective, and
its range lies dense in the subset K introduced in Lemma . The adjoint operator K∗: X →
L
◇(∂Ω) satisfies
K∗v =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
v∣∂Ω,
v ∈K,
,
v ∈H
◇(Ω).
In particular, there holds K∗K = Λ−Λ1, i.e., (> .).
Proof
We recall that the two Neumann problems (> .) and (> .) have well-defined
unique solutions uand u1 in the space H
◇(Ω/D) and H
◇(Ω), respectively, which are
given by the corresponding weak formulations
∫Ω/D grad u⋅gradvdx = ∫∂Ω f vds
for every v∈H
◇(Ω/D),
∫Ω grad u1 ⋅grad vdx = ∫∂Ω f vds
for every v ∈H
◇(Ω).
(.)
Moreover, the two solutions depend continuously (in H) on the given boundary data f ∈
L
◇(∂Ω). Accordingly, w = K f is a well defined element of X and K a bounded linear
operator from L
◇(∂Ω) to X: The jump condition ∫∂Di[w]ds = is a consequence of the
definition (> .) of ci and the uniqueness of the trace of u1 on ∂D.
Now, choose any f ∈L
◇(∂Ω), and denote by uand u1 the corresponding solutions of
(> .) and (> .). As in the definition of K f we can extend uto a function
ˆu=
⎧⎪⎪⎨⎪⎪⎩
u
in Ω/D,
ci
in Di, i = , . . . , m,
in X, such that K f = ˆu−u1. First, for v ∈H
◇(Ω) we have
(K f ,v)X = (ˆu,v)X −(u1,v)X = ∫Ω/D gradu⋅gradvdx −∫Ω grad u1 ⋅grad vdx = 
by virtue of (> .), and, hence, R(K)H
◇(Ω). It thus follows from Lemma that
R(K) ⊂K and N(K∗) = R(K)
 ⊃H
◇(Ω) and, in particular, that K∗v = for every
v ∈H
◇(Ω).
Second, for v ∈K we compute
(K f ,v)X = (ˆu,v)X −(u1,v)X = (ˆu,v)X,
since u1 and v are orthogonal to each other according to Lemma . Together with (> .)
thus follows that
(K f ,v)X = ∫Ω/D grad u⋅grad vdx = ∫∂Ω f vds = (f ,v)L(∂Ω),
i.e., that K∗v = v∣∂Ω. In particular, for v = K f = ˆu−u1 ∈K we obtain
K∗K f = K∗(ˆu−u1) = (u−u1)∣∂D,
and, hence, the assertion (> .) follows, cf. (> .).

Sampling Methods 

Assume now that R(K) were not dense in K. Then there is some ≠v ∈K ∩R(K)
 =
K ∩N(K∗), and since = K∗v = v∣∂Ω this function v has vanishing Dirichlet boundary
values on ∂Ω. Moreover, as v belongs to K, it is harmonic in Ω/D with vanishing Neumann
boundary values on ∂Ω, see Lemma . Thus, v+ = v∣Ω/D = because of the unique solvabil-
ity of the Cauchy problem for harmonic functions. Using Lemma once more, it follows
that v−= v∣D is also harmonic with vanishing Neumann boundary values on ∂D, and,
hence, v−is constant on each Di, say v−∣Di = v−
i , i = , . . . , m. Since ∫∂Di [v]ds = −v−
i ∣∂D∣,
and as v belongs to X, these constants must all be zero. This is a contradiction to v ≠,
and, hence, R(K) is dense in K.
Finally, to show injectivity of K we assume K f = for some f ∈L
◇(∂Ω). Then u= u1
in Ω/D and u1 = ci in Di, i = , . . . , m. Since u1 is harmonic in all of the domain Ω the
field must be constant in Ω (principle of unique continuation) and the flux f = ∂u1/∂
vanishes on ∂Ω.
∎
This theorem – together with Lemma – reveals that the range of K∗consists
of all traces of potentials w ∈K, whereas the range of Λ−Λ1 only consists of a
dense subset of this set. Accordingly, we need to find a way to deduce the range of
K∗from the given data to decrypt the information hidden in these traces according to
Theorem .
To this end we exploit the so-called Picard criterion, a formulation of which can be
found in the > appendix (Theorem ) for the ease of completeness. The Picard criterion
is based on the singular value decomposition of the operator K, which is largely equivalent
to the spectral decomposition of the operator K∗K = Λ−Λ1.
Corollary 
The operator Λ−Λ1 is a compact and self adjoint operator from L
◇(∂Ω)
into itself. As such, L
◇(∂Ω) has an orthonormal eigenbasis {fj} and associated eigenvalues
λj, such that
(Λ−Λ1)fj = λj fj,
n ∈N.
(.)
These eigenvalues are positive, and converge to zero as n →∞. Throughout we shall assume
that they are sorted in non-increasing order.
Proof
That Λand Λ1 are compact operators can be seen from the fact that the trace
space of H(Ω/D) on ∂Ω, i.e., H/(∂Ω), is compactly embedded in L(∂Ω). Accordingly,
the difference operator Λ−Λ1 is compact as well as self adjoint, as follows readily from
(> .). One can thus find an orthonormal eigenbasis of Λ−Λ1 and the associated
eigenvalues converge to zero for j →∞. It remains to prove that they are all positive; this
follows from (> .) and the injectivity of K by Theorem .
∎
As we have mentioned before, a point z ∈Ω belongs to D, if and only if the trace ϕz of
Uz is the trace of a potential in K, i.e., if it belongs to the range of K∗. As we show in the
> appendix, cf. Corollary , this can be tested in the following way.


Sampling Methods
Theorem 
Let {fj} and {λj} be the eigenbasis and eigenvalues of Λ−Λ1. Then, for any
point z ∈Ω,
z ∈D ⇐⇒
∞
∑
n=
∣(ϕz, fj)L(∂Ω)∣
λj
< ∞
(.)
with ϕz = Uz∣∂Ω from (> .).
Remark 
With the notations /∞= and sign α = {α/∣α∣,
α /= ,
,
α = , for any α ∈C we
note that
χD(z) = sign
⎡⎢⎢⎢⎢⎣
∑
j
∣(ϕz, fj)L(∂Ω)∣

λj
⎤⎥⎥⎥⎥⎦
−
,
z ∈Ω,
is the characteristic function of D. In particular, this result provides a constructive proof
of the uniqueness of the inverse problem.
..
Conducting Obstacles
Next, we turn to the case of anisotropic conducting obstacles. To this end we assume that
for each x ∈Ω the conductivity σ(x) is a real, symmetric positive definite n × n-matrix,
measurable and essentially bounded as a function of x, and that the associated quadratic
form is bounded from below by some positive constant c > , i.e.,
p ⋅(σ(x)p)≥c for almost every x ∈D and every p ∈Rn with ∣p∣= and
σ(x)= I on Ω/D,
(.)
where D denotes the obstacles, which are assumed to have the same topological prop-
erties as before. Another assumption that seems to be necessary for the validity of the
Factorization Method is that
p ⋅(σ(x)p) ≤κ < 
for every p ∈Rn with ∣p∣= , and almost every x ∈D,
(.)
which states that the background conductivity of the object is strictly larger than within the
inclusions. Instead of (> .) one can alternatively require that the conductivity within
the inclusions is strictly larger than in the background, with straightforward modifications
of the analysis; however, we will stick to the above assumption for the ease of simplicity.
We mention that the assumption that the background conductivity be strictly larger (or
smaller) than within the object can be relaxed to just being larger (or smaller), for the prize
that the outcome of the method is unspecified for sampling points right on the boundary of
the inclusions, cf. []. However, it is an open problem whether the Factorization Method
is applicable, if inequality (> .) holds in some obstacles, while p ⋅(σ(x)p) ≥γ > in
other inlusions; numerically, the method does not seem to deterior in this “mixed case.”
With conducting obstacles the potential corresponding to a boundary current f ∈
L
◇(∂Ω) is given as the (weak) solution u ∈H
◇(Ω) of the boundary value problem
div(σ grad u) = 
in Ω,
∂
∂u = f
on ∂Ω,
∫∂Ω uds = ,
(.)

Sampling Methods 

which replaces the model (> .) from > Sect. ..above. Accordingly, we denote by Λ
the Neumann–Dirichlet map associated with (> .), i.e., Λ : f ↦g = u∣∂Ω.
As before, the corresponding inverseproblem is to determine the shape of the obstacles
D from the relative data Λ −Λ1. Here, again, Λ1 corresponds to the “unperturbed” case
σ = σ1 = everywhere in Ω.
We mention that the problem whether not only D but the conductivity σ itself is
uniquely determined by these data is completely settled when n = – as long as σ is
isotropic, cf. []. For n = this question is still open for general scalar L∞−conductivities.
Partial answers are known, we refer to
> Chap. . However, the set D is uniquely
determined as we will see below in Theorem .
Now we proceed to derive a factorization of Λ −Λ1 in three factors as in (> .), i.e.,
Λ −Λ1 = AGA∗.
(.)
To this end we imagine the effect of a virtual source φ on the boundary of the obstacle D,
given that the boundary of the object Ω is insulated: The corresponding potential v is the
solution of the boundary value problem
Δv = 
in Ω/D,
−∂
∂v = φ
on ∂D,
∂
∂v = 
on ∂Ω,
∫∂Ω vds = .
(.)
Recall that the normal vector
on ∂D has been fixed to point into the interior of Ω/D, and
therefore the minus sign in front of the normal derivative on ∂D reflects the fact that φ is
considered to be a source, and not a sink. We will require that this source has vanishing
mean on each connected component Di of D, i.e.,
φ ∈H−/
∗
(∂D) = {φ ∈H−/(∂D) : ∫∂Di
φds = , i = , . . . , m},
(.)
where the integrals have to be interpreted as dual pairings between H−/functions and
the unit constant from H/. For later use we remark that the dual space of H−/
∗
(∂D) can
be identified with the subspace
H/
∗(∂D) = {ψ ∈H/(∂D) : ∫∂Di
ψds = , i = , . . . , m}
(.)
of H/(∂D).
Associated with (> .), we define the operator
A : {H−/
∗
(∂D)
→
L
◇(∂Ω),
φ
↦
v∣∂Ω,
(.)
and remark that the adjoint operator A∗: L
◇(∂Ω) →H/
∗(∂D) of A is easily seen to
map f ∈L
◇(∂Ω) onto the trace of the solution uof (> .) on the boundary of the
obstacle – after an appropriate renormalization of this trace on each component ∂Di of ∂D.


Sampling Methods
More precisely the following holds
(A∗f )(x) = u(x) −ci
for x ∈∂Di , i = , . . . , m,
(.)
with ci as in (> .).
In order to establish (> .) it remains to determine the operator G in the middle.
We define G via the weak solution w of the diffraction problem
div(σ grad w) = 
in Ω/∂D,
∂
∂w = 
on ∂Ω,
∫∂Ω wds = ,
[w]∂D = ψ,
[ ⋅(σ grad w)]∂D = ,
(.)
and the solution w1 of the corresponding problem with σ replaced by one everywhere.
Again, the normal
on ∂D is pointing into the exterior of D. Note that when σ = 
throughout all of Ω, then the corresponding solution w1 of (> .) can be represented
as a modified double layer potential with density ψ and the Neumann function for the
Laplacian as kernel, i.e.,
w1(x) = ∫∂D
∂
∂y
N(x, y)ψ(y)ds(y),
x ∈Ω/∂D.
For a general conductivity tensor, the weak form of (> .) is obtained by integrating
the differential equation against any test function v ∈H(Ω) and using partial integration,
which yields
∫Ω/∂D grad w ⋅(σ grad v)dx = 
for every v ∈H(Ω).
(.)
Now we can make the Ansatz w = w1 + ˆw with ˆw ∈H(Ω) to rewrite this as a standard
variational problem in H(Ω): Find ˆw ∈H(Ω) such that
∫Ω grad ˆw ⋅(σ grad v)dx = −∫Ω/∂D gradw1 ⋅(σ grad v)dx
for every v ∈H(Ω). From this follows readily that problem (> .) has a unique weak
solution in H(Ω/∂D), provided that ψ ∈H/(∂D), i.e., that ψ belongs to the trace space
of H(D). In accordance with the definition of A∗, however, we will restrict ψ to H/
∗(∂D).
The flux of w and w1 across ∂D is well defined in H−/(∂D), cf., e.g., [, Thm. .],
and there holds
∫∂Di
∂
∂(w+ −w+
1)ds = ∫∂Di
⋅(σ grad w−)ds −∫∂Di
∂
∂w−
1ds
= ∫Di
div(σ grad w)dx −∫Di
Δw1dx = .
We can therefore define the bounded operator G in the following way:
G :
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
H/
∗(∂D)
→
H−/
∗
(∂D),
ψ
↦
∂
∂(w+ −w+
1)∣
∂D.
(.)

Sampling Methods 

Theorem 
With A and G defined as above, the difference Λ −Λ1 of the two Neumann–
Dirichlet operators associated with (> .) and (> .), respectively, satisfies
Λ −Λ1 = AGA∗.
Proof
Consider an arbitrary element f ∈L
◇(∂Ω) and the corresponding function ψ =
A∗f , which satisfies
ψ∣∂Di = u∣∂Di −ci,
where uis given by (> .), and ci is as in (> .). The function ψ belongs to H/
∗(∂D),
and it is easy to verify that the associated solution w1 of (> .) – where σ is replaced by
one – is given by
w1 =
⎧⎪⎪⎨⎪⎪⎩
u−u1
in Ω/D,
ci −u1
in Di, i = , . . . , m,
where u1 is the solution of (> .). Similarly, the solution w of (> .) is given by
w =
⎧⎪⎪⎨⎪⎪⎩
u−u
in Ω/D,
ci −u
in Di, i = , . . . , m,
with u from (> .). Accordingly, w+ −w+
1 = u+
1 −u+, and hence,
φ = GA∗f = ∂
∂(u+
1 −u+)∣
∂D.
If we insert this particular source term φ into (> .), then we conclude readily that the
associated solution v of (> .) is given by v = u+ −u+
1. It thus follows from (> .)
that
AGA∗f = Aφ = g −g1 = (Λ −Λ1)f
as required.
∎
At this occasion we recall that every function w ∈W of (> .) has a well-defined
normal derivative φ ∈H−/
∗
(∂D) at the inner boundary ∂D, and hence, solves the corre-
sponding boundary value problem (> .). And vice versa, the solution of (> .) for
any φ ∈H−/
∗
(∂D) belongs to W. Thus, we can reformulate Theorem as follows.
Theorem 
A point z ∈Ω belongs to D, if and only if the trace ϕz of the dipole potential
Uz in z, defined by (> .), belongs to R(A).
As in the insulating case it remains to derive a constructive algorithm to test whether the
trace of some dipole potential belongs to R(A), or not. The next step on our way towards
this goal is an investigation of the functional analytic properties of the operator G. In the
following we will often consider operators acting between a reflexive Banach space X and
its dual space X∗. We will denote the action of an element ℓ∈X∗on an element ψ ∈X by
⟨ℓ,ψ⟩and the pair of spaces by ⟨X∗, X⟩in order to indicate that the first argument belongs


Sampling Methods
to X∗and the second to X. A particular example is the Sobolev space H/
∗(∂D) with dual
space H−/
∗
(∂D).
Theorem 
The operator G : H/
∗(∂D) →H−/
∗
(∂D) is self adjoint (i.e., G coincides
with G∗: H/
∗(∂D) →H−/
∗
(∂D) if the bi-dual of H/
∗(∂D) is identified with itself) and
coercive, i.e., there exists γ > with
⟨Gψ,ψ⟩≥γ∥ψ∥
H/(∂D)
for all ψ ∈H/
∗(∂D).
(.)
Here, ⟨⋅,⋅⟩denotes the dual pairing in the dual system ⟨H−/
∗
(∂D), H/
∗(∂D)⟩.
Proof
The proof proceeds in a couple of steps.
. At first we establish the symmetry of G. Take any ψ and ˜ψ from H/
∗(∂D), define w and
w1 as in the proof of Theorem , and – using ˜ψ instead of ψ in (> .) – define ˜w and
˜w1 accordingly. Then we conclude that
⟨Gψ, ˜ψ⟩= ∫∂D
˜ψ ∂
∂w+ds −∫∂D
˜ψ ∂
∂w+
1ds
= ∫∂D ( ˜w+ −˜w−) ∂
∂w+ds −∫∂D ( ˜w+
1 −˜w−
1) ∂
∂w+
1ds
= ∫∂D ˜w+ ∂
∂w+ds −∫∂D ˜w−( ⋅(σ grad w−))ds
−∫∂D ˜w+
1
∂
∂w+
1ds + ∫∂D ˜w−
1
∂
∂w−
1ds.
Now we can use (> .), and apply Green’s formula in D or Ω/D, respectively, in each of
these integrals (care has to be taken concerning the orientation of the normal on ∂D), to
obtain
⟨Gψ, ˜ψ⟩= −∫Ω/D grad ˜w ⋅gradwdx −∫D grad ˜w ⋅(σ grad w) dx
+ ∫Ω/D grad ˜w1 ⋅grad w1dx + ∫D grad ˜w1 ⋅grad w1dx
= ∫Ω/∂D grad ˜w1 ⋅grad w1dx −∫Ω/∂D grad ˜w ⋅(σ grad w) dx,
(.)
from which the symmetry of G is obvious.
. Turning to the coerciviy assertion (> .) we fix some ψ ∈H/
∗(∂D) and employ the
weak form (> .) of (> .) with v = w −w1 ∈H(Ω). Starting from (> .) with
ψ = ˜ψ we thus obtain
⟨Gψ,ψ⟩= ∫Ω/∂D ∣grad w1∣dx −∫Ω/∂D gradw ⋅(σ gradw) dx
= ∫Ω/∂D ∣grad w1∣dx −∫Ω/∂D gradw ⋅(σ gradw) dx
+ ∫Ω/∂D gradw ⋅(σ grad(w −w1))dx

Sampling Methods 

= ∫Ω/∂D ∣grad w1∣dx + ∫Ω/∂D grad w ⋅(σ gradw) dx
−∫Ω/∂D gradw ⋅(σ gradw1)dx
= ∫Ω/∂D grad w1 ⋅((−σ)gradw1)dx + ∫Ω/∂D grad(w −w1) ⋅(σ grad(w −w1))dx
≥∫Ω/∂D grad w1 ⋅((−σ)gradw1)dx.
The integrand of the last integral vanishes in Ω/D, and can be bounded in D from below
using the restriction (> .) on the conductivity. Accordingly we have
⟨Gψ,ψ⟩≥(−κ)∫D ∣grad w1∣dx.
(.)
. To accomplish the proof of (> .) we need to show that
∥grad w1∥L(D) ≥c∥ψ∥H/(∂D)
(.)
for some constant c > . Assume the contrary: Let ψ(j) ∈H/
∗(∂D) and the corresponding
w(j)
1
be such that ∥ψ(j)∥H/(∂D) = for every j, and that ∥grad w(j)
1 ∥L(D) converges to
zero as j tends to infinity. Define ˜w(j)
1
∈H(Ω/∂D) as
˜w(j)
1
=
⎧⎪⎪⎨⎪⎪⎩
w(j)
1
in Ω/D,
w(j)
1 −c(j)
i
in Di, i = , . . . , m,
with
c(j)
i
=

∣∂Di∣∫∂Di
(w(j)
1 )
−
ds,
i = , . . . , m.
Then ˜w(j)
1 ∣Di has vanishing mean on ∂Di, and ∥grad ˜w(j)
1 ∥L(Di) →for every i = , . . . , m
as j →∞. By virtue of the Poincaré inquality this implies that ˜w(j)
1 tends to zero in H(D).
From (> .) thus follows that the normal derivative
∂
∂
˜w(j)
1
at ∂D (from either side)
tends to zero in H−/(∂D), and hence, that ˜w(j)
1 ∣Ω/D converges in H(Ω/D) to the solution
of the homogeneous Neumann problem, normalized at the outer boundary. In other words,
˜w(j)
1 converges to zero in H(D) and in H(Ω/D) as j →∞. Recurring to (> .) once
again, we observe that
ψ(j)∣∂Di + c(j)
i
= [w(j)
1 ]
∂Di + c(j)
i
= [ ˜w(j)
1 ]
∂Di ,
(.)
and since ψ(j) ∈H/
∗(∂D) it follows by integration over ∂Di that
c(j)
i
=

∣∂Di∣∫∂Di
[ ˜w(j)
1 ]
∂Di ds −

∣∂Di∣∫∂Di
ψ(j)ds =

∣∂Di∣∫∂Di
[ ˜w(j)
1 ]
∂Di ds →
as j runs to infinity. Inserting this into (> .) we conclude that
ψ(j)∣∂Di = [ ˜w(j)
1 ]
∂Di −c(j)
i
→,
j →∞,


Sampling Methods
in H/(∂Di), i = , . . . , m, but this contradicts ∥ψ(j)∥H/(∂D) = . Therefore (> .) is
true for some c > and every ψ ∈H/
∗(∂D), and hence, (> .) follows from (> .)
and (> .).
∎
By virtue of Theorem all assumptions of Corollary are satisfied for the factorization
of the relative data Λ −Λ1 established in Theorem . Therefore we can now conclude the
main result of this section.
Theorem 
Let z ∈Ω and ϕz be defined as before.
Then:
z ∈D ⇐⇒
∞
∑
n=
∣(ϕz, fj)L(∂Ω)∣
λj
< ∞,
where fj and λj are the orthonormal eigenfunctions and eigenvalues of Λ −Λ1.
..
Local Data
It is an important feature of the Factorization Method that it can be easily adapted to appli-
cations where the given data correspond to what is called the local Neumann–Dirichlet
map Λℓ. This is the map that takes Neumann boundary values supported on some rela-
tively open subset Γ ⊂∂Ω only, and returns the corresponding boundary potentials on the
very same subset (normalized to have vanishing mean, say). The local Neumann–Dirichlet
map occurs whenever part of the boundary is inaccessible to measurements, in which case
Γ corresponds to that part of the boundary of Ω where electrodes can be attached. Math-
ematically, the local Neumann–Dirichlet map can be interpreted as a Galerkin projection
Λℓ= PΛP∗
(.)
of the full Neumann–Dirichlet map, where
P : {L
◇(∂Ω)
→
L
◇(Γ),
g
↦
g∣Γ −

∣Γ∣∫Γ gds,
(.)
and P∗is its Ladjoint, i.e.,
P∗f =
⎧⎪⎪⎨⎪⎪⎩
f
on Γ,

on ∂D/Γ.
From Theorem we immediately conclude that if the conductivity distribution satisfies
(> .) and (> .), then the difference of the two local Neumann–Dirichlet maps Λℓ
and Λℓ
1 can be factorized in the form
Λℓ−Λℓ
1 = (PA)G(PA)∗
with A and G as before. Moreover, the coercivity of G allows a constructive way to check
whether a given function belongs to R(PA), considered as an operator from H−/
∗
(∂D)

Sampling Methods 

to L
◇(Γ). Note that it is obvious from Theorem that the function Pϕz belongs to R(PA)
when z ∈D; the converse statement requires a little more effort.
Theorem 
Let Γ be a relatively open subset of ∂Ω, and let P be the projector defined
in (> .). Then z ∈D, if and only if Pϕz ∈R(PA).
Proof
According to the definition (> .) of A the test function Pϕz belongs to R(PA),
if and only if ϕz coincides on Γ (up to a constant) with the trace of a solution v of (> .).
In this case, however, the dipole potential Uz and the function v are both harmonic func-
tions in Ω/(D ∪{z}), and have the same Cauchy data on Γ (again, up to a constant). Now
we choose a connected subset Ω′ of Ω/(D ∪{z}), whose boundary contains a portion of
Γ that is also a relatively open subset of ∂Ω. Then Uz and v coincide up to a constant in Ω′
according to Holmgren’s theorem, and hence, near all of ∂Ω. This shows that ϕz ∈R(A),
and hence, the assertion follows from Theorem .
∎
Accordingly, if Γ is a relatively open subset of ∂Ω, then Theorem also extends readily
to the local situation, if the eigenfunctions and eigenvalues of Λ−Λ1 are replaced by those
of Λℓ−Λℓ
1.
Note that Theorem requires that Γ is a relatively open subset of ∂Ω, and in fact, the
Factorization Method no longer applies for discrete measurements or finitely many bound-
ary currents. Still, this is precisely the situation that is encountered in practice, as data are
always finite dimensional. Due to the rapid decay of the eigenvalues of Λ−Λ1, however, the
full relative data can be very well approximated by operators of finite rank, such as those
corresponding to real data; see [] for detailed numerical examples.
..
Other Generalizations
...
The Half Space Problem
The Factorization Method can also be applied to a related inverse electrostatic problem in
full space with near field data, if the same manifold of codimension one is used to generate a
source and to measure the resulting change of the potential. In fact, this problem which has
been studied in [] and [], is very similar to the setting for the Helmholtz equation that
we will consider in the following section. We also like to refer to [] where this approach
has been applied to some real two dimensional data.
For quite a few applications, however, the impedance tomography problem is more
appropriately modeled in a half space, rather than in the full space or within a bounded
domain. For this setting new difficulties arise, as the data (may) live on the entire,
unbounded boundary of the surface, which calls for weighted Sobolev spaces for an appro-
priate theoretical analysis. In the sequel we restrict our attention to three space dimensions
(n = ), as the two dimensional case needs some additional attention, cf. [], and at the
same time appears to be less interesting from a practical point of view.


Sampling Methods
We consider the half space Ω = {x ∈R:
⋅x < }, where
∈Ris a fixed unit vector,
which coincides with the outer normal on the hyperplane {x :
⋅x = }, which is the
boundary of Ω. The main difficulty in the analysis of this problem is that solutions of the
corresponding conductivity problem
div(σ gradu) = 
in Ω,
∂
∂u = f
on ∂Ω,
(.)
need no longer belong to L(Ω); instead one has to resort to weighted Sobolev spaces,
such as
U = {u ∈D′(Ω) : (+ ∣⋅∣)−/u ∈L(Ω),∣grad u∣∈L(Ω)},
to search for a unique solution of (> .). If σ is given by (> .), then a weak solution
u ∈U can be shown to exist provided that f belongs to
L,−(∂Ω) = {f : (+ ∣⋅∣)/f ∈L(∂Ω)},
in which case the trace of u belongs to the dual space L,(∂Ω) of L,−(∂Ω). Note that no
normalization of u is required in (> .) because solutions in U are implicitly normalized
to vanish at infinity. We refer to [] for further details about the forward problem.
Within this function space setting the Neumann–Dirichlet operator is defined in a nat-
ural way as an operator Λ : L,−(∂Ω) →L,(∂Ω), and the difference between Λ and Λ1
(the latter corresponding to the homogeneous half space) admits a factorization (> .)
as before, where now
A : {H−/
∗
(∂D)
→
L,(∂Ω),
φ
↦
v∣∂Ω,
and v solves the same boundary value problem as in (> .), except for the missing nor-
malization over the boundary ∂Ω. Furthermore, the self adjoint operator G is defined as
before (with the appropriate definition of a weak solution of (> .)), and is coercive
again.
We emphasize that the dipole potential (> .) for the half space is explicitly known,
i.e., we have (up to a negligible multiplicative constant)
ϕz(x) = (x −z) ⋅p
∣x −z∣,
x ∈∂Ω.
(.)
With these notations, the characterization of the inclusions can be established in much the
same way as before, see [].
Theorem 
A point z ∈Ω belongs to D, if and only if ϕz of (> .) belongs to R(A).
For real applications the measuring device will only cover a bounded region Γ ⊂∂Ω.
The corresponding local Neumann–Dirichlet operator Λℓcan then be embedded in the
standard Lframework from the previous section, and the usual Picard series can be used

Sampling Methods 

to implement the range test. For the ease of completeness we briefly mention that for such
local data the test dipole ϕz can be replaced by the function
˜ϕz(x) =

∣x −z∣,
x ∈Γ,
which is the trace of the corresponding Neumann function (again, up to a multiplicative
constant), as the latter has a vanishing normal derivative on the boundary of the half space.
We hasten to add, though, that ˜ϕz must not be used for full data, as it does not belong to
L,(∂Ω). Numerically, however, this modification of the method has no significant benefit.
...
The Crack Problem
Another case of interest are cracks, i.e., lower dimensional manifolds of codimension one,
that are insulating, say. This setting has important applications in nondestructive testing
of materials. Consider a domain Ω ⊂Rn, with n = or n = again, and the union Σ =
⋃m
i=Σi ⊂Ω of m smooth, bounded manifolds (the insulating cracks), such that Σi ∩Σ j = /
and Ω/Σ are connected. Given a boundary current f ∈L
◇(∂Ω), the induced potential
satisfies the model equations
Δu= 
in Ω/Σ,
∂
∂u= 
on Σ,
∂
∂u= f
on ∂Ω,
(.)
and the corresponding Neumann–Dirichlet operator is the map that takes f onto the trace
of uon ∂Ω:
Λ : {L
◇(∂Ω)
→
L
◇(∂Ω),
f
↦
u∣∂Ω.
The crack case can be analyzed in a similar way as in > Sect. .., cf. [], using a factor-
ization Λ −Λ1 = K∗K, where K is almost identical to the operator in (> .), except that
it maps into H(Ω/Σ). There is a more important difference, though. As the crack has no
interior points, the range test will always fail with the hitherto used test function ϕz, as the
dipole singularity of Uz is too strong to belong to H(Ω/Σ), even when z ∈Σ. To detect
a crack we therefore need to construct a new test function by integrating the function ϕz
over z along some “test arc” (in R) or some “test surface” (in R).
The range test can then be implemented by placing linear (planar) test cracks in dif-
ferent sampling points with various orientations, see [] for numerical reconstructions
in two space dimensions. The amount of work thus grows substantially, as we now have 
degrees of freedom to sample (a test point and a normal direction) instead of only one in
the previous cases. Also, in a numerical realization, test cracks will – at best – only touch
the crack tangentially, but in theory this already suffices to ruin the range test. It turns out
that in practice the usual implementation with the test function ϕz performs as good as
the more elaborate but expensive variant described above. As said before, in theory, ϕz will
never belong to the range of K; in practice, however, it will “almost” do so, i.e., the Picard
series (> .) will grow much more slowly in the close neighborhood of the crack.


Sampling Methods
One-dimensional cracks in three-dimensional objects cannot be reconstructed in this
way, because the potential does not “see” inhomogeneities of this size. However, one can
use an asymptotic analysis similar to the derivation of MUSIC type algorithms that are dis-
cussed in > Sect. ..below. Here we give a brief sketch of an argument provided in [],
and refer to this paper for further details. The basic idea is that realistic “one-dimensional”
cracks in a D world are not exactly one-dimensional, but better modeled as extremely thin
tubular inclusions of small diameter δ > . The corresponding relative data Λδ −Λ1, where
Λδ is the Neumann–Dirichlet operator associated with the tubular inclusion and Λ1 is as
usual, turn out to satisfy an asymptotic expansion in δ,
Λδ −Λ1 = δˆM + o(δ),
possibly after selecting an appropriate (sub)sequence δk →. The operator ˆM that con-
stitutes the dominating term of this expansion admits a factorization similar to (> .).
In contrast to the MUSIC framework below, this operator has infinite dimensional range.
Although the operators of the corresponding factorization are somewhat different from the
ones that we have encountered above, the bottom line is the same as for one-dimensional
cracks in two space dimensions: The same integrated test function belongs to the range
of the operator A of this factorization, if and only if the corresponding test arc is part of
the crack. The singular value decomposition of ˆM can be used to evaluate this test, and in
practice this singular value decomposition can be approximated by the one of Λδ −Λ1,
i.e., by the given data.
.
The Factorization Method in Inverse Scattering
Theory
The second part of this chapter is devoted to the Factorization Method for problems in
inverse scattering theory for time-harmonic waves. The scattering of an incident plane
wave by a medium gives rise to a scattered field which is measured “far away” from the
medium. The Factorization Method characterizes the shape of the scattering medium from
this far field information. The measurement operator will be the far field operator F which
maps the density of the incident Herglotz-field to the corresponding far field pattern of the
scattered field.
The far field operator F allows a factorization of the form (> .) where the operators
A and G depend on the specific situation. We will discuss two typical cases and start with
the scattering by a sound-soft obstacle D in
> Sect. ... This is an example of a non-
absorbing medium which is mathematically reflected by the fact that the far field operator
is normal – though not self adjoint as for the corresponding problem in impedance tomog-
raphy. It was this example for which the Factorization Method was developed for the first
time in []. In
> Sect. ..we will study the scattering of time-harmonic electromag-
netic plane waves by an absorbing medium. In this case the corresponding far field operator
fails to be normal.

Sampling Methods 

Each case study will start with a short repetition of the corresponding direct problem.
Then the inverse problem will be stated and a factorization of the form (> .) will be
derived. As in impedance tomography, a crucial point is to establish in each case a certain
coercivity condition for G. In addition, one needs to prove a range identity which describes
the range of A via the known – possibly non-normal – data operator F.
Here and throughout the following sections, S= {x ∈R: ∣x∣= } denotes the unit
sphere in R.
..
Inverse Acoustic Scattering by a Sound-Soft Obstacle
This section is devoted to the analysis of the Factorization Method for the most simplest
case in scattering theory. We consider the scattering of time-harmonic plane waves by an
impenetrable obstacle D ⊂Rwhich we model by assuming Dirichlet boundary conditions
on the boundary ∂D of D. As before, we assume that D is a finite union D = ⋃m
i=Di of
bounded domains Di such that Di ∩D j = / for i /= j. Furthermore, we assume that the
boundaries ∂Di are Lipschitz continuous, and that the exterior R/D of D is connected.
Finally, let k > be the wave number and
ui(x) = exp(ikx ⋅ˆθ),
x ∈R,
(.)
be the incident plane wave of direction ˆθ ∈S. The obstacle D gives rise to a scattered field
us ∈C(R/D) ∩C(R/D) which superposes ui and results in the total field u = ui + us
which satisfies the Helmholtz equation
Δu + ku = 
outside D,
(.)
and the Dirichlet boundary condition
u = 
on ∂D.
(.)
The scattered field us satisfies the Sommerfeld radiation condition
∂us
∂r −ikus = O (r−)
for r = ∣x∣→∞
(.)
uniformly with respect to ˆx = x/∣x∣∈S.
The direct scattering problem is to determine the scattered field us for a given obstacle
D ⊂R, some ˆθ ∈Sand k > .
For the treatment of this direct problem we refer to [] (see also > Sect. ..). There
it is also shown that the scattered field us has the asymptotic behavior
us(x) = exp(ik∣x∣)
π∣x∣
u∞(ˆx) + O(∣x∣−),
∣x∣→∞,
(.)
uniformly with respect to ˆx = x/∣x∣∈S. The function u∞: S→C is analytic and is
called the far field pattern of us. It depends on the wave number k, the direction ˆθ ∈S,


Sampling Methods
and on the domain D. Since we will keep k > fixed, only the dependence on ˆθ is indicated:
u∞= u∞(ˆx; ˆθ) for ˆx, ˆθ ∈S.
In the inverse scattering problem the far field pattern u∞(ˆx; ˆθ) is known for all ˆx, ˆθ ∈
Sand some fixed k > and the domain D has to be determined. We refer again to [] or
> Chap. for the presentation of the most important properties of this inverse scattering
problem. The knowledge of u∞(ˆx; ˆθ) for all ˆx, ˆθ ∈Sdetermines the integral kernel of the
far field operator F from L(S) into itself, which is defined by
(Fg)(ˆx) = ∫Su∞(ˆx; ˆθ)g(ˆθ)ds(ˆθ)
for ˆx ∈S.
(.)
The far field operator F is compact, normal (i.e., F commutes with its adjoint F∗), and the
so-called scattering operator I +
ik
πF is unitary in L(S).
As in > Sect. .., the first step is to derive a factorization of F in the form (> .).
The operator A is the data to pattern operator which maps f ∈H/(∂D) to the far field
pattern v∞of the radiating (i.e., v satisfies the Sommerfeld radiation condition (> .))
solution v ∈H
loc(R/D) of
Δv + kv = in the exterior of D,
v = f on ∂D.
(.)
Here, H
loc(R/D) is the space of functions v with v∣B/D ∈H(B/D) for all balls B ⊂R.
Existence and uniqueness is assured (see, e.g., [; Chapter ]).
Theorem 
Define the operator A : H/(∂D) →L(S) by Af = v∞where v∞is the
far field pattern of the unique radiating solution v ∈H
loc(R/D) of (> .). Then A is
one-to-one with dense range, and the following factorization holds
F = −AS∗A∗,
(.)
where A∗: L(S) →H−/(∂D) is the dual of A and S∗: H−/(∂D) →H/(∂D) is the
dual of the single layer boundary operator S : H−/(∂D) →H/(∂D) defined by
(Sφ)(x) = ∫∂D φ(y)Φ(x, y)ds(y),
x ∈∂D.
(.)
Here, Φ denotes the fundamental solution of the Helmholtz equation, i.e.,
Φ(x, y) = exp(ik∣x −y∣)
π∣x −y∣
,
x, y ∈R, x /= y,
(.)
and the explicit definition (> .) of this operator makes only sense for smooth functions
φ. It has to be extended to functionals φ ∈H−/(∂D) by a density or duality argument.
Proof
The injectivity of A follows immediately from Rellich’s Lemma (see [] or
> Chap. ). The denseness of the range of A can be shown by approximating any
g ∈L(S) by a finite sum of spherical harmonics to which the corresponding field can
be written down explicitely.

Sampling Methods 

To derive the factorization, define the auxiliary operator H : L(S) →H/(∂D) by
(Hg)(x) = ∫Sg(ˆθ)exp(ikx ⋅ˆθ)ds(ˆθ) = ∫Sg(ˆθ)ui(x; ˆθ)ds(ˆθ),
x ∈∂D.
First we note that u∞(⋅; ˆθ) = −Aui(⋅; ˆθ) by the definition of A and thus, by the super-
position principle, Fg = −AHg for all g ∈L(S), i.e., F = −AH. We compute the dual
H∗: H−/(∂D) →L(S) as
(H∗φ)(ˆx) = ∫∂D φ(y)exp(−ik ˆx ⋅y)ds(y),
ˆx ∈S.
The fundamental solution Φ has the asymptotic behavior
Φ(x, y) = exp(ik∣x∣)
π∣x∣
exp(−ik ˆx ⋅y) + O(∣x∣−),
∣x∣→∞,
(.)
uniformly with respect to ˆx ∈Sand y ∈∂D, and thus has the far field pattern Φ∞(ˆx, y) =
exp(−ik ˆx ⋅y). Therefore, again by superposition, H∗φ = ASφ, i.e., H = S∗A∗. Substituting
this into F = −AH yields (> .).
∎
Therefore, F allows a factorization in the form (> .) with G = −S∗. The most impor-
tant properties of this operator are collected in the following theorem. (For a proof see,
e.g., [, ].)
Theorem 
Assume that kis not a Dirichlet eigenvalue of −Δ in D. Then the following
holds.
(a)
S is an isomorphism from the Sobolev space H−/(∂D) onto H/(∂D).
(b)
Im⟨φ, Sφ⟩< for all φ ∈H−/(∂D) with φ /= . Here, ⟨⋅,⋅⟩denotes the duality pairing
in ⟨H−/(∂D), H/(∂D)⟩.
(c)
Let Si be the single layer boundary operator (> .) corresponding to the wave number
k = i. The operator Si is self adjoint and coercive as an operator from H−/(∂D) onto
H/(∂D), i.e., there exists c> with
⟨φ, Siφ⟩≥c∥φ∥
H−/(∂D)
for all φ ∈H−/(∂D).
(.)
(d)
The difference S −Si is compact from H−/(∂D) into H/(∂D).
From this theorem the following coercivity result can be derived.
Assume that kis not a Dirichlet eigenvalue of −Δ in D. Then there exists c> with
∣⟨φ, Sφ⟩∣≥c∥φ∥
H−/(∂D)
for all φ ∈H−/(∂D).
(.)
This establishes the first step of the Factorization Method. In the second step the domain
D is characterized by the range of the operator A.
Theorem 
For any z ∈R, define the function ϕz ∈L(S) by
ϕz(ˆx) = exp(−ik ˆx ⋅z),
ˆx ∈S.
(.)


Sampling Methods
Then z belongs to D, if and only if ϕz ∈R(A).
Proof
Let first z ∈D. From (> .) we conclude that ϕz is the far field pattern of Φ(⋅, z),
thus ϕz = Af where f = Φ(⋅, z)∣∂D ∈H/(∂D).
Let now z ∉D and assume, on the contrary, that ϕz = Af for some f ∈H/(∂D). Let v
be as in the definition of Af . Then ϕz = v∞. From Rellich’s Lemma and unique continuation
we conclude that Φ(⋅, z) and v coincide in R/(D ∪{z}). By the same arguments as in the
proof of Theorem this is a contradiction since v is regular and Φ(⋅, z) is singular at z. ∎
From the factorization (> .) we conclude that R(F) ⊂R(A) and thus
ϕz ∈R(F)
9⇒
z ∈D.
Therefore, the condition on the left hand side determines only a subset of D. One can show,
cf. [], that for the case of D being a ball the left hand side is only satisfied for the center of
this ball. Nevertheless, the (regularized version) of the test ϕz ∈R(F) leads to the Linear
Sampling Method, cf. > Sect. ...
In the third step of the Factorization Method, the range R(A) of A has to be expressed
by the known data operator F. This is achieved by a second factorization of F based on the
spectral decomposition of the normal operator F. From now on we make the assumption
that kis not a Dirichlet eigenvalue of −Δ in D. Then the far field operator is one-to-one
as it follows directly from the factorization (> .) and part (a) of Theorem .
Since F is compact, normal, and one-to-one, there exists a complete set of orthonormal
eigenfunctions ψj ∈L(S) with corresponding eigenvalues λj ∈C, j = ,,, . . . (see, e.g.,
[]). Furthermore, since the operator I + ik/(π)F is unitary, the eigenvalues λj of F
lie on the circle of radius /r and center i/r where r = k/(π). The spectral theorem for
normal operators yields that F has the form
Fψ =
∞
∑
j=
λj(ψ,ψ j)L(S)ψj,
ψ ∈L(S).
(.)
Therefore, F has a second factorization in the form
F = (F∗F)/G(F∗F)/,
(.)
where the self adjoint operator (F∗F)/: L(S) →L(S) and the signum G: L(S) →
L(S) of F are given by
(F∗F)/ψ =
∞
∑
j=
√
∣λj∣(ψ,ψj)L(S)ψj,
ψ ∈L(S),
(.)
Gψ =
∞
∑
j=
λj
∣λj∣(ψ,ψ j)L(S)ψj,
ψ ∈L(S).
(.)
Also this operator Gsatisfies a coercivity condition of the form (> .).

Sampling Methods 

Theorem 
Assume that kis not a Dirichlet eigenvalue of −Δ in D. Then there exists
c> with
∣(ψ,Gψ)L(S)∣≥c∥ψ∥
L(S)
for all ψ ∈L(S).
(.)
Proof
It is sufficient to prove (> .) for ψ ∈L(S) of the form ψ = ∑j c jψj with
∥ψ∥
L(S) = ∑j ∣c j∣= . With the abbreviation s j = λj/∣λj∣it is
∣(Gψ,ψ)L(S)∣=
?????????????
⎛
⎝
∞
∑
j=
s jc jψj,
∞
∑
j=
c jψj
⎞
⎠
L(S)
?????????????
=
???????????
∞
∑
j=
s j∣c j∣
???????????
.
The complex number ∑∞
j=s j∣c j∣belongs to the closure of the convex hull
C = conv{s j :
j ∈N} ⊂C of the complex numbers s j. We conclude that
∣(Gψ,ψ)L(S)∣≥inf{∣z∣: z ∈C}
for all ψ ∈L(S) with ∥ψ∥L(S) = . From the facts that λj lie on the circle with center i/r
passing through the origin and that λj tends to zero as j tends to infinity we conclude that
the only accumulation points of the sequence {s j} can be +or −. From the factorization
(> .) and Theorem it can be shown (see the proof of Theorem .of []) that indeed
is the only accumulation point, i.e., s j →as j tends to infinity. Therefore, the set C is con-
tained in the part of the upper half-disk which is above the line ℓ= {tˆs + (−t): t ∈R}
passing through ˆs and . Here, ˆs is the point in {s j : j ∈N} with the smallest real part.
Therefore, the distance of the origin to this convex hull C is positive, i.e., there exists c
with (> .).
∎
From Theorem and > Eq. (.) the scattering operator F can be written as
F = AGA∗= (F∗F)/G(F∗F)/,
(.)
where we have set G= −S∗. Both of the operators G j, j = ,, are coercive in the sense
of (> .) and (> .), respectively. By the range identity of Corollary the ranges of
A and (F∗F)/coincide. The combination of this result and Theorem yields the main
result of this section. (To derive the second equivalence of (> .), Theorem of Picard
has been applied.)
Theorem 
Assume that kis not a Dirichlet eigenvalue of −Δ in D. For any z ∈Rdefine
again ϕz ∈L(S) by (> .), i.e.,
ϕz(ˆx) := exp(−ik ˆx ⋅z),
ˆx ∈S.
Then
z ∈D ⇐⇒ϕz ∈R((F∗F)/) ⇐⇒∑
j
∣(ϕz,ψ j)L(S)∣

∣λj∣
< ∞.
(.)
Here, λj ∈C are the eigenvalues of the normal operator F with corresponding normalized
eigenfunctions ψj ∈L(S).


Sampling Methods
Formula (> .) provides a simple and fast technique to visualize the object D by
plotting the inverse of the series on the right hand side. In practice, this will be a finite sum
instead of a series, but the value of the finite sum is much larger for points z outside than
for points inside of D. We refer to the original paper [] for some typical plots.
Remark 
It is illuminating to compare the presentation in this section with the one for
impedance tomography from > Sect. ... The relative potential u −u1 considered there
corresponds to the scattered wave us = u −ui, i.e., the total field minus the incoming field;
the incoming field is the potential that is induced by the excitation if the background is
homogeneous, whereas the total field is the corresponding solution in the presence of the
scatterer.
In both cases, the operator that maps the excitation onto the associated “relative data”
can be factorized in three operators: the one that is applied first, i.e., A∗, maps the excita-
tion/the incoming field onto the boundary of the obstacle(s), the operator A that is applied
last, maps appropriate boundary data on the obstacle onto the “outgoing field” and its
measured data. Accordingly, the operator in the middle encodes the “refraction” at the
obstacle(s).
As such, we can view the factorization from impedance tomography as a generalization
of Huygen’s principle to the diffusion problem (> .), although the time causality from
scattering theory has no apparent physical analog in stationary diffusion processes.
..
Inverse Electromagnetic Scattering by an
Inhomogeneous Medium
This section is devoted to the analysis of the Factorization Method for the inverse scattering
of electromagnetic time-harmonic plane waves by an inhomogeneous non-magnetic and
conducting medium. Let k = ω√εμ> be the wave number with angular frequency ω,
electric permittivity ε, and magnetic permeability μin vacuum. The incident plane wave
has the form
Hi(x) = p exp(ik ˆθ ⋅x),
Ei(x) = −

iωε
curl Hi(x),
(.)
for some polarization vector p ∈Cand some direction ˆθ ∈Ssuch that p⋅ˆθ = . This pair
satisfies the time harmonic Maxwell system in vacuum, i.e.,
curl Ei −iωμHi = 
in R,
(.)
curl Hi + iωεEi = 
in R.
(.)
This incident wave is scattered by a medium with space dependent electric permittivity ε =
ε(x) and conductivity σ = σ(x). We assume that the magnetic permeability μ is constant
and equal to the permeability μof vacuum. Furthermore, we assume that ε ≡εand σ ≡
outside of some bounded domain. The total fields are superpositions of the incident and
scattered fields, i.e., E = Ei + Es and H = Hi + Hs and satisfy the Maxwell system
curl E −iωμH = 
in R,
(.)

Sampling Methods 

curl H + iωεE = σE
in R.
(.)
Also, the tangential components of E and H are continuous on interfaces where σ or ε are
discontinuous. Finally, the scattered fields have to be radiating, i.e., satisfy the Silver-Müller
radiation condition
√μHs(x) × ˆx −√εEs(x) = O ( 
∣x∣)
as ∣x∣→∞
(.)
uniformly w.r.t. ˆx = x/∣x∣∈S. The complex-valued relative electric permittivity εr is
defined by
εr(x) = ε(x)
ε
+ iσ(x)
ωε
.
(.)
Note that εr ≡outside of some bounded domain. The > Eq. (.) can then be written
in the form
curl H + iωεεrE = 
in R.
(.)
It is preferable to work with the magnetic field H only. This is motivated by the fact that
the magnetic field is divergence free as seen from (> .) and the fact that div curl = .
In general, this is not the case for the electric field E. Eliminating the electric field E from
the system (> .), (> .) leads to
curl[ 
εr
curl H] −kH = 
in R.
(.)
The incident field Hi satisfies
curlHi −kHi = 
in R.
(.)
Subtracting both equations yields
curl[ 
εr
curl Hs] −kHs = curl[q curl Hi]
in R,
(.)
where the contrast q is defined by q = −/εr. The Silver–Müller radiation condition turns
into
curl Hs(x) × ˆx −ikHs(x) = O ( 
∣x∣),
∣x∣→∞.
(.)
The continuity of the tangential components of E and H translates into analogous require-
ments for Hs and curl Hs.
It will be necessary to allow more general source terms on the right-hand side of
(> .). In particular, we will consider the problem to determine a radiating solution
v ∈Hloc(curl,R) of
curl[ 
εr
curlv] −kv = curl f
in R
(.)
for given f ∈L(R)with compact support. (For any open set D the space L(D)
denotes the space of vector functions v : D →Csuch that all components are in L(D).)


Sampling Methods
The solutions v of (> .) as well as of (> .) and (> .) have to be understood in
the variational sense, i.e., v ∈Hloc(curl,R) satisfies
∫R[ 
εr
curlv ⋅curlψ −kv ⋅ψ]dx = ∫Rf ⋅curlψdx
(.)
for all ψ ∈H(curl,R) with compact support. For any domain Ω, the Sobolev space
H(curl, Ω) is the space of all vector fields v ∈L(Ω)such that also curlv ∈L(Ω).
Furthermore, Hloc(curl,R) = {v : v∣B ∈H(curl, B) for all balls B ⊂R}.
Outside of the supports of εr −and f the solution satisfies curlv −kv = . Taking the
divergence of this equation and using the identities div curl = and curl= −Δ + graddiv
this system is equivalent to the pair of equations
Δv + kv = 
and
div v = .
Classical interior regularity results (cf. [] combined with []) yield that v is analytic
outside of the supports of εr −and f . In particular, the radiation condition (> .) is
well defined.
There are several ways to show the Fredholm property of
> Eq. (.). We refer to
[] for the treatment by a variational equation with non-local boundary conditions or to
[] for a treatment by an integro-differential equation of Lippmann–Schwinger type.
The question of uniqueness of radiating solutions to (> .) is closely related to the
validity of the unique continuation principle. It is known to hold for piecewise Hölder-
continuously differentiable functions εr (see []).
As in the case of the Helmholtz equation, every radiating vector field v satisfying
curlv −kv = outside of some ball has the asymptotic behavior
v(x) = exp(ik∣x∣)
π∣x∣
v∞(ˆx) + O(∣x∣−),
∣x∣→∞,
uniformly with respect to ˆx = x/∣x∣∈S(see again []). The vector field v∞is uniquely
determined and again called the far field pattern of v. It is a tangential vector field, i.e.,
v∞∈L
t(S) where
L
t(S) = {w ∈L(S): w(ˆx) ⋅ˆx = , ˆx ∈S}.
The inverse problem is to determine the shape D of the support of the contrast q from the
far field pattern H∞(ˆx; ˆθ, p) for all ˆx, ˆθ ∈Sand p ∈Cwith p⋅ˆθ = . Because of the linear
dependence of H∞on p it is sufficient to know H∞only for a basis of two vectors for p. As
in impedance tomography the task of determining only D is rather modest since it is well
known that one can even reconstruct q uniquely from this set of data, see []. However,
the proof of uniqueness is non-constructive while the Factorization Method will provide
an explicit characterization of the characteristic function of D which can, e.g., be used for
numerical purposes. Also, the Factorization Method can – with only minor modifications –
be carried over for anisotropic media (as in
> Sect. ..) where it is well known that εr
can only be determined up to a smooth change of coordinates.
For the remaining part of this section we make the following assumption.

Sampling Methods 

Assumption 
Let D ⊂Rbe a finite union D = ⋃m
i=Di of bounded domains Di such
that Di ∩D j = / for i /= j. Furthermore, we assume that the boundaries ∂Di are Lipschitz
continuous and the exterior R/D of D is connected. Let εr ∈L∞(D) satisfy
()
Im εr ≥in D.
()
There exists c> with Reεr ≤−con D and ∥Imεr∥
∞< c(−c).
()
For every f ∈L(R)with compact support there exists a unique radiating solution of
(> .).
We extend εr by one outside of D and define the contrast by q = −/εr, thus Im q ≥
and Req ≤−γ∣q∣on D for some γ > .
Condition () is, e.g., satisfied for Hölder-continuously differentiable parameters ε and
σ (see []).
The far field operator F : L
t(S) →L
t(S) is defined as
(Fp)(ˆx) := ∫SH∞(ˆx; θ, p(θ))ds(θ),
ˆx ∈S.
(.)
F is a linear operator since H∞depends linearly on the polarization p.
The first step in the Factorization Method is to derive a factorization of F in the form
F = AT∗A∗where the operators A : L(D)→L
t(S) and T : L(D)→L(D)are
defined as follows.
The data-to-pattern operator A : L(D)→L
t(S) is defined by Af := v∞, where
v∞denotes the far field pattern corresponding to the radiating (variational) solution v ∈
Hloc(curl,R) of
curl[ 
εr
curlv] −kv = curl
⎡⎢⎢⎢⎣
q
√
∣q∣
f
⎤⎥⎥⎥⎦
in R.
(.)
Again, the contrast is given by q = −/εr. We note that the solution exists by part () of
Assumption .
The operator T : L(D)→L(D)is defined by T f = (sign q)f −
√
∣q∣curlw∣D,
where w ∈Hloc(curl,R) is the radiating solution of
curlw −kw = curl[
√
∣q∣f ]
in R.
(.)
The solution exists and is unique (see, e.g., []).
Theorem 
Let Assumption hold. Then F from (> .) can be factorized as
F = AT∗A∗,
(.)
where A∗: L
t(S) →L(D)and T∗: L(D)→L(D)denote the adjoints of A and T,
respectively. Furthermore, A∗is injective.
For a proof of this and the following result we refer to [].


Sampling Methods
Remark 
The solution w of (> .) can be expressed in the form (see [])
w(x) = curl∫D
√
∣q(y)∣f (y)Φ(x, y)dy,
x ∈R,
which yields an explicit expression of T.
The following theorem corresponds to Theorem and collects properties of the
operator T needed for the analysis of the Factorization Method.
Theorem 
Let the conditions of Assumption hold and let T : L(D)→L(D)be
defined above. Then the following holds
(a)
The imaginary part Im T = 
i(T −T∗) is non-positive, i.e.,
Im(T f , f )L(D)≤
for all f ∈L(D).
(b)
Define the operator Tin the same way as T but for k = i. Then −ReTis coercive and
T −Tis compact in L(D).
(c)
T is an isomorphism from L(D)onto itself.
As in
> Sect. ..we first characterize the domain D by the range R(A) of A. The
proof of the following result can again be found in [].
Theorem 
Let the conditions of Assumption hold. For any z ∈Rand fixed p ∈Cwe
define ϕz ∈L
t(S) as the far field pattern of the electric dipole at z with moment p, i.e.,
ϕz(ˆx) = −ik(ˆx × p)exp(−ik ˆx ⋅z),
ˆx ∈S.
(.)
Then z belongs to D, if and only if ϕz ∈R(A).
In contrast to the data operators Λ−Λ1 or Λ−Λ1 of Sect. ., or the far field operator
F of
> Sect. .., the far field operator for absorbing media – as in the present case –
fails to be normal or even self adjoint. Therefore, the approaches of the previous sections
– i.e., the application of the range identities of Corollaries and – are not applicable.
However, application of Theorem to the far field operator F from L
t(S) into itself and
the operator G = T∗: L(D)→L(D)yields the characterization of D via an auxiliary
operator
F# = ∣Re F∣+ Im F,
(.)
cf. (> .), which is easily obtained from the given far field data.
Theorem 
Let the conditions of Assumption hold. For any z ∈Rdefine again ϕz ∈
L
t(S) by (> .). Then, with F# of (> .) there holds
z ∈D ⇐⇒ϕz ∈R(F/
# ) ⇐⇒∑
j
∣(ϕz,ψ j)L(S)∣

∣λj∣
< ∞.
(.)

Sampling Methods 

Here, λj ∈C are the eigenvalues of the self adjoint and positive compact operator F# with
corresponding normalized eigenfunctions ψ j ∈L
t(S).
..
Historical Remarks and Open Questions
Historically, the Factorization Method originated from the Linear Sampling Method which
will be explained in
> Sect. ..(see also
> Sect. ..). The Linear Sampling Method
studies the far field equation Fg = ϕz in contrast to the Factorization Method which
characterizes the domain D by exactly those points z for which the modified far field
equation F/
# g = ϕz is solvable where F# = (F∗F)/in the case of
> Sect. ..and
F# = ∣Re F∣+ Im F in the case of
> Sect. ... It is easily seen that the points for which
the far field equation Fg = ϕz is solvable determines only a subset of D – which can consist
of a single point only, as the example of a ball shows.
The implementation of the Factorization Method is as simple and universal as of the
Linear Sampling Method. Only the far field operator F – i.e., in practice a finite dimensional
approximation – has to be known. No other a priori information on the unknown domain
D such as the number of components or the kind of boundary condition has to be known
in advance. The mathematical justification, however, has to be proven for every single situ-
ation. Since their first presentations, the Factorization Method has been justified for several
problems in inverse acoustic and electromagnetic scattering theory such as the scattering
by inhomogeneous media ([, , , ]), scattering by periodic structures ([, ]), and
scattering by obstacles under different kinds of boundary conditions ([, ]). The Factor-
ization Method can also be adapted for scattering problems for a crack ([]) with certain
modifications; we refer to the remarks concerning the crack problem in
> Sect. ...
The Factorization Method for elastic scattering problems and wave guides is studied in
[] and [], respectively.
In many situations near field measurements on some surface Γ for point sources on
the same surface Γ as incident fields rather than far field measurements for plane waves as
incident fields are available. The corresponding “near field operator” M : L(Γ) →L(Γ)
allows a factorization in the form M = BGB′ where B′ is the adjoint with respect to the
bilinear form ∫Γ uvds rather than the (sesquilinear) inner product ∫Γ uvds. The validity of
the range identity for these kinds of factorizations is not known so far and is one of the
open problems in this field. For certain situations (see []), the corresponding far field
operator F can be computed from M and the Factorization Method can then be applied
to F.
Also the cases where the background medium is more complicated than the free space
can be treated, see [, ] for scattering problems in a half space and [] for scattering
problems in layered media.
The justification of the Factorization Method for arbitrary elliptic boundary value
problems or even more general problems is treated in [, , ].


Sampling Methods
.
Related Sampling Methods
This section is devoted to some alternate examples of sampling methods which were devel-
oped during the last decade: the Linear Sampling Method, first introduced by Colton and
Kirsch in []; the closely related MUSIC; the Singular Sources Method by Potthast (see
[]); and Ikehata’s Probe Method (see []). However, it is not the aim of this section to
report on all sampling methods. In particular, we do not discuss the enclosure method or
the no-response test but refer to the monograph [] and the survey article [].
..
The Linear Sampling Method
Here we reconsider the inverse scattering problem for time-harmonic plane acoustic waves
of
> Sect. .., i.e., the problem to determine the shape of an acoustically soft obsta-
cle D from the knowledge of the far field pattern u∞(ˆx; ˆθ) for all ˆx, ˆθ ∈S. We refer to
(> .)–(> .) for the mathematical model and the definition of the far field operator
F from L(S) into itself.
The Factorization Method for inverse scattering problems studies solvability of the
equation F/
# g = ϕz in L(S) where F# = (F∗F)/in the case where F is normal (as, e.g.,
in >Sect. ..) and F# = ∣Re F∣+Im F in the general case with absorbtion, see Theorems 
and , respectively. In contrast to this equation, the Linear Sampling Method considers the
far field equation
Fg = ϕz
in L(S).
(.)
We mention again that in general no solution of this equation exists. However, one can
compute “approximate solutions” g = gz,ε of (> .) such that ∥g∥L(S) behaves differ-
ently for z being inside or outside of D. We refer to
> Chap. , Theorem .for a more
precise formulation of this behavior.
The drawback of this result – and all the other attempts to justify the Linear Sampling
Method rigorously – is that there is no guarantee that the solution of a regularized ver-
sion of (> .), e.g., by Tikhonov regularization, will actually pick the density g = gz,ε
with the properties of the aforementioned “approximate solution.” We refer to [] for a
discussion of this fact. However, numerically the method has proven to be very effective
for a large class of inverse scattering problems, see, e.g., [] for the scattering by cracks,
[] for inverse scattering problems for anisotropic media, [] for wave guide scattering
problems, [, , ] for electromagnetic scattering problems, and [, , ] for elastic
scattering problems. Modifications of the Linear Sampling Method and combinations with
other methods can be found in [, , ].
For the cases in which the Factorization Method in the form (F∗F)/g = ϕz is
applicable, a complete characterization of the unknown obstacle D by a modification of
the Linear Sampling Method can be derived by replacing the indicator value ∥g∥L(S)
by (g, ϕz)L(S). This is summarized in the following theorem (see [, ] and, for the
following presentation, []).

Sampling Methods 

Theorem 
Let u∞= u∞(ˆx; ˆθ) be the far field pattern corresponding to the scattering
problem (> .)–(> .) with associated far field operator F, and assume that kis not
a Dirichlet eigenvalue of −Δ in D. Furthermore, for every z ∈D let gz ∈L(S) denote the
solution of (F∗F)/gz = ϕz, i.e., the solution obtained by the Factorization Method, and for
every z ∈Rand ε > let g = gz,ε ∈L(S) be the Tikhonov approximation of (> .),
i.e., the unique solution of
(εI + F∗F)g = F∗ϕz
(.)
which is computed by the Linear Sampling Method (if Tikhonov’s regularization tech-
nique is chosen). Here, ϕz ∈L(S) is defined in (> .). Furthermore, let vgz,ε(z) =
(gz,ε, ϕz)L(S) = ∫Sgz,ε(ˆθ)exp(ik ˆθ ⋅z)ds(ˆθ) denote the corresponding Herglotz wave
function evaluated at z.
(a)
For every z ∈D the limit
limε→vgz,ε(z)
exists. Furthermore, there exists c > ,
depending on F only, such that for all z ∈D the following estimates hold
c∥gz∥
L(S) ≤lim
ε→∣vgz,ε(z)∣≤∥gz∥
L(S).
(.)
(b)
For z ∉D the absolute values ∣vgz,ε(z)∣tend to infinity as ε tends to zero.
Proof
Using an orthonormal system {ψj : j ∈N} of eigenfunctions ψj corresponding to
eigenvalues λj ∈C of F one computes the Tikhonov approximation gz,ε from (> .) as
gz,ε =
∞
∑
j=
λj
∣λj∣+ ε(ϕz,ψ j)L(S)ψj.
From vg(z) = (g, ϕz)L(S) for any g ∈L(S) we conclude that
vgz,ε(z) =
∞
∑
j=
λj
∣λj∣+ ε ∣(ϕz,ψ j)L(S)∣
.
(.)
(a) Let now z ∈D. Then (F∗F)/gz = ϕz is solvable in L(S) by Theorem and thus
(ϕz,ψ j)L(S) = ((F∗F)/gz,ψ j)L(S) = (gz,(F∗F)/ψj)L(S) =
√
∣λj∣(gz,ψ j)L(S).
Therefore, we can express vgz,ε(z) as
vgz,ε(z) =
∞
∑
j=
λj∣λj∣
∣λj∣+ ε ∣(gz,ψ j)L(S)∣
= ∥gz∥
L(S)
∞
∑
j=
ρ j
λj∣λj∣
∣λj∣+ ε,
(.)
where ρ j = ∣(gz,ψ j)L(S)∣
/∥gz∥
L(S) is non-negative with ∑j ρ j = . An elementary
argument (theorem of dominated convergence) yields convergence
∞
∑
j=
ρ j
λj∣λj∣
∣λj∣+ ε T→
∞
∑
j=
ρ j
λj
∣λj∣=
∞
∑
j=
ρ js j
as ε tends to zero where again s j = λj/∣λj∣. The properties of ρ j imply that the limit belongs
to the closure C of the convex hull of the complex numbers {s j : j ∈N}. The same argument
as in the proof of Theorem yields that C has a positive distance c from the origin, i.e.,


Sampling Methods
∣∑∞
j=ρ js j∣≥c which proves the lower bound. The upper estimate is seen directly from
(> .).
(b) Let now z ∉D and assume on the contrary that there exists a sequence {εn} which tends
to zero and such that ∣vn(z)∣is bounded. Here we have set vn = vgz,εn for abbreviation. Since
s j converges to there exists j∈N with Re λj > for j ≥j. From (> .) for ε = εn
we get
vn(z) =
j−
∑
j=
λj
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣
+
∞
∑
j=j
λj
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣
.
Since the finite sum is certainly bounded for n ∈N there exists c> such that
???????????
∞
∑
j=j
λj
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣
???????????
≤c
for all n ∈N.
Observing that for any complex number w ∈C with Re w ≥and Imw ≥we have that
Rew + Imw ≥∣w∣we conclude (note that also Im λj > )
c≥
???????????
∞
∑
j=j
λj
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣
???????????
≥
∞
∑
j=j
Re λj + Im λj
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣

≥
∞
∑
j=j
∣λj∣
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣
≥
J
∑
j=j
∣λj∣
∣λj∣+ εn
∣(ϕz,ψ j)L(S)∣

for all n ∈N and all J ≥j. Letting n tend to infinity yields boundedness of the finite
sum uniformly w.r.t. J and thus convergence of the series
∑∞
j=j

∣λ j∣∣(ϕz,ψ j)L(S)∣
. From
(> .) therefore follows that z ∈D, which is the desired contradiction.
∎
Obviously, this kind of modification of the original Linear Sampling Method can be
done for all inverse scattering problems for which Theorem holds. This includes scat-
tering by acoustically hard obstacles or inhomogeneous non-absorbing media or, with
appropriate modifications, scattering by open arcs.
..
MUSIC
The Linear Sampling Method investigates “to what extent” the far field equation
Fg = ϕz
is solvable for a number of sampling points z within some region of interest. As we have
mentioned before, this equation has a solution in very rare cases only, and usually not for
every z ∈D.
However, if the obstacle is very small, then it turns out that the far field operator almost
degenerates to a finite rank operator, in which case the “numerical range” of F and (F∗F)/
would be the same finite dimensional subspace, where the latter is known to contain ϕz

Sampling Methods 

for every z ∈D – under appropriate assumptions on the particular problem setting (see
Sects. .and > .).
To investigate this observation in more detail we embed the real scene in a parame-
terized family of problems, where the parameter δ > reflects the scale of the problem.
Assume that the scatterer D = ⋃m
i=Di consists of m obstacles given as
Di = zi + δUi
i = , . . . , m,
(.)
where each domain Ui contains the origin, has Lipschitz continuous boundary, and the
closure of Ui has a connected complement. We shall call zi the location of Di and Ui its
shape. We focus our presentation on an inhomogeneous medium setting for acoustic scat-
tering, i.e., the Helmholtz equation, to provide analogies to both settings from Sect. ..
Let ρand cbe the density and the speed of sound in vacuum, k = ω/cbe the associated
wave number with frequency ω, and ui(x) = exp(ikx ⋅ˆθ) be an incoming plane wave.
Then, if we assume that the density ρi and the sound of speed ci in each object Di are real
and constant, then the total field uδ = ui +us
δ solves the Helmholtz equation (see, e.g., [])
div ( 
ρ grad uδ) + ωηuδ = 
in R,
(.)
with the radiation condition
∂us
δ
∂r −ikus
δ = O (r−)
for r = ∣x∣→∞,
(.)
uniformly with respect to ˆx = x/∣x∣, and the parameter η equals η= /ρin R/D, and
ηi = c
/(c
i ρi) in Di, i = , . . . , m, respectively. We mention that for constant η = /ρ
it has been shown in [] that the standard Factorization Method (with F# = (F∗F)/)
applies for this setting with fixed scaling parameter δ. We know of no result, however,
where the Factorization Method is used to reconstruct the supports of ρ −ρand η −η
in this setting simultaneously, although there are partial results for a similar problem (in a
bounded domain, and with a different sign of η) arising in optical tomography, cf. [, ].
The idea to approach this problem is based on an asymptotic expansion of the far
field u∞
δ of the scattered wave with respect to the parameter δ in (> .). We quote the
following result from [].
Theorem 
The far field of the scattering problem (> .)–(> .) for the scatterers
given in (> .) satisfies
u∞
δ (ˆx; ˆθ) = δk
m
∑
i=
(( ρi
ρ
−) ˆx ⋅Mi ˆθ −( ηi
η
−)∣Ui∣)exp (ik(ˆθ −ˆx) ⋅zi) + o(δ),
(.)
and the associated far field operator can be rewritten as
F = δˆF + o(δ)
(.)


Sampling Methods
in the norm of L(L(S)), where the rank of the operator ˆF is at most m. Here, ∣Ui∣is the
Lebesgue measure of Ui, and Mi ∈R×are symmetric positive definite matrices that depend
on the shape Ui, the so-called polarization tensors.
As is obvious, the scattered field and its far field vanish as δ →. The corresponding
rate δreflects the space dimension; in Rthe corresponding field decays like δas δ →.
The importance of Theorem stems from the fact that the leading order approxima-
tion ˆF of the far field operator F has finite rank, whereas F has infinite dimensional range.
The rank of ˆF is m, unless some of the scatterers have the same material parameters as
the background vacuum. Note that the dominating term of u∞
δ consists of two parts: The
first contribution stems from the change in the density ρ and corresponds to the far field
of a dipole (point source) in zi; likewise, the second term corresponds to the far field of a
monopole in zi, and this is the result of a change in the parameter η.
It is easy to deduce from Theorem that we can factorize ˆF quite naturally in three
factors.
Theorem 
The operator ˆF : L(S) →L(S) admits a factorization of the form
ˆF = −BMB′,
(.)
where B : Cm →L(S) maps a vector [p, . . . , pm, a, . . . , am]T ∈Cm with pi ∈Cand
ai ∈C, i = , . . . , m, to the far field of
u(x) =
m
∑
i=
(pi ⋅gradz Φ(x, zi) + aiΦ(x, zi)),
where Φ is as in (> .), M ∈Rm×m is a real block diagonal matrix with m blocks of
size × and m single elements on its diagonal, and M is nonsingular, if and only if ρi ≠ρ
and ηi ≠ηfor all i = , . . . , m. The operator B′ is the dual operator of B with respect to the
bilinear forms of Cm and L(S), i.e., B′g consists of the gradients and point values of the
Herglotz wave function
vg(x) = ∫Sg(ˆθ)exp(ikx ⋅ˆθ)ds(ˆθ),
x ∈R,
evaluated at the points zi, i = , . . . , m.
As M in (> .) is invertible, the range of ˆF and the range of B coincide, and it consists
of the far fields of the monopoles and all possible dipoles emanating from the locations
zi of Di, i = , . . . , m. Using the unique continuation principle we can thus conclude the
following result.
Corollary 
If each scatterer has a different parameter η than the background medium,
then a point z ∈Ris the location zi of one of the scatterers, if and only if ϕz of (> .)
belongs to the range of ˆF.

Sampling Methods 

When δ is small, it follows from (> .) that numerically the range of F and the range
of ˆF are the same, essentially. By this we mean that the dominating m singular values of F
are small perturbations of the nonzero singular values of ˆF, and the corresponding singular
subspaces are also close to each other. Moreover, we expect to see a sharp gap between the
mth and the m + st singular value of F. We can search for this gap to determine the
number m of the scatterers, and then determine the angle between the test function ϕz and
the m-dimensional dominating singular subspace of F. When z is close to the location of
one of the scatterers then this angle will be small, otherwise this angle will be larger. This
way images can be produced that enable one to visualize the approximate locations of the
scatteres, but not their shape.
This approach applies for all problem settings that have been discussed in Sects. .
and
> ., and many more. In impedance tomography, for example, the corresponding
asymptotic expansion of the boundary potential has the form
uδ(x)−u1(x) = δn
m
∑
i=
−κi
κi
gradz N(x, zi)⋅Mi gradu1(zi)+ o(δn),
x ∈∂Ω, (.)
where n is again the space dimension, N the Neumann function (> .), and Mi the
associated polarization tensor; cf. [] or > Sect. ... The leading order approximation
of the difference between the associated Neumann–Dirichlet operators, Λδ −Λ1, can be
factorized in a similar way as in Theorem , and has an nm-dimensional range that is
spanned by dipole potentials sitting in the locations zi of the obstacles Di, i = , . . . , m;
recall that n is the space dimension.
For the full Maxwell’s equations considered in
> Sect. .., the range space of the
corresponding far field operator F of (> .) consists of the magnetic far fields corre-
sponding to electric dipoles at the infinitesimal scatterers; if the scatterers also differ in their
magnetic permeability, then the range space also contains the far fields of the magnetic
dipoles in zi, i = , . . ., m.
The method described above for reconstructing the locations of small scatterers is often
called MUSIC in the inverse problems community. Originally, the MUSIC algorithm is a
signal processing tool for frequency estimation from the noisy spectrum of some signal
(MUSIC stands for MUltiple SIgnal Classification.), cf., e.g., []. In a seminal report []
this algorithm was suggested to detect “point scatterers” on the basis of the Born approx-
imation, which led to an algorithm that is not exactly the same, but related to the one we
have sketched above. The relation between this algorithm and the Factorization Method
has subsequently been recognized in [, ]. However, although the form of the factor-
ization (> .) is similar to the ones for the Factorization Method derived in Sects. .
and > ., it is slightly different in its precise interpretation; this has been exemplified in
[] by taking the limit of each of the factors from Theorem as δ →.
The derivation of asymptotic formulas as in Theorem goes back to the landmark
paper []. In [], formula (> .) from [] was used to provide the rigorous founda-
tion of the MUSIC type algorithm from above. Important extensions and generalizations


Sampling Methods
to other problem settings include [, , , , ]; for a more detailed survey and further
references we refer to > Chap. and the monographs [, ].
Numerical illustrations of this approach can be found in various papers; see, for
example, [, , ].
..
The Singular Sources Method
As in > Sect. .., we reconsider the simple inverse scattering problem for the Helmholtz
equation in Rto determine the shape of an acoustically soft obstacle D from the knowl-
edge of the far field pattern u∞(ˆx; ˆθ) for all ˆx, ˆθ ∈S. We refer again to (> .)–(> .)
for the mathematical model and the definition of the far field operator F from L(S) into
itself. Note that again us = us(x; ˆθ) and u∞= u∞(ˆx; ˆθ) denote the scattered field and far
field pattern, respectively, corresponding to the incident plane wave of direction ˆθ ∈S.
The basic tool in the Singular Sources Method is to consider also the scattered field vs =
vs(x; z) which corresponds to the incident field vi(x) = Φ(x, z) of (> .) of a point
source, where z ∉D is a given point. The scattered field vs(z; z) evaluated at the source
point blows up when z tends to a boundary point. One can prove (see [, ]) that there
exists a constant c > (depending on D and k only) such that
∣vs(z; z)∣≥
c
d(z, ∂D)
for all z ∉D.
(.)
Here, d(z, ∂D) = inf {∣z −y∣: y ∈∂D} denotes the distance of z to the boundary of D.
The idea of the Singular Sources Method is to fix z ∉D and ε > and a bounded domain
Gz ⊂Rsuch that its exterior is connected and z ∉Gz and D ⊂Gz. Runge’s Approximation
Theorem (see, e.g., []) yields the existence of g ∈L(S) depending on z, Gz, and ε such
that
∥vg −Φ(⋅, z)∥C(Gz) ≤ε,
(.)
where vg denotes the Herglotz wave function, defined by
vg(x) = ∫Sg(ˆθ)exp(ikx ⋅ˆθ)ds(ˆθ),
x ∈R.
In the following, only the dependence on ε is indicated by writing gε. The following
convergence result for the Singular Sources Method is known (see [, ]).
Theorem 
Let u∞= u∞(ˆx; ˆθ), ˆx, ˆθ ∈Sbe the far field pattern of the scattering problem
(> .–.). Fix z ∉D and a bounded domain Gz ⊂Rsuch that its exterior is connected
and z ∉Gz and D ⊂Gz. For any ε > choose g = gε ∈L(S) with (> .). Then
lim
δ→lim
ε→∫S(Fgε)(−ˆθ)gδ(ˆθ)ds(ˆθ) = vs(z; z),
i.e., by substituting the form of F,
lim
δ→lim
ε→∫S∫Su∞(−ˆθ; ˆη)gε(ˆη)gδ(ˆθ)ds(ˆη)ds(ˆθ) = vs(z; z).

Sampling Methods 

Note that the limits are iterated, i.e., first the limit w.r.t. ε has to be taken and then the
limit w.r.t. δ.
Combining this result with (> .) yields
lim
δ→lim
ε→∣∫S∫Su∞(−ˆθ; ˆη)gε(ˆη)gδ(ˆθ)ds(ˆη)ds(ˆθ)∣≥
c
d(z, ∂D).
(.)
This result assures that for z sufficiently close to the boundary ∂D (and regions Gz chosen
appropriately) the quantity
lim
δ→lim
ε→∣∫S∫Su∞(−ˆθ; ˆη)gε(ˆη)gδ(ˆθ)ds(ˆη)ds(ˆθ)∣
becomes large.
It is convenient to use domains Gz of the special form
Gz,p = (z + ρp) + {x ∈R: ∣x∣< R, x
∣x∣⋅p > −cos β}
for some (large) radius R > , opening angle β ∈[, π/), direction of opening p ∈S,
and ρ > . The dependence on β, ρ, and R is not indicated since they are kept fixed. This
domain Gz,p is a ball centered at z + ρp with radius R from which the cone of direction −p
and opening angle β has been removed. Obviously, it is chosen such that z ∉Gz,p. These
sets Gz,p are translations and rotations of the reference set
ˆG = {x ∈R: ∣x∣< R, x
∣x∣⋅ˆp > −cos β}
for ˆp = (,,)⊺, i.e., Gz,p = z + M ˆG for some orthogonal M ∈R×.
With these transformations, we can consider the singular sources method as a sampling
method with sampling objects z and M.
From the arguments used in the proof of Theorem it is not clear whether or not
the common limit limε,δ→exists. However, if kis not a Dirichlet eigenvalue of −Δ
in D, then the following stronger result than (> .) can be obtained by using the
factorization (> .).
Theorem 
Let z ∉D and Gz ⊂Rbe a bounded domain such that its exterior is con-
nected and z ∉Gz and D ⊂Gz. For any ε > choose gε ∈L(S) with (> .) with respect
to the H−norm, i.e.,
∥vgε −Φ(⋅, z)∥H(Gz) ≤ε.
Assume furthermore that kis not a Dirichlet eigenvalue of −Δ in D. Then there exists a
constant c > depending only on D and k such that
∣lim
ε→∫S∫Su∞(ˆθ; ˆη)gε(ˆη)gε(ˆθ)ds(ˆη)ds(ˆθ)∣= lim
ε→∣(Fgε, gε)L(S)∣≥
c
d(z, ∂D).
For a proof we refer to []. Numerical reconstructions with the Singular Sources
Methods are shown in [].


Sampling Methods
..
The Probe Method
The Probe Method has originally been proposed in [] for the inverse problem of
impedance tomography of
> Sect. .., and here we also restrict our attention to this
setting. To be precise, let σ ∈L∞(Ω) be a (complex valued) admittivity function, and
define u ∈H
◇(Ω) as the unique (weak) solution of the boundary value problem
div(σ grad u) = 
in Ω,
σ ∂
∂u = f
on ∂Ω,
∫∂Ω uds = ,
(.)
where f ∈L
◇(∂Ω). For the spaces H
◇(Ω) and L
◇(∂Ω) we refer to > Sect. ...
As in > Sect. ..we assume that σ ∈L∞(Ω) is a perturbation of the constant back-
ground admittivity function σ1 = . More precisely, let D ⊂Ω be again the finite union of
domains such that Ω/D is connected and σ = in Ω/D, and let there be a constant c> 
such that
Im σ(x) ≤
and
Re σ(x) ≥+ c
on D.
(.)
The case < c≤Re σ(x) ≤−ccan be treated in a similar way (see []). The
unique solvability of the direct problem, i.e., the boundary value problem (> .),
guarantees existence of the Neumann-to-Dirichlet operators Λ, Λ1 : L
◇(∂Ω) →L
◇(∂Ω)
corresponding to σ and σ1 = , respectively.
As in > Sect. .., the goal of the inverse problem is to determine the support D of
σ −from the knowledge of the absolute data Λ, or the relative data Λ −Λ1. The difference
to the setting in > Sect. ..is that σ is now a scalar and complex valued function.
In the probe method the sampling objects are curves in Ω starting at the boundary ∂Ω
of Ω. In the original paper [] these curves are called needles. We keep this notation but
mention that – perhaps in contrast to the colloquial meaning – these needles do not need
to be straight segments but can be curved in general. By choosing a family of needles the
Probe Method determines the first point on the needle which intersects the boundary ∂D
(see Theorem below). Therefore, in contrast to the Factorization Method and the Linear
Sampling Method the Probe Method tests on curves instead of points.
Deﬁnition
A needle C is the image of a continuously differentiable function η : [,] →Ω
such that η() ∈∂Ω and η(t) ∈Ω for all t ∈(,] and η′(t) /= for all t ∈[,] and
η(t) /= η(s) for t /= s. We call η a parameterization of the needle.
The following monotonicity property is the basic ingredient for the Probe Method.
Under the above assumptions on σ ∈L∞(Ω) there exists c > such that

c ∫D ∣grad u1∣dx ≤Re⟨f ,(Λ1 −Λ)f ⟩≤c ∫D ∣grad u1∣dx
(.)
for every f ∈L
◇(∂Ω). Here, u1 ∈H
◇(Ω) denotes the unique solution of (> .) for the
constant background case σ1 = .
Let η : [,] →Ω be the parameterization of a given needle, t ∈(,] a fixed parameter,
and Ct = {η(s) : ≤s ≤t} the part of the needle from s = to s = t. Let Φ(x, y) denote

Sampling Methods 

the fundamental solution of the Laplace equation, e.g.,
Φ(x, y) =

π∣x −y∣,
x /= y,
in R. The Approximation Theorem of Runge (see, e.g., []) yields the existence of a
sequence wn ∈H(Ω) of harmonic functions in Ω such that
∥wn −Φ (⋅, η(t))∥H(U) →,
n →∞,
(.)
for every open subset U with U ⊂Ω/Ct. We set fn = ∂wn/∂
on ∂Ω and note that fn
depends on Ct but not on the unknown domain D. The dependence on Ct is denoted by
writing fn(Ct). It can – at least in principle – be computed beforehand.
Theorem 
Let the above assumptions on σ hold and fix a needle with parameterization
η : [,] →Ω. Define the set T ⊂[,] by
T = {t ∈[,] : sup
n∈N
{∣Re⟨fn(Ct),(Λ −Λ1)fn(Ct)⟩∣< ∞}.
(.)
Here, fn(Ct) = ∂wn/∂∈H−/(∂Ω) is determined from (> .) (So far, we have chosen
the boundary current f in (> .) from L
◇(∂Ω) for convenience; however, the quadratic
form in (> .) extends as dual pairing ⟨H−/(∂Ω), H/(∂Ω)⟩to f ∈H−/(∂Ω) with
vanishing mean.). Then T /= /, and one can define t∗= sup {t ∈[,] : [, t] ∈T }, which
satisfies
t∗= {min {t ∈[,] : η(t) ∈∂D},
if C∩D /= /,
,
if C∩D = /.
(.)
We recall that C= C = {η(t) : t ∈[,]}.
For a proof we refer to [, ].
Note that for every needle the set T of the form (> .) is determined by the given
data: It depends on η and the approximating functions wn. Formula (> .) provides
a constructive way to determine ∂D from Λ −Λ1: One has to choose a family of needles
which cover the domain Ω, and for each needle one computes t∗as the largest point of T ; if
t∗< then η(t∗) ∈∂D. Obviously, this procedure is very expensive from a computational
point of view. However, if one samples with “linear” needles only, i.e., rays of the form
C = {z + tp : t ≥}∩Ω for z ∈Ω and unit vectors p ∈S, then the computational effort can
be reduced considerably since the approximating sequence (> .) has to be computed
only once for a reference needle. However, by using only rays as needles one can not expect
to detect the boundary of D completely. Only the “visible points” of ∂D can be detected,
i.e., those which can be connected completely in Ω/D by straight lines to ∂Ω.
In an implementation of the definition of T of (> .) one has to decide whether a
supremum is finite or infinite. Numerically, this is certainly not an easy task. In [] it has
been suggested to replace T of (> .) by


Sampling Methods
TM = {t ∈[,] : sup
n∈N
{∣Re⟨fn(Ct),(Λ −Λ1)fn(Ct)⟩∣≤M}
for some M > , for which a result analogously to the one in Theorem can be established.
We refer to [] for more details.
Again, the probe method is general enough to have extensions to a number of related
inverse problems in elasticity (see []) and scattering theory (see []). For numerical
reconstructions we refer to [].
.
Appendix
In this appendix we collect some functional analytic results on range identities. The Factor-
ization Method makes use of the fact that the unknown domain D can be characterized by
the range of some compact operator A : X →Y, where A is related to the known operator
M : Y →Y through the factorization
M = AGA∗.
(.)
Throughout this whole chapter we assume that Y is a Hilbert space and X a reflexive Banach
space with dual X∗. We denote by A∗: Y →X∗the adjoint of A, where Y is identified with
its dual.
For a computable characterization of D, the range of the operator A has to be expressed
by the operator M which is the goal of the range identity.
In the simplest case where also X is a Hilbert space and G is the identity I, the range
identity is easily obtained via the singular system of A and the Theorem of Picard. We recall
that {σj, xj, y j : j ∈J} is a singular system of a linear and compact operator T : X →Y
between Hilbert spaces X and Y if {x j : j ∈J} and {y j : j ∈J} are complete countable
orthonormal systems in the subspaces N(T)⊥⊂X and N(T∗)⊥⊂Y, respectively, and
σj ∈R>such that Txj = σjy j and T∗y j = σjx j for all j ∈J.
We note that {σ 
j , xj : j ∈J}, together with a basis of the null space N(T) of T and
associated eigenvalue , is an eigensystem of the self adjoint and non-negative operator
T∗T. Furthermore,
Tx = ∑
j∈J
σj(x, xj)X y j,
x ∈X,
T∗y = ∑
j∈J
σj(y, y j)Y x j,
y ∈Y.
Theorem 
(Picard) Let X, Y be Hilbert spaces and T : X →Y be a compact operator
with singular system {σj, xj, y j : j ∈J}. Then there holds: An element y ∈Y belongs to the
range R(T) of T, if and only if,
y ∈N(T∗)⊥
and
∑
j∈J
∣(y, y j)Y∣

σ 
j
< ∞.

Sampling Methods 

For a proof we refer to, e.g., []. Applying this theorem to the factorization (> .)
with G = I, and when X∗is identified with X, one obtains.
Corollary 
Let A : X →Y be a compact operator between Hilbert spaces X and Y with
dense range and M = AA∗: Y →Y. Then the ranges of A and M/coincide. Here, the self
adjoint and non-negative operator M/: Y →Y is given by
M/y = ∑
j∈J
√
λj(y, y j)Y y j,
y ∈Y,
where {y j : j ∈J} are the orthonormal eigenelements of the self adjoint, compact, and non-
negative operator M corresponding to the positive eigenvalues λj. It follows that
y ∈R(A)
⇐⇒
∑
j∈J
∣(y, y j)Y∣

λj
< ∞.
For more general factorizations of the form M = AGA∗, the following (preliminary)
characterization is useful (see []; for an equivalent formulation see Theorem of []).
Theorem 
Let X be a reflexive Banach space with dual X∗and dual form ⟨⋅,⋅⟩in
⟨X∗, X⟩. Furthermore, let Y be a Hilbert space and M : Y →Y and A : X →Y be linear
bounded operators such that the factorization (> .) holds for some linear and bounded
operator G : X∗→X, which satisfies a coercivity condition of the form: There exists c > 
with
∣⟨φ,Gφ⟩∣≥c∥φ∥
X∗
for all φ ∈R(A∗) ⊂X∗.
(.)
Then, for any ϕ ∈Y, ϕ /= ,
ϕ ∈R(A) ⇐⇒inf {∣(ψ, Mψ)Y∣: ψ ∈Y,(ψ, ϕ)Y = } > .
(.)
Proof
The form ∣(ψ, Mψ)Y∣can be estimated by
∣(ψ, Mψ)Y∣= ∣⟨A∗ψ,GA∗ψ⟩∣≥c∥A∗ψ∥
X∗
for all ψ ∈Y.
(.)
Let first ϕ = Aφfor some φ∈X. For ψ ∈Y with (ψ, ϕ)Y = there holds that
∣(ψ, Mψ)Y∣≥c∥A∗ψ∥
X∗=
c
∥φ∥
X
∥A∗ψ∥
X∗∥φ∥
X
≥
c
∥φ∥
X
∣⟨A∗ψ, φ⟩∣=
c
∥φ∥
X
∣(ψ, Aφ
`
=ϕ
)Y∣
=
c
∥φ∥
X
.
This provides the lower bound of the infimum.
Second, assume that ϕ ∉R(A). Define the closed subspace V := {ψ ∈Y : (ψ, ϕ)Y = }.
Then A∗(V) is dense in R(A∗) ⊂X∗. Indeed, this is equivalent to the statement that the
annihilators [A∗(V)]⊥and [R(A∗)]⊥= N(A) coincide. Therefore, let φ ∈[A∗(V)]⊥, i.e.,
⟨A∗ψ, φ⟩= for all ψ ∈V, i.e., (ψ, Aφ)Y = for all ψ ∈V, i.e., Aφ ∈V ⊥= span{ϕ}. Since
ϕ ∉R(A) this implies Aφ = , i.e., φ ∈N(A). Therefore, A∗(V) is dense in R(A∗).


Sampling Methods
Choose a sequence { ˆψn} in V such that A∗ˆψn →−

∥ϕ∥
Y A∗ϕ as n tends to infinity and
set ψn = ˆψn + ϕ/∥ϕ∥
Y. Then (ψn, ϕ)Y = and A∗ψn →. The first equation of (> .)
yields
∣(ψn, Mψn)Y∣≤∥G∥∥A∗ψn∥
X∗
and thus (ψn, Mψn)Y
→
, n
→
∞, which proves that inf {∣(ψ, Mψ)Y∣: ψ ∈Y,
(ψ, ϕ)Y = } = .
∎
We note that the inf-condition only depends on M and not on the factorization.
Therefore, we have as a corollary.
Corollary 
Let Y be a Hilbert space and Xand Xbe reflexive Banach spaces with duals
X∗
and X∗
, respectively. Furthermore, let M : Y →Y have two factorizations of the form
M = AGA∗
= AGA∗
as in (> .) with compact operators A j : Xj →Y and bounded
operators G j : X∗
j →Xj, which both satisfy the coercivity condition (> .). Then the
ranges of Aand Acoincide.
Corollary is useful for the analysis of the Factorization Method as long as M is nor-
mal. However, there are many scattering problems for which the corresponding far field
operator fails to be normal, e.g., in the case of absorbing media. For these problems, one
can utilize the self adjoint operator
M# = ∣Re M∣+ Im M,
(.)
which can be computed from M. Note that Re M = 
(M + M∗) and Im M = 
i(M −M∗)
are again self adjoint and compact, and the absolute value ∣Re M∣of Re M is defined to be
∣Re M∣ψ = ∑
j∈J
∣λj∣(ψ,ψ j)Yψj,
ψ ∈Y,
where {λj,ψ j : j ∈J} denotes the spectral system of Re M.
Now we can apply Corollary to obtain the following result (see [] for the lengthy
proof, and [] for a weaker form of assumption (d)).
Theorem 
Let X be a reflexive Banach space with dual X∗and dual form ⟨⋅,⋅⟩in
⟨X∗, X⟩. Furthermore, let Y be a Hilbert space and M : Y →Y and A : X →Y be lin-
ear bounded operators such that the factorization (> .) holds true for some linear and
bounded operator G : X∗→X. Furthermore, let the following conditions be satisfied:
(a)
The range of A is dense in Y.
(b)
There holds ReG = G+G, where Gsatisfies (> .) and G: X∗→X is compact.
(c)
The imaginary part Im G of G is non-negative, i.e., Im⟨φ,Gφ⟩≥for all φ ∈X∗.
(d)
G is injective or Im G is positive on the nullspace of ReG.
Then the self adjoint operator M# of (> .) is positive and the ranges of A and M/
#
coincide.

Sampling Methods 

As an immediate corollary we have
Corollary 
Let M : Y →Y and A : X →Y and G : X∗→X be as in Theorem , and let
G be self adjoint, i.e., G∗= G, and satisfy (> .). Then the ranges of A and M/coincide,
and
y ∈R(A)
⇐⇒
∑
j∈J
∣(y, y j)Y∣

λj
< ∞,
where {λj, y j : j ∈J} denotes a spectral system of the self adjoint and compact operator
M = AGA∗.
.
Cross-References
> EIT
> Expansion Methods
> Inverse Scattering
References and Further Reading
. Alves C, Ammari H () Boundary integral for-
mulae for the reconstruction of imperfections of
small diameter in an elastic medium. SIAM J Appl
Math :–
. Ammari H, Griesmaier R, Hanke M () Iden-
tification of small inhomogeneities: asymptotic
factorization. Math Comput :–
. Ammari H, Iakovleva E, Lesselier D () Two
numerical methods for recovering small inclu-
sions from the scattering amplitude at a fixed
frequency. SIAM J Sci Comput :–
. Ammari H, Iakovleva E, Moskow S () Recov-
ery of small inhomogeneities from the scattering
amplitude at a fixed frequency. SIAM J Math Anal
:–
. Ammari H, Kang H () Reconstruction of
small inhomogeneities from boundary measure-
ments, vol of lecture notes in mathematics.
Springer, New York
. Ammari H, Kang H () Polarization and
moment tensors with applications to inverse
problems and effective medium theory. Springer,
New York
. Ammari H, Vogelius MS, Volkov D ()
Asymptotic formulas for perturbations in the
electromagnetic fields due to the presence of
inhomogeneities of small diameter II. The full
Maxwell equations. J Math Pures Appl :–
. Aramini R, Brignone M, Piana M () The lin-
ear sampling method without sampling. Inverse
Prob :–
. Arens T () Linear sampling methods for
D inverse elastic wave scattering. Inverse Prob
:–
. Arens T () Why linear sampling works.
Inverse Prob :–
. Arens T, Grinberg NI () A complete factor-
ization method for scattering by periodic struc-
tures. Computing :–
. Arens T, Kirsch A () The Factorization
Method in inverse scattering from periodic struc-
tures. Inverse Prob :–
. Arens T, Lechleiter A () The linear sampling
method revisited. J Int Equ Appl :–
. Astala K, Päivärinta L () Calderón’s inverse
conductivity problem in the plane. Ann Math
:–
. Azzouz M, Oesterlein C, Hanke M, Schilcher K
() The factorization method for electrical
impedance tomography data from a new planar
device. International J Biomedical Imaging, Arti-
cle ID , pages, doi:.//.


Sampling Methods
. Beretta E, Vessella S () Stable determination
of boundaries from Cauchy data. SIAM J Math
Anal :–
. Van Berkel C, Lionheart WRB () Recon-
struction of a grounded object in an electrostatic
halfspace with an indicator function. Inverse Prob
Sci Eng :–
. Borcea L () Electrical impedance tomogra-
phy. Inverse Prob :R–R
. Bourgeois L, Lunéville E () The linear sam-
pling method in a waveguide: a modal formula-
tion. Inverse Prob :
. Brignone M, Bozza G, Aramini R, Pastorino M,
Piana M () A fully no-sampling formula-
tion of the linear sampling method for three-
dimensional inverse electromagnetic scattering
problems. Inverse Prob :
. Brühl M () Gebietserkennung in der elek-
trischen Impedanztomographie. PhD thesis, Uni-
versität Karlsruhe, Karlsruhe
. Brühl M () Explicit characterization of inclu-
sions in electrical impedance tomography. SIAM
J Math Anal :–
. Brühl M, Hanke M, Pidcock M () Crack
detection using electrostatic measurements. Math
Model Numer Anal :–
. Brühl M, Hanke M, Vogelius M () A direct
impedance tomography algorithm for locating
small inhomogeneities. Numer Math :–
. Burger M, Osher S () A survey on level
set methods for inverse problems and optimal
design. Eur J Appl Math :–
. Cakoni F, Colton D () The linear sampling
method for cracks. Inverse Prob :–
. Cakoni F, Colton D, Haddar H () The linear
sampling method for anisotropic media. J Com-
put Appl Math :–
. Cedio-Fengya D, Moskow S, Vogelius MS ()
Identification of conductivity imperfections of
small diameter by boundary measurements.
Comtinuous
dependence
and
computational
reconstruction. Inverse Prob :–
. Charalambopoulos A, Gintides D, Kiriaki K
() The linear sampling method for the trans-
mission problem in three-dimensional linear
elasticity. Inverse Prob :–
. Charalambopoulos A, Gintides D, Kiriaki K,
Kirsch A () The Factorization Method for
an acoustic wave guide. In: th international
workshop on mathematical methods in scattering
theory and biomedical engineering. World Scien-
tific, Singapore, pp –
. Charalambopoulus
A,
Kirsch
A,
Anagnos-
topoulus KA, Gintides D, Kiriaki K () The
Factorization Method in inverse elastic scatter-
ing from penetrable bodies. Inverse Prob :
–
. Cheney M () The linear sampling method
and the MUSIC algorithm. Inverse Prob :
–
. Collino F, Fares M, Haddar H () Numeri-
cal and analytical study of the linear sampling
method in electromagnetic inverse scattering
problems. Inverse Prob :–
. Colton D, Haddar H, Monk P () The linear
sampling method for solving the electromagnetic
inverse scattering problem. SIAM J Sci Comput
:–
. Colton D, Kirsch A () A simple method for
solving inverse scattering problems in the reso-
nance region. Inverse Prob :–
. Colton D, Kress R () Inverse acoustic and
electromagnetic
scattering
theory,
nd edn.
Springer, Berlin
. Colton D, Kress R () Using fundamen-
tal solutions in inverse scattering. Inverse Prob
:R–R
. Colton D, Päivärinta L () The uniqueness of
a solution to an inverse scattering problem for
electromagnetic waves. Arch Ration Mech Anal
:–
. Devaney AJ () Super-resolution process-
ing of multi-static data using time reversal and
MUSIC. Unpublished manuscript
. Engl H, Hanke M, Neubauer A () Regulariza-
tion of inverse problems. Kluwer, Dordrecht
. Fata SN, Guzina BB () A linear sampling
method for near field inverse problems in elasto-
dynamics. Inverse Prob :–
. Friedman A, Vogelius MS () Identification
of small inhomogeneities of extreme conductiv-
ity by boundary measurements: a theorem on
continuous dependence. Arch Ration Mech Anal
:–
. Gebauer B, Hyvönen N () Factorization
method and irregular inclusions in electri-
cal impedance tomography. Inverse Prob :
–
. Gebauer B, Hyvönen N () Factorization
methodandinclusionsof mixedtypeinan inverse

Sampling Methods 

elliptic boundary value problem. Inverse Prob
Imaging :–
. Gebauer S () The factorization method
for real elliptic problems. Z Anal Anwend :
–
. GiraultV,RaviartP-A () Finiteelementmeth-
ods for Navier–Stokes equations. Springer, Berlin
. Griesmaier R () An asymptotic factorization
method for detecting small objects using elec-
tromagnetic scattering. SIAM J Appl Math :
–
. Griesmaier R () Reconstruction of thin
tubular inclusions in three-dimensional domains
using electrical impedance tomography. SIAM J.
Imaging Sci :–
. Grinberg N () Obstacle localization in
an homogeneous half-space. Inverse Prob :
–
. Grinberg N () Obstacle visualization via the
factorization method for the mixed boundary
value problem. Inverse Prob :–
. Guzina BB, Bonnet M () Topological deriva-
tive for the inverse scattering of elastic waves. Q J
Mech Appl Math :–
. Haddar H, Monk P () The linear sampling
method for solving the electromagnetic inverse
medium problem. Inverse Prob :–
. Hähner P () An inverse problem in electro-
statics. Inverse Prob :–
. Hanke M () Why linear sampling really
seems to work. Inverse Prob Imaging :–
. Hanke M, Brühl M () Recent progress in
electrical impedance tomography. Inverse Prob
:S–S
. Hanke M, Schappel B () The factorization
method for electrical impedance tomography in
the half space. SIAM J Appl Math :–
. Harrach B, Seo JK () Detecting inclusions
in electrical impedance tomography without ref-
erence measurements. SIAM J Appl Math :
–
. Hettlich F () Fréchet derivatives in inverse
obstacle scattering. Inverse Prob :–
. Hettlich F, Rundell W () A second degree
method for nonlinear inverse problems. SIAM J
Numer Anal :–
. Hyvönen N () Characterizing inclusions in
optical tomography. Inverse Prob :–
. Hyvönen N () Approximating idealized
boundary data of electric impedance tomography
by electrode measurements. Math Models Meth-
ods Appl Sci :–
. Ikehata M () Reconstruction of an obstacle
from the scattering amplitude at a fixed frequency.
Inverse Prob :–
. Ikehata M () Reconstruction of the shape of
the inclusion by boundary measurements. Com-
mun Part Diff Eq :–
. Ikehata
M
()
Size
estimation
of
inclusions.
J
Inverse
Ill-Posed
Prob
:
–
. Kaltenbacher B, Neubauer A, Scherzer O ()
Iterative regularization methods for nonlinear ill-
posed problems. de Gruyter, Berlin
. Kirsch A () The domain derivative and two
applications in inverse scattering theory. Inverse
Prob :–
. Kirsch A () Characterization of the shape
of a scattering obstacle using the spectral
data of the far field operator. Inverse Prob :
–
. KirschA () Factorization of the far field oper-
ator for the inhomogeneous medium case and an
application in inverse scattering theory. Inverse
Prob :–
. Kirsch A () New characterizations of solu-
tions in inverse scattering theory. Appl Anal
:–
. Kirsch A () The MUSIC-algorithm and the
Factorization Method in inverse scattering the-
ory for inhomogeneous media. Inverse Prob :
–
. Kirsch A () The Factorization Method for
a class of inverse elliptic problems. Math Nachr
:–
. Kirsch A () An integral equation for
Maxwell’s equations in a layered medium with
an application to the Factorization Method. J Int
Equ Appl :–
. Kirsch A () An integral equation for the scat-
tering problem for an anisotropic medium and
the Factorization Method. In: th international
workshop on mathematical methods in scatter-
ing theory and biomedical engineering. World
Scientific, Singapore, pp –
. Kirsch A, Grinberg N () The factoriza-
tion
method for
inverse problems. Oxford
lecture series in mathematics and its appli-
cations,
vol
.
Oxford
University
Press,
Oxford


Sampling Methods
. Kirsch A, Ritter S () A linear sampling
method for inverse scattering from an open arc.
Inverse Prob :–
. Kress R, Kühn L () Linear sampling methods
for inverse boundary value problems in potential
theory. Appl Numer Math :–
. Lechleiter A () The factorization method is
independent of transmission eigenvalues. Inverse
Prob Imaging :–
. Lechleiter A, Hyvönen N, Hakula H () The
factorization method applied to the complete
electrodemodel of impedancetomography.SIAM
J Appl Math :–
. Lukaschewitsch M, Maass P, Pidcock M ()
Tikhonov regularization for electrical impedance
tomography on unbounded domains. Inverse
Prob :–
. Luke R, Potthast R () The no response test – a
sampling method for inverse scattering problems.
SIAM J Appl Math :–
. McLean W () Strongly elliptic systems and
boundary integral operators. Cambridge Univer-
sity Press, Cambridge
. Monk P () Finite element methods for
Maxwell’s equations. Oxford Science, Oxford
. Nachman AI, Päivärinta L, Teirilä A () On
imaging obstacles inside inhomogeneous media.
J Funct Anal :–
. Pike R, Sabatier P () Scattering: scattering
and inverse scattering in pure and applied science.
Academic, New York/London
. Pironneau O () Optimal shape design for
elliptic systems. Springer, New York
. Potthast R () A fast new method to solve
inverse scattering problems. Inverse Prob :
–
. Potthast R () Point sources and multipoles in
inverse scattering theory. Chapman & Hall/CRC,
Boca Raton
. Potthast R () A survey on sampling and
probe methods for inverse problems. Inverse Prob
:R–R
. Ringrose JR () Compact non-self-adjoint
operators. Van Nostrand Reinhold, London
. Sokolowski J, Zolesio JP () Introduction to
shape optimization. Springer, New York
. Therrien CW () Discrete random signals and
statistical signal processing. Prentice-Hall, Engle-
wood Cliffs
. Vogelius MS, Volkov D () Asymptotic for-
mulas for perturbations in the electromagnetic
fields due to the presence of inhomogeneities of
small diameter. MAN :–
. Zou Y, Guo Z () A review of electrical
impedance techniques for breast cancer detec-
tion. Med Eng Phys :–

Inverse Scattering
David Colton ⋅Rainer Kress
.
Introduction......................................................................
.
Direct Scattering Problems......................................................
..
The Helmholtz Equation...............................................................
..
Obstacle Scattering .....................................................................
..
Scattering by an Inhomogeneous Medium..........................................
..
The Maxwell Equations.................................................................
..
Historical Remarks.....................................................................
.
Uniqueness in Inverse Scattering...............................................
..
Scattering by an Obstacle..............................................................
..
Scattering by an Inhomogeneous Medium..........................................
..
Historical Remarks......................................................................
.
Iterative and Decomposition Methods in Inverse Scattering ..................
..
Newton Iterations in Inverse Obstacle Scattering ..................................
..
Decomposition Methods...............................................................
..
Iterative Methods Based on Huygens’ Principle....................................
..
Newton Iterations for the Inverse Medium Problem...............................
..
Least Squares Methods for the Inverse Medium Problem........................
..
Born Approximation....................................................................
..
Historical Remarks......................................................................
.
Qualitative Methods in Inverse Scattering.....................................
..
The Far Field Operator and Its Properties...........................................
..
The Linear Sampling Method.........................................................
..
The Factorization Method.............................................................
..
Lower Bounds for the Surface Impedance..........................................
..
Transmission Eigenvalues..............................................................
..
Historical Remarks......................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Inverse Scattering
Abstract: We give a survey of the mathematical basis of inverse scattering theory,
concentrating on the case of time-harmonic acoustic waves. After an introduction and his-
torical remarks we give an outline of the direct scattering problem. This is then followed by
sections on uniqueness results in inverse scattering theory and iterative and decomposition
methods to reconstruct the shape and material properties of the scattering object. We con-
clude by discussing qualitative methods in inverse scattering theory, in particular the linear
sampling method and its use in obtaining lower bounds on the constitutive parameters of
the scattering object.
.
Introduction
Scattering theory is concerned with the effects that obstacles and inhomogeneities have
on the propagation of waves and in particular time-harmonic waves. In the context of
this book, scattering theory provides the mathematical tools for imaging via acoustic and
electromagnetic waves with applications to such fields as radar, sonar, geophysics, medical
imaging, and nondestructive testing.
For reasons of brevity, in this survey we focus our attention on the case of acoustic waves
and only give passing references to the case of electromagnetic waves. We will furthermore
give few proofs, referring the reader interested in further details to []. Since the literature
in the area is enormous, we have only referenced a limited number of papers and hope that
the reader can use these as starting point for further investigations.
Mathematical acoustics begins with the modeling of acoustic waves, i.e., sound waves.
The two main media for the propagation and scattering of sound waves are air and water
(underwater acoustics). A third important medium with properties close to those of water
is the human body, i.e., biological tissue (ultrasound). Since sound waves are considered as
small perturbations in a gas or a fluid, the equation of acoustics, i.e., the wave equation

c
∂p
∂t= Δp
(.)
for the pressure p = p(x, t) is obtained by linearization of the equations for the motion of
fluids. Here, c = c(x) denotes the local speed of sound and the fluid velocity is proportional
to grad p. For time-harmonic acoustic waves of the form
p(x, t) = Re{u(x) e−iωt}
(.)
with frequency ω > , it follows that the complex-valued space dependent part u satisfies
the reduced wave equation
Δu + ω
cu = .
(.)
Here we emphasize that the physical quantity describing the sound wave is the real-valued
sound pressure p(x, t) and not the complex-valued amplitude u(x) in the representation

Inverse Scattering 

u(x) e−iωt. For a homogeneous medium, the speed of sound c is constant and (> .)
becomes the Helmholtz equation
△u + ku = ,
(.)
where the wave number k is given by the positive constant k = ω/c.
A solution to the Helmholtz equation whose domain of definition contains the exterior
of some sphere is called radiating if it satisfies the Sommerfeld radiation condition
lim
r→∞r (∂us
∂r −ikus) = ,
(.)
where r = ∣x∣and the limit holds uniformly in all directions x/∣x∣. Here, and in the sequel,
∣x∣:=
√
x
+ x
+ x
denotes the Euclidean norm of x = (x, x, x) ∈IR. For more details
on the physical background of linear acoustic waves, the reader is referred to [].
We will confine our presentation of scattering theory for time-harmonic acoustic waves
to two basic problems, namely, scattering by a bounded impenetrable obstacle and scatter-
ing by a penetrable inhomogeneous medium of compact support. For a vector d ∈IRwith
∣d∣= , the function eik x⋅d satisfies the Helmholtz equation for all x ∈IR. It is called a
plane wave, since ei(k x⋅d−ωt) is constant on the planes k x ⋅d −ωt = const. Note that these
wave fronts travel with velocity c in the direction d. Assume that an incident field is given
by the plane wave ui(x) = eik x⋅d. Then the simplest obstacle scattering problem is to find
the scattered field us as a radiating solution to the Helmholtz equation in the exterior of a
bounded scatterer D such that the total field u = ui + us satisfies the Dirichlet boundary
condition
u = 
on ∂D
(.)
corresponding to a sound-soft obstacle with the total pressure, i.e., the excess pressure over
the static pressure p, vanishing on the boundary. Concerning the geometry of scattering
obstacles, for simplicity, we always will assume that D is a bounded domain with a con-
nected boundary ∂D of class C. In particular, this implies that the complement IR/D is
connected. However, our results remain valid for a finite number of scattering obstacles.
Boundary conditions other than the Dirichlet condition also need to be considered
such as the Neumann or sound-hard boundary condition
∂u
∂
= 
on ∂D
(.)
or, more generally, the impedance boundary condition
∂u
∂
+ iλu = 
on ∂D,
(.)
where
is the outward unit normal to ∂D and λ is a positive constant called the surface
impedance. More generally the impedance λ can also vary on ∂D. Since grad u is pro-
portional to the fluid velocity, the impedance boundary condition describes obstacles for
which the normal velocity of the fluid on the boundary is proportional to the excess pres-
sure on the boundary. The Neumann condition corresponds to a vanishing normal velocity


Inverse Scattering
on the boundary. In order to avoid repetitions by considering all possible types of boundary
conditions, we will in general confine ourselves to presenting the basic ideas in acoustic
obstacle scattering for the case of a sound-soft obstacle.
The simplest scattering problem for an inhomogeneous medium assumes that the speed
of sound is constant outside a bounded domain D. Then the total field u = ui + us satisfies
△u + knu = 
in IR
(.)
and the scattered wave us fulfills the Sommerfeld radiation condition (> .), where the
wave number is given by k = ω/cand n = c
/ cis the refractive index given by the ratio
of the square of the sound speeds c = cin the homogeneous host medium and c = c(x) in
the inhomogeneous medium. The refractive index is positive, satisfies n(x) = for x /∈D,
and we assume n to be continuously differentiable in IR(our results are also in general
valid for n being merely piecewise continuous in IR). An absorbing medium is modeled
by adding an absorption term which leads to a refractive index with a positive imaginary
part of the form
n = c

c+ i γ
k
in terms of a possibly space dependent absorption coefficient γ.
Summarizing, given the incident wave and the physical properties of the scatterer, the
direct scattering problem is to find the scattered wave and in particular its behavior at large
distances from the scattering object, i.e., its far field behavior. The inverse scattering problem
takes this answer to the direct scattering problem as its starting point and asks what is the
nature of the scatterer that gave rise to such far field behavior?
To be more specific, it can be shown that radiating solutions us to the Helmholtz
equation have the asymptotic behavior
us(x) = eik∣x∣
∣x∣{u∞(ˆx) + O ( 
∣x∣)},
∣x∣→∞,
(.)
uniformly for all directions ˆx = x/∣x∣, where the function u∞defined on the unit sphere
Sis known as the far field pattern of the scattered wave. For plane wave incidence we
indicate the dependence of the far field pattern on the incident direction d and the obser-
vation direction ˆx by writing u∞= u∞(ˆx, d). The inverse scattering problem can now be
formulated as the problem of determining either the sound-soft obstacle D or the index of
refraction n (and hence also D) from a knowledge of the far field pattern u∞(ˆx, d) for ˆx
and d on the unit sphere S(or a subset of S).
One of the earliest mathematical results in inverse scattering theory was Schiffer’s proof
in that the far field pattern u∞(ˆx, d) for ˆx, d ∈Suniquely determines the shape of a
sound-soft obstacle D. Unfortunately, Schiffer’s proof does not immediately generalize to
other boundary conditions. This problem was remedied by Kirsch and Kress in who,
using an idea originally proposed by Isakov, showed that u∞(ˆx, d) for ˆx, d ∈Suniquely
determines the shape of D as long as the solution of the direct scattering problem depends
continuously on the boundary data []. In particular, it is not necessary to know the
boundary condition a priori in order to guarantee uniqueness! The uniqueness problem for

Inverse Scattering 

inverse scattering by an inhomogeneous medium was solved by Nachman [], Novikov
[], and Ramm [] in who based their analysis on the fundamental work of Sylvester
and Uhlmann []. Their uniqueness proof was subsequently considerably simplified by
Hähner [].
The first attempt to reconstruct the shape of a sound-soft scattering obstacle from a
knowledge of the far field pattern in a manner acknowledging the nonlinear and ill-posed
nature of the problem was made by Roger in using Newton’s iteration method [].
A characterization and rigorous proof of the existence of the Fréchet derivative of the solu-
tion operator in Newton’s method was then established by Kirsch [] and Potthast [] in
and , respectively. An alternative approach to solving the inverse scattering prob-
lem was proposed by Colton and Monk in and by Kirsch and Kress in who broke
up the inverse scattering problem into a linear, ill-posed problem and a nonlinear, well
posed problem [, ]. The optimization method of Kirsch and Kress has the attractive
feature of needing only a single incident field for its implementation. On the other hand,
to use such methods it is necessary to know the number of components of the scatterer as
well as the boundary condition satisfied by the field on the surface of the scatterer. These
problems were overcome by Colton and Kirsch in through the derivation of a linear
integral equation with the far field data as its kernel (i.e., multi-static data is needed for
its implementation) []. This method, subsequently called the linear sampling method,
was further developed by Colton et al. [] and numerous other researchers. A signifi-
cant development in this approach to the inverse scattering problem was the introduction
of the factorization method by Kirsch in []. For further historical information on
these “sampling” methods in inverse scattering theory, we refer the reader to the chapter
in this handbook on sampling methods as well as the monographs [, ].
Optimization methods and sampling methods for the inverse scattering problem for
inhomogeneous media have been extensively investigated by numerous authors. In general,
the optimization methods are based on rewriting the scattering problem corresponding to
(> .) as the Lippmann–Schwinger integral equation
u(x) = eikx⋅d −k
π ∫IR
eik∣x−y∣
∣x −y∣m(y)u(y) dy,
x ∈IR,
(.)
where m := −n and the object is to determine m from a knowledge of
u∞(ˆx, d) = −k
π ∫IRe−ik ˆx⋅ym(y)u(y) dy,
ˆx, d ∈S.
(.)
On the other hand, sampling methods have also been used to study the inverse scattering
problem associated with (> .) where now the object is to only determine the support
of m. For details and further references see [, , ].
Finally, as pointed out in [], an alternative direction in inverse scattering theory than
that discussed above is to only try to obtain lower and upper bounds on a few relevant
features of the scattering object rather than attempting a complete reconstruction. This
relatively new direction in inverse scattering theory will be discussed in > Sect. ..


Inverse Scattering
.
Direct Scattering Problems
..
The Helmholtz Equation
Most of the basic properties of solutions to the Helmholtz
> Eq. (.) can be deduced
from the fundamental solution
Φ(x, y) := 
π
eik∣x−y∣
∣x −y∣,
x ≠y.
(.)
For fixed y ∈IRit satisfies the Helmholtz equation in IR/{y}. In addition, it satisfies
the radiation condition (> .) uniformly with respect to y on compact subsets of IR.
Physically speaking, the fundamental solution represents an acoustic point source located
at the point y. In addition to plane waves, point sources will also occur as incident fields in
scattering problems.
Green’s integral theorems provide basic tools for investigating the Helmholtz equation.
As an immediate consequence they imply the Helmholtz representation
u(x) = ∫∂D {∂u
∂
(y) Φ(x, y) −u(y) ∂Φ(x, y)
∂(y) } ds(y),
x ∈D,
(.)
for solutions u ∈C(D) ∩C(D) to the Helmholtz equation. The representation (> .)
implies that solutions to the Helmholtz equation inherit analyticity from the fundamental
solution. Any solution u to the Helmholtz equation in D satisfying
u = ∂u
∂
= 
on Γ
(.)
for some open subset Γ ⊂∂D must vanish identically in D. This can be seen via extending
the definition of u by (> .) for x ∈(IR/D) ∪Γ. Then, by Green’s integral theorem,
applied to u and Φ(x,⋅), we have u = in IR/D. Clearly u solves the Helmholtz equation
in (IR/∂D) ∪Γ and therefore by analyticity u = in D since D and IR/D are connected
through the gap Γ in ∂D.
As a consequence of the radiation condition (> .) the Helmholtz representation is
also valid in the exterior domain IR/D, i.e., we have
u(x) = ∫∂D {u(y) ∂Φ(x, y)
∂(y)
−∂u
∂
(y) Φ(x, y)}ds(y),
x ∈IR/D,
(.)
for radiating solutions u ∈C(IR/D) ∩C(IR/D) to the Helmholtz equation. From
> Eq. (.) we observe that radiating solutions u to the Helmholtz equation satisfy
Sommerfeld’s finiteness condition
u(x) = O ( 
∣x∣),
∣x∣→∞,
(.)
uniformly for all directions and that the validity of the Sommerfeld radiation condition
(> .) is invariant under translations of the origin.

Inverse Scattering 

We are now in a position to introduce the fundamental notion of the far field pattern
of radiating solutions to the Helmholtz equation.
Theorem 
Every radiating solution u to the Helmholtz equation has an asymptotic
behavior of the form
u(x) = eik∣x∣
∣x∣{u∞(ˆx) + O ( 
∣x∣)},
∣x∣→∞,
(.)
uniformly in all directions ˆx = x/∣x∣, where the function u∞defined on the unit sphere Sis
called the far field pattern of u. Under the assumptions of (> .) we have
u∞(ˆx) = 
π ∫∂D {u(y) ∂e−ik ˆx⋅y
∂(y) −∂u
∂
(y) e−ik ˆx⋅y} ds(y),
ˆx ∈S.
(.)
Proof
This follows from (> .) by using the estimates
eik∣x−y∣
∣x −y∣= eik∣x∣
∣x∣{e−ik ˆx⋅y + O ( 
∣x∣)},
∂
∂(y)
eik∣x−y∣
∣x −y∣= eik∣x∣
∣x∣{∂e−ik ˆx⋅y
∂(y) + O ( 
∣x∣)}
which hold uniformly for all y ∈∂D and all directions x/∣x∣as ∣x∣→∞.
∎
From the representation (> .), it follows that the far field pattern is an analytic
function on S. As an extension of (> .), each radiating solution u to the Helmholtz
equation has an Atkinson–Wilcox expansion of the form
u(x) = eik∣x∣
∣x∣
∞
∑
ℓ=

∣x∣ℓFℓ( x
∣x∣)
(.)
that converges absolutely and uniformly on compact subsets of IR/B, where B ⊃D is a
ball centered at the origin. The coefficients in the expansion (> .) are determined in
terms of the far field pattern F= u∞by the recursion
ikℓFℓ= ℓ(ℓ−)Fℓ−+ BFℓ−,
ℓ= ,, . . . ,
(.)
where B denotes the Laplace–Beltrami operator for the unit sphere. The following conse-
quence of the expansion (> .) is known as Rellich’s lemma.
Lemma 
Let u be a radiating solution to the Helmholtz equation for which the far field
pattern u∞vanishes identically. Then u vanishes identically.
Proof
This follows from (> .) and (> .) together with the analyticity of solutions
to the Helmholtz equation.
∎
Corollary 
Let u ∈C(IR/D) ∩C(IR/D) be a radiating solution to the Helmholtz
equation in IR/D for which
Im∫∂D u ∂¯u
∂
ds ≥.
(.)
Then u = in IR/D.


Inverse Scattering
Proof
Using Green’s integral theorem, the radiation condition can be utilized to establish
that
lim
r→∞∫∣x∣=r {∣∂u
∂∣

+ k∣u∣} ds = −k Im ∫∂D u ∂¯u
∂
ds.
Now the assumption (> .) implies limr→∞∫∣x∣=r ∣u∣ds = and the statement follows
from Lemma .
∎
Scattering from infinitely long cylindrical obstacles or inhomogeneities leads to the
Helmholtz equation in IR. The two-dimensional case can be used as an approximation
for scattering from finitely long cylinders, and more importantly, it can serve as a model
case for testing numerical approximation schemes in direct and inverse scattering. Without
giving details, we can summarize that our analysis remains valid in two dimensions after
appropriate modifications of the fundamental solution and the radiation condition. The
fundamental solution to the Helmholtz equation in two dimensions is given by
Φ(x, y) := i
H()
(k∣x −y∣),
x ≠y,
(.)
in terms of the Hankel function H()

of the first kind of order zero. In IRthe Sommerfeld
radiation condition has to be replaced by
lim
r→∞
√r (∂u
∂r −iku) = ,
r = ∣x∣,
(.)
uniformly for all directions x/∣x∣. According to the form (> .) of the radiation
condition, the definition of the far field pattern (> .) has to be replaced by
u(x) = eik∣x∣
√
∣x∣
{u∞(ˆx) + O ( 
∣x∣)},
∣x∣→∞,
(.)
and the representation (> .) assumes the form
u∞(ˆx)=
ei π

√
πk ∫∂D{u(y) ∂e−ik ˆx⋅y
∂(y) −∂u
∂
(y) e−ik ˆx⋅y}ds(y)
(.)
for ˆx = x/∣x∣.
..
Obstacle Scattering
After renaming the unknown functions, the direct scattering problem for sound-soft obsta-
cles is a special case of the following exterior Dirichlet problem: Given a function f ∈
C(∂D), find a radiating solution u ∈C(IR/D) ∩C(IR/D) to the Helmholtz equation
that satisfies the boundary condition
u = f
on ∂D.
(.)
Theorem 
The exterior Dirichlet problem for the Helmholtz equation has at most one
solution.

Inverse Scattering 

Proof
Let u satisfy the homogeneous boundary condition u = on ∂D. If u were
continuously differentiable up to the boundary, we could immediately apply Corollary 
to obtain u = in IR/D. However, in the formulation of the exterior Dirichlet problem, u
is only assumed to be in C(IR/D). We refrain from discussing possibilities to overcome
this regularity gap and refer to the literature [].
∎
Theorem 
The exterior Dirichlet problem has a unique solution.
Proof
The existence of a solution can be elegantly based on boundary integral equations.
In the layer approach, one tries to find the solution in the form of a combined acoustic
double- and single-layer potential
u(x) = ∫∂D {∂Φ(x, y)
∂(y)
−iΦ(x, y)}φ(y) ds(y)
(.)
for x ∈IR/D with a density φ ∈C(∂D). Then, after introducing the single- and double-
layer integral operators S, K : C(∂D) →C(∂D) by
(Sφ)(x) := ∫∂D Φ(x, y)φ(y) ds(y),
x ∈∂D,
(.)
(Kφ)(x) := ∫∂D
∂Φ(x, y)
∂(y)
φ(y) ds(y),
x ∈∂D,
(.)
and using their regularity and jump relations it can be seen that (> .) solves the exterior
Dirichlet problem provided the density φ is a solution of the integral equation
φ + Kφ −iSφ = f .
(.)
Due to their weakly singular kernels the operators S and K turn out to be compact. Hence,
the existence of a solution to (> .) can be established with the aid of the Riesz–Fredholm
theory for compact operators by showing that the homogeneous form of (> .) only
allows the trivial solution φ = .
Let φ be a solution of the homogeneous equation and let the subscripts ± denote the
limits obtained by approaching ∂D from IR/D and D, respectively. Then the potential u
defined by (> .) in all of IR/∂D satisfies the homogeneous boundary condition u+ = 
on ∂D whence u = in IR/D follows by Theorem . The jump relations for single- and
double-layer potentials now yield
−u−= φ,
−∂u−
∂
= iφ
on ∂D.
Hence, using Green’s first integral theorem, we obtain
i ∫∂D ∣φ∣ds = ∫∂D ¯u−
∂u−
∂
ds = ∫D {∣grad u∣−k∣u∣} dx,
and taking the imaginary part yields φ = .
∎


Inverse Scattering
We note that, in addition to existence of a solution, the Riesz–Fredholm theory also
establishes well-posedness, i.e., the continuous dependence of the solution on the data.
Instead of the classical setting of continuous functions spaces, the existence analysis can
also be considered in the Sobolev space H/(∂D) for the boundary integral operators
leading to solutions in the energy space H
loc(IR/D) (see [, ]).
We further note that without the single-layer potential included in (> .), the cor-
responding double-layer integral equation suffers from non-uniqueness if k is a so-called
irregular wave number or internal resonance, i.e., if there exist nontrivial solutions u to the
Helmholtz equation in the interior domain D satisfying homogeneous Neumann boundary
conditions ∂u/∂= on ∂D.
For the numerical solution of the boundary integral equations in scattering theory via
spectral methods in two and three dimensions we refer to []. For boundary element
methods we refer to [].
In general, for the scattering problem the boundary values are as smooth as the bound-
ary, since they are given by the restriction of the analytic function ui to ∂D. Therefore,
we may use the Helmholtz representation (> .) and Green’s second integral theorem
applied to ui and Φ(x,⋅) to obtain the following theorem:
Theorem 
For scattering from a sound-soft obstacle D we have
u(x) = ui(x) −∫∂D
∂u
∂
(y) Φ(x, y) ds(y),
x ∈IR/D,
(.)
and the far field pattern of the scattered field us is given by
u∞(ˆx) = −
π ∫∂D
∂u
∂
(y) e−ik ˆx⋅y ds(y),
ˆx ∈S.
(.)
The representation (> .) for the scattered field through the so-called secondary
sources on the boundary is known as Huygens’ principle. Here we will use it for the motiva-
tion of the Kirchhoff or physical optics approximation as an intuitive procedure to simplify
the direct scattering problem. For large wave numbers k, i.e., for small wave lengths, in a
first approximation a convex object D locally may be considered at each point x of ∂D as
a plane with normal (x). This suggests
∂u
∂
= ∂ui
∂
on the part ∂D−:= {x ∈∂D : (x) ⋅d < } that is illuminated and
∂u
∂
= 
in the shadow region ∂D+ := {x ∈∂D : (x) ⋅d ≥}. Thus, the Kirchhoff approximation
for the scattering of a plane wave with incident direction d at a convex sound-soft obstacle
is given by
u(x) ≈eik x⋅d −∫∂D−
∂eik y⋅d
∂(y) Φ(x, y) ds(y)
(.)
for x ∈IR/D.

Inverse Scattering 

..
Scattering by an Inhomogeneous Medium
Recall the scattering problem for an inhomogeneous medium with refractive index n as
described by (> .) for the total wave u = ui + us with incident field ui and the scat-
tered wave us satisfying the Sommerfeld radiation condition. The function m := −n has
support D.
The counterpart of the Helmholtz representation is given by the Lippmann–Schwinger
equation
u(x) = ui(x) −k∫D Φ(x, y)m(y)u(y) dy,
x ∈IR,
(.)
which can be shown to be equivalent to the scattering problem.
In order to establish existence of a solution to (> .) via the Riesz–Fredholm
theory it must be shown that the homogeneous equation has only the trivial solution, or
equivalently, that the only solution to (> .) satisfying the radiation condition is iden-
tically zero. For this, in addition to Rellich’s lemma, the following unique continuation
principle is required: Any solution u ∈C(G) of > Eq. (.) in a domain G ⊂IRsuch that
n ∈C(G) and u vanishes in an open subset of G vanishes identically. Hence, we have the
following result on existence and uniqueness for the inhomogeneous medium scattering
problem.
Theorem 
For a refractive index n ∈C(IR) with Re n ≥and Im n ≥, the Lippmann–
Schwinger equation or, equivalently, the inhomogeneous medium scattering problem has a
unique solution.
Proof
From Green’s first integral theorem it follows that
∫∂D u ∂¯u
∂
ds = ∫D {∣grad u∣−kn∣u∣} dx.
Taking the imaginary part and applying Corollary yields u = in IR/D in view of the
assumptions on n and the proof is finished by the unique continuation principle.
∎
From (> .) we see that
us(x) = −k∫IRΦ(x, y)m(y)u(y) dy,
x ∈IR.
Hence, the far field pattern u∞is given by
u∞(ˆx) = −k
π ∫IRe−ik ˆx⋅ym(y)u(y) dy,
ˆx ∈S.
(.)
We note that for k∥m∥∞sufficiently small, u can be obtained by the method of succes-
sive approximations. If in (> .) we replace u by the first term in this iterative process,
we obtain the Born approximation
u∞(ˆx) ≈−k
π ∫IRe−ik ˆx⋅ym(y)ui(y) dy,
ˆx ∈S.
(.)


Inverse Scattering
For numerical solutions of the inverse medium scattering problem by finite element
methods coupled with boundary element methods via nonlocal boundary conditions we
refer to [].
..
The Maxwell Equations
We now consider the Maxwell equations as the foundation of electromagnetic scattering
theory. Our presentation is organized parallel to that on the Helmholtz equation, i.e.,
on acoustic scattering, and will be confined to homogeneous isotropic media. Consider
electromagnetic wave propagation in an isotropic dielectric medium in IRwith constant
electric permittivity ε and magnetic permeability μ. The electromagnetic wave is described
by the electric field E and the magnetic field H satisfying the time dependent Maxwell
equations
curlE + μ ∂H
∂t = ,
curlH −ε ∂E
∂t = .
(.)
For time-harmonic electromagnetic waves of the form
E(x, t) = Re {ε−/E(x) e−iωt},
H(x, t) = Re{μ−/H(x) e−iωt}
(.)
with frequency ω > , the complex-valued space dependent parts E and H satisfy the
reduced Maxwell equations
curl E −ikH = ,
curl H + ikE = ,
(.)
where the wave number k is given by the positive constant k = √εμ ω. We will only be
concerned with the reduced Maxwell equations and will henceforth refer to them as the
Maxwell equations.
A solution E, H to the Maxwell equations whose domain of definition contains the
exterior of some sphere is called radiating if it satisfies one of the Silver–Müller radiation
conditions
lim
r→∞(H × x −rE) = 
(.)
or
lim
r→∞(E × x + rH) = ,
(.)
where r = ∣x∣and the limits hold uniformly in all directions x/∣x∣. For more details on the
physical background of electromagnetic waves, we refer to [, ].
For the Maxwell equations, the counterpart of the Helmholtz representation (> .)
is given by the Stratton–Chu formula
E(x)=−curl∫∂D (y) × E(y) Φ(x, y) ds(y)
+ 
ik curlcurl∫∂D (y) × H(y) Φ(x, y) ds(y),
x ∈D,
(.)
for solutions E, H ∈C(D) ∩C(D) to the Maxwell equations. A corresponding represen-
tation for H can be obtained from (> .) with the aid of H = curl E/ik.

Inverse Scattering 

The representation (> .) implies that each continuously differentiable solution to
the Maxwell equations automatically has analytic Cartesian components. Therefore, one
can employ the vector identity curlcurl E = −ΔE + grad div E to prove that for a solution
E, H to the Maxwell equations both E and H are divergence free and satisfy the vector
Helmholtz equation. Conversely, if E is a solution to the vector Helmholtz equation △E +
kE = satisfying div E = , then E and H := curl E/ik satisfy the Maxwell equations.
It can be shown that solutions E, H to the Maxwell equations in D satisfying
× E =
× H = 
on Γ
(.)
for some open subset Γ ⊂∂D must vanish identically in D.
As a consequence of the Silver–Müller radiation condition the Stratton–Chu formula
is also valid in the exterior domain IR/D, i.e., we have
E(x)=curl∫∂D (y) × E(y) Φ(x, y) ds(y)
−
ik curlcurl∫∂D (y) × H(y) Φ(x, y) ds(y),
x ∈IR/D,
(.)
for radiating solutions E, H ∈C(IR/D) ∩C(IR/D) to the Maxwell equations. Again,
a corresponding representation for H can be obtained from (> .) with the aid of
H = curl E/ik.
From (> .) it can be seen that the radiation condition (> .) implies (> .)
and vice versa. Furthermore, one can deduce that radiating solutions E, H to the Maxwell
equations automatically satisfy the Silver–Müller finiteness conditions
E(x) = O ( 
∣x∣),
H(x) = O ( 
∣x∣),
∣x∣→∞,
(.)
uniformly for all directions and that the validity of the Silver–Müller radiation condi-
tions (> .) and (> .) is invariant under translations of the origin. From the
Helmholtz representation (> .) for radiating solutions to the Helmholtz equation and
the Stratton–Chu formulas for radiating solutions to the Maxwell equations, it can be
deduced that for solutions to the Maxwell equations the Silver–Müller radiation condi-
tion is equivalent to the Sommerfeld radiation condition for the Cartesian components of
E and H.
The Stratton–Chu formula (> .) can be used to introduce the notion of the electric
and magnetic far field patterns.
Theorem 
Every radiating solution E, H to the Maxwell equations has the asymptotic
form
E(x) = eik∣x∣
∣x∣{E∞(ˆx) + O ( 
∣x∣)},
H(x) = eik∣x∣
∣x∣{H∞(ˆx) + O ( 
∣x∣)}
(.)
for ∣x∣→∞uniformly in all directions ˆx = x/∣x∣, where the vector fields E∞and H∞defined
on the unit sphere Sare called the electric far field pattern and magnetic far field pattern,
respectively. They satisfy


Inverse Scattering
H∞=
× E∞
and
⋅E∞=
⋅H∞= 
(.)
with the unit outward normal
on S. Under the assumptions of (> .) we have
E∞(ˆx) = ik
π ˆx × ∫∂D { (y) × E(y) + [ (y) × H(y)] × ˆx} e−ik ˆx⋅y ds(y)
(.)
for ˆx ∈Sand a corresponding expression for H∞.
Rellich’s lemma carries over immediately from the Helmholtz to the Maxwell equations.
Lemma 
Let E, H be a radiating solution to the Maxwell equations for which the electric
far field pattern E∞vanishes identically. Then E and H vanish identically.
The electromagnetic counterpart of Corollary is given by the following result.
Corollary 
Let E, H ∈C(IR/D) ∩C(IR/D) be a radiating solution to the Maxwell
equations in IR/D for which
Re∫∂D
⋅E × ¯H ds ≤.
Then E = H = in IR/D.
For two vectors d, p ∈IRwith ∣d∣= and p ⋅d = the plane waves
Ei(x) = p eik x⋅d,
Hi(x) = d × p eik x⋅d
(.)
satisfy the Maxwell equations for all x ∈IR. The orthogonal vectors p and d×p describe the
polarization direction of the electric and the magnetic field, respectively. Given the incident
field Ei, Hi and a bounded domain D ⊂IR, the simplest obstacle scattering problem is
to find the scattered field Es, Hs as a radiating solution to the Maxwell equations in the
exterior of the scatterer D such that the total field E = Ei + Es, H = Hi + Hs satisfies the
perfect conductor boundary condition
× E = 
on ∂D,
(.)
where
is the outward unit normal to ∂D. A more general boundary condition is the
impedance or Leontovich boundary condition
× H −λ ( × E) ×
= 
on ∂D,
(.)
where λ is a positive constant or function called the surface impedance.
Theorem 
The scattering problem for a perfect conductor has a unique solution.

Inverse Scattering 

Proof
Uniqueness follows from Corollary . The existence of a solution again can be
based on boundary integral equations. In the layer approach, one tries to find the solution
in the form of a combined magnetic and electric dipole distribution
Es(x) =curl∫∂D a(y)Φ(x, y) ds(y)
+i curlcurl∫∂D (y) × (S
a)(y)Φ(x, y) ds(y),
x ∈IR/∂D.
(.)
Here Sdenotes the single-layer operator (> .) in the potential theoretic limit case
k = and the density a is assumed to belong to the space C,α
div(∂D) of Hölder con-
tinuous tangential fields with Hölder continuous surface divergence. After defining the
electromagnetic boundary integral operators M and N by
(Ma)(x) := ∫∂D (x) × curlx {a(y)Φ(x, y)}ds(y),
x ∈∂D,
(.)
and
(Na)(x) := (x) × curlcurl∫∂D (y) × a(y) Φ(x, y) ds(y),
x ∈∂D,
(.)
it can be shown that Es given by (> .) together with Hs = curl Es/ik solves the perfect
conductor scattering problem provided the density a satisfies the integral equation
a + Ma + iNPS
a = −
× Ei.
(.)
Here the operator P is defined by Pb := ( × b) ×
for (not necessarily tangential) vec-
tor fields b. Exploiting the smoothing properties of the operator Sit can be shown that
M + iNPS
is a compact operator from C,α
div(∂D) into itself. The existence of a solution
to (> .) can now be based on the Riesz–Fredholm theory by establishing that the
homogeneous form of (> .) only has the trivial solution [].
∎
Note that, analogous to the acoustic case, without the electric dipole distribution
included in (> .), the corresponding magnetic dipole integral equation is not uniquely
solvable if k is a irregular wave number, i.e., if there exists a nontrivial solution E, H to the
Maxwell equations in D satisfying the homogeneous boundary condition
×E = on ∂D.
Instead of the classical setting of Hölder continuous functions, the integral equation
can also be considered in the Sobolev space H/
div(∂D) of tangential fields in H/(∂D)
that have a weak surface divergence in H/(∂D) (see []).
In addition to electromagnetic obstacle scattering, one can also consider scatter-
ing from an inhomogeneous medium where outside a bounded domain D the electric
permittivity ε and magnetic permeability μ are constant and the conductivity σ van-
ishes, i.e., ε = ε, μ = μand σ = in IR/D. For simplicity we will assume that the
magnetic permeability is constant throughout IR. Then, again assuming the time har-
monic form (> .) with ε and μ replaced by εand μ, respectively, the total fields
E = Ei + Es, H = Hi + Hs satisfy
curl E −ikH = ,
curl H + iknE = 
in IR
(.)


Inverse Scattering
and the scattered wave Es, Hs satisfies the Silver–Müller radiation condition, where the
wave number is given by k = √εμω and n = (ε + i σ/ω)/εis the refractive index.
Establishing uniqueness requires an electromagnetic analogue of the unique continuation
principle and existence can be based on an electromagnetic variant of the Lippman–
Schwinger equation [].
The scattering of time-harmonic electromagnetic waves by an infinitely long cylinder
with a simply connected cross section D leads to boundary value problems for the two-
dimensional Helmholtz equation in the exterior IR/D of D. If the electric field is polarized
parallel to the axis of the cylinder and if the axis of the cylinder is parallel to the xaxis,
then
E = (,,u),
H = 
ik ( ∂u
∂x
,−∂u
∂x
,)
satisfies the Maxwell equations if and only if u = u(x, x) satisfies the Helmholtz equation.
The homogeneous perfect conductor boundary condition is satisfied on the boundary
of the cylinder if the homogeneous Dirichlet boundary condition u = on ∂D is satis-
fied. If the magnetic field is polarized parallel to the axis of the cylinder, then the roles of
E and H have to be reversed, i.e.,
H = (,,u),
E = i
k ( ∂u
∂x
,−∂u
∂x
,),
and the perfect conductor boundary condition corresponds to the Neumann boundary
condition ∂u/∂= on ∂D with the unit normal
to the boundary ∂D of the cross section
D. Hence, the analysis of two-dimensional electromagnetic scattering problems coincides
with that of two-dimensional acoustic scattering problems.
..
Historical Remarks
> Equation (.) carries the name of Helmholtz (–) for his contributions to math-
ematical acoustics. The radiation condition (> .) was introduced by Sommerfeld in 
to characterize an outward energy flux. The expansion (> .) was first established by
Atkinson in and generalized by Wilcox in . The fundamental Lemma is due to
Rellich () and Vekua (). The combined single- and double-layer approach (> .)
for the existence analysis was introduced independently by Leis, Brakhage and Werner,
and Panich in the s in order to remedy the non-uniqueness deficiency of the classical
double-layer approach due to Vekua, Weyl, and Müller from the s. Huygens’ principle
is also referred to as the Huygens–Fresnel principle and named for Huygens (–)
and Fresnel (–) in recognition of their contributions to wave optics. The physical
optics approximation (> .) is named for Kirchhoff (–) for his contributions
to optics. The terms Lippmann–Schwinger equation and Born approximation are adopted
from quantum physics. The equation (> .) are named for Maxwell (–) for his
fundamental contributions to electromagnetic theory. The radiation conditions (> .)

Inverse Scattering 

and (> .) were independently introduced in the s by Silver and Müller. The inte-
gral representations (> .) and (> .) were first presented by Stratton and Chu in
. Extending the ideas of Leis, Brakhage and Werner, and Panich from acoustics to elec-
tromagnetics, the approach (> .) was introduced independently by Knauff and Kress,
Jones, and Mautz and Harrington in the s in order to remedy the non-uniqueness
deficiency of the classical approach due to Weyl and Müller from the s.
.
Uniqueness in Inverse Scattering
..
Scattering by an Obstacle
The first step in studying any inverse scattering problem is to establish a uniqueness result,
i.e., if a given set of data is known exactly, does this data uniquely determine the support
and/or the material properties of the scatterer? We will begin with the case of scattering by
an impenetrable obstacle and then proceed to the case of a penetrable obstacle.
From
> Sect. ..we recall that the direct obstacle scattering problem is to find u ∈
C(IR/D)∩C(IR/D) such that the total field u = ui +us satisfies the Helmholtz equation
Δu + ku = 
in IR/D
(.)
and the sound-soft boundary condition
u = 
on ∂D,
(.)
where ui(x) = eik x⋅d, ∣d∣= , and us is a radiating solution. We also recall from Theorem 
that us has the asymptotic behavior
us(x, d) = eik∣x∣
∣x∣{u∞(ˆx, d) + O ( 
∣x∣)},
∣x∣→∞,
(.)
uniformly for all directions ˆx = x/∣x∣, where u∞is the far field pattern of the scattered field
Es. By Green’s integral theorem and the far field representation (> .) it can be shown
that the far field pattern satisfies the reciprocity relation []
u∞(ˆx, d) = u∞(−d,−ˆx),
ˆx, d ∈S.
(.)
The inverse scattering problem we are concerned with is to determine D from a knowl-
edge of u∞(ˆx, d) for ˆx and d on the unit sphere Sand fixed wave number k. In particular,
for the acoustic scattering problem (> .) and (> .) we want to show that D is
uniquely determined by u∞(ˆx, d) for ˆx, d ∈S. We note that by the reciprocity relation
(> .) the far field pattern u∞is an analytic function of both ˆx and d and hence it
would suffice to consider u∞for ˆx and d restricted to a surface patch of the unit sphere S.
Theorem 
Assume that Dand Dare two obstacles such that the far field patterns cor-
responding to the exterior Dirichlet problem (> .) and (> .) for Dand Dcoincide
for all incident directions d. Then D= D.


Inverse Scattering
Proof
Let us
and us
be the scattered fields corresponding to Dand D, respectively.
By the analyticity of the scattered field as a function of x and Rellich’s Lemma , the scat-
tered fields satisfy us
(⋅, d) = us
(⋅, d) in the unbounded component G of the complement
of D∪Dfor all d ∈S. This in turn implies that the scattered fields corresponding to
Φ(⋅, z) as incident field and Dor Das the scattering obstacle satisfy us
(x, z) = us
(x, z)
for all x, z ∈G. Now assume that D≠D. Then, without loss of generality, there exists
x∗∈∂G such that x∗∈∂Dand x∗/∈D. Then setting zn := x∗+ 
n (x∗) we have that
limn→∞us
(x∗, zn) exists but limn→∞us
(x∗, zn) = ∞which is a contradiction and hence
D= D.
∎
An open problem is to determine if one incident plane wave at a fixed wave number k
is sufficient to uniquely determine the scatterer D. If it is known a priori that in addition to
the sound-soft boundary condition (> .) that D is contained in a ball of radius R such
that kR < ., then D is uniquely determined by its far field pattern for a single incident
direction d and fixed wave number k [] (see also []). D is also uniquely determined if
instead of assuming that D is contained in a ball of sufficiently small radius it is assumed
that D is close to a given obstacle []. It is also known that for a wide class of sound-
soft scatterers, a finite number of incident fields is sufficient to uniquely determine D [].
Finally, if it is assumed that D is polyhedral, then a single incident plane wave is sufficient
to uniquely determine D [, ].
We conclude this section on uniqueness results for the inverse scattering problem for an
obstacle by considering the scattering of electromagnetic waves by a perfectly conducting
obstacle D. From
> Sect. ..we recall that the direct obstacle scattering problem is to
find E, H ∈C(IR/D) ∩C(IR/D) such that the total field E = Ei + Es, H = Hi + Hs
satisfies the Maxwell equations
curl E −ikH = ,
curl H + ikE = 
in IR/D
(.)
and the perfect conductor boundary condition
× E = 
on ∂D,
(.)
where Ei, Hi is the plane wave given by (> .) and Es, Hs is a radiating solution. We
also recall from Theorem that us has the asymptotic behavior
Es(x, d, p) = eik∣x∣
∣x∣{E∞(ˆx, d, p) + O ( 
∣x∣)},
∣x∣→∞,
(.)
where E∞is the electric far field pattern of the scattered field Es.
The inverse scattering problem is to determine D from a knowledge of E∞(ˆx, d, p)
for ˆx and d on the unit sphere S, three linearly independent polarizations p, and fixed
wave number k. We note that E∞is an analytic function of ˆx and d and is linear with
respect to p. The following theorem can be proved using the same ideas as in the proof of
Theorem .

Inverse Scattering 

Theorem 
Assume that Dand Dare two perfect conductors such that for a fixed wave
number k the electric far field patterns for both scatterers coincide for all incident directions
d and three linearly independent polarizations p. Then D= D.
In the case when D consists of finitely many polyhedra, a single incident wave is
sufficient to uniquely determine D [].
..
Scattering by an Inhomogeneous Medium
We now return to scattering of acoustic waves, but instead of scattering by a sound-
soft obstacle, we consider scattering by an inhomogeneous medium where the governing
equation (see > Sect. ..) is
Δu + knu = 
in IR
(.)
for u = ui + us ∈C(IR), where n ∈C(IR) is the refractive index satisfying Re n > 
and Im n ≥, ui(x) = eik x⋅d and us is radiating. We let D denote the support of m :=
−n. By Theorem the scattered wave us again has the asymptotic behavior (> .).
The inverse scattering problem we are now concerned with is to determine the index of
refraction n (and hence D) from a knowledge of u∞(ˆx, d) for ˆx and d on the unit sphere
Sand fixed wave number k. In particular, we want to show that n is uniquely determined
from u∞(ˆx, d) for ˆx, d ∈Sand fixed wave number k.
Theorem 
The refractive index n in (> .) is uniquely determined by u∞(ˆx, d) for
ˆx, d ∈Sand a fixed value of the wave number k.
Proof
Let B be an open ball centered at the origin and containing the support of m = −n.
The first step in the proof is to construct a solution of (> .) in B of the form
w(x) = ei z⋅x (+ r(x)) ,
(.)
where z ⋅z = , z ∈Cand
∥r∥L(B) ≤
C
∣Re z∣
for some positive constant C and ∣Re z∣sufficiently large. This is done in [] by using
Fourier series. The second step is to show that, given two open balls Band Bcentered
at the origin and containing the support of m such that B⊂B, the set of solutions
{u(⋅, d) : d ∈S} satisfying (> .) is complete in
H := {w ∈C(B) : Δw + knw = 
in B}
with respect to the norm in L(B) []. Now assume that nand nare refractive indices
such that the corresponding far field patterns satisfy u,∞(⋅, d) = u,∞(⋅, d), d ∈S, and


Inverse Scattering
assume that the supports of −nand −nare contained in B. Then using Rellich’s
Lemma and Green’s integral theorem it can be shown that
∫B
u(⋅, ˜d)u(⋅, d)(n−n) dx = 
for all d, ˜d ∈Sand hence
∫B
ww(n−n) dx = 
(.)
for all solutions w,w∈C(B) of Δw+ knw = and Δw+ knw= in B. Now
choose z:= y + ρa + ib and z:= y −ρa −ib such that {y, a, b} is an orthogonal basis in
IRwith the properties that ∣a∣= and ∣b∣= ∣y∣+ ρand substitute these values of z into
(> .) arriving at functions wand w. Substitute these functions into (> .) and let
ρ →∞to arrive at
∫B
ei y⋅x (n(x) −n(x)) dx = 
for arbitrary y ∈IR, i.e., n(x) = n(x) for x ∈Bby the Fourier integral theorem.
∎
In the case of scattering by a sound-soft obstacle, the proof of uniqueness given in Theo-
rem remains valid in IR. However, this is not the case for scattering by an inhomogeneous
medium. Indeed, until recently, the question of whether or not Theorem remains valid
in IRwas one of the outstanding open problems in inverse scattering theory. The prob-
lem was finally resolved in by Bukhgeim [] who showed that in IRthe index of
refraction n is uniquely determined by u∞(ˆx, d) for ˆx, d ∈Sand a fixed value of the wave
number k.
We conclude this section with a few remarks on scattering by an anisotropic medium.
Let n be as above and recall that D is the support of m := −n. Let A be a × matrix-
valued function whose entries a jk are continuously differentiable functions in D such that
A is symmetric and satisfies
ξ ⋅(Im A)ξ ≤,
ξ ⋅(Re A)ξ > γ∣ξ∣
for all ξ ∈Cand x ∈D, where γ is a positive constant. We assume that A(x) = I for
x ∈IR/D. The anisotropic scattering problem is to find u = ui + us ∈H
loc(IR) such that
∇⋅A∇u + knu = 
in IR
(.)
in the weak sense where again ui(x) = eik x⋅d and us is radiating. The existence of a unique
solution to this scattering problem has been established by Hähner [].
The scattered field again has the asymptotics (> .). The inverse scattering problem
is now to determine D from a knowledge of the far field pattern u∞(ˆx, d) for ˆx, d ∈S.
We note that the matrix A is not uniquely determined by u∞and hence without further a
priori assumptions the determination of D is the most that can be hoped for [, ]. To
this end we have the following theorem due to Hähner [].
Theorem 
Assume γ > . Then D is uniquely determined by u∞(ˆx, d) for ˆx, d ∈S.

Inverse Scattering 

We note that Theorem remains valid if the condition on Re A is replaced by the
condition
ξ ⋅(Re A−)ξ ≥μ∣ξ∣
for all ξ ∈Cand x ∈D where μ is a positive constant such that μ > []. Note that the
isotropic case when A = I is handled by Theorem .
Uniqueness theorems for the Maxwell equations in an isotropic inhomogeneous
medium have been established by Colton and Päivärinta [] and Hähner []. The proof
is similar to that of Theorem for the scalar problem except that technical problems arise
due to the fact that we must now construct a solution E, H to the Maxwell equations in an
inhomogeneous isotropic medium such that E has the form
E(x) = eiz⋅x (η + r(x)),
where z, η ∈C, η ⋅z = and z ⋅z = k. In contrast to the case of acoustic waves, it is no
longer true that r(x) decays to zero as ∣Re z∣tends to infinity. Finally, the generalization of
Theorem to the case of the Maxwell equations in an anisotropic media has been done by
Cakoni and Colton [].
..
Historical Remarks
As previously mentioned, the first uniqueness theorem for the acoustic inverse obstacle
problem was given by Schiffer in for the case of a sound-soft obstacle [], whereas in
Nachman [], Novikov [] and Ramm [] established a uniqueness result for the
inverse scattering problem for an inhomogeneous medium. In Isakov [, ] proved
a series of uniqueness theorems for the transmission problem with discontinuities of u
across ∂D. His ideas were subsequently utilized by Kirsch, Kress and their co-workers to
establish uniqueness theorems for a variety of inverse scattering problems for both acoustic
and electromagnetic waves (for references see []). In particular, the proofs of Theorems 
and are based on the ideas of Kirsch and Kress [, ].
A global uniqueness theorem for the Maxwell equations in an isotropic inhomoge-
neous medium was first established in by Colton and Päivärinta [] (see also []).
The results of [, ] are for the case when the magnetic permeability μ is constant. For
uniqueness results in the case when μ is no longer constant we refer to [, ].
.
Iterative and Decomposition Methods in Inverse
Scattering
..
Newton Iterations in Inverse Obstacle Scattering
We now turn to reconstruction methods for the inverse scattering problem for sound-soft
scatterers and as a first group we describe iterative methods. Here the inverse problem is
interpreted as a nonlinear ill-posed operator equation which is solved by iteration methods


Inverse Scattering
such as regularized Newton methods, Landweber iterations, or conjugate gradient meth-
ods. For a fixed incident field ui, the solution to the direct scattering problem defines the
boundary to far field operator F : ∂D ↦u∞which maps the boundary ∂D of the scatterer
D onto the far field pattern u∞of the scattered wave us. In particular, F is the imaging
operator that takes the scattering object D into its image u∞via the scattering process.
In terms of this imaging operator, i.e., the boundary to far field operator, given a far field
pattern u∞, the inverse problem consists in solving the operator equation
F(∂D) = u∞
(.)
for the unknown boundary ∂D. As opposed to the direct obstacle scattering problem which
is linear and well-posed, the operator equation (> .), i.e., the inverse obstacle scattering
problem, is nonlinear and ill-posed. It is nonlinear since the solution to the direct scatter-
ing problem depends nonlinearly on the boundary and it is ill-posed because the far field
mapping is extremely smoothing due to the analyticity of the far field pattern.
In order to define the operator F properly, the most appropriate approach is to choose
a fixed reference domain D and consider a family of scatterers Dh with boundaries rep-
resented in the form ∂Dh = {x + h(x) : x ∈∂D}, where h : ∂D →IRis of class C
and is sufficiently small in the Cnorm on ∂D. Then we may consider the operator F as a
mapping from a ball
V := {h ∈C(∂D) : ∥h∥C< a} ⊂C(∂D)
with sufficiently small radius a > into L(S). However, for ease of presentation, we
proceed differently and restrict ourselves to boundaries ∂D that can be parameterized by
mapping them globally onto the unit sphere S, i.e.,
∂D = {p(ˆx) : ˆx ∈S}
(.)
for some injective Cfunction p : S→IR. As a simple example, the reader should
consider the case of star-like domains where
p(ˆx) = r(ˆx)ˆx,
ˆx ∈S,
(.)
with a radial distance function r : S→(,∞). Then, with some appropriate subspace
W ⊂C(S), we may interpret the operator F as a mapping
F : W →L(S),
F : p ↦u∞,
and consequently the inverse obstacle scattering problem consists in solving
F(p) = u∞
(.)
for the unknown function p.
Since F is nonlinear we may linearize
F(p + q) = F(p) + F′(p)q + O (q)

Inverse Scattering 

in terms of a Fréchet derivative F′. Then, given an approximation p for the solution of
(> .), in order to obtain an update p + q, we solve the approximate linear equation
F(p) + F′(p)q = u∞
(.)
for q. We note that the linearized equation inherits the ill-posedness of the nonlinear
equation and therefore regularization is required. As in the classical Newton iterations,
this linearization procedure is iterated until some stopping criteria is satisfied.
In principle the parameterization of the update ∂Dp+q = {p(ˆx) + q(ˆx) : ˆx ∈S} is not
unique. To cope with this ambiguity the simplest possibility is to allow only perturbations
of the form
q(ˆx) = z(ˆx) (p(ˆx)),
x ∈S,
(.)
with a scalar function z. We denote the corresponding linear space of normal Lvector
fields by L
normal(S).
The Fréchet differentiability of the operator F is addressed in the following theorem.
Theorem 
The boundary to far field mapping F : p ↦u∞is Fréchet differentiable. The
derivative is given by
F′(p)q = vq,∞,
where vq,∞is the far field pattern of the radiating solution vq to Helmholtz equation in IR/D
satisfying the Dirichlet boundary condition
vq = −⋅q ∂u
∂
on ∂D
(.)
in terms of the total field u = ui + us.
The boundary condition (> .) for the derivative can be obtained formally by using
the chain rule to differentiate the boundary condition u = on ∂D with respect to the
boundary. Extensions of Theorem to the Neumann boundary condition, the perfect
conductor boundary condition, and to the impedance boundary condition in acoustics
and electromagnetics are also available.
To justify the application of regularization methods for stabilizing (> .), injectivity
and dense range of the operator F′(p) : L
normal(S) →L(S) need to be established.
This is settled for the Dirichlet condition and, for λ sufficiently large, for the impedance
boundary condition and remains an open problem for the Neumann boundary condition.
In the classical Tikhonov regularization, (> .) is replaced by
αq + [F′(p)]
∗F′(p)q = [F′(p)]
∗{u∞−F(p)}
(.)
with some positive regularization parameter α and the Ladjoint [F′(p)]∗of F′(p). For
details on the numerical implementation we refer to [] and the references therein. The
numerical examples strongly indicate that it is advantageous to use some Sobolev norm
instead of the Lnorm as the penalty term in the Tikhonov regularization. Numerical


Inverse Scattering
examples in three dimensions have been reported by Farhat et al. [] and by Harbrecht
and Hohage [].
In closing this section on Newton iterations we note as their main advantages that this
approach is conceptually simple and, as the numerical examples in the literature indicate,
leads to highly accurate reconstructions with reasonable stability against errors in the far
field pattern. On the other hand, it should be noted that for the numerical implementation
an efficient forward solver is needed and good a priori information is required in order to
ensure convergence. On the theoretical side the convergence of regularized Newton iter-
ations for inverse obstacle scattering problems has not been completely settled, although
some progress has been made through the work of Hohage [] and Potthast [].
Newton type iterations can also be employed for the simultaneous determination of
the boundary shape and the impedance function λ in the impedance boundary condition
(> .) [].
..
Decomposition Methods
The main idea of decomposition methods is to break up the inverse obstacle scattering
problem into two parts: the first part deals with the ill-posedness by constructing the
scattered wave us from its far field pattern u∞and the second part deals with the non-
linearity by determining the unknown boundary ∂D of the scatterer as the location where
the boundary condition for the total field ui +us is satisfied in a least-squares sense. In the
potential method, for the first part, enough a priori information on the unknown scatterer
D is assumed so one can place a closed surface Γ inside D. Then the scattered field us is
sought as a single-layer potential
us(x) = ∫Γ φ(y)Φ(x, y) ds(y),
x ∈IR/D,
(.)
with an unknown density φ ∈L(Γ). In this case the far field pattern u∞has the
representation
u∞(ˆx) = 
π ∫Γ e−ik ˆx⋅yφ(y) ds(y),
ˆx ∈S.
Given the far field pattern u∞, the density φ is now found by solving the integral equation
of the first kind
S∞φ = u∞
(.)
with the compact integral operator S∞: L(Γ) →L(S) given by
(S∞φ)(ˆx) := 
π ∫Γ e−ik ˆx⋅yφ(y) ds(y),
ˆx ∈S.
Due to the analytic kernel of S∞, the integral equation (> .) is severely ill-posed. For
a stable numerical solution of (> .) Tikhonov regularization can be applied, i.e., the
ill-posed equation (> .) is replaced by
αφα + S∗
∞S∞φα = S∗
∞u∞
(.)
with some positive regularization parameter α and the adjoint S∗
∞of S∞.

Inverse Scattering 

Given an approximation of the scattered wave us
α obtained by inserting a solution φα
of (> .) into the potential (> .), the unknown boundary ∂D is then determined
by requiring the sound-soft boundary condition
ui + us = 
on ∂D
(.)
to be satisfied in a least-squares sense, i.e., by minimizing the Lnorm of the defect
∥ui + us
α∥

L(Λ)
(.)
over a suitable set of admissible surfaces Λ. Instead of solving this minimization problem
one can also visualize ∂D by color coding the values of the modulus ∣u∣of the total field
u ≈ui + us
α on a sufficiently fine grid over some domain containing the scatterer.
Clearly we can expect (> .) to have a solution φ ∈L(Γ) if and only if u∞is the far
field of a radiating solution to the Helmholtz equation in the exterior of Γ with sufficiently
smooth boundary values on Γ. Hence, the solvability of (> .) is related to the regularity
properties of the scattered wave which in general cannot be known in advance for the
unknown scatterer D. Nevertheless, it is possible to provide a solid theoretical foundation
to the above procedure [, ]. This is achieved by combining the minimization of the
Tikhonov functional for (> .) and the defect minimization for (> .) into one cost
functional
∥S∞φ −u∞∥
L(S) + α∥φ∥
L(Γ) + γ ∥ui + us
α∥

L(Λ).
(.)
Here γ > denotes a coupling parameter which has to be chosen appropriately for the
numerical implementation in order to make the two terms in (> .) are of the same
magnitude, for example γ = ∥u∞∥L(S)/∥ui∥∞.
Note that the potential approach can also be employed for the inverse problem to
recover the impedance given the shape of the scatterer. In this case the far field equation
(> .) is solved with Γ replaced by the known boundary ∂D. After the density φ is
obtained, λ can be determined in a least-squares sense from the impedance boundary
condition (> .) after evaluating the trace and the normal derivative of the single-layer
potential (> .) on ∂D.
The point source method of Potthast [] can also be interpreted as a decomposition
method. Its motivation is based on Huygens’ principle from Theorem , i.e., the scattered
field representation
us(x) = −∫∂D
∂u
∂
(y) Φ(x, y) ds(y),
x ∈IR/D,
(.)
and the far field representation
u∞(ˆx) = −
π ∫∂D
∂u
∂
(y) e−ik ˆx⋅y ds(y),
ˆx ∈S.
(.)
For z ∈IR/D we choose a domain Bz such that z /∈Bz and D ⊂Bz, and approximate the
point source Φ(⋅, z) by a Herglotz wave function, i.e., a superposition of plane waves such
that
Φ(y, z) ≈∫Seik y⋅d gz(d) ds(d),
y ∈Bz,
(.)


Inverse Scattering
for some gz ∈L(S). Under the assumption that there does not exist a nontrivial solu-
tion to the Helmholtz equation in Bz with homogeneous Dirichlet boundary condition on
∂Bz, the Herglotz wave functions are dense in H/(∂Bz) [, ], and consequently the
approximation (> .) can be achieved uniformly with respect to y on compact subsets
of Bz. We can now insert (> .) into (> .) and use (> .) to obtain
us(z) ≈π ∫Sgz(ˆx)u∞(−ˆx) ds(ˆx)
(.)
as an approximation for the scattered wave us. Knowing an approximation for the scattered
wave the boundary ∂D can be found as above from the boundary condition (> .).
The approximation (> .) can in practice be obtained by solving the ill-posed linear
integral equation
∫Seik y⋅d gz(d) ds(d) = Φ(y, z),
y ∈∂Bz,
(.)
via Tikhonov regularization and the Morozov discrepancy principle. Note that although
the integral equation (> .) is in general not solvable, the approximation property
(> .) is ensured through the above denseness result on Herglotz wave functions.
An advantage of decomposition methods is that the separation of the ill-posedness and
the nonlinearity is conceptually straightforward. A second and main advantage consists in
the fact that their numerical implementation does not require a forward solver. As a dis-
advantage, as in the Newton method of the previous section, if we go beyond visualization
of the level surfaces of ∣u∣and proceed with the minimization, good a priori information
on the unknown scatterer is needed for the iterative solution of the optimization problem.
The accuracy of the reconstructions using decomposition methods is slightly inferior to
that using Newton iterations.
..
Iterative Methods Based on Huygens’Principle
We recall Huygens’ principle (> .) and (> .). In view of the sound-soft boundary
condition, from (> .), we conclude that
ui(x) = ∫∂D
∂u
∂
(y) Φ(x, y) ds(y),
x ∈∂D.
(.)
Now we can interpret (> .) and (> .) as a system of two integral equations for the
unknown boundary ∂D of the scatterer and the induced surface flux
φ := −∂u
∂
on ∂D.
It is convenient to call (> .) the data equation since it contains the given far field for
the inverse problem and (> .) the field equation since it represents the boundary con-
dition. Both equations are linear with respect to the flux and nonlinear with respect to
the boundary. Equation (> .) is severely ill-posed whereas (> .) is only mildly
ill-posed.

Inverse Scattering 

Obviously there are three options for an iterative solution of (> .) and (> .). In
a first method, given an approximation for the boundary ∂D one can solve the mildly ill-
posed integral equation of the first kind (> .) for φ. Then, keeping φ fixed, >Eq. (.)
is linearized with respect to ∂D to update the boundary approximation. This approach
has been proposed by Johansson and Sleeman []. In a second approach, following ideas
first developed for the Laplace equation by Kress and Rundell [], one also can solve the
system (> .) and (> .) simultaneously for ∂D and φ by Newton iterations, i.e., by
linearizing both equations with respect to both unknowns. This idea has been analyzed by
Ivanyshyn and Kress [, ]. Whereas in the first method the burden of the ill-posedness
and nonlinearity is put on one equation, in a third method a more even distribution of
the difficulties is obtained by reversing the roles of (> .) and (> .), i.e., by solving
the severely ill-posed equation (> .) for φ and then linearizing (> .) to obtain the
boundary update. With a slight modification this approach may also be interpreted as a
decomposition method since to some extent it separates the ill-posedness and the non-
linearity. It combines the decomposition method from the previous
> Sect. ..with
elements of Newton iterations from
> Sect. ... Therefore it has also been termed as a
hybrid method and as such was analyzed by Kress and Serranho [, ].
For a more detailed description of these three methods, using the parameterization
(> .), we introduce the parameterized single-layer operator and far field operator
A, A∞: C(S) × L(S) →L(S) by
A(p,ψ)(ˆx) := ∫SΦ(p(ˆx), p(ˆy))ψ(ˆy) ds(ˆy),
ˆx ∈S,
and
A∞(p,ψ)(ˆx) := 
π ∫Se−ik ˆx⋅p( ˆy)ψ(ˆy) ds(ˆy),
ˆx ∈S.
Then (> .) and (> .) can be written in the operator form
A∞(p,ψ) = u∞
(.)
and
A(p,ψ) = −ui ○p,
(.)
where we have incorporated the surface element into the density function via
ψ(ˆx) := J(ˆx) φ(p(ˆx))
(.)
with the Jacobian J of the mapping p. The linearization of these equations requires the
Fréchet derivatives of the operators A and A∞with respect to p. These can be obtained by
formally differentiating their kernels with respect to p, i.e.,
(A′(p,ψ)q)(ˆx) = ∫Sgradx Φ(p(ˆx), p(ˆy)) ⋅[q(ˆx) −q(ˆy)]ψ(ˆy) ds(ˆy),
x ∈S,
and
(A′
∞(p,ψ)q)(ˆx) = −ik
π ∫Se−ik ˆx⋅p( ˆy) ˆx ⋅q(ˆy)ψ(ˆy) ds(ˆy),
x ∈S.


Inverse Scattering
For fixed p, provided kis not a Dirichlet eigenvalue of the negative Laplacian in D,
both in a Hölder space setting A(p,⋅) : C,α(S) →C,α(S) or in a Sobolev space set-
ting A(p,⋅) : H−/(S) →H/(S), the operator A(p,⋅) is a homeomorphism [].
In this case, given an approximation to the boundary parameterization p, the field equa-
tion (> .) can be solved for the density ψ. Then, keeping ψ fixed, linearizing the data
equation (> .) with respect to p leads to the linear equation
A′
∞(p,[A(p,⋅)]−(ui ○p)
,-------------------------------------------------.------------------------------------------------/
−ψ
)q = −u∞−A∞(p,[A(p,⋅)]−(ui ○p)
,-------------------------------------------------.------------------------------------------------/
−ψ
)
(.)
for q to update the parameterization p via p + q. This procedure can be iterated.
For fixed p the operator A′
∞(p,[A(p,⋅)]−(ui ○p)) has a smooth kernel and therefore
is severely ill-posed. This requires stabilization, for example, via Tikhonov regularization.
The following theorem ensures injectivity and dense range as prerequesits for Tikhonov
regularization. We recall the form (> .) introduced for uniqueness of the parameter-
ization of the update and the corresponding linear space L
normal(S) of normal Lvector
fields.
Theorem 
Assume that kis not a Neumann eigenvalue of the negative Laplacian in D.
Then the operator
A′
∞(p,[A(p,⋅)]−(ui ○p)) : L
normal(S) →L(S)
is injective and has dense range.
One can relate this approach to the Newton iterations for the nonlinear equation
(> .) for the boundary to far field operator of
> Sect. ... In the case when k
is not a Dirichlet eigenvalue of the negative Laplacian in D one can write
F(p) = −A∞(p,[A(p,⋅)]−(ui ○p)).
By the product and chain rule this implies
F′(p)q = −A′
∞(p,[A(p,⋅)]−(ui ○p)) q
+A∞(p,[A(p,⋅)]−A′(p,[A(p,⋅)]−(ui ○p))q)
−A∞(p,[A(p,⋅)]−((grad ui) ○p) ⋅q).
(.)
Hence, we observe a relation between the above iterative scheme and the Newton iterations
for the boundary to far field map as expressed by the following theorem.
Theorem 
The iteration scheme given by (> .) can be interpreted as Newton
iterations for (> .) with the derivative of F approximated by the first term in the
representation (> .).

Inverse Scattering 

As to be expected from this close relation to Newton iterations for (> .), the quality
of the reconstructions via (> .) can compete with those of Newton iterations with the
benefit of reduced computational costs.
The second approach for iteratively solving the system (> .) and (> .) consists
in simultaneously linearizing both equations with respect to both unknowns. In this case,
given approximations p and ψ both for the boundary parameterization and the density, the
system of linear equations
A′
∞(p,ψ)q + A∞(p, χ) = −A∞(p,ψ) + u∞
(.)
and
A′(p,ψ)q + ((grad ui) ○p) ⋅q + A(p, χ) = −A(p,ψ) −ui ○p
(.)
has to be solved for q and χ in order to obtain updates p+q for the boundary parameteriza-
tion and ψ + χ for the density. This procedure again is iterated and coincides with Newton’s
method for the system (> .) and (> .).
For uniqueness reasons the updates must be restricted, for example, to normal fields of
the form (> .). Due to the smoothness of the kernels both > Eqs. (.) and (> .)
are severely ill-posed and require regularization with respect to both unknowns. In partic-
ular for the parameterization update it is appropriate to incorporate penalties for Sobolev
norms of q to guarantee smoothness of the boundary whereas for the density Lpenalty
terms on χ are sufficient.
The simultaneous iterations (> .) and (> .) again exhibit connections to the
Newton iteration for (> .).
Theorem 
Assume that kis not a Dirichlet eigenvalue of the negative Laplacian in D
and set ψ := −[A(p,⋅)]−(ui ○p). If q satisfies the linearized boundary to far field equation
(> .), then q and
χ := −[A(p,⋅)]−(A′(p,ψ)q + ((gradui) ○p) ⋅q)
satisfy the linearized data and field equations (> .) and (> .). Conversely, if q and
χ satisfy (> .) and (> .), then q satisfies (> .).
Theorem illustrates the difference between the iteration method based on (> .)
and (> .) and the Newton iterations for (> .). In general when performing
(> .) and (> .) in the sequence of updates the relation A(p,ψ) = −(ui○p) between
the approximations p and ψ for the parameterization and the density will not be satisfied.
This observation also indicates a possibility to use (> .) and (> .) for implement-
ing a Newton scheme for (> .). It is only necessary to replace the update ψ + χ for the
density by −[A(p+q,⋅)]−(ui ○(p+q)), i.e., at the expense of throwing away χ and solving
a boundary integral equation for a new density. For a numerical implementation and three
dimensional examples we refer to [].


Inverse Scattering
In a third method, in order to evenly distribute the burden of the ill-posedness and the
nonlinearity of the inverse obstacle scattering problem, instead of solving the field equation
(> .) for the density and then linearizing the data equation, one can also solve the
severely ill-posed data equation (> .) for the density and then linearize the mildly ill-
posed field equation (> .) to update the boundary. In this case, given an approximation
for the boundary parameterization p, first the data equation (> .) is solved for the
density ψ. Then, keeping ψ fixed, the field equation (> .) is linearized to obtain the
linear equation
A′(p,ψ) q + ((gradui) ○p) ⋅q = −A(p,ψ) −ui ○p
(.)
for q to update the parameterization p via p + q. This procedure of alternatingly solving
(> .) and (> .) can be iterated. To some extent this procedure mimics a decompo-
sition method in the sense that it decomposes the inverse problem into a severely ill-posed
linear problem and a nonlinear problem.
The hybrid method suggested by Kress and Serranho [, ] can be considered as a
slight modification of the above procedure. In this method, given an approximation p for
the parameterization of the boundary, the data equation (> .) is solved for the density ψ
via regularization. Injectivity and dense range of the operator A∞(p,⋅) : L(S) →L(S)
are guaranteed provided kis not a Dirichlet eigenvalue for the negative Laplacian in
D []. Then one can define the single-layer potential
us(x) = ∫SΦ(x, p(ˆy))ψ(ˆy) ds(ˆy)
and evaluate the boundary values of u := ui + us and its derivatives on the surface rep-
resented by p via the jump relations. Finally an update p + q is found by linearizing the
boundary condition u ○(p + q) = , i.e., by solving the linear equation
u ○p + ((grad u) ○p) ⋅q = 
(.)
for q. For uniqueness of the update representation the simplest possibility is to allow only
perturbations of the form (> .). Then injectivity for the linear equation (> .) can
be established for the exact boundary.
After introducing the operator
( ̃A(p,ψ) q)(ˆx) := ∫Sgradx Φ(p(ˆx), p(ˆy))⋅q(ˆx)ψ(ˆy) ds(ˆy)−

ψ(ˆx)[ (p(ˆx)) ⋅q(ˆx)]
J(ˆx)
and observing the jump relations for the single-layer potential and (> .), the
> Eq. (.) can be rewritten as
̃A(p,ψ) q + ((gradui) ○p) ⋅q = −A(p,ψ) −ui ○p.
(.)
Comparing this with (> .) we discover a relation between solving the data and
field equation iteratively via (> .) and (> .) and the hybrid method of Kress and
Serranho. In the hybrid method the Fréchet derivative of A with respect to p is replaced
by the operator ̃A where one linearizes only with respect to the evaluation surface for the

Inverse Scattering 

single-layer potential but not with respect to the integration surface. For the numerical
implementation of the hybrid method and numerical examples in three dimensions we
refer to [].
All three methods of this section can be applied to the Neumann boundary condition,
the perfect conductor boundary condition, and to the impedance boundary condition in
acoustics and electromagnetics. They also can be employed for the simultaneous recon-
struction of the boundary shape and the impedance function λ in the impedance boundary
condition (> .) [].
..
Newton Iterations for the Inverse Medium Problem
Analogously to the inverse obstacle scattering problem, we can reformulate the inverse
medium problem as a nonlinear operator equation. To this end we define the far field
operator F : m ↦u∞that maps m := −n to the far field pattern u∞for plane wave
incidence ui(x) = eik x⋅d. Since by Theorem we know that m is uniquely determined by
a knowledge of u∞(ˆx, d) for all incident and observation directions ˆx, d ∈S, we inter-
pret F as an operator from C(B) into L(S× S) for a ball B that contains the unknown
support of m.
In view of the Lippmann–Schwinger equation (> .) and the far field representation
(> .), we can write
(F(m))(ˆx, d) = −k
π ∫B e−ik ˆx⋅ym(y)u(y, d) dy,
ˆx, d ∈S,
(.)
where u(⋅, d) is the unique solution of
u(x, d) + k∫B Φ(x, y)m(y)u(y, d) dy = ui(x, d),
x ∈B.
(.)
From (> .) it can be seen that the Fréchet derivative vq of u with respect to m (in
direction q) satisfies the Lippmann–Schwinger equation
vq(x, d) + k∫B Φ(x, y)[m(y)vq(y, d) + q(y)u(y, d)] dy = ,
x ∈B.
(.)
From this and (> .) it follows that the Fréchet derivative of F is given by
(F′(m) q)(ˆx, d) = −k
π ∫B e−ik ˆx⋅y[m(y)vq(y, d) + q(y)u(y, d)] dy,
ˆx, d ∈S,
which coincides with the far field pattern of the solution vq(⋅, d) of (> .). Hence, we
have proven the following theorem.
Theorem 
The far field mapping F : m ↦u∞is Fréchet differentiable. The derivative is
given by
F′(m)q = vq,∞,


Inverse Scattering
where vq,∞is the far field pattern of the radiating solution vq to
Δv + knv = −kuq
in IR.
(.)
This characterization of the Fréchet derivative can be used to establish injectivity of
F′(m). We now have all the prerequisites available for a regularized Newton iteration
analogous to (> .).
A similar approach as that given above is also possible for the electromagnetic inverse
medium problem.
..
Least Squares Methods for the Inverse Medium
Problem
In view of the Lippmann–Schwinger equation (> .) and the far field representation
(> .), the inverse medium problem is equivalent to solving the system consisting of
the field equation
u(x, d) + k∫B Φ(x, y)m(y)u(y, d) dy = ui(x, d),
x ∈B, d ∈S,
(.)
and the data equation
−k
π ∫B e−ik ˆx⋅ym(y)u(y, d) dy = u∞(ˆx, d),
ˆx, d ∈S,
(.)
where B is a ball containing the support of m. In principle one can first solve the ill-
posed linear equation (> .) to determine the source mu from the far field pattern
and then solve the nonlinear equation (> .) to construct the contrast m. After defin-
ing the volume potential operator T : L(B × S) →L(B × S) and the far field operator
F : L(B × S) →L(S× S) by
(Tv)(x, d) := −k∫B Φ(x, y)v(y, d) dy,
x ∈B, d ∈S,
and
(Fv)(ˆx, d) := −k
π ∫B e−ik ˆx⋅dv(y, d) dy,
ˆx, d ∈S,
we rewrite the field equation (> .) as
ui + Tmu = u
(.)
and the data equation (> .) as
Fmu = u∞.
(.)
We can now define the cost function
μ(m,u) :=
∥ui + Tmu −u∥
L(B×S)
∥ui∥
L(B×S)
+
∥u∞−Fmu∥
L(S×S)
∥u∞∥
L(S×S)
(.)

Inverse Scattering 

and reformulate the inverse medium problem as the optimization problem to minimize μ
over the contrast m ∈V and the fields u ∈W where V and W are appropriately chosen
admissible sets. The weights in the cost function are chosen such that the two terms are of
the same magnitude.
This optimization problem is similar in structure to that used in (> .) in connec-
tion with the decomposition method for the inverse obstacle scattering problem. However,
since by Theorem all incident directions are required, the discrete versions of the opti-
mization problem suffer from a large number of unknowns. Analogous to the two step
approaches of > Sects. ..and > ..for the inverse obstacle scattering problem, one
way to reduce the computational complexity is to treat the fields and the contrast separately,
for example, by a modified conjugate gradient method as proposed by Kleinman and van
den Berg []. In a modified version of this approach, van den Berg and Kleinman []
transformed the Lippmann–Schwinger equation (> .) into the equation
mui + mTw = w
(.)
for the contrast sources w := mu, and instead of simultaneously updating the contrast
m and the fields u, the contrast is updated together with the contrast source w. The cost
function (> .) is now changed to
μ(m,w) :=
∥mui + mTw −w∥
L(B×S)
∥ui∥
L(B×S)
+
∥u∞−Fmu∥
L(S×S)
∥u∞∥
L(S×S)
.
The above approach for the acoustic inverse medium problem can be adapted to the case
of electromagnetic waves.
..
Born Approximation
The Born approximation turns the inverse medium scattering problem into a linear
problem and therefore is often employed in practical applications. In view of (> .),
for plane wave incidence we have the linear integral equation
−k
π ∫IRe−ik (ˆx−d)⋅ym(y) dy = u∞(ˆx, d)
ˆx, d ∈S.
(.)
Solving (> .) for the unknown m corresponds to inverting the Fourier transform
of m restricted to the ball of radius k centered at the origin, i.e., only incomplete data
is available. This causes uniqueness ambiguities and leads to severe ill-posedness of the
inversion. Thus, the ill-posedness which seemed to have disappeared through the inversion
of the Fourier transform is back on stage. For details we refer to [].
A counterpart of the Born approximation in inverse obstacle scattering starts from the
far field of the physical optics approximation (> .) for a convex sound-soft scatterer D
in the back scattering direction, i.e.,
u∞(−d; d) = −
π ∫(y)⋅d<
∂
∂(y) eik d⋅y ds(y).


Inverse Scattering
Analogously, replacing d by −d, we have
u∞(d;−d) = −
π ∫(y)⋅d>
∂
∂(y) e−ik d⋅y ds(y).
Combining the last two equations and using Green’s integral theorem we find
∫IRχ(y)eik d⋅y dy = π
k{u∞(−d; d) + u∞(d;−d)},
d ∈S,
(.)
with the characteristic function χ of the scatterer D. > Equation (.) is known as the
Bojarski identity. Hence, in the physical optics approximation, the Fourier transform has
again to be inverted from incomplete data since the physical optics approximation is valid
only for large wave numbers k. For details we refer to [].
..
Historical Remarks
The boundary condition (> .) was obtained by Roger [] who first employed Newton
type iterations for the approximate solution of inverse obstacle scattering problems.
Rigorous foundations for the Fréchet differentiability were given by Kirsch [] in the
sense of a domain derivative via variational methods and by Potthast [] via bound-
ary integral equation techniques. The potential method as a prototype of decomposition
methods has been proposed by Kirsch and Kress []. The point source method has been
suggested by Potthast []. The iterative methods based on Huygens’ principle were intro-
duced by Johansson and Sleeman [], by Ivanyshyn and Kress [] (extending a method
proposed by Kress and Rundel [] from potential theory to acoustics), and by Kress []
and Serranho []. The methods described in > Sects. ..–..have been investigated
by numerous researchers over the past years.
.
Qualitative Methods in Inverse Scattering
..
The Far Field Operator and Its Properties
A different approach to solving inverse scattering problems than the use of iterative meth-
ods is the use of qualitative methods []. These methods have the advantage of requiring
less a priori information than iterative methods (e.g., it is not necessary to know the topol-
ogy of the scatterer or the boundary conditions satisfied by the total field) and in addition
reduces a nonlinear problem to a linear problem. On the other hand, the implementation
of such methods often requires more data than iterative methods do and in the case of
a penetrable inhomogeneous medium only recovers the support of the scatterer together
with some estimates on its material properties.

Inverse Scattering 

We begin by considering the scattering problem for a sound-soft obstacle (> .) and
(> .). The far field operator F : L(S) →L(S) for this problem is defined by
(Fg)(ˆx) := ∫Su∞(ˆx, d)g(d) ds(d),
ˆx ∈S,
(.)
where u∞is the far field pattern associated with (> .) and (> .). By superposition,
Fg is seen to be the far field pattern corresponding to the Herglotz wave function
vg(x) := ∫Seikx⋅d g(d) ds(d),
x ∈IR,
(.)
as incident field. The function g ∈L(S) is known as the kernel of the Herglotz wave
function. The far field operator F is compact. It can also be shown that for the case of
scattering by a sound-soft obstacle, the far field operator is normal []. Of basic importance
to us is the following theorem [].
Theorem 
The far field operator F corresponding to (> .) and (> .) is injective
with dense range if and only if there does not exist a Dirichlet eigenfunction for D which is a
Herglotz wave function.
Proof
The proof is based on the reciprocity relation (> .). In particular, for the L
adjoint F∗: L(S) →L(S), the reciprocity relation implies that
F∗g = RFRg,
(.)
where R : L(S) →L(S) is defined by (Rg)(d) := g(−d). Hence, the operator F is
injective if and only if its adjoint F∗is injective. Recalling that the denseness of the range
of F is equivalent to the injectivity of F∗, by (> .) we need only to show the injectivity
of F. To this end, we note that Fg = is equivalent to the existence of a Herglotz wave
function vg with kernel g for which the far field pattern of the corresponding scattered
field vs is v∞= . By Rellich’s lemma this implies that vs = in IR/D and the boundary
condition vg + vs = on ∂D now shows that vg = on ∂D. Since by hypothesis vg is not a
Dirichlet eigenfunction, we can conclude that vg = in D and hence g = .
∎
We will now turn our attention to the far field operator associated with the inhomo-
geneous medium problems (> .) and (> .). In both cases we again define the
far field operator by (> .) where u∞is now the far field pattern corresponding to
(> .) or (> .). We first consider
> Eq. (.) which corresponds to scattering
by an inhomogeneous medium. The analogue of Theorem is the following [].
Theorem 
The far field operator F corresponding to (> .) is injective with dense
range if and only if there does not exist a solution v,w ∈L(D),v −w ∈H(D) of the


Inverse Scattering
interior transmission problem
Δv + kv = 
in D
(.)
Δw + knw = 
in D
(.)
v = w
on ∂D
(.)
∂v
∂
= ∂w
∂
on ∂D
(.)
such that v is a Herglotz wave function. Values of k > for which there exists a nontrivial
solution of (> .)–(> .) are called transmission eigenvalues.
A similar theorem holds for
> Eq. (.) which corresponds to scattering by an aniso-
tropic medium where now (> .) is replaced by
∇⋅A∇w + knw = 
in D
(.)
and in (> .) the normal derivative ∂w
∂
is replaced by
⋅A∇w. If the coefficients in
(> .) or (> .) are real valued, then the far field operator is normal.
In the case of electromagnetic waves, the far field operator becomes
(Fg)(ˆx) := ∫SE∞(ˆx, d, g(d)) ds(d),
ˆx ∈S,
(.)
where now g ∈L
t(S), the space of square integrable tangential vector fields defined on
S, and E∞is the electric far field pattern defined by (> .). Theorems analogous to
Theorems and are also valid in this case [].
..
The Linear Sampling Method
The linear sampling method is a non-iterative method for solving the inverse scattering
problem that was first introduced by Colton and Kirsch [] and Colton et al. []. To
describe this method we first consider the case of scattering by a sound-soft obstacle, i.e.,
(> .) and (> .), and assume that for every z ∈D there exists a solution g = g(⋅, z) ∈
L(S) to the far field equation
Fg = Φ∞(⋅, z),
(.)
where
Φ∞(ˆx, z) = 
π e−ik ˆx⋅z,
ˆx ∈S.
Since the right hand side of (> .) is the far field pattern of the fundamental solution
(> .), it follows from Rellich’s lemma that
∫Sus(x, d)g(d) ds(d) = Φ(x, z)
for x ∈IR/D. From the boundary condition u = on ∂D we see that
vg(x) + Φ(x, z) = 
for x ∈∂D,
(.)

Inverse Scattering 

where vg is the Herglotz wave function with kernel g. We can now conclude from (> .)
that vg becomes unbounded as z →x ∈∂D and hence
lim
z→∂D
z∈D
∥g(⋅, z)∥L(S) = ∞,
i.e., ∂D is characterized by points z where the solution of (> .) becomes unbounded.
Unfortunately, in general the far field equation (> .) does not have a solution nor
does the above analysis say anything about what happens when z ∈IR/D. To address these
issues we first define the single-layer operator S : H−/(∂D) →H/(∂D) by
(Sφ)(x) := ∫∂D φ(y)Φ(x, y) ds(y),
x ∈∂D,
define the Herglotz operator H : L(∂D) →H−/(∂D) as the operator mapping g to the
trace of the Herglotz wave function (> .) on ∂D and let F : H−/(∂D) →L(S) be
defined by
(Fφ)(ˆx) := ∫∂D φ(y)e−ik ˆx⋅y ds(y),
ˆx ∈S.
Then, using on the one hand the fact that Herglotz wave functions are dense in the space
of solutions to the Helmholtz equation in D with respect to the norm in the Sobolev space
H(D) and on the other the factorization of the far field operator F as
F = −
π FS−H,
one can prove the following result [, ].
Theorem 
Assume that kis not a Dirichlet eigenvalue of the negative Laplacian for D
and let F be the far field operator corresponding to (> .) and (> .). Then
. For z ∈D and a given є > there exists gz,є ∈L(S) such that
∥Fgz,є −Φ∞(⋅, z)∥L(S) < є
and the corresponding Herglotz wave function vgz,є converges to a solution of
Δu + ku = 
in D
u = −Φ(⋅, z)
on ∂D
in H(D) as є →.
. For z ∈IR/D and a given є > , every gz,є ∈L(S) that satisfies
∥Fgz,є −Φ∞(⋅, z)∥L(S) < є
is such that limє→∥vgz,є∥H(D) = ∞.
We note that the difference between cases () and () of this theorem is that for z ∈D the
far field pattern Φ∞(⋅, z) is in the range of F, whereas for z ∈IR/D this is no longer true.


Inverse Scattering
The linear sampling method is based on attempting to compute the function gz,є in the
above theorem by using Tiknonov regularization to solve Fg = Φ∞(⋅, z). In particular, one
expects that the regularized solution will be relatively smaller for z in D than z in IR/D and
this behavior can be visualized by color coding the values of the regularized solution on
a grid over some domain containing D. A more precise statement of this observation will
be made in the next section after we have discussed the factorization method for solving
the inverse scattering problem. Further discussion of why linear sampling works if regu-
larization methods are used to solve (> .) can be found in [, ]. In addition to the
inverse scattering problems > .and
> .it is also possible to treat mixed bound-
ary value problems as well as scattering by both isotropic and anisotropic inhomogeneous
media where in the latter case we must assume that k is not a transmission eigenvalue. For
full details we refer the reader to []. Note that in each case it is not necessary to know the
material properties of the scatterer in order to determine the support of the scatterer from
a knowledge of the far field pattern via solving the far field equation Fg = Φ∞(⋅, z).
The linear sampling method can also be extended to the case of electromagnetic waves
where the far field equation (> .) is now replaced by
∫SE∞(ˆx, d, g(d)) ds(d) = Ee,∞(ˆx, z, q),
where E∞(ˆx, d, p) is the electric far field pattern corresponding to the incident field
(> .), g ∈L
t(S), and Ee,∞is the electric far field pattern of the electric dipole
Ee(x, z, q) := i
k curlx curlx qΦ(x, z),
He(x, z, q) := curlx qΦ(x, z).
(.)
Full details can be found in the lecture notes [].
We close this section by briefly describing a version of the linear sampling method
based on the reciprocity gap functional which is applicable to objects situated in a piecewise
homogeneous background medium. Assume that an unknown scattering object is embed-
ded in a portion B of a piecewise inhomogeneous medium where the index of refraction is
constant with wave number k. Let B⊂B be a domain in B having a smooth boundary ∂B
such that the scattering obstacle D satisfies D ⊂Band let
be the unit outward normal
to ∂B. We now define the reciprocity gap functional by
R(u,v) := ∫∂B
(u ∂v
∂
−v ∂u
∂) ds,
where u and v are solutions of the Helmholtz equation in B/D and u,v ∈C(B/D). In
particular, we want u to be the total field due to a point source situated at x∈B/Band
v = vg to be a Herglotz wave function with kernel g. We then consider the integral equation
R(u,vg) = R(u, Φz),
where Φz := Φ(⋅, z) is the fundamental solution (> .) and u = u(⋅, x) where xis now
assumed to be on a smooth surface C in B/Bthat is homotopic to ∂B. If D is a sound-
soft obstacle, we assume that kis not a Dirichlet eigenvalue of the negative Laplacian in

Inverse Scattering 

D, and if D is an isotropic inhomogeneous medium, we assume that k is not a transmission
eigenvalue. We then have the following theorem [].
Theorem 
Assume that the above assumptions on D are satisfied. Then
. If z ∈D then there exists a sequence {gn} in L(S) such that
lim
n→∞R(u,vgn) = R(u, Φz),
x∈C,
and vgn converges in L(D).
. If z ∈B/D then for every sequence {gn} in L(S) such that
lim
n→∞R(u,vgn) = R(u, Φz),
x∈C,
we have that limn→∞∥vgn∥L(D) = ∞.
In particular, Theorem provides a method for determining D from a knowledge of
the Cauchy data of u on ∂Bin a manner analogous to that of the linear sampling method.
Numerical examples using this method can be found in []. The extension of Theorem 
to the Maxwell equations, together with numerical examples, can be found in [].
..
The Factorization Method
The linear sampling method is complicated by the fact that in general Φ∞(⋅, z) is not in the
range of the far field operator F for either z ∈D or z ∈IR/D. For the case of acoustic waves
when F is normal (e.g., the scattering problem corresponding to (> .) and (> .) or
(> .) for n real valued), the problem was resolved by Kirsch in [, ] who proposed
replacing the far field equation Fg = Φ∞(⋅, z) by
(F∗F)/g = Φ∞(⋅, z),
(.)
where F∗is again the adjoint of F in L(S). In particular, if G : H/(∂D) →L(S)
is defined by G f = v∞where v∞is the far field pattern of the solution to the radiating
exterior Dirichlet problem (see Theorem ) with boundary data f ∈L(∂D), then the
following theorem is valid [].
Theorem 
Assume that kis not a Dirichlet eigenvalue of the negative Laplacian for D.
Then the ranges of G : H/(∂D) →L(S) and (F∗F)/: L(S) →L(S) coincide.
A result analogous to Theorem is also valid for the scattering problem corresponding
to (> .) for n real valued where we now must assume the k is not an interior trans-
mission eigenvalue []. Note that Theorem provides an alternate method to the linear
sampling method for solving the inverse scattering problem corresponding to the scatter-
ing of acoustic waves by a sound-soft obstacle. This follows from the fact that Φ∞(⋅, z) is
in the range of G if and only if z ∈D, i.e.,
> Eq. (.) is solvable if and only if z ∈D.


Inverse Scattering
This is an advantage over the linear sampling method since if (> .) is solved by using
Tikhonov regularization, then as the noise level on u∞tends to zero the norm of the regu-
larized solution remains bounded if and only if z ∈D. A similar statement cannot be made
if regularization methods are used to solve Fg = Φ∞(⋅, z). However, using Theorem , the
following theorem has been established by Arens and Lechleiter [] (see also []).
Theorem 
Let F be the far field operator associated with the scattering problems
(> .) and (> .) and assume that kis not a Dirichlet eigenvalue of the negative
Laplacian for D. For z ∈D let gz ∈L(S) be the solution of (F∗F)/gz = Φ∞(⋅, z) and
for every z ∈IRand є > let gz,є be the solution of Fg = Φ∞(⋅, z) obtained by Tikhonov
regularization, i.e., the unique solution of єg +F∗Fg = F∗Φ∞. Then the following statements
are valid:
. Let vgz,є be the Herglotz wave function with kernel gz,є. Then for every z ∈D the limit
limє→vgz,є(z) exists. Furthermore, there exists c > , depending only on F, such that for
every z ∈D we have that
c∥gz∥
L(S)∥≤lim
є→∣vgz,є(z)∣≤∥gz∥
L(S).
. For z /∈D we have that limє→vgz,є(z) = ∞.
Using Theorem to solve the inverse scattering problem associated with the scattering
problems (> .) and (> .) is called the factorization method. This method has been
extended to a wide variety of scattering problems for both acoustic and electromagnetic
waves, and for details we refer the reader to []. Since this method and its generaliza-
tions are fully discussed in the chapter in this handbook on sampling methods, we will
not pursue the topic further here. A drawback of both the linear sampling method and
the factorization method is the large amount of data needed for the inversion procedure.
In particular, although the linear sampling method can be applied for limited aperture far
field data, one still needs multistatic data defined on an open subset of S.
..
Lower Bounds for the Surface Impedance
One of the advantages that the linear sampling method has over other qualitative methods
in inverse scattering theory is that the far field equation can not only be used to determine
the support of the scatterer but in some circumstances can also be used to obtain lower
bounds on the constitutive parameters of the scattering object. In this section we will con-
sider two such problems, the determination of the surface impedance of a partially coated
object and the determination of the index of refraction of a non-absorbing scatterer. In
the first case we will need to consider a mixed boundary value problem for the Helmholtz
equation, whereas in the second case we will need to investigate the spectral properties of
the interior transmission problem introduced in Theorem of the previous section.

Inverse Scattering 

Mixed boundary value problems typically model the scattering by objects that are
coated by a thin layer of material on part of the boundary. In the study of inverse problems
for partially coated obstacles, it is important to mention that, in general, it is not known
a priori whether or not the scattering object is coated and if so what is the extent of the
coating. We will focus our attention in this section on the special case when on the coated
part of the boundary the total field satisfies an impedance boundary condition and on the
remaining part of the boundary the total field (or the tangential component in the case of
electromagnetic waves) vanishes. This corresponds to the case when a perfect conductor
is partially coated by a thin dielectric layer. For other mixed boundary value problems in
scattering theory and their associated inverse problems, we refer the reader to [] and the
references contained therein.
Let D ⊂IRbe as described in > Sect. .and let ∂D be dissected as ∂D = ΓD ∪Π ∪ΓI
where ΓD and ΓI are disjoint, relatively open subsets of ∂D having Π as their common
boundary. Let λ ∈L∞(ΓI) be such that λ(x) ≥λ> for all x ∈ΓI. We consider the
scattering problem for the Helmholtz equation (> .) where u = ui + us satisfies the
boundary condition
u = 
on ΓD,
∂u
∂
+ iλu = 
on ΓI,
(.)
ui(x) = eik x⋅d and us is a radiating solution. It can be shown that this direct scattering
problem has a unique solution in Hloc(IR/D) []. We again define the far field operator
by (>.) where u∞is now the far field pattern corresponding to the boundary condition
(> .).
In [] it is shown that there exists a unique solution uz ∈H(D) of the interior mixed
boundary value problem
Δuz + kuz = 
in D
(.)
uz + Φ(⋅, z) = 
on ΓD
(.)
∂
∂(uz + Φ(⋅, z)) + iλ (uz + Φ(⋅, z)) = 
on ΓI
(.)
for z ∈D where Φ is the fundamental solution to the Helmholtz equation. Then, if, Φ∞(⋅, z)
is the far field pattern of Φ(⋅, z), we have the following theorem [].
Theorem 
Let є > , z ∈D, and uz be the unique solution of (> .–.). Then
there exists a Herglotz wave function vgz,є with kernel gz,є ∈L(S) such that
∥uz −vgz,є∥H(D) ≤є.
Moreover, there exists a positive constant c independent of є such that
∥Fgz,є −Φ∞(⋅, z)∥L(S) ≤cє.


Inverse Scattering
We can now use Green’s formula to show that []
∫∂D λ∣uz + Φ(⋅, z)∣ds = −k
π −Imuz(z).
From this we immediately deduce the inequality
∥λ∥L∞(ΓI) ≥−k/π −Imuz(z)
∥uz + Φ(⋅, z)∥
L(∂D)
.
(.)
How is the inequality (> .) of practical use? To evaluate the right hand side of
(> .) we need to know ∂D and uz. Both are determined by solving the far field
equation Fg = Φ∞(⋅, z) using Tikhonov regularization and then using the linear sampling
method to determine ∂D and the regularized solution g ∈L(S) to construct the Herglotz
wave function vg. By Theorem we expect that vg is an approximation to uz. However, at
this time, there is no analogue of Theorem for the mixed boundary value problem and
hence this is not guaranteed. Nevertheless in all numerical experiments to date this approx-
imation appears to be remarkably accurate and thus allows us to obtain a lower bound for
∥λ∥L∞(ΓI) via (> .).
The corresponding scattering problem for the Maxwell equations is to find a solution
E = Ei + Es to (> .) satisfying the mixed boundary condition
× E =
on ΓD
× curl E −iλ( × E) ×
=
on ΓI,
(.)
where Ei is the plane wave (> .) and Es is radiating. The existence of a unique solution
E in an appropriate Sobolev space is shown in []. We again define the far field operator
by (> .) where E∞is now the electric far field pattern corresponding to (> .).
Analogous to (> .–.) we now have the interior mixed boundary value problem
curlcurl Ez −kEz = 
in D
(.)
× [Ez + Ee(⋅, z, q)] = 
on ΓD
(.)
× curl[Ez + Ee(⋅, z, q)] −iλ [ × (Ez + Ee(⋅, z, q))] = 
on ΓI,
(.)
where z ∈D and Ee is the electric dipole defined by (> .). The existence of a unique
solution to (> .–.) in an appropriate Sobolev space is established in []. From
the analysis in [] we have the inequality
∥λ∥L∞(ΓI) ≥−k∣q∣/π + k Re(q ⋅Ez)
∥Ez + Ee(⋅, z, q)∥
L
t (∂D)
(.)
analogous to (> .) for the Helmholtz equation. For numerical examples using
(> .) we refer the reader to [].

Inverse Scattering 

Similar inequalities as those derived above for the impedance boundary value prob-
lem can also be obtained for the conductive boundary value problem, i.e., the case when a
dielectric is partially coated by a thin, highly conducting layer [, ].
..
Transmission Eigenvalues
We have previously encountered transmission eigenvalues in Theorem where they
were connected with the injectivity and dense range of the far field operator. In this sec-
tion we shall examine transmission eigenvalues and the interior transmission problem
in more detail. This investigation is particularly relevant to the inverse scattering prob-
lem since transmission eigenvalues can be determined from the far field pattern [] and,
as will be seen, can be used to obtain lower bounds for the index of refraction.
We begin by considering the interior transmission problem (> .–.) from
Theorem and will be concerned with the existence and countability of transmission
eigenvalues. The existence of transmission eigenvalues was first established by Päivärinta
and Sylvester [], and their results were strengthened by Cakoni et al. [].
Theorem 
Assume that n is real valued such that n(x) > for all x ∈D or < n(x) < 
for all x ∈D. Then there exist an infinite number of transmission eigenvalues.
We note that it can be shown that as supx∈D ∣n(x) −∣→then the first transmission
eigenvalue tends to infinity, i.e., in the Born approximation transmission eigenvalues do
not exist [].
Similar results as in Theorem can be obtained for an anisotropic medium and for
the Maxwell equations [].
By Theorem the existence of transmission eigenvalues is established. It can also
be shown that the set of transmission eigenvalues is discrete [, , , ]. The follow-
ing theorem [] establishes a lower bound for the first transmission eigenvalue which is
reminiscent of the famous Faber–Krahn inequality for the first Dirichlet eigenvalue for the
negative Laplacian (which we denote by λ).
Theorem 
Assume that n(x) > for x ∈D and let k> be the first transmission
eigenvalue for the interior transmission problem (> .–.). Then
k
≥
λ(D)
sup
x∈D
n(x).
Theorem has been generalized to the case of anisotropic media and the Maxwell
equations [].
Finally, in the case of the interior transmission problem (> .–.) where there
are cavities in D, i.e., regions D⊂D where n(x) = for x ∈D, it can be shown that


Inverse Scattering
transmission eigenvalues exist, form a discrete set and the first transmission eigenvalue k
satisfies []
k
≥
λ(D)
sup
x∈D/D
n(x).
Note that, since in each of the above cases D can be determined by the linear sampling
method, λ(D) is known and hence given kthe above inequalities yield a lower bound for
the supremum of the index of refraction.
..
Historical Remarks
The use of qualitative methods to solve inverse scattering problems began with the 
paper of Colton and Kirsch [] and the paper of Colton et al. []. These papers were
in turn motivated by the dual space method of Colton and Monk developed in [, ].
Both [] and [] were concerned with the case of scattering of acoustic waves. The exten-
sion of the linear sampling method to electromagnetic waves was first outlined by Kress
[] and then discussed in more detail by Colton et al. [] and Haddar and Monk [].
The factorization method was introduced in and by Kirsch [, ] for acoustic
scattering problems. Attempts to extend the factorization method to the case of electro-
magnetic waves have been only partly successful. In particular, the factorization method for
the scattering of electromagnetic waves by a perfect conductor remains an open question.
In addition to the linear sampling and factorization methods there have been a number
of other qualitative methods developed primarily by Ikehata and Potthast and their co-
workers. Although space is too short to discuss these alternate qualitative methods in this
survey, we refer the reader to [, ] for details and references.
The countability of transmission eigenvalues for acoustic waves was established by
Colton et al. [] and Rynne and Sleeman [] and for the Maxwell equations by Cakoni
and Haddar [] and Kirsch []. The existence of transmission eigenvalues for acoustic
waves was first given by Päivärinta and Sylvester [] for the isotropic case and for the
anisotropic case by Cakoni and Haddar [] and Kirsch [] who also established the exis-
tence of transmission eigenvalues for Maxwell’s equations. These results were subsequently
improved by Cakoni et al. []. Inequalities for the first transmission eigenvalues were first
obtained by Colton et al. [] and Cakoni et al. [, ].
.
Cross-References
> EIT
> EM Algorithms
> Iterative Solution Methods
> Radar

Inverse Scattering 

> Regularization Methods for Ill-Posed Problems
> Sampling Methods
> Tomography
> Wave Phenomena
References and Further Reading
. Alessandrini G, Rondi L () Determining
a sound–soft polyhedral scatterer by a single
far–field measurement. Proc Am Math Soc
:–
. Arens T () Why linear sampling works.
Inverse Prob :–
. Arens T, Lechleiter A () The linear sampling
method revisited. J Integral Eqn Appl :–
. van den Berg R, Kleinman R () A con-
trast source inversion method. Inverse Prob
:–
. Bukhgeim A () Recovering a potential from
Cauchy data in the two-dimensional case. J
Inverse Ill–Posed Prob :–
. Cakoni F, Colton D () A uniqueness theorem
for an inverse electromagnetic scattering prob-
lem in inhomogeneous anisotropic media. Proc
Edinburgh Math Soc :–
. Cakoni F, Colton D () Qualitative methods
in inverse scattering theory. Springer, Berlin
. Cakoni F, Colton D () The determination
of the surface impedance of a partially coated
obstacle from far field data. SIAM J Appl Math
:–
. Cakoni F,ColtonD,Haddar H () Thecompu-
tation of lower bounds for the norm of the index
of refraction in anisotropic media from far field
data. J Integral Eqn Appl :–
. Cakoni F, Colton D, Haddar H () The interior
transmission problem for regions with cavities.
SIAM J Math Anal :–
. Cakoni F, Colton D, Haddar H () On
the determination of Dirichlet and transmission
eigenvalues from far field data. Comp Rend Math-
ematique :–
. Cakoni F, Colton D, Monk P () The elec-
tromagnetic inverse scattering problem for partly
coated Lipschitz domains. Proc R Soc Edinburgh
A:–
. Cakoni F, Colton D, Monk P The linear sam-
pling method in inverse electromagnetic scatter-
ing. SIAM.
. Cakoni F, Fares M, Haddar H () Analysis of
two linear sampling methods applied to electro-
magnetic imaging of buried objects. Inverse Prob
:–
. Cakoni F, Gintides D, Haddar H () The exis-
tence of an infinite discrete set of transmission
eigenvalues. SIAM J Math Anal :–
. Cakoni F, Haddar H () A variational
approach for the solution of the electro-magnetic
interior transmission problem for anisotropic
media. Inverse Prob Imaging :–
. Cakoni F, Haddar H () On the existence of
transmission eigenvalues in an inhomogeneous
medium. Appl Anal :–
. Colton D, Haddar H () An application of
thereciprocitygap functional to inversescattering
theory. Inverse Prob :–
. Colton D, Haddar H, Monk P () The linear
sampling method for solving the electromagnetic
inverse scattering problem. SIAM J Sci Comput
:–
. Colton D, Kirsch A () A simple method for
solving inverse scattering problems in the reso-
nance region. Inverse Prob :–
. Colton D, Kirsch A, Päivärinta L () Far field
patterns for acoustic waves in an inhomogeneous
medium. SIAM J Math Anal :–
. Colton D, Kress R () Eigenvalues of the
far field operator for the Helmholtz equation
in an absorbing medium. SIAM J Appl Math
:–
. Colton D, Kress R () Inverse acoustic and
electromagnetic
scattering
theory,
nd edn.
Springer, Berlin
. Colton D, Kress R () On the denseness
of Herglotz wave functions and electromagnetic


Inverse Scattering
Herglotz pairs in Sobolev spaces. Math Methods
Appl Sci :–
. Colton D, Monk P () A novel method for
solving the inverse scattering problem for time
harmonic acoustic waves in the resonance region
II. SIAM J Appl Math :–
. Colton D, Monk P () The inverse scattering
problem for acoustic waves in an inhomogeneous
medium. Quart J Mech Appl Math :–
. Colton D, Monk P () Target identification
of coated objects. IEEE Trans Antennas Prop
:–
. Colton D, Päivärinta L () The uniqueness of
a solution to an inverse scattering problem for
electromagnetic waves. Arch Rational Mech Anal
:–
. ColtonD,PäivärintaL,Sylvester J()Theinte-
rior transmission problem. Inverse Probl Imaging
:–
. Colton D, Piana M, Potthast R () A sim-
ple method using Mozorov’s discrepancy prin-
ciple for solving inverse scattering problems.
Inverse Prob :–
. Colton D, Sleeman B () An approximation
property of importance in inverse scattering the-
ory. Proc Edinburgh Math Soc :–
. Farhat C, Tezaur R, Djellouli R () On the
solution of three-dimensional inverse obstacle
acoustic scattering problems by a regularized
Newton method. Inverse Prob :–
. Gintides D () Local uniqueness for the
inverse scattering problem in acoustics via
the
Faber–Krahn
inequality.
Inverse
Prob
:–
. Gylys–Colwell F () An inverse problem for
the Helmholtz equation. Inverse Prob :–
. Haddar H, Monk P () The linear sampling
method for solving the electromagnetic inverse
medium problem. Inverse Prob :–
. Hähner P () A periodic Faddeev–type solu-
tion operator. J Diff Eqn :–
. Hähner P () On the uniqueness of the shape
of a penetrable anisotropic obstacle. J Comp Appl
Math :–
. Hähner P () Electromagnetic wave scatter-
ing. In: Pike R, Sabatier P (eds) Scattering. Aca-
demic, New York
. Harbrecht H, Hohage T () Fast methods
for three-dimensional inverse obstacle scattering
problems. J Integral Eqn Appl :–
. Hohage T () Iterative methods in inverse
obstacle Scattering: regularization theory of linear
and nonlinear exponentially ill-posed problems.
Dissertation, Linz
. Isakov V () On the uniqueness in the inverse
transmission scattering problem. Comm Partial
Diff Eqns :–
. Isakov V () Inverse problems for partial dif-
ferential equations. Springer, Berlin
. Ivanyshyn O () Nonlinear boundary inte-
gral equations in inverse scattering. Dissertation,
Gäottingen
. Ivanyshyn O, Kress R () Nonlinear inte-
gral equations in inverse obstacle scattering. In:
Fotiatis M (ed) Mathematical methods in scat-
tering theory and biomedical engineering. World
Scientific, Singapore, pp –
. Ivanyshyn O, Kress R () Identification of
sound-soft D obstacles from phaseless data.
Inverse Prob Imaging :–
. Johansson T, Sleeman B () Reconstruction of
an acoustically sound-soft obstacle from one inci-
dent field and the far field pattern. IMA J Appl
Math :–
. Jones DS () Acoustic and electromagnetic
waves. Clarendon, Oxford
. Kirsch A () The domain derivative and two
applications in inverse scattering. Inverse Prob
:–
. Kirsch A () Characterization of the shape of
a scattering obstacle using the spectral data of the
far field operator. Inverse Prob :–
. KirschA () Factorization of the far field oper-
ator for the inhomogeneous medium case and an
application in inverse scattering theory. Inverse
Prob :–
. Kirsch A () An integral equation approach
and
the
interior
transmission
problem
for
Maxwell’s
equations.
Inverse
Prob
Imaging
:–
. Kirsch A () On the existence of transmission
eigenvalues. Inverse Prob Imaging :–
. Kirsch A, Grinberg N () The factorization
method for inverse problems. Oxford University
Press, Oxford

Inverse Scattering 

. KirschA, Kress R () An optimization method
in inverse acoustic scattering. In: Brebbia CA et al
(ed) Boundary elements IX, Vol . Fluid flow and
potential applications. Springer, Berlin
. Kirsch A, Kress R () Uniqueness in inverse
obstacle scattering. Inverse Prob :–
. Kleinman R, van den Berg P () A modified
gradientmethodfor two dimensional problems in
tomography. J Comp Appl Math :–
. Kress R () Electromagnetic waves scattering.
In: Pike R, Sabatier P (eds) Scattering. Academic,
New York
. Kress R () Newton’s Method for inverse
obstacle scattering meets the method of least
squares. Inverse Prob :–
. Kress R, Rundell W () Inverse scatter-
ing for shape and impedance. Inverse Prob :
–
. Kress R, Rundell W () Nonlinear inte-
gral equations and the iterative solution for an
inverse boundary value problem. Inverse Prob
:–
. Langenberg K () Applied inverse problems
for acoustic, electromagnetic and elastic wave
scattering. In: Sabatier P (ed) Basic methods of
tomography and inverse problems. Adam Hilger,
Bristol
. Lax PD, Phillips RS () Scattering theory. Aca-
demic, New York
. Liu H () A global uniqueness for formally
determined electromagnetic obstacle scattering.
Inverse Prob :
. Liu H, Zou J () Uniqueness in an inverse
acoustic obstacle scattering problem for both
sound-hard and sound-soft polyhedral scatterers.
Inverse Prob :–
. McLean W () Strongly elliptic systems and
boundary integral equations. Cambridge Univer-
sity Press, Cambridge
. Monk P () Finite element methods for
Maxwell’s equations. Oxford University Press,
Oxford
. Morse PM, Ingard KU () Linear acoustic the-
ory. In: Faugge S (ed) Encyclopedia of physics.
Springer, Berlin
. Mäuller C () Foundations of the mathemat-
ical theory of electromagnetic waves. Springer,
Berlin
. Nachman A () Reconstructions from bound-
ary measurements. Ann Math :–
. Nédélec JC () Acoustic and electromagnetic
equations. Springer, Berlin
. Novikov R () Multidimensional inverse spec-
tral problems for the equation −Δψ+ (v(x)−
Eu(x)) ψ = .TransFunctAnal Appl :–
. Ola P, Päivärinta L, Somersalo E () An
inverse boundary value problem in electrody-
namics. Duke Math J :–
. Ola P, Somersalo E () Electromagnetic
inverse problems and generalized Sommer-feld
potentials. SIAM J Appl Math :–
. Päivärinta L, Sylvester J () Transmission
eigenvalues. SIAM J Math Anal :–
. Piana M () On uniqueness for anisotropic
inhomogeneous
inverse
scattering
problems.
Inverse Prob :–
. Potthast R () Fréchet differetiability of
boundary integral operators in inverse acoustic
scattering. Inverse Prob :–
. Potthast R () Point-sources and multipoles
in inverse scattering theory. Chapman and Hall,
London
. Potthast R () On the convergence of a
new Newton-type method in inverse scattering.
Inverse Prob :–
. Potthast R () A survey on sampling and
probe methods for inverse problems. Inverse Prob
:R–R
. Ramm A () Recovery of the potential
from fixed energy scattering data. Inverse Prob
:–
. Rjasanow S, Steinbach O () The fast solu-
tion of boundary integral equations. Springer,
Berlin
. Roger R () Newton Kantorovich algorithm
applied to an electromagnetic inverse problem.
IEEE Trans Antennas Prop :–
. Rondi L () Unique determination of non-
smooth sound-soft scatterers by finitely many
far-field measurements. Indiana Univ Math J
:–
. Rynne BP, Sleeman BD () The interior
transmission problem and inverse scattering
from inhomogeneous media. SIAM J Math Anal
:–


Inverse Scattering
. Serranho P () A hybrid method for inverse
scattering for shape and impedance. Inverse Prob
:–
. Serranho P () A hybrid method for inverse
obstacle
scattering
problems.
Dissertation,
Gäottingen
. Serranho P () A hybrid method for sound-
soft obstacles in D. Inverse Prob Imaging :
–
. Stefanov P, Uhlmann G () Local uniqueness
for the fixed energy fixed angle inverse problem
in obstacle scattering. Proc Am Math Soc :
–
. Sylvester J, Uhlmann G () A global unique-
ness theorem for an inverse boundary value prob-
lem. Ann Math :–

Electrical Impedance
Tomography
Andy Adler ⋅Romina Gaburro ⋅William Lionheart
.
Introduction......................................................................
..
Measurement Systems and Physical Derivation. ...................................
..
The Concentric Anomaly: A Simple Example......................................
..
Measurements with Electrodes.......................................................
.
Uniqueness of Solution..........................................................
..
The Isotropic Case.......................................................................
...
Calderón’s Paper.........................................................................
...Uniqueness at the Boundary...........................................................
...Complex Geometrical Optics Solutions for the Schrödinger Equation. ........
...Dirichlet-to-Neumann Map and Cauchy Data for the Schrödinger
Equation. .................................................................................
...Global Uniqueness for n ≥...........................................................
...Global Uniqueness in the Two-Dimensional Case.................................
...Some Open Problems for the Uniqueness...........................................
...Stability of the Solution at the Boundary............................................
...Global Stability for n ≥...............................................................
...Global Stability for the Two-Dimensional Case....................................
...Some Open Problems for the Stability...............................................
..
The Anisotropic Case...................................................................
...Non-uniqueness.........................................................................
...Uniqueness up to Diffeomorphism...................................................
...Anisotropy which is Partially a Priori Known......................................
..
Some Remarks on the Dirichlet-to-Neumann Map................................
...EIT with Partial Data...................................................................
...The Neumann-to-Dirichlet Map......................................................
.
The Reconstruction Problem....................................................
..
Locating Objects and Boundaries....................................................
..
Forward Solution........................................................................
..
Regularized Linear Methods..........................................................
..
Regularized Iterative Nonlinear Methods...........................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Electrical Impedance Tomography
..
Direct Nonlinear Solution.............................................................
.
Conclusions......................................................................

Electrical Impedance Tomography 

.
Introduction
Electrical Impedance Tomography (EIT) is the recovery of the conductivity (or conductiv-
ity and permittivity) of the interior of a body from a knowledge of currents and voltages
applied to its surface. In geophysics, where the method is used in prospecting and archae-
ology, it is known as electrical resistivity tomography. In industrial process tomography
it is known as electrical resistance tomography or electrical capacitance tomography. In
medical imaging, when at the time of writing it is still an experimental technique rather
than routine clinical practice, it is called EIT. A very similar technique is used by weakly
electric fish to navigate and locate prey and in this context it is called electrosensing.
The simplest mathematical formulation of inverse problem of EIT can be stated as fol-
lows. Let Ω be a conducting body described by a bounded domain in ℝn, n ≥, with
electrical conductivity a bounded and positive function γ(x) (later we will consider also γ
complex). In absence of internal sources, the electrostatic potential u in Ω is governed by
the elliptic partial differential equation
Lγu := ∇⋅γ∇u = 
in
Ω.
(.)
It is natural to consider the weak formulation of (> .) in which u ∈H(Ω) is a
weak solution to (> .). Given a potential ϕ ∈H/(∂Ω) on the boundary, the induced
potential u ∈H(Ω) solves the Dirichlet problem
{ Lγu = 
in
Ω,
u∣∂Ω = ϕ.
(.)
The current and voltage measurements taken on the surface of Ω, ∂Ω are given by the
so-called Dirichlet-to-Neumann map (associated with γ) or voltage-to-current map
Λγ : u∣∂Ω ∈H/(∂Ω) →γ ∂u
∂
∈H−/(∂Ω).
Here,
denotes the unit outer normal to ∂Ω, and the restriction to the boundary is
considered in the sense of the trace theorem on Sobolev spaces. We require that ∂Ω be at
least Lipschitz continuous and γ ∈L∞(Ω) with essinfRe γ = m > .
The forward problem under consideration is the map γ ∈Dm ↦Λγ, where Dm = {γ ∈
L∞(Ω)∣essinfγ ≥m}. The inverse problem for complete data is then the recovery of γ
from Λγ. As is usual in inverse problems, we will consider the questions of () uniqueness
of solution (or from a practical point of view sufficiency of data), () stability/instability
with respect to errors in the data, and () practical algorithms for reconstruction. It is also
worth pointing out to the reader who is not very familiar with EIT the well known fact
that the behavior of materials under the influence of external electric fields is determined
not only by the electrical conductivity γ but also by the electric permittivity ε so that the
determination of the complex valued function γ(x, ω) = σ(x) + iωε(x) would be the
more general and realistic problem, where i =
√
−and ω is the frequency. The simple case
where ω = will be treated in this work. For a description of the formulation of the inverse


Electrical Impedance Tomography
problem for the complex case, we refer for example to []. Before we address questions
()–() mentioned above, we will consider how the problem arises in practice.
..
Measurement Systems and Physical Derivation
For the case of direct current, that is, the voltage applied is independent of time, the deriva-
tion is simple. Of course here Ω ⊂ℝ. Let us first suppose that we can apply an arbitrary
voltage ϕ ∈H/(Ω) to the surface. We assume that the exterior ℝ/Ω is an electrical insu-
lator. An electric potential (voltage) u results in the interior and the current J that flows
satisfies the continuum Ohm’s law J = −γ∇u; the absence of current sources in the interior
is expressed by the continuum version of Kirchoff’s law ∇⋅J = which together result in
(> .). The boundary conditions are controlled or measured using a system of conduct-
ing electrodes which are typically applied to the surface of the object. In some applications,
especially geophysical, these may be spikes that penetrate the object, but it is common to
model these as points on the surface. Systems are used that to a reasonable approximation
apply a known current on (possibly a subset) or electrodes and measure the voltage that
results on electrodes (again possibly a subset, in some cases disjoint from those carrying
a non-zero current). In other cases it is a predetermined voltage applied to electrodes and
the current measured; there being practical reasons determined by electronics or safety
for choosing one over the other. In medical EIT applying known currents and measuring
voltages is typical. One reason for this is the desire to limit the maximum current for safety
reasons. In practice, the circuit that delivers a predetermined current can only do so while
the voltage required to do that is within a certain range so both maximum current and
voltage are limited. For an electrode (let us say indexed by ℓ) not modelled by a point but
covering a region Eℓ⊂∂Ω the current to that electrode is the integral
Iℓ= ∫Eℓ
−J ⋅
dx.
(.)
Away from electrodes we have
γ ∂u
∂
= ,
on ∂Ω /
L
⋃
ℓ=
Eℓ
(.)
as the air surrounding the object is an insulator. On the conducting electrode we have
u∣Eℓ= Vℓa constant, or as a differential condition
× ∇u = 
on ∂Ω /
L
⋃
ℓ=
Eℓ.
(.)
Taken together, (> .–.) are called the shunt model. This ideal of a perfectly conducting
electrode is of course only an approximation, and we note that while the condition u ∈
H(Ω) is a sensible condition, ensuring finite total dissipated power, it is not sufficient to
ensure (> .) is well defined. Indeed for smooth γ and smooth ∂Eℓthe condition results
in a square root singularity in the current density on the electrode. We will come back to a
more realistic model of electrodes.

Electrical Impedance Tomography 

It is more common to use alternating current in geophysical and process monitoring
applications, and essential in medical applications. Specifically the direction of the current
must be reversed within a sufficiently short time to avoid electrochemical effects. This also
means that the time average of the applied current should be zero. In medical applications,
current in one direction for sufficient duration would result in transport of ions, and one
of the effects of this can be stimulation of nerves. It would also degrade electrode behavior
due to charge build up and ionic changes in the electrode. As a general rule, higher levels
of current and voltage are considered safer at higher temporal frequencies. The simplest
EIT system therefore operates at a fixed frequency using an oscillator or digital signal pro-
cessing to produce a sinusoidal current. Measurements are then taken of the magnitude,
or in the some cases the components that are in phase and π/out of phase with the orig-
inal sine wave. Of course when current or voltage is first applied to the object a transient
results, and typical EIT systems are designed to start measuring after this transient term
has decayed so as to be negligible.
In geophysics a technique that is complementary to EIT called induced polarization
tomography IPT is used to find polarizable minerals. In effect this uses a square wave pulse
and measures the transient response []. In process tomography, a technique known as
electrical capacitance tomography is designed for imaging insulating materials with differ-
ent dielectric permittivities, for example oil and gas in a pipe [] []. Again square waves
or pulses are used.
In medical and geophysical problems the response of the materials may vary with fre-
quency. For example, in a biological cell, higher frequency current might penetrate a largely
capacitive membrane and so be influence by the internal structures of the cell while lower
frequency currents pass around the cell. This has led to Electrical Impedance Tomography
Spectroscopy (EITS)[], and in geophysics Spectral Induced Polarization Tomography
(SIPT)[]. The spectral response can be established either by using multiple sinusoidal
frequencies or by sampling the transient response to a pulse.
Our starting point for the case of alternating current is the time harmonic Maxwell
equations at a fixed angular frequency ω. Here it is assumed that the transient components
of all fields are negligible and represent the time harmonic electric and magnetics vector
fields using the complex representation F(x, t) = Re (Fexp(iωt)) and we have
∇× E = −iωB
(.)
∇× H = J + iωD.
(.)
The electric and magnetic fields E and H are related to the current density J, electric dis-
placement D, and magnetic flux B by the material properties conductivity σ, permittivity
є, and permeability μ by
J = σE, D = єE, B = μH.
(.)
The fields E and H are evaluated on directed curves, while the “fluxes" J,D, and B on sur-
faces. In biomedical applications one can typically take μ to be constant and to be the


Electrical Impedance Tomography
same inside the body as outside in air. In non-destructive testing and geophysical appli-
cations there may well be materials with differing permeability. We are also assuming
linear relations in (> .). For example, the first is the continuum Ohm’s law. We allow
for the possibility that the material properties are frequency dependent. In this, dispersion
is important in EIS and SIPT. For the moment we also assume isotropy (so that the material
properties are scalars).
There are many inverse problems governed by time harmonic Maxwell’s equations. For
very large values of ω this includes optical and microwave tomographic techniques and
scattering problems such as radar which we do not discus in this chapter. There are also
systems where the fields arise from alternating current in a coil, and measurements are
made either with electrodes or with other coils. Mutual (or magnetic) induction tomogra-
phy (MIT) falls in to this category and has been tried in medical and process monitoring
applications []. In these cases the eddy current approximation [] to Maxwell’s equations
is used. While for direct current EIT (that is ERT) the object is assumed surrounded by an
insulator, in MIT one must account for the magnetic fields in the surrounding space, there
being no magnetic “shielding”.
We now come to the assumptions used to justify the usual mathematical model of EIT
that are distinct from many other inverse problems for Maxwell’s equations. We already
have
Assumption 
Transients components of all fields are negligible.
This assumption simply means we have waited a sufficient “settling time” before making
measurements.
We are interested in relatively low frequencies where magnetic effects can be neglected,
this translates in to two assumptions
Assumption 
ω√єμ is small compared with the size of Ω.
This means that the wavelength of propagating waves in the material is large. A measure-
ment accuracy of −= /,is ambitious at higher frequencies means that for wave
effects to be negligible
d ω√єμ < cos−,
,,
(.)
where d is the diameter of the body. Taking the relative permittivity to be and R = .m
gives a maximum frequency of MHz.
Assumption 
√
ωσ μ/is small compared with the size of Ω.
The quantity
δ =
√

ωσ μ
(.)

Electrical Impedance Tomography 

⊡Fig. -
A system of eletrodes used for chest EIT at Oxford Brookes University. The positions of the
electrodes were measured manually with a tape measure and the cross sectional shape was
also determined by manual measurements. These electrodes have a disk of jell containing
silver chloride solution that makes contact with the skin. Each eletrode was attached to the
EIT system by a screened lead, not shown in this picture for clarity
is known as the skin depth. For a frequency of kHz and a conductivity of .Sm−typical
in medical applications, the skin depth is m. In geophysics lower frequencies are typical
but length scales are larger. In a conducting cylinder the electric field decays with distance
r from the boundary at a rate e−r/δ due to the opposing magnetic field. At EIT frequencies
this simple example suggests that accurate forward modelling of EIT should take account
of this effect although it is currently not thought to be a dominant source of error.
The effect of Assumptions and combined together is that we can neglect ∇× E in
Maxwell’s equations resulting in the standard equation for complex EIT
∇⋅(σ + iωє)∇u = .
(.)
Here the expression γ = σ +iωє is called complex conductivity, or logically the admittivity,
while /σ is called resistivity and the rarely-used complex /γ impedivity. A scaling argu-
ment is given for the approximation (> .) in [], and numerical checks on the validity
of the approximation in [] and [].
It is often not so explicitly stated but, while in the direct current case one can neglect the
conductivity of the air surrounding the body, for the alternating current case the electrodes
are coupled capacitively and, while σ can be assumed to be zero for air, the permittivity
of any material is no smaller than that of a vacuum є= .× −, although dry air
approaches that value. One requires then
Assumption 
ωє in the exterior is negligible compared to ∣σ + iωє∣in the interior.
For example, with a conductivity or .Sm−, the magnitude of the exterior admittivity
reaches −of that value for a frequency of .MHz. For a more detailed calculation,
the capacitance between the electrodes externally could be compared with the impedance


Electrical Impedance Tomography
Electrode
Inclusions
Internal wall
Earthed screen
Radial earthed
screen
⊡Fig. -
A cross section through a typical ECT sensor around a pipe (internal wall) showing the
external screen with radial screens designed to reduce the external capacitive coupling
between electrodes
between electrodes. In ECT, frequencies above MHz are used and the exterior capaci-
tance can not be neglected. Indeed, an exterior grounded shield is used so that the exterior
capacitive coupling is not affected by the surroundings (see > Fig. -).
..
The Concentric Anomaly: A Simple Example
A simple example helps us to understand the instability in the inverse conductivity prob-
lem. Let Ω be the unit disk in ℝwith polar coordinates (r, θ) and consider a concentric
anomaly in the conductivity of radius ρ < 
γ(x) = { a,
∣x∣≤ρ
a,
ρ < ∣x∣≤.
(.)
From separation of variables, matching Dirichlet and Neumann boundary conditions
at ∣x∣= ρ, we find for n ∈ℤ
Λγeinθ = ∣n∣+ μρ∣n∣
−μρ∣n∣einθ,
(.)
where μ = (a−a)/(a+ a). From this, one sees the effect of the Dirichlet to Neu-
mann map on the complex Fourier series, and the effect on the real Fourier series is easily
deduced. This example was considered in [] as an example of the eigenvalues and eigen-
functions of Λγ, and also by [] as an example of instability. We see that ∣∣γ −a∣∣L∞(Ω) =
∣a−a∣independently of ρ and yet Λγ →Λain the operator norm. Hence, if an inverse
map Λγ ↦γ exists, it cannot be continuous in this topology. Similar arguments can be
used to show instability of inversion in other norms.

Electrical Impedance Tomography 

This example reveals many other features of the more general problem. For example,
experimentally one observes saturation: for an object placed away from the boundary,
changes in the conductivity of an object with a conductivity close to the background are
fairly easily detected, but for an object of very high or low conductivity further changes in
conductivity of that object have little effect. This saturation effect was explored for offset
circular objects (using conformal mappings) by Seagar []. For a numerical study of sat-
uration see
> Fig -. This is also an illustration of the non linearity of γ →Λγ. One can
also see in this example that smaller objects (with the same conductivity) produce smaller
changes in measured data as one might expect.
On the unit circle Sone can define an equivalent norm on the Sobolev space Hs
◇(S)
(see definitions in > Sect. ..) by
∣∣
∞
∑
n=−∞,n≠
cnmreinθ∣∣

s
=
∞
∑
n=−∞,n≠
nsc
n.
(.)
It is clear for this example that Λγ : Hs
⋅(S) →Hs−
◇(S), for any s. Roughly the current is a
derivative of potential and one degree of differentiability less smooth. Technically Λγ (for
any positive γ ∈C∞(Ω)) is a first order pseudo-differential operator []. The observation
that for our example e−inθΛγeinθ = ∣n∣+ o(n−p) as ∣n∣→∞for any p > −illustrates
that the change in conductivity and radius of the interior object is of somewhat secondary
importance! In the language of pseudodifferential operators for a general γ such that γ −
vanishes in a neighborhood of the boundary, Λγ and Λdiffer by a smoothing operator.
We see also from (> .) that Λ−
γ is also well defined operator on L
◇→L
◇with
eigenvalues O(∣n∣−) and is therefore a Hilbert–Schmidt operator. This is also known for
the general case [].
Early work on medical applications of EIT [], [] hoped that the forward problem
in EIT would be approximated by generalized ray transform – that is integrals along cur-
rent stream lines. The example of a concentric anomaly was used to illustrate that EIT is
nonlocal []. If one applies the voltage cos(θ + α), which for a homogeneous disk would
result in current streamlines that are straight and parallel, a change in conductivity in a
small radius ρ from the centre changes all measured currents, not just on lines passing
through the region of changed conductivity ∣x∣≤ρ. In the s a two dimensional algo-
rithm that backprojected filtered data along equipotential lines was popularized by Barber
and Brown []. Berenstein [] later showed that the linearized EIT problem in a unit
disc can be interpreted as the Radon transform with respect to the Poincaré metric and a
convolution operator and that Barber and Brown’s algorithm is an approximate inverse to
this.
In process applications of EIT and related techniques the term soft field imaging is used,
which by analogy to soft field X-rays means a problem that is non linear and non-local.
However, in the literature when the “soft field effect” is invoked, it is often not clear if it is
the nonlinear or non local aspect to which they refer and in our opinion the term is best
avoided.


Electrical Impedance Tomography
..
Measurements with Electrodes
A typical electrical imaging system uses a system of conducting electrodes attached to the
surface of the body under investigation. One can apply current or voltage to these elec-
trodes and measure voltage or current, respectively. For one particular measurement the
voltages (with respect to some arbitrary reference) are Vℓand the currents Iℓ, which we
arrange in vectors as V and I ∈ℂL . The discrete equivalent of the Dirichlet-to-Neumann
Λγ map is the transfer admittance, or mutual admittance matrix Y which is defined by
I = YV.
It is easy to see that the vector = (,, . . . ,)T is in the null space of Y, and that the range
of Y is orthogonal to the same vector. Let S be the subspace of ℂL perpendicular to then
it can be shown that Y∣S is invertible from S to S. The generalized inverse (see > Chap. )
Z = Y† is called the transfer impedance. This follows from uniqueness of solution of shunt
model boundary value problem.
The transfer admittance, or equivalently transfer impedance, represents a complete set
of data which can be collected from the L electrodes at a single frequency for a stationary
linear medium. It can be seen from the weak formulation of (> .) that Y and Z are
symmetric (but for ω ≠not Hermittian). In electrical engineering this observation is
called reciprocity. The dimension of the space of possible transfer admittance matrices is
clearly no bigger than L(L−)/,and so it is unrealistic to expect to recover more unknown
parameters than this. In the analogous case of planar resistor networks with L “boundary”
electrodes the possible transfer admittance matrices can be characterized completely [],
a characterization which is known at least partly to hold in the planar continuum case [].
A typical electrical imaging system applies current or voltage patterns which form a basis
of the space S, and measures some subset of the resulting voltages which, as they are only
defined up to an additive constant, can be taken to be in S.
We have seen that the shunt model is non physical. In medical application with elec-
trodes applied to skin and in “phantom” tanks used to test EIT systems with ionic solutions
in contact with metal electrodes, a contact impedance layer exists between the solution or
skin and the electrode. This modifies the shunting effect so that the voltage under the elec-
trode is no longer constant. The voltage on the electrode is still a constant Vℓ, so now on
Eℓthere is a voltage drop across the contact impedance layer
ϕ + zℓσ ∂ϕ
∂
= Vℓ,
(.)
where the contact impedance zℓcould vary over Eℓbut is usually assumed constant. Exper-
imental studies have shown [] that a contact impedance on each electrode is required
for an accurate forward model. This new boundary condition together with (> .) and
(> .) forms the Complete Electrode Model or CEM. For experimental validation of
this model see [], theory [], and numerical calculations [, ]. A nonzero contact
impedance removes the singularity in the current density, although high current densities
still occur at the edges of the electrodes ( > Fig. -). For further details on the singularity
in the current density see [].

Electrical Impedance Tomography 

0.25
Boundary current density - passive electrode
Boundary current density - active electrode
0.15
0.05
–0.05
–0.15
–0.2
–0.250
5
a
10
15
20
0
5
10
15
20
–0.1
0.2
0.1
0
0.25
0.15
0.05
–0.05
–0.15
–0.2
–0.25
–0.1
0.2
0.1
0
46
Electrode region
1.5 Ohm* cm2
15 Ohm* cm2
150 Ohm* cm2
Background 62 Ohm*cm
44
42
40
38
36
34
320
b
5
10
15
20
25
⊡Fig. -
(Continued)


Electrical Impedance Tomography
10.4
10.2
10
9.8
9.6
9.4
9.2
9
8.8
8.6
8.4
10
c
10.5
11
11.5
12
12.5
4
8
7.8
7
6.5
6
11
d
11.5
12
12.5
13
13.5
14
3
⊡Fig. -
The current density on the boundary with the CEM is greatest at the edge of the electrodes,
even for passive electrodes. This eﬀect is reduced as the contact impedance increases.
Diagrams courtesy of Andrea Borsic. (a) Current density on the boundary for passive and
active electrodes. In fact there is a jump discontinuity at the edge of electriodes for
non-zeros contact impedance although our ploting routine has joined the left and right
limits. (b) The eﬀect of contact impedance on the potential beneath an electrode. The
potential is continuous. (c) Interior current near an active electrode. (d) Interior current near
a passive electrode

Electrical Impedance Tomography 

The set of imposed current patterns, or excitation patterns, is designed to span S, or at
least that part of it that can be accurately measured in a given situation. In medical EIT,
with process ERT following suit, early systems designed at Sheffield [] assumed a two
dimensional circular domain. Identical electrodes were equally spaced on the circumfer-
ence and, taking them to be numbered anticlockwise, the excitation patterns used were
adjacent pairs, that is proportional to
Ii
ℓ=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
,
i = ℓ
−,
i = ℓ+ 
,
otherwise ,
(.)
for i = , . . . L −. The electronics behind this is balanced current source connected
between two electrodes [, Chap. ], and this is somewhat easier to achieve in practice
than a variable current source at more than two electrodes. For general geometries, where
the electrodes are not placed on a closed curve, other pairs of electrodes are chosen. For
example Ii
= −, while Ii
ℓ= δiℓ, ℓ≠.
Measurements of voltage can only be differential and so voltage measurements are
taken between pairs of electrodes, for example adjacent pairs, or between each and
some fixed electrode. In pair drive systems, similar to the original Sheffield system,
voltages on electrodes with nonzero currents are not measured, resulting in incomplete
knowledge of Z.
In geophysical surface resistivity surveys it is common to use a pair drive and pair mea-
surement system, using electrodes in a line where a two dimensional approximation is used,
or laid out in a rectangular or triangular grid where the full three dimensional problem is
solved. Measurements taken between pairs of non-current carrying electrodes. The choice
of measurement strategy is limited by the physical work involved in laying out the cables
and by the switching systems. Often electrodes will be distributed along one line and a
two dimensional approximate reconstruction used as this gives adequate information for
less cost. A wider spacing of the current electrodes is used where the features of interest is
located at a greater depth below the ground. In another geophysical configuration, cross
borehole tomography, electrodes are deployed down several vertical cylindrical holes in
the ground, typically filled with water, and current passed between electrodes in the same
or between different bore holes. Surface electrodes may be used together with those in the
bore holes. In some systems the current is measured to account for a non-ideal current
source.
In capacitance tomography a basis of voltage patterns is applied and the choice V i
ℓ= δiℓ
is almost universal. The projection of these vectors to S (we call an “electrode-wise basis”)
is convenient computationally as a current pattern.
Given a multiple drive system capable of driving an arbitrary vector of currents in S
(in practice with in some limits on the maximum absolute current and on the maximum
voltage) we have a choice of excitation patterns. While exact measurements of ZIi for Ii
in any basis for S is clearly sufficient, the situation is more complicated with measure-
ments of finite precision in the presence of noise. If a redundant set of currents is taken,


Electrical Impedance Tomography
the problem of estimating Z becomes one of multivariate linear regression. The choice of
current patterns is then a design matrix. Another approach seeks the minimum set of cur-
rent patterns that results in usable measurements. Applying each current pattern and taking
a set of measurements takes a finite time, during which the admittivity changes. Without
more sophisticated statistical methods (such as Kalman filters []), there are diminish-
ing returns in applying redundant current patterns. Suppose that the total power V∗ZI is
constrained (we want to keep our patient electrically safe) and the current best estimate of
the admittivity gives a transfer admittance Zcalc, then it is reasonable to apply currents I
such that (Z −Zcalc)I is above the threshold of voltages that can be accurately measured
and modelled. One approach is to choose current patterns that are the right generalized
singular vectors of Z−Zcalc with singular values bigger than an error threshold. The gener-
alized singular values are with respect to the norm ∣∣I∣∣Z := ∣∣ZI∣∣on S and are the extrema
of the distinguishability defined as
∣∣(Z −Zcalc)I∣∣
∣∣I∣∣Z
,
(.)
for I ∈S. These excitation patterns are called “optimal current patterns” [] and can be
calculated from an iterative procedure involving repeated measurement. For circular disk
with rotationally symmetric admittivity and equally spaced identical electrodes, the singu-
lar vectors will be discrete samples of a Fourier basis, and these trigonometric patterns are
a common choice for multiple drive systems using a circular array of electrodes.
.
Uniqueness of Solution
Uniqueness of solution is very important in inverse problems, although when talking to
engineers it is often better to speak of sufficiency of data to avoid confusion. Interestingly
it is generally true that results that show insufficiency of data, which one cannot recover an
unknown function even if an infinite number of measurements of arbitrary precision are
taken, have more impact in applied areas. While there are still unsolved problems in the
uniqueness theory for the EIT inverse problem, there has been considerable progress over
the last three decades and many important questions have been answered. While for an
isotropic real conductivity γ (with certain smoothness assumptions for dimensions n ≥),
γ is uniquely determined by the complete data Λγ (see [], [], []), an anisotropic con-
ductivity tensor is not uniquely determined by the boundary data, although some progress
on what can be determined in this case has been made (see [], [], [], []). Aside from
knowing what can and cannot be determined with ideal data, there are two important ways
the theoretical work has a practical impact. Firstly, in some cases, the proof of uniqueness
of solution suggests a reconstruction algorithm. As we will see for the two-dimensional
case the most effective approach (the so called ¯∂-method) to uniqueness theory has now
been implemented as a fast, practical algorithm. The other is an understanding of the insta-
bility and conditional stability of the inverse problem. This helps us to determine what a
priori information is helpful in reducing the sensitivity of the solution to errors in the data.

Electrical Impedance Tomography 

In A. P. Calderón published a paper with the title “On a inverse boundary value
problem” [], where he addressed the problem of whether it is possible to determine the
conductivity of a body by making current and voltage measurements at the boundary.
It seems that Calderón thought of this problem when he was working as an engineer in
Argentina for the Yacimientos Petroliféros Fiscales (YPF), but it was only decades later
that he decided to publish his results. This short paper is considered the first mathematical
formulation of the problem. For a reprinted version of this manuscript we refer to []. The
authors wish to recall also the work due to Druskin (see [], [], []) which has been
carried on independently from Calderón’s approach and has been devoted to the study of
the problem from a geophysical point of view.
..
The Isotropic Case
...
Calderón’s Paper
Calderón considered a domain Ω in ℝn, n ≥, with Lipschitz boundary ∂Ω. He took γ
be a real bounded measurable function in Ω with a positive lower bound. Let Qγ be the
quadratic form (associated to Λγ) defined by
Qγ(ϕ) = ⟨ϕ, Λγϕ⟩= ∫Ω γ ∣∇u ∣dx,
(.)
where u ∈H(Ω) solves the Dirichlet problem (> .). Physically Qγ(ϕ) is the Ohmic
power dissipated when the boundary voltage ϕ is applied. The bilinear form associated
with Qγ is then obtained by using the polarization identity
Bγ(ϕ, ψ) = 
{Qγ(ϕ + ψ) −Qγ(ϕ) −Qγ(ψ)}
= 
{∫Ω (γ∣∇(u + v)∣−γ∣∇u∣−γ∣∇v∣) dx}
= ∫Ω γ∇u ⋅∇v dx,
(.)
where Lγv = in Ω and v∣∂Ω = ψ ∈H

(∂Ω). Clearly a complete knowledge of any of Λγ,
Qγ and Bγ are equivalent. Calderón considered the “forward” map
Q : γ →Qγ
and proved that Q is bounded and analytic in the subset of L∞(Ω) consisting of functions
γ which are real and have a positive lower bound. He then investigated the injectivity of
the map, and in order to do so, he linearized the problem. He in fact proved the injectivity
of the Fréchet derivative of Q at γ = . Here we will fill in a few details of the linearization
for a general γ. Let u be the solution to (> .) and U = u + w satisfy Lγ+δU = , with
U∣∂Ω = ϕ. The perturbation in potential satisfies w∣∂Ω = , we are considering the Dirichlet
data fixed and investigating how the Neumann data varies when γ is perturbed to γ + δ.


Electrical Impedance Tomography
We have
Lδu + Lγw + Lδw = .
(.)
Now let G : H−(Ω) →H
(Ω) be the Green’s operator that solves the equivalent of
Poisson’s equation for Lγ with zero Dirichlet boundary conditions. That is for g ∈H−(Ω),
LγGg = g and G(g)∣∂Ω = and we have the operator equation
(+ GLδ)w = −GLδu.
(.)
An advantage of using the L∞norm is that it is clear ∣∣Lδ∣∣→in the H(Ω) →H−(Ω)
operator norm as ∣∣δ∣∣∞→. This means we can choose δ small enough that ∣∣GLδ∣∣< (in
the operator norm on H(Ω)) and this ensures that the term in the bracket in (> .) is
invertible and the operator series in
w = −(
∞
∑
k=
(−GLδ)k)u
(.)
is convergent. This proves that the map γ ↦u and hence Q is not just C∞but analytic with
(> .) its Taylor series. We see immediately that the linearization of the map γ ↦Λγ is
Λγ+δϕ = Λγϕ + γ ∂
∂GLδu + δ ∂u
∂
+ O (∣∣δ∣∣
∞).
(.)
A strength of this argument is that it gives the Fréchet derivative in these norms, rather
than just the Gateaux derivative. It is easy to deduce that the Fréchet derivative of Q at γ in
the direction δ is given by
dQ(γ)δ(ϕ) = ∫Ω δ∣∇u∣dx.
(.)
In many practical situations it is more common to fix the Neumann boundary condi-
tions and measure the change in boundary voltage as the conductivity changes. Suppose
Lγu = , Lγ+δU = , w = U −u with
γ∂u/∂= (γ + δ)∂U/∂= g ∈H−/
⋅
(∂Ω),
then a similar argument to the above shows
∫∂Ω wγ ∂u
∂
dx = −∫Ω δ∣∇u∣dx + O (∣∣δ∣∣
∞).
(.)
The polarization identity is often applied to (> .) giving
∫∂Ω wγ ∂v
∂
dx = −∫Ω δ∇u ⋅∇v dx + O (∣∣δ∣∣
∞),
(.)
where Lγv = . This is often used in practice with
γ ∂v
∂
= χEi/∣Ei∣−χE j/∣Ej∣,
(.)
which represents the difference in the characteristic functions of a pair of electrodes. In
the case of the shunt model, this makes the left hand side of (> .) the change in the

Electrical Impedance Tomography 

difference between voltages on that pair of electrodes when the conductivity is perturbed.
The formula (> .) and its relatives are referred to as the Geselowitz Sensitivity Theorem
in the bioengineering literature. With the complete electrode model (> .) still holds,
but with u and v satisfying (> .)[].
We now return to Calderón’s argument: for γ = we have that Lu = ∇u. To prove the
injectivity of dQ() we have to show that if the integral appearing in (> .) vanishes
for all the harmonic functions in Ω, then δ = in Ω. Suppose the integral in (> .)
vanishes for all u ∈H(Ω) such that ∇u = in Ω, then
∫Ω δ∇u ⋅∇v = ,
(.)
whenever ∇u = ∇v = in Ω. For any z ∈ℝn consider a ∈ℝn such that ∣a∣= ∣z∣, a ⋅z = 
and consider the harmonic functions
u(x) = eπi(z⋅x)+π(a⋅x),
v(x) = eπi(z⋅x)−π(a⋅x),
(.)
which is equivalent to choosing
u(x) = ex⋅ρ,
v(x) = e−x⋅¯ρ,
where ρ ∈ℂn with
ρ ⋅ρ = .
Here we use the real dot product on complex vectors ρ ⋅ρ := ρTρ. With the choice made in
(> .), (> .) leads to
π∣z∣∫δ(x)eπi(z⋅x) dx = ,
for each
z,
therefore δ(x) = , for all x ∈Ω. Calderón also observed that if the linear operator dQ()
had a closed range, then one could have concluded that Q itself was injective in a suffi-
ciently small neighborhood of γ=constant. However, conditions on the range of dQ(),
which would allow us to use the implicit function theorem, are either false or not known.
Furthermore, if the range was closed, one could have also concluded that the inverse of
dQ() was a bounded linear operator by the open mapping theorem. Calderón concluded
the paper by giving an approximation for the conductivity γ if
γ = + δ
and δ is small enough in the L∞norm, by making use of the same harmonic functions
(> .). Calderón’s technique is based on the construction of low frequency oscillat-
ing solutions. Sylvester and Uhlmann proved in their fundamental paper [] a result of
uniqueness using high frequencies oscillating solutions of Lγu = . Their solutions are of
type
u(x, ξ, t) = ex⋅ξ γ−
(+ ψ(x, ξ, t)),
which behaves (for high frequencies ξ) in the same way as the solutions used by Calderón.
These oscillating solutions have come to be known as complex geometrical optics (CGO)


Electrical Impedance Tomography
solutions. Before going in to more details of the use of CGO solutions we give an earlier
result using a different approach.
...
Uniqueness at the Boundary
In Kohn and Vogelius [] proved that boundary values, and derivatives at the bound-
ary, of a smooth isotropic conductivity γ could be determined from the knowledge of Qγ.
Their result is given by the following theorem.
Theorem 
Let Ω be a domain in ℝn (n ≥) with smooth boundary ∂Ω. Suppose γi ∈
C∞(Ω), i = ,is strictly positive, and that there is a neighborhood B of some x⋆∈∂Ω so
that
Qγ(f ) = Qγ(f ),
for all f ,
f ∈H

(∂Ω),
supp(f ) ⊂B.
Then
∂∣α∣
∂xα γ(x⋆) = ∂∣α∣
∂xα γ(x⋆),
∀α.
Theorem is a local result in the sense that we only need to know Qγ in a open set
of the boundary in order to determine the Taylor series of γ on that open set. The global
reformulation of this result given in terms of Λγ is given below.
Theorem 
Let γi ∈C∞(Ω), i = ,be strictly positive. If Λ= Λ, then
∂∣α∣
∂xα γ= ∂∣α∣
∂xα γ, on ∂Ω,
∀α.
For a sketch of the proof of Theorem see [, Sketch of proof of Theorem ., p. ].
This result settled the identifiability question in the real-analytic category of conductivities.
Kohn and Vogelius have extended this result to piecewise real-analytic (piecewise constant,
for example) conductivities in []. The proof of this result is based on [] together with
the Runge approximation theorem for solutions of Lγu = .
...
Complex Geometrical Optics Solutions for the Schrödinger
Equation
In , Sylvester and Uhlmann [], [] constructed in dimension n ≥complex geo-
metrical optics solutions in the whole space for the Schrödinger equation with potential
q. Before we state their result, the well known relation between the conductivity equation
and the Schrödinger equation will be derived. This relationship is also important in diffuse
optical tomography (see > Chap. ).

Electrical Impedance Tomography 

Lemma 
Let γ ∈C(Ω) be strictly positive, then we have
γ−
Lγ (γ−
) = ∇−q,
(.)
where
q =
∇(γ

)
γ


.
Proof of Lemma .
Lγu = γ∇u + ∇γ ⋅∇u,
(.)
therefore
γ−
Lγu = γ

∇u + ∇γ ⋅∇u
γ


.
Consider for w = γ

u
∇w −q w = ∇(γ

u) −(∇γ

)u
= ∇⋅(∇(γ

u)) −(∇γ

) u
= ∇⋅((∇γ

u) + γ

(∇u)) −(∇γ

) u
= (∇γ

)u + ∇γ

⋅∇u + γ

∇u −(∇γ

)u
= γ

∇u + ∇γ ⋅∇u
γ


= γ−
Lγu,
which proves (>.).
◻
The term q is usually called the potential of the Schrödinger equation, by analogy with the
potential energy in quantum mechanics, this definition being somehow confusing given
that in EIT u is the electric potential. The results in [], [] state the existence of complex
geometrical optics solutions for the Schrödinger equation with potential q bounded and
compactly supported in ℝn. We cite the result as given in [], which relies on the weighted
Lspace L
δ(ℝn) = {f : ∫
ℝn (+ ∣x∣)δ∣f (x)∣dx}. For δ < this norm controls the “growth
at infinity.” The Sobolev spaces Hk
δ(ℝn) are formed in the standard way from L
δ(ℝn)
Hk
δ(ℝn) = {f ∈W k(ℝn) ∣Dα f ∈L
δ(ℝn),
for all ∣α∣≤k},
where α is a multi-index, Dα f denotes the αth weak derivative of f , and W k(ℝn) is the
set of k times weakly differentiable functions on ℝn.
Theorem 
Let q ∈L∞(ℝn), n ≥, with q(x) = for ∣x∣≥R > and −< δ < . Then
there exists є(δ) and such that for every ρ ∈ℂn satisfying
ρ ⋅ρ = 


Electrical Impedance Tomography
and
∣∣(+ ∣x∣)/q∣∣L∞(ℝn) + 
∣ρ∣
≤є
there exists a unique solution to
(∇−q)u = 
(.)
of the form
u(x, ρ) = ex⋅ρ (+ ψq(x, ρ)),
(.)
with ψq(⋅, ρ) ∈L
δ(ℝn). Moreover ψq(⋅, ρ) ∈H
δ(ℝn) and for ≤s ≤there exists C =
C(n, s, δ) > such that
∣∣ψq(⋅, ρ)∣∣H≤
C
∣ρ∣−s .
(.)
Sketch of the proof of Theorem . Let u be a solution of (> .) of type (> .), then
ψq must satisfy
(∇+ ρ ⋅∇−q)ψq = q.
(.)
The idea is that > Eq. (.) can be solved for ψq by constructing an inverse for (∇+
ρ ⋅∇) and solving the integral equation
ψq = (∇+ ρ ⋅∇)−(q(+ ψq))
(.)
for ψq. For more details about how to solve the above equation we refer to [, Lemma
.] where it is shown that the integral > Eq. (.) can only be solved in L
δ(ℝn)
for large ∣ρ∣.
◻
Other approaches for the construction of complex geometrical optics solutions for the
Schrödinger equation have been considered in [], []. We refer to [] for more details
about references on this topic and a more in-depth explanation about the constructions of
this kind of solutions.
...
Dirichlet-to-Neumann Map and Cauchy Data for the
Schrödinger Equation
If is not a Dirichlet eigenvalue for the Schrödinger equation, then the Dirichlet-to-
Neumann map associated to a potential q can be defined by
˜Λq(f ) = ∂u
∂∣
∂Ω
,
where u solves the Dirichlet problem
{ (∇−q)u = 
in Ω
u∣∂Ω = f .
.

Electrical Impedance Tomography 

As a consequence of Lemma , for any q = ∇γ/
γ/
we have
˜Λq(f ) = ∂
∂(γ

γ−
u)∣∂Ω
= ( ∂γ


∂
(γ−
u) + γ

∂(γ−
u)
∂
)∣∂Ω
= ( 
γ−
∂γ
∂
γ−
+ γ

∂(γ−
u)
∂
)∣∂Ω
= 
(γ−∂γ
∂)∣∂Ω f + γ

∣∂Ω Λγ (γ−
∣∂Ω f ) .
So the two Dirichlet-to-Neumann maps ˜Λq and Λγ are related in the following way
˜Λq(f ) = 
(γ−∂γ
∂)∣∂Ω f + γ

∣∂Ω Λγ (γ−
∣∂Ω f ) ,
(.)
for any f ∈H

(∂Ω). For q ∈L∞(∂Ω) we also define the Cauchy data as the set
Cq = {(u∣∂Ω, ∂u
∂∣∂Ω) ∣u ∈H(Ω),
(∇−q)u = 
in
Ω}.
If is not an eigenvalue of ∇−q, then Cq is the graph given by
Cq = {(f , ˜Λq(f )) ∈H

(∂Ω) × H−
(∂Ω)}.
What we saw so far is very general and holds in any dimension n ≥. We will distin-
guish in the rest of our discussion on the uniqueness of Calderón’s problem between the
higher dimensional case n ≥and the two-dimensional one.
...
Global Uniqueness for n ≥
Sylvester and Uhlmann proved in [] a result of global uniqueness for C(Ω) conduc-
tivities by solving in this way the identifiability question with the following result. Their
result follows in dimension n ≥from a more general one for the Schrödinger equation,
which is useful in its own right for other inverse problems.
Theorem 
Let qi ∈L∞(Ω), i=, . Assume Cq= Cq, then q= q.
Proof of Theorem . This result has been proved by constructing oscillatory solutions of
(∇−qi)ui = in ℝn with high frequencies. We start by stating that the following equality
∫Ω(q−q)uu= 
(.)
is true for any ui ∈H(Ω) solution to
(∇−qi)ui = 
in
Ω,
i = ,.


Electrical Impedance Tomography
Equality (> .) follows by
∫Ω(q−q)uu= ∫∂Ω (∂u
∂
u−u
∂u
∂
) dS,
which can be easily obtained by the divergence theorem. We extend qi on the whole ℝn by
taking qi = on ℝn/Ω and we take solutions of
(∇−qi)ui = 
in
ℝn,
i = ,
of the form
ui = ex⋅ρi (+ ψqi(x, ρi)),
i = ,,
(.)
with ∣ρi∣large. ρi, i = ,is chosen to be of the form
ρ= η
+ i( k + l

)
ρ= −η
+ i( k −l

),
(.)
with η, k, l ∈ℝn and satisfying
η ⋅k = k ⋅k = η ⋅l = ,
∣η∣= ∣k∣+ ∣l∣,
(.)
the choices of η, k, l having been made so that ρi ⋅ρi = , i = ,. With these choices of ρi,
i = ,we have
uu= [ex⋅η
+ix⋅( k+l
) + ex⋅η
+ix⋅( k+l
)ψq] ⋅[e−x⋅η
+ix⋅( k−l
) + e−x⋅η
+ix⋅( k−l
)ψq]
= eix⋅k (+ ψq+ ψq+ ψqψq)
and therefore
̂
(q−q)(−k) = −∫Ω eix⋅k(q−q)(ψq+ ψq+ ψqψq)dx.
(.)
By recalling that
∣∣ψqi∣∣L(Ω) ≤C
∣ρi∣
and letting ∣l∣→∞we obtain q= q(see [, proof of Theorem ., p. ]).
◻
As a consequence of this result we finally obtain result [] stated here below.
Theorem 
Let γi ∈C(Ω), γi strictly positive, i=, . If Λγ= Λγ, then γ= γin Ω.
Theorem has been proved in [] in a straightforward manner by constructing highly
oscillatory solutions to Lγu = in Ω. In this chapter we follow the line of [] in the
exposition of such result as a consequence of the more general Theorem . Such a choice
has been made because of the clearer exposition made in [].

Electrical Impedance Tomography 

We will proceed by showing that Theorem implies Theorem for sake of complete-
ness. The reader can find it also in []. The argument used is the following. Let γi ∈C(Ω)
be strictly positive and Λγ= Λγ. Then by [] we have
γ∣∂Ω = γ∣∂Ω,
∂γ
∂∣
∂Ω
= ∂γ
∂
∣
∂Ω
,
therefore (> .) implies Cq= Cqi.e., q= q=: q because of Theorem . Recall that
qi = ∇γ/
i
γ/
i
,
i = ,,
which leads to
∇γ


−q γ


= 
∇γ


−q γ


= 
i.e.,
∇(γ


−γ


) −q (γ


−γ


) = 
with
(γ


−γ


)∣
∂Ω
= .
Therefore it must be that
γ= γ
in
Ω,
by uniqueness of the solution of the Cauchy problem.
The identifiability question was then pushed forward to he case of γ ∈C, (Ω) with an
affirmative answer by Nachman, Sylvester, and Uhlmann in []. Nachman extended
then this result to domains with C, boundaries (see []). The condition on the boundary
was relaxed to ∂Ω Lipschitz by Alessandrini in in []; he proved uniqueness at the
boundary and gave stability estimates for γ ∈W, p(Ω), with p > n by making use of
singular solutions with an isolated singularity at the centre of a ball. This method enables
one to construct solutions of Lγu = on a ball behaving asymptotically like the singular
solutions of the Laplace–Beltrami equation with separated variables. His results hold in
dimension n ≥. Results of global uniqueness in the interior were also found in [] among
piecewise analytic perturbations of γ, giving an extension of Kohn and Vogelius result in
[] to Lipschitz domains.
...
Global Uniqueness in the Two-Dimensional Case
The two-dimensional inverse conductivity problem must often be treated as a special case.
Although results in [] gave a positive answer to the identifiability question in the case of


Electrical Impedance Tomography
piecewise analytic conductivities, it was not until that Nachman [] proved a global
uniqueness result to Calderón problem for conductivities in W, p(Ω), for some p > .
An essential part of his argument is based on the construction of the complex geometrical
optics solutions and the ¯∂-method (sometimes written “d-bar method”) in inverse scat-
tering introduced in one dimension by Beals and Coifman (see [], []). The result of
[] has been improved in for conductivities having one derivative in an appropriate
sense (see []) and the question of uniqueness was settled in L∞(Ω) finally by Astala and
Päivärinta [] using ¯∂-methods. They proved
Theorem 
Let Ω be a bounded domain in ℝand γi ∈L∞, i = ,be real functions such
that for some constant M, M−< γi < M. Then
Λγ= Λγ7⇒γ= γ.
Let us first explain the complex version of the problem used by []. We use the complex
variable z = x+ix, use the notation ∂= ∂/∂z, ¯∂= ∂/∂¯z. Then we have the following result
[].
Lemma 
Let Ω be the unit disk in the plane and u ∈H(Ω) be a solution of Lγu = .
Then there exists a real function v ∈H(Ω), unique up to a constant, such that f = u + iv
satisfies the Beltrami equation
¯∂f = μ∂f ,
(.)
where μ = (−γ)/(+ γ).
Conversely if f ∈H(Ω) satisfies (> .), with a real valued μ, then u = Re f and
v = Im f satisfy
Lγu = 
and
Lγ−v = ,
(.)
where γ = (−μ)/(+ μ).
Astala and Päivärinta reduce the general case of Ω to that of the disk, and show that the
generalized Hilbert transform Hμ : u∣∂Ω ↦v∣∂Ω uniquely determines, and is determined
by Λγ. They go on to construct CGO solutions to (> .) of the form
fμ(z, k) = eikz (+ O ( 
z)) as ∣z∣→∞
(.)
and using a result connecting pseudoanalytic functions with quasi-regular maps prove that
Hμ determines μ. The original method Nachman used to prove uniquness has resulted in
the development of ¯∂reconstruction methods which are described below (> Sect. ..).
The authors would also like to recall the work of Druskin [] which provides some answers
to the -D geophysical settings.

Electrical Impedance Tomography 

...
Some Open Problems for the Uniqueness
One of the main open problems in dimension n ≥is to investigate whether global unique-
ness holds for the minimal assumption γ ∈L∞(Ω) or else to find what are the minimal
assumptions on γ in order to guarantee uniqueness. We refer to [], [] for the unique-
ness results under the assumptions γ ∈W/,p, with p > n and γ ∈W/,∞, respectively.
These open problems influence of course also the stability issue of finding appropriate
assumptions (possibly on γ) in order to improve the unstable nature of EIT. This issue
will be studied in the next section.
...
Stability of the Solution at the Boundary
The result of uniqueness at the boundary of Theorem has been improved in [] to a
stability estimate. The result is the following.
Theorem 
Let γi ∈C∞(Ω), i = ,, satisfy
< 
E ≤γi ≤E,
i = ,
(.)
∣∣γi∣∣C(Ω) ≤E,
i = ,.
(.)
Given any < σ <

n+, there exists C = C(Ω, E, n, σ) such that
∣∣γ−γ∣∣L∞(∂Ω) ≤C∣∣Λγ−Λγ∣∣∗
(.)
and
∣∣∂γ
∂
−∂γ
∂
∣∣
L∞(∂Ω)
≤C∣∣Λγ−Λγ∣∣σ
∗,
(.)
where ∣∣⋅∣∣∗denotes the norm in the space of bounded linear operators from H

(∂Ω) to
H−
(∂Ω).
This result improves the one of Theorem in the sense that we no longer require that
γ ∈C∞(Ω) to determine γ itself and its derivative at the boundary. We only need γ to
be continuous on Ω to determine the boundary values of γ, where if γ ∈C(Ω) then we
can determine γ and its first derivative on ∂Ω as well. Subsequent results of stability at the
boundary along the same lines have been proved in [], [], [], [], [], and [].
...
Global Stability for n ≥
In Alessandrini [] proved that, in dimension n ≥, under an a priori assumption on
γ of type
∣∣γ∣∣Hs (Ω) ≤E,
for some s > n
+ ,


Electrical Impedance Tomography
γ depends continuously on Λγ with a modulus of continuity of logarithmic type. The result
is stated below.
Theorem 
Let n ≥. Suppose that s > n
and that γi ∈C∞(Ω), i = ,is a conductivity
satisfying
< 
E ≤γi ≤E,
i = ,
(.)
∣∣γi∣∣Hs+(Ω) ≤E,
i = ,.
(.)
Then there exists C = C(Ω, E, n, s) and τ = τ(n, s), with < τ < such that
∣∣γ−γ∣∣L∞(Ω) ≤C (∣log ∣∣Λγ−Λγ∣∣∗∣−τ + ∣∣Λγ−Λγ∣∣∗).
(.)
It has been proved [], [] that a similar stability estimate holds if (> .) is replaced
by
∣∣γi∣∣W,∞(Ω) ≤E,
i = ,.
(.)
Mandache [] proved that logarithmic stability is optimal for dimension n ≥if the
a priori assumption is of the form
∣∣γ∣∣Ck (Ω) ≤E,
(.)
for any finite k = ,,, . . . . One of the main open problems in the stability issue is then to
improve this logarithmic-type stability estimate under some additional a priori condition.
In [] it has been shown that (> .) can be improved to a Lipschitz-type estimate in the
case in which γ is piecewise constant with jumps on a finite number of domains. We refer
to [] for a more in depth discussion about the stability in EIT and open problems in that
regard. A similar estimate to (> .) for the potential case can be found in [].
...
Global Stability for the Two-Dimensional Case
Logarithmic-type stability estimates in dimension n = were obtained by [] and [],
[]. The results obtained in the last require only γ be Hölder continuous of positive
exponent
∣∣γ∣∣Cα (Ω) ≤E,
(.)
for some α, < α ≤.
...
Some Open Problems for the Stability
The main open problem is to improve the logarithmic-type estimate found in [] in any
dimension n ≥. One approach would be to investigate whether the a priori regularity
assumptions (> .) can be further relaxed. On the other hand, since it has been observed
[] that this logarithmic-type of estimate cannot be avoided under any a priori assumption

Electrical Impedance Tomography 

of type (> .) for any finite k = ,,, . . . , it seems natural to think that another direction
to proceed would be the one of looking for different a priori assumptions rather than the
one of type (> .). For a complete analysis of open problems in this area we refer to [].
..
The Anisotropic Case
...
Non-uniqueness
In anisotropic media the conductivity depends on the direction, therefore it is represented
by a matrix γ = (γi j)n
i, j=, which is symmetric and positive definite. Anisotropic conductiv-
ity appears in nature, for example as a homogenization limit in layered or fibrous structures
such as rock stratum or muscle, as a result of crystalline structure or of deformation of an
isotropic material. Let Ω ⊂ℝn be a domain with smooth boundary ∂Ω (a Lipschitz bound-
ary will be enough in most cases). The Dirichlet problem associated in the anisotropic case
takes the form
⎧⎪⎪⎪⎨⎪⎪⎪⎩
n
∑
i,j=
∂
∂x i (γi j
∂u
∂x j ) = 
in
Ω,
u∣∂Ω = f ,
(.)
where f ∈H

(∂Ω) is a prescribed potential at the boundary. The Dirichlet-to-Neumann
map associated to γ is defined by
Λγ f = γ∇u ⋅∣∂Ω,
(.)
for any u solution to (> .). Here γ∇u ⋅
= ∑n
i,j=(γi j
∂u
∂x j )
i and as usual
= ( i)n
i=
is the unit outer normal to ∂Ω. The weak formulation of (> .) is commonly used and
will be given below for sake of completeness.
Deﬁnition 
The Dirichlet-to-Neumann map associated to (> .) is the map
Λγ : H

(∂Ω) →H−
(∂Ω)
given by
⟨Λγ f , η⟩= ∫Ω σ(x)∇u(x) ⋅∇ϕ(x) dx,
(.)
for any f , η ∈H

(∂Ω), u, ϕ ∈H(Ω), ϕ∣∂Ω = η and u is the weak solution to (> .).
A conductor is isotropic when γ = (γi j) is rotation invariant, i.e., when at each point
RTγR = γ,
for all rotations R. This is the case when γ = α I, where α is a scalar function and I the
identity matrix.
We saw in > Sect. ..that the uniqueness problem for the isotropic case can be
considered solved; on the other hands, in the anisotropic case, Λγ does not in general
determine γ. Tartar (see []) observed the following non-uniqueness result.


Electrical Impedance Tomography
Proposition 
If ψ : Ω →Ω is a Cdiffeomorphism such that ψ(x) = x, for each x ∈∂Ω,
then γ and ˜γ = (Dψ)γ(Dγ)T
det(Dψ)
○ψ−have the same Dirichlet-to-Neumann map.
The proof of this result is given below as a tutorial for the first-time reader of this
material.
Let us consider the change of variables y = ψ(x) on the Dirichlet integral
∫Ω γi j(x) ∂u
∂xi
∂u
∂x j dx = ∫Ω ˜γi j(y) ∂˜u
∂yi
∂˜u
∂y j dx,
(.)
where
˜γ(y) = (Dψ) γ(Dψ)T
det(Dψ)
○ψ−(y)
and
˜u(y) = u ○ψ−(y).
Notice that the solution u of the Dirichlet problem
{ ∇⋅γ∇u = 
inΩ
u∣∂Ω = f
minimizes the integral appearing on the left hand side of (> .), therefore ˜u = u ○ψ−
minimizes the Dirichlet integral appearing on the right hand side of the same. One can
then conclude that ˜u solves
{ ∇⋅(˜γ∇˜u) = 
in
Ω
˜u∣∂Ω = ˜f = u ○ψ−.
Let us consider now the solution v of
{ ∇⋅(γ∇v) = 
in
Ω
v∣∂Ω = g
and let ˜v be obtained by v by the change of variable, therefore ˜v solves
{ ∇⋅(˜γ∇˜v) = 
in
Ω
˜v∣∂Ω = ˜g = g ○ψ−.
By the change of variables in the Dirichlet integrals we get
∫Ω γi j
∂u
∂xi
∂v
∂x j dx = ∫Ω ˜γi j
∂˜u
∂yi
∂˜v
∂y j dy,
which can be written as
∫Ω γ∇u ⋅∇v dx = ∫Ω ˜γ∇˜u ⋅∇˜v dy,
which is equivalent to
∫Ω ∇⋅(vγ∇u) dx −∫Ω v∇⋅(γ∇u) dx = ∫Ω ∇⋅(˜v˜γ∇˜u) dy −∫Ω ˜v∇⋅(˜γ∇˜u) dy

Electrical Impedance Tomography 

and by the divergence theorem
∫∂Ω vγ∇u ⋅
ds = ∫∂Ω ˜v˜γ∇˜u ⋅
ds,
but ˜v = v ○ψ−= v = g and ˜u = u ○ψ−= u = f at the boundary ∂Ω, then
∫∂Ω gΛγ(f ) ds = ∫∂Ω gΛ ˜γ(f ) ds
then Λγ = Λ ˜γ.
◻
Since Tartar’s observation has been made, different lines of research have been pursued.
One direction was to prove the uniqueness of γ up to diffeomorphisms that fix the bound-
ary, whereas the other direction was to study conductivities with some a priori information.
The first direction of research is summarized in what follows.
...
Uniqueness up to Diﬀeomorphism
The question here is to investigate whether Tartar’s observation is the only obstruction to
unique identifiability of the conductivity. We start by observing that the physical problem
of determining the conductivity of a body is closely related to the geometrical problem
of determining a Riemannian metric from its Dirichlet-to-Neumann map for harmonics
functions [].
Let (M, g) be a compact Riemannian manifold with boundary. The Laplace–Beltrami
operator associated to the metric g is given in local coordinates by
Δg :=
n
∑
i j = 
(det g)−
∂
∂xi {(det g)

gi j ∂u
∂x j }.
The Dirichlet-to-Neumann map associated to g is the operator Λg mapping functions
u∣∂M ∈H/(∂M) into (n −)-forms Λσ(u∣∂M) ∈H−/(Ωn−(∂M))
Λg(f ) = i⋆(∗gdu),
(.)
for any u ∈H(M) solution to Δgu = in M, with u∣∂M = f . Here i is the inclusion
map i : ∂M →M and i⋆denotes the pull-back of forms under the map i. In any local
coordinates (> .) becomes
Λg(f ) =
n
∑
i,j=
i gi j ∂u
∂x j
√
det g∣∂M.
(.)
The inverse problem is to recover g from Λg. In dimension n ≥, the conductivity γ
uniquely determines a Riemannian metric g such that
γ = ∗g,
(.)


Electrical Impedance Tomography
where ∗g is the Hodge operator associated the the metric g mapping -forms on M into
(n −)-forms (see [], [], []). In any local coordinates (> .) becomes
(gi j) = (detγkl)

n−(γi j)
and
(γi j) = (det gkl)

(gi j),
(.)
where (gi j), (γi j) denotes the matrix inverse of (gi j) and (γi j), respectively. It has been
shown in [] that if M is a domain in ℝn, then for n ≥
Λg = Λγ.
(.)
In dimension n ≥if ψ is a diffeomorphism of M that fixes the boundary, we have
Λψ∗g = Λg,
(.)
where ψ∗g is the pull-back of g under ψ. For the case n = , the situation is different as the
two-dimensional conductivity determines a conformal structure of metrics under scalar
field, i.e., there exists a metric g such that γ = φ∗g, for a positive function φ. Therefore in
n = , if ψ is a diffeomorphism of M that fixes the boundary, we have
Λφψ∗g = Λg,
(.)
for any smooth positive function such that φ∣∂M = . It seems natural to think that (> .)
and (> .) are the only obstructions to uniqueness for n ≥and n = , respectively. In
, Lee and Uhlmann [] formulated the following two conjectures.
Conjecture 
Let M be a smooth, compact n-manifold, with boundary, n ≥and let g, ˜g
be smooth Riemannian metrics on M such that
Λg = Λ ˜g.
Then there exists a diffeomorphism ψ : M →M with ψ∣∂M = Id, such that g = ψ⋆˜g.
Conjecture 
Let M be a smooth, compact -manifold with boundary, and let g, ˜g be
smooth Riemannian metrics on M such that
Λg = Λ ˜g.
Then there exists a diffeomorphism ψ : M →M with ψ∣∂M = Id, such that ψ⋆˜g is a conformal
multiple of g, in other words there exists ϕ ∈C∞(M) such that
ψ⋆˜g = ϕ g.
Conjecture has been proved in [] in a particular case. The result is the following.
Theorem 
Let M be a compact, connected, real-analytic n-manifold with connected real-
analytic boundary, and assume that π(M, ∂M) = (this assumption means that every

Electrical Impedance Tomography 

closed path in M with base point in ∂M is homotopic to some path that lies entirely in ∂M).
Let g and ˜g be real-analytic metrics on M such that
Λg = Λ ˜g,
and assume that one of the following conditions holds:
. M is strongly convex with respect to both g and ˜g
. Either g or ˜g extends to a complete real-analytic metric on a non-compact real-analytic
manifold ˜M (without boundary) containing M
Then there exists a real-analytic diffeomorphism ψ : M →M with ψ∣∂M = Id, such that
g = ψ⋆˜g.
Theorem has been proved by showing that one can recover the full Taylor series of
the metric at the boundary from Λg. The diffeomorphism ψ is then constructed by analytic
continuation from the boundary. As we previously mentioned the full Taylor series of γ was
recovered by Kohn and Vogelius in [] from the knowledge of Λγ in the isotropic case and
then a new proof was given in [] by showing that the full symbol of the pseudodifferential
operator Λγ determines the full Taylor series of γ at the boundary. In [] a simpler method
suggested by R. Melrose consisting of factorizing Δg, is used. In Sylvester proved in
[] Conjecture in a particular case. His result is the following.
Theorem 
Let Ω be a bounded domain in ℝwith a Cboundary and let γ, γbe
anisotropic Cconductivities in Ω such that
∥log (det γi) ∥C< ε (M, Ω),
f or
i = , ,
(.)
with M ≥∥γi ∥C, for i=, and ε(M, Ω) sufficiently small. If
Λγ= Λγ,
then there exists a Cdiffeomorphism ψ of Ω such that ψ∣∂Ω = Id and such that
ψ⋆γ= γ.
Nachman [] extended this result in by proving the same theorem but remov-
ing the hypothesis (> .). In Lassas and Uhlmann [] extended the result of
[]. They assumed that the Dirichlet-to-Neumann map is measured only on a part of
the boundary which is assumed to be real-analytic in the case n ≥and C∞-smooth in
the two-dimensional case. The metric is here recovered (up to diffeomorphism) and the
manifold is reconstructed. Since a manifold is a collection of coordinate patches, the idea
is to construct a representative of an equivalent class of the set of isometric Riemannian
manifolds (M, g). Let us recall that if Γ is an open subset of ∂M, we define
Λg,Λ(f ) = Λg(f )∣Γ,
for any f with suppf ⊆Γ. The main result of [] is given below.


Electrical Impedance Tomography
Theorem 
Let us assume that one of the following conditions is satisfied:
. M is a connected Riemannian surface;
. n ≥and (M, g) is a connected real-analytic Riemannian manifold and the boundary
∂M is real-analytic in the non-empty set Γ ⊂∂M.
Then
. For dim M = the Λg, Γ-mapping and Γ determine the conformal class of the Riemannian
manifold (M, g).
. For a real-analytic Riemannian manifold (M, g), dim M > which boundary is real
analytic in Γ, the Λg, Γ-mapping and Γ determine the Riemannian manifold (M, g).
This result improved the one in [] also because here the only assumption on the topol-
ogy of the manifold is the connectedness, while in [] the manifold was simply connected
and the boundary of the manifold was assumed to be geodesically convex. Theorem has
been extended in [] to a completeness hypothesis on M.
...
Anisotropy which is Partially a Priori Known
Another approach to the anisotropic problem is to assume that the conductivity γ is A Pri-
ori known to depend on a restricted number of unknown spatially dependent parameters.
In Kohn and Vogelius (see []) considered the case where the conductivity matrix
γ = (γi j) is completely known with the exception of one eigenvalue. The main result is the
following.
Theorem 
Let γ, ˜γ be two symmetric, positive definite matrices with entries in L∞(Ω),
and let {γi}, {˜γi} and {ei}, {˜ei} be the corresponding eigenvalues and eigenvectors. For
x∈∂Ω, let B be a neighborhood of xrelative to Ω, and suppose that
γ, ˜γ ∈C∞(B);
(.)
∂Ω ∩B
is
C∞;
(.)
e j = ˜e j,
λj = ˜λj
in
B,
f or
≤j ≤n −;
(.)
en(x) ⋅(x) ≠.
(.)
If
Qγ(ϕ) = Q ˜γ(ϕ)
f or every
ϕ ∈H

(∂Ω),
with supp ϕ ⊂B ∩∂Ω, then
Dk ˜λn(x) = Dkλn(x),
for every k = (k, . . . ,kn), ki ∈ℤ+, i = . . . n.

Electrical Impedance Tomography 

In , Alessandrini [] considered the case in which γ is a priori known to be of type
γ(x) = A(a(x)),
where t →A(t) is a given matrix-valued function and a = a(x) is an unknown scalar func-
tion. He proved results of uniqueness and stability at the boundary and then uniqueness
in the interior among the class of piecewise real-analytic perturbations of the parameter
a(x). The main hypothesis he used is the so-called monotonicity assumption
DtA(t) ≥C I,
where C > is a constant. In , Lionheart [] proved that the parameter a(x) can be
uniquely recovered for a conductivity γ of type
γ(x) = a(x) A(x),
where A(x) is given. Results in [] have been extended in by Alessandrini and
Gaburro [] to a class of conductivities
γ(x) = A(x, a(x)),
where A(x, t) is given and satisfies the monotonicity condition with respect to the
parameter t
DtA(x, t) ≥C I,
where C > is a constant (see [] or [] for this argument). In [] the authors improved
results of [] since they relaxed the hypothesis on A(⋅, t) for the global uniqueness in the
interior and the result there obtained can be applied to [] as well. The technique of []
can also be applied to the so called one-eigenvalue problem introduced in []. Results of
[] have been recently extended to manifolds [] and to the case when the local Dirichlet-
to-Neumann map is prescribed on an open portion of the boundary [].
..
Some Remarks on the Dirichlet-to-Neumann Map
...
EIT with Partial Data
In many applications of EIT, one can actually only take measurements of voltages and cur-
rents on some portion of the boundary. In such situation the Dirichlet-to-Neumann map
can only be defined locally.
Let Ω ⊆ℝn be a domain with conductivity γ. If Γ is a non-empty open portion of ∂Ω,
we shall introduce the subspace of H

(∂Ω)
H

co(Γ) = {f ∈H

(∂Ω) ∣supp f ⊂Γ}.
(.)
Deﬁnition 
The local Dirichlet-to-Neumann map associated to γ and Γ is the operator
ΛΓ
γ : H

co(Γ) →(H

co(Γ))
∗
(.)


Electrical Impedance Tomography
defined by
⟨ΛΓ
γ f , η⟩= ∫Ω γ∇u ⋅∇ϕ(x) dx,
(.)
for any f , η ∈H

co(Γ), where u ∈H(Ω) is the weak solution to
{ ∇⋅(γ(x)∇u(x)) = ,
in
Ω,
u = f ,
on
∂Ω,
and ϕ ∈H(Ω) is any function such that ϕ∣∂Ω = η in the trace sense.
Note that, by (> .), it is easily verified that ΛΓ
σ is self adjoint. The inverse problem
is to recover γ from ΛΓ
γ.
The procedure of reconstructing the conductivity by local measurements has been stud-
ied first by Brown [], where the author gives a formula for reconstructing the isotropic
conductivity pointwise at the boundary of a Lipschitz domain Ω without any a priori
smoothness assumption of the conductivity. Nakamura and Tanuma [] give a formula for
the pointwise reconstruction of a conductivity continuous at one point xof the boundary
from the local D-N map when the boundary is Cnear x. Under some additional regular-
ity hypothesis, the authors give a reconstruction formula for the normal derivatives of γ on
∂Ω at x∈∂Ω up to a certain order. A direct method for reconstructing the normal deriva-
tive of the conductivity from the local Dirichlet-to-Neumann (D-N) map is presented in
[]. The result in [] has been improved by Kang and Yun [] to an inductive recon-
struction method by using only the value of γ at x. The authors derive here also Hölder
stability estimates for the inverse problem to identify Riemannian metrics (up to isometry)
on the boundary via the local D-N map. An overview on reconstructing formulas of the
conductivity and its normal derivative can be found in [].
For related uniqueness results in the case of local boundary data, we refer to Alessan-
drini and Gaburro [], Bukhgeim and Uhlmann [], Kenig, Sjöstrand and Uhlmann [],
and Isakov [], and, for stability, [] and Heck and Wang []. Results of stability for cases
of piecewise constant conductivities and local boundary maps have also been obtained by
Alessandrini and Vessella [] and by Di Cristo []. We also refer to [, Sect. ].
...
The Neumann-to-Dirichlet Map
In many applications of EIT especially in medical imaging, rather than the local Dirichlet-
to-Neumann map, one should consider the so-called local Neumann-to-Dirichlet (N-D)
map. That is, the map associating to specified current densities supported on a portion
Γ ⊂∂Ω the corresponding boundary voltages, also measured on the same portion Γ of
∂Ω. Usually electrodes are only applied to part of the body, and in geophysics of course we
have an extreme example where Γ is a small portion of the surface of the earth Ω. It seems
appropriate at this stage to recall the definition of the N-D map and its local version for
sake of completeness [].

Electrical Impedance Tomography 

Let us introduce the following function spaces (see [])
H

◇(∂Ω) = {ϕ ∈H

(∂Ω)∣∫∂Ω ϕ = },
H
−

◇(∂Ω) = {ψ ∈H−
(∂Ω)∣⟨ψ, ⟩= }.
Observe that if we consider the (global) D-N map Λγ, that is the map introduced in
(> .) ΛΓ
γ in the special case when Γ = ∂Ω, we have that, it maps onto H
−

◇(∂Ω), and,
when restricted to H

◇(∂Ω), it is injective with bounded inverse. Then we can define the
global Neumann-to-Dirichlet map as follows.
Deﬁnition 
The Neumann-to-Dirichlet map associated to γ, Nγ :
H
−

◇(∂Ω) →
H

◇(∂Ω) is given by
Nγ = (Λγ∣
H


◇(∂Ω))
−
.
(.)
Note that Nγ can also be characterized as the self adjoint operator satisfying
⟨ψ, Nγψ⟩= ∫Ω γ(x)∇u(x) ⋅∇u(x) dx,
(.)
for every ψ ∈H
−

◇(∂Ω), where u ∈H(Ω) is the weak solution to the Neumann problem
⎧⎪⎪⎪⎨⎪⎪⎪⎩
Lγu = ,
in
Ω,
γ∇u ⋅∣∂Ω = ψ,
on
∂Ω,
∫∂Ω u = .
(.)
We are now in position to introduce the local version of the N-D map. Let Γ be an open
portion of ∂Ω and let Δ = ∂Ω/Γ. We denote by H


(Δ) the closure in H

(∂Ω) of the space
H

co(Δ) previously defined in (> .) and we introduce
H
−

◇(Γ) = {ψ ∈H
−

◇(∂Ω)∣⟨ψ, f ⟩= ,
for any f ∈H


(Δ)},
(.)
that is, the space of distributions ψ ∈H−
(∂Ω) which are supported in ¯Γ and have zero
average on ∂Ω. The local N-D map is then defined as follows.
Deﬁnition 
The local Neumann-to-Dirichlet map associated to γ, Γ is the operator N Γ
γ :
H
−

◇(Γ) →(H
−

◇(Γ))
⋆
⊂H

◇(∂Ω) given by
⟨N Γ
γ i, j⟩= ⟨Nγ i, j⟩,
(.)
for every i, j ∈H
−

◇(Γ).


Electrical Impedance Tomography
.
The Reconstruction Problem
..
Locating Objects and Boundaries
The simplest form of the inverse problem is to locate a single object with a conductivity con-
trast in a homogeneous medium. Some real situations approximate this, such as a weakly
electric fish locating a single prey or the location of an insulating land mine in homo-
geneous soil. Typically the first test done on an EIT system experimentally is to locate a
cylindrical or spherical object in a cylindrical tank. Linearization about γ = simplifies to
∇w = −∇δ ⋅∇u + O (∣∣δ∣∣
L∞).
(.)
We see the disturbance in the potential w is, to first order, the solution of Poisson’s equation
with a dipole source centred on the object oriented in the direction of the unperturbed elec-
tric field. With practice experienced experimenters (like electric fish) can roughly locate
the object from looking at a display of the voltage changes. When it is known a priori that
there is single object, either small or with a known shape, the reconstruction problem is
simply fitting a small number of model parameters (e.g., position, diameter, conductivity
contrast) to the measured voltage data, using an analytical or numerical forward solver.
This can be achieved using standard nonlinear optimization methods. For the two dimen-
sional case a fast mathematically rigorous method of locating an object (not required to be
circular) from one set of Cauchy data is presented by Hanke [].
In the limiting case where the object is insulating, object location becomes a free
boundary problem, where the Dirichlet to Neumann map is known on the known bound-
ary and only zero Neumann data known on the unknown boundary. This is treated
theoretically for example by [] and numerically in []. A practical example is the
location of the air core of a hydrocyclone, a device used in chemical engineering [].
In more complex cases the conductivity may be piecewise constant with a jump dis-
continuity on a smooth surface. In that case there are several methods that have been
tested at least on laboratory data for locating the surface of the discontinuity. One would
expect in general that the location of a surface can be achieved more accurately or with less
data than the recovery of a spatially varying function and this is confirmed by numerical
studies.
A natural method of representing the surface of discontinuity is as a level set of a
smooth surface (see > Chap. ). This approach has the advantage that no change in param-
eterisation is required as the number of connected components changes, in contrast for
example to representing a number of star-shaped objects using spherical polar coordinates.
The approach to using the level set method in EIT is exactly the same as its use in scatter-
ing problems apart from the forward problem (> Chap. ). Level set methods have been
tested on experimental ERT and ECT data by Soleimani et al. [] and we reproduce some
of their results in > Fig. -and we use some of their results in > Fig. -.
Another approach to locating a discontinuity, common with other inverse boundary
value problems for PDEs are “sampling and probe methods” in which a test is performed

Electrical Impedance Tomography 

a
b
c
d
e
f
⊡Fig. -
Comparison of level set reconstruction of D experimental data compared to generalized
Tikhonov regularization using a Laplacian smoothing matrix (EIDORS-D []). Thanks to
Manuchehr Soleimani for reconstruction results, and Wu Quaing Yang and colleagues for
the ECT data [], the experimental ERT data was from []. Level set reconstruction (c)
from experimental ERT data for high contrast objects (a) compared with generalized
tikhonov regularization. Level set reconstruction (f) from experimental ECT data for a pipe
(d) compared with generalized tikhonov regularization.
at each point in a grid to determine if that point is in the object. Linear sampling and factor-
ization methods are treated in > Chap. . Theory and numerical results for the application
of Linear Sampling to ERT for a half space are given by Hanke and Schappel[]. Sampling
methods generally require the the complete transfer impedance matrix and where only
incomplete measurements are available they must be interpolated.
Also in the spirit of probe methods is the monotonicity method of Tamburrino and
Rubinacci []. This method follows from the observation that for γ real the map γ ↦Zγ
is monotone in the sense that γ≤γ⇒Zγ−Zγ≥, where a matrix Z ≥if its eigen-
values are non-negative. Suppose that for some partition {Ωi} of Ω (for example pixels or
voxels)
γ = ∑
i
γi χΩi
(.)


Electrical Impedance Tomography
and each γi ∈{m, M}, < m < M. For each i let Zm
i be the transfer impedance for
a conductivity that is M on Ωi and m elsewhere. Now suppose Z −Zm
i has a negative
eigenvalue, then we know γi = m. For each set in the partition the test is repeated, and it
is inferred that some of the conductivity values are definitely m, the equivalent procedure
is repeated for each ZM
i . In practice, for large enough sets in the partition and M −m big
enough, most conductivity values in the binary image are determined, although this is not
guaranteed. In practice the method is very fast as Zm
i and ZM
i can be precomputed and one
only needs to find the smallest eigenvalue of two modestly sized matrices for each set in the
partition. In the presence of noise, of course, one needs a sufficiently negative eigenvalue
to be sure of the result of the test, and the method does assume that the conductivity is of
the given form (> .). If conductivies on some sets are undetermined they can then be
found using other methods. For example [] use a Markov Chain Monte Carlo method
to determine the expected value and variance of undetermined pixels in ECT.
..
Forward Solution
Most reconstruction algorithms for EIT necessitate solution of the forward problem, that
is, to predict the boundary data given the conductivity. In addition, methods that use lin-
earisation typically require electric fields in the interior. The simplest case is an algorithm
that uses a linear approximation calculated at a homogenous background conductivity.
For simple geometries this might be done using an analytical method, while for arbitrary
boundaries Boundary Element Method is a good choice, and is also suitable for the case
where the conductivity is piecewise constant with discontinuities on smooth surfaces. For
general conductivities the choice is between finite difference, finite volume, and finite ele-
ment methods. All have been used in EIT problems. Finite element method (FEM) has the
advantage that the mesh can be adapted to a general boundary surface and to the shape and
location of electrodes, whereas regular grids in finite difference/volume methods can result
in more efficient computation, traded off against the fine discretization needed to repre-
sent irregular boundaries. One could also use a hybrid method such as finite element on
a bounded domain of variable conductivity coupled to BEM for a homogeneous (possibly
unbounded) domain.
In reconstruction methods that iteratively adjust the conductivity and re-solve the
forward problem, a fast forward solution is needed, whereas in methods using a lin-
ear approximation, the forward solution can be solved off-line and speed is much less
important.
The simplest, and currently in EIT the most widely used, FE method is first order tetra-
hedral elements. Here a polyhedral approximation Ωh to Ω is partitioned in to a finite set
of tetrahedra Tk, k = , . . . , nt which overlap at most in a shared face, and with vertices
xi, i = < ..nv. The potential is approximated as a sum
uh(x) = ∑uiϕi(x),
(.)

Electrical Impedance Tomography 

where the ϕi are piecewise linear continuous functions with ϕi(xj) = δi j. The finite
element system matrix K ∈ℂnv×nv is given by
Ki j = ∫
Ωp
γ∇ϕi ⋅∇ϕj dx.
(.)
On each tetrahedron ∇ϕi is constant, which reduces calculation of (> .) in the
isotropic case to the mean of γ on each tetrahedron. One then chooses an approximation
to the conductivity in some space spanned by basis functions ψi(x)
γ = ∑
i
γiψi.
(.)
One can choose these functions to implement some a priori constraints such as smooth-
ness and to reduce the number of unknowns in the discrete inverse problem. Or one can
choose basis functions just as the characteristic functions of the tetrahedra, which makes
the calculation, and updating, of the system matrix very simple. In this case, all a priori
information must be incorporated later, such as by a regulariziation term. In general the
integrals
∫
Ωp
ψl∇ϕi ⋅∇ϕj dx
(.)
are evaluated using quadrature if they cannot be done explicitly. If the inverse solution uses
repeated forward solutions with updated conductivity but with a fixed mesh, the coeffi-
cients (> .) can be calculated once for each mesh and stored. For a boundary current
density j = γ∇u ⋅, we define the current vector Q ∈ℝnv by
qi = ∫
∂Ω
jϕi dx,
(.)
and the FE system is
Ku = Q,
(.)
where u is the vector of ui. One additional condition is required for a unique solution, as
the voltage is only determined up to an additive constant. One way to do this is to choose
one (“grounded”) vertex ig and enforce uig = by deleting the ig row and column from
the system (> .). It is clear from (> .) that for a pair of vertices indexed by i, j
that are not both in any tetrahedron, Ki j = . The system (> .) is equivalent to Ohm’s
and Kirchoff’s law for a resistor network with resistors connecting nodes i and j when
the corresponding vertices in the FE mesh share and edge (where some dihedral angles
are obtuse we must allow the possibility of negative conductances). It is worth noting that
whatever basis is used to represent the approximate conductivity (including an anisotropic
conductivity), the finite element system has only one degree of freedom per edge and we
cannot hope, even with perfect data and arithmetic, to recover more than ne (the number
of edges) unknowns from our discretization of the inverse problem.
The above formulation implements the shunt model. The Complete Electrode Model
(CEM) with specified currents can be implemented following Vauhkonen [] using an


Electrical Impedance Tomography
augmented matrix. We define
K○
i j = Ki j +
L
∑
l=

zl ∫El
ϕiϕjdx,
(.)
where, here, ∣El∣denotes the area of the lth electrode, and
K∂
ℓℓ= 
zℓ
∣Eℓ∣
for
ℓ= , . . . , L,
(.)
K○∂
iℓ= −∫El l

zℓ
ϕi dx
i = , . . . , n, ℓ= , . . . , L.
(.)
The system matrix for the CEM, KCEM ∈ℂ(nv+L)×(nv+L) is
KCEM = [
K○
K○∂
K○∂T
K∂].
(.)
In this notation, the linear system of equations has the form
KCEM˜u = ˜Q,
(.)
where ˜u = (u, . . . ,unv, V, . . . VL)T and ˜Q = (, . . . ,, I, . . ., IL)T. The constraint V ∈S
(see > Sect. ..) is often used to ensure uniqueness of solution. The transfer impedance
matrix is obtained directly as
Z = (K∂−K○∂TK○†K○∂)
†
(.)
although it is usual to solve the system (> .) as u in the interior is used in the calcula-
tion of the linearization. This formulation should only be used for reasonably large zℓ, as
small zℓwill result in the block K∂dominating the matrix. For an accurate forward model
it is necessary to estimate the contact impedance accurately. This is more important when
measurements from current carrying electrodes are used in the reconstruction, or when
the electrodes are large (even if they are “passive” Iℓ= ). The CEM boundary condition
is rather unusual and most commercial FE systems will not include the boundary condi-
tion easily. This is one of the reasons forward solution code for EIT is generally written
specifically for the purpose, such as the EIDORS project []. It is possible to calculate the
transadmitance matrix Y = Z† more easily with standard solvers. One sets Robin bound-
ary u + zℓγ∂u/∂
= Vℓon each Eℓand the zero Neumann condition (> .) using V
forming a basis for S, one then takes the integral of the current over each electrode as the
current I = YV. For a given current pattern I one applies the Robin conditions V = Y†I
and the solver gives the correct u. Advantages of commercial solver are that they might
contain a wide variety of element types, fast solvers, and mesh generators. Disadvantages
are that they may be hard to integrate as part of a nonlinear inverse solver, and it might be
harder to calculate the linearization efficiently.
In fact implementing code to assemble a system matrix is quite straightforward; much
harder for EIT is the generation of three dimensional meshes. For human bodies with irreg-
ular boundaries of inaccurately known shape this is a major problem. To apply boundary

Electrical Impedance Tomography 

conditions accurately without overfine meshes it is also important that the electrodes are
approximated by unions of faces of the elements. While the accuracy of the finite element
method is well understood in terms of the error in the solution u, in EIT we require that
the dependence of the boundary data on the conductivity is accurate, something that is not
so well understood. In addition, if the conductivities vary widely, it may be necessary to
remesh to obtain the required accuracy, and ideally this capability with be integrated with
the inverse solver [].
..
Regularized Linear Methods
Methods based on linearization are popular in medical and process versions of EIT. The
reasons are twofold. Process and medical applications benefit from very rapid data acqui-
sition times with even early systems capable measuring a transfer impedance matrix in less
that .s, and it was often required to produce an image in real time. The application of a
precomputed (reguarized) inverse of the linearized forward problem required only about

L(L−)floating point operations. For reasons of both speed and economy, early systems
also assumed a two dimensional object with a single ring of electrodes arranged in a plane.
The second reason for using a linear approximation is that in medical applications espe-
cially there is uncertainty in the body shape, electrode position, and contact impedance.
This means that a computed forward solution, based on an assumed conductivity (typ-
ically constant), has a much larger error than the errors inherent in the measurements.
A compromise called difference imaging (by contrast to absolute imaging) uses a forward
solution to calculate the linearization (> .) and then forms an image of the difference
of the conductivities between two different times, for example inspiration and expiration
in a study of the lungs. Alternatively, measurements can be taken simultaneously at two
frequencies and a difference image formed of the permittivity.
Given a basis of applied current patterns Ii and a chosen set of measurements Mi
expressed as a set of independent vectors in S that are /∣El∣for one electrode El, −/∣Ek∣for
another electrode Ek (the two electrodes between which we measure the voltage), and a set
of functions ψi with our approximate admittivity satisfying ˜γ = ∑γkψk, the discretization
of the Fréchet derivative is the Jacobian matrix
J(i j)k =
∂
∂γk
MT
i ZIj = −∫Ω ϕk∇vi ⋅∇u j dx,
(.)
where L ˜γui = L ˜γv j = (at least approximately) with ui satisfying the CEM with current Ii
and v j with Mj. If the finite element approximation is used to solve the forward problem,
it has the interesting feature that the natural approximation to the Fréchet derivative in
the FE context coincides with the Fréchet derivative of the FE approximation. The indices
(ij) are bracketed together as they are typically “flattened” so the matrix of measurements
becomes a vector and J a matrix (rather than a tensor). Let ˜V be the vector of all voltage


Electrical Impedance Tomography
measurements, ˜Vcalc the calculated voltages, and γ the vector of γi. Our regularized least-
squares version of the linearized problem is now
γreg = arg min
γ
∣∣Jγ −( ˜V −˜Vcalc)∣∣+ αΨ(γ),
(.)
where Ψ is a penalty function and α a regularization parameter. The same formulation is
used for difference imaging where ˜Vcalc is replaced by measured data at a different time or
frequency. Typical choices for Ψ are quadratic penalties such a weighted sum of squares
of the γi, the two-norm of (a discretization) a partial differential operator R applied to
γ −γ, for some assumed background γ. ∣∣R(γ −γ)∣∣. Another common choice is a
weighted sum of squares, i.e., L a positive diagonal matrix. In Total Variation regulariza-
tion Ψ approximates ∣∣∇(γ −γ)∣∣, and can be used where discontinuities are expected
in the conductivity. Where there is a jump discontinuity on a surface (a curve in the two-
dimensional case) the total variation is the integral of the absolute value of the jump over
the surface (curve). The choice of regularization parameter α, the choice of penalty func-
tion, and the solution methods are covered in > Chaps. and > (see also > Fig. -).
The singular values of J are found to decay faster than exponentially (see > Fig. -), so it
is a severely illconditioned problem and regularization is needed even for very accurate data.
There are also to some extent diminishing returns in increasing the number of electrodes
without also increasing the measurement accuracy.
For a quadratic penalty function the minimization problem with Ψ(γ) = ∣∣R(γ −γ)∣∣
the solution to (> .) is given by the well-known Tikhonov inversion formula
γreg −γ= (J∗J + αR∗R)
−J∗(˜V −˜Vcalc).
(.)
For a total variation penalty Ψ(γ) = ∣∣R(γ−γ)∣∣minimization is more difficult, and stan-
dard gradient based optimization methods have difficulty with the singularity in Ψ where
a component of Rγ vanishes. One way around this is to use the Primal Dual Interior Point
Method; for details see [], and for comparison of TV and a quadratic penalty applied to
a difference image of the chest, see > Figs. -and > -.
..
Regularized Iterative Nonlinear Methods
As the problem is nonlinear, clearly a solution based on linearization is inaccurate. Intu-
itively there are two aspects to the nonlinearity that are lost in a linear approximation. If
one considers an object of constant conductivity away from the boundary the norm of the
voltage data will exhibit a sigmoid curve as the conductivity of that object varies, as seen
in the example of a concentric anomaly and illustrated numerically in
> Fig. -. This
means that voltage measurements saturate, or tends to a limiting value, as the conductivity
contrast to the background tends to zero or infinity. Typically this means that linear approx-
imations underestimate conductivity contrast. One has to be carefull in communications
between mathematicians and engineers: the latter will sometimes take linearity (e.g., of
Y(γ)) to mean a function that is homogenous of degree one, ignoring the requirement for

Electrical Impedance Tomography 

Generalized tikhonov
Total variation
a
b
⊡Fig. -
Time diﬀerence EIT image of a human thorax during breathing, comparison of generalized
Tikhonov ∣∣Rγ∣∣
and of the TV ∣∣Rγ∣∣regularized algorithms. Both are represented on the
same color scale and in arbitrary conductivity units. (a) Generalized Tikhonov and (b) Total
variation see [] for details
1
2
5
10
20
30
50
0.2
0.3
0.4
0.5
1
1.5
2
⊡Fig. -
An “L-curve": data mismatch ∣∣V −Vcalc(γ)∣∣(vertical) versus regularization norm ∣∣R(γ −γ)∣∣
(horizontal) for a range of orders of magnitude of the regularization. In each case, a single
step of the iterative solution was taken. Three representative images are shown illustrating
the “overregularization,”appropriateregularization, and “underregularization."The data are
from the RPI chest phantom [] shown left
“superposition of solutions.” If we consider two small spherical objects in a homogeneous
background we know from (> .) that to first order the change in u due to the objects
is approximately the sum of two dipole fields. The effect of nonlinearity, the higher order
terms in (> .) can be thought of as interference between these two fields, analogous
to higher order scattering in wave scattering problems. The practical effect is that linear
approximations are not only poor at getting the correct conductivity contrast, but also poor


Electrical Impedance Tomography
at resolving a region between two objects that are close together. Many nonlinear solution
methods calculate an update of the admittivity from solving a linear system, that update
is applied to the conductivity in the model and the forward solution solved again. One
severe problem with linear reconstruction methods that do not include a forward solver
is that one cannot test if the updated admittivity even fits the data better than the initial
assumption (for example of a constant admittivity). Such algorithms tend to produce some
plausible image even if the data are erroneous.
The usual approach taken in geophysical and medical EIT to nonlinear reconstruction
is to numerically perform the (nonlinear generalized Tikhonov) minimization
γreg = arg min
γ
∣∣˜Vcalc(γ) −V∣∣+ αΨ(γ)
(.)
using standard numerical optimization techniques ( > Fig. .). As the Jacobian is known
explicitly, it is efficient to use gradient optimization methods such as Gauss-Newton (see
> Chap. ), and in that context the update step is very similar to the solution of the linear
problem (> .), and is a linear system for quadratic Ψ. Assuming conductivity initalized
as the background level γ, a typical iterative update scheme for succesive approximations
to the conductivity is
γn+= γn + (J∗
nJn + αR∗R)
−(J∗
n( ˜V −˜Vcalc(γn) + αR∗R(γ−γn)).
(.)
In contrast to the linearized problem, the nonlinear problem requires repeated solution
of the forward solution ˜Vcalc(γn) for variable conductivity, typically using the finite ele-
ment or finite difference method. One also has to constrain the conductivity Re γ to be
positive and this is made easier by a choice of ϕi as the characteristic functions of a
partition on Ω. This could be a rectangular grid or a courser tetrahedral mesh than that
used for u. Accurate modelling of electrodes requires a fine discretization near electrodes,
and yet one cannot hope to recover that level of detail in the admittivity near an electrode.
In many practical situations a priori bounds are known for the conductivity and permittiv-
ity and as the logarithmic stability result predicts, enforcing these bounds has a stabilising
effect on the reconstruction. The positivity constraint can be enforced by a change of vari-
ables to log γ and this is common practice, with the Jacobian adjusted accordingly. It is
generally better to perform a line search in the update direction from (> .) to mini-
mize the cost function in (> .) rather than simply applying the update. Most commonly
this search is approximated, for example, by fitting a few points to a low order polyno-
mial although implementation details of this are rarely well documented. It is also worth
mentioning that most absolute reconstruction algorithms start by finding a homogeneous
conductivity γbest fitting the data before the iterative method starts.
In geophysical ERT nonlinear solution is well-established. Although it is more com-
mon, for reasons of economy, to measure only along a line and reconstruct on the plane
beneath that line, fully three dimensional reconstruction is also widely used. The most
common reconstruction code used is RESDINV[] which builds on the work of Loke
and Barker at the University of Birmingham[]. The code is available commercially from

Electrical Impedance Tomography 

Loke’s company Geotomo Software. RESDINV has a finite difference forward solver used
when the ground is assumed flat, and a finite element solver for known non-flat topog-
raphy. In geophysical applications there is the advantage that obtaining a trangularization
of the surface is common surveying practice. The Jacobian is intialized using an analyti-
cal initial solution assuming homogeneous conductivity. Regularized nonlinear inversion
is performed using Gauss-Newton, with recalculation of Jacobian[], or using a quasi-
Newton method in which a rank one update is performed on the Jacobian. The penalty
function used in the regularization is of the form Ψ(γ) = ∣∣Rγ∣∣
where R is an approxi-
mate differential operator that penalizes horizontal and vertical variations differently. Total
variation regularization Ψ(γ) = ∣∣Rγ∣∣is also an option in this code. When data is likely
to be noisy, one can select one can select a “robust error norm,” in which the one-norm
is used also to measure the fit of the data to the forward solution. A maximum and mini-
mum value of the regularization parameter can be set by the user, but in a manner similar
to the classical Levenburg-Marquard method, for well-posed least squares problems the
parameter can be varied within that range depending on the residual at each iteration.
Although it is common in inverse problems to think of (> .) as a regularization
scheme a more rational justification for the method is probabilistic. We consider the error
in the measured data to be a sampled from a zero mean, possibly correlated, random vari-
ables. We then represent our a priori belief about the distribution of γ as a probability
distribution. The minimization (> .) is the Maximum a posteriori (MAP) estimate for
the case of independent Gaussian error and with prior distribution with log probability
density proportional to −Ψ(γ). A more sophisticated approach goes beyond Gauss distri-
butions and MAP estimates and samples the posterior distribution using Markov Chain
Monte Carlo methods [] (see > Chap. ). As this involves a large number of forward
problem solutions this is infeasible for large scale three dimensional EIT problems. How-
ever, as computers increase in speed and memory size relative to price, we expect this will
eventually become a feasible approach. It will make it easier to approach EIT with a specific
question such as “what is the volume of the region with a specified conductivity?” with the
answer expressed as an estimate of the probability distribution. Going back to (> .)
the regularization parameter αcontrols the ratio of the variances of the prior and error
distribution. In practice this choice of this parameter is somewhat subjective, and the usual
techniques in choice of regularization parameter, and the caution in their application, are
relevant.
Results of a geophysical ERT study are shown in > Fig. -and we would like to thank
the Geophysical Tomography Team, British Geological Survey (www.bgs.ac.uk/research/
tomography) for this figure and the description of the survey we sumarize below. In this
case ERT was used to identify the concentrations of leachate, the liquid that escapes from
buried waste in a landfill site. The leachate can be extracted and recirculated to enhance
the production of landfill gas, which can ultimately be used for electricity generation. It
was important to use a non-invasive technique – the more standard practice of drilling
exploratory wells could lead to new flow paths. Data were collected sequentially on 
parallel survey lines, using a regular grid of electrode positions. The inter-line spacing was
m with a minimum electrode spacing of m along line. For the current sources, electrode


Electrical Impedance Tomography
⊡Fig. -
A three dimensional ERT survey of a commercial landﬁll site to map the volumetric
distribution of leachate (opaque blue). Leachate is abstracted and re-circulated to further
enhance the production of landﬁll gas, and subsequently the generation of electricity. This
image was provided by the Geophysical Tomography Team, BGS, and is reproduced with the
permission of the British Geological Survey ©NERC. All rights Reserved (Reproduction of any
BGS materials does not amount to an endorsement by NERC or any of its employees of any
product or service and no such endorsement should be stated or implied.)
spacings between and m were used, while electrode spacings for the measurement
electrodes were between and m.
The inversion was performed using RESDINV with the FE forward solver with a mesh
generated using the measured surface topography. The two-norm was used for both penalty
term and error norm. Due to the large number of datum points (approx ,in total), the
dataset was split in four approximately equal volumes for subsequent inversion. The result-
ing resistivity models were then merged to produce a final model for the entire survey area.
The resulting D resistivity model was used to identify a total of drilling locations for
intrusive investigation. Eight wells were drilled and the results (i.e., initial leachate strikes
within these wells) were used to calibrate the resistivity model. Based on this ground-truth
calibration a resistivity threshold value of Ωm was used to represent the spatial distribu-
tion of leachate for volumetric analysis within the waste mass. A commercial vizualization
package was used to display cross sections, iso-resistivity surfaces, as well as topography
and features on the surface and the boreholes. For other similar examples of geophysical
ERT see [, ].

Electrical Impedance Tomography 

0
100
200
300
400
500
600
700
800
900
1000
10−15
10−10
10−5
100
16 Electrodes
24 Electrodes
32 Electrodes
⊡Fig. -
Normalized singular values of the Jacobian matrix from circular D model with L = , and
electrodes. EIT measurements are made with trigonometric patterns such that the
number of independent measurements from L electrodes is 
(L −)L. Note the use of more
degrees of freedom in the conductivity than the data so as to be able to study the eﬀect of
diﬀerent numbers of electrodes using SVD
 0
0.2
 0
0.2
 0
0.2
−1
−0.5
0
0.5
1
 0
0.2
10−3
10−2
10−1
100
101
102
103
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
0.00
0.25
0.50
0.75
⊡Fig. -
Saturation of EIT signals as a function of conductivity contrast. Left: Slices through a ﬁnite
element model of a D circular medium with a circular conductivity target at four horizontal
positions. EIT voltages are simulated at electrodes for trigonometric current patterns.
Right: Change in a voltage diﬀerence as a function of conductivity contrast (target vs.
background) for each horizontal position (horizontal centre of contrast speciﬁed in legend).
Vertical axis is normalized with respect to the maximum change from the central target, and
scaled by the sign of conductivity change
Experiments on tanks in process tomography or as simulated bodies for medical EIT
show that several iterations of a nonlinear method can improve the accuracy of conductiv-
ity and the shape of conductivity contours for known objects. In medical EIT it has yet to
be demonstrated that the shape and electrode position can be measured and modelled with
sufficient accuracy that the error in the linear approximation is greater than the modelling
error. Although these technical difficulties are not, we hope, insurmountable.


Electrical Impedance Tomography
..
Direct Nonlinear Solution
Nachman’s [, ] (see also []) uniqueness result for the two dimensional case was
essentially constructive and has resulted in a family of reconstruction algorithms called
¯∂-methods or scattering transform methods. Nachman’s method was implemented by Sil-
tanen at al [] in . Of course there are few practical situations in which the two
dimensional approximation is a good one – both the conductivity and the electrodes have
to be translationally invariant. Flow in a pipe with long electrodes is one example in which it
is a good approximation. We will sketch the main steps in the method (following Knudsen
et al. []) and refer the interested reader to the references for details.
We assume Ω is the unit disk for simplicity and we start with the Faddeev Green’s
function
Gk(x) := eikx gk(x),
gk(x) =

(π)∫
ℝ
eix⋅ξ
∣ξ∣+ k(ξ+ iξ) dξ
(.)
#1
#3
#5
10
1
0.1
⊡Fig. -
(Continued)

Electrical Impedance Tomography 

#1
#3
#5
10
1
0.1
⊡Fig. -
Iteration (horizonal axis) and regularization parameter selection (vertial axis) for two choices
of regularization matrix R. Data are from the RPI chest phantom []. The regularization
parameter (α in .) in the middle row () was selected at the “knee" of the L-curve,
indicating an appropritate level of regularization. Overregularization (top row) is shown for
α, and underregularization (bottom row) for .α. Columns indicate , , or iterations of
(> .). With increased iteration, we see improved separation of targets and more accurate
conductivity estimates, although these improvements trade oﬀagainst increased electrode
artefacts due to model mismatch. The diﬀerence between the Laplacian and weighted
diagonal regularization is shown in the increased smoothness of (a), especially in the
underregularized case (a) Laplacian regularization and (b) Weighted diagonal regularization
and the single layer potential
(Skϕ)(x) := ∫
∂Ω
Gk(x −y)ϕ(y) dθ(y).
(.)
Here k = k+ ikand by abuse of notation we consider x as a vector in x ⋅ξ and a complex
number x+ixin the complex product kx. By θ(y) we mean the angular polar coordinate


Electrical Impedance Tomography
of y. We assume we have the measured Dirichlet to Neumann map Λγ and of course we
know Λ. The first step in the algoritm is for each fixed k to solve the linear Fredholm
integral equation for a function ψ(⋅, k) on the boundary
ψ(⋅, k)∣∂Ω = eikx −Sk(Λγ −Λ)ψ(⋅, k)∣∂Ω.
(.)
This is an explicit calculation of the Complex Geometrics Optics soloution of Theorem .
It is fed in to the calculation of what is called the non-physical scattering transform t : ℂ→ℂ
defined by
t(k) = ∫
∂Ω
e
¯k ¯x(Λγ −Λ)ψ(⋅, k) dθ.
(.)
Note here that (> .) is a linear equation to solve the resulting ψ depends nonlinearly
on the data Λγ, and of course as ψ depends on the data t is a nonlinear function of the
data. The second step is to find the conductivity from the scattering data as follows. Let
ex(k) := exp(i(kx + ¯k ¯x)). For each fixed x we solve another integral equation
V(x, k) = +

(π)∫
ℝ
t(k′)
(k −k′)¯k′ e−x(k′)V(x, k′) dk′
dk′

(.)
finally setting γ(x) = V(x,). The integral > Eq. (.) is the solution to the partial
differential equation
¯∂kV(x, k) =

π¯k t(k)e−x(k)V(x, k),
k ∈ℂ,
(.)
where ¯∂k = ∂/∂¯k. > Equation (.) is refered to as the ¯∂equation hence the name of the
method.
The reconstruction procedure is therefore a direct nonlinear method in which the steps
are the solution of linear equations. The only forward modelling required is the construc-
tion of Λ. In some practical realisations of this methods [] an approximation to the
scattering transform is used in which ψ is replaced by an exponential
texp(k) = ∫
∂Ω
e
¯k ¯x(Λγ −Λ) dθ.
(.)
In practical reconstruction schemes t or texp are replaced by an approximation truncated
to zero for ∣k∣> R for some R > , which effectiviely also truncates the domain of integra-
tion in (> .) to the disk of radius R. Reconstruction of data from a two dimensional
agar phantom simulating a chest was performed in [] using truncated texp, and in []
a difference imaging version of the ¯∂-method is implemented using a truncated scattering
transform and applied to chest data. A rigorous regularization scheme for two dimensional
¯∂-reconstruction is given in []. In this case the regularization is applied to the data, in
a similar spirit to X-ray CT recontruction in which the data is filtered and then backpro-
jected (see > Chap. ) and the regularization is applied in the filter on the data. In this
sense it is harder to understand the regularized algorithm in terms of systematic a priori

Electrical Impedance Tomography 

informationa applied to the image. As in CT, this is traded off against having a fast explicit
reconstruction algorithm that avoids iteration.
So far our discussion of ¯∂-methods has been confined to two dimensional problems. At
the time of writing three dimesnional direct reconstruction methods are in their infancy.
A three dimensional ¯∂-algorithm for small conductivities is outlined in [] and it is yet
to be seen if this will result in practical implementation with noisy data on a finite array
of electrodes. See the thesis of Bikowski [] for the latest steps in this direction. If these
efforts are succesful, the impact on EIT is likely to be revolutionary.
.
Conclusions
Electrical impendance tomography and its relatives are among the most challenging
inverse problems in imaging as they are nonlinear and highly illposed. The problem has
inspired detailed theoretical and numerical study and this has had an influence across a
wide range of related inverse boundary value problems for (systems of) partial differential
equations. Medical and industrial process applications have yet to realize their potential as
routine methods while the equivalent methods in geophysics are well established. A family
of direct nonlinear solution techniques until recently only valid for the two dimensional
problem, may soon be extended to practical three dimensional algorithms. If this hap-
pens fast three dimensional nonlinear reconstruction may be possible on relatively modest
computers. In some practical situations in medical and geophysical EIT the conductivity is
anisotropic, in which case the solution is non-unique. A specification of the a priori infor-
mation needed for a unique solution is poorly understood and practical reconstruction
algorithms have yet to be proposed in the anisotropic case.
For a more complete summary of uniqueness results, we refer the reader to the review
article of Uhlmann []. For a review of biomedical applications of EIT, we refer the reader
to the recent book by Holder [], while subsequent progress in the medical area can gen-
erally be found in special issues of the journal Physiological Measurement arising from
the annual conferences on Biomedical Applications of EIT. A good reference for details of
geophysical EIT reconstruction can be found in the manual[] and the notes by Loke[].
For applications in process tomography see [] and the proceedings of the biennial World
Congress on Industrial Process Tomography (www.isipt.org/wcipt).
References and Further Reading
. Adler A,LionheartWRB() Usesandabuses
of EIDORS: an extensible software base for EIT.
Physiol Meas :S–S
. Alessandrini G () Stable determination of
conductivity by boundary measurements. Appl
Anal :–


Electrical Impedance Tomography
. Alessandrini G () Singular solutions of
elliptic equations and the determination of con-
ductivity by boundary measurements. J Differ
Equations ():–
. Alessandrini G () Determining conductivity
by boundary measurements, the stability issue.
In: Spigler R (ed) Applied and industrial mathe-
matics. Kluwer, Dordrecht, pp –
. Alessandrini G () Open issues of stability
for the inverse conductivity problem. J Inverse
Ill-Posed Prob :–
. Alessandrini G, Gaburro R () Determin-
ing Conductivity with Special Anisotropy by
Boundary Measurements. SIAM J Math Anal
:–
. Alessandrini G, Gaburro R () The local
Calderón problem and the determination at the
boundary of the conductivity. Commun Part
Differ Eq :–
. Alessandrini G, Vessella S () Lipschitz sta-
bility for the inverse conductivity problem. Adv
Appl Math :–
. Ammari H, Buffa A, Nédélec J-C () A
justification of eddy currents model for the
Maxwell equations. SIAM J Appl Math :
–
. Aronszajn N () A unique continuation the-
orem for solutions of elliptic partial differential
equations or inequalities of second order. J Math
Pures Appl :–
. Astala K, Päivärinta L () Calderón’s inverse
conductivity problem in the plane. Annals of
Mathematics :–
. Barber D, Brown B () Recent develop-
ments in applied potential tomography – APT.
In: Bacharach SL (ed) Information process-
ing in medical imaging. Nijhoff, Amsterdam,
pp –
. Barceló JA, Faraco D, Ruiz A () Stability of
the inverse problem in the plane for less regular
conductivities. J Differ Equations :–
. Barceló JA, Barceló T, Ruiz A () Stability
of Calderón inverse conductivity problem in the
plane. J Math Pures Appl :–
. Berenstein CA, Casadio Tarabusi E () Inte-
gral geometry in hyperbolic spaces and electri-
cal impedance tomography. SIAM J Appl Math
:
. Bikowski
J
()
Electrical
impedance
tomography reconstructions in two and three
dimensions; from Calderón to direct methods.
PhD thesis, Colorado State University, Fort
Collins
. Borcea L () Electrical impedance tomogra-
phy. Inverse Prob :R–R; Borcea L ()
Addendumto electrical impedancetomography.
Inverse Prob :–
. Borsic A, Graham BM, Adler A, Lionheart WRB
() Total variation regularization in electrical
impedance tomography. IEEE Trans Med Imag-
ing ():–
. Brown R () Recovering the conductivity at
the boundary from the Dirichlet to Neumann
map: a pointwise result. J Inverse Ill-Posed Prob
:–
. Beals R, Coifman R () Transformation
spectrales et equation d’evolution non lineares.
Seminaire Goulaouic-Meyer-Schwarz, exp , pp
–
. Beals R, Coifman RR () Linear spectral
problems, non-linear equations and the ¯∂-
method. Inverse Prob :
. Brown R, Torres R () Uniqueness in the
inverse conductivity problem for conductivities
with /derivatives in Lp, p > n. J Fourier Anal
Appl :–
. Brown R, Uhlmann G () Uniqueness in the
inverse conductivity problem with less regu-
lar conductivities in two dimensions. Commun
Part Differ Eq :–
. Bukhgeim AL, Uhlmann G () Recovery a
potential from partial Cauchy data. Commun
Part Differ Eq :–
. Calderón AP () On an inverse boundary
value problem. In: Seminar on numerical anal-
ysis and its applications to continuum physics
(Rio de Janeiro, ). Soc Brasil Mat, Rio de
Janeiro, pp –
. Calderón AP () On an inverse boundary
value problem. Comput Appl Math (–):–
(Note this reprint has some different typo-
graphical errors from the original: in particular
on the first page the Dirichlet data for w is ϕ not
zero)
. Chambers
JE,
Meldrum
PI,
Ogilvy
RD,
Wilkinson PB () Characterisation of a

Electrical Impedance Tomography 

NAPL-contaminated former quarry site using
electrical impedance tomography. Near Surface
Geophysics :–
. Chambers JE, Kuras O, Meldrum PI, Ogilvy RD,
Hollands J () Electrical resistivity tomog-
raphy applied to geologic, hydrogeologic, and
engineering investigations at a former waste-
disposal site. Geophysics :B–B
. Cheng K, Isaacson D, Newell JC, Gisser DG
() Electrode models for electric current
computed tomography. IEEE Trans Biomed Eng
:–
. Cheney M, Isaacson D, Newell JC ()
Electrical Impedance Tomography. SIAM Rev
:–
. Cornean H, Knudsen K, Siltanen S ()
Towards a D-bar reconstruction method for
three dimensional EIT. J Inverse Ill-Posed Prob
:
. Colin de Verdière Y, Gitler I, Vertigan D
() Réseaux électriques planaires II. Com-
ment Math Helv :–
. Di Cristo M () Stable determination of
an inhomogeneous inclusion by local bound-
ary
measurements.
J Comput
Appl
Math
:–
. Ciulli S, Ispas S, Pidcock MK () Anomalous
thresholds and edge singularities in electrical
impedance tomography. J Math Phys :
. Dobson DC () Stability and regularity of an
inverseellipticboundaryvalueproblem.Techni-
cal reportTR-, Rice University, Department
of Mathematical Sciences
. Doerstling BH () A -d reconstruction algo-
rithm for the linearized inverse boundary value
problem for Maxwell’s equations. PhD thesis,
Rensselaer Polytechnic Institute, Troy
. Druskin V () The unique solution of the
inverse problem of electrical surveying and elec-
trical well-logging for piecewise-constant con-
ductivity. Izv Phys Solid Earth :–(in
Russian)
. Druskin V () On uniqueness of the determi-
nation of the three-dimensional underground
structures from surface measurements with var-
iously positioned steady-state or monochro-
matic field sources. Sov Phys Solid Earth
:–(in Russian)
. Druskin V () On the uniqueness of inverse
problems for incomplete boundary data. SIAM J
Appl Math ():–
. Gaburro R () Sul Problema Inverso della
Tomografia da Impedenza Elettrica nel Caso
di Conduttivitá Anisotropa. Tesi di Laurea in
Matematica, Universitá degli Studi di Trieste
. Gaburro R () Anisotropic conductivity.
Inverse boundary value problems. PhD thesis,
University of Manchester Institute of Science
and Technology (UMIST), Manchester
. Gaburro R, Lionheart WRB () Recovering
Riemannian metrics in monotone families from
boundary data. Inverse Prob :(pp)
. Geotomo Software () RESDINV ver .,
Rapid D resistivity and IP inversion using
the least-squares method. Geotomo Software,
Malaysia. www.geoelectrical.com
. Gisser DG, Isaacson D, Newell JC () Electric
current computed tomography and eigenvalues.
SIAM J Appl Math :–
. Griffiths H, Jossinet J () Bioelectric tissue
spectroscopy from multifrequency EIT. Physiol
Meas (A):–
. Griffiths H () Magnetic induction tomogra-
phy. Meas Sci Technol :–
. Hanke M () On real-time algorithms for the
location search of discontinuous conductivities
with one measurement. Inverse Prob :
. Hanke M, Schappel B () The factorization
method for electrical impedance tomography in
the half-space. SIAM J Appl Math :–
. Hähner P () A periodic Faddeev-type
solution operator. J Differ
Equations
:
–
. Huang SM, Plaskowski A, Xie CG, Beck MS
() Capacitance-based tomographic flow
imaging system. Electronics Lett :–
. Heck H, Wang J-N, () Stability estimates for
the inverse boundary value problem by partial
Cauchy data. Inverse Prob :–
. Heikkinen LM, Vilhunen T, West RM, Vauhko-
nen M () Simultaneous reconstruction of
electrode contact impedances and internal elec-
trical properties: II. Laboratory experiments.
Meas Sci Technol :
. Henderson
RP,
Webster
JG
()
An
Impedance
camera
for
spatially
specific


Electrical Impedance Tomography
measurements
of
the
thorax.
IEEE Trans
Biomed Eng BME-():–
. Holder DS () Electrical impedance tomog-
raphy methods history and applications. Insti-
tute of Physics, Bristol
. Ikehata M () The enclosure method and
its applications, chapter . In: Analytic exten-
sion formulas and their applications (Fukuoka,
/Kyoto, ). Kluwer; Int Soc Anal Appl
Comput :–
. Ikehata M, Siltanen S () Numerical method
for nding the convex hull of an inclusion in con-
ductivity from boundary measurements. Inverse
Prob :–
. Ingerman D, Morrow JA () On a char-
acterization of the kernel of the Dirichlet-to-
Neumann map for a planar region. SIAM J Math
Anal :
. Isaacson D () Distinguishability of conduc-
tivities by electric current computed tomogra-
phy. IEEE TransMed Imaging :–
. Isaacson D, Mueller JL, Newell J, Siltanen
S
()
Reconstructions
of
chest
phan-
toms by the
d-bar method for electrical
impedance
tomography.
IEEE
Trans
Med
Imaging :–
. Isaacson D, Mueller JL, Newell J, Siltanen S
() Imaging cardiac activity by the D-bar
method for electrical impedance tomography.
Physiol Meas :S–S
. Isakov V () Completeness of products of
solutions and some inverse roblems for PDE. J
Differ Equations :–
. Isakov V () On the uniqueness in the
inverse conductivity problem with local data.
Inverse Prob Imaging :–
. Kaipio J, Kolehmainen V, Somersalo E, Vauhko-
nen M () Statistical inversion and Monte
Carlo sampling methods in electrical impedance
tomography. Inverse Prob :–
. Kang H, Yun K () Boundary determina-
tion of conductivities and Riemannian metrics
via local Dirichlet-to-Neumann operator. SIAM
J Math Anal :–
. Kenig C, Sjöstrand J, Uhlmann G () The
Calderón problem with partial data. Ann Math
:–
. Kim Y, Woo HW () A prototype sys-
tem and reconstruction algorithms for electrical
impedance technique in medical body imaging.
Clin Phys Physiol Meas :–
. Kohn R, Vogelius M () Identification of
an Unknown Conductivity by Means of Mea-
surements at the Boundary. SIAM-AMS Proc
:–
. Kohn
R, Vogelius M
()
Determining
conductivity by boundary measurements II.
Interior results. Comm Pure Appl Math :
–
. Knudsen K, Lassas M, Mueller JL, Siltanen
S () Regularized D-bar method for the
inverse conductivity problem. Inverse Prob
Imaging :–
. Lassas
M,
Uhlmann
G
() Determin-
ing a Riemannian manifold from boundary
measurements. Ann Sci École Norm Sup :
–
. Lassas M, Taylor M, Uhlmann G () The
Dirichlet-to-Neumann map for complete Rie-
mannian manifolds with boundary. Commun
Geom Anal :–
. Lionheart WRB () Conformal Uniqueness
Results in Anisotropic Electrical Impedance
Imaging. Inverse Prob :–
. Lee JM, Uhlmann
G ()
Determining
anisotropic
real-analytic
conductivities
by
boundary measurements. Commun Pure Appl
Math :–
. Liu L () Stability estimates for the two-
dimensional inverse conductivity problem. PhD
thesis, University of Rochester, New York
. Loke MH () Tutorial: -D and -D elec-
trical imaging surveys, Geotomo software.
www.geoelectrical.com
. Loke MH, Barker RD () Rapid least- squares
inversion by a quasi- Newton method. Geophys
Prospect :
. Loke MH, Chambers JE, Ogilvy RD ()
Inversion of D spectral induced polariza-
tion
imaging
data.
Geophys
Prospect
:
–
. Mandache N () Exponential instability in an
inverse problem for the Schrödinger equation.
Inverse Prob :–
. Meyers NG () An Lp estimate for the gra-
dient of solutions of second order elliptic diver-
gence equations. Ann Scuola Norm Sup-Pisa
():–

Electrical Impedance Tomography 

. Molinari M, Blott BH, Cox SJ, Daniell GJ ()
Optimal imaging with adaptive mesh refine-
ment in electrical tomography. Physiol Meas
():–
. Nachman
A
()
Reconstructions
from
boundary
measurements.
Ann
Math
:–
. Nachman A () Global uniqueness for a two
dimensional inverse boundary value problem.
Ann Math :–
. Nachman A, Sylvester J, Uhlmann G () An
n-dimensional Borg-Levinson theorem. Com-
mun Math Phys :–
. Nakamura G, Tanuma K () Local determi-
nation of conductivity at the boundary from
the Dirichlet-to-Neumann map. Inverse Prob
:–
. Nakamura G, Tanuma K () Direct deter-
mination of the derivatives of conductivity at
the boundary from the localized Dirichlet to
Neumann map. Commun Korean Math Soc
:–
. Nakamura G, Tanuma K () Formulas
for reconstrucing conductivity and its normal
derivative at the boundary from the local-
ized Dirichlet to Neumann map. In: Hon Y-
C, Yamamoto M, Cheng J, Lee J-Y (eds) Pro-
ceeding international conference on inverse
problem-recent development in theories and
numerics. World Scientific, River Edge, pp
–
. Novikov RG () A multidimensional inverse
spectral problem for the equation −Δψ+(v(x)−
Eu(x))ψ = . (Russian) Funktsional. Anal i
Prilozhen , :–, ; translation in Funct
Anal Appl , :–()
. Paulson K, Breckon W, Pidcock M () Elec-
trode modeling in electrical-impedance tomog-
raphy. SIAM J Appl Math :–
. Päivärinta L, Panchenko A, Uhlmann G ()
Complex geometrical optics solutions for Lip-
schitz conductivities. Rev Mat Iberoam :
–
. Polydorides N, Lionheart WRB () A
Matlab toolkit for three-dimensional electrical
impedance tomography: a contribution to the
electrical impedance and diffuse optical recon-
struction software project. Meas Sci Technol
:–
. Seagar AD () Probing with low frequency
electric current. PhD thesis, University of Can-
terbury, Christchurch
. Seagar AD, Bates RHT () Full-wave com-
puted tomography. Pt : Low-frequency elec-
tric current CT. Inst Electr Eng Proc Pt A
:–
. Siltanen S, Mueller JL, Isaacson D () An
implementation of the reconstruction algorithm
of A. Nachman for the -D inverse conductivity
problem. Inverse Prob :–
. Soleimani M, Lionheart WRB () Nonlinear
image reconstruction for electrical capacitance
tomography experimental data using. Meas Sci
Technol ():–
. Soleimani M, Lionheart WRB, Dorn O ()
Level set reconstruction of conductivity and
permittivity from boundary electrical measure-
ments using expeimental data. Inverse Prob Sci
Eng :–
. Somersalo E, Cheney M, Isaacson D () Exis-
tence and uniqueness for electrode models for
electric current computed tomography. SIAM J
Appl Math :–
. Soni NK () Breast imaging using electrical
impedance tomography. PhD thesis, Dartmouth
College, NH
. Sylvester J () An anisotropic inverse bound-
ary value problem. Commun Pure Appl Math
:–
. Sylvester J, Uhlmann G () A uniqueness the-
orem for an inverse boundary value problem
in electrical prospection. Commun Pure Appl
Math :–
. Sylvester J, Uhlmann G () A global unique-
ness theorem for an inverse boundary valued
problem. Ann Math :–
. Sylvester J, Uhlmann G () Inverse bound-
ary value problems at the boundary – contin-
uous dependence. Commun Pure Appl Math
:–
. Tamburrino A, Rubinacci G () A new
non-iterative inversion method for electri-
cal resistance tomography. Inverse Prob :
–
. Uhlmann G () Topical review: electrical
impedance tomography and Calderón’s prob-
lem. Inverse Prob :(pp)


Electrical Impedance Tomography
. Vauhkonen M, Lionheart WRB, Heikkinen LM,
Vauhkonen PJ, Kaipio JP () A MATLAB
package for the EIDORS project to recon-
struct two-dimensional EIT images. Physiol
Meas :–
. Vauhkonen M () Electrical impedance
tomography and prior information. PhD thesis,
University of Kuopio, Kuopio
. Vauhkonen
M,
Karjalainen
PA,
Kaipio
JP
()
A
Kalman
Filter
approach
to
track fast impedance changes in electrical
impedance tomography. IEEE Trans Biomed
Eng :–
. West RM, Soleimani M, Aykroyd RG, Lionheart
WRB () Speed Improvement of MCMC
Image Reconstruction in Tomography by Par-
tial Linearization. Int J Tomogr Stat , No. S:
–
. West RM, Jia X, Williams RA () Paramet-
ric modelling in industrial process tomography.
Chem Eng J :–
. Yang WQ, Spink DM, York TA, McCann
H () An image reconstruction algorithm
based on Landwebers iteration method for
electrical-capacitance tomography. Meas Sci
Technol :–
. York T () Status of electrical tomography
in industrial applications. J Electron Imaging
:–

Synthetic Aperture Radar
Imaging
Margaret Cheney ⋅Brett Borden
.
Introduction.....................................................................
.
Historical Background..........................................................
.
Mathematical Modeling.........................................................
..
Scattering of Electromagnetic Waves.................................................
..
Basic Facts About the Wave Equation................................................
..
Basic Scattering Theory................................................................
...
The Lippmann–Schwinger Integral Equation......................................
...The Lippmann–Schwinger Equation in the Frequency Domain.................
...The Born Approximation..............................................................
..
The Incident Field.......................................................................
..
Model for the Scattered Field..........................................................
..
The Matched Filter......................................................................
..
The Small-Scene Approximation.....................................................
..
The Range Profile.......................................................................
.
Survey on Mathematical Analysis of Methods.................................
..
Inverse Synthetic-Aperture Radar (ISAR)...........................................
...
The Data Collection Manifold........................................................
...ISAR in the Time Domain.............................................................
..
Synthetic-Aperture Radar..............................................................
...Spotlight SAR............................................................................
...Stripmap SAR............................................................................
..
Resolution for ISAR and Spotlight SAR.............................................
...Down-Range Resolution in the Small-Angle Case.................................
...Cross-Range Resolution in the Small-Angle Case..................................
.
Numerical Methods..............................................................
..
ISAR and Spotlight SAR Algorithms.................................................
..
Range Alignment.......................................................................
.
Open Problems..................................................................
..
Problems Related to Unmodeled Motion............................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Synthetic Aperture Radar Imaging
..
Problems Related to Unmodeled Scattering Physics...............................
..
New Applications of Radar Imaging.................................................
.
Conclusion.......................................................................
.
Cross-References.................................................................

Synthetic Aperture Radar Imaging 

Abstract: The purpose of this chapter is to explain the basics of radar imaging and to list
a variety of associated open problems. After a short section on the historical background,
the article includes a derivation of an approximate scalar model for radar data. The basics
in Inverse Synthetic-Aperture Radar (ISAR) are discussed, and a connection is made with
the Radon transform. Two types of Synthetic-Aperture Radar (SAR), namely spotlight SAR
and stripmap SAR, are outlined. Resolution analysis is included for ISAR and spotlight
SAR. Some numerical algorithms are discussed. Finally, the chapter ends with a listing of
open problems and a bibliography for further reading.
.
Introduction
“Radar” is an acronym for RAdio Detection And Ranging. Radar was originally developed
[, , , , ] as a technique for detecting objects and determining their positions by
means of echo-location, and this remains the principal function of modern radar systems.
However, radar systems have evolved over more than decades to perform an additional
variety of very complex functions; one such function is imaging [, –, , , , ,
, ].
Radar-based imaging is a technology that has been developed mainly within the engi-
neering community. There are good reasons for this: some of the critical challenges are
() transmitting microwave energy at high power, () detecting microwave energy, and
() interpreting and extracting information from the received signals. The first two prob-
lems are concerned with the development of appropriate hardware; however, these prob-
lems have now largely been solved, although there is ongoing work to make the hardware
smaller and lighter. The third problem essentially encompasses a set of mathematical
challenges, and this is the area where most of the current effort is taking place.
Radar imaging shares much in common with optical imaging: both processes involve
the use of electromagnetic waves to form images. The main difference between the two is
that the wavelengths of radar are much longer than those of optics. Because the resolv-
ing ability of an imaging system depends on the ratio of the wavelength to the size of the
aperture, radar imaging systems require an aperture many thousands of times larger than
optical systems in order to achieve comparable resolution. Since kilometer-sized anten-
nas are not practicable, fine-resolution radar imaging has come to rely on the so-called
synthetic apertures in which a small antenna is used to sequentially sample a much larger
measurement region.
.
Historical Background
Radar technology underwent rapid development during World War II; most of this work
concerned developing methods to transmit radio waves and detect scattered waves. The
invention of Synthetic-Aperture Radar (SAR) is generally credited to Carl Wiley, of the


Synthetic Aperture Radar Imaging
Goodyear Aircraft Corporation, in . The mid-s saw the development of the first
operational systems, under the sponsorship of the US Department of Defense. These sys-
tems were developed by a collaboration between universities, such as the University of
Illinois and the University of Michigan, together with companies such as Goodyear Air-
craft, General Electric, Philco, and Varian. In the late s, the National Aeronautics and
Space Administration (NASA) began sponsoring unclassified work on SAR. Around this
time, the first digital SAR processors were developed (earlier systems having used ana-
log optical processing). In , the SEASAT-A satellite was sent up, and even though it
operated only for days, the images obtained from it were so useful that it became obvi-
ous that more such satellites were needed. In , the Shuttle Imaging Radar (SIR) series
began, and many shuttle missions since then have involved radar imaging of the earth.
In the s, satellites were sent up by many countries (including Canada, Japan, and the
European Space Agency), and SAR systems were sent to other planets and their moons,
including Venus, Mars, and Titan. Since the beginning of the new millennium, more satel-
lites have been launched, including for example, the new European Space Agency satellite
ENVISAT, and the TerraSAR-X satellite, which was developed and launched by a (mainly
European) public–private partnership.
Code letters for the radar frequency bands were originally used during wartime, and
the usage has persisted. These abbreviations are listed in
> Table -. The HF band
usually carries radio signals; VHF carries radio and broadcast television; the UHF band
carries television, navigation radar, and cell phone signals. Some radar systems oper-
ate at VHF and UHF; these are typically systems built for penetrating foliage, soil, and
buildings. Most of the satellite synthetic-aperture radar systems operate in the L, S,
and C bands. The S band carries wireless Internet. Many military systems operate at
X band.
⊡Table -
Radar frequency bands
Band designation
Approximate frequency range
Approximate wavelengths
HF (high frequency)
–MHz
m
VHF (very high frequency)
–MHz
m
UHF (ultra high frequency)
–MHz
m
L-band
–GHz
cm
S-band
–GHz
cm
C-band
–GHz
cm
X-band
–GHz
cm
Ku-band (under K)
–GHz
cm
K-band
–GHz
.cm
Ka-band (above K)
–GHz
cm
mm-wave
–GHz
mm

Synthetic Aperture Radar Imaging 

.
Mathematical Modeling
SAR relies on a number of very specific simplifying assumptions about radar scattering
phenomenology and data collection scenarios:
. Most imaging radar systems make use of the start–stop approximation [], in which
both the radar sensor and scattering object are assumed to be stationary during the time
interval over which the pulse interacts with the target.
. The target or scene is assumed to behave as a rigid body.
. SAR imaging methods assume a linear relationship between the data and scene.
..
Scattering of Electromagnetic Waves
The present discussion considers only scattering from targets that are stationary.
For linear materials, Maxwell’s equations can be used [] to obtain an inhomogeneous
wave equation for the electric field E at time t and position x:
∇E(t,x) −

c(x)
∂E(t,x)
∂t
= s(t,x)
(.)
and a similar equation for the magnetic field B. Here c(x) denotes the speed of propaga-
tion of the wave (throughout the atmosphere, this speed is approximately independent of
position and equal to the constant vacuum speed c) and s is a source term that, in gen-
eral, can involve E and B. For typical radar problems, the wave speed is constant in the
region between the source and the scattering objects (targets) and varies only within the
target volume. Consequently, here scattering objects are modeled solely via the source term
s(t,x).
One Cartesian component of > Eq. (.) is:
(∇−
c
∂
∂t)E(t,x) = s(t,x),
(.)
where atmospheric propagation between source and target has been assumed.
..
Basic Facts About the Wave Equation
A fundamental solution [] of the inhomogeneous wave equation is a generalized function
[, ] satisfying
(∇−
c
∂
∂
t
) g(t,x) = −δ(t)δ(x).
(.)
The solution of (> .) that is useful is
g(t,x) = δ(t −∣x∣/c)
π∣x∣
= ∫
e−iω(t−∣x∣/c)
π∣x∣
dω,
(.)


Synthetic Aperture Radar Imaging
where in the second equality the identity
δ(t) = 
π ∫e−iωt dω
(.)
was used. The function g(t,x) can be physically interpreted as the field at (t,x) due to a
source at the origin x = at time t = and is called the outgoing fundamental solution or
(outgoing) Green’s function.
The Green’s function [] can be used to solve the constant-speed wave equation with
any source term. In particular, the outgoing solution of
(∇−
c
∂
∂
t
)u(t,x) = s(t,x),
(.)
is
u(t,x) = −∬g(t −t′,x −y)s(t′,y)dt′ dy.
(.)
In the frequency domain, the equations corresponding to (> .) and (> .) are
(∇+ k)G = −δ
and
G(ω,x) = eik∣x∣
π∣x∣,
(.)
where the wave number k is defined as k = ω/c.
..
Basic Scattering Theory
In constant wave velocity radar problems, the source s is a sum of two terms, s = sin + ssc,
where sin models the transmitting antenna, and ssc models the scattering object. The solu-
tion E to > Eq. (.), which is written as Etot, therefore splits into two parts: Etot =
Ein + Esc. The first term, Ein, satisfies the wave equation for the known, prescribed source
sin. This part is called the incident field, because it is incident upon the scatterers. The sec-
ond term, Esc, is due to target scattering, and this part is called the scattered field. We use
the same decomposition in the simplified scalar model.
One approach to finding the scattered field is to simply solve (> .) directly, using, for
example, numerical time-domain techniques. For many purposes, however, it is convenient
to reformulate the scattering problem in terms of an integral equation.
...
The Lippmann–Schwinger Integral Equation
In scattering problems the source term ssc (typically) represents the target’s response to an
incident field. This part of the source function will generally depend on the geometric and
material properties of the target and on the form and strength of the incident field. Conse-
quently, ssc can be quite complicated to describe analytically, and in general it will not have
the same direction as sin. Fortunately, for this article, it is not necessary to provide a detailed

Synthetic Aperture Radar Imaging 

analysis of the target’s response; for stationary objects consisting of linear materials, the
scalar model ssc is written as the time-domain convolution
ssc(t,x) = ∫v(t −t′,x)Etot(t′,x)dt′,
(.)
where v(t,x) is called the reflectivity function and depends on target orientation. In
general, this function also accounts for polarization effects.
The expression (> .) is used in (> .) to express Esc in terms of the Lippmann–
Schwinger integral equation []
Esc(t,x) = ∫g(t −τ,x −z)∬v(τ −t′,z)Etot(t′,z)dt′ dτ dz.
(.)
...
The Lippmann–Schwinger Equation in the Frequency
Domain
In the frequency domain, the electric field and reflectivity function become
E(ω,x) = ∫eiωtE(t,x)dt
and
V(ω,z) = ∫eiωtv(t,z)dt,
(.)
respectively. Thus the frequency-domain version of (> .) is
(∇+ ω
c) E(ω,x) = S(ω,x)
(.)
and of (> .) is
Esc(ω,x) = −∫G(ω,x −z)V(ω,z)Etot(ω,z)dz .
(.)
The reflectivity function V(ω,x) can display a sensitive dependence on ω [, , ].
When the target is small in comparison with the wavelength of the incident field, for exam-
ple, V is proportional to ω(this behavior is known as “Rayleigh scattering”). At higher
frequencies (shorter wavelengths), the dependence on ω is typically less pronounced. In the
so-called “optical region,” V(ω,x) is often approximated as being independent of ω (see,
however, []); the optical approximation is used in this article, and the ω dependence is
simply dropped. In the time domain, this corresponds to v(t,z) = δ(t)V(z), and the delta
function can be used to carry out the t′ integration in (> .).
...
The Born Approximation
For radar imaging, the field Esc is measured at the radar antenna and, from these measure-
ments, the goal is to determine V. However, both V and Esc in the neighborhood of the
target are unknown, and in (> .) these unknowns are multiplied together. This non-
linearity makes it difficult to solve for V. Consequently, almost all work on radar imaging


Synthetic Aperture Radar Imaging
relies on the Born approximation, which is also known as the weak-scattering or single-
scattering approximation [, ]. The Born approximation replaces Etot on the right side
of (> .) by Ein, which is known. This results in a linear formula for Esc in terms of V:
Esc(t,x) ≈EB(t,x) ≡∬g(t −τ,x −z)V(z)Ein(τ,z)dτ dz.
(.)
In the frequency domain, the Born approximation is
Esc
B (ω,x) = −∫
eik∣x−z∣
π∣x −z∣V(z)Ein(ω,z)dz.
(.)
The Born approximation is very useful because it makes the imaging problem linear. It is
not, however, always a good approximation; see > Sect. ..
..
The Incident Field
The incident field Ein is obtained by solving (> .), where sin is taken to be the relevant
component of the current density on the source antenna and ssc is zero. This article uses
a simplified point-like antenna model, for which sin(t,x) = p(t)δ(x −x), where p is the
waveform transmitted by the antenna. Typically p consists of a sequence of time-shifted
pulses, so that p(t) = ∑p(t −tn).
In the frequency domain, the corresponding source for (> .) is Sin(ω,x) =
P(ω)δ(x −x), where P denotes the inverse Fourier transform of p:
p(t) = 
π ∫e−iωtP(ω)dω.
(.)
Use of (> .) shows that the incident field in the frequency domain is
Ein(ω,x) = −∫G(ω,x −y)P(ω)δ(y −x)dy
= −P(ω) eik∣x−x∣
π∣x −x∣.
(.)
..
Model for the Scattered Field
In monostatic radar systems, the transmit and receive antennas are co-located – often the
same antenna is used. Use of (> .) in (> .) shows that the Born-approximated
scattered field at the transmitter location xis
Esc
B (ω,x) = P(ω) ∫
eik∣x−z∣
(π)∣x−z∣V(z)dz.
(.)

Synthetic Aperture Radar Imaging 

Fourier transforming (> .) results in an expression for the time-domain field:
Esc
B (t,x) = ∬
e−iω(t−∣x−z∣/c)
π(π∣x−z∣)P(ω)V(z)dω dz
= ∫
p(t −∣x−z∣/c)
(π∣x−z∣)
V(z)dz.
(.)
Under the Born approximation, the scattered field is a superposition of scattered fields
from point-like targets V(z′) ∝δ(z −z′).
..
The Matched Filter
An important aspect of (> .) is the /Rgeometrical decay (where R = ∣x−z∣). When
R is large (which it usually is), this decay factor results in a received signal that is extremely
small – so small, in fact, that it can be dominated by thermal noise in the receiver. Thus it is
difficult even to detect the presence of a target. Target detection is typically accomplished
by means of a matched filter [, , ].
Below the matched filter is derived for scattering from a single fixed, point-like target.
For such a target, by > Eq. (.) and (> .), the signal scattered is simply a time-delayed
version of the transmitted waveform:
srec(t) = ρs(t −τ) + n(t),
where τ corresponds to the R/c delay, ρ is a proportionality factor related to the scatterer
reflectivity V(z) and the geometric decay (π∣x−z∣)−, and n denotes noise.
The strategy is to apply a filter (convolution operator) to srec in order to improve the
signal-to-noise ratio. The filter’s impulse response (convolution kernel) is denoted by h,
which implies that the filter output is
η(t) = (h ∗srec)(t) = ρηs(t) + ηn(t),
(.)
where
ηs(t) = ∫h(t −t′)s(t′ −τ)dt′
and
ηn(t) = ∫h(t −t′)n(t′)dt′.
The signal output ηs(τ) at time τ should be as large as possible relative to the noise output
ηn(τ).
The noise is modeled as a random process. Thermal noise in the receiver is well approx-
imated by white noise, which means that ⟨n(t)n∗(t′)⟩= Nδ(t −t′), where N corresponds
to the noise power and ⟨⋅⟩denotes expected value. Since the noise is random, so is ηn. Thus
the signal-to-noise (SNR) ratio to be maximized is
SNR =
∣ηs(τ)∣
⟨∣ηn(τ)∣⟩.
(.)


Synthetic Aperture Radar Imaging
First, the denominator of (> .) is
⟨∣ηn(τ)∣⟩= ⟨∣∫h(τ −t′)n(t′)dt′∣

⟩= ⟨∫h(τ −t′)n(t′)dt′ (∫h(τ −t′′)n(t′′)dt′′)
∗
⟩
= ∬h(τ −t′)h∗(τ −t′′)⟨n(t′)n∗(t′′)⟩

Nδ(t′−t′′)
dt′ dt′′
= N ∫∣h(τ −t′)∣
dt′ = N ∫∣h(t)∣dt,
where in the last line the change of variables t = τ −t′ has been made, and where the star
denotes complex conjugation. Thus (> .) becomes
SNR = ∣∫h(τ −t′)s(t′ −τ)dt′∣
N ∫∣h(t)∣dt
= ∣∫h(t)s(−t)dt∣
N ∫∣h(t)∣dt
,
(.)
where in the numerator the change of variables t = τ −t′ has been made. To the numerator
of (> .), the Cauchy–Schwarz inequality can be used to conclude that the numerator,
and therefore the quotient (> .), is maximized when h is chosen, so that
h(t) = s∗(−t).
This is the impulse response of the matched filter. Thus to obtain the best signal-to-noise
ratio, the received signal should be convolved with the time-reversed, complex-conjugated
version of the expected signal.
With this choice, the filter (> .) can be written as
η(t) = ∫h(t −t′′)srec(t′′)dt′′ = ∫s∗(t′′ −t)srec(t′′)dt′′ = ∫s∗(t′)srec(t′ + t)dt′,
(.)
which is a correlation between s and srec. If s = srec, (> .) is called an autocorrelation.
Radar receivers which perform this kind of signal processing are known as “correlation
receivers.”
The Eﬀect of Matched Filtering on Radar Data
When applied to (> .), the output of the correlation receiver is
η(t,x) ≈∫p∗(t′ −t)Esc
B (t′,x)dt′
= ∫( 
π ∫eiω′(t′−t)P∗(ω′)dω′)∬
e−iω(t′−∣x−z∣/c)
π(π∣x−z∣)P(ω)V(z)dωdz dt′
= ∭

π ∫ei(ω−ω)t′dt′

δ(ω′−ω)
e−iω(t−∣x−z∣/c)
(π∣x−z∣)P(ω)P∗(ω′)V(z)dω′dωdz
= ∬
e−iω(t−∣x−z∣/c)
(π∣x−z∣)∣P(ω)∣V(z)dωdz.
(.)

Synthetic Aperture Radar Imaging 

Thus, the effect of matched filtering is simply to replace P(ω) in the first line of (> .)
by π∣P(ω)∣.
..
The Small-Scene Approximation
The small-scene approximation, namely
∣x −y∣= ∣x∣−ˆx ⋅y + O (∣y∣
∣x∣),
(.)
where ˆx denotes a unit vector in the direction x, is often applied to situations in which the
scene to be imaged is small in comparison with its average distance from the radar. This
approximation is valid for ∣x∣≫∣y∣.
Use of (> .) in (> .) gives rise to the large-∣x∣expansion of the Green’s function
[, ]
G(ω,x −y) =
eik∣x−y∣
π∣x −y∣= eik∣x∣
π∣x∣e−ikˆx⋅y (+ O (∣y∣
∣x∣)) (+ O (k∣y∣
∣x∣)).
(.)
Here, the first-order term must be included in the exponential because kˆx ⋅y can take on
values that are large fractions of π.
Small-Scene, Matched-Filtered Radar Data
In (> .), the origin of coordinates can be chosen to be in or near the target, and then the
small-scene expansion (> .) (with z playing the role of y) can be used in the matched-
filtered version of (> .). This results in the expression for the matched-filtered data:
ηB(t) =

(π)∣x∣∬e−iω(t−∣x∣/c+ˆx⋅z/c)∣P(ω)∣V(z)dω dz.
(.)
The inverse Fourier transform of (> .) gives
DB(ω) =
eik∣x∣
(π)∣x∣∣P(ω)∣∫e−ikˆx⋅zV(z)dz

F[V](kˆx)
.
(.)
Thus we see that each frequency component of the data provides us with a Fourier
component of the reflectivity V.
..
The Range Proﬁle
Signals with large bandwidth are commonly used in synthetic-aperture imaging. When
the bandwidth is large, the pulse p is said to be a high-range-resolution (HRR) pulse.


Synthetic Aperture Radar Imaging
An especially simple large bandwidth signal is one for which ∣P(ω)∣is constant over its
support. In this case, the ω-integral in > Eq. (.) reduces to a scaled sinc(t) function
centered on
t = ∣x∣/c + ˆx⋅z/c,
and the width of this sinc function is inversely proportional to the bandwidth. When the
support of ∣P(ω)∣is infinite, of course, this sinc(t) becomes a delta function. Thus large-
bandwidth (HRR), matched-filtered data can be approximated by
ηB(t) ≈

(π)∣x∣∫δ(t −∣x∣/c + ˆx⋅z/c)V(z)dz.
(.)
Since time delay and range are related in monostatic radar systems as t
=
R/c,
> Eq. (.) can be seen to be a relation between the radar data ηB(t) and the integral
0
0
−20
−40
 y (m)
Radar
10 log|hB|2 (dB)
10
20
30
40
⊡Fig. -
Example of an HRR range proﬁle of an aircraft (orientation displayed in inset)

Synthetic Aperture Radar Imaging 

of the target reflectivity function over the plane
R = ∣x∣+̂x⋅z
(with respect to the radar). Such data are said to form a “range profile” of the target. An
example of an HRR range profile is displayed in > Fig. -.
.
Survey on Mathematical Analysis of Methods
The mathematical models discussed above assume that the target V(z) is stationary during
its interaction with a radar pulse. However, synthetic-aperture imaging techniques assume
that the target moves with respect to the radar between pulses.
..
Inverse Synthetic-Aperture Radar (ISAR)
A fixed radar system staring at a rotating target is equivalent (by change of reference frame)
to a stationary target viewed by a radar moving (from pulse to pulse) on a circular arc.
This circular arc will define, over time, a synthetic aperture and sequential radar pulses
can be used to sample those data that would be collected by a much larger radar antenna.
Radar imaging based on such a data collection configuration is known as Inverse Synthetic-
Aperture Radar (ISAR) imaging [, , , , , ]. This imaging scheme is typically used
for imaging airplanes, spacecraft, and ships. In these cases, the target is relatively small and
usually isolated.
Modeling Rotating Targets
The target reflectivity function in a frame fixed to the target is denoted by q. Then, as seen
by the radar, the reflectivity function is V(x) = q(O(θn)x), where O is an orthogonal
matrix and where tn = θn denotes the time at the start of the n-th pulse of the sequence.
For example, if the radar is in the plane perpendicular to the axis of rotation (so-called
“turntable geometry”), then the orthogonal matrix O can be written
O(θ) =
⎛
⎜
⎝
cos θ
−sin θ

sin θ
cos θ




⎞
⎟
⎠
(.)
and V(x) = q(xcos θ −xsin θ, xsin θ + xcos θ, x).
Radar Data from Rotating Targets
The use of V(x) = q (O(θn)x) in (> .) provides a model for the data from the nth
pulse:
DB(ω, θn) =
eik∣x∣
(π)∣x∣∣P(ω)∣∫e−ikˆx⋅zq(O(θn)z

y
)dz.
(.)


Synthetic Aperture Radar Imaging
w1
w2
f
⊡Fig. -
The data-collection manifold for turntable geometry
In (> .), the change of variables y = O(θn)z is made. Then use is made of the fact
that the inverse of an orthogonal matrix is its transpose, which means that x⋅O−(θn)y =
O(θn)x⋅y. The result is that (> .) can be written in the form
DB(ω, θn) =
eik∣x∣
(π)∣x∣∣P(ω)∣∫e−ikO(θ n)ˆx⋅yq(y)dy

∝F[q](kO(θ n)ˆx)
.
(.)
Thus, the frequency-domain data are proportional to the inverse Fourier transform of q,
evaluated at points in a domain defined by the angles of the sampled target orientation
and the radar bandwidth (see > Fig. -). Consequently, a Fourier transform produces a
target image.
The target rotation angle is usually not known. However, if the target is rotating with
constant angular velocity, the image produced by the Fourier transform gives rise to a
stretched or contracted image, from which the target is usually recognizable [, , , ].
...
The Data Collection Manifold
The Fourier components of the target that can be measured by the radar are those in the set
Ωz = {kO(θn)ˆx},
(.)
where n ranges over the indices of pulses for which the point z is in the antenna beam, and
where k = ω/c with ω ranging over the angular frequencies received by the radar receiver.
The region determined in this manner is called the data-collection manifold. The extent of
the set of angles is called the synthetic aperture, and the extent of the set of frequencies
is called the bandwidth. Typical synthetic apertures are on the order of a few degrees and
bandwidths of π × × rad/s are not uncommon.
> Figure -shows an exam-
ple of data collection manifold corresponding to turntable geometry;
> Fig. -shows
others that correspond to more complex motion. Typical SAR data-collection manifolds

Synthetic Aperture Radar Imaging 

12
10
8
6
4
2
0
–10
–5
0
5
10
–10
–5
0
5
10
ky (GHz)
kx (GHz)
kz (GHz)
⊡Fig. -
The dark surfaces represent some typical data-collection manifolds that are subsets of a
more complete “data dome”
are two-dimensional manifolds. The larger the data-collection manifold at z, the better the
resolution at z.
Examples of ISAR images are shown in > Figs. -and > -.
...
ISAR in the Time Domain
Fourier transforming (> .) into the time domain results in
ηB(t, θn) ∝∬e−iω(t−∣x∣/c+O(θ n)ˆx⋅y/c)∣P(ω)∣dω q(y)dy.
(.)
Evaluation of ηB at a shifted time results in the simpler expression
ηB (t + ∣x∣
c
, θn) = ∬e−iω(t+O(θ n)ˆx⋅y/c)∣P(ω)∣dω q(y)dy.
(.)
With the temporary notation τ = −O(θn)ˆx ⋅y/c, the ω integral on the right side of
(> .) can be written as
∫e−iω(t−τ)∣P(ω)∣dω = ∫δ(s −τ)β(t −s)ds,
(.)
where
β(t −s) = ∫e−iω(t−s)∣P(ω)∣dω.


Synthetic Aperture Radar Imaging
30
–40
–30
–20
–10
0
20
Image intensity (dB)
10
–20
0
20
Cross-range (m)
Down-range (m)
⊡Fig. -
An ISAR image of a Boeing from a ○aperture []
⊡Fig. -
On the left is an ISAR image of a ship; on the right is an optical image of the same ship
(Courtesy Naval Research Laboratory)

Synthetic Aperture Radar Imaging 

With (> .), ηB can be written
ηB (t + ∣x∣
c
, θn) = ∫β(t −s)∫δ (s + O(θn)ˆx
c
⋅y) q(y)dy ds
= β ∗R[q](−O(θn)ˆx
c
),
where
R[q](s, ˆµ) = ∫δ(s −ˆµ ⋅y)q(y)dy
(.)
is the Radon transform [, ]. Here ˆµ denotes a unit vector. In other words, the Radon
transform of q is defined as the integral of q over the plane s = ˆµ ⋅y.
ISAR systems typically use a high-range-resolution (large bandwidth) waveform, so
that β ≈δ (see > Sect. ..). Thus ISAR imaging from time-domain data becomes a
problem of inverting the Radon transform.
..
Synthetic-Aperture Radar
In ISAR, the target rotates and the radar is stationary, whereas in Synthetic-Aperture Radar
(SAR), the target is stationary and the radar moves. (In typical ISAR data collection scenar-
ios, both the radar and the target are actually in motion, and so this distinction is somewhat
arbitrary.), For most SAR systems [, , , , , ], the antenna is pointed toward the
earth. For an antenna viewing the earth, an antenna beam pattern must be included in the
model. For highly directive antennas, often simply the antenna “footprint,” which is the
illuminated area on the ground, is used.
For a receiving antenna at the same location as the transmitting antenna, the scalar
Born model for the received signal is
SB(ω) = ∫eik∣x−y∣A(ω,x,y)V(y)dy,
(.)
where A incorporates the geometrical spreading factors ∣x−y∣−, transmitted waveform,
and antenna beam pattern. More details can be found in [].
SAR data collection systems are usually configured to transmit a series of pulses with
the nth pulse transmitted at time tn. The antenna position at time tn is denoted by γn.
Because the time scale on which the antenna moves is much slower than the time scale
on which the electromagnetic waves propagate, the time scales separate into a slow time,
which corresponds to the n of tn, and a fast time t.
In (> .) the antenna position xis replaced by γn:
D(ω, n) = F[V](ω, s) ≡∫eik∣γ n−y∣A(ω, n,y)V(y)dy,
(.)
where with a slight abuse of notation, the xin the argument of A has been replaced by n.
This notation also allows for the possibility that the waveform and antenna beam pattern


Synthetic Aperture Radar Imaging
could be different at different points along the flight path. The time-domain version of
(> .) is
d(t, n) = ∫e−iω[t−∣γ n−y∣/c]A(ω, n,y)V(y)dy.
(.)
The goal of SAR is to determine V from the data d.
As in the case of ISAR, assuming that γ and A are known, the data depend on two
variables, so it should be possible to form a two-dimensional image. For typical radar
frequencies, most of the scattering takes place in a thin layer at the surface. The ground
reflectivity function V is therefore assumed to be supported on a known surface. For
simplicity this surface is assumed to be a flat plane, so that V(x) = V(x)δ(x), where
x = (x, x).
SAR imaging comes in two basic varieties: spotlight SAR [, ] and stripmap SAR
[, , , ].
...
Spotlight SAR
Spotlight SAR is illustrated in
> Fig. -. Here, the moving radar system stares at a spe-
cific location (usually on the ground), so that at each point in the flight path the same
scene is illuminated from a different direction. When the ground is assumed to be a
horizontal plane, the iso-range curves are large circles whose centers are directly below
the antenna at γn. If the radar antenna is highly directional and the antenna footprint
is sufficiently far away, then the circular arcs within the footprint can be approximated
as lines. Consequently, the imaging method is mathematically the same as that used
in ISAR.
⊡Fig. -
In spotlight SAR, the radar is trained on a particular location as the radar moves. In this
ﬁgure, the equi-range circles (dotted lines) are formed from the intersection of the radiated
spherical wavefront and the surface of a (ﬂat) earth

Synthetic Aperture Radar Imaging 

In particular, the origin of coordinates is taken within the footprint, and the small-
scene expansion is used, which results in an expression for the matched-filtered frequency-
domain data:
D(ω, n) = eik∣γ n∣∫eik̂γ n⋅yV(y)A(ω, n,y)dy.
(.)
Within the footprint, A is approximated as a product A = A(ω, n)A(y). The function A
can be taken outside the integral; the function Acan be divided out after inverse Fourier
transforming.
As in the ISAR case, the time-domain formulation of spotlight SAR leads to a prob-
lem of inverting the Radon transform. An example of a spotlight SAR image is shown in
> Fig. -.
...
Stripmap SAR
Stripmap SAR sweeps the radar beam along with the platform without staring at a particu-
lar location on the ground (> Fig. -). The equi-range curves are still circles, but the data
no longer depend only on the direction from the antenna to the scene. Moreover, because
the radar does not stare at the same location, there is no natural origin of coordinates for
which the small-scene expansion is valid.
To form a stripmap SAR image, the expression (> .) must be inverted without
the help of the small-scene approximation. One strategy is to use a filtered adjoint of the
forward map F defined by > Eq. (.).
⊡Fig. -
Stripmap SAR acquires data without staring. The radar typically has ﬁxed orientation with
respect to the ﬂight direction and the data are acquired as the beam footprint sweeps over
the ground


Synthetic Aperture Radar Imaging
The Formal Adjoint of F
The adjoint F† is an operator such that
⟨f , Fg⟩ω,s = ⟨F† f , g⟩x,
(.)
where ⟨⋅,⋅⟩denotes inner product in the appropriate variables. More specifically, (> .)
can be written as
∫
f (ω, s)(Fg)∗(ω, s)dω ds = ∫(F† f )(x)g∗(x)dx.
(.)
Use of (> .) in (> .) and an interchange of the order of integration lead to
F† f (x) = ∬e−ik∣γ(s)−x∣A(ω, s,x)f (ω, s)dω ds.
(.)
The Imaging Operator
Thus, the imaging operator is assumed to be of the form
I(z) = B[D](z) ≡∬e−ik∣γ(s)−zT∣Q(ω, s,z)D(ω, s)dω ds,
(.)
where zT = (z,) and Q is a filter to be determined below. The time-domain version is
I(z) = B[d](z) ≡∬eiω(t−∣γ(s)−zT∣/c)Q(ω, s,z)dωd(t, s)ds dt.
(.)
If the filter Q were to be chosen to be identically , then, because of (> .), the time-
domain inversion would have the form
I(z) = ∬δ(t −∣γ(s) −zT∣/c)d(t, s)ds dt
= ∫d(∣γ(s) −zT∣/c, s)ds,
(.)
which can be interpreted as follows: At each antenna position s, the data is backprojected
(smeared out) to all the locations z that are at the correct travel time ∣γ(s)−zT∣/c from the
antenna location γ(s). Then all the contributions are summed coherently (i.e., including
the phase).
> Figure -shows the partial sums over s as the antenna (white triangle)
moves along a straight flight path from bottom to top.
An alternative interpretation is that to form the image at the reconstruction point z,
all the contributions from the the data at all points (t, s) for which t = ∣γ(s) −zT∣/c are
coherently summed.
Note the similarity between (> .) and (> .): (> .) backprojects over lines
or planes, whereas (> .) backprojects over circles. The inversion (> .) first applies
the filter Q and then backprojects.
Other SAR Algorithms
The image formation algorithm discussed here is filtered backprojection. This algorithm
has many advantages, one of them being great flexibility. This algorithm can be used for
any antenna beam pattern, for any flight path, and for any waveform; a straightfoward
extension [] can be used in the case when the topography is not flat.

Synthetic Aperture Radar Imaging 

Nevertheless, there are various other algorithms that can be used in special cases, for
example, if the flight path is straight, if the antenna beam is narrow, or if a chirp waveform
is used. Discussions of these algorithms can be found in the many excellent radar imaging
books such as [, , , , ].
..
Resolution for ISAR and Spotlight SAR
To determine the resolution of an ISAR image, the relationship between the image and the
target is analyzed.
For turntable geometry, (> .) is used. The viewing direction is taken to be
x= (,,), with ˆeθ = O(θ)xand ˜k = k. Then (> .) is proportional to
̃D(˜k, θ) = ∫e−i˜kˆeθ⋅yq(y)dy
= ∬e−i˜k(ycos θ+ysin θ) ∫q(y, y, y)dy

˜q(y,y)
dydy.
(.)
The data depend only on the quantity ˜q(y, y) = ∫q(y, y, y)dy, which is a pro-
jection of the target onto the plane orthogonal to the axis of rotation. In other words,
in the turntable geometry, the radar imaging projection is the projection onto the hori-
zontal plane. With the notation y = (y, y), so that y = (y, y), it is clear that ˆeθ ⋅y =
(Pˆeθ) ⋅y, where P : R→Rdenotes the projection onto the first two components of a
three-dimensional vector.
The data-collection manifold Ω = {˜kˆeθ : ω< ω < ωand ∣θ∣< Φ} is shown in
> Fig. -. Then (> .) can be written as
̃D(˜k, θ) = χΩ(˜kˆeθ)F[˜q](˜kˆeθ),
(.)
where χΩ(˜kˆeθ) denotes the function that is if ˜kˆeθ ∈Ω and otherwise.
The image is formed by taking the two-dimensional inverse Fourier transform of
(> .):
I(x) = ∬eix⋅˜k(Pˆeθ) ̃D(˜k, θ)˜k d˜kdθ ∝∫Ω eix⋅˜k(Pˆeθ) ∬e−iy⋅˜k(Pˆeθ) ˜q(y)dy˜k d˜k dθ
= ∫∬Ω ei(x−y)⋅˜k(Pˆeθ)˜k d˜k dθ

K(x −y)
˜q(y)dy.
(.)
The function K is the point-spread function (PSF); it is also called the imaging kernel,
impulse response, or sometimes ambiguity function. The PSF can be written as
K(x) ∝∬Ω eix⋅˜k(Pˆeθ)˜k d˜k dθ = ∫
∣ξ∣=˜k
∣ξ∣=˜k
∫
Φ
−Φ eix⋅˜k(Pˆeθ)˜k d˜k dθ.
(.)


Synthetic Aperture Radar Imaging
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
30
25
20
15
10
5
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
5
0
20
15
10
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
20
15
10
5
0
–5
–10
–15
–20
–20–15–10 –5
0
5
10 15 20
⊡Fig. -
This shows successive steps in the backprojection procedure for a straight ﬂight path and an
isotropic antenna. The ﬁrst image is the true scene; the second is the magnitude of the data.
The successive images show the image when the antenna has traveled as far as the location
indicated by the small triangle
It can be calculated by writing
x = r(cos ψ,sin ψ)
and
(Pˆeθ) = (cos ϕ,sin ϕ),
(.)
so that x ⋅(Pˆeθ) = r cos(ϕ −ψ). The “down-range” direction corresponds to ψ = and
“cross-range” corresponds to ψ = π/.
...
Down-Range Resolution in the Small-Angle Case
For many radar applications, the target is viewed from only a small range of aspects ˆeθ; in
this case, the small-angle approximations cos ϕ ≈and sin ϕ ≈ϕ can be used.
In the down-range direction (ψ = ), under the small-angle approximation, (> .)
becomes
K(r,) ≈∫
˜k
˜k
˜k ∫
Φ
−Φ ei˜kr dϕ d˜k
= Φ ∫
˜k
˜k
˜kei˜kr d˜k = Φ
i
d
dr ∫
˜k
˜k
ei˜kr d˜k
= Φ
i
d
dr [ei˜kr b
sincbr
],
(.)

Synthetic Aperture Radar Imaging 

where b = ˜k−˜k= πB/c, B is the bandwidth in Hertz, and ˜k= (˜k+˜k)/= π( + ) =
π , where
is the center frequency in Hertz.
Since ˜k≫b, the leading order term of (> .) is obtained by differentiating the
exponential:
K(r,) ≈b˜kΦ ei˜kr sinc 
br,
(.)
yielding peak-to-null down-range resolution π/b = c/(B). Here, it is the sinc function
that governs the resolution.
...
Cross-Range Resolution in the Small-Angle Case
In the cross-range direction (ψ = π/), the approximation cos(ϕ −ψ)= sin ϕ ≈ϕ
holds under the small-angle assumption. With this approximation, the computation
of (> .) is
Supp (K) radon
Re (K) radon
^
200
0
0
4
–1
0
0
1
1
1
–1
0
0
1
–1
–1
–1
Cross-section
Supp (K) radon
Re (K) radon
^
Cross-section
2000
1000
1000
–1000
0
0
–1
1
0
0
–1000
1
–2000
–200
–400
–200
–400
400
200
0
0
4
–200
–400
–200
–400
400
⊡Fig. -
From left to right: the data collection manifold, the real part of K, cross sections (horizontal is
rapidly oscillating, vertical is slowly oscillating) through the real part of K for the two cases.
Down-range is horizontal (Reprinted with permission from [])


Synthetic Aperture Radar Imaging
K(, r) ≈∫
˜k
˜k
˜k ∫
Φ
−Φ ei˜krϕ dϕ d˜k
= ∫
˜k
˜k
˜k ei˜krΦ −e−i˜krΦ
i˜kr
d˜k
= 
ir [ei˜krΦb sinc ( 
brΦ) −e−i˜krΦb sinc ( 
brΦ)]
= b˜kΦ sinc ( 
brΦ) sinc(˜krΦ).
(.)
Since ˜k≫b,
K(, r) ≈b˜kΦ sinc(˜krΦ).
(.)
Thus the peak-to-null cross-range resolution is π/(˜kΦ) = c/(Φ) = λ/(Φ). Since
the angular aperture is Φ, the cross-range resolution is λdivided by twice the angular
aperture.
Example
> Figure -shows a numerical calculation of K for ϕ = ○, and two different frequency
bands: [˜k, ˜k] = [,], (i.e, b = and ˜k= , and [˜k, ˜k] = [,]) (i.e., b = 
and ˜k= ). The first case is not relevant for most radar systems, which do not transmit
frequencies near zero, but is relevant for other imaging systems such as X-ray tomography.
These results are plotted in > Fig. -.
.
Numerical Methods
..
ISAR and Spotlight SAR Algorithms
The Polar Format Algorithm (PFA)
For narrow-aperture, turntable-geometry data, such as shown in > Fig. -, the Polar For-
mat Algorithm (PFA) is commonly used. The PFA consists of the following steps, applied
to frequency-domain data.
. Interpolate from a polar grid to a rectangular grid (see > Fig. -.)
. Use the two-dimensional Discrete (inverse) Fourier Transform to form an image of q.
Alternatively, algorithms for computing the Fourier transform directly from a nonuniform
grid can be used [, , ].
Inversion by Filtered Backprojection
For the n-dimensional Radon transform, one of the many inversion formulas [, ] is
f =

(π)n−R†I−n (R[f ]) ,
(.)

Synthetic Aperture Radar Imaging 

Polar data grid
Rectangular
interpolating grid
Measured data
Interpolated data
⊡Fig. -
This illustrates the process of interpolating from a polar grid to a rectangular grid
where I−n is the Riesz operator (filter)
Iα f = F−[∣∣−αF f ]
(.)
operating on the s variable, and the operator R† is the formal adjoint of R. (Here the term
“formal” means that the convergence of the integrals is not considered; the identities are
applied only to functions that decay sufficiently rapidly, so that the integrals converge.) The
adjoint is defined by the relation
⟨Rf , h⟩s,̂µ = ⟨f ,R†h⟩x,
(.)
where
⟨Rf , h⟩s,̂µ = ∬R(s,̂µ)h∗(s,̂u)ds d̂µ
(.)
and
⟨f ,R†h⟩x = ∫
f (x)[R†h]∗(x)dx.
Using (> .) in (> .) and interchanging the order of integration shows that the
adjoint R† operates on h(s, µ) via
(R†h)(x) = ∫Sn−h(x ⋅̂µ,̂µ)d̂µ.
(.)
Here R† integrates over the part of h corresponding to all planes (n = ) or lines (n = )
through x. When R† operates on Radon data, it has the physical interpretation of backpro-
jection. For example, in the case where h represents Radon data from a point-like target, for
a fixed direction ˆµ, the quantity h(x ⋅̂µ,̂µ), as a function of x, is constant along each plane
(or line if n = ) x⋅ˆµ = constant. Thus, at each ˆµ, the function h(x⋅̂µ,̂µ) can be thought of
as an image in which the data h for direction ˆu is backprojected (smeared) onto all points
x that could have produced the data for that direction. The integral in (> .) then sums


Synthetic Aperture Radar Imaging
Single pulse ambiguity
Multiple pulse “imaging”
⊡Fig. -
This ﬁgure illustrates the process of backprojection. The range proﬁles (inset) suggest the
time delays that would result from an interrogating radar pulse incident from the indicated
direction. Note that scatterers that lie at the same range from one view do not lie at the
same range from other views
the contributions from all the possible directions. (See > Fig. -.) The inversion formula
(>.) is thus a filtered backprojection formula. Fast backprojection algorithms have been
developed by a number of authors (e.g., [, ]).
..
Range Alignment
ISAR imaging relies on target/radar relative motion. An assumption made throughout is
that the target moves as a rigid body – an assumption that ignores the flexing of aircraft lift
and control surfaces, or the motion of vehicle treads. Moreover, arbitrary rigid body motion
can always be separated into a rotation about the body’s center of mass and a translation
of that center of mass. Backprojection shows how the rotation part of the relative radar/
target motion can be used to reconstruct a two-dimensional image of the target in ISAR
and spotlight SAR. But, usually while the target is rotating and the radar system is col-
lecting data, the target will also be translating and this has not been accounted for in the

Synthetic Aperture Radar Imaging 

–50
–50
–40
–40
–30
–30
–20
–20
–10
–10
0
10
20
30
40
50
0
10
20
30
40
50
x (m)
y (m)
⊡Fig. -
A radar image from a circular ﬂight path, together with an optical image of the same scene.
The bright ring in the top half of the radar image is a “top-hat”calibration target used to
focus the image (Courtesy US Air Force Sensors Directorate)
imaging algorithm. In > Eqs. (.) and (> .), for example, R = ∣x∣was implicitly set
to a constant.
Typically, the radar data are preprocessed to subtract out the effects of target translation
before the imaging step is performed. Under the start–stop approximation, the range pro-
file data ηB(t, θn) is approximately a shifted version of the range profile (see > Sect. ..)
at the previous pulse. Thus ηB(t, θn+) ≈ηB(t + Δtn, θn), where Δtn is a range offset that
is determined by target motion between pulses.
The collected range profiles can be shifted to a common origin if Δtn can be determined
for each θn. One method to accomplish this is to assume that one of the peaks in each
of the range profiles (for example, the strongest peak) is always due to the same target
feature and so provides a convenient origin. This correction method is known as “range
alignment” and must be very accurate in order to correct the offset error to within a fraction


Synthetic Aperture Radar Imaging
0.2
0.1
0.0
0
10
20
30
y (m)
Alignment peak
10 log|hB|2 (dB)
q°
⊡Fig. -
Range alignment preprocessing in synthetic-aperture imaging. The eﬀects of target
translation must be removed before backprojection can be applied
of a wavelength. (Note that the wavelength in question is that of the signal output by the
correlation receiver and not the wavelength of the transmitted waveform. In HRR systems,
however, this wavelength can still be quite small.) Typically, ηB(t + Δtn, θn) is correlated
with ηB(t, θn+), and the correlation maximum is taken to indicate the size of the shift Δtn.
This idea is illustrated in > Fig. -which displays a collection of properly aligned range
profiles.
When the scattering center used for range alignment is not a single point but, rather,
several closely spaced and unresolved scattering centers, then additional constructive and
destructive interference effects can cause the range profile alignment feature – assumed to
be due to a single well-localized scatterer – to vary rapidly across the synthetic aperture
(i.e., such scattering centers are said to “scintillate”). For very complex and scintillating
targets, other alignment methods are used: for example, if the target is assumed to move
along a “smooth” path, then estimates of its range, range rate, range acceleration, and range
jerk (time derivative of acceleration) can be used to express target range as a polynomial
in time
R(θn) = R() + ˙Rθn + 

¨Rθ
n + 

...
Rθ
n.
(.)
In terms of this polynomial,
Δtn = R(θn) −R()
c
= 
˙Rθn + 
¨Rθ
n + 

...
Rθ
n
c
,
(.)
where ˙R, ¨R, and ...
R are radar measurables.
Of course, the need for range alignment preprocessing is not limited to ISAR imaging;
similar motion compensation techniques are needed in SAR as well (> Fig. -).

Synthetic Aperture Radar Imaging 

Motion compensated
Uncompensated
Three-meter resolution
⊡Fig. -
The eﬀect of motion compensation in a Ku-band image (Courtesy Sandia National
Laboratories)
.
Open Problems
In the decades since the invention of synthetic-aperture radar imaging, there has been
much progress, but many open problems still remain. And most of these open problems
are mathematical in nature.
As outlined at the beginning of > Sect. ., SAR imaging is based on specific assump-
tions, which in practice may not be satisfied. When they are not satisfied, artifacts appear
in the image.
..
Problems Related to Unmodeled Motion
SAR image-formation algorithms assume the scene to be stationary. Motion in the scene
gives rise to mispositioning or streaking (see > Figs. -, > -). This effect is analyzed
in [].
However, it is of great interest to use radar to identify moving objects; systems that can
do this are called Moving Target Indicator (MTI) systems or Ground Moving Target Indicator
(GMTI) systems.
. How can artifacts associated with targets that move during data collection [] be mit-
igated? Moving targets cause Doppler shifts and also present different aspects to the
radar []. An approach for exploiting unknown motion is given in [].


Synthetic Aperture Radar Imaging
⊡Fig. -
A Ku-band image showing streaking due to objects moving in the scene (Courtesy Sandia
National Laboratories and SPIE)
. Both SAR and ISAR are based on known relative motion between target and sensor, for
example, including the assumption that the target behaves as a rigid body. When this
is not the case, the images are blurred or uninterpretable. Better methods for finding
the relative motion between target and sensor are also needed [, ]. Better algorithms
are needed for determining the antenna position from the radar data itself. Such meth-
ods include autofocus algorithms [, ], some of which use a criterion such as image
contrast to focus the image.
. When the target motion is complex (pitching, rolling and yawing), it may be pos-
sible to form a three-dimensional image; fast, accurate methods for doing this
are needed []. How can moving objects be simultaneously tracked [] and
imaged?
..
Problems Related to Unmodeled Scattering Physics
. How can images be formed without the Born approximation? The Born approximation
leaves out many physical effects, including not only multiple scattering and creeping
waves, but also shadowing, obscuration, and polarization changes. But without the
Born approximation (or the Kirchhoff approximation, which is similar), the imaging
problem is nonlinear. In particular, how can images be formed in the presence of mul-
tiple scattering? (See [, , , , ].) Artifacts due to the Born approximation can be

Synthetic Aperture Radar Imaging 

⊡Fig. -
A -in. resolution of SAR image from Sandia National Laboratories. Only certain parts of the
airplanes reﬂect radar energy. The inset is an optical image of the airplanes (Courtesy Sandia
National Laboratories)
seen in > Fig. -, where the vertical streaks near the tail are due to multiple scattering
in the engine inlets. Can multiple scattering be exploited [, ] to improve resolution?
. Scattering models need to be developed that include as much of the physics as possible,
but that are still simple enough for use in the inverse problem. An example of a simple
model that includes relevant physics is [].
. How can polarization information [, , , , ] be exploited? This problem is closely
connected to the issue of multiple scattering: the usual linear models predict no change
in the polarization of the backscattered electric field. Consequently linear imaging
methods cannot provide information about how scatterers change the polarization of
the interrogating field. A paper that may be useful here is [].
. How can prior knowledge about the scene be incorporated in order to improve resolu-
tion? There is interest in going beyond simple aperture/bandwidth-defined resolution
[, ]. One approach that has been suggested is to apply compressive sensing ideas
[, , ] to SAR.
. How can information in the radar shadow be exploited? In many cases it is eas-
ier to identify an object from its shadow than from its direct-scattering image.
(See
> Fig. -) A backprojection method for reconstructing an object’s three-
dimensional shape from its shadows obtained at different viewing angles is proposed
in []. What determines the resolution of this reconstruction?


Synthetic Aperture Radar Imaging
⊡Fig. -
A -in. resolution image from Sandia National Laboratories. Note the shadows of the
historical airplane, helicopter, and trees (Courtesy Sandia National Laboratories)
..
New Applications of Radar Imaging
. Can radar systems be used to identify individuals by their gestures or gait? Time-
frequency analysis of radar signals gives rise to micro-Doppler time-frequency images
[], in which the motion of arms and legs can be identified.
. How can radar be used to form images of urban areas? It is difficult to form sar
images of urban areas, because in cities the waves undergo complicated multipath scat-
tering. Areas behind buildings lie in the radar shadows, and images of tall buildings
can obscure other features of interest. In addition, urban areas tend to be sources of
electromagnetic radiation that can interfere with the radiation used for imaging.
One approach that is being explored is to use a persistent or staring radar system []
that would fly in circles [] around a city of interest (See, For example
> Fig. -).
Thus, the radar would eventually illuminate most of the areas that would be shadowed
when viewed from a single direction. However, this approach has the added difficulty
that that same object will look different when viewed from different directions. How
can the data from a staring radar system be used to obtain the maximum amount of
information about the (potentially changing) scene?
. If sensors are flown on Unoccupied Aerial Vehicles (UAVs), where should these UAVs
fly? The notion of swarms of UAVs [] gives rise not only to challenging problems in
control theory but also to challenging imaging problems.

Synthetic Aperture Radar Imaging 

. Many of these problems motivate a variety of more theoretical open problems such as
the question of whether backscattered data uniquely determines a penetrable object or
a non-convex surface [, ]. There is a close connection between radar imaging and
the theory of Fourier Integral Operators []. How can this theory be extended to the
case of dispersive media and to nonlinear operators? Is it possible to develop a theory
of the information content [, ] of an imaging system?
.
Conclusion
Radar imaging is a mathematically rich field with many interesting open problems.
.
Cross-References
> Inverse Scattering
> Linear Inverse Problems
> Tomography
> Wave Phenomena
Acknowledgments
The authors would like to thank the Naval Postgraduate School and the Air Force Office of
Scientific Research, (Because of this support the US Government is authorized to repro-
duce and distribute reprints for Governmental purposes notwithstanding any copyright
notation thereon. The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the official policies or endorsements,
either expressed or implied, of the Air Force Research Laboratory or the US Government.)
which supported the writing of this article under agreement number FA---.
References and Further Reading
. Baraniuk R, Steeghs P (Apr ) Compressive
radar imaging. IEEE radar conference, Waltham
. Bethke B, Valenti M, How JP, Vian J ()
Cooperativevisionbasedestimationandtracking
using multiple UAVs. Conference on cooperative
control and optimization, Gainesville, Jan 
. Bleistein N, Cohen JK, Stockwell JW () The
mathematics of multidimensional seismic inver-
sion. Springer, New York
. Boerner W-M, Yamaguchi Y (June ) A state-
of-the-art review in radar polarimetry and its
applications in remote sensing. IEEE Aerospace
Electr Syst Mag :–
. Borden B () Radar imaging of airborne tar-
gets. Institute of Physics, Bristol
. Borden B () Mathematical problems in
radar
inverse
scattering.
Inverse
Probl
:
R–R


Synthetic Aperture Radar Imaging
. Bowen EG () Radar days. Hilgar, Bristol
. Buderi R () The invention that changed the
world. Simon & Schuster, New York
. Carrara WC, Goodman RG, Majewski RM ()
Spotlight synthetic aperture radar: signal pro-
cessing algorithms. Artech House, Boston
. Cetin M, Karl WC, Castañon DA () Analysis
of the impact of feature-enhanced SAR imag-
ing on ATR performance. Algorithms for SAR
imagery IX, Proceedings of SPIE vol 
. Chen VC, Ling H () Time-frequency trans-
forms for radar imaging and signal analysis.
Artech House, Boston
. CheneyM() A mathematical tutorial on syn-
thetic aperture radar. SIAM Rev :–
. Cheney M, Bonneau RJ () Imaging that
exploits multipath scattering from point Scatter-
ers. Inverse Probl :–
. Cheney M, Borden B () Imaging mov-
ing targets from scattered waves. Inverse Probl
:
. Cheney M, Borden B () Fundamentals of
radar imaging. SIAM, Philadelphia
. Chew WC, Song JM (July ) Fast Fourier
transform of sparse spatial data to sparse Fourier
data. IEEE antenna and propagation interna-
tional symposium, vol pp –
. Cloude
SR
()
Polarization
coherence
tomography. Radio Sci :RS. doi:./
RS
. Cloude SR, -Papathanassiou KP (Sept )
Polarimetric SAR interferometry. IEEE Trans
Geosci Remote Sens (, part ):–
. Cook CE, Bernfeld M () Radar signals. Aca-
demic, New York
. Cumming IG, Wong FH () Digital pro-
cessing
of
synthetic
aperture
radar
data:
algorithms and implementation. Artech House,
Boston
. Curlander JC, McDonough RN () Synthetic
aperture radar. Wiley, New York
. Cutrona LJ () Synthetic aperture radar. In:
Skolnik M (ed) Radar handbook, nd edn.
McGraw-Hill, New York
. Dickey FM, Doerry AW () Recovering
shape from shadows in synthetic aperture radar
imagery. In: Ranney KI, Doerry AW (eds) Radar
sensor technology XII. Proceedings of SPIE, vol
, 
. Ding Y, Munson DC Jr () A fast back-
projection algorithm for bistatic SAR imaging.
Proceedings of the IEEE international conference
on image processing, Rochester, –Sept 
. Edde B () Radar: principles, technology,
applications. Prentice-Hall, Englewood Cliffs
. Elachi C () Spaceborne radar remote sens-
ing: applications and techniques. IEEE Press,
New York
. Ertin E, Austin CD, Sharma S, Moses RL, Potter
LC () GOTCHA experience report: three-
dimensional SAR imaging with complete circular
apertures.Proceedingsof SPIE,vol p 
. Fienup JR (July ) Detecting moving targets in
SAR imagery by focusing. IEEE Trans Aerospace
Electr Syst :–
. Franceschetti G, Lanari R () Synthetic aper-
ture radar processing. CRC Press, New York
. Friedlander FG () Introduction to the the-
ory of distributions. Cambridge University Press,
New York
. Garnier J, Sølna K () Coherent interfer-
ometric imaging for synthetic aperture radar
in the presence of noise. Inverse problems :

. Giuli D (Feb ) Polarization diversity in
radars. Proc IEEE ():–
. Greengard L, Lee J-Y () Accelerating the
nonuniform fast Fourier transform. SIAM Rev
:–
. Jackson JD () Classical electrodynamics, nd
edn. Wiley, New York
. Jakowatz CV, Wahl DE, Eichel PH, Ghiglia DC,
Thompson PA () Spotlight-mode synthetic
aperture radar: a signal processing approach.
Kluwer, Boston
. Ishimaru A () Wave propagation and scatter-
ing in random media. IEEE Press, New York
. Klug A, Crowther RA (Aug ) Three-
dimensional image reconstruction from the
viewpoint
of
information
theory.
Nature
:–. doi:./a.
. Langenberg
KJ,
Brandfass
M,
Mayer
K,
Kreutter
T,
Brüll
A,
Felinger
P,
Huo
D
() Principles of microwave imaging and
inverse scattering. EARSeL Adv Remote Sens :
–
. Lerosey G, de Rosny J, Tourin A, Fink M
(Feb ) Focusing beyond the diffraction

Synthetic Aperture Radar Imaging 

limit with far-field time reversal. Science :
–
. Lee-Elkin F () Autofocus for D imaging.
Proc SPIE :O
. Mensa DL () High resolution radar imaging.
Artech House, Dedham
. Moses R, Çetin M, Potter L (Apr ) Wide
angle
SAR
imaging
(SPIE
Algorithms
for
Synthetic Aperture Radar Imagery XI). SPIE,
Orlando
. Natterer F () The mathematics of computer-
ized tomography, SIAM, Philadelphia
. Natterer F, Wübbeling F () Mathemati-
cal methods in imaging reconstruction. SIAM,
Philadelphia
. Natterer F, Cheney M, Borden B (Dec ) Res-
olution for radar and X-ray tomography. Inverse
Probl :S–S
. Nguyen N, Liu QH () The regular Fourier
matrices and nonuniform fast Fourier trans-
forms. SIAM J Sci Comp :–
. Newton RG () Scattering theory of waves
and particles. Dover, Mineola
. Nolan CJ, Cheney M (Sept ) Synthetic aper-
ture inversion for arbitrary flight paths and
non-flat topography. IEEE Trans Image Process
:–
. Nolan CJ, Cheney M, Dowling T, Gaburro
R () Enhanced angular resolution from
multiply scattered waves. Inverse Probl :
–
. North DO () Analysis of the factors which
determine signal/noise discrimination in radar.
Report PPR C, RCA Laboratories, Prince-
ton (classified). Reproduction: North DO (July
) An analysis of the factors which deter-
mine signal/noise discrimination in pulsed car-
rier Systems. Proc IEEE (): –
. Oppenheim AV, Shafer RW () Digital signal
processing. Prentice-Hall, Englewood Cliffs
. O’Sullivan JA, Blahut RE, Snyder DL ()
Information-theoretic image formation. IEEE
Trans Inform Theory :–
. Oughstun KE, Sherman GC () Electromag-
netic pulse propagation in causal dielectrics.
Springer, New York
. Perry RP, DiPietro RC, Fante RL (Jan ) SAR
imaging of moving targets. IEEE Trans Aerospace
Electr Syst ():–
. Pike R, Sabatier P () Scattering: scattering
and inverse scattering in pure and applied Sci-
ence. Academic, New York
. Potter LC, Moses RL () Attributed scattering
centers for SAR ATR. IEEE Trans Image Process
:–
. Potts D, Steidl G, Tasche M () Fast Fourier
transforms for nonequispaced data: a Tutorial. In:
Benedetto JJ, Ferreira P (eds) Modern sampling
theory: mathematics and applications, Chap .
Birkhäuser, Boston, pp –
. Ramachandra KV () Kalman filtering tech-
niques for radar tracking, CRC Press, Boca Raton
. Rihaczek
AW
()
Principles
of
high-
resolution radar. McGraw-Hill, New York
. Skolnik M () Introduction to radar systems.
McGraw-Hill, New York
. Soumekh M () Synthetic aperture radar sig-
nal processing with MATLAB algorithms. Wiley,
New York
. Stakgold I () Green’s functions and bound-
ary value problems, nd edn. Wiley-Interscience,
New York
. Stefanov P, Uhlmann G () Inverse backscat-
tering for the acoustic equation. SIAM J Math
Anal :–
. Stimson GW () Introduction to airborne
radar. SciTech, Mendham
. Stuff MA, Sanchez P, Biancala M () Extrac-
tion of three-dimensional motion and geomet-
ric invariants. Multidim Syst signal Process :
–
. Sullivan RJ () Radar foundations for imaging
and advanced concepts. SciTech, Raleigh
. Swords SS () Technical history of the begin-
nings of radar. Peregrinus, London
. Treves F () Basic linear partial differential
equations. Academic, New York
. Treuhaft RN, Siqueira PR () Vertical struc-
ture of vegetated land surfaces from interfero-
metric and polarimetric radar. Radio Sci ():
–
. Trischman JA, Jones S, Bloomfield R, Nelson E,
Dinger R () An X-band linear frequency
modulated radar for dynamic aircraft measure-
ment. AMTA Proceedings. AMTA, New York,
p 
. Ulaby FT, Elachi C (eds) Radar polarimetry for
geoscience applications. Artech House, Norwood


Synthetic Aperture Radar Imaging
. Walsh TE (Nov ) Military radar systems:
history, current position, and future forecast.
Microwave J :, , –
. Weglein AB, Araùjo FV, Carvalho PM, Stolt RH,
Matson KH, Coates RT, Corrigan D, Foster DJ,
Shaw SA, Zhang H (Inverse scattering series and
seismic exploration. Inverse Probl :R–R.
doi: ./-///R
. Wehner D () High-resolution radar, nd edn.
Scitech, Raleigh
. Woodward PM () Probability and informa-
tion theory, with applications to radar. McGraw-
Hill, New York
. Xiao S, Munson DC, Basu S, Bresler Y ()
An NlogN back-projection algorithm for SAR
image formation. Proceedings of th Asilomar
conference on signals, systems, and computers,
Pacific Grove, Oct–Nov 

Tomography
Gabor T. Herman
.
Introduction.....................................................................
.
Background......................................................................
.
Mathematical Modeling and Analysis.........................................
.
Numerical Methods and Case Examples........................................
.
Conclusion........................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Tomography
Abstract: We define tomography as the process of producing an image of a distribution
(of some physical property) from estimates of its line integrals along a finite number of
lines of known locations. We touch upon the computational and mathematical procedures
underlying the data collection, image reconstruction, and image display in the practice of
tomography. The emphasis is on reconstruction methods, especially the so-called series
expansion reconstruction algorithms.
We illustrate the use of tomography (including three-dimensional displays based on recon-
structions) both in electron microscopy and in x-ray computerized tomography (CT), but
concentrate on the latter. This is followed by a classification and discussion of reconstruc-
tion algorithms. In particular, we discuss how to evaluate and compare the practical efficacy
of such algorithms.
.
Introduction
To get the flavor of tomography in general, we first discuss a special case: x-ray comput-
erized tomography (CT) for reconstructing the distribution within a transverse section
of the human body of a physical parameter (the “relative linear attenuation at energy
¯e” whose value at the point (x, y) in the section is denoted by μ¯e(x, y)) from multiple
x-ray projections. A typical method by which data are collected for transverse section
imaging in CT is indicated in
> Fig. -. A large number of measurements are taken.
Each of these measurements is related to an x-ray source position combined with an
x-ray detector position, and from the measurements we can (based on physical principles)
estimate the line integral of μ¯e along the line between the source and the detector. The
mathematical problem is: given a large number of such projections, reconstruct the image
μ¯e(x, y).
A chapter such as this can only cover a small part of what is known about tomogra-
phy. A much extended treatment in the same spirit as this chapter is given in []. For
additional information on mathematical matters related to CT, the reader may consult
the books [, , , , ]. In particular, because of the mathematical orientation of this
handbook, we will not get into the details of how the line integrals are estimated from the
measurements. (Such details can be found in []. They are quite complicated: in addi-
tion to the actual measurement with the patient in the scanner a calibration measurement
needs to be taken, both of these need to be normalized by the reference detector indicated
in
> Fig. -, correction has to be made for the beam hardening that occurs due to the
x-ray beam being polychromatic rather than consisting of photons at the desired energy
¯e, etc.)

Tomography 

Source
y
Compensator
Reference
detector
Z = 0
Reconstruction
region
Z = D
X
Z = Zs
Z = Zd
0
l
Detector
L
q
⊡Fig. -
Data collection for CT (Reproduced from [])
.
Background
The problem of image reconstruction from projections has arisen independently in a large
number of scientific fields. A most important version of the problem in medicine is CT; it
has revolutionized diagnostic radiology over the past decades. The Nobel prize in
physiology and medicine was awarded to Allan M. Cormack and Godfrey N. Hounsfield
for the development of x-ray computerized tomography [, ]. The Nobel prize
in chemistry was awarded to Aaron Klug, one of the pioneers in the use of reconstruc-
tion from electron microscopic projections for the purpose of elucidation of biologically
important molecular complexes [, ]. The Nobel prize in physiology and medicine
was awarded to Paul C. Lauterbur and Peter Mansfield for their discoveries concerning
magnetic resonance imaging, which also included the use of image reconstruction from
projections methods [].


Tomography
In some sense this problem was solved in by Johann Radon []. Let ℓdenote
the distance of the line L from the origin, let θ denote the angle made with the x axis by
the perpendicular drawn from the origin to L (see > Fig. -), and let m(ℓ, θ) denote the
integral of μ¯e along the line L. Radon proved that
μ¯e(x, y) = −
πlim
ε→∫
∞
ε

q ∫
π

m(x cos θ + y sin θ + q, θ) dθ dq,
(.)
where m(ℓ, θ) denotes the partial derivative of m(ℓ, θ) with respect to ℓ. The implication
of this formula is clear: the distribution of the relative linear attenuation in an infinitely
thin slice is uniquely determined by the set of all its line integrals. However,
. Radon’s formula determines an image from all its line integrals. In CT we have only
a finite set of measurements; even if they were exactly integrals along lines, a finite
number of them would not be enough to determine the image uniquely, or even
accurately. Based on the finiteness of the data one can produce objects for which the
reconstructions will be very inaccurate [, Sect. .].
. The measurements in computed tomography can only be used to estimate the line inte-
grals. Inaccuracies in these estimates are due to the width of the x-ray beam, scatter,
hardening of the beam, photon statistics, detector inaccuracies, etc. Radon’s inversion
formula is sensitive to these inaccuracies.
. Radon gave a mathematical formula; we need an efficient algorithm to evaluate it. This
is not necessarily trivial to obtain. There has been a very great deal of activity to find
algorithms that are fast when implemented on a computer and yet produce acceptable
reconstructions in spite of the finite and inaccurate nature of the data. This chapter
concentrates on this topic.
.
Mathematical Modeling and Analysis
The mathematical model for CT is illustrated in > Fig. -. An engineering realization of
this model is shown in > Fig. -. The tube contains a single x-ray source, and the detector
unit contains an array of x-ray detectors. Suppose for the moment that the x-ray Tube and
Collimator on the one side and the Data Acquisition/Detector Unit on the other side are
stationary, and the patient on the table is moved between them at a steady rate. By shooting
a fan beam of x-rays through the patient at frequent regular intervals and detecting them
on the other side, we can build up a two-dimensional x-ray projection of the patient that
is very similar in appearance to the image that is traditionally captured on an x-ray film.
Such a projection is shown in
> Fig. -a. The brightness at a point is indicative of the
total attenuation of the x-rays from the source to the detector. This mode of operation is not
CT, it is just an alternative way of taking x-ray images. In the CT mode, the patient is kept
stationary, but the tube and the detector unit rotate (together) around the patient. The fan
beam of x-rays from the source to the detector determines a slice in the patient’s body. The
location of such a slice is shown by the horizontal line in
> Fig. -a. Data are collected
for a number of fixed positions of the source and detector; these are referred to as views.

Tomography 

⊡Fig. -
Engineering rendering of a CT scanner: Cut-away rendering of GE Discovery(TM) CT
HD scanner (Provided by GE Healthcare)
a
b
c
⊡Fig. -
(a) Digitally radiograph with line marking the location of the cross section for which the
following images were obtained. (b) Sinogram of the projection data. (c) Reconstruction
from the projection data (Images were obtained using a Siemens Sensation CT scanner by R.
Fahrig and J. Starman at Stanford University)
For each view, we have a reading by each of the detectors. All the detector readings for all the
views can be represented as a sinogram, shown in > Fig. -b. The intensities in the sino-
gram are proportional to the line integrals of the x-ray attenuation coefficient between the
corresponding source and detector positions. From these line integrals, a two-dimensional
image of the x-ray attenuation coefficient distribution in the slice of the body can be pro-
duced by the techniques of image reconstruction. Such an image is shown in > Fig. -c.
Inasmuch as different tissues have different x-ray attenuation coefficients, boundaries of
organs can be delineated and healthy tissue can be distinguished from tumors. In this way,
CT produces cross-sectional slices of the human body without surgical intervention.


Tomography
a
b
c
d
⊡Fig. -
Three-dimensional displays of bone structures of patients produced during –by
the software developed in the author’s research group at the University of Pennsylvania for
the General Electric Company. (a) Facial bones of an accident victim prior to operation.
(b) The same patient at the time of a -year postoperative follow-up. (c) A tibial fracture.
(d) A pelvic fracture (Reproduced from [])
We can use the reconstructions of a series of parallel transverse sections to discover and
display the precise shape of selected organs; see > Fig. -. Such displays are obtained by
further computer processing of the reconstructed cross sections [].

Tomography 

As a second illustration of the many applications of tomography (for a more complete
coverage see [, Sect .]), we note that three-dimensional reconstruction of nanoscale
objects (such as biological macromolecules) can be accomplished using data recorded with
a transmission electron microscope (see
> Fig. -) that produces electron micrographs,
such as the one illustrated in
> Fig. -, in which the grayness at each point is indica-
tive of a line integral of a physical property of the object being imaged. From multiple
electron micrographs one can recover the structure of the object that is being imaged; see
> Fig. -.
What we have just illustrated in our electron microscopy example is a reconstruction
of a three-dimensional object from two-dimensional projections, as opposed to what is
shown in > Fig. -, which describes the collection of data for the reconstruction of a two-
dimensional object. In fact, recently developed CT scanners are not like that, they collect a
series of two-dimensional projections of the three-dimensional object to be reconstructed.
Helical CT (also referred to as spiral CT) first started around [, ] and has
become standard for medical diagnostic x-ray CT. Typical state-of-the art versions of such
systems have a single x-ray source and multiple detectors in a two-dimensional array. The
main innovation over previously used technologies is the presence of two independent
To high-voltage
power supply 
Anode
Objective
lens
To vacuum
pumps
Filament
Condenser lens
Specimen
stage
Intermediate and
projector lenses
Phosphorescent
screen
Photographic
plates
⊡Fig. -
Schematic drawing of a transmission electron microscope (Illustration provided by
C. San Martín of the Centro Nacional de Biotecnología, Spain)


Tomography
⊡Fig. -
Part of an electron micrograph containing projections of multiple copies of the human
adenovirus type (Illustration provided by C. San Martín of the Centro Nacional de
Biotecnología, Spain)
⊡Fig. -
Top: Reconstructed values, from electron microscopic data such as in > Fig. -, of the
human adenovirus type in three mutually orthogonal slices through the center of
the reconstruction. Bottom: Computer graphic display of the surface of the virus based
on the three-dimensional reconstruction (Illustration provided by C. San Martín of the
Centro Nacional de Biotecnología, Spain)

Tomography 

⊡Fig. -
Helical (also known as spiral) CT (Illustration provided by G. Wang of the Virginia Polytechnic
Institute & State University)
motions: while the source and detectors rotate around the patient, the table on which the
patient lies is continuously moved between them (typically orthogonally to the plane of
rotation), see
> Fig. -. Thus, the trajectory of the source relative to the patient is a
helix (hence the name “helical CT”). Helical CT allows rapid imaging as compared with
the previous commercially viable approaches, which has potentially many advantages. One
example is when we wish to image a long blood vessel that is made visible to x-rays by the
injection of some contrast material: helical CT may very well allow us to image the whole
vessel before the contrast from a single injection washes out and this may not be possible
by the slower scanning modes. We point out that the CT scanner illustrated in > Fig. -
is in fact modern helical CT scanner.
For the sake of not overcomplicating our discussion, in this chapter we restrict our
attention (except where it is explicitly stated otherwise) to the problem of reconstructing
two-dimensional objects from one-dimensional projections, rather than to what is done
by modern helical cone-beam scanning (as in > Fig. -) and volumetric reconstruction.
Schematically, the method of our data collection is shown in
> Fig. -. The source and
the detector strip are on the either side of the object to be reconstructed and they move
in unison around a common center of rotation denoted by O in > Fig. -. The data col-
lection takes place in M distinct steps. The source and detector strip are rotated between
two steps of the data collection by a small angle, but are assumed to be stationary while


Tomography
Source position
Angular scan
S1
Sm
S0
Sm–1
Sm–2
N
B
1
0
–1
–N
Reading
Object to be
O
m
n
reconstructed
q
D
l
ι
Detector
strip
⊡Fig. -
Schematic of a standard method of data collection (divergent beam). This is consistent with
the data collection mode for CT that is shown in > Fig. -(Reproduced from [])
the measurement is taken. The M distinct positions of the source during the M steps of the
data collection are indicated by the points S, . . . , SM−in
> Fig. -. In simulating this
geometry of data collection, we assume that the source is a point source. The detector strip
consists of N + detectors, spaced equally on an arc whose center is the source position.
The line from the source to the center of rotation goes through the center of the central
detector. (This description is that of the geometry that is assumed in much of what follows
and it does not exactly match the data collection by any actual CT scanner. In particular,
in real CT scanners the central ray usually does not go through the middle of the central
detector, as a /detector offset is quite common.) The object to be reconstructed is a pic-
ture such that its picture region (i.e., a region outside of which the values assigned to the
picture are zero) is enclosed by the broken circle shown in
> Fig. -. We assume that
the origin of the coordinate system (with respect to which the picture values μ¯e(x, y) are
defined) is the center of rotation, O, of the apparatus.
Until now we have used μ¯e(x, y) to denote the relative linear attenuation at the point
(x, y), where (x, y) was in reference to a rectangular coordinate system, see
> Fig. -.
However, it is often more convenient to use polar coordinates. We use the phrase a function
of two polar variables to describe a function f whose values f (r, ϕ) represent the value of
some physical parameter (such as the relative linear attenuation) at the geometrical point
whose polar coordinates are (r, ϕ).

Tomography 

We define the Radon transform R f of a function f of two polar variables as follows:
for any real number pairs (ℓ, θ),
[R f ] (ℓ, θ) = ∫
∞
−∞f (
√
ℓ+ z, θ + tan−(z/ℓ))dz, if ℓ≠,
[R f ] (, θ) = ∫
∞
−∞f (z, θ + π/)dz.
(.)
Observing > Fig. -, we see that [R f ] (ℓ, θ) is the line integral of f along the line L. (Note
that the dummy variable z in (> .) does not exactly match the variable z as indicated in
> Fig. -. In (> .) z = corresponds to the point where the perpendicular dropped
on L from the origin meets L.)
In tomography, we may assume that a picture function has bounded support; i.e., that
there exists a real number E, such that f (r, ϕ) = if r > E. (E can be chosen as the radius
of the broken circle in > Fig. -, which should enclose the square-shaped reconstruction
region in > Fig. -.) For such a function, [R f ] (ℓ, θ) = if ℓ> E.
The input data to a reconstruction algorithm are estimates (based on physical mea-
surements) of the values of [R f ](ℓ, θ) for a finite number of pairs (ℓ, θ); its output is an
estimate, in some sense, of f . Suppose that estimates of [R f ](ℓ, θ) are known for I pairs:
(ℓ, θ), . . . , (ℓI, θI). For ≤i ≤I, we define Ri f by
Ri f = [R f ] (ℓi, θi).
(.)
In what follows, we use yi to denote the available estimate of Ri f and we use y to denote
the I-dimensional vector whose ith component is yi. We refer to the vector y as the mea-
surement vector. When designing a reconstruction algorithm we assume that the method
of data collection, and hence the set {(ℓ, θ) , . . .,(ℓI, θI)}, is fixed and known. The
reconstruction problem is
given the data y, estimate the picture f .
We shall usually use f ∗to denote the estimate of the picture f .
In the mathematical idealization of the reconstruction problem, what we are look-
ing for is an operator R−, which is an inverse of R in the sense that, for any picture
function f , R−R f is f (i.e., R−associates with the function Rf the function f ). Just
as (> .) describes how the value of Rf is defined at any real number pair (ℓ, θ) based
on the values f assumes at points in its domain, we need a formula that for functions p of
two real variables defines R−p at points (r, ϕ). Such a formula is
[R−p] (r, ϕ) =

π∫
π

∫
E
−E

r cos(θ −ϕ) −ℓp(ℓ, θ) dℓdθ,
(.)
where p(ℓ, θ) denotes the partial derivative of p(ℓ, θ) with respect to ℓ; it is of interest
to compare this formula with (> .). That the R−defined in this fashion is indeed the
inverse of R is proven, e.g., in [, Sect. .].
A major category of algorithms for image reconstruction calculate f ∗based on (> .),
or on alternative expressions for the inverse Radon transform R−. We refer to this category


Tomography
as transform methods. While (> .) provides an exact mathematical inverse, in practice
it needs to be evaluated based on finite and imperfect data using the not unlimited capa-
bilities of computers. The essence of any transform method is a numerical procedure (i.e.,
one that can be implemented on a digital computer), which estimates the value of a double
integral, such as the one that appears on the right-hand side of (> .), from given values
of yi = p (ℓi, θi), ≤i ≤I. A very widely used example of transform methods is the so-
called filtered backprojection (FBP) algorithm. The reason for this name can be understood
by looking at the right-hand side of (> .): the inner integral is essentially a filtering of
the projection data for a fixed θ and the outer integral backprojects the filtered data into
the reconstruction region. However, the implementational details for the divergent beam
data collection specified in
> Fig. -are less than obvious, the solution outlined below
is based on [].
The data collection geometry we deal with is also described in
> Fig. -. The
x-ray source is always on a circle of radius D around the origin. The detector strip is an
arc centered at the source. Each line can be considered as one of a set of divergent lines
S
P
l
D
B
O
Object to be
reconstructed
q
s
b
Source 
position
Detector
strip
⊡Fig. -
Geometry of divergent beam data collection. Every one of the diverging lines is determined
by two parameters β and σ. Let O be the origin and S be the position of the source, which
always lies on a circle of radius D around O. Then β + π/is the angle the line OS makes with
the baseline B and σ is the angle the divergent line makes with SO. The divergent line is also
one of a set of parallel lines. As such it is determined by the parameters ℓand θ. Let P be the
point at which the divergent line meets the line through O that is perpendicular to it. Then ℓ
is the distance from O to P and θ is the angle that OP makes with the baseline (Reproduced
from []. Copyright )

Tomography 

(σ, β), where β determines the source position and σ determines which of the lines diverg-
ing from this source position we are considering. This is an alternative way of specifying
lines to the (ℓ, θ) notation used previously (in particular in
> Fig. -). Of course, each
(σ, β) line is also an (ℓ, θ) line, for some values of ℓand θ that depend on σ and β. We use
g(σ, β) to denote the line integral of f along the line (σ, β). Clearly,
g(σ, β) = [R f ](D sin σ, β + σ).
(.)
As shown in
> Fig. -, we assume that projections are taken for M equally spaced
values of β with angular spacing Δ, and that for each view the projected values are sam-
pled at N + equally spaced angles with angular spacing λ. Thus g is known at points
(nλ, mΔ), −N ≤n ≤N, ≤m ≤M −, and MΔ = π. Even though the projection
data consist of estimates (based on measurements) of g(nλ, mΔ), we use the same nota-
tion g(nλ, mΔ) for these estimates. The numerical implementation of the FBP method for
divergent beams is carried out in two stages.
First we define, for −N ≤n′ ≤N,
gc(n′λ, mΔ) = λ
N
∑
n=−N
cos(nλ)g(nλ, mΔ)q() ((n′ −n)λ)
+ λ cos(n′λ)
N
∑
n=−N
g(nλ, mΔ)q()((n′ −n)λ).
(.)
The functions q() and q() determine the nature of the “filtering” in the filtered backpro-
jection method. They are not arbitrary, but there are many possible choices for them, for
a detailed discussion see [, Chap. ]. Note that the first sum in (> .) is a discrete
convolution of q() and the projection data weighted by a cosine function, and the second
sum is a discrete convolution of q() and the projection data.
Second, we specify our reconstruction by
f ∗(r, ϕ) = DΔ
π
M−
∑
m=

Wgc (σ′, mΔ) ,
(.)
where
σ′ = tan−
r cos(mΔ −ϕ)
D + r sin(mΔ −ϕ),
−π
≤σ′ ≤π
,
(.)
and
W = ((r cos(mΔ −ϕ))+ (D + r sin(mΔ −ϕ)))
/
,
W > .
(.)
The meanings of σ′ and W are that when the source is at angle mΔ, the line that
goes through (r, ϕ) is (σ′, mΔ) and the distance between the source and (r, ϕ) is W.
Implementation of (> .) involves interpolation for approximating gc(σ′, mΔ) from val-
ues of gc(n′λ, mΔ). The nature of such an interpolation is discussed in some detail in
[, Sect. .]. Note that (> .) can be described as a “weighted backprojection.” Given
a point (r, ϕ) and a source position mΔ, the line (σ′, mΔ) is exactly the line from the
source position mΔ through the point (r, ϕ). The contribution of the convolved ray sum


Tomography
gc(σ′, mΔ) to the value of f ∗at points (r, ϕ) that the line goes through is inversely
proportional to the square of the distance of the point (r, ϕ) from the source position mΔ.
In this chapter we concentrate on the other major category of reconstruction algo-
rithms, the so-called series expansion methods. In transform methods, the techniques of
mathematical analysis are used to find an inverse of the Radon transform. The inverse
transform is described in terms of operators on functions defined over the whole contin-
uum of real numbers. For implementation of the inverse Radon transform on a computer
we have to replace these continuous operators by discrete ones that operate on functions
whose values are known only for finitely many values of their arguments. This is done at the
very end of the derivation of the reconstruction method. The series expansion approach is
basically different. The problem itself is discretized at the very beginning: estimating the
function is translated into finding a finite set of numbers. This is done as follows.
For any specified picture region, we fix a set of J basis functions {b, . . ., bJ}. These ought
to be chosen so that, for any picture f with the specified picture region that we may wish
to reconstruct, there exists a linear combination of the basis functions that we consider an
adequate approximation to f .
An example of such an approach is the n × n digitization in which we cover the picture
region by an n × n array of identical small squares, called pixels. In this case J = n. We
number the pixels from to J, and define
bj(r, ϕ) = {,
if (r, ϕ)is inside the jth pixel,
,
otherwise.
(.)
Then the n × n digitization of the picture f is the picture ˆf defined by
ˆf (r, ϕ) =
J
∑
j=
x jbj(r, ϕ),
(.)
where xj is the average value of f inside the jth pixel. A shorthand notation we use for
equations of this type is ˆf = ∑J
j=x jbj.
Another (and usually preferable) way of choosing the basis functions is the following.
Generalized Kaiser–Bessel window functions, which are also known by the simpler name
blobs, form a large family of functions that can be defined in a Euclidean space of any
dimension []. Here we restrict ourselves to a subfamily in the two-dimensional plane,
whose elements have the form
ba,α,δ(r, ϕ) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
Ca,α,δ (−( r
a )
) I(α
√
−( r
a )
),
if ≤r ≤a,
,
otherwise,
(.)
where Ik denotes the modified Bessel function of the first kind of order k, a stands for
the nonnegative radius of the blob and α is a nonnegative real number that controls the
shape of the blob. The multiplying constant Ca,α,δ is defined below. Note that such a blob
is circularly symmetric, since its value does not depend on ϕ. It has the value zero for all
r ≥a and its first derivatives are continuous everywhere. The “smoothness” of blobs can

Tomography 

b
a
0.6
Blob profile :: Row 121
0.55
0.5
0.45
0.4
0.35
Pixel Values
0.3
0.25
0.2
0.15
0.1
0.05
0
0
20
40
60
80
100
120
140
160
180
200
220
240
Columns
⊡Fig. -
(a) A × digitization of a blob. (b) Its values on the central row (Reproduced from [])
be controlled by the choice of the parameters a, α and δ, they can be made very smooth
indeed as shown in > Fig. -.
For now let us consider the parameters a, α and δ, and hence the function ba,α,δ, to
be fixed. This fixed function gives rise to a set of J basis functions {b, . . . , bJ} as follows.
We define a set G = {g, . . . , gJ} of grid points in the picture region. Then, for ≤j ≤J,
bj is obtained from ba,α,δ by shifting it in the plane so that its center is moved from the
origin to g j. This definition leaves a great deal of freedom in the selection of G, but it was
found in practice advisable that it should consists of those points of a set (in rectangular
coordinates)
Gδ = {(mδ
,
√
nδ

)∣m and n are integers and m + n is even}
(.)
that are also in the picture region. Here, δ has to be a positive real number and Gδ is referred
to as the hexagonal grid with sampling distance δ. Having fixed δ, we complete the definition
in (> .) by
Ca,α,δ =
√
δα
πaI(α).
(.)
Pixel-based basis functions (> .) have a unit value inside the pixels and zero out-
side. Blobs on the other hand, have a bell-shaped profile that tapers smoothly in the radial
direction from a high value at the center to the value at the edge of their supports (i.e., at
r = a in (> .)); see > Fig. -. The smoothness of blobs suggests that reconstructions
of the form (> .) are likely to be resistant to noise in the data. This has been shown
to be particularly useful in fields in which the projection data are noisy, such as positron
emission tomography and electron microscopy.
For blobs to achieve their full potential, the selection of the parameters a, α, and δ
is important. When they are properly chosen [], one can approximate homogeneous


Tomography
a
b
c
⊡Fig. -
(a) A × digitization of a bone cross section. (b) Its approximation with default blob
parameters and (c) with slightly diﬀerent parameters. The display window is very narrow for
better indication of errors (Reproduced from [])
regions very well, in spite of the bell-shaped profile of the individual blobs. This is illustrated
in > Fig. -b, in which a bone cross section shown in > Fig. -a is approximated by
a linear combination of blob basis functions with the parameters a = ., α = .,
and δ = .. There are some inaccuracies very near the sharp edges, but the interior of
the bone is approximated with great accuracy. However, if we change the parameters ever
so slightly to a = ., α = ., and δ = ., then the best approximation that can be
obtained by a linear combination of blob basis functions is shown in > Fig. -c, which
is clearly inferior.
Irrespective how the basis functions have been chosen, any picture ˆf that can be rep-
resented as a linear combination of the basis functions bj is uniquely determined by the
choice of the coefficients xj, ≤j ≤J, in the formula (> .). We use x to denote the
vector whose jth component is x j and refer to x as the image vector.
It is easy to see that, under some mild mathematical assumptions,
Ri f ≃Ri ˆf =
J
∑
j=
x jRibj,
(.)
for ≤i ≤I. Since the bj are user defined, usually the Ribj can be easily calculated by
analytical means. For example, in the case when the bj are defined by (> .), Ribj is
just the length of intersection with the jth pixel of the line of the ith position of the source-
detector pair. We use ri,j to denote our calculated value of Ribj. Hence,
ri,j ≃Ribj.
(.)
Recall that yi denotes the physically obtained estimate of Ri f . Combining this with
(> .) and (> .), we get that, for ≤i ≤I,
yi ≃
J
∑
j=
ri,jx j.
(.)

Tomography 

Let R denote the matrix whose (i, j)th element is ri,j. We refer to this matrix as the
projection matrix. Let e be the I-dimensional column vector whose ith component, ei, is
the difference between the left- and right-hand sides of (> .). We refer to this as the
error vector. Then (> .) can be rewritten as
y = Rx + e.
(.)
The series expansion approach leads us to the following discrete reconstruction problem:
based on (> .),
given the data y, estimate the image vector x.
If the estimate that we find as our solution to the discrete reconstruction problem is the
vector x∗, then the estimate f ∗to the picture to be reconstructed is given by
f ∗=
J
∑
j=
x∗
j bj.
(.)
In (> .), the vector e is unknown. The simple approach of trying to solve (> .)
by first assuming that e is the zero vector is dangerous: y = Rx may have no solutions, or
it may have many solutions, possibly none of which is any good for the practical problem
at hand. Some criteria have to be developed, indicating which x ought to be chosen as a
solution of (> .). One way of doing this is by considering both the image vector x and
the error vector e to be samples of random variables, denoted by X and E, respectively.
As an example of such an approach, let μ denote a J-dimensional vector of real numbers
and let V denote a J × J positive definite symmetric matrix of real numbers. We can define
a function pX over the set of all J-dimensional vectors of real numbers by
pX(x) =

(π)J/(det V)/exp (−
(x −μ)T V−(x −μ)).
(.)
This pX is a probability density function of a random variable X on the set of all
J-dimensional vectors of real numbers whose mean vector is μx = μ and whose covariance
matrix is VX = V. A random variable X defined in such a fashion is called a multivariate
Gaussian random variable.
Let us now consider the random variables X and E associated with x and e of (> .)
without assuming any special form for them. In any case, pX is referred to as the prior
probability density function, since pX(x) indicates the likelihood of coming across an image
vector similar to x. In CT, it makes sense to adjust pX to the area of the body we are imaging;
the probabilities of the same picture representing a cross section of the head or of the thorax
should be different. Based on pX and pE, a reasonable approach to solving the discrete
reconstruction problem is: given the data y, choose the image vector x for which the value of
pE(y −Rx)pX(x)
(.)
is as large as possible. Note that the second term in the product is large for vectors x that
have large prior probabilities, while the first term is large for vectors x that are consistent


Tomography
with the data (at least if pE peaks at the zero vector). The relative importance of the two
terms depends on the nature of pX and pE. If pX is flat (many image vectors are equally
likely) and pE is highly peaked near the zero vector, then our criterion will produce an
image vector x∗that fits the measured data y in the sense that Rx∗will be nearly the same
as y. On the other hand, if pE is flat (large errors are nearly as likely as small ones) but pX
is highly peaked, our having made our measurements will have only a small effect on our
preconceived idea as to how the image vector should be chosen. The x∗that maximizes
(> .) is called the Bayesian estimate. The discussion in this paragraph is quite general,
since we have not assumed anything regarding the form of the random variables X and E.
If we assume that both X and E are multivariate Gaussian, then maximizing (> .)
becomes relatively simple. In that case it is easy to see from (> .) that, assuming μE is
the zero vector, the x that maximizes (> .) is the same x that minimizes
(y −Rx)T V −
E (y −Rx) + (x −μX)T V−
X (x −μX).
(.)
When more precise information regarding the mean vector μX is not available, one can
use for it a uniform picture, with an estimated (based on the projection data) average
value assigned to every pixel; how well this works out in practice is illustrated below in
> Sect. .. We also illustrate there an alternative choice that is appropriate for cardiac
imaging in which μX is a time-averaged reconstruction. The noise model expressed by the
first term of (> .) is only approximate, but it is a reasonable accurate approximation of
the effect of photon statistics in CT ([, Sect. .]).
As representative examples of the series expansion methods for image reconstruction
we now discuss the algebraic reconstruction techniques (ART). All ART methods of image
reconstruction are iterative procedures: they produce a sequence of vectors x(), x(), . . .
that is supposed to converge to x∗. The process of producing x(k+) from x(k) is referred to
as an iterative step.
In ART, x(k+) is obtained from x(k) by considering a single one of the I approximate
equations, see (> .). In fact, the equations are used in a cyclic order. We use ik to denote
k(mod I)+; i.e., i= , i= , . . ., iI−= I, iI = , iI+= , . . ., and we use ri to denote the
J-dimensional column vector whose jth component is ri,j. In other words, ri is the trans-
pose of the ith row of R. (In what follows we assume that, for ≤i ≤I, ∥ri∥= ⟨ri, ri⟩≠,
where, as usual, ∥●∥denotes the norm and ⟨●,●⟩denotes the inner product.) An important
point here is that this specification is incomplete because it depends on how we index the
lines for which the integrals are estimated. As stated above, we assume that estimates of
[R f ](ℓ, θ) are known for I pairs: (ℓ, θ), . . .,(ℓI, θI). However, we have not specified the
geometrical locations of the lines that are parametrized by these pairs. Since the order in
which we do things in ART depends on the indexing i for the set of lines for which data
are collected, the specification of ART as a reconstruction algorithm is complete only if it
includes the indexing method for the lines, which we refer to as the data access ordering.
We return to this point later on in this chapter.
A particularly simple variant of ART is the following.
x() is arbitrary,
x(k+) = x(k) + c(k)rik,
(.)

Tomography 

where
c(k) = λ(k) yik −⟨rik x(k)⟩
∥rik∥
,
(.)
with each λ(k) a real number, referred to as a relaxation parameter. It is easy to check that,
for k ≥, if λ(k) = , then
yik =
J
∑
j=
rik,jx(k+)
j
,
(.)
i.e., the ikth approximate equality is exactly satisfied after the kth step. This behavior is
illustrated in > Fig. -for a two-dimensional case with two equalities.
This method has an interesting, although by itself not particularly useful, mathematical
property. Let
L = {x∣Rx = y}.
(.)
25
20
15
10
5
0
5
10
15
20
25
x1
x1
8
9
=
4x1 + x2 = 24 } = L1
x(0)
x2
x2
H1 ={
4
8
=
=
x(1)
x
x
x
x*
(3)
(2) (4)
5
4
2x1 + 5x2 =30 } = L2
x1
x2
H2 ={
⊡Fig. -
Demonstration of the method of (> .) and (> .) (with λ(k) = , for all k) for the simple
case when I = J = (Illustration based on []. Copyright . With permission from Elsevier)


Tomography
A sequence x(), x(), x(), . . . generated by (> .) and (> .) converges to a vector
x∗in L, provided that L is not empty and that, for some εand εand for all k,
< ε≤λ(k) ≤ε< .
(.)
Furthermore, if xis chosen to be the vector with zero components, then
∥x∗∥< ∥x∥,
(.)
for all x in L other than x∗. A proof of this can be found in [, Sect. .].
The reason why this result is not useful by itself is that the condition that L is not
empty is unlikely to be satisfied in a real tomographic situation. However, as it is shown in
[, Sect. .], it can be used to derive an alternative ART algorithm that is useful in real
applications, as we now explain.
Let us make the simplifying assumptions in (> .) that VX and VE are both multi-
ples of identity matrices of appropriate sizes. In other words, we assume that components
of a sample of X −μX are uncorrelated, and that each component is a sample from the
same Gaussian random variable; and we also assume that components of a sample of E are
uncorrelated and that each component is a sample from the same zero mean Gaussian ran-
dom variable. We use sto denote the diagonal entries of VX and nto denote the diagonal
entries of VE and let t = s/n. According to (> .), the Bayesian estimate is the vector x
that minimizes
t∥y −Rx∥+ ∥x −μX∥.
(.)
Note that a small value of t indicates that prior knowledge of the expected value of the
image vector is important relative to the measured data, while a large value of t indicates
the opposite. The following variant of ART converges to this Bayesian estimate, provided
only that the condition expressed in (> .) holds:
u() is the I-dimensional zero vector,
x() = μX,
u(k+) = u(k) + c(k)eik,
x(k+) = x(k) + tc(k)rik,
(.)
where
c(k) = λ(k) t (yik −⟨rik, x(k)⟩) −u(k)
ik
+ t∥rik∥
.
(.)
Note that both in (> .) and in (> .) the updating of x(k) is very simple: we just
add to x(k) a multiple of the vector rik. In practice, this updating of x(k) can be computa-
tionally very inexpensive. Consider, e.g., the basis functions associated with a digitization
into pixels (> .). Then ri,j is just the length of intersection of the ith line with the jth
pixel. This has two consequences. First, most of the components of the vector rik are zero.
At most n −pixels can be intersected by a straight line in an n × n digitization of a pic-
ture. Thus, of the ncomponents of rik, at most n −(and typically only about n) are
nonzero. Second, the location and size of the nonzero components of rik can be rapidly
calculated from the geometrical location of the ikth line relative to the n × n grid using a

Tomography 

a b
d
c
e f
k
l
h
g
j
i
L
q
⊡Fig. -
A digital diﬀerence analyzer (DDA) for lines (Reproduced from [])
digital difference analyzer (DDA) methodology demonstrated in
> Fig. -(for details,
see [, Sect. .]). Thus, the projection matrix R does not need to be stored in the com-
puter. Only one row of the matrix is needed at a time, and all information about this row is
easily calculable. For this reason such methods are also referred to as row-action methods.
We investigate this point further, since it is basic to the understanding of the compu-
tational efficacy of ART. Suppose that we have obtained, using a DDA, the list j, . . . , jU of
indices such that rik,j = unless j is one of the j, . . ., jU. Then evaluation of ⟨rik, x(k)⟩or
of ∥rik∥requires only U multiplications, which in our application is much smaller than J.
The updating of x can be achieved by a further U multiplications. This is because only those
xj need to be altered for which j = ju for some u, ≤u ≤U, and the alteration requires
adding to x(k)
j
a fixed multiple of rik,j. This shows that a single step of either of the ART
algorithms described above is very simple to implement in a computationally efficient way.
.
Numerical Methods and Case Examples
Having seen that there is a variety of reconstruction algorithms, it is natural to ask for guid-
ance as to when it is better to apply one rather than the others. Unfortunately, any general
answer is likely to be misleading since the relative efficacy of algorithms depends on many
things: the underlying task at hand, the method of data collection, the hardware/software
available for implementing the algorithms, etc. The practical appropriateness of an algo-
rithm under some specific circumstances needs experimental evaluation.
We are now going to illustrate this by comparing, from certain points of view, the vari-
ous reconstruction algorithms mentioned in the previous section. Except where otherwise
stated, the generation of images and their projection data, the reconstructions from such


Tomography
⊡Fig. -
Central part of an x-ray. CT reconstruction of a cross section of the head of a patient. This
served as the basis for our piecewise-homogeneous head phantom (Reproduced from [])
data, the evaluation of the results, and the graphical presentation of both the images and
the evaluation results were done within the software package SNARK[].
We studied a cross section of a human head that was reconstructed by CT (see
> Fig. -). Based on this cross section we described a skull enclosing the brain with
ventricles, two tumors, and a hematoma (blood clot) using five ellipses, eight segments
of circles, and two triangles. The tumors were placed so that they are vertically above the
blood clot in the display. We used SNARKto obtain the density in each of × 
pixels of size .cm. The resulting array of numbers is represented in > Fig. -. The
nature of this display deserves careful discussion. The displayed values are linear attenu-
ation coefficients μ¯e(x, y) at energy ¯e = keV of the appropriate tissue types measured
in cm−. Thus the values range between zero (background, can be thought of as air) and
.(bone of the skull). However, the interesting part of the picture is inside the skull.
The values there range from .(cerebrospinal fluid) to .(metastatic breast tumor).
The small differences between these tissues would not be noticeable if we used black to
display zero, white to display .and corresponding grayness for values in between. To see
clearly the features in the interior of the skull, we use zero (black) to represent the value
.(or anything less) and (white) to represent the value .(or anything more).
This way the small change in density by .corresponds to a change of in display

Tomography 

⊡Fig. -
A piecewise-homogeneous head phantom (Reproduced from [])
grayness, which is visible. We did this to produce
> Fig. -and the displays of all the
reconstructions of the head phantoms used as illustrations in this chapter.
In > Fig. -a we show an actual brain cross section. The left half of the image shows
a malignant tumor that has a highly textured appearance. In order to simulate the occur-
rence of a similarly textured object in our phantom we produced the phantom shown in
> Fig. -b. Because of the medical relevance of imaging brains with such tumors, for
the rest of this chapter we use the head phantom with this tumor added to it. (Due to our
display method, it seems that there is a large range of values in the tumor. However, this
is an illusion: the range of values in the tumor is less than % of the range of values in the
picture that is displayed in > Fig. -.)
One problem with the phantoms as defined so far is that a brain is far from being
homogeneous: it has gray matter, white matter, blood vessels, and capillaries carrying oxy-
genated blood to and deoxygenated blood from the brain, etc. This is even more so for
bone, whose strength to a large extent is derived from its structural properties. There are
methods that can obtain remarkably accurate reconstruction of piecewise homogeneous
objects, but their performance may not be medically efficacious when applied to CT data
from real objects with local inhomogeneities. So as not to fall into the trap of drawing too
optimistic conclusions from experiments using piecewise homogeneous objects, we super-
imposed on our head phantom a random local variation that is obtained by picking, for


Tomography
a
b
⊡Fig. -
(a) An actual brain cross section with a tumor (Image is reproduced, with permission, from
the Roswell Park Cancer Institute website). (b) The head phantom of > Fig. -with a
“large tumor”added to it (Reproduced from [])
each pixel, a sample from a Gaussian random variable X with mean μX = and standard
deviation σX = .and then multiplying the previously estimated linear attenuation
coefficient at that energy level with that sample. In > Fig. -we show the result of this.
A reconstruction is a digitized picture. If it is a reconstruction from simulated projec-
tion data of a test phantom, we can judge its quality by comparing it with the digitization of
the phantom. Naturally, both the picture region and the grid must be the same size for the
reconstruction and the digitized phantom. We now discuss how to illustrate and measure
the resemblance between a reconstruction and a phantom.
Visual evaluation is of course the most straightforward way. One may display both the
phantom and the reconstruction and observe whether all features in which one is inter-
ested in the phantom are reproduced in the reconstruction and whether any spurious
features have been introduced by the reconstruction process. A difficulty with such a qual-
itative evaluation is its subjectiveness, people often disagree on which of the two pictures
resembles a third one more closely.
A more quantitative way of evaluating pictures is the following. Select a column of pixels
that goes through a number of interesting features. For example, in our digitized head
phantom the st of the columns goes through the ventricles, both tumors, and the
hematoma. In
> Fig. -a we indicate this column. A way to evaluate the quality of a
reconstruction is to compare the graphs of the pixel densities for this column in the
phantom (shown in > Fig. -b) and the reconstruction.
It also appears desirable to use a single value that provides a rough measure of the
closeness of the reconstruction to the phantom. We now describe two different methods
of doing this. In our definition of these two picture distance measures we use tu,v and ru,v
to denote the densities of the vth pixel of the uth row of the digitized test phantom and the

Tomography 

a
0.217
Head_phantom_with_tumor_0.0025_inhomogeneity :: Column 131
0.216
0.215
0.214
0.213
0.212
0.211
0.21
0.209
0.208
0.207
0.206
0.205
0.204
0
b
20
40
60
80
100
Rows
Pixel values
120
140
160
180
200
220
240
Head_phantom_with_tumor_0.0025_inhomogen
⊡Fig. -
(a) A head phantom with local inhomogeneities with the st of the columns indicated
by a vertical line. (b) The densities along this column in the phantom (Reproduced from [])
reconstruction, respectively, and ¯t to denote the average of the densities in the digitized
test phantom. We assume that both pictures are n × n. Let
d = (
n
∑
u=
n
∑
v=
(tu,v −ru,v)/
n
∑
u=
n
∑
v=
(tu,v −¯t))
/
.
(.)


Tomography
r =
n
∑
u=
n
∑
v=
∣tu,v −ru,v∣/
n
∑
u=
n
∑
v=
∣tu,v∣.
(.)
(∣x∣denotes the absolute value of x.) These are often-used measures in the literature.
These measures emphasize different aspects of picture quality. The first one, d, is a nor-
malized root mean squared distance measure. A large difference in a few places causes the
value of d to be large. Note that the value of d is if the reconstruction is a uniformly
dense picture with the correct average density. The second one, r, is a normalized mean
absolute distance measure. As opposed to d, it emphasizes the importance of a lot of small
errors rather than of a few large errors. Note that the value of r is if the reconstruction is
a uniformly dense picture with zero density.
However, a collection of a few numbers cannot possibly take care of all the ways in
which two pictures may differ from each other. Rank ordering reconstructions based on
a few measures of closeness to the phantom can be misleading. We recommend instead
a statistical hypothesis testing–based methodology that allows us to evaluate the relative
efficacy of reconstruction methods for a given task.
This evaluation methodology considers the following to be the relevant basic ques-
tion: given a specific medical problem, what is the relative merit of two (or more) image
reconstruction algorithms in presenting images that are helpful for solving the problem?
(Compare this with the alternative essentially unanswerable question: which is the best
reconstruction algorithm?) Ideally, the evaluation should be based on the performance of
human observers. However, that is costly and complex, since a number of observers have to
be used, each has to read many images, conditions have to be carefully controlled, etc. Such
reasons lead us to use numerical observers instead of humans. The evaluation methodology
consists of four steps:
. Generation of random samples from a statistically described ensemble of images (phan-
toms) representative of the medical problem and computer simulation of the data
collection by the device under investigation.
. Reconstruction from the data so generated by each of the algorithms.
. Assignment of a figure of merit (FOM) to each reconstruction. The FOM should
measure the usefulness of the reconstruction for solving the medical problem.
. Calculation of statistical significance (based on the FOMs of all the reconstructions) by
which the null hypothesis that the reconstructions are equally helpful for solving the
problem at hand can be rejected.
We now discuss details. For relevance to a particular medical task, the steps must be
adjusted to that task. The task for which comparative evaluations of various pairs of recon-
struction algorithms are reported below is that of detecting small low-contrast tumors in
the brain based on reconstructions from CT data.
The ensemble of images generated for this task is based on the head phantom with
a large tumor and local inhomogeneities. Note that this by itself provides us a statisti-
cal ensemble because the local inhomogeneities are introduced using a Gaussian random

Tomography 

variable. However, there is an additional (for the task more relevant) variability within the
ensemble that is achieved as follows. We specify a large number of pairs of potential tumor
sites, the locations of the sites in a pair are symmetrically placed in the left and right halves
of the brain. In any sample from the ensemble, exactly one of each pair of the sites will
actually have a tumor placed there, with equal probability for either site. The tumors are
circular in shape of radius .cm and with linear attenuation as for the meningioma in the
original phantom. In
> Fig. -a we illustrate one sample from this ensemble. Once a
sample has been picked, we generate projection data for it by simulating a CT scanner, with
all its physical inaccuracies as compared to the idealized Radon transform. (Such inaccura-
cies include: the finite number of measurements, statistical noise due to the finite number
of x-ray photons used during the measurements, the hardening of the polychromatic x-ray
beam as it passes through the body, the width of the detector, and the scattering of x-ray
photons.) Further variability is introduced at this stage, since the data are generated by
simulating noise due to photon statistics. In > Fig. -b we show a reconstruction from
one such projection data set. The tumors are hard to see in this reconstruction, but that is
exactly the point: we are trying to evaluate which of two reconstruction algorithms pro-
vides images in which the tumors are easier to identify. If we make the task too easy (by
having large and/or high-contrast tumors), then all reasonable reconstruction algorithms
would perform perfectly from the point of view of the task. On the other hand, if the task
is too difficult (very small and very low-contrast tumors), then correct detection would
become essentially a matter of luck, rather than of algorithm performance. Our ensemble
was chosen to be in between these extremes. The FOM that we chose to use is specific to
the type of ensemble of phantoms that we have just specified.
a
b
⊡Fig. -
(a) A random sample from the ensemble of phantoms for the task-oriented comparison of
reconstruction algorithms. (b) A reconstruction from noisy projection data taken of the
phantom illustrated in (a) (Reproduced from [])


Tomography
Given a phantom and one of its reconstructions, as in
> Fig. -, we define the
imagewise region of interest FOM (IROI) as
IROI =
B
∑
b=
(αr
t(b) −αr
n(b))
$
%
%
%
&
B
∑
b=
(αr
n(b) −
B
B
∑
b′=
αr
n(b′))
/
B
∑
b=
(αp
t (b) −αp
n(b))
$
%
%
%
&
B
∑
b=
(αp
n(b) −
B
B
∑
b′=
αp
n(b′))
.
(.)
The specification of the terms in this formula is as follows. For any digitized picture and for
any potential tumor site, let the average density in that picture for that site be the sum over
all pixels whose center falls within the site of the pixel densities divided by the number
of such pixels. Let us number the pairs of potential tumor sites from to B, and let (for
≤b ≤B) αp
t (b) (respectively, αp
n(b)) denote the average density in the phantom for site
of the bth pair that has (respectively, has not) the tumor in it. We specify similarly αr
t(b)
(respectively, αr
n(b)), for the reconstruction. The first thing to note about the resulting
formula (> .) is that the numerator and the denominator in the big fraction are exactly
the same except that the numerator refers to the reconstruction and the denominator refers
to the phantom. Thus, if the reconstruction is perfect (in the sense of being identical to the
phantom) then IROI = . Analyzing the contents of the numerator and the denominator,
we see that they are (except for constants that cancel out) the mean difference between the
average values at the sites with tumors and the sites without tumors, divided by the standard
deviation of the average values at the non-tumor sites. It has been found by experiments
with human observers that this FOM correlates well with the performance of people [].
In order to obtain statistically significant results, we need to sample the ensemble of
phantoms and generate projection data a number (say C) of times. (For the experiments
reported below we used C = .) Suppose that we wish to compare the task-oriented perfor-
mance of two reconstruction algorithms. For ≤c ≤C, let IROI(c) and IROI(c) denote
the values of IROI, as defined by (> .), for the reconstructions by the two algorithms
from projection data of the cth phantom. The null hypothesis that the two reconstruction
methods are equally good for the task at hand translates into the statistical statement that
each value of IROI(c) −IROI(c) is a sample of a continuous random variable D whose
mean is . We have no idea of the shape of the probability density function pD of this ran-
dom variable, but by the central limit theorem (see, e.g., [, Sect. .]), for a sufficiently
large C,
s =
C
∑
c=
(IROI(c) −IROI(c))
(.)
can be assumed to be a sample from a Gaussian random variable S with mean . This fact
allows us to say (for details see [, Sect. .]) that, at least approximately, S is a Gaussian
random variable whose mean is and whose variance is
VS =
C
∑
c=
(IROI(c) −IROI(c))
.
(.)

Tomography 

It is a consequence of the null hypothesis that s is a sample from a zero-mean random
variable. However, even if that were true, we would not expect our particular sample s to
be exactly . Suppose for now that s > . This makes us suspect that in fact the first algo-
rithm is better than the second one (for our task) and so the null hypothesis may be false.
The question is: how significant is the observed value s for rejecting the null hypothesis?
To answer this question we consider the P-value, which is the probability of a sample of
S being as large or larger than s. If the null hypothesis were correct, we would not expect
to come across an s defined by (> .) for which the P-value is very small. Thus, the
smallness of the P-value is a measure of significance for rejecting the null hypothesis that
the two reconstruction algorithms are equally good for our task in favor of the alternative
hypothesis that the first one is better than the second one. This is for the case when s > .
If s < , then the P-value is the probability of a sample of S being as small or smaller than
s and the alternative hypothesis is that the second algorithm is better than the first one.
Having specified various methodologies for reconstruction algorithm evaluation, we
now apply them to specific algorithms. Whenever we report on the performance of an
algorithm for the reconstruction of a single two-dimensional phantom, the phantom is
the one shown in > Fig. -. For experiments involving statistical hypothesis testing, we
use the ensemble illustrated by
> Fig. -. In either case, the data collection geometry
is the one described in
> Fig. -with the number of source positions M = . Con-
sequently, the angle mΔ shown in
> Fig. -is .m degrees. The source positions are
equally spaced around a circle of radius cm. The distance of the source from the detector
strip is .cm. There are detectors, and the distance between two detectors along
the arc of the detector strip is .cm. We refer to this geometry of data collection as
the standard geometry.
The reconstruction algorithm estimates a digitization of the phantom from the pro-
jection data.
> Figure -shows the × digitization of the head phantom,
a reconstruction by FBP from perfect projections (line integrals) for the geometry just
described, and the values of the digitized phantom and the reconstruction along the
st column. The picture distance measures for this reconstruction are d = .and
r = .. Even though the data are perfect, the reconstruction is not. This is because a
picture is not uniquely determined by its integrals along a finite number of lines. The best
that a reconstruction algorithm can do is to estimate the picture.
There are interesting observations that one can make regarding this reconstruction.
One is that, generally speaking, the brain appears smoother in it than in the phantom. This
is because the FBP algorithm that we use was designed to perform efficaciously on real
data and it does some smoothing to counteract the effect of noise. Consequently, small
variations due to inhomogeneity are also smoothed. The most noticeable features in the
reconstruction that are not present in the phantom are the streaks that seem to emanate
from straight interfaces between the skull and the brain. (Similar features are observable in
the real reconstruction shown in > Fig. -.) Their presence can be explained by consid-
ering Radon’s formula (> .), which expresses the distribution of the linear attenuation
coefficient in terms of its line integrals. Consider an ℓand a θ such that m(ℓ, θ) is the
integral along a line that is very near to a straight edge between the skull and the brain.


Tomography
a
b
0.217
Head_phantom_with_tumor_0.0025_inhomogeneity_divergent_noiseless 
:: multiple images’ :: Column 131
0.216
0.215
0.214
0.213
0.212
0.211
0.21
0.209
0.208
0.207
0.206
0.205
0.204
0
c
20
40
60
80
100
Rows
Pixel values
120
140
160
180
200
220
240
Head_phantom_with_tumor_0.0025_inhomogen
Head_phantom_convolution_DCON_0001_r_a
⊡Fig. -
(a) Head phantom (the same as > Fig. -a). (b) Its reconstruction from “perfect”data
collected for the standard geometry. (c) Line plots of the st column of the phantom (light)
and the reconstruction (dark) (Reproduced from [])
Due to the fact that attenuation is much larger for bone than for brain, numerical estima-
tion of the partial derivative m(ℓ, θ) from the discretely sampled projection data is likely
to be inaccurate, introducing errors into the calculated reconstruction. Phantoms that lack
such anatomical features should not be used for algorithm evaluation, since the resulting
reconstructions do not indicate the errors that will occur in a real application in which the
object to be reconstructed is likely to have such straight interfaces. This is illustrated in
> Fig. -.

Tomography 

a
b
⊡Fig. -
(a) A simple head phantom without straight edges between bone and brain. (b) Its
reconstruction from “perfect”data collected for the standard geometry. In this
reconstruction there are no false features of the kind that emanate from the straight edges
in > Fig. -b (Reproduced from [])
The reconstructions shown in
> Figs. -and > -are from “perfect” data; i.e.,
from line integrals based on the geometrical description of the phantoms. When data are
collected by an actual CT scanner there are many physical reasons why the data so obtained
can only provide approximations to such line integrals. In testing reconstruction algo-
rithms we should use realistic projection data, which is what was done for the remaining
two-dimensional reconstructions in this chapter. The exact method of simulated data col-
lection (using SNARK[]) is described in [, Sect. .], here we just give an outline.
The data were collected for the head phantom shown in
> Fig. -a according to the
standard geometry. For photon statistics we chose an average of million x-ray photons
originating in the direction of each detector during the scanning of the head. A realis-
tic spectrum of the polychromatic x-ray source was also simulated. The focal spot of the
x-ray source was assumed to be a point, but the detectors were assumed to have width of
.cm (i.e., there are no gaps between the detectors). It was assumed that the number
of scattered photons that are counted during the measurements is % of the number of
unscattered photons that are counted. The data so obtained was corrected for beam hard-
ening, to provide us with an estimate of the monochromatic projection data. The outcome
of this correction is what we refer to as the standard projection data. For the experiments
involving statistical evaluation, the same assumptions were made except that the phan-
tom was randomly selected from the previously described ensemble; for an example, see
> Fig. -a. Our illustrations are restricted to demonstrating the effects of various
choices that can be made in ART and the comparison of ART with FBP.
We start with the variant of ART described by (> .) and (> .). We choose x()
to represent a uniform picture, with the estimated (based on the standard projection data)
average value of the phantom assigned to every pixel. (The estimation of the average value
from projection data is described in [, Sect. .].)


Tomography
We first show that the order of equations in the system (the data access ordering
discussed in the previous section) can have a significant effect on the practical perfor-
mance of the algorithm, especially on the early iterates. With data collection such as the
geometry depicted in
> Fig. -, it is tempting to use the sequential ordering: access
the data in the order g(−Nλ,), g((−N + )λ,), . . ., g(Nλ,), g(−Nλ, Δ), g((−N +
)λ, Δ), . . ., g(Nλ, Δ),. . ., . . . , g(−Nλ,(M−)Δ), g((−N+)λ,(M−)Δ), . . ., g(Nλ,(M−
)Δ), where g(σ, β) denotes here the measured value of what is mathematically defined in
(> .). However, this sequential ordering is inferior to what is referred to as the efficient
ordering in which the order of projection directions mΔ and, for each view, the order of
lines within the view is chosen so as to minimize the number of commonly intersected
pixels by a line and the lines selected recently. This can be made mathematically precise by
considering the decomposition into a product of prime numbers of M and of N + [].
SNARK[] calculates the efficient order, but this is only useful if both M and of N + 
decompose into several prime numbers, as is the case for our standard geometry for which
M = = ××××××and N += = ××. While the sequential order-
ing produces the sequences m = ,,,,, . . . and n = ,,,,, . . ., the efficient ordering
produces the sequences m = ,,,,, . . . and n = ,,,,, . . . These
changes in data access ordering (keeping all other choices the same) translate into faster
initial convergence of ART, as is illustrated in > Fig. -by plotting the picture distance
measure r of (>.) against the number of times the algorithm cycled through all the data
(all I equations). To produce this illustration we used blob basis functions and λ(k) = .,
for all k. While it is clearly demonstrated that initially r gets reduced much faster with the
0.26
ART with blobs :: Relative error
0.24
0.22
0.2
0.18
0.16
Relative error
0.14
0.12
0.1
0.08
0.06
0.04
0
2
4
6
8
10
Iteration
12
14
16
18
20
ART 0.05 inefficient blob
ART 0.05 efficient blob
⊡Fig. -
Values of the picture distance measure r for ART reconstructions from the standard
projection data with sequential ordering (light) and eﬃcient ordering (dark), plotted at
multiples of I iterations (Reproduced from [])

Tomography 

efficient ordering, for the standard projection data it does not seem to matter much, since
both orderings need about five cycles through the data to obtain a near-minimal value of r.
In other applications in which the number of projection directions is much larger (e.g., in
the order of ,as is often the case in electron microscopy), one cycle through the data
using the efficient ordering yields about as good a reconstruction as one is likely to get,
but the sequential ordering needs several cycles through the data. In addition, the efficacy
of the reconstruction produced by the efficient ordering may very well be superior to that
produced by the sequential ordering.
This is illustrated in
> Fig. -and
> Table -. The reconstructions produced
by the efficient and sequential orderings after five cycles through the data (the images of
a
b
c
d
⊡Fig. -
Reconstructions from the standard projection data using ART. (a) ART with blobs,
λ(k) = ., Ith iteration and eﬃcient ordering. (b) ART with blobs, λ(k) = ., Ith iteration
and sequential ordering. (c) ART with pixels, λ(k) = ., Ith iteration and eﬃcient ordering.
(d) ART with blobs, λ(k) = ., Ith iteration and eﬃcient ordering (Based on [, Fig. .])


Tomography
⊡Table -
Picture distance measures and timings (in seconds, of the implementations in SNARK) for
thereconstructionsin >Figs.-and >-.Thelastcolumnreportsthevalues,produced
by a task-oriented evaluation experiment, of the IROI for the various algorithms (Based on
[, Table .])
Reconstruction in d
r
t
IROI
> Fig. 16-23a
.
.
.
.
> Fig. 16-23b
.
.
.
.
> Fig. 16-23c
.
.
.
.
> Fig. 16-23d
.
.
.
.
> Fig. 16-25b
.
.
.
.
x(I)) are shown in
> Fig. -a, b, respectively. Visually there is hardly any difference
between them. This is confirmed by the picture distance measures in > Table -, they
are only slightly better for the efficient ordering than for the sequential ordering. On the
other hand, the execution time (within the SNARK[] environment) is somewhat less
for the sequential ordering. However, the task-oriented evaluation is unambiguous in its
result: the IROI is larger for the efficient ordering and the associated P-value is less than
−. This means that we can reject the null hypothesis that the two data access orderings
are equally good in favor of the alternative hypothesis that the efficient ordering is better
with extreme confidence.
Next we emphasize the importance of the basis functions. In
> Fig. -we plot the
picture distance measure r against the number of times ART cycled through all the data,
where we kept all other choices the same (in particular, efficient data access ordering and
λ(k) = ., for all k). The two cases that we compare are when the basis functions are based
on pixels (> .) and when they are based on blobs (> .). The results are impressive:
as measured by r, blob basis functions are much better. The result of the Ith iteration
of the blob reconstruction is shown in
> Fig. -a, while that of the Ith iteration of
the pixel reconstruction is shown in > Fig. -c. The blob reconstruction appears to be
clearly superior. In >Table -we see a great improvement in the picture distance measure
r but not in d. This reflects the fact, not visible in our display mode, that there are a few
but relatively large errors in the blob reconstruction near the edges of the bone of the
skull. From the points of view of the task-oriented figure of merit IROI, ART with blobs
is found superior to ART with pixels with the relevant P-value less than −. As imple-
mented in SNARK, ART with blobs requires significantly more time than ART with
pixels, but there exist more sophisticated implementations of ART with blobs that are much
faster.
Underrelaxation is also a must when ART is applied to real, and hence imperfect, data.
In the experiments reported so far λ(k) was set equal to .for all k. If we do not use
underrelaxation (i.e., we set λ(k) to for all k), we get from the standard projection data
the unacceptable reconstruction shown in
> Fig. -d. Note that in this case we used
the Ith iterate, further iterations give worse results. The reason for this is in the nature of

Tomography 

0.105
ART with blobs :: Relative error
0.1
0.095
0.09
0.085
0.08
0.075
0.07
0.065
0.06
0.055
0.05
0.045
0.04
0.035
0
2
Relative error
4
6
8
10
Iteration
12
14
16
18
20
ART 0.05 pixel
ART 0.05 blob
⊡Fig. -
Values of the picture distance measure r for ART reconstructions from the standard
projection data with pixels (light) and blobs (dark), plotted at multiples of I iterations
(Reproduced from [])
ART: after one iterative step with λ(k) = , the associated measurement is satisfied exactly
as shown in (> .) and so the process jumps around satisfying the noise in the mea-
surements. Underrelaxation reduces the influence of the noise. The correct value of the
relaxation parameter is application dependent; the noisier the data the more we should be
underrelaxing. Note in
> Table -that the figure of merit IROI produced by the task-
oriented study for the case without underrelaxation is much smaller than for the other
cases.
Now we compare the best of our ART reconstruction (> Fig. -a, reproduced in
> Fig. -a) with one produced by a carefully selected variant of FBP, see (> .)–
(.). For comparison, we show in
> Fig. -b the reconstruction from our standard
projection data obtained by FBP for divergent beams with linear interpolation and sinc
window (also called the Shepp–Logan window, see []). For details of the meanings of
these choices and the reasons for them, see [, Chap. ]. The visual quality is similar to the
best ART reconstruction. According to the picture distance measures in > Table -, ART
is superior to FBP, and the same is true according to IROI with extreme significance (the
P-value is less than −). This experiment confirms the reports in the literature that ART
with blobs, underrelaxation and efficient ordering generally outperforms FBP in numerical
evaluations of the quality of the reconstructions.
One thing though is indisputable: the ART with blob reconstruction took nearly
times longer than FBP. However, this should not be the determining factor, especially
since the implementation of ART with blobs in SNARKis far from optimal and can be


Tomography
a
b
⊡Fig. -
Comparison of reconstructions from the standard projection data using (a) ART (the same as
> Fig. -a) and (b) FBP (Based on [, Fig. .])
greatly improved. An advantage of ART over FBP is its flexibility. Even though until now
we have reported its application only to data collected according to the standard geome-
try, ART is capable of reconstructing from data collected over any set of lines, as we soon
demonstrate by an example of using ART for helical CT. FBP-type algorithms need to be
reinvented for each new mode of data collection.
We now switch over to demonstrating the ART algorithm specified in (> .) and
(> .). As stated before, that algorithm converges to the Bayesian estimate that is the
minimizer of (> .), provided that the condition expressed in (> .) holds. This is
the case if we set λ(k) = ., for all k, which is what we chose for the experiments on which
we now report. The other choices that we made are blob basis functions, efficient ordering
and that, in (> .), t = and μX represents a uniform picture with the estimated average
value of the phantom assigned to every component.
There are alternative methods in the literature for minimizing (> .), a particularly
popular one is the method of conjugate gradients (CG); for a description of it that is appro-
priate for our context, see [, Sect. .]. The CG method is also an iterative one, but one
in which all the data are considered simultaneously in each iterative step. For this reason,
the time of one iterative step of the CG method is approximately the same as that needed
by ART for one cycle through all the data. In
> Fig. -we show a comparison of the
picture distance measure r for CG and for ART.
> Figure -and the picture distance measures in > Table -imply that the qual-
ity of the reconstruction obtained by the th iterate of the conjugate gradient method
should be as good as that obtained by the Ith iterate of additive ART. However, this is not
really so, as can be seen by looking at the reconstructed image in
> Fig. -b. Indeed
it needs another iterations of the conjugate gradient method before the visual quality

Tomography 

0.55
ART vs conjugate gradients :: Relative error
0.5
0.45
0.4
0.35
Relative error
0.3
0.25
0.2
0.15
0.1
0.05
0
2
4
6
8 10 12 14 16 18 20
Iteration
22 24 26 28 30 32 34 36 38 40
Conjugate gradients snr = 10 blob
Bayesian ART snr = 10 blob 0.05
⊡Fig. -
Values of the picture distance measure r for reconstructions from the standard projection
data using the conjugate gradient method (light) and ART (dark), plotted for comparable
computational costs (Reproduced from [])
⊡Table -
Picture distance measures and timings (in seconds) for the reconstructions that minimize
(> .) (Based on [, Table .])
Algorithm
d
r
t
ART, Ith iterate
.
.
.
conjugate gradient method, th iterate
.
.
.
of the reconstruction matches that of the ART of (> .) and (> .) after I itera-
tions, shown in > Fig. -a. So (for the standard projection data) the conjugate gradient
method is not as fast as ART. This slower convergence of conjugate gradients relative to
ART seems to be shared by other series expansion reconstruction methods that use all the
data simultaneously in each iteration; see, e.g., [].
If we wish to reconstruct a three-dimensional body by the methods discussed till now,
the only option available to us is to reconstruct the body cross section by cross section and
then stack the cross sections to form the three-dimensional distribution. This may cause
a number of problems, the most important of which are associated with time require-
ments. During the time needed to collect all the data, the patient may move, causing a
misalignment between the cross sections. More basically, in moving organs such as the
heart, changes in the organ over time are unavoidable, and it is usually not possible to
collect data for all cross sections simultaneously.


Tomography
a
b
⊡Fig. -
Reconstructions from the standard projection data using iterative methods that minimize
(> .). (a) ART, Ith iterate. (d) Conjugate gradient method, th iterate (Based
on [, Fig. .])
Sometimes, it is actually the change in the object over time that is the desired infor-
mation. If we wish to see cardiac wall motion, then it is essential that we reconstruct the
whole three-dimensional object at short time intervals. One may consider this as a four-
dimensional (spatio-temporal) reconstruction. One approach to obtaining reconstructions
of dynamically moving objects, such as the heart, from data that can be collected by helical
CT (see
> Fig. -) is to assume that the movement is cyclic. Assuming also that there
exists a way of recording where we are in the cyclic movement as we take the D views of
the moving D object, it is possible to bin the views into subsets such that all views that
are binned into any one of the subsets have been taken at approximately the same phase
of the cyclic movement, and so they are views of approximately the same (time frozen)
D object. In the case of the heart this can be done by recording the electrocardiogram and
noting on it the times when views have been taken. These views can then be binned, after
the fact, according to the phases of the cardiac cycle.
We complete this section by giving a summary of such experiments, details can be
found in [, Chap. ]. The reconstructions were done by ART (here we made good
use of the fact that ART does not require any particular arrangement of the lines for
which the data were collected), using three-dimensional blobs [] as the basis functions.
We designed a phantom of the human thorax based on the description of the so-called
FORBILD thorax phantom. We added to that stationary phantom two dynamically chang-
ing spheres representing the myocardium and a single contrast material–filled cavity. We
assumed that we are interested in this phantom at equally spaced (in time) phases of the
cardiac cycle. The first row of
> Fig. -shows a central cross section of this dynamic
phantom at the two extremes of the phases.

Tomography 

⊡Fig. -
The central cross section of the thorax phantom at the two extreme phases of the cardiac
cycle. First row: the phantom. Second row: reconstruction from data collected at the time
when the heart was in the appropriate phase after ﬁve cycles of simple ART. Third row:
reconstruction from data collected at the time when the heart was in the appropriate phase
after three cycles of Bayesian ART initialized with the reconstruction by two cycles of simple
ART (Based on [, Fig. .])


Tomography
Projection taking was done by integrating the density of the phantom along the lines
between the x-ray source position and detectors in a two-dimensional array. For every
source position, data were collected for equally spaced detectors in each of the rows
in the array. The size of each detector was assumed to be .× .cm. Data were
collected (i.e., the pulsing of the x-ray source was simulated) at every .second, using
a total of ,pulses. The number of turns of the helix in which the x-ray source moved
during the data collection was . The radius of the helix was cm, and the total movement
parallel to the axis of the helix was .cm. The distance from the source to the detector
array was cm. Integrals of the density were collected for I = ,,rays (,
pulses times rows of detectors). Detector area and the effect of photon statistics were
also simulated. The numbers used in this paragraph are not inappropriate for helical CT,
but a state-of-the-art helical CT scanner would have more and smaller detectors and would
be pulsed more frequently. In all our experiments we used J = ,,three-dimensional
blobs to describe the reconstructed three-dimensional distributions.
In the first experiment, we reconstructed the phases of the cardiac cycle indepen-
dently of each other. This was done by subdividing all the projection data into subsets,
each corresponding to one of the phases. A ray sum was put into a particular subset if it
was collected due to a pulsing of the x-ray source at a time nearer to the central time for
that phase than to the central time of any other phase. This results in a number of consecu-
tive pulses producing data for the same phase and then there is a relatively large gap before
the collected data are again used for that phase. This very nonuniform mode of data col-
lection results in unacceptably bad reconstructions, two of which are demonstrated in the
second row of > Fig. -. These reconstructions were produced using the simple ART of
(> .) and (> .) with the three-dimensional blob basis functions, with all compo-
nents of xgiven the estimated average value based on the projection data, all λ(k) = .
and an efficient ordering. The results are shown at the end of the fifth cycle through the
data associated with the particular phase of the cardiac cycle.
In the second experiment we used the other extreme: all the data were combined into
a single projection data set, without any attention paid to the phases of the cardiac cycle.
Because of the stationarity of most of the phantom and the overabundance of the projection
data, we get (using the same choices for ART as in the previous paragraph) reconstructions
that are good overall, but naturally the movement of the heart is blurred out due to the var-
ious views used in the reconstruction having been taken all through the cardiac cycle. We
note that in this case there is no need to cycle through the data five times: the reconstruc-
tion at the end of the second cycle through the data is just about indistinguishable from
the reconstruction at the end of the fifth cycle through the data.
However, our aim here is to see the dynamic changes in the heart. This can be achieved
by using the Bayesian approach of (> .) and (> .). We selected in (> .)
μX as the reconstruction obtained at the end of the second ART cycle through all the data
as described in the previous paragraph and t = .. For each separate phase of the cardiac
cycle, we used the algorithm specified by (> .) and (> .) for a further three cycles
through the data that are associated with that particular phase. The relaxation parameter

Tomography 

was again the constant .. The results, for the two extreme phases of the cardiac cycle,
are shown in the last row of
> Fig. -. Here the overall reconstruction of the thorax is
quite good and, at the same time, one can observe that the heart is dynamically changing.
With a state-of-the-art helical CT scanner (that would have more and smaller detectors
and would be pulsed more often) we would get even better reconstructions.
.
Conclusion
Tomography is the process of producing an image of a distribution from estimates of its
line integrals along a finite number of lines of known locations. There are a number of
mathematical approaches to achieve this and we discussed and illustrated some of them.
Of the investigated approaches, we found the performance of the method referred to as
ART with blobs particularly good, especially if it is used with the appropriate data access
ordering and relaxation parameters.
.
Cross-References
> Iterative Solution Methods
> Large Scale Inverse Problems
> Linear Inverse Problems
> Mathematical Tools for Visualization
> Regularization Methods for Ill-Posed Problems
> Statistical Inverse Problems
> Thermoacoustic Tomography
References and Further Reading
We subdivided the recommended readings into cate-
gories. For a more comprehensive and up-to-date
list see [] that has references, of which
have been published since .
Books related tomography [, , –, , , , ,
, , ].
Papers on transform reconstruction methods and
their applications [, , , –, , , , , ,
].
Papers on series expansion reconstruction methods
and their applications [, , , , , , , ,
–, –, , ].
Papers on comparison of reconstruction methods
[, , , , –, , ].
Papers on three-dimensional display of reconstruc-
tions [, , , ].
. Artzy E, Frieder G, Herman GT () The
theory, design, implementation and evalua-
tion of a three-dimensional surface detection
algorithm. Comput Graph Image Process :
–
. Banhart J () Advanced tomographic meth-
ods in materials research and engineering.
Oxford University Press, Oxford
. Bracewell RN () Strip integration in radio
astronomy. Aust J Phys :–
. Browne JA, De Pierro AR () A row-action
alternative to the EM algorithm for maximizing


Tomography
likelihood in emission tomography. IEEE Trans
Med Imaging :–
. Censor Y, Zenios SA () Parallel optimiza-
tion: theory, algorithms and applications. Oxford
University Press, New York
. Censor Y, Altschuler MD, Powlis WD () On
the use of Cimmino’s simultaneous projections
method for computing a solution of the inverse
problemin radiation therapy treatment planning.
Inverse Probl :–
. Chen LS, Herman GT, Reynolds RA, Udupa JK
() Surface shading in the cuberille environ-
ment (erratum appeared in ():–, ).
IEEE Comput Graph Appl ():–
. Cormack AM () Representation of a func-
tion by its line integrals, with some radiological
applications. J Appl Phys :–
. Crawford CR, King KF () Computed-
tomography scanning with simultaneous patient
motion. Med Phys :–
. Crowther RA, DeRosier DJ, Klug A () The
reconstruction of a threedimensional structure
from projections and its application to elec-
tron microscopy. Proc R Soc Lon Ser-A A:
–
. Davidi R, Herman GT, Klukowska J ()
SNARK: a programming system for the recon-
struction of D images from D projections.
http://www.snark.com, 
. DeRosier DJ, Klug A () Reconstruction
of three-dimensional structures from electron
micrographs. Nature :–
. Edholm P, Herman GT, Roberts DA () Image
reconstruction from linograms: implementation
and evaluation. IEEE Trans Med Imaging :
–
. Edholm PR, Herman GT () Linograms in
image reconstruction from projections. IEEE
Trans Med Imaging :–
. Eggermont PPB, Herman GT, Lent A () Iter-
ative algorithms for large partitioned linear sys-
tems, with applications to image reconstruction.
Linear Algebra Appl :–
. Epstein
CS
()
Introduction
to
the
mathematics of medical imaging, nd edn. SIAM,
Philadelphia
. Frank J (a) Electron tomography: methods
for three-dimensional visualization of structures
in the cell, nd edn. Springer, New York
. Frank J (b) Three-dimensional electron
microscopy of macromolecular assemblies: visu-
alization of biological molecules in their native
state. Oxford University Press, New York
. Gordon R, Bender R, Herman GT () Alge-
braic Reconstruction Techniques (ART) for
three-dimensional
electron
microscopy
and
x-ray photography. J Theor Biol :–
. Hanson KM () Method of evaluating image-
recovery algorithms based on task performance.
J Opt Soc Am A :–
. Herman GT () Advanced principles of recon-
struction algorithms. In: Newton TH, Potts DG
(eds) Radiology of skull and brain, vol : Techni-
cal aspects of computed tomography. C.V. Mosby,
St. Louis, pp –
. Herman GT () Fundamentals of comput-
erized tomography: image reconstruction from
projections, nd edn. Springer, London
. Herman GT, Kuba A () Advances in discrete
tomography and its applications. Birkhäuser,
Boston
. Herman GT, Lent A () Iterative reconstruc-
tion algorithms. Comput Biol Med :–
. Herman GT, Liu HK () Three-dimensional
display of human organs from computed tomo-
grams. Comput Graph Image Process :–
. Herman GT, Meyer LB () Algebraic recon-
struction techniques can be made computa-
tionally efficient. IEEE Trans Med Imaging
:–
. Herman GT, Naparstek A () Fast image
reconstruction based on a Radon inversion for-
mula appropriate for rapidly collected data. SIAM
J Appl Math :–
. HermanGT,TuyHK,LangenbergKJ,Sabatier PC
() Basic methods of tomography and inverse
problems. Institute of Physics Publishing, Bristol
. Hounsfield GN () Computerized transverse
axial scanning tomography: Part I, description of
the system. Br J Radiol :–
. Hudson HM, Larkin RS () Accelerated image
reconstruction using ordered subsets of projec-
tion data. IEEE Trans Med Imaging :–
. Kalender WA () Computed tomography:
fundamentals, system technology, image quality,
applications, nd edn. Wiley-VCH, Munich
. Kalender WA, Seissler W, Klotz E, Vock P ()
Spiral volumetric CT with single-breath-hold

Tomography 

technique, continuous transport, and continuous
scanner rotation. Radiology :–
. Katsevich A () Theoretically exact filtered
backprojection-type inversion algorithm for spi-
ral CT. SIAM J Appl Math :–
. Kinahan PE, Matej S, Karp JP, Herman GT,
Lewitt RM () A comparison of transform and
iterative reconstruction techniques for a volume-
imaging PET scanner with a large axial accep-
tance angle. IEEE Trans Nucl Sci :–
. Lauterbur PC () Medical imaging by nuclear
magneticresonancezeugmatography.IEEE Trans
Nucl Sci :–
. Levitan E, Herman GT () A maximum a
posteriori probability expectation maximization
algorithm for image reconstruction in emis-
sion tomography. IEEE Trans Med Imaging :
–
. LewittRM() Multidimensional digital image
representation using generalized Kaiser-Bessel
window functions. J Opt Soc Am A :–
. Lewitt RM () Alternatives to voxels for image
representation in iterative reconstruction algo-
rithms. Phys Med Biol :–
. Lorensen W, Cline H () Marching cubes: a
high-resolution D surface reconstruction algo-
rithm. Comput Graph ():–
. Maki DD, Birnbaum BA, Chakraborty DP, Jacobs
JE, Carvalho BM, Herman GT () Renal cyst
pseudo-enhancement:Beamhardening effectson
CT numbers. Radiology :–
. Marabini R, Rietzel E, Schroeder R, Herman
GT, Carazo JM () Threedimensional recon-
struction from reduced sets of very noisy images
acquired following a single-axis tilt schema:
application of a new three-dimensional recon-
struction algorithm and objective comparison
with weighted backprojection. J Struct Biol
:–
. Marabini R, Herman GT, Carazo J-M ()
D reconstruction in electron microscopy using
ART with smooth spherically symmetric volume
elements (blobs). Ultramicroscopy :–
. Matej S,LewittRM() Practical consideration
for D image-reconstruction using spherically-
symmetrical volume elements. IEEE Trans Med
Imaging :–
. Matej S, Herman GT, Narayan TK, Furuie SS,
Lewitt RM, Kinahan PE () Evaluation of
task-oriented performance of several fully D
PET reconstruction algorithms. Phys Med Biol
:–
. Matej S, Furuie SS, Herman GT () Relevance
of statistically significant differences between
reconstruction algorithms. IEEE Trans Image
Process :–
. Narayan TK, Herman GT () Prediction
of human observer performance by numerical
observers: an experimental study. J Opt Soc Am
A :–
. Natterer F, Wübbeling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Poulsen HF () Three-dimensional x-ray
diffraction microscopy: mapping polycrystals
and their dynamics. Springer, Berlin
. Radon J () Über die Bestimmung von Funk-
tionen durch ihre Integralwerte längs gewisser
Mannigfaltigkeiten. Ber Verh Sächs Akad Wiss,
Leipzig, Math Phys Kl :–
. Ramachandran
GN,
Lakshminarayanan
AV
()
Three-dimensional
reconstruction
from
radiographs
and
electron
micro-
graphs:
application
of
convolutions
instead
of Fourier transforms. Proc Natl Acad Sci USA
:–
. Scheres SHW, Gao H, Valle M, Herman GT,
Eggermont PPB, Frank J, Carazo J-M ()
Disentangling conformational states of macro-
molecules in D-EM through likelihood opti-
mization. Nat Methods :–
. Scheres SHW, Nuñez-Ramirez R, Sorzano COS,
Carazo JM, Marabini R () Image processing
for electron microscopy single-particle analysis
using XMIPP. Nat Protocols :–
. Shepp LA, Logan BF () The Fourier recon-
struction of a head section. IEEE Trans Nucl Sci
:–
. Shepp LA, Vardi Y () Maximum likelihood
reconstruction for emission tomography. IEEE
Trans Med Imaging :–
. Sorzano COS, Marabini R, Boisset N, Rietzel
E, Schröder R, Herman GT, Carazo JM ()
The effect of overabundant projection directions
on D reconstruction algorithms. J Struct Biol
:–
. Udupa JK, Herman GT () D imaging in
medicine, nd edn. CRC Press, Boca Raton


Optical Imaging
Simon R. Arridge ⋅Jari P. Kaipio ⋅Ville Kolehmainen ⋅
Tanja Tarvainen
.
Introduction......................................................................
.
Background......................................................................
..
Spectroscopic Measurements..........................................................
..
Imaging Systems.........................................................................
.
Mathematical Modeling and Analysis..........................................
..
Radiative Transfer Equation...........................................................
..
Diffusion Approximation..............................................................
...
Boundary Conditions for the DA.....................................................
...
Source Models for the DA..............................................................
...
Validity of the DA.......................................................................
...Numerical Solution Methods for the DA............................................
..
Hybrid Approaches Utilizing the DA................................................
..
Green’s Functions and the Robin to Neumann Map...............................
..
The Forward Problem..................................................................
..
Schrödinger Form.......................................................................
..
Perturbation Analysis...................................................................
...
Born Approximation....................................................................
...
Rytov Approximation...................................................................
..
Linearization.............................................................................
...
Linear Approximations.................................................................
...
Sensitivity Functions....................................................................
..
Adjoint Field Method...................................................................
...
Time Domain Case......................................................................
..
Light Propagation and Its Probabilistic Interpretation ............................
.
Numerical Methods and Case Examples.......................................
..
Image Reconstruction in Optical Tomography.....................................
..
Bayesian Framework for Inverse Optical Tomography Problem.................
...
Bayesian Formulation for the Inverse Problem.....................................
...Inference..................................................................................
...Likelihood and Prior Models..........................................................
...Nonstationary Problems...............................................................
...Approximation Error Approach......................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Optical Imaging
..
Experimental Results...................................................................
...
Experiment and Measurement Parameters..........................................
...Prior Model..............................................................................
...Selection of FEM Meshes and Discretization Accuracy...........................
...Construction of Error Models.........................................................
...Computation of the MAP Estimates.................................................
.
Conclusions......................................................................
.
Cross References.................................................................

Optical Imaging 

Abstract: This chapter discusses diffuse optical tomography. We present the origins of
this method in terms of spectroscopic analysis of tissue using near-infrared light and its
extension to an imaging modality. Models for light propagation at the macroscopic and
mesoscopic scale are developed from the radiative transfer equation (RTE). Both time and
frequency domain systems are discussed. Some formal results based on Green’s function
models are presented, and numerical methods are described based on discrete finite ele-
ment method (FEM) models and a Bayesian framework for image reconstruction. Finally,
some open questions are discussed.
.
Introduction
Optical Imaging in general covers a wide range of topics. In this chapter we mean tech-
niques for indirect imaging using light as a method for obtaining observations of a subject.
In a typical experiment, a highly scattering medium is illuminated by a narrow collimated
beam, and the light that propagates through the medium is collected by an array of detec-
tors. There are many variants of this basic scenario. For instance, the source may be pulsed
or time harmonic, coherent or incoherent, and the illumination may be spatially structured
or multispectral. Likewise, the detector may be time or frequency resolved, polarization or
phase sensitive, located in the near or far field and so on. The inverse problem that is consid-
ered is to reconstruct the optical properties of the medium from boundary measurements.
The mathematical formulation of the corresponding forward problem is dictated primarily
by spatial scale, ranging from the Maxwell equations at the microscale to the radiative trans-
port equation at the mesoscale and to the diffusion theory at the macroscale. In addition,
experimental time scales vary from the femtosecond on which light pulses are generated,
through the nanosecond on which diffuse waves propagate, to the millisecond scale on
which biological activation takes place and still longer for pathophysiologic changes.
In this chapter, we concentrate primarily on the macroscopic scale and the diffusion
model for light propagation. The derivation of this model and its limits of applicability are
discussed in > Sect. ... Historically, a large amount of early development considered
analytic forms for the Green’s function of the diffusion equation and series expressions for
the effect of perturbations of these propagators by inhomogeneities; usually only first-order
linear methods were considered. These are discussed in > Sect. ... As computational
methods become more readily available, more sophisticated approaches using optimiza-
tion and Bayesian methods are becoming more accepted. We discuss these approaches in
> Sect. ..
.
Background
> Figure -schematically illustrates the two main types of measurement system: time
resolved and intensity modulated. In the former a short duration pulse ∼–ps is
employed, and in the latter a steady state intensity is created, modulated at a frequency
in the range –,MHz. Obviously, the spectrum of frequencies in the time domain is


Optical Imaging
Tissue
Time
Tissue
Phase shift
DC amplitude
AC amplitude
Time
Iin = Ain(1+Mineiwt)
Iout = Aout(1+Mouteiwt)
⊡Fig. -
Optical transillumination measurements made with a time-resolved system (left) or an
intensity-modulated system (right)
many order higher than in the frequency domain systems themselves, although the higher
frequencies are very heavily damped and carry no information. A third domain is “DC”
systems – these are the same as frequency domain, without the modulation. They are much
simpler and cheaper, but without a complex wave, the inverse problem is nonunique [].
..
Spectroscopic Measurements
Attenuation of light in the near infrared (NIR) is due to absorption and scattering. The
parameter of most interest is absorption which is caused by chromophores of variable con-
centration such as hemoglobin in its oxygenated and deoxygenated states. In the absence
of scattering, the change in light intensity obeys the Beer–Lambert law
−ln Iin
Iout
= μa d = αc[c]d,
(.)
where d is the source-detector separation, which is equal to the optical pathlength, [c] is
the concentration of chromophore c, and αc is the absorption coefficient per unit length per
unit concentration of chromophore c and can usually be obtained in vitro. In the presence
of scattering the optical pathlength of transmitted photons follows a much more com-
plex relationship. Hence attenuation measurements alone do not allow quantification of
chromophore concentration.
Continuous intensity (DC) instruments measure changes in the intensity of light leav-
ing the tissue surface []. This is frequently done in a purely spectroscopic manner, i.e., to
obtain only global changes in chromophore concentration. In order to quantify concentra-
tion changes additional information is required. One approach is to derive an approximate

Optical Imaging 

differential path length factor (DPF), which restores the approximate Beer–Lambert law for
small changes in concentration
−δ ln Iin
Iout
= DPFδμa d.
(.)
Since there are typically several contributing chromphores, light of different wavelengths
in the NIR region is employed and regression techniques are used to find their relative
weightings []. It was shown empirically [] that the DPF is simply the mean time of
light multiplied by the speed of light in the tissue. In fact this relationship follows naturally
from the diffusion approximation of light transport []. Furthermore, it is equally well
approximated by the change in phase of an intensity-modulated system, at least at low
modulation frequencies.
Intensity-modulated measurements were first reported by []. Most systems use a
heterodyne technique to mix the transmitted light with a reference beam of slightly dif-
ferent modulation frequency, thus producing a lower frequency envelope that is easier to
detect using RF equipment. Time-resolved systems were first developed using a streak cam-
era [, , ], an instrument with exceptionally high time resolution in the picosecond
range but with high cost, relatively low dynamic range, and a significant inherent temporal
nonlinearity due to a sinusoidal ramp voltage. Alternatively time-correlation single pho-
ton counting (TCSPC) systems measure arrival times of individual photons by comparison
with a reference pulse using a time-to-amplitude converter (TAC) device [, , ]. These
systems have a high dynamic range and excellent temporal linearity.
..
Imaging Systems
Imaging methods can be divided into direct systems which seek to detect heterogeneities
in tissue by analyzing the transmitted (or, in some cases, reflected) light and indirect
systems which attempt to solve the inverse problem of image reconstruction. The lat-
ter is the main emphasis of this article although the former is historically the precedent,
in a similar manner in which x-ray radiographs were the precursor to x-ray computed
tomography (CT).
Transillumination of candle light for a patient suffering from hydrocephalus was
reported as early as , but the first significant attempt at diagnostic imaging using optical
radiation was for breast lesions and was made by Cutler [], who used a lamp held under
the breast in a darkened room. However even at this stage, multiple scattering effects caused
a notable degradation in image quality. The recognition of this fact led to many attempts
to eliminate or minimize the degradation due to scattering ranging from collimation []
and polarization discrimination [] to coherence gating using holographic gating [] or
heterodyne detection [].
With the introduction of time-resolved detectors came the natural attempt to use tem-
poral gating to discriminate early arriving photons (which necessarily have the shortest
optical path and therefore suffer the least number of scatterings) from later arriving pho-
tons which have undergone multiple scatterings and therefore have ill determined photon


Optical Imaging
paths. The early implementations of this idea used a Kerr gate as an ultrafast shutter [].
However, this technique is limited to relatively low-scattering media due to the small
dynamic range of the Kerr shutter. Other studies have been based on the streak camera []
or TCSPC [] systems described in > Sect. ...
The attempt to physically discriminate between photons that have undergone different
numbers of scattering events is inherently limited by the statistical likelihood of the low
scattering number photons arriving at the detector. For the relatively optically thick tissues
that are of interest in breast cancer screening or brain imaging, these photons are over-
whelmed by noise. For this reason, indirect methods that solve an inverse problem based
on recovering the spatially varying optical parameters that provide the best fit of a photon
transport model with the measured data are becoming more prevalent. Within this frame-
work the three basic strategies (time resolved, intensity modulated, and DC systems) have
all been developed and reported. In addition, many different geometrical arrangements
have been investigated. Initial studies have been on D slice-by-slice imaging, although it
is apparent that the photon propagation must in reality be described by a D model. Fully
D methods are now appearing.
In the remainder of this article, we will discuss the inverse problem and the strategies
that have been adopted in order to solve it. In order to analyze this problem, we first have
to consider the model of photon transport in dense media.
.
Mathematical Modeling and Analysis
..
Radiative Transfer Equation
In optical imaging, light transport through a medium containing scattering particles is
described by transport theory []. In transport theory, the particle conservation within a
small volume element of phase space is investigated. The wave phenomenon of particles is
ignored. The transport theory can be modeled through stochastic methods and determin-
istic methods. In the stochastic approach, individual particle interactions are modeled as
the particles are scattered and absorbed within the medium. The two stochastic methods
that have been used in optical imaging are the Monte Carlo method and the random walk
theory, of which two, the Monte Carlo is the most often used [].
In deterministic approach, particle transport is described with integro-differential
equations which can be solved either analytically or numerically []. In optical imaging,
a widely accepted model for light transport is the radiative transport equation (RTE). The
RTE is a one-speed approximation of the transport equation, and thus it basically assumes
that the energy (or speed) of the particles does not change in collisions and that the refrac-
tive index is constant within the medium. For discussion of photon transport in medium
with spatially varying refractive index, see, e.g., [, , , ].
Let Ω ⊂Rn, n = ordenote the physical domain where n is the dimension of the
domain. The medium is considered isotropic in the sense that the probability of scattering

Optical Imaging 

between two directions depends only on the relative angle between those directions and
not on an absolute direction. For discussion of light propagation in anisotropic medium,
see, e.g., []. Furthermore, let ∂Ω denote the boundary of the domain and ˆs ∈Sn−denote
a unit vector in the direction of interest. The RTE is written in time domain as

c
∂ϕ(r, ˆs)
∂t
+ ˆs ⋅∇ϕ(r, ˆs) + (μs + μa)ϕ(r, ˆs)
= μs ∫Sn−Θ(ˆs ⋅ˆs′)ϕ(r, ˆs′)dˆs′ + q(r, ˆs)
(.)
and in frequency domain as
iω
c ϕ(r, ˆs) + ˆs ⋅∇ϕ(r, ˆs) + (μs + μa)ϕ(r, ˆs)
= μs ∫Sn−Θ(ˆs ⋅ˆs′)ϕ(r, ˆs′)dˆs′ + q(r, ˆs),
(.)
where c is the speed of light in the medium, i is the imaginary unit, ω is the angular mod-
ulation frequency of the input signal, and μs = μs(r) and μa = μa(r) are the scattering
and absorption coefficients of the medium, respectively. The scattering coefficient rep-
resents the probability per unit length of a photon being scattered and the absorption
coefficient represents the probability per unit length of a photon being absorbed. Fur-
thermore, ϕ(r, ˆs) is the radiance, Θ(ˆs ⋅ˆs′) is the scattering phase function, and q(r, ˆs) is
the source inside Ω. The radiance can be defined such that the amount of power trans-
fer in the infinitesimal angle dˆs in direction ˆs at time t through an infinitesimal area dS
is given by
ϕ(r, ˆs; t)ˆs ⋅ˆdSdˆs,
where ˆ is the normal to the surface dS []. The scattering phase function Θ(ˆs ⋅ˆs′)
describes the probability that a photon with an initial direction ˆs′ will have a direction
ˆs after a scattering event. In optical imaging, the most usual phase function for isotropic
material is the Henyey–Greenstein scattering function [] which is of the form
Θ(ˆs ⋅ˆs′) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩

π
−g
(+g−gˆs⋅ˆs′),
n = ,

π
−g
(+g−gˆs⋅ˆs′)/,
n = ,
(.)
where g is the scattering shape parameter that defines the shape of the probability density
and it gets values between −< g < . With the value g = , the scattering probability den-
sity is a uniform distribution. For forward dominated scattering g > and for backward
dominated scattering g < . The time-domain and frequency-domain representations of
the RTE are related through Fourier transform.
In order to obtain a unique solution for the RTE, the ingoing radiance distribution on
the boundary ∂Ω, that is, ϕ(r, ˆs) for ˆs ⋅ˆ < , where ˆ is the outward unit normal needs
to be known []. Several boundary conditions can be applied to the RTE [, , ].
In optical imaging, the boundary condition which assumes that no photons travel in an
inward direction at the boundary ∂Ω is used []
ϕ(r, ˆs) = ,
r ∈∂Ω,
ˆs ⋅ˆn < .
(.)


Optical Imaging
This boundary condition, also known as the free surface boundary condition and the vac-
uum boundary condition, implies that once a photon escapes the domain Ω it does not
reenter it. The boundary condition (> .) can be modified to include a boundary source
ϕ(r, ˆs) at the source position ε j ⊂∂Ω and it can be written in the form []
ϕ(r, ˆs) = {ϕ(r, ˆs),
r ∈∪jε j,
ˆs ⋅ˆn < 
,
r ∈∂Ω/ ∪j ε j,
ˆs ⋅ˆn < .
(.)
In optical imaging, the measurable quantity is the exitance Jn(r) on the boundary of the
domain. It is defined as []
Jn(r) = ∫Sn−(ˆs ⋅ˆ)ϕ(r, ˆs)dˆs,
r ∈∂Ω.
(.)
..
Diﬀusion Approximation
In optical imaging, light propagation in tissues is usually modeled with the diffusion
approximation (DA) to the RTE. The most typical approach to derive the DA from the
RTE is to expand the radiance, the source term, and the phase function into series using
the spherical harmonics and truncate the series [, , ]. If the spherical harmonics
series is truncated at the Nth moment, PN approximation is obtained [, ]. The first-
order spherical harmonics approximation is referred as the Papproximation and the DA
can be regarded as a special case for that. The most typical approach for utilizing the PN
approximations in optical imaging has been to use them in angular discretization of the
numerical solution of the RTE [, ].
An alternative to the PN approximation is the Boltzmann hierarchy approach, in which
moments of radiance are used to form a set of coupled equations that approximate the
RTE []. Furthermore, the DA can be derived using asymptotic techniques [, ] leading
to generalized diffusion equation or by using projection algebra [, ]. If the speed of
light is not constant, a diffusion equation with spatially varying indices of refraction can
be derived [].
Here, a short review of the derivation of the DA is given according to [, ]. First, the
Papproximation is derived, and then, the DA is formed as a special case for that. In the
DA framework, the radiance is approximated by
ϕ(r, ˆs) ≈

∣Sn−∣Φ(r) +
n
∣Sn−∣ˆs ⋅J(r),
(.)
where Φ(r) and J(r) are the photon density and photon current which are defined as
Φ(r) = ∫Sn−ϕ(r, ˆs)dˆs
(.)
J(r) = ∫Sn−ˆsϕ(r, ˆs)dˆs.
(.)

Optical Imaging 

By inserting the approximation (> .) and similar approximations written for the source
term and phase function into > Eq. .and following the derivation in [, ], the P
approximation is obtained
(iω
c + μa) Φ(r) + ∇⋅J(r) = q(r),
(.)
(iω
c + μa + μ′
s) J(r) + 
n∇Φ(r) = q(r),
(.)
where μ′
s = (−g)μs is the reduced scattering coefficient, q(r) and q(r) are the isotropic
and dipole components of the source, and gis the mean of the cosine of the scattering angle
[, ]
g= ∫Sn−(ˆs ⋅ˆs′)Θ(ˆs ⋅ˆs′)dˆs.
(.)
In the case of the Henyey–Greenstein scattering function, > Eq. ., we have g= g.
To derive the diffusion approximation, it is further assumed that the light source is
isotropic, thus q(r) = , and that iω
c J(r) = . The latter assumption, which in time-domain
case is of the form 
c
∂J(r)
∂t
= , is usually justified by specifying the condition μa ≪μ′
s [].
Utilizing these approximations, > Eq. .gives the Fick’s law
J(r) = −κ∇Φ(r),
(.)
where
κ = κ(r) = (n (μa + μ′
s))
−
(.)
is the diffusion coefficient. Substituting > Eq. .into > Eq. ., the frequency-domain
version of the DA is obtained. It is of the form
−∇⋅κ∇Φ(r) + μaΦ(r) + iω
c Φ(r) = q(r).
(.)
The DA has an analog in time domain as well. It is of the form
−∇⋅κ∇Φ(r) + μaΦ(r) + 
c
∂Φ(r)
∂t
= q(r).
(.)
The time-domain and frequency-domain representations of the DA are related through
Fourier transform, similarly as in the case of the RTE.
...
Boundary Conditions for the DA
The boundary condition (> .) cannot be expressed in terms of variables of the diffu-
sion approximation. Instead, there are a few boundary conditions that have been applied
to the DA. The simplest boundary condition is the Dirichlet boundary condition which
is also referred as the zero-boundary condition. It sets the photon density to zero on the
boundary, thus Φ(r) = , r ∈∂Ω [, ]. Alternatively, an extrapolated boundary con-
dition can be used [, , ]. In the approach, the photon density is set to zero on an
extrapolated boundary which is a virtual boundary outside the medium located at a certain


Optical Imaging
distance from the real boundary. Both the zero-boundary condition and the extrapolated
boundary condition are physically incorrect and they have mostly been used because of
their mathematical simplicity [].
The most often used boundary condition in optical imaging is the Robin boundary
condition which is also referred as the partial current boundary condition [, , , ,
, ]. It can be derived as follows. Within the Papproximation framework (> .), the
total inward- and outward-directed photon fluxes at a point r ∈∂Ω are
J−(r) = −∫ˆs⋅ˆn<(ˆs ⋅ˆn)ϕ(r, ˆs)dˆs = γnΦ(r) −
ˆ ⋅J(r)
(.)
J+(r) = ∫ˆs⋅ˆn>(ˆs ⋅ˆn)ϕ(r, ˆs)dˆs = γnΦ(r) + 
ˆ ⋅J(r),
(.)
where γn is a dimension-dependent constant which obtains values γ= /π and γ= /
[]. To derive the Robin boundary condition for the DA, it is assumed that the total
inward-directed photon flux on the boundary is zero, thus
J−(r) = ,
r ∈∂Ω.
(.)
Utilizing > Eq. .and the Fick’s law (> .), the Robin boundary condition can be
derived. It is of the form
Φ(r) +

γn
κ ∂Φ(r)
∂ˆ
= ,
r ∈∂Ω.
(.)
The boundary condition (> .) can be extended to include the reflection on the
boundary that is caused by different refractive indices between the object and the sur-
rounding medium. In that case, > Eq. .is modified to the form
J−(r) = RJ+(r),
r ∈∂Ω,
(.)
where R = R(x) is the reflection coefficient on the boundary ∂Ω, with ≤R ≤[].
Thus, if R = , no boundary reflection occurs and > Eq. .is reduced into > Eq. ..
The parameter R can be derived from Fresnel’s law [], or, if the refractive index of the
surrounding medium is nout = , by an experimental fit
R ≈−.n−
in + .n−
in + .+ .nin,
(.)
where nin is the refractive index of the medium []. Utilizing (> Eqs. .) and (> .)
and the Fick’s law (> .), the Robin boundary condition with mismatched refractive
indices can be derived. It takes the form
Φ(r) +

γn
κζ ∂Φ(r)
∂ˆ
= ,
r ∈∂Ω,
(.)
where ζ = (+ R)/(−R), with ζ = in the case of no surface reflection. The boundary
conditions of the DA for an interface between two highly scattering materials have been
discussed, for example, in [].

Optical Imaging 

The exitance, > Eq. (.), can be written utilizing > Eqs. ., > ., the Fick’s law
(> .), and the boundary condition (> .). In the DA framework, the exitance is of
the form
Jn(r) = J+(r) −J−(r) = ˆ ⋅J(r)
= −κ ∂Φ(r)
∂ˆ
= γn
ζ Φ(r),
r ∈∂Ω.
(.)
...
Source Models for the DA
In the DA framework, light sources are usually modeled by two approximate models,
namely the collimated source model and the diffuse source model. In the case of the
collimated source model, the source is modeled as an isotropic point source
q(r) = δ(r −rs),
(.)
where position rs is located at a depth /μ′
s below the source site [, ]. In the case of
the diffuse source model, the source is modeled as an inward-directed diffuse boundary
current Is at the source position ε j ⊂∂Ω []. In the case of the diffuse source model,
> Eq. .can be modified as
J−(r) = RJ+(r) + (−R)Is,
r ∈∪jε j.
(.)
Then, following the similar procedure as earlier, the Robin boundary condition with the
diffuse source model is obtained. It is of the form
Φ(r) +

γn
κζ ∂Φ(r)
∂ˆ
= {
Is
γn ,
r ∈∪jε j
,
r ∈∂Ω/ ∪j ε j.
(.)
...
Validity of the DA
The basic condition for the validity of the DA is that the angular distribution of the radi-
ance is almost uniform. In order to achieve this, the medium must be scattering dominated,
thus μa ≪μs. Most of the tissue types are highly scattering and the DA can be regarded
as a good approximation for modeling light propagation within them. The DA has been
found to describe light propagation with a good accuracy in situations in which its assump-
tions are valid [, ] and it has been successfully applied in many applications of optical
tomography.
However, the condition stating that the angular distribution of the radiance must be
almost uniform is violated close to the highly collimated light sources. In addition, the
condition cannot be fulfilled in strongly absorbing or low-scattering tissues such as the
cerebrospinal fluid which surrounds the brain and fills the brain ventricles. Furthermore, in
addition to above conditions, the DA cannot accommodate realistic boundary conditions
or discontinuities at interfaces. The diffusion theory has been found to fail in situations in


Optical Imaging
which its approximations are not valid such as close to the sources [, ] and within the
low-scattering regions [, ].
...
Numerical Solution Methods for the DA
The analytical solutions of the RTE and its approximations are often restricted to cer-
tain specific geometries and therefore their exploitability in optical imaging is limited.
Therefore, the equations describing light propagation are usually solved with numerical
methods. The most often applied numerical methods are the finite difference method and
the finite element method (FEM). The latter is generally regarded as more flexible when
issues of implementing different boundary conditions and handling complex geometries
are considered, and therefore it is most often chosen as the method for solving equations
governing light transport in tissues.
The FE model for the time-varying DA was introduced in []. It was later extended to
address the topics of boundary conditions and source models [, ] and the frequency-
domain case of the DA []. It can be regarded as the most typical approach to numerically
solve the DA.
..
Hybrid Approaches Utilizing the DA
To overcome the limitations of the diffusion theory close to the light sources and within
low-scattering and non-scattering regions, different hybrid approaches and approximative
models have been developed.
The hybrid Monte Carlo diffusion method was developed to overcome the limitations
of the DA close to the light sources. In the approach, Monte Carlo simulation is combined
with the diffusion theory. The method was introduced in [] to describe light reflectance
in a semi-infinite turbid medium and it was extended for turbid slabs in []. In the hybrid
Monte Carlo diffusion approach, Monte Carlo method is used to simulate light propagation
close to the light source and the DA is analytically solved elsewhere in the domain. Monte
Carlo is known to describe light propagation accurately. However, it has the disadvantage
of requiring a long computation time. This has effects on computation times of the hybrid
Monte Carlo approaches as well. A hybrid radiative transfer–diffusion model to describe
light propagation in highly scattering medium was introduced in []. In the approach,
light propagation is modeled with the RTE close to the light sources and the DA is used
elsewhere in the domain. The solution of the RTE is used to construct a Dirichlet boundary
condition for the DA on a fictitious interface within the object. Both the RTE and the DA
are numerically solved with the FEM.
Different hybrid approaches and approximative models have been applied for highly
scattering media with low-scattering and non-scattering regions. Methods that combine

Optical Imaging 

Monte Carlo simulation with diffusion theory have been applied for turbid media with
low-scattering regions. The finite element approximation of the DA and the Monte Carlo
simulation were combined in [] to describe light propagation in a scattering medium
with a low-scattering layer. However, also in this case, the approach suffers from the
time-consuming nature of the Monte Carlo methods. Moreover, the hybrid Monte Carlo
diffusion methods often require iterative mapping between the models which increases
computation times even more. The radiosity-diffusion model [, ] can be applied for
highly scattering media with non-scattering regions. The method uses the FE solution of
the DA to model light propagation within highly scattering regions and the radiosity model
to model light propagation within non-scattering regions. A coupled transport and diffu-
sion model was introduced in []. In the model, the transport and diffusion models are
coupled and iterative mapping between the models is used for the forward solution. Fur-
thermore, a coupled radiative transfer equation and diffusion approximation model for
optical tomography was introduced in [] and extended for domains with low-scattering
regions in []. In the approach, the RTE is used as the forward model in sub-domains
in which the assumptions of the DA are not valid and the DA is used elsewhere in the
domain. The RTE and DA are coupled through boundary conditions between the RTE and
DA sub-domains and solved simultaneously using the FEM.
..
Green’s Functions and the Robin to Neumann Map
Some insight into light propagation in diffusive media can be gained by examining infinite
media. In particular, verification of optical scattering and absorption parameters is fre-
quently made with source and detector fibers immersed in a large container, and far from
the container walls. In a finite domain, however, we will need to use boundary conditions.
We will distinguish between solutions to the homogeneous equation with inhomogeneous
boundary conditions and the inhomogeneous equation with homogeneous boundary con-
ditions. In the latter case, we can use a Green’s function acting on q. In the former case we
use a Green’s function acting on a specified boundary function.
We will use the notation GΩ for the Green’s function for the inhomogeneous form of
(> .) with homogeneous boundary conditions, and G∂Ω for the Green’s function for
the homogeneous form of (> .) with inhomogeneous boundary conditions, i.e., we
have GΩ solving
−∇⋅κ(r)∇GΩ(r,r′, t, t′) + (μa(r) + 
c
∂
∂t)GΩ(r,r′, t, t′) = δ(r′)δ(t′)
(.)
r,r′ ∈Ω/∂Ω, t > t′
GΩ(rd,r′, t, t′) + ζκ(rd)∂GΩ(rd,r′, t, t′)
∂
= 
(.)
rd ∈∂Ω


Optical Imaging
and G∂Ω solving
−∇⋅κ(r)∇G∂Ω(r,rs, t, t′) + (μa(r) + 
c
∂
∂t)G∂Ω(r,rs, t, t′) = 
(.)
r ∈Ω/∂Ω, t > t′
G∂Ω(rd,rs, t, t′) + ζκ(rd)∂G∂Ω(rd,rs, t, t′)
∂
= δ(rs)δ(t′)
(.)
rs,rd ∈∂Ω.
For a given Green’s function G, we define the corresponding Green’s operator as the integral
transform with G as its kernel:
G f := ∫
∞
−∞∫Ω G(r,r′, t, t′)f (r′, t′)dnr′dt.
For the measureable we define the boundary derivative operator as
B := −κ ∂
∂,
where appropriate we will use the simplifying notation
GB := BG
to mean the result of taking the boundary data for a Green’s function.
Since (> .) is parabolic, we must not simultaneously specify both Dirichlet and
Neumann boundary conditions on the whole of ∂Ω. The same is true if we convert to
the frequency domain and use a complex elliptic equation to describe the propagation
of the Fourier Transform of Φ. Instead we specify their linear combination through the
Robin condition (> .). Then for any specified value q on ∂Ω we will get data y given
by (> .). The linear mapping Λq →y is termed the Robin to Neumann map and can be
considered the result of a boundary derivative operator B acting on the Green’s operator
with kernel G∂Ω
ΛRtN(κ, μa)q = BG∂Ωq.
Since the Neumann data and Dirichlet data are related by (> .) we may also define the
Robin to Dirichlet map ΛRtD(κ, μa) and specify the relationship
ΛRtD(κ, μa) −ζΛRtN(κ, μa) −I = 
(.)
..
The Forward Problem
The Robin to Neumann map is a linear operator mapping boundary sources to boundary
data. For the inverse problem we have to consider a nonlinear mapping from the space of
μa, κ coefficients to the boundary data.
When considering an incoming flux J−with corresponding boundary term q, the data
is a function of one variable
yq = Fq (μa
κ ),
(.)

Optical Imaging 

which gives the boundary data for the particular source term q = D−(J−). Using this nota-
tion we consider the forward mapping for a finite number of sources {q j; j = . . . S} as a
parallel set of projections
y = F (μa
κ ),
(.)
where
F := (F, . . . ,FS)T
(.)
y := (y, . . . , yS)T .
(.)
We will consider (> .) as a mapping from two continuous functions in solution
space μa, κ ∈X(Ω)×X(Ω) to continuous functions in data space y ∈Y(∂Ω). If the data is
sampled as well (which is the case in practice), then F is sampled at a set of measurement
positions {rdi; i = , . . . M}.
The inverse problem of diffusion-based optical tomography (DOT) is to determine
κ, μa from the values of y for all incoming boundary distributions q. If κ, μa are found
we can determine μ′
s through (> .).
..
Schrödinger Form
Problem (> .) can be put into Schrödinger form using the Liouville transformation. We
make the change of variables U = κ/Φ, by which (> .) becomes
−κ∇Φ −κ/∇κ/⋅∇Φ + (μa + iω
c ) Φ = q.
Using
∇U = κ/∇Φ + ∇Φ ⋅∇κ/+ Φ∇κ/
leads to
−∇U(r; ω) + k(r; ω)U(r; ω) = q(r; ω)
κ/(r)
(.)
r ∈Ω/∂Ω
(.)
U(rd; ω) + ζκ(rd)∂U(rd; ω)
∂
= κ/(rd)q(rd; ω)
(.)
rd ∈∂Ω,
(.)
where
k= ∇κ/
κ/
+ μa
κ + iω
cκ.
If kis real (i.e., ω = ), there exist infinitely many κ, μa pairs with the same real k, so
that the measurement of DC data cannot allow the separable unique reconstruction of κ
and μa []. For ω ≠the unique determination of a complex kshould be possible by
extension of the uniqueness theorem of Sylvester and Uhlmann []. From the complex k


Optical Imaging
it is in principle possible to obtain separable reconstruction of first κ from the imaginary
part of kand μa from the real part; see [] for further discussion.
In a homogeneous medium, with constant optical parameters μa, κ, we can sim-
plify (> .) to
−∇Φ(r; ω) + kΦ(r; ω) = qH(r; ω),
(.)
with the same boundary condition (> .) and with
k= ( μac + iω
cκ
);
qH = q(r; ω)
κ
.
(.)
This equation is also seen directly from (> .) for constant κ.
The solution in simple geometries is easily derived using the appropriate Green’s
functions []. In an infinite medium this is simply a spherical wave
Φ(r; ω) ≡G(r,rs; ω) = e±k∣r−rs∣
∣r −rs∣,
(.)
where the notation G(r,rs; ω) defines the Green’s function for a source at position rs. Due
to the real part of the wave number k this wave is damped. This fact is the main reason that
results from diffraction tomography are not always straightforwardly applicable in optical
tomography. In particular, for the case ω = , the wave is wholly non-propagating. Even as
ω →∞the imaginary part of the wave number never exceeds the real part. This is a simple
consequence of the parabolic nature of the diffusion approximation. Although hyperbolic
approximations can be made too, they do not ameliorate the situation.
..
Perturbation Analysis
An important tool in scattering problems in general is the approximation of the change
in field due to a change in state, developed in a series based on known functions for the
reference state. There are two common approaches which we now discuss.
...
Born Approximation
For the Born approximation, we assume that we have a reference state x= (μa, κ)T, with
a corresponding wave Φ, and that we want to find the scattered wave Φδ due to a change
in state xδ = (α, β)T. We have
κ = κ + β ,
μa = μa + α .
(.)
Note that it is not necessary to assume that the initial state is homogeneous.
Putting (> .) into (> .) gives
−∇⋅(κ + β)∇˜Φ(r; ω) + (μa + α + iω
c ) ˜Φ(r; ω) = q(r; ω)
(.)

Optical Imaging 

with
˜Φ = Φ + Φδ.
(.)
> Equation .can be solved using the Green’s operator for the reference state
˜Φ = G[q+ ∇⋅β∇˜Φ −α ˜Φ].
(.)
With Gthe Green’s function for the reference state, we have
˜Φ(r; ω) = Φ(r; ω) + ∫Ω (G(r,r′; ω)∇r′ ⋅β(r′)∇r′ ˜Φ(r′; ω) −α(r′) ˜Φ(r′; ω)) dnr′
= Φ(r; ω) −∫Ω (β(r′)∇r′G(r,r′; ω) ⋅∇r′ ˜Φ(r′; ω)
α(r′)G(r,r′; ω) ˜Φ(r′; ω)),
(.)
where we used the divergence theorem and assumed β(rd) = ;rd ∈∂Ω.
If we define a “potential” as the differential operator
V(α, β):= ∇⋅β∇−α,
(.)
we can recognize (> .) as a Dyson equation and write it in the form
[I −GV] ˜Φ = Gq.
(.)
This may by formally solved by a Neumann series,
G
[I −GV] = G+ GVG+ GVGVG+ ⋯
(.)
or equivalently, by using (> .) in (> .) to obtain the Born series
˜Φ = Φ() + Φ() + Φ() + ⋯,
(.)
where
Φ() = Φ
Φ() = GVΦ
Φ() = GVGVΦ
⋮
...
Rytov Approximation
The Rytov approximation is derived by considering the logarithm of the field as a complex
phase [, ]:
Φ(r; ω) = eu(r;ω)
(.)
so that, in place of (> .) we have
ln ˜Φ = ln Φ + uδ.
(.)


Optical Imaging
Putting Φ = euinto (> .) we get
−Φ∇⋅κ∇u−Φκ ∣∇u∣+ ˜Φ (μa + iω
c ) = q.
(.)
Putting Φ = Φeuδ and (> .) into (> .) we get
−˜Φ∇⋅(κ + β)∇(u+ uδ) −˜Φ(κ + β)∣∇(u+ uδ)∣
+ ˜Φ (μa + α + iω
c ) = q.
(.)
Subtracting (> .) from (> .) and assuming ˜Φ = Φ over the support of qwe get
−κ (∇u⋅∇uδ + ∣∇uδ∣
) −∇⋅κ∇uδ = ∇⋅β∇(u+ uδ) + β ∣∇(u+ uδ)∣
−α.
(.)
We now make use of the relation
∇⋅κ∇uδΦ = Φ∇⋅κ∇uδ + κ∇Φ ⋅∇uδ + uδ∇⋅κ∇Φ
(.)
= Φ (∇⋅κ∇uδ + Φκ∇u⋅∇uδ) + uδ∇⋅κ∇Φ.
(.)
The last term on the right is substituted from (> .) to give
∇⋅κ∇uδ + κ∇u⋅∇uδ = ∇⋅κ∇uδΦ
Φ
+ uδ (μa + iω
c ) −q
Φ .
(.)
Substituting (> .) into (> .) and using
Φ∇⋅β∇u+ Φβ ∣∇u∣= ∇⋅β∇(Φu)
we arrive at
−∇⋅κ∇uδΦ + (μa + iω
c )uδΦ = ∇⋅β∇Φ −αΦ + Φ∇⋅β∇uδ + κ ∣∇uδ∣
. (.)
The approximation comes in neglecting the last two terms on the right, which are second
order in the small perturbation. The left-hand side is the unperturbed operator and so the
formal solution for uδΦ is again obtained through the Green’s operator with kernel G.
Thus, the first-order Rytov approximation becomes
uδ(r; ω) = Φ()(r; ω)
Φ(r; ω)
(.)
=
−
Φ(r; ω)(β(r′)∇r′G(r,r′; ω) ⋅∇r′Φ(r′; ω)
α(r′)G(r,r′; ω)Φ(r′; ω)).
(.)

Optical Imaging 

⊡Fig. -
Top: absorption and scattering images used to generate complex ﬁelds. Disk diameter
mm, absorption range µa ∈[.–.]mm−, scatter range µ′
s ∈[–]mm−. The complex
ﬁeld Φ was calculated using a D FEM for a δ-function source on the boundary at the
o’clock position. A reference ﬁeld Φ was calculated for the same source and a
homogeneous disk with µa = .mm−, µ′
s = mm−. Bottom: the diﬀerence in ﬁelds Φ −Φ
(real and imaginary) and the diﬀerence of logs ln Φ −ln Φ (real and imaginary)
The Rytov approximation is usually argued to be applicable for larger perturbations
than the Born approximation, since the neglected terms are small as long as the gradient
of the field is slowly varying. See [] for a much more detailed discussion.
Illustrations of the scattered field in the Born and Rytov formulations are shown in
> Fig. -. Since in the frequency domain the field is complex, so is its logarithm. The
real part corresponds to the log of the field amplitude and its imaginary part to the phase.
From the images in > Fig. -it is apparent that perturbations are more readily detected in
amplitude and phase than in the field itself. This stems from the very high dynamic range
of data acquired in optical tomography which in turn stems from the high attenuation and
attendant damping. It is the primary motivation for the use of the Rytov approximation,
despite the added complications.
..
Linearization
Linearization is required either to formulate a linear reconstruction problem (i.e., assuming
small perturbations on a known background) or as a step in an iterative approach to the
nonlinear inverse problem. We will formulate this in the frequency domain. In addition,
we may work with either the wave itself, which leads to the Born approximation or its
logarithm, which leads to the Rytov approximation.


Optical Imaging
...
Linear Approximations
In the Born approximation to the linearized problem, we assume that the difference in
measured data is given just by the first term in the Born series (> .)
Φδ(r; ω) ≡Φ()(r; ω)
(.)
= −∫Ω (β(r′)∇r′GΩ,(r,r′; ω) ⋅∇r′Φ(r′; ω)α(r′)GΩ,(r,r′; ω)Φ(r′; ω)) dnr′.
(.)
From (> .) we obtain for a detector at position rd ∈∂Ω
y(rd; ω) = y(rd; ω) + ∫Ω KT
q (rd,r′; ω) (α(r′)
β(r′)) dnr′,
(.)
where Kq is given by
Kq(rd,r′; ω) = (
GB
Ω,(rd,r′; ω)Φ(r′; ω)
∇r′GB
Ω,(rd,r′; ω) ⋅∇r′Φ(r′; ω)).
(.)
The subscript q refers to the incoming flux that generates the boundary condition for the
particular field Φ.
Since the Rytov approximation was derived by considering the change in the logarithm
of the field, we have in place of (> .) simply
KRyt
q (rd,r′; ω) =

y(rd; ω)Kq(rd,r′; ω).
(.)
Assuming that we are given measured data g for a sufficient number of input fluxes the
linearized problem consists in solving for α, β from
yδ = Kq (α
β),
(.)
where Kq is a linear operator with kernel given by (> .) or (> .) and
yδ = g −y,
(.)
where yis the data that would arise from state x.
We can now distinguish between a linearized approach to the static determination of
x = x+ (α, β)T and a dynamic imaging problem that assumes a reference measurement
g. In the former case we assume that our model is sufficiently accurate to calculate y. In
the latter case we use the reference measurement to solve
yδ = g −g.
(.)
This in fact is where the majority of reported results with measured data are taken. By this
mechanism inconsistencies in the modeling of the forward problem (most notably using
D instead of D) are minimized. However, for static or “absolute” imaging, we still require
an accurate model, even for the linearized problem.

Optical Imaging 

...
Sensitivity Functions
If we take q to be a δ-function at a source position rs ∈∂Ω, then Φ is given by a Green’s
function too, and we obtain the Photon Measurement Density Function (PMDF)
ρ(rd,r′,rs; ω) = (
GB
Ω,(rd,r′; ω)G∂Ω,(r′,rs; ω)
∇r′GB
Ω,(rd,r′; ω) ⋅∇r′G∂Ω,(r′,rs; ω))
(.)
with the Rytov form being
ρRyt(rd,r′,rs; ω) =

GB
∂Ω,(rd,rs; ω)ρ(rd,r′,rs; ω).
It is instructive to visualize the various ρ-functions which exhibit notable differences for
μa and κ and between the Born and Rytov functions, as seen in > Fig. -.
Clearly
Kq = ∫∂Ω ρ(rd,r′,rs; ω)q(rs)drs = G′q,
where G′ is a linear operator with kernel ρ.
We can also define the linearized Robin to Neumann map
Λ′
RtN(μa, κ)(α
β) = ∫∂Ω H(rd,rs; ω)q(rs)drs = Hq,
where H is a linear operator with kernel H given by
H(rd,rs) = ∫Ω ρT (rd,r′,rs; ω)(α(r′)
β(r′)) dnr.
Note that there are no equivalent Rytov forms. This is because the log of the Robin to
Neumann map is not linear.
⊡Fig. -
Top row: sensitivity function ρ for µa; left to right: real, imaginary, amplitude, and phase;
bottom row: the same functions for κ


Optical Imaging
..
Adjoint Field Method
A key component in the development of a reconstruction algorithm is the use of the adjoint
operator. The application of these methods in optical tomography has been discussed in
detail by Natterer and coworkers [, ].
Taking the adjoint of Kq defines a mapping Y(∂Ω) →X(Ω) × X(Ω)
K ∗
q b = ∫∂Ω Kq(rd,r′; ω)b(rd; ω)dS
(.)
= ∫∂Ω
⎛
⎝
G
B
Ω,(rd,r′; ω)Φ(r′; ω)
∇r′G
B
Ω,(rd,r′; ω) ⋅∇r′Φ(r′; ω)
⎞
⎠b(rd; ω)dS.
(.)
Consider the reciprocity relation
G
B
Ω,(rd,r; ω) = −G ∗
∂Ω,(r,rd; ω)
(.)
with G ∗
∂Ω,the Green’s function that solves the adjoint problem
−∇⋅κ(r)∇G ∗
∂Ω,(r,rd; ω) + (μa(r) −iω
c ) G ∗
∂Ω,(r,rd; ω) = 
(.)
r ∈Ω/∂Ω
G ∗
∂Ω,(r,rd; ω) + ζκ(rd)∂G ∗
∂Ω,(r,rd; ω)
∂
= δ(rd; ω)
(.)
rd ∈∂Ω.
Now we can define a function Ψ by applying the adjoint Green’s operator to the function
b ∈Y(∂Ω) to give
Ψ(r; ω) = −∫∂Ω G
B
Ω,(rd,r; ω)b(rd; ω)dS
(.)
= ∫∂Ω G ∗
∂Ω,(r,rd; ω)b(rd; ω)dS.
(.)
By using (> .) we have that Ψ solves
−∇⋅κ(r)∇Ψ(r; ω) + (μa(r) −iω
c ) Ψ(r; ω) = 
r ∈Ω/∂Ω
(.)
Ψ(rd; ω) + ζκ(rd)∂Ψ(rd; ω)
∂
= b(rd; ω)
rd ∈∂Ω
(.)
and therefore K ∗
q is given by
K ∗
q b = (
−ΦΨ
−∇Φ ⋅∇Ψ).
(.)
Finally, we have an adjoint form for the PMDF (> .)
ρ(rd,r′,rs; ω) = (
−G
∗
∂Ω,(r′,rd; ω)G∂Ω,(r′,rs; ω)
−∇r′ G
∗
∂Ω,(r′,rd; ω) ⋅∇r′G∂Ω,(r′,rs; ω))
(.)

Optical Imaging 

...
Time Domain Case
In the time domain, we form the correlation of the propagated wave and the back
propagated residual. > Equation .becomes
K ∗
q b = ⎛
⎝
∫
T
−Φ(t)Ψ(t)dt
∫
T
−∇Φ(t) ⋅∇Ψ(t)dt
⎞
⎠,
(.)
where Ψ(t) is the solution to the adjoint equation
(−
c
∂
∂t −∇⋅κ(r)∇+ μa(r)) Ψ(r, t) = 
r ∈Ω/∂Ω, t ∈[, T]
(.)
Ψ(r, T) = , r ∈Ω
(.)
Ψ(rd, t) + ζκ(rd)∂Ψ(rd, t)
∂
= b(rd, t)
rd ∈∂Ω, t ∈[, T].
(.)
This is much more expensive, although it allows to apply temporal domain filters to
optimize the effect of “early light” (that light that has undergone relatively few scattering
events). In > Fig. -are shown the sensitivity functions over a sequence of time intervals.
Notice that the functions are more concentrated along the direct line of propagation for
early times and become more spread out for later times.
..
Light Propagation and Its Probabilistic
Interpretation
In time-domain systems the source is a pulse in time which we express as a δ-function
q(rd, t) = q(rd)δ(t),
(.)
where q(rd) is the source distribution on ∂Ω. Furthermore, if the input light fiber is small,
the spatial distribution can be considered a δ-function too, located at a source position
rs j ∈∂Ω
q(rd, t) = δ(rd −rs j)δ(t).
(.)
For this model, the measured signal y(rd, t) is the impulse response (Green’s function) of
the system, restricted to the boundary. When measured at a detector rdi ∈∂Ω it is found to
be a unimodal positive function of t with exponential decay to zero as t →∞. Some exam-
ples are shown in > Fig. -, showing measured data from the system described in []
together with modeled data using a D finite element method. The function can be inter-


Optical Imaging
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
20
40
60
10
20
30
40
50
60
⊡Fig. -
Top row: time-window sensitivity function ρ for µa – left to right: time gates –,ps,
,–,ps, ,–,ps, ,–,ps. Bottom row: the same functions for κ
preted as a conditional probability density of the time of arrival of a photon, given the
location of its arrival.
Consider the Green’s functions for (> .) in infinite space:
G(r,r′, t, t′) = e−μa t−
∣r−r′∣
κ(t−t′)
(πκt)/
t > t′.
(.)
> Equation (.) has the form of the Probability Density Function (PDF) for a lossy ran-
dom walk; for a fixed point in time, the distribution is spatially a Gaussian; for a fixed
point in space, the distribution in time shows a sharp rise followed by an asymptotically
exponential decay. In the probabilistic interpretation we assume
G(r, t,r′, t′)
∫G(r, t,r′, t′)dt ≡Pr′,t′(t∣r)
(.)
as a conditional PDF in the sense that a photon that has arrived at a given point does so
in time interval [t, t + δt] with probability Pr′,t(t∣r)δt. Furthermore, the absolute PDF for
detecting a photon at point r at time t is given by
G(r, t,r′, t′) ≡Pr′,t′(r, t) = Pr′(r)Pr′,t′(t∣r).

Optical Imaging 

0.0
2,000.0
4,000.0
6,000.0
8,000.0
Time
0.0
0.2
0.4
0.6
0.8
1.0
Intensity (a.u.)
⊡Fig. -
Example of temporal response functions for diﬀerent source detector spacings. Circles
represent measured data and dashed lines are the modeled data using a D ﬁnite element
method. Each curve is normalized to a maximum of (Data courtesy of E. Hillman and
J. Hebden, University College London)
Here, Pr′(r) is interpreted as the relative intensity I(r)/Iof the detected number of
photons relative to the input number.
For most PDFs based on physical phenomena there exists a Moment Generating
Function (MGF)
M(s) = E[P(t)est],
(.)
where E[.] is the expectation operator, whence the moments (around zero) are deter-
mined by
mn = ∂nM(s)
∂sn
∣
s=
(.)
and in principle the PDF P(t) can be reconstructed via a Taylor series for its MGF
M(s) = m+ ms + ⋯mn
sn
n!+
(.)
However, explicit evaluation of this series is impractical. Furthermore, we may assume that
only a small number of independent moments exist, in which case we reconstruct the series


Optical Imaging
implicitly given only the first few moments. In the results presented here, only the first three
moments m, m, and mare used. They have the physical interpretations
m
Total intensity
I(r)
m
m
Mean time
< t > (r)
m
m
−( m
m
)

Variance time
σ 
t (r).
In order to test the validity of the moment method to construct the time-varying
solution we created a finite element model of a × ×cm slab. The optical parame-
ters were set to an arbitrary heterogeneous distribution by adding a number of Gaussian
blobs of different amplitude and spatial width in both μa and κ to a background of
μa = .cm−, κ =.cm. A source was placed at the center of one face of the slab and nine
detectors placed in a rectangular array on the opposite face of the slab. The time-of-flight
histogram of transmitted photons at each detector was calculated in two ways: () by solv-
ing the time dependent system (> .) using a fully implicit finite differencing step in time
() by solving for the moments m, m, musing (> .) and deriving the time-varying
solution via (> .).
One case is shown in > Fig. -. The comparison is virtually perfect despite the grossly
heterogeneous nature of the example which precludes the exact specification of a Green’s
function. The moment-based method is several hundred times faster.
0.012
0.01
0.008
0.006
0.004
0.002
0
0
500
1,000
1,500
2,000
2,500
3,000
⊡Fig. -
An example of the output time-of-ﬂight histograms at each of nine detectors on the
transmission surface of a slab. Solid curves are computed from ( > .) using the zeroth,
ﬁrst, and second moments and implicit extrapolation; crosses are computed using
ﬁnite-diﬀerencing in time of the system ( > .) and steps of size ps

Optical Imaging 

.
Numerical Methods and Case Examples
..
Image Reconstruction in Optical Tomography
Optical tomography is generally recognized as a nonlinear inverse problem; linear methods
can certainly be applied (e.g., []) but are limited to the case of small perturbations on a
known background.
The following is an overview of the general approach: we construct a physically accu-
rate model that describes the progress of photons from the source optodes through the
media and to the detector optodes. This is termed the forward problem (> Sect. ..). This
model is parameterized by the spatial distribution of scattering and absorption properties
in the media. We adjust these properties iteratively until the predicted measurements from
the forward problem match the physical measurements from the device. This is termed the
inverse problem.
The model-based approach is predicated on the implicit assumption that there exists
in principle an “exact model” given by the physical description of the problem and the
task is to develop a computational technique that matches measured data within an
accuracy below the expected level of measurement error. In other words, we assume
that model inaccuracies are insignificant with respect to experimental errors. How-
ever, the computational effort in constructing a forward model of sufficient accuracy
can be prohibitive at best. In addition, the physical model may have shortcomings
that lead to the data being outside the range of the forward model (nonexistence of
solution).
In the approximation error method [] we abandon the need to produce an “exact
model.” Instead, we attempt to determine the statistical properties of the modeling
errors and compensate for them by incorporating them into the image reconstruction
using a Bayesian approach (> Sect. ...). The steps involved in using the approx-
imation error theory are () the construction and sampling of a prior, () the con-
struction of a mapping between coarse and fine models, () calculation of forward
data from the samples on both coarse and fine models, () statistical estimation of
the mean and covariance of the differences between data from the coarse and fine
models. In [] this technique was shown to result in reconstructed images using a
relatively inaccurate forward model that were of almost equal quality to those using a
more accurate forward model; the increase in computational efficiency was an order
of magnitude.
Reconstruction from optical measurements is difficult because it is fundamentally ill
posed. We usually aim to reconstruct a larger number of voxels than there are measure-
ments (which results in nonuniqueness), and the presence of noisy measurements can result
in an exponential growth in the image artifacts. In order to stabilize the reconstruction,
regularization is required. Whereas such regularization is often based on ad hoc consider-
ations for improving image quality, the Bayesian approach provides a rigorous framework
in which the reconstructed images are chosen to belong to a distribution with principled
characteristics (the prior).


Optical Imaging
..
Bayesian Framework for Inverse Optical Tomography
Problem
In the Bayesian framework for inverse problems, all unknowns are treated and modeled as
random variables []. The measurements are often considered as random variables also in
non-Bayesian framework for inverse problems. The actual modeling of the measurements
as random variables is, however, often implicit, which is most manifest when least square
functionals are involved in the formulation of the problem. In the Bayesian framework,
however, both the measurements and the unknowns are explicitly modeled as random vari-
ables. The construction of the likelihood (observation) models and the prior models is the
starting point of the Bayesian approach to (inverse) problems.
Once the probabilistic models for the unknowns and the measurement process have
been constructed, the posterior distribution π(x ∣y) is formed, which distribution reflects
the uncertainty of the interesting unknowns x given the measurements y. This distribu-
tion can then be explored to answer all questions which can be expressed in terms of
probabilities. For general discussion of Bayesian inference, see, for example, [].
Bayesian inverse problems are a special class of problems in Bayesian inference. Usually,
the dimension of a feasible representation of the unknowns is significantly larger than the
number of measurements, and thus, for example, a maximum likelihood estimate is either
impossible or extremely unstable to compute. In addition to the instability, the variances
of the likelihood model are almost invariably much smaller than the variances of the prior
models. The posterior distribution is often extremely narrow and, in addition, may be a
nonlinear manifold.
...
Bayesian Formulation for the Inverse Problem
In the following, we denote the unknowns with the vector x, the measurements with y, and
all probability distributions (densities) by π. Typically, we would have x = (μa, μs), with
μa and μs identified with the coordinates in the used representations.
The complete statistical information of all the random variables is given by the joint
distribution π(x, y). This distribution expresses all the uncertainty of the random vari-
ables. Once the measurements y have been obtained, the uncertainty in the unknowns x is
(usually) reduced. The measurements are now reduced from random variables to numbers
and the uncertainty of x is expressed as the posterior distribution π(x ∣y). This distribu-
tion contains all information on the uncertainty of the unknowns x when the information
on measurements y is utilized.
The conditional distribution of the measurements given the unknown is called the like-
lihood distribution and is denoted by π(y ∣x). The marginal distribution of the unknown
is called the prior (distribution) and is denoted by π(x). By the definition of conditional
probability we have
π(x, y) = π(y ∣x)π(x) = π(x ∣y)π(y).
(.)

Optical Imaging 

Furthermore, the marginal distributions can be obtained by marginalizing (integrating)
over the remaining variables, that is, π(x) = ∫π(x, y) dy and π(y) = ∫π(x, y) dx. The
following rearrangement is called Bayes’ theorem
π(x ∣y) = π(y)−π(y ∣x)π(x).
(.)
If we were given the joint distribution, we could simply use the above definitions to
compute the posterior distribution. Unfortunately, the joint distribution is practically never
available in the first place. However, it turns out that in many cases the derivation of the
likelihood density is a straightforward task. Also, a feasible probabilistic model for the
unknown can often be obtained. Then one can use Bayes’ theorem to obtain the poste-
rior distribution. The demarcation between the Bayesian and frequentist paradigms is that,
here the posterior is obtained by using a (prior) model for the distribution of the unknown
rather than the marginal density, which cannot be computed since the joint distribution
is not available in the first place. We stress that all distributions have to be interpreted as
models.
...
Inference
Point estimates are the Bayesian counterpart of the “solutions” suggested by regularization
methods. The most common point estimates are the maximum a posteriori estimate (MAP)
and the conditional mean estimate (CM). Let the unknowns and measurements be the finite
dimensional random vectors x ∈RN, y ∈RM.
The computation of the MAP estimate is an optimization problem while the computa-
tion of the CM estimate is an integration problem:
xMAP = sol max
x
π(x ∣y)
(.)
xCM = E(x ∣y) = ∫x π(x ∣y) dx,
(.)
where sol reads as “solution of” the maximization problem, E(⋅) denotes expectation, and
the integral in (> .) is an N-tuple integral.
The most common estimate of spread is the conditional covariance
Γx∣y = ∫(x −E(x ∣y))(x −E(x ∣y))T π(x ∣y) dx.
(.)
Here, Γx ∣y is an N × N matrix and the integral (> .) refers to a matrix of associated
integrals.
Often, the marginal distributions of single variables are also of interest. These are
formally obtained by integrating over all other variables
π(xℓ∣y) = ∫
x−ℓ
π(x ∣y) dx−ℓ,
(.)


Optical Imaging
where the notation (⋅)−ℓrefers to all components excluding the ℓth component. Note that
π(xℓ∣y) is a function of a single variable, and can be visualized by plotting. The credibil-
ity intervals are the Bayesian counterpart to the frequentist confidence intervals, but the
interpretation is different. The p%-credibility interval is a subset which contains p% of the
probability mass of the posterior distribution.
...
Likelihood and Prior Models
The likelihood model π(y ∣x) consists of modeling the forward problems and the related
observational errors. In the likelihood model, all unknowns are treated as fixed. The most
common likelihood model is based on the additive error model
y = F(x) + e,
where e is the additive error term with distribution πe(e) which is usually modeled as
mutually independent with x. In this case, we can get rid of the unknown additive error
term by pre-marginalizing over it. We have formally π(y ∣x, e) = δ(y −F(x) + e) and
using the Bayes theorem
π(y ∣x) = πe(y −F(x)).
For more general derivation and other likelihood models, see, for example, [].
In the special case of Gaussian additive errors πe(e) = N(e∗, Γe), we get
π(y ∣x) ∝exp (−
∥Le(y −F(x) −e∗)∥),
where Γ−
e
= LT
e Le. In the very special case of πe(e) = N(,γI) we of course get the
ordinary least squares functional for the posterior potential. This particular model, how-
ever, should always be subjected to assessment since it usually corresponds to an idealized
measurement system.
For prior models π(x) for the unknowns whose physical interpretation is a distributed
parameter, Markov random fields are a common choice. The most common type is an
improper prior model of the form
π(x) ∝exp (−
∥Lx(x −x∗)∥),
(.)
where Lx is derived from a spatial differential operator. For example, ∥Lx(x −x∗)∥might
be a discrete approximation for ∫Ω ∣Δx(⃗r)∣d⃗r. Such improper prior models may work
well technically since the null space of Lx is usually such that it is annihilated in the poste-
rior model. It must be noted, however, that there are constructions that yield proper prior
models [, , ]. These are needed, for example, for the construction of approxima-
tion error models discussed in > Sect. .... Moreover, structural information related to
inhomogeneities and anisotropicity of smoothness can be decoded in these models [].

Optical Imaging 

...
Nonstationary Problems
Inverse problems in which the unknowns are time varying are referred to as nonstationary
inverse problems []. These problems are also naturally cast in the Bayesian framework.
Nonstationary inverse problems are usually written as evolution-observation models in
which the evolution of the unknown is typically modeled as a stochastic process. The
related algorithms are sequential and in the most general form are of the Markov chain
Monte Carlo type []. However, the most commonly used algorithms are based on the
Kalman recursions [, , ].
A suitable statistical framework for dealing with unknowns that are modeled with
stochastic processes and which are observed either directly or indirectly is the state esti-
mation framework. In this formalism, the unknown is referred to as the state variable, or
simply the state. For treatises on state estimation and Kalman filtering theory in general,
see for example [, ]. For the general nonlinear non-Gaussian treatment, see [], and
state estimation with inverse problems, see [].
The general discrete time state space representation of a dynamical system is of the form
xk+= Fk(xk,wk)
(.)
yk = Ak(xk,vk),
(.)
where wk is the state noise process and vk is the observation noise process, and (> .)
and (> .) are the evolution model and observation model, respectively. Here, the evolu-
tion model replaces the prior model in stationary inverse problems, while the observation
model is usually the same as the (stationary) likelihood model. We do not state the exact
assumptions here, since the assumptions may vary somewhat resulting in different varia-
tions of Kalman recursions, see for example []. It suffices here to state that the sequences
of mappings Ft and At are assumed to be known and that the state and observation
noise processes are temporally uncorrelated and that their (second order, possibly time-
varying) statistics are known. With these assumptions, the state process is a first-order
Markov process. The first-order Markov property facilitates recursive algorithms for the
state estimation problem. The Kalman recursions were first derived in [].
Formally, the state estimation problem is to compute the distribution of a state variable
xk ∈RN given a set of observations y j ∈RM, j ∈I where I is a set of time indices. In
particular, the aim is to compute the related conditional means and covariances. Usually,
I is a contiguous set of indices and we denote Yℓ= (y, . . . , yℓ).
We can then state the following common state estimation problems:
•
Prediction. Compute the conditional distribution of xk given Yℓ, k > ℓ.
•
Filtering. Compute the conditional distribution of xk given Yℓ, k = ℓ.
•
Smoothing. Compute the conditional distribution of xk given Yℓ, k < ℓ.
The solution of the state estimation problems in linear Gaussian cases is usually carried
out by employing the Kalman filtering or smoothing algorithms that are based on Kalman


Optical Imaging
filtering. These are recursive algorithms and may be either real-time, online, or batch-type
algorithms.
In nonlinear and/or non-Gaussian cases, extended Kalman filtering (EKF) variants are
usually employed. The EKF algorithms form a family of estimators that do not possess
any optimality properties. For many problems, however, the EKF algorithms provide fea-
sible state estimates. For EKF algorithms, see for example [, ]. Since the observation
models with optical tomography are nonlinear, the EKF algorithms are a natural choice for
nonstationary DOT problems, see [, , ].
The idea in extended Kalman filters is straightforward: the nonlinear mappings are
approximated with the affine mappings given by the first two terms of the Taylor expansion.
The version of extended Kalman filter that is most commonly used is the local linearization
version, in which version the mappings are linearized at the best currently available state
estimates, either the predicted or the filtered state. This necessitates the recomputation of
the Jacobians ∂At/∂xt at each time instant.
The EKF recursions take the form
xk∣k−= Fk−(xk−∣k−) + sk−+ Bk−(uk−)
(.)
Γk∣k−= JFk−Γk−∣k−JT
Fk−+ Γwk−
(.)
Kk = Γk∣k−JT
Ak (JAk Γk∣k−JT
Ak + Γvk)
−
(.)
Γk∣k = (I −KkJAk) Γk∣k−
(.)
xk∣k = xk∣k−+ Kk (yk −Ak(xk∣k−)),
(.)
where xk∣k−and xk∣k are the prediction and filtering estimates, respectively, and Γk∣k−and
Γk∣k are the approximate prediction and filtering covariances, respectively. Note that the
Jacobian mappings (linearizations) are needed only in the computation of the covariances
and the Kalman gain Kt.
The applications of EKF algorithms to nonstationary DOT problems have been con-
sidered in [, , ]. In [], a random walk evolution model was constructed and used
for tracking of targets in a cylindrical tank geometry. In [], a cortical mapping problem
was considered, in which the evolution model was augmented to include auxiliary periodic
processes to allow for separation of cyclical phenomena from evoked responses. In [],
an elaborate physiological model was added to that of [] to form the evolution model.
...
Approximation Error Approach
The approximation error approach was introduced in [, ] originally to handle pure
model reduction errors. For example, in electrical impedance (resistance) tomography
(EIT, ERT) and deconvolution problems, it was shown that significant model reduction is
possible without essentially sacrificing the quality of estimates. With model reduction we
mean that very low dimensional finite element approximations can be used for the forward
problem. The approximation error approach relies heavily on the Bayesian framework of

Optical Imaging 

inverse problems, since the approximation and modeling errors are modeled as additive
errors over the prior model.
In this following, we discuss the approximation error approach in a setting in which
one distributed parameter is of interest, while another one is not, and there are additional
uncertainties that are related, for example to unknown boundary data. In addition, we
formulate the problem to take into account model reduction errors. In the case of optical
tomography, this would mean using very approximate forward solvers, for example.
Let now the unknowns be (μa, μs, ξ, e), where e represents additive errors and ξ
represents auxiliary uncertainties such as unknown boundary data, and μa is of interest
only. Let
y = ¯A(μa, μs, ξ) + e ∈Rm
denote an accurate model for the relation between the measurements and the unknowns.
In the approximation error approach, we proceed as follows. Instead of using the accu-
rate forward model (μa, μs, ξ) ↦¯A(μa, μs, ξ) with (μa, μs, ξ) as the unknowns, we fix
the random variables (μs, ξ) ←(μs,, ξ) and use a computationally (possibly drastically
reduced) approximative model
μa ↦A(μa, μs,, ξ).
Thus, we write the measurement model in the form
y = ¯A(μa, μs, ξ) + e
(.)
= A(μa, μs,, ξ) + ( ¯A(μa, μs, ξ) −A(μa, μs,, ξ)) + e
(.)
= A(μa, μs,, ξ) + ε + e,
(.)
where we define the approximation error ε = φ(μa, μs, ξ) = ¯A(μa, μs, ξ) −A(μa, μs,, ξ).
Thus, the approximation error is the discrepancy of predictions of the measurements (given
the unknowns) when using the accurate model ¯A(μa, μs, ξ) and the approximate model
A(μa, μs,, ξ).
Using the Bayes’ formula repeatedly, it can be shown that
π(y ∣x) = ∫πe(y −A(x, μs,, ξ) −ε)πε∣x(ε ∣x) dε
(.)
since e and x are mutually independent. Note that (> .) and (> .) are exact.
In the approximation error approach, the following Gaussian approximations are used:
πe ≈N(e∗, Γe) and πε∣x ≈N(ε∗,μa, Γε∣μa). Let the normal approximation for the joint
density π(ε, μa) be
π(ε, μa) ∝exp
⎧⎪⎪⎨⎪⎪⎩
−
( ε −ε∗
μa −μa,∗
)
T
( Γεε
Γεμa
Γμaε
Γμaμa
)
−
( ε −ε∗
μa −μa,∗
)
⎫⎪⎪⎬⎪⎪⎭
(.)
whence
ε∗,μa = ε∗+ Γεμa Γ−
μa μa(μa −μa,∗)
(.)
Γε∣μa = Γεε −Γεμa Γ−
μaμa Γμaε.
(.)


Optical Imaging
Define the normal random variable
= e + ε so that
∣μa ∼N( ∗∣μa, Γ ∣μa). Thus, we
obtain for the approximate likelihood distribution
π(y ∣μa) ≈N(y −A(μa, μs,, ξ) −
∗∣μa, Γ ∣μa).
Since we are after computational efficiency, a normal approximation π(μa) ≈N(μa,∗, Γμa)
for the prior model is also usually employed. Thus, we obtain the approximation for the
posterior distribution
π(μa ∣y) ∝π(y ∣μa)π(μa)
(.)
∝exp (−
∥L ∣μa(y −A(μa, μs,∗, ξ∗) −
∗∣μa)∥
(.)
+ ∥Lμa(μa −μa,∗)∥),
(.)
where Γ−
∣μa = LT
∣μa L ∣μa and Γ−
μaμa = LT
μa Lμa. See [] or more details on the particular
problem of marginalizing over the scattering coefficient.
The approximation error approach has been applied to various kinds of approximation
and modeling errors as well as other inverse problems. Model reduction, domain trun-
cation, and unknown anisotropy structures in diffuse optical tomography were treated in
[, , , ]. Missing boundary data in the case of image processing and geophysical
EIT were considered in [] and [], respectively. Furthermore, in [–] the problem
of recovery from simultaneous geometry errors and model reduction was found to be pos-
sible. In [], the radiative transfer model was replaced with the diffusion approximation.
It was found that also in this kind of a case, the statistical structure of the approxima-
tion errors enabled the use of a significantly less complex model, again simultaneously
with significant model reduction for the diffusion approximation. But also here, both the
absorption and scattering coefficients were estimated simultaneously.
The approximation error approach was extended to nonstationary inverse problems
in [] in which linear nonstationary (heat transfer) problems were considered, and in
[] and [] in which nonlinear problems and state space identification problems were
considered, respectively. A modification in which the approximation error statistics can
be updated with accumulating information was proposed in [] and an application to
hydrogeophysical monitoring in [].
..
Experimental Results
In this section we show an example where the error model is employed for compensating
the modeling errors caused by reduced discretization accuracy h and experimental DOT
data is used for the observations.

Optical Imaging 

...
Experiment and Measurement Parameters
The experiment was carried out with the frequency-domain (FD) DOT instrument at
Helsinki University of Technology []. The measurement domain Ω is a cylinder with
radius r = mm and height mm, see > Fig. -. The target consists of homoge-
neous material with two small cylindrical perturbations, as illustrated in > Fig. -. The
background optical properties of the phantom are approximately μa,bg = .mm−and
μs,bg = mm−at wavelength λ ≈nm. The cylindrical perturbations, which both have
ma= ma,bg
m′s= 2m′s,bg
ma,bg ≈ 0.01 mm−1
m′s,bg ≈ 1 mm−1
ma= 2ma,bg
m′s= m′s,bg
⊡Fig. -
Top: Measurement domain Ω. The dots denote the location of the sources and detectors.
The (red) lines above and below the sources and detectors denote the truncated model
domain Ω. The green line denotes the central slice of the domain Ω. Bottom: Central slice of
the target (µa left, µ′
s right)


Optical Imaging
diameter and height of .mm, are located such that the central plane of the perturbations
coincide with the central xy-plane of the cylinder domain Ω. For an illustration of the cross
sections of μa and μ′
s, see bottom row in > Fig. -. The optical properties of perturbation
are approximately μa,p= .mm−, μs,p= mm−(i.e., purely absorption contrast) and
the properties of perturbation are μa,p= .mm−, μs,p= mm−(i.e., purely scatter
contrast), respectively. The source and detector configuration in the experiment consisted
of sources and detectors arranged in interleaved order on two rings located mm
above and below the central xy-plane of the cylinder domain. The locations of sources
and detectors are shown with dots in > Fig. -. The measurements were carried out at
λ = nm with an optical power of mW and modulation frequency π f = MHz.
The log amplitude and phase shift of the transmitted light was recorded at farthermost
detector locations for each source, leading to a real-valued measurement vector
y = ( re(log(z))
im(log(z))) ∈R
for the experiment. The statistics of measurement noise in the measurement y are not
known. Thus, we employ the same implicit (ad hoc) noise model that was used for
reconstructions from the same measurement realization in []. The noise model is
e ∼N(, Γe),
where Γe is a diagonal data scaling matrix which is tuned such that the initial (weighted)
least squares (LS) residual
∥Le(y −y)∥= ,
Γ−
e = LT
e Le
between the measured data y and forward solution yat the initial guess x = xbecomes
unity for both data types (log amplitude re(log(z)) and phase im(log(z))).
...
Prior Model
In this study, we use a proper Gaussian smoothness prior as the prior model for the
unknowns. The absorption and scatter images μa and μ′
s are modeled as mutually inde-
pendent Gaussian random fields with a joint prior model
π(xδ) ∝exp {−
∥Lxδ(xδ −xδ∗)∥},
LT
xδ Lxδ = Γ−
xδ ,
(.)
where
Γxδ = (Γμa


Γμ′s
).
The construction of the blocks Γμa and Γμ′s has been explained for a two-dimensional case
in [], the extension to three-dimensional case is straightforward. The parameters in the
prior model were selected as follows. The correlation length for both μa and μ′
s in the prior
was set as mm. The correlation length can be viewed (roughly) as our prior estimate about

Optical Imaging 

0.001
0.022
0.1
2.2
0.001
0.022
0.1
2.2
⊡Fig. -
Two random samples from the prior density (.). The images display the cross section of
the D parameters at the central slice of the cylinder domain Ω. Left: Absorption µa. Right:
Scatter µ′
s
the expected spatial size of perturbations in the target domain. The prior mean for absorp-
tion and scatter were set as μa∗= .mm−and μs∗= mm−, and the marginal standard
deviations of absorption and scatter in each voxel were chosen such that σμa = .and
σμ′s = , respectively. This choice corresponds to assuming that the values of absorption
and scatter are expected to lie within the intervals μa ∈[,.] and μ′
s ∈[, ] with prior
probability of .%. > Figure -shows two random samples from the prior model.
...
Selection of FEM Meshes and Discretization Accuracy
To select the discretization accuracy δ for the accurate forward model AΩ,δ(xδ) we adopted
a similar procedure as in []. In this process, we computed relative error in the FEM solu-
tion with respect the discretization level h and identified δ as that mesh density beyond
which the relative error
∥AΩ,h −AΩ,h′∥
∥AΩ,h′∥
in both, amplitude and phase, parts of the forward solution was stabilized. The mesh for the
reference model AΩ,h′ in the convergence analysis consisted of Nn = ,node points
and Ne = ,,(approximately) uniform tetrahedral elements. We found that the
errors in the FEM solution were stabilized when using a (uniform) tetrahedral mesh with
(approximately) ,nodes or more, and thus we chose for the accurate model AΩ,δ


Optical Imaging
⊡Table -
Mesh details for test case. Nn is the number of nodes, Ne is the number of tetrahedral ele-
ments in the mesh, and np is the number of voxels in the representation of µa and µ′
s. t is the
wall clock time for one forward solution
Model
Nn
Ne
np
t (s)
AΩ,δ
,
,
,

AΩ,h
,
,
,
.
a mesh with Nn = ,node points. For the target model AΩ,h we chose a mesh with
Nn = ,nodes, see > Table -. For the representation of the unknowns (μa, μ′
s), the
domain Ω was divided into np = ,cubic voxels (i.e., number of unknowns n = ,)
in both models AΩ,δ and AΩ,h. Thus, the projector P : xδ ↦xh between the models is the
identity matrix.
...
Construction of Error Models
To construct the enhanced error model, we proceeded as in > Sect. .... The size of the
random ensemble S from the prior model π(xδ), Eq. ., was L = . > Figure -
shows central xy-slices from two realizations of absorption and scatter images from the
ensemble (the location of the slice is denoted by green line in > Fig. -). Using the ensem-
ble, Gaussian approximations ε ∼N(ε∗, Γε) for the error between the accurate model AΩ,δ
and the target models were computed.
To assess the magnitude of the modeling error, we estimate signal-to-noise (SNR) ratio
of the modeling error as
SNR = log(
∥AΩ,δ∥
∥ε∗∥+ trace(Γε)),
where AΩ,δ is the mean of the accurate model AΩ,δ over the ensemble S. The SNR is
estimated separately for the amplitude and phase part of the forward model.
Consider now the modeling error between the accurate model AΩ,δ (Nn = ,
nodes) and target model AΩ,h (Nn = ,nodes) in the first test case. In this case, the esti-
mated SNRs for the modeling error in log amplitude and phase are approximately and ,
corresponding to error levels of % and %, respectively. These error levels exceed clearly
typical levels of measurement noise in DOT measurements. Left image in > Fig. -dis-
plays the covariance matrix Γε, revealing the correlation structure of ε. Combining the high
magnitude and complicated correlation structure of the modeling error ε with the fact that
the inverse problem is sensitive to modeling errors, one can expect significant artifacts in
the reconstructions with conventional noise model when employing the target model AΩ,h.
Right image in > Fig. -shows normalized eigenvalues λ/λmax of Γε for the modeling
error between models AΩ,δ and AΩ,h in the first test case. As can be seen, the eigenvalues
are decaying rapidly and already the th eigenvalue is less than % of the maximum.

Optical Imaging 

0.4
0.3
0.2
0.1
0
–0.1
50 100 150 200 250 300 350
300
200
100
100
10–2
10–4
10–6
10–8
10–10
0
50
100
150
200
250
300
350
⊡Fig. -
Modeling error between the accurate model AΩ,δ and target model AΩ,h, see > Table -.
Left: Covariance structure of the approximation error ε. The displayed quantity is the signed
standard deviation sign(Γε) ⋅
√
∣Γε∣, where the product refers to the element-by-element
(array) multiplication. Right: Normalized eigenvalues λ/λmax of Γε. of Γε
Roughly speaking, this rapid decay of the eigenvalues can be interpreted such that the
variability in the modeling error can be well explained with a relatively small number of
principal components. In other words, one can take this as a sign that the structure of the
modeling error is not “heavily dependent” on the realization of x or the prior model π(x),
and thus the error model can be expected to perform well.
Notice that the setting up of the error model is a computationally intensive task, while
the use of the model is as with the conventional error model. The computation time for
setting up the error model is roughly equivalent to size of the ensemble times the time
for forward solution in the accurate and approximate models. However, the error model
needs to be estimated only once for a fixed measurement setup and this estimation can
be offline.
...
Computation of the MAP Estimates
The MAP-CEM and MAP-EEM estimates are computed by a Polak Ribiere conjugate gra-
dient algorithm which is equipped with an explicit line search. Similarly as in the initial
estimation, the positivity prior of the absorption and scatter images is taken into account
by using (scaled) logarithmic parameterization
log( μa
μa
),
log ( μ′
s
μs
)
in the unconstrained optimization process, for details see [].


Optical Imaging
0.0073
0.0050
0.0068
0.0076
0.0085
0.61
0.95
1.01
1.12
0.0091
0.94
1.32
⊡Fig. -
Pure discretization errors. Central horizontal slice from the D reconstructions of absorption
µa and scattering µ′
s. Top row: MAP estimate with the conventional error model (MAP-CEM)
using the accurate forward model AΩ,δ (number of nodes in the FEM mesh Nn = , ).
Left: µa,CEM. Right: µa,EEM. Bottom row: MAP estimates with the conventional (MAP-CEM) and
enhanced error models (MAP-EEM) using the target model AΩ,h (the number of nodes
Nn = , ). Correct model domain Ω = Ω is used in the target model AΩ,h. Columns from left
to right: µa,CEM, µa,EEM, µs,CEM, and µs,EEM. The number of unknowns x = (µa, µ′
s)
T in the
estimation with both models, AΩ,δ and AΩ,h, was , 
Results are shown in > Figs. -and > -and computation times in > Table -.
The images in the top row display the MAP estimate with the conventional noise model
using the accurate forward model AΩ,δ (Nn = ,). The estimated values of global
parameters μa= .mm−and μs= .mm−are relatively close to the back-
ground values μa,bg = .mm−and μs,bg = mm−of the target phantom. As can be
seen, the structure of the phantom is reconstructed well but the contrast of the recovered
inclusions is low compared to the (presumed) contrast. However, the low contrast is related
to the measurement setup, not the reconstruction algorithm; the same measurement real-
ization has previously been used for absolute reconstructions with different algorithm in
[], resulting to similar reconstruction quality and contrast in the optical properties. See
also [] for similar results with the same measurement system. The MAP-CEM estimate
with the accurate model AΩ,δ can be considered here as a reference estimate using con-
ventional noise model in absence of modeling errors caused by reduced discretization or
domain truncation.

Optical Imaging 

0.0073
0.0091
0.0050
0.0068
0.0076
0.0085
0.61
0.95
1.01
1.12
0.94
1.32
⊡Fig. -
Pure discretization errors. Vertical slices from the D reconstructions of absorption µa and
scattering µ′
s. The slices have been chosen such that the inclusion in the parameter is visible.
The arrangement of the images is equivalent to > Fig. -
⊡Table -
Reconstruction times for (> Figs. -and > -). tinit is the (wall clock) time for initial
estimation, tMAP for the MAP estimation, and ttot the total reconstruction time (initial + MAP)
Noise model
Forward model
tinit (s)
tMAP (s)
ttot (s)
CEM
AΩ,δ
min s
min s
min s
CEM
AΩ,h
min s
min s
min s
EEM
AΩ,h
s
min s
min s
The MAP-CEM estimate using the coarse target model AΩ,h (Nn = ,) is shown in
the first and third images in the bottom row in > Figs. -and > -. As can be seen,
the use of reduced discretization has caused significant errors in the reconstruction and
also the levels of μa and μ′
s are erroneous.


Optical Imaging
The MAP estimate with the enhanced error model using the coarse target model AΩ,h
is shown in the second and fourth images in the bottom row in > Figs. -and > -.
As can be seen, the estimate is very similar to the MAP-CEM estimate with the accurate
model AΩ,δ, showing that the use of enhanced error model has efficiently compensated
for the errors caused by reduced discretization accuracy. These results indicate that the
enhanced error model allows significant reduction in computation time without compro-
mise in the reconstruction quality; whereas the reconstruction time for the MAP-CEM
using accurate model AΩ,δ is very close to h, the computation time for MAP-EEM is
only min.
.
Conclusions
In this chapter we mainly discussed the use of the diffusion approximation for optical
tomography. Because of the exponentially ill-posed nature of the corresponding inverse
problem, diffuse optical tomography (DOT) gives low resolution images. Current research
is focused on several areas: the use of auxiliary (multimodality) information to improve
DOT images, the development of smaller-scale (mesoscopic) imaging methods based on
the radiative transfer equation, the development of fluorescence and bioluminescence
imaging techniques which give stronger contrast to features of interest. These methods are
closely tied to development of new experimental systems, and to application areas which
are driving the continued interest in this technique.
.
Cross References
> EIT
> Inverse Scattering
> Regularization Methods
> Imaging in Random Media
> Photoacoustic and Thermo Acoustic Tomography
References and Further Reading
. Amaldi E () The production and slowing
down of neutrons. In Flügge S (ed) Encyclopedia
of physics, vol /. Springer, Berlin, pp –
. Aronson R () Boundary conditions for diffu-
sion of light. J Opt Soc Am A :–
. Aydin ED () Three-dimensional photon
migration through voidlike regions and channels.
Appl Opt ():–
. Aydin ED, de OliveiraCRE, Goddard AJH ()
A finite element-spherical harmonics radiation
transport model for photon migration in tur-
bid media. J Quant Spectrosc Radiat Transf :
–
. Bal G () Transport through diffusive and
nondiffusive regions, embedded objects, and
clear layers. SIAM J Appl Math ():–

Optical Imaging 

. Bal G () Radiative transfer equation with
varying refractive index: a mathematical perspec-
tive. J Opt Soc Am A :–
. Bal G () Inverse transport theory and appli-
cations. Inv Probl :(pp)
. Bal G, Maday Y () Coupling of transport and
diffusion models in linear transport theory. Math
Model Numer Anal ():–
. BluestoneAV,AbdoulaevG,SchmitzCH,Barbour
RL,Hielscher AH () Three-dimensional opti-
cal tomography of hemodynamics in the human
head. Opt Express ():–
. Contini D, Martelli F, Zaccanti G () Pho-
ton migration through a turbid slab described
by a model based on diffusion approximation. I.
Theory Appl Opt ():–
. Dehghani H, Arridge SR, Schweiger M, Delpy
DT () Optical tomography in the pres-
ence of void regions. J Opt Soc Am A ():
–
. Fantini S, Franceschini MA, Gratton E ()
Effective source term in the diffusion equation
for photon transport in turbid media. Appl Opt
():–
. Ferwerda HA () The radiative transfer equa-
tion for scattering media with a spatially varying
refractive index. J Opt A Pure Appl Opt ():
L–L
. Furutsu K () Diffusion equation derived
from space-time transport equation. J Opt Soc
Am ():–
. Groenhuis RAJ, Ferwerda HA, Ten Bosch JJ
() Scattering and absorption of turbid mate-
rials determined from reflection measurements.
Part : Theory Appl Opt ():–
. Hebden JC, Gibson A, Md Yusof R, Everdell N,
Hillman EMC, Delpy DT, Arridge SR, Austin T,
Meek JH, Wyatt JS () Three-dimensional
optical tomographyof theprematureinfantbrain.
Phys Med Biol :–
. Khan T, Jiang H () A new diffusion approx-
imation to the radiative transfer equation for
scattering media with spatially varying refractive
indices. J Opt A Pure Appl Opt :–
. Kim AD, Ishimaru A () Optical diffusion
of continuos-wave, pulsed, and density waves in
scattering media and comparisons with radiative
transfer. Appl Opt ():–
. Klose AD, Larsen EW () Light transport in
biological tissue based on the simplified spherical
harmonics equations.
J Comput Phys :
–
. Kolehmainen V, Arridge SR, Vauhkonen M,
Kaipio JP () Simultaneous reconstruction of
internal tissue region boundaries and coefficients
in optical diffusion tomography. Phys Med Biol
:–
. Marti-Lopez L, Bouza-Dominguez J, Hebden JC,
Arridge SR, Martinez-Celorio RA () Valid-
ity conditions for the radiative transfer equation.
J Opt Soc Am A ():–
. Wang LV () Rapid modeling of diffuse
reflectance of light in turbid slabs. J Opt Soc Am
A ():–
. Wright S, Schweiger M, Arridge SR ()
Reconstruction in optical tomography using the
PN approximations. Meas Sci Technol :–
. Ackroyd RT () Finite element methods for
particle transport : applications to reactor and
radiation physics. Research Studies, Taunton
. Anderson BDO, Moore JB () Optimal filter-
ing. Prentice Hall, Englewood Cliffs
. Arridge SR () Optical tomography in medi-
cal imaging. Inverse Probl ():R–R
. Arridge SR, Cope M, Delpy DT () Theoret-
ical basis for the determination of optical path-
lengths in tissue: temporal and frequency analy-
sis. Phys Med Biol :–
. Arridge SR, Dehghani H, Schweiger M, Okada E
() The finite element model for the propaga-
tion of light in scattering media: a direct method
for domains with non-scattering regions. Med
Phys ():–
. Arridge SR, Kaipio JP, Kolehmainen V, Schweiger
M, Somersalo E, Tarvainen T, Vauhkonen M
() Approximation errors and model reduc-
tion with an application in optical diffusion
tomography. Inverse Probl ():–
. Arridge
SR,
Lionheart
WRB
()
Non-
uniqueness
in
diffusion-based
optical
tomography. Opt Lett :–
. Arridge SR, Schotland JC () Optical tomog-
raphy: forward and inverse problems. Inverse
Prob ():(pp)
. Arridge SR, Schweiger M, Hiraoka M, Delpy
DT () A finite element approach for model-
ing photon transport in tissue. Med Phys ():
–
. Arridge SR, Kaipio JP, Kolehmainen V, Schweiger
M, Somersalo E, Tarvainen T, Vauhkonen M


Optical Imaging
() Approximation errors and model reduc-
tion with an application in optical diffusion
tomography. Inverse Probl :–
. Benaron DA, Stevenson DK () Optical time-
of-flight and absorbance imaging of biological
media. Science :–
. Berg R, Svanberg S, Jarlman O () Medical
transillumination
imaging
using
short-pulse
laser diodes. Appl Opt :–
. Berger JO () Statistical decision theory and
Bayesian analysis. Springer, New York
. Calvetti D, Kaipio JP, Somersalo E () Aris-
totelian prior boundary conditions. Int J Math
:–
. Case MC, Zweifel PF () Linear transport
theory. Addison-Wesley, New York
. Cope M, Delpy DT () System for long term
measurement of cerebral blood and tissue oxy-
genation on newborn infants by near infra-
red transillumination. Med Biol Eng Comput
:–
. Cutler M() Transilluminationas an aid inthe
diagnosis of breast lesions. Surg Gynecol Obstet
:–
. Delpy DT, Cope M, van der Zee P, Arridge SR,
Wray S, Wyatt J () Estimation of optical path-
length through tissue from direct time of flight
measurement. Phys Med Biol :–
. Diamond SG, Huppert TJ, Kolehmainen V,
Franceschini MA, Kaipio JP, Arridge SR, Boas
DA () Dynamic physiological modeling for
functional diffuse optical tomography. Neuroim-
age :–
. Dorn O () Das inverse Transportproblem in
der Lasertomographie. PhD thesis, University of
Münster
. Doucet A, de Freitas N, Gordon N () Sequen-
tial Monte Carlo methods in practice. Springer,
New York
. Duderstadt JJ, Martin WR () Transport
theory. Wiley, New York
. Durbin J, Koopman J () Time series analysis
by state space methods. Oxford University Press,
Oxford
. Firbank M, Arridge SR, Schweiger M, Delpy DT
() An investigation of light transport through
scattering bodies with non-scattering regions.
Phys Med Biol :–
. Haskell RC, Svaasand LO, Tsay T-T, Feng T-C,
McAdams MS, Tromberg BJ () Boundary
conditions for the diffusion equation in radiative
transfer. J Opt Soc Am A ():–
. Hayashi T, Kashio Y, Okada E () Hybrid
Monte Carlo-diffusion method for light propaga-
tion in tissue with a low-scattering region. Appl
Opt ():–
. Hebden JC, Kruger RA, Wong KS () Time
resolved imaging through a highly scattering
medium. Appl Opt ():–
. Heino J, Somersalo E () Estimation of optical
absorption in anisotropic background. Inverse
Prob :–
. Heino J, Somersalo E () A modelling error
approach for the estimation of optical absorption
in the presence of anisotropies. Phys Med Biol
:–
. Heino J, Somersalo E, Kaipio JP () Com-
pensation
for
geometric
mismodelling
by
anisotropies in optical tomography. Opt Express
():–
. Henyey LG, Greenstein JL () Diffuse radia-
tion in the galaxy. AstroPhys J :–
. Hielscher AH, Alcouffe RE, Barbour RL ()
Comparison of finitedifference transport and
diffusion calculations for photon migration in
homogeneous and hetergeneous tissue. Phys
Med Biol :–
. Ho PP, Baldeck P, Wong KS, Yoo KM, Lee D,
Alfano RR () Time dynamics of photon
migration in semiopaque random media. Appl
Opt :–
. Huttunen JMJ, Kaipio JP () Approxima-
tion error analysis in nonlinear state estimation
with an application to state-space identification.
Inverse Prob :–
. Huttunen JMJ, Kaipio JP () Approximation
errors in nostationary inverse problems. Inverse
Prob Imaging ():–
. Huttunen
JMJ,
Kaipio
JP
()
Model
reduction
in
state
identification
problems
with
an
application
to
determination
of
thermal parameters. Appl Numer Math :
–
. Huttunen JMJ, Lehikoinen A, Hämäläinen J,
Kaipio JP () Importance filtering approach
for
the
nonstationary
approximation
error
method. Inverse Prob in review
. Ishimaru A () Wave propagation and scat-
tering in random media, vol . Academic,
New York

Optical Imaging 

. Jarry G, Ghesquiere S, Maarek JM, Debray S, Bui
M-H, Laurent HD () Imaging mammalian
tissues and organs using laser collimated transil-
lumination. J Biomed Eng :–
. Jöbsis FF () Noninvasive infrared monitor-
ing of cerebral and myocardial oxygen suffi-
ciency and circulatory parameters. Science :
–
. Kaipio J, Somersalo E () Statistical and com-
putational inverse problems. Springer, New York
. Kaipio J, Somersalo E () Statistical and com-
putational inverse problems. J Comput Appl
Math :–
. Kaipio JP, Kolehmainen V, Vauhkonen
M,
Somersalo E () Inverse problems with
structural prior information. Inverse Probl :
–
. Kak AC, Slaney M () Principles of computer-
ized tomographic imaging. IEEE, New York
. Kalman RE () A new approach to linear fil-
tering and prediction problems. Trans ASME.
J Basic Eng D():–
. Kolehmainen V, Prince S, Arridge SR, Kaipio
JP () A state estimation approach to non-
stationary optical tomography problem. J Opt
Soc Am A :–
. Kolehmainen
V,
Schweoger
M,
Nissilä
I,
Tarvainen T, Arridge SR, Kaipio JP ()
Approximation errors and model reduction in
three-dimensional optical tomography. J Optical
Soc Amer A :–
. Kolehmainen V, Tarvainen T, Arridge SR, Kaipio
JP
()
Marginalization
of
uninteresting
distributed parameters in inverse problems –
application to diffuse optical tomography. Int J
Uncertainty Quantification, In press
. Lakowicz JR, Berndt K () Frequency domain
measurement of photon migration in tissues.
Chem Phys Lett ():–
. Lehikoinen
A,
Finsterle
S,
Voutilainen
A,
Heikkinen
LM,
Vauhkonen
M,
Kaipio
JP
() Approximation errors and truncation of
computational domains with application to geo-
physical tomography. Inverse Probl Imaging :
–
. Lehikoinen
A,
Huttunen
JMJ,
Finsterle
S,
Kowalsky MB, Kaipio JP: Dynamic inversion
for
hydrological
process
monitoring
with
electrical resistance tomography under model
uncertainties, Water Resour Res : W,
doi:./WR, 
. Mitic G, Kolzer J, Otto J, Plies E, Solkner G,
Zinth W () Timegated transillumination of
biological tissue and tissuelike phantoms. Opt
Lett :–
. Natterer F, Wübbeling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Nissilä I, Noponen T, Kotilahti K, Tarvainen T,
Schweiger M, Lipiänen L, Arridge SR, Katila T
() Instrumentation and calibration meth-
ods for the multichannel measurement of phase
and amplitude in optical tomography. Rev Sci
Instrum ():
. Nissinen
A,
Heikkinen
LM,
Kolehmainen
V, Kaipio JP () Compensation of errors
due to discretization, domain truncation and
unknown
contact
impedances
in
electrical
impedance tomography. Meas Sci Technol ,
doi: ./–///
. Nissinen A, Kolehmainen V, Kaipio JP: Com-
pensation of modelling errors due to unknown
domain
boundary
in
electrical
impedance
tomography, IEEE Trans Med Imaging, in
review, .
. Nissinen A, Heikkinen LM, Kaipio JP ()
Approximation errors in electrical impedance
tomography – an experimental study. Meas Sci
Technol , doi: ./-///
. Ntziachristos V, Ma X, Chance B () Time-
correlated
single
photon
counting
imager
for
simultaneous
magnetic
resonance
and
near-infrared mammography. Rev Sci Instrum
:–
. Okada E, Schweiger M, Arridge SR, Firbank
M, Delpy DT () Experimental validation of
Monte Carlo and Finite-Element methods for the
estimation of the optical path length in inhomo-
geneous tissue. Appl Opt ():–
. Prince S, Kolehmainen V, Kaipio JP, Franceschini
MA, Boas D, Arridge SR () Time series
estimation of biological factors in optical dif-
fusion tomography.
Phys
Med Biol
():
–
. Schmidt A, Corey R, Saulnier P () Imag-
ing through random media by use of low-
coherence optical heterodyning. Opt Lett :
–


Optical Imaging
. Schmidt FEW, Fry ME, Hillman EMC, Hebden
JC, Delpy DT () A -channel time-resolved
instrument for medical optical tomography. Rev
Sci Instrum ():–
. Schmitt JM, Gandbjbakhche AH, Bonner RF
() Use of polarized light to discriminate
short-path photons in a multiply scattering
medium. Appl Opt :–
. Schotland JC, Markel V () Inverse scatter-
ing with diffusing waves. J Opt Soc Am A :
–
. Schweiger M, Arridge SR () The finite ele-
ment model for the propagation of light in scat-
tering media: frequency domain case. Med Phys
():–
. Schweiger M, Arridge SR, Hiraoka M, Delpy
DT () The finite element model for the
propagation of light in scattering media: bound-
ary and source conditions. Med Phys ():
–
. Schweiger M, Arridge SR, Nissilä I () Gauss–
Newton method for image reconstruction in dif-
fuse optical tomography. Phys Med Biol :–

. Schweiger M, Nissilä I, Boas DA, Arridge SR
() Image reconstruction in optical tomogra-
phy in the presence of coupling errors. Appl Opt
():–
. Spears KG, Serafin J, Abramson NH, Zhu X,
Bjelkhagen H () Chronocoherent imaging
for medicine. IEEE Trans Biomed Eng :
–
. Sylvester J, Uhlmann G () A global uniquness
theorem for an inverse boundary value problem.
Ann Math :–
. Tarvainen T, Kolehmainen V, Pulkkinen A,
Vauhkonen M, Schweiger M, Arridge SR, Kaipio
JP () Approximation error approach for
compensating for modelling errors between the
radiative transfer equation and the diffusion
approximation
in
diffuse
optical
tomogra-
phy. Inverse Probl , doi: ./–/
//
. Tarvainen T, Vauhkonen M, Kolehmainen V,
Arridge SR, Kaipio JP () Coupled radiative
transfer equation and diffusion approximation
model for photon migration in turbid medium
with low-scattering and non-scattering regions.
Phys Med Biol :–
. Tarvainen T, Vauhkonen M, Kolehmainen V,
Kaipio JP () A hybrid radiative transfer –
diffusion model for optical tomography. Appl
Opt ():–
. Tarvainen T, Vauhkonen M, Kolehmainen V,
Kaipio JP () Finite element model for the
coupled radiative transfer equation and diffu-
sion approximation. Int J Numer Meth Engng
():–
. Tervo J, Kolmonen P, Vauhkonen M, Heikkinen
LM, Kaipio JP () A finite-element model
of electron transport in radiation therapy and
a related inverse problem. Inverse Prob :
–
. Wang L, Ho PP, Liu C, Zhang G, Alfano RR
() Ballistic -D imaging through scattering
walls using an ultrafast optical Kerr gate. Science
:–
. Wang L, Jacques SL () Hybrid model of
Monte Carlo simulation diffusion theory for light
reflectance by turbid media. J Opt Soc Am A
():–

Photoacoustic and
Thermoacoustic
Tomography: Image
Formation Principles
Kun Wang ⋅Mark A. Anastasio
.
Introduction......................................................................
.
Imaging Physics and Contrast Mechanisms....................................
..
The Thermoacoustic Effect and Signal Generation................................
..
Image Contrast in Laser-Based PAT..................................................
..
Image Contrast in RF-Based PAT.....................................................
..
Functional PAT..........................................................................
.
Principles of PAT Image Reconstruction........................................
..
PAT Imaging Models in Their Continuous Forms..................................
..
Universal Backprojection Algorithm.................................................
..
The Fourier-Shell Identity..............................................................
...
Special Case: Planar Measurement Geometry......................................
..
Spatial Resolution from a Fourier Perspective......................................
...Effects of Finite Transducer Bandwidth.............................................
...Effects of Non-Point-Like Transducers..............................................
.
Speed-of-Sound Heterogeneities and Acoustic Attenuation...................
..
Frequency-Dependent Acoustic Attenuation. ......................................
..
Weak Variations in the Speed-of-Sound Distribution.............................
.
Data Redundancies and the Half-Time Reconstruction Problem.............
..
Data Redundancies......................................................................
..
Mitigation of Image Artifacts Due to Acoustic Heterogeneities. ................
.
Discrete Imaging Models........................................................
..
Continuous-to-Discrete Imaging Models...........................................
..
Finite-Dimensional Object Representations. .......................................
..
Discrete-to-Discrete Imaging Models................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
...Numerical Example: Impact of Representation Error on Computed
Pressure Data............................................................................
..
Iterative Image Reconstruction.......................................................
...Numerical Example: Influence of Representation Error on
Image Accuracy..........................................................................
.
Conclusions.......................................................................
.
Cross-References.................................................................

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

Abstract: Photoacoustic tomography (PAT), also known as thermoacoustic or optoacous-
tic tomography, is a rapidly emerging imaging technique that holds great promise for
biomedical imaging. PAT is a hybrid imaging technique, and can be viewed either as an
ultrasound mediated electromagnetic modality or an ultrasound modality that exploits
electromagnetic-enhanced image contrast. In this chapter, we provide a review of the
underlying imaging physics and contrast mechanisms in PAT. Additionally, the imag-
ing models that relate the measured photoacoustic wavefields to the sought-after optical
absorption distribution are described in their continuous and discrete forms. The basic
principles of image reconstruction from discrete measurement data are presented, which
includes a review of methods for modeling the measurement system response.
.
Introduction
Photoacoustic tomography (PAT), also known as thermoacoustic or optoacoustic tomog-
raphy, is a rapidly emerging imaging technique that holds great promise for biomedical
imaging [, , , , ]. PAT is a hybrid technique that exploits the thermoacous-
tic effect for signal generation. It seeks to combine the high electromagnetic contrast
of tissue with the high spatial resolution of ultrasonic methods. Accordingly, PAT can
be viewed either as an ultrasound mediated electromagnetic modality or an ultrasound
modality that exploits electromagnetic-enhanced image contrast []. Since the s,
there have been numerous fundamental studies of photoacoustic imaging of biological
tissue [, , , , , , ], and the development of PAT continues to progress at a
tremendous rate [, , , , , , , , ].
When a short electromagnetic pulse (e.g., microwave or laser) is used to irradiate a bio-
logical tissue, the thermoacoustic effect results in the emission of acoustic signals that can
be measured outside the object by use of wide-band ultrasonic transducers. The objective
of PAT is to produce an image that represents a map of the spatially variant electromag-
netic absorption properties of the tissue, from knowledge of the measured acoustic signals.
Because the optical absorption properties of tissue is highly related to its molecular consti-
tution, PAT images can reveal the pathological condition of the tissue [, ] and therefore
facilitate a wide-range of diagnostic tasks. Moreover, when employed with targeted probes
or optical contrast agents, PAT has the potential to facilitate high-resolution molecular
imaging [, ] of deep structures, which cannot be achieved easily with pure optical
methods.
From a physical perspective, the image reconstruction problem in PAT can be inter-
preted as an inverse source problem []. Accordingly, PAT is a computed imaging modality
that utilizes an image reconstruction algorithm to form the image of the absorbed opti-
cal energy distribution. A variety of analytic image reconstruction algorithms have been
developed for three-dimensional (D) PAT, assuming point-like ultrasound transducers
with canonical measurement apertures [, , , , , –]. All known analytic recon-
struction algorithms that are mathematically exact and numerically stable require complete
knowledge of the photoacoustic wavefield on a measurement aperture that either encloses


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
the entire object or extends to infinity. In many potential applications of PAT imaging, it is
not feasible to acquire such measurement data. Because of this, iterative, or more generally,
optimization-based, reconstruction algorithms for PAT are being developed actively [, ,
, , ] that provide the opportunity for accurate image reconstruction from incomplete
measurement data. Iterative reconstruction algorithms also allow for accurate modeling
of physical non-idealities in the data, such as those introduced by acoustic inhomogeneity
and attenuation, or the response of the imaging system.
In this chapter, the physical principles of PAT are reviewed. We start with a review of
the underlying imaging physics and contrast mechanisms in PAT. Subsequently, the imag-
ing models that relate the measured photoacoustic wavefields to the sought-after optical
absorption distribution are described in their continuous and discrete forms. The basic
principles of image reconstruction from discrete measurement data are presented, which
includes a review of methods for modeling the measurement system response. We defer a
detailed description of analytic reconstruction algorithms and the mathematical properties
of PAT to > Chap. (Mathematics of Photoacoustic and Thermoacoustic Tomography).
.
Imaging Physics and Contrast Mechanisms
In PAT, a laser or microwave source is used to irradiate an object, and the thermoacoustic
effect results in the generation of a pressure wavefield p(r, t) [, , ], where r ∈ℝ
and t is the temporal coordinate. The resulting pressure wavefield can be measured by
use of wide-band ultrasonic transducers located on a measurement aperture Ω⊂ℝ,
which is a D surface that partially or completely surrounds the object. In this section, we
review the physics that underlies the image contrast mechanism in PAT employing laser
and microwave sources.
..
The Thermoacoustic Eﬀect and Signal Generation
The generation of photoacoustic wavefields in an inviscid and lossless medium is described
by the general photoacoustic wave equation [, ]
(∇−
c
∂
∂t) p(r, t) = −β
κc
∂T(r, t)
∂t
,
(.)
where ∇is the D Laplacian operator, T(r, t) denotes the temperature rise within the
object at location r and time t due to absorption of the probing electromagnetic radia-
tion, and p(r, t) denotes the resulting induced acoustic pressure. The quantities β, κ, and c
denote the thermal coefficient of volume expansion, isothermal compressibility, and speed
of sound, respectively. Because an inviscid medium is assumed, the propagation of shear
waves is neglected in > Eq. (.), which is typically reasonable for soft-tissue imaging
applications. Note that the spatial–temporal samples of p(r, t), which are subsequently
degraded by the response of the imaging system, represent the measurement data in a PAT
experiment.

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

When the temporal width of the exciting electromagnetic pulse is sufficiently short,
the pressure wavefield is produced before significant heat conduction can take place. In
this situation, the excitation is said to be in thermal confinement. Specifically, this occurs
when the temporal width τ of the exciting electromagnetic pulse satisfies []
τ <
d
c
αth
,
(.)
where dc and αth denote the characteristic dimension (m) of the heated region and the
thermal diffusivity (m/s).
Under conditions of thermal confinement, the temperature function T(r, t) satisfies
ρCV
∂T(r, t)
∂t
= H(r, t),
(.)
where ρ and CV denote the mass density (kg/m) and specific heat capacity of the medium
at constant volume. The quantity H(r, t) [J/(ms)] is called the heating function that
describes the energy per unit volume and time that is deposited in the medium by the
exciting electromagnetic pulse. On substitution from > Eq. (.) into > Eq. (.), one
obtains the simplified photoacoustic wave equation
(∇−
c
∂
∂t) p(r, t) = −β
Cp
∂H(r, t)
∂t
,
(.)
where Cp = ρcκCV [J/(kg K)] denotes the specific heat capacity of the medium at constant
pressure. It is sometimes convenient to work the velocity potential ϕ(r, t) that is related
to the pressure as p(r, t) = −ρ ∂ϕ(r,t)
∂t
. It can be readily verified that > Eq. (.) can be
re-expressed in terms of ϕ(r, t) as
(∇−
c
∂
∂t) ϕ(r, t) =
β
ρCp
H(r, t).
(.)
The photoacoustic wave equations described by > Eqs. (.) and ( > .) have been
solved for a variety of canonical absorbers [–]. > Figure -shows an example corre-
sponding to a uniform spherical absorber. In this case, the optical absorber was assumed to
possess a speed of sound cthat matched the background medium. Note that the pressure
possesses an “N-shape” waveform. Solutions have also been derived for the case where the
optical absorbers have acoustical properties that are different from those of the background
medium [].
In practice, it is appropriate to consider the following separable form for the heating
function
H(r, t) = A(r)I(t),
(.)
where A(r) (J/m) is the absorbed energy density and I(t) denotes the temporal profile of
the illuminating pulse.
When the exciting electromagnetic pulse duration τ is short enough to satisfy the
acoustic stress-confiment condition
τ < dc
c ,
(.)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
0
1
2
3
4
−0.2
−0.1
0
0.1
0.2
Normalized time: c0 t /Rs 
Pressure at r = 2 Rs 
Pressure: p
a
Velocity potential at r = 2Rs 
Normalized time: c0 t /Rs 
b
0
1
2
3
4
0
0.05
0.1
Velocity potential: φ
⊡Fig. -
The pressure (a) and velocity potential (b) waveforms produced by the thermoacoustic
eﬀect for a uniform sphere of radius Rs
in addition to the thermal-confinement condition in > Eq. (.), one can approximate
I(t) by a Dirac delta function I(t) ≈δ(t). Physically, > Eq. (.) requires that all of the
thermal energy has been deposited by the electromagnetic pulse before the mass density
or volume of the medium has had time to change. In this case, the absorbed energy density
A(r) is related to the induced pressure wavefield p(r, t) at t = as
p(r, t = ) = ΓA(r),
(.)
where Γ is the dimensionless Grueneisen parameter. As discussed in detail later, the goal
of PAT is to determine A(r), or equivalently, p(r, t = ) from measurements of p(r, t)
acquired on a measurement aperture. It is also useful to note that under the acoustic

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

stress-confinement condition, > Eq. (.) coupled with appropriate boundary conditions
is mathematically equivalent to the initial value problem []
(∇−
c
∂
∂t) p(r, t) = ,
(.)
subject to
p(r, t = ) = ΓA(r)
and
∂p(r, t)
∂t
∣
t== .
(.)
The effects of heterogeneous speed of sound or acoustic attenuation are not addressed
above, but will be described later. In the following two subsections, a review of the physical
object properties that give rise to image contrast, that is, variations in A(r), are reviewed
for the case of optical and microwave illumination.
..
Image Contrast in Laser-Based PAT
When an optical laser pulse is employed to induce the thermoacoustic effect, the heating
function can be explicitly expressed as
H(r, t) = μa(r)Φ(r, t),
(.)
where μa(r) (/m) is the optical absorption coefficient of the medium and Φ(r, t) [J/(ms)]
is the optical fluence rate []. Assuming Φ(r, t) ≡Φs(r)I(t), > Eq. (.) can be
expressed as
H(r, t) = μa(r)Φs(r)


A(r)
I(t),
(.)
where the absorbed energy density, which is the sought-after quantity in PAT, is now
identified as
A(r) ≡μa(r)Φs(r).
(.)
> Equation (.) reveals that image contrast in laser-based PAT is determined by the opti-
cal absorption properties of the object as well as variations in the fluence of the illuminating
optical radiation. Because only the optical absorption properties are intrinsic to the object,
in implementation of PAT it is desirable to make the optical fluence Φs(r) as uniform as
possible so one can unambiguously interpret A(r) ∝μa(r). This presents experimen-
tal challenges, and computational methods for quantitative determination of μa(r) are
being developed actively [, , ]. However, in most current implementations of PAT,
an estimate of A(r) represents the final image.
There are many desirable characteristics of laser-based PAT for biological imaging. The
optical absorption coefficient μa(r) is a function of the molecular composition of tissue
[] and is therefore sensitive to tissue pathologies and functions. Specifically, PAT can
deduce physiological parameters such as the oxygen saturation of hemoglobin and the total
concentration of hemoglobin, as well as certain features of cancer such as elevated blood
content of tissue due to angiogenesis [].


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
Although pure optical imaging methods also are sensitive to such physiological param-
eters, they are limited by their relatively poor spatial resolution and inability to image deep
tissue structures. PAT circumvents these limitation because diffusely scattered photons that
are absorbed at deep locations are still useful for signal generation via the thermoacous-
tic effect. When the wavelength of the optical source lies in the range –nm, light
can penetrate up to several centimeters in biological tissue. As described by > Eq. (.),
the optical fluence Φs(r), which contains ballistic and diffusely scattered photons, modu-
lates μa(r). However, as described later, the spatial resolution of the reconstructed estimate
of A(r) is not directly affected by this and is determined largely by the properties of the
measured pressure signal p(r, t).
..
Image Contrast in RF-Based PAT
When an RF pulse is employed to induce the thermoacoustic effect, the nature of the image
contrast is different from that described above. A detailed analyis of this has been con-
ducted by Li et al., in []. Consider the case of an RF pulse whose temporal width is much
longer than the oscillation period of the electromagnetic wave at the center frequency ωc.
The RF-source is assumed to produce a plane-wave with linear polarization and can be
described as
ein(t) = S(t)cos(ωct),
(.)
where S(t) is a slowly varying envelope function. Furthermore, consider that the medium
is isotropic and the electrical conductivity of the medium σ(r, ω) can be approximated as
σ(r, ω) ≈σ(r, ωc),
(.)
where ω represents the temporal frequency variable. Under the stated conditions, it is the
short-time averaged heating function
< H(r, t) >≡
Tc ∫
t+Tc
t
dt∣H(r, t)∣,
(.)
where Tc = π
ωc , which gives rise to signal generation in RF-based PAT []. It has been
demonstrated [] that this quantity can be expressed as
< H(r, t) >= A(r)S(t)

,
(.)
where S(t)

represents the electric field intensity of the RF source and
A(r) ≡σ(r, ωc)∣˜E(r, ωc)∣
∣˜ein(ωc)∣
,
(.)
where ˜E(r, ωc) and ˜ein(ωc) denote the temporal Fourier transforms of E(r, t) and ein(t)
evaluated at ω = ωc, with E(r, t) denoting the local electric field. Note that > Eq. (.)
represents the quantity that is estimated by conventional PAT reconstruction algorithms.

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

> Equation (.) reveals that image contrast in RF-based PAT is determined by the
electrical conductivity of the material, which is described by the complex permittivity,
as well as variations in the illuminating electric field at temporal frequency component
ω = ωc. Because only the electrical conductivity is intrinsic to the object material, it is
desirable to make ∣˜E(r, ωc)∣as uniform as possible, so one can unambiguously interpret as
the distribution of the conductivity. It has been demonstrated in computer-simulation and
experimental studies [] that estimates of A(r) produced by conventional image recon-
struction algorithms can be nonuniform and contain distortions due to diffraction of the
electromagnetic wave within the object to be imaged. There remains a need to develop
improved image reconstruction methods to mitigate these.
The complex permittivity of tissue has a strong dependence on the water content, tem-
perature, and ion concentration. Because of this, any variations in blood flow in tissue
will give rise to changes in the quantity of water and consequently to changes in its com-
plex permittivity. RF-based PAT therefore has the high sensitivity to tissue properties of a
microwave technique, but requires solution of a tractable acoustic inverse source problem
for image reconstruction.
..
Functional PAT
A highly desirable characteristic of PAT is its ability to provide detailed functional, in addi-
tional to anatomical, information regarding biological systems. In this section, we provide
a brief review of functional imaging using PAT. For additional details, the reader is referred
to Parts IX and X in reference [] and the references therein.
Due to optical contrast mechanism discussed in > Sect. .., Laser-based functional
PAT operating in the near-infrared (NIR) frequency range can be employed to determine
information regarding the oxygenated and deoxygenated hemoglobin within the blood of
tissues. This can permit the study of vascularization and hemodynamics, which is relevant
to brain imaging and cancer detection.
Functional PAT imaging of hemoglobin can beachieved by exploiting the known
characteristic absorption spectra of oxygenated hemoglobin (HbO) and deoxygenated
hemoglobin (Hb). Consider the situation where the optical fluence Φs(r) is known, and
therefore the optical absorption coefficient μa(r) can be determined from the recon-
structed absorbed energy density A(r) via > Eq. (.). Let μλa (r) and μλ
a (r) denote the
reconstructed estimates of μa(r) corresponding to the cases where the wavelength of the
optical source is set at λand λ. From knowledge of these two estimates, the hemoglobin
oxygen saturation distribution, denoted by SO(r), is determined as
SO(r) =
μλ
a (r)єλ
Hb −μλa (r)єλ
Hb
μλa (r)єλ
ΔHb −μλ
a (r)єλ
ΔHb
,
(.)
where єλ
Hb and єλ
HbOdenote molar extinction coefficients of Hb and HbO, and єλ
ΔHb ≡
єλ
HbO−єλ
Hb. The distribution of the total hemoglobin concentration, denoted by HbT(r),
can be determined as


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
HbT(r) = μλa (r)єλ
ΔHb −μλ
a (r)єλ
ΔHb
єλ
Hbєλ
HbO−єλ
Hbєλ
HbO
.
(.)
An experimental investigation of functional PAT imaging of a rat brain was described
in []. While in different physiological states, a rat was imaged using laser light at wave-
lengths and nm to excite the photoacoustic signals. A two-dimensional (D)
scanning geometry was employed and the estimates of A(r) were reconstructed by use of a
backprojection reconstruction algorithm. Subsequently, estimates of SO(r) and HbT(r)
were computed, and are displayed in > Fig. -.
⊡Fig. -
Noninvasive spectroscopic photoacoustic imaging of HbT and SOin the cerebral cortex of a
rat brain. (a) and (b) Brain images generated by - and nm laser light, respectively; (c)
and (d) image of SOand HbT in the areas of the cortical venous vessels (Reproduced from
Wang X et al () J Biomed Opt :)

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

.
Principles of PAT Image Reconstruction
In the remainder of this chapter, we describe some basic principles that underlie image
reconstruction in PAT. We begin by considering the image reconstruction problem in
its continuous form. Subsequently, issues related to discrete imaging models that are
employed in iterative image reconstruction methods are reviewed.
A schematic of a general PAT imaging geometry is shown in > Fig. -. A short laser
or RF pulse is employed to irradiate an object and, as described earlier, the thermoacous-
tic effect results in the generation of a pressure wavefield p(r, t). The pressure wavefield
propagates out of the object and is measured by use of wide-band ultrasonic transducers
located on a measurement aperture Ω⊂ℝ, which is a D surface that partially or com-
pletely surrounds the object. The coordinate r∈Ωwill denote a particular transducer
location. Although we will assume that the ultrasound transducers are point-like, it should
be noted that alternative implementations of PAT are being actively developed that employ
integrating ultrasound detectors [, ].
..
PAT Imaging Models in Their Continuous Forms
When the object possesses homogeneous acoustic properties that match a uniform and
lossless background medium, and the duration of the irradiating optical pulse is negligi-
ble (acoustic stress confinement is obtained), the pressure wavefield p(r, t) recorded at
transducer location rcan be expressed [] as a solution to > Eq. (.):
p(r, t) =
β
πCp ∫Vdr A(r) d
dt
δ (t −∣r−r∣
c)
∣r−r∣
,
(.)
where cis the (constant) speed of sound in the object and background medium. The
function A(r) is compactly supported, bounded and non-negative, and the integration in
Measurement
aperture
x
Tissue
Ultrasound
receiver
y
R0
φ
Probing
optical pulse
⊡Fig. -
A schematic of the PAT imaging geometry


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
> Eq. (.) is performed over the object’s support volume V. > Equation (.) repre-
sents a canonical imaging model for PAT. The inverse problem in PAT is to determine an
estimate of A(r) from knowledge of the measured p(r, t). Note that, as described later,
the measured p(r, t) will generally need to be corrected for degradation caused by the
temporal and spatial response of the ultrasound transducer.
The imaging model in > Eq. (.) can be expressed in an alternate but mathematically
equivalent form as
g(r, t) = ∫Vdr A(r) δ (t −∣r−r∣
c
),
(.)
where the integrated data function g(r, t) is defined as
g(r, t) ≡πCpc
β
t ∫
t
dt′ p(r, t′).
(.)
Note that g(r, t) represents a scaled version of the acoustic velocity potential ϕ(r, t).
> Equation (.) represents a spherical Radon transform [, ], and indicates that the
integrated data function describes integrals over concentric spherical surfaces of radii ct
that are centered at the receiving transducer location r. When these spherical surfaces can
be approximated as planes, which would occur when imaging sufficiently small objects that
are placed at the center of the scanning system, > Eq. (.) can be approximated as a D
Radon transform [, ].
..
Universal Backprojection Algorithm
A number of analytic image-reconstruction algorithms [, , , ] for PAT have been
developed in recent years for inversion of > Eq. (.) or ( > .). A detailed description
of analytic algorithms will be provided in > Chap. (Mathematics of Photoacoustic and
Thermoacoustic Tomography). However, the so-called universal backprojection algorithm
[] is reviewed below.
The three canonical measurement geometries in PAT employ measurement apertures
Ωthat are planar [], cylindrical [], or spherical []. The universal backprojection
algorithm proposed by Xu and Wang [] has been explicitly derived for these geometries.
In order to present the algorithm in a general form, let S denote a surface, where S = Ωfor
the spherical and cylindrical geometries. For the planar geometry, let S = Ω+ Ω′
, where
Ω′
is a planar surface that is parallel to Ωand the object resides between Ωand Ω′
.
It has been verified that the initial pressure distribution p(r, t = ) = ΓA(r) can be
mathematically determined from knowledge of the measured p(r, t), r∈Ω, by use of
the formula
p(r, t = ) = 
π ∫S dS ∫
∞
−∞dk ˜p(r, k) [nS
⋅∇˜G(in)
k
(r,r)],
(.)
where ˜p(r, k) denotes the temporal Fourier transform of p(r, t) that is defined with
respect to the reduced variable ¯t = ct as
˜p(r, k) = ∫
∞
−∞d¯t p(r, ¯t)exp(ik¯t).
(.)

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

Here, nS
denotes the unit vector normal to the surface S pointing toward the source,
∇denotes the gradient operator acting on the variable r, and ˜G(in)
k
(r,r) = exp(−ik∣r−r∣)
π∣r−r∣
is a Green’s function of the Helmholtz equation.
> Equation (.) can be expressed in the form of a filtered backprojection algorithm
as
p(r, t = ) = ∫Σ
dΣ
b(r, ¯t = ∣r −r∣)
Σ
,
(.)
where Σis the solid angle of the whole measurement surface Ωwith respect to the recon-
struction point inside Ω. Note that Σ= π for the spherical and cylindrical geometries,
while Σ= π for the planar geometry. The solid angle differential dΣis given by
dΣ=
dΩ
∣r −r∣
nS
⋅(r −r)
∣r −r∣
,
(.)
where dΩis the differential surface area element on Ω. The filtered data function b(r, ¯t)
is related to the measured pressure data as
b(r, ¯t) = p(r, ¯t) −¯t ∂p(r, ¯t)
∂¯t
.
(.)
> Equation (.) has a simple interpretation. It states that p(r, t = ), or equivalently
A(r), can be determined by backprojecting the filtered data function onto a collection of
concentric spherical surfaces that are centered at each transducer location r.
..
The Fourier-Shell Identity
Certain insights regarding the spatial resolution of images reconstructed in PAT can be
gained by formulating a Fourier domain mapping between the measured pressure data
and the Fourier components of A(r) []. Below we review a mathematical relationship
between the pressure wavefield data function and its normal derivative measured on an
arbitrary aperture that encloses the object and the D Fourier transform of the optical
absorption distribution evaluated on concentric (Ewald) spheres []. We have referred to
this relationship as a “Fourier-shell identity,” which is analogous to the well-known Fourier
slice theorem of X-ray tomography.
Consider a measurement aperture Ωthat is smooth and closed, but is otherwise
arbitrary, and let ˆs ∈Sdenote a unit vector on the D unit sphere S. The D spatial
Fourier transform of A(r), denoted as ¯A( ), is defined as
¯A( ) = ∫V dr A(r)e−i ⋅r,
(.)
where the D spatial frequency vector
= ( x,
y,
z) is the Fourier conjugate of r. It has
been demonstrated [] that
¯A( = kˆs) =
iCp
kβ˜I(k) ∫Ω
dS′ [ˆn′ ⋅∇˜p (r ′
, k) + ik ˆn′ ⋅ˆs ˜p (r ′
, k)]e−ikˆs⋅r ′
,
(.)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
where ˜p(r, k) is defined in > Eq. (.), dS′ is the differential surface element on Ω, and
ˆn′ is the unit outward normal vector to Ωat the point r ′
∈Ω. > Equation (.) has
been referred to as the Fourier-shell identity of PAT. Because ˆs can be chosen to specify any
direction, ¯A(
= kˆs) specifies the Fourier components of A(r) that reside on a spherical
surface of radius ∣k∣, whose center is at the origin. Therefore, > Eq. (.) specifies concen-
tric “shells” of Fourier components of A(r) from knowledge of ˜p(r, k) and its derivative
along the ˆn′ direction at each point on the measurement aperture. As reviewed below, this
will permit a direct and simple analysis of certain spatial resolution characteristics of PAT.
For a D time-harmonic inverse source problem, it is well-known[, , ] that
measurements of the radiated wavefield and its normal derivative on a surface that encloses
the source specify the Fourier components of the source function that reside on an Ewald
sphere of radius k = ω
c , where ω is the temporal frequency. In PAT, the temporal depen-
dence I(t) of the heating function H(r, t) is not harmonic and, in general, ˜I(k) ≠. In the
ideal case where I(t) = δ(t), ˜I(k) = c. Consequently, when > Eq. (.) is applied to each
temporal frequency component k of ˜p(r, k), the entire D Fourier domain, with excep-
tion of the origin, is determined by the resulting collection of concentric spherical shells.
This is possible because of the separable form of the heating function in > Eq. (.).
...
Special Case: Planar Measurement Geometry
The Fourier-shell identity can be used to obtain reconstruction formulas for canonical
measurement geometries. For example, consider the case of an infinite planar aperture
Ω. Specifically, we assume a D object is centered at the origin of a Cartesian coordinate
system, and the measurement aperture Ωcoincides with the plane y = d > R, where R is
the radius of the object. In this situation, r ′
= (x′, d, z′), dS′ = dx′dz′, and ˆn′ = ˆy, where ˆy
denotes the unit vector along the positive y-axis. The components of the unit vector ˆs will
be denoted as (sx, sy, sz). > Equation (.) can be expressed as the following two terms:
¯A( = kˆs) = ¯A( = kˆs) + ¯A( = kˆs),
(.)
where
¯A( = kˆs) ≡
iCp
kcβ˜I(k)e−ikds y ∫∫∞dx′dz′ ∂˜p(x′, y, z′, k)
∂y
∣
y=d
e−ik(x′sx+z′sz),
(.)
and
¯A( = kˆs) ≡−Cp sy
cβ˜I(k) e−ikds y ∫∫∞dx′dz′ ˜p(x′, d, z′, k)e−ik(x′sx+z′sz),
(.)
where, without confusion, we employ the notation ˜p(x, y, z, k) = ˜p(r, k).
It can be readily verified that > Eqs. (.) and ( > .) can be re-expressed as
¯A( = kˆs) ≡
iCp
kcβ˜I(k)e−ikds y ∂
∂y
¯˜p(ksx, y, ksz, k)∣
y=d
(.)

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

and
¯A( = kˆs) ≡−Cp sy
cβ˜I(k) e−ikds y ¯˜p(ksx, d, ksz, k),
(.)
where ¯˜p( x, y,
z, k) is the D spatial Fourier transform of ˜p(x, y, z, k) with respect to x
and z (the detector plane coordinates):
¯˜p( x, y,
z, k) ≡

π∫∫∞dxdz ˜p(x, y, z, k)e−i(x
x+z
z).
(.)
The free-space propagator for time-harmonic homogeneous wavefields (see e.g.,
ref.[], Chapter .) can be utilized to compute the derivative in > Eq. (.) as
∂¯˜p(ksx, y, ksz, k)
∂y
= ik
√
−sx −sz ¯˜p(ksx, y, ksz, k) = iksy ¯˜p(ksx, y, ksz, k),
(.)
where sy ≥. > Equations (.)–(.) and (> .) establish that
¯A( = kˆs) = ¯A( = kˆs)
for
sy ≥.
(.)
> Equation (.) permits estimation of ¯A( = kˆs) on concentric half-shells in the domain
y ≥, and is mathematically equivalent to previously studied Fourier-based reconstruc-
tion formulas[, ]. Note that A(r) is real-valued and therefore the Fourier components
in the domain
y < can be determined by use of the Hermitian conjugate symmetry
property of the Fourier transform.
..
Spatial Resolution from a Fourier Perspective
The Fourier-shell identity described in > Sect. ..is a convenient tool for understanding
the spatial resolution characteristics of PAT. Below, we analyze the effects of finite trans-
ducer temporal bandwidth and aperture size on spatial resolution [, ]. The analysis is
applicable to any measurement aperture Ωthat corresponds to a coordinate surface of a
curvilinear coordinate system.
...
Eﬀects of Finite Transducer Bandwidth
Consider a point-like ultrasonic transducer whose temporal filtering characteristics are
described by the transfer function ˜B(k;r). The r-dependence of ˜B(k;r) permits trans-
ducers located at different measurement locations to be characterized by distinct transfer
functions. The temporal Fourier transform of the measured pressure signal that has been
degraded by the temporal response of the transducer will be denoted as ˜pb(r, k), in
order to distinguish it from the ideal pressure signal ˜p(r, k). Because the temporal trans-
ducer response can be described by a linear time-invariant system, the degraded and ideal
pressure data are related as
˜pb(r, k) = ˜B(k;r)˜p(r, k).
(.)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
Consider the case where Ωcorresponds to a coordinate surface of a curvilinear
coordinate system, that is, r∈Ωis a vector that varies in only two of its three com-
ponents. For such surfaces, B (k;r ′
) can be interpreted as a D function that does not
vary in the ˆn′ direction and therefore ˆn′⋅∇˜B (k;r ′
) = . If the Fourier-shell identity in
> Eq. (.) is applied with the degraded data function ˜pb(r, k) replacing the ideal data,
the D Fourier components of the resulting image, denoted by Ab(r), are recovered as
¯Ab( = kˆs) =
iCp
kβ˜I(k) ∫Ω
dS′ ˜B (k;r ′
)[ˆn′⋅∇˜p (r′
, k) + ik ˆn′⋅ˆs ˜p (r′
, k)]e−ikˆs⋅r′
. (.)
On comparison of > Eqs. (.) and (> .), we observe that the spatially variant trans-
ducer transfer function ˜B(k;r) modulates the integrand of the Fourier-shell identity. In
this general case, the spatial-resolution of A(r) will be spatially variant.
If a collection of identical transducers spans Ω, ˜B(k;r) = ˜B(k) will not depend on r
and > Eq. (.) reduces to the simple form
¯Ab( = kˆs) = ˜B(k) ¯A( = kˆs),
(.)
where ¯A(
= kˆs) is the exact Fourier data as defined in > Eq. (.). As shown in
> Fig. -, the one-dimensional (D) transfer function ˜B(k) of the transducer serves as
k
|B(k)|
O
k_low
k_high
a
b
O
k_low
k
A (k)
k_high
⊡Fig. -
(a) An example of a transducer transfer function ∼B(k). (b) The D function ∼B(k) acts as a
radially symmetric ﬁlter in the D Fourier domain. The shaded region indicates the
bandpass of D Fourier components that results from application of > Eq. (.)
(Reproduced from Anastasio MA et al () Inverse Probl :S–S)

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

a radially symmetric D filter that modifies ¯A(
= kˆs). This establishes that the image
degradation is described by a shift-invariant linear system:
Ab(r) = A(r) ∗B(r),
(.)
where ∗denotes a D convolution and
B(r) = B(∣r∣) = ∫
∞

dk ˜B(k)sin(k∣r∣)
k∣r∣
k
(.)
is the point-spread function. > Equation (.) is consistent with the results derived in
ref. [].
...
Eﬀects of Non-Point-Like Transducers
In addition to a non-ideal temporal response, a transducer cannot be exactly point-like,
and will have a finite aperture size. To understand the effects of this on spatial resolution,
we consider here a transducer that has an ideal temporal response (i.e., ˜B(k;r) = ) but
a finite aperture size []. We will assume that the surface of the transducer aperture is a
subset of the measurement aperture Ω.
It will be useful to employ a local D coordinate system whose origin coincides with
the center of the detecting surface ΩL ⊆Ω, for a transducer at some arbitrary but fixed
location r ′
∈Ω. A vector in this system will be denoted as rL, and the collection of rL ∈ΩL
spans all locations on the detecting surface of this transducer. For a transducer located at
a different position r∈Ω, the local coordinate vector will be denoted as
r 
L = Tr{rL},
(.)
where Tr{⋅} denotes the corresponding coordinate transformation. This indicates that the
collection of vectors r 
L corresponding to rL ∈ΩL reside in a local coordinate system whose
origin is at r, and span all locations on the detecting surface of the transducer centered at
that location.
The measured pressure data ˜pa(r, k), where the subscript “a” denotes the data are
obtained in the presence of a finite aperture, can be expressed as
˜pa(r, k) = ∫ΩL
dSL W(rL)˜p (r+ r 
L , k),
(.)
where dSL is the differential surface element on ΩL and the aperture function W(rL)
describe the sensitivity of the transducers at location rL on their surfaces. We assume the
aperture function is identical for all transducers, and therefore W(rL) can be described
simply in terms of the local coordinate rL. Note that r 
L is a function of rL, as described by
> Eq. (.).


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
If the Fourier-shell identity in > Eq. (.) is applied with the degraded data function
˜pa(r, k), the D Fourier components of the corresponding image Aa(r) are recovered as
¯Aa( = kˆs) =
iCp
kβ˜I(k) ∫ΩL
dSL W(rL)
× ∫Ω
dΩ′
e−ikˆs⋅r ′
[ˆn′⋅∇˜p (r ′
+ r 
L , k) + ik ˆn′⋅ˆs ˜p (r ′
+ r 
L , k)] .
(.)
By use of the change-of-variable r≡r ′
+ r 
L in > Eq. (.), one obtains
¯Aa( = kˆs) =
iCp
kβ˜I(k) ∫ΩL
dSL W(rL)
× ∫Ω
dΩe−ikˆs⋅(r−r 
L )[ˆn′⋅∇˜p(r, k) + ik ˆn′⋅ˆs ˜p(r, k)],
(.)
which cannot be simplified further.
The fact that > Eq. (.) does not reduce to a simple form analogous to > Eq. (.)
reflects that the image degradation due to a finite transducer aperture is generally not
described by a shift-invariant system []. A shift-invariant description is obtained for pla-
nar apertures where > Eq. (.) reduces to r 
L = rL, where r 
L no longer has a dependence
on r. In this case, > Eq. (.) can be expressed as
¯Aa( = kˆs) = ¯W(kˆs) ¯Aa( = kˆs),
(.)
where
¯W(kˆs) ≡∫ΩL
dSL W(rL)eikˆs⋅rL.
(.)
Because, in this case, rL resides on a plane and W(rL) is a real-valued function,
> Eq. (.) corresponds to the complex-conjugate of the D Fourier transform of
the aperture function. The point-spread function obtained by computing the D inverse
Fourier transform of ¯W(kˆs) reduces to a result given in [].
.
Speed-of-Sound Heterogeneities and Acoustic
Attenuation
In practice, the object to be imaged may not possess uniform acoustic properties, and
the images reconstructed by use of algorithms that ignore this can contain artifacts and
distortions. Below, we review some methods that can compensate for an object’s frequency-
dependent acoustic attenuation and heterogeneous speed-of-sound distribution.
..
Frequency-Dependent Acoustic Attenuation
Because the thermoacoustically inducted pressure signals measured in PAT are broad-
band and ultrasonic attenuation is frequency dependent, in certain applications it may be

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

important to compensate for this effect. Below, we describe a method described in [] for
achieving this.
Acoustic waves propagating in a lossy medium are attenuated with a linear attenuation
coefficient α(ω) of the general form []
α(ω) = α∣ω∣n ,
(.)
where ω is the angular frequency of the wave []. For ultrasonic waves in tissue, n ≈and
α≈(−/π) cm−rad−s .
Assuming a uniform speed-of-sound distribution, a photoacoustic wave equation with
consideration of acoustic attenutation can be expressed as []
∇p (r, t) −
c

∂
∂tp (r, t) + L(t) ∗p (r, t) = −β
Cp
A(r) ∂
∂t I (t),
(.)
where ∗denotes temporal convolution, cis now a reference phase velocity, and the
function L(t) describes the effect of acoustic attenuation and is defined as
L(t) = 
π ∫
∞
−∞dω (K(ω)−ω
c

)exp(−iωt),
(.)
where
K(ω) ≡
ω
c(ω) + iα(ω).
(.)
Note that in this section, p (r, t) denotes the pressure that is affected by acoustic
attenuation.
The phase velocity, denoted here by c(ω), also has a temporal frequency dependence
according to the Kramers–Kronig relations. For n = , this relationship is given by

c(ω) = 
c
−
π αln∣ω
ω
∣,
(.)
where ωis the reference frequency for which c(ω) = c.
Let ̃p (r, ω) denote the temporal Fourier transform of the pressure data:
̃p (r, ω) = ∫
∞
−∞dt p (r, t)exp (iωt).
(.)
It has been shown [] that the Fourier transform of the attenuated data ̃p (r, ω) is
related to the unattenuated data pideal (r, t) as
̃p (r, ω) = I(ω)( c
c(ω) + icαsgn(ω))
−
× ∫
∞
−∞pideal (r, t)exp {i [ω c
c(ω) + icα∣ω∣] t} dt,
(.)
where
pideal (r, t) = β
Cp ∫dr′A(r′) d
dt
δ (t −
∣r−r′∣
c)
π ∣r −r′∣,
(.)
is the solution to the photoacoustic wave equation in the absence of attenuation.


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
>Equation (.) permits one to investigate the effect of acoustic attenuation in PAT. It
can also be discretized to produce a linear system of equations that can be inverted numeri-
cally for removal of the effects of acoustic dispersion in the measured photoacoustic signals.
Subsequently, a conventional PAT image reconstruction algorithm could be employed to
estimate A(r). A numerical example of this is provided in ref. [].
..
Weak Variations in the Speed-of-Sound Distribution
The conventional PAT imaging models described in > Sect. ..assume that the object’s
speed of sound is constant and equal to that of the background medium. In certain
biomedical imaging applications, this assumption does not reasonably hold true. For
example, the speed of sound of breast tissue can vary from ,to ,m/s. Acous-
tic inhomogeneities can introduce significant wavefront aberrations in the photoacoustic
signal that are not accounted for in the available reconstruction algorithms.
For a weakly acoustic scattering object, with consideration of phase aberrations due to
the acoustic heterogeneities effects, the forward PAT imaging model can be expressed as a
generalized Radon transform [, ]
ˆg(r, ¯t) = ∫Vdr A(r)δ[¯t −ct f (r,r)]ct f (r,r)
∣r−r∣
,
(.)
where t f (r,r) is the time of flight (TOF) for a pressure wave to travel from point r within
the object to transducer location r. For objects possessing weak acoustic heterogeneities,
the TOF can be computed accurately as
t f (r,r) = ∫r ′∈L(r,r)
dr ′

c(r ′),
(.)
where c(r) is the spatially variant acoustic speed and the set L(r,r) describes a line
connecting rand r.
The generalized Radon transform describes weighted integrals of A(r) over iso-TOF
surfaces that are not spherical in general. The iso-TOF surfaces are determined by the
heterogeneous acoustic speed distribution c(r) of the object. In the absence of acoustic
heterogeneities, these are spherical surfaces with varying radii that are centered at r, and
> Eq. (.) reduces to the spherical Radon transform in > Eq. (.). To establish this
imaging model for PAT imaging, the iso-TOF surfaces that > Eq. (.) integrates over
need to be determined explicitly by use of a priori knowledge of the speed-of-sound dis-
tribution c(r). Estimates of c(r) can be obtained by performing an adjunct ultrasound
computed tomography study of the object []. Subsequently, ray-tracing methods can
be employed to identify the iso-TOF surfaces for each transducer position r. Once these
path lengths are computed, the points in the object that have the same path lengths can be
grouped together to form iso-TOF surfaces. No known analytic methods are available for
inversion of > Eq. (.). Accordingly, iterative methods have been employed for image
reconstruction [, ].

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

A higher-order geometrical acoustics-based imaging model has also been recently pro-
posed [] that takes into account the first-order effect in the amplitude of the measured
signal and higher-order perturbation to the travel times. By incorporating higher-order
approximations to the travel time that incorporates the effect of ray bending, the accuracy
of reconstructed images was significantly improved. More general reconstruction meth-
ods based on the concept of time-reversal are discussed in
> Chap. (Mathematics of
Photoacoustic and Thermoacoustic Tomography).
.
Data Redundancies and the Half-Time
Reconstruction Problem
In this section, we review data redundancies that result from symmetries in the PAT imag-
ing model [, , , ], which are related to the so-called half-time reconstruction problem
of PAT []. Specifically, we describe how an image can be reconstructed accurately from
knowledge of half of the temporal components recorded at all transducer locations on a
closed measurement aperture.
..
Data Redundancies
Consider the spherical Radon transform imaging model in > Eq. (.). Two half-time
data functions g()(r, ¯t) and g()(r, ¯t) can be defined as
g()(r, ¯t) = {g(r, ¯t)
:
R−RA ≤¯t ≤R

:
otherwise,
(.)
and
g()(r, ¯t) = {g(r, ¯t)
:
R< ¯t ≤R+ RA

:
otherwise.
(.)
Here, Rdenotes the radius of the measurement aperture Ω, and RA denotes the radius
of support of A(r). We assume that the object is acoustically homogeneous with speed of
sound cand ¯t ≡ct. Note that the data functions g()(r, ¯t) and g()(r, ¯t) each cover
different halves of the complete data domain Ω× [R−RA, R+ RA], and therefore
g(r, ¯t) = g()(r, ¯t) + g()(r, ¯t).
In the limit where R→∞, the spherical Radon transform reduces to a conventional
Radon transform that integrates over D planes. In that case, an obvious conjugate-
view symmetry exists [], and therefore, either of the half-time data functions g()(r, ¯t)
and g()(r, ¯t) contains enough information, in a mathematical sense, for exact image
reconstruction. Accordingly, a twofold data redundancy exists because the complete data
function g(r, ¯t) contains twice as much information as is theoretically necessary for exact
image reconstruction.
In the case where Ris finite, a simple conjugate view symmetry does not exist.
Nevertheless, it has been demonstrated that a two fold data redundancy exists in the com-
plete data function g(r, ¯t). This has been heuristically [] and mathematically [] by


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
use of a layer-stripping procedure [, , , ]. This established that A(r) can be recov-
ered uniquely and stably from knowledge of either half-time data functions g()(r, ¯t) or
g()(r, ¯t). A similar conclusion has been derived in ref. [] using a different mathematical
approach.
Analytic inversion formulae for recovering A(r) from knowledge of the half-time
data functions g()(r, ¯t) or g()(r, ¯t) are not currently available. However, iterative
reconstruction algorithms can be employed [, ] to determine A(r).
..
Mitigation of Image Artifacts Due to Acoustic
Heterogeneities
If the spatially variant speed of sound c(r) is known, one can numerically invert a dis-
cretized version of > Eq. (.) to determine an estimate of A(r) []. However, in many
applications of PAT, c(r) is not known, and images are simply reconstructed by use of
algorithms that assume a constant speed of sound. This can result in conspicuous image
artifacts.
Let ˆg(r, ¯t) denote a data function that is contaminated by the effects of speed-of-sound
variations within the object that is related to A(r) according to > Eq. (.). Let ˆA(r)
denote an estimate of A(r) that is reconstructed from ˆg(r, ¯t) by use of a conventional
reconstruction algorithm that assumes an acoustically homogeneous object. The quantities
ˆg()(r, ¯t) and ˆg()(r, ¯t) denote half-time data functions that are defined in analogy with
> Eqs. (.) and (> .) with g(r, ¯t) replaced by ˆg(r, ¯t).
An image reconstructed from ˆg()(r, ¯t) can sometimes contain reduced artifact levels
as compared to one reconstructed from the complete data ˆg(r, ¯t). To demonstrate this, in
the discussion below, we consider the D problem and the spatially variant speed-of-sound
distribution shown in
> Fig. -. This speed-of-sound distribution is comprised of two
r0
r1
c0
c1
R0
t2
t1
_
_
Ultrasound
receiver
⊡Fig. -
A speed-of-sound distribution comprised of two uniform concentric regions. Superimposed
on the ﬁgure are examples of how the surfaces of integration that contribution to the data
function g(r, t) are perturbed

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

uniform concentric disks that have cand c, with c≠c, and radii rand r, respectively.
The background medium is assumed to have a speed of sound, c.
The acoustic heterogeneity will cause the data function ˆg(r, ¯t) to differ from the
ideal one g(r, ¯t). The magnitude of this difference will be smaller, in general, for small
values of ¯t than for large values of ¯t. This can be understood by noting that, in gen-
eral, ∣t f (r,r) −∣r−r∣
c∣will become larger as the path length through the speed-of-sound
heterogeneity increases. This causes the surfaces of integration that contribute to ˆg(r, ¯t) to
become less spherical for larger values of ¯t. Accordingly, the data function ˆg(r, ¯t) becomes
less consistent with the spherical Radon transform model.
The discussion above suggests that a half-time reconstruction method that employs
ˆg()(r, ¯t) can produce images with reduced artifact and distortion levels than contained
a
b
⊡Fig. -
Images of a phantom object reconstructed from experimentally measured (a) full-time, (b)
ﬁrst half-time data functions (Reproduced from Anastasio MA et al () IEEE Trans Med
Imaging : –)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
in images reconstructed from the complete, or full-time, data ˆg(r, ¯t). An example of
this is shown in
> Fig. -. The data corresponded to a physical phantom study using a
microwave source, as described in ref. []. Measurements were taken at equally spaced
positions on the D circular scanning aperture of radius mm and for each measure-
ment, the received pressure signal was sampled at ,points, at a sampling frequency
of MHz. Images were reconstructed from full- and half-time data, via the EM algo-
rithm as described in ref. []. The contrast and resolution of the images reconstructed
from half-time data appears to be superior to that of the images reconstructed from the
full-time data.
.
Discrete Imaging Models
The imaging models discussed so far were expressed in their continuous forms. In practice,
PAT imaging systems record temporal and spatial samples of p(r, t), while the absorbed
energy density is described by the function A(r). Accordingly, a realistic imaging model
should be described mathematically as a continuous-to-discrete (C-D) mapping []. More-
over, when iterative reconstruction algorithms are employed, a discrete representation of
A(r) is required to establish a suitable discrete-to-discrete approximate imaging model. In
this section, we review these concepts within the context of PAT.
The remainder of this section is organized as follows. In
> Sect. .., we review
the C-D versions of the continuous-to-continuous (C-C) models in > Eqs. (.) and
(> .). Finite dimensional object representations are surveyed in > Sect. ..that are
used to establish the discrete-to-discrete (D-D) models in > Sect. ... In > Sect. ..,
we briefly review some approaches to iterative image reconstruction that have been applied
in PAT. The section concludes with a numerical example that demonstrates the effects of
object representation error on image reconstruction accuracy.
..
Continuous-to-Discrete Imaging Models
In practice, p(r, t) and g(r, t) are discretized temporally and determined at a finite num-
ber of receiver locations. The vectors p,g ∈ℝN will represent lexicographically ordered
representations of the sampled data functions, where the dimension N is defined by the
product of the number of temporal samples acquired at each transducer location (S) and
the number of transducer locations (M). Let > Eqs. (.) and (> .) be expressed in
operator notation as
p(r, t) = HpA(r),
(.)
and
g(r, t) = HgA(r).
(.)
In general, a C-D operator can be interpreted as a discretization operator Dστ acting on
C-C operator HCC []. Let y denote p or g and let HCC denote Hp or Hg. The notation y[n]

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

will be used to denote the nth element of the vector y. The C-D versions of > Eqs. (.)
and (> .) can be expressed as
y = DστHCCA(r) = HCDA(r),
(.)
where Dστ is discretization operator that characterizes the temporal and spatial sampling
characteristics of the ultrasonic transducer.
For the case where y = p and p(r, t) = HpA(r), Dστ will be denoted as D(p)
στ and is
defined as
p[mS+s] = [D(p)
στ p(r, t)]
[mS+s] ≡∫
∞
−∞dt τs(t)∫Ω
dΩp(r, t)σm(r),
(.)
where m = ,, . . ., M is the index that specifies the mth transducer location r,m on the
measurement aperture Ω, s = ,, . . . , S is the index of the time sample, and σm(r) and
τs(t) are functions that describe the spatial and temporal sampling apertures, respectively.
They are determined by the sampling properties of ultrasonic transducers. In the ideal case,
where both apertures are described by Dirac delta functions, the sth temporal sample for
the mth transducer location represents the pressure at time sΔT and location r,m, where
ΔT is the temporal sampling interval, that is,
p[mS+s] = p(r,m, sΔT).
(.)
We can express explicitly the C-D imaging model involving the pressure data as
p[mS+s] = ∫Vdr A(r) hmS+s(r),
(.)
where V denotes the support volume of A(r) and
hmS+s(r) ≡∫
∞
−∞dtτs(t)∫Ω
dΩh(r,r; t)σm(r)
(.)
defines a point response function. The kernel h(r,r; t) is defined as
h(r,r; t) = ∫
∞
−∞dt I(t)G(r,r; t, t),
(.)
where I(t) is the temporal illumination function and G(r,r; t, t) is the Green’s function
G(r,r; t, t) =
β
πCp∣r −r∣
dδ(t)
dt
∣
t=t−∣r−r∣
c
.
(.)
By use of the singular value decomposition of the C-D operator in > Eq. (.), a
pseudoinverse solution can be computed numerically to estimate A(r) [].
In order to establish a C-D imaging model involving the integrated pressure data, to
first order, we can approximate the integral operator in > Eq. (.) as
g[mS+s] = πCpcsΔT
β
s
∑
q=
p[mS+q].
(.)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
For the case where y = g and g(r, t) = HgA(r), Dστ will be denoted as D(g)
στ and is defined
as
g[mS+s] = [D(g)
στ g(r, t)]
[mS+s] ≡sΔT
s
∑
q=∫
∞
−∞dt τq(t)∫Ω
dΩσm(r) d
dt ( g(r, t)
t
).
(.)
Note that, in practice, g is not measured and is computed from the measured p by use
of > Eq. (.). Therefore, it is not physically meaningful to interpret g as being directly
sampled from the raw measurement data.
..
Finite-Dimensional Object Representations
When iterative image reconstruction algorithms are employed, a finite dimensional
representation of A(r) [] is required. In this section we review some finite dimen-
sional representations that have been employed in PAT. In the subsequent section,
computer-simulation studies are conducted to demonstrate the effects of error in the object
representation.
An N-dimensional representation of A(r) can be described as
Aa(r) =
N
∑
n=
θ[n]ϕn(r),
(.)
where the subscript a indicates that Aa(r) is an approximation of A(r). The functions
ϕn(r) are called expansion functions and the expansion coefficients θ[n] are elements of the
N-dimensional vector θ. The goal of iterative image reconstruction methods is to estimate
θ, for a fixed choice of the expansion functions ϕn(r).
The most commonly employed expansion functions are simple image voxels
ϕn(x, y, z) = {,
if
∣x −xn∣,∣y −yn∣,∣z −zn∣≤є/
,
otherwise
(.)
where rn = (xn, yn, zn) specify the coordinates of the nth grid point of a uniform Cartesian
lattice and є defines the spacing between lattice points.
In PAT, spherical expansion functions of the form
ϕn(x, y, z) = {,
if
√
(x −xn)+ (y −yn)+ (z −zn)≤є/
,
otherwise
(.)
have also proven to be useful [, ]. The merit of this kind of expansion function
is that the acoustic wave generated by each voxel can be calculated analytically. This
facilitates determination of the system matrix utilized by iterative image reconstruction
methods, as discussed below. Numerous other effective choices for the expansion func-
tions [] exist, including wavelets or other sets of functions that can yield sparse object
representations [].

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

In addition to an infinite number of choices for the expansion functions, there are an
infinite number of ways to define the expansion coefficients θ. Some common choices
include
θ[n] = Vcube
Vvoxel ∫Vdr ϕn(r)A(r),
(.)
or
θ[n] = ∫Vdr δ(r −rn)A(r).
(.)
For a given N, different choices of ϕn and θ will yield object representations that possess
different representation errors
δA(r) = A(r) −Aa(r).
(.)
An example of the effects of such representation errors on iterative reconstruction methods
is provided in > Sect. ...
..
Discrete-to-Discrete Imaging Models
Discrete-to-discrete (D-D) imaging models are required for iterative image reconstruc-
tion. These can be obtained systematically by substitution of a finite-dimensional object
representation into the C-D imaging model in > Eq. (.):
ya = HCDAa(r) =
N
∑
n=
θ[n]HCD{ϕn(r)} ≡Hθ,
(.)
where the D-D operator H is commonly referred to as the system matrix. The system
matrix H is of dimension (MS) × N, and an element of H will be denoted by H[n,m]. Note
that the data vector ya ≠y, due to the fact that a finite-dimensional approximate object
representation was employed. In other words, ya represents an approximation of the mea-
sured pressure data, denoted by pa, or the corresponding approximate integrated pressure
data ga.
For the case where ya = pa, the system matrix H will be denoted as H(p) and its
elements are defined as
H(p)
[mS+s, n] = ∫Vdr ϕn(r)hmS+s(r) = D(p)
στ {pn(r, t)},
(.)
where hmS+s(r) is defined in > Eq. (.) and
pn(r, t) = ∫V dr ϕn(r)h(r,r; t).
(.)
> Equation (.) provides a clear two-step procedure for computing the system
matrix. First, pn(r, t) is computed. Physically, this represents the pressure data, in its con-
tinuous form, received by an ideal point transducer when the absorbing object corresponds
to ϕn(r). Secondly, a discretization operator is applied that samples the ideal data and
degrades it by the transducer response. Alternatively, the elements of the system matrix


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
can be measured experimentally by scanning an object whose form matches the expansion
functions through the object volume and recording the resulting pressure signal at each
transducer location r,m, for each value of n (location of expansion function), at time inter-
vals sΔT. For the case of spherical expansion elements, this approach was implemented in
[].
This two-step approach for determining H be formulated as
H = S ○H,
(.)
where, ‘○’ denotes an element-wise product. Each element of His defined as
H[mS+s,n] = pn(r,m, sΔT).
(.)
The MS × N matrix S can be interpreted as a sensitivity map, whose elements are defined
as
S[mS+s,n] = Dστ{pn(r, t)}
pn(r,m, sΔT) .
(.)
For the case where ya = ga, similar interpretations hold. The system matrix H will be
denoted as H(g), and its elements are defined as
H(g)
[mS+s,n] = D(g)
στ {gn(r, t)},
(.)
where,
gn(r, t) = πCpct
β
∫
t

dζ∫V dr ϕn(r)h(r,r; ζ).
(.)
...
Numerical Example: Impact of Representation Error on
Computed Pressure Data
Consider a uniform sphere of radius Rs = mm as the optical absorber (acoustic source).
Assuming Dirac delta (i.e., ideal) temporal and spatial sampling, the pressure data were
computed at a measurement location rmm away from the center of the sphere by use
of D-D and C-C imaging models. For the uniform sphere, the pressure waveform can be
computed analytically as
p(r, sΔT) = d
dt [
β
πCpct g(r, t)]∣
t=sΔT
=
⎧⎪⎪⎨⎪⎪⎩
βc

Cp∣r−rc∣(∣r−rc∣−csΔT),
if ∣csΔT −∣r−r∣∣≤Rs
,
otherwise
(.)
where, rc is the center of the spherical source, and ΔT is the sampling interval. As discussed
in > Sect. .., the pressure possesses an ‘N’ shape waveform as shown as the dashed red
curve in > Fig. -. Finite-dimensional object representations of the object were obtained
according to > Eq. (.) with ϕn(r) corresponding to the uniform spheres described in

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

39.35
41.35
43.35
45.35
−1
−0.5
0
0.5
1
Time (μs)
P
Analytical
2563
643
⊡Fig. -
Pressure data generated by continuous imaging model(red dash) and discrete imaging
model using × × voxels (blue solid) and × × voxels(green solid)
> Eq. (.). The expansion coefficients were computed according to > Eq. (.). Two
approximate object representations were considered. The first representation employed
N = spherical expansion functions of radius .mm, while the second employed
N = expansion functions of radius .mm. The resulting pressure signals are shown
as > Fig. -, where the speed of sound c= .mm/μs, and ΔT = .μs. As expected,
the error in the computed pressure data increases as the voxel size is increased. In prac-
tice, this error would represent a data inconsistency between the measured data and the
assumed D-D imaging model, which can result in image artifacts as demonstrated by the
example below.
..
Iterative Image Reconstruction
Once the system matrix H is determined, as described in the previous section, an estimate
of A(r) can be computed in two distinct steps. First, from knowledge of the measured data
and system matrix, > Eq. (.) is inverted to estimate the expansion coefficients θ. Sec-
ond, the estimated expansion coefficients are employed with > Eq. (.) to determine
the finite-dimensional approximation Aa(r). Each of steps introduces error into the final
estimate of A(r). In the first step, due to noise in the measured data ya, modeling errors
in H, and/or if H is not full rank, the true values coefficients θ cannot generally be deter-
mined. The estimated θ will therefore depended the definition of the approximate solution
and the particular numerical algorithm used to determine it. Even if θ could somehow be
determined exactly, the second step would introduce error due to the approximate finite-
dimensional representation of A(r) employed. This error is influenced by the choice of N
and ϕn(r), and is object dependent.


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
Due to the large size of H, iterative methods are often employed to estimate θ.
Iterative approaches offer a fundamental and flexible way to incorporate a prior informa-
tion regarding the object, to improve the accuracy of the estimated θ. A vast literature
on iterative image reconstruction methods exists [, , , ], which we leave to the
reader to explore. Examples of applications of iterative reconstruction methods in PAT
are described in references [, , , , , ]. A numerical example demonstrating how
object representation error can affect the accuracy of iterative image reconstruction is
provided next.
...
Numerical Example: Inﬂuence of Representation Error on
Image Accuracy
We assume focused transducers are employed that receive only acoustic pressure signals
transmitted from the imaging plane, and therefore the D spherical Radon transform
imaging model to a D circular mean model. A D phantom comprised of uniform
disks possessing different gray levels, radii, and locations, was assumed to represent A(r).
The radius of the phantom was .(arbitrary units). A finite-dimensional representa-
tion Aa(r) was formed according to > Eq. (.), with N = and ϕn(r) chosen
to be conventional pixels described by a D version of > Eq. (.). The expansion
coefficients θ[n] were computed by use of > Eq. (.).
> Figure -displays the com-
puted expansion coefficient vector θ that has been reshaped into a × for display
purposes.
A circular measurement aperture Ωof radius .that enclosed the object was
employed. At each of uniformly
spaced transducer locations, r,m, on the
0
0.25
0.5
0.75
1
⊡Fig. -
The D numerical phantom θ representing the object function A(r)
.

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

−0.2
0.2
0.6
1
a
b
c
−0.2
0.2
0.6
1
80
160
240
0.2
0.6
1
Pixel
A
⊡Fig. -
Images reconstructed by the least squares conjugate gradient algorithm from pressure data
obtained by (a) numerical imaging model and (b) analytical imaging model. (c) Vertical
proﬁles through the center of subﬁgure(a)(solid blue), subﬁgure(b)(solid green), and
> Fig. -(dashed red)


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
measurement circle, simulated pressure data pa were computed from the integrated data g
by use of the formula
pa[mS+s] =
β
πCpc
[ g[mS+s+]/(s + ) −g[mS+s−]/(s −)
ΔT
].
(.)
Two versions of the pressure data were computed, corresponding to the cases where g was
computed analytically or by use of the assumed D-D imaging model. These simulated pres-
sure data are denoted by panal y
a
and pnum
a
respectively. At each transducer location, 
temporal samples of p(r, t) were computed. Accordingly, the pressure vector pa was a
column vector of length × .
The conjugate gradient algorithm was employed to find the least squares estimate ˆθ,
ˆθ = arg min
θ ∥pa −Hθ∥,
(.)
where pa = panal y
a
or pnum
a
. For the noiseless data, the images reconstructed from panal y
a
and pnum
a
after iterations are shown as
> Fig. -a, b, respectively. The image recon-
structed from the data pnum
a
is free of significant artifacts and is nearly identical to the
original object. This is expected because the finite-dimensional object representation was
used to produce the simulated measurement data and establish the system matrix, and
therefore the system of equations in > Eq. (.) is consistent. Generating simulation
data in this way would constitute an “inverse crime.” Conversely, the image reconstructed
from the data panal y
a
contained high-frequency artifacts due to the fact that the system
of equations in > Eq. (.) is inconsistent. The error in the reconstructed images could
be minimized by increasing the dimension of the approximate object representation. This
simple example demonstrates the importance of carefully choosing a finite-dimensional
object representation in iterative image reconstruction.
.
Conclusions
Photoacoustic tomography is a rapidly emerging biomedical imaging modality that
possesses many challenges for image reconstruction. In this chapter, we have reviewed the
physical principles of PAT. Contrast mechanisms in PAT were discussed, and the imag-
ing models that relate the measured photoacoustic wavefields to the sought-after optical
absorption distribution were described in their continuous and discrete forms.
.
Cross-References
> Iterative Solution Methods
> Linear Inverse Problems
> Optical Imaging
> Tomography

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

References and Further Reading
. Anastasio MA, Zhang J, Modgil D, La Riviere PJ
() Application of inverse source concepts
to photoacoustic tomography.
Inverse Prob
():S–S
. Anastasio MA, Zhang J, Sidky EY, Zou Y, Xia D,
Pan X () Feasibility of half-data image recon-
struction in D reflectivity tomography with a
spherical aperture. IEEE Trans Med Imaging
:–
. Anastasio MA, Zhang J, Pan X () Image
reconstruction in thermoacoustic tomography
with compensation for acoustic heterogeneities.
In: SPIE, vol . SPIE, pp –
. Anastasio MA, Zhang J () Image reconstruc-
tion in photoacoustic tomography with truncated
cylindrical measurement apertures. In: Proceed-
ings of the SPIE conference, vol . p 
. Anastasio MA, Zhang J, Pan X () Image
reconstruction in thermoacoustic tomography
with compensation for acoustic heterogeneties.
In: Proceedings of the SPIE medical imaging con-
ference, vol . pp –
. Anastasio MA, Zhang J, Pan X, Zou Y, Keng G,
Wang LV () Half-time image reconstruction
in thermoacoustic tomography. IEEE Trans Med
Imaging :–
. Anastasio MA, Zou Y, Pan X () Reflectiv-
ity tomography using temporally truncated data.
In: IEEE EMBS/BMES conference proceedings,
vol . IEEE, pp –
. Axelsson O () Iterative solution methods.
Cambridge University Press, Cambridge
. Barrett H, Myers K () Foundations of image
science. Wiley series in pure and applied optics.
Wiley, Hoboken
. Beard PC, Laufer JG, Cox B, Arridge SR ()
Quantitative photoacoustic imaging: measure-
ment of absolute chromophore concentrations
for physiological and molecular imaging. In:
Wang LV (ed) Photoacoustic imaging and
spectroscopy. CRC Press, Boca Raton
. Bertero M, Boccacci P () Inverse problems in
imaging. Institute of Physics Publishing, Bristol
. Cheong W, Prahl S, Welch A () A review of
the optical properties of biological tissues. IEEE J
Quantum Electron :–
. Cox BT, Arridge SR, Kstli KP, Beard PC ()
Two-dimensional
quantitative
photoacoustic
image reconstruction of absorption distributions
in scattering media by use of a simple iterative
method. Appl Opt :–
. Devaney AJ () The inverse problem for ran-
dom sources. J Math Phys :–
. Devaney AJ () Inverse source and scatter-
ing problems in ultrasonics. IEEE T Son Ultrason
:–
. Diebold GJ () Photoacoustic monopole radi-
ation: waves from objects with symmetry in one,
two, and three dimension. In: Wang LV (ed)
Photoacoustic imaging and spectroscopy. CRC
Press, Boca Raton
. Diebold GJ, Sun T, Khan MI (Dec ) Photoa-
coustic monopole radiation in one, two, and three
dimensions. Phys Rev Lett ():–
. Diebold
GJ,
Westervelt
PJ
()
The
photoacoustic effect generated by a spher-
ical droplet in a fluid. J Acoust Soc Am
():–
. Ephrat P, Keenliside L, Seabrook A, Prato
FS,
Carson
JJL
()
Three-dimensional
photoacoustic imaging by sparse-array detection
and iterative image reconstruction. J Biomed
Opt (): 
. Esenaliev RO, Karabutov AA, Oraevsky AA
() Sensitivity of laser opto-acoustic imaging
in detection of small deeply embedded tumors.
IEEE J Sel Top Quantum Electron :–
. Fessler JA () Penalized weighted least-
squares reconstruction for positron emission
tomography.
IEEE
Trans
Med
Imaging
:–
. Fessler JA, Booth SD () Conjugate-gradient
preconditioning methods for shiftvariant PET
image reconstruction. IEEE Trans Image Process
():–
. Finch D, Haltmeier M, Rakesh () Inver-
sion of spherical means and the wave equation
in even dimensions. SIAM J Appl Math ():
–
. Finch D, Patch S, Rakesh () Determining a
function from its mean values over a family of
spheres. SIAM J Math Anal :–
. Haltmeier M, Scherzer O, Burgholzer P, Paltauf G
() Thermoacoustic computed tomography
with
large
planar
receivers.
Inverse
Prob
():–


Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
. Jin X, Wang LV () Thermoacoustic tomogra-
phy with correction for acoustic speed variations.
Phys Med Biol ():–
. Joines W, Jirtle R, Rafal M, Schaeffer D ()
Microwave
power
absorption
differences
between normal and malignant tissue. Radiat
Oncol Biol Phys :–
. Khokhlova TD, Pelivanov IM, Kozhushko VV,
Zharinov AN, Solomatin VS, Karabutov AA
()
Optoacoustic
imaging
of
absorbing
objects in a turbid medium: ultimate sensitivity
and application to breast cancer diagnostics.
Appl Opt ():–
. Köstli KP, Beard PC () Two-dimensional
photoacoustic imaging by use of fouriertrans-
form image reconstruction
and a detector
with an anisotropic response. Appl Opt ():
–
. Köstli KP, Frenz M, Bebie H, Weber HP ()
Temporal backward projection of optoacoustic
pressure transients using Fourier transform
methods. Phys Med Biol ():–
. Kruger R, Reinecke D, Kruger G () Thermoa-
coustic computed tomography-technical consid-
erations. Med Phys :–
. Kruger RA, Kiser WL, Reinecke DR, Kruger GA,
Miller KD () Thermoacoustic optical molec-
ular imaging of small animals. Mol Imaging
:–
. Kruger RA, Liu P, Fang R, Appledorn C ()
Photoacoustic ultrasound (PAUS) reconstruction
tomography. Med Phys :–
. Ku G, Fornage BD, Jin X, Xu M, Hunt KK,
Wang LV () Thermoacoustic and photoa-
coustic tomography of thick biological tissues
toward breast imaging. Technol Cancer Res Treat
:–
. Kuchment P, Kunyansky L () Mathematics
of thermoacoustic tomography. Eur J Appl Math
:–
. Kunyansky LA () Explicit inversion for-
mulae for the spherical mean radon transform.
Inverse Prob :–
. Langenberg KJ () Basic methods of tomog-
raphy and inverse problems. Adam Hilger,
Philadelphia
. Lewitt RM () Alternatives to voxels for image
representation in iterative reconstruction algo-
rithms. Phys Med Biol ():–
. Li C, Pramanik M, Ku G, Wang LV () Image
distortion in thermoacoustic tomography caused
by microwave diffraction. Phys Rev E Stat Non-
linear Soft Matter Phys ():
. Li C, Wang LV () Photoacoustic tomogra-
phy and sensing in biomedicine. Phys Med Biol
():R–R
. Maslov K, Wang LV () Photoacoustic imag-
ing of biological tissue with intensitymodu-
lated continuous-wave laser. J Biomeded Opt
():
. Wernick MN, Aarsvold JN () Emission
tomography, the fundamentals of PET and
SPECT. Elsevier, San Diego
. Modgil D, Anastasio MA, Wang K, LaRiv-
ière PJ() Image reconstruction in pho-
toacoustic tomography with variable speed of
sound using a higher order geometrical acoustics
approximation. In: SPIE, vol . p A
. Norton S, Linzer M () Ultrasonic reflectiv-
ity imaging in three dimensions: Exact inverse
scattering solutions for plane, cylindrical, and
spherical apertures. IEEE Trans Biomed Eng :
–
. Oraevsky AA, Jacques SL, Tittel FK ()
Measurement of tissue optical properties by
time-resolved detection of laser-induced tran-
sient stress. Appl Opt :–
. Oraevsky AA, Karabutov AA () Ultimate
sensitivity of time-resolved optoacoustic detec-
tion. In: SPIE, vol . pp –
. Oraevsky
AA,
Karabutov
AA
()
Optoacoustic
tomography.
In:
Vo-Dinh
T
(ed) Biomedical photonics handbook. CRC
Press, Boca Raton
. Paltauf G, Nuster R, Burgholzer P ()
Characterization
of
integrating
ultrasound
detectors for photoacoustic tomography. J Appl
Phys ():
. Paltauf
G,
Schmidt-Kloiber
H,
Guss
H
()
Light
distribution
measurements
in
absorbing materials by optical detection of
laser-induced stress waves. Appl Phys Lett ():
–
. Paltauf G, Viator J, Prahl S, Jacques S ()
Iterative reconstruction algorithm for optoacous-
tic imaging. J Acoust Soc Am :–
. Pan X, Zou Y, Anastasio MA () Data
redundany and reduced-scan reconstruction in

Photoacoustic and Thermoacoustic Tomography: Image Formation Principles 

reflectivity tomography. IEEE Trans Image Pro-
cess :–
. Patch SK () Thermoacoustic tomography—
consistency conditions and the partial scan prob-
lem. Phys Med Biol ():–
. Provost J, Lesage F () The application
of
compressed
sensing
for
photo-acoustic
tomography.
IEEE
Trans
Med
Imaging
:–
. La Riviere PJ, Zhang J, Anastasio MA ()
Image reconstruction in optoacoustic tomog-
raphy for dispersive acoustic media. Opt Lett
:–
. Sushilov NV, CobboldSC (Apr ) Frequency-
domain wave equation and its timedomain solu-
tions in attenuating media. J Acoust Soc Am
():–
. Tam AC () Application of photo-acoustic
sensing techniques. Rev Mod Phys :–
. Wang LV (ed) () Photoacoustic imaging and
spectroscopy. CRC Press, Boca Raton
. Wang LV, Wu H-I () Biomedical optics, prin-
ciples and imaging. Wiley, Hoboken
. Wang LV, Zhao XM, Sun HT, Ku G ()
Microwave-induced acoustic imaging of biolog-
ical tissues. Rev Sci Instrum :–
. Wang X, Xie X, Ku G, Wang LV, Stoica G ()
Noninvasive imaging of hemoglobin concentra-
tion and oxygenation in the rat brain using high-
resolution photoacoustic tomography. J Biomed
Opt ():
. Wang Y, Xie X, Wang X, Ku G, Gill KL, ONeal DP,
Stoica G, Wang LV () Photoacoustic tomog-
raphy of a nanoshell contrast agent in the in vivo
rat brain. Nano Lett :–
. Xu M, Wang LV () Time-domain recon-
struction for thermoacoustic tomography in a
spherical geometry. IEEE Trans Med Imaging
:–
. Xu M, Wang LV () Analytic explanation
of spatial resolution related to bandwidth and
detector aperture size in thermoacoustic or
photoacoustic reconstruction. Phys Rev E :

. Xu M, Wang L () Universal back-projection
algorithm
for
photoacoustic
computed
tomography. Phys Rev E :
. Xu
M,
Wang
LV
()
Biomedical
photoacoustics. Rev Sci Instrum :
. Xu Y, Feng D, Wang LV () Exact frequency-
domain
reconstruction
for
thermoacoustic
tomography i: planar geometry. IEEE Trans Med
Imaging :–
. Xu Y, Wang LV () Effects of acoustic
heterogeneity in breast thermoacoustic tomogra-
phy. IEEE Trans Ultrason Ferroelectr Freq Con-
trol :–
. Xu Y, Xu M, Wang LV () Exact frequency-
domain
reconstruction
for
thermoacoustic
tomography-ii: cylindrical geometry. IEEE Trans
Med Imaging :–
. Yuan
Z,
Jiang
H
()
Quantitative
photoacoustic tomography: Recovery of optical
absorption coefficient maps of heterogeneous
media. Appl Phys Lett ():
. Zhang J, Anastasio MA, Pan X, Wang LV ()
Weighted expectation maximization reconstruc-
tion algorithms for thermoacoustic tomography.
IEEE Trans Med Imaging :–
. Zou Y, Pan X, Anastasio MA () Data trun-
cation and the exterior reconstruction problem
in reflection-mode tomography. In: IEEE nuclear
science symposium conference record, vol .
IEEE, pp –


Mathematics of
Photoacoustic and
Thermoacoustic
Tomography
Peter Kuchment ⋅Leonid Kunyansky
.
Introduction......................................................................
.
Mathematical Models of TAT...................................................
..
Point Detectors and the Wave Equation Model....................................
..
Acoustically Homogeneous Media and Spherical Means..........................
..
Main Mathematical Problems Arising in TAT......................................
..
Variations on the Theme: Planar, Linear, and Circular
Integrating Detectors...................................................................
.
Mathematical Analysis of the Problem.........................................
..
Uniqueness of Reconstruction........................................................
...
Acoustically Homogeneous Media...................................................
...
Acoustically Inhomogeneous Media..................................................
..
Stability...................................................................................
..
Incomplete Data.........................................................................
...
Uniqueness of Reconstruction........................................................
...“Visible” (“audible”) Singularities.....................................................
...Stability of Reconstruction for Incomplete Data Problems.......................
..
Discussion of the Visibility Condition...............................................
...Visibility for Acoustically Homogeneous Media...................................
...Visibility for Acoustically Inhomogeneous Media.................................
..
Range Conditions.......................................................................
...
The Range of the Spherical Mean Operator M....................................
...The Range of the Forward Operator W.............................................
..
Reconstruction of the Speed of Sound...............................................
.
Reconstruction Formulas, Numerical Methods, and Case Examples........
..
Full Data (Closed Acquisition Surfaces).............................................
...
Constant Speed of Sound..............................................................
...Variable Speed of Sound...............................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Mathematics of Photoacoustic and Thermoacoustic Tomography
..
Partial (Incomplete) Data..............................................................
...Constant Speed of Sound..............................................................
...Variable Speed of Sound...............................................................
.
Final Remarks and Open Problems............................................
.
Cross-References.................................................................

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Abstract: The chapter surveys the mathematical models, problems, and algorithms of the
thermoacoustic tomography (TAT) and photoacoustic tomography (PAT). TAT and PAT
represent probably the most developed of the several novel “hybrid” methods of medical
imaging. These new modalities combine different physical types of waves (electromag-
netic and acoustic in case of TAT and PAT) in such a way that the resolution and contrast
of the resulting method are much higher than those achievable using only acoustic or
electromagnetic measurements.
.
Introduction
We provide here just a very brief description of the thermoacoustic tomography/photoa-
coustic tomography (TAT/PAT) procedure, since the relevant physics and biology details
can be found in another chapter [] in this volume, as well as in the surveys and books
[,]. In TAT (PAT), a short pulse of radio-frequency EM wave (correspondingly, laser
beam) irradiates a biological object (e.g., in the most common application, human breast),
thus causing small levels of heating. The resulting thermoelastic expansion generates a
pressure wave that starts propagating through the object. The absorbed EM energy and
the initial pressure it creates are much higher in the cancerous cells than in healthy tis-
sues (see the discussion of this effect in [–]). Thus, if one could reconstruct the initial
pressure f (x), the resulting TAT tomogram would contain highly useful diagnostic infor-
mation. The data for such a reconstruction are obtained by measuring time-dependent
pressure p(x, t) using acoustic transducers located on a surface S (we will call it the obser-
vation or acquisition surface) completely or partially surrounding the body (see > Fig. -).
Thus, although the initial irradiation is electromagnetic, the actual reconstruction is based
on acoustic measurements. As a result, the high contrast is produced due to a much
higher absorption of EM energy by cancerous cells (ultrasound alone would not pro-
duce good contrast in this case), while the good (submillimeter) resolution is achieved
by using ultrasound measurements (the radio-frequency EM waves are too long for high-
resolution imaging). Thus, TAT, by using two types of waves, combines their advantages,
while eliminating their individual deficiencies.
The physical principle upon which TAT/PAT is based was discovered by Alexander
Graham Bell in [] and its application for imaging of biological tissues was suggested
a century later []. It began to be developed as a viable medical imaging technique in the
middle of the s [,].
Some of the mathematical foundations of this imaging modality were originally devel-
oped starting in the s for the purposes of the approximation theory, integral geometry,
and sonar and radar (see [,,,,] for references and extensive reviews of the result-
ing developments). Physical, biological, and mathematical aspects of TAT/PAT have been
recently reviewed in [,,,,,,,,].
TAT/PAT is just one, probably the most advanced at the moment, example of the several
recently introduced hybrid imaging methods, which combine different types of radiation
to yield high quality of imaging unobtainable by single-radiation modalities (see [,,,
,] for other examples).


Mathematics of Photoacoustic and Thermoacoustic Tomography
⊡Fig. -
Thermoacoustic tomography/photoacoustic tomography (TAT/PAT) procedure with a
partially surrounding acquisition surface
.
Mathematical Models of TAT
In this section, we describe the commonly accepted mathematical model of the TAT pro-
cedure and the main mathematical problems that need to be addressed. Since for all our
purposes PAT results in the same mathematical model (although the biological features
that TAT and PAT detect are different; see details in Ref. []), we will refer to TAT only.
..
Point Detectors and the Wave Equation Model
We will mainly assume that point-like omnidirectional ultrasound transducers, located
throughout an observation (acquisition) surface S, are used to detect the values of the pres-
sure p(y, t), where y ∈S is a detector location and t ≥is the time of the observation.
We also denote by c(x) the speed of sound at a location x. Then, it has been argued, that
the following model describes correctly the propagating pressure wave p(x, t) generated
during the TAT procedure (e.g., [,,,,]):
{ptt = c(x)Δx p,
t ≥, x ∈ℝ
p(x,) = f (x), pt(x,) = .
(.)
Here f (x) is the initial value of the acoustic pressure, which one needs to find in order to
create the TAT image. In the case of a closed acquisition surface S, we will denote by Ω the
interior domain it bounds. Notice that in TAT the function f (x) is naturally supported
inside Ω. We will see that this assumption about the support of f sometimes becomes
crucial for the feasibility of reconstruction, although some issues can be resolved even if f
has nonzero parts outside the acquisition surface.
The data obtained by the point detectors located on a surface S are represented by the
function
g(y, t):= p(y, t)
for y ∈S, t ≥.
(.)
> Figure -illustrates the space-time geometry of (> .).

Mathematics of Photoacoustic and Thermoacoustic Tomography 

⊡Fig. -
The observation surface S and the domain Ω containing the object to be imaged
We will incorporate the measured data g into the system (> .), rewriting it as follows:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
ptt = c(x)Δx p,
t ≥, x ∈ℝ
p(x,) = f (x), pt(x,) = 
p∣S = g(y, t),
(y, t) ∈S × ℝ+.
(.)
Thus, the goal in TAT/PAT is to find, using the data g(y, t) measured by transducers,
the initial value f (x) at t = of the solution p(x, t) of (> .).
We will use the following notation:
Deﬁnition 
We will denote by W the forward operator
W : f (x) ↦g(y, t),
(.)
where f and g are described in (> .).
Remark 
•
The reader should notice that if a different type of detector is used, the system (> .)
will still hold, while the measured data will be represented differently from (> .)
(see > Sect. ..). This will correspondingly influence the reconstruction procedures.
•
We can consider the same problem in the space ℝn of any dimension, not just in D. This
is not merely a mathematical abstraction. Indeed, in the case of the so-called integrating
line detectors (> Sect. ..), one deals with the D situation.
..
Acoustically Homogeneous Media and Spherical
Means
If the medium being imaged is acoustically homogeneous (i.e., c(x) equals to a constant,
which we will assume to be equal to in appropriate units), as it is approximately the case
in breast imaging, one deals with the constant coefficient wave equation problem


Mathematics of Photoacoustic and Thermoacoustic Tomography
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
ptt = Δx p,
t ≥, x ∈ℝ
p(x,) = f (x), pt(x,) = 
p∣S = g(y, t),
(y, t) ∈S × ℝ+.
(.)
In this case, the well-known Poisson–Kirchhoff formulas [, Chap. VI, Sect. ., Formula
()] for the solution of the wave equation gives in D:
p(x, t) = a ∂
∂t (t(R f )(x, t)),
(.)
where
(R f )(x, r):= 
π ∫
∣y∣=
f (x + ry)dA(y)
(.)
is the spherical mean operator applied to the function f (x), dAis the standard area element
on the unit sphere in ℝ, and a is a constant. (Versions in all dimensions are known, see
(> .) and (> .).) One can derive from here that knowledge of the function g(x, t)
for x ∈S and all t ≥is equivalent to knowing the spherical mean R f (x, t) of the function
f for any points x ∈S and any t ≥. One thus needs to study the spherical mean operator
R : f →R f , or, more precisely, its restriction to the points x ∈S only, which we will denote
by M:
Mf (x, t) := 
π ∫
∣y∣=
f (x + ty)dA(y),
x ∈S, t ≥.
(.)
Due to the connection between the spherical mean operator and the wave equation, one
can choose to work with the former, and in fact many works on TAT do so. The spherical
mean operator M resembles the classical Radon transform, the common tool of computed
tomography [], which integrates functions over planes rather than spheres. This analogy
with Radon transform, although often purely ideological, rather than technical, provides
important intuition and frequently points in reasonable directions of study. However, when
the medium cannot be assumed to be acoustically homogeneous, and thus c(x) is not con-
stant, the relation between TAT and integral geometric transforms, such as the Radon and
spherical mean transforms to a large extent breaks down, and thus one has to work with
the wave equation directly.
In what follows, we will address both models of TAT (the PDE model and the integral
geometry model) and thus will deal with both forward operators W and M.
..
Main Mathematical Problems Arising in TAT
We now formulate a list of problems related to TAT, which will be addressed in detail in the
rest of the article. (This list is more or less standard for a tomographic imaging method.)

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Sufficiency of the data: The first natural question to ask is as follows: Is the data collected
on the observation surface S sufficient for the unique reconstruction of the initial pres-
sure f (x) in (> .)? In other words, is the kernel of the forward operator W zero? Or,
to put it differently, for which sets S ∈ℝthe data collected by transducers placed along
S determines f uniquely? Yet another interpretation of this question is through observ-
ability of solutions of the wave equation on the set S: does observation on S of a solution
of the problem (> .) determine the solution uniquely?
When the speed of sound is constant, and thus the spherical mean model applies, the
equivalent question is whether the operator M has zero kernel on an appropriate class
of functions (say, continuous functions with compact support).
As it is explained in [], the choice of precise conditions on the local function class,
such as continuity, is of no importance for the answer to the uniqueness question,
while behavior at infinity (e.g., compactness of support) is. So, without loss of gener-
ality, when discussing uniqueness, one can assume f (x) in ( > .) to be infinitely
differentiable.
Inversion formulas and algorithms: Since a practitioner needs to see the actual tomogram,
rather than just know its existence, the next natural question arises: If uniqueness the
data collected on S is established, what are the actual inversion formulas or algorithms?
Here again one can work with smooth functions, in the end extending the formulas by
continuity to a wider class.
Stability of reconstruction: If we can invert the transform and reconstruct f from the
data g, how stable is the inversion? The measured data are unavoidably corrupted by
errors, and stability means that small errors in the data lead to only small errors in the
reconstructed tomogram.
Incomplete data problems: What happens if the data is “incomplete,” for instance if one
can only partially surround the object by transducers? Does this lead to any specific
deterioration in the tomogram, and if yes, to what kind of deterioration?
Range descriptions: The next question is known to be important for analysis of tomo-
graphic problems: What is the range of the forward operator W : f ↦g that maps
the unknown function f to the measured data g? In other words, what is the space of all
possible “ideal” data g(t, y) collected on the surface S? In the constant speed of sound
case, this is equivalent to the question of describing the range of the spherical mean oper-
ator M in appropriate function spaces. Such ranges often have infinite co-dimensions,
and the importance of knowing the range of Radon type transforms for analyzing prob-
lems of tomography is well known. For instance, such information is used to improve
inversion algorithms, complete incomplete data, and discover and compensate for cer-
tain data errors (e.g., [,,,,] and references therein). In TAT, range descriptions
are also closely connected with the speed of sound determination problem listed next (see
> Sect. ..for a discussion of this connection).
Speed of sound reconstruction: As the reader can expect, reconstruction procedures
require the knowledge of the speed of sound c(x). Thus, the problem arises of
the recovery of c(x) either from an additional scan, or (preferably) from the same
TAT data.


Mathematics of Photoacoustic and Thermoacoustic Tomography
..
Variations on the Theme: Planar, Linear, and Circular
Integrating Detectors
In the described above most basic and well-studied version of TAT, one utilizes point-like
broadband transducers to measure the acoustic wave on a surface surrounding the object
of interest. The corresponding mathematical model is described by the system ( > .).
In practice, the transducers cannot be made small enough, since smaller detectors yield
weaker signals resulting in low signal-to-noise ratios. Smaller transducers are also more
difficult to manufacture.
Since finite size of the transducers limits the resolution of the reconstructed images,
researchers have been trying to design alternative acquisition schemes using receivers that
are very thin but long or wide. Such are D planar detectors [, ] and D linear and
circular [,,,] detectors.
We will assume throughout this section that the speed of sound c(x) is constant and
equal to .
Planar detectors are made from a thin piezoelectric polymer film glued onto a flat
substrate (see, e.g., []). Let us assume that the object is contained within the sphere
of radius R. If the diameter of the planar detector is sufficiently large (see [] for details),
it can be assumed to be infinite. The mathematical model of such an acquisition technique
is no longer described by ( > .). Let us define the detector plane Π(s, ω) by equation
x ⋅ω = s, where ω is the unit normal to the plane and s is the (signed) distance from the ori-
gin to the plane. Then, while the propagation of acoustic waves is still modeled by (> .),
the measured data gplanar(s, t, ω) (up to a constant factor which we will, for simplicity,
assume to be equal to ) can be represented by the following integral:
gplanar(s, ω, t) = ∫
Π(s,ω)
p(x, t)dA(x)
where dA(x) is the surface measure on the plane. Obviously,
gplanar(s, ω,) = ∫
Π(s,ω)
p(x,)dA(x) = ∫
Π(s,ω)
f (x)dA(x) ≡F(s, ω),
i.e., the value of g at t = coincides with the integral F(s, ω) of the initial pressure f (x)
over the plane Π(s, ω) orthogonal to ω.
One can show [, ] that for a fixed ω, function gplanar(s, ω, t) is the solution to D
wave equation
∂g
∂s= ∂g
∂t,
and thus
gplanar(s, ω, t) = 
[gplanar(s, ω, s −t) + gplanar(s, ω, s + t)]
= 
[F(s + t, ω) + F(s −t, ω)].

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Since the detector can only be placed outside the object, i.e., s ≥R, the term F(s + t, ω)
vanishes, and one obtains
gplanar(s, ω, t) = F(s −t, ω).
In other words, by measuring gplanar(s, ω, t), one can obtain values of the planar integrals
of f (x). If, as proposed in [,], one conducts measurements for all planes tangent to the
upper half-sphere of radius R (i.e., s = R, ω ∈S
+), then the resulting data yield all values of
the standard Radon transform of f (x). Now the reconstruction can be carried out using
one of the many known inversion algorithms for the latter transform [].
Linear detectors are based on optical detection of acoustic signal. Some of the pro-
posed optical detection schemes utilize as the sensitive element a thin straight optical fiber
in combination with Fabry–Perot interferometer [,]. Changes of acoustic pressure on
the fiber change (proportionally) its length; this elongation, in turn, is detected by interfer-
ometer. A similar idea is used in []; in this work the role of a sensitive element is played
by a laser beam passing through the water in which the object of interest is submerged,
and thus the measurement does not perturb the acoustic wave. In both cases, the length
of the sensitive element exceeds the size of the object, while the diameter of the fiber (or
of the laser beam) can be made extremely small (see [] for a detailed discussion), which
removes restrictions on resolution one can achieve in the images.
Let us assume that the fiber (or laser beam) is aligned along the line l(s, s, ω, ω) =
{x∣x = sω+ sω+ sω}, where vectors ω, ω, and ω form an orthonormal basis in
ℝ. Then the measured quantities glinear(s, s, ω, ω, t) are equal (up to a constant factor
which, we will assume, equals to ) to the following line integral:
glinear(s, s, ω, ω, t) = ∫
ℝ
p(sω+ sω+ sω, t)ds.
Similar to the case of planar detection, one can show [,,], that for fixed vectors ω, ω
the measurements glinear(s, s, ω, ω, t) satisfy the D wave equation
∂g
∂s

+ ∂g
∂s

= ∂g
∂t.
The initial values glinear(s, s, ω, ω,) coincide with the line integrals of f (x) along lines
l(s, s, ω, ω). Suppose one makes measurements for all values of s(τ), s(τ) corre-
sponding to a curve γ = {x∣x = s(τ)ω+ s(τ)ω, τ≤τ ≤τ} lying in the plane spanned
by ω, ω. Then one can try to reconstruct the initial value of g from the values of g on γ.
This problem is a D version of (> .) and thus the known algorithms (see > Sect. .)
are applicable.
In order to complete the reconstruction from data obtained using line detectors, the
measurements should be repeated with different directions of ω. For each value of ω the
D problem is solved; the solutions of these problems yield values of line integrals of f (x).
If this is done for all values of ω lying on a half circle, the set of the recovered line integrals
of f (x) is sufficient for reconstructing this function. Such a reconstruction represents the


Mathematics of Photoacoustic and Thermoacoustic Tomography
inversion of the well known in tomography X-ray transform. The corresponding theory
and algorithms can be found, for instance, in [].
Finally, the use of circular integrating detectors was considered in []. Such a detector
can be made out of optical fiber combined with an interferometer. In [], a closed-
form solution of the corresponding inverse problem is found. However, this approach
is very new and neither numerical examples, nor reconstructions from real data have
been obtained yet.
.
Mathematical Analysis of the Problem
In this section, we will address most of the issues described in > Sect. .., except the
reconstruction algorithms, which will be discussed in > Sect. ..
..
Uniqueness of Reconstruction
The problem discussed here is the most basic one for tomography: Given an acquisition
surface S along which we distribute detectors, is the data g(y, t) for y ∈S, t ≥(see
(> .)) sufficient for a unique reconstruction of the tomogram f ? A simple counting of
variables shows that S should be a hypersurface in the ambient space (i.e., a surface in ℝor
a curve in ℝ). As we will see below, although there are some simple counterexamples and
remaining open problems, for all practical purposes, the uniqueness problem is positively
resolved, and most surfaces S do provide uniqueness. We address this issue for acoustically
homogeneous media first and then switch to the variable speed case.
Before doing so, however, we would like to dispel a concern that arises when one looks
at the problem of recovering f from g in (> .). Namely, an impression might arise that
we consider an initial-boundary value (IBV) problem for the wave equation in the cylinder
Ω × ℝ+, and the goal is to recover the initial data f from the known boundary data g. This
is clearly impossible, since according to standard PDE theorems (e.g., []), one can solve
this IBV problem for arbitrary choice of the initial data f and boundary data g (as long as
they satisfy simple compatibility conditions, which are fulfilled for instance if f vanishes
near S and g vanishes for small t, which is the case in TAT). This means that apparently g
contains essentially no information about f at all. This argument, however, is flawed, since
the wave equation in ( > .) holds in the whole space, not just in Ω. In other words, S
is not a boundary, but rather an observation surface. In particular, considering the wave
equation in the exterior of S, one can derive that if f is supported inside Ω, the boundary
values g of the solution p of ( > .) also determine the normal derivative of p at S for
all positive times. Thus, we in fact have (at least theoretically) the full Cauchy data of the
solution p on S, which should be sufficient for reconstruction. Another way of addressing
this issue is to notice that if the speed of sound is constant, or at least non-trapping (see
the definition below in > Sect. ...), the energy of the solution in any bounded domain

Mathematics of Photoacoustic and Thermoacoustic Tomography 

(in particular, in Ω) must decay in time. The decay when t →∞together with the boundary
data g guarantees the uniqueness of solution, and thus uniqueness of recovery f .
These arguments, as the reader will see, play a role in understanding reconstruction
procedures.
...
Acoustically Homogeneous Media
We assume here the sound speed c(x) to be constant (in appropriate units, one can choose
it to be equal to , which we will do to simplify considerations).
In order to state the first important result on uniqueness, let us recall the system
(> .), allowing an arbitrary dimension n of the space:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
ptt = Δx p,
t ≥, x ∈ℝn
p(x,) = f (x), pt(x,) = 
p∣S = g(y, t),
(y, t) ∈S × ℝ+.
(.)
We introduce the following useful definition:
Deﬁnition 
A set S is said to be uniqueness set, if when used as the acquisition surface,
it provides sufficient data for unique reconstruction of the compactly supported tomogram f
(i.e., the observed data g in (> .) determines uniquely function f ). Otherwise, it is called
a non-uniqueness set.
In other words, S is a uniqueness set if the forward operator W (or, equivalently, M) has
zero kernel.
We have not indicated above the smoothness class of f (x). However, it is not hard to
show (e.g., []) that the uniqueness does not depend on smoothness of f ; for simplicity,
the reader can assume that f is infinitely differentiable. On the other hand, compactness
of support is important in what follows.
We will start with a very general statement about the acquisition (observation) sets S
that provide insufficient information for unique reconstruction of f (see [] for the proof
and references):
Theorem 
If S is a non-uniqueness set, then there exists a nonzero harmonic polyno-
mial Q, which vanishes on S.
This theorem implies, in particular, that all “bad” (non-uniqueness) observation sets
are algebraic, i.e., have a polynomial vanishing on them. Turning this statement around,
we conclude that any set S that is a uniqueness set for harmonic polynomials, is sufficient
for unique TAT reconstruction (although, as we will see in > Sect. .., this does not
mean practicality of the reconstruction).
The proof of Theorem , which the reader can find in [, ], is not hard and in fact is
enlightening, but providing it would lead us too far from the topic of this survey.


Mathematics of Photoacoustic and Thermoacoustic Tomography
We will consider first the case of closed acquisition surfaces, i.e., the ones that
completely surround the object to be imaged. We will address the general situation
afterward.
Closed Acquisition Surfaces S
Theorem 
([]) If the acquisition surface S is the boundary of bounded domain Ω (i.e., a
closed surface), then it is a uniqueness set. Thus, the observed data g in (> .) determines
uniquely the sought function f ∈L
comp(ℝn). (The statement holds, even though f is not
required to be supported inside S.)
Proof
Indeed, since there are no nonzero harmonic functions vanishing on a closed
surface S, Theorem implies Theorem .
∎
There is, however, another, more intuitive, explanation of why Theorem holds true
(although it requires somewhat stronger assumptions, or a more delicate proof than the
one indicated below). Namely, since the solution p of ( > .) has compactly supported
initial data, its energy is decaying inside any bounded domain, in particular inside Ω (see
> Sect. ...and [,] and references therein about local energy decay). On the other
hand, if there is non-uniqueness, there exists a nonzero f such that g(y, t) = for all y ∈S
and t. This means that we can add homogeneous Dirichlet boundary conditions p ∣S= 
to (> .). But then the standard PDE theorems [] imply that the energy stays constant
in Ω. Combination of the two conclusions means that p is zero in Ω for all times t. It is well
known [] that such a solution of the wave equation must be identically zero everywhere,
and thus f = .
This energy decay consideration can be extended to some classes of non-compactly
supported functions f of the Lp classes, leading to the following result of []:
Theorem 
[] Let S be the boundary of a bounded domain in ℝn and f ∈Lp(ℝn). Then
. If p ≤
n
n−and the spherical mean of f over almost every sphere centered on S is equal to
zero, then f = .
. The previous statement fails when p >
n
n−and S is a sphere.
In other words, a closed surface S is a uniqueness set for functions f ∈Lp(ℝn) when p ≤
n
n−,
and might fail to be such when p >
n
n−.
This result shows that the assumption, if not necessarily of compactness of support of f ,
but at least of a sufficiently fast decay of f at infinity, is important for the uniqueness
to hold.
General Acquisition Sets S
Theorems and imply the following useful statement:

Mathematics of Photoacoustic and Thermoacoustic Tomography 

SN
⊡Fig. -
Coxeter cross of N lines
Theorem 
If a set S is not algebraic, or if it contains an open part of a closed analytic
surface Γ, then it is a uniqueness set.
Indeed, the first claim follows immediately from Theorem . The second one works out
as follows: if an open subset of an analytic surface Γ is a non-uniqueness set, then by an
analytic continuation type argument (see []), one can show that the whole Γ is such a set.
However, this is impossible, due to Theorem .
There are simple examples of non-uniqueness surfaces. Indeed, if S is a plane in D (or
a line in D, or a hyperplane in dimension n) and f (x) in ( > .) is odd with respect
to S, then clearly the whole solution of (> .) has the same parity and thus vanishes on S
for all times t. This means that, if one places transducers on a planar S, they might register
zero signals at all times, while the function f to be reconstructed is not zero. Thus, there
is no uniqueness of reconstruction when S is a plane. On the other hand (see [, ]), if
f is supported completely on one side of the plane S (the standard situation in TAT), it is
uniquely recoverable from its spherical means centered on S, and thus from the observed
data g.
The question arises what are other “bad” (non-uniqueness) acquisition surfaces than
planes. This issue has been resolved in D only. Namely, consider a set of N lines on the
plane intersecting at a point and forming at this point equal angles. We will call such a
figure the Coxeter cross ΣN (see > Fig. -). It is easy to construct a compactly supported
function that is odd simultaneously with respect of all lines in ΣN. Thus, a Coxeter cross is
also a non-uniqueness set. The following result, conjectured in [] and proven in the full
generality in [], shows that, up to adding finitely many points, this is all that can happen
to non-uniqueness sets:
Theorem 
[] A set S in the plane ℝis a non-uniqueness set for compactly supported
functions f , if and only if it belongs to the union ΣN ⋃Φ of a Coxeter cross ΣN and a finite
set of points Φ.


Mathematics of Photoacoustic and Thermoacoustic Tomography
⊡Fig. -
The conjectured structure of a most general non-uniqueness set in D
Again, compactness of support is crucial for the proof provided in []. There are no
other proofs known at the moment of this result (see the corresponding open problem in
> Sect. .). In particular, there is no proven analog of Theorem for non-closed sets S
(unless S is an open part of a closed analytic surface).
The n-dimensional (in particular, D) analog of Theorem has been conjectured [],
but never proven, although some partial advances in this direction have been made in
[,].
Conjecture 
A set S in ℝn is a non-uniqueness set for compactly supported functions f ,
if and only if it belongs to the union Σ ⋃Φ, where Σ is the cone of zeros of a homogeneous
(with respect to some point in ℝn) harmonic polynomial, and Φ is an algebraic subset of ℝn
of dimension at most n −(see > Fig. -).
Uniqueness Results for a Finite Observation Time
So far, we have addressed only the question of uniqueness of reconstruction in the non-
practical case of the infinite observation time. There are, however, results that guarantee
uniqueness of reconstruction for a finite time of observation. The general idea is that it is
sufficient to observe for the time that it takes the geometric rays (see > Sect. ...) from
the interior Ω of S to reach S. In the case of a constant speed, which we will assume to
be equal to , the rays are straight and are traversed with the unit speed. This means that
if D is the diameter of Ω (i.e., the maximal distance between two points in the closure of
Ω), then after time t = D, all rays coming from Ω have left the domain. Thus, one hopes
that waiting till time t = D might be sufficient. In fact, due to the specific initial conditions
in ( > .), namely, that the time derivative of the pressure is equal to zero at the initial
moment, each singularity of f emanates two rays, and at least one of them will reach S in
time not exceeding D/. And indeed, the following result of [] holds:

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Theorem 
[] If S is smooth and closed surface bounding domain Ω and D is the
diameter of Ω, then the TAT data on S collected for the time ≤t ≤.D, uniquely
determines f .
Notice that a shorter collection time does not guarantee uniqueness. Indeed, if S is a
sphere and the observation time is less than .D, due to the finite speed of propagation,
no information from a neighborhood of the center can reach S during observation. Thus,
values of f in this neighborhood cannot be reconstructed.
...
Acoustically Inhomogeneous Media
We assume that the speed of sound is strictly positive, c(x) > c > , and such that c(x)−
has compact support, i.e., c(x) = for large x.
Trapping and Non-trapping
We will frequently impose the so-called non-trapping condition on the speed of sound c(x)
in ℝn. To introduce it, let us consider the Hamiltonian system in ℝn
x,ξ with the Hamiltonian
H = c(x)

∣ξ∣:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
x′
t = ∂H
∂ξ = c(x)ξ
ξ′
t = −∂H
∂x = −
∇(c(x))∣ξ∣
x∣t== x,
ξ∣t== ξ.
(.)
The solutions of this system are called bicharacteristics and their projections into ℝn
x are
rays (or geometric rays).
Deﬁnition 
We say that the speed of sound c(x) satisfies the non-trapping condition, if
all rays with ξ≠tend to infinity when t →∞.
The rays that do not tend to infinity, are called trapped.
A simple example, where quite a few rays are trapped, is the radial parabolic sound
speed c(x) = c∣x∣.
It is well known (e.g., []) that singularities of solutions of the wave equation are car-
ried by geometric rays. In order to make this statement more precise, we need to recall the
notion of a wave front set WF(u) of a distribution u(x) in ℝn. This set carries detailed
information on singularities of u(x).
Deﬁnition 
Distribution u(x) is said to be microlocally smooth near a point (x, ξ),
where x, ξ∈ℝn and ξ≠, if there is a smooth “cut-off” function ϕ(x) such that ϕ(x) ≠
and that the Fourier transform ̂
ϕu(ξ) of the function ϕ(x)u(x) decays faster than any power
∣ξ∣−N when ∣ξ∣→∞, in directions that are close to the direction of ξ. (We remind the reader
that if this Fourier transform decays that way in all directions, then u(x) is smooth (infinitely
differentiable) near the point x).
The wave front set WF(u) ⊂ℝn
x × (ℝn
ξ /) of u consists of all pairs (x, ξ) such that u
is not microlocally smooth near (x, ξ).


Mathematics of Photoacoustic and Thermoacoustic Tomography
In other words, if (x, ξ) ∈WF(u), then u is not smooth near x, and the direction
of ξindicates why it is not: the Fourier transform does not decay well in this direction.
For instance, if u(x) consists of two smooth pieces joined non-smoothly across a smooth
interface Σ, then WF(u) can only contain pairs (x, ξ) such that x ∈Σ and ξ is normal to
Σ at x.
It is known that the wave front sets of solutions of the wave equation propagate with
time along the bicharacteristics introduced above. This is a particular instance of a more
general fact that applies to general PDEs and can be found in [, ]. As a result, if after
time T all the rays leave the domain Ω of interest, the solution becomes smooth (infinitely
differentiable) inside Ω.
The notion of so-called local energy decay, which we survey next, is important for the
understanding of the non-trapping conditions in TAT.
Local Energy Decay Estimates
Assuming that the initial data f (x) ( > .) is compactly supported and the speed c(x)
is non-trapping, one can provide the local energy decay estimates [,,]. Namely, in
any bounded domain Ω, the solution p(x, t) of (> .) satisfies, for a sufficiently large T
and for any (k, m), the estimate
∣∂k+∣m∣
∂k
t ∂mx
∣≤Ck,m
k(t)∥f ∥L,
for x ∈Ω, t > T.
(.)
Here
k(t) = t−n+−k for even n and
k(t) = e−δt for odd n and some δ > . Any value T
larger than the diameter of Ω works in this estimate.
Uniqueness Result for Non-trapping Speeds
If the speed is non-trapping, the local energy decay allows one to start solving the prob-
lem ( > .) from t = ∞, imposing zero conditions at t = ∞and using the measured
data g as the boundary conditions. This leads to recovery of the whole solution, and in
particular its initial value f (x). As the result, one obtains the following simple uniqueness
result of []:
Theorem 
[] If the speed c(x) is smooth and non-trapping and the acquisition surface
S is closed, then the TAT data g(y, t) determines the tomogram f (x) uniquely.
Notice that the statement of the theorem holds even if the support of f is not completely
inside of the acquisition surface S.
Uniqueness Results for Finite Observation Times
As in the case of constant coefficients, if the speed of sound is non-trapping, appropriately
long finite observation time suffices for the uniqueness. Let us denote by T(Ω) the supre-
mum of the time it takes the ray to reach S, over all rays originating in Ω. In particular, if
c(x) is trapping, T(Ω) might be infinite.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Theorem 
[] The data g measured till any time T larger than T(Ω) is sufficient for
unique recovery of f .
..
Stability
By stability of reconstruction of the TAT tomogram f from the measured data g we mean
that small variations of g in an appropriate norm lead to small variations of the recon-
structed tomogram f , also measured by an appropriate norm. In other words, small errors
in the data lead to small errors in the reconstruction.
We will try to give the reader a feeling of the general state of affairs with stability,
referring to the literature (e.g., [,,,,]) for further exact details.
We will consider as functional spaces the standard Sobolev spaces Hs of smoothness s.
We will also denote, as before, by W the operator transforming the unknown f into the
data g.
Let us recall the notions of Lipschitz and Hölder stability. An even weaker logarith-
mic stability will not be addressed here. The reader can find discussion of the general
stability notions and issues, as applied to inverse problems, in [].
Deﬁnition 
The operation of reconstructing f from g is said to be Lipschitz stable
between the spaces Hsand Hs, if the following estimate holds for some constant C:
∥f ∥Hs≤C∥g∥Hs.
The reconstruction is said to be Hölder stable (a weaker concept), if there are constants
s, s, s, C, μ > , and δ > such that
∥f ∥Hs≤C∥g∥μ
Hs
for all f such that ∥f ∥Hs≤δ.
Stability can be also interpreted in the terms of the singular values σj of the forward
operator f ↦g in L, which have at most power decay when j →∞. The faster is the
decay, the more unstable the reconstruction becomes. The problems with singular values
decaying faster than any power of j are considered to be extremely unstable. Even worse are
the problems with exponential decay of singular values (analytic continuation or solving
Cauchy problem for an elliptic operator belong to this class). Again, the book [] is a good
source for finding detailed discussion of such issues.
Consider as an example inversion of the standard in X-ray CT and MRI Radon
transform that integrates a function f over hyperplanes in ℝn. It smoothes function by
“adding (n −)/derivatives.” Namely, it maps continuously Hs-functions in Ω into the
Radon projections of class Hs+(n−)/. Moreover, the reconstruction procedure is Lipshitz
stable between these spaces (see [] for detailed discussion).
One should notice that since the forward mapping is smoothing (it “adds derivatives”
to a function), the inversion should produce functions that are less smooth than the data,


Mathematics of Photoacoustic and Thermoacoustic Tomography
which is an unstable operation. The rule of thumb is that the stronger is smoothing, the
less stable is inversion (this can be rigorously recast in the language of the decay of singular
values). Thus, problems that require reconstructing non-smooth functions from infinitely
differentiable (or even worse, analytic) data, are extremely unstable (with super-algebraic
or exponential decay of singular values correspondingly). This is just a consequence
of the standard Sobolev embedding theorems (see, e.g., how this applies in TAT case
in []).
In the case of a constant sound speed and the acquisition surface completely sur-
rounding the object, as we have mentioned before, the TAT problem can be recast
as inversion of the spherical mean transform M (see > Sect. .). Due to analogy
between the spheres centered on S and hyperplanes, one can conjecture that the Lip-
schitz stability of the inversion of the spherical mean operator M is similar to that
of the inversion of the Radon transform. This indeed is the case, as long as f is sup-
ported inside S, as has been shown in []. In the cases when closed-form inversion
formulas are available (see > Sect. ...), this stability can also be extracted from
them. If the support of f does reach outside, reconstruction of the part of f that
is outside is unstable (i.e., is not even Hölder stable, due to the reasons explained in
> Sect. ..).
In the case of variable non-trapping speed of sound c(x), integral geometry does
not apply anymore, and one needs to address the issue using, for instance, time reversal.
In this case, stability follows by solving the wave equation in reverse time starting from
t = ∞, as it is done in []. In fact, Lipschitz stability in this case holds for any obser-
vation time exceeding T(Ω) (see [], where microlocal analysis is used to prove this
result).
The bottom line is that TAT reconstruction is sufficiently stable, as long as the speed
of sound is non-trapping.
However, trapping speed does cause instability []. Indeed, since some of the rays are
trapped inside Ω, the information about some singularities never reaches S (no matter
for how long one collects the data), and thus, as it is shown in [], the reconstruction is
not even Hölder stable between any Sobolev spaces, and the singular values have super-
algebraic decay. See also > Sect. ..below for a related discussion.
..
Incomplete Data
In the standard X-ray CT, incompleteness of data arises, for instance, if not all projection
angles are accessible, or irradiation of certain regions is avoided, or as in the ROI (region
of interest) imaging, only the ROI is irradiated.
It is not that clear what incomplete data means in TAT. Usually one says that one
deals with incomplete TAT data, if the acquisition surface does not surround the object
of imaging completely. For instance, in breast imaging it is common that only a half-
sphere arrangement of transducers is possible. We will see, however, that incomplete data

Mathematics of Photoacoustic and Thermoacoustic Tomography 

effects in TAT can also arise due to trapping, even if the acquisition surface completely
surrounds the object.
The questions addressed here are the following:
. Is the collected incomplete data sufficient for unique reconstruction?
. If yes, does the incompleteness of the data have any effect on stability and quality of
the reconstruction?
...
Uniqueness of Reconstruction
Uniqueness of reconstruction issues can be considered essentially resolved for incom-
plete data in TAT, at least in most situations of practical interest. We will briefly survey
here some of the available results. In what follows, the acquisition surface S is not closed
(otherwise the problem is considered to have complete data).
Uniqueness for Acoustically Homogeneous Media
In this case, Theorem contains some useful sufficient conditions on S that guarantee
uniqueness. Microlocal results of [, , ], as well as the PDE approach of [] fur-
ther applied in [] provide also some other conditions. We assemble some of these in the
following theorem:
Theorem 
Let S be a non-closed acquisition surface in TAT. Each of the following con-
ditions on S is sufficient for the uniqueness of reconstruction of any compactly supported
function f from the TAT data collected on S:
. Surface S is not algebraic (i.e., there is no nonzero polynomial vanishing on S).
. Surface S is a uniqueness set for harmonic polynomials (i.e., there is no nonzero harmonic
polynomial vanishing on S).
. Surface S contains an open piece of a closed analytic surface Γ.
. Surface S contains an open piece of an analytic surface Γ separating the space ℝn such
that f is supported on one side of Γ.
. For some point y ∈S, the function f is supported on one side of the tangent plane
Ty to S at y.
For instance, if the acquisition surface S is just a tiny non-algebraic piece of a sur-
face, data collected on S determines the tomogram f uniquely. However, one realizes that
such data is unlikely to be useful for any practical reconstruction. Here the issue of sta-
bility of reconstruction kicks in, as it will be discussed in the stability subsection further
down.
Uniqueness for Acoustically Inhomogeneous Media
In the case of a variable speed of sound, there still are uniqueness theorems for partial
data [,], e.g.,


Mathematics of Photoacoustic and Thermoacoustic Tomography
Theorem 
[] Let S be an open part of the boundary ∂Ω of a strictly convex domain
Ω and the smooth speed of sound equals outside Ω. Then the TAT data collected on S for a
time T > T(Ω) determines uniquely any function f ∈H
(Ω), whose support does not reach
the boundary.
A modification of this result that does not require strict convexity is also available
in [].
While useful uniqueness of reconstruction results exist for incomplete data problems,
all such problems are expected to show instability. This issue is discussed in the subsec-
tions below. This will also lead to a better understanding of incomplete data phenomena
in TAT.
...
“Visible”(“audible”) Singularities
According to the discussion in > Sect. ..., the singularities (the points of the wave
front set WF(f ) of the function f in (> .)) are transported with time along the bichar-
acteristics ( > .). Thus, in the x-space they are transported along the geometric rays.
These rays may or may not reach the acquisition surface S, which triggers the introduction
of the following notion:
Deﬁnition 
A phase space point (x, ξ) is said to be “visible” (sometimes the word
“audible” is used instead), if the corresponding ray (see (> .)) reaches in finite time the
observation surface S.
A region U ⊂ℝn is said to be in the visibility zone, if all points (x, ξ) with x∈U are
visible.
An example of wave propagation through inhomogeneous medium is presented in
> Fig. -. The open observation surface S in this example consists of the two horizontal
and the left vertical sides of the square.
> Figure -a shows some rays that bend, due
to acoustic inhomogeneity, and leave through the opening of the observation surface S
(the right side of the square).
> Figure -b presents a flat phantom, whose wavefront
set creates these escaping rays, and thus is mostly invisible. Then > Fig. -c–f show the
propagation of the corresponding wave front.
Since the information about the horizontal boundaries of the phantom escapes, one
does not expect to reconstruct it well. > Figure -shows two phantoms and their recon-
structions from the partial data: (a–b) correspond to the vertical flat phantom, whose only
invisible singularities are at its ends. One sees essentially good reconstruction, with a little
bit of blurring at the endpoints. On the other hand, reconstruction of the horizontal phan-
tom with almost the whole wave front set invisible, does not work. The next
> Fig. -
shows a more complex square phantom, whose singularities corresponding to the horizon-
tal boundaries are invisible, while the vertical boundaries are fine. One sees clearly that the

Mathematics of Photoacoustic and Thermoacoustic Tomography 

⊡Fig. -
(a) Some rays starting along the interval x ∈[−., −.] in the vertical directions escape on
the right; (b) a ﬂat phantom with “invisible wavefront”; (c–f)propagation of the ﬂat front:
most of the energy of the signal leaves the square domain through the hole on the right
⊡Fig. -
Reconstruction with the same speed of sound as in > Fig. -: (a–b) phantom with strong
vertical fronts and its reconstruction; (c–d) phantom with strong horizontal fronts and its
reconstruction
invisible parts have been blurred away. On the other hand,
> Fig. -a in > Sect. .
shows that one can reconstruct an image without blurring and with correct values, if the
image is located in the visibility region. The reconstructed image in this figure is practically
indistinguishable from the phantom shown in > Fig. -a.


Mathematics of Photoacoustic and Thermoacoustic Tomography
⊡Fig. -
Reconstruction with the same speed of sound as in > Fig. -: (a) phantom; (b) its
reconstruction; (c) a magniﬁed fragment of (b)
Remark 
If S is a closed surface and xis a point outside of the convex hall of S, there is a
vector ξ≠such that (x, ξ) is “invisible.” Thus, the visibility zone does not reach outside
the closed acquisition surface S.
...
Stability of Reconstruction for Incomplete Data Problems
In all examples above, uniqueness of reconstruction held, but the images were still blurred.
The question arises whether the blurring of “invisible” parts is avoidable (after all, the
uniqueness theorems seem to claim that “everything is visible”). The answer to this is, in
particular, the following result of [], which is an analog of similar statements in X-ray
tomography:
Theorem 
[] If there are invisible points (x, ξ) in Ω ×(ℝn
ξ /), then inversion of the
forward operator W is not Hölder stable in any Sobolev spaces. The singular values σj of W
in Ldecay super-algebraically.
Thus, the presence of invisible singularities makes the reconstruction severely ill-posed.
In particular, according to Remark , this theorem implies the following statement:
Corollary 
Reconstruction of the parts of f (x) supported outside the closed observation
surface S is unstable.
On the other hand,
Theorem 
[] All visible singularities of f can be reconstructed with Lipschitz stability
(in appropriate spaces).
Such a reconstruction of visible singularities can be obtained in many ways, for instance
just by replacing the missing data by zeros (with some smoothing along the junctions with
the known data, in order to avoid artifact singularities). However, there is no hope for stable
recovery of the correct values of f (x), if there are invisible singularities.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

⊡Fig. -
Reconstruction from incomplete data using closed-form inversion formula in D; detectors
are located on the left half circle of radius .(a) phantom (b) reconstruction from complete
data (c) reconstruction from the incomplete data
..
Discussion of the Visibility Condition
...
Visibility for Acoustically Homogeneous Media
In the constant speed case, the rays are straight, and thus the visibility condition has a
simple test:
Proposition 
(e.g., [,,]) If the speed is constant, a point xis in the visible region,
if and only if any line passing through xintersects at least once the acquisition surface S (and
thus a detector location).
> Figure -illustrates this statement. It shows a square phantom and its reconstruc-
tion from complete data and from the data collected on the half-circle S surrounding the
left half of object. The parts of the interfaces where the normal to the interface does not
cross S are blurred.
...
Visibility for Acoustically Inhomogeneous Media
When the speed of sound is variable, an analog of Proposition holds, with lines replaced
by rays.
Proposition
(e.g., [,,]) A point xis in the visible region, if and only if for any ξ≠
at least one of the two geometric rays starting at (x, ξ) and at (x,−ξ) (see (> .))
intersects the acquisition surface S (and thus a detector location).
The reader can now see an important difference between the acoustically homogeneous
and inhomogeneous media. Indeed, even if S surrounds the support of f completely,
trapped rays will never find their way to S, which will lead, as we know by now, to
instabilities and blurring of some interfaces.


Mathematics of Photoacoustic and Thermoacoustic Tomography
⊡Fig. -
Reconstruction of a square phantom from full data in the presence of a trapping parabolic
speed of sound (the speed is radial with respect to the center of the picture): (a) an
oﬀ-center phantom; (b) its reconstruction; (c) a magniﬁed fragment of (b); (d) reconstruction
of a centered square phantom
Thus, presence of rays trapped inside the acquisition surface creates effects of incom-
plete data type. This is exemplified in > Fig. -with a square phantom and its reconstruc-
tion shown in the presence of a trapping (parabolic) speed. Notice that the square centered
at the center of symmetry of the speed is reconstructed very well (see > Fig. -d), since
none of the rays carrying its singularities is trapped.
..
Range Conditions
In this section we address the problem of describing the ranges of the forward operators
W (see ( > .)) and M (see ( > .)), the latter in the case of an acoustically homoge-
neous medium (i.e., for c = const). The ranges of these operators, similarly to the range
of the Radon and X-ray transforms (see []), are of infinite co-dimensions. This means
that ideal data g from a suitable function space satisfy infinitely many mandatory identi-
ties. Knowing the range is useful for many theoretical and practical purposes in various
types of tomography (reconstruction algorithms, error corrections, incomplete data com-
pletion, etc.), and thus this topic has attracted a lot of attention (e.g., [,,,,] and
references therein).
As we will see in the next section, range descriptions in TAT are also intimately related
to recovery of the unknown speed of sound.
We recall [,,] that for the standard Radon transform
f (x) →g(s, ω) = ∫
x⋅ω=s
f (x)dx,∣ω∣= ,
where f is assumed to be smooth and supported in the unit ball B = {x ∣∣x∣≤}, the range
conditions on g(s, ω) are
. smoothness and support: g ∈C∞
([−,] × S), where S is the unit sphere of vectors ω,
. evenness: g(−s,−ω) = g(s, ω),

Mathematics of Photoacoustic and Thermoacoustic Tomography 

. moment conditions: for any integer k ≥, the kth moment
Gk(ω) =
∞
∫
−∞
skg(ω, s)ds
extends from the unit sphere S to a homogeneous polynomial of degree k in ω.
The seemingly “trivial” evenness condition is sometimes the hardest to generalize to
other transforms of Radon type, while it is often easier to find analogs of the moment
conditions. This is exactly what happens in TAT.
For the operators W,M in TAT, some sets of range conditions of the moment type
had been discovered over the years [, , ], but complete range descriptions started to
emerge only since [,–,,,].
Range descriptions for the more general operator W are harder to obtain than for M,
and complete range descriptions are not known for even dimensions or for the case of the
variable speed of sound.
Let us address the case of the spherical mean operator M first.
...
The Range of the Spherical Mean Operator M
The support and smoothness conditions are not hard to come up with, at least when S is a
sphere. By choosing appropriate length scale, we can assume that the sphere is of radius 
and centered at the origin, and that the interior domain Ω is the unit ball B = {x ∣∣x∣= }.
If f is smooth and supported inside B (i.e., f ∈C∞
(B)), then it is clear that the measured
data satisfies the following:
Smoothness and support conditions:
g ∈C∞
(S × [,]).
(.)
An analog of the moment conditions for g(y, r) := Mf was implicitly present in [,]
and explicitly formulated as such in []:
Moment conditions: for any integer k ≥, the moment
Mk(y) =
∞
∫

rk+d−g(y, r)dr
(.)
extends from S to an (in general, nonhomogeneous) polynomial Qk(x) of degree at most k.
These two types of conditions happen to be incomplete, i.e., infinitely many others exist.
The Radon transform experience suggests to look for an analog of evenness conditions.
And indeed, a set of conditions called orthogonality conditions was found in [,,].


Mathematics of Photoacoustic and Thermoacoustic Tomography
Orthogonality conditions: Let −λ
k be the eigenvalue of the Laplace operator Δ in B with
zero Dirichlet conditions and ψk be the corresponding eigenfunctions. Then the following
orthogonality condition is satisfied:
∫
S×[,]
g(x, t)∂ψλ(x)jn/−(λt)tn−dxdt = .
(.)
Here jp(z) = cpz−pJp(z) is the so-called spherical Bessel function.
The range descriptions obtained in [, , ] demonstrated that these three types of
conditions completely describe the range of the operator M on functions f ∈C∞
(B). At
the same time, the results of [,] showed that the moment conditions can be dropped in
odd dimensions. It was then discovered in [] that the moment conditions can be dropped
altogether in any dimension, since they follow from the other two types of conditions:
Theorem 
[] Let S be the unit sphere. A function g(y, t) on the cylinder S × ℝ+ can be
represented as Mf for some f ∈C∞
(B) if an only if it satisfied the above smoothness and
support and orthogonality conditions (> .), (> .).
The statement also holds in the finite smoothness case, if one replaces the requirements by
f ∈Hs
(B) and g ∈Hs+(n−)/

(S × [,]).
The range of the forward operator M has not been described when S is not a sphere,
but, say, a convex smooth closed surface. The moment and orthogonality conditions hold
for any S, and appropriate smoothness and support conditions can also been formulated,
at least in the convex case. However, it has not been proven that they provide the complete
range description.
It is quite possible that for nonspherical S the moment conditions might have to be
included into the range description.
A different range description of the Fredholm alternative type was developed in []
(see also [] for description of this result).
...
The Range of the Forward Operator W
We recall that the operator W (see ( > .)) transforms the initial value f in ( > .)
into the observed on S values g of the solution. There exist Kirchhoff–Poisson formulas
representing the solution p, and thus g = W f in terms of the spherical means of f (i.e., in
terms of Mf ). However, translating the result of Theorem into the language of W is not
straightforward, since in even dimensions these formulas are nonlocal ([] p. ):
W f (y, t) =
√π
Γ(n/) (
t
∂
∂t)
(n−)/
tn−(Mf )(y, t), for odd n.
(.)

Mathematics of Photoacoustic and Thermoacoustic Tomography 

and
W f (y, t) =

Γ(n/) (
t
∂
∂t)
(n−)/
t
∫

rn−(Mf )(y, r)
√
t−r
dr, for even n.
(.)
The non-locality of the transformation for even dimensions reflects the absence of
Huygens’ principle (i.e., absence of sharp rear fronts of waves) in these dimensions; it also
causes difficulties in establishing the complete range descriptions. In particular, due to the
integration in (> .) Mf (y, t) does not vanish for large times t anymore. One can try to
use other known operators intertwining the two problems (see [] and references therein),
some of which do preserve vanishing for large values of t, but this so far has lead only to
very clumsy range descriptions.
However, for odd dimensions, the range description of W can be obtained. In order to
do so, given the TAT data g(y, t), let us introduce an auxiliary time-reversed problem in
the cylinder B × [,]:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
qtt −Δq = for (x, t) ∈B × [,]),
q(x,) = qt(x,) = for x ∈B,
q(y, t) = g(y, t) for (y, t) ∈S × [,]).
(.)
We can now formulate the range description from [,]:
Theorem 
[, ] For odd dimensions n and S being the unit sphere, a function g ∈
C∞
(S × [,]) can be represented as W f for some f ∈C∞
(B) if and only if the following
condition is satisfied:
The solution q of (> .) satisfies qt(x,) = for all x ∈B.
Orthogonality type and Fredholm alternative type range conditions, equivalent to the
one in the theorem above, are also provided in [,].
..
Reconstruction of the Speed of Sound
Unsurprisingly, all inversion procedures outlined in > Sect. .rely upon the knowledge
of the speed of sound c(x). Although often, e.g., in breast imaging, the medium is assumed
to be acoustically homogeneous, this is not a good assumption in many other cases. It has
been observed (e.g., [, ]) that replacing even slightly varying speed of sound with its
average value might significantly distort the image; not only the numerical values, but also
the shapes of interfaces between the tissues will be reconstructed incorrectly. Thus, the
question of estimating c(x) correctly becomes important. One possible approach [] is


Mathematics of Photoacoustic and Thermoacoustic Tomography
to use an additional transmission ultrasound scan to reconstruct the speed beforehand.
The question arises of whether one could determine the speed of sound c(x) and the
tomogram f (x) (assuming that f is not zero) simultaneously from the TAT data. In fact,
one needs only to determine c(x) (without knowing f ), since then inversion procedures
of > Sect. .would apply to recover f .
At the first glance, this seems to be an overly ambitious project. Indeed, if we denote
the forward operator W by Wc, to indicate its dependence on the speed of sound c(x),
then the problem becomes, given the data g, to find both c and f from the equality
Wc f = g.
(.)
A similar situation arises in the SPECT emission tomography (see [] and references
therein), where the role of the speed of sound is played by the unknown attenuation. It
is known, however, that in SPECT the attenuation can be recovered for a “generic” f .
What is the reason for such a strange situation? It looks like for any c one could solve
the > Eq. (.) for an f , and thus no information about c is contained in the data g. This
argument is incorrect for the following reason: the range of the forward operator, as we
know already from the previous section, has infinite co-dimension. Thus, this range has
a lot of space to “rotate” when c changes. Imagine for an instance that the rotation is so
powerful that for different values of c the ranges have only zero (the origin) in common.
Then, knowing g in the range, one would know which c it came from. Thus, the problem of
recovering the speed of sound from the TAT data is closely related to the range descriptions.
Numerical inversions using algebraic iterative techniques (e.g., [, ]) show that
recovering both c and f might be indeed possible.
Unfortunately, very little is known at the moment concerning this problem. Direct
usage of range conditions attempted in [] has led only to extremely weak and not prac-
tically useful results so far. A revealing relation to the transmission eigenvalue problem
well known in inverse problems (see [] for the survey) was recently discovered by D.
Finch. Unfortunately, the transmission eigenvalue problem remains still unresolved. How-
ever, one can derive from this relation the following result regarding uniqueness of the
reconstruction of the speed of sound, due to M. Agranovsky (a somewhat restricted version
is due to D. Finch et al., both unpublished):
Theorem 
If two speeds satisfy the inequality c(x) ≥c(x) for all x ∈Ω and produce
for some functions f, fthe same nonzero TAT data g (i.e., Wcf= g,Wcf= g), then
c(x) = c(x).
It is known [, Corollary ..] that if a function f (x) is such that Δf (x) ≠and for
two acoustic speeds c(x) and c(x) it produces the same TAT data g, then c= c.
It is clear that the problem of finding the speed of sound from the TAT data is still
mostly unresolved.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

.
Reconstruction Formulas, Numerical Methods,
and Case Examples
Numerous formulas, algorithms, and procedures for reconstruction of images from TAT
measurements have been developed by now. Most of these techniques require the data
being collected on a closed surface (closed curve in D) surrounding the object to be
imaged. Such methods are discussed in > Sect. ... We review methods that work under
the assumption of constant speed of sound in > Sect. .... The techniques applicable in
the case of the known variable speed of sound are considered in > Sect. .... Closed
surface measurements cannot always be implemented, since in some practical situations
the object cannot be completely surrounded by the detectors. In this case, one has to resort
to various approximate reconstruction techniques as discussed in > Sect. ...
..
Full Data (Closed Acquisition Surfaces)
...
Constant Speed of Sound
When the speed of sound within the tissues is a known constant, the TAT problem can
be reformulated (see > Sect. .) in terms of the values of the spherical means of the ini-
tial condition f (x). These means can be easily recovered from the measurements of the
acoustic pressure using formulas (> .) and (> .) (see the discussion in []). In this
case, image reconstruction becomes equivalent to inverting the spherical mean transform
M. Thus, in what follows, we consider the problem of reconstructing a function f (x) sup-
ported within the region bounded by a closed surface S from known values of its spherical
integrals g(y, r) with centers on S:
g(y, r) = ∫
𝕊n−
f (y + rω)rn−dω,
y ∈S,
(.)
where dω is the standard measure on the unit sphere.
Series Solutions for Spherical Geometry
The first inversion procedures for the case of closed acquisition surfaces were described
in [, ], where solutions were found for the cases of circular (in D) and spherical (in
D) surfaces, respectively. These solutions were obtained by the harmonic decomposition
of the measured data and of the sought function f (x), followed by equating coefficients
of the corresponding Fourier series. In particular, the D algorithm of [] pertains to the
case when the detectors are located on a circle of radius R. This method is based on the
Fourier decomposition of f and g in angular variables:
f (x) =
∞
∑
−∞
fk(ρ)eikφ,
x = (ρ cos(φ), ρ sin(φ))
(.)


Mathematics of Photoacoustic and Thermoacoustic Tomography
g(y(θ), r) =
∞
∑
−∞
gk(r)eikθ,
y = (R cos(θ), R sin(θ)),
where
(Hmu)(s) = π ∫
∞

u(t)Jm(st)tdt
is the Hankel transform and Jm(t) is the Bessel function. As shown in [], the Fourier
coefficients fk(ρ) can be recovered from the known coefficients gk(r) by the following
formula:
fk(ρ) = Hm (

Jk(λ∣R∣)H[ gk(r)
πr ]).
This method requires division of the Hankel transform of the measured data by the
Bessel functions Jk, which have infinitely many zeros. Theoretically, there is no prob-
lem: the range conditions (> Sect. ..) on the exact data g imply that the Hankel
transform H[(πr)−gk(r)] has zeros that cancel those in the denominator. However,
since the measured data always contain errors, the exact cancelation does not happen,
and one needs a sophisticated regularization scheme to guarantee that the error remains
bounded.
This difficulty can be avoided (see, e.g., []) by replacing the Bessel function Jin the
inner Hankel transform by the Hankel function H()
. This yields the following formula for
fk(ρ) :
fk(ρ) = Hk
⎛
⎝

H()
k (λ∣R∣) ∫
∞

gk(r)H()
(λr)dr⎞
⎠.
Unlike Jm, Hankel functions H()
m (t) do not have zeros for any real values of t, which
removes the problems with division by zeros []. (A different way of avoiding divisions
by zero was found in [].)
This derivation can be repeated in D, with the exponentials eikθ replaced by the
spherical harmonics, and with cylindrical Bessel functions replaced by their spherical
counterparts. By doing this, one arrives at the Fourier series method of [] (see also []).
The use of the Hankel function H()

above is similar to the way the spherical Hankel
function h()

is utilized in [] to avoid the divisions by zero.
Eigenfunction Expansions for a General Geometry
The series methods described in the previous section rely on the separation of variables that
occurs only in spherical geometry. A different approach was proposed in []. It works for
arbitrary closed surfaces, but is practical only for those with explicitly known eigenvalues
and eigenfunctions of the Dirichlet Laplacian in the interior. These include, in particular,
the surfaces of such bodies as spheres, half-spheres, cylinders, cubes and parallelepipeds,
as well as the surfaces of crystallographic domains.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

Let λ
m and um(x) be the eigenvalues and an orthonormal basis of eigenfunctions of
the Dirichlet Laplacian −Δ in the interior Ω of a closed surface S:
Δum(x) + λ
mum(x) = ,
x ∈Ω,
Ω ⊆ℝn,
(.)
um(x) = ,
x ∈S,
∣∣um∣∣
≡∫
Ω
∣um(x)∣dx = .
As before, one would like to reconstruct a compactly supported function f (x) from
the known values of its spherical integrals g(y, r) (see ( > .)) with centers on S.
Since um(x) is the solution of the Dirichlet problem for the Helmholtz equation with
zero boundary conditions and the wave number λm, this function admits the Helmholtz
representation
um(x) = ∫S Φλm(∣x −y∣) ∂
∂num(y)ds(y)
x ∈Ω,
(.)
where Φλm(∣x −y∣) is a free-space Green’s function of the Helmholtz equation (> .),
and n is the exterior normal to S.
The function f (x) can be expanded into the series
f (x) =
∞
∑
m=
αmum(x), where
(.)
αm = ∫Ω um(x)f (x)dx.
A reconstruction formula for αm (and thus for f (x)) will result, if one substitutes
representation (> .) into (> .) and interchanges the orders of integration:
αm = ∫Ω um(x)f (x)dx = ∫S I(y, λm) ∂
∂num(y)dA(x),
(.)
where
I(y, λ) = ∫Ω Φλ(∣x −y∣)f (x)dx = ∫
diam Ω

g(y, r)Φλ(r)dr.
(.)
Now f (x) can be obtained by summing the series (> .). This method becomes compu-
tationally efficient when the eigenvalues and eigenfunctions are known explicitly, especially
if a fast summation formula for the series (> .) is available. This is the case when the
acquisition surface S is the surface of a cube, and thus the eigenfunctions are products
of sine functions. The resulting D reconstruction algorithm is extremely fast and precise
(see []).
The above method has an interesting property. If the support of the source f (x) extends
outside Ω, the algorithm still yields theoretically exact reconstruction of f (x) inside Ω.
Indeed, the value of the expression (> .) for all x lying outside Ω is zero. Thus, when
one computes (> .) for x ∈ℝn/Ω, values of f (x) are multiplied by zero and do not
affect further computation in any way. This feature is shared by the time reversal method


Mathematics of Photoacoustic and Thermoacoustic Tomography
(see the corresponding paragraph in > Sect. ...). The closed-form FBP type recon-
struction techniques considered in the next subsection, do not have this property. In other
words, in presence of a source outside the measurement surface, reconstruction within Ω
can be incorrect.
The reason for this difference is that all currently known closed-form FBP-type for-l
mulas rely (implicitly or explicitly) on the assumption that the wave propagates outside S
in the whole free space and has no sources outside. On the other hand, the eigenfunction
expansion method and the time reversal rely only upon the time decay of the wave inside S,
which is not influenced by f having a part outside S.
Closed-Form Inversion Formulas
Closed-form inversion formulas play a special role in tomography. They bring about better
theoretical understanding of the problem and frequently serve as starting points for the
development of efficient reconstruction algorithms. A well-known example of the use of
explicit inversion formulas is the so-called filtered backprojection (FBP) algorithm in X-ray
tomography, which is derived from one of the inversion formulas for the classical Radon
transform (see, e.g., []).
The very existence of closed-form inversion formulas for TAT had been in doubt,
till the first such formulas were obtained in odd dimensions by Finch et al. in [],
under the assumption that the acquisition surface S is a sphere. Suppose that the func-
tion f (x) is supported within a ball of radius R and that the detectors are located on
the surface S = ∂B of this ball. Then some of the formulas obtained in [] read as
follows:
f (x) = −

πR Δx ∫
∂B
g(y,∣y −x∣)
∣y −x∣
dA(y),
(.)
f (x) = −

πR ∫
∂B
(
r
∂
∂rg(y, r))
111111111111111r=∣y−x∣
dA(y),
(.)
f (x) = −

πR ∫
∂B
(
r
∂
∂r (r ∂
∂r
g(y, r)
r
))
111111111111111r=∣y−x∣
dA(y),
(.)
where dA(y) is the surface measure on ∂B and g represents the values of the spherical
integrals (> .).
These formulas have a FBP (filtered backprojection) nature. Indeed, differentiation with
respect to r in (> .) and (> .) and the Laplace operator in (> .) represent the
filtration, while the (weighted) integrals correspond to the backprojection, i.e., integration
over the set of spheres passing through the point of interest x and centered on S.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

The so-called universal backprojection formula in D was found in [] (it is also
valid for the cylindrical and plane acquisition surfaces, see > Sect. ..). In our nota-
tion, this formula takes the form
f (x) =

πdiv ∫
∂B
n(y)(
r
∂
∂r
g(y, r)
r
)
111111111111111r=∣y−x∣
dA(y),
(.)
or, equivalently,
f (x) = −
π∫
∂B
∂
∂n (
r
∂
∂r
g(y, r)
r
)
111111111111111r=∣y−x∣
dA(y),
(.)
where n(y) is the exterior normal vector to ∂B. One can show [, , ] that formulas
(> .) through (> .) are not equivalent on non-perfect data: the result will differ if
these formulas are applied to a function that does not belong to the range of the spherical
mean transform M. A family of inversion formulas valid in ℝn for arbitrary n ≥was
found in []:
f (x) =

(π)n−div ∫
∂B
n(y)h(y,∣x −y∣)dA(y),
(.)
where
h(y, t) = ∫
ℝ+
Y(λt)
⎡⎢⎢⎢⎢⎣
R
∫

J(λr)g(y, r)dr−J(λt)
R
∫

Y(λr)g(y, r)dr
⎤⎥⎥⎥⎥⎦
λn−dλ, (.)
J(t) = Jn/−(t)
tn/−
,
Y(t) = Yn/−(t)
tn/−
,
(.)
and Jn/−(t) and Yn/−(t) are respectively the Bessel and Neumann functions of order
n/−. In D, J(t) and Y(t) are simply t−sin t and t−cos t and formulas (> .) and
(> .) reduce to (> .).
In D, > Eq. (.) also can be simplified [], which results in the formula
f (x) =

πdiv ∫
∂B
n(y)
⎡⎢⎢⎢⎢⎣
R
∫

g(y, r)

r−∣x −y∣dr
⎤⎥⎥⎥⎥⎦
dl(y),
(.)
where ∂B now stands for the circle of radius R and dl(y) is the standard arc length.
A different set of closed-form inversion formulas applicable in even dimensions was
found in []. Formula (> .) can be compared to the following inversion formulas
from []:
f (x) =

πR Δ ∫
∂B
R
∫

g(y, r)log(r−∣x −y∣) dr dl(y),
(.)


Mathematics of Photoacoustic and Thermoacoustic Tomography
or
f (x) =

πR ∫
∂B
R
∫

∂
∂r (r ∂
∂r
g(y, r)
r
)log(r−∣x −y∣) dr dl(y).
(.)
Finally, a unified family of inversion formulas was derived in []. In our notation, it
has the following form:
f (x) = −
πR ∫
∂B
( ∂
∂t Kn(y, t))
111111111111111t=∣x−y∣
< y −x, y −ξ >
∣x −y∣
dA(y),
(.)
Kn(y, t) = −

(π)n−∫
ℝ+
λn−Y(λt)
⎛
⎜
⎝∫
ℝ+
J(λr)g(y, r)dr
⎞
⎟
⎠
dλ
where ∂B is the surface of a ball in ℝn of radius R, functions J and Y are as in ( > .),
and ξ is an arbitrary fixed vector. In particular, in D
J(t) =
√

π
sin t
t
, J(t) =
√

π
cos t
t
and, after simple calculation, the above inversion formula reduces to
f (x) = −

πR ∫
∂B
( ∂
∂r

r
∂
∂r
g(y, r)
r
)
111111111111111r=∣x−y∣
< y −x, y −ξ >
∣x −y∣
dA(y).
(.)
Different choices of vector ξ in the above formula result in different inversion formulas. For
example, if ξ is set to zero, the ratio <y−x,y−ξ>
∣x−y∣
equals R cos α, where α is the angle between
the exterior normal n(y) and the vector y −x; when combined with the derivative in t
this factor produces the normal derivative, and the inversion formula (> .) reduces to
(> .). On the other hand, the choice of ξ = x in (> .) leads to a formula
f (x) = −

πR ∫
∂B
(r ∂
∂r

r
∂
∂r
g(y, r)
r
)
111111111111111r=∣x−y∣
dA(y),
which is reminiscent of formulas (> .)–(> .).
Greens’Formula Approach and Some Symmetry Considerations
Let us suppose for a moment that the acoustic detectors could measure not only the
pressure p(y, t) at each point of the acquisition surface S, but also the normal derivative
∂p/∂n on S. Then the problem of reconstructing the initial pressure f (x) becomes rather
simple. Indeed, one can use the knowledge of the free-space Green’s function for the wave
equation and invoke the Green’s theorem to represent the solution p(x, t) of (> .) in
the form of integrals over S involving p(x, t) and its normal derivative and the Green’s
function and its normal derivative. (This can be done in the Fourier or time domains.)
This would require infinite observation time, but in D the time T(Ω) will suffice, after

Mathematics of Photoacoustic and Thermoacoustic Tomography 

which the wave escapes the region of interest (a cutoff also would work approximately in D
similar to the time-reversal method). This Green’s function approach happens to be, explic-
itly or implicitly, the starting point of all closed-form inversions described above. The trick
is to rewrite the formula in such a way that the unknown in reality normal derivative ∂p/∂n
disappears from the formula.
This was achieved in [] by reducing the question to some integrals involving special
functions and making the key observation that the integral
Iλ(x, y) = ∫
∂B
J(λ∣x −z∣) ∂
∂nY(λ∣y −z∣)dA(z),
x, y ∈B ⊂ℝn
is a symmetric function of its arguments:
Iλ(x, y) = Iλ(y, x) for x, y ∈B ⊂ℝ.n
(.)
Similarly, the derivation of (> .) in [] employs the symmetry of the integral
Kλ(x, y) = ∫
∂B
J(λ∣x −z∣)Y(λ∣y −z∣)dA(z),
x, y ∈B ⊂ℝn.
In fact, the symmetry holds for any integral
Wλ(x, y) = ∫
∂B
U(λ∣x −z∣)V(λ∣y −z∣)dA(z),
x, y ∈B ⊂ℝn,
where U(λ∣x∣) and V(λ∣x∣) are any two radial solutions of Helmholtz equation
Δu(x) + λu(x) = .
(.)
It is straightforward to verify this symmetry when S is a sphere and B is the correspond-
ing ball, and the points x, y lie on the boundary S only, rather than anywhere in B. This
follows immediately from the rotational symmetry of S. The same is true for the normal
derivatives on S of Wλ(x, y) in x and y.
This boundary symmetry happens to imply the needed full symmetry (> .)
for x, y ∈B.
Indeed, Wλ(x, y) is a solution of the Helmholtz equation separately as a function of x
and of y. Let us introduce a family of solutions {wn(x)}∞
n=of (> .) in B, such that the
members of this family form an orthonormal basis for all solutions of the latter equation
in B. For example, the spherical waves, i.e., the products of spherical harmonics and Bessel
functions, can serve as such a basis.


Mathematics of Photoacoustic and Thermoacoustic Tomography
Then Wλ(x, y) can be expanded n the following series:
Wλ(x, y) =
∞
∑
n=
∞
∑
m=
bn,mwm(y)wn(x).
(.)
Since Wλ(x, y) is a solution to the Helmholtz equation in ∂B × ∂B, coefficients bn,m are
completely determined by the boundary values of Wλ. Since the boundary values are sym-
metric, the coefficients are symmetric, i.e., bn,m = bm,n which by ( > .) immediately
implies Wλ(x, y) = Wλ(y, x) for all pairs (x, y) ∈B × B.
This consideration extends to infinite cylinders and planes. This explains why the
“universal backprojection formula” (> .) is valid also for infinite cylinders and planes
[]. Since the sort of symmetry used is shared only by these three smooth surfaces, we
believe it is unlikely that a closed-form formula could exist for any other smooth acquisi-
tion surface. However, an exact formula has recently be obtained by L. Kunyansky for the
case when observation surface S is a surface of a cube (unpublished).
Algebraic Iterative Algorithms
Iterative algebraic techniques are among the favorite tomographic methods of reconstruc-
tion and have been used in CT for quite a while []. They amount to discretizing the
equation relating the measured data with the unknown source, followed by iterative solu-
tion of the resulting linear system. Iterative algebraic reconstruction algorithms frequently
produce better images than those obtained by other methods. However, they are notori-
ously slow. In TAT, they have been used successfully for reconstructions with partial data
( [,,]), see > Sect. ...
Parametrix Approaches
Some of the earlier non-iterative reconstruction techniques [] were of approximate
nature. For example, by approximating the integration spheres by their tangent planes at
the point of reconstruction and by applying one of the known inversion formulas for the
classical Radon transform, one can reconstruct an approximation to the image. Due to
the evenness symmetry in the classical Radon projections (see > Sect. ..), the normals
to the integration planes need only fill a half of a unit sphere, in order to make possible
the reconstruction from an open measurement surface. A more sophisticated approach
is represented by the so-called straightening methods [, ] based on the approximate
reconstruction of the classical Radon projections from the values of the spherical mean
transform Mf of the function f (x) in question. These methods yield not a true inversion,
but rather what is called in microlocal analysis a parametrix. Application of a parametrix
reproduces the function f with an additional, smoother term. In other words, the loca-
tions (and often the sizes) of jumps across sharp material interfaces, as well as the whole
wave front set WF(f ), are reconstructed correctly, while the accuracy of the lower spatial
frequencies cannot be guaranteed. (Sometimes, the reconstructed function has a more gen-
eral form Af , where A is an elliptic pseudo-differential operator [,] of order zero. In
this case, the sizes of the jumps across the interfaces might be altered.) Unlike the approxi-
mations resulting from the discretization of the exact inversion formulas (in the situations

Mathematics of Photoacoustic and Thermoacoustic Tomography 

when such formulas are known), the parametrix approximations do not converge, when
the discretization of the data is refined and the noise is eliminated. Parametrix reconstruc-
tions can be either accepted as approximate images, or used as starting points for iterative
algorithms.
These methods are closely related to the general scheme proposed in [] for the inver-
sion of the generalized Radon transform with integration over curved manifolds. It reduces
the problem to a Fredholm integral equation of the second kind, which is well suited for
numerical solution. Such an approach amounts to using a parametrix method as an efficient
pre-conditioner for an iterative solver; the convergence of such iterations is much faster
than that of algebraic iterative methods.
Numerical Implementation and Computational Examples
By discretizing exact formulas presented above, one can easily develop accurate and effi-
cient reconstruction algorithms. The D case is especially simple: computation of deriva-
tives in the formulas (> .)–(> .) and (> .) can be easily done, for instance by
using finite differences; it is followed by the backprojection (described by the integral over
∂B), which requires prescribing quadrature weights for quadrature nodes that coincide
with the positions of the detectors. The backprojection step is stable; the differentiation is
a mildly unstable operation. The sensitivity to noise in measurements across the formulas
presented above seems to be roughly the same. It is very similar to that of the widely used
FBP algorithm of classical X-ray tomography []. In D, the implementation is just a little
bit harder: the filtration step in formulas (> .)–(> .) can be reduced to comput-
ing two Hilbert transforms (see []), which, in turn, can be easily done in the frequency
domain.
The number of floating point operations (flops) required by such algorithms is deter-
mined by the slower backprojection step. In D, if the number of detectors is mand
the size of the reconstruction grid is m × m × m, the backprojection step (and the whole
algorithm) will require O(m) flops. In practical terms, this amounts to several hours of
computations on a single processor computer for a grid of size × × .
In D, the operation count is just O(m). As it is discussed in > Sect. .., the D
problem needs to be solved, when integrating line detectors are used. In this situation, the
D problem needs to be solved m times in order to reconstruct the image, which raises the
total operation count to O(m) flops.
> Figure -shows three examples of simulated reconstruction using formula
( > .). The phantom we use (> Fig. -a) is a linear combination of several charac-
teristic functions of disks and ellipses. > Figure -b illustrates the image reconstruction
within the unit circle from equi-spaced projections each containing spherical inte-
grals. The detectors were placed on the concentric circle of radius .. The image shown
in > Fig. -c corresponds to the reconstruction from the simulated noisy data that were
obtained by adding to projections values of a random variable scaled so that the Linten-
sity of the noise was % of the intensity of the signal. Finally,
> Fig. -d shows how
application of a smoothing filter (in the frequency domain) suppresses the noise; it also
somewhat blurs the edges in the image.


Mathematics of Photoacoustic and Thermoacoustic Tomography
⊡Fig. -
Example of a reconstruction using formula (> .): (a) phantom; (b) reconstruction
from accurate data; (c) reconstruction from the data contaminated with % noise;
(d) reconstruction from the noisy data with additional smoothing
...
Variable Speed of Sound
The reconstruction formulas and algorithms described in the previous section work under
the assumption that the speed of sound within the region of interest is constant (or at
least close to a constant). This assumption, however, is not always realistic, e.g., if the
region of interest contains both soft tissues and bones, the speed of sound will vary signif-
icantly. Experiments with numerical and physical phantoms show [,] that if acoustic
inhomogeneities are not taken into account, the reconstructed image might be severely
distorted. Not only the numerical values could be reconstructed incorrectly, but so would
the material interface locations and discontinuity magnitudes.
Below we review some of the reconstruction methods that work in acoustically inho-
mogeneous media. We will assume that the speed of sound c(x) is known, smooth,
positive, constant for large x, and non-trapping. In practice, a transmission ultrasound
scan can be used to reconstruct c(x) prior to thermoacoustic reconstruction, as it is done
in [].
Time Reversal
Let us assume temporarily that the speed of sound c is constant and the spatial dimension
is odd. Then Huygens’ principle guarantees that the sound wave will leave the region of
interest Ω in time T = c/(diam Ω), so that p(x, t) = for all x ∈Ω and t ≥T. Now one
can solve the wave equation back in time from t = T to t = in the domain Ω × [T,],

Mathematics of Photoacoustic and Thermoacoustic Tomography 

with zero initial conditions at T and boundary conditions on S provided by the data g col-
lected by the detectors. Then the value of the solution at t = will coincide with the initial
condition f (x) that one seeks to reconstruct. Such a solution of the wave equation is easily
obtained numerically by finite difference techniques [,]. The required number of float-
ing point operations is actually lower than that of methods based on discretized inversion
formulas (O(m) for time reversal on a grid m × m × m in D versus O(m) for inversion
formulas), which makes this method quite competitive even in the case of constant speed
of sound.
Most importantly, however, the method is also applicable if the speed of sound c(x) is
variable and/or the spatial dimension is even. In these cases, the Huygens’ principle does
not hold, and thus the solution to the direct problem will not vanish within ∂Ω in finite
time. However, the solution inside Ω will decay with time. Under the non-trapping con-
dition, as it is shown in (> .) (see [, , ]), the time decay is exponential in odd
dimensions, but only algebraic in even dimensions. Although, in order to obtain theoreti-
cally exact reconstruction, one would have to start the time reversal at T = ∞, numerical
experiments (e.g., []) and theoretical estimates [] show that in practice it is sufficient
to start at the values of T when the signal becomes small enough, and to approximate
the unknown value of p(x, T) by zero (a more sophisticated cutoff is used in []). This
works [, ] even in D (where decay is the slowest) and in inhomogeneous media.
However, when trapping occurs, the “invisible” parts blur away (see > Sect. ..for the
discussion).
Eigenfunction Expansions
An “inversion formula” that reconstructs the initial value f (x) of the solution of the wave
equation from values on the measuring surface S can be easily obtained using time reversal
and Duhamel’s principle []. Consider in Ω the operator A = −c(x)Δ with zero Dirich-
let conditions on the boundary S = ∂Ω. This operator is self-adjoint, if considered in the
weighted space L(Ω; c−(x)). Let us denote by E the operator of harmonic extension,
which transforms a function ϕ on S to a harmonic function on Ω that coincides with
ϕ on S. Then f can be reconstructed [] from the data g in ( > .) by the following
formula:
f (x) = (Eg∣t=) −
∞
∫

A−
sin (τA

)E(gtt)(x, τ)dτ,
(.)
which is valid under the non-trapping condition on c(x).However, due to the involvement
of functions of the operator A, it is not clear how useful this formula can be.
One natural way to try to implement numerically the formula ( > .) is to use the
eigenfunction expansion of the operator A in Ω (assuming that such expansion is known).
This quickly leads to the following procedure []. The function f (x) can be reconstructed
inside Ω from the data g in (> .), as the following L(B)-convergent series:
f (x) = ∑
k
fkψk(x),
(.)


Mathematics of Photoacoustic and Thermoacoustic Tomography
where the Fourier coefficients fk can be recovered from the data using one of the following
formulas:
fk = λ−
k gk() −λ−
k
∞
∫

sin (λkt)g′′
k (t)dt,
fk = λ−
k gk() + λ−
k
∞
∫

cos (λkt)g′
k(t)dt, or
fk = −λ−
k
∞
∫

sin (λkt)gk(t)dt = −λ−
k
∞
∫
∫
S
sin (λkt)g(x, t) ∂ψk
∂n (x)dxdt,
(.)
where
gk(t) = ∫
S
g(x, t)∂ψk
∂n (x)dx.
One notices that this is a generalization of the expansion method of [] discussed
in > Sect. ...to the case of a variable speed of sound. Unlike the algorithm of [],
this method does not require the knowledge of the whole space Green’s function for A
(which is in this case unknown). However, computation of a large set of eigenfunctions
and eigenvalues followed by the summation of the series ( > .) at the nodes of the
computational grid may prove to be too time consuming.
It is worthwhile to mention again that the non-trapping condition is crucial for the sta-
bility of any TAT reconstruction method in acoustically inhomogeneous media. As it was
discussed in > Sect. .., trapping can significantly reduce the quality of reconstruction.
It is, however, most probable that trapping does not occur much in biological objects.
..
Partial (Incomplete) Data
Reconstruction formulas and algorithms of the previous sections work under the assump-
tion that the acoustic signal is measured by detectors covering a closed surface S that
surrounds completely the object of interest. However, in many practical applications of
TAT, detectors can be placed only on a certain part of the surrounding surface. Such is the
case, e.g., when TAT is used for breast screening – one of the most promising applications
of this modality. Thus, one needs methods and algorithms capable of accurate reconstruc-
tion of images from partial (incomplete) data, i.e., from the measurements made on open
surfaces (or open curves in D).
Most exact inversion formulas and methods discussed above are based (explicitly or
implicitly) on some sort of the Green’s formula, Helmholtz representation, or eigenfunc-
tion decomposition for closed surfaces, and thus they cannot be extended to the case of
partial data. The methods that do work in this situation rely on approximation techniques,
as discussed below.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

...
Constant Speed of Sound
Even the case of an acoustically homogeneous medium is quite challenging when recon-
struction needs to be done from partial data (i.e., when the acquisition surface S is not
closed). As it was discussed in > Sect. .., if the detectors located around the object in
such a way that the “visibility” condition is not satisfied, accurate reconstruction is impos-
sible: the “invisible” interfaces will be smoothed out in the reconstructed image. On the
other hand, if the visibility condition is satisfied, the reconstruction is only mildly unsta-
ble (similarly to the inversion of the classic Radon transform) [, ]. If, in addition, the
uniqueness of reconstruction from partial data is guaranteed (which is usually the case,
see > Sect. ...), one can hope to be able to develop an algorithm that would reconstruct
quality images.
Special cases of open acquisition surfaces are a plane or an infinite cylinder, for which
exact inversion formulas are known (see, e.g., [, , , ] for the plane and [] for a
cylinder). Of course, the plane or a cylinder would have to be truncated in any practical
measurements. The resulting acquisition geometry will not satisfy the visibility condition,
and material interfaces whose normals do not intersect the acquisition surface will be
blurred.
Iterative algebraic techniques (see the corresponding paragraph in > Sect. ...) were
among the first methods successfully used for reconstruction from surfaces only partially
surrounding the object (e.g., [,,]). As it is mentioned in > Sect. ..., such methods
are very slow. For example, reconstructions in [] required the use of a cluster of computers
and took iterations to converge.
Parametrix-type reconstructions in the partial data case were proposed in []. A cou-
ple of different parametrix-type algorithms were proposed in [, ]. They are based on
applying one of the exact inversion formulas for full circular acquisition to the available
partial data, with zero-filled missing data, and some correction factors. Namely, since the
missing data is replaced by zeros, each line passing through a node of the reconstruction
grid will be tangent either to one or to two circles of integration. Therefore, some directions
during the backprojection step will be represented twice, and some only once. This, in turn,
will cause some interfaces to appear twice stronger then they should be. The use of weight
factors was proposed in [,] in order to partially compensate for this distortion. In par-
ticular, in [] smooth weight factors (depending on a reconstruction point) are assigned
to each detector in such a way that the total weight for each direction is exactly one. This
method is not exact; the error is described by a certain smoothing operator. However, the
singularities (or jumps) in the image will be reconstructed correctly. As shown by numer-
ical examples in [], such a correction visually significantly improves the reconstruction.
Moreover, iterative refinement is proposed in [,] to further improve the image, and it
is shown to work well in numerical experiments.
Returning to non-iterative techniques, one should mention an interesting attempt
made in [, ] to generate the missing data using the moment range conditions for M
(see > Sect. ..). The resulting algorithm, however, does not seem to recover the values
well; although, as expected, it reconstructs all visible singularities.


Mathematics of Photoacoustic and Thermoacoustic Tomography
An accurate D non-iterative algorithm for reconstruction from data measured on an
open curve S was proposed in []. It is based on precomputing approximations of plane
waves in the region of interest Ω by the single layer potentials of the form
∫
S
Z(λ∣y −x∣)ρ(y)dl(y),
where ρ(y) is the density of the potential, which needs to be chosen appropriately, dl(y)is
the standard arc length, and Z(t) is either the Bessel function J(t), or the Neumann func-
tion Y(t). Namely, for a fixed ξ one finds numerically the densities ρξ,J(y) and ρξ,Y(y) of
the potentials
WJ(x, ρξ,J) = ∫S J(λ∣y −x∣)ρξ,J(y)dl(y),
(.)
WY(x, ρξ,Y) = ∫S Y(λ∣y −x∣)ρξ,Y(y)dl(y),
(.)
where λ = ∣ξ∣, such that
WJ(x, ρξ,J) + WY(x, ρξ,Y) ≈exp(−iξ ⋅x) for all x ∈Ω.
(.)
Obtaining such approximations is not trivial. One can show that exact equality in (> .)
cannot be achieved, due to different behavior at infinity of the plane wave and the approx-
imating single-layer potentials. However, as shown by numerical examples in [], if each
point in Ω is “visible” from S, very accurate approximations can be obtained, while keeping
the densities ρξ,J and ρξ,Y under certain control.
Once the densities ρξ,J and ρξ,Y have been found for all ξ, function f (x) can be easily
reconstructed. Indeed, for the Fourier transform ˆf (ξ) of f (x)
ˆf (ξ) = 
π ∫Ω f (x)exp(−iξ ⋅x)dx,
one obtains, using (> .)
ˆf (ξ) ≈
π ∫Ω f (x)[WJ(x, ρξ,J) + WY(x, ρξ,Y)]dx
=

π ∫S [∫Ω f (x)J(λ∣y −x∣)dx] ρξ,J(y)dl(y)
+ 
π ∫S [∫Ω f (x)Y(λ∣y −x∣)dx] ρξ,Y(y)dl(y),
(.)
where the inner integrals are computed from the data g:
∫Ω f (x)J(λ∣y −x∣)dx = ∫R+ g(y, r)J(λr)dr,
(.)
∫Ω f (x)Y(λ∣y −x∣)dx = ∫R+ g(y, r)Y(λr)dr.
(.)
Formula (> .), in combination with (> .) and (> .), yields values of ˆf (ξ)
for arbitrary ξ. Now f (x) can be recovered by numerically inverting the Fourier transform,
or by a reduction to a FBP inversion [] of the regular Radon transform.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

⊡Fig. -
Examples of reconstruction from incomplete data using the technique of []. Detectors are
located on the part of circular arc of radius .lying left of the line x= . (a) reconstruction
from accurate data (b) reconstruction from the data with added % noise (c) reconstruction
from noisy data with additional smoothing ﬁlter
The most computationally expensive part of the algorithm, which is computing the
densities ρξ,J and ρξ,Y, needs to be done only once for a given acquisition surface. Thus, for
a scanner with a fixed S, the resulting densities can be precomputed once and for all. The
actual reconstruction part then becomes extremely fast.
Examples of reconstructions from incomplete data using this technique of [] are
shown in > Fig. -. The images were reconstructed within the unit square [−,]×[−,],
while the detectors were placed on the part of the concentric circle of radius .lying to the
left of line x= . We used the same phantom as in > Fig. -a; the reconstruction from
the data with added % noise is shown in
> Fig. -b;
> Fig. -c demonstrates the
results of applying additional smoothing filter to reduce the effects of noise in the data.
...
Variable Speed of Sound
The problem of numerical reconstruction in TAT from the data measured on open sur-
faces in the presence of a known variable speed of sound currently remains largely open.
One of the difficulties was discussed in
> Sect. ..: even if the speed of sound c(x) is
non-trapping, it can happen that some of the characteristics escape from the region of inter-
est to infinity without intersecting the open measuring surface. Then stable reconstruction
of the corresponding interfaces will become impossible. It should be possible, however, to
develop stable reconstruction algorithms in the case when the whole object of interest is
located in the visible zone.
The generalization of the method of [] to the case of variable speed of sound is so
far problematic, since this algorithm is based on the knowledge of the open space Green’s
function for the Helmholtz equation. In the case of a nonconstant c(x), this Green’s func-
tion is position-depended, and its numerical computation is likely to be prohibitively time
consuming.
A promising approach to this problem, currently under development, is to use time
reversal with the missing data replaced by zeros, or maybe by a more clever extension


Mathematics of Photoacoustic and Thermoacoustic Tomography
(e.g., using the range conditions, as in [,]). This would produce an initial approxima-
tion to f (x), which one can try to refine by fixed-point iterations; however, the pertinent
questions concerning such an algorithm remain open.
An interesting technique of using a reverberant cavity enclosing the target to compen-
sate for the missing data is described in [].
.
Final Remarks and Open Problems
We list here some unresolved issues of mathematics of TAT/PAT, as well as some develop-
ments that were not addressed in the main text.
. The issue of uniqueness acquisition sets S (i.e., such that transducers distributed
along S provide sufficient information for TAT reconstruction) can be considered to
be resolved, for most practical purposes. However, there remain significant unresolved
theoretical questions. One of them consists of proving an analog of Theorem for non-
compactly supported functions with a sufficiently fast (e.g., super-exponential) decay
at infinity. The original (and the only known) proof of this theorem uses microlocal
techniques [, ] that significantly rely upon the compactness of support. However,
one hopes that the condition of a fast decay should suffice for this result. In particular,
there is no proven analog of Theorem for non-closed sets S (unless S is an open part
of a closed analytic surface).
Techniques developed in [] (see also [] for their further use in TAT) might
provide the right approach.
This also relates to still unresolved situation in dimensions and higher. Namely,
one would like to prove Conjecture .
. Concerning the inversion methods, one notices that closed-form formulas are known
only for spherical, cylindrical, and planar acquisition surfaces. The question arises
whether closed-form inversion formulas could be found for any other smooth closed
surface? It is the belief of the authors that the answer to this question is negative.
Another feature of the known closed-form formulas that was mentioned before is
that they do not work correctly if the support of the sought function f (x) lies partially
outside the acquisition surface. Time reversal and eigenfunction expansion methods do
not suffer from this deficiency. The question arises whether one could find closed-form
formulas that reconstruct the function inside S correctly, in spite of it having part of its
support outside. Again, the authors believe that the answer is negative.
. The complete range description of the forward operator W in even dimensions is still
not known. It is also not clear whether one can obtain complete range descriptions
for nonspherical observation sets S or for a variable sound speed. The moment and
orthogonality conditions do hold in the case of a constant speed and arbitrary closed
surface, but they do not provide a complete description of the range. For acoustically
inhomogeneous media, an analog of orthogonality conditions exists, but it also does
not describe the range completely.

Mathematics of Photoacoustic and Thermoacoustic Tomography 

. The problem of unique determination of the speed of sound from TAT data is
largely open.
. As it was explained in the text, knowing full Cauchy data of the pressure p (i.e., its
value and the value of its the normal derivative) on the observation surface S leads to
unique determination and simple reconstruction of f . However, the normal derivative
is not measured by transducers and thus needs to be either found mathematically or
measured in a different experiment. Thus, feasibility of techniques [, ] relying on
full Cauchy data requires further mathematical and experimental study.
. In the standard X-ray CT, as well as in SPECT, the local tomography technique [,]
is often very useful. It allows one to emphasize in a stable way singularities (e.g., tis-
sue interfaces) of the reconstruction, even in the case of incomplete data (in the latter
case, the invisible parts will be lost). An analog of local tomography can be easily imple-
mented in TAT, for instance, by introducing an additional high-pass filter in the FBP
type formulas.
. The mathematical analysis of TAT presented in the text did not take into account
the issue of modeling and compensating for the acoustic attenuation. This subject is
addressed in [,,,,], but probably cannot be considered completely resolved.
. Quantitative PAT: This chapter, as well as most other papers devoted to TAT/PAT is
centered on finding the initial pressure f (x). This pressure, which is proportional to the
initial energy deposition, is related to the optical parameters (attenuation and scattering
coefficients) of the tissue. The nontrivial issue of recovering these parameters, after the
initial pressure f (x) is found, is addressed in the recent works [,,].
. The TAT technique discussed in the chapter uses active interrogation of the medium.
There is a discussion in the literature of a passive version of TAT, where no irradiation
of the target is involved [].
.
Cross-References
> Linear Inverse Problems (TAT and PAT are examples of linear inverse problems)
> Photoacoustic and Thermoacoustic Tomography: Image Formation Principles (Basic
principles of TAT and PAT)
> Tomography (General discussion of tomography)
Acknowledgments
The work of both authors was partially supported by the NSF DMS grant . The first
author was also supported by the NSF DMS grant and by the KAUST grant KUS-
CI--through the IAMCS. The work of the second author was partially supported by
the DOE grant DE-FG-ER. The authors express their gratitude to NSF, DOE,
KAUST, and IAMCS for the support.


Mathematics of Photoacoustic and Thermoacoustic Tomography
References and Further Reading
. Agranovsky M, Berenstein C, Kuchment P ()
Approximation by spherical waves in Lp-spaces.
J Geom Anal ():–
. Agranovsky M, Finch D, Kuchment P ()
Range conditions for a spherical mean transform.
Inverse Probl Imaging ():–
. Agranovsky M, Kuchment P () Uniqueness
of reconstruction and an inversion procedure
for thermoacoustic and photoacoustic tomogra-
phy with variable sound speed. Inverse Probl
:–
. Agranovsky M, Kuchment P, Kunyansky L ()
On reconstruction formulas and algorithms for
the thermoacoustic and photoacoustic tomogra-
phy, Chapter . In: Wang LH (ed) Photoacous-
tic imaging and spectroscopy. CRC Press, Boca
Raton, pp –
. Agranovsky
M,
Kuchment
P,
Quinto
ET
() Range descriptions for the spherical
mean Radon transform. J Funct Anal :
–
. Agranovsky M, Nguyen L () Range con-
ditions for a spherical mean transform and
global extension of solutions of Darboux equa-
tion. Preprint arXiv:.To appear in J
d’Analyse Mathematique
. Agranovsky M, Quinto ET () Injectivity sets
for the Radon transform over circles and com-
plete systems of radial functions. J Funct Anal
:–
. Ambartsoumian G, Kuchment P () On
the injectivity of the circular radon transform.
Inverse Probl :–
. Ambartsoumian G, Kuchment P () A range
description for the planar circular Radon trans-
form. SIAM J Math Anal ():–
. Ammari H () An Introduction to mathemat-
ics of emerging biomedical imaging. Springer,
Berlin
. Ammari H, Bonnetier E, Capdebosq Y, Tanter M,
Fink M () Electrical impedance tomogra-
phy by elastic deformation. SIAM J Appl Math
():–
. Ammari H, Bossy E, Jugnon V, Kang H. Quanti-
tative photo-acoustic imaging of small absorbers.
SIAM Review, to appear
. Anastasio MA, Zhang J, Modgil D, Rivière PJ
() Application of inverse source concepts
to photoacoustic tomography Inverse Probl
:S–S
. Anastasio MA, Zhang J, Sidky EY, Zou Z, Dan X,
Pan X () Feasibility of half-data image recon-
struction in -D reflectivity tomography with
a spherical aperture. IEEE Trans Med Imaging
():–
. Anastasio M, Zhang J, Pan X, Zou Y, Ku G,
Wang LV () Half-time image reconstruction
in thermoacoustic tomography. IEEE Trans Med
Imaging :–
. Andersson L-E () On the determination of a
function from spherical averages. SIAM J Math
Anal ():–
. Andreev V, Popov D et al () Image recon-
struction in D optoacoustic tomography system
with hemispherical transducer array. Proc SPIE
:–
. Bal G, Jollivet A, Jugnon V () Inverse trans-
port theory of photoacoustics. Inverse Probl
:, doi:./-///
. Bell AG () On the production and repro-
duction of sound by light. Am J Sci :
–
. Beylkin G () The inversion problem and
applications of the generalized Radon transform.
Commun Pur Appl Math :–
. BowenT () Radiation-inducedthermoacous-
tic soft tissue imaging. Proc IEEE Ultrason Symp
:–
. Burgholzer P, Grün H, Haltmeier M, Nuster R,
Paltauf G () Compensation of acoustic atten-
uation for high-resolution photoacoustic imag-
ing with line detectors using time reversal. In:
Proceedings of the SPIE number –Pho-
tonics West, BIOS , San Jose
. Burgholzer P, Hofer C, Paltauf G, Haltmeier M,
Scherzer O () Thermoacoustic tomogra-
phy with integrating area and line detectors.
IEEE Trans Ultrason Ferroelectr Freq Control
():–
. Burgholzer P, Hofer C, Matt GJ, Paltauf G, Halt-
meier M, Scherzer O () Thermoacoustic
tomography using a fiber-based Fabry–Perot

Mathematics of Photoacoustic and Thermoacoustic Tomography 

interferometer as an integrating line detector.
Proc SPIE :–
. Clason C, Klibanov M () The quasi-
reversibility
method
in
thermoacoustic
tomography
in
a
heterogeneous
medium.
SIAM J Sci Comput :–
. Colton D, Paivarinta L, Sylvester J () The
interior transmission problem. Inverse Probl
():–
. Courant R, Hilbert D () Methods of mathe-
matical physics. Partial differential equations, vol
II. Interscience, New York
. Cox BT, Arridge SR, Beard PC () Photoa-
coustic tomography with a limited aperture pla-
nar sensor and a reverberant cavity. Inverse Probl
:S–S
. Cox BT, Arridge SR, Beard PC () Estimat-
ing chromophore distributions from multiwave-
length photoacoustic images. J Opt Soc Am A
:–
. Cox BT, Laufer JG, Beard PC () The chal-
lenges for quantitative photoacoustic imaging.
Proc SPIE :
. Diebold GJ, Sun T, Khan MI () Photoacous-
tic monopole radiation in one, two, and three
dimensions. Phys Rev Lett ():–
. Egorov Yu V, Shubin MA () Partial differen-
tial equations I. Encyclopaedia of mathematical
sciences, vol . Springer, Berlin, pp –
. Faridani A, Ritman EL, Smith KT () Local
tomography. SIAM J Appl Math ():–
. Fawcett JA () Inversion of n-dimensional
spherical
averages.
SIAM
J
Appl
Math
():–
. Finch D, Haltmeier M, Rakesh () Inversion
of spherical means and the wave equation in even
dimensions. SIAM J Appl Math ():–
. Finch D, Patch S, Rakesh () Determining a
function from its mean values over a family of
spheres. SIAM J Math Anal ():–
. Finch D, Rakesh () Range of the spherical
mean value operator for functions supported in
a ball. Inverse Probl :–
. Finch D, Rakesh. Recovering a function from its
spherical mean values in two and three dimen-
sions. In [], pp –
. Finch D, Rakesh () The spherical mean value
operator with centers on a sphere. Inverse Probl
():S–S
. Gebauer B, Scherzer O () Impedance-
acoustic
tomography.
SIAM
J
Appl
Math
():–
. Gelfand I, Gindikin S, Graev M () Selected
topics in integral geometry. Transl Math Monogr
vol , American Mathematical Society, Provi-
dence
. Grün H, Haltmeier M, Paltauf G, Burgholzer P
() Photoacoustic tomography using a fiber
based Fabry-Perot interferometer as an integrat-
ing line detector and image reconstruction by
model-based time reversal method. Proc SPIE
:
. Haltmeier M, Burgholzer P, Paltauf G, Scherzer O
() Thermoacoustic computed tomography
with
large
planar
receivers.
Inverse
Probl
:–
. Haltmeier M, Scherzer O, Burgholzer P, Nuster R,
Paltauf G () Thermoacoustic tomography
and the circular radon transform: exact inver-
sion formula. Math Mod Methods Appl Sci
():–
. Helgason
S
()
The
Radon
transform.
Birkh äuser, Basel
. Hörmander L () The analysis of linear par-
tial differential operators, vols and . Springer,
New York
. Hristova Y () Time reversal in thermoa-
coustic tomography: error estimate. Inverse Probl
:–
. Hristova Y, Kuchment P, Nguyen L ()
On reconstruction and time reversal in ther-
moacoustic tomography in homogeneous and
non-homogeneous acoustic media. Inverse Probl
:
. Isakov V () Inverse problems for partial dif-
ferential equations, nd edn. Springer, Berlin
. Jin X, Wang LV () Thermoacoustic tomogra-
phy with correction for acoustic speed variations.
Phys Med Biol :–
. John F () Plane waves and spherical means
applied to partial differential equations. Dover,
New York
. Kowar R, Scherzer O, Bonnefond X. Causality
analysis of frequency dependent wave attenua-
tion, preprint arXiv:.
. Kruger RA, Liu P, Fang YR, Appledorn CR ()
Photoacoustic ultrasound (PAUS)reconstruction
tomography. Med Phys :–


Mathematics of Photoacoustic and Thermoacoustic Tomography
. Kuchment P, Lancaster K, Mogilevskaya L ()
On local tomography. Inverse Probl :–
. Kuchment P, Kunyansky L () Mathematics
of thermoacoustic tomography. Eur J Appl Math
():–
. Kuchment P, Kunyansky L, Synthetic focusing in
ultrasoundmodulatedtomography.InverseProbl
Imaging, to appear
. Kunyansky L () Explicit inversion formulae
for the spherical mean Radon transform. Inverse
probl :–
. Kunyansky L () A series solution and a fast
algorithm for the inversion of the spherical mean
Radon transform. Inverse Probl :S–S
. Kunyansky L () Thermoacoustic tomogra-
phy with detectors on an open curve: an effi-
cient reconstruction algorithm. Inverse Probl
():
. Lin V, Pinkus A () Approximation of multi-
variate functions. In: Dikshit HP, Micchelli CA
(eds) Advances in computational mathematics.
World Scientific, Singapore, pp –
. Louis AK, Quinto ET () Local tomographic
methods in Sonar. In: Surveys on solution meth-
ods for inverse problems. Springer, Vienna, pp
–
. Maslov K, Zhang HF, Wang LV () Effects
of wavelength-dependent fluence attenuation
on the noninvasive photoacoustic imaging of
hemoglobin oxygen saturation in subcutaneous
vasculature in vivo. Inverse Probl :S–S
. Natterer F () The mathematics of computer-
ized tomography. Wiley, New York
. Nguyen L () A family of inversion formu-
las in thermoacoustic tomography. Inverse Probl
Imaging ():–
. Nguyen LV. On singularities and instability of
reconstruction in thermoacoustic tomography,
preprint arXiv:.v
. Norton SJ () Reconstruction of a two-
dimensional reflecting medium over a circu-
lar domain: exact solution. J Acoust Soc Am
:–
. Norton SJ, Linzer M () Ultrasonic reflectiv-
ity imaging in three dimensions: exact inverse
scattering solutions for plane, cylindrical, and
spherical apertures. IEEE Trans Biomed Eng
:–
. Olafsson G, Quinto ET (eds) The radon trans-
form, inverse problems, and tomography. Ameri-
can Mathematical Society Short Course January
–, , Atlanta, Georgia, Proc Symp Appl
Math, vol , AMS, RI, 
. Oraevsky AA, Jacques SL, Esenaliev RO, Tittel FK
() Laser-based ptoacoustic imaging in bio-
logical tissues. Proc SPIE A:–
. Palamodov VP () Reconstructive integral
geometry. Birkhäuser, Basel
. Palamodov V () Remarks on the gen-
eral Funk–Radon transform and thermoacoustic
tomography. Preprint arxiv: math.AP/
. Paltauf G, Nuster R, Burgholzer P () Weight
factors for limited angle photoacoustic tomogra-
phy. Phys Med Biol :–
. Paltauf G, Nuster R, Haltmeier M, Burgholzer P
() Thermoacoustic computed tomography
using a Mach–Zehnder interferometer as acous-
tic line detector. Appl Opt ():–
. Paltauf G, Nuster R, Haltmeier M, Burgholzer P
() Experimental evaluation of reconstruc-
tion algorithms for limited view photoacoustic
tomography with line detectors. Inverse Probl
:S–S
. Paltauf G, Viator JA, Prahl SA, Jacques SL ()
Iterative reconstruction algorithm for optoacous-
tic imaging J. Acoust Soc Am ():–
. Paltauf G, Nuster R, Burgholzer P () Char-
acterization of integrating ultrasound detec-
tors for photoacoustic tomography. J Appl Phys
:
. Passechnik VI, Anosov AA, Bograchev KM
() Fundamentals and prospects of passive
thermoacoustic tomography. Crit Rev Biomed
Eng (–):–
. Patch SK () Thermoacoustic tomography –
consistency conditions and the partial scan prob-
lem. Phys Med Biol :–
. Patch S () Photoacoustic or thermoacous-
tic tomography: consistency conditions and the
partial scan problem, in [], –
. Patch SK, Haltmeier M () Thermoacoustic
tomography – ultrasound attenuation artifacts.
IEEE Nucl Sci Sym Conf :–
. Popov DA, Sushko DV () A parametrix
for the problem of optical-acoustic tomography.
Dokl Math ():–

Mathematics of Photoacoustic and Thermoacoustic Tomography 

. Popov DA, Sushko DV () Image restoration
in optical-acoustic tomography. Probl Inform
Transm ():–
. La Rivière PJ, Zhang J, Anastasio MA ()
Image reconstruction in optoacoustic tomog-
raphy for dispersive acoustic media. Opt Lett
():–
. Shubin MA () Pseudodifferential operators
and spectral theory. Springer, Berlin
. Stefanov P, Uhlmann G () Integral geometry
of tensor fields on a class of non-simple Rieman-
nian manifolds. Am J Math ():–
. Stefanov P, Uhlmann G () Thermoacoustic
tomography with variable sound speed. Inverse
Probl :
. Steinhauer D. A uniqueness theorem for ther-
moacoustic tomography in the case of limited
boundary data, preprint arXiv:.
. Tam AC () Applications of photoacoustic
sensing techniques. Rev Mod Phys ():–
. Tuchin VV (ed) () Handbook of optical
biomedical diagnostics. SPIE, Bellingham
. Vainberg B () The short-wave asymptotic
behavior of the solutions of stationary problems,
and the asymptotic behavior as t →∞of the
solutions of nonstationary problems. Russ Math
Surv ():–
. Vainberg B () Asymptotics methods in the
equations of mathematical physics. Gordon &
Breach, New York
. Vo-Dinh T (ed) () Biomedical photonics
handbook. CRC Press, Boca Raton
. Wang K, Anastasio MA. Photoacoustic and ther-
moacoustic tomography: image formation prin-
ciples, Chapter in this volume
. Wang L (ed) () Photoacoustic imaging and
spectroscopy. CRC Press, Boca Raton
. Wang LV, Wu H () Biomedical optics. Prin-
ciples and imaging. Wiley, New York
. Xu M, Wang L-HV () Time-domain recon-
struction for thermoacoustic tomography in a
spherical geometry. IEEE Trans Med Imaging
:–
. Xu M, Wang L-HV () Universal back-
projection algorithm for photoacoustic com-
puted tomography. Phys Rev E:
. Xu Y, Feng D, Wang L-HV () Exact
frequency-domain reconstruction for thermoa-
coustic tomography: I Planar geometry. IEEE
Trans Med Imag :–
. Xu Y, Xu M, Wang L-HV () Exact frequency-
domain
reconstruction
for
thermoacoustic
tomography: II Cylindrical
geometry.
IEEE
Trans Med Imaging :–
. Xu Y, Wang L, Ambartsoumian G, Kuchment P
() Reconstructions in limited view thermoa-
coustic tomography. Med Phys ():–
. Xu Y, Wang L, Ambartsoumian G, Kuchment P
() Limited view thermoacoustic tomogra-
phy,Ch..In:Wang LH (ed) Photoacoustic imag-
ing and spectroscopy. CRC Press, Boca Raton, pp
–
. Zangerl G, Scherzer O, Haltmeier M () Cir-
cular integrating detectors in photo and ther-
moacoustic tomography. Inverse Probl Sci Eng
():–
. Yuan Z, Zhang Q, Jiang H () Simultaneous
reconstruction of acoustic and optical properties
of heterogeneous media by quantitative photoa-
coustic tomography. Opt Express ():
. Zhang J, Anastasio MA () Reconstruction of
speed-of-sound and electromagnetic absorption
distributions in photoacoustic tomography. Proc
SPIE :


Wave Phenomena
Matti Lassas ⋅Mikko Salo ⋅Gunther Uhlmann
.
Introduction.....................................................................
.
Background......................................................................
..
Wave Imaging and Boundary Control Method.....................................
..
Travel Times and Scattering Relation.................................................
..
Curvelets and Wave Equations........................................................
.
Mathematical Modeling and Analysis..........................................
..
Boundary Control Method............................................................
...Inverse Problems on Riemannian Manifolds.......................................
...From Boundary Distance Functions to Riemannian Metric......................
...From Boundary Data to Inner Products of Waves.................................
...From Inner Products of Waves to Boundary Distance Functions...............
...Alternative Reconstruction of Metric via Gaussian Beams.......................
..
Travel Times and Scattering Relation................................................
...Geometrical Optics.....................................................................
...Scattering Relation......................................................................
..
Curvelets and Wave Equations........................................................
...Curvelet Decomposition...............................................................
...Curvelets and Wave Equations........................................................
...Low Regularity Wave Speeds and Volterra Iteration...............................
.
Conclusion.......................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Wave Phenomena
Abstract: This chapter discusses imaging methods related to wave phenomena, and in
particular, inverse problems for the wave equation will be considered. The first part of the
chapter explains the boundary control method for determining a wave speed of a medium
from the response operator, which models boundary measurements. The second part dis-
cusses the scattering relation and travel times, which are different types of boundary data
contained in the response operator. The third part gives a brief introduction to curvelets
in wave imaging for media with nonsmooth wave speeds. The focus will be on theoretical
results and methods.
.
Introduction
This chapter discusses imaging methods related to wave phenomena. Of the different types
of waves that exist, we will focus on acoustic waves and problems which can be modeled by
the acoustic wave equation. In the simplest case, this is the second-order linear hyperbolic
equation
∂
tu(x, t) −c(x)Δu(x, t) = 
for a sound speed c(x). This equation can be considered as a model for other hyperbolic
equations, and the methods presented here can in some cases be extended to study wave
phenomena in other fields such as electromagnetism or elasticity.
We will mostly be interested in inverse problems for the wave equation. In these prob-
lems one has access to certain measurements of waves (the solutions u) on the surface of
a medium, and one would like to determine material parameters (the sound speed c) of
the interior of the medium from these boundary measurements. A typical field where such
problems arise is seismic imaging, where one wishes to determine the interior structure
of Earth by making various measurements of waves at the surface. We will not describe
seismic imaging applications in more detail here, since they are discussed elsewhere in this
volume.
Another feature in this chapter is that we will consistently consider anisotropic materi-
als, where the sound speed depends on the direction of propagation. This means that the
scalar sound speed c(x), where x = (x, x, . . . , xn) ∈Ω ⊂Rn, is replaced by a positive
definite symmetric matrix (g jk(x))n
j,k=, and the wave equation becomes
∂
tu(x, t) −
n
∑
j,k=
g jk(x)
∂u
∂x j∂xk (x, t) = .
Anisotropic materials appear frequently in applications such as in seismic imaging.
It will be convenient to interpret the anisotropic sound speed (g jk) as the inverse of a
Riemannian metric, thus modeling the medium as a Riemannian manifold. The benefits of
such an approach are twofold. First, the well-established methods of Riemannian geom-
etry become available to study the problems, and second, this provides an efficient way
of dealing with the invariance under changes of coordinates present in many anisotropic

Wave Phenomena 

wave imaging problems. The second point means that in inverse problems in anisotropic
media, one can often only expect to recover the matrix (g jk) up to a change of coordinates
given by some diffeomorphism. In practice, this ambiguity could be removed by some a
priori knowledge of the medium properties (such as the medium being in fact isotropic,
see > Sect. ...).
.
Background
This chapter contains three parts which discuss different topics related to wave imaging.
The first part considers the inverse problem of determining a sound speed in a wave equa-
tion from the response operator, also known as the hyperbolic Dirichlet-to-Neumann map,
by using the boundary control method, see [, , ]. The second part considers other types
of boundary measurements of waves, namely the scattering relation and boundary distance
function, and discusses corresponding inverse problems. The third part is somewhat differ-
ent in nature and does not consider any inverse problems but rather gives an introduction
to the use of curvelet decompositions in wave imaging for nonsmooth sound speeds. We
briefly describe these three topics.
..
Wave Imaging and Boundary Control Method
Let us consider an isotropic wave equation. Let Ω ⊂Rn be an open, bounded set
with smooth boundary ∂Ω, and let c(x) be a scalar-valued positive function in C∞(Ω)
modeling the wave speed in Ω. First, we consider the wave equation
∂
tu(x, t) −c(x)Δu(x, t) = 
in Ω × R+,
(.)
u∣t== ,
ut∣t== ,
c(x)−n+∂nu = f (x, t)
in ∂Ω × R+,
where ∂n denotes the Euclidean normal derivative and n is the unit interior normal. We
denote by u f = u f (x, t) the solution of (> .) corresponding to the boundary source
term f .
Let us assume that the domain Ω ⊂Rn is known. The inverse problem is to reconstruct
the wave speed c(x) when we are given the set
{(f ∣∂Ω×(,T), u f ∣∂Ω×(,T)) : f ∈C∞
(∂Ω × R+)},
that is, the Cauchy data of solutions corresponding to all possible boundary sources f ∈
C∞
(∂Ω × R+), T ∈(,∞]. If T = ∞then this data is equivalent to the response operator
ΛΩ : f ↦u f ∣∂Ω×R+,
(.)
which is also called the nonstationary Neumann-to-Dirichlet map. Physically, ΛΩ f
describes the measurement of the medium response to any applied boundary source f ,


Wave Phenomena
and it is equivalent to various physical measurements. For instance, measuring how much
energy is needed to force the boundary value c(x)−n+∂nu∣∂Ω×R+ to be equal to any given
boundary value f ∈C∞
(∂Ω×R+) is equivalent to measuring the map ΛΩ on ∂Ω×R+, see
[, ]. Measuring ΛΩ is also equivalent to measuring the corresponding Neumann-to-
Dirichlet map for the heat or the Schrödinger equations, or measuring the eigenvalues and
the boundary values of the normalized eigenfunctions of the elliptic operator −c(x)Δ,
see [].
The inverse problems for the wave equation and the equivalent inverse problems for
the heat or the Schrödinger equations go back to works of M. Krein at the end of the
s, who used the causality principle in dealing with the one-dimensional inverse prob-
lem for an inhomogeneous string, utt −c(x)uxx = , see, for example, []. In his works,
causality was transformed into analyticity of the Fourier transform of the solution. A more
straightforward hyperbolic version of the method was suggested by A. Blagovestchen-
skii at the end of s to s [, ]. The multidimensional case was studied by M.
Belishev [] in the late s who understood the role of the PDE control for these prob-
lems and developed the boundary control method for hyperbolic inverse problems in
domains of Euclidean space. Of crucial importance for the boundary control method
was the result of D. Tataru in [, ] concerning a Holmgren-type uniqueness the-
orem for nonanalytic coefficients. The boundary control method was extended to the
anisotropic case by M. Belishev and Y. Kurylev []. The geometric version of the bound-
ary control method which we consider in this chapter was developed in [, , , ].
We will consider the inverse problem in the more general setting of an anisotropic wave
equation in an unbounded domain or on a non-compact manifold. These problems have
been studied in detail in [, ] also in the case when the measurements are done only
on a part of the boundary. In this paper we present a simplified construction method
applicable for non-compact manifolds in the case when measurements are done on the
whole boundary. We demonstrate these results in the case when we have an isotropic
wave speed c(x) in a bounded domain of Euclidean space. For this we use the fact
that in the Euclidean space the only conformal deformation of a compact domain fixing
the boundary is the identity map. This implies that after the abstract manifold struc-
ture (M, g) corresponding to the wave speed c(x) in a given domain Ω is constructed,
we can construct in an explicit way the embedding of the manifold M to the domain
Ω and determine c(x) at each point x ∈Ω. We note on the history of this result that
using Tataru’s unique continuation result [], Theorem concerning this case can be
proven directly using the boundary control method developed for domains in Euclidean
space in [].
The reconstruction of non-compact manifolds has been considered also in [, ]
with different kind of data, using iterated time reversal for solutions of the wave equa-
tion. We note that the boundary control method can be generalized also for Maxwell and
Dirac equations under appropriate geometric conditions [, ], and its stability has been
analyzed in [, ].

Wave Phenomena 

..
Travel Times and Scattering Relation
The problem considered in the previous section of recovering a sound speed from the
response operator is highly overdetermined in dimensions n ≥. The Schwartz kernel
of the response operator depends on n variables and the sound speed c depends on n
variables.
In > Sect. ..we will show that other types of boundary measurements in wave
imaging can be directly obtained from the response operator. One such measurement is
the boundary distance function, a function of n −variables, which measures the travel
times of shortest geodesics between boundary points. The problem of determining a sound
speed from the travel times of shortest geodesics is the inverse kinematic problem. The more
general problem of determining a Riemannian metric (corresponding to an anisotropic
sound speed) up to isometry from the boundary distance function is the boundary rigidity
problem. The problem is formally determined if n = but overdetermined for n ≥.
This problem arose in geophysics in an attempt to determine the inner structure of the
Earth by measuring the travel times of seismic waves. It goes back to Herglotz [] and
Wiechert and Zoeppritz [] who considered the case of a radial metric conformal to the
Euclidean metric. Although the emphasis has been in the case that the medium is isotropic,
the anisotropic case has been of interest in geophysics since the Earth is anisotropic. It has
been found that even the inner core of the Earth exhibits anisotropic behavior [].
To give a proper definition of the boundary distance function, we will consider a
bounded domain Ω ⊂Rn with smooth boundary to be equipped with a Riemannian metric
g, that is, a family of positive definite symmetric matrices g(x) = (g jk(x))
n
j,k=depending
smoothly on x ∈Ω. The length of a smooth curve γ : [a, b] →Ω is defined to be
Lg(γ) = ∫
b
a
⎛
⎝
n
∑
j,k=
g jk(γ(t))˙γ j(t)˙γk(t)⎞
⎠
/
dt.
The distance function dg(x, y) for x, y ∈Ω is the infimum of the lengths of all piecewise
smooth curves in Ω joining x and y. The boundary distance function is dg(x, y) for x,
y ∈∂Ω.
In the boundary rigidity problem, one would like to determine a Riemannian metric g
from the boundary distance function dg. In fact, since dg = dψ∗g for any diffeomorphism
ψ : Ω →Ω which fixes each boundary point, we are looking to recover from dg the metric
g up to such a diffeomorphism. Here, ψ∗g(y) = Dψ(y)tg(ψ(y))Dψ(y) is the pullback of
g by ψ.
It is easy to give counterexamples showing that this cannot be done in general, consider
for instance, the closed hemisphere, where boundary distances are given by boundary arcs
so making the metric larger in the interior does not change dg. Michel [] conjectured
that a simple metric g is uniquely determined, up to an action of a diffeomorphism fixing
the boundary, by the boundary distance function dg(x, y) known for all x and y on ∂Ω.


Wave Phenomena
A metric is called simple if for any two points in Ω, there is a unique length minimizing
geodesic joining them, and if the boundary is strictly convex.
The conjecture of Michel has been proved for two-dimensional simple manifolds [].
In higher dimensions it is open but several partial results are known, including the recent
results of Burago and Ivanov for metrics close to Euclidean [] and close to hyperbolic []
(see the survey []). Earlier and related works include results for simple metrics conformal
to each other [, , , –], for flat metrics [], for locally symmetric spaces of negative
curvature [], for two-dimensional simple metrics with negative curvature [, ], a local
result [], a semiglobal solvability result [], and a result for generic simple metrics [].
In case the metric is not simple, instead of the boundary distance function one can
consider the more general scattering relation which encodes, for any geodesic starting and
ending at the boundary, the start point and direction, the end point and direction, and
the length of the geodesic. We will see in > Sect. ..that also this information can be
determined directly from the response operator. If the metric is simple, then the scattering
relation and boundary distance function are equivalent, and either one is determined by
the other.
The lens rigidity problem is to determine a metric up to isometry from the scattering
relation. There are counterexamples of manifolds which are trapping, and the conjecture
is that on a nontrapping manifold the metric is determined by the scattering relation up to
isometry. We refer to [] and the references therein for known results on this problem.
..
Curvelets and Wave Equations
In > Sect. ..we describe an alternative approach to the analysis of solutions of wave
equations, based on a decomposition of functions into basic elements called curvelets or
wave packets. This approach also works for wave speeds of limited smoothness unlike some
of the approaches presented earlier. Furthermore, the curvelet decomposition yields effi-
cient representations of functions containing sharp wave fronts along curves or surfaces,
thus providing a common framework for representing such data and analyzing wave phe-
nomena and imaging operators. Curvelets and related methods have been proposed as
computational tools for wave imaging, and the numerical aspects of the theory are a subject
of ongoing research.
A curvelet decomposition was introduced by Smith [] to construct a solution oper-
ator for the wave equation with C,sound speed, and to prove Strichartz estimates for
such equations. This started a body of research on Lp estimates for low-regularity wave
equations based on curvelet type methods, see, for instance, Tataru [–], Smith [],
and Smith and Sogge []. Curvelet decompositions have their roots in harmonic analysis
and the theory of Fourier integral operators, where relevant works include Córdoba and
Fefferman [] and Seeger et al. [] (see also Stein []).
In a rather different direction, curvelet decompositions came up in image analysis as an
optimally sparse way of representing images with Cedges, see Candés and Donoho []

Wave Phenomena 

(the name “curvelet” was introduced in []). The property that curvelets yield sparse
representations for wave propagators was studied in Candés and Demanet [, ]. Numer-
ical aspects of curvelet-type methods in wave computation are discussed in [, ].
Finally, both theoretical and practical aspects of curvelet methods related to certain seismic
imaging applications are studied in [, , , , ].
.
Mathematical Modeling and Analysis
..
Boundary Control Method
...
Inverse Problems on Riemannian Manifolds
Let Ω ⊂Rn be an open, bounded set with smooth boundary ∂Ω and let c(x) be a scalar-
valued positive function in C∞(Ω), modeling the wave speed in Ω. We consider the closure
Ω as a differentiable manifold M with a smooth, nonempty boundary. We consider also a
more general case, and allow (M, g)to be a possibly non-compact, complete manifold with
boundary. This means that the manifold contains its boundary ∂M and M is complete with
metric dg defined below. Moreover, near each point x ∈M there are coordinates (U, X),
where U ⊂M is a neighborhood of x and X : U →Rn if x is an interior point, or X : U →
Rn−×[,∞) is x is a boundary point such that for any coordinate neighborhoods (U, X)
and (̃U, ̃X), the transition functions X ○̃X−: ̃X(U ∩̃U) →X(U ∩̃U) are C∞-smooth.
Note that all compact Riemannian manifolds are complete according to this definition.
Usually we denote the components of X by X(y) = (x(y), . . ., xn(y)).
Let u be the solution of the wave equation
utt(x, t) + Au(x, t) = 
in
M × R+,
(.)
u∣t== ,
ut∣t== ,
B ,ηu∣∂M×R+ = f .
Here, f ∈C∞
(∂M × R+) is a real-valued function, A = A(x, D) is an elliptic partial
differential operator of the form
Av = −
n
∑
j,k=
μ(x)−∣g(x)∣−
∂
∂x j (μ(x)∣g(x)∣

g jk(x) ∂v
∂xk (x)) + q(x)v(x), (.)
where g jk(x) is a smooth, symmetric, real, positive definite matrix, ∣g∣= det(g jk(x))
−,
and μ(x) > and q(x) are smooth real-valued functions. On existence and proper-
ties of the solutions of > Eq. (.), see []. The inverse of the matrix (g jk(x))
n
j,k=,
denoted (g jk(x))
n
j,k=defines a Riemannian metric on M. The tangent space of M at x
is denoted by Tx M and it consists of vectors p which in local coordinates (U, X), X(y) =
(x(y), . . ., xn(y)) are written as p = ∑n
k=pk
∂
∂xk . Similarly, the cotangent space T∗
x M


Wave Phenomena
of M at x consists of covectors which are written in the local coordinates as ξ = ∑n
k=ξkdxk.
The inner product which g determines in the cotangent space T∗
x M of M at the point x
is denoted by ⟨ξ, η⟩g = g(ξ, η) = ∑n
j,k=g jk(x)ξ jηk for ξ, η ∈T∗
x M. We use the same
notation for the inner product at the tangent space TxM, that is, ⟨p, q⟩g = g(p, q) =
∑n
j,k=g jk(x)pjqk for p, q ∈TxM.
The metric defines a distance function, which we call also the travel time function,
dg(x, y) = inf ∣μ∣,
∣μ∣= ∫

⟨∂sμ(s), ∂sμ(s)⟩/
g ds,
where ∣μ∣denotes the length of the path μ, and the infimum is taken over all piecewise
C-smooth paths μ : [,] →M with μ() = x and μ() = y.
We define the space L(M, dVμ) with inner product
⟨u,v⟩L(M,dVμ ) = ∫M u(x)v(x) dVμ(x),
where dVμ = μ(x)∣g(x)∣/dxdx. . . dxn. By the above assumptions, A is formally
selfadjoint, that is,
⟨Au,v⟩L(M,dVμ ) = ⟨u, Av⟩L(M,dVμ )
for u,v ∈C∞
(Mint).
Furthermore, let
B ,ηv = −∂v + ηv,
where η : ∂M →R is a smooth function and
∂v =
n
∑
j,k=
μ(x)g jk(x) k
∂
∂x j v(x),
where
(x) = ( ,
, . . . ,
m) is the interior conormal vector field of ∂M, satisfying
∑n
j,k=g jk
jξk = for all cotangent vectors of the boundary, ξ ∈T∗(∂M). We assume
that
is normalized, so that ∑n
j,k=g jk
j
k = . If M is compact, then the operator A in the
domain D(A) = {v ∈H(M) : ∂v∣∂M = }, where Hs(M) denotes the Sobolev spaces
on M, is an unbounded selfadjoint operator in L(M, dVμ).
An important example is the operator
A= −c(x)Δ + q(x)
(.)
on a bounded smooth domain Ω ⊂Rn with ∂v = c(x)−n+∂nv, where ∂nv is the Euclidean
normal derivative of v.
We denote the solutions of (> .) by
u(x, t) = u f (x, t).
For the initial boundary value problem (> .) we define the nonstationary Robin-to-
Dirichlet map, or the response operator Λ by
Λ f = u f ∣∂M×R+.
(.)

Wave Phenomena 

The finite time response operator ΛT corresponding to the finite observation time T > 
is given by
ΛT f = u f ∣∂M×(,T).
(.)
For any set B ⊂∂M × R+, we denote L(B) = {f ∈L(∂M × R+) : supp(f ) ⊂B}. This
means that we identify the functions and their zero continuations.
By [], the map ΛT can be extended to bounded linear map ΛT : L(B) →H/(∂M ×
(, T)) when B ⊂∂M × (, T) is compact. Here, Hs(∂M × (, T)) denotes the Sobolev
space on ∂M × (, T). Below we consider ΛT also as a linear operator ΛT : L
cpt(∂M ×
(, T)) →L(∂M × (, T)), where L
cpt(∂M × (, T)) denotes the compactly supported
functions in L(∂M × (, T)).
For t > and a relatively compact open set Γ ⊂∂M, let
M(Γ, t) = {x ∈M : dg(x, Γ) < t}.
(.)
This set is called the domain of influence of Γ at time t.
When Γ ⊂∂M is an open relatively compact set and f ∈C∞
(Γ × R+), it follows from
finite speed of wave propagation (see, e.g., []) that the wave u f (t) = u f (⋅, t) is supported
in the domain M(Γ, t), that is,
u f (t) ∈L(M(Γ, t)) = {v ∈L(M) : supp(v) ⊂M(Γ, t)}.
(.)
We will consider the boundary of the manifold ∂M with the metric g∂M = ι∗g inherited
from the embedding ι : ∂M →M. We assume that we are given the boundary data, that is,
the collection
(∂M, g∂M) and Λ,
(.)
where (∂M, g∂M) is considered as a smooth Riemannian manifold with a known differ-
entiable and metric structure and Λ is the nonstationary Robin-to-Dirichlet map given in
(> .).
Our goal is to reconstruct the isometry type of the Riemannian manifold (M, g), that
is, a Riemannian manifold which is isometric to the manifold (M, g).This is often stated by
saying that we reconstruct (M, g)up to an isometry. Our next goal is to prove the following
result:
Theorem 
Let (M, g) to be a smooth, complete Riemannian manifold with a nonempty
boundary. Assume that we are given the boundary data (> .). Then it is possible to
determine the isometry type of manifold (M, g).
...
From Boundary Distance Functions to Riemannian Metric
In order to reconstruct (M, g) we use a special representation, the boundary distance repre-
sentation, R(M), of M and later show that the boundary data (> .) determine R(M).
We consider next the (possibly unbounded) continuous functions h : C(∂M) →R. Let us


Wave Phenomena
choose a spesific point Q∈∂M and a constant C> and using these, endow C(∂M)
with the metric
dC(h, h) = ∣h(Q) −h(Q)∣+ sup
z∈∂M
min(C,∣h(z) −h(z)∣).
(.)
Consider a map R : M →C(∂M),
R(x) = rx(⋅);
rx(z) = dg(x, z), z ∈∂M,
(.)
that is, rx(⋅) is the distance function from x ∈M to the points on ∂M. The image R(M) ⊂
C(∂M) of R is called the boundary distance representation of M. The set R(M) is a metric
space with the distance inherited from C(∂M) which we denote by dC, too. The map R,
due to the triangular inequality, is Lipschitz,
dC(rx, ry) ≤dg(x, y).
(.)
We note that when M is compact and C= diam (M), the metric dC : C(∂M) →R is a
norm which is equivalent to the standard norm ∥f ∥∞= maxx∈∂M ∣f (x)∣of C(∂M).
We will see below that the map R : M →R(M) ⊂C(∂M) is an embedding. Many
results of differential geometry, such as Whitney or Nash embedding theorems, concern the
question how an abstract manifold can be embedded to some simple space such as a higher
dimensional Euclidean space. In the inverse problem we need to construct a “copy” of the
unknown manifold in some known space, and as we assume that the boundary is given,
we do this by embedding the manifold M to the known, although infinite dimensional
function space C(∂M).
Next we recall some basic definitions on Riemannian manifolds, see, for example, []
for an extensive treatment. A path μ : [a, b] →N is called a geodesic if, for any c ∈[a, b]
there is ε > such that if s, t ∈[a, b] such that c −ε < s < t < c + ε, the path μ([s, t]) is a
shortest path between its endpoints, that is,
∣μ([s, t])∣= dg(μ(s), μ(t)).
In the future, we will denote a geodesic path μ by γ and parameterize γ with its arclength
s, so that ∣μ([s, s])∣= dg(μ(s), μ(s)). Let x(s),
x(s) = (x(s), . . ., xn(s)),
be the representation of the geodesic γ in local coordinates (U, X). In the interior of
the manifold, that is, for U ⊂Mint the path x(s) satisfies the second-order differential
equations
dxk(s)
ds
= −
n
∑
i,j=
Γk
i j(x(s))dxi(s)
ds
dx j(s)
ds
,
(.)
where Γk
i j are the Christoffel symbols, given in local coordinates by the formula
Γk
i j(x) =
n
∑
p=

gkp(x)(∂g jp
∂xi (x) + ∂gip
∂x j (x) −∂gi j
∂x p (x)).

Wave Phenomena 

Let y ∈M and ξ ∈TxM be a unit vector satisfying the condition g(ξ, (y)) > in the
case when y ∈∂M. Then, we can consider the solution of the initial value problem for the
differential equation (> .) with the initial data
x() = y,
dx
ds () = ξ.
This initial value problem has a unique solution x(s) on an interval [, s(y, ξ)) such that
s(y, ξ) > is the smallest value s> for which x(s) ∈∂M, or s(y, ξ) = ∞in case no
such sexists. We will denote x(s) = γy,ξ(s) and say that the geodesic is a normal geodesic
starting at y if y ∈∂M and ξ = (y).
Example 
In the case when (M, g) is such a compact manifold that all geodesics are the
shortest curves between their endpoints and all geodesics can be continued to geodesics
that hit the boundary, we can see that the metric spaces (M, dg) and (R(M), ∥⋅∥∞) are
isometric. Indeed, for any two points x, y ∈M, there is a geodesic γ from x to a boundary
point z, which is a continuation of the geodesic from x to y. As in the considered case the
geodesics are distance minimizing curves, we see that
rx(z) −ry(z) = dg(x, z) −dg(y, z) = dg(x, y),
and thus ∥rx −ry∥∞≥dg(x, y). Combining this with the triangular inequality, we see that
∥rx −ry∥∞= dg(x, y) for x, y ∈M and R is isometry of (M, dg) and (R(M), ∥⋅∥∞).
Notice that when even M is a compact manifold, the metric spaces (M, dg) and
(R(M),∥⋅∥∞) are not always isometric. As an example, consider a unit sphere in Rwith a
small circular hole near the South pole of, say, diameter ε. Then, for any x, y on the equator
and z ∈∂M, π/−ε ≤rx(z) ≤π/and π/−ε ≤ry(z) ≤π/. Then dC(rx, ry) ≤ε, while
dg(x, y) may be equal to π.
Next, we introduce the boundary normal coordinates on M. For a normal geodesic
γz, (s) starting from z ∈∂M consider dg(γz, (s), ∂M). For small s,
dg(γz, (s), ∂M) = s,
(.)
and z is the unique nearest point to γz, (s) on ∂M. Let τ(z) ∈(,∞] be the largest value
for which (> .) is valid for all s ∈[, τ(z)]. Then for s > τ(z),
dg(γz, (s), ∂M) < s,
and z is no more the nearest boundary point for γz, (s). The function τ(z) ∈C(∂M) is
called the cut locus distance function and the set
ω = {γz, (τ(z)) ∈M : z ∈∂M, and τ(z) < ∞},
(.)
is the cut locus of M with respect to ∂M. The set ω is a closed subset of M having zero
measure. In particular, M/ω is dense in M. In the remaining domain M/ω we can use the
coordinates
x ↦(z(x), t(x)),
(.)


Wave Phenomena
where z(x) ∈∂M is the unique nearest point to x and t(x) = dg(x, ∂M). (Strictly speaking,
one also has to use some local coordinates of the boundary, y : z ↦(y(z), . . ., y(n−)(z))
and define that
x ↦(y(z(x)), t(x)) = (y(z(x)), . . . , y(n−)(z(x)), t(x)) ∈Rn,
(.)
are the boundary normal coordinates.) Using these coordinates we show that R : M →
C(∂M) is an embedding. The result of Lemma is considered in detail for compact
manifolds in [].
Lemma 
Let (M, dg) be the metric space corresponding to a complete Riemannian man-
ifold (M, g) with a nonempty boundary. The map R : (M, dg) →(R(M), dC) is a
homeomorphism. Moreover, given R(M) as a subset of C(∂M) it is possible to construct a
distance function dR on R(M) that makes the metric space (R(M), dR) isometric to (M, dg).
Proof
We start by proving that R is a homeomorphism. Recall the following simple result
from topology:
Assume that X and Y are Hausdorff spaces, X is compact and F : X →Y is a
continuous, bijective map from X to Y. Then F : X →Y is a homeomorphism.
Let us next extend this principle. Assume that (X, dX) and (Y, dY) are metric spaces
and let Xj ⊂X, j ∈Z+ be compact sets such that ⋃j∈Z+ Xj = X. Assume that F : X →Y
is a continuous, bijective map. Moreover, let Yj = F(X j) and assume that there is a point
p ∈Y such that
a j = inf
y∈Y/Yj dY(y, p) →∞as j →∞.
(.)
Then by the above, the maps F : ∪n
j=Xj →∪n
j=Yj are homeomorphisms for all n ∈Z+.
Next, consider a sequence yk ∈Y such that yk →y in Y as k →∞. By removing first
elements of the sequence (yk)∞
k=if needed, we can assume that dY(yk, y) ≤. Let now
N ∈Z+ be such that for j > N we have a j > b := dY(y, p) + . Then yk ∈⋃N
j=Yj and
as the map F : ⋃N
j=Xj →⋃N
j=Yj is a homeomorphism, we see that F−(yk) →F−(y)
in X as k →∞. This shows that F−: Y →X is continuous and thus F : X →Y is a
homeomorphism.
By definition, R : M →R(M) is surjective and, by (> .), continuous. In order to
prove the injectivity, assume the contrary, that is, rx(⋅) = ry(⋅) but x ≠y. Denote by zany
point where
min
z∈∂M rx(z) = rx(z).
Then
dg(x, ∂M) = min
z∈∂M rx(z) = rx(z)
(.)
= ry(z) = min
z∈∂M ry(z) = dg(y, ∂M),

Wave Phenomena 

and z∈∂M is a nearest boundary point to x. Let μx be the shortest path from zto x.
Then, the path μx is a geodesic from x to zwhich intersects ∂M first time at z. By using
the first variation on length formula, we see that μx has to hit to znormally, see [].
The same considerations are true for the point y with the same point z. Thus, both x and
y lie on the normal geodesic γz, (s) to ∂M. As the geodesics are unique solutions of a
system of ordinary differential equations (the Hamilton–Jacobi equation (> .)), they
are uniquely determined by their initial points and directions, that is, the geodesics are
non-branching. Thus we see that
x = γz(s) = y,
where s= rx(z) = ry(z). Hence R : M →C(∂M) is injective.
Next, we consider the condition (> .) for R : M →R(M). Let z ∈M and consider
closed sets X j = {x ∈M : dC(R(x), R(z)) ≤j}, j ∈Z+. Then for x ∈X j we have by
definition (> .) of the metric dC that
dg(x, Q) ≤j + dg(z, Q),
implying that the sets Xj, j ∈Z+ are compact. Clearly, ⋃j∈Z+ Xj = X. Let next Yj = R(X j) ⊂
Y = R(M) and p = R(Q) ∈R(M). Then for rx ∈Y/Yj we have
dC(rx, p) ≥rx(Q) −p(Q) = dg(x, Q)
≥j −dg(z, Q) −C→∞as j →∞
and thus the condition (> .) is satisfied. As R : M →R(M) is a continuous, bijective
map, it implies that R : M →R(M) is a homeomorphism.
Next we introduce a differentiable structure and a metric tensor, gR, on R(M) to have
an isometric diffeomorphism
R : (M, g) →(R(M), gR).
(.)
Such structures clearly exists – the map R pushes the differentiable structure of M and the
metric g to some differentiable structure on R(M) and the metric gR := R∗g which makes
the map (> .) an isometric diffeomorphism. Next we construct these coordinates and
the metric tensor in those on R(M) using the fact that R(M) is known as a subset of
C(∂M).
We will start by construction of the differentiable and metric structures on
R(M)/R(ω), where ω is the cut locus of M with respect to ∂M. First, we show that we
can identify in the set R(M) all the elements of the form r = rx ∈R(M) where x ∈M/ω.
To do this, we observe that r = rx with x = γz, (s), s < τ(z) if and only if
() r(⋅) has a unique global minimum at some point z ∈∂M;
() there is ̃r ∈R(M) having a unique global minimum at the same z and r(z) < ̃r(z).
This is equivalent to saying that there is y with ry(⋅) having a unique global minimum
at the same z and rx(z) < ry(z).
Thus we can find R(M/ω) by choosing all those r ∈R(M) for which the above
conditions () and () are valid.


Wave Phenomena
Next, we choose a differentiable structure on R(M/ω) which makes the map R :
M/ω →R(M/ω) a diffeomorphism. This can be done by introducing coordinates near
each r∈R(M/ω). In a sufficiently small neighborhood W ⊂R(M) of rthe coordinates
r ↦(Y(r), T(r)) = (y(argminz∈∂Mr), min
z∈∂M r)
are well defined. These coordinates have the property that the map x ↦(Y(rx), T(rx))
coincides with the boundary normal coordinates (> .) and (> .). When we choose
the differential structure on R(M/ω) that corresponds to these coordinates, the map
R : M/ω →R(M/ω)
is a diffeomorphism.
Next we construct the metric gR on R(M). Let r∈R(M/ω).As above, in a sufficiently
small neighborhood W ⊂R(M) of rthere are coordinates r ↦X(r) := (Y(r), T(r)) that
correspond to the boundary normal coordinates. Let (y, t) = X(r). We consider next
the evaluation function
Kw : W →R,
Kw(r) = r(w),
where w ∈∂M. The inverse of X : W →Rn is well defined in a neighborhood U ⊂Rn of
(y, t) and thus we can define the function
Ew = Kw ○X−: U →R
that satisfies
Ew(y, t) := dg (w,γz(y), (y)(t)),
(y, t) ∈U,
(.)
where γz(y), (y)(t) is the normal geodesic starting from the boundary point z(y) with
coordinates y = (y, . . . , yn−) and (y) is the interior unit normal vector at y.
Let now gR = R∗g be the push-forward of g to R(M/ω). We denote its representation
in X-coordinates by g jk(y, t). Since X corresponds to the boundary normal coordinates,
the metric tensor satisfies
gmm = ,
gαm = ,
α = , . . . , n −.
Consider the function Ew(y, t) as a function of (y, t) with a fixed w. Then its differ-
ential, dEw at point (y, t) defines a covector in T∗
(y,t)(U) = Rn. Since the gradient of a
distance function is a unit vector field, we see from (> .) that
∥dEw(y, t)∥
(g jk) := ( ∂
∂t Ew(y, t))

+
n−
∑
α,β=
(gR)αβ(y, t)∂Ew
∂yα (y, t)∂Ew
∂yβ (y, t) = .
Let us next fix a point (y, t) ∈U. Varying the point w ∈∂M we obtain a set of covectors
dEw(y, t) in the unit ball of (T∗
(y,t)U, g jk) which contains an open neighborhood of
(, . . .,,). This determines uniquely the tensor g jk(y, t). Thus we can construct the
metric tensor in the boundary normal coordinates at arbitrary r ∈R(M/ω). This means
that we can find the metric gR on R(M/ω) when R(M) is given.

Wave Phenomena 

To complete the reconstruction, we need to find the differentiable structure and the
metric tensor near R(ω).Let r() ∈R(ω) and x() ∈Mint be such a point that r() = rx() =
R(x()). Let zbe some of the closest points of ∂M to the point x(). Then there are points
z, . . . , zn−on ∂M, given by z j = μz,θ j(s), where μz,θ j(s) are geodesics of (∂M, g∂M)
and θ, . . . , θn−are orthonormal vectors of Tz(∂M)with respect to metric g∂M and s> 
is sufficiently small, so that the distance functions y ↦dg(zi, y), i = ,,, . . . , n −form
local coordinates y ↦(dg(zi, y))
n−
i=on M in some neighborhood of the point x() (we
omit here the proof which can be found in [, Lemma .]).
Let now W ⊂R(M) be a neighborhood of r() and let ̃r ∈W. Moreover, let V =
R−(W) ⊂M and ̃x = R−(̃r) ∈V. Let us next consider arbitrary points z, . . . , zn−on ∂M.
Our aim is to verify whether the functions x ↦Xi(x) = dg(x, zi), i = ,, . . . , n −form
smooth coordinates in V. As M/ω is dense on M and we have found topological structure
of R(M) and constructed the metric gR on R(M/ω), we can choose r(j) ∈R(M/ω) such
that limj→∞r(j) = ̃r in R(M). Let x(j) ∈M/ω be the points for which r(j) = R(x(j)).
Now the function x ↦(Xi(x))
n−
i=defines smooth coordinates near ̃x if and only if for
functions Zi(r) = Kzi(r) we have
lim
j→∞det ((gR(dZi(r), dZl(r)))
n−
i,l=)∣r=r(j)
(.)
= lim
j→∞det((g(dXi(x), dXl(x)))
n−
i,l=) ∣x=x(j) /= .
Thus for all ̃r ∈W we can verify for any points z, . . . , zn−∈∂M whether the condition
(> .) is valid or not and this condition is valid for all̃r ∈W if and only if the functions
x ↦Xi(x) = dg(x, zi), i = ,, . . . , n −form smooth coordinates in V. Moreover, by the
above reasoning we know that any r() ∈R(ω) has some neighborhood W and some points
z, . . . , zn−∈∂M for which the condition (> .) is valid for all̃r ∈W. By choosing such
points, we find also near r() ∈(ω) smooth coordinates r ↦(Zi(r))
n−
i=which make the
map R : M →R(M) a diffeomorphism near x().
Summarizing, we have constructed differentiable structure (i.e., local coordinates) on
the whole set R(M), and this differentiable structure makes the map R : M →R(M) a
diffeomorphism. Moreover, since the metric gR = R∗g is a smooth tensor, and we have
found it in a dense subset R(M/ω) of R(M), we can continue it in the local coordinates.
This gives us the metric gR on the whole R(M), which makes the map R : M →R(M) an
isometric diffeomorphism.
∎
In the above proof, the reconstruction of the metric tensor in the boundary nor-
mal coordinates can be considered as finding the image of the metric in the travel time
coordinates.
Let us next consider the case when we have an unknown isotropic wave speed c(x) in a
bounded domain Ω ⊂Rn. We will assume that we are given the set Ω and an abstract Rie-
mannian manifold (M, g), which is isometric to Ω endowed with its travel time metric cor-
responding to the wave speed c(x).Also, we assume that we are given a map ψ : ∂Ω →∂M,
which gives the correspondence between the boundary points of Ω and M. Next we show


Wave Phenomena
that it is then possible to find an embedding from the manifold M to Ω which gives us the
wave speed c(x) at each point x ∈Ω. This construction is presented in detail e.g., in [].
For this end, we need first to reconstruct a function σ on M which corresponds to the
function c(x)on Ω. This is done on the following lemma.
Lemma 
Assume we are given a Riemannian manifold (M, g) such that there exists an
open set Ω ⊂Rn and an isometry Ψ : (Ω,(σ(x))
−δi j) →(M, g) and a function α on M
such that α(Ψ(x)) = σ(x). Then knowing the Riemannian manifold (M, g), the restriction
ψ = Ψ∣∂Ω : ∂Ω →∂M, and the boundary value σ∣∂Ω, we can determine the function α.
Proof
First, observe that we are given the boundary value α∣∂M of α(Ψ(x)) = σ(x).
By assumption the metric g on M is conformally Euclidean, that is, the metric tensor, in
some coordinates, has the form g jk(x) = σ(x)−δ jk, where σ(x) > . Hence the function
β =

ln(α), when m = , and β = α(n−)/, when n ≥, satisfies the so-called scalar
curvature equation
Δgβ −kg = 
(n = ),
(.)
(n −)
n −Δgβ −kgβ = 
(n ≥),
(.)
where kg is the scalar curvature of (M, g),
kg(x) =
n
∑
k,j,l=
g jl(x)Rk
jkl(x)
where Ri
jkl is the curvature tensor given in terms of the Christoffel symbols as
Ri
jkl(x) =
∂
∂xk Γi
l j(x) −∂
∂xl Γi
kj(x) +
n
∑
r=
(Γr
l j(x)Γi
kr(x) −Γr
kj(x)Γi
lr(x)).
The idea of these equations is that if β satisfies, for example, > Eq. (.) in the case
m ≥, then the metric β/(n−)g has zero scalar curvature. Together with boundary data
(> .) being given, we obtain Dirichlet boundary value problem for β in M.
Clearly, Dirichlet problem for > Eq. (.) has a unique solution that gives α when
n = . In the case n ≥, to show that this boundary value problem has a unique solution, it
is necessary to check that is not an eigenvalue of the operator (n−)
n−Δg −kg with Dirichlet
boundary condition. Now, the function β = α(n−)/is a positive solution of the Dirichlet
problem for
> Eq. (.) with boundary condition β∣∂M = α(n−)/∣∂M. Assume that
there is another possible solution of this problem,
̃β = vβ,
v > ,
v∣∂M = .
(.)
Then both (M, β/(n−)g) and (M, ̃β/(n−)g) have zero scalar curvatures. Denoting g=
β/(n−)g, g= ̃β/(n−)g, we obtain that v should satisfy the scalar curvature equation
(n −)
n −Δgv −kgv = .

Wave Phenomena 

Here, we have kg= as ghas vanishing scalar curvature. Together with boundary condi-
tion (> .), this equation implies that v ≡, that is, β = ̃β. This immediately yields that
is not the eigenvalue of the Dirichlet operator (> .) because, otherwise, we could
obtain a positive solution ̃β = β + cψ, where ψis the Dirichlet eigenfunction, corre-
sponding to zero eigenvalue, and ∣c∣is sufficiently small. Thus β, and henceforth α, can
be uniquely determined by solving Dirichlet boundary value problems for (> .) and
(> .).
∎
Our next goal is to embed the abstract manifold (M, g) with conformally Euclidean
metric into Ω with metric (σ(x))
−δi j. To achieve this goal, we use the a priori knowledge
that such embedding exists and the fact that we have already constructed α corresponding
to σ(x) on M.
Lemma 
Let (M, g) be a compact Riemannianmanifold, α(x) a positive smooth function
on M, and ψ : ∂Ω →∂M a diffeomorphism. Assume also that there is a diffeomorphism
Ψ : Ω →M such that
Ψ∣∂Ω = ψ,
Ψ∗g = (α(Ψ(x)))
−δi j.
Then, if Ω, (M, g), α, and ψ are known, it is possible to construct the diffeomorphism Ψ
by solving ordinary differential equations.
Proof
Let ζ = (z, τ) be the boundary normal coordinates on M/ω. Our goal is to
construct the coordinate representation for Ψ−= X,
X : M/ω →Ω,
X(z, τ) = (x(z, τ), . . ., xn(z, τ)).
Denote by hi j(x) = α(Ψ(x))
−δi j the metric tensor in Ω. Let Γi,jk = ∑p gipΓp
jk be the
Christoffel symbols of (Ω, hi j) in the Euclidean coordinates and let ̃Γσ,μ
be Christoffel
symbols of (M, g),in ζ-coordinates. Next, we consider functions hi j, Γk,i j, etc. as functions
on M/ω in (z, τ)-coordinates evaluated at the point x = x(z, τ), for example, Γk,i j(z, τ) =
Γk,i j(x(z, τ)). Then, since Ψ is an isometry, the transformation rule of Christoffel symbols
with respect to the change of coordinates implies
̃Γσ,μ
=
n
∑
i,j,k=
Γk,i j
∂xi
∂ζ μ
∂x j
∂ζ
∂xk
∂ζσ +
n
∑
i,j=
hi j
∂xi
∂ζσ
∂x j
∂ζ μ∂ζ ,
(.)
where
hi j(z, τ) =

α(Ψ(z, τ))δi j.
(.)


Wave Phenomena
Using > Eqs. (.) and (> .), we can write
∂x j
∂ζ μ ∂ζ
in the form
∂x j
∂ζ μ∂ζ (ζ) =
n
∑
p,σ,μ, =
α(ζ)δ jp⎛
⎝
̃Γσ,μ
∂ζσ
∂x p −
n
∑
n=


∂α−
∂ζσ
× [ ∂ζσ
∂xn δpi + ∂ζσ
∂xi δpn −∂ζσ
∂x p δni] ∂xi
∂ζ μ
∂xn
∂ζ
⎞
⎠.
(.)
As α and ̃Γσ,μ are known as a function of ζ, the right-hand side of (> .) can be written
in the form
∂x j
∂ζ μ∂ζ
= F j
μ, (ζ, ∂x
∂ζ ),
(.)
where F j
μ, are known functions. Choose
= m, so that
∂x j
∂ζ μ∂ζn = d
dτ ( ∂x j
∂ζ μ ).
Then, > Eq. (.) becomes a system of ordinary differential equations along normal
geodesics for the matrix ( ∂x j
∂ζ μ (τ))
n
j,μ=. Moreover, since diffeomorphism Ψ : ∂Ω →∂M
is given, the boundary derivatives ∂x j
∂ζ μ , μ = , . . ., n −, are known for ζn = τ = . By
relation (> .),
∂x j
∂ζn = ∂x j
∂τ = α−∂x j
∂n = −α−nj
for ζn = τ = where n = (n, . . . ,nn) is the Euclidean unit exterior normal vector. Thus,
∂x j
∂τ (z,) are also known. Solving a system of ordinary differential equations (> .) with
these initial conditions at τ = , we can construct ∂x j
∂ζ μ (z, τ) everywhere on M/ω. In particu-
lar, taking μ = n, we find dx j
dτ (z, τ). Using again the fact that (x(z,), . . ., xn(z,)) = ψ(z)
are known, we obtain the functions x j(z, τ), z fixed, ≤τ ≤τ∂M(z), that is, reconstruct
all normal geodesics on Ω with respect to metric hi j. Clearly, this gives us the embedding
of (M, g) onto (Ω, hi j).
∎
Combining the above results we get the following result for the isotropic wave equation.
Theorem 
Let Ω ⊂Rn to be a bounded, open set with smooth boundary and c(x) ∈
C∞(Ω) be a strictly positive function. Assume that we know Ω, c∣∂Ω, and the nonstationary
Robin-to-Neumann map Λ∂Ω. Then it is possible to determine the function c(x).
We note that in Theorem the boundary value c∣∂Ω of the wave speed c(x) can be
determined using the finite velocity of wave propagation (> .) and the knowledge of Ω
and Λ∂Ω, but we will not consider this fact in this chapter.

Wave Phenomena 

...
From Boundary Data to Inner Products of Waves
Let u f (x, t) denote the solutions of the hyperbolic equation (> .), ΛT be the finite
time Robin-to-Dirichlet map for the equation (> .) and let dSg denote the Riemannian
volume form on the manifold (∂M, g∂M). We start with the Blagovestchenskii identity.
Lemma 
Let f , h ∈C∞
(∂M × R+). Then
∫M u f (x, T)uh(x, T) dVμ(x) =
(.)
= 
∫L ∫∂M (f (x, t)(ΛTh)(x, s) −(ΛT f )(x, t)h(x, s)) dSg(x)dtds,
where
L = {(s, t) : ≤t + s ≤T, t < s, t, s > }.
Proof
Let
w(t, s) = ∫M u f (x, t)uh(x, s) dVμ(x).
Then, by integration by parts, we see that
(∂
t −∂
s)w(t, s) = ∫M [∂
tu f (x, t)uh(x, s) −u f (x, t)∂
suh(x, s)] dVμ(x) =
= −∫M[Au f (x, t)uh(x, s) −u f (x, t)Auh(x, s)] dVμ(x) =
= −∫∂M[B ,ηu f (t)uh(s) −u f (t)B ,ηuh(s)] dSg(x) =
= ∫∂M[ΛTu f (x, t)uh(x, s) −u f (x, t)ΛTuh(x, s)] dSg(x).
Moreover,
w∣t== w∣s== ,
∂tw∣t== ∂sw∣s== .
Thus, w is the solution of the initial boundary value problem for the one-dimensional wave
equation in the domain (t, s) ∈[,T] × [,T] with known source and zero initial and
boundary data (> .). Solving this problem, we determine w(t, s) in the domain where
t + s ≤T and t < s (see > Fig. -). In particular, w(T, T) gives the assertion.
∎
The other result is based on the following fundamental theorem by D. Tataru [, ].
Theorem 
Let u(x, t) solve the wave equation utt + Au = in M × R and u∣Γ×(,T) =
∂u∣Γ×(,T) = , where / ≠Γ ⊂∂M is open. Then
u = in KΓ,T,
(.)


Wave Phenomena
t
(T, T )
2T
L
s
⊡Fig. -
Domain of integration in the Blagovestchenskii identity
Γ
t = 0
t = T1
t = 2T1
⊡Fig. -
Double cone of inﬂuence
where
KΓ,T= {(x, t) ∈M × R : dg(x, Γ) < T−∣t −T∣}
is the double cone of influence (see > Fig. -).
(The proof of this theorem, in full generality, is in []. A simplified proof for the
considered case is in [].)
The observability Theorem gives rise to the following approximate controllability:
Corollary 
For any open Γ ⊂∂M and T> ,
clL(M) {u f (⋅, T) : f ∈C∞
(Γ × (, T))} = L(M(Γ, T)).
Here,
M(Γ, T) = {x ∈M : dg(x, Γ) < T} = KΓ,T∩{t = T}

Wave Phenomena 

is the domain of influence of Γ at time Tand L(M(Γ, T)) = {a ∈L(M) : supp(a) ⊂
M(Γ, T)}.
Proof
Let us assume that a ∈L(M(Γ, T)) is orthogonal to all u f (⋅, T), f ∈C∞
(Γ ×
(, T)). Denote by v the solution of the wave equation
(∂
t + A)v = ;
v∣t=T= ,
in M × R,
∂tv∣t=T= a;
B ,ηv∣∂M×R = .
Using integration by parts we obtain for all f ∈C∞
(Γ × (, T))
∫
T

∫∂M f (x, s)v(x, s) dSg(x) ds = ∫M a(x)u f (x, T)dVμ(x) = ,
due to the orthogonality of a and the solutions u f (t). Thus v∣Γ×(,T) = . Moreover, as v is
odd with respect to t = T, that is, v(x, T+ s) = −v(x, T−s), we see that v∣Γ×(T,T) = .
As u satisfies the wave equation, standard energy estimates yield that u ∈C(R; H(M)),
and hence u∣∂M×R ∈C(R; H/(∂M)). Combining the above, we see that v∣Γ×(,T) = ,
and as B ,ηv∣Γ×(,T) = , we see using Theorem that a = .
∎
Recall that we denote u f (t) = u f (⋅, t).
Lemma 
Let T > and Γj ⊂∂M, j = , . . . , J, be nonempty, relatively compact open sets,
≤T−
j < T+
j ≤T. Assume we are given (∂M, g∂M) and the response operator ΛT. This data
determines the inner product
JT
N(f, f) = ∫N u f(x, t)u f(x, t) dVμ(x)
for given t > and f, f∈C∞
(∂M × R+), where
N =
J
⋂
j=
(M (Γj, T+
j )/ M (Γj, T−
j )) ⊂M.
Proof
Let us start with the case when f= f= f and T−
j = for all j = ,, . . . , J.
Let B = ⋃J
j=(Γj × [T −Tj, T]). For all h ∈C∞
(B) it holds by (> .) that
supp(uh(⋅, T)) ⊂N, and thus
∥u f (T) −uh(T)∥
L(M,dVμ )
= ∫N (u f (x, T) −uh(x, T))
dVμ(x) + ∫M/N (u f (x, T))
dVμ(x).
Let χN(x) be the characteristic function of the set N. By Corollary , there is h ∈C∞
(B)
such that the norm ∥χNu f (T) −uh(T)∥L(M,dVμ ) is arbitrarily small. This shows that
JT
N(f, f) can be found by
JT
N(f , f ) = ∥u f (T)∥
L(M,dVμ ) −
inf
h∈C∞
(B) F(h),
(.)


Wave Phenomena
where
F(h) = ∥u f (T) −uh(T)∥
L(M,dVμ ).
As F(h) can be computed with the given data (> .) by Lemma , it follows that we can
determine JT
N(f , f ) for any f ∈C∞
(∂M × R+). Now, since
JT
N(f, f) = 
(JT
N(f+ f, f+ f) −JT
N(f−f, f−f)),
the claim follows in the case when T−
j = for all j = ,, . . . , J.
Let us consider the general case when T−
j may be nonzero. We observe that we can
write the characteristic function χN(x) of the set N = ⋂J
j=(M (Γj, T+
j )/ M (Γj, T−
j )) as
χN(x) =
K
∑
k=
ck χNk(x) −
K
∑
k=K+
ck χNk(x),
where ck ∈R are constants which can be determined by solving a simple linear system of
equations and the sets Nk are of the form
Nk = ⋃
j∈Ik
M(Γj, tj),
where Ik ⊂{,, . . . , J} and tj ∈{T+
j : j = ,, . . . , J} ∪{T−
j : j = ,, . . . , J}. Thus
JT
N(f, f) =
K
∑
k=
ckJT
Nk(f, f) −
K
∑
k=K+
ckJT
Nk(f, f),
where all the terms JT
Nk(f, f) can be computed using the boundary data (> .).
∎
...
From Inner Products of Waves to Boundary Distance
Functions
Let us consider open sets Γj ⊂∂M, j = ,, . . . , J and numbers T+
j > T−
j ≥. For a collection
{(Γj, T+
j , T−
j ) : j = , . . . , J} we define the number
P ({(Γj, T+
j , T−
j ) : j = , . . . , J}) = sup
f
JT
N(f , f ),
where T = (max T+
j ) + ,
N =
J
⋂
j=
(M (Γj, T+
j )/ M (Γj, T−
j ))
and the supremum is taken over functions f
∈
C∞
(∂M × (, T)) satisfying
∥u f (T)∥L(M) ≤. When Γq
j ⊂∂M, j = ,, . . . , J, are open sets, so that Γq
j →{z j} as

Wave Phenomena 

q →∞, that is, {z j} ⊂Γq
j ⊂Γq−
j
for all q and ⋂∞
q=Γ
q
j = {z j}, we denote
P ({(z j, T+
j , T−
j ) : j = , . . . , J}) = lim
q→∞P ({(Γq
j , T+
j , T−
j ) : j = , . . . , J}).
Theorem 
Let {zn}∞
n=be a dense set on ∂M and r(⋅) ∈C(∂M) be an arbitrary
continuous function. Then r ∈R(M) if and only if for all N > it holds that
P ({(z j, r(zn) + 
N , r(zn) −
N ) : j = , . . . , N}) > .
(.)
Moreover, condition (> .) can be verified using the boundary data (> .). Hence
the boundary data determine uniquely the boundary distance representation R(M) of
(M, g) and therefore determines the isometry type of (M, g).
Proof
“If”–part. Let x ∈M and denote for simplicity r(⋅) = rx(⋅). Consider a ball
B/N(x) ⊂M of radius /N and center x in (M, g). Then, for z ∈∂M
B/N(x) ⊂M (z, r(z) + 
N )/M (z, r(z) −
N ).
By Corollary , for any T > r(z) there is f ∈C∞
(∂M × (, T)) such that the function
u f (⋅, T) does not vanish a.e. in B/N(x). Thus for any N ∈Z+ and T = max{r(zn) :
n =,, . . . , N} we have
P ({(z j, r(zn) + 
N , r(zn) −
N ) : j = , . . . , N})
≥∫B/N(x) ∣u f (x, T)∣dVμ(x) > 
“Only if”–part. Let (> .) be valid. Then for all N > there are points
xN ∈AN =
N
⋂
n=
(M (zn, r(zn) + 
N )/M (zn, r(zn) −
N ))
(.)
as the set AN has to have a nonzero measure. By choosing a suitable subsequence of xN
(denoted also by xN), there exists a limit x = limN→∞xN.
Let j ∈Z+. It follows from (> .) that
r(z j) −
N ≤dg(xN, z j) ≤r(z j) + 
N
for all N ≥j.
As the distance function dg on M is continuous, we see by taking limit N →∞that
dg(x, z j) = r(z j),
j = ,, . . ..
Since {z j}∞
j=are dense in ∂M, we see that r(z) = dg(x, z) for all z ∈∂M, that is, r = rx. ∎
Note that this proof provides an algorithm for construction of an isometric copy of
(M, g) when the boundary data (> .) are given.


Wave Phenomena
...
Alternative Reconstruction of Metric via Gaussian Beams
Next we consider an alternative construction of the boundary distance representation
R(M), developed in [, , ]. In the previous considerations, we used in Lemma 
the sets of type N = ⋂J
j=(M (Γj, T+
j )/ M (Γj, T−
j )) ⊂M and studied the norms
∥χNu f (⋅, T)∥L(M). In the alternative construction considered below we need to consider
only the sets N of the form N = M(Γ, T). For this end, we consider solutions u f (x, t)
with special sources f which produce wave packets, called the Gaussian beams [, ]. For
simplicity, we consider just the case when
A = −Δg + q,
and give a very short exposition on the construction of the Gaussian beam solutions.
Details can be found in, for example, in (see e.g., ref.[], Chapter .) where the proper-
ties of Gaussian beams are discussed in detail. In this section, we consider complex valued
solutions u f (x, t).
Gaussian beams, called also “quasiphotons,” are a special class of solutions of the wave
equation depending on a parameter ε > which propagate in a neighborhood of a geodesic
γ = γy,ξ([, L]), g(ξ, ξ) = . Below, we consider first the construction in the case when γ is
in the interior of M.
To construct Gaussian beams we start by considering an asymptotic sum, called formal
Gaussian beam,
Uε(x, t) = Mε exp {−(iε)−θ(x, t)}
N
∑
k=
uk(x, t)(iε)k,
(.)
where x ∈M, t ∈[t−, t+], and Mε = (πε)−n/is the normalization constant. The func-
tion θ(x, t) is called the phase function and uk(x, t), k = ,, . . . , N are the amplitude
functions. A phase function θ(x, t) is associated with a geodesic t ↦γ(t) ∈M if
Im θ(γ(t), t) = ,
(.)
Im θ(x, t) ≥Cdg(x,γ(t))
,
(.)
for t ∈[t−, t+]. These conditions guarantee that for any t the absolute value of Uε(x, t)
looks like a Gaussian function in the x variable which is centered at γ(t). Thus the formal
Gaussian beam can be considered to move in time along the geodesic γ(t). The phase
function can be constructed, so that it satisfies the eikonal equation
( ∂
∂t θ(x, t))

−g jl(x) ∂
∂x j θ(x, t) ∂
∂xl θ(x, t) ≍,
(.)
where ≍means the coincidence of the Taylor coefficients of both sides considered as
functions of x at the points γ(t), t ∈[t−, t+], that is,
v(x, t) ≍
if ∂α
xv(x, t)∣x=γ(t) = for all α ∈Nn and t ∈[t−, t+].

Wave Phenomena 

The amplitude functions uk, k = , . . . , N can be constructed as solutions of the transport
equations
Lθuk ≍(∂
t −Δg + q)uk−,
with u−= .
(.)
Here Lθ is the transport operator
Lθu = ∂tθ∂tu −⟨∇θ,∇u⟩g + (∂
t −Δg) θ ⋅u,
(.)
where ∇u(x, t)= ∑j g jk(x) ∂u
∂xk (x, t) ∂
∂xk is the gradient on (M, g), and ⟨V, W⟩g = ∑n
j=
g jk(x)Vj(x)Wk(x). The following existence result is proven, for example, in [, , ].
Theorem 
Let y ∈Mint, ξ ∈TxM be a unit vector and γ = γy,ξ(t), t ∈[t−, t+] ⊂R be a
geodesic lying in Mint when t ∈(t−, t+).
Then there are functions θ(x, t) and uk(x, t) satisfying (> .)–(> .) and a
solution uε(x, t) of equation
(∂
t −Δg + q)uε(x, t) = ,
(x, t) ∈M × [t−, t+],
(.)
such that
∣uε(x, t) −ϕ(x, t)Uε(x, t)∣≤CNε
̃N(N),
(.)
where ̃N(N) →∞when N →∞. Here ϕ ∈C∞
(M × R) is a smooth cut-off function
satisfying ϕ = near the trajectory {(γ(t), t) : t ∈[t−, t+]} ⊂M × R.
In the other words, for an arbitrary geodesic in the interior of M there is a Gaussian
beam that propagates along this geodesic.
Next we consider a class of boundary sources in (> .) which generate Gaussian
beams. Let z∈∂M, t> , and let x ↦z(x) = (z(x), . . . , zn−(x)) be a local system
of coordinates on W ⊂∂M near z. For simplicity, we denote these coordinates as z =
(z, . . . , zn−) and make computations without reference to the point x. Consider a class of
functions fε = fε,z,t(z, t) on the boundary cylinder ∂M × R, where
fε(z, t) = B ,η ((πε)−n/ϕ(z, t)exp{iε−Θ(z, t)}V(z, t)).
(.)
Here ϕ ∈C∞
(∂M × R) is one near (z, t) and
Θ(z, t) = −(t −t) + 
⟨H(z −z),(z −z)⟩+ i
(t −t),
(.)
where ⟨⋅,⋅⟩is the complexified Euclidean inner product, ⟨a, b⟩= ∑a jbj, and H∈Cn×n
is a symmetric matrix with a positive definite imaginary part, that is, (H)jk = (H)kj and
Im H> , where (Im H)jk = Im(H)jk. Finally, V(z, t) is a smooth function supported
in W × R+, having nonzero value at (z, t). The solution u fε(x, t) of the wave equation
∂
tu −Δgu + qu = ,
in M × R+,
u∣t== ∂tu∣t== ,
(.)
B ,ηu∣∂M×R+ = fε(z, t)


Wave Phenomena
is a Gaussian beam propagating along the normal geodesic γz, . Let S(z) ∈(,∞] be the
smallest values s > , so that γz, (s) ∈∂M, that is, the first time when the geodesic γz,
hits to ∂M, or S(z) = ∞if no such value s > exists. Then the following result in valid
(see, e.g., []).
Lemma 
For any function V ∈C∞
(W ×R+) being one near (z, t), t> , and < t<
S(z) and N ∈Z+ there are CN so that the solution u fε(x, t) of problem (> .) satisfies
estimates
∣u fε(x, t) −ϕ(x, t)Uε(x, t)∣≤CNε
̃N(N),
≤t < t+ t
(.)
where Uε(x, t) is of the form (> .), for all < ε < , where ̃N(N) →∞when N →∞
and ϕ ∈C∞
(M × R) is ϕ one near the trajectory {(γz, (t), t + t) : t ∈[, t]} ⊂M × R.
Let us denote
Py,τv(x) = χM(y,τ)(x)v(x).
Then, the boundary data (∂M, g∂M) and the operator Λ uniquely determine the values
∥Py,τu f (t)∥L(M) for any f ∈C∞
(∂M × R+), y ∈∂M and t, τ > . Let fε be of form
(> .) and (> .) and uε(x, t) = u f (x, t), f = fε be a Gaussian beam propagating
along γz, described in Lemma . The asymptotic expansion (> .) of a Gaussian beam
implies that for s < S(z) and τ > ,
lim
ε→∥Py,τuε(⋅, s + t)∥L(M) =
⎧⎪⎪⎨⎪⎪⎩
h(s),
for dg(γz, (s), y) < τ,
,
for dg(γz, (s), y) > τ,
(.)
where h(s) is a strictly positive function. By varying τ > , we can find dg(γz, (s), y) =
rx(y), where x = γz, (t). Moreover, we see that S(z) can be determined using the bound-
ary data and (> .) by observing that S(z) is the smallest number S > such that if
tk →S is an increasing sequence, then
dg(γz, (sk), ∂M) = inf
y∈∂M dg(γz, (sk), y) →
as k →∞.
Summarizing, for any z∈∂M we can find S(z) and furthermore, for any ≤t < S(z)
we can find the boundary distance function rx(y) with x = γz, (t). As any point x ∈M
can be represented in this form, we see that the boundary distance representation R(M)
can be constructed from the boundary data using the Gaussian beams.
..
Travel Times and Scattering Relation
We will show in this section that if (Ω, g) is a simple Riemannian manifold then by looking
at the singularities of the response operator we can determine the boundary distance func-
tion dg(x, y), x, y ∈∂Ω, that is, the travel times of geodesics going through the domain.
The boundary distance function is a function of n −variables. Thus the inverse problem

Wave Phenomena 

of determining the Riemannian metric from the boundary distance function is formally
determined in two dimensions and formally overdetermined in dimensions n ≥.
Let Ω ⊂Rn be a bounded domain with smooth boundary. If the response operators for
the two manifolds (Ω, g) and (Ω, g) are the same then we can assume, after a change of
variables which is the identity at the boundary, the two metrics gand ghave the same
Taylor series at the boundary []. Therefore, we can extend both metrics smoothly to be
equal outside Ω and Euclidean outside a ball of radius R. We denote the extensions to Rn
by g j, j = ,, as before. Let uj(t, x, ω) be the solution of the continuation problem
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∂u
∂t−Δg ju j
=
, in Rn × R
u j(x, t)
=
δ(t −x ⋅ω), t < −R,
(.)
where ω ∈Sn−= {x ∈Rn;∣x∣= }.
It was shown in [] that if the response operators for (Ω, g) and (Ω, g) are equal
then the two solutions coincide outside Ω, namely
u(t, x, ω) = u(t, x, ω),
x ∈Rn/Ω.
(.)
In the case that the manifold (Ω, g j), j = ,is simple, we will use methods of geomet-
rical optics to construct solutions of (.) to show that if the response operators of gand
gare the same then the boundary distance functions of the metrics gand gcoincide.
...
Geometrical Optics
Let g denote a smooth Riemannian metric which is Euclidean outside a ball of radius R.
We will construct solutions to the continuation problem for the metric g (which is
either gor g). We fix ω. Let us assume that there is a solution to > Eq. (.) of the form
u(x, t, ω) = a(x, ω)δ(t −ϕ(x, ω)) + v(x, ω),
u = , t < −R,
(.)
where a, ϕ are functions to be determined and v ∈L
loc. Notice that in order to satisfy the
initial conditions in (> .), we require that
a = ,
ϕ(x, ω) = x ⋅ω for x ⋅ω < −R.
(.)
By replacing > Eq. (.) in > Eq. (.), it follows that
∂u
∂t−Δgu = Aδ′′(t −ϕ(x, ω)) + Bδ′(t −ϕ(x, ω))
−(Δga)δ(t −ϕ(x, ω)) + ∂v
∂t−Δgv,
(.)


Wave Phenomena
where
A = a(x, ω)⎛
⎝−
n
∑
i,j=
gi j ∂ϕ
∂xi
∂ϕ
∂x j
⎞
⎠
(.)
B = 
n
∑
j,k=
g jk ∂a
∂xk
∂ϕ
∂x j + aΔgϕ.
(.)
We choose the functions ϕ, a in the expansion (> .) to eliminate the singularities δ′′
and δ′ and then construct v, so that
∂v
∂t−Δgv = (Δga)δ(t −ϕ(x, ω)),
v = , t < −R.
(.)
The Eikonal Equation
In order to solve the equation A = , it is sufficient to solve the equation
n
∑
i,j=
gi j ∂ϕ
∂xi
∂ϕ
∂x j = ,
ϕ(x, ω) = x ⋅ω, x ⋅ω < −R.
(.)
> Equation (.) is known as the eikonal equation. Here we will describe a method,
using symplectic geometry, to solve this equation.
Let Hg(x, ξ) =

(∑n
i,j=gi j(x)ξiξ j −) the Hamiltonian associated to the metric g.
Note that the metric induced by g in the cotangent space T∗Rn is given by the principal
symbol of the Laplace–Beltrami operator g−(x, ξ) = ∑n
i,j=gi j(x)ξiξ j. > Equation (.)
together with the initial condition can be rewritten as
Hg(x, dϕ) = ,
ϕ(x, ω) = x ⋅ω, x ⋅ω < −R,
where dϕ = ∑n
i=
∂ϕ
∂x i dxi is the differential of ϕ.
Let S = {(x, ξ) : Hg(x, ξ) = }, and let Mϕ = {(x,∇ϕ(x)) : x ∈Rn}, then solving
> Eq. (.), is equivalent to finding ϕ such that
Mϕ ⊂S, with Mϕ = {(x, ω); x ⋅ω < −R}.
(.)
In oder to find ϕ so that (> .) is valid, we need to find a Lagrangian submani-
fold L, so that L ⊂S, L = {(x, ω); x ⋅ω < −R} and the projection of T∗Rn to Rn is a
diffeomorphism []. We will construct such a Lagrangian manifold by flowing out from
N = {(x, ω) : x ⋅ω = s and s < −R} by the geodesic flow associated to the metric g. We
recall the definition of geodesic flow.
We define the Hamiltonian vector field associated to Hg
Vg = (∂Hg
∂ξ ,−∂Hg
∂x ).
(.)
The bicharacteristics are the integral curves of Hg
d
ds xm =
n
∑
j=
gmjξ j,
d
ds ξm = −

n
∑
i,j=
∂gi j
∂xm ξiξ j, m = , . . ., n.
(.)

Wave Phenomena 

The projections of the bicharacteristics in the x variable are the geodesics of the metric g
and the parameter s denotes arc length. We denote the associated geodesic flow by
Xg(s) = (xg(s), ξg(s)).
If we impose the condition that the bicharacteristics are in S initially, then they belong
to S for all time, since the Hamiltonian vector field Vg is tangent to S. The Hamiltonian
vector field is transverse to N then the resulting manifold obtained by flowing N along
the integral curves of Vg will be a Lagrangian manifold L contained in S. We shall write
L = Xg(N).
Now the projection of N into the base space is a diffeomorphism, so that L = {(x, dxϕ)}
locally near a point of N. We can construct a global solution of (> .) near Ω if the
manifold is simple. We recall the definition of simple domains.
Deﬁnition 
Let Ω be a bounded domain of Euclidean space with smooth boundary and g
a Riemannian metric on Ω. We say that (Ω, g) is simple if given two points on the boundary
there is a unique minimizing geodesic joining the two points on the boundary and, moreover,
∂Ω is geodesically convex.
If (Ω, g) is simple then we extend the metric smoothly in a small neighborhood, so
that the metric g is still simple. In this case we can solve the eikonal equation globally in a
neighborhood of Ω.
The Transport Equation
The equation B = is equivalent to solving the following equation:
n
∑
i,j=
gi j ∂ϕ
∂x j
∂a
∂xi + a
Δgϕ = .
(.)
> Equation (.) is called the transport equation. It is a vector field equation for
a(x, ω), which is solved by integrating along the integral curves of the vector field v =
∑n
i,j=gi j ∂ϕ
∂x j
∂
∂x i . It is an easy computation to prove that v has length and that the integral
curves of v are the geodesics of the metric g.
The solution of the transport equation (> .) is then given by:
a(x, ω) = exp (−
∫γ Δgϕ),
(.)
where γ is the unique geodesic such that γ() = y, ˙γ() = ω, y⋅ω = and γ passes through
x. If (Ω, g) is a simple manifold then a ∈C∞(Rn).
To end the construction of the geometrical optics solutions, we observe that the
function v(t, x, ω) ∈L
loc by using standard regularity results for hyperbolic equations.
Now we state the main result of this section in the following theorem.
Theorem 
Let (Ω, gi), i = ,be simple manifolds, and assume that the response
operators for (Ω, g) and (Ω, g) are equal. Then dg= dg.


Wave Phenomena
Sketch of proof.
Assume that we have two metrics g, gwith the same response operator.
Then by (> .) the solutions of (> .) are the same outside Ω. Therefore the main
singularity of the solutions in the geometrical optics expansion must be the same outside
Ω. Thus we conclude that
ϕ(x, ω) = ϕ(x, ω),
x ∈Rn/Ω.
(.)
Now ϕj(x, ω) measures the geodesic distance to the hyperplane x ⋅ω = −R in the
metric g. From this we can easily conclude that the geodesic distance between two points
in the boundary for the two metrics is the same, that is dg(x, y) = dg(x, y), x, y ∈∂Ω.
This type of argument was used in [] to study a similar inverse problem for the more
complicated system of elastodynamics. In particular, it is proven in [] that from the
response operator associated to the equations of isotropic elastodynamics one can deter-
mine, under the assumption of simplicity of the metrics, the lengths of geodesics of the
metrics defined by
ds= cp(x)ds
e,
ds= cs(x)ds
e,
(.)
where dse is the length element corresponding to the Euclidian metric, and cp(x) =
√
(λ+μ)
ρ
, cs(x) =
√μ
ρ denote the speed of compressional waves and shear waves
respectively. Here λ, μ are the Lamé parameters and ρ the density.
Using Mukhometov’s result [, ] we can recover both speeds from the response oper-
ator. This shows in particular that if we know the density, one can determine the Lamé
parameters from the response operator. By using the transport equation of geometrical
optics, similar to (> .), and the results on the ray transform (see, e.g., []), Rachele
shows that under certain a priori conditions one can also determine the density ρ [].
...
Scattering Relation
In the presence of caustics (i.e., the exponential map is not a diffeomorphism) the expan-
sion (> .) is not valid since we cannot solve the eikonal equation globally in Ω.
The solution of (> .) is globally a Lagrangian distribution (see, e.g., []). These
distributions can locally be written in the form
u(t, x, ω) = ∫Rm eiϕ(t,x,ω,θ)a(t, x, ω, θ) dθ,
(.)
where ϕ is a phase function and a(t, x, ω) is a classical symbol.
Every Lagrangian distribution is determined (up to smoother terms) by a Lagrangian
manifold and its symbol. The Lagrangian manifold associated to u(t, x, ω) is the flow out
from t = x ⋅ω, t < −R by the Hamilton vector field of pg(t, x, τ, ξ) = τ−∑n
j,k=g jk(x)ξ jξk.
Here (τ, ξ) are the dual variables to (t, x), respectively. The projection in the (x, ξ) vari-
ables of the flow is given by the flow out from N by geodesic flow, that is, the Lagrangian
submanifold L described above.

Wave Phenomena 

The scattering relation (also called lens map) Cg ⊂(T∗(R×∂Ω)/)×(T∗(R×∂Ω)/)
of a metric g = (gi j) on Ω with dual metric g−= (gi j) is defined as follows. Consider
bicharacteristic curves, γ : [a, b] →T∗(Ω × R), of the Hamilton function pg(t, x, τ, ξ),
which satisfy the following: γ(]a, b[) lies in the interior, γ intersects the boundary non-
tangentially at γ(a) and γ(b), and time increases along γ. Then the canonical projection
from (T∗
R×∂Ω(R × Ω)/) × (T∗
R×∂Ω(R × Ω)/) onto (T∗(R × ∂Ω)/) × T∗(R × ∂Ω)/)
maps the endpoint pair (γ(b),γ(a))to a point in Cg. In other words, Cg gives the geodesic
distance between points in the boundary and also the points of exit and direction of exit
of the geodesic if we know the point of entrance and direction of entrance.
It is well known that Cg is a homogeneous canonical relation on ((T∗(R × ∂Ω)/) ×
(T∗(R × ∂Ω)/). (See [] for the concept of a scattering relation.) Cg is, in fact, a
diffeomorphism between open subsets of T∗(R × ∂Ω)/.
In analogy with Theorem we have the following theorem.
Theorem 
Let gi, i = ,be Riemannian metrics on Ω such that the response operators
for (Ω, g) and (Ω, g) are equal. Then
Cg= Cg.
Sketch of proof.
Since by (> .) we know the solutions of (> .) outside Ω. There-
fore the associated Lagrangian manifolds to the Lagrangian distributions u j must be the
same outside Ω. By taking the projection of these Lagrangians onto the boundary we get
the desired claim.
In the case that (Ω, g) is simple, the scattering relation does not give any new infor-
mation. In fact ((t, x, τ, ξ),(t, x, τ, ξ)) ∈Cg if t−t= dg(x, x) and ξ j =
−τ ∂d g(x,x)
∂x j
, j = ,. In other words dg is the generating function of the scattering relation.
This result was generalized in [] to the case of the equations of elastodynamics with
residual stress. It is shown that knowing the response operator we can recover the scatter-
ing relations associated to P and S waves. For this one uses Lagrangian distributions with
appropriate polarization.
The scattering relation contains all travel time data; not just information about mini-
mizing geodesics as is the case of the boundary distance function. The natural conjecture
is that on a nontrapping manifold, this is enough to determine the metric up to isometry.
We refer to [] and the references therein for results on this problem.
..
Curvelets and Wave Equations
In this section we will discuss in more detail the use of curvelets in wave imaging. We
begin by explaining the curvelet decomposition of functions, using the standard second
dyadic decomposition of phase space. The curvelets provide tight frames of L(Rn) and
give efficient representations of sharp wave fronts. We then discuss why curvelets are useful
for solving the wave equation. This is best illustrated in terms of the half-wave equation


Wave Phenomena
a
2k
2
k
2
2
k
2
b
2−k
–
⊡Fig. -
A curvelet φγ with γ = (k, ω, x) is concentrated (a) in the frequency domain near a box of
length ∼k and width ∼k/, and (b) in the spatial side near a box of length ∼−k and width
∼−k/
(a first-order hyperbolic equation), where a good approximation to the solution is obtained
by decomposing the initial data in curvelets and then by translating each curvelet along the
Hamilton flow for the equation. Then we explain how one deals with wave speeds of limited
smoothness, and how one can convert the approximate solution operator into an exact one
by doing a Volterra iteration.
The treatment below follows the original approach of Smith [] and focuses on
explaining the theoretical aspects of curvelet methods for solving wave equations. We
refer to the works mentioned in the introduction for applications and more practical
considerations.
...
Curvelet Decomposition
We will explain the curvelet decomposition in its most standard form, as given in [].
In a nutshell, curvelets are functions which are frequency localized in certain frequency
shells and certain directions, according to the second dyadic decomposition and parabolic
scaling. On the spatial side, curvelets are concentrated near lattice points which correspond
to the frequency localization.
To make this more precise, we recall the dyadic decomposition of the frequency space
{ξ ∈Rn} into the ball {∣ξ∣≤} and dyadic shells {k ≤∣ξ∣≤k+}. The second dyadic
decomposition further subdivides each frequency shell {k ≤∣ξ∣≤k+} into slightly over-
lapping “boxes” of width roughly k/(thus each box resembles a rectangle whose major
axis has length ∼k and all other axes have length ∼k/). See
> Fig. -a for an illustra-
tion. The convention that the width (k/) of the boxes is the square root of the length (k)
is called parabolic scaling; this scaling is crucial for the wave equation as will be explained
later.

Wave Phenomena 

In the end, the second dyadic decomposition amounts to having a collection of non-
negative functions h, hω
k
∈C∞
c (Rn), which form a partition of unity in the sense
that
= h(ξ)+
∞
∑
k=
∑
ω
hω
k (ξ).
Here, for each k, ω runs over roughly (n−)k/unit vectors uniformly distributed over the
unit sphere, and hω
k is supported in the set
k−/≤∣ξ∣≤k+/,
∣ξ
∣ξ∣−ω∣≤−k/.
We also require a technical estimate for the derivatives
∣⟨ω, ∂ξ⟩j∂α
ξ hω
k (ξ)∣≤C j,α−k(j+∣α∣/),
with C j,α independent of k and ω. Such a partition of unity is not hard to construct, we
refer to [, Sect...] for the details.
On the frequency side, a curvelet at frequency level k with direction ω will be sup-
ported in a rectangle with side length ∼k in direction ω and side lengths ∼k/in the
orthogonal directions. By the uncertainty principle, on the spatial side one expects a
curvelet to be concentrated in a rectangle with side length ∼−k in direction ω and ∼−k/
in other directions. Motivated by this, we define a rectangular lattice Ξω
k in Rn, which has
spacing −k in direction ω and spacing −k/in the orthogonal directions, thus
Ξω
k =
⎧⎪⎪⎨⎪⎪⎩
x ∈Rn ; x = a−kω +
n
∑
j=
bj−k/ω j where a, bj ∈Z
⎫⎪⎪⎬⎪⎪⎭
and {ω, ω, . . . , ωn} is a fixed orthonormal basis of Rn. See > Fig. -b.
We are now ready to give a definition of the curvelet frame.
Deﬁnition 
For a triplet γ = (k, ω, x) with ω as described above and for x ∈Ξω
k , we
define the corresponding fine scale curvelet φγ in terms of its Fourier transform by
ˆφγ(ξ) = (π)−n/−k(n+)/e−ix⋅ξhω
k (ξ).
The coarse scale curvelets for γ = (, x) with x ∈Zn are given by
ˆφγ(ξ) = (π)−n/e−ix⋅ξh(ξ).
The distinction between coarse-and fine-scale curvelets is analogous to the case of
wavelets. The coarse-scale curvelets are used to represent data at low frequencies {∣ξ∣≤}
and they are direction independent, whereas the fine-scale curvelets depend on the
direction ω.
The next list collects some properties of the (fine-scale) curvelets φγ.


Wave Phenomena
•
Frequency localization. The Fourier transform ˆφγ(ξ) is supported in the shell {k−/<
∣ξ∣< k+/} and in a rectangle with side length ∼k in the ω direction and side length
∼k/in directions orthogonal to ω.
•
Spatial localization. The function φγ(y) is concentrated in (i.e., decays away from) a
rectangle centered at x ∈Ξω
k , having side length −k in the ω direction and side lengths
−k/in directions orthogonal to ω.
•
Tight frame. Any function f ∈L(Rn) may be written in terms of curvelets as
f (y) = ∑
γ
cγφγ(y),
where cγ are the curvelet coefficients of f :
cγ = ∫Rn f (y)φγ(y) dy.
One has the Plancherel identity
∫Rn∣f (y)∣dy = ∑
γ
∣cγ∣.
The last statement about how to represent a function f ∈L(Rn) in terms of curvelets
can be proved by writing
ˆf (ξ) = h(ξ)ˆf (ξ) +
∞
∑
k=
∑
ω
hω
k (ξ)ˆf (ξ)
and then by expanding the functions hω
k (ξ) ˆf (ξ) in Fourier series in suitable rectangles,
and finally by taking the inverse Fourier transform. Note that any Lfunction can be rep-
resented as a superposition of curvelets φγ, but that the φγ are not orthogonal and the
representation is not unique.
...
Curvelets and Wave Equations
Next we explain, in a purely formal way, how curvelets can be used to solve the Cauchy
problem for the wave equation
(∂
t + A(x, Dx))u(t, x) = F(t, x)
in R × Rn,
u(, x) = u(x),
∂tu(, x) = u(x).
Further details and references are given in the next section. Here A(x, Dx)
=
∑n
j,k=g jk(x)Dx jDxk is a uniform elliptic operator, meaning that g jk = gkj and < λ ≤
∑n
j,k=g jk(x)ξ jξk ≤Λ < ∞uniformly over x ∈Rn and ξ ∈Sn−. We assume that g jk are
smooth and have uniformly bounded derivatives of all orders.

Wave Phenomena 

It is enough to construct an operator S(t) : u↦u(t, ⋅) such that u(t, x) =
(S(t)u)(x) solves the above wave equation with F ≡and u≡. Then, by Duhamel’s
principle, the general solution of the above equation will be
u(t, x) = ∫
t
S(t −s)F(s, x) ds + (∂tS(t)u)(x) + (S(t)u)(x).
To construct S(t), we begin by factoring the wave operator ∂
t + A(x, Dx) into two first-
order hyperbolic operators, known as half-wave operators. Let P(x, Dx) =
√
A(x, Dx) be
a formal square root of the elliptic operator A(x, Dx). Then we have
∂
t + A(x, Dx) = (∂t −iP)(∂t + iP)
and the Cauchy problem for the wave equation with data F ≡, u≡, u= f is reduced
to solving the two first-order equations
(∂t −iP)v = ,
v() = f ,
(∂t + iP)u = v,
u() = .
If one can solve the first equation, then solvability of the second equation will follow from
Duhamel’s principle (the sign in front of P is immaterial).
Therefore, we only need to solve
(∂t −iP)v(t, x) = ,
v(, x) = f (x).
For the moment, let us simplify even further and assume that A(x, Dx) is the Laplacian
−Δ, so that P will be the operator given by
̂
P f (ξ) = ∣ξ∣ˆf (ξ).
Taking the spatial Fourier transform of the equation for v and solving the resulting ordinary
differential equation gives the full solution
v(t, y) = (π)−n ∫Rn ei(y⋅ξ+t∣ξ∣) ˆf (ξ) dξ.
Thus, the solution is given by a Fourier integral operator acting on f :
v(t, y) = (π)−n ∫Rn eiΦ(t,y,ξ)a(t, y, ξ) ˆf(ξ) dξ.
In this particular case, the phase function is Φ(t, y, ξ) = y ⋅ξ + t∣ξ∣and the symbol is
a(t, y, ξ) ≡.
So far we have not used any special properties of f . Here comes the key point. If f is a
curvelet, then the phase function is well approximated on supp(f ) by its linearization in ξ:
Φ(t, y, ξ) ≈∇ξΦ(t, y, ω) ⋅ξ
for ξ ∈supp(f ).
(This statement may seem somewhat mysterious, but it really is one reason why curvelets
are useful for wave imaging. A slightly more precise statement is as follows: if Ψ(t, y, ξ) is


Wave Phenomena
smooth for ξ ≠, homogeneous of order in ξ, and its derivatives are uniformly bounded
over t ∈[−T, T] and y ∈Rn and ξ ∈Sn−, then
∣Ψ(t, y, ξ) −∇ξΨ(t, y, ω) ⋅ξ∣≲
whenever ξ ⋅ω ∼k and ∣ξ −(ξ ⋅ω)ω∣≲k/. Also the derivatives of Ψ(t, y, ξ) −
∇ξΨ(t, y, ω) ⋅ξ satisfy suitable symbol bounds. Parabolic scaling is crucial here, we refer
to [, Sect. ..] for more on this point.) Thus, if f = φγ then the solution v with this
initial data is approximately given by
v(t, y) ≈(π)−n ∫Rn ei(y+tω)⋅ξ ˆφγ(ξ) dξ = φγ(y + tω).
Thus the half-wave equation for P =
√
−Δ, whose initial data is a curvelet in direction ω,
is approximately solved by translating the curvelet along a straight line in direction ω.
We now return to the general case, where A(x, ξ) is a general elliptic symbol
∑n
j,k=g jk(x)ξ jξk. We define
p(x, ξ) =
√
A(x, ξ).
Then p is homogeneous of order in ξ, and it generates a Hamilton flow (x(t), ξ(t)) in the
phase space T∗Rn = Rn × Rn, determined by the ordinary differential equations
˙x(t) = ∇ξp(x(t), ξ(t)),
˙ξ(t) = −∇x p(x(t), ξ(t)).
If A(x, ξ) is smooth then the curves (x(t), ξ(t)) starting at some point (x(), ξ()) =
(x, ω) are smooth and exist for all time. Note that if p(x, ξ) = ∣ξ∣then one has straight
lines (x(t), ξ(t)) = (x + tω, ω).
Similarly as above, the half-wave equation
(∂t −iP)v(t, x) = ,
v(, x) = f (x)
can be approximately solved as follows:
. Write the initial data f in terms of curvelets as f (y) = ∑γ cγφγ(y).
. For a curvelet φγ(y) centered at x pointing in direction ω, let φγ(t, y) be another
curvelet centered at x(t) pointing in direction ξ(t). That is, translate each curvelet φγ
for time t along the Hamilton flow for P.
. Let v(t, y) = ∑γ cγφγ(t, y) be the approximate solution.
Thus the wave equation can be approximately solved by decomposing the initial data into
curvelets and then by translating each curvelet along the Hamilton flow.

Wave Phenomena 

...
Low Regularity Wave Speeds and Volterra Iteration
Here we give some further details related to the formal discussion in the previous section,
following the arguments in []. The precise assumption on the coefficients will be
g jk(x) ∈C,(Rn).
This means that ∂αg jk ∈L∞(Rn) for ∣α∣≤, which is a minimal assumption which
guarantees a well-defined Hamilton flow.
As discussed in > Sect. ..., by Duhamel’s formula it is sufficient to consider the
Cauchy problem
(∂
t + A(x, Dx))u(t, x) = 
in R × Rn,
u(, x) = ,
∂tu(, x) = f .
Here, A(x, Dx) = ∑n
j,k=g jk(x)Dx jDxk and g jk ∈C,(Rn), g jk = gkj, and < λ ≤
∑n
j,k=g jk(x)ξ jξk ≤Λ < ∞uniformly over x ∈Rn and ξ ∈Sn−.
To deal with the nonsmooth coefficients, we introduce the smooth approximations
Ak(x, ξ) =
n
∑
i,j=
gi j
k (x)ξiξ j,
gi j
k = χ(−k/Dx)gi j
where χ ∈C∞
c (Rn) satisfies ≤χ ≤, χ(ξ) = for ∣ξ∣≤/, and χ(ξ) = for ∣ξ∣≥. We
have written (χ(−k/Dx)g)ˆ(ξ) = χ(−k/ξ)ˆg(ξ). Thus gi j
k are smooth truncations of gi j
to frequencies ≤k/. We will use the smooth approximation Ak in the construction of the
solution operator at frequency level k, which is in keeping with paradifferential calculus.
Given a curvelet φγ(y) where γ = (k, ωγ, xγ), we wish to consider a curvelet φγ(t, y)
which corresponds to a translation of φγ for time t along the Hamilton flow for Hk(x, ξ) =
√
Ak(x, ξ). In fact, we shall define
φγ(t, y) = φγ (Θγ(t)(y −xγ(t)) + xγ),
where xγ(t) and the n × n matrix Θγ(t) arise as the solution of the equations
˙x = ∇ξHk(x, ω),
˙ω = −∇xHk(x, ω) + (ω ⋅∇xHk(x, ω)) ω,
˙Θ = −Θ (ω ⊗∇xHk(x, ω) −∇xHk(x, ω) ⊗ω)
with initial condition (xγ(), ωγ(), Θγ()) = (xγ, ωγ, I). Here v ⊗w is the matrix with
(v ⊗w)x = (w ⋅x)v. The idea is that (xγ(t), ωγ(t)) is the Hamilton flow for Hk restricted
to the unit cosphere bundle S∗Rn = {(x, ξ) ∈T∗Rn ; ∣ξ∣= }, and Θγ(t) is a matrix
which tracks the rotation of ωγ along the flow and satisfies Θγ(t)ωγ(t) = ωγ for all t. See
> Fig. -for an illustration.


Wave Phenomena
xγ
t = 0
ϕγ(t, ·)
xγ(t )
ωγ(t)
ωγ
ϕγ(·)
⊡Fig. -
The translation of a curvelet φγ for time t along the Hamilton ﬂow
We define an approximate solution operator at frequency level k by
Ek(t)f (y) =
∑
γ′:k′=k
(f , φγ′)L(Rn)φγ′(t, y).
Summing over all frequencies, we consider the operator
E(t)f =
∞
∑
k=
Ek(t)f .
This operator essentially takes a function f , decomposes it into curvelets, and then
translates each curvelet at frequency level k for time t along the Hamilton flow for Hk.
It is proved in [, Theorem .] that E(t) is an operator of order , mapping Hα(Rn)
to Hα(Rn) for any α. The fact that E(t) is an approximate solution operator is encoded in
the result that the wave operator applied to E(t),
T(t) = (∂
t + A(x, Dx)) E(t),
which is a priori a second-order operator, is in fact an operator of order and maps
Hα+(Rn) to Hα(Rn) for −≤α ≤. This is proved in [, Theorem .], and is due
to the two facts. The first one is that when A is replaced by the smooth approximation Ak,
the corresponding operator
∑
k
(∂
t + Ak(x, Dx)) Ek(t)
is of order because the second-order terms cancel. Here, one uses that translation along
the Hamilton flow approximately solves the wave equation. The second fact is that the part
involving the nonsmooth coefficients,
∑
k
(Ak(x, Dx) −A(x, Dx)) Ek(t)
is also of order using that Ak is truncated to frequencies ≤k/and using estimates for
A −Ak obtained from the C,regularity of the coefficients.
To obtain the full parametrix, one needs to consider the Hamilton flows both for √Ak
and −√Ak, corresponding to the two half-wave equations appearing in the factorization of

Wave Phenomena 

the wave operator, and one also needs to introduce corrections to ensure that the initial val-
ues of the approximate solution are the given functions. For simplicity we will not consider
these details here and only refer to [, Sect. ]. The outcome of this argument is an operator
s(t, s), which is strongly continuous in t and s as a bounded operator Hα(Rn) →Hα+(Rn)
satisfies s(t, s)f ∣t=s = and ∂ts(t, s)f ∣t=s = f , and further the operator
T(t, s) = (∂
t + A(x, Dx))s(t, s)
is bounded Hα(Rn) →Hα(Rn) for −≤α ≤.
We conclude this discussion by explaining the Volterra iteration scheme, which is used
for converting the approximate solution operator to an exact one, as in [, Theorem .].
We look for a solution in the form
u(t) = s(t,)f + ∫
t
s(t, s)G(s) ds
for some G ∈L([−t, t]; Hα(Rn)). From the properties of s(t, s), we see that u satisfies
(∂
t + A(x, Dx))u = T(t,)f + G(t) + ∫
t
T(t, s)G(s) ds.
Thus, u is a solution if G is such that
G(t) + ∫
t
T(t, s)G(s) ds = −T(t,)f .
Since T(t, s) is bounded on Hα(Rn) for −≤α ≤, with norm bounded by a uniform
constant when ∣t∣,∣s∣≤t, the last Volterra equation can be solved by iteration. This yields
the required solution u.
.
Conclusion
In this chapter, inverse problems for the wave equation were considered with different
types of data. All considered data correspond to measurements made on the boundary
of a body in which the wave speed is unknown and possibly anisotropic. The case of
the complete data, that is, with measurements of amplitudes and phases of waves cor-
responding to all possible sources on the boundary, was considered using the boundary
control method. We showed that the wave speed can be reconstructed from the boundary
measurements up to a diffeomorphism of the domain. This corresponds to the determi-
nation of the wave speed in the local travel-time coordinates. Next, the inverse problem
with less data, the scattering relation, was considered. The scattering relation consists of
the travel times and the exit directions of the wave fronts produced by the point sources
located on the boundary of the body. Such data can be considered to be obtained by mea-
suring the waves up to smooth errors, or measuring only the singularities of the waves.
The scattering relation is a generalization of the travel time data, that is, the travel times
of the waves through the body. Finally, we considered the use of wavelets and curvelets


Wave Phenomena
in the analysis of the waves. Using the curvelet representation of the waves, the singu-
larities of the waves can be efficiently analyzed. In particular, the curvelets are suitable
for the simulation of the scattering relation, even when the wave speed is nonsmooth.
Summarizing, in this chapter modern approaches to study inverse problems for wave
equations based on the control theory, the geometry, and the microlocal analysis, were
presented.
.
Cross-References
> Inverse Scattering
> Photoacoustic and Thermoacoustic Tomography: Image Formation Principles
References and Further Reading
. Anderson M, Katsuda A, Kurylev Y, Lassas M,
Taylor M () Boundary regularity for the Ricci
equation, geometric convergence, and Gel’fand’s
inverse boundary problem. Invent Math :
–
. Andersson F, de Hoop MV, Smith HF, Uhlmann G
() A multi-scale approach to hyperbolic evo-
lution equations with limited smoothness. Com-
mun Part Diff Equat (–):–
. Babich VM, Ulin VV () The complex space-
time ray method and “quasiphotons” (Russian).
Zap Nauchn Sem LOMI :–
. Belishev M () An approach to multidi-
mensional inverse problems for the wave equa-
tion (Russian). Dokl Akad Nauk SSSR ():
–
. Belishev M () Boundary control in recon-
struction of manifolds and metrics (the BC
method). Inverse Probl :R–R
. Belishev M, Kachalov A () Boundary control
and quasiphotons in a problem of the reconstruc-
tion of a Riemannian manifold from dynamic
data (Russian). Zap Nauchn Sem POMI :
–
. Belishev M, Kurylev Y () To the reconstruc-
tion of a Riemannian manifold via its spectral
data (BC-method). Commun Part Diff Equat :
–
. Bernstein IN, Gerver ML () Conditions
on distinguishability of metrics by hodographs,
methods
and
algorithms
of
interpretation
of
seismological
information.
Computerized
seismology, vol . Nauka, Moscow, pp –
(in Russian)
. Besson G, Courtois G, Gallot S () Entropies
et rigidités des espaces localement symétriques de
courbure strictment négative. Geom Funct Anal
:–
. Beylkin G () Stability and uniqueness of
the solution of the inverse kinematic problem
in the multidimensional case. J Soviet Math :
–
. Bingham K, Kurylev Y, Lassas M, Siltanen S
() Iterative time reversal control for inverse
problems. Inverse Probl Imaging :–
. Blagovešˇcenskii A () A one-dimensional
inverse boundary value problem for a second
order hyperbolicequation (Russian). Zap Nauchn
Sem LOMI :–
. Blagovešˇcenskii A () Inverse boundary prob-
lem for the wave propagation in an anisotropic
medium (Russian). Trudy Mat Inst Steklova
:–
. Brytik V, de Hoop MV, Salo M () Sensitivity
analysis of wave-equation tomography: a multi-
scale approach. J Fourier Anal Appl ():–

. Burago D, Ivanov S () Boundary rigidity and
filling volume minimality of metrics close to a flat
one. Ann Math ():–

Wave Phenomena 

. Burago D, Ivanov S Area minimizers and bound-
ary rigidity of almost hyperbolic metrics (in
preparation)
. Candès EJ, Demanet L () Curvelets and
Fourier integral operators. C R Math Acad Sci
Paris :–
. Candès EJ, Demanet L () The curvelet repre-
sentation of wave propagators is optimally sparse.
Comm Pure Appl Math :–
. Candès EJ, Donoho DL () Curvelets - a sur-
prisingly effective nonadaptive representation for
objects with edges. In: Schumaker LL et al (eds)
Curves and surfaces. Vanderbilt University Press,
Nashville, pp –
. Candès EJ, Donoho DL () New tight frames
of curvelets and optimal representations of
objects with piecewise Csingularities. Commun
Pure Appl Math :–
. Candès EJ, Demanet L, Ying L () Fast com-
putation of Fourier integral operators. SIAM J Sci
Comput :–
. Chavel I () Riemannian geometry. A modern
introduction. Cambridge University Press, Cam-
bridge, xvi+pp
. Córdoba A, Fefferman C () Wave packets
andFourier integral operators.CommunPartDiff
Equat :–
. Creager KC () Anisotropy of the inner core
from differential travel times of the phases PKP
and PKIPK. Nature :–
. Croke C () Rigidity for surfaces of non-
positive curvature. Comment Math Helv :
–
. Croke C () Rigidity and the distance between
boundary points. J Diff Geom ():–
. Dahl M, Kirpichnikova A, Lassas M ()
Focusing waves in unknown media by modified
time reversal iteration. SIAM J Control Optim
:–
. de Hoop MV () Microlocal analysis of seis-
mic inverse scattering: inside out. In: Uhlmann G
(ed)
Inverse
problems
and
applications.
Cambridge University Press, Cambridge, pp
–
. de Hoop MV, Smith H, Uhlmann G, van der
Hilst RD () Seismic imaging with the gen-
eralized Radon transform: a curvelet transform
perspective. Inverse Probl ():–
. Demanet L, Ying L () Wave atoms and
time upscaling of wave equations. Numer Math
():–
. Duchkov AA, Andersson F, de Hoop MV ()
Discrete, almost symmetric wave packets and
multiscale geometric representation of (seis-
mic) waves. IEEE Trans Geosc Remote Sens
():–
. Duistermaat JJ () Fourier integral operators,
Birkhäuser, Boston
. Greenleaf A, Kurylev Y, Lassas M, Uhlmann G
() Invisibility and inverse problems. Bull
Amer Math :–
. Gromov M () Filling Riemannian manifolds.
J Diff Geom ():–
. Guillemin V () Sojourn times and asymptotic
properties of the scattering Matrix. Proceedings
of the Oji seminar on algebraic analysis and the
RIMS symposium on algebraic analysis (Kyoto
University, Kyoto, ). Publ Res Inst Math Sci
(/, Suppl):–
. Hansen S, Uhlmann G () Propagation of
polarization for the equations in elastodynam-
ics with residual stress and travel times. Math
Annalen :–
. Herglotz G () Uber die Elastizitaet der Erde
bei Beruecksichtigung ihrer variablen Dichte.
Zeitschr fur Math Phys :–
. Hörmander L () The analysis of linear par-
tial differential operators III. Pseudodifferential
operators. Springer, Berlin, viii+pp
. Isozaki H, Kurylev Y, Lassas M () For-
ward and Inverse scattering on manifolds with
asymptotically cylindrical ends. J Funct Anal :
–
. Ivanov S Volume comparison via boundary dis-
tances, arXiv:–
. Katchalov A, Kurylev Y () Multidimensional
inverse problem with incomplete boundary spec-
tral data. Commun Part Diff Equat :–
. Katchalov A, Kurylev Y, Lassas M () Inverse
boundary
spectral
problems.
Chapman
&
Hall/CRC Press, Boca Raton, xx+pp
. Katchalov A, Kurylev Y, Lassas M () Energy
measurements and equivalence of boundary data
for inverse problems on non-compact manifolds.
Geometric methods in inverse problems and PDE
control. In: Croke C, Lasiecka I, Uhlmann G,


Wave Phenomena
Vogelius M (eds) IMA volumes in mathemat-
ics and applications, vol . Springer, New York,
pp –
. Katchalov A, Kurylev Y, Lassas M, Mandache N
() Equivalence of time-domain inverse prob-
lems and boundary spectral problem. Inverse
Probl :–
. Katsuda A, Kurylev Y, Lassas M () Stability
of boundary distance representation and recon-
struction of Riemannian manifolds. Inverse Probl
Imaging :–
. Krein MG () Determination of the density of
an inhomogeneous string from its spectrum (in
Russian). Dokl Akad Nauk SSSR ():–
. Kurylev Y () Multidimensional Gel’fand
inverse problem and boundary distance map. In:
Soga H (ed) Inverse problems related to geometry.
Ibaraki University Press, Japan, pp –
. Kurylev Y, Lassas M () Hyperbolic inverse
problem with data on a part of the boundary.
Differential equations and mathematical physics
(Birmingham, AL, ). AMS/IP Stud Adv Math
:–, AMS
. Kurylev Y, Lassas M () Hyperbolic inverse
boundary-value problem and time-continuation
of
the
non-stationary
Dirichlet-to-Neumann
map. Proc Roy Soc Edinburgh Sect A :
–
. Kurylev Y, Lassas M () Inverse problems and
index formulae for Dirac Operators. Adv Math
:–
. Kurylev Y, Lassas M, Somersalo E ()
Maxwell’s equations with a polarization indepen-
dent wave velocity: direct and inverse problems.
J Math Pures Appl :–
. Lasiecka I, Triggiani R () Regularity theory
of hyperbolic equations with Nonhomogeneous
Neumann
boundary conditions. II. General
boundary data. J Diff Equat :–
. Lassas M, Uhlmann G () On determining
a Riemannian manifold from the Dirichlet-to-
Neumann map. Ann Sci Ecole Normale Super-
iéure :–
. Lassas M, Sharafutdinov V, Uhlmann G ()
Semiglobal boundary rigidity for Riemannian
metrics. Math Annalen :–
. Michel R () Sur la rigidité imposée par la
longueur des géodésiques. Invent Math : –
. Mukhometov RG () The reconstruction
problem of
a
two-dimensional
Riemannian
metric, and integral geometry (Russian). Dokl
Akad Nauk SSSR ():–
. Mukhometov RG () A problem of recon-
structing a Riemannian metric. Siberian Math J
:–
. Mukhometov RG, Romanov VG () On the
problem of finding an isotropic Riemannian met-
ric in an n-dimensional space (Russian). Dokl
Akad Nauk SSSR ():–
. Otal JP () Sur les longuer des géodésiques
d’une métrique a courbure négative dans le
disque. Comment Math Helv :–
. Pestov L, Uhlmann G () Two dimensional
simple compact manifolds with boundary are
boundary rigid. Ann Math :–
. Rachele L () An inverse problem in elastody-
namics: determination of the wave speeds in the
interior. J Diff Equat :–
. Rachele L () Uniqueness of the density in
an inverse problem for isotropic Elastodynamics.
Trans Amer Math Soc ():–
. Ralston J () Gaussian beams and propaga-
tion of singularities. Studies in partial differ-
ential equations. MAA Studies in Mathematics,
vol . Mathematical Association of America,
Washington, pp –
. Salo M () Stability for solutions of wave equa-
tions with C,coefficients. Inverse Probl Imaging
():–
. Seeger A, Sogge CD, Stein EM () Regular-
ity properties of Fourier integral operators. Ann
Math :–
. Sharafutdinov V () Integral geometry of ten-
sor fields. VSP, Utrech, The Netherlands
. Smith HF () A parametrix construction for
wave equations with C, coefficients. Ann Inst
Fourier Grenoble ():–
. Smith HF () Spectral cluster estimates for C,
metrics. Amer J Math ():–
. Smith HF, Sogge CD () On the Lp norm
of spectral clusters for compact manifolds with
boundary. Acta Math :–
. Stefanov P, Uhlmann G () Rigidity for met-
rics with the same lengths of geodesics. Math Res
Lett :–

Wave Phenomena 

. Stefanov P, Uhlmann G () Boundary rigidity
and stability for generic simple metrics. J Amer
Math Soc :–
. Stefanov P, Uhlmann G () Local lens rigid-
ity with incomplete data for a class of non-
simple Riemannian manifolds. J Diff Geom :
–
. Stein EM () Harmonic analysis: real-variable
methods, orthogonality, and oscillatory integrals.
Princeton mathematical series, . Monographs
in harmonic analysis, III. Princeton University
Press, Princeton
. Sylvester J () An anisotropic inverse bound-
ary value problem. Comm Pure Appl Math
():–
. Sylvester J, Uhlmann G () A global unique-
ness theorem for an inverse boundary value prob-
lem. Ann Math :–
. Sylvester J, Uhlmann G () Inverse problems in
anisotropic media, Contemp Math :–
. D Tataru: Unique continuation for solutions to
PDEs, between Hörmander’s theorem and Holm-
gren’s theorem. Commun Part Diff Equat :
–
. Tataru D () On the regularity of boundary
traces for the wave equation. Ann Scuola Norm
Sup Pisa CL Sci :–
. Tataru D () Unique continuation for oper-
ators with partially analytic coefficients. J Math
Pures Appl :–
. Tataru D () Strichartz estimates for operators
with nonsmooth coefficients and the nonlinear
wave equation. Amer J Math () –
. Tataru D () Strichartz estimates for second
order hyperbolic operators with nonsmooth coef-
ficients. II. Amer J Math ():–
. Tataru D () Strichartz estimates for second
order hyperbolic operators with nonsmooth coef-
ficients. III. J Amer Math Soc :–
. Uhlmann G () Developments in inverse
problems since Calderón’s foundational paper. In:
Christ M, Kenig C, Sadosky C (eds) Essays in
harmonic analysis and partial differential equa-
tions, Chap. . University of Chicago Press,
Chicago
. Wiechert E, Zoeppritz K () Uber Erdbeben-
wellen. Nachr Koenigl Geselschaft Wiss Goettin-
gen :–


Statistical Methods in
Imaging
Daniela Calvetti⋅Erkki Somersalo
.
Introduction......................................................................
.
Background......................................................................
..
Images in the Statistical Setting........................................................
..
Randomness, Distributions and Lack of Information..............................
..
Imaging Problems.......................................................................
.
Mathematical Modeling and Analysis..........................................
..
Prior Information, Noise Models and Beyond......................................
..
Accumulation of Information and Priors............................................
..
Likelihood: Forward Model and Statistical Properties of Noise..................
..
Maximum Likelihood and Fisher Information.....................................
..
Informative or Noninformative Priors?..............................................
..
Adding Layers: Hierarchical Models.................................................
.
Numerical Methods and Case Examples.......................................
..
Estimators................................................................................
...
Prelude: Least Squares and Tikhonov Regularization..............................
...Maximum Likelihood and Maximum A Posteriori................................
...Conditional Means......................................................................
..
Algorithms...............................................................................
...Iterative Linear Least Squares Solvers................................................
...Nonlinear Maximization...............................................................
...EM Algorithm...........................................................................
...
Markov Chain Monte Carlo Sampling...............................................
..
Statistical Approach: What Is the Gain?.............................................
...Beyond the Traditional Concept of Noise...........................................
...Sparsity and Hypermodels.............................................................
.
Conclusion.......................................................................
.
Cross-References.................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Statistical Methods in Imaging
Abstract: The theme of this chapter is statistical methods in imaging, with a marked
emphasis on the Bayesian perspective. The application of statistical notions and techniques
in imaging requires that images and the available data are redefined in terms of random
variables, the genesis and interpretation of randomness playing a major role in deciding
whether the approach will be along frequentist or Bayesian guidelines. The discussion on
image formation from indirect information, which may come from non-imaging modal-
ities, is coupled with an overview of how statistics can be used to overcome the hurdles
posed by the inherent ill-posedness of the problem. The statistical counterpart to classi-
cal inverse problems and regularization approaches to contain the potentially disastrous
effects of ill-posedness is the extraction and implementation of complementary informa-
tion in imaging algorithms. The difficulty in expressing quantitative and uncertain notions
about the imaging problem at hand in qualitative terms, which is a major challenge in
a deterministic context, can be more easily overcome once the problem is expressed in
probabilistic terms. An outline of how to translate some typical qualitative traits into a
format which can be utilized by statistical imaging algorithms is presented. In line with
the Bayesian paradigm favored in this chapter, basic principles for the construction of pri-
ors and likelihoods are presented, together with a discussion of numerous computational
statistics algorithms, including Maximum Likelihood estimators, Maximum A Posteriori
and Conditional Mean estimators, Expectation Maximization, Markov chain Monte Carlo,
and hierarchical Bayesian models. Rather than aiming to be a comprehensive survey, the
present chapter hopes to convey a wide and opinionated overview of statistical methods in
imaging.
.
Introduction
Images, alone or in sequences, provide a very immediate and effective way of transfer-
ring information, as the human eye–brain complex is extremely well adapted at extracting
quickly their salient features, let them be edges, textures, anomalies, or movement. While
the amount of information that can be compressed in an image is tremendously large and
varied, the image processing ability of the human eye is so advanced to outperform the
most advanced of algorithms. One of the reasons why the popularity of statistical tools in
imaging continues to grow is the flexibility that this modality offers when it comes to utiliz-
ing qualitative attributes of the images or to recover them from indirect, corrupt specimens.
The utilization of qualitative clues to augment scarce data is akin to the process followed
by the eye–brain system.
Statistics, which according to Pierre–Simon Laplace, is “common sense expressed in
terms of numbers,” is well suited for quantifying qualitative attributes. The opportunity to
augment poor quality data with complementary information which may be based on our
preconception of what we are looking for or on information coming from sources other
than the data makes statistical methods particularly attractive in imaging applications.

Statistical Methods in Imaging 

In this chapter we present a brief overview of some of the key concepts and most popu-
lar algorithms in statistical imaging, highlighting the similarity and the differences with
the closest deterministic counterparts. A particular effort is made to demonstrate that
the statistical methods lead to new ideas and algorithms that the deterministic methods
do not give.
.
Background
..
Images in the Statistical Setting
The mathematical vessel that we will use here to describe a black and white image is a
matrix with nonnegative entries, each representing the light intensity at one pixel of the
discretized image. Color images can be thought of as the result of superimposing a few
color intensity matrices; in most application a color image is represented by three matrices,
for example, encoding the red, green, and blue intensity at each pixel. While color imaging
applications can also be approached with statistical methods, here we will only consider
gray scale images. Thus, an image X is represented as a matrix
X = [xi j] ,
≤i ≤n, ≤j ≤m, xi j ≥.
In our treatment we will not worry about the range of the image pixel values, assuming
that, if necessary, the values are appropriately normalized. Notice that this representation
tacitly assumes that we restrict our discussion to rectangular images discretized into a
rectangular arrays of pixels. This hypothesis is neither necessary nor fully justified, but
it simplifies the notation in the remainder of the chapter. In most imaging algorithms the
first step consists of storing the image into a vector by reshaping the rectangular matrix.
We use here a columnwise stacking, writing
X = [x()
x()
. . .
x(m)] ,
x(j) ∈Rn, ≤j ≤m,
and further
x = vec (X) =
⎡⎢⎢⎢⎢⎢⎢⎣
x()
⋮
x(m)
⎤⎥⎥⎥⎥⎥⎥⎦
∈RN,
N = n × m.
Images can be either directly observed or represent a function of interest, as is for
example, the case for tomographic images.
..
Randomness, Distributions and Lack of Information
We start this section by introducing some notations. A multivariate random variable
X : Ω →RN is a measurable mapping from a probability space Ω equipped with a
σ-algebra and a probability measure P. The elements of RN, as well as the realizations of X,


Statistical Methods in Imaging
are denoted by lower case letters, that is, for ω ∈Ω given, X(ω) = x ∈RN. The probability
distribution μX is the measure defined as
μX(B) = P(X−(B)),
B ⊂RN measurable.
If μX is absolutely continuous with respect to the Lebesgue measure, there is a mea-
surable function πX, the Radon–Nikodym derivative of μX with respect to the Lebesgue
measure such that
μX(B) = ∫B πX(x)dx.
For the sake of simplicity, we shall assume that all the random variables define
probability distributions which are absolutely continuous with respect to the Lebesgue
measure.
Consider two random variables X : Ω →RN and Y : Ω →RM. The joint probability
density is defined first over Cartesian products,
μX,Y (B × D) = P(X−(B) ∩Y−(D)),
and then extended to the whole product σ-algebra over RN × RM. Under the assumption
of absolute continuity, the joint density can be written as
μX,Y(B × D) = ∫B ∫D πX,Y(x, y)dydx,
where πX,Y is a measurable function. This definition extends naturally to the case of more
than two random variables.
Since the notation just introduced here gets quickly rather cumbersome, we will sim-
plify it by dropping the subscripts, writing πX,Y(x, y) = π(x, y), that is, letting x and y
be at the same time variables and indicators of their parent upper case random vari-
ables. Furthermore, since the ordering of the random variables is irrelevant – indeed,
P(X−(B) ∩Y−(D)) = P(Y−(D) ∩X−(B)) – we will occasionally interchange the
roles of x and y in the densities, without assuming that the probability densities should
be symmetric in x and y. In other words, we will use π as a generic symbol for “probability
density.”
With these notations, given two random variables X and Y, define the marginal
densities
π(x) = ∫RM π(x, y)dy,
π(y) = ∫RN π(x, y)dx,
which express the probability densities of X and Y, respectively, on their own, while the
other variable is allowed to take on any value. By fixing y, and assuming that π(y) ≠, we
have that
∫RN
π(x, y)
π(y) dx = ,
hence the nonnegative function
x ↦π(x ∣y) def= π(x, y)
π(y)
(.)

Statistical Methods in Imaging 

defines a probability distribution for X referred to as the conditional density of X, given
Y = y. Similarly, we define the conditional density of Y given X = x as
π(y ∣x) def= π(x, y)
π(x) .
(.)
This, rather expedite way of defining the conditional densities does not fully explain why
this interpretation is legitimate; a more rigorous explanation can be found in textbooks on
probability theory [, ].
The concept of probability measure does not require any further interpretation to yield
a meaningful framework for analysis, and this indeed is the viewpoint of theoretical prob-
ability. When applied to real-world problems, however, an interpretation is necessary, and
this is exactly where the opinions of statisticians start to diverge. In frequentist statistics,
the probability of an event is its asymptotic relative frequency of occurrence as the number
of repeated experiments tend to infinity, and the probability density can be thought of as
a limit of histograms. A different interpretation is based on the concept of information. If
the value of a quantity is either known or it is at potentially retrievable from the available
information, there is no need to leave the deterministic realm. If, on the other hand, the
value of a quantity is uncertain in the sense that the available information is insufficient
to determine it, to view it as a random variable appears natural. In this interpretation of
randomness, it is immaterial whether the lack of information is contingent (“imperfect
measurement device, insufficient sampling of data”) or fundamental (“quantum physical
description of an observable”). It should also be noted that the information, and therefore
the concept of probability, is subjective, as the value of a quantity may be known to one
observer and unknown to another [, ]. Only in the latter case the concept of probabil-
ity is needed. The interpretation of probability in this chapter follows mostly the subjective,
or Bayesian tradition, although most of the time the distinction is immaterial. Connections
to non-Bayesian statistics are made along the discussion.
Most imaging problems can be recast in the form of a statistical inference problem.
Classically, inverse problems are stated as follows: Given an observation of a vector y ∈RM,
find an estimate of the vector x ∈RN, based on the forward model mapping x to y. Statistical
inference, on the other hand, is concerned with identifying a probability distribution that
the observed data is presumably drawn from. In the frequentist statistics, the observation
y is seen as a realization of a random variable Y, the unknown x being a deterministic
parameter that determines the underlying distribution π(y ∣x), or likelihood density and
hence the estimation of x is the object of interest. In contrast, in the Bayesian setting, both
variables x and y are first extended to random variables, Y and X, respectively, as discussed
in more detail in the following sections. The marginal density π(x), which is independent
of the observation y, is called the prior density and denoted by πprior(x), while the likeli-
hood is the conditional density π(y ∣x). Combining the formulas > .and > ., we
obtain
π(x ∣y) = πprior(x)π(y ∣x)
π(y)
,


Statistical Methods in Imaging
which is the celebrated Bayes’ formula []. The conditional distribution π(x ∣y) is the
posterior distribution and, in the Bayesian statistical framework, the solution of the inverse
problem.
..
Imaging Problems
A substantial body of classical imaging literature is devoted to problems where the data
consists of an image, represented here as a vector y ∈RM that is either a noisy, blurred, or
otherwise corrupt version of the image x ∈RN of primary interest. The canonical model
for this class of imaging problems is
y = Ax + “noise,”
(.)
where the properties of the matrix A depend on the imaging problem at hand. A more
general imaging problems is of the form
y = F(x) + “noise,”
(.)
where the function F : RN ↦RM may be nonlinear function and the data y need not
even represent an image. This is a common setup in medical imaging applications with a
nonlinear forward model.
In classical, nonstatistical framework, imaging problems, and more generally, inverse
problems, are often, somewhat arbitrarily, classified as being linear or nonlinear, depend-
ing on whether the forward model F in (> .) is linear or nonlinear. In the statistical
framework, this classification is rather irrelevant. Since probability densities depend not
only on the forward map but also on the noise and, in the Bayesian case, the prior models,
even a linear forward map can result in a nonlinear estimation problem. We review some
widely studied imaging problems to highlight this point.
. Denoising: Denoising refers to the problem of removing noise from an image which is
otherwise deemed to be a satisfactory representation of the information. The model for
denoising can be identified with (> .), with M = N and the identity, A = I ∈RN×N
as forward map.
. Deblurring: Deblurring is the process of removing a blur, due for example, to an imaging
device being out of focus, to motion of the object during imaging (“motion blur”), or to
optical disturbances in atmosphere during image formation. Since blurred images are
often contaminated by exogenous noise, denoising is an integral part of the deblurring
process. Given the image matrix X = [xi j], the blurring is usually represented as
yi j = ∑
k,ℓ
ai j,kℓxkℓ+ “noise.”
Often, but not without loss of generality, the blurring matrix can be assumed to be a
convolution kernel,
ai j,kℓ= ai−k,j−ℓ,

Statistical Methods in Imaging 

with the obvious abuse of notations. It is a straightforward matter to arrange the ele-
ments, so that the above problem takes on the familiar matrix-vector form y = Ax, and
in the presence of noise, the model coincides with (> .).
. Inpainting: Here, it is assumed that part of the image x is missing due to an occlusion, a
scratch, or other damage. The problem is to paint in the occlusion based on the visible
part of the image. In this case, the matrix A in the linear model (> .) is a sampling
matrix, picking only those pixels of x ∈RN that are present in y ∈RM, M < N.
. Image formation: Image formation is the process of translating data into the form of an
image. The process is common in medical imaging, and the description of the forward
model connecting the sought image to data may involve linear or nonlinear transfor-
mations. An example of a linear model arises in tomography: The image is explored one
line at the time, in the sense that the data consist of line integrals indirectly measuring
the amount of radiation absorbed in the trajectory from source to detector, or the num-
ber of photons emitted at locations along the trajectory between pairs of detectors. The
problem is of the form (> .). An example of a nonlinear imaging model (> .)
arises in near-infrared optical tomography, in which the object of interest is illumi-
nated by near-infrared light sources, and the transmitted and scattered light intensity
is measured in order to form an image of the interior optical properties of the body.
Some of these examples will be worked out in more details below.
.
Mathematical Modeling and Analysis
..
Prior Information, Noise Models and Beyond
The goal in Bayesian statistical methods in imaging is to identify and explore probability
distributions of images rather than looking for single images, while in the non-Bayesian
framework, one seeks to infer on deterministic parameter vectors defining the distribu-
tion that the observations are drawn from. The main player in non-Bayesian statistics is
the likelihood function, in the notation of > Sect. .., π(y ∣x), where y = yobserved.
In Bayesian statistics, the focus is on the posterior density π(x ∣y), y = yobserved, the
likelihood function being a part of it as indicated by Bayes’ formula.
We start the discussion with the Bayesian concept of prior distribution, the non-
Bayesian modeling paradigm being discussed in connection with the likelihood function.
..
Accumulation of Information and Priors
To the question, what should be in a prior for an imaging problem, the best answer is, what-
ever can be built using available information about the image which can supplement the
measured data. The information to be accounted by the prior can be gathered in many dif-
ferent ways. Any visually relevant characteristic of the sought image is suitable for a prior,


Statistical Methods in Imaging
including but not limited to texture, light intensity, and boundary structure. Although it
is often emphasized that in a strict Bayesian framework the prior and the likelihood must
be constructed separately, in several imaging problems the setup may be impractical, and
the prior and likelihood need to be set up simultaneously. This is the case, for example,
when the noise is correlated with the signal itself. Furthermore, some algorithms may
contain intermediate steps that formally amount to updating of the a priori belief, a proce-
dure that may seem dubious in the traditional formal Bayesian setting but can be justified
in the framework of hierarchical models. For example, in the restoration of images with
sharp contrasts from severely blurred, noisy copies, an initially very vague location of the
gray scale discontinuities can be made more precise by extrapolation from intermediate
restorations, leading to a Bayesian learning model.
It is important to understand that in imaging the use of complementary information
to improve the performance of the algorithms at hand is a very natural and widespread
practice and often necessary to link the solution of the underlying mathematical prob-
lem to the actual imaging application. There are several constituents of an image that
are routinely handled under the guidance of a priori belief even in fully deterministic
settings. A classical example is the assignment of boundary conditions for an image, a
problem which has received a lot of attention over the span of a couple of decades (see,
e.g., [] and references therein). In fact, since it is certainly difficult to select the most
appropriate boundary condition for a blurred image, ultimately the choice is based on
a combination of a priori belief and algorithmic considerations. The implementation of
boundary conditions in deterministic algorithms can therefore be interpreted as using a
prior, expressing an absolute belief in the selected boundary behavior. The added flexibil-
ity which characterizes statistical imaging methodologies makes it possible to import in the
algorithm the postulated behavior of the image at the boundary with a certain degree of
uncertainty.
The distribution of gray levels within an image and the transition between areas with
different gray scale intensities are the most likely topics of a priori beliefs, hence primary
targets for priors. In the nonstatistical imaging framework, a common choice of regular-
ization, for the underlying least squares problems is a regularization functional, which
penalizes growth in the norm of the derivative of the solution, thus discouraging solu-
tions with highly oscillatory components. The corresponding statistical counterpart is a
Markov model, based, for example, on the prior assumption that the gray scale intensity at
each pixel is a properly weighted average of the intensities of its neighbors plus a random
innovation term which follows a certain statistical distribution. As an example, assum-
ing a regular quadrilateral grid discretization, the typical local model can be expressed in
terms of probability densities of pixel values X j conditioned on the values of its neighbor-
ing pixels labeled according to their relative position to Xj as Xup, Xdown, Xleft, and Xright,
respectively. The conditional distribution is derived by writing
Xj∣(Xup = xup, Xdown = xdown, Xleft = xleft, Xright = xright)
(.)
= 
(xup + xdown + xleft + xright) + Φ j,

Statistical Methods in Imaging 

where Φ j is a random innovation process. For boundary pixels, an appropriate modifi-
cation reflecting the a priori belief of the extension of the image outside the field of view
must be incorporated. In a large variety of application, Φ j is assumed to follow a normal
distribution
Φ j ∼N (, σ 
j ),
the variance σ 
j reflecting the expected deviation from the average intensity of the neigh-
boring pixels. The Markov model can be expressed in matrix-vector form as
LX = Φ,
where the matrix L is the five-point stencil discretization of the Laplacian in two dimen-
sions, and the vector Φ ∈RN contains the innovation terms Φ j. As we assume the
innovation terms to be independent, the probability distribution of Φ is
Φ ∼N(, Σ),
Σ =
⎡⎢⎢⎢⎢⎢⎢⎢⎣
σ 

σ 

⋱
σ 
N
⎤⎥⎥⎥⎥⎥⎥⎥⎦
,
and the resulting prior model is a second-order Gaussian smoothness prior,
πprior(x) ∝exp (−
∥Σ−/Lx∥).
Observe that the variances σ 
j allow a spatially inhomogeneous a priori control of the
texture of the image. Replacing the averaging weights /in (> .) by more general
weights pk, ≤k ≤leads to a smoothness prior with directional sensitivity. Ran-
dom draws from such anisotropic Gaussian priors are shown in
> Fig. -, where each
pixel with coordinate vector rj in a quadrilateral grid has eight neighboring pixels with
coordinates rk
j , and the corresponding weights pk are chosen as
pk = 
τ
(vT
j (rj −rk
j ))

∣rj −rk
j ∣

,
τ = .,
and the unit vector v j is chosen either as a vector pointing out of the center of the image
(top row) or in a perpendicular direction (bottom row). The former choice thus assumes
that pixels are more strongly affected by the adjacent values in the radial direction, while
in the latter case, they have less influence than those in the angular direction. The factor τ
is added to make the matrix diagonally dominated.
The just described construction of the smoothness prior is a particular instance of pri-
ors based on the assumption that the image is a Markov random field, (MRF). Similarly to
the four point average example, Markov random fields assume that the conditional proba-
bility distribution of a single pixel value Xj, conditioned on the remaining image depends
only on the neighbors of Xj,
π(x j ∣xk, k ≠j) = π(xj ∣xk ∈N j),


Statistical Methods in Imaging
⊡Fig. -
Random draws from anisotropic Markov models. In the top row, the Markov model assumes
stronger dependency between neighboring pixels in the radial than in angular direction,
while in the bottom row the roles of the directions are reversed. See text for a more detailed
discussion
where N j is the list of neighbor pixels of Xj, such as the four adjacent pixels in the model
(> .). In fact, the Hammersley–Clifford theorem (see []) states that prior distributions
of MRF models are of the form
πprior(x) ∝exp ⎛
⎝−
N
∑
j=
Vj(x)⎞
⎠,
where the function Vj(x) depends only on x j and its neighbors. The simplest model in this
family is a Gaussian white noise prior, where N j = / and Vj(x) = x
j /(σ ), that is,
πprior(x) ∝exp (−
σ ∥x∥).
Observe that this prior assumes mutual independency of the pixels, which has qualitative
repercussions on the images based on it.
There is no theoretical reason to restrict the MRFs to Gaussian fields, and in fact, some
of the non-Gaussian fields have had a remarkable popularity and success in the imaging
context. Two non-Gaussian priors are particularly worth mentioning here, the ℓ-prior,
where N j = / and Vj(x) = α∣xj∣, that is,
πprior(x) ∝exp (−α∥x∥),
∥x∥=
N
∑
j=
∣x j∣,

Statistical Methods in Imaging 

and the closely related Total Variation (TV) prior,
πprior(x) ∝exp (−αTV(x)),
TV(x) =
N
∑
j=
Vj(x),
with
Vj(x) = 
∑
k∈N j
∣x j −xk∣.
The former is suitable for imaging sparse images, where all but few pixels are believed to
coincide with the background level that is set to zero. The latter prior is particularly suit-
able for blocky images, that is, for images consisting of piecewise smooth simple shapes.
There is a strong connection to the recently popular concept of compressed sensing, see, for
example, [].
MRF priors, or priors with only local interaction between pixels, are by far the most
commonly used priors in imaging. It is widely accepted, and to some extent demonstrated
(see [] and the discussion in it) that the posterior density is sensitive to local properties of
the prior only, while the global properties are predominantly determined by the likelihood.
Thus, as far as the role of priors is concerned, it is important to remember that until the
likelihood is taken into account, there is no connection with the measured data, hence no
reason to believe that the prior should generate images that in the large scale resemble
what we are looking for. In general, priors are usually designed to carry very general, often
qualitative and local information, which will be put into proper context with the guidance
of the data through the integration with the likelihood. To demonstrate the local structure
implied by different priors, in
> Fig. -we show some random draws from the priors
discussed above.
..
Likelihood: Forward Model and Statistical Properties
of Noise
If an image is worth a thousand words, a proper model of the noise corrupting it is worth at
least a thousand more, in particular when the processing is based on the statistical methods.
So far, the notion of noise has remained vague, and its role unclear. It is the noise, and in
fact its statistical properties, that determines the likelihood density. We start by considering
two very popular noise models.
Additive, nondiscrete noise: An additive noise model assumes that the data and the
unknown are in a functional relation of the form
y = F(x) + e,
(.)
where e is the noise vector. If the function F is linear, or it has been linearized, the problem
simplifies to
y = Ax + e.
(.)


Statistical Methods in Imaging
⊡Fig. -
Random draws from various MRF priors. Top row: white noise prior. Middle row: sparsity
prior or ℓ-prior with positivity constraint. Bottom row: total variation prior
The stochastic extension of (> .) is
Y = F(X) + E,
where Y, X and E are multivariate random vectors.
The form of the likelihood is determined not only by the assumed probability distri-
butions of Y, X, and E but also by the dependency between pairs of these variables. In the
simplest case X and E are assumed to be mutually independent and the probability density
of the noise vector known,
E ∼πnoise(e).
resulting in a likelihood function of the form
π(y ∣x) ∝πnoise (y −F(x)),

Statistical Methods in Imaging 

which is one of the most commonly used in applications. A particularly popular model for
additive noise is a Gaussian noise,
E ∼N(, Σ),
where the covariance matrix Σ is positive definite. Therefore, if we write Σ−= DTD, where
D can be the Cholesky factor of Σ−or D = Σ−/, the likelihood can be written as
π(y ∣x) ∝exp (−
(y −F(x))TΣ−(y −F(x)))
= exp (−
∥D(y −F(x))∥).
(.)
In the general case where X and E are not independent, we need to specify the joint density
(X, E) ∼π(x, e)
and the corresponding conditional density
πnoise(e ∣x) = π(x, e)
πprior(x).
In this case the likelihood becomes
π(y ∣x) ∝πnoise(y −F(x) ∣x).
This clearly demonstrates the problems which may arise if we want to adhere to the
claim that “likelihood should be independent of the prior.” Because the interdependency
of the image x and the noise is much more common than we might be inclined to believe,
the independency of noise and signal is often in conflict with reality. An instance of such
situation occurs in electromagnetic brain imaging using magnetoencephalography (MEG)
or electroencephalography (EEG), when the eye muscle during a visual task act as noise
source, but can hardly be considered as independent from the brain activation due to a
visual stimulus. Another example related to boundary conditions will be discussed later
on. Also, since the noise term should account not only for the exogenous measurement
noise but also for the shortcomings of the model, including discretization errors, the
interdependency is in fact an ubiquitous phenomenon too often neglected.
Most additive noise models assume that the noise follows a Gaussian distribution, with
zero mean and given covariance. The computational advantages of a Gaussian likelihood
are rather formidable and have been a great incentive to use Gaussian approximations of
non-Gaussian densities. While it is commonplace and somewhat justified, for example, to
approximate Poisson densities with Gaussian densities when the mean is sufficiently large
[], there are some important imaging applications where the statistical distribution of the
noise must be faithfully represented in the likelihood.
Counting noise: The weakness of a signal can complicate the deblurring and denoising
problem, as is the case in some image processing applications in astronomy [, , ],
microscopy [, ], and medical imaging [, ]. In fact, in the case of weak signals,
charge coupled device (CCD) devices, instead of recording an integrated signal over a time


Statistical Methods in Imaging
window, count individual photons or electrons. This leads to a situation where the noise
corrupting the recorded signal is no longer exogenous but rather an intrinsic property of
the signal itself, that is, the input signal itself is a random process with an unpredictable
behavior. Under rather mild assumptions – stationarity, independency of increments, zero
probability of coincidence – it can be shown (see, e.g., []) that the counting signal fol-
lows a Poisson distribution. Consider for example, the astronomical image of a very distant
object, collected with an optical measurement device whose blurring is described by a
matrix A. The classical description of such data would follow (> .), with the error term
collecting the background noise and the thermal noise of the device. The corresponding
counting model is
y j ∼Poisson ((Ax)j + b), y j, yk independent if j ≠k,
or, explicitly,
π(y ∣x) =
m
∏
j=
((Ax)j + b)y j
(y j)!
exp (−(Ax)j + b),
where b ≥is a background radiation level, assumed known. Observe that while the data
are counts, therefore integer numbers, the expectation need not to be.
Similar or slightly modified likelihoods can be used to model the positron emission
tomography (PET) and single photon emission computed tomography (SPECT) signals,
see [, ].
The latter example above demonstrates clearly that the description of imaging problems
as linear or nonlinear, without a specification of the noise model, in the context of statis-
tical methods, does not play a significant role: Even if the expectation is linear, traditional
algorithms for solving linear inverse problems are useless, although they may turn out to
be useful within iterative solvers for solving locally linearized steps.
..
Maximum Likelihood and Fisher Information
When switching to a parametric non-Bayesian framework, the statistical inference prob-
lem amounts to estimating a deterministic parameter that identifies the probability dis-
tribution from which the observations are drawn. To apply this framework in imaging
problems, the underlying image x, which in the Bayesian context was itself a random
variable, can be thought of as a parameter vector that specifies the likelihood function,
f (y; x) = π(y ∣x),
as implied by the notation f (y; x) also.
In the non-Bayesian interpretation, a measure of how much information about the
parameter x is contained in the observation is given in terms of the Fisher information
matrix J,
Jj,k = E {∂log f
∂x j
∂log f
∂xk
} = ∫
∂log f (y; x)
∂x j
∂log f (y; x)
∂xk
f (y; x)dy.
(.)

Statistical Methods in Imaging 

In this context, the observation y only is a realization of a random variable Y, whose proba-
bility distribution is entirely determined by the distribution of the noise. The gradient of the
logarithm of the likelihood function is referred to as the score, and the Fisher information
matrix is therefore the covariance of the score.
Assuming that the likelihood is twice continuously differentiable and regular enough to
allow the exchange of integration and differentiation, it is possible to derive another useful
expression for the information matrix. It follows from the identity
∂log f
∂xk
= 
f
∂f
∂xk
,
(.)
that we may write the Fisher information matrix as
Jj,k = ∫
∂log f
∂x j
∂f
∂xk
dy =
∂
∂xk ∫
∂log f
∂x j
f dy −∫
∂log f
∂x j∂xk
f dy.
Using the identity (> .) with k replaced by j, we observe that
∫
∂log f
∂x j
f dy = ∫
∂f
∂x j
dy =
∂
∂x j ∫
f dy = ,
since the integral of f is one, which leads us to the alternative formula
Jj,k = −∫
∂log f
∂x j∂xk
f dy = −E{∂log f
∂x j∂xk
}.
(.)
The Fisher information matrix is closely related to non-Bayesian estimation theory. This
will be discussed later in connection with Maximum Likelihood estimation.
..
Informative or Noninformative Priors?
Not seldom the use of priors in imaging applications is blamed for biasing the solution in a
direction not supported by the data. The concern of the use of committal priors has led to
the search of “noninformative priors” [], or weak priors that would “let the data speak.”
The strength or weakness of a prior is a rather elusive concept, as the importance of the
prior in Bayesian imaging is in fact determined by the likelihood: the more information we
have about the image in data, the less has to be supplied by the prior. On the other hand, in
imaging applications where the likelihood is built on very few data points, the prior needs
to supply the missing information, hence has a much more important role. As pointed
out before, it is a common understanding that in imaging applications, prior should carry
small-scale information about the image that is missing from the likelihood that in turn
carries information about the large scale features, and in that sense complements the data.


Statistical Methods in Imaging
..
Adding Layers: Hierarchical Models
Consider the following simple denoising problem with additive Gaussian noise,
Y = X + N,
N ∼N(, Σ),
with noise covariance matrix Σ presumed known, whose likelihood model is tantamount
to saying that
Y ∣X = x ∼N(x, Σ).
From this perspective, the denoising problem is reduced to estimating the mean of a
Gaussian density in the non-Bayesian spirit, and the prior distribution is a hierarchical
model, expressing the degree of uncertainty of the mean x.
Parametric models are common when defining the prior densities, but similarly to the
above interpretation of the likelihood, the parameters are often poorly known. For example,
when introducing a prior
X ∼N(θ, Γ)
with unknown θ, we are expressing a qualitative prior belief that “X differs from an
unknown value by an error with a given Gaussian statistics,” which says very little about
the values of X itself unless information about θ is provided. Similarly as in the denoising
problem, it is natural to augment the prior with another layer of information concerning
the parameter θ. This layering of the inherent uncertainty is at the core of hypermodels,
or Bayesian hierarchical models. Hierarchical models are not restricted to uncertainties in
the prior, but can be applied to lack of information of the likelihood model as well.
In hierarchical models, both the likelihood and the prior may depend on additional
parameters,
π(y ∣x) →π(y ∣x,γ),
πprior(x) →πprior(x ∣θ),
with both parameters γ and θ poorly known. In this case it is natural to augment the
model with hyperpriors. Assuming for simplicity that the parameters γ and θ are mutu-
ally independent so that we can define the hyperprior distributions π(γ) and π(θ), the
joint probability distribution of all the unknowns is
π(x, y, θ,γ) = π(y ∣x,γ)πprior(x ∣θ)π(γ)π(θ).
From this point on, the Bayesian inference can proceed along different paths. It is pos-
sible to treat the hyperparameters as nuisance parameters and marginalize them out by
computing
π(x, y) = ∫∫π(x, y, θ,γ)dθdγ
and then proceed as in a standard Bayesian inference problem. Alternatively, the hyper-
parameters can be included in the list of unknowns of the problem and their posterior
density
π(ξ ∣y) = π(x, y, θ,γ)
π(y)
,
ξ =
⎡⎢⎢⎢⎢⎢⎣
x
θ
γ
⎤⎥⎥⎥⎥⎥⎦

Statistical Methods in Imaging 

needs to be explored. The estimation of the hyperparameters can be based on the optimiza-
tion or on the evidence, as will be illustrated below with a specific example.
To clarify the concept of a hierarchical model itself, we consider some examples where
hierarchical models arise naturally.
Blind deconvolution: Consider the standard deblurring problem defined in > Sect. ...
Usually, it is assumed that the blurring kernel A is known, and the likelihood, with additive
Gaussian noise with covariance Σ, becomes
π(y ∣x) ∝exp (−
(y −Ax)TΣ−(y −Ax)) .
(.)
In some cases, although A is poorly known, its parametric expression is known and the
uncertainty only affects the values of some parameters, as is the case when the shape of the
continuous convolution kernel a(r −s) is known but the actual width is not. If we express
the kernel a as a function of a width parameter,
a(r −s) = aγ(r −s) = 
γ a(γ(r −s)),
γ > ,
and denote by Aγ the corresponding discretized convolution matrix, the likelihood
becomes
π(y ∣x,γ) ∝exp (−
(y −Aγx)TΣ−(y −Aγx)),
and additional information concerning γ, for example, bound constraints, can be included
via a hyperprior density.
The procedure just outlined can be applied to many problems arising from adaptive
optics imaging in astronomy []; while the uncertainty in the model is more complex
than in the explanatory example above, the approach remains the same.
Conditionally Gaussian hypermodels: Gaussian prior models are often criticized for
being a too restricted class, not being able to adequately represent prior beliefs concerning,
for example, the sparsity or piecewise smoothness of the solution. The range of qualitative
features that can be expressed with normal densities can be considerably expanded by con-
sidering conditionally Gaussian families instead. As an example, consider the problem of
finding a sparse image from linearly blurred noisy copy of it. The likelihood model in this
case may be written as in (> .). To set up an appropriate prior, consider a conditionally
Gaussian prior
πprior(x ∣θ) ∝(

θ…θN
)
/
exp ⎛
⎝−

N
∑
j=
x
j
θ j
⎞
⎠
= exp ⎛
⎝−

N
∑
j=
⎡⎢⎢⎢⎣
x
j
θ j
+ log θ j
⎤⎥⎥⎥⎦
⎞
⎠.
(.)
If θ j = θ= constant, we obtain the standard white noise prior which cannot be expected
to favors sparse solutions. On the other hand, since θ j is the variance of the pixel Xj, sparse
images correspond to vectors θ with most of the components close to zero. Since we do not
know a priori which of the variances should significantly differ from zero, when choosing


Statistical Methods in Imaging
a stochastic model for θ, it is reasonable to select a hyperprior that favors sparsity without
actually specifying the location of the outliers. Two distributions that are particularly well
suited for this are the gamma distribution,
θ j ∼Gamma(k, θ),
k, θ> ,
π(θ j) = θk−
j
exp (−θ j
θ
),
and the inverse gamma distribution,
θ j ∼InvGamma(k, θ),
k, θ> ,
π(θ j) = θ−k−
j
exp (−θ
θ j
).
The parameters k and θare referred to as the shape and the scaling, respectively. The
inverse gamma distribution corresponds to assuming that the precision, defined as /θ j,
is distributed according to the gamma distribution Gamma(k,/θ). The computational
price of introducing hyperparameters is that instead of one image x, we need to estimate the
image x and its variance image θ. Fortunately, for conditionally Gaussian families there are
efficient algorithms for computing these estimates, which will be discussed in the section
concerning algorithms.
The hyperprior based on the gamma distribution, in turn, contains parameters
(k and θ) to be determined. Nothing prevents us from defining another layer of hyper-
priors concerning these values. It should be noted that in hierarchical models the selection
of the parameters higher up in the hierarchy tend to have less direct effect on the parame-
ters of primary interest. Since this last statement has not been formally proved to be true,
it should be considered as a piece of computational folklore.
Conditionally Gaussian hypermodels have been successfully applied in machine learn-
ing [], in electromagnetic brain activity mapping [], and in imaging applications for
restoring blocky images []. Recently, their use in compressed sensing has been proposed
[].
.
Numerical Methods and Case Examples
The solution of an imaging inverse problem in the statistical framework is the posterior
probability density. Because this format of the solution is not practical for most applica-
tions, it is common to summarize the distribution in one or a few images. This leads to the
challenging problem of exploring the posterior distributions and finding single estimators
supported by the distribution.
..
Estimators
In this section, we review some of the commonly used estimators and subsequently, discuss
some of the popular algorithms suggested in the literature to compute the corresponding
estimates.

Statistical Methods in Imaging 

...
Prelude: Least Squares and Tikhonov Regularization
In the case where the forward model is linear, the problem of estimating an image from a
degraded, noisy recording is equivalent in a determinist setting to looking for a solution of
a linear system of equations of the form
Ax = y,
(.)
where the right-hand side is corrupt by noise. When A is not a square matrix and/or it is ill
conditioned, one needs to specify what a “solution” means. The most straight forward way
is to specify it as a least squares solution.
There is a large body of literature, and a wealth of numerical algorithms, for the solution
of large-scale least squares problems arising from problems similar to imaging applica-
tions (see, e.g., []). Since dimensionality alone makes these problems computationally
very demanding, they may require an unreasonable amount of computer memory and
operations unless a compact representation of the matrix A can be exploited. Many of the
available algorithms make additional assumptions about either the underlying image or
the structure of the forward model regardless of whether there is a good justification.
In a determinist setting, the entries of the least squares solution of (> .) with a
right-hand side corrupt by noise are not necessarily in the gray scale range of the image
pixels. Moreover, the inherent ill conditioning of the problem, which varies with the imag-
ing modality and the conditions under which the observations were collected, usually
requires reqularization, see, for example, [, , , ]. A standard regularization method
is to replace the original ill-posed least squares problem by a nearby well-posed problem by
introducing a penalty term to avoid that the computed solution is dominated by amplified
noise components, reducing the problem to minimizing a functional of the form
T(x) = ∥Ax −y∥+ αJ(x),
(.)
where J(x) is the penalty functional and α > is the regularization parameter. The mini-
mizer of the functional (> .) is the Tikhonov regularized solution. The type of additional
information used in the design of the penalty term may include upper bounds on the norm
of the solution or of its derivatives, nonnegative constraints for its entries or bounds on
some of the components. Often, expressing characteristics that are expected of the sought
image in qualitative terms is neither new nor difficult: the translation of these beliefs into
mathematical terms and their implementation is a more challenging step.
...
Maximum Likelihood and Maximum A Posteriori
We begin with the discussion of the Maximum Likelihood estimator in the framework
of non-Bayesian statistics, and denote by x a deterministic parameter determining the
likelihood distribution of the data, modeled as a random variable. Let ̂x = ̂x(y) denote
an estimator of x, based on the observations y. Obviously, ̂x is also a random variable,


Statistical Methods in Imaging
because of its dependency on the stochastic observations y; moreover, it is an unbiased
estimator if
E{̂x(y)} = x,
that is, if, in the average, it returns the exact value. The covariance matrix C of an
unbiased estimator therefore measures the statistical variation around the true value,
Cj,k = E{(̂xj −x j)(̂xk −xk)},
thus the name mean square error. Evidently, the smaller the mean square error, for example,
in the sense of quadratic forms, the higher the expected fidelity of the estimator. The Fisher
information matrix (> .) gives a lower bound for the covariance matrix of all unbiased
estimators. Assuming that J is invertible, the Cramér–Rao lower bound states that for an
unbiased estimator,
J−≤C
in the sense of quadratic forms, that is, for any vector
uTJ−u ≤uTCu.
An estimator is called efficient if the error covariance reaches the Cramér–Rao
bound.
The maximum likelihood estimator ̂xML(y) is the maximizer of the function x ↦
f (x; y), and in practice, it is found by locating the zero(s) of the score,
∇x log f (x; y) = ⇒x = ̂xML(y).
Notice that in the non-Bayesian context, likelihood refers solely to the likelihood of the
observations y, and the maximum likelihood estimation is a way to choose the underlying
parametric model so that the observations become as likely as possible.
The popularity of the Maximum Likelihood estimator, in addition to being an
intuitively obvious choice, stems from the fact that it is asymptotically efficient estimator
in the sense that when the number of independent observations of the data increases, the
covariance of the estimator converges towards the inverse of the Fisher information matrix,
assuming that it exists. More precisely, assuming a sequence y, y, . . . of independent
observations and defining ̂xn = ̂x(y, . . ., yn) as
̂xn = argmax
⎧⎪⎪⎨⎪⎪⎩

n
n
∑
j=
f (x, y j)
⎫⎪⎪⎬⎪⎪⎭
,
asymptotically the probability distribution of ̂xn approaches a Gaussian distribution with
mean x and covariance J−.
The assumption of the regularity of the Fisher information matrix limits the use of the
ML estimator in imaging applications. To understand this claim, consider the simple case
of linear forward model and additive Gaussian noise,
Y = Ax + E,
E ∼N(, Σ).

Statistical Methods in Imaging 

The likelihood function in this case is
f (x; y) = (

π∣Σ∣)
/N
exp (−
(y −Ax)TΣ−(y −Ax)),
from which it is obvious that by formula (> .),
J = ATΣ−A.
In the simplest imaging problems such as of denoising, the invertibility of J is not an issue.
However, in more realistic and challenging applications such as deblurring, the ill condi-
tioning of A renders J singular, and the Cramér–Rao bound becomes meaningless. It is not
uncommon to regularize the information matrix by adding a diagonal weight to it which,
from the Bayesian viewpoint, is tantamount to adding prior information but in a rather
uncontrolled manner.
For further reading of mathematical methods in estimation theory, we refer to
[, , ].
We consider the Maximum Likelihood estimator in the context of regularization and
Bayesian statistics. In the case of a Gaussian additive noise observation model, under the
assumption that the noise at each pixel is independent of the signal, and that the forward
map is linear, F(x) = Ax, the likelihood (> .) is of the form
π(y ∣x) ∝exp (−
∥D(Ax −y)∥),
where Σ is the noise covariance matrix and DTD = Σ−is the Cholesky decomposition of
its inverse. The maximizer of the likelihood function is the solution of the minimization
problem
xML = argmin {∥D(Ax −y)∥},
which, in turn, is the least squares solution of the linear system
DAx = Dy.
Thus, we can reinterpret least squares solutions as Maximum Likelihood estimates
under an additive, independent Gaussian error model. Within the statistical framework,
the Maximum Likelihood estimator is defined analogously for any error model which
admits a maximizer for the likelihood, but in the general case the computation of the
minimizer cannot be reduced to the solution of a linear least squares problem.
In a statistical framework the addition of a penalty terms to keep the solution of the least
squares problem from becoming dominated by amplified noise components is tantamount
to using a prior to augment the likelihood. If the observation model is linear, the prior and
the likelihood are both Gaussian,
πprior(x) ∝exp (−
xTΓ−x),


Statistical Methods in Imaging
and the noise is independent of the signal, the corresponding posterior is of the form
π(x ∣y) ∝exp (−
(∥D(Ax −y)∥+ ∥Rx∥)),
where R satisfies RTR = Γ−, so typically it is the Cholesky factor of Γ−, or alternatively,
R = Γ−/.
The maximizer of the posterior density, or the Maximum A Posteriori (MAP) estimate,
is the minimizer of the negative exponent, hence the solution of the minimization problem
xMAP = argmin{∥D(Ax −y)∥+ ∥Rx∥}
= argmin
⎧⎪⎪⎨⎪⎪⎩
∥[DA
R ] x −[Dy
]∥
⎫⎪⎪⎬⎪⎪⎭
,
or, equivalently, the Tikhonov solution (> .) with penalty J(x) = ∥Rx∥and regulariza-
tion parameter α = . Again, it is important to note that the direct correspondence between
the Tikhonov regularization and the MAP estimate only holds for linear observation mod-
els and Gaussian likelihood and prior. The fact that the MAP estimate in this case is the
least squares solution of the linear system
[DA
R ] x = [Dy
]
(.)
is a big incentive to stay with Gaussian likelihood and Gaussian priors as long as possible.
As in the case of the ML estimate, the definition of MAP estimate is independent of the
form of the posterior, hence applied also to non-Gaussian, nonindependent noise models,
with the caveat that in the general case the search for a maximizer of the posterior may
require much more sophisticated optimization tools.
...
Conditional Means
The recasting in statistical terms of imaging problems effectively shifts the interest from
the image itself to its probability density. The ML and MAP estimators discussed in the
previous section suffer from the limitations, which come from summarizing an entire dis-
tribution with one realization. The ML estimator is known to suffer from instabilities due to
the typical ill conditioning of the forward map in imaging problems, and it will not be dis-
cussed further here. The computed MAP estimate, on the other hand, may correspond to
an isolated spike in the probability density away from the bulk of the mass of the density,
and its computation may suffer from numerical complications. Furthermore, a concep-
tually more serious limitation is the fact that MAP estimators do not carry information
about the statistical dispersion of the distribution. A tight posterior density suggests that
any ensemble of images which are in statistical agreement with the data and the given prior
show little variability, hence any realization from that ensemble can be thought of as very
representative of the entire family. A wide posterior, on the other hand, suggests that there

Statistical Methods in Imaging 

is a rather varied family of images that are in agreement with the data and the prior, hence
lowering the representative power of any individual realization.
In the case where either the likelihood or the prior is not Gaussian, the mean of the
posterior density, often referred to as Conditional Mean (CM) or Posterior Mean, may be
a better choice because it is the estimator with least variance (see [, ]). Observe, however,
that in the fully Gaussian case the MAP and CM estimate coincide.
The CM estimate is, by definition
xCM = ∫RN xπ(x ∣y)dx,
while the a posteriori covariance matrix is
ΓCM = ∫RN(x −xCM)(x −xCM)Tπ(x ∣y)dx,
hence requiring the evaluation of the high-dimensional integrals. When the integrals have
no closed form solution, as is the case for many imaging problems where, for example, the
a priori information contains bounds on pixel values, a numerical approximation of the
integral must be used to estimate xCM and ΓCM. The large dimensionality of the parameter
space, which easily is of the order of hundreds of thousands when x represents an image,
rules out the use of standard numerical quadratures, leaving Monte Carlo integration the
only currently known feasible alternative.
The conceptual simplicity of Monte Carlo integration, which estimates the integral
value as the average of a large sample of the integrand evaluated over the support of the
integration, requires a way of generating a large sample from the posterior density. The
generation of a sample from a given distribution is a well known problem in statistical infer-
ence, which has inspired families of sampling schemes generically referred to as Markov
chain Monte Carlo (MCMC) methods, which will be discussed in > Sect. ....
Once a representative sample from the posterior has been generated, the CM estimate
is approximately the sample mean. By definition, the CM estimate must be near the bulk of
the density, although it is not necessarily a highly probable point. In fact, for multimodal
distributions, the CM estimate may fall between the modes of the density and even belong
to a subset of RN with probability zero, although such a situation is rather easy to detect.
There is evidence, however, that in some imaging applications the CM estimate is more
stable than the MAP estimate, see []. While the robustness of the CM estimate does not
compensate for the lack of information about the width of the posterior, the possibility of
estimating the posterior covariance matrix via sampling is an argument for the sampling
approach, since the sample can also be used to estimate the posterior width.
..
Algorithms
The various estimators based on the posterior distribution are simple to define, but the
actual computation may be a major challenge. In the case of Gaussian likelihood and prior,
combined with linear forward map, the MAP and CM estimates coincide and an explicit


Statistical Methods in Imaging
formula exists. If the problem is very high dimensional, even this case may be computa-
tionally challenging. Before going to specific algorithms, we review the linear Gaussian
theory.
The starting point is the linear additive model
Y = AX + E,
X ∼N(, Γ),
E ∼N(, Σ).
Here, we assume that the mean of X and the noise E both vanish, an assumption that is
easy to remove. Above, X and E need not be mutually independent, and we may postulate
that they are jointly Gaussian and the cross correlation matrix
C = E{XET} ∈RN×M
may not vanish. The joint probability distribution of X and Y is also Gaussian, with zero
mean and variance
E {[X
Y][XT
YT]} = E{[
XXT
X(AX + E)T
(AX + E)XT
(AX + E)(AX + E)T]}
= [
Γ
ΓAT + C
AΓ + CT
AΓAT + Σ].
Let L ∈R(N+M)×(N+M) denote the inverse of the above matrix, assuming that it exists, and
write a partitioning of it in blocks according to the dimensions N and M,
L = [
Γ
ΓAT + C
AΓ + CT
AΓAT + Σ]
−
= [L
L
L
L].
With this notation, the joint probability distribution of X and Y is
π(x, y) ∝exp (−
(xTLx + xTLy + yTLx + yTLy)).
To find the posterior density, one completes the square in the exponent with respect to x,
π(x ∣y) ∝exp (−
(x −L−
Ly)
T L(x −L−
Ly)),
where terms independent of x that contribute only to the normalization are left out.
Therefore,
X ∣Y = y ∼N (L−
Ly,L−
).
Finally, we need to express the matrix blocks Li j in terms of the matrices of the model. The
expressions follow from the classical matrix theory of Schur complements []: We have
L−
= Γ −(ΓAT + C)(AΓAT + Σ)−(AΓ + CT),
(.)
and
L−
Ly = (ΓAT + C)(AΓAT + Σ)−y.
(.)
Although a closed form solution, to evaluate the expression (> .) for the posterior mean
may require iterative solvers.

Statistical Methods in Imaging 

When the image and the noise are mutually independent, implying that C = , we find
a frequently encountered form of the MAP estimate arising from writing the Gaussian
posterior density directly by using Bayes’ formula, that is,
π(x ∣y) ∝πprior(x)π(y ∣x)
∝exp (−
xTΓ−x −
(y −Ax)TΣ−(y −Ax)),
and so the MAP estimate, and simultaneously the posterior mean estimate, is the maxi-
mizer of the above expression, or, equivalently, the minimizer of the quadratic functional
H(x) = (y −Ax)TΣ−(y −Ax) + xTΓ−x.
By substituting the factorizations
Σ−= DTD,
Γ−= RTR,
the minimization problem becomes the previously discussed standard least squares prob-
lem of minimizing
H(x) = ∥D(y −Ax)∥+ ∥Rx∥,
(.)
leading to the least squares problem (> .). Whether one should use this formula or
(> .) depends on the application, and in particular, on the sparsity properties of the
covariance matrices and their inverses.
...
Iterative Linear Least Squares Solvers
The computation of the ML or MAP estimate under the Gaussian additive linear noise
model and, in the latter case, with a Gaussian prior, amounts to the solution of system
of linear equations (> .), (> .), or (> .) in the least squares sense. Since the
dimensions of the problem are proportional to the number of pixels in the image except
when the observation model has a particular structure or sparsity properties which can be
exploited to reduce the memory allocation, solution by direct methods is unfeasible, hence
making in general the iterative solvers the methods of choice.
Among the iterative methods specifically designed for the solution of least squares
problems, the LSQR version with shifts [, ] of the Conjugate Gradient for Least Squares
(CGLS) method originally proposed in [] combines robustness and numerical efficiency.
CGLS-type iterative methods have been designed to solve the system Ax = y, minimize
∥Ax−y∥or minimize ∥Ax−y∥+δ∥x∥, where the matrix A may be square or rectangular
– either overdetermined or underdetermined – and may have any rank. The matrix A does
not need to be stored, but instead its action is represented by a routine for computing
matrix-vector products of the form v ↦Av and u ↦ATu.
Minimizing the expression (> .) may be transformed in a standard form by writing
it as
min {∥D(y −AR−w)∥+ ∥w∥},
w = Rx


Statistical Methods in Imaging
In practice, the matrix R−should not be computed, unless it is trivial to obtain. Rather,
R−acts as a preconditioner, and its action should be implemented together with the action
of the matrix A as a routine called from the iterative linear solver. The interpretation of the
action of the prior as a preconditioner has led to the concept of priorconditioner, see [, ]
for details.
...
Nonlinear Maximization
In the more general case where either the observation model is nonlinear or the likeli-
hood and prior are non-Gaussian, the computation of the ML and MAP estimates require
the solution of a maximization problem. Maximizers of nonlinear functions can be found
by quasi-Newton methods with global convergence strategy. Since Newton-type methods
proceed by solving a sequence of linearized problems whose dimensions are proportional
to the size of the image, iterative linear solvers are typically used for the solution of the lin-
ear subproblem [, ]. In imaging applications, it is not uncommon that the a priori
information includes nonnegativity constraints on the pixel values or bounds on their
range. In these cases the computation of the MAP estimate amounts to a constrained max-
imization problem and may be very challenging. Algorithms for maximization problems
with nonnegativity constraints arising in imaging applications based on the projected gra-
dient have been proposed in the literature, see [] and references therein. We shall not
review Newton-based methods here, since usually the fine points are related to the par-
ticular applications at hand and not so much to the statistical description of the problem.
Instead, we review some algorithms that stem directly from the statistical setting of the
problem and are therefore different from the methods used in regularized deterministic
literature.
...
EM Algorithm
The MAP estimator is the maximizer of the posterior density π(x ∣y), or, equivalently, the
maximizer the logarithm of it,
L(x ∣y) = log π(x ∣y) = log π(y ∣x) + log πprior(x) + constant,
where the simplest form of Bayes’ rule was used to represent the posterior density as a prod-
uct of the likelihood and the prior. However, note that above, the vector x may represent
the unknown of primary interest, or if hierarchical models are used, the model parameters
related to the likelihood and/or prior may be included in it.
The Expectation Maximization algorithm is a method developed originally for maxi-
mizing the likelihood function and later extended to the Bayesian setting to maximize the
posterior density, in a situation where part of the data is “missing.” While in many statisti-
cal application the concept of missing data appears natural, for example, when incomplete
census data or patient data are discussed, in imaging applications this concept is a rather

Statistical Methods in Imaging 

arbitrary and to some extent artificial. However, during the years, EM has found its way
to numerous imaging applications, partly because it often leads to algorithms that are easy
to implement. Early versions of the imaging algorithms with counting data such as the
Richardson–Lucy iteration [, ], popular in astronomical imaging, were independently
derived. Later, similar EM-based algorithms were rederived in the context of medical imag-
ing [, , ]. Although EM algorithms are discussed in more detail elsewhere in this
book, we include a brief discussion here in order to put EM in the context of general
statistical imaging formalism.
As pointed out above, in imaging problems data is not missing: Data, per definitionem,
is what one is able to observe and register. Therefore, the starting point of the EM algorithm
in image applications is to augment the actual data y by fictitious, nonexistent data z that
would make the problem significantly easier to handle.
Consider the statistical inference problem of estimating a random variable X based on
an observed realization of Y, denoted by Y = y = yobs. We assume the existence of a third
random variable Z, and postulate that the joint probability density of these three variables
is available and is denoted by π(x, y, z). The EM algorithm consists of the following steps:
. Initialize x = xand set k = .
. E-step: Define the probability distribution, or a fictitious likelihood density,
πk(z) = π(z ∣xk, y) ∝π(xk, y, z),
y = yobs,
and calculate the integral
Qk(x) = ∫L(x ∣y, z)πk(z)dz,
L(x ∣y, z) = log(π(x ∣y, z)).
(.)
. M-step: Update xk by defining
xk+= argmax Qk(x).
(.)
. If a given convergence criterion is satisfied, exit, otherwise increase k by one and repeat
from step until convergence.
The E-step above can be interpreted as computing the expectation of the real-valued ran-
dom variable log(π(x, y, Z)), x and y fixed, with respect to a conditional measure of Z
conditioned on X = x j and Y = y = yobs, hence the name expectation step.
The use of the EM algorithm is often advocated on the basis of the convergence
proof given in []. Unfortunately the result is often erroneously quoted as an auto-
matic guarantee of convergence, without verifying the required hypotheses. The validity
of the convergence is further obfuscated by the error in the proof (see []), and in fact,
counterexamples of lack of convergence are well known [, ]. We point out that as
far as convergence is concerned, global convergence of quasi-Newton algorithm is well
established and compared to the EM algorithm, the algorithm is often more effective [].
As the concept of missing data is not well defined in general, we outline the use of the
EM algorithm in an example that is meaningful in imaging applications.


Statistical Methods in Imaging
SPECT imaging: The example discussed here follows the article []. Consider the
SPECT image formation problem, where the two dimensional object is divided in N pixels,
each one emitting photons that are recorded through collimators by M photon counting
devices. If xj is the expected number of photons emitted by the jth pixel, the photon count
at ith photon counter, denoted by Yi, is an integer-valued random variable and can be
modeled by a Poisson process,
Yi ∼Poisson ⎛
⎝
M
∑
j=
ai jx j
⎞
⎠= Poisson((Ax)i),
the variables Yi being mutually independent, and the matrix elements ai j of A ∈RM×N
being known. We assume that X, the stochastic extension of the unknown vector x ∈RN,
is a priori distributed according to a certain probability distribution,
X ∼πprior(x) ∝exp(−V(x)).
To apply the EM algorithm, we need to decide how to define the “missing data.” Photon
counter devices detect the emitted photons added over the line of sight; evidently, the prob-
lem would be more tractable if we knew the number of emitted photons from each pixel
separately. Therefore, we define a fictitious measurement,
Zi j ∼Poisson(ai jx j),
and posit that these variables are mutually independent. Obviously, after the measurement
Y = y, we have
N
∑
j=
Zi j = yi.
(.)
To perform the E-step, assuming that xk is given, consider first the conditional density
πk(z) = π(z ∣xk, y).
A basic result from probability theory states that if N independent random variables
Λ j are a priori Poisson distributed with respective means μj, and in addition
N
∑
j=
Λ j = K,
then, a posteriori, the variables Λ j conditioned on the above data are binomially dis-
tributed,
Λ j ∣⎛
⎝
N
∑
j=
Λ j = K⎞
⎠∼Binom ⎛
⎝K,
μj
∑N
j=μj
⎞
⎠.
In particular, the conditional expectation of Λ j is
E
⎧⎪⎪⎨⎪⎪⎩
Λ j ∣
N
∑
j=
Λ j = K
⎫⎪⎪⎬⎪⎪⎭
= K
μj
∑N
j=μj
.

Statistical Methods in Imaging 

We therefore conclude that the conditional density πk(z) is a product of binomial distribu-
tions of Zi j with a priori means μj = ai jxk
j , ∑N
j=μj = (Axk)i and K = yi, so in particular,
E
⎧⎪⎪⎨⎪⎪⎩
Zi j ∣
N
∑
j=
Zi j = yi
⎫⎪⎪⎬⎪⎪⎭
= ∫zi jπk(z)dz = yi
ai jxk
j
(Axk)i
def= zk
i j.
(.)
Furthermore, by Bayes’ theorem,
π(x ∣y, z) = π(x ∣z) = π(z ∣x)πprior(x),
where we used the fact that the true observations y add no information on x that would
not be included in z, we have, by definition of the Poisson likelihood and the prior,
L(x ∣y, z) = ∑
i j
(zi j log(ai jx j) −ai jx j) −V(x) + constant,
and therefore, up to an additive constant, we have
Qk(x) = ∑
i j
(zk
i j log(ai jx j) −ai jx j) −V(x),
where zk
i j is defined in (> .). This completes the E-step.
The M-step requires the minimization of Qk(x) given above. Assuming that V is
differentiable, the minimizer should satisfy

xℓ
m
∑
i=
zk
iℓ−
m
∑
i=
aiℓ−∂V
∂xℓ
(x) = .
How complicated it is to find a solution to this condition depends on the prior contribution
V, and may require an internal Newton iteration. In [], an approximate “one-step late”
(OSL) algorithm was suggested, which is tantamount to a fixed point iteration: Initiating
with ˜x= xk, an update scheme ̃xt →̃xt+is given by
̃xt+
ℓ
=
∑m
i=zk
iℓ
∑m
i=aiℓ+ ∂V
∂xℓ(̃xt)
,
and this step is repeated until a convergence criterion is satisfied at some t = t∗. Finally,
the M-step is completed by updating xk+= ̃xt∗.
The EM algorithm has been applied to other imaging problems such as blind deconvo-
lution problem [] and PET imaging [, ].
...
Markov Chain Monte Carlo Sampling
In Bayesian statistical imaging, the real solution of the imaging problem is the posterior
density of the image interpreted as a multivariate random variable. If a closed form of the
posterior is either unavailable or not suitable for the tasks at hand, the alternative is to
resort to exploring the density by generating a representative sample from it. Markov chain
Monte Carlo (MCMC) samplers yield samples from a target distribution by moving from


Statistical Methods in Imaging
a point in a chain to next by the transition rule which characterizes the specific algorithm.
MCMC sampling algorithms are usually subdivided into those which are variants of the
Metropolis–Hastings (MH) algorithm or the Gibbs sampler. While the foundations of the
MH algorithm were laid first [, , ], Gibbs samplers have sometimes the appeal of
being more straightforward to implement.
The basic idea of Monte Carlo integration is rather simple. Assume that π(x) is a proba-
bility density in RN, and let {X, X, X, . . .}denote a stochastic process, where the random
variables Xi are independent and identically distributed, Xi ∼π(x). The Central Limit
Theorem asserts that for any measurable f : RN →R,

n
n
∑
i=
f (Xi)
n→∞
D→∫RN f (x)π(x)dx
almost certainly,
(.)
and moreover, the convergence takes place asymptotically with the rate /√n, indepen-
dently of the dimension N. The difficulty is to find a computationally efficient way of
drawing independently from a given distribution π. Indeed, when N is large, it may be
even difficult to decide where the numerical support of the density is. In MCMC methods,
instead of producing an independent chain, the idea is to produce a Markov process {Xi}
with the property that π is the equilibrium distribution. It can be shown (see [, , ])
that with rather mild assumptions (irreducibility, aperiodicity) the limit (> .) holds,
due to the Law of Large Numbers.
In applications to imaging, the computational burden associated with MCMC methods
has become proverbial, and is often presented as the main obstacle to the use of Bayesian
method in imaging. It is easy to imagine that sampling random variable with hundreds of
thousands of components will require a large amount of computer resources, and that col-
lecting and storing a large number of images will require much more time than estimating
a single one. On the other hand, since an ensemble of images from a distribution carries a
lot of additional information which cannot be included in single point estimates, it seems
unreasonable to rate methods simply according to computational speed. That said, since
collecting a well mixed, representative sample poses several challenges, in the description
of the Gibbs sampling and Metropolis–Hastings algorithms we will point out references to
variants which can improve the independence and mixing of the ensemble, see [–].
In its first prominent appearance in the imaging arena [], the Gibbs sampler was
presented as part of a stochastic relaxation algorithm to efficiently compute MAP estimates.
The systematic, or fully conditional Gibbs sampler algorithm proceeds as follows [].
Let π(x) be a probability density defined on RN, denoted by π(x) = π(x, . . . , xN),
x ∈RN to underline that it is the joint density of the components of X. Furthermore,
denote by π(xj ∣x−j) the conditional density of the jth component x j given all the other
components, collected in the vector x−j ∈RN−. Let xbe the initial element of the Markov
chain. Assuming that we are at a point xi in the chain, we need a rule stating how to proceed
to the next point xi+, i.e., we need to describe the updating method of proceeding from
the current element xi to xi+. This is done by updating sequentially each component as
follows.

Statistical Methods in Imaging 

Fully conditional Gibbs sampling update: Given xi, compute the next element xi+by
the following algorithm:
draw
xi+

from
π (x∣xi
−);
draw
xi+

from
π (x∣xi+

, xi
, . . . , xi
N);
draw
xi+

from
π (x∣xi+

, xi+
, xi
. . ., xi
N);
⋮
draw
xi+
N
from
π (xN ∣xi+
−N).
In imaging applications, this Gibbs sampler may be impractical because of the large num-
ber of components of the random variable to be updated to generate a new element of the
chain. In addition, if some of the components are correlated, updating them independently
may slow down the chain to explore the full support of the distribution, due to slow move-
ment at each step. The correlation among components can be addressed by updating blocks
of correlated components together, although this will imply that the draws must be from
multivariate instead of univariate conditional densities.
It follows naturally from the updating scheme that the speed at which the chain will
reach equilibrium is strongly dependent on how the system of coordinate axes relates to
the most prominent correlation directions. A modification of the Gibbs sampler that can
ameliorate the problems caused by correlated components performs a linear transforma-
tion of the random variable using correlation information. Without going into details, we
refer to [, , ] for different variants of Gibbs sampler.
The strategy behind the Metropolis–Hastings samplers is to generate a chain with the
target density as equilibrium distribution by constructing at each step the transition prob-
ability function from the current Xi = x to next realization of Xi+in the chain in the
following way. Given an initial transition probability function q(x, x′) with Xi = x, x′
drawn from q(x, x′) is a proposal for the value of Xi+. Upon acceptance of Xi+= x′,
which occurs with probability α(x, x′), defined by
α(x, x′) = min {π(x′)q(x′, x)
π(x, x′)
,},
π(x)q(x, x′) > .
We add it to the chain, otherwise we reject the proposed value and we set Xi+= x. In the
latter case the chain did not move and the value x is replicated in the chain. The transition
probability p(x, x′) of the Markov chain thus defined is
p(x, x′) = q(x, x′)α(x, x′),
while the probability to stay put is
−∫RN q(x, y)α(x, y)dy.
This construction guarantees that the transition probability satisfies the detailed bal-
ance equation π(x)p(x, x′) = π(x′)p(x′, x), from which it follows that, for reasonable
choices of the function q, π(x) is the equilibrium distribution of the chain.


Statistical Methods in Imaging
This algorithm is particularly convenient when the target distribution π(x) is a poste-
rior. In fact since the only way in which π enters is via the ratio of its values at two points,
it is sufficient to compute the density modulo a proportionality constant, which is how we
usually define the posterior. Specific variants of the MH algorithm correspond to different
choices of q(x, x′); in the original formulation [], a symmetric proposal, for example, a
random walk, was used, so that q(x, x′) = q(x′, x), implying that
α(x, x′) = min{π(x′)/π(x),},
while the general formulation above is due to Hastings []. An overview of the different
possible choices for q(x, x′) can be found in [].
A number of hybrid sampling schemes which combine different chains or use MH vari-
ants to draw from the conditional densities inside Gibbs samplers have been proposed in
the literature; see [, ] and references therein. Since the design of efficient MCMC sam-
plers must address the specific characteristics of the target distribution, it is to be expected
that as the use of densities becomes more pervasive in imaging, new hybrid MCMC scheme
will be proposed.
The convergence of Monte Carlo integration based on MCMC methods is a key factor
in deciding when to stop sampling. This is particularly pertinent in imaging applications,
where the calculations needed for additions of a point to the chain may be quite time con-
suming. Due to the lack of a systematic way of translating theoretical convergence results
of MCMC chains [, ] into pragmatic stopping rules, in practice the issue is reduced to
monitoring the behavior of the already collected sample.
As already pointed out, MCMC algorithms are not sampling independently from the
posterior. When computing sample based estimates for the posterior mean and covariance,
̂xCM = 
n
n
∑
j=
x j,
̂ΓCM = 
n
n
∑
j=
(x j −̂xCM)(x j −̂xCM)
T.
a crucial question is how accurately these estimates approximate the posterior mean and
covariance. The answer depends on the sample size n and the sampling strategy itself.
Ideally, if the sample vectors x j are realizations of independent identically distributed ran-
dom variables, the approximations converge with the asymptotic rate /√n, in agreement
with the Central Limit Theorem. In practice, however, the MCMC sampling produces
sample points that are mutually correlated, and the convergence is slower.
The convergence of the chain can be investigated using the autocovariance function
(ACF) of the sample [, ]. Assume that we are primarily interested in estimating a real-
valued function f : RN →R of the unknown, and we have generated an MCMC sample, or
a realization {x, . . . , xn} of a stationary stochastic process {X, . . . , Xn}. The random vari-
ables X j are equally distributed, their distribution being the posterior distribution π(x) of
a random variable X. The estimation of the mean quantity f (X) can be done by calculating
̂μ = 
n
n
∑
j=
f (x j),

Statistical Methods in Imaging 

while the theoretical mean of f (X) is
μ = E{f (X)} = ∫
f (x)π(x)dx.
Each sample yields a slightly different value for ̂μ, which is itself a realization of the random
variable F defined as
F = 
n
n
∑
j=
f (X j).
The problem is now how to estimate the variance of F, which gives us an indication of
how well the computed realization approximates the mean. The identical distribution of
the random variables X j implies that
E{F} = 
n
n
∑
j=
E{f (X j)}
FGGGGGGGGGGGGGGGGGGGGHGGGGGGGGGGGGGGGGGGGGI
=μ
= μ,
while the variance of F, which we want to estimate starting from the available realization
of the by stochastic process, is
var(F) = E{F} −μ.
To this end, we need to introduce some definitions and notations.
We define the autocovariance function of the stochastic process f (X j) with lag k ≥
to be
C(k) = E{f (X j)f (X j+k)} −μ
which, if the process is stationary, is independent of j. The normalized ACF is defined as
c(k) = C(k)
C().
The ACF can be estimated from an available realization as follows
̂C(k) =

n −k
n−k
∑
j=
f (x j)f (x j+k) −̂μ.
(.)
It follows from the definition of F that
E{F} = 
n
n
∑
i,j=
E{f (Xi)f (X j)}.
Let us now focus on the random matrix [f (Xi)f (X j)]
n
i,j=. The formula above takes
its expectation and subsequently computes the average of its entries. By stationarity, the
expectation is a symmetric Toeplitz matrix, hence its diagonal entries are all equal to
E{f (Xi)f (Xi)} = C() + μ,
while the kth subdiagonal entries are all equal to
E{f (Xi)f (Xi+k)} = C(k) + μ.


Statistical Methods in Imaging
This observation provides us with a simple way to perform the summation by accounting
for the elements along the diagonals, leading to the formula
E{F} = 
n(nC() + 
n−
∑
k=
(n −k)C(k)) + μ,
from which it follows that the variance of F is
var(F) = 
n (C() + 
n−
∑
k=
(−k
n) C(k)).
If we assume that the ACF is negligible when k > n, for some nsignificantly smaller than
the sample size n, we may use the approximation
var(F) ≈
n (C() + 
n
∑
k=
C(k)) = C()
n
τ,
where
τ = + 
n
∑
k=
c(k).
(.)
If we account fully for all contributions,
τ = + 
n−
∑
k=
(−k
n) c(k),
(.)
which is the Cesàro mean of the normalized ACFs or low-pass filtered mean with the tri-
angular filter. The quantity τ is called the Integrated Autocorrelation Time (IACT) and can
be interpreted as the time that it takes for our MCMC to produce an independent sample.
If the convergence rate for independence samplers is /√n, the convergence rate for the
MCMC sampler is /
√
n/τ. If the variables Xj are independent, then τ = , and the result
is exactly what we would expects from the Central Limit Theorem, because in this case,
C() = n var(f (X)).
The estimate of τ requires an estimate for the normalized ACF, which can be obtained
with the formula (> .) and a values for nto use in formula (> .). In the choice
of nit is important to remember that ̂C(k) is a realization of a random sequence C(k),
which in practice contains noise. Some practical rules for choosing nare suggested in [].
In [], it is shown that since the sequence
γ(k) = c(k) + c(k + ),
k = ,,, . . .
is strictly positive, strictly decreasing, and strictly convex, that is,
γ(k) > ,
γ(k + ) < γ(k),
γ(k + ) < 
(γ(k) + γ(k + )),
when the sample-based estimated sequence,
̂γ(k) = ̂c(k) + ̂c(k + ),
k = ,,, . . .

Statistical Methods in Imaging 

fails to be so, this is an indication that the contribution is predominantly coming from
noise, hence it is wise to stop summing the terms to estimate τ. Geyer proposes three Initial
Sequence Estimators, in the following order:
. Initial Positive Sequence Estimator (IPSE): Choose nto be the largest integer for which
the sequence remains positive,
n= nIPSE = max{k ∣γ(k) > }.
. Initial Monotone Sequence Estimator (IMSE): Choose nto be the largest integer for
which the sequence remains positive and monotone,
n= nIMSE = max{k ∣γ(k) > , γ(k) < γ(k −)}.
. Initial Convex Sequence Estimator (ICSE): Choose nto be the largest integer for which
the sequence remains positive, monotone and convex,
n= nICSE = max {k ∣γ(k) > , γ(k) < γ(k −), γ(k −)
< 
(γ(k) + γ(k −))}.
From the proof in [], it is obvious that also the sequence {c(k)} itself must be positive
and decreasing. Therefore, to find nfor IPSE or IMSE, there is no need for passing to the
sequence {γ(k)}. As for ICSE, again from the proof in the cited article, it is also clear that
the sequence
η(k) = c(k + ) + c(k + ),
k = ,,, . . .
too, is positive, monotonous and convex. Therefore to check the condition for ICSE, it
might be advisable to form both sequences {γ(k)} and {η(k)}, and set nICSE equal to the
maximum index for which both γ(k) and η(k) remain strictly convex.
Summarizing a practical rule, using for instance, the IMSE, to compute τ is:
. Estimate the ACF sequence ̂C(k) from the sample by formula (> .) and normalize
it by ̂C() to obtain ̂c(k).
. Find nequal to the largest integer for which the sequencêc(),̂c(), . . .,̂c(n) remains
positive and strictly decreasing. Notice that the computation of ACF’s can be stopped
when such an nis reached.
. Calculate the estimate for the IACT τ,
τ = + 
n
∑
k=
(−k
n) c(k) ≈+ 
n
∑
k=
c(k).
(.)
Notice that if n is not much larger than n, the sample is too small.


Statistical Methods in Imaging
The accuracy of the approximation of μ by ̂μ is often expressed, with some degree of
imprecision, by writing an estimate
μ = ̂μ ± (C()
n
τ)
/
with the % belief. This interpretation is based on the fact that, with a probability of about
%, the values of a Gaussian random variable are within ±STD from the mean. Such
an approximate claim is justified when n is large, in which case the random variable F is
asymptotically Gaussian by the Central Limit Theorem.
..
Statistical Approach: What Is the Gain?
Statistical methods are often pitted against deterministic ones, and the true gain of the
approach is sometimes lost, especially if the statistical methods are used only to produce
single estimates. Indeed, it is not uncommon that the statistical framework is seen simply
as an alternative way of explaining regularization. Another criticism of statistical methods
concerns the computation times. While there is no doubt that computing a posterior mean
using MCMC methods is more computationally intensive than resorting to optimization
based estimators, it also obvious that a comparison in these terms does not make much
sense, since a sample contains enormously more information of the underlying distribution
than an estimate of its mode.
To emphasize what there is to be gained when using the statistical approach, we con-
sider some algorithms that have been found useful and are based on the interpretation
images as random variables.
...
Beyond the Traditional Concept of Noise
The range of interpretation of the concept of noise in imaging is usually very restricted,
almost exclusively referring to uncertainties in observed data due to exogenous sources.
In the context of deterministic regularization the noise model is almost always additive,
in agreement with the paradigm that only acknowledges noise as the difference between
a “true” and “noisy” data, giving no consideration to its statistical properties. Already the
proper noise modeling of counting data clearly demonstrates the shortcomings of such
models. The Bayesian – or subjective – use of probability as an expression of uncertainty
allows to extend the concept of noise to encompass a much richer terrain of phenomena,
including shortcomings in the forward model, prior, or noise statistics itself.
To demonstrate the possibilities of the Bayesian modeling, consider an example where
it is assumed that a forward model with additive noise,
y = F(x) + e.
(.)

Statistical Methods in Imaging 

which describes, to the best of our knowledge, as completely as possible, the interdepen-
dency of the data y and the unknown. We refer to it as the detailed model. Here the noise
e is thought to be exogenous and its statistical properties are known.
Assume further that the detailed model is computationally too complex to be used with
the imaging algorithms and the application at hand for one or several of the following rea-
sons. The dimensionality of the image x may be too high for the model to be practical;
the model may contain details such as boundary conditions that need to be simplified in
practice; the deblurring kernel may be non-separable, while in practice, a fast algorithm
for separable kernels may exist. To accommodate these difficulties, a simpler model is
constructed. Let z be possibly a simpler representation of x, obtained for example via a
projection to a coarser grid, and let f denote the corresponding forward map. It is common
procedure to write a simplified model of the form
y = f (z) + e,
(.)
which, however, may not explain the data as well as the detailed model (> .). To
properly account for the errors added by the model reduction, we should write instead
y = F(x) + e = f (z) + [F(x) −f (z)] + e
= f (z) + ε(x, z) + e,
ε(x, z) = F(x) −f (z),
(.)
where the term ε(x, z) is referred to as modeling error.
In the framework of deterministic imaging, modeling errors pose unsurmountable
problems because they depend on both the unknown image x and its reduced counterpart
z. A common way to address errors coming from model reduction is to artificially increase
the variance of the noise included in the reduced model until it masks the modeling error.
Such an approach introduces a statistical structure in the noise that does not correspond
to the modeling error and may easily waste several orders of magnitude of the accuracy
of the data. On the other hand, neglecting the error introduced by model reduction may
lead to overly optimistic estimates of the performance of algorithms. The very question-
able procedure of testing algorithms with data simulated with the same forward map used
for the inversion is referred to as inverse crime []. Inverse criminals, who tacitly assume
that ε(x, z) = , should not be surprised if the unrealistically good results obtained from
simulated data are not robust when using real data.
While modeling error often is neglected also in the statistical framework, its statistical
properties can be described in terms of the prior. Consider the stochastic extension of
ε(x, z),
̃E = ε(X, Z),
where X and Z are the stochastic extensions of x and z, respectively. Since, unlike an
exogenous noise term, the modeling error is not independent of the unknowns Z and X,
the likelihood and the prior cannot be described separately, but instead must be specified
together.
To illustrate how ubiquitous modeling error are, consider the following example.
Boundary clutter and image truncation: Consider a denoising/deblurring example of
the type encountered in astronomy, microscopy and image processing. Let u : R→R be


Statistical Methods in Imaging
a continuous two-dimensional model of a scenery that is recorded through an out of focus
device. The noiseless model for the continuous problem is a convolution integral,
v(r) = ∫Ra(r −s)u(s)ds,
r ∈R,
the convolution kernel a(r −s) describing the point spread of the device. We assume that
r ↦a(r) decays rapidly enough to justify an approximation as a compactly supported
function.
Let Q ⊂Rdefine a bounded field of view. We consider the following imaging problem:
Given a noisy version of the blurred image v over the field of view Q, estimate the underlying
image u over the field of view Q.
Assume that a sufficiently fine discretization of Q into N pixels is given, and denote
by ri ∈Q the center of the ith pixel. Assume further that the point spread function a is
negligibly small outside a disc D of radius δ > . By selecting an extended field of view Q′
such that
Q + D = {s ∈R∣s = r + r′, r ∈Q, r′ ∈D} ⊂Q′,
we may restrict the domain of integration in the definition of the convolution integral
v(ri) = ∫Ra(ri −s)u(s)ds ≈∫Q′ a(ri −s)u(s)ds.
After discretizing Q′ into N′ pixels pj with center points s j, N of which are within the field
of view, coinciding with RJ we can restate the problem in the form
v(si) ≈∫Q′ a(si −s)u(s)ds ≈
N′
∑
j=
∣pj∣a(si −s j)u(s j)
= ai ju(s j),
ai j = ∣pj∣a(si −s j),
≤i ≤N.
After accounting for the contribution of exogenous noise at each recorded pixel, we arrive
at the complete discrete model
y = A′x + e,
A′ ∈RN×N′,
(.)
where x j = u(s j) and yi represents the noisy observation of v(si). If the pixelization is
fine enough, we may consider this model to be a good approximation of the continuous
problem.
A word of caution is in order when using this model, because the right hand side
depends not only on pixels within the field of view, where we want to estimate the under-
lying image, but also on pixels in the frame C = Q′/Q around it. The vector x is therefore
partitioned into two vectors, where the first one, denoted by z ∈RN, contains values in the
pixels within the field of view, and the second one, ζ ∈RK, K = N′ −N, consists of values
of pixels in the frame. After suitably rearranging the indices, we may write x in the form
x =
⎡⎢⎢⎢⎢⎢⎣
z
ζ
⎤⎥⎥⎥⎥⎥⎦
∈
RN
×
RK
,

Statistical Methods in Imaging 

and, after partitioning the matrix A′ accordingly,
A′ = [A
B] ∈RN×N × RN×K,
we can rewrite the model (> .) in the form
y = Az + Bζ + e = Az + ε + e,
where the modeling errors are collected in second term ε, which we will refer to as bound-
ary clutter. It is well known that ignoring the contribution to the recorded image coming
from and beyond the boundary may cause severe artifacts in the estimation of the image
x within the field of view. In a determinist framework, the boundary clutter term is often
compensated for by extending the image outside the field of view in a manner believed
to be closest to the actual image behavior. Periodic extension, or extensions obtained by
reflecting the image symmetrically or antisymmetrically are quite popular in the literature,
because they will significantly simplify the computations; details on such an approach can
be found, for example, in [].
Consider a Gaussian prior and a Gaussian likelihood,
X ∼N(, Γ),
E ∼N(, Σnoise),
and partition the prior covariance matrix according to the partitioning of x,
Γ ∈[Γ
Γ
Γ
Γ
],
Γ∈RN×N, Γ= ΓT
∈RN×K, Γ∈RK×K.
The covariance matrix of the total noise term, which also includes the boundary clutter
̃E, is
E{(̃E + E)(̃E + E)T} = BΓBT + Σnoise = Σ
and the cross covariance of the image within the field of view and the noise is
C = E{Z(̃E + E)T} = ΓBT.
The posterior distribution of the vector Z conditioned on Y = y now follows from (> .)
and (> .). The posterior mean is
zCM = (ΓA + ΓBT)(AΓAT + BΓBT + Σnoise)−y,
and the posterior covariance is
Γpost = Γ−(ΓA + ΓBT)(AΓAT + BΓBT + Σnoise)−(ΓA + ΓBT)T.
A computationally efficient and robust algorithm for computing the conditional mean
is proposed in []. For further applications of the modeling error approach in imaging,
see [, , ].


Statistical Methods in Imaging
...
Sparsity and Hypermodels
The problem of reconstructing sparse images or more generally images that can be repre-
sented as sparse linear combinations of prescribed basis images using data consisting of
few measurements has recently received a lot of attention, and has become a central issue
in compressed sensing []. Bayesian hypermodels provide a very natural framework for
deriving algorithms for sparse reconstruction.
Consider a linear model with additive Gaussian noise, the likelihood being given by
(> .) and a conditionally Gaussian prior (> .) with hyperparameter θ. As explained
in > Sect. .., if we select the hyperprior πhyper(θ) in such a way that it favors solutions
with variances Θj close to zero except for only few outliers, the overall prior for (X, Θ)
will be biased towards sparse solutions. Two hyperpriors well suited for sparse solutions
are the gamma and the inverse gamma hyperpriors. For the sake of definiteness, consider
the inverse gamma hyperprior with mutually independent components,
πhyper(θ j) = θ−k−
j
exp (−θ
θ j
) = exp (−θ
θ j
−(k + )log θ j).
Then the posterior distribution for the pair (X, Θ) is of the form
π(x, θ ∣y) ∝exp ⎛
⎝−
(y −Ax)TΣ−(y −Ax) −
xTD−
θ x −
N
∑
j=
V(θ j)⎞
⎠
where
V(θ j) = θ
θ j
+ (k + 
)log θ j,
Dθ = diag(θ) ∈RN×N.
An estimate for (X, Θ) can be found by maximizing π(x, θ ∣y) with respect to the pair
(x, θ) using, for example, a quasi-Newton optimization scheme. Alternatively, the follow-
ing two algorithms that make use of the special form of the expression above can also
be used.
In the articles [, ] on Bayesian machine learning, the starting point is the obser-
vation that the posterior density x ↦π(x, θ ∣y) is Gaussian and therefore it is pos-
sible to integrate it explicitly with respect to x. It can be shown, after some tedious but
straightforward algebraic manipulations, that the marginal posterior distribution is
π(θ ∣y) = ∫RN π(x, θ ∣y)dx
∝(

det(Mθ))
/
exp ⎛
⎝−
N
∑
j=
V(θ j) + 
̃yTM−
θ ̃y⎞
⎠,
where
Mθ = ATΣ−A + D−
θ ,
̃y = ATΣ−y.

Statistical Methods in Imaging 

The most probable estimate, or the maximum evidence estimator ̂θ of Θ is, by definition,
the maximizer of the above marginal, or equivalently, the maximizer of its logarithm,
L(θ) = −
log (det(Mθ)) −
N
∑
j=
V(θ j) + 
̃yTM−
θ ̃y
which must satisfy
∂L
∂θ j
= ,
≤j ≤N.
It turns out that, although the computation of the determinant may in general be a chal-
lenge, its derivatives can be expressed in a formally simple form. To this end separate the
element depending on θ j from D−
θ , writing
D−
θ = 
θ j
e jeT
j + D†
θ′,
where e j is the jth coordinate unit vector, θ′ is the vector θ with the jth element replaced
by a zero and “†” denotes the pseudo-inverse. Then
Mθ = ATΣ−A + D†
θ′ + 
θ j
e jeT
j = Mθ′ + 
θ j
e jeT
j
(.)
= Mθ′ (I + 
θ j
qeT
j ),
q = M−
θ′ e j.
It follows from the properties of the determinant that
det (Mθ) = det (I + 
θ j
qeT
j )det (Mθ′) = (+ q j
θ j
)det (Mθ′),
where q j = eT
j q. After expressing the inverse of Mθ in the expression of L(θ) via the
Sherman–Morrison–Woodbury formula [] as
M−
θ = M−
θ′ −

θ j + q j
qqT,
we find that the function L(θ) can be written as
L(θ) = 
log(+ q j
θ j
) −V(θ j) + 

(qT̃y)
θ j + q j
+ terms that are independent of θ j.
The computation of the derivative of L(θ) with respect to θ j and its zeros is now straight-
forward, although not without challenges because reevaluation the vector q may potentially
be expensive. For details, we refer to the article [].
After having found an estimate ̂θ, an estimate for X can be obtained by observing that
the conditional density π(x ∣y, ̂θ) is Gaussian,
π(x ∣y, ̂θ) ∝exp (−
(y −Ax)TΣ−(y −Ax) −
xT̂θx),


Statistical Methods in Imaging
and an estimate for x is obtained by solving in the least squares sense the linear system
⎡⎢⎢⎢⎣
Σ−/A
D−/
̂θ
⎤⎥⎥⎥⎦
x = [Σ−/y

].
(.)
In imaging applications, this is a large-scale linear problem and typically, iterative solvers
need to be employed [].
A different approach, leading to a fast algorithm of estimating the MAP estimate
(x, θ)MAP was suggested in []. The idea is to maximize the posterior distribution using
an alternating iteration: Starting with an initial value θ = θ, ℓ= , the iteration proceeds
as follows:
. Find xℓ+that maximizes x ↦L(x, θℓ) = log (π(x, θℓ∣y)).
. Update θℓ+by maximizing θ ↦L(xℓ+, θ) = log(π(xℓ+, θ ∣y)).
The efficiency of this algorithm is based on the fact that for θ = θℓfixed, the maximization
of L(x, θℓ) in the first step is tantamount to minimizing the quadratic expression

∥Σ−/(y −Ax)∥+ 
∥D−/
θ ℓ
x∥

,
the non-quadratic part being independent of x. Thus, step only requires an (approxi-
mate) linear least squares solution of the system similar to (> .). On the other hand,
when x = xℓ+is fixed, the minimizer of the second step is found as a zero of the gradient
of the function L(xℓ+, θ) with respect to θ. This step, too, is straightforward, since the
component equations are mutually independent,
∂
∂θ j
L(xℓ+, θ) = −( 
(xℓ+
j
)
+ θ) 
θ
j
+ (k + 
) 
θ j
= ,
leading to the explicit updating formula
θℓ+
j
=

k + ((xℓ+
j
)
+ θ).
For details and performance of the method in image applications, we refer to [].
.
Conclusion
This chapter gives an overview of statistical methods in imaging. Acknowledging that it
would be impossible to give a comprehensive review of all statistical methods in imaging
in a chapter, we have put the emphasis on the Bayesian approach, while making repeated
forays in the frequentists’ field. These editorial choices are reflected in the list of references,
which only covers a portion of the large body of literature published on the topic. The use of
statistical methods in subproblems of imaging science is much wider than presented here,
extending for example, from image segmentation to feature extraction, interpretation of
functional MRI signals, and radar imaging.

Statistical Methods in Imaging 

.
Cross-References
> EM algorithms
> Iterative Solution Methods
> Linear Inverse Problems
> Total Variation in Imaging
References and Further Reading
. Arridge SR, Kaipio JP, Kolehmainen V, Schweiger
M, Somersalo E, Tarvainen T, Vauhkonen M
() Approximation errors and model reduc-
tion with an application in optical diffusion
tomography. Inverse Probl :–
. Bardsley J, Vogel CR () A nonnegatively con-
strained convex programming method for image
reconstruction. SIAM J Sci Comput :–
. Bernardo J () Bayesian theory. Wiley,
Chichester
. Bertero M, Boccacci P () Introduction to
inverse problems in imaging. IOP, Bristol
. Besag J () Spatial interaction and the statis-
tical analysis of lattice systems. J Stat Roy Soc
:-
. Besag J () On the statistical analysis of dirty
pictures. J Roy Stat Soc B :–
. Besag J, Green P () Spatial statistics and
Bayesian computation, J Roy Stat Soc B :–
. Billingsley P () Probability and measure, rd
edn. Wiley, New York
. Björck Å () Numerical methods for least
squares problems. SIAM, Philadelphia
. Boyles RA () On the convergence of the EM
algorithm. J Roy Stat Soc B :–
. Bruckstein AM, Donoho DL, Elad M ()
From sparse solutions of systems of equations to
sparse modeling of signals and images. SIAM Rev
:–
. Calvetti D () Preconditioned iterative meth-
ods for linear discrete ill-posed problems from
a Bayesian inversion perspective. J Comp Appl
Math :–
. Calvetti D, Somersalo E () Statistical com-
pensation of boundary clutter in image deblur-
ring. Inverse Probl :–
. Calvetti D, Somersalo E () Introduction to
Bayesian scientific computing - ten lectures on
subjective probability. Springer, Berlin
. Calvetti D, Somersalo E () Hypermodels in
the Bayesian imaging framework. Inverse Probl
:
. Calvetti D, Hakula H, Pursiainen S, Somersalo E
() Conditionally Gaussian hypermodels for
cerebral source localization. SIAM J Imaging Sci
:–
. Cramér H () Mathematical methods in
statistics. Princeton University Press, Princeton
. De Finetti B () Theory of probability, vol .
Wiley, New York
. Dempster AP, Laird NM, Rubin DB () Max-
imum likelihood from incomplete data via EM
algorithm. J Roy Stat Soc B :–
. Dennis JE, Schnabel RB () Numerical
methods for unconstrained optimization and
nonlinear equations, SIAM, Philadelphia
. Donatelli M, Martinelli A, Serra-Capizzano S
() Improved image deblurring with anti-
reflective boundary conditions. Inverse Probl
:–
. Franklin JN () Well-posed stochastic exten-
sion of ill-posed linear problem. J Math Anal
Appl :–
. Fox C, Nicholls G () Exact MAP states and
expectations from perfect sampling: Greig, Por-
teous and Seheult revisited. AIP Conf Proc ISSU
:–
. Gantmacher FR () Matrix theory. AMS, New
York
. Gelfand AE, Smith AFM () Sampling-based
approaches to calculating marginal densities.
J Amer Stat Assoc :–


Statistical Methods in Imaging
. Geman S, Geman D () Stochastic relaxation,
Gibbs distributions and Bayesian rerstoration of
images. IEEE Trans Pattern Anal Mach Intell
:–
. Geyer C () Practical Markov chain Monte
Carlo. Stat Sci :–
. Golub G, VanLoan () Matrix computations.
Johns Hopkins University Press, London
. Green PJ () Bayesian reconstructions from
emission tomography data using modified EM
algorithm. IEEE Trans Med Imaging :–
. Green PJ, Mira A () Delayed rejection in
reversiblejump Metropolis-Hastings. Biometrika
:–
. Haario H, Saksman E, Tamminen J ()
An adaptive Metropolis Algorithm. Bernoulli :
–
. Haario H, Laine M, Mira A, Saksman E ()
DRAM: Efficient adaptive MCMC. Stat Comput
:–
. Hansen PC () Rank-deficient and ill-posed
inverse problems. SIAM, Philadelphia
. Hansen PC () Discrete inverse problems.
Insights and algorithms. SIAM, Philadelphia
. Hastings WK () Monte Carlo sampling
methods using Markov chains and their applica-
tions. Biometrika :–
. Herbert T, Leahy R () A generalized EM algo-
rithm for D Bayesian reconstruction from Pois-
son data using Gibbs priors. IEEE Trans Med
Imaging :–
. Hestenes MR, Stiefel E () Methods of con-
jugate gradients for solving linear systems. J Res
Natl Bureau Stand :–
. Huttunen JMJ, Kaipio JP () Model reduction
in state identification problems with an appli-
cation to determination of thermal parameters.
Appl Num Math :–
. Jeffreys H () An invariant form for the prior
probability in estimation problem. Proc Roy Soc
London A :–
. Ji S, Carin L () Bayesian compressive sensing
and projection optimization. Proceedings of th
international conference on machine learning,
Cornvallis
. Kaipio J, Somersalo E () Statistical and com-
putational inverse problems. Springer, Berlin
. Kaipio JP, Somersalo E () Statistical inverse
problems: discretization, model reduction and
inverse crimes. J Comp Appl Math :–
. Kelley T () Iterative methods for optimiza-
tion. SIAM, Philadelphia
. Laksameethanasan D, Brandt SS, Engelhardt P,
Renaud O, Shorte SL () A Bayesian recon-
struction method for micro-rotation imaging
in light microscopy. Microscopy Res Tech :
–
. Lagendijk RL, Biemond J () Iterative identifi-
cation and restoration of images. Kluwer, Boston
. LeCam L () Asymptotic methods in statisti-
cal decision theory. Springer, New York
. Lehikoinen
A,
Finsterle
S,
Voutilainen
A,
Heikkinen LM,
Vauhkonen
M,
Kaipio
JP
() Approximation errors and truncation of
computational domains with application to geo-
physical tomography. Inverse Probl Imaging :
–
. Liu JS () Monte Carlo strategies in scientific
computing. Springer, Berlin
. Lucy LB () An iterative technique for the
rectification of observed distributions. Astron J
:–
. Melsa JL, Cohn DL () Decision and estima-
tion theory. McGraw-Hill, New York
. Metropolis N, Rosenbluth
AW, Teller
AH,
Teller E () Equations of state calculations
by fast computing machines. J Chem Phys :
–
. Mugnier LM, Fusco T, Conan, J-L () Mis-
tral: a myopic edge-preserving image restora-
tion method, with application to astronomical
adptive-optics-corrected long-exposure images.
J Opt Soc Am A :–
. Nummelin E () MC’s for MCMC’ists. Int Stat
Rev :–
. Ollinger JM, Fessler JA () Positron-emission
tomography. IEEE Signal Proc Mag :–
. Paige CC, Saunders MA () LSQR: An algo-
rithm for sparse linear equations and sparse least
squares. TOMS :–
. Paige CC, Saunders MA () Algorithm ;
LSQR: Sparse linear equations and least-squares
problems. TOMS :–
. Richardson HW () Bayesian-based iterative
method of image restoration. J Opt Soc Am
:–
. Robert CP, Casella () Monte Carlo statistical
methods. Springer, New York
. Saad Y () Iterative methods for sparse linear
systems. SIAM, Philadelphia

Statistical Methods in Imaging 

. Shepp LA, Vardi Y () Maximum likelihood
reconstruction in positron emission tomography.
IEEE Trans Med Imaging MI-:–
. Smith AFM, Roberts RO () Bayesian com-
putation via Gibbs sampler and related Markov
chain Monte Carlo methods. J Roy Stat Soc B
:–
. Snyder DL () Random point processes.
Wiley, New York
. Starck JL, Pantin E, Murtagh F () Deconvo-
lution in astronomy: a Review. Publ Astron Soc
Pacific :–
. Tan SM, Fox C, Nicholls GK, Lecture notes
(unpublished),
Chap
,
http://www.math.
auckland.ac.nz/
. Tierney L () Markov chains for exploring
posterior distributions. Ann Stat :–
. Tipping ME () Sparse Bayesian learning and
the relevance vector machine. J Mach Learning
Res :–
. Tipping ME, Faul AC () Fast marginal like-
lihood maximisation for sparse Bayesian models.
Proceedings of the th international workshop
on artificial intelligence and statistics, Key West,
–January
. Van Kempen GMP, Van Vliet LJ, Verveer PJ
() A quantitative comparison of image
restoration methods in confocal microscopy.
J Microscopy :–
. Wei GCG, Tanner MA () A Monte Carlo
implementation of the EM algorithm and the
poor man’s data augmentation algoritms. J Amer
Stat Aassoc :–
. Wu J () On the convergence properties of the
EM algorithm. Ann Stat :–
. Zhou J, Coatrieux J-L, Bousse A, Shu H, Luo L
() A Bayesian MAP-EM algorithm for PET
image reconstruction using wavelet transform.
Trans Nucl Sci :—


Supervised Learning by
Support Vector Machines
Gabriele Steidl
.
Introduction.....................................................................
.
Background .....................................................................
.
Mathematical Modeling and Analysis.........................................
..
Linear Learning.........................................................................
...Linear Support Vector Classification. ................................................
...Linear Support Vector Regression....................................................
...Linear Least Squares Classification and Regression................................
..
Nonlinear Learning.....................................................................
...Kernel Trick..............................................................................
...Support Vector Classification. ........................................................
...Support Vector Regression.............................................................
...Relations to Sparse Approximation in RKHSs, Interpolation by Radial
Basis Functions and Kriging...........................................................
...Least Squares Classification and Regression........................................
...Other Models............................................................................
...Multi-class Classification and Multitask Learning.................................
...Applications of SVMs..................................................................
.
Survey of Mathematical Analysis of Methods .................................
..
Reproducing Kernel Hilbert Spaces..................................................
..
Quadratic Optimization...............................................................
..
Results from Generalization Theory................................................
.
Numerical Methods............................................................
.
Conclusions.....................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Supervised Learning by Support Vector Machines
Abstract: During the last decades support vector machine learning has become a very
active field of research with a large amount of both sophisticated theoretical results and
exciting real-word applications. This chapter gives a brief introduction into the basic con-
cepts of supervised support vector learning and touches some recent developments in this
broad field.
.
Introduction
The desire to learn from examples is as old as mankind, but has reached a new dimension
with the invention of computers. This chapter concentrates on learning by support vector
machines (SVMs), which meanwhile deliver state-of-the-art performance in many real-
world applications. However, it should be mentioned at the beginning that there exist many
alternatives to SVMs ranging from classical k-nearest neighbor methods over trees and
neural networks to other kernel-based methods. Overviews can be found, e.g., in [, ,
, ].
SVMs are a new generation learning system based on various components including
•
Statistical learning theory
•
Optimization theory (duality concept)
•
Reproducing kernel Hilbert spaces (RKHSs)
•
Efficient numerical algorithms
This synthesis and their excellent performance in practice make SVM-like learning attrac-
tive for researchers from various fields. A non-exhausive list of SVM applications includes
text categorization, see [, ], hand-written character recognition, see [], texture and
image classification, see [], protein homology detection, see [], gene expression, see [],
medical diagnostics, see [], and pedestrian and face detection, see [, ]. There exist var-
ious benchmark data sets for testing and comparing new learning algorithms and a good
collection of books and tutorials on SVMs as those of [, , , , , ]. The first and
latter ones contain a mathematically more rigerous treatment of statistical learning aspects.
Least squares SVMs are handled in [] and SVMs from the approximation theoretic point
of view are considered in [].
Let X ⊂Rd, Y ⊂R
˜d, where for simplicity only ˜d = is considered, and Z := X ×Y. The
aim of the following sections is to learn a target function X →Y from given training sam-
ples Z := {(x, y), . . .,(xm, ym)} ⊂Z. A distinction is made between classification and
regression tasks. In classification Y is a discrete set, in general as large as the number of
classes the samples belong to. Here binary classification with just two labels in Y was most
extensively studied. An example, where binary classification is useful is SPAM detection.
Another example in medical diagnostics is given in
> Fig. -. Here it should be men-
tioned that in many practical applications, the original input variables are pre-processed to
transform them into a new useful space, which is often easier to handle, but preserves the
necessary discriminatory information. This process is also known as feature extraction.

Supervised Learning by Support Vector Machines 

⊡Fig. -
Examples of physiological (SR) and pathological (VT) electrical heart activity curves
measured by an implanted cardioverter–deﬁbrillator. For the classiﬁcation of such
signals []
In contrast to classification, regression aims at approximating the “whole” real-valued
function from some function values, so that Y is not countable here. The above exam-
ples, as all problems considered in this chapter, are from the area of supervised learning.
This means that all input vectors come along with their corresponding target function val-
ues (labeled data). In contrast, semi-supervised learning makes use of labeled and unlabeled
data and in unsupervised learninglabeled data are not available, so that one can only exploit
the input vectors xi. The latter methods can be applied, e.g., to discover groups of similar
exemplars within the data (clustering), to determine the distribution of the data within the
input space (density estimation), or to perform projections of data from high-dimensional
spaces to lower-dimensional spaces. There are also learning models that involve more com-
plex interactions between the learner and the environment. An example is reinforcement
learning, which is concerned with the problem of finding suitable actions in a given situ-
ation in order to maximize the reward. In contrast to supervised learning, reinforcement
learning does not start from given optimal (labeled) outputs, but must instead find them
by a process of trial and error. For reinforcement learning the reader may consult [].
Learning models can also differ in the way in which the training data are generated and
presented to the learner. For example, a distinction can be made between batch learning,
where all the data are given at the start of the training phase and online learning, where the
learner receives one example at a time and updates the hypothesis function in response to
each new example.
This chapter is organized as follows: An overview of the historical background is given
in
> Sect. ..
> Section .contains an introduction into classical SVM methods.
It starts with linear methods for (binary) support vector classification and regression
and considers also linear least squares classification/regression. Then the kernel trick is
explained and used to transfer the linear models into so-called feature spaces, which results
in nonlinear learning methods. Some other models related to SVM as well as multi-class
classification and multi task learning are addressed at the end of the section. > Section .


Supervised Learning by Support Vector Machines
provides some mathematical background concerning RKHSs and quadratic optimization.
The last subsection sketches very briefly some results in statistical learning theory. Numer-
ical methods to make the classical SVMs efficient in particular for large data sets are
presented in > Sect. .. This chapter ends with some conclusions in > Sect. ..
.
Background
Modern learning theory has a long and interesting history going back as far as Gauss
and Legendre, but got its enormeous impetus from the advent of computing machines. In
the s, revolutionary changes took place in understanding the principles of inductive
inference from a philosophical perspective, e.g., by Popper and from the point of view of
statistical theory, e.g., by Kolmogorov, Glivenko and Cantelli and applied statistics, e.g., by
Fisher. A good overview over the leading ideas and developments in this time can be found
in the comments and bibliographical remarks of Vapnik’s book, []. The starting point of
statistical learning theory, which considers the task of minimizing a risk functional based
on empirical data dates back to the s. Support vector machines, including their RKHS
interpretation were only discovered in the s and led to an explosion in applications
and theoretical analysis.
Let us start with the problem of linear regression, which is much older than linear
classification. The method of least squares was first published by Legendre, . It was
considered as a statistical procedure by Gauss, , who claimed, to the annoyance of Leg-
endre but in accordance with most historians, to have applied this method since . The
original least squares approach finds for given points xi ∈Rd and corresponding yi ∈R,
i = , . . . , m a hyperplane f (x) = ⟨w, x⟩+ b having minimal least squares distance from
the points (xi, yi):
m
∑
i=
(⟨w, xi⟩+ b −yi)→min
w,b .
(.)
This leads to the solution of a linear system of equations that can be ill-conditioned or
possess several solutions. Therefore, regularized versions were introduced later. The linear
least squares approach is optimal in the case of linear targets corrupted by Gaussian noise.
Sometimes it is useful to find a linear function, which does not minimize the least squares
error, but, e.g., the ℓ-error
m
∑
i=
∣⟨w, xi⟩+ b −yi∣→min
w,b
which is more robust against outliers. This model with the constraint that the sum of the
errors is equal to zero was already studied by Laplace in , see, []. Another popular
choice is the ℓ∞-error
max
i=,...,m ∣⟨w, xi⟩+ b −yi∣→min
w,b
which better incorporates outliers. In contrast to the least squares method, the solutions of
the ℓ- and ℓ∞-problems cannot be computed via linear systems of equations but require

Supervised Learning by Support Vector Machines 

15
20
25
30
35
40
45
5
6
7
8
9
10
11
12
15
20
25
30
35
40
45
5
6
7
8
9
10
11
12
15
20
25
30
35
40
45
5
6
7
8
9
10
11
12
⊡Fig. -
Linear approximation with respect to the ℓ-, ℓ-, and ℓ∞-norm of the error (left to right). The
ℓapproximation is more robust against outliers while the ℓ∞-norm takes them better into
account
to solve linear optimization problems.
> Figure -shows a one-dimensional exam-
ple, where data are approximated by lines with minimal ℓ-, ℓ- and ℓ∞error norm,
respectively.
Regularized least squares methods which penalize the quadratic weight ∥w∥as in
> Sect. ...were examined under the name ridge regression by []. This method can be
considered as a special case of the regularization theory for ill-posed problems developed
by Tikhonov and Arsenin. Others than the least squares loss function like the є-insensitive
loss were brought into play by []. This loss function enforces a sparse representation of
the weights in terms of support vectors, which are (small) subsets of the training samples
{xi : i = , . . . , m}.
The simplest form of classification is binary classification, where one has just to sepa-
rate between two classes. Linear hyperplanes H(w, b) separating points, also called linear
discriminants or perceptrons were already studied by [] and became interesting for neural
network researchers in the early s. One of the first algorithms that constructs a separat-
ing hyperplane for linearly separable points was Rosenblatt’s perceptron, see []. It is an
iterative online and mistake-driven algorithm, which starts with an initial weight guess w
for the hyperplane and adapts the weight at each time a training point is misclassified by the
current weight. If the data are linearly separable the procedure converges and the number of
mistakes (number of updates of w) does not exceed (R/γ), where R := mini=,...,m ∥xi∥
and γ is the smallest distance between a training point and the hyperplane. For linearly
seaprable points there exist various hyperplanes separating them.
An optimal hyperplane for linearly separable points in the sense that the minimal dis-
tance of the points from the plane becomes maximal was constructed as generalized portrait
algorithm by []. This learning method is also known as linear hard margin support vector
classifier. The method was generalized to nonseparable points by [] which leads to soft
margin classifiers. Finally, the step from linear to nonlinear classifiers via feature maps was
taken by []. Their idea to combine a linear algorithm with a kernel approach inspired the
further examination of specific kernels for applications.


Supervised Learning by Support Vector Machines
However, the theory of kernels and their applications is older than SVMs. [], system-
atically developed the theory of RHKSs in the s though it was discovered that many
results were independently obtained by []. The work of [] brought the RKHS to the fore
in statistical problems, see also []. Empirical risk minimization (ERM) over RKHSs was
considered by [] in connection with splines and by [] in relation with neural networks.
[] realized that the kernel trick works not only for SVMs but for many other methods as
principal component analysis in unsupervised learning.
The invention of SVMs has led to a gigantic amount of developments in learning theory
and practice. The size of this chapter would be not enough to list the references on this topic.
Beyond various applications, also advanced generalization results, suitable choices of ker-
nels, efficient numerical methods in particular for large data sets, relations to other sparse
representation methods, multi-class classification and multitask learning were addressed.
The reader will find some references in the corresponding sections.
.
Mathematical Modeling and Analysis
..
Linear Learning
This section starts with linear classification and regression, which provide the easiest algo-
rithms to understand some of the main building blocks that appear also in the more
sophisticated nonlinear support vector machines. Moreover, concerning the classification
task, this seems to be the best approach to explain its geometrical background. The simplest
function to feed a classifier with or to use as an approximation of some unknown function
in regression tasks is a linear (multivariate) function
f (x) = fw(x) := ⟨w, x⟩,
x ∈X ⊂Rd.
(.)
Often it is combined with some appropriate real number b, i.e., one considers the linear
polynomial f (x) + b = ⟨w, x⟩+ b. In the context of learning, w is called weight vector and
b offset, intercept or bias.
...
Linear Support Vector Classiﬁcation
Let us consider binary classification first and postpone multi-class classification
to > Sect. .... As binary classifier F = Fw,b : X →{−,} one can use
F(x) := sgn(fw(x) + b) = sgn(⟨w, x⟩+ b)
with the agreement that sng() := . The hyperplane
H(w, b) := {x : ⟨w, x⟩+ b = }

Supervised Learning by Support Vector Machines 

has the normal vector w/∥w∥and the distance of a point ˜x ∈Rd to the hyperplane is
given by
∣⟨w
∥w∥, ˜x⟩+
b
∥w∥∣
see > Fig. -left. In particular, ∣b∣/∥w∥is the distance of the hyperplane from the origin.
The training set Z consists of two classes labeled by ±with indices I+ := {i : yi = }
and I−:= {i : yi = −}. The training set is said to be separable by the hyperplane H(w, b) if
⟨w, xi⟩+ b > for i ∈I+ and ⟨w, xi⟩+ b < for i ∈I−, i.e.,
yi(⟨w, xi⟩+ b) > .
The points in Z are called (linearly) separable if there exists a hyperplane separating them.
In this case, their distance from a separating hyperplane is given by
yi (⟨w
∥w∥, xi⟩+
b
∥w∥),
i = , . . . , m.
The smallest distance of a point from the hyperplane
γ := min
i=,...,m yi (⟨w
∥w∥, xi⟩+
b
∥w∥)
(.)
is called margin. > Figure -, right shows a separating hyperplane of two classes together
with its margin. Of course for a separable training set, there may exist various separating
hyperplane. One way to ensure a unique solution is to pose additional requirements on the
hyperplane in form of minimizing a cost functional.
˜x
−λ
w
w
b
w
w
w
H
Class 1
Class 2
⊡Fig. -
Left: Hyperplane H with normal w/∥w∥and distance ∣b∣/∥w∥from the origin. The distance of
the point
−x from the hyperplane is the value λ fulﬁlling ⟨w,
−x −λw/∥w∥⟩+ b = , i.e.,
λ = (⟨w,
−x⟩+ b)/∥w∥. Right: Linearly separable training set together with a separating
hyperplane and the corresponding margin γ


Supervised Learning by Support Vector Machines
Hard margin classifier: One obvious way is to choose those separating hyperplane that has
the maximal distance from the data, i.e., a maximal margin. The corresponding classifiers
are called maximal margin classifiers or hard margin classifiers. The hyperplane and the
corresponding half-spaces do not chance if we rescale the defining vectors to (c w, c b),
c > . The so-called generalized portrait algorithm of [], constructs a hyperplane that
maximizes γ under the constraint ∥w∥= . The same hyperplane can be obtained as follows:
By (> .) we have that
γ ∥w∥= min
i=,...,m yi (⟨w, xi⟩+ b)
so that one can use the scaling
γ ∥w∥= 
⇔
γ =

∥w∥.
Now γ becomes maximal if and only if ∥w∥becomes minimal and the scaling means that
yi (⟨w, xi⟩+ b) ≥for all i = , . . . , m. Therefore, the hard margin classifier aims to
find parameters w and b solving the following quadratic optimization problem with linear
constraints:
Linear SV hard margin classification (Primal problem)

∥w∥
→
min
w,b
subject to
yi (⟨w, xi⟩+ b)
≥
,
i = , . . . , m.
If the training data are linearly separable the problem has a unique solution. A brief intro-
duction into quadratic programming methods is given in > Sect. ... To transform the
problem into its dual form consider the Lagrangian
L(w, b, α) = 
∥w∥+
m
∑
i=
αi (−yi (⟨w, xi⟩+ b)) ,
αi ≥.
Since
∂L
∂w = w −
m
∑
i=
αi yixi = 
⇔
w =
m
∑
i=
αi yixi,
(.)
∂L
∂b =
m
∑
i=
αi yi = 
(.)
the Lagrangian can be rewritten as
L(w, b, α) = −

m
∑
i=
m
∑
j=
yiαi y jαj⟨xi, xj⟩+
m
∑
i=
αi
(.)
and the dual problem becomes

Supervised Learning by Support Vector Machines 

Linear SV hard margin classification (Dual problem)


m
∑
i=
m
∑
j=
yiαi y jαj⟨xi, xj⟩−
m
∑
i=
αi →min
α
subject to
∑m
i=yiαi = ,
αi ≥, i = , . . . , m.
Note that we have rewritten the dual maximization problem into a minimization problem
by using that max ϕ = min −ϕ. Let m denote the vector with m coefficients , α := (αi)m
i=,
y := (yi)m
i=, Y := diag(yi)m
i=, and
K := (⟨xi, xj⟩)
m
i,j=.
(.)
Note that K is symmetric and positive semi-definite. The the dual problem can be rewritten
in matrix-vector form as
Linear SV hard margin classification (Dual problem)

αTYKYα −⟨m, α⟩→min
α
subject to
⟨y, α⟩= , α ≥.
Let α∗be the minimizer of this dual problem. The intercept b does not appear in the dual
problem and one has to determine its optimal value in another way. By the Kuhn–Tucker
conditions the equations
α∗
i (yi(⟨w∗, xi⟩+ b∗) −) = ,
i = , . . . , m
hold true, so that α∗
i
> is only possible for those training data with yi(⟨w∗, xi⟩+
b∗) = . These are exactly the (few) points having margin distance γ from the hyperplane
H(w∗, b∗). Define
IS := {i : α∗
i > },
S := {xi : i ∈IS}.
(.)
The vectors from S are called support vectors. In general, ∣S∣≪m and by (> .) the
optimal weight w∗and the optimal function fw∗have a sparse representation in terms of
the support vectors
w∗= ∑
i∈IS
α∗
i yixi,
fw∗(x) = ∑
i∈IS
α∗
i yi⟨xi, x⟩.
(.)
Moreover,
b∗= yi −⟨w∗, xi⟩= yi −fw∗(xi),
i ∈Is
(.)
and hence, using (> .),
∥w∗∥= ∑
i∈IS
α∗
i yi ∑
j∈IS
α∗
j y j⟨xi, xj⟩= ∑
i∈IS
α∗
i yi fw∗(xi) = ∑
i∈IS
α∗
i (−yib∗) = ∑
i∈IS
α∗
i
so that
γ = /∥w∥= ⎛
⎝∑
i∈IS
α∗
i
⎞
⎠
−/
.
Soft margin classifier: If the training data are not linearly separable which is the case in
most applications, the hard margin classifier is not applicable. The extension of hard margin


Supervised Learning by Support Vector Machines
classifiers to the nonseparable case was done by [] by bringing additional slack variables
and a parameter C > into the constrained model:
Linear SV soft margin classification (Primal problem)

∥w∥+ C
m
∑
i=
ξi →min
w,b,ξ
subject to
yi (⟨w, xi⟩+ b) ≥−ξi,
ξi ≥, i = , . . . , m.
For C = ∞, this is again the hard margin classifier model. As before, we define the margin
as γ = /∥w∗∥, where w∗is the solution of the above problem. If the slack variable fulfills
≤ξ∗
i < , then xi is correctly classified, and in the case yi (⟨w∗, xi⟩+ b∗) = −ξ∗
i the
distance of xi from the hyperplane is γ −ξ∗
i /∥w∗∥. If < ξ∗
i , we have a misclassification. By
penalizing the sum of the slack variables one tries to keep them small.
The above constraint minimization model can be rewritten as an unconstraint one by
using a margin-based loss function. A function L : {−,} × R →[,∞) is called margin-
based if there exists a representing function l : R →[,∞) such that
L(y, t) = l(yt).
In soft margin classification the appropriate choice of a loss function is the hinge loss
function lh determined by
lh(x) := max{,−x}.
Then the unconstraint primal problem reads
Linear SV soft margin classification (Primal problem)

∥w∥+ C
m
∑
i=Lh (yi,(⟨w, xi⟩+ b)) →min
w,b .
The Lagrangian of the linear constraint problem has the form
L(w, b, ξ, α) = 
∥w∥+ C
m
∑
i=
ξi +
m
∑
i=
αi (−ξi −yi (⟨w, xi⟩+ b)) −
m
∑
i=
βiξi,
where αi, βi ≥. Partial differentiation of the Lagrangian with respect to w and b results
in (> .), (> .) and with respect to ξ in
∂L
∂ξ = C m −α −β = .
Using these relations, the Lagrangian can be rewritten in the same form as in (> .) and
the dual problem becomes in matrix-vector form
Linear SV soft margin classification (Dual problem)

αTYKYα −⟨m, α⟩
subject to
⟨y, α⟩= , ≤α ≤C.

Supervised Learning by Support Vector Machines 

Let α∗be the minimizer of the dual problem. Then the optimal weight w∗and fw∗are
again given by (> .) and depend only on the few support vectors defined by (> .).
By the Kuhn–Tucker conditions the equations
α∗
i (yi (⟨w∗, xi⟩+ b∗) −+ ξ∗
i ) = 
and
β∗
i ξ∗
i = (C −α∗
i ) ξ∗
i = ,
i = , . . . , m
hold true. For < α∗
i < C, it follows that ξ∗
i = and yi(⟨w∗, xi⟩+ b∗) = , i.e., the points
xi have margin distance γ = /∥w∗∥from H(w∗, b∗). Moreover,
b∗= yi −⟨w∗, xi⟩,
i ∈˜IS := {i : < α∗
i < C}.
(.)
For α∗
i = C, one concludes that yi(⟨w∗, xi⟩+ b∗) = −ξ∗
i , i.e., xi has distance γ −ξ∗
i /∥w∗∥
from the optimal hyperplane.
...
Linear Support Vector Regression
Of course one can also approximate unknown functions by linear (multivariate) polyno-
mials of the form (> .).
Hard margin regression:
The model for linear hard margin regression is given by
Linear SV hard margin regression (Primal problem)

∥w∥→minw,b
subject to
⟨w, xi⟩+ b −yi
≤
є,
−⟨w, xi⟩−b + yi
≤
є,
i = , . . . , m.
The constraints make sure that the test data yi lie within an є distance from the value
f (xi) + b of the approximating linear polynomial. The Lagrangian reads
L(w, b, ξ±, α±, β±) = 
∥w∥+
m
∑
i=
α−
i (⟨w, xi⟩+ b −yi −є)+
m
∑
i=
α+
i (−⟨w, xi⟩−b + yi −є),
where α±
i ≥. Setting partial derivatives to zero leads to
∂L
∂w = w +
m
∑
i=
(α−
i −α+
i ) xi = 
⇔
w =
m
∑
i=
(α+
i −α−
i ) xi,
(.)
∂L
∂b =
m
∑
i=
(α+
i −α−
i ) = .
(.)
Using these relations and setting
α := α+ −α−,
the Lagrangian can be written as
L(w, b, ξ±, α±, β±) = −

m
∑
i=
m
∑
j=
αiαj⟨xi, xj⟩−є
m
∑
i=
(α+
i + α−
i ) +
m
∑
i=
yiαi
and the dual problem becomes in matrix-vector form


Supervised Learning by Support Vector Machines
Linear SV hard margin regression (Dual problem)

(α+ −α−)TK(α+ −α−) + є⟨m, α+ + α−⟩−⟨y, α+ −α−⟩→min
α+,α−
subject to
⟨m, α+ −α−⟩= ,
α± ≥.
This is a quadratic optimization problem with linear constraints. Let (α+)∗,(α−)∗be the
solution of this problem and α∗= (α+)∗−(α−)∗. Then, by (> .), the optimal weight
and the optimal function have in general a sparse representation in terms of the support
vectors xi, i ∈IS, namely
w∗= ∑
i∈IS
α∗
i xi,
fw∗(x) = ∑
i∈IrS
α∗
i ⟨xi, x⟩,
IrS := {i : α∗
i /= }.
(.)
The corresponding Kuhn–Tucker conditions are
(α−
i )
∗(є −⟨w∗, xi⟩−b∗+ yi) = ,
(.)
(α+
i )
∗(є + ⟨w∗, xi⟩+ b∗−yi) = .
If (α−
i )∗> or (α+
i )∗> , then
b∗= yi −⟨w∗, xi⟩+ є,
b∗= yi −⟨w∗, xi⟩−є,
respectively. Since both conditions cannot be fulfilled for the same index, it follows that
(α−
i )∗(α+
i )∗= and consequently, either α∗
i = (α+
i )∗≥or α∗
i = −(α−
i )∗≤. Thus, one
can obtain the intercept by
b∗= yi −⟨w∗, xi⟩−є,
i ∈IS.
(.)
According modifications toward with an index i belonging to a negative coefficient α∗
i have
to be done if IS is empty.
Soft margin regression: Relaxing the constraints in the hard margin model leads to the
following linear soft margin regression problem with C > :
Linear SV soft margin regression (Primal problem)

∥w∥+ C ∑m
i=(ξ+
i + ξ−
i ) →minw,b,ξ±
i
subject to
⟨w, xi⟩+ b −yi
≤
є + ξ−
i ,
−⟨w, xi⟩−b + yi
≤
є + ξ+
i ,
ξ+
i , ξ−
i
≥
.
For C = ∞this recovers the linear hard margin regression problem. The above constraint
model can be rewritten as an unconstraint one by using a distance-based loss function.

Supervised Learning by Support Vector Machines 

A function L : Y ×R →[,∞) is called distance-based if there exists a representing
function l : R →[,∞) such that
L(y, t) = l(y −t).
In soft margin regression, the appropriate choice is Vapnik’s є-insensitive loss function
defined by
lє(x) := max{,∣x∣−є}.
The function lє is depicted in > Fig. -, left. Then the unconstrained primal model reads
Linear SV soft margin regression (Primal problem)

∥w∥+ C
m
∑
i=Lє(yi,⟨w, xi⟩+ b) →min
w,b .
The Lagrangian of the constraint problem is given by
L(w, b, ξ±, α±, β±) = 
∥w∥+ C
m
∑
i=
(ξ+
i + ξ−
i ) +
m
∑
i=
α−
i (⟨w, xi⟩+ b −yi −є −ξ−
i )
+
m
∑
i=
α+
i (−⟨w, xi⟩−b + yi −є −ξ+
i ) −
m
∑
i=
β+
i ξ+
i −
m
∑
i=
β−
i ξ−
i ,
where α±
i ≥and β±
i ≥, i = , . . . , m. Setting the partial derivatives to zero leads to
(> .), (> .) and
∂L
∂ξ+ = C m −α+ −β+ = ,
∂L
∂ξ−= C m −α−−β−= .
Using these relation the Lagrangian can be written exactly as in the hard margin problem
and the dual problem becomes in matrix-vector form
−5 −4 −3 −2 −1
0
1
2
3
4
5
0
0.5
1
1.5
2
2.5
3
ξ
ξ5
ξ9
ε
ε
⊡Fig. -
Vapnik’s є-insensitive loss function for є = (left). Example of linear SV soft margin
regression (right)


Supervised Learning by Support Vector Machines
Linear SV soft margin regression (Dual problem)

(α+ −α−)
T K(α+ −α−) + є⟨m, α+ + α−⟩−⟨y, α+ −α−⟩→min
α+,α−
subject to
⟨m, α+ −α−⟩= ,
≤α+, α−≤C
If (α+)∗,(α−)∗are the solution of this problem and α∗= (α+)∗−(α−)∗, then the optimal
weight w∗and the optimal function fw∗are given by (> .). The corresponding Kuhn–
Tucker conditions are
(α−
i )
∗(є + (ξ−
i )
∗−⟨w∗, xi⟩−b∗+ yi) = ,
(α+
i )
∗(є + (ξ+
i )
∗+ ⟨w∗, xi⟩+ b∗−yi) = ,
(C −(α+
i )
∗)(ξ+
i )
∗= ,
(C −(α−
i )
∗)(ξ−
i )
∗= ,
i = , . . . , m.
If < (α+
i )∗or < (α−
i )∗, then
b∗= yi −⟨w∗, xi⟩+ є + ξ+
i ,
b∗= yi −⟨w∗, xi⟩−є −ξ−
i ,
respectively. Both equations cannot be fulfilled at the same time so that one can conclude
that either α∗
i = (α+
i )∗≥or α∗
i = −(α−
i )∗≤. In case α∗
i = (α+
i )∗< C, this results in the
intercept
b∗= yi −⟨w∗, xi⟩−є,
i ∈˜IS.
(.)
...
Linear Least Squares Classiﬁcation and Regression
Instead of the hinge loss function for classification and the є-insensitive loss function for
regression other loss functions can be used. Popular margin-based and distance-based loss
functions are the logistic loss for classification and regression
l(yt) := ln(+ e−yt)
and
l(y −t) := −ln
e y−t
(+ e y−t),
respectively. In contrast to the loss functions in the previous subsections, logistic loss func-
tions are differentiable in t so that often standard methods as gradient descent methods or
Newton (like) methods can be applied for computing the minimizers of the correspond-
ing problems. For details, see, e.g., [] or []. Other loss functions for regression are the
pinball loss, the Huber function, and the p-th power absolute distance loss ∣y −t∣p, p > . For
p = , the latter is the least squares loss
llsq(y −t) = (y −t).
Since (y −t)= (−yt)for y ∈{−,} the least squares loss is also margin-based and
one can handle least squares classification and regression using just the same model with
y ∈{−,} for classification and y ∈R for regression. In the unconstrained form one has to
minimize

Supervised Learning by Support Vector Machines 

Linear LS classification/regression (Primal problem)

∥w∥+ C

m
∑
i=(⟨w, xi⟩+ b −yi)

Ll sq(yi,⟨w,xi⟩+b)
→min
w,b .
This model was published as ridge regression by [] and is a regularized version of the
Gaussian model (> .). Therefore, it can be considered as a special case of regularization
theory introduced by Tikhonov and Arsenin. The minimizer can be computed via a linear
system of equations. To this end, rewrite the unconstrained problem in matrix-vector form

∥w∥+ C
∥X
Tw + bm −y∥→min
w,b ,
where
X := (x. . . xm) =
⎛
⎜
⎝
x,
. . .
xm,
⋮
x,d
. . .
xm,d
⎞
⎟
⎠
.
Setting the gradient (with respect to w and b) to zero one obtains
= 
C w + XX
Tw + bXm −Xy,
= 
T
mX
Tw −
T
m y + mb
⇔
b = ¯y −⟨w, ¯x⟩,
(.)
where ¯y :=

m ∑m
i=yi and ¯x :=

m ∑m
i=xi. Hence b and w can be obtained by solving the
linear system of equations
( 
¯xT
¯x

mXXT +

mC I )(b
w) = (
¯y

mXy).
(.)
Instead of the above problem one solves in general the “centered” problem
Linear LS classification/regression in centered version (Primal problem)

∥˜w∥+ C

m
∑
i=
(⟨˜w, ˜xi⟩+ ˜b −yi)→min
˜w,˜b
,
where ˜xi := xi −¯x, i = , . . . , m. The advantage is that ¯˜x = m, where m is the vector
consisting of m zeros.. Thus, (> .) with ˜xi instead of xi becomes a separable system
and one obtains immediately that ˜b∗= ¯y and that ˜w∗follows by solving the linear system
with positive definite, symmetric coefficient matrix
(˜X˜X
T + 
C I)w = ˜Xy.


Supervised Learning by Support Vector Machines
This means that ˜w is just the solution of the centered primal problem without intercept.
Finally, one can check by the following argument that indeed w∗= ˜w∗:
( ˜w∗, ˜b∗) := argmin
˜w,˜b
{ 
∥˜w∥+ C

m
∑
i=
(⟨˜w, ˜xi⟩+ ˜b −yi)},
˜w∗= argmin
˜w
{ 
∥˜w∥+ C

m
∑
i=
(⟨˜w, ˜xi⟩+ ¯y −yi)}
= argmin
˜w

∥˜w∥+ C

m
∑
i=
(⟨˜w, xi⟩+ ¯y −⟨˜w, ¯x⟩−yi)}
and with (> .) on the other hand
(w∗, b∗) := argmin
w,b
{ 
∥w∥+ C

m
∑
i=
(⟨w, xi⟩+ b −yi)},
w∗= argmin
w
{ 
∥w∥+ C

m
∑
i=
(⟨w, xi⟩+ ¯y −⟨w, ¯x⟩−yi)}.
Note that XTX = K ∈Rm,m, but this is not the coefficient matrix in (> .). When turning
to nonlinear methods in
> Sect. ..it will be essential to work with K = XTX instead
of XXT. This can be achieved by switching to the dual setting. In the following, this dual
approach is shown although it makes often not sense for the linear setting since the size
of the matrix K is in general larger than those of XXT ∈Rd,d. First, one reformulates the
primal problem into a constraint one:
Linear LS classification/regression (Primal problem)

∥w∥+ C

m
∑
i=
ξ
i →min
w,b,ξ
subject to
⟨w, xi⟩+ b −yi = ξi, i = , . . . , m.
The Lagrangian reads
L(w, b, ξ, α) = 
∥w∥+ C

m
∑
i=
ξ
i −
m
∑
i=
αi (⟨w, xi⟩+ b −yi −ξi)
and
∂L
∂w = w −
m
∑
i=
αixi = 
⇔
w =
m
∑
i=
αixi,
∂L
∂b =
m
∑
i=
αi = ,
∂L
∂ξ = C ξ + α = .
The equality constraint in the primal problem together with the above equalities leads to
the following linear system of equations to determine the optimal α∗and b∗:
( 
T
m
m
K + 
C I )(b
α) = (
y).
(.)

Supervised Learning by Support Vector Machines 

The optimal weight and the corresponding optimal function read
w∗=
m
∑
i=
α∗
i xi,
fw∗(x) =
m
∑
i=
α∗
i ⟨xi, x⟩.
(.)
In general there is no sparse representation with respect to the vectors xi, see also [,
Theorem .]. Therefore this method is not called a support vector method in this chap-
ter. Finally, note that the centered approach also helps to avoid the intercept in the dual
approach. Since this is no longer true when turning to the nonlinear setting the intercept
is kept here.
..
Nonlinear Learning
A linear form of a decision or regression function may not be suitable for the task at
hand.
> Figure -shows two examples, where a linear classifier is not appropriate. A
basic idea to handle such problems was proposed by [] and consists of the following two
steps, which will be further explained in the rest of this subsection:
. Mapping of the input data X ⊂X into a feature space Φ(X) ⊂ℓ(I), where I is a
countable (possibly finite) index set, by a nonlinear feature map
Φ : X →ℓ(I).
. Application of the linear classification/regression model to the feature set
{(Φ(x), y), . . .,(Φ(xm), ym)}.
This means that instead of a linear function (> .) we are searching for a function of
the form
f (x) = fw(x) := ⟨w, Φ(x)⟩ℓ(I)
(.)
now. This nonlinear function on X becomes linear in the feature space Φ(X).
X2
X2
X1
X1
1
0.5
0
–0.5
–1
–1
–0.5
0
0.5
1
4
3
2
0
–1
–2
–3
–4
1
–4
–3
–2
–1
0
1
2
3
4
⊡Fig. -
Two sets, where linear classiﬁcation is not appropriate


Supervised Learning by Support Vector Machines
X1
X2
1
0.5
0
–0.5
–1
–1
–0.5
0
0.5
1
1
1
⊡Fig. -
Linearly non-separable training data in the original space X ⊂R(left) and become
separable in the feature space Φ(X ), where Φ(x, x) := (x
, x
) (right)
> Figure -shows an example of a feature map. In this example, the set {(xi, yi) : i =
, . . . ,} is not linearly separable while {(Φ(xi), yi) : i = , . . . ,} is linearly separable. In
practical applications, in contrast to this example, the feature map often maps into a higher
dimensional, possibly also infinite dimensional space.
Together with the so-called kernel-trick to avoid the direct work with the feature
map Φ, this approach results in the successful support vector machine (SVM).
...
Kernel Trick
In general, one avoids to work directly with the feature map by dealing with the dual
problem and applying the so-called kernel-trick. For a feature map Φ, define a kernel
K : X × X →R associated with Φ by
K(x, t) := ⟨Φ(x), Φ(t)⟩l(I).
(.)
More precisely, in practice one often follows the opposite way, namely one starts with a
suitable kernel, which is known to have a representation of the form (> .) without
knowing Φ explicitly.
A frequently
applied group of kernels are continuous,
symmetric, positive
(semi-)definite kernels like the Gaussian
K(x, t) = e−∥x−t∥/c.

Supervised Learning by Support Vector Machines 

These kernels, which are also called Mercer kernels, will be considered in detail
in
> Sect. ... By Mercer’s theorem it can be shown that a Mercer kernel possesses
a representation
K(x, t) = ∑
j∈I
√
λjψj(x)
√
λjψj(t),
x, t ∈X
with L-orthonormal functions ψj and positive values λj, where the right-hand side con-
verges uniformly. Note that the existence of such a representation is clear, but in general
without knowing the functions ψj explicitly. The set {φ j :=
√
λjψj : j ∈I} forms an
orthonormal basis of a reproducing kernel Hilbert space (RKHS) HK. These spaces are
considered in more detail in > Sect. ... Then the feature map is defined by
Φ(x) := (φ j(x))j∈I = (
√
λjψj(x))
j∈I
.
Using the orthonormal basis, one knows that for any f ∈HK there exists a unique sequence
w = w f := (wj)j∈I ∈ℓ(I) such that
f (x) = ∑
j∈I
wjφ j(x) = ⟨w, Φ(x)⟩,
and
wj = ⟨f , φ j⟩HK,
(.)
where the convergence is uniform. Conversely, every sequence w ∈ℓ(I) defines a function
fw lying in HK by (> .). Moreover, Parseval’s equality says that
∥fw∥HK := ∥w∥ℓ(I).
(.)
For nonlinear classification and regression purposes one can follow exactly the lines
of the previous
> Sect. ..except that one has to work in Φ(X) instead of X. Using
(> .) instead of (> .) and
K := (⟨Φ(xi), Φ(xj)⟩ℓ(I))
m
i,j== (K(xi, xj))
m
i,j=
(.)
instead of the kernel matrix K := (⟨xi, xj⟩)
m
i,j=in (> .), the linear models from the
previous > Sect. ..can be rewritten as in the following subsections. Note again that K
is positive semi-definite.
...
Support Vector Classiﬁcation
In the following, the linear classifiers are generalized to feature spaces.
Hard margin classifier:
The hard margin classification model is
SVM hard margin classification (Primal problem)

∥w∥
ℓ(I) →min
w,b
subject to
yi (⟨w, Φ(xi)⟩ℓ(I) + b) ≥, i = , . . . , m.


Supervised Learning by Support Vector Machines
Interestingly, if Φ is associated with a Mercer kernel K, then f (x) = ⟨w, Φ(x)⟩ℓ(I) lies in
the RKHS HK, and the model can be rewritten using (> .) from the point of view of
RKHS as
SVM hard margin classification in RKHS (Primal problem)

∥f ∥
HK →min
f ∈HK
subject to
yi (f (xi) + b) ≥, i = , . . ., m.
The dual problem reads
SVM hard margin classification (Dual problem)


m
∑
i=
m
∑
j=
yiαi y jαj⟨Φ(xi), Φ(xj)⟩ℓ(I) −
m
∑
i=
αi →min
α
subject to
m
∑
i=
yiαi = ,
αi ≥, i = , . . . , m.
and with K defined by (> .) the matrix-vector form of the dual problem looks as those
for the linear hard margin classifier.
Let α∗be the minimizer of the dual problem. Then, by (> .) together with the feature
space modification, the optimal weight and the function fw∗become
w∗= ∑
i∈IS
α∗
i yiΦ(xi),
fw∗(x) = ∑
i∈IS
α∗
i yi⟨Φ(xi), Φ(x)⟩ℓ(I) = ∑
i∈IS
α∗
i yiK(xi, x).
(.)
Thus, one can compute fw∗knowing only the kernel and not the feature map itself.
One property of a Mercer kernel used for learning purposes should be that it can be simply
evaluated at points from X ×X. For example this is the case for the Gaussian. Finally, using
(> .) in the feature space, the intercept can be computed by
b∗= yi −⟨w∗, Φ(xi)⟩ℓ(I) = yi −∑
j∈IS
α∗
j y jK(xj, xi),
i ∈IS
and the margin γ = /∥w∗∥
ℓ(I) by using ∥w∗∥
ℓ(I) = (α∗)TYKYα∗.
Soft margin classifier:
The soft margin classification model in the feature space is
SVM soft margin classification (Primal problem)

∥w∥
ℓ(I) + C
m
∑
i=
ξi →min
w,b,ξ subject to yi (⟨w, Φ(xi)⟩ℓ(I) + b) ≥−ξi, i = , . . . , m
ξi ≥, i = , . . . , m.
If Φ is associated with a Mercer kernel K, the corresponding unconstrained version reads
in the RKHS

Supervised Learning by Support Vector Machines 

SVM soft margin classification in RKHS (Primal problem)

∥f ∥
HK + C
m
∑
i=Lh (yi, f (xi) + b)+ →min
f ∈HK .
With K defined by (> .) the matrix-vector form of the dual problem looks as those for
the linear soft margin classifier. The function fw∗reads as in (> .) and, using (> .),
the intercept can be computed by
b∗= yi −⟨w∗, Φ(xi)⟩ℓ(I) = yi −∑
j∈˜IS
α∗
j y jK(xj, xi),
i ∈˜IS.
...
Support Vector Regression
In the following, the linear regression models are generalized to feature spaces.
Hard margin regression:
One obtains
SVM hard margin regression (Primal problem)

∥w∥
ℓ(I) →minw,b
subject to
⟨w, Φ(xi)⟩ℓ(I) + b −yi
≤
є,
−⟨w, Φ(xi)⟩ℓ(I) −b + yi
≤
є,
i = , . . . , m.
If Φ is associated with a Mercer kernel K, then f (x) = ⟨w, Φ(x)⟩ℓ(I) lies in the RKHS
HK, and the model can be rewritten using (> .) from the point of view of RKHS as
SVM hard margin regression in RKHS (Primal problem)

∥f ∥
HK →min f ∈HK
subject to
f (xi) + b −yi
≤
є,
−f (xi) −b + yi
≤
є,
i = , . . . , m.
The dual problem reads in matrix-vector form as the dual problem for the linear SV hard
margin regression except that we have to use the kernel matrix K defined by (> .). Let
(α+)∗,(α−)∗be the solution of this dual problem and α∗= (α+)∗−(α−)∗. Then one can
compute the optimal function fw∗using (> .) with the corresponding feature space
modification as
fw∗(x) = ∑
i∈IrS
α∗
i K(xi, x).
(.)
One obtains sparse representations in terms of the support vectors. By (> .), the
intercept can be computed by
b∗= yi −∑
j∈IrS
α∗
j K(xj, xi) −є,
i ∈IS.


Supervised Learning by Support Vector Machines
Soft margin regression: In the feature space, the the soft margin regression model is
SVM soft margin regression (Primal problem)

∥w∥
ℓ(I) + C ∑m
i=(ξ+
i + ξ−
i ) →minw,b,ξ±
i
s.t.
⟨w, Φ(xi)⟩ℓ(I) + b −yi ≤є + ξ−
i ,
−⟨w, Φ(xi)⟩ℓ(I) −b + yi ≤є + ξ+
i ,
ξ+
i , ξ−
i ≥.
Having a feature map associated with a Mercer kernel K the corresponding unconstrained
problem can be written as the following minimization problem in the RKHS HK:
SVM soft margin regression in RKHS (Primal problem)

∥f ∥HK + C
m
∑
i=
Lє(yi, f (xi) + b) →min
f ∈HK .
The dual problem looks as the dual problem for linear SV soft margin regression but with
kernel (> .). From the solution of the dual problem (α+)∗,(α−)∗one can compute
the optimal function fw∗as in (> .) and the optimal intercept using (> .) as
b∗= yi −∑
j∈IrS
α∗
j K(xj, xi) −є,
i ∈˜IS.
> Figure -, left shows an SVM soft margin regression function for the data in
> Fig. -.
...
Relations to Sparse Approximation in RKHSs, Interpolation
by Radial Basis Functions and Kriging
SVM regression is related to various tasks in approximation theory. Some of them will be
sketched in the following.
Relation to sparse approximation in RKHSs: Let HK be a RKHS with kernel K. Consider
the problem of finding for an unknown function ˜f ∈HK with given ˜f (xi) = yi, i = , . . . , m
an approximating function of the form
f (x) :=
m
∑
i=
αiK(x, xi) ∈HK
(.)
with only few summands. A starting point would be to minimize the HK-norm of the
error and to penalize the ℓ-norm of α given by ∥α∥:= ∣{i : αi /= }∣to enforce

Supervised Learning by Support Vector Machines 

sparsity. Unfortunately, the complexity when solving such ℓ-penalized problems increases
exponentially with m. One remedy is to replace the ℓ-norm by the ℓ-norm, i.e., to deal
with

∥˜f (x) −
m
∑
i=
αiK(x, xi)∥

HK
+ є∥α∥→min
α ,
(.)
where є > . This problem and its relation to support vector regression were considered by
[, ]. Using the relations in RKHS from
> Sect. .., in particular the reproducing
property (H), this problem becomes

α
TKα −
m
∑
i=
αi yi + 
∥˜f ∥
HK + є∥α∥→min
α ,
(.)
where K is defined by (> .). With the splitting
αi = α+
i −α−
i , α±
i ≥, α+
i α−
i = , i = , . . . , m
and consequently ∣αi∣= α+
i + α−
i , the sparse approximation model (> .) has finally the
form of the dual problem of the SVM hard margin regression without intercept:
SVM hard margin regression without intercept (Dual problem)

(α+ −α−)TK(α+ −α−) + є⟨m, α+ + α−⟩−⟨y, α+ −α−⟩→min
α+,α−
subject to
α± ≥.
Note that for є > the additional constraints α+
i α−
i = , i = , . . . , m are automatically
fulfilled by the minimizer since otherwise, the Kuhn–Tucker conditions (> .) without
intercept would imply the contradiction f (xi) = yi + є = yi −є.
Relation to the interpolation by radial basis functions: For є = , problem (> .),
resp. (> .) becomes
F(α) := 
α
TKα −α
Ty →min
α .
If K is positive definite, the solution of this problem is given by the solution of
∇F(α) = Kα −y = ,
⇔
Kα = y
and the approximating function f reads
f (x) = ⟨K−y,(K(x, xi))m
i=⟩.
(.)
This is just the solution of the interpolation problem to find f of the form (> .) such
that f (xi) = yi for all i = , . . . , m. If the kernel K of the positive definite matrix arises from
a radial basis function κ(x) = k(∥x −t∥), i.e., K(x, t) = κ(x −t) as, e.g., from a Gaussian
or an inverse multiquadric described in > Sect. .., this interpolation problem is called
interpolation by radial basis function.


Supervised Learning by Support Vector Machines
If the kernel K arises from a conditionally positive definite radial function κ of order ,
e.g., from a thin plate spline, the matrix K is in general not positive semi-definite. In this
case, it is useful to replace the function f in (> .) by
f (x) :=
m
∑
i=
αiK(x, xi) +
n
∑
k=
βk pk(x),
where n is the dimension of the polynomial space Π −(Rd) and {pk : k = , . . . , n}
is a basis of Π −(Rd). The additional degrees of freedom in the interpolation problem
f (xi) = yi, i = , . . . , m are compensated by adding the new conditions
m
∑
i=
αi pk(xi) = ,
k = , . . ., n.
This leads to the final problem of finding α := (αi)m
i=and β := (βk)n
k=such that
(K
P
PT
)(α
β) = (y
),
P := (pk(xi))m,n
i,k=.
(.)
If the points {xi : i = , . . . , m} , m ≥dim(Π −(Rd)) are Π −(Rd)-unisolvent, i.e., the
zero polynomial is the only polynomial from Π −(Rd) that vanishes on all of them, then
the linear system of equations (> .) has a unique solution. To verify that the coefficient
matrix in (> .) is indeed invertible, consider the corresponding homogeneous system.
Then the second system of equations PTα = means that α satisfies (> .). Multiplying
the first system of equations by αT gives = αTKα + (PTα)Tβ = αTKα. By the definition of
conditionally positive definite functions of order
this is only possible if α = . But then
Pβ = . Since the points {xi : i = , . . . , m} are Π −(Rd)-unisolvent this implies that
β = .
The interpolation by radial basis functions having (conditionally) positive definite ker-
nels was examined, including fast evaluation techniques for the interpolating function f ,
by many authors, for an overview see, e.g., [, , ].
Relation to kriging: The interpolation results can be derived in another way by kriging.
Kriging is a group of geostatistical techniques to interpolate the unknown value of a ran-
dom field from observations of its value at nearby locations. Based on the pioneering work
of [] on the plotter of the distance-weighted average gold grades at the Witwatersrand
reef in South Africa, the French mathematician [] developed its theoretical foundations.
Let S(x) denote a random field such that the expectation value fulfills E(S(X)) = ,
which is the setting in the so-called simple kriging. Let K(xi, xj) := Cov(S(xi), S(x j))
and K := (K(xi, xj))
m
i,j=. The aim is to approximate the value S(x) from observations
S(xi) = yi, i = , . . ., m by the kriging estimator
ˆS(x) =
m
∑
i=
ωi(x)S(xi)

Supervised Learning by Support Vector Machines 

in such a way that the variance of the error is minimal, i.e.,
Var(ˆS −S) = Var(ˆS) + Var(S) −Cov(S, ˆS)
=
m
∑
i=
m
∑
j=
ωi(x)ω j(ˆx)K(xi, xj) −
m
∑
i=
ωi(x)K(x, xi) + Var(S) →min
ω(x).
Setting the gradient to zero, the minimizer ω∗= (ω∗
(x), . . . , ω∗
m(x))
T is given by the
solution of the following linear system of equations
Kω = (K(x, xi))m
i=.
In case K is invertible, we get
S(x) = ⟨y,K−(K(x, xi))m
i=⟩= ⟨K−y,(K(x, xi))m
i=⟩.
Supposing the same values K(xi, xj) as in the interpolation task, this is exactly the same
value as f (x) from the radial basis interpolation problem (> .).
...
Least Squares Classiﬁcation and Regression
Also in the feature space, least squares classification and regression can be treated by the
same model. Least Squares – Support Vector Classifiers were introduced by [], while least
squares regression was also considered within regularization network approaches, e.g., by
[, ]. The least squares model in the feature space is
LS classification/regression in feature space (Primal problem)

∥w∥
ℓ(I) + C

m
∑
i=
(⟨w, Φ(xi)⟩ℓ(I) + b −yi)→min
w,b .
and becomes in the case that the feature map is related with a Mercer kernel K a problem
in a RKHS HK:
LS classification/regression in RKHS (Primal problem)

∥f ∥HK + C

m
∑
i=
(f (xi) + b −yi)→min
w,b .
Setting gradients to zero, one can try to solve this primal problem via a linear system of
equations (> .) with X := (Φ(x) . . . Φ(xm)). However, one has to compute with XX
T
here, which is only possible if the feature space is finite dimensional. In contrast, the dual
approach leads to the linear system of equations (> .), which involves only the kernel
matrix K = XTX from (> .). Knowing the dual variable α∗, the optimal function fw∗
can be computed using (> .) with the feature space modification as
fw∗(x) =
m
∑
i=
α∗
i ⟨Φ(xi), Φ(x)⟩
m
∑
i=
α∗
i K(xi, x).


Supervised Learning by Support Vector Machines
15
20
25
30
35
40
45
5
6
7
8
9
10
11
12
15
20
25
30
35
40
45
5
6
7
8
9
10
11
12
⊡Fig. -
Support vector machine (SVM) regression using the Gaussian with c = for the data in
> Fig. -. SVM soft margin regression curve with C = .and є = .(left). Least squares
SVM regression curve with C = 
In general there is no sparse representation with respect to the vectors xi. For more infor-
mation on least squares kernel methods the reader may consult []. > Figure -, right
shows an least squares SVM function for the data in > Fig. -.
...
Other Models
There are numerous models related to the classification and regression models of the pre-
vious subsections. A simple classification model, which uses only the hinge loss function
without penalizing the weight, was proposed by []:
m
∑
i=
Lh (yi,⟨w, xi⟩+ b) →min
w,b .
This approach is called robust linear programming (RLP) and requires only linear pro-
gramming methods. Note that the authors weighted the training errors by /n±, where
n± := ∣{i; yi = ±}∣. The linear SV soft margin classifier adds just the penalizer λ
∥w∥
with
λ = /C, < C < ∞to the RLP term that leads to quadratic programming. Alternatively,
one can add instead the ℓ-norm the ℓ-norm of the weight as it was done by []:
m
∑
i=
Lh (yi,⟨w, xi⟩+ b) + λ∥w∥→min
w,b .
As in (> .), the ℓpenalizer enforces the sparsity of the solution vector w∗. Note that
the sparsity of w∗itself and not a sparse representation of w∗as linear combination of the
support vectors xi is announced here. The ℓ-penalizer was introduced in the statistical

Supervised Learning by Support Vector Machines 

⊡Fig. -
From left to right: (i) Sample CT slice from a three-dimensional scan of the data set with
contours of bladder and prostate. (ii) A zoom within the region of interest shows that the
organs are very diﬃcult to distinguish visually. (iii) Manual classiﬁcation by an expert as
“accepted truth.”(iv) A classiﬁcation result: The images are ﬁltered by a three-dimensional
steerable pyramid ﬁlter bank with angular orientations and four decomposition levels.
Then local histograms are built for the ﬁlter responses with ten bins per channel. Including
the original grey values, this results in features per image voxel, which are used for
classiﬁcation by the “ℓ-ℓ-SV”machine
context of linear regression in conjunction with the least squares loss function by [] and
is called “LASSO” (Least Absolute Shrinkage and Selection Operator):
m
∑
i=
Llsq (yi,⟨w, xi⟩+ b) + λ∥w∥→min
w,b
As emphazised in
> Sect. ..., the ℓ-norm is more or less a replacement for the ℓ-
norm to make problems computable. Other substitutes of the ℓ-norm are possible, e.g.,
∥w∥ℓ≈∑d
j=(−e ∣w j∣),
> that gives
m
∑
i=
Lh (yi,⟨w, xi⟩+ b) + λ
d
∑
j=
(−e ∣w j∣) →min
w,b .
This is a non-convex model and was proposed by [, ] as FSV (Features Selection
concaVe). Numerical solution methods via successive linearization algorithms and differ-
ence of convex functions algorithms were applied.
Further, one can couple several penalizers and generalize the models to the feature space
to obtain nonlinear classifiers as it was done, e.g., by []. > Figure -shows an example
for the binary classification of specific organs in CT scans with a ℓ-ℓ-SV machine taken
from the above paper, where more information can be found. In particular, a χkernel was
used here.
...
Multi-class Classiﬁcation and Multitask Learning
So far only binary classification was considered. Assume now that one wants to learn
K > classes.
> Figure -shows a typical example of classes for the classification
of mammals, see [].


Supervised Learning by Support Vector Machines
Addax
African+Wild+Dog
Cheetah
Black+Rat
Bobcat
Deer+Mouse
Flying+Squirrel
Serval
Caribou
Dingo
Hyena
Deer
⊡Fig. -
Classiﬁcation of mammal images: from classes of animals that were used for
classiﬁcation in []. Typically these classes share common characteristics as in the diﬀerent
rows above (deers, canines, felines, and rodents), e.g., the texture or shape
Some attempts to extend the binary case to multi-classes were achieved by adding con-
straints for every class, see [, ]. In case of many classes, this approach often results
in quadratic problems, which are hard to solve and difficult to store. In the following, two
general approaches to handle multiple classes are presented namely with
(i)
Vector-valued binary class labeling
(ii)
K class labeling
The dominating approach for solving multi-class problems using SVMs is based on reduc-
ing a single multi-class problem to multiple binary problems. For instance a common
method is to build a set of binary classifiers, where each classifier distinguishes between

Supervised Learning by Support Vector Machines 

one of the labels and the rest. This one-versus-all classifier cannot capture correlations
between different classes since it breaks the multi-class problem into independent binary
problems. More general, one can assign to each class a vector-valued binary class label
(y(), . . . , y(κ))T ∈{−,}κ and use a classifier based on
F(x) := (sgn(⟨w(k), x⟩+ b(k)))
κ
k=.
For example, in the one-versus-all method, the classes can be labeled by {(−+ δr,k)K
k=:
r = , . . . , K}, i.e., κ = K and the assignment of x to a class can be made according to the
shortest Hamming distance of F(x) from these class labels. In the one-versus-all example,
there was κ = K. More sophisticated methods use values κ > K and error-correcting output
codes as []. Note that κ different labels are in general possible with binary vectors of
length κ, which is an upper bound for the number of classes that could be learned. In the
learning process one can obtain w(k) ∈Rm and b(k) ∈R by solving, e.g.,


κ
∑
k=
∥w(k)∥+
κ
∑
k=
Ck
m
∑
i=
L (yi,⟨w(k), xi⟩+ b(k)) →
min
w(k),b(k),
(.)
where L is some loss function. Note that this problem can be decoupled with respect to
k. Let W := (w() . . . w(κ)) ∈Rd,κ be the weight matrix. Then the first sum in (> .)
coincides with the squared Frobenius norm of W defined by
∥W∥
F :=
κ
∑
k=
d
∑
i=
(w(k)
i
)

.
Let us consider the second labeling approach. Here one assumes that each class label is an
integer from Y := {, . . . , K}. As before, one aims to learn weight vectors w(k), k = , . . . , K
(the intercept is neglected for simplicity here). The classifier is given by
FW(x) := argmax
k=,...,K
⟨w(k), x⟩.
A training sample (xi, yi) is correctly classified by this classifier if
⟨w(yi), xi⟩≥⟨w(k), xi⟩+ ,
∀k = , . . . , K, k /= yi.
Without adding at the left-hand side of the inequality, correct classification is still attained
if there is strong inequality for k /= yi. This motivates to learn the weight vectors by solving
the minimization problem

∥W∥
F →min
W
subject to
⟨w(yi), xi⟩+ δyi,k −⟨w(k), xi⟩≥,
∀k = , . . . , K and i = , . . . , m.
After introducing slack variables to relax the constraints one gets

∥W∥
F + C ∑m
i=ξi →min
W,ξi
subject to
⟨w(yi), xi⟩+ δyi,k −⟨w(k), xi⟩≥−ξi,
∀k = , . . . , K and i = , . . . , m.


Supervised Learning by Support Vector Machines
This can be rewritten as the following unconstrained problem:

∥W∥
F + C
m
∑
i=
lh (⟨w(yi), xi⟩+ max
k (⟨w(k), xi⟩−δyi,k)) →min
W .
In this functional the learning tasks are coupled in the loss function.
In general the aim of multitask-learning is to learn data that are common across mul-
tiple related supervised learning tasks, i.e., to faciliate “cooperative” learning. Recently,
multitask-learning has received attention in various applications, see the paper of [].
Learning of vector-valued functions in RKHSs was considered, e.g., by []. Inspired by
the “sparseness” models in
> Sect. ..., which focus on the sparsity of the weight
vector w one can ask for similar approaches for a weight matrix W. As a counterpart of
the ℓ-norm of a weight vector there can serve a low rank of the weight matrix. But as in
ℓ-penalized minimization problems such problems are computationally not manageable.
A remedy is to replace the low rank condition by demanding a small trace norm or nuclear
norm of W defined by
∥W∥∗:= ∑
j
σj,
where σj are the singular values of W. Then a minimization problem to learn the weight
matrix reads, e.g., as

∥W∥∗+ C
K
∑
k=
m
∑
i=
L(yi,⟨w(k), xi⟩),→min
W
where L is mainly the least squares loss function. Such models were considered by [], []
and []. Other approaches use the norm
∥W∥,:=
d
∑
j=
∥(w(k)
j
)
K
k=∥
which favors a small number of nonzero rows in W instead of the trace norm, see [] and
[]. Another interesting model was proposed by [] and learns in addition to a weight
matrix an orthogonal matrix U ∈O by minimizing
∥W∥,+ C
K
∑
k=
m
∑
i=
L(yi,⟨w(k),Uxi⟩). →
min
W,U∈O .
The numerical solution of multitask problems which are convex, but non-smooth
require sophisticated techniques. The trace norm minimization problem can be, e.g.,
reformulated as a semi-definite program (SDP) and then existing SDP solvers can be used
as long as the size of the problem is moderate, see the papers of [, ]. A smooth, but
nonconvex reformulation of the problem and a subsequent solution by a conjugate gradi-
ent or alternating minimization method was proposed, e.g., by []. Accelerated proximal
gradient methods (multistep methods) and Bregman iterative methods were applied in the
papers of [, , , ]. A new primal-dual reformulation of the problem in conjunction
with a gradient projection method to solve the reduced dual problem was given by [].

Supervised Learning by Support Vector Machines 

...
Applications of SVMs
SVMs have been applied to many real-world problems. Some applications were already
sketched in the previous subsections. Very often SVMs are used in connection with other
techniques, in particular feature extraction/selection methods to specify the input domain.
A non-exhausive list of SVM applications includes text categorization, see [, ],
hand-written character recognition, see [], texture and image classification, see [], pro-
tein homology detection, see [], gene expression, see [], medical diagnostics, see [], and
pedestrian and face detection, see [, ].
This subsection describes only two applications of SVM classification and shows how
the necessary design choices can be made. In particular, one has to choose an appropriate
SVM kernel for the given application. Default options are Gaussians or polynomial ker-
nels and the corresponding SVMs often already outperform other classification methods.
Even for such parameterized families of kernels one has to specify the parameters like the
standard deviation of the Gaussian or the degree of the polynomial. In the Gaussian case a
good choice of the standard deviation in the classification problem is the distance between
closest points within different classes. In the absence of reliable criteria one could use a
validation set or cross-validation the determine useful parameters. Various applications
require more elaborate kernels which implicitly describe the feature space.
Hand-written digit recognition:
The problem of hand-written digit recognition was the
first real world task on which SVMs were successfully tested. The results are reported in
detail in []. This SVM application was so interesting because other algorithms incorpo-
rating prior knowledge on the USPS database have been designed. The fact that SVMs
perform better than these specific systems without using prior detailed information is
remarkable, see [].
Different SVM models have been tested on two databases:
•
United States Postal Service (USPS): ,training and ,test patterns of the
numbers , . . .,, represented by × gray level matrices, see > Fig. -
•
National Institute of Standard and Technology (NIST): ,training and ,test
patterns, represented by × gray level matrices
In the following, the results for the USPS database are considered. For constructing the
decision rules SVMs with polynomial and Gaussian kernels were used:
K(x, t) := (⟨x, t⟩/)n ,
K(x, t) := e−∥x−t∥/(σ ).
The overall machine consists of classifiers, each one separating one class from the rest
(one-versus-all classifier). Then the ten-class classification was done by choosing the class
with the largest output number.
All types of SVMs demonstrated approximately the same performance shown in the
following tables, cf. []. The tables contain the parameters for the hard margin machines,
the corresponding performance, and the average (over one classifier) number of support


Supervised Learning by Support Vector Machines
⊡Fig. -
Examples of patterns from the United States Postal Service (USPS) database, see []
vectors. Moreover, it was observed that the different types of SVMs use approximately the
same set of support vectors.
Degree n






Error
.
.
.
.
.
.
Number of SV






Results for SVM classification with polynomial kernels
σ
.
.
.
.
.
.
Error
.
.
.
.
.
.
Number of SV






Results for SVM classification with Gaussian kernels
Finally, it is worth to mention that the training data are not linearly separable; ≈%
of these data were misclassified by a linear learning machine. For a degree polynomial
kernel only the four examples in > Fig. -were misclassified. For polynomials of degree
the training data are separable. The number of support vectors increases only slowly with
the degree of the polynomial.
Color image recognition: Image recognition is another area where SVMs were success-
fully applied. Chapelle et al. [] have reported their SVM classification results for color
image recognition. The database was a subset (Corel) of the Corel Stock Photo Collec-
tion consisting of ,photos associated with categories. Each category was split into
/for training and /for testing. Again the one-versus-all classifier was applied.
The images were not used themselves as inputs but each image was associated to its
color histogram. Since each color is a point in a three-dimensional vector space and the

Supervised Learning by Support Vector Machines 

4
4
8
5
⊡Fig. -
Labeled USPS examples of training errors for the SVM with second-degree polynomial
kernel, see []
number of bins per color was fixed at , the dimension of such a histogram (and thus of
the feature space) is d = . Note that low-level features like histograms have the advantage
that they are invariant with respect to many operations, and allow the comparison of images
of different sizes. Of course, local high-level image features like edges are not captured by
low-level features. Chapelle and coworkers have used both the RGB (Red Green Blue) and
the HSV/HSB (Hue Saturation Value/Brightness) histogram representation. Note that HSV
arranges the geometry of RGB in an attempt to be more perceptually relevant. As kernels
they have used
K(x, t) := e−dist(x,t)/σ ,
where dist denotes a measure of similarity in the feature space that has to be determined.
For histograms, the χfunction
dist(x, t) :=
d
∑
i=
(xi −ti)
xi + ti
is accepted as an appropriate distance measure. It is not clear if the corresponding kernel
is a Mercer kernel. For the distances distp(x, t) := ∥x −t∥p
p, p = ,this is the case.
As can be seen in the following table, the SVM with the χand the ℓdistance perform
similarly, and significantly better than the SVM with the squared ℓdistance. Therefore,
the Gaussian kernel is not the best choice here. RGB- and HSV-based methods perform
similarly.
Linear
Degree poly
χ
ℓ
Gaussian
RGB
.
.
.
.
.
HSV
.
.
.
.
.
Error rates (percent) using different SVM kernels
For comparision, Chapelle and coworkers conducted some experiments of color image
histogram (HSV-based) classifications with the K-nearest neighbor algorithm with χand
ℓ. Here K = gives the best result presented in the following table:
χ
ℓ
.
.
Error rates (percent) with k-nearest neighbor algorithm
The χ-based SVM is roughly twice as good as the χ-based K-nearest neighbor technique.


Supervised Learning by Support Vector Machines
.
Survey of Mathematical Analysis of Methods
..
Reproducing Kernel Hilbert Spaces
General theory:
For simplicity, let X ⊂Rd be a compact set throughout this subsection.
Moreover, only spaces of real-valued functions are considered. Let C(X) denote the set of
continuous functions on X. Together with the norm
∥f ∥C(X ) = sup{∣f (x)∣: x ∈X}
this becomes a Banach space. Further, we denote by L(X) the Hilbert space of (equiva-
lence classes) of quadratic integrable, real-valued functions on X with inner product and
norm given by
⟨f , g⟩L:= ∫X f (x)g(x) dx,
∥f ∥L= (∫X f (x)dx)
/
.
Since X is compact, the space C(X) is continuously embedded into L(X), which means
that ∥f ∥L(X ) ≤C∥f ∥C(X ) for all f ∈C(X). A function K : X × X →R is symmetric,
if K(x, t) = K(t, x) for all x, t ∈X. With a symmetric function K ∈L(X × X) one can
associate an integral operator TK : L(X) →L(X) by
TK f (t) := ∫X K(x, t)f (x) dx.
This operator is a compact and self-adjoint operator and K is called its kernel. The fol-
lowing spectral theorem holds true for compact, self-adjoint operators, i.e., in particular
for TK.
Theorem (Spectral theorem for compact, self-adjoint operators)
Let T be a compact,
self-adjoint operator on the Hilbert space H. Then there exists a countable (possibly finite)
orthonormal system {ψi : i ∈I} and a zero sequence (λi)i∈I, λi ∈R/{} such that
H = kerT ⊕span{ψi : i ∈I}
and
T f = ∑
j∈I
λj⟨f ,ψj⟩H ψj
∀f ∈H.
(.)
The numbers λj are the nonzero eigenvalues of T and ψ j are the corresponding eigenfunctions.
If T is a positive operator, i.e.,
⟨T f , f ⟩H = ∫X ∫X K(x, t)f (x)f (t) dxdt ≥
∀f ∈H,
then the values λj are positive.

Supervised Learning by Support Vector Machines 

Consider the special operator TK for a symmetric kernel K ∈L(X × X). Using the
L-orthonormal eigenfunctions {ψi : i ∈I} of TK, one can also expand the kernel itself as
K(x, t) = ∑
j∈I
λjψj(x)ψj(t),
where the sum converges as those in (> .) in general only in L(X × X). One can
tighten the statement if K is continuous and symmetric. Then TK : C(X) →C(X) is a
compact operator on the Pre-Hilbert spaces C(X) equipped with the L-norm into itself
and the functions ψj are continuous. If f ∈C(X), then the right-hand side in (> .)
converges absolutely and uniformly. To prove such a convergence result also for the kernel
expansion we need moreover that the operator TK is positive. Unfortunately, it is not true
that a positive kernel K implies a positive operator TK. There is another criterion, which
will be introduced in the following. A matrix K ∈Rm,m is called positive semi-definite if
α
TK α ≥
∀α ∈Rm
and positive definite if strong inequality holds true for all α /= . A symmetric kernel K :
X × X →R is positive (semi)-definite if the matrix K := (K(xi, xj))
m
i,j=is positive (semi)-
definite for all finite sets {x, . . ., xm} ⊂X. Now a symmetric kernel K ∈C(X × X) is
positive semi-definite if and only if the corresponding integral operator TK is positive.
Theorem (Mercer’s theorem)
Let K ∈C(X × X) be a continuous, symmetric and
positive semi-definite function with corresponding integral operator TK. Then K can be
expanded into an absolutely and uniformly convergent series in terms of TK’s orthonormal
eigenfunctions ψj and the associated eigenvalues λ j > as follows:
K(x, t) = ∑
j∈I
λjψj(x)ψj(t).
(.)
Moreover, if K is positive definite, then {ψi : i ∈I} form an orthonormal basis of L(X).
A continuous, symmetric, positive semi-definite kernel is called a Mercer kernel. Mercer
kernels are closely related to “reproducing kernel Hilbert spaces.”
Let H be a real Hilbert space of functions f : X →R. A function K : X × X →R is
called a reproducing kernel of H if
(H)
Kt := K(⋅, t) ∈H
∀t ∈H,
(H)
f (t) = ⟨f , Kt⟩H
∀f ∈H and ∀t ∈X
(Reproducing Property).
In particular, property (H) implies for f := ∑m
i=αiKxi and g := ∑n
j=βjKx j that
⟨f , g⟩H :=
m
∑
i=
n
∑
j=
αiβjK(xi, tj),
∥f ∥
H =
m
∑
i=
m
∑
i,j=
αiαjK(xi, tj) = α
TK α,
(.)


Supervised Learning by Support Vector Machines
where α = (α, . . . , αm)T and K := (K(xi, xj))
m
i,j=. If such a kernel exists for H, then it
is uniquely determined. A Hilbert space that exhibits a reproducing kernel is called repro-
ducing kernel Hilbert space (RKHS). To emphasize the relation with the kernel we write
H = HK for such spaces. In HK, the set of all finite linear combinations of Kt, t ∈X is
dense, i.e.,
HK = span {Kt : t ∈X}.
(.)
Moreover, the kernel K of a RKHS must be a symmetric, positive semi-definite function,
see, []. Finally, based on the Riesz representation theorem another characterization of
RKHSs can be given. It can be shown that a Hilbert space H is a RKHS if and only if the
the point evaluation functionals δx : H →R determined by δx f := f (x) are continuous
on H, i.e.,
∣f (x)∣≤C∥f ∥H
∀f ∈H.
Conversely, by the following theorem, any Mercer kernel gives rise to a RKHS.
Theorem 
Let K ∈C(X × X) be a continuous, symmetric, and positive semi-definite
function. Then there exists a unique Hilbert space HK of functions on X which is a RKHS with
kernel K. The space HK consists of continuous functions on X and the embedding operator
ιK : HK(X) →C(X)(→L(X)) is continuous.
Proof
. First, one constructs a Hilbert space which fulfills (H) and (H). By (H), the
space HK has to contains all functions Kt, t ∈X and since the space is linear also their
finite linear combinations. Therefore, we define
H:= span{Kt : t ∈X}.
According to (> .) we equip this space with the inner product and corresponding
norm
⟨f , g⟩H:=
m
∑
i=
n
∑
j=
αiβjK(xi, tj),
∥f ∥
H= α
TK α.
It can easily be checked that this is indeed an inner product. In particular ∥f ∥H= for
some f = ∑m
i=αiKxi implies that f (t) = ∑m
i=αiK(t, xi) = for all t ∈X by the following
argument: Set xm+:= t. By the positive semi-definiteness of K is follows for any є ∈R that
(α
T,є)(K(xi, xj))
m+
i,j=(α
є) = α
TK α + є
m
∑
i=
αiK(xi, t) + єK(t, t) ≥.
With αTK α = ∥f ∥H= this can be rewritten as
є (f (t) + єK(t, t)) ≥.

Supervised Learning by Support Vector Machines 

Since K is positive semi-definite we have that K(t, t) ≥. Assume that f (t) < . Then
choosing < є < −f (t)/K(t, t) if K(t, t) > and < є if K(t, t) = leads to a contradic-
tion. Similarly, assuming that f (t) > and choosing −f (t)/K(t, t) < є < ) if K(t, t) > 
and є < if K(t, t) = gives a contradiction. Thus f (t) = .
Now one defines HK to be the completion of Hwith the associated norm. This space
has the reproducing property (H) and is therefore a RKHS with kernel K.
. To prove that HK is unique, assume that there exists another Hilbert space H of
functions on X with kernel K. By (H) and (> .), it is clear that His a dense subset
of H. By (H) it follows that ⟨f , g⟩H = ⟨f , g⟩HK for all f , g ∈H. Since both H and HK are
completions of Hthe uniqueness follows.
. Finally, one concludes by the Schwarz inequality that
∣f (t)∣= ∣⟨f , Kt⟩HK∣≤∥f ∥HK∥Kt∥HK = ∥f ∥HK
√
K(t, t)
so that f is continuous since K is continuous. Moreover, ∥f ∥C(X ) ≤C∥f ∥HK with C :=
maxt∈X
√
K(t, t), which means that the embedding ιK is continuous.
∎
Since the completion of His rather abstract, another characterization of HK based
on Mercer’s theorem is useful. Let {ψi : i ∈I} be the L-orthonormal eigenfunctions of
TK with corresponding eigenvalues λj > from the Mercer theorem. Then we have by
Schwarz’s inequality and Mercer’s theorem for w := (wi)i∈I ∈ℓ(I) that
∑
i∈I
∣wi
√
λiψi(x)∣≤∥w∥ℓ(∑
i∈I
λiψ
i (x))
/
= ∥w∥ℓ
√
K(x, x)
so that the series ∑i∈I wi
√λiψi(x) converges absolutely and uniformly for all (wi)i∈I ∈
ℓ(I). Now another characterization of HK can be given.
Corollary 
Let K ∈C(X × X) be a continuous, symmetric, and positive semi-definite
kernel with expansion (> .). Then the Hilbert space
H := {∑
i∈I
wi
√
λiψi : (wi)i∈I ∈ℓ(I)}
with inner product
⟨f , g⟩H := ∑
i∈I
wiωi =
∞
∑
j=

λj
⟨f ,ψj⟩L⟨g,ψj⟩L
for f := ∑i∈I wi
√λiψi and g := ∑j∈I ω j
√
λjψj is the RKHS with kernel K, i.e., H = HK.
The system {φi := √λiψi : i ∈I} is an orthonormal basis of H.
If K is positive definite, then H can be also characterized by
H =
⎧⎪⎪⎨⎪⎪⎩
f ∈L(X) :
∞
∑
j=

λj
∣⟨f ,ψj⟩L∣< ∞.
⎫⎪⎪⎬⎪⎪⎭


Supervised Learning by Support Vector Machines
Proof
We see that {√λiψi : i ∈I} is an orthonormal basis of H by the above definition
of the inner product. The second equality in the definition of the inner product follows by
the orthonormality of the ψi in L.
It remains to show that K fulfills (H) and (H). Concerning (H) it holds Kt =
∑i∈I
√λiψi(t)√λiψi and since
∑
i∈I
(
√
λiψi(t))

= K(t, t) < ∞,
it follows that Kt ∈H. Using the orthonormal basis property one can conclude with respect
to (H) that
⟨f , Kt⟩H = ⟨∑
j∈I
wj
√
λjψj,∑
i∈I
√
λiψi(t)
√
λiψi⟩
H
= ∑
i∈I
wi
√
λiψi(t) = f (t).
∎
Kernels: The choice of appropriate kernels for SVMs depend on the application. Default
options are Gaussians or polynomial kernels, which are described together with some more
examples of Mercer kernels below:
. Let X := {x ∈Rd : ∥x∥≤R} with radius R > . Then the dot product kernels
K(x, t) :=
∞
∑
j=
a j(x ⋅t)j,
a j ≥,
∞
∑
j=
a jRj < ∞
are Mercer kernels on X. A proof that these kernels are indeed positive semi-definite is
given in []. A special case appears if X contains the coordinate vectors e j, j = , . . . , d
and the kernel is K(x, t) = + x ⋅t. Note that even in one dimension d = , this kernel
is not positive definite. Here the corresponding RKHS HK is the space of linear functions
and {, x, . . . , xd} forms an orthonormal basis of HK.
The special dot product K(x, t) := (c + x ⋅t)n, c ≥, n ∈N, also known as polynomial
kernel was introduced in statistical learning theory by []. More general dot products
were described, e.g., by []. See also all-subset kernels and ANOVA kernels in [].
. Next, consider translation invariant kernels
K(x, t) := κ(x −t),
where κ : Rd →R is a continuous function, which has to be even, i.e., κ(−x) = κ(x)
for all x ∈Rd to ensure that K is symmetric. We are interested if K is a Mercer kernel on
Rd and hence on any subset X of Rd. First, we know from Bochner’s theorem that K is
positive semi-definite if and only if it is the Fourier transform of a finite nonnegative Borel
measure on Rd. Let κ ∈L(Rd). Then, K is positive definite if and only if κ is bounded and
its Fourier transform is nonnegative and non-vanishing.
A special example on R (d = ) is the spline kernel K generated by the “hat function”
κ(x) := max{,−∣x∣/}. Its Fourier transform is ˆκ(ω) = (sin ω/ω)≥. Multivariate
examples of this form can be constructed by using, e.g., box splines. Spline kernels and
corresponding RKHSs were discussed, e.g., by [].
. A widely used class of translation invariant kernels are kernels associated with radial
functions. A function κ : Rd →R is said to be radial if there exists a function k : [,∞) →R

Supervised Learning by Support Vector Machines 

such that κ(x) = k(∥x∥) for all x ∈Rd. For radial kernels define
K(x, t) := k(∥x −t∥).
A result of Schoenberg [] says that K is positive semi-definite on Rd if and only if the
function k is completely monotone on [,∞). Recall that k is completely monotone on
(,∞) if k ∈C∞(,∞) and
(−)l k(l)(r) ≥
∀l ∈Nand ∀r > .
The function k is called completely monotone on [,∞) if it is in addition in C[,∞).
It holds that K is positive definite if and only if one of the following conditions is fulfilled
(i) k(√⋅) is completely monotone on [,∞) and not constant.
(ii) there exists a finite nonnegative Borel measure
on [,∞), i.e., not concentrated at
zero such that
k(r) = ∫
∞

e−rt d (t).
The proofs of these results on radial kernels are contained, e.g., in [].
For c > , the kernels K arising from the following radial functions κ are positive
definite:
κ(x) := e−∥x∥/c
Gaussian,
κ(x) := (c+ ∥x∥)−s, s > 
inverse multiquadric,
where the positive definiteness of the Gaussian follows from (i) and those of the inverse
multiquadric from (ii) with
k(r) =

Γ(s) ∫
∞

ts−e−cte−rt dt.
Positive definite kernels arising from Wendland’s radial basis functions with compact
support, see [], were applied in SVM classification by [].
Finally, we mention the following techniques for creating Mercer kernels.
Theorem
Let K j ∈C(X ×X), j = ,be Mercer kernels and p a polynomial with positive
coefficients. Then the following functions are also Mercer kernels:
(i)
K(x, t) := K(x, t) + K(x, t).
(ii)
K(x, t) := K(x, t)K(x, t).
(iii)
K(x, t) := p(K(x, t)).
(iv)
K(x, t) := eK(x,t).
Beyond the above Mercer kernels other kernels like kernels for text and structured data
(strings, trees), diffusion kernels on graphs, or kernel incorporating generative information
were used in practice, see [].


Supervised Learning by Support Vector Machines
Conditionally positive semi-definite radial functions: In connection with radial basis
functions so-called conditionally positive semi-definite functions κ(x) := k(∥x∥) were
applied for approximation tasks. Let Π −(Rd) denote the space of polynomials on Rd
of degree <
. This space has dimension dim(Π −(Rd)) = (d+ −). A continuous radial
function κ : Rd →R is conditionally positive semi-definite of order
if for all m ∈N, all
pairwise distinct points x, . . . , xm ∈Rd, and all α ∈Rm/{} satisfying
m
∑
i=
αi p(xi) = 
∀p ∈Π −(Rd)
(.)
the relation
α
TKα ≥,
K := (κ(xi −x j))
m
i,j=
holds true. If equality is attained only for α = the function κ is said to be conditionally
positive definite of order .
The following result is due to Micchelli, : For k ∈C[,∞)∩C∞(,∞), the function
κ(x) := k(∥x∥) is conditionally positive semi-definite of order
if and only if (−) k( )
is completely monotone on (,∞). If k is not a polynomial of degree at most , then κ is
conditionally positive definite of order .
Using this result one can show that the following functions are conditionally positive
definite of order :
κ(x) := (−)⌈s⌉(c+ ∥x∥)s, s > , s /∈N,
= ⌈s⌉
multiquadric,
κ(x) := (−)⌈s/⌉∥x∥s, s > , s /∈N,
= ⌈s/⌉,
κ(x) := (−)k+∥x∥k log ∥x∥, k ∈N,
= k + 
thin plate spline.
A relation of a combination of thin plate splines and polynomials to the reproducing
kernels of certain RKHSs can be found in [].
..
Quadratic Optimization
This subsection collects the basic material from optimization theory to understand the
related parts in the previous
> Sect. ., in particular the relation between primal and
dual problems in quadratic programming. More on this topic can be found in any book on
optimization theory, e.g., in [].
A (nonlinear) optimization problem in Rd has the general form
Primal problem (P)
θ(x) →min
x
subject to
g(x) ≤, h(x) = 
where θ : Rd →R is a real-valued function and g : Rd →Rm, h : Rd →Rp are vector-
valued functions. In general, only the case p < d is of interest since otherwise we are
confronted with the solution of a (nonlinear) system of equations. The region
G := {x ∈Rd : g(x) ≤, h(x) = },

Supervised Learning by Support Vector Machines 

where the objective function θ is defined and where all constraints are satisfied, is called
feasible region. There are classes of problems (P), which are well-examined as convex opti-
mization problems and in particular special classes of convex problems, namely linear and
quadratic problems. Problem (P) is called convex, if θ is a convex function and G is a
convex region. Recall, that x∗∈G is a local minimizer of θ in G if there exists a neigh-
borhood U(x∗) of x∗such that θ(x∗) ≤θ(x) for all x ∈U(x∗) ∩G. For convex problems,
any local minimizer x∗of θ in G is also a global minimizer of θ in G and therefore a solu-
tion of the minimization problem. This subsection deals mainly with the following setting,
which gives rise to a convex optimization problem:
(C)
θ convex and differentiable
(C)
gi, i = , . . . , m convex and differentiable
(C)
hj, j = , . . . , p affine linear
Important classes of problems fulfilling (C)–(C) are quadratic programs, where the objec-
tive function is quadratic and the constraints are (affine) linear and linear programs, where
the objective function is also linear. The constrained optimization problems considered
in > Sect. .are of this kind.
The function L : Rd × Rm
+ × Rp →R defined by
L(x, α, β) := θ(x) +
m
∑
i=
αi gi(x) +
p
∑
j=
βjhj(x)
is called the Lagrangian associated with (P) and the coefficients αi and βj are called
Lagrange multipliers. Recall that (x∗, λ∗) ∈Ω × Ξ, Ω ⊂Rd, Ξ ⊂Rn is called a saddle
point of a function Φ : Ω × Ξ →R if
Φ(x∗, λ) ≤Φ(x∗, λ∗) ≤Φ(x, λ∗)
∀(x, λ) ∈Ω × Ξ.
There is the following close relation between saddle point problems and min-max
problems:
Lemma 
Let Φ : Ω × Ξ →R. Then the inequality
max
λ∈Ξ min
x∈Ω Φ(x, λ) ≤min
x∈Ω max
λ∈Ξ Φ(x, λ)
holds true supposed that all extreme points exist. Moreover, in this case, the equality
max
λ∈Ξ min
x∈Ω Φ(x, λ) = Φ(x∗, λ∗) = min
x∈Ω max
λ∈Ξ Φ(x, λ)
is fulfilled if and only if (x∗, λ∗) is a saddle point of Φ.
The solution of (P) is related to the saddle points of its associated Lagrangian as detailed
in the following theorem.


Supervised Learning by Support Vector Machines
Theorem 
If (x∗,(α∗, β∗)) ∈Rd × (Rm
+ × Rp) is a saddle point of the Lagrangian
associated with the minimization problem (P), i.e.,
L(x∗, α, β) ≤L(x∗, α∗, β∗) ≤L(x, α∗, β∗)
∀x ∈Rd,∀(α, β) ∈Rm
+ × Rp,
then x∗is a solution of (P). Assume that the functions θ, g, h satisfy the conditions (C)–(C)
and that g fulfills in addition the following Slater condition:
there exists x∈Ω such that g(x) > and h(x) = .
Then, if x∗is a solution of (P) there exist α∗∈Rm
+ and β∗∈Rp such that (x∗,(α∗, β∗)) is a
saddle point of the associated Lagrangian.
By the next theorem, the minimizers of (P) can be also described via the following
conditions on the Lagrangian: there exist x∗∈G, α∗∈Rm
+ and β∗∈Rp such that
(KTC)∇xL(x∗, α∗, β∗) = ,
(KTC)(α∗)
Tg(x∗) = ,
α∗≥.
These conditions were independently established by Karush, Kuhn, and Tucker and are
mainly called Kuhn–Tucker conditions.
Theorem 
Let θ, g and h fulfill (C)–(C). If x∗satisfies (KTC)–(KTC), then x∗is a
solution of (P). Assume that g fulfills in addition the Slater condition. Then, if x∗is a solution
of (P), it also fulfills (KTC)–(KTC).
If there are only equality constraints in (P), then a solution is determined by
∇xL(x∗, β∗) = ,
h(x∗) = .
For the rest of this subsection, assume that (C)–(C) and the Slater condition hold
true. Let a solution x∗of (P) exist. Then, by Lemma and Theorem there exist α∗and β∗
such that
L(x∗, α∗, β∗) = max
α∈Rm
+ ,β min
x
L(x, α, β).
Therefore, one can try to find x∗as follows: for any fixed (α, β) ∈Rm
+ × Rp compute
ˆx(α, β) := argmin
x
L(x, α, β).
(.)
If θ is uniformly convex, i.e., there exists γ > such that
μθ(x) + (−μ)θ(y) ≥θ(μx + (−μ)y) + μ(−μ)γ∥x −y∥
∀x, y ∈Rd, μ ∈[,] ,
then ˆx(α, β) can be obtained as the unique solution of
∇xL(x, α, β) = .

Supervised Learning by Support Vector Machines 

This can be substituted into L which results in ψ(α, β) := L(ˆx(α, β), α, β) and α∗and β∗
are the solution of
ψ(α, β) →max
α,β
subject to
α ≥.
This problem, which is called the dual problem of (P) can often be more easily solved
than the original problem since one has only simple inequality constraints. However, this
approach is only possible if (> .) can easily be solved. Then, finally x∗= ˆx(α∗, β∗).
The objective functions in the primal problems in
> Sect. .are not strictly convex
(and consequently also not uniformly convex) since there does not appear the intercept b
in these functions. So let us formulate the dual problem with ψ(x, α, β) := L(x, α, β) as
follows:
Dual problem (D)
ψ(x, α, β) →max
x,α,β
subject to
∇xL(x, α, β) = , α ≥.
The solutions of the primal and dual problem, i.e., their minimum and maximum, respec-
tively, coincide according to the following theorem of Wolfe.
Theorem 
Let θ, g and h fulfill (C)–(C) and the Slater condition. Let x∗be a minimizer
of (P). Then there exist α∗, β∗such that x∗, α∗, β∗solves the dual problem and
θ(x∗) = ψ(x∗, α∗, β∗).
Duality theory can be handled in a more sophisticated way using tools from Pertur-
bation Theory in Convex Analysis, see, e.g., []. Let us briefly mention the general idea.
Let v : Rm →(−∞,∞] be an extended function, where only extended functions /≡∞are
considered in the following. The Fenchel conjugate of v is defined by
v∗(α) := sup
p∈Rm{⟨α, x⟩−v(x)}
and the biconjugate of v by v∗∗:= (v∗)∗. In general, the inequality v∗∗(x) ≤v(x) holds
true and becomes an equality if and only if v is convex and lower semicontinuous. (Later
the inequality is indicated by the fact that one minimizes the primal and maximizes the
dual problem.) For convex, lower semicontinuous functions θ : Rd →(−∞,∞], γ : Rm →
(−∞,∞] and g : Rd →Rm one considers the primal problems
(Pu)
v(u) = inf
x∈Rd{θ(x) + γ(g(x) + u)},
(P)
v() = inf
x∈Rd{θ(x) + γ(g(x))}


Supervised Learning by Support Vector Machines
where u ∈Rm is the “perturbation.” With L(x, α) := θ(x) + ⟨g(x), α⟩the dual problem
reads
(Du)
v∗∗(u) = sup
α∈Rm{⟨α,u⟩−γ∗(α) + inf
x∈Rd L(x, α)},
(D)
v∗∗() = sup
α∈Rm{−γ∗(α) + inf
x∈Rd L(x, α)}.
For the special setting with the indicator function
γ(y) = ιRm
−(y) := { 
if y ≤,
∞
otherwise
the primal problem (P) is equivalent to
θ(x) →min
x
subject to
g(x) ≤
and since γ∗= ιRm
+ the dual problem (D) becomes
sup
α∈Rm inf
x∈Rd L(x, α)
subject to
α ≥.
Again, if θ and g are convex and differentiable and θ is uniformly convex, then the unique
solution ˆx(α) of ∇xL(x, α) = is the solution of the infimum problem and the dual
problem becomes supα∈Rm L(ˆx(α), α) subject to α ≥.
..
Results from Generalization Theory
There exists a huge amount of results on the generalization abilities of statistical learning
methods and in particular of support vector machines. The following subsection can only
give a rough impression on the general tasks considered in this field from a simplified math-
ematical point of view that ignores technicalities, e.g., the definition of the correct measure
and function spaces and what measurable in the related context means. Most of the mate-
rial is borrowed from [], where the reader can find a sound mathematical treatment of
the topic.
To start with, remember that the aim in > Sect. .was to find a function f : X →R
from samples Z := {(xi, yi) : i = , . . . , m} such that f (x) is a good prediction of y at x
for (x, y) ∈X × Y. Let P denote an unknown probability distribution on X × Y. Then a
general assumption is that the data used in training and testing are identically independent
distributed (iid) according to P. The loss function or cost function L : X × Y × R →[,∞)
describes the cost of the discrepancy between the prediction f (x) and the observation y
at x. The choice of the loss function depends on the specific learning goal. In the models
of this paper, the loss functions depend on x only via f (x) such that we can simply write
L(y, f (x)). In
> Sect. ., the hinge loss function and the least squares loss function

Supervised Learning by Support Vector Machines 

were used for classification tasks. Originally, one was interested in the /classification
loss L/: Y × R →{,} defined by
L/(y, t) := { 
if y = sgn(t),

otherwise.
To the loss function there is associated a risk, which is the expected loss of f :
RL,P(f ) := ∫X ×Y L(x, y, f (x))dP(x, y) = ∫X ∫Y L(x, y, f (x))dP(y∣x)dPX.
For example, the /loss function has the risk
RL/,P(f ) = P ((x, y) ∈X × Y : sgnf (x) /= y).
A function f is considered to be “better” the smaller the risk is. Therefore, one is interested
in the minimal risk or Bayes risk defined by
R∗
L,P :=
inf
f :X →R RL,P(f ),
(.)
where the infimum is taken over all possible (measurable) functions. However, since the
distribution P is unknown, it is impossible to find a minimizer of RL,P. In learning tasks
one can exploit finite training sets Z of iid data. A learning method on X × Y maps every
data set Z ∈(X × Y)m to a function fZ : X →R. A learning method should produce for
sufficiently large training sets Z nearly optimal decision functions fZ with high probability.
A measurable learning method is called L-risk consistent for P if
lim
m→∞Pm (Z ∈(X × Y)m : RL,P(fZ) ≤R∗
L,P + ε) = 
∀ε > 
and universally L-risk consistent, if it is L-risk consistent for all distributions P on X ×Y. The
first learning method that was proved to be universally consistent was the nearest neighbor
method, see []. Many uniformly consistent classification and regression methods are pre-
sented in [, ]. Consistency does not address the speed of convergence, i.e., convergence
rates. Unfortunately, the no-free-lunch theorem of [], says that for every learning method
there exists a distribution P for which the learning methods cannot produce a “good” deci-
sion function in the above sense with an a priori fixed speed of convergence. To obtain
uniform convergence rates one has to pose additional requirements on P.
Instead of the risk one can deal with the empirical risk defined by
RL,Z(f ) := 
m
m
∑
i=
L (xi, yi, f (xi)).
Then the law of large numbers shows that RL,Z(f ) becomes a “good” approximation of
RL,P(f ) for a fixed f if m is “large enough.” However finding the minimizer of of
inf
f :X →R RL,Z(f )
(.)


Supervised Learning by Support Vector Machines
does in general not lead to a good approximation of R∗
L,P. For example, the function that
classifies all xi ∈X correctly and is zero elsewhere is a minimizer of the above func-
tional (> .) but gives in general a poor approximation of the optimal decision function
according to (> .). This is an example of overfitting, where the learning method approx-
imates the training data too closely and has poor generalization/prediction properties. One
common way to cope with this phenomenon is to choose a smaller set F of functions, e.g.,
subsets of continuous functions, which should have good approximation properties. In the
SVMs treated in > Sect. ., this set F was a RKHS HK. Then one considers the empirical
risk minimization (ERM)
inf
f ∈F RL,Z(f ).
(.)
Let a minimizer fZ of (> .) be somehow “computed.” (In this subsection, we do
not address the question of the existence and uniqueness of a minimizer of the various
functionals.) Then one is of course interested in the error RL,P(fZ) −R∗
L,P. Using the
infinite-sample counterpart of the ERM
R∗
L,P,F := inf
f ∈F RL,P(f )
this error can be splitted as
RL,P(fZ) −R∗
L,P = RL,P(fZ) −R∗
L,P,F

sample error
+
R∗
L,P,F −R∗
L,P

approximation error
.
The first error, called sample error is a probabilistic one since it depends on random sam-
ples, while the second error, called approximation error is a deterministic one. Finding a
good balance between both errors is sometimes called bias-variance problem, where the
bias is related to the approximation error and the variance to the sampling error.
Concerning the approximation error, it turns out that for RKHS F = HK on com-
pact metric spaces X which are dense in C(X) and continuous, P-integrable, “Nemitski
losses” this error becomes zero, see [, Corollary .]. In particular, this is true for RKHS
with the Gaussian kernel and the loss functions considered in
> Sect. .. For relations
between the approximation error, interpolation spaces and K-functionals see [] and the
references therein.
Concerning the sample error, there is a huge amount of results and this chapter can only
cover some basic directions. For a survey on recent developments in the statistical analysis
of classification methods, see []. Based on Hoeffding’s inequality the first of such relations
goes back to []. See also [, , , ]. To get an impression how such estimates look
like, two of them from [, Propositions .and .] are presented in the following. If F
is finite and L(x, y, f (x)) ≤B, then it holds for all m ≥that
Pm ⎛
⎝Z ∈(X × Y)m : RL,P(fZ) −R∗
L,P,F ≥B
√
τ + ln(∣F∣)
m
⎞
⎠≤e−τ
∀τ > .

Supervised Learning by Support Vector Machines 

If the function class F is infinite, in particular not countable, one needs some bounds
on the “complexity” of F. The most classical of such a “complexity” measure is the VC
dimension, see [], applied in connection with the /loss function. Another possibil-
ity is the use of covering numbers or its counterpart entropy numbers going back to [].
The ε-covering number of a metric set T with metric d is the size of the smallest ε-net
of T, i.e.,
N(T, d, ε) := inf {n ≥: ∃s, . . . , sn ∈T such that T ⊂
n
⋃
i=
Bd(si, ε)},
where Bd(s, ε) is the closed ball with center s and radius ε. Estimating covering numbers
for function spaces is standard in the field of function spaces and in approximation theory.
Then, for compact F ⊂L∞(X) one has basically to replace ∣F∣in the above relation by its
covering number:
Pm ⎛
⎝Z ∈(X × Y)m : RL,P(fZ) −R∗
L,P,F ≥B
√
τ + ln(N(F,∥⋅∥∞, ε))
m
+ ε∣L∣M,
⎞
⎠≤e−τ
for all τ > and for all ε > , where one assumes in addition that ∥f ∥∞≤M, f ∈F and
that L is locally Lipschitz continuous with constant ∣L∣M,here.
Next let us turn to the SVM setting, where an additional term comes along with the
loss function, namely one is interested in minimizers of
inf
f ∈HK {RL,Z(f ) + λ∥f ∥
HK},
λ > 
with a regularization term λ∥f ∥
HK that penalizes functions f with large RKHS
norms. The techniques developed for ERM analysis can be extended to the SVM
setting.
First let us mention that under some assumptions on the loss function, which are
fulfilled for the setting in
> Sect. ., a unique minimizer fZ,λ exists and has the
form
fZ,λ =
m
∑
i=
αiK(xi,⋅).
This was established in the representer theorem by [] for special continuous loss functions
and generalized, e.g., in []. There also exist a representer-like theorems for the minimizer
fP,λ of the infinite-sample setting
inf
f ∈HK {RL,P(f ) + λ∥f ∥
HK},
see, []. One can show for the infinite-sample setting that the error
A(λ) := inf
f ∈HK {RL,P(f ) + λ∥f ∥
HK} −R∗
L,P,HK


Supervised Learning by Support Vector Machines
tends to zero as λ goes to zero and that limλ→RL,P(fP,λ) = R∗
L,P,HK. Let us come to the
essential question how close RP,λ(fZ,λ) is to R∗
L,P. Recall that R∗
L,P = R∗
L,P,HK for the above
mentioned RKHS. An ERM analysis like estimation has, e.g., the form
Pm ⎛
⎝Z ∈(X × Y)m : RL,P(fZ,λ) + λ∥fZ,λ∥
HK −R∗
L,P,HK
≥A(λ) + (λ−/∣L∣λ−/,+ )
√
τ + ln(N(BHK,∥⋅∥∞, λ/ε))
m
+ ε∣L∣λ−/,
⎞
⎠≤e−τ,
for τ > , where one assumes that the continuous kernel fulfills ∥K∥∞≤, L(x, y,) ≤
and BH is the closed unit ball in H, see [, , , Theorem .]. For a certain decay of
the covering number ln(N(BHK,∥⋅∥∞, ε)) in ε and a RKHS for which the approxima-
tion error becomes zero, one can then conclude that for zero sequences (λm)m≥with an
additional suitable decay property related to the decay of the covering number, the relation
RL,P(fZ,λm) →R∗
L,P holds true in probability.
The above relations can be further specified for classification and regression tasks with
special loss functions. With respect to classification one can found, e.g., upper bounds for
the risk in terms of the margin or the number of support vectors. For the /loss function
the reader may consult, e.g., []. For the hinge loss function and the soft margin SVM
with C = /(λm) it holds, e.g., that
∣IS∣
m ≥λ∥fZ,λ∥
HK + RL,Z(fZ,λ),
see [, Proposition .]. For a suitable zero sequence (λm)m≥and a RKHS with zero
approximation error the following relation is satisfied:
lim
m→∞Pm (Z ∈(X × Y)m : ∣{i : α∗
i (Z) > }∣
m
≥R∗
L,P −ε) = ,
ε > .
Finally, let us address the setting, where the risk function defining the learning task is
hard to handle numerically. One example is the risk function associated with the /loss
function. This function is neither continuous nor convex. One remedy is to replace such
unpleasant loss functions L by a convex surrogate Lsur where one has to ensure that the
minimizer fZ in (> .)) for the surrogate loss fulfills RL,P(fZ) ≈R∗
L,P. For the hinge
function as surrogate of the /loss function, [], has proved that
RL/,P(f ) −R∗
L/,P ≤RLh,P(f ) −R∗
Lh,P
for all measurable functions f . Thus, if RLh,P(fZ)−R∗
Lh,P is small, this follows for the orig-
inal risk function, too. For a systematical treatment of surrogate loss functions the reader
may consult [, Chap. ].

Supervised Learning by Support Vector Machines 

.
Numerical Methods
This section concentrates on the support vector machines in
> Sect. .. Numerical
methods for the other models were always sketched when they were introduced. Sup-
port vector machines require finally the minimization of a quadratic functional subject
to linear constraints (QP). These minimization problems involve a symmetric, fully popu-
lated kernel matrix having the size m of the training set. Hence, this matrix has in general
m(m+)/distinct nonzero coefficients one has to work with. Therefore, one has to distin-
guish between small to moderate size problems, where such a matrix can be stored into the
RAM of the computer and large size problems, say with more than a million training data.
For quadratic programming with small to moderate data sizes, there exist various mean-
while standard algorithms. They are implemented in commercial software packages like
CPLEX or MOSEK, see also the MATLAB optimization toolbox or in freeware packages
like MINOS and LOQO. Among them, the primal-dual interior point algorithms belong to
the most reliable and accurate techniques. The main idea of interior point methods is to
solve the primal and dual problems simultaneously by enforcing the Kuhn–Tucker con-
ditions to iteratively find a feasible solution. The duality gap, i.e., the difference between
the minimum of the primal problem and the maximum of the dual problem, is used to
determine the quality of the current set of variables and to check whether the stopping cri-
teria are fulfilled. For QP algorithms including recent algorithms for solving large QPs the
reader may consult [].
The problem of learning large data sets was mainly addressed based on “working set”
methods. The idea is the following: if one knew in advance which constraints were active,
it would be possible to cancel all of the inactive constraints that simplifies the problem.
The simplest method in this direction is known as chunking. It starts with an arbitrary
subset (“chunk” = working set) of the data and trains the SVM using an optimizer on this
subset. The algorithm then keeps the support vectors and deletes the others. Next, the M
points (M algorithm parameter) from the remaining part of the data, where the “current
SVM” makes the largest errors are added to these support vectors to form a new chunk.
This procedure is iterated. In general, the working set grows until in the last iteration the
machine is trained on the set of support vectors representing the active constraints. Chunk-
ing techniques in SVMs were already used by [] and were improved and generalized in
various papers.
Currently, more advanced “working set” methods, namely decomposition algorithms are
one of the major tools to train SVMs. These methods select in each iteration a small fixed
size subset of variables as working set and a QP problem is solved with respect to this set,
see, e.g., []. A special type of decomposition methods is the sequential minimal optimiza-
tion (SMO), which uses only working sets of two variables. This method was introduced by
[] for classification, see [] for regression. The main advantage of these extreme small
working sets is that the partial QP problems can be solved analytically. For the soft margin
SVM in the dual form from > Sect. .(with a variable exchange α ↦Yα)

αTKα −⟨y, α⟩
subject to
⟨m, α⟩= , ≤yα ≤C.


Supervised Learning by Support Vector Machines
the SMO algorithm looks as follows:
SMO-type decomposition methods
.
Fix α( ) as initial feasible solution and set k := .
.
If α(k) solves the dual problem up to a desired precision, stop.
Otherwise, select a working set B := {i, j} ⊂{, . . ., m}. Define N := {, . . . , m}/B
and α(k)
B
and α(k)
N
as sub-vectors of α(k) corresponding to B and N, respectively
.
Solve the following subproblem with fixed α(k)
N
for αB:

(αT
B (α(k)
N )
T
)(KBB
KBN
KNB
KNN)( αB
α(k)
N
) −(yT
B yT
N) ( αB
α(k)
N
)
= 
(αi αj)(Kii
Ki j
Ki j
K j j)(αi
αj) −(αi αj)(KBNα(k)
N
−yB) + constant →minαB
subject to αi + αj = −T
m−α(k)
N ,
≤yiαi, y jαj ≤C.
Set α(k+)
B
to be the minimizer.
.
Set α(k+)
N
:= α(k)
N , k ↦k + and goto Step .
The analytical solution in Step is given as follows: For simplicity, set β := −T
m−α(k)
N
and
(ci c j)T := KBNα(k)
N −yB. Substituting αj = β−αi from the first constraint into the objective
function, one gets

α
i (Kii −Ki j + K j j) + αi(βKi j −βK j j −ci + c j) + constant →min
αi .
If K is positive definite, it holds that Kii −Ki j + K j j > and the above function has a
unique finite global minimizer αi,g. One has to take care about the second constraint. This
constraint requires that αi ∈[L,U], where L and U are defined by
(L,U) :=
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
(max(, β −C),min(C, β))
if
yi = , y j = ,
(max(, β),min(C, β + C))
if
yi = , y j = −,
(max(−C, β −C),min(, β))
if
yi = −, y j = ,
(max(−C, β),min(, β + C))
if
yi = −, y j = −.
Hence the minimizer in Step is given by (α∗
i , β −α∗
i ), where
αi :=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
αig
if
αig ∈[L,U],
L
if
αig < L,
U
if
αig > U.
It remains to determine the selection of the working set. (The determination of the stop-
ping criteria is beyond the scope of this chapter). Indeed, current decomposition methods
vary mainly according to different working set selections. The SVMlight algorithm of [],
was originally based on a rule for the selection the working set of []. Moreover, this
algorithm uses a shrinking technique to speed up the computational time. Shrinking is
based on the idea that if a variable α(k)
i
remains equal to zero or C for many iteration
steps, then it will probably not change anymore. The variable can be removed from the

Supervised Learning by Support Vector Machines 

optimization problem such that a more efficient overall optimization is obtained. Another
shrinking implementation is used in the software package LIBSVM of []. A modifica-
tion of Joachims’ algorithm for regression, called “SVMTorch” was given by []. An often
addressed working set selection due to [] is the maximal violating pair strategy. A more
general way of choosing the two-element working set, namely by choosing a constant factor
violating pair was given, including a convergence proof, by []. For convergence results see
also the paper of []. The maximal violating pair strategy relies on first-order (i.e., gradi-
ent) information of the objective function. Now for QP, second-order information directly
relates to the decrease of the objective function. The paper of [] proposes a promising
working set selection based on second-order information.
For an overview of SVM solvers for large data sets the reader may also consult [,
]. An extensive list of SVM software including logistic loss functions and least squares
loss functions can be found on the webpages www.kernel-machines.org and www.support-
vector-machines.org.
.
Conclusions
The invention of SVMs in the s led to an explosion of applications and theoretical
results. This paper can only give a very basic introduction into the meanwhile classical
techniques in this field. It is restricted to supervised learning although SVMs have also a
large impact on semi- and unsupervised learning.
Some new developments are sketched as multitask learningwhere, in contrast to single-
task learning, only limited work was involved until now and novel techniques taken from
convex analysis come into the play.
An issue that is not addressed in this chapter is the robustness of SVMs. There is
some ongoing research on connections between stability, learning and prediction of ERM
methods, see, e.g., the papers of [, ].
Another field that has recently attained attention is the use of kernels as diffusion kernels
on graphs, see [, ].
References and Further Reading
. Aizerman
M,
Braverman
E,
Rozonoer
L
() Uncovering shared structures in mul-
ticlassification.
Int
Conf
Mach
Learn
:
–
. Amit Y, Fink M, Srebro N, Ullman S ()
Theoretocal foundations of the potential func-
tion method in pattern recognition learning.
Automat Rem Contr :–
. Anthony M, Bartlett PL () Neural network
learning: theoretical foundations. Cambridge
University Press, Cambridge
. Argyriou A, Evgeniou T, Pontil M () Con-
vex multi-task feature learning. Mach Learn
():–
. Aronszajn N () Theory of reproducing
kernels. Trans Am Math Soc :–


Supervised Learning by Support Vector Machines
. Bartlett PL, Jordan MI, McAuliffe JD ()
Convexity, classification, and risk bounds. J Am
Stat Assoc :–
. Bennett KP, Mangasarian OL () Robust lin-
ear programming discrimination of two linearly
inseparable sets. Optim Methods Softw :–
. Berlinet A, Thomas-Agnan C () Reproduc-
ing kernel Hilbert spaces in probability and
statistics. Kluwer, Dordrecht
. Bishop CM () Pattern recognition and
machine learning. Springer, Heidelberg
. Björck A () Least squares problems. SIAM,
Philadelphia
. Bonnans JF, Shapiro A () Perturbation
analysis of optimization problems. Springer,
New York
. Boser GE, Guyon I, Vapnik V () A train-
ing algorithm for optimal margin classifiers. In:
Proceedings of the fifth annual ACM workshop
on computational learning theory, Madison,
pp –
. Bottou L, Chapelle L, DeCoste O, Weston J (eds)
() Large scale kernel machines. MIT Press,
Cambridge
. Boucheron S, Bousquet O, Lugosi G ()
Theory of classification: a survey on some recent
advances. ESAIM Probab Stat :–
. Bousquet O, Elisseeff A () Algorithmic sta-
bility and generalization performance. In: Leen
TK, Dietterich TG, Tresp V (eds) Advances in
neural information processing systems . MIT
Press, Cambridge, pp –
. Bradley PS, Mangasarian OL () Feature
selection via concave minimization and sup-
port vector machines. In: Proceedings of the
th
international
conference
on
machine
learning, Morgan Kaufmann, San Francisco,
pp –
. Brown M, Grundy W, Lin D, Cristianini
N, Sugnet C, Furey T, Ares M, Haussler D
() Knowledge-based analysis of microar-
ray gene-expression data by using support
vector machines. Proc Natl Acad Sci ():
–
. Buhmann MD () Radial basis functions.
Cambridge University Press, Cambridge
. Burges CJC () A tutorial on support vector
machines for pattern recognition. Data Min
Knowl Discov ():–
. Cai J-F, Candès EJ, Shen Z () A singular
value thresholding algorithm for matrix com-
pletion. Technical report, UCLA computational
and applied mathematics
. Caruana R () Multitask learning. Mach
Learn ():–
. Chang
C-C,
Lin
C-J
()
LIBSVM:
a
library
for
support
vector
machines.
www.csie.ntu.edu.tw/cjlin/papers/libsvm.ps.gz
. Chapelle O, Haffner P, Vapnik VN () SVMs
for histogram-based image classification. IEEE
Trans Neural Netw ():–
. Chen P-H, Fan R-E, Lin C-J () A study
on SMO-type decomposition methods for sup-
port vector machines. IEEE Trans Neural Netw
:–
. Collobert R, Bengio S () Support vector
machines for large scale regression problems.
J Mach Learn Res :–
. Cortes C, Vapnik V () Support vector net-
works. Mach Learn :–
. Cristianini N, Shawe-Taylor J () An intro-
duction to support vector machines. Cambridge
University Press, Cambridge
. Cucker F, Smale S () On the mathemati-
cal foundations of learning. Bull Am Math Soc
:–
. Cucker F, Zhou DX () Learning theory: an
approximation
point
of
view.
Cambridge
University Press, Cambridge
. Devroye L, Gyrfi L, Lugosi G () A proba-
bilistic theory of pattern recognition. Springer,
New York
. Devroye LP () Any discrimination rule can
have an arbitrarily bad probability of error for
finite sample size. IEEE Trans Pattern Anal
Mach Intell :–
. Dietterich TG, Bakiri G () Solving mul-
ticlass learning problems via error-correcting
output codes. J Artfic Int Res :–
. Dinuzzo F, Neve M, Nicolao GD, Gianazza UP
() On the representer theorem and equiv-
alent degrees of freedom of SVR. J Mach Learn
Res :–
. Duda RO, Hart PE, Stork D () Pattern clas-
sification, nd edn. Wiley, New York
. Edmunds DE, Triebel H () Function spaces,
entropy numbers, differential operators. Cam-
bridge University Press, Cambridge

Supervised Learning by Support Vector Machines 

. Elisseeff A, Evgeniou A, Pontil M () Stabil-
ity of randomised learning algorithms. J Mach
Learn Res :–
. Evgeniou T, Pontil M, Poggio T () Regular-
ization networks and support vector machines.
Adv Comput Math ():–
. Fan R-E, Chen P-H, Lin C-J () Working
set selection using second order information for
training support vector machines. J Mach Learn
Res :–
. Fasshauer GE () Meshfree approxima-
tion methods with MATLAB. World Scientific,
New Jersey
. Fazel M, Hindi H, Boyd SP () A rank mini-
mization heuristic with application to minimum
order system approximation. In: Proceedings
of the American control conference, Arlington,
pp –
. Fisher RA () The use of multiple measure-
ments in taxonomic problems. Ann Eugenics
:–
. Flake GW, Lawrence S () Efficient SVM
regression training with SMO. Technical report,
NEC Research Institute
. Gauss CF () Theory of the motion of the
heavenly bodies moving about the sun in conic
sections. (trans: Davis CH). Dover, New York;
first published 
. Girosi F () An equivalence between sparse
approximation and support vector machines.
Neural Comput ():–
. Golub GH, Loan CFV () Matrix computa-
tion, rd edn. John Hopkins University Press,
Baltimore
. Gyrfi L, Kohler M, Krzy˙zak A, Walk H ()
A distribution-free theory of non-parametric
regression. Springer, New York
. Hastie T, Tibshirani R, Friedman J ()
The elements of statistical learning. Springer,
New York
. Herbrich R () Learning Kernel classifiers:
theory and algorithms. MIT Press, Cambridge
. Hoerl AE, Kennard RW () Ridge regression:
biased estimation for nonorthogonal problems.
Technometrics ():–
. Huang T, Kecman V, Kopriva I, Friedman J
() Kernel based algorithms for mining huge
data sets: supervised semi-supervised and unsu-
pervised learning. Springer, Berlin
. Jaakkola TS, Haussler D () Probabilistic
kerbnel regression models. In: Proceedings of
the conference on artificial inteligence and
statistics
. Joachims T () Making large-scale SVM
learning practical. In: Schlkopf B, Burges C,
Smola A (eds) Advances in Kernel methods-
support vector learning. MIT Press, Cambridge,
pp –
. JoachimsT () Learningto classifytextusing
support vector machines. Kluwer, Boston
. Kailath T () RKHS approach to detection
and estimation problems: Part I: deterministic
signals in Gaussian noise. IEEE Trans Inform
Theory ():–
. Keerthi
SS, Shevade SK, Battacharyya
C,
Murthy KRK () Improvements to Platt’s
SMO algorithm for SMV classifier design.
Neural Comput :–
. Kimeldorf GS, Wahba G () Some results on
Tchebycheffian spline functions. J Math Anal
Appl :–
. Kolmogorov AN, Tikhomirov VM ()
ε-entropy and ε-capacity of sets in functional
spaces. Am Math Soc Trans :–
. Kondor RI, Lafferty J () Diffusion kernels
on graphs and other discrete structures. In:
Kauffman M (ed) Proceedings of the interna-
tional conference on machine learning, Morgan
Kaufman, San Mateo
. Krige DG () A statistical approach to some
basic mine valuation problems on the witwa-
tersrand. J Chem Met Mining Soc S Africa
():–
. Kuhn HW, Tucker AW () Nonlinear pro-
gramming. In: Proceedings of the Berkley sym-
posium on mathematical statistics and proba-
bility, University of California Press, Berkeley,
pp –
. Laplace PS () Théorie Analytique des Prob-
abilités, rd edn. Courier, Paris
. LeCun Y, Jackel LD, Bottou L, Brunot A,
Cortes C, Denker JS, Drucker H, Guyon I,
Müller U, Säckinger E, Simard P, Vapnik
V
()
Comparison
of
learning
algo-
rithms for handwritten digit recognition. In:
Fogelman-Souleé F, Gallinari P (eds) Proceed-
ings of ICANN’, vol . EC& Cie, Paris,
pp –


Supervised Learning by Support Vector Machines
. Legendre AM () Nouvelles Méthodes pour
la Determination des Orbites des Cométes.
Courier, Paris
. Leopold E, Kinderman J () Text cate-
gogization with support vector machines how
to represent text in input space? Mach Learn
(–):–
. Lin CJ () On the convergence of the decom-
position method for support vector machines.
IEEE Trans Neural Netw :–
. Lu Z, Monteiro RDC, Yuan M () Convex
optimization methods for dimension reduction
and coefficient estimation in multivariate linear
regression. Submitted to Math Program
. Ma S, Goldfarb D, Chen L () Fixed point
and Bregman iterative methods for matrix rank
minimization. Technical report -, UCLA
Computational and applied mathematics
. Mangasarian OL () Nonlinear program-
ming. SIAM, Madison
. Mangasarian OL, Musicant DR () Succes-
sive overrelaxation for support vector machines.
IEEE Trans Neural Netw :–
. Matheron G () Principles of geostatistics.
Econ Geol :–
. Micchelli CA () Interpolation of scattered
data:distancematicesandconditionallypositive
definite functions. Constr Approx :–
. Micchelli CA, Pontil M () On learning
vector-valued functions. Neural Comput :
–
. Mitchell TM() Machinelearning.McGraw-
Hill, Boston
. MukherjeeS,Niyogi P,Poggio T,RifkinR()
Learning theory: stability is sufficient for gener-
alization and necessary and sufficient for con-
sistency of empirical risk minimization. Adv
Comput Math :–
. Neumann J, Schnörr C, Steidl G () Effi-
cient wavelet adaptation for hybrid wavelet–
large margin classifiers. Pattern Recogn :
–
. Obozinski G, Taskar B, Jordan MI () Joint
covariate selection and joint subspace selection
for multiple classification problems. Stat Com-
put (in press)
. Osuna E, Freund R, Girosi F () Training of
support vector machines: an application to face
detection. In: Proceedings of the CVPR’, IEEE
Computer Society, Washington, pp –
. Parzen E () Statistical inference on time
series by RKHS methods. Technical report,
Department of Statistics, Stanford University
. Pinkus A () N-width in approximation
theory. Springer, Berlin
. Platt JC () Fast training of support vec-
tor machines using sequential minimal opti-
mization. In: Schölkopf B, Burges CJC, Smola
AJ (eds) Advances in Kernel methods – sup-
port vector learning. MIT Press, Cambridge, pp
–
. Poggio T, Girosi F () Networks for approxi-
mation and learning. Proc IEEE ():–
. Pong TK, Tseng P, Ji S, Ye J () Trace norm
regularization: reformulations, algorithms and
multi-task learning. University of Washington,
preprint
. Povzner AY () A class of Hilbert func-
tion spaces. Doklady Akademii Nauk USSR :
–
. Rosenblatt F () The perceptron: a prob-
abilistic model for information storage and
organization in the brain. Psychol Rev :
–
. Schoenberg IJ () Metric spaces and com-
pletely monotone functions. Ann Math :
–
. Schölkopf B, Herbrich R, Smola AJ ()
A
generalized
representer
theorem.
In:
Helmbold D, Williamson B (eds) Proceedings
of the th annual conference on computa-
tional learning theory. Springer, New York, pp
–
. Schölkopf B, Smola AJ () Learning with
Kernels: support vector machnes, regulariza-
tion, optimization, and beyond. MIT Press,
Cambridge
. Shawe-Taylor J, Cristianini N () Kernel
methods for pattern analysis, th edn. Cam-
bridge University Press, New York
. Smola AJ, Schölkopf B, Müller KR () The
connection between regularization operators
and support vector kernels. Neural Netw :
–
. Spellucci P () Numerische verfahren der
nichtlinearen optimierung. Birkhäuser, Basel/-
Boston/Berlin
. Srebro N, Rennie JDM, Jaakkola TS ()
Maximum-margin
matrix
factorization.
In
NIPS, MIT Press, Cambridge, pp –

Supervised Learning by Support Vector Machines 

. Steinwart I () Sparseness of support vector
machines. J Mach Learn Res :–
. Steinwart I, Christmann A () Support
vector machines. Springer, New York
. Stone
C
()
Consistent
nonparametric
regression. Ann Stat :–
. Strauss DJ, Steidl G () Hybrid wavelet-
support vector classification of waveforms.
J Comput Appl Math :–
. Strauss DJ, Steidl G, Delb D () Feature
extraction by shape-adapted local discriminant
bases. Signal Process :–
. Sutton
RS,
Barton
AG
()
Reinforce-
ment learning: an introduction. MIT Press,
Cambridge
. Suykens
JAK,
Gestel
TV,
Brabanter
JD,
Moor BD, Vandewalle J () Least squares
support vector machines. World Scientific,
Singapore
. Suykens JAK, Vandevalle J () Least squares
support vector
machine classifiers. Neural
Process Lett ():–
. Tao PD, An LTH () A d.c. optimization
algorithm for solving the trust-region subprob-
lem. SIAM J Optimiz ():–
. Tibshirani R () Regression shrinkage and
selection via the lasso. J R Stat Soc B ():
–
. Tikhonov AN, Arsenin VY () Solution of
ill-posed problems. Winston, Washington
. Toh K-C, Yun S () An accelerated proximal
gradient algorithm for nuclear norm regular-
ized least squares problems. Technical report,
Department of Mathematics, National Univer-
sity of Singapore, Singapore
. Tsypkin Y () Adaptation and learning in
automatic systems. Academic, New York
. Vapnik V () Statistical learning theory.
Wiley, New York
. Vapnik VN () Estimation of dependicies
based on empirical data. Springer, New York
. Vapnik VN, Chervonenkis A () Theory
of pattern regognition (in Russian). Nauka,
Moscow; German translation:
Theorie der
Zeichenerkennung, Akademie-Verlag, Berlin,
edition
. Vapnik VN, Lerner A () Pattern recognition
using generalized portrait method. Automat
Rem Contr :–
. Vidyasagar M () A theory of learning and
generalization: with applications to neural net-
works and control systems. nd edn. Springer,
London
. Viola P, Jones M () Robust real-time face
detection. Int J Comput Vision ():–
. Vito ED, Rosasco L, Caponnetto A, Piana M,
Verri A () Some properties of regularized
kernel methods. J Mach Learn Res :–
. Wahba G () Spline models for observational
data. SIAM, New York
. Weimer M, Karatzoglou A, Smola A ()
Improving maximum margin matrix factoriza-
tion. Mach Learn ():–
. Wendland H () Scattered data approxima-
tion. Cambridge University Press, Cambridge
. Weston J, Elisseeff A, Schölkopf B, Tipping M
() Use of the zero-norm with linear mod-
els and kernel methods. J Mach Learn Res :
–
. Weston J, Watkins C () Multi-class sup-
port vector machines. In: Verlysen M (ed) Pro-
ceedings of ESANN’, D-Facto Publications,
Brussels
. Wolfe P () Duality theorem for nonlinear
programming. Q Appl Math :–
. Zdenek D () Optimal quadratic program-
ming algorithms with applications to variational
inequalities. Springer, New York
. Zhang T () Statistical behaviour and con-
sistency of classification methods based on con-
vex risk minimization. Ann Stat :–
. Zoutendijk G () Methods of feasible direc-
tions. A study in linear and nonlinear program-
ming. Elsevier, Amsterdam


Total Variation in Imaging
V. Caselles ⋅A. Chambolle ⋅M. Novaga
.
Introduction.....................................................................
.
Notation and Preliminaries on BV Functions.................................
..
Definition and Basic Properties......................................................
..
Sets of Finite Perimeter: The Coarea Formula.....................................
..
The Structure of the Derivative of a BV Function.................................
.
Mathematical Analysis: The Regularity of Solutions of the TV Denoising
Problem.........................................................................
..
The Discontinuities of Solutions of the TV Denoising Problem................
..
Hölder Regularity Results.............................................................
.
Mathematical Analysis: Some Explicit Solutions.............................
.
Numerical Methods: Iterative Methods........................................
..
Notation..................................................................................
..
Chambolle’s Algorithm................................................................
..
Primal-Dual Approaches..............................................................
.
Numerical Methods: Maximum Flow Methods...............................
..
Discrete Perimeters and Discrete Total Variation. ................................
..
Graph Representation of Energies for Binary MRF...............................
.
Other Problems: Anisotropic Total Variation Models........................
..
Global Solutions of Geometric Problems..........................................
..
A Convex Formulation of Continuous Multi-label Problems...................
.
Other Problems: Image Restoration...........................................
..
Some Restoration Experiments......................................................
..
The Image Model.......................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Total Variation in Imaging
.
Final Remarks: A Different Total Variation-Based Approach
to Denoising.....................................................................
.
Conclusion......................................................................
.
Cross-References................................................................

Total Variation in Imaging 

Abstract: The use of total variation as a regularization term in imaging problems was
motivated by its ability to recover the image discontinuities. This is at the basis of its numer-
ous applications to denoising, optical flow, stereo imaging and D surface reconstruction,
segmentation, or interpolation to mention some of them. On one hand, we review here
the main theoretical arguments that have been given to support this idea. On the other,
we review the main numerical approaches to solve different models where total variation
appears. We describe both the main iterative schemes and the global optimization meth-
ods based on the use of max-flow algorithms. Then, we review the use of anisotropic total
variation models to solve different geometric problems and its use in finding a convex for-
mulation of some non-convex total variation problems. Finally, we study the total variation
formulation of image restoration.
.
Introduction
The Total Variation model in image processing was introduced in the context of image
restoration [] and image segmentation, related to the study of the Mumford-Shah seg-
mentation functional []. Being more related to our purposes here, let us consider the
case of image denoising and restoration.
We assume that the degradation of the image occurs during image acquisition and can
be modeled by a linear and translation invariant blur and additive noise:
f = h ∗u + n,
(.)
where u : R→R denotes the ideal undistorted image, h : R→R is a blurring kernel,
f is the observed image which is represented as a function f : R→R, and n is an addi-
tive white noise with zero mean and standard deviation σ. In practice, the noise can be
considered as Gaussian.
A particular and important case contained in the above formulation is the denoising
problem which corresponds to the case where h = δ, so that > Eq. (.) is written as
f = u + n,
(.)
where n is an additive Gaussian white noise of zero mean and variance σ .
The problem of recovering u from f is ill posed. Several methods have been proposed
to recover u. Most of them can be classified as regularization methods which may take
into account statistical properties (Wiener filters), information theoretic properties [],
a priori geometric models [], or the functional analytic behavior of the image given in
terms of its wavelet coefficients (see [] and references therein).
The typical strategy to solve this ill-conditioning is regularization []. In the linear
case, the solution of (> .) is estimated by minimizing a functional
Jγ(u) =∥Hu −f ∥
+γ ∥Qu ∥
,
(.)


Total Variation in Imaging
which yields the estimate
uγ = (HtH + γQtQ)−Ht f ,
(.)
where Hu = h ∗u, and Q is a regularization operator. Observe that to obtain uγ we
have to solve a system of linear equations. The role of Q is, on one hand, to move the
small eigenvalues of H away from zero while leaving the large eigenvalues unchanged,
and, on the other hand, to incorporate the a priori (smoothness) knowledge that we have
on u.
If we treat u and n as random vectors and we select γ = and Q = R−/
s
R/
n with Rs and
Rn being the image and noise covariance matrices, respectively, then (> .) corresponds
to the Wiener filter that minimizes the mean square error between the original and restored
images.
One of the first regularization methods consisted in choosing between all possible
solutions of (> .), the one which minimized the Sobolev (semi) norm of u
∫R∣Du∣dx,
(.)
which corresponds to the case Qu = Du. In the Fourier domain the solution of (> .)
given by (> .) is ˆu =
ˆh
∣ˆh∣+γπ∣ξ∣ˆf . From the above formula, we see that high frequencies
of f (hence, the noise) are attenuated by the smoothness constraint.
This formulation was an important step, but the results were not satisfactory, mainly
due to the inability of the previous functional to resolve discontinuities (edges) and oscilla-
tory textured patterns. The smoothness required by the finiteness of the Dirichlet integral
(> .) constraint is too restrictive. Indeed, functions in W,(R) (i.e., functions u ∈
L(R) such that Du ∈L(R)) cannot have discontinuities along rectifiable curves. These
observations motivated the introduction of Total Variation in image restoration problems
by L. Rudin, S. Osher, and E. Fatemi in their work []. The a priori hypothesis is that
functions of bounded variation (the BV model) [] are a reasonable functional model for
many problems in image processing, in particular, for restoration problems []. Typically,
functions of bounded variation have discontinuities along rectifiable curves, being contin-
uous in some sense (in the measure theoretic sense) away from discontinuities []. The
discontinuities could be identified with edges. The ability of total variation regularization
to recover edges is one of the main features which advocates for the use of this model but
its ability to describe textures is less clear, even if some textures can be recovered, up to a
certain scale of oscillation. An interesting experimental discussion of the adequacy of the
BV model to describe real images can be found in [].
In order to work with images, we assume that they are defined in a bounded domain
Ω ⊆Rwhich we assume to be the interval [, N]. As in most of the works, in
order to simplify this problem, we shall assume that the functions h and u are peri-
odic of period N in each direction. That amounts to neglecting some boundary effects.
Therefore, we shall assume that h,u are functions defined in Ω and, to fix ideas, we
assume that h,u ∈L(Ω). Our problem is to recover as much as possible of u, from
our knowledge of the blurring kernel h, the statistics of the noise n, and the observed
image f .

Total Variation in Imaging 

On the basis of the BV model, Rudin–Osher–Fatemi [] proposed to solve the
following constrained minimization problem:
Minimize
∫Ω ∣Du∣
subject to
∫Ω ∣h ∗u(x) −f (x)∣dx ≤σ ∣Ω∣.
(.)
Notice that the image acquisition model (> .) is only incorporated through a global
constraint. Assuming that h ∗= (energy preservation), the additional constraint that
∫Ω h ∗u dx = ∫Ω f (x) is automatically satisfied by its minima []. In practice, the above
problem is solved via the following unconstrained minimization problem:
Minimize ∫Ω ∣Du∣+ 
λ ∫Ω ∣h ∗u −f ∣dx,
(.)
where the parameter λ is positive. Recall that we may interpret λ as a penalization param-
eter which controls the trade-off between the goodness of fit of the constraint and the
smoothness term given by the Total Variation. In this formulation, a methodology is
required for a correct choice of λ. The connections between (> .) and (> .) were
studied by A. Chambolle and P.L. Lions in [], where they proved that both problems are
equivalent for some positive value of the Lagrange multiplier λ.
In the denoising case, the unconstrained variational formulation (> .) with h = δ is
Minimize ∫Ω ∣Du∣+ 
λ ∫Ω ∣u −f ∣dx,
(.)
and it has been the object of much theoretical and numerical research (see [, ] for a
survey). Even if this model represented a theoretical and practical progress in the denois-
ing problem due to the introduction of BV functions as image models, the experimental
analysis readily showed its main drawbacks. Between them, let us mention the staircasing
effect (when denoising a smooth ramp plus noise, the staircase is an admissible result), the
pixelization of the image at smooth regions, and the loss of fine textured regions to men-
tion some of them. This can be summarized with the simple observation that the residuals
f −u, where u represents the solution of (> .), do not look like noise. This has motivated
the development on nonlocal filters [] for denoising, the use of a stochastic optimization
techniques to estimate u [], or the consideration of the image acquisition model as a set
of local constraints [, ] to be discussed below.
Let us finally mention that, following the analysis of Y. Meyer in [], the solution u
of (> .) permits to obtain a decomposition of the data f as a sum of two components
u + v, where u contains the geometric sketch of f , while v is supposed to contain its noise
and textured parts. As Meyer observed, the Lnorm of the residual v := f −u in (> .) is
not the right one to obtain a decomposition of f in terms of geometry plus texture and he
proposed to measure the size of the textured part v in terms of a dual BV norm, showing
that some models of texture have indeed a small dual BV norm.
In spite of its limitations, the Total Variation model has become one of the basic image
models and has been adapted to many tasks: optical flow, stereo imaging and D surface
reconstruction, segmentation, interpolation, or the study of u +v models to mention a few
cases. On the other hand, when compared to other robust regularization terms, it combines


Total Variation in Imaging
simplicity and geometric character and makes it possible a rigorous analysis. The theoret-
ical analysis of the behavior of solutions of (> .) has been the object of several works
[, , , , , ] and will be summarized in > Sects. .and ..
Recall that one of the main reasons to introduce the Total Variation as a regulariza-
tion term in imaging problems was its ability to recover discontinuities in the solution.
This intuition has been confirmed by the experimental evidence and has been the motiva-
tion for the study of the local regularity properties of (> .) in [, ]. After recalling
in
> Sect. .some basic notions and results in the theory of bounded variation func-
tions, we prove in
> Sect. ..that the set of jumps (in the BV sense) of the solution
of (> .) is contained in the set of jumps of the datum f []. In other words, model
(> .) does not create any new discontinuity besides the existing ones. As a refinement
of the above statement, the local Hölder regularity of the solutions of (> .) is studied in
> Sect. ... This has to be combined with results describing which discontinuities are
preserved. No general statement in this sense exists but many examples are described in
the papers [, , , ]. The preservation of a jump discontinuity depends on the curvature
of the level line at the given point, the size of the jump and the regularization parameter
λ. This is illustrated in the example given in
> Sect. .. The examples support the idea
that total variation is not perfect but may be a reasonable regularization term in order to
restore discontinuities.
Being considered as a basic model, the numerical analysis of the total variation model
has been the object of intensive research. Many numerical approaches have been proposed
in order to give fast, efficient methods which are also versatile to cover the whole range of
applications. In
> Sect. .we review some basic iterative methods introduced to solve
the Euler–Lagrange equations of (> .). In particular, we review in
> Sect. ..the
dual approach introduced by A. Chambolle in []. In > Sect. ..we review the primal-
dual scheme of Zhu and Chan []. Both of them are between the most popular schemes by
now. In
> Sect. ..we discuss global optimization methods based on graph-cut tech-
niques adapted to solve a quantized version of (> .). Those methods have also become
very popular due to its efficiency and versatility in applications and are an active area of
research, as it can be seen in the references. Then, in > Sect. ..we review the applica-
tions of anisotropic TV problems to find the global solution of geometric problems. Similar
anisotropic TV formulations appear as convexifications of nonlinear energies for dispar-
ity computation in stereo imaging, or related problems [, ], and they are reviewed in
> Sect. ...
In
> Sect. .we review the application of Total Variation in image restoration
(> .), describing the approach where the image acquisition model is introduced as a
set of local constraints [, , ].
We could not close this chapter without reviewing in
> Sect. .a recent algorithm
introduced by C. Louchet and L. Moisan [], which uses a Bayesian approach leading to
an estimate of u as the expected value of the posterior distribution of u given the data f .
This estimate requires to compute an integral in a high-dimensional space and the authors
use a Monte-Carlo method with Markov Chain (MCMC) []. In this context, the mini-
mization of the discrete version of (> .) corresponds to a Maximum a Posterior (MAP)
estimate of u.

Total Variation in Imaging 

.
Notation and Preliminaries on BV Functions
..
Deﬁnition and Basic Properties
Let Ω be an open subset of RN. Let u ∈L
loc(Ω). Recall that the distributional gradient of
u is defined by
∫Ω σ ⋅Du = −∫Ω u(x)div σ(x) dx
∀σ ∈C∞
c (Ω,RN),
(.)
where C∞
c (Ω;RN) denotes the vector fields with values in RN which are infinitely dif-
ferentiable and have compact support in Ω. The total variation of u in Ω is defined by
V(u, Ω) := sup {∫Ω u div σ dx : σ ∈C∞
c (Ω;RN),∣σ(x)∣≤∀x ∈Ω},
(.)
where for a vector v = (v, . . .,vN) ∈RN we set ∣v∣:= ∑N
i=v
i . Following the usual
notation, we will denote V(u, Ω) by ∣Du∣(Ω) or by ∫Ω ∣Du∣.
Deﬁnition 
Let u ∈L(Ω). We say that u is a function of bounded variation in Ω if
V(u, Ω) < ∞. The vector space of functions of bounded variation in Ω will be denoted by
BV(Ω).
Using Riesz representation theorem [], the above definition can be rephrased by
saying that u is a function of bounded variation in Ω if the gradient Du in the sense of
distributions is a (vector-valued) Radon measure with finite total variation V(u, Ω).
Recall that BV(Ω) is a Banach space when endowed with the norm ∥u∥:= ∫Ω ∣u∣dx +
∣Du∣(Ω). Recall also that the map u →∣Du∣(Ω) is L
loc(Ω)-lower semicontinuous, as a sup
(> .) of continuous linear forms [].
..
Sets of Finite Perimeter: The Coarea Formula
Deﬁnition 
A measurable set E ⊆Ω is said to be of finite perimeter in Ω if χE ∈BV(Ω).
The perimeter of E in Ω is defined as P(E, Ω) := ∣DχE∣(Ω). If Ω = RN, we denote the
perimeter of E in RN by P(E).
The following inequality holds for any two sets A, B ⊆Ω:
P(A ∪B, Ω) + P(A ∩B, Ω) ≤P(A, Ω) + P(B, Ω).
(.)
Theorem 
Let u ∈BV(Ω). Then for a.e. t ∈R the set {u > t} is of finite perimeter in Ω
and one has the coarea formula:
∫Ω ∣Du∣= ∫
∞
−∞P({u > t}, Ω) dt.


Total Variation in Imaging
In other words, the total variation of u amounts to the sum of the perimeters of its upper level
sets.
An analogous formula with the lower-level sets is also true. For a proof we refer to [].
..
The Structure of the Derivative of a BV Function
Let us denote by LN and HN−, respectively, the N-dimensional Lebesgue measure and the
(N −)-dimensional Hausdorff measure in RN (see [] for precise definitions).
Let u ∈[L
loc(Ω)]
m (m ≥). We say that u has an approximate limit at x ∈Ω if there
exists ξ ∈Rm such that
lim
ρ↓

∣B(x, ρ)∣∫B(x,ρ) ∣u(y) −ξ∣dy = .
(.)
The set of points where this does not hold is called the approximate discontinuity set of
u and is denoted by Su. Using Lebesgue’s differentiation theorem, one can show that the
approximate limit ξ exists at LN-a.e. x ∈Ω and is equal to u(x): in particular, ∣Su∣= . If
x ∈Ω / Su, the vector ξ is uniquely determined by (> .) and we denote it by ˜u(x).
We say that u is approximately continuous at x if x /∈Su and ˜u(x) = u(x), that is, if x
is a Lebesgue point of u (with respect to the Lebesgue measure).
Let u ∈[L
loc(Ω)]
m and x ∈Ω / Su; we say that u is approximately differentiable at x if
there exists an m × N matrix L such that
lim
ρ↓

∣B(x, ρ)∣∫B(x,ρ)
∣u(y) −˜u(x) −L(y −x)∣
ρ
dy = .
(.)
In that case, the matrix L is uniquely determined by (> .) and is called the approximate
differential of u at x.
For u ∈BV(Ω), the gradient Du is an N-dimensional Radon measure that decom-
poses into its absolutely continuous and singular parts Du = Dau + Dsu. Then Dau =
∇u dx, where ∇u is the Radon–Nikodym derivative of the measure Du with respect to the
Lebesgue measure in RN. The function u is approximately differentiable LN-a.e. in Ω and
the approximate differential coincides with ∇u(x) LN-a.e. The singular part Dsu can be
also split in two parts: the jump part D ju and the Cantor part Dcu.
We say that x ∈Ω is an approximate jump point of u if there exist u+(x) /= u−(x) ∈R
and ∣u(x)∣= such that
lim
ρ↓

∣B+ρ(x,
u(x))∣∫B+
ρ (x,
u(x)) ∣u(y) −u+(x)∣dy = 
lim
ρ↓

∣B−ρ(x,
u(x))∣∫B−
ρ (x,
u(x)) ∣u(y) −u−(x)∣dy = ,
where B+
ρ(x,
u(x)) = {y ∈B(x, ρ) : ⟨y −x,
u(x)⟩> } and B−
ρ(x,
u(x)) = {y ∈
B(x, ρ) : ⟨y −x,
u(x)⟩< }. We denote by Ju the set of approximate jump points of u.

Total Variation in Imaging 

If u ∈BV(Ω), the set Su is countably HN−rectifiable, Ju is a Borel subset of Su, and
HN−(Su / Ju) = []. In particular, we have that HN−-a.e. x ∈Ω is either a point of
approximate continuity of ˜u or a jump point with two limits in the above sense. Finally, we
have
D ju = Dsu
Ju = (u+ −u−) uHN−
Ju
and Dcu = Dsu
(Ω / Su).
For a comprehensive treatment of functions of bounded variation we refer to [].
.
Mathematical Analysis: The Regularity of Solutions
of the TV Denoising Problem
..
The Discontinuities of Solutions of the TV Denoising
Problem
Given a function f ∈L(Ω) and λ > we consider the minimum problem
min
u∈BV(Ω) ∫Ω ∣Du∣+ 
λ ∫Ω(u −f )dx .
(.)
Notice that problem (> .) always admits a unique solution uλ, since the energy
functional is strictly convex.
As we mentioned in
> Sect. ., one of the main reasons to introduce the Total
Variation as a regularization term in imaging problems was its ability to recover the discon-
tinuities in the solution. This section together with > Sects. ..and > .is devoted to
analyze this assertion. In this section we prove that the set of jumps of uλ (in the BV sense)
is contained in the set of jumps of f , whenever f has bounded variation. Thus, model
(> .) does not create any new discontinuity besides the existing ones. > Section ..
is devoted to review a local Hölder regularity result of []: the local Ho¨lder regularity
of the data is inherited by the solution. This has to be combined with results describing
which discontinuities are preserved. In
> Sect. .we give an example of explicit solu-
tion of (> .) which shows that the preservation of a jump discontinuity depends on the
curvature of the level line at the given point, the size of the jump, and the regularization
parameter λ. Other examples are given in the papers [, , , , ]. The examples support
the idea that total variation may be a reasonable regularization term in order to restore
discontinuities.
Let us recall the following observation, which is proved in [, , ].
Proposition 
Let uλ be the (unique) solution of (> .). Then, for any t ∈R, {uλ > t}
(respectively, {uλ ≥t}) is the minimal (respectively, maximal) solution of the minimal surface
problem
min
E⊆Ω P(E, Ω) + 
λ ∫E(t −f (x)) dx
(.)


Total Variation in Imaging
(whose solution is defined in the class of finite-perimeter sets, hence, up to a Lebesgue-
negligible set). In particular, for all t ∈R but a countable set, {uλ = t} has zero measure
and the solution of (> .) is unique up to a negligible set.
A proof that {uλ > t} and {uλ ≥t} both solve (> .) is found in [, Proposi-
tion .]. A complete proof of this proposition, which we do not give here, follows from the
co-area formula, which shows that, up to a renormalization, for any u ∈BV(Ω) ∩L(Ω),
∫Ω ∣Du∣+ 
λ ∫Ω(u −f )dx = ∫R (P ({u > t}, Ω) + 
λ ∫{u>t}(t −f ) dx) dt,
and from the following comparison result for solutions of (> .) which is proved in
[, Lemma ].
Lemma 
Let f , g ∈L(Ω) and E and F be respectively minimizers of
min
E
P(E, Ω) −∫E f (x) dx and min
F
P(F, Ω) −∫F g(x) dx.
Then, if f < g a.e., ∣E / F∣= (in other words, E ⊆F up to a negligible set).
Proof.
Observe that we have
P(E, Ω) −∫E f (x) dx ≤P(E ∩F, Ω) −∫E∩F f (x) dx
P(F, Ω) −∫F g(x) dx ≤P(E ∪F, Ω) −∫E∪F g(x) dx.
Adding both inequalities and using that for two sets of finite perimeter we have (> .)
P(E ∩F, Ω) + P(E ∪F, Ω) ≤P(E, Ω) + P(F, Ω), we obtain that
∫E / F(g(x) −f (x)) dx ≤.
Since g(x) −f (x) > a.e., this implies that E / F is a null set.
∎
The proof of this last lemma is easily generalized to other situations (Dirichlet bound-
ary conditions, anisotropic, and/or nonlocal perimeters, see [] and also [] for a similar
general statement). Eventually, we mention that the result of Proposition remains true if
the term (u(x)−f (x))/(λ) in (> .) is replaced with a term of the form Ψ(x,u(x)),
with Ψ of class Cand strictly convex in the second variable, and replacing (t −f (x))/λ
with ∂uΨ(x, t) in (> .).
From Proposition and the regularity theory for surfaces of prescribed curvature (see
for instance, []), we obtain the following regularity result (see also []).
Corollary 
Let f
∈Lp(Ω), with p > N. Then, for all t ∈R the super-level set
Et := {uλ > t} (respectively, {uλ ≥t}) has boundary of class C,α, for all α < (p −N)/p,
out of a closed singular set Σ of Hausdorff dimension at most N −. Moreover, if p = ∞, the
boundary of Et is of class W,q out of Σ, for all q < ∞, and is of class C,if N = .

Total Variation in Imaging 

We now show that the jump set of uλ is always contained in the jump set of f . Before
stating this result let us recall two simple lemmas.
Lemma 
Let U be an open set in RN and v ∈W,p(U), p ≥. We have that
div ⎛
⎝
∇v
√
+ ∣∇v∣
⎞
⎠(y) = Trace (A(∇v(y))Dv(y))
a.e. in U,
where A(ξ) =

(+∣ξ∣)

(δi j −
ξi ξ j
(+∣ξ∣))
N
i,j=, ξ ∈RN.
The proof follows simply by taking φ ∈C∞
(U), integrating by parts in U, and
regularizing v with a smoothing kernel.
Lemma 
Let U be an open set in RN and v ∈W,(U). Assume that u has a minimum
at y∈U and
lim
ρ→+

∣B(y, ρ)∣∫B(y,ρ)
∣u(y) −u(y) −∇u(y) ⋅(y −y) −
⟨Dv(y)(y −y), y −y⟩∣
ρ
dy = .
(.)
Then Dv(y) ≥.
If A is a symmetric matrix and we write A ≥(respectively, A ≤) we mean that A is
positive (respectively, negative) semidefinite.
The result follows by proving that HN−-a.e. for ξ in SN−(the unit sphere in RN) we
have ⟨Dv(y)ξ, ξ⟩≥.
Recall that if v ∈W,(U), then (> .) holds a.e. on U [, Theorem ..].
Theorem 
Let f ∈BV(Ω) ∩L∞(Ω). Then, for all λ > ,
Juλ ⊆J f
(.)
(up to a set of zero HN−-measure).
Before giving the proof let us explain its main idea which is quite simple. Notice that,
by (> .), formally the Euler–Lagrange equation satisfied by ∂Et is
κEt + 
λ(t −f ) = 
on ∂Et,
where κEt is the sum of the principal curvatures at the points of ∂Et. Thus if x ∈Juλ / J f ,
then we may find two values t< tsuch that x ∈∂Et∩∂Et/ J f . Notice that Et⊆Et
and the boundaries of both sets have a contact at x. Of the two, the smallest level set is the
highest and has smaller mean curvature. This contradicts its contact at x.


Total Variation in Imaging
Proof.
Let us first recall some consequences of Corollary . Let Et := {uλ > t}, t ∈R,
and let Σt be its singular set given by Corollary . Since f ∈L∞(Ω), around each point
x ∈∂Et / Σt, t ∈R, ∂Et is locally the graph of a function in W,p for all p ∈[,∞) (hence
C,α for any α ∈(,)). Let Q be a countable dense set in R such that {uλ > t} is a set of
finite perimeter for any t ∈Q. Moreover, if N := ⋃t∈Q Σt, then HN−(N) = .
Let us prove that HN−(Juλ / J f ) = . Observe that we may write []
Juλ =
⋃
t,t∈Q,t<t
∂Et∩∂Et.
Thus it suffices to prove that for all t, t∈Q, t< t, we have
HN−(∂Et∩∂Et/ (N ∪J f )) = .
(.)
Let us denote by B′
R the ball of radius R > in RN−centered at . Let CR := B′
R × (−R, R).
Let us fix t, t∈Q, t< t. Given x ∈∂Et∩∂Et/N, by Corollary , we know that there is
some R > such that, after a change of coordinates that aligns the xN-axis with the normal
to ∂Et∩∂Etat x, we may write the set ∂Eti ∩CR as the graph of a function vi ∈W,p (B′
R),
∀p ∈[,∞), x = (,vi()) ∈CR ⊆Ω, ∇vi() = , i ∈{,}. Without loss of generality,
we assume that vi > in B′
R, and that Eti is the supergraph of vi, i = ,. From t< tand
Lemma , it follows Et⊆Et, which gives in turn v≥vin B′
R.
Notice that, since ∂Eti is of finite HN−measure, we may cover ∂Et∩∂Et/N by a
countable set of such cylinders. Thus, it suffices to prove that
HN−((∂Et∩∂Et∩CR) / (N ∪J f )) = 
(.)
holds for any such cylinder CR as constructed in the last paragraph.
Let us denote the points x ∈CR as x = (y, z) ∈B′
R ×(−R, R). Then (> .) will follow
if we prove that
HN−(MR) = ,
(.)
where
MR := {y ∈B′
R : v(y) = v(y)} / {y ∈B′
R : (y,v(y)) ∈J f }.
Recall that, by [, Theorem .], HN−-a.e. in y ∈B′
R, the function f (y,⋅) ∈
BV((−R, R)) and the jumps of f (y,⋅) are the points z such that (y, z) ∈J f . Recall that
vi is a local minimizer of
min
v
Ei(v) := ∫B′
R
√
+ ∣∇v∣dy −
λ ∫B′
R ∫
v(y)

(ti −f (y, z)) dz dy.
By taking a positive smooth test function ψ(y) of compact support in B′
R, and computing
limє→+ 
є (Ei(v + єψ) −Ei(v)) ≥, we deduce that
div
∇vi(y)
√
+ ∣∇vi(y)∣+ 
λ (ti −f (y,vi(y) + ) ≤,
HN−-a.e. in B′
R.
(.)
In a similar way, we have
div
∇vi(y)
√
+ ∣∇vi(y)∣+ 
λ (ti −f (y,vi(y) −) ≥,
HN−-a.e. in B′
R.
(.)

Total Variation in Imaging 

Finally, we observe that since v,v∈W,p(B′
R) for any p ∈[,∞) and v≥vin B′
R,
by Lemma we have that D(v−v)(y) ≤HN−-a.e. on {y ∈B′
R : v(y) = v(y)}.
Thus, if HN−(MR) > , then there is a point ¯y ∈MR such that ∇v(¯y) = ∇v(¯y),
D(v−v)(¯y) ≤, f (¯y,⋅) is continuous at v(¯y) = v(¯y), and both Eqs. (> .) and
(> .) hold at ¯y. As a consequence, using Lemma and subtracting the two equations,
we obtain
≥trace(A(∇v(¯y))Dv(¯y)) −trace(A(∇v(¯y))Dv(¯y)) = t−t
λ
> .
This contradiction proves (> .).
..
Hölder Regularity Results
Let us review the local regularity result proved in []: if the datum f is locally Hölder
continuous with exponent β ∈[,] in some region Ω′ ⊂Ω, then a local minimizer u of
(> .) is also locally Hölder continuous in Ω′ with the same exponent.
Recall that a function u ∈BV(Ω) is a local minimizer of (> .) if for any v ∈BV(Ω)
such that u −v has support in a compact subset K ⊂Ω, we have
∫K ∣Du∣+ 
∫K ∣u(x) −f (x)∣dx ≤∫K ∣Dv∣+ 
∫K ∣v(x) −f (x)∣dx.
(.)
It follows that u satisfies Eq. []
−div z + u = f
(.)
with z ∈L∞(Ω,RN) with ∥z∥∞≤, and z ⋅Du = ∣Du∣[].
As in
> Sect. ..[], the analysis of the regularity of the local minimizers of u
is based on the following observation: for any t ∈R, the level sets {u > t} (respectively,
{u ≥t}) are solutions (the minimal and maximal, indeed) of the prescribed curvature
problem (> .) which is defined in the class of finite-perimeter sets and hence up to a
Lebesgue-negligible set. The local regularity of u can be described in terms of the distance
of any two of its level sets. This is the main idea in [] which can be refined to obtain
the Hölder regularity of solutions of (> .). As we argued in > Sect. .., outside the
jump discontinuities of f (modulo an HN−-null set), any two level sets at different heights
cannot touch and hence the function u is continuous there. To be able to assert a Hölder
type regularity property for u, one needs to prove a local estimate of the distance of the
boundaries of two level sets. This can be done here under the assumption of local Hölder
regularity for f [].
Theorem 
Let N ≤and let u be a solution of (> .). Assume that f is in C,β locally
in some open set A ⊆Ω, for some β ∈[,]. Then u is also C,β locally in A.
The Lipschitz case corresponds to β = .


Total Variation in Imaging
One can also state a global regularity result for solutions of the Neumann problem when
Ω ⊂RN is a convex domain. Let f : Ω →R be a uniformly continuous function, with
modulus of continuity ω f : [,+∞) →[,+∞), that is ∣f (x) −f (y)∣≤ω f (∣x −y∣) for all
x, y ∈Ω. We consider the solution u of (> .) with homogeneous Neumann boundary
condition, that is, such that (> .) for any compact set K ⊂Ω and any v ∈BV(Ω)
such that v = u out of K. This solution is unique, as can be shown adapting the proof
of [, Corollary C..] (see also [] for the required adaptations to deal with the boundary
condition), which deals with the case Ω = RN.
Then, the following result holds true []:
Theorem 
Assume N ≤. Then, the function u is uniformly continuous in Ω, with
modulus ωu ≤ω f .
Again, it is quite likely here that the assumption N ≤is not necessary for this result.
.
Mathematical Analysis: Some Explicit Solutions
Recall that a convex body in RN is a compact convex subset of RN. We say that a convex
body is non-trivial if it has nonempty interior.
We want to exhibit the explicit solution of (> .) when f = χC and C is a non-trivial
convex body in RN. This will show that the preservation of a jump discontinuity depends
on the curvature of ∂C at the given point, the size of the jump, and the regularization
parameter λ.
Let uλ,C be the unique solution of the problem:
min
u∈BV(RN )∫RN ∣Du∣+ 
λ ∫RN(u −χC)dx.
(.)
The following result was proved in [].
Proposition 
We have that ≤uλ,C ≤, uλ,C = in RN / C and uλ,C is concave in
{uλ,C > }.
The proof of ≤uλ,C ≤follows from a weak version of the maximum principle
[]. Thanks to the convexity of C, by comparison with the characteristic function of hyper-
planes, one can show that uλ,C = out of C []. To prove that uλ,C is concave in {uλ,C > },
one considers first the case where C is of class C,and λ > is small enough. Then one
proves that uλ,C is concave by approximating uλ,C by the solution uє of
u −λdiv ⎛
⎝
∇u
√
є+ ∣∇u∣
⎞
⎠= 
in C
∇u
√
є+ ∣∇u∣⋅
C = −
in ∂C,
(.)

Total Variation in Imaging 

as є →+, using Korevaar’s concavity theorem []. Then one considers the case where C
is of class C,and we take any λ > . In this case, the concavity of uλ,C in {uλ,C > } is
derived after proving Theorems and below. The final step proceeds by approximating a
general convex body C by convex bodies of class C,[].
Moreover, since uλ,C = out of C, the upper-level set {uλ,C > s} ⊆C for any s ∈(,].
Then, as in Proposition , one can prove that for any s ∈(,] the level set {uλ,C > s} is a
solution of
(P)μ
min
E⊆C P(E) −μ∣E∣
(.)
for the value of μ = λ−(−s). When taking λ ∈(,+∞) and s ∈(,] we are able to cover
the whole range of μ ∈[,∞) []. By Lemma we know that if μ < μ′ and Cμ, Cμ′ are
minimizers of (P)μ, (P)μ′, respectively, then Cμ ⊆Cμ′. This implies that the solution of
(P)μ is unique for any value μ ∈(,∞) up to a countable exceptional set. Thus the sets
Cμ can be identified with level sets of uλ,C for some λ > and, therefore, we obtain its
uniqueness from the concavity of uλ,C. One can prove it as follows [, , ].
Theorem 
There is a value μ∗> such that
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
if μ < μ∗,
Cμ = /,
if μ > μ∗,
Cμ is unique (and convex),
if μ = μ∗,
there are two solutions / and Cμ∗,
where Cμ∗is the unique Cheeger set of C. Moreover for any λ < ∥χC∥∗we have μ∗:= −∥uλ,C∥∞
λ
and Cμ := {uλ,C > −μλ} for any μ > μ∗, where
∥χC∥∗:= max {∫RN u χC dx : u ∈BV(RN), ∫RN ∣Du∣≤}.
The set Cμ∗coincides with the level set {uλ,C = ∥uλ,C∥∞} and is of class C,.
We call a Cheeger set in a nonempty open-bounded subset Ω of RN any set G ⊆Ω
which minimizes
CΩ := min
F⊆Ω
P(F)
∣F∣.
(.)
The theorem contains the assertion that there is a unique Cheeger set in any nonempty
convex body of RN and μ∗= CΩ. This result was proved in [] for uniformly convex
bodies of class C, and in [] in the general case. Notice that the solution of (> .) gives
a practical algorithm to compute the Cheeger set of C.
Theorem 
Let C be a non-trivial convex body in RN. Let
HC(x) :=
⎧⎪⎪⎨⎪⎪⎩
−inf{μ : x ∈Cμ} if
x ∈C

if
x ∈RN / C.
Then uλ,C(x) := (+ λHC(x))+χC.


Total Variation in Imaging
If N = and μ > μ∗, the set Cμ coincides with the union of all balls of radius /μ
contained in C []. Thus its boundary outside ∂C is made by arcs of circle which are tangent
to ∂C. In particular, if C is a square, then the Cheeger set corresponds to the arcs of circle
with radius R > such that
P(Cμ∗)
∣Cμ∗∣= 
R . We can see that the corners of C are rounded and
the discontinuity disappears as soon as λ > (see the left part of
> Fig. -). This is a
general fact at points of ∂C where its mean curvature is infinite.
Remark
By adapting the proof of [, Proposition ] one can prove the following result. If
Ω is a bounded subset of RN with Lipschitz continuous boundary, and u ∈BV(Ω)∩L(Ω)
is the solution of the variational problem
min
u∈BV(Ω)∩L(Ω){∫Ω ∣Du∣+ 
λ ∫Ω(u −)dx + ∫∂Ω ∣u∣dHN−},
(.)
then ≤u ≤and for any s ∈(,] the upper level set {u ≥s} is a solution of
min
F⊆Ω P(F) −λ−(−s)∣F∣.
(.)
If λ > is big enough, indeed greater than /∥χΩ∥∗, then the level set {u = ∥u∥∞} is
the maximal Cheeger set of Ω. In particular, the maximal Cheeger set can be computed by
solving (> .), and for that we can use the algorithm in [] described in > Sect. ...
In the right side of > Fig. -we display the Cheeger set of a cube.
Other explicit solutions corresponding to the union of convex sets can be found in
[, ]. In particular, Allard [] describes the solution corresponding to the union of two
disks in the plane and also the case of two squares with parallel sides touching by a vertex.
Some explicit solutions for functions whose level sets are a finite number of convex sets in
Rcan be found in [].
⊡Fig. -
Left: The denoising of a square. Right: The Cheeger set of a cube

Total Variation in Imaging 

.
Numerical Methods: Iterative Methods
..
Notation
Let us fix our main notations. We denote by X the Euclidean space RN×N. The Euclidean
scalar product and the norm in X will be denoted by ⟨⋅,⋅⟩X and ∥⋅∥X, respectively.
Then the image u ∈X is the vector u = (ui,j)N
i,j=, and the vector field ξ is the map
ξ : {, . . . , N}×{, . . ., N} →R. To define the discrete total variation, we define a discrete
gradient operator. If u ∈X, the discrete gradient is a vector in Y = X × X given by
∇u := (∇xu,∇yu),
where
(∇xu)i,j = {ui+,j −ui,j
if
i < N

if
i = N,
(.)
(∇yu)i,j = {ui,j+−ui,j
if
j < N

if
j = N
(.)
for i, j = , . . . , N. Notice that the gradient is discretized using forward differences and
∇+,+u could be a more explicit notation. For simplicity we have preferred to use ∇u. Other
choices of the gradient are possible, this one will be convenient for the developments below.
The Euclidean scalar product in Y is defined in the standard way by
⟨ξ, ˜ξ⟩Y =
∑
≤i,j≤N
(ξ
i,j ˜ξ
i,j + ξ
i,j ˜ξ
i,j)
for every ξ = (ξ, ξ), ˜ξ = (˜ξ, ˜ξ) ∈Y. The norm of ξ = (ξ, ξ) ∈Y is, as usual, ∥ξ∥Y =
⟨ξ, ξ⟩/
Y . We denote the Euclidean norm of a vector v ∈Rby ∣v∣. Then the discrete total
variation is
Jd(u) = ∥∇u∥Y =
∑
≤i,j≤N
∣(∇u)i,j∣.
(.)
We have
Jd(u) =
sup
ξ∈Y, ∣ξi, j∣≤∀(i,j)
⟨ξ,∇u⟩Y.
(.)
By analogy with the continuous setting, we introduce a discrete divergence div as the
dual operator of ∇, i.e., for every ξ ∈Y and u ∈X we have
⟨−div ξ,u⟩X = ⟨ξ,∇u⟩Y.


Total Variation in Imaging
One can easily check that div is given by
(div ξ)i,j
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
ξ
i,j −ξ
i−,j
if
< i < N
ξ
i,j
if
i = 
−ξ
i−,j
if
i = N
+
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
ξ
i,j −ξ
i,j−
if
< j < N
ξ
i,j
if
j = 
−ξ
i,j−
if
j = N
(.)
for every ξ = (ξ, ξ) ∈Y.
We have
Jd(u) := max
ξ∈V ⟨u,div ξ⟩,
(.)
where
V = {ξ ∈Y : ∣ξi,j∣−≤, ∀i, j ∈{, . . ., N}}.
..
Chambolle’s Algorithm
Let us describe the dual formulation for solving the problem:
min
u∈X
Jd(u) + 
λ∥u −f ∥
X,
(.)
where f ∈X. Using (> .) we have
min
u∈X Jd(u) + 
λ∥u −f ∥
X = min
u∈X max
ξ∈V ⟨u,div ξ⟩+ 
λ∥u −f ∥
X
= max
ξ∈V min
u∈X ⟨u,div ξ⟩+ 
λ∥u −f ∥
X.
Solving explicitly the minimization in u, we have u = f −λdiv ξ. Then
max
ξ∈V min
u∈X ⟨u,div ξ⟩+ 
λ∥u −f ∥
X = max
ξ∈V ⟨f ,div ξ⟩−λ
∥div ξ∥
X
= −λ
min
ξ∈V (∥div ξ −f
λ∥

X
−∥f
λ∥

X
).
Thus if ξ∗is the solution of
min
ξ∈V ∥div ξ −f
λ∥

X
,
(.)
then u = f −λdiv ξ∗is the solution of (> .).
Notice that div ξ∗is the projection of f
λ onto the convex set
Kd := {div ξ : ∣ξi,j∣≤, ∀i, j ∈{, . . . , N}}.

Total Variation in Imaging 

As in [], the Karush–Kuhn–Tucker Theorem yields the existence of Lagrange mul-
tipliers αi,j ≥for the constraints ξ ∈V such that we have for each (i, j) ∈{, . . ., N}
∇[div ξ −λ−f ]i,j −α∗
i,jξi,j = ,
(.)
with either α∗
i,j > and ∣ξi,j∣= , or α∗
i,j = and ∣ξi,j∣≤. In the latter case, we have
∇[div ξ −λ−f ]i,j = . In any case, we have
α∗
i,j = ∣∇[div ξ −λ−f ]i,j∣.
(.)
Let
> , ξ= , p ≥. We solve (> .) using the following gradient descent (or
fixed point) algorithm
ξp+
i,j = ξp
i,j + ∇[div ξp −λ−f ]i,j −∣∇[div ξp −λ−f ]i,j∣ξp+
i,j ,
(.)
hence
ξp+
i,j =
ξp
i,j + ∇[div ξp −λ−f ]i,j
+ ∣∇[div ξp −λ−f ]i,j∣.
(.)
Observe that ∣ξp
i,j∣≤for all i, j ∈{, . . . , N} and every p ≥.
Theorem 
In the discrete framework, assuming that
< 
, then div ξp converges to the
projection of f
λ onto the convex set Kd. If div ξ∗is that projection, then u = f −λdiv ξ∗is the
solution of (> .).
In
> Fig. -we display some results obtained using Chambolle’s algorithm with
different set of parameters, namely λ = ,.
Today, the algorithms of Nesterov [] or Beck and Teboulle [] provide more efficient
way to solve this dual problem.
..
Primal-Dual Approaches
The primal gradient descent formulation is based on the solution of (> .). The dual
gradient descent algorithm corresponds to (> .). The primal-dual formulation is based
on the formulation
min
u∈X max
ξ∈V G(u, ξ) := ⟨u,div ξ⟩+ 
λ∥u −f ∥
X
and performs a gradient descent in u and gradient ascent in ξ.
Given the intermediate solution (uk, ξk) at iteration step k we update the dual variable
by solving
max
ξ∈V G(uk, ξ).
(.)
Since the gradient ascent direction is ∇ξG(uk, ξ) = −∇uk, we update ξ as
ξk+= PV (ξk −τk
λ ∇uk),
(.)


Total Variation in Imaging
⊡Fig. -
Denoising results obtained with Chambolle’s algorithm. (a) Top left: the original image. (b)
Top right: the image with a Gaussian noise of standard deviation σ = . (c) Bottom left: the
result obtained with λ = . (d) Bottom right: the result obtained with λ = 
where τk denotes the dual stepsize and PV denotes the projection onto the convex set V.
The projection PV can be computed as in (> .) or simply as
(PVξ)i,j =
ξi,j
max(∣ξi,j∣,).
Now we update the primal variable u by a gradient descent step of
min
u∈X G(u, ξk+).
(.)
The gradient ascent direction is ∇uG(u, ξk+) and the update is
uk+= uk −θk (λdiv ξk++ uk −f ),
(.)
where θk denotes the primal stepsize.
The primal-dual scheme was introduced in [] where the authors observed its excel-
lent performance although, as they point out, there is no global convergence proof. The
convergence is empirically observed for a variety of suitable stepsize pairs (τ, θ) and is
given in terms of the product τθ. For instance, convergence is reported for increasing values
θk and τkθk ≤., see [].

Total Variation in Imaging 

The primal gradient descent and the dual projected gradient descent method are special
cases of the above algorithm. Indeed if one solves the problem (> .) exactly (taking
τk = ∞in (> .)) the resulting algorithm is
uk+= uk −θk (−λdiv ∇uk
∣∇uk∣+ uk −f ) ,
(.)
with the implicit convention that we may take any element in the unit ball of Rwhen
∇uk = .
If we solve (> .) exactly and still apply gradient ascent to (> .), the resulting
algorithm is
ξk+= PV (ξk + τk∇(div ξk −f
λ )),
(.)
which essentially corresponds to (> .).
The primal-dual approach can be extended to the total variation deblurring problem
min
u∈X
Jd(u) + 
λ∥Bu −f ∥
X,
(.)
where f ∈X and B is a matrix representing the discretization of the blurring operator H.
The primal-dual scheme is based on the formulation
min
u∈X max
ξ∈V
⟨u,div ξ⟩+ 
λ∥Bu −f ∥
X,
(.)
and the numerical scheme can be written as
ξk+= PV (ξk −τk∇uk)
uk+= uk −θk (−div ξk++ λBt(Buk+−f )).
(.)
Since B is the matrix of a convolution operator, the second equation can be solved explicitly
using the FFT. Again, convergence is empirically observed for a variety of suitable stepsize
pairs (τ, θ) and is given in terms of the product τθ, see [].
For a detailed study of different primal-dual methods we refer to [].
.
Numerical Methods: Maximum Flow Methods
It has been noticed probably first in [] that maximal flow/minimum cut techniques could
be used to solve discrete problems of the form (> .), that is, to compute finite sets
minimizing a discrete variant of the perimeter and an additional external field term. Com-
bined with (a discrete equivalent of) Proposition , this leads to efficient techniques for
solving (only) the denoising problem (> .), including a method, due to D. Hochbaum,
to compute an exact solution in polynomial time (up to machine precision). A slightly
more general problem is considered in [], where the authors describe in detail algorithms
which solve the problem with an arbitrary precision.


Total Variation in Imaging
..
Discrete Perimeters and Discrete Total Variation
We will call a discrete total variation any convex, nonnegative function J : RM →[,+∞]
satisfying a discrete co-area formula:
J(u) = ∫
+∞
−∞
J(χ{u≥s}) ds,
(.)
where χ{u≥s} ∈{,}M denotes the vector such that χ{u≥s}
i
= if ui ≤s and χ{u≥s}
i
= if
ui ≥s.
As an example, we can consider the (anisotropic) discrete total variation
J(u) = ∑
≤i<N
≤j≤N
∣ui+,j −ui,j∣+ ∑
≤i≤N
≤j<N
∣ui,j+−ui,j∣.
(.)
In this case u = (ui,j)N
i,j=can be written as a vector in RM with M = N. Then, (> .)
obviously holds since for any a, b ∈R, we have
∣a −b∣= ∫
+∞
−∞∣χ{a>s} −χ{b>s}∣ds.
Observe, on the other hand, that the discretization (> .) does not enter this cate-
gory (unfortunately). In fact, a discrete total variation will be always very anysotropic (or
“crystalline”).
We assume that J is not identically +∞. Then, we can derive from (> .) the
following properties [].
Proposition 
Let J be a discrete total variation. Then:
. J is positively homogeneous: J(λu) = λJ(u) for any u ∈RM and λ ≥.
. J is invariant by addition of a constant: J(c+u) = J(u) for any u ∈RM and c ∈R, where
= (, . . . ,) ∈RM is a constant vector. In particular, J() = .
. J is lower-semicontinuous.
.
p ∈∂J(u) ⇔(∀z ∈R, p ∈∂J(χ{u≥z}).
. J is submodular: for any u,u′ ∈{,}M,
J(u ∨u′) + J(u ∧u′) ≤J(u) + J(u′).
(.)
More generally, this will hold for any u,u′ ∈RM.
Conversely, if J : {,}M →[,+∞] is a submodular function with J() = J() = , then
the co-area formula ( > .) extends it to RM into a convex function, hence a discrete total
variation.

Total Variation in Imaging 

If J is a discrete total variation, then the discrete counterpart of Proposition holds as
follows
Proposition 
Let J be a discrete total variation. Let f ∈RM and let u ∈RM be the
(unique) solution of
min
u∈RM λJ(u) + 
∥u −f ∥.
(.)
Then, for all s > , the characteristic functions of the super-level sets Es = {u ≥s} and
E′
s = {u > s} (which are different only if s ∈{ui, i = , . . . , M}) are respectively the largest
and smallest minimizer of
min
θ∈{,}M λJ(θ) +
M
∑
i=
θi(s −fi).
(.)
The proof is quite clear, since the only properties which were used for showing
Proposition where (a) the co-area formula of Theorem ; (b) the submodularity of the
perimeters (> .).
As a consequence, problem (> .) can be solved by successive minimizations
of (> .), which in turn can be done by computing a maximal flow through a graph, as
will be explained in the next section. It seems that efficiently solving the successive mini-
mizations has been first proposed in the seminal work of Eisner and Severance [] in the
context of augmenting-path maximum-flow algorithms. It was then developed, analyzed,
and improved by Gallo et al. [] for preflow-based algorithms. Successive improvements
were also proposed by Hochbaum [], specifically for the minimization of (> .). We
also refer to [, ] for variants, and to [] for detailed discussions about this approach.
..
Graph Representation of Energies for Binary MRF
It was first observed by Picard and Ratliff [] that binary Ising-like energies, that is, of the
form
∑
i,j
αi,j∣θi −θ j∣−∑
i
βiθi ,
(.)
αi,j ≥, βi ∈R, θi ∈{,}, could be represented on a graph and minimized by
standard optimization techniques, and more precisely using maximum flow algorithms.
Kolmogorov and Zabih [] showed that the submodularity of the energy is a necessary
condition, while, up to sums of ternary submodular interactions, it is also a sufficient con-
dition in order to be representable on a graph. (But other energies are representable, and
it does not seem to be known whether any submodular J can be represented on a graph,
see [, Appendix B] and the references therein.)
In case J(u) has only pairwise interactions, as in (> .), then problem (> .) has
exactly the form (> .), with αi,j = λ if nodes i and j correspond to neighboring pixels,
else, and βi is s −fi.


Total Variation in Imaging
Let us build a graph as follows: we consider V = {, . . . , M}∪{S}∪{T},where the two
special nodes S and T are respectively called the “source” and the “sink.” We consider then
oriented edges (S, i) and (i, T), i = , . . . , M, and (i, j), ≤i, j ≤M, and to each edge we
associate a capacity defined as follows:
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
c(S, i) = β−
i
i = , . . ., M,
c(i, T) = β+
i
i = , . . . , M,
c(i, j) = αi,j
≤i, j ≤M.
(.)
Here β+
i = max{, βi} and β−
i = max{,−βi}, so that βi = β+
i −β−
i . By convention we
consider there is no edge between two nodes if the capacity is zero. Let us denote by E the
set of edges with nonzero capacity and by G = (V,E) the resulting oriented graph.
We then define a “cut” in the graph as a partition of E into two sets S and T , with S ∈S
and T ∈T . The cost of a cut is then defined as the total sum of the capacities of the edges
that start on the source side of the cut and land on the sink side:
C(S,T ) =
∑
(μ, )∈E
μ∈S, ∈T
c(μ, ).
Then, if we let θ ∈{,}M be the characteristic function of S ∩{, . . . , M}, we have
C(S,T ) =
M
∑
i=
(−θi)β−
i + θiβ+
i +
M
∑
i,j=
αi,j(θi −θ j)+
=
M
∑
i,j=
αi,j(θi −θ j)+ +
M
∑
i=
θiβi +
M
∑
i=
β−
i .
If αi,j
=
αj,i (but other situations are also interesting), this is nothing else than
energy (> .), up to a constant.
Thus, the problem of finding a minimum of (> .) or (> .) can be reformulated
as the problem of finding a minimal cut in the graph. Very efficient algorithms are available,
based on a duality result of Ford and Fulkerson []. It states that the maximum flow on the
graph constrained by the capacities of the edges is equal to the minimal cost of a cut. The
problem reduces then to find the maximum flow in the graph. This is precisely defined as
follows: starting from S, we “push” a quantity (xμ, ) along the oriented edges (μ, ) ∈E of
the graph, with the constraint that along each edge,
≤xμ,
≤c(μ, )
and that each “interior” node i must satisfy the flow conservation constraint
∑
μ
xμ,i = ∑
μ
xi,μ
(while the source S only sends flow to the network, and the sink T only receives).
It is clear that the total flow f (x) = ∑i xS,i = ∑i xi,T which can be sent is bounded from
above and not hard to show that a bound is given by a minimal cost cut (S,T ). The duality

Total Variation in Imaging 

theorem of Ford and Fulkerson expresses the fact that this bound is actually reached by the
maximal flow (xμ, )(μ, )∈E (which maximizes f (x)), and the partition (S,T ) is obtained
by cutting along the saturated edges (μ, ), where xμ,
= cμ, while x ,μ = .
We can find starting from S the first saturated edge along the graph, and cut there, or
do the same starting from T and scanning the reverse graph: for βi = s−fi, this will usually
give the same solution except for a finite number of levels s, which correspond exactly to
the levels {ui : i = , . . . , M} of the solution of (> .) and are called the “breakpoints.”
Several efficient algorithms are available to compute a maximum flow in polynomial
time []. Although the time complexity of the algorithm in [], of Boykov and Kolmogorov,
is not polynomial, this algorithm seems to outperform others in terms of time computa-
tions, as it is particularly designed for the graphs with low connectivity which arise in image
processing.
The idea of a “parametric maximum flow algorithm” [] is to reuse the same graph
(and the “residual graph” which remains after a run of a max-flow algorithm) to solve
problem (> .) for increasing values s ∈{s, s, . . . , sn}. This is easily shown to
solve (> .) up to an arbitrary precision (and in polynomial time, see []). It seems
this idea was already present in a paper of Eisner and Severance [].
However, it was shown in [] by D. Hochbaum that in fact the exact solution
to (> .) can be computed, also in polynomial time. Let us now explain the basic idea
of this approach, for details we refer to [, ].
Let u = (ui)M
i=be the (unique) solution of (> .). Proposition tells us that as s
varies, problem (> .) has the same solution χ{u≥s} as long as s does not cross any of
the values {ui : i = , . . . , M}, which are precisely the breakpoints.
Assume we have found, for two levels s< s, solutions θ≥θof (> .) and assume
also that these solutions differ. It means that there is a breakpoint uiin between: there is
at least one location i(and possibly other) with s≤ui≤s.
Suppose for a while that the value uiwere the only breakpoint between sand s(i.e.,
at no other location i, we can have both s≤ui≤sand ui≠ui).
In this case, for s ∈[s, s], the optimal energy should be
F(s) = F(s) = (λJ(θ) −
M
∑
i=
θ
i fi) + s
M
∑
i=
θ
i
if s ≤ui, and
F(s) = F(s) = (λJ(θ) −
M
∑
i=
θ
i fi) + s
M
∑
i=
θ
i
for s ≥ui. And the value uiis the necessary (only) solution of the equation F(ui) =
F(ui).
Observe that in any case, as θ≥θand they are different, the slope of the affine func-
tion F(s) is strictly above the slope of the affine function F(s). Since also F(s) ≤F(s)
(as θis optimal for s) and F(s) ≤F(s), there is always a (unique) value s∈[s, s]
for which F(s) = F(s).


Total Variation in Imaging
The idea of the algorithm is now clear: we have to compute a new maximal flow (which,
in fact, reuses the residual flows from the computations of θand θ) to solve (> .) for
the level s = s. We find a solution θ, of energy
F(s) = (λJ(θ) −
M
∑
i=
θ
i fi) + s
M
∑
i=
θ
i .
Then, there are two cases:
•
Either F(s) = F(s) = F(s) – In this case we have found a breakpoint, and there
is no other in the interval [s, s]. Hence, the level sets {u ≥s} have been found for all
values s ∈[s, s]: χ{u≥s} = θfor s ∈[s, s] and θfor s ∈[s, s].
•
Or F(s) < F(s) = F(s) – Then, in particular, it must be that the solution θ
differs from both θand θ(otherwise the energies would be the same). Hence, we
can start again to try solving the problem at the levels sand swhich solve F(s) =
F(s) and F(s) = F(s). Now, since there are only a finite number of possible
sets θ solving (> .) (bounded by M, as the solutions are nonincreasing with s), this
situation can occur at most a finite number of times, bounded by M.
In practice, this can be done in a very efficient way, using “residual graphs” to start the
new maximal flow algorithms, and to compute efficiently the new levels where to cut (there
is no need, in fact, to compute the values λJ(θ) + ∑i θi fi and ∑i θi for this). See [, ]
for details.
For experimental results in the case of total variation denoising we refer to
[, , , ].
.
Other Problems: Anisotropic Total Variation Models
..
Global Solutions of Geometric Problems
The theory of anisotropic perimeters developed in [] permits to extend model (> .)
to general anisotropic perimeters, including as particular cases the geodesic active con-
tour model with an inflating force [, ], and a model for edge linking []. This
permits to find the global minima of geometric problems that appear in image processing
[, , , ].
The anisotropic total variation and perimeter:
Let us define the general notion of total
variation with respect to an anisotropy. Let us assume that Ω is an open-bounded subset
of RN with Lipschitz boundary. Let
Ω denote the outer unit normal to ∂Ω.
Following [], we say that a function ϕ : Ω × RN →[,∞) is a metric integrand if ϕ is
a Borel function satisfying the conditions:
for a.e. x ∈Ω, the map ξ ∈RN →ϕ(x, ξ) is convex,
(.)
ϕ(x, tξ) = ∣t∣ϕ(x, ξ)
∀x ∈Ω,
∀ξ ∈RN,
∀t ∈R,
(.)

Total Variation in Imaging 

and there exists a constant Λ > such that
≤ϕ(x, ξ) ≤Λ∥ξ∥
∀x ∈Ω,
∀ξ ∈RN.
(.)
We could be more precise and use the term symmetric metric integrand, but for simplicity
we use the term metric integrand. Recall that the polar function ϕ: Ω ×RN →R∪{+∞}
of ϕ is defined by
ϕ(x, ξ∗) = sup{⟨ξ∗, ξ⟩: ξ ∈RN ϕ(x, ξ) ≤}.
(.)
The function ϕ(x,⋅) is convex and lower semicontinuous.
Let X∞(Ω) := {z ∈L∞(Ω;RN) : div z ∈L∞(Ω)} and
Kϕ(Ω) := {σ ∈X∞(Ω) : ϕ(x, σ(x)) ≤for a.e. x ∈Ω, [σ ⋅
Ω] = }.
Deﬁnition 
Let u ∈L(Ω). We define the ϕ-total variation of u in Ω as
∫Ω ∣Du∣ϕ := sup {∫Ω u div σ dx : σ ∈Kϕ(Ω)}.
(.)
We set BVϕ(Ω) := {u ∈L(Ω) : ∫Ω ∣Du∣ϕ < ∞} which is a Banach space when endowed
with the norm ∣u∣BVϕ(Ω) := ∫Ω ∣u∣dx + ∫Ω ∣Du∣ϕ.
We say that E ⊆RN has finite ϕ-perimeter in Ω if χE ∈BVϕ(Ω). We set
Pϕ(E, Ω) := ∫Ω ∣DχE∣ϕ.
If Ω = RN, we denote Pϕ(E) := Pϕ(E,RN). By assumption (> .), if E ⊆RN has finite
perimeter in Ω it has also finite ϕ-perimeter in Ω.
A variational problem and its connection with geometric problems:
Let ϕ : Ω×RN →
R be a metric integrand in Ω and h ∈L∞(Ω), h(x) > a.e., with ∫Ω

h(x) dx < ∞. Let us
consider the problem
min
u∈BVϕ(Ω)∫Ω ∣Du∣ϕ + ∫∂Ω ϕ(x,
Ω)∣u∣dHN−+ λ
∫Ω h (u −f )dx.
(.)
To shorten the expressions inside the integrals we shall write h,u instead of h(x),u(x),
with the only exception of ϕ(x,
Ω). The following result was proved in [].
Theorem 
. Let f ∈L(Ω, hdx), i.e., ∫Ω f (x)h(x) dx < ∞. Then there is a unique
solution of the problem (> .).
. Let u ∈BVϕ(Ω) ∩L(Ω, h dx) be the solution of the variational problem (> .) with
f = . Then ≤u ≤and the level sets Es := {x ∈Ω : u(x) ≥s}, s ∈(,], are solutions of
min
F⊆Ω Pϕ(F) −μ∣F∣h,
(.)
where ∣F∣h = ∫F h(x) dx. As in the Euclidian case, the solution of (> .) is unique for any
s ∈(,] up to a countable exceptional set.


Total Variation in Imaging
. When λ is big enough, the level set associated to the maximum of u, {u = ∥u∥∞}, is the
maximal (ϕ, h)-Cheeger set of Ω, i.e., is a minimizer of the problem
inf {Pϕ(F)
∣F∣h
: F ⊆Ω of finite perimeter, ∣F∣h > }.
(.)
The maximal (ϕ, h)-Cheeger set (together with the solution of the family of prob-
lems (> .)) can be computed by adapting Chambolle’s algorithm [] described in
> Sect. ...
Examples:
We illustrate this formalism with two examples: (a) the geodesic active
contour model and (b) a model for edge linking.
(a) The geodesic active contour model: Let I : Ω →R+ be a given image in L∞(Ω), G be a
Gaussian function, and
g(x) =

√
+ ∣∇(G ∗I)∣,
(.)
(where in G ∗I we have extended I to RN by taking the value outside Ω). Observe that
g ∈C(Ω) and inf x∈Ω g(x) > . The geodesic active contour model [, ] with an inflating
force corresponds to the case where ϕ(x, ξ) = g(x)∣ξ∣and ∣Du∣ϕ = g(x)∣Du∣and h(x) = ,
x ∈Ω. The purpose of this model is to locate the boundary of an object of the image at the
points where the gradient is large. The presence of the inflating term helps to avoid minima
collapsing into a point. The model was intially formulated [, ] in a level set framework.
In this case we may write Pg(F) instead of Pϕ(F), and we have Pg(F) := ∫∂∗F g dHN−,
where ∂∗F is the reduced boundary of F [].
In this case, the Cheeger sets are a particular instance of geodesic active contours with
an inflating force whose constant is μ = C g,
Ω . An interesting feature of this formalism is
that it permits to define local Cheeger sets as local (regional) maxima of the function u.
They are Cheeger sets in a sub-domain of Ω. They can be identified with boundaries of
the image and the above formalism permits to compute several active contours at the same
time (the same holds true for the edge linking model).
(b) An edge linking model: Another interesting application of the above formalism is to edge
linking [, ]. Given a set Γ ⊆Ω (which may be curves if Ω ⊆Ror pieces of surface if
Ω ⊆R), we define dΓ(x) = dist(x, Γ) and the anisotropy ϕ(x, ξ) = dΓ(x)∣ξ∣. In that case,
we experimentally see that the Cheeger set determined by this anisotropy links the set of
curves (or surfaces) Γ. If Γ is a set of edges computed with an edge detector we obtain a set
or curves (N = ) or surfaces (N = ) linking them.
Notice that, for a given choice of ϕ, we actually find many local ϕ-Cheeger sets, disjoint
from the global minimum, that appear as local minima of the Cheeger ratio on the tree
of connected components of upper-level sets of u. The computation of those sets is par-
tially justified by [, Proposition .]. These are the sets which we show on the following
experiments.
Let us mention that the formulation of active contour models without edges proposed
by Chan-Vese in [] can also be related to the general formulation (> .).
On
> Fig. -, we display some local ϕ-Cheeger sets of D images for the choices
of metric ϕ corresponding to the geodesic active contour model with an inflating force

Total Variation in Imaging 

⊡Fig. -
Geodesic active contours and edge-linking experiments. The ﬁrst row shows the images I to
be processed. The ﬁrst three columns correspond to segmentation experiments, the last
three are edge-linking experiments. The second row shows the weights g used for each
experiment (white is , black is ), in the ﬁrst two cases g = (
√
+ ∣∇(G ∗I)∣)
−
, for the third
g = .(
√
.+ ∣∇(G ∗I)∣)
−
and for the linking experiments g = dS, the scaled distance
function to the given edges. The third row shows the disjoint minimum g-Cheeger sets
extracted from u (shown in the background), there are ,,,,, and sets, respectively. The
last linking experiment illustrates the eﬀect of introducing a barrier in the initial domain
(black square)
(the first three columns) and to edge linking problems (the last three columns).
The first row displays the original images, the second row displays the metric g =
(
√
+ ∣∇(G ∗I)∣)
−
or g = dS. The last row displays the resulting segmentation or set
of linked edges, respectively. Let us remark here a limitation of this approach, that can be
observed in the last subfigure. Even if the linking is produced, the presence of a bottle-
neck (bottom right subfigure) makes the dS-Cheeger set to be a set with large volume. This
limitation can be circumvented by adding barriers in the domain Ω: we can enforce hard
restrictions on the result by removing from the domain some points that we do not want
to be enclosed by the output set of curves.
..
A Convex Formulation of Continuous Multi-label
Problems
Let us consider the variational problem
min
u∈BV(Ω),≤u≤M ∫Ω ∣Du∣+ ∫Ω W(x,u(x)) dx,
(.)


Total Variation in Imaging
where W : Ω × R →R+ is a potential which is Borel measurable in x and continuous
in u, but not necessarily convex. Thus the functional is nonlinear and non-convex. The
functional can be relaxed to a convex one by considering the subgraph of u as unknown.
Our purpose is to write the nonlinearities in (> .) in a “convex way” by introducing
a new auxiliary variable []. This will permit to use standard optimization algorithms. The
treatment here will be heuristic.
Without loss of generality, let us assume that M = . Let ϕ(x, s) = H(u(x) −s), where
H = χ[,+∞) is the Heaviside function and s ∈R. Notice that the set of points where
u(x) > s (the subgraph of u) is identified as ϕ(x, s) = . That is, ϕ(x, s) is an embed-
ding function for the subgraph of u. This permits to consider the problem as a binary set
problem. The graphs of u is a “cut” in ϕ.
Let
A := {ϕ ∈BV(Ω × [,]) : ϕ(x, s) ∈{,},∀(x, s) ∈Ω × [,]}.
Using the definition of anisotropic total variation [], we may write the energy in
(> .) in terms of ϕ as
min
ϕ∈A ∫Ω ∫

(∣Dxϕ∣+ W(x, s)∣∂sϕ(x, s)∣) dx dt
+ ∫Ω(W(x,)∣ϕ(x,) −∣+ W(x,)∣ϕ(x,)∣) dx,
(.)
where the boundary conditions ϕ(x,) = , ϕ(x,) = are taken in a variational sense.
Although the energy (> .) is convex in ϕ, the problem is non-convex since the
minimization is carried on A which is a non-convex set. The proposal in [] is to relax
the variational problem by allowing ϕ to take values in [,]. This leads to the following
class of admissible functions
̃
A := {ϕ ∈BV(Ω × [,]) : ϕ(x, s) ∈[,], ∀(x, s) ∈Ω × [,], ϕs ≤}.
(.)
The associated variational problem is written as
min
ϕ∈̃
A ∫Ω ∫

(∣Dxϕ∣+ W(x, s)∣∂sϕ(x, s)∣) dx dt
+ ∫Ω(W(x,)∣ϕ(x,) −∣+ W(x,)∣ϕ(x,)∣) dx.
(.)
This problem is now convex and can be solved using the dual or primal-dual numerical
schemes explained in > Sect. ..and > ... Formally, the level sets of a solution of
(> .) give solutions of (> .). This can be proved using the developments in [, ].
In [] the authors address the problem of convex formulation of multi-label problems
with finitely many values including (> .) and the case of non-convex neighborhood
potentials like the Potts model or the truncated total variation. The general framework
permits to consider the relaxation in BV(Ω) of functionals of the form
F(u) := ∫Ω f (x,u(x),∇u(x)) dx,
(.)

Total Variation in Imaging 

where u ∈W,(Ω) and f : Ω × R × RN →[,∞[ is a Borel function such that f (x, z, ξ) is
convex in ξ for any (x, z) ∈Ω × RN and also satisfies some coercivity assumption in ξ. Let
f ∗denote the Legendre-Fenchel conjugate of f with respect to ξ. If
K := {ϕ = (ϕx, ϕs) : Ω × R →R: ϕ is smooth and f ∗(x, s, ϕx(x, s)) ≤ϕs(x, s)},
then the lower semicontinuous relaxation of F is
F(u) = sup
ϕ∈K ∫Ω ∫R ϕ ⋅Dχ{(x,s):s<u(x)}.
Based on this formula, one can use a dual or a primal-dual numerical scheme to minimize
F(u) if one knows how to compute the projection onto the convex set K. We refer to []
for details.
.
Other Problems: Image Restoration
To approach the problem of image restoration from a numerical point of view, we shall
assume that the image formation model incorporates the sampling process in a regular
grid
fi,j = (h ∗u)i,j + ni,j,
(i, j) ∈{, . . . , N},
(.)
where u : R→R denotes the ideal undistorted image, h : R→R is a blurring kernel,
f is the observed sampled image which is represented as a function f : {, . . ., N}→R,
and ni,j is, as usual, a white Gaussian noise with zero mean and standard deviation σ.
Let us denote by ΩN the interval [, N[. As we said in the introduction, in order to
simplify this problem, we assume that h,u are functions defined in ΩN and are periodic of
period N in each direction. To fix ideas, we assume that h,u ∈L(ΩN), so that h ∗u is a
continuous function in ΩN and the samples (h ∗u)i,j, (i, j) ∈{, . . ., N}, have sense.
Let us define the discrete functional
Jβ
d(u) =
∑
≤i,j≤N
√
β+ ∣(∇u)i,j∣,
β ≥.
For any function w ∈L(ΩN), its Fourier coefficients are
ˆw l
N , j
N = ∫ΩN
w(x, y)e−πi (l x+jy)
N
for (l, j) ∈Z.
Our plan is to compute a band-limited approximation to the solution of the restoration
problem for (> .). For that we define
B := {u ∈L(ΩN) : ˆu is supported in {−
+ 
N , . . ., 
}}.
We notice that B is a finite dimensional vector space of dimension Nwhich can be identi-
fied with X. Both J(u) = ∫ΩN ∣Du∣and J
d(u) are norms on the quotient space B/R, hence
they are equivalent. With a slight abuse of notation we shall indistinctly write u ∈B or
u ∈X.


Total Variation in Imaging
We shall assume that the convolution kernel h ∈L(ΩN) is such that ˆh is supported in
{ −
+ 
N , . . . , 
} and ˆh(,) = .
In the discrete framework, the ROF model for restoration is
Minimizeu∈XJβ
d(u)
(.)
subject to
N
∑
i,j=
∣(h ∗u)i,j −fi,j∣≤σ N.
(.)
Notice again that the image acquisition model (> .) is only incorporated through a
global constraint. In practice, the above problem is solved via the following unconstrained
formulation
min
u∈X max
α≥Jβ
d(u) + α

⎡⎢⎢⎢⎢⎣

N
N
∑
i,j=
∣(h ∗u)i,j −fi,j∣−σ 
⎤⎥⎥⎥⎥⎦
,
(.)
where α ≥is a Lagrange multiplier. The appropriate value of α can be computed using
Uzawa’s algorithm [], so that the constraint (> .) is satisfied. Recall that if we inter-
pret α−as a penalization parameter which controls the importance of the regularization
term, and we set this parameter to be small, then homogeneous zones are well denoised
while highly textured regions will loose a great part of its structure. On the contrary, if
α−is set to be small, texture will be kept but noise will remain in homogeneous regions.
On the other hand, as the authors of [] observed, if we use the constrained formula-
tion (> .)–(> .) or, equivalently (> .), then the Lagrange multiplier does not
produce satisfactory results since we do not keep textures and denoise flat regions simul-
taneously, and they proposed to incorporate the image acquisition model as a set of local
constraints.
Following [], we propose to replace the constraint (> .) by
G ∗(h ∗u −f )i,j ≤σ ,
∀(i, j) ∈{, . . . , N},
(.)
where G is a discrete convolution kernel such that Gi,j > for all (i, j) ∈{, . . ., N}. The
effective support of G must permit the statistical estimation of the variance of the noise with
(> .) []. Then we shall minimize the functional Jβ
d(u) on X submitted to the family
of constraints (> .) (plus eventually the constraint ∑N
i,j=(h ∗u)i,j = ∑N
i,j=fi,j). Thus,
we propose to solve the optimization problem:
min
u∈B Jβ
d(u)
subject to G ∗(h ∗u −f )
i,j ≤σ 
∀(i, j).
(.)
This problem is well posed, i.e., there exists a solution and is unique if β > and inf c∈R G ∗
(f −c)> σ . In case that β = and inf c∈R G∗(f −c)> σ , then h∗u is unique. Moreover,
it can be solved with a gradient descent approach and Uzawa’s method [].

Total Variation in Imaging 

To guarantee that the assumptions of Uzawa’s method hold we shall use a gradient
descent strategy. For that, let v ∈X and γ > . At each step we have to solve a problem
like
min
u∈B γ∣u −v∣
X + Jβ
d(u)
subject to G ∗(h ∗u −f )
i,j ≤σ 
∀(i, j).
(.)
We solve (> .) using the unconstrained formulation
min
u∈X max
α≥Lγ(u,{α};v),
where α = (αi,j)N
i,j=and
Lγ(u,{α};v) = γ∣u −v∣
X + Jβ
d(u) +
N
∑
i,j=
αi,j (G ∗(h ∗u −f )
i,j −σ ).
Algorithm:
TV-based restoration algorithm with local constraints
. Set u= or, better, u= f . Set n = .
. Use Uzawa’s algorithm to solve the problem
min
u∈X max
α≥Lγ(u,{α};un),
(.)
i.e.,
(a) Choose any set of values α
i,j ≥, (i, j) ∈{, . . . , N}, and un
= un.
Iterate from p = until convergence of αp the following steps.
(b) With the values of αp solve DP(γ,un):
min
u
Lγ(u,{αp};un)
starting with the initial condition un
p. Let un
p+be the solution obtained.
(c) Update α in the following way:
αp+
i,j = max (αp
i,j + ρ (G ∗(h ∗un
p −f )

i,j −σ ),)
∀(i, j).
Let un+be the solution of (> .). Stop when convergence of un.
We notice that, since γ > , Uzawa’s algorithm converges if f ∈h ∗B. Moreover, if u
satisfies the constraints, then un tends to a solution u of (> .) as n →∞[].
Finally, to solve problem (> .) in Step (b) of the algorithm we use either the
extension of Chambolle’s algorithm [] to the restoration case if we use β = or the
quasi-Newton method as in [] adapted to solve (> .) when β > . For more details,
we refer to [, ] and references therein.


Total Variation in Imaging
..
Some Restoration Experiments
To simulate our data we use the modulation transfer function (MTF) corresponding to
SPOT HRG satellite with Hipermode sampling (see [] for more details):
ˆh(η, η) = e−πβ∣η∣e−πα
√
η
+η
sinc(η)sinc(η)sinc(η),
(.)
where η, η∈[−/,/], sinc(η) = sin(πη)/(πη), α = ., and β= .. Then we
filter the reference image given in > Fig. -a with the filter (> .) and we add some
Gaussian white noise of zero mean and standard deviation σ (in our case σ = , which is a
realistic assumption for the case of satellite images []) to obtain the image displayed in
> Fig. -b.
> Figure -a displays the restoration of the image in
> Fig. -b obtained using
the algorithm of last section with β = . We have used a Gaussian function G of radius .
The mean value of the constraint is mean((G ∗(Ku −f ))) = .and RMSE = ..
> Figure -b displays the function αi,j obtained.
> Figure -displays some details of the results that are obtained using a single global
constraint (> .) and show its main drawbacks.
> Figure -a corresponds to the
result obtained with the Lagrange multiplier α = (thus, the constraint (> .) is sat-
isfied). The result is not satisfactory because it is difficult to denoise smooth regions and
keep the textures at the same time. > Figure -b shows that most textures are lost when
using a small value of α (α = ) and
> Fig. -c shows that some noise is present if we
⊡Fig. -
Reference image and a ﬁltered and noised image. (a) Left: reference image. (b) Right: the
data. This image has been generated applying the MTF given in (> .) to the top image
and adding a Gaussian white noise of zero mean and standard deviation σ = 

Total Variation in Imaging 

⊡Fig. -
Restored image with local Lagrange multipliers. (a) Left: the restored image corresponding
to the data given in > Fig. -b. The restoration has been obtained using the algorithm of
last section with a Gaussian function G of radius . (b) Right: the function αi,j obtained
use a larger value of α (α = ,). This result is to be compared with the same detail of
> Fig. -a which is displayed in > Fig. -d.
..
The Image Model
For the purpose of image restoration, the process of image formation can be modeled in a
first approximation by the formula []
f = Q {Π(h ∗u) + n},
(.)
where u represents the photonic flux, h is the point spread function of the optical-sensor
joint apparatus, Π is a sampling operator, i.e., a Dirac comb supported by the centers of the
matrix of digital sensors, n represents a random perturbation due to photonic or electronic
noise, and Q is a uniform quantization operator mapping R to a discrete interval of values,
typically the integers in [,].
The modulation transfer function for satellite images:
We describe here a simple
model for the modulation transfer function of a general satellite. More details can be found
in [] where specific examples of MTF for different acquisition systems are shown. The
MTF used in our experiments (> .) corresponds to a particular case of the general
model described below [].
Recall that the MTF, that we denote by ˆh, is the Fourier transform of the point spread
function of the system. Let (η, η) ∈[−/,/] denote the coordinates in the frequency
domain. There are different parts in the acquisition system that contribute to the global


Total Variation in Imaging
⊡Fig. -
A detail of the restored images with global and local constraints. Top: (a–c) display a detail of
the results that are obtained using a a single global constraint (> .) and show its main
drawbacks. Figure (a) corresponds to the result obtained with the value of α such that the
constraint (> .) is satisﬁed, in our case α = . Figure (b) shows that most textures are
lost when using a small value of α (α = ) and Fig. (c) shows that some noise is present if we
use a larger value of α (α = , ). Bottom: (d) displays the same detail of > Fig. -a which
has been obtained using restoration with local constraints
transfer function: the optical system, the sensor, and the blur effects due to motion. Since
each subsystem is considered as linear and translation invariant, it is modeled by a convo-
lution operator. The kernel k of the joint system is thus the convolution of the point spread
functions of the separated systems.
•
Sensors: In CCD arrays every sensor has a sensitive region where all the photons that
arrive are integrated. This region can be approximated by a unit square [−c/, c/]
where c is the distance between consecutive sensors. Its impulse response is then the
convolution of two pulses, one in each spatial direction. The corresponding transfer
function also includes the effect of the conductivity (diffusion of information) between
neighboring sensors, which is modeled by an exponential decay factor, thus:
ˆhS(η, η) = sinc(ηc)sinc(ηc)e−πβc∣η∣e−πβc∣η∣,
where sinc(η) = sin(πη)/(πη) and β, β> .

Total Variation in Imaging 

•
Optical system: The optical system has essentially two effects on the image: it projects
the objects from the object plane to the image plane and degrades it. The degradation of
the image due to the optical system makes that a light point source loses definition and
appears as a blurred (small) region. This effect can be explained by the wave nature of
light and its diffraction theory. Discarding other degradation effects due to the imper-
fect optical systems like lens aberrations [], the main source of degradation will be
the diffraction of the light when passing through a finite aperture: those systems are
called diffraction-limited systems.
Assuming that the optical system is linear and translation invariant, we know that it
can be modeled by a convolution operator. Indeed, if the system is linear and translation
invariant, it suffices to know the response of the system to a light point source located at
the origin, which is modeled by a Dirac delta function δ, since any other light distribu-
tion could be approximated (in a weak topology) by superpositions of Dirac functions.
The convolution kernel is, thus, the result of the system acting on δ.
If we measure the light intensity and we use a circular aperture, the MTF is
considered as an isotropic low-pass filter
ˆhO(η, η) = e−παc
√
η
+η
,
α > .
•
Motion: Each sensor counts the number of photons that arrive to its sensitive region
during a certain time of acquisition. During the sampling time the system moves a
distance τ and so does the sensor; this produces a motion blur effect in the motion
direction (d, d):
ˆhM(η, η) = sinc(⟨(η, η),(d, d)⟩τ).
Finally, the global MTF is the product of each of these intermediate transfer
functions modeling the different aspects of the satellite:
ˆh(η, η) = ˆhS ˆhO ˆhM.
•
Noise: We shall describe the typical noise in case of a CCD array. Light is constituted
by photons (quanta of light) and those photons are counted by the detector. Typically,
the sensor registers light intensity by transforming the number of photons which arrive
to it into an electric charge, counting the electrons which the photons take out of the
atoms. This is a process of a quantum nature and therefore there are random fluctua-
tions in the number of photons and photoelectrons on the photoactive surface of the
detector. To this source of noise we have to add the thermal fluctuations of the cir-
cuits that acquire and process the signal from the detector’s photoactive surface. This
random thermal noise is usually described by a zero-mean white Gaussian process. The
photoelectric fluctuations are more complex to describe: For low light levels, photoelec-
tric emission is governed by Bose–Einstein statistics, which can be approximated by a
Poisson distribution whose standard deviation is equal to the square root of the mean;
for high light levels, the number of photoelectrons emitted (which follows a Poisson
distribution) can be approximated by a Gaussian distribution which, being the limit


Total Variation in Imaging
of a Poisson process, inherits the relation between its standard deviation and its mean
[]. In the first approximation this noise is considered as spatially uncorrelated with a
uniform power spectrum, thus a white noise. Finally, both sources of noise are assumed
to be independent.
Taken together, both sources of noise are approximated by a Gaussian white noise,
which is represented in the basic equation (> .) by the noise term n. The average signal-
to-noise ratio, called the SNR, can be estimated by the quotient between the signals average
and the square root of the variance of the signal.
The detailed description of the noise requires a knowledge of the precise system of
image acquisition. More details in the case of satellite images can be found in [] and
references therein.
.
Final Remarks: A Diﬀerent Total Variation-Based
Approach to Denoising
Let us briefly comment on the interesting work [, , , , ] which interprets the total
variation model for image denoising in a Bayesian way, leading to a different algorithm
based on stochastic optimization which produces better results. We follow in the section
the presentation in [].
We work again in the discrete setting and consider the image model
fi,j = ui,j + ni,j
(i, j) ∈{, . . ., N},
(.)
where ni,j is a white Gaussian noise with zero mean and standard deviation σ.
The solution of (> .) can be viewed as a Maximum a Posteriori (MAP) estimate of
the original image u. Let β > and let pβ be the prior probability density function defined
by
pβ(u) ∝e−βJd(u)
u ∈X,
where we have omitted the normalization constant. The prior distribution models the gra-
dient norms of each pixel as independent and identically distributed random variables
following a Laplace distribution. Although the model does not exactly fit the reality since
high-gradient norms in real images are concentrated along curves and are not indepen-
dent, it has been found to be convenient and efficient for many tasks in image processing
and we follow it here.
Since the probability density of f given u is the density for n = f −u, then
p(f ∣u) ∝e−
∥f −u∥
X
σ.

Total Variation in Imaging 

Using Bayes rule, the posterior density of u given f is
pβ(u∣f ) = 
Z p(f ∣u)pβ(u) = 
Z e
−(
∥f −u∥
X
σ
+βJd(u))
,
(.)
where Z = ∫RNe
−(
∥f −u∥
X
σ
+βJd(u))
du is the normalization constant making the mass of
pβ(u∣f ) to be . Then the maximization of the a posteriori density (> .) is equivalent
to the minimization problem (> .) provided that βσ = λ.
The estimation of u proposed in [, , , , ] consists in computing the expected
value of u given f :
E(u∣f ) = 
Z ∫RNupβ(u∣f ) du = 
Z ∫RNue
−(
∥f −u∥
X
σ
+βJd(u))
du.
(.)
This estimate requires to compute an integral in a high-dimensional space. In [, , ],
the authors propose to approximate this integral with a Markov Chain Monte-Carlo
algorithm (MCMC). In
> Fig. -a we display the result of denoising the image in
> Fig. -b which has a noise of standard deviation σ = with the parameter β = 
σ .
In
> Fig. -b we display the denoising of the same image using Chambolle’s algorithm
with λ = . Notice that in both cases the parameter λ is the same.
⊡Fig. -
(a) Left: the result obtained by computing E(u∣f) and βσ= λ = , σ = (image courtesy of
Cécile Louchet). (b) Right: the result obtained using Chambolle’s algorithm with λ = 


Total Variation in Imaging
.
Conclusion
We have given in this chapter an overview of recent developments on the total variation
model in imaging. Its strong influence comes from its ability to recover the image disconti-
nuities and is the basis of numerous applications to denoising, optical flow, stereo imaging
and D surface reconstruction, segmentation, or interpolation to mention some of them.
We have reported the recent theoretical progress on the understanding of its main qual-
itative features. We have also reviewed the main numerical approaches to solve different
models where total variation appears. We have described both the main iterative schemes
and the global optimization methods based on the use of max-flow algorithms. Then, we
reviewed the use of anisotropic total variation models to solve different geometric prob-
lems and its recent use in finding a convex formulation of some nonconvex total variation
problems. We have also studied the total variation formulation of image restoration and
displayed some results. We have also reviewed a very recent point of view which inter-
prets the total variation model for image denoising in a Bayesian way, leading to a different
algorithm based on stochastic optimization which produces better results.
.
Cross-References
For complementary information on variational or PDE approaches in image processing,
numerical methods, inverse problems, or regularization methods we refer to
> Large Scale Inverse Problems
> Linear Inverse Problems
> Numerical Methods for Variational Approach in Image Analysis
> Partial Differential Equation: Images and Movies
> Regularization Methods for Ill-Posed Problems
> Statistical Inverse Problems
> Variational Approach in Image Analysis
Acknowledgement
We would like to thank Cécile Louchet for providing us the experiments of
> Sect. .
and Gabriele Facciolo and Enric Meinhardt for the experiments in > Sect. ... We would
like to thank Otmar Scherzer for pointing out to us references [, , , ]. V. Caselles
acknowledges partial support by PNPGC project, reference MTM-, by GRC ref-
erence SGR and by “ICREA Acadèmia” prize for excellence in research, the last
two funded by the Generalitat de Catalunya.

Total Variation in Imaging 

References and Further Reading
. Ahuja RK, Magnanti TL, Orlin JB () Net-
work flows: theory, algorithms, and applications.
Prentice Hall, Englewood Cliffs
. Allard WK () Total variation regulariza-
tion for image denoising: I. Geometric theory.
Siam J Math Anal ():–. Total variation
regularization for image denoising: II. Examples.
SIAM J Imag Sci ():–
. AlmansaA,Ballester C,CasellesV,Haro G()
A TV based restoration model with local con-
straints. J Sci Comput :–
. Almansa A, Aujol JF, Caselles V, Facciolo G
() Irregular to regular sampling, denoising
and deconvolution. SIAM J Mult Model Simul
():–
. Alter F, Caselles V () Uniqueness of the
Cheeger set of a convex body. Nonlinear Anal
TMA :–
. Alter F, Caselles V, Chambolle A () Evolu-
tion of convex sets in the plane by the minimizing
total variation flow. Interfaces Free Boundaries
:–
. Alter F, Caselles V, Chambolle A () A char-
acterization of convex calibrable sets in RN. Math
Ann :–
. Amar M, Bellettini G () A notion of total
variation depending on a metric with discon-
tinuous coefficients. Ann Inst Henri Poincaré
:–
. Ambrosio L () Corso introduttivo alla teoria
geometrica della misura ed alle superfici minime.
Scuola Normale Superiore, Pisa
. Ambrosio L, Fusco N, Pallara D () Func-
tions of bounded variation and free discontinu-
ity problems. Oxford Mathematical Monographs.
Clarendon, Oxford
. Andreu F, Caselles V, Mazón JM () Parabolic
quasilinear equations minimizing linear growth
functionals.
Progress
in
mathematics
.
Birkhauser Verlag, Basel
. Andrews HC, Hunt BR () Digital signal pro-
cessing. Prentice Hall, Englewood Cliffs
. Beck A, Teboulle M () A fast iterative
shrinkage-thresholding
algorithm
for
linear
inverse problems. SIAM J Imaging Sci :–
. Bellettini G, Caselles V, Novaga M () The
total variation flow in RN. J Diff Eq :–
. Bellettini G, Caselles V, Novaga M ()
Explicit solutions of the eigenvalue problem.
−div ( Du
∣Du∣)
=
u in RN. SIAM J Math Ana
:–
. Boykov Y, Kolmogorov V () An experimen-
tal comparison of min-cut/max-flow algorithms
for energy minimization in vision. IEEE Trans
Pattern Anal Mach Intell ():–
. Buades A, Coll B, Morel JM () A non local
algorithm for image denoising. In: Proceedings of
the IEEE conference on CVPR, San Diego, vol ,
pp –
. Caselles V, Chambolle A, Novaga M ()
Uniqueness of the Cheeger set of a convex body.
Pac J Math :–
. Caselles V, Chambolle A () Anisotropic
curvature-driven flow of convex sets. Non-linear
Anal TMA :–
. Caselles V, Chambolle A, Novaga M () The
discontinuity set of solutions of the TV denoising
problem and some extensions. SIAM Mult Model
Simul :–
. Caselles V, Chambolle A, Novaga M Regular-
ity for solutions of the total variation denois-
ing problem. To appear at Revista Matemática
Iberoamericana
. Caselles V, Facciolo G, Meinhardt E ()
Anisotropic Cheeger sets and applications. SIAM
J Imag Sci ():–
. Caselles V, Kimmel R, Sapiro G () Geodesic
active contours. Int J Comput Vis ():–
. Chambolle A, Lions PL () Image recovery
viatotal variationminimizationandrelatedprob-
lems. Numer Math :–
. Chambolle A () An algorithm for total varia-
tionminimizationandapplications.J Math Imag-
ing Vis :–
. Chambolle A () An algorithm for mean
curvature motion. Interfaces Free Boundaries
:–
. Chambolle A () Total variation minimiza-
tion and a class of binary MRF models. In:
Rangarajan A, Vemuri BC, Yuille AL (eds) th
International Workshop onEnergyMinimization
Methods in Computer Vision and Pattern Recog-
nition, EMMCVPR , St. Augustine, FL, USA,
November –, , Proceedings, vol of


Total Variation in Imaging
Lecture Notes in Computer Science, pp –.
Springer
. Chambolle A, Darbon J () On total varia-
tion minimization and surface evolution using
parametric maximum flows. Int J Comp Vision
():–
. Chambolle A, Cremers D, Pock T () A con-
vex approach for computing minimal partitions.
Peprint R.I. , CMA Ecole Polytechnique
. Chan TF, Golub GH, Mulet P () A non-
linear primal-dual method for total variation
based image restoration. SIAM J Sci Comput :
–
. Chan TF, Esedoglu S, Nikolova M () Algo-
rithms for finding global minimizers of image
segmentation and denoising methods. SIAM J
Appl Math :–
. Chan TF, Vese LA () Active contours without
edges. IEEE Trans Image Process ():–
. Darbon J, Sigelle M () Image restoration
with discrete constrained total variation part I:
Fast and exact optimization. J Math Imaging Vis
():–
. De Giorgi E, Ambrosio L () Un nuovo tipo
di funzionale del calcolo delle variazioni. Atti
Accad Naz Lincei Rend Cl Sci Mat Fis Natur s
():–
. De Giorgi E, Carriero M, Leaci A () Exis-
tence theorem for a minimum problem with
free discontinuity set. Arch Rational Mech Anal
():–
. Demoment G () Image reconstruction and
restoration: overview of common estimation
structures and problems. IEEE Trans Acoust
Speech Signal Process ():–
. Durand S, Malgouyres F, Rougé B () Image
deblurring, spectrum interpolation and applica-
tion to satellite imaging. ESAIM Control Optim
Calc Var :–
. Eisner MJ, Severance DG () Mathematical
techniques for efficient record segmentation in
large shared databases. J Assoc Comput Mach
():–
. Esser E, Zhang X, Chan T () A general
framework for a class of first order primal-dual
algorithms for tv minimization. CAM Reports
-, UCLA, Center for Applied Mathematics
. Gallo G, Grigoriadis MD, Tarjan RE () A fast
parametric maximum flow algorithm and appli-
cations. SIAM J Comput :–
. Goldfarb D, Yin Y () Parametric maximum
flow algorithms for fast total variation minimiza-
tion. Technical report, Rice University
. Gousseau Y, Morel JM () Are natural images
of bounded variation? SIAM J Math Anal :
–
. Greig DM, Porteous BT, Seheult AH () Exact
maximum a posteriori estimation for binary
images. J R Stat Soc B :–
. Hochbaum DS () An efficient algorithm for
image segmentation, Markov random fields and
related problems. J ACM ():–; elec-
tronic
. Kaipio
JP,
Kolehmainen
V,
Somersalo
E,
Vauhkonen M () Statistical inversion and
Monte-Carlo sampling methods in electrical
impedance tomography. Inverse Prob :–

. Kaipio JP, Somersalo E () Statistical and
computational inverse problems. Applied math-
ematical sciences, vol . Springer, New York
. Kichenassamy
S,
Kumar
A,
Olver
P,
Tannenbaum A,
Yezzi
A
()
Conformal
curvature flows: from phase transitions to active
vision. Arch Rat Mech Anal :–
. Kolehmainen
V,
Siltanen
S,
Järvenpää
S,
Kaipio JP, Koistinen P, Lassas M, Pirttilä J,
Somersalo E () Statistical
inversion for
X-ray tomography with few radiographs II:
Application to dental radiology. Phys Med Biol
:–
. Kolmogorov V, Boykov Y, Rother C ()
Applications of parametric maxflow in computer
vision. In: Proceedings of the IEEE th inter-
national conference on computer vision (ICCV
), pp –
. Kolmogorov V, Zabih R () What energy
functions can be minimized via graph cuts?
IEEE Trans Pattern Anal Mach Intell ():
–
. Korevaar N () Capillary surface convexity
above convex domains. Indiana Univ Math J
:–
. Lassas M, Siltanen S () Can one use total
variation prior for edge-preserving Bayesian
inversion? Inverse Prob ():–
. Louchet C, Moisan L () Total variation
denoising using posterior expectation. In: Pro-
ceedings of the European signal processing con-
ference (EUSIPCO), Lausanne, August 

Total Variation in Imaging 

. Meyer Y () Oscillating patterns in image pro-
cessing and nonlinear evolution equations, The
fifteenth Dean Jacqueline B. Lewis memorial lec-
tures. University Lecture Series, , American
Mathematical Society, Providence
. Nesterov Y () Smooth minimization of
nonsmooth functions. Math Prog Ser A :
–
. Nikolova M () Local strong homogeneity
of a regularized estimator. SIAM J Appl Math
:–
. Picard JC, Ratliff HD () Minimum cuts and
related problems. Networks ():–
. Pock T, Schoenemann T, Cremers D, Bischof H
() A convex formulation of continuous
multi-label problems. In: European conference
on computer vision (ECCV), Marseille, France,
October 
. Rougé B () Théorie de l’echantillonage et
satellites d’observation de la terre. Analyse de
Fourier et traitement d’images, Journées X-UPS
. Rudin L, Osher S () Total variation based
image restoration with free local constraints. In:
Proceedings of the IEEE ICIP-, vol , Austin,
pp –
. Rudin L, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D :–
. Scherzer
O,
Grasmair
M,
Grossauer
H,
Haltmeier M,
Lenzen
F
()
Variational
methods in imaging. Applied mathematical
sciences, vol . Springer, New York
. Vese L () A study in the BV space of a
denoising-deblurring variational problem. Appl
Math Optim :–
. Zhao HK, Osher S, Merriman B, Kang M
() Implicitand non-parametric shape recon-
struction from unorganized points using varia-
tional level set method. Comput Vis Image Und
():–
. Zhu M, Chan TF () An efficient primal-
dual hybrid gradient algorithm for total variation
image restoration. UCLA CAM Report number
-, 
. Ziemer WP () Weakly differentiable func-
tions, GTM . Springer, New York


Numerical Methods and
Applications in Total
Variation Image
Restoration
Raymond Chan ⋅Tony Chan ⋅Andy Yip
.
Introduction.....................................................................
.
Background.....................................................................
.
Mathematical Modeling and Analysis.........................................
..
Variants of Total Variation............................................................
...Basic Definition........................................................................
...Multichannel TV.......................................................................
...Matrix-Valued TV.....................................................................
...Discrete TV.............................................................................
...Nonlocal TV............................................................................
..
Further Applications..................................................................
...Inpainting in Transformed Domains...............................................
...Superresolution........................................................................
...Image Segmentation...................................................................
...Diffusion Tensors Images.............................................................
.
Numerical Methods and Case Examples......................................
..
Dual and Primal-Dual Methods.....................................................
...Chan–Golub–Mulet’s Primal-Dual Method.......................................
...Chambolle’s Dual Method............................................................
...Primal-Dual Hybrid Gradient Method.............................................
...Semi-Smooth Newton’s Method.....................................................
...Primal-Dual Active-Set Method.....................................................
..
Bregman Iteration......................................................................
...Original Bregman Iteration...........................................................
...The Basis Pursuit Problem............................................................
...Split Bregman Iteration...............................................................
...Augmented Lagrangian Method.....................................................
..
Graph Cut Methods....................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Numerical Methods and Applications in Total Variation Image Restoration
...Leveling the Objective.................................................................
...Defining a Graph.......................................................................
..
Quadratic Programming..............................................................
..
Second-Order Cone Programming.................................................
..
Majorization-Minimization..........................................................
..
Splitting Methods......................................................................
.
Conclusion......................................................................
.
Cross-References................................................................

Numerical Methods and Applications in Total Variation Image Restoration 

Abstract: Since their introduction in a classic paper by Rudin, Osher, and Fatemi [],
total variation minimizing models have become one of the most popular and successful
methodologies for image restoration. New developments continue to expand the capabil-
ity of the basic method in various aspects. Many faster numerical algorithms and more
sophisticated applications have been proposed. This chapter reviews some of these recent
developments.
.
Introduction
Images acquired through an imaging system are inevitably degraded in various ways. The
types of degradation include noise corruption, blurring, missing values in the pixel domain
or transformed domains, intensity saturation, jittering, etc. Such degradations can have
adverse effects on high-level image processing tasks such as object detection and recogni-
tion. Image restoration aims at recovering the original image from its degraded version(s)
to facilitate subsequent processing tasks. Image data differ from many other kinds of data
due to the presence of edges, which are important features in human perception. It is
therefore essential to preserve and even reconstruct edges in the processing of images.
Variational methods for image restoration have been extensively studied in the past cou-
ple of decades. A promise of these methods is that the geometric regularity of the resulting
images is explicitly controlled by using well-established descriptors in geometry. For exam-
ple, smoothness of object boundaries can be easily manipulated by controlling their length.
There has also been much research in designing variational methods for preserving other
important image features such as textures.
Among the various restoration problems, denoising is perhaps the most fundamental
one. Indeed, all algorithms for solving ill-posed restoration problems have to have some
denoising capabilities either explicitly or implicitly, for otherwise they cannot cope with
any error (noise) introduced during image acquisition or numerical computations. More-
over, the noise removal problem boils down to the fundamental problem of modeling
natural images which has great impacts on any image processing tasks. Therefore, research
on image denoising has been very active.
.
Background
Total variation (TV)–based image restoration models are introduced by Rudin, Osher, and
Fatemi (ROF) in their seminal work [] on edge preserving image denoising. It is one of
the earliest and best known examples of variational partial differential equation (PDE)-
based edge preserving denoising models. In this model, the geometric regularity of the
resulting image is explicitly imposed by reducing the amount of oscillation while allowing
for discontinuities (edges). The unconstrained version introduced in [] reads:
inf
u∈L(Ω) ∫Ω ∣∇u∣+ μ ∫Ω(u −f )dx.
(.)


Numerical Methods and Applications in Total Variation Image Restoration
Here, Ω is the image domain, f : Ω →R is the observed noisy image, u : Ω →R is the
denoised image, and μ ≥is a parameter depending on the noise level. The first term
is the total variation (TV) which is a measure of the amount of oscillation in the result-
ing image u. Its minimization would reduce the amount of oscillation which presumably
reduces noise. The second term is the Ldistance between u and f , which encourages the
denoised image to inherit most features from the observed data. Thus the model trades off
the closeness to f by gaining the regularity of u. The noise is assumed to be additive and
Gaussian with zero mean. If the noise variance level σ is known, then the parameter μ can
be treated as the Lagrange multiplier, restraining the resulting image to be consistent with
the known noise level, i.e., ∫Ω(u −f )dx = ∣Ω∣σ [].
The ROF model is simple and elegant for edge preserving denoising. Since its intro-
duction, this model has ignited a great deal of research in constructing more sophisticated
variants which can give better reconstructed images, designing faster numerical algorithms
for solving the optimization problem numerically, and finding new applications in various
domains. In a previous book chapter [] published in , the authors surveyed some
recent progresses in the research of total variation-based models. The present chapter aims
at highlighting some exciting latest developments in numerical methods and applications
of total variation-based methods since the last survey.
.
Mathematical Modeling and Analysis
In this section, the basic definition of total variation and some of its variants are presented.
Then, some recent TV based mathematical models in imaging are reviewed.
..
Variants of Total Variation
...
Basic Deﬁnition
The use of TV as a regularizer has been shown to be very effective for processing images
because of its ability to preserve edges. Being introduced for different reasons, several
variants of TV can be found in the literature. Some variants can handle more sophisti-
cated data such as vector-valued imagery and matrix-valued tensors; some are designed
to improve restoration quality and some are modified versions for the ease of numerical
implementation. It is worthwhile to review the basic definition and its variants.
In Rudin, Osher, and Fatemi’s work [], the TV of an image f : Ω →R is defined as
∫Ω ∣∇f ∣dx,
(.)
where Ω ⊆Ris a bounded open set. Since the image f may contain discontinuities, the
gradient ∇f must be interpreted in a generalized sense. It is well known that elements
of the Sobolev space W,(Ω) cannot have discontinuities []. Therefore, the TV cannot

Numerical Methods and Applications in Total Variation Image Restoration 

be defined through the completion of the space Cof continuously differentiable func-
tions under the Sobolev norm. The ∇f is thus interpreted as a distributional derivative
and its integral is interpreted as a distributional integral []. Under this framework, the
minimization of TV naturally leads to a PDE with a distribution as a solution.
Besides defining TV as a distributional integral, other perspectives can offer some
unique advantages. A set theoretical way is to define TV as a Radon measure of the domain
Ω []. This has an advantage of allowing Ω to be a more general set. But a more practical
and simple alternative is the “dual formulation.” It uses the usual trick in defining weak
derivatives – integration by parts – together with the Fenchel transform,
∫Ω ∣∇f ∣= sup {∫Ω f divg dx ∣g ∈C
c(Ω,R),∣g(x)∣≤∀x ∈Ω},
(.)
where f ∈L(Ω) and div is the divergence operator. Using this definition, one can bypass
the discussion of distributions. It also plays an important role in many recent works in dual
and primal-dual methods for solving TV minimization problems. The space BV can now
be defined as
BV(Ω) := {f ∈L(Ω) ∣∫Ω ∣∇f ∣< ∞}.
Equipped with the norm ∥f ∥BV = ∥f ∥L+ ∫Ω ∣∇f ∣, this space is complete and is a proper
superset of W,(Ω) [].
...
Multichannel TV
Many practical images are acquired in a multi-channel way, where each channel empha-
sizes a specific kind of signal. For example, color images are often acquired through the
RGB color components, whereas microscopy images consist of measurements of different
fluorescent labels. The signals in the different channels are often correlated (contain redun-
dant information). Therefore, in many practical situations, regularization of multi-channel
images should not be done independently on each channel.
There are several existing ways to generalize TV to vectorial data. A review of some
generalizations can be found in []. Many generalizations are very intuitive. But only some
of them have a natural dual formulation. Sapiro and Ringach [] proposed to define
∫Ω ∣∇f ∣:= ∫Ω




M
∑
i=
∣∇fi∣dx = ∫Ω ∣∇f ∣F dx,
where f = (f(x), f(x), . . ., fM(x)) is the vectorial data with M channels. Thus, it is the
integral of the Frobenius norm ∣⋅∣F of the Jacobian ∇f . The dual formulation given in [] is
sup {∫Ω⟨f ,div g⟩dx ∣g ∈C
c(Ω,R×M),∣g(x)∣F ≤∀x ∈Ω},
where ⟨f ,div g⟩= ∑M
i=fidiv gi.


Numerical Methods and Applications in Total Variation Image Restoration
...
Matrix-Valued TV
In applications such as Diffusion Tensor Images (DTI), the measurements at each spa-
tial location are represented by a diffusion tensor, which is a × symmetric positive
semi-definite matrix. Recent efforts have been devoted to generalize the TV to matrix-
valued images. Some natural generalizations can be obtained by identifying an M × N
matrix with an MN vector, so that a vector-valued total variation can be applied. This was
done by Tschumperlé and Deriche [], which generalized the vectorial TV of [] and by
Wang et al. [] and Christiansen et al. [], which generalized the vectorial TV of []. The
main challenge is to preserve the positive definiteness of the denoised solution. This will
be elaborated in > Sect. ....
Another interesting approach proposed by Setzer et al. [] is the so-called operator-
based regularization. Given a matrix-valued function f = (fi j(x)), define a matrix func-
tion A := (ai j) where ai j = ∣∇fi j∣. Let Φ(A) be the matrix obtained by replacing each
eigenvalue λ of A with
√
λ. Then the total variation is defined to be ∫Ω ∣Φ(A)∣F dx, where
∣⋅∣F is the Frobenius norm. While this formulation seems complicated, its first variation
turns out to have a nice simple formula. However, when combined with the ROF model,
the preservation of positive definiteness is an issue.
...
Discrete TV
The ROF model is cast as an infinite dimensional optimization problem over the BV space.
To solve the problem numerically, one must discretize the problem at some stage. The
approach proposed by Rudin et al. in [] is to “optimize then discretize.” The gradient
flow equation is discretized with a standard finite difference scheme. This method works
very well, in the sense that the numerical solution converges to a steady state which is qual-
itatively consistent with the expected result of the (continuous) ROF model. However, to
the best of the authors’ knowledge, a theoretical proof of convergence of the numerical
solution to the exact solution of the gradient flow equation as the grid size tends to zero is
not yet available. A standard convergence result of finite difference schemes for nonlinear
PDE is based on the compactness of TV-bounded sets in L[]. However, proving TV
boundedness in two or more dimensions is often difficult.
An alternative approach is to “discretize then optimize.” In this case, one only has to
solve a finite dimensional optimization problem, whose numerical solution can in many
cases be shown to converge. But the convergence of the exact solution of the finite dimen-
sional problems to the exact solution of the original infinite dimensional problem is often
hard to obtain too. So, both approaches suffer from the theoretical convergence problem.
But the latter method has a precise discrete objective to optimize.
To discretize the ROF objective, the fitting term is often straightforward. But the
discretization of the TV term has a strong effect on the numerical schemes. The most
commonly used versions of discrete TV are

Numerical Methods and Applications in Total Variation Image Restoration 

∥f ∥TV =
m−
∑
i=
n−
∑
j=
√
(fi+,j −fi,j)+ (fi,j+−fi,j)Δx,
(.)
∥f ∥TV =
m−
∑
i=
n−
∑
j=
(∣fi+,j −fi,j∣+ ∣fi,j+−fi,j∣) Δx,
(.)
where f = (fi,j) is the discrete image and Δx is the grid size. They are sometimes referred as
the isotropic and anisotropic versions respectively for they are a formal discretization of the
isotropic TV ∫Ω
√
f x + f y dx and the anisotropic TV ∫Ω(∣fx∣+ ∣fy∣) dx respectively. The
anisotropic TV is not rotational invariant; an image and its rotation can have a different TV
value. Therefore the discrete TV (> .) deviates from the original isotropic TV. But being
a piecewise linear function, some numerical techniques for quadratic and linear problems
can be applied. Indeed, by introducing some auxiliary variables, the corresponding discrete
ROF objective can converted into a canonical quadratic programming problem [].
Besides using finite difference approximations, a recent popular way is to represent TV
on graphs []. To make the problem fully discrete, the range of the image is quantized to
a finite set of K integers only, usually –. The image is “leveled,” so that f k
i,j = if the
intensity of the (i, j)th pixel is at most k, and f k
i,j = otherwise. Then the TV is given by
∥f ∥TV =
K−
∑
k=
∑
i,j
∑
s,t
wi,j,s,t ∣f k
i,j −f k
s,t∣,
(.)
where wi,j,s,t is a nonnegative weight. A simple choice is the four-connectivity model where
wi,j,s,t = if ∣i−s∣+∣j−t∣≤and wi,j,s,t = otherwise. In this case, it becomes the anisotropic
TV (> .). Different choices of the weights penalize edges in different orientations.
A related concept introduced by Shen and Kang is the quantum total variation [].
They studied the ROF model when the range of an image is a finite discrete set (preas-
signed or determined on the fly), but the image domain is a continuous one. The model is
suitable for problems such as bar code scanning, image quantization, and image segmen-
tation. An elegant analysis of the model and some stochastic gradient descent algorithms
were presented there.
...
Nonlocal TV
First proposed by Buades et al. [], the nonlocal means algorithm renounces the use of
local smoothness to denoise an image. Patches which are spatially far away but photomet-
rically similar are also utilized in the estimation process – a paradigm which has been used
in texture synthesis []. The denoising results are surprisingly good. Since then, the use of
nonlocal information becomes increasingly popular. In particular, Bresson and Chan []
and Gilboa and Osher [] considered the nonlocal TV. The nonlocal gradient ∇NL f for a
pair of points x ∈Ω and y ∈Ω is defined by
∇NL f (x,y) =
√
w(x,y)(f (x) −f (y)),


Numerical Methods and Applications in Total Variation Image Restoration
where w(x,y) is a nonnegative weight function which is presumably a similarity measure
between a patch around x and a patch around y. As an illustration, a simple choice of the
weight function is
w(x,y) = αe−∣x−y∣/σ 
+ αe−∣F(x)−F(y)∣/σ 
,
where αi and σi are positive constants and F(x) is a feature vector derived from a patch
around x. The constants αi may sometimes be defined to depend on x, so that the total
weight over all y ∈Ω is normalized to . In this case, the weight function is nonsymmetric
with respective to its arguments. The first term in w is a measure of geometric similarity,
so that nearby pixels have a higher weight. The second term is a measure of photometric
similarity. The feature vector F can be the color histogram or any texture descriptor over a
window around x. The norm of the nonlocal gradient at x is defined by
∣∇NL f ∣(x) =
√
∫Ω[∇NL f (x,y)]dy,
which adds up all the squared intensity variation relative to f (x), weighted by the similarity
between the corresponding pair of patches. The nonlocal TV is then naturally defined by
summing up all the norms of the nonlocal gradients over the image domain:
∫Ω ∣∇NL f ∣dx.
Therefore the nonlocal TV is small if for each pair of similar patches, the intensity differ-
ence between their centers is small. An advantage of using the nonlocal TV to regularize
images is its tendency to preserve highly repetitive patterns better. In practice, the weight
function is often truncated to reduce the computation costs spent in handling the many
less similar patches.
..
Further Applications
...
Inpainting in Transformed Domains
After the release of the image compression standard JPEG, images can be formatted
and stored in terms of wavelet coefficients. For instance, in Acrobat .or later, users can
opt to use JPEGto compress embedded images in a PDF file. During the process of
storing or transmission, some wavelet coefficients may be lost or corrupted. This prompts
the need of restoring missing information in wavelet domains. The setup of the problem is
as follows. Denote the standard orthogonal wavelet expansion of the images f and u by
f (α) = ∑
j,k
αj,kψj,k(x),
j ∈Z, k ∈Z,
and
u(β) = ∑
j,k
βj,kψj,k(x),
j ∈Z, k ∈Z,

Numerical Methods and Applications in Total Variation Image Restoration 

where {ψj,k} is the wavelet basis, and {αj,k},{βj,k} are the wavelet coefficients of f and u
given by
αj,k = ⟨f ,ψj,k⟩
and
βj,k = ⟨u,ψj,k⟩,
(.)
respectively, for j ∈Z, k ∈Z. For convenience, u(β) is denoted by u when there is
no ambiguity. Assume that the wavelet coefficients in the index set I are known, i.e., the
available wavelet coefficients are given by
ξ j,k =
⎧⎪⎪⎨⎪⎪⎩
αj,k,
(j, k) ∈I,
,
(j, k) ∈Ω/I.
The aim of wavelet domain inpainting is to reconstruct the wavelet coefficients of u from
the given coefficients ξ. It is well known that the inpainting problem is ill posed, i.e., it
admits more than one solution. There are many different ways to fill in the missing coef-
ficients, and therefore many different reconstructions in the pixel domain are possible.
Regularization methods can be used to incorporate prior information about the recon-
struction. In [], Chan, Shen, and Zhou used TV to solve the wavelet inpainting problem,
so that the missing coefficients are filled while preserving sharp edges in the pixel domain
faithfully. More precisely, they considered the minimization of the following objective
F(β) = 
∑
j,k
χ j,k (ξ j,k −βj,k)
+ λ ∥u∥TV ,
(.)
with χ j,k = if (j, k) ∈I and χ j,k = if (j, k) ∈Ω/I, and λ is the regularization parameter.
The first term in F is the data-fitting term and the second is the TV regularization term.
The method Chan, Shen, and Zhou used to optimize the objective is the standard gradient
descent. The method is very robust but it often slows down significantly before it converges.
In [], Chan, Wen, and Yip proposed an efficient optimization transfer algorithm to
minimize the objective (> .). An auxiliary variable ζ is introduced to yield a new
objective function:
G(ζ, β) = + τ
τ (∥χ (ζ −ξ)∥
+ τ ∥ζ −β∥
) + λ ∥u(β)∥TV ,
where χ denotes a diagonal matrix with diagonal entries χ j,k and τ is an arbitrary positive
parameter. The function G is a quadratic majorizing function [] of F. The method also
has a flavor of the splitting methods introduced in
> Sect. ... But a major difference
is that the method here solves the original problem (> .) without any alteration. It can
be easily shown that
F(β) = min
ζ
G(ζ, β)
for any positive regularization parameter τ. Thus, the minimization of G w.r.t. (ζ, β) is
equivalent to the minimization of F w.r.t. β for any τ > . Unlike the gradient descent
method of [], the optimization transfer algorithm avoids the use of derivatives of the TV.
It also does not require smoothing out the TV to make it differentiable. The experimental
results in [] showed that the algorithm is very efficient and outperforms the gradient
descent method.


Numerical Methods and Applications in Total Variation Image Restoration
...
Superresolution
Image superresolution refers to the process of increasing spatial resolution by fusing infor-
mation from a sequence of low-resolution images of the same scene. The images are
assumed to contain subpixel information (due to subpixel displacements or blurring), so
that the superresolution is possible.
In [], Chan et al. proposed a unified TV model for superresolution imaging prob-
lems. They focused on the problem of reconstructing a high-resolution image from several
decimated, blurred, and noisy low-resolution versions of the high-resolution image. They
derived a low-resolution image formation model which allows multiple shifted and blurred
low-resolution image frames, so that it subsumes several well-known models. The model
also allows an arbitrary pattern of missing pixels (in particular an arbitrary pattern of
missing frames). The superresolution image reconstruction problem is formulated as an
optimization problem which combines the image formation model and the TV inpainting
model. In this method, TV minimization is used to suppress noise amplification, repair
corrupted pixels in regions without missing pixels, and to reconstruct intensity levels in
regions with missing pixels.
Image Formation Model
The observation model, Chan et al. considered, consists of various degradation processes.
Assume that a number of m × n low-resolution frames are captured by an array of charge-
coupled device (CCD) sensors. The goal is to reconstruct an Lm×Ln high-resolution image.
Thus, the resolution is increased by a factor of L in each dimension. Let u be the ideal
Lm × Ln high-resolution clean image.
. Formation of low-resolution frames. A low-resolution frame is given by
Dp,qCu,
where C is an averaging filter with window size L-by-L, and Dp,q is the downsam-
pling matrix which, starting at the (p, q)th pixel, samples every other L pixels in both
dimensions to form an m × n image.
. Blurring of frames. This is modeled by
Hp,qDp,qCu,
where Hp,q is the blurring matrix for the (p, q)th frame.
. Concatenation of frames. The full set of Lframes are interlaced to form an mL × nL
image:
Au,
where
A = ∑
p,q
DT
p,qHp,qDp,qC.

Numerical Methods and Applications in Total Variation Image Restoration 

. Additive Noise.
Au + η,
where each pixel in η is a Gaussian white noise.
. Missing pixels and missing frames.
f = ΛD(Au + η),
where D denotes the set of missing pixels and ΛD is the downsampling matrix from
the image domain to D.
. Multiple observations. Finally, multiple observations of the same scene, but with differ-
ent noise and blurring, are allowed. This leads to the model
fr = ΛDr(Aru + ηr)
r = , . . . , R,
(.)
where
Ar = ∑
p,q
DT
p,qHp,q,r Dp,qC.
TV Superresoluton Imaging Model
To invert the degradation processes in (.), a Tikhonov-type regularization model has
been used. It requires minimization of the following energy:
F(u) = 

R
∑
r=
∥ΛDrAru −fr∥+ λ∥u∥TV.
(.)
This model simultaneously performs denoising, deblurring, inpainting, and superresolu-
tion reconstruction. Experimental results show that reasonably good reconstruction can
be obtained even if five-sixth of the pixels are missing and the frames are blurred.
...
Image Segmentation
TV minimization problems also arise from image segmentation. When one seeks for a par-
tition of the image into homogeneous segments, it is often helpful to regularize the shape
of the segments. This can increase the robustness of the algorithm against noise and avoid
spurious segments. It may also allow the selection of features of different scales. In the clas-
sical Mumford–Shah model [], the regularization is done by minimizing the total length
of the boundary of the segments. In this case, if one represents a segment by its characteris-
tic function, then the length of its boundary is exactly the TV of the characteristic function.
Therefore, the minimization of length becomes the minimization of TV of characteristic
functions.
Given an observed image f on an image domain Ω, the piecewise constant Mumford–
Shah model seeks a set of curves C and a set of constants c = (c, c, . . . , cL) which
minimize the energy functional given by:
FMS(C,c) =
L
∑
l=∫Ωl
[f (x) −cl]dx + β ⋅Length(C).
(.)


Numerical Methods and Applications in Total Variation Image Restoration
The curves in C partition the image into L mutually exclusive segments Ωl for l =
,, . . ., L. The idea is to partition the image, so that the intensity of f in each segment
Ωl is well approximated by a constant cl. The goodness-of-fit is measured by the Ldif-
ference between f and cl. On the other hand, a minimum description length principle is
employed which requires the curves C to be as short as possible. This increases the robust-
ness to noise and avoids spurious segments. The parameter β > controls the trade-off
between the goodness-of-fit and the length of the curves C.
The Mumford–Shah objective is non-trivial to optimize especially when the curves
need to be split and merged. Chan and Vese [] proposed a level set-based method which
can handle topological changes effectively. In the two-phase version of this method, the
curves are represented by the zero level set of a Lipschitz level set function ϕ defined on
the image domain. The objective function then becomes
FCV(ϕ, c, c) = ∫Ω H(ϕ(x))[f (x) −c]dx
+ ∫Ω[−H(ϕ(x))][f (x) −c]dx + β ∫Ω ∣∇H(ϕ)∣.
The function H is the Heaviside function defined by H(x) = if x ≥, H(x) = 
otherwise. In practice, we replace H by a smooth approximation Hє, e.g.,
Hє(x) = 
[+ 
π arctan (x
є )].
Although this method makes splitting and merging of curves a simple matter, the energy
functional is non-convex which possesses many local minima. These local minima may
correspond to undesirable segmentations, see [].
Interestingly, for fixed cand c, the above non-convex objective can be reformulated
as a convex problem, so that a global minimum can be easily computed, see [, ]. The
globalized objective is given by
FCEN(u, c, c) = ∫Ω {[f (x) −c]−[f (x) −c]}u(x)dx + β ∫Ω ∣∇u∣,
(.)
which is minimized over all u satisfying the bilateral constraints ≤u ≤, and all scalars
cand c. After a solution u is obtained, a global solution to the original two-phase
Mumford–Shah objective can be obtained by thresholding u with μ for almost every
μ ∈[,], see [, ]. Some other proposals for computing global solutions can be found
in [].
To optimize the globalized objective function (> .), Chan et al. [] proposed to use
anexactpenaltymethodtoconvertthebilaterallyconstrainedproblemtoan unconstrained
problem. Then the gradient descent method is applied. This method is very robust and
easy to implement. Moreover, the exact penalty method treats the constraints gracefully,
as if there is no constraint at all. But of course the gradient descent is not particular fast.
In [], Krishnan et al. considered the following discrete two-phase Mumford–Shah
model:
FCEN(u, c, c) = ⟨s,u⟩+ β∥u∥TV + α
∥u −
∥
,
(.)

Numerical Methods and Applications in Total Variation Image Restoration 

where ⟨⋅,⋅⟩is the l inner product, s = (si,j) and
si,j = (fi,j −c)−(fi,j −c).
(.)
The variable u is bounded by the bilateral constraints ≤u ≤. When α = , this problem
is convex but not strictly convex. When α > , this problem is strictly convex. The additive
constant 
is introduced in the third term so that the minimizer does not bias toward u = 
or u = . This problem is exactly a TV denoising problem with bound constraints. Krishnan
et al. proposed to use the primal-dual active-set method to solve the problem. Superlinear
convergence has been established.
...
Diﬀusion Tensors Images
Recently, Diffusion Tensor Imaging (DTI), a kind of magnetic resonance (MR) modality,
becomes increasing popular. It enables the study of anatomical structures such as nerve
fibers in human brains non-invasively. Moreover, the use of direction-sensitive acquisitions
results in its lower signal-to-noise ratio compared to convectional MR. At each voxel in
the imaging domain, the anisotropy of diffusion water molecules is interested. Such an
anisotropy can be described by a diffusion tensor D, which is a × positive semi-definite
matrix. By standard spectral theory results, D can be factorized into
D = VΛV T,
where V is an orthogonal matrix whose columns are the eigenvectors of D and Λ is a diag-
onal matrix whose diagonal entries are the corresponding eigenvalues. These eigenvalues
provide the diffusion rate along the three orthogonal directions defined by the eigenvec-
tors. The goal is to estimate the matrix D (one at each voxel) from the data. Under the
Stejskal–Tanner model, the measurement Sk from the imaging device and the diffusion
tensor are related by
Sk = Se−bgT
k Dgk ,
(.)
where Sis the baseline measurement, gk is the prescribed direction in which the measure-
ment is done, and b > is a scalar depending the strength of the magnetic field applied
and the acquisition time. Since D has six degrees of freedom, six measurements at differ-
ent orientations are needed to reconstruct D. In practice, the measurements are very noisy.
Thus matrix D obtained by directly solving (> .) for k = ,, . . . ,may not be positive
semi-definite and is error-prone. It is thus often helpful to take more than six measure-
ments and to use some least squares methods or regularization to obtain a robust estimate
while preserving the positive semi-definiteness for physical correctness.
In [] Wang et al. and in [] Christiansen et al. proposed an extension of the ROF to
denoise tensor-valued data. Two major differences between the two works are that the for-
mer regularizes the Cholesky factor of D and uses a channel-by-channel TV regularization
whereas the latter regularizes the tensor D directly and uses a multi-channel TV.


Numerical Methods and Applications in Total Variation Image Restoration
The method in [] is two staged. The first stage is to estimate the diffusion tensors from
the raw data based on the Stejskal–Tanner model (> .). The obtained tensors are often
noisy and may not be positive semi-definite. The next stage is to use the ROF model to
denoise the tensor while restricting the results to be positive semi-definite. The trick they
used to ensure positive semi-definiteness is very simple and practical. They observed that
a symmetric matrix is positive semi-definite if and only if it has a Cholesky factorization
of the form
D = LLT,
where L is a lower triangular matrix
L =
⎡⎢⎢⎢⎢⎢⎣
l


l
l

l
l
l
⎤⎥⎥⎥⎥⎥⎦
.
Then one can easily express D in terms of li j for ≤j ≤i ≤:
D = D(L) =
⎡⎢⎢⎢⎢⎢⎣
l 

ll
ll
ll
l 
+ l 

ll+ ll
ll
ll+ ll
l 
+ l 
+ l 

⎤⎥⎥⎥⎥⎥⎦
.
The ROF problem, written in a continuous domain, is then formulated as
min
L
⎧⎪⎪⎨⎪⎪⎩

∑
i j ∫Ω [di j(L) −ˆdi j]
dx + λ



∑
i j
[∫Ω ∣∇di j(L)∣]
⎫⎪⎪⎬⎪⎪⎭
,
where ˆD = ( ˆdi j) is the observed noisy tensor field and L is the unknown lower triangular
matrix-valued function from Ω to R×. Here the matrix-valued version of TV is used.
The objective is then differentiated w.r.t. the lower triangular part of L to obtain a system
of six first-order optimality conditions. Once the optimal L is obtained, the tensor D can
be formed by taking D = LLT which is positive semi-definite.
The original ROF problem is strictly convex so that one can obtain the globally optimal
solution. However, in this problem, due to the nonlinear change of variables from D to L,
the problem becomes non-convex. But the authors of [] reported that in their experi-
ments different initial data often resulted in the same solution, so that the non-convexity
does not pose any significant difficulty to the optimization of the objective.
.
Numerical Methods and Case Examples
Fast numerical methods for TV minimization continues to be an active research area.
Researchers from different fields have been bringing many fresh ideas to the problem and
led to many exciting results. Some categories of particular mention are dual/primal-dual
methods, Bregman iterative methods, and graph cut methods. Many of these methods have
a long history with a great deal of general theories developed. But when it comes to their
application to the ROF model, many further properties and specialized refinements can be

Numerical Methods and Applications in Total Variation Image Restoration 

exploited to obtain even faster methods. Having said so, different algorithms may adopt
different versions of TV. They have different properties and thus may be used for different
purposes. Thus, some caution needs to be taken when one attempts to draw conclusions
such as method A is faster than method B. Moreover, different methods have different
degree of generality. Some methods can be extended directly to deblurring, while some can
only be applied to denoising. (Of course, one can use an outer iteration to solve a deblur-
ring problem by a sequence of denoising problems, so that any denoising algorithm can be
used. But the convergence of the outer iteration has little, if not none, to do with the inner
denoising algorithm.) This section surveys some recent methods for TV denoising and/or
deblurring. The model considered here is a generalized ROF model which simultaneously
performs denoising and deblurring. The objective function reads
F(u) = 
∫Ω(Ku −f )dx + λ ∫Ω ∣∇u∣,
(.)
where K is a blurring operator and λ > is the regularization parameter. For simplicity, we
assume that K is invertible. When K is the identity operator, (> .) is the ROF denoising
model.
..
Dual and Primal-Dual Methods
The ROF objective is non-differentiable in flat regions where ∣∇u∣= . This leads to much
difficulty in the optimization process since gradient information (hence Taylor’s expan-
sion) becomes unreliable in predicting the function value even locally. Indeed, the staircase
effects of TV minimization can introduce some flat regions which make the problem
worse. Even if the standard procedure of replacing the TV with a reasonably smoothed
version is used so that the objective becomes differentiable, the Euler–Lagrange equation
for (> .) is still very stiff to solve. Higher-order methods such as Newton’s methods
often fail to work because higher-order derivatives are even less reliable.
Due to the difficulty in optimizing the ROF objective directly, much recent research has
been directed toward solving some reformulated versions. In particular, methods based on
dual and primal-dual formulations have been shown to be very fast in practice. Actually,
the dual problem (see (> .) below) also has its own numerical difficulties to face, e.g.,
the objective is rank deficient and some extra work is needed to deal with the constraints.
But the dual formulation brings many well-developed ideas and techniques from numer-
ical optimization to bear on this problem. Primal-dual methods have also been studied
to combine information from the primal and dual solutions. Several successful dual and
primal-dual methods are reviewed.
...
Chan–Golub–Mulet’s Primal-Dual Method
Some early work in dual and primal-dual methods for the ROF model can be found in
[, ]. In particular, Chan, Golub, and Mulet (CGM) [] introduced a primal-dual sys-
tem involving a primal variable u and a Fenchel dual variable p. It remains one of the


Numerical Methods and Applications in Total Variation Image Restoration
most efficient methods today and is perhaps the most intuitive one. It is worthwhile to
review it and see how it relates to the more recent methods. Their idea is to start with the
Euler–Lagrange equation of (> .):
KTKu −KT f −λdiv ⎛
⎝
∇u
√
∣∇u∣+ є
⎞
⎠= .
(.)
Owing to the singularity of the third term, they introduced an auxiliary variable
p =
∇u
√
∣∇u∣+ є
to form the system
p
√
∣∇u∣+ є = ∇u
KTKu −KT f −λdivp = .
Thus the blowup singularity is canceled. They proposed to solve this system by Newton’s
method which is well known to converge quadratically locally if the Jacobian of the system
is Lipschitz. Global convergence is observed when coupled with a simple Armijo line search
[]. The variable p is indeed the same as the Fenchel dual variable g in (> .) when ∇u ≠
and є = . Thus p is a smoothed version of the dual variable g. Without the introduction
of the dual variable, a direct application of the Newton’s method to the Euler–Lagrange
equation (> .) often fails to converge because of the small domain of convergence.
...
Chambolle’s Dual Method
A pure dual method is proposed by Chambolle in [], where the ROF objective is written
solely in terms of the dual variable. By the definition of TV in (> .), it can be deduced
using duality theory that
inf
u { 
∫Ω(Ku −f )dx + λ ∫Ω ∣∇u∣}
⇐⇒inf
u sup
∣p∣≤
{ 
∫Ω(Ku −f )dx + λ ∫Ω udivp dx}
(.)
⇐⇒sup
∣p∣≤
inf
u { 
∫Ω(Ku −f )dx + λ ∫Ω udivp dx}
⇐⇒sup
∣p∣≤
{−λ
∫Ω ∣K−Tdivp −f
λ∣

dx}.
(.)
The resulting problem has a quadratic objective with quadratic constraints. In contrast,
the primal objective is only piecewise smooth which is badly behaved when ∇u = . Thus
the dual objective function is very simple, but additional efforts are needed to handle the
constraints.

Numerical Methods and Applications in Total Variation Image Restoration 

One can write down the Karush–Kuhn–Tucker (KKT) optimality system [] of the dis-
cretized objective, which amounts to solving a nonlinear system of equations involving
complementarity conditions and inequality constraints on the Lagrange multipliers. Inter-
estingly, the Lagrange multipliers have a closed-form solution which greatly simplifies the
problem. More precisely, the KKT system consists of the equations
μp = H(p)
(.)
μ(∣p∣−) = 
(.)
μ ≥
(.)
∣p∣≤,
(.)
where μ is the nonnegative Lagrange multiplier and
H(p) := ∇[(KTK)
−divp −
λ K−f ] .
Since
μ∣p∣= ∣H(p)∣,
if ∣p∣= , then μ = ∣H(p)∣; if ∣p∣< , then the complementarity (> .) implies μ = and
by (> .) H(p) = , so that μ is also equal to ∣H(p)∣. This simplifies the KKT system
into a nonlinear system of p only:
∣H(p)∣p = H(p).
(.)
Chambolle proposes a simple semi-implicit scheme to solve the system:
pn+= pn + τH(pn)
pn + τ∣H(pn)∣.
Here τ is a positive parameter controlling the stepsize. The method is proven to be
convergent for any
τ ≤
∥(KTK)
−∥,
(.)
where ∥⋅∥is the spectral norm. This method is also faithful to the original ROF problem;
it does not require approximating the TV by smoothing.
The convergence rate of this method is at most linear but for denoising problems it usu-
ally converges fast (measured by the relative residual norm of the optimality condition)
in the beginning but stagnates after some iterations (at a level several orders of magni-
tude higher than the machine epsilon). This is very typical for simple relaxation methods.
Fortunately, visually good results (measured by the number of pixels having a grey level
different from the optimal one after they are quantized to their -bit representation) are
often achieved before the method stagnates []. However, when applied to deblurring,
K is usually ill conditioned, so that the step size restriction (> .) is too stringent. In
this case, another outer iteration is often used in conjunction with the method, see the
splitting methods in > Sect. ...


Numerical Methods and Applications in Total Variation Image Restoration
Chambolle’s method has been successfully adapted to solve a variety of related image
processing problems, e.g., the ROF with non-local TV [], multichannel TV [], and seg-
mentation problems []. We remark that many other approaches for solving (> .) have
been proposed. A discussion of some first-order methods including projected gradient
methods and Nesterov methods can be found in [, , ].
...
Primal-Dual Hybrid Gradient Method
As mentioned in the beginning of
> Sect. ., the primal and dual problems have their
own advantages and numerical difficulties to face. It is therefore tempting to combine the
best of both. In [], Zhu and Chan proposed the primal-dual hybrid gradient (PDHG)
algorithm which alternates between primal and dual formulations.
The method is based on the primal-dual formulation
G(u,p) := 
∫Ω(Ku −f )dx + λ ∫Ω udivp dx →inf
u sup
∣p∣≤
,
cf. formulation (> .). By fixing its two variables one at a time, this saddle point
formulation has two subproblems:
sup
∣p∣≤
G(u,p)
inf
u G(u,p).
While one may obtain an optimal solution by solving the two subproblems to a high accu-
racy alternatively, the PDHG method applies only one step of gradient descent/ascent
to each of the two subproblems alternatively. The rationale is that when neither
of the two variables are optimal, there is little to gain by iterating each subprob-
lem until convergence. Starting with an initial guess u, the following two steps are
repeated:
pk+= P∣p∣≤(pk −τk∇uk)
uk+= uk −θk(KT (Kuk −f ) + λdivpk+).
Here, P∣p∣≤is the projector onto the feasible set {p : ∣p∣≤}. The stepsizes τk and θk can
be chosen to optimize the performance. Some stepping strategies were presented in [].
In [], Zhu, Wright, and Chan studied a variety of stepping strategies for a related dual
method.
Numerical results in [] show that this simple algorithm is faster than the split
Bregman iteration (see
> Sect. ...), which is faster than Chambolle’s semi-implicit
dual method (see
> Sect. ...). Some interesting connections between the PDHG
algorithm and other algorithms such as proximal forward backward splitting, alternating
minimization, alternating direction method of multipliers, Douglas Rachford splitting, split
inexact Uzawa, and averaged gradient methods applied to different formulations of the
ROF model are studied by Esser et al. in []. Such connections reveal some convergence

Numerical Methods and Applications in Total Variation Image Restoration 

theory of the PDHG algorithm in several important cases (special choices of the stepsizes)
in a more general setting.
...
Semi-Smooth Newton’s Method
Given the dual problem, it is natural to consider other methods to solve its optimality
conditions (> .)–(> .). A standard technique in optimization to handle comple-
mentarity and Lagrange multipliers is to combine them into a single equality constraint.
Observe that the constraints a ≥, b ≥and ab = can be consolidated into the equality
constraint
ϕ(a, b) :=
√
a+ b−a −b = ,
(.)
where ϕ is known as the Fisher–Burmeister function. Therefore the KKT system (> .)–
(> .) can be written as
μp = H(p)
ϕ(μ,−∣p∣) = .
Ng et al. [] observed that this system is semi-smooth and therefore proposed solving this
system using a semi-smooth Newton’s method. In this method, if the Jacobian of the system
is not defined in the classical sense due to the system’s lack of enough smoothness, then
the Jacobian is replaced by a generalized Jacobian evaluated at a nearby point. It is proven
that this method converges superlinearly if the system to solve is at least semi-smooth and
if the generalized Jacobians at convergence satisfy some invertibility conditions. For the
dual problem (> .), the Newton’s equation may be singular. This problem is fixed by
regularizing the Jacobian.
...
Primal-Dual Active-Set Method
Hintermüller and Kunisch [] considered the Fenchel dual approach to formulate a con-
strained quadratic dual problem and derived a very effective active-set method to handle
the constraints. The method separates the variables into active and inactive sets, so that
they can be treated differently accordingly to their characteristics. They considered the
case of anisotropic discrete TV norm (> .), so that the dual variable is bilaterally con-
strained, i.e., −≤p ≤, whereas the constraints in (> .) are quadratic. In this setting,
superlinear convergence can be established.
To deal with the bilateral constraints on p, they proposed to use the Primal-Dual Active-
Set (PDAS) algorithm. Consider the general quadratic problem,
min
y,y≤ψ

⟨y, Ay⟩−⟨f , y⟩,


Numerical Methods and Applications in Total Variation Image Restoration
where ψ is a given vector in Rn. This problem includes (> .) as a special instance. The
KKT conditions are given by
Ay +
= f ,
⊙(ψ −y) = ,
≥,
ψ −y ≥,
where
is a vector of Lagrange multipliers and ⊙denotes the entrywise product. The idea
of the PDAS algorithm is to predict the active variables A and inactive variables I to speed
up the determination of the final active and inactive variables. The prediction is done by
comparing the closeness of
and ψ−y to zero. If ψ−y is c times closer to zero than
does,
then the variable is predicted as active. The PDAS algorithm is given by
. Initialize y,
. Set k = .
. Set I k = {i :
k
i −c(ψ −yk)i ≤} and Ak = {i :
k
i −c(ψ −yk)i > }.
. Solve
Ayk++
k+= f ,
yk+= ψ
on Ak,
k+= 
on I k.
. Stop, or set k = k + and return to Step .
Notice that the constraints a ≥, b ≥and ab = can be combined as a single equality
constraint:
min(a, cb) = 
for any positive constant c. Thus the KKT system can be written as
Ay +
= f ,
C(y, ) = ,
where C(y, ) = min( , c(ψ −y)) for an arbitrary positive constant c. The function C is
piecewise linear whereas the Fisher–Burmeister formulation (> .) is nonlinear. More
importantly, applying Newton’s method (using a generalized derivative) to such a KKT sys-
tem yields exactly the PDAS algorithm. This allows Hintermüller et al. to explain the local
superlinear convergence of the PDAS algorithm for a class of optimization problems that
include the dual of the anisotropic TV deblurring problem []. In [], some conditional
global convergence results based on the properties of the blurring matrix K have also been
derived. Their formulation is based on the anisotropic TV norm and the dual problem
requires an extra l regularization term when a deblurring problem is solved.
The dual problem (> .) is rank deficient and does not have a unique solution in
general. In [], Hintermüller and Kunisch proposed to add a regularization term, so that

Numerical Methods and Applications in Total Variation Image Restoration 

the solution is unique. The regularized objective function is
∫Ω ∣K−divp −λ−f ∣dx + γ ∫Ω ∣Pp∣dx,
where P is the orthogonal projector onto the null space of the divergence operator div.
Later in [], Hintermüller and Stadler showed that adding such a regularization term to
the dual objective is equivalent to smoothing out the singularity of the TV in the primal
objective. More precisely, the smoothed TV is given by ∫Ω Φ(∣∇f ∣) dx, where
Φ(s) = {s
if ∣s∣≥γ,
γ
+

γ s
if ∣s∣< γ.
An advantage of using this smoothed TV is that the staircase artifacts are reduced.
In [, ], Krishnan et al. considered the TV deblurring problem with bound con-
straints on the image u. An algorithm, called non-negatively constrained CGM, combining
the CGM and the PDAS algorithms has been proposed. The image u and its dual p are
treated as in the CGM method, whereas the bound constraints on u are treated as in the
PDAS method. The resulting optimality conditions are shown to be semi-smooth. The
scheme can also be interpreted as a semi-smooth quasi-Newton’s method and is proven
to converge superlinearly. The method is formulated for isotropic TV, but it can also
be applied to anisotropic TV after minor changes. However, Hintermüller and Kunisch’s
PDAS method [] can only be applied to anisotropic TV because they used PDAS that
can only handle linear constraints to treat the constraints on p.
..
Bregman Iteration
...
Original Bregman Iteration
The Bregman iteration is proposed by Osher et al. in [] for TV denoising. It has also been
generalized to solving many convex inverse problems, e.g., []. In each step, the signal
removed in the previous step is added back. This is shown to alleviate the loss of contrast
problem presented in the ROF model. Starting with the noisy image f= f , the following
steps are repeated for j = ,,, . . .:
. Set
uj+= arg min
u
{ 
∫Ω(u −fj)dx + λ ∫Ω ∣∇u∣} .
. Set fj+= fj + (f −u j+).
In the particular case when f consists of a disk over a constant background, it can be proved
that the loss of contrast can be totally recovered. Some theoretical analysis of the method
can be found in [].


Numerical Methods and Applications in Total Variation Image Restoration
For a general regularization functional J(u), the Bregman distance is defined as
Dp
J (u,v) = J(u) −J(v) −⟨p,u −v⟩,
where p is an element of the subgradient of J. In case of TV denoising, J(u) = λ ∫Ω ∣∇u∣.
Then, starting with f= f , the Bregman iteration is given by
. Set
u j+= arg min
u
{ 
∫Ω(u −f )dx + D
p j
J (u,u j)}.
. Set fj+= fj + (f −u j+).
. Set pj+= fj+−f .
In fact, steps and can be combined to pj+= pj + f −u j+without the need of keeping
track of fj. The above expression is for illustrating how the residual is added back to fj.
In this iteration, it has been shown that the Bregman distance between uj and the clean
image is monotonically decreasing as long as the L-distance is larger than the magnitude
of the noise component. But if one iterates until convergence, then uj →f , i.e., one just
gets the noisy image back. This counter-intuitive feature is indeed essential to solving other
TV minimization problems, e.g., the basis pursuit problem presented next.
...
The Basis Pursuit Problem
An interesting feature of the Bregman iteration is that, in the discrete setting, if one replaces
the term ∥u −f ∥in the objective by ∥Au −f ∥, where Au = f is underdetermined, then
upon convergence of the Bregman iterations, one obtains the solution of the following basis
pursuit problem []:
min
u {J(u)∣Au = f }.
When ∥Au −f ∥is used in the objective instead of ∥u −f ∥, the Bregman iteration is
given by:
. Set
u j+= arg min
u
{ 
∫Ω(Au −f )dx + D
p j
J (u,u j)}.
. Set fj+= fj + (f −Au j+).
. Set pj+= AT(fj+−f ).
...
Split Bregman Iteration
Recently, Goldstein and Osher [] proposed the split Bregman iteration which can be
applied to solve the ROF problem efficiently. The main idea is to introduce a new variable

Numerical Methods and Applications in Total Variation Image Restoration 

so that the TV minimization becomes a Lminimization problem which can be solved effi-
ciently by the Bregman iteration. This departs from the original Bregman iteration which
solves a sequence of ROF problems to improve the quality of the restored image by bring-
ing back the loss signal. The original Bregman iteration is not iterated until convergence.
Moreover, it assumes the availability of a basic ROF solver. The split Bregman method,
on the other hand, is an iterative method whose iterates converge to the solution of the
ROF problem. In this method, a new variable q = ∇u is introduced into the objective
function:
min
u,q { 
∫Ω(u −f )dx + λ ∫Ω ∣q∣dx}.
(.)
This problem is solved using a penalty method to enforce the constraint q = ∇u. The
objective with an added penalty is given by:
G(u,q) = α
∫Ω ∣q −∇u∣dx + 
∫Ω(u −f )dx + λ ∫Ω ∣q∣dx.
(.)
Notice that if the variables (u,q) are denoted by y, then the above objective can be
identified as
min
y
{α
∫Ω ∣Ay∣dx + J(y)},
where
Ay = q −∇u,
J(y) = 
∫Ω(u −f )dx + λ ∫Ω ∣q∣dx.
This is exactly the basis pursuit problem when α →∞. Actually, even with a fixed finite α,
as mentioned in
> Sect. ..., when the Bregman iteration is used, it converges to the
solution of the problem
min
y {J(y)∣Ay = },
so that the constraint q = ∇u is satisfied at convergence.
It is interesting to note that the split Bregman iteration can be viewed as a forward–
backward splitting method []. Yet another point of view is provided next.
...
Augmented Lagrangian Method
In [, ], it is recognized that the split Bregman iteration is an augmented Lagrangian
method []. This explains some good convergence behaviour of the split Bregman itera-
tion. To motivate the augmented Lagrangian method, consider a general objective function


Numerical Methods and Applications in Total Variation Image Restoration
J(u) with equality constraint H(u) = . The idea of penalty methods is to solve a sequence
of unconstrained problems
min
u {J(u) + 
β∥H(u)∥}
with β →+, so that the constraint H(u) = is enforced asymptotically. However, one may
run into the embarrassing situation where both H(u(β)) (where u(β) is the optimal u for
a given β) and β converge to zero in the limit. This could mean that the objective function
is stiff when β is very small. The idea of augmented Lagrangian methods is to use a fixed
parameter. But the penalty term is added to the Lagrangian function, so that the resulting
problem is equivalent to the original problem even without letting β →+. The augmented
Lagrangian function is
L(u, ) = J(u) +
⋅H(u) + 
β∥H(u)∥,
where
is a vector of Lagrange multipliers. Solving ∂L
∂u = ∂L
∂
= for a saddle point yields
exactly H(u) = for any β > . The Bregman iteration applied to the penalized objec-
tive (> .) is indeed computing a saddle point of the augmented Lagrangian function
of (> .) rather than optimizing (> .) itself. Therefore, the constraint ∇u = q
accompanied with (> .) is exact even with a fixed α.
..
Graph Cut Methods
Recently, there is a burst of interest in graph cut methods for solving various variational
problems. The promises of these methods are that they are fast for many practical prob-
lems and they can provide globally optimal solution even for “non-convex problems.” The
discussion below is extracted from [, ]. Readers are referred to [, ] and the refer-
ences therein for a more thorough discussion of the subject. Since graph cut problems are
combinatoric, the objective has to be cast in a fully discrete way. That is, not only the image
domain has to be discretized to a finite set but also the range of the intensity values has
be discretized to a finite set. Therefore, in this framework, the given m-by-n image f is a
function from Zm × Zn to ZK. The ROF problem thus becomes
F(u) = 

m
∑
i=
n
∑
j=
(ui,j, −fi,j)+ λ∥u∥TV →
min
u: Zm×Zn→ZK,
where ∥u∥TV is a discrete TV (> .). The next question is how to transform this problem
to a graph cut problem in such a way that it can be solved efficiently. It turns out that
the (fully discretized) ROF problem can be converted to a finite sequence of graph cut
problems. This is due to the co-area formula which is unique to TV. Details are described
next.

Numerical Methods and Applications in Total Variation Image Restoration 

...
Leveling the Objective
Some notations and basic concepts are in place. For simplicity, the following discrete TV
is adopted:
∥u∥TV =
m−
∑
i=
n−
∑
j=
∣ui+,j −ui,j∣+ ∣ui,j+−ui,j∣,
which is the anisotropic TV in (> .), but with the range of u restricted to ZK. Recall
that the binary image uk is defined such that each uk
i,j equals if ui,j ≤k and equals 
otherwise. Thus it is the kth lower level set of u. Then the co-area formula states that the
discrete TV can be written as
∥u∥TV =
K−
∑
k=
∥uk∥TV.
Thus it reduces to the TV of each “layer”. Note that the TV of the (K −)st level set must
be zero, and therefore, the above sum is only up to K −.
The fitting term in the objective can also be treated similarly as follows. Notice that for
any function gi,j(s), it holds that
gi,j(s) =
s−
∑
k=
[gi,j(k + ) −gi,j(k)] + gi,j()
=
K−
∑
k=
[gi,j(k + ) −gi,j(k)]χk<s + gi,j(),
where χk<s = if k < s and otherwise. Define gi,j(s) := 
(s −fi,j). Then,

(ui,j −fi,j)= gi,j(ui,j)
=
K−
∑
k=
[gi,j(k + ) −gi,j(k)] χk<ui, j + gi,j()
=
K−
∑
k=
[gi,j(k + ) −gi,j(k)](−uk
i,j) + gi,j().
As a result, the ROF objective can be expressed as
K−
∑
k=
⎧⎪⎪⎨⎪⎪⎩
∑
i,j
[gi,j(k + ) −gi,j(k)](−uk
i,j) + λ∥uk∥TV
⎫⎪⎪⎬⎪⎪⎭
+ C,
where C = ∑i,j gi,j().
By defining the objective function
F k(vk) = ∑
i,j
[gi,j(k + ) −gi,j(k)](−vk
i,j) + λ∥vk∥TV,


Numerical Methods and Applications in Total Variation Image Restoration
where vk is a binary function, the ROF problem is seen to be equivalent to
min
v,v,...,vK−
K−
∑
k=
F k(vk)
subject to the inclusion constraints vk
i,j ≤vk+
i,j
for all i, j, k. The constraints make sure
the binary functions {vk}k define the lower level sets of some function v. A very impor-
tant result is that the minimization can be done independently for each vk; amazingly, the
solutions {vk} satisfy the inclusion property automatically! See [] for further details.
...
Deﬁning a Graph
To minimize each F k w.r.t. a binary function vk, a graph cut method is used. First observe
that since gi,j(k) = 
(k −fi,j), F k can be simplified to
F k(vk) = ∑
i,j
[ 
−(k −fi,j)] (−vk
i,j) + λ∥vk∥TV.
By absorbing some constants and dropping the superscript on vk, the objective takes the
following form
F k(v) = ∑
i,j
(k −fi,j)vi,j + λ∥v∥TV.
(.)
Then, a graph with mn + nodes is constructed in the following way.
. Each of the mn pixels is a node, labeled by (i, j) for i = ,, . . . , m and j = ,, . . . , n.
. Add two additional nodes, called the source S and the sink T.
. For each (i, j), connect it to (i + , j) and (i, j + ) with capacity λ.
. For each (i, j), connect it to S with capacity fi,j −k if k−fi,j < , and to T with capacity
k −fi,j if k −fi,j > .
A cut (a.k.a. an st-cut) in the graph is a partition (S,T ) such that S ∈S and T ∈T .
The cost of the cut C(S,T ) is defined as the sum of the capacities of all edges from S to T .
For a given cut, let vi,j equals if (i, j) ∈S and equals if (i, j) ∈T . Then it can be verified
that
C(S,T ) = ∑
i,j
max{k −fi,j,}vi,j + max{fi,j −k,}(−vi,j) + λ∥vk∥TV,
which is the same as F k in (> .), up to the constant ∑i,j max{fi,j −k,}. Therefore,
computing the minimum cut is equivalent to minimizing (> .). It is also well known
that the minimum cut problem is equivalent to the maximum flow problem.
Recall that there are K−graphs to cut. A simple way is to do them one by one using any
classical maximum flow algorithm. But one can exploit the inclusion property to reduce
the work, for instance, see the divide-and-conquer algorithm proposed in [].

Numerical Methods and Applications in Total Variation Image Restoration 

In graph cut methods, a fundamental question is what kind of optimization problems
can be transformed to a graph cut problem. A particularly relevant question is whether
a function is levelable, i.e., its minimization can be done by first solving the simpler
problem on each of its level set, followed assembling the resulting level sets. Interest-
ingly, the only levelable convex regularization function (satisfying some very natural and
mild conditions) is TV []. This indicates that TV is much more than just an ordinary
semi-norm.
..
Quadratic Programming
The discrete anisotropic TV is a piecewise linear function. Fu et al. [] showed that by
introducing some auxiliary variables, one can transform the TV to a linear function but
with some additional linear constraints. Together with the fitting term, the problem to
solve has a quadratic objective function with linear constraints.
The objective function considered by is Fu et al.
F(u) = 
∥Ku −f ∥+ λ ∑
i,j
∣ui+,j −ui,j∣+ ∣ui,j+−ui,j∣,
which can also be written as
F(u) = 
∥Ku −f ∥+ λ∥Ru∥
where R is a mn-by-mn matrix. If the original isotropic TV is used, then it cannot be
written in this form.
The trick they used is to let v = Ru and then split it into positive and negative parts:
v+ = max(v,) and v−= max(−v,). Then, the objective can be written as
G(u,v+,v−) = 
∥Ku −f ∥+ λ(Tv+ + Tv−),
which is a quadratic function. But some linear constraints are added:
Ru = v+ −v−,
v+,v−≥.
Now, this problem can be solved by standard primal-dual interior-point methods. Here
“dual” refers to the Lagrange multipliers for the linear constraints. The major steps can be
summarized as follows:
. Write down the KKT system of optimality conditions, which has a form of f (x, μ, s) = 
where x ≥is the variable of the original problem (x = (u,v+,v−) in the present case); μ
is the Lagrange multipliers for the equality constraints; s ≥is the Lagrange multipliers
for the inequality constraints.
. Relax the complementarity xs = (part of f (x, μ, s) = ) to xs = , where
> .


Numerical Methods and Applications in Total Variation Image Restoration
. Solve the relaxed problem f (x, μ, s) = by Newton’s method.
. After each Newton’s iteration, reduce the value of
so that the solution of f (x, μ, s) = 
is obtained at convergence.
In this method, the relaxed complementarity xs =
forces the variables x, s to lie in the
interior of the feasible region. Once the variables are away from the boundary, the problem
becomes a nice unconstrained quadratic problem locally. The main challenge here is that
the linear system to solve in each Newton’s iteration becomes increasingly ill conditioned.
Under this framework, bound constraints such as umin ≤u ≤umax or any linear equality
constraints can be easily added.
..
Second-Order Cone Programming
The trick to “linearize” the TV presented in the last section does not work for isotropic TV.
Goldfarb and Yin [] proposed a second-order cone programming (SOCP) formulation
which works for the isotropic version (> .). Moreover, its connection to SOCP allows
the use of available SOCP solvers to obtain the solutions. The problem they considered is
the constrained ROF problem:
min
u ∥u∥TV
subject to
∥u −f ∥≤σ,
where σ is the standard deviation of the noise which is assumed to be known.
Let wx
i,j = ui+,j −ui,j and wy
i,j = ui,j+−ui,j. The TV becomes
∑
i,j
√
(wx
i,j)

+ (wy
i,j)

.
By introducing the variables v = f −u and t and the constraint
(wx
i,j)
+ (wy
i,j)

≤t
i,j,
the TV minimization problem becomes
min ∑
i,j
ti,j
s.t.
u + v = f
wx
i,j = ui+,j −ui,j
wy
i,j = ui,j+−ui,j
(σ,v) ∈conemn+
(ti,j,wx
i,j,wy
i,j) ∈cone.

Numerical Methods and Applications in Total Variation Image Restoration 

Here conen is the second-order cone in Rn:
{x ∈Rn : ∥(x, x, . . . , xn)∥≤x}.
The optimal solution satisfies
t
i,j = (wx
i,j)
+ (wy
i,j)

,
so that
∑
i,j
ti,j = ∑
i,j
√
(wx
i,j)

+ (wy
i,j)

= ∥u∥TV.
A SOCP formulation of the dual ROF problem is also given in [].
The SOCP can be solved by interior-point methods. The above formulation can be
slightly simplified by eliminating u. But the number of variables (hence the size of the
Newton’s equation) is still several times larger than the original problem. Goldfarb and Yin
proposed a domain decomposition method to split the large programming problem into
smaller ones, so that each subproblem can be solved efficiently. Of course, the convergence
rate of the method deteriorates as the domain is further split.
..
Majorization-Minimization
Majorization-Minimization (MM) (or Minorization-Maximization) [] is a well-studied
technique in optimization. The main idea is that at each step of the method, the objective
function is replaced by a simple one, called the surrogate function, such that its mini-
mization is easy to carry out and the result gives a smaller objective value of the original
problem. For a given objective, usually many surrogate functions are possible. In many
cases, one can even reduce multidimensional problems into a set of one-dimensional
problems. Methods of this class have been heavily used in statistics communities. Indeed
expectation-maximization (EM) algorithms are special cases of MM.
The use of MM to solving discrete TV problems can be traced back to the study of
emission and transmission tomography reconstruction problems by Lange and Carson in
[]. Recently, some authors have applied the method to solving TV deblurring prob-
lems []. However, the method is actually the same as the classical lagged diffusivity fixed
point iteration proposed by [] for the particular surrogate function used in []. Never-
theless, it is still worthy to present the framework here because other surrogate functions
can lead to different schemes.
Denote by uk the kth iterate. In this method, the surrogate function (majorizer)
Q(u∣uk) is defined such that
F(uk) = Q(uk∣uk)
F(u) ≤Q(u∣uk),
for all u.


Numerical Methods and Applications in Total Variation Image Restoration
Then, the next iterate is defined to be the minimizer of the surrogate function
uk+:= arg min
u
Q(u∣uk).
In this way, the following monotonic decreasing property holds:
F(uk+) ≤Q(uk+∣uk) ≤Q(uk∣uk) = F(uk).
Presumably, the function Q should be chosen so that its minimum is easy to compute.
In many applications, it may even be chosen to have a separable form
Q(u∣uk) = Q(u∣uk) + Q(u∣uk) + ⋯+ Qn(un∣uk),
so that its minimization reduces to n’s one-dimensional (D) problems. A promise of this
method is that each iteration is very easy to carry out, which compensates its linear-only
convergence.
To construct a surrogate QTV for TV, first note that
√a = (
√
b)(
√a
√
b
) ≤
√
b
+
a

√
b
for all a, b ≥. Let Dx and Dy be the forward difference operator in x and in y directions
respectively. Then,
∥u∥TV = ∑
i,j
√
(Dxui,j)+ (Dyui,j)
≤
∑
i,j
√
(Dxuk
i,j)

+ (Dyuk
i,j)

+ 
∑
i,j
(Dxui,j)+ (Dyui,j)
√
(Dxuk
i,j)

+ (Dyuk
i,j)
.
The surrogate is thus defined as
QTV(u∣uk) = 
∥uk∥TV + 
∑
i,j
(Dxui,j)+ (Dyui,j)
√
(Dxuk
i,j)

+ (Dyuk
i,j)

which is quadratic in u. Notice that the D discrete gradient matrix is given by
∇= [∇n ⊗Im
In ⊗∇m
],
where ∇m is the m-by-m D forward difference matrix (under Neumann boundary
conditions)
∇m =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−

−

⋱
⋱
−


⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

Numerical Methods and Applications in Total Variation Image Restoration 

Let λk
i,j = /
√
(Dxuk
i,j)

+ (Dyuk
i,j)

and let
Λk = diag(λk
,, . . . , λk
m,n, λk
,, . . . , λk
m,n).
The surrogate becomes
QTV(u∣uk) = 
∥uk∥TV + 
uT∇TΛk∇u.
In this case, the minimization of QTV cannot be reduced to a set of D problems. But it
does become quadratic.
Finally, the majorizer for the ROF model is:
Q(u∣uk) = 
∥Ku −f ∥+ λQTV(u∣uk).
While this method completely bypasses the need to optimize the TV term directly, each
iteration requires solving the linear system
(KTK + λ∇TΛk∇)uk+= KT f .
This scheme is exactly the lagged diffusivity fixed point iteration. Assume that K is full
rank, then the linear system is positive definite. A standard way is to use preconditioned
conjugate gradient to solve. Many preconditioners have been proposed for this problem in
the s, e.g., cosine transform, multigrid and multiplicative operator splitting, see []
and the references therein. However, due to the highly varying coefficients in Λk, it can be
non-trivial to solve efficiently.
..
Splitting Methods
Recently there have been several proposals for solving TV deblurring problems based on
the idea of separating the deblurring process and the TV regularization process. Many of
them are based on the idea that the minimization of an objective of the form
F(u) = J(u) + J(Au),
with Aa linear operator, can be approximated by the minimization of either of the following
two objectives
G(u,v) = J(u) + α
∥u −v∥+ J(Av),
G(u,v) = J(u) + α
∥Au −v∥+ J(v),
where α is a large scalar. Then G is minimized w.r.t. u and v alternatively. In this way, at
each iteration, the minimization of Jand Jare done separately. The same idea can be
generalized to split an objective with n terms to an objective with n variables.


Numerical Methods and Applications in Total Variation Image Restoration
Consider the discrete ROF model:
F(u) = 
∥Ku −f ∥+ λ∥∣∇u∣∥.
Huang et al. [] and Bresson and Chan [] considered the splitting
G(u,v) = 
∥Ku −f ∥+ α
∥u −v∥+ λ∥∣∇v∣∥.
In this case, the minimization w.r.t. u becomes
(KTK + αI)u = KT f + αv,
which can be solved with Fast Fourier Transform (FFT) in O(N log N) operations when
the blurring matrix K can be diagonalized by a fast transform matrix. The minimization
w.r.t. v is the ROF denoising problem which can be solved using any of the aforementioned
denoising method. Both [] and [] employed Chambolle’s dual algorithm. The point is
that solving TV denoising is much easier than solving TV deblurring (directly). Moreover,
some algorithms such as those based on graph cut cannot be applied to deblurring directly.
The reason is that the pixel values in the fitting are no longer separable, which in turn makes
the fitting term not “levelable.” However, using the splitting technique, one can now apply
graph cut methods to solve each denoising problem.
This method is generally very fast. Moreover, it often works for a large range of α. But
when α is too large, the Chambolle’s iteration may slow down. This splitting method has
also been applied to other image processing problems such as segmentation [].
An alternative splitting is proposed by Wang et al. []. The bivariate function they used
is given by
G(u,v) = 
∥Ku −f ∥+ α
∥∣∇u −v∣∥+ λ∥∣v∣∥.
The minimization w.r.t. u requires solving
(KTK −αΔ)u = KT f + αv,
where Δ is the D Laplacian. This equation can again be solved with FFT in O(N log N)
operations. The minimization w.r.t. v is decoupled into N minimization problems (one
for each pixel) of two variables. A simple closed-form solution for the D minimization
problems is available. Therefore, the computation cost per iteration is even less than the
approach taken in [] and []. Remark that this objective is indeed the same as the split
Bregman method (> .). A difference is that when the split Bregman iteration con-
verges, it holds exactly that ∇u = v. But the simple alternating minimization used in most
splitting methods does not guarantee ∇u = v at convergence.
An alternative splitting is introduced by Bect et al. in []. It is based on the observation
that, for any symmetric positive definite matrix B with ∥B∥< , it holds that
⟨Bv,v⟩= min
u∈RN {∥u −v∥+ ⟨Cu,u⟩}

Numerical Methods and Applications in Total Variation Image Restoration 

for all v ∈RN, where C = B(I −B)−. Then, the ROF model can be formulated as the
minimization of the following bivariate function:
G(u,v) = 
μ (∥u −v∥+ ⟨Cu,u⟩) + 
(∥f ∥−⟨Kv, f ⟩) + λ∥∣∇v∣∥,
where μ > such that μ∥KTK∥< and B = μKTK. The minimization of G w.r.t. u has a
closed-form solution u = (I −B)v = (I −μKTK)v. The minimization of G w.r.t. v is a TV
denoising problem. At convergence, the minimizer of F is exactly recovered. An interesting
property of this splitting is that it does not involve any matrix inversion in the alternating
minimization of G.
.
Conclusion
In this chapter, some recent developments of numerical methods for TV minimization and
their applications are reviewed. The chosen topics only reflect the interest of the authors
and are by no means comprehensive. It is also hoped that this chapter can serve as a guide
to recent literature on some of these recent developments.
.
Cross-References
> Compressive Sensing
> Duality and Convex Minimization
> Iterative Solution Methods
> Large Scale Inverse Problems
> Mumford-Shah, Phase Field Models
> Regularization Methods for III-posed Problems
> Total Variation in Imaging
> Variational Approach in Image Analysis
References and Further Reading
. Acar A, Vogel C () Analysis of bounded vari-
ation penalty methods for ill-posed problems.
Inverse Probl ():–
. Adams R, Fournier J () Sobolev spaces, vol
of Pure and applied mathematics, nd edn.
Academic, New York
. Aujol J-F () Some first-order algorithms for
total variation based image restoration. J Math
Imaging Vis ():–
. Aujol J-F, Gilboa G, Chan T, Osher S ()
Structure-texture
image
decomposition
–
modeling, algorithms, and parameter selection.
Int J Comput Vis ():–
. Bect J, Blanc-Féraud L, Aubert G, Chambolle
A () A l -unified variational framework
for image restoration. In Proceedings of ECCV,
vol of Lecture notes in computer sciences,
pp –


Numerical Methods and Applications in Total Variation Image Restoration
. Bioucas-Dias J, Figueiredo M, Nowak R ()
Total variation-based image deconvolution: a
majorization-minimization approach. In Pro-
ceedings of IEEE international conference on
acoustics, speech and signal processing ICASSP
, vol , pp –
. Blomgren P, Chan T () Color TV: total vari-
ation methods for restoration of vector-valued
images. IEEE Trans Image Process :–
. Boyd
S,
Vandenberghe
L
()
Convex
optimization.
Cambridge
University
Press,
Cambridge
. Bresson X, Chan T () Non-local unsuper-
vised variational image segmentation models.
UCLA CAM Report, –
. Bresson X, Chan T () Fast dual minimization
of the vectorial total variation norm and appli-
cations to color image processing. Inverse Probl
Imaging ():–
. Buades A, Coll B, Morel J () A review of
image denoising algorithms, with a new one.
Multiscale Model Simulat ():–
. Burger M, Frick K, Osher S, Scherzer O ()
Inverse total variation flow. Multiscale Model
Simulat ():–
. Carter J () Dual methods for total variation-
based image restoration. Ph.D. thesis, UCLA,
Los Angeles, CA, USA
. Chambolle A () An algorithm for total varia-
tionminimizationandapplications.J Math Imag-
ing Vis :–
. Chambolle A, Darbon J () On total varia-
tion minimization and surface evolution using
parametric maximum flows. Int J Comput Vis
():–
. Chambolle A, Lions P () Image recovery via
total variation minimization and related prob-
lems. Numer Math :–
. Chan R, Chan T, Wong C () Cosine trans-
form based preconditioners for total variation
deblurring. IEEE Trans Image Process :–

. Chan R, Wen Y, Yip A () A fast opti-
mization transfer algorithm for image inpainting
in wavelet domains. IEEE Trans Image Process
():–
. Chan T, Vese L () Active contours without
edges. IEEE Trans Image Process ():–
. Chan T, Golub G, Mulet P () A nonlin-
ear primal-dual method for total variation-based
image restoration. SIAM J Sci Comp :–

. Chan T, Esedo¯glu S, Park F, Yip A () Recent
developments in total variation image restora-
tion. In: Paragios N, Chen Y, Faugeras O (eds)
Handbook of mathematical models in computer
vision. Springer, Berlin, pp –
. Chan T, Esedo¯glu S, Nikolova M (a) Algo-
rithms for finding global minimizers of image
segmentation and denoising models. SIAM J
Appl Math ():–
. Chan T, Shen J, Zhou H (b) Total varia-
tion wavelet inpainting. J Math Imaging Vis ():
–
. Chan T, Ng M, Yau C, Yip A () Superreso-
lution image reconstruction using fast inpainting
algorithms. Appl Comput Harmon Anal ():
–
. Christiansen O, Lee T, Lie J, Sinha U, Chan T
() Total variation regularization of matrix-
valued images. Int J Biomed Imaging :
. Combettes P, Wajs V () Signal recovery by
proximal forward-backward splitting. Multiscale
Model Simulat ():–
. Darbon J, Sigelle M () Image restoration
with discrete constrained total variation part I:
Fast and exact optimization. J Math Imaging Vis
:–
. EfrosA,LeungT () Texturesynthesisbynon-
parametric sampling. In: Proceedings of the IEEE
international conference on computer vision, vol
, Corfu, Greece, pp –
. Esser E, Zhang X, Chan T () A general
framework for a class of first order primal-dual
algorithms for TV minimization. UCLA CAM
Report, –
. Fu H, Ng M, Nikolova M, Barlow J () Effi-
cient minimization methods of mixed l-land
l-lnorms for image restoration. SIAM J Sci
Comput ():–
. Gilboa G, Osher S () Nonlocal operators
with applications to image processing. Multiscale
Model Simulat ():–
. Giusti E () Minimal surfaces and functions
of bounded variation. Birkhäuser, Boston
. Glowinki R, Le Tallec P () Augmented
Lagrangians and operator-splitting methods in
nonlinear mechanics. SIAM, Philadelphia
. Goldfarb D, Yin W () Second-order cone
programming methods for total variation based

Numerical Methods and Applications in Total Variation Image Restoration 

image restoration. SIAM J Sci Comput ():
–
. Goldstein T, Osher S () The split Bregman
method for l-regularization problems. SIAM J
Imaging Sci ():–
. Hintermüller
M,
Kunisch
K
()
Total
bounded variation regularization as a bilaterally
constrained optimisation problem. SIAM J Appl
Math :–
. Hintermüller M, Stadler G () A primal-
dual algorithm for TV-based inf-convolution-
type image restoration. SIAM J Sci Comput :
–
. Hintermüller M, Ito K, Kunisch K () The
primal-dual active set strategy as a semismooth
Newton’s method. SIAM J Optim ():–
. Huang Y, Ng M, Wen Y () A fast total varia-
tion minimization method for image restoration.
Multiscale Model Simulat ():–
. Kanwal RP () Generalized functions: theory
and applications. Birkhäuser, Boston
. Krishnan D, Lin P, Yip A () A primal-dual
active-set method for non-negativity constrained
total variation deblurring problems. IEEE Trans
Image Process ():–
. Krishnan D, Pham Q, Yip A () A primal dual
active set algorithm for bilaterally constrained
total variation deblurring and piecewise con-
stant Mumford-Shah segmentation problems.
Adv Comput Math (–):–
. Lange K () Optimization. Springer, New
York
. Lange K, Carson R () EM reconstruction
algorithms for emission and transmission tomog-
raphy. J Comput Assist Tomogr :–
. Law Y, Lee H, Yip A () A multi-resolution
stochastic level set method for Mumford-Shah
image segmentation. IEEE Trans Image Process
():–
. LeVeque R () Numerical methods for con-
servation laws, nd edn. Birkhäuser, Basel
. Mumford D, Shah J () Optimal approxima-
tion by piecewise smooth functions and associ-
ated variational problems. Commun Pure Appl
Math :–
. Ng M, Qi L, Tang Y, Huang Y () On semis-
mooth Newton’s methods for total variation min-
imization. J Math Imaging Vis ():–
. Osher S, Burger M, Goldfarb D, Xu J, Yin W
() An iterative regularization method for
total variation based image restoration. Multi-
scale Model Simulat :–
. Royden H () Real analysis, rd edn. Prentice-
Hall, Englewood Cliffs
. Rudin L, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D :–
. Sapiro G, Ringach D () Anisotropic diffusion
of multivalued images with applications to color
filtering. IEEE Trans Image Process :–
. Setzer
S
()
Split
Bregman
algorithm,
Douglas-Rachford splitting and frame shrinkage.
In: Proceedings of scale-space , pp –
. Setzer S, Steidl G, Popilka B, Burgeth B ()
Variational methods for denoising matrix fields.
In: Laidlaw D, Weickert J (eds) Visualization and
processing of tensor fields: advances and perspec-
tives, mathematics and visualization. Springer,
Berlin, pp –
. Shen J, Kang S () Quantum TV and applica-
tion in image processing. Inverse Probl Imaging
():–
. Strang G () Maximal flow through a domain.
Math Program ():–
. Tschumperlé D, Deriche R () Diffusion ten-
sor regularization with constraints preservation.
In: Proceedings of IEEE computer soci-
ety conference on computer vision and pattern
recognition, vol , Kauai, Hawaii, pp –.
IEEE Computer Science Press
. Vogel C, Oman M () Iteration methods
for total variation denoising. SIAM J Sci Comp
:–
. Wang, Y, Yang J, Yin W, Zhang Y () A new
alternating minimization algorithm for total vari-
ation image reconstruction. SIAM J Imaging Sci
():–
. Wang Z, Vemuri B, Chen Y, Mareci T () A
constrained variational principle for direct esti-
mation and smoothing of the diffusion tensor
field from complex DWI. IEEE Trans Med Imag-
ing ():–
. Weiss P, Aubert G, Blanc-Fèraud L () Effi-
cient schemes for total variation minimization
under constraintsinimageprocessing.SIAMJ Sci
Comput ():–
. Wu C, Tai XC () Augmented Lagrangian
method, dual methods, and split Bregman itera-
tion for ROF, vectorial TV, and high order mod-
els. UCLA CAM Report, –


Numerical Methods and Applications in Total Variation Image Restoration
. Yin
W,
Osher
S,
Goldfarb
D,
Darbon
J
() Bregman iterative algorithms for l-
minimization with applications to compressed
sensing. SIAM J Imaging Sci ():–
. Zhu M, Chan T () An efficient primal-
dual hybrid gradient algorithm for total variation
image
restoration.
UCLA
CAM
Report,
–
. Zhu M, Wright SJ, Chan TF (to appear) Duality-
based algorithms for total-variation-regularized
image restoration. Comput Optim Appl

Mumford and Shah Model
and its Applications to
Image Segmentation and
Image Restoration
Leah Bar ⋅Tony F. Chan ⋅Ginmo Chung ⋅Miyoun Jung ⋅
Nahum Kiryati ⋅Rami Mohieddine ⋅Nir Sochen ⋅
Luminita A. Vese
.
Introduction: Description of the Mumford and Shah Model.................
.
Background: The First Variation..............................................
..
Minimizing in u with K Fixed.......................................................
..
Minimizing in K........................................................................
.
Mathematical Modeling and Analysis: The Weak Formulation
of the Mumford and Shah Functional.........................................
.
Numerical Methods: Approximations to the Mumford
and Shah Functional...........................................................
..
Ambrosio and Tortorelli Phase-Field Elliptic Approximations..................
...Approximations of the Perimeter by Elliptic Functionals........................
...Ambrosio-Tortorelli Approximations...............................................
..
Level Set Formulations of the Mumford and Shah Functional..................
...Piecewise-Constant Mumford and Shah Segmentation Using Level Sets......
...Piecewise-Smooth Mumford and Shah Segmentation Using Level Sets........
...Extension to Level Set Based Mumford–Shah Segmentation
with Open Edge Set K.................................................................
.
Case Examples: Variational Image Restoration with Segmentation-Based
Regularization..................................................................
..
Non-blind Restoration.................................................................
..
Semi-Blind Restoration................................................................
..
Image Restoration with Impulsive Noise............................................
..
Color Image Restoration..............................................................
..
Space-Variant Restoration.............................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
..
Level Set Formulations for Joint Restoration
and Segmentation. .....................................................................
..
Image Restoration by Nonlocal Mumford–Shah Regularizers...................
.
Conclusion.......................................................................
.
Recommended Reading....................................l.....................

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Abstract: We present in this chapter an overview of the Mumford and Shah model for
image segmentation. We discuss its various formulations, some of its properties, the math-
ematical framework, and several approximations. We also present numerical algorithms
and segmentation results using the Ambrosio–Tortorelli phase-field approximations on
one hand, and using the level set formulations on the other hand. Several applications of
the Mumford–Shah problem to image restoration are also presented.
.
Introduction: Description of the Mumford and Shah
Model
An important problem in image analysis and computer vision is the segmentation one, that
aims to partition a given image into its constituent objects, or to find boundaries of such
objects. This chapter is devoted to the description, analysis, approximations, and appli-
cations of the classical Mumford and Shah functional proposed for image segmentation.
In [–], David Mumford and Jayant Shah have formulated an energy minimization
problem that allows to compute optimal piecewise-smooth or piecewise-constant approx-
imations u of a given initial image g. Since then, their model has been analyzed and
considered in depth by many authors, by studying properties of minimizers, approxima-
tions, and applications to image segmentation, image partition, image restoration, and
more generally to image analysis and computer vision.
We denote by Ω ⊂Rd the image domain (an interval if d = , or a rectangle in the
plane if d = ). More generally, we assume that Ω is open, bounded, and connected. Let
g : Ω →R be a given gray-scale image (a signal in one dimension, a planar image in two
dimensions, or a volumetric image in three dimensions). It is natural and without losing
any generality to assume that g is a bounded function in Ω, g ∈L∞(Ω).
As formulated by Mumford and Shah [], the segmentation problem in image analysis
and computer vision consists in computing a decomposition
Ω = Ω∪Ω∪. . . ∪Ωn ∪K
of the domain of the image g such that
(a) The image g varies smoothly and/or slowly within each Ωi.
(b) The image g varies discontinuously and/or rapidly across most of the boundary K
between different Ωi.
From the point of view of approximation theory, the segmentation problem may be
restated as seeking ways to define and compute optimal approximations of a general func-
tion g(x) by piecewise-smooth functions u(x), i.e., functions u whose restrictions ui to
the pieces Ωi of a decomposition of the domain Ω are continuous or differentiable.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
In what follows, Ωi will be disjoint connected open subsets of a domain Ω, each one
with a piecewise-smooth boundary, and K will be a closed set, as the union of boundaries
of Ωi inside Ω, thus
Ω = Ω∪Ω∪. . . ∪Ωn ∪K,
K = Ω ∩(∂Ω∪. . . ∪∂Ωn).
The functional E to be minimized for image segmentation is defined by [–],
E(u, K) = μ∫Ω(u −g)dx + ∫Ω/K ∣∇u∣dx + ∣K∣,
(.)
where u : Ω →R is continuous or even differentiable inside each Ωi (or u ∈H(Ωi)) and
may be discontinuous across K. Here, ∣K∣stands for the total surface measure of the hyper-
surface K (the counting measure if d = , the length measure if d = , the area measure
if d = ). Later, we will define ∣K∣by Hd−(K), the d −dimensional Hausdorff measure
in Rd.
As explained by Mumford and Shah, dropping any of these three terms in (> .),
inf E = : without the first, take u = , K = /; without the second, take u = g, K = /;
without the third, take for example, in the discrete case K to be the boundary of all pixels
of the image g, each Ωi be a pixel and u to be the average (value) of g over each pixel. The
presence of all three terms leads to nontrivial solutions u, and an optimal pair (u, K) can
be seen as a cartoon of the actual image g, providing a simplification of g.
An important particular case is obtained when we restrict E to piecewise-constant
functions u, i.e., u = constant ci on each open set Ωi. Multiplying E by μ−, we have
μ−E(u, K) = ∑
i ∫Ωi
(g −ci)dx +
∣K∣,
where
= /μ. It is easy to verify that this is minimized in the variables ci by setting
ci = meanΩi(g) = ∫Ωi g(x)dx
∣Ωi∣
,
where ∣Ωi∣denotes here the Lebesgue measure of Ωi (e.g., area if d = , volume if d = ),
so it is sufficient to minimize
E(K) = ∑
i ∫Ωi
(g −meanΩi g)dx +
∣K∣.
It is possible to interpret Eas the limit functional of E as μ →[].
Finally, the Mumford and Shah model can also be seen as a deterministic refinement
of Geman and Geman’s image restoration model [].
.
Background: The First Variation
In order to better understand, analyze, and use the minimization problem (> .), it is
useful to compute its first variation with respect to each of the unknowns.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

We first recall the definition of Sobolev functions u ∈W,(U) [], necessary to
properly define a minimizer u when K is fixed.
Deﬁnition 
Let U ⊂Rd be an open set. We denote by W,(U) (or by H(U)) the set of
functions u ∈L(Ω), whose first-order distributional partial derivatives belong to L(U).
This means that there are functions u, . . . ,ud ∈L(U) such that
∫U u(x) ∂φ
∂xi
(x)dx = −∫U ui(x)φ(x)dx
for ≤i ≤d and for all functions φ ∈C∞
c (U).
We may denote by ∂u
∂xi the distributional derivative ui of u and by ∇u = ( ∂u
∂x, . . . , ∂u
∂xd )
its distributional gradient. In what follows, we denote by ∣∇u∣(x) the Euclidean norm of
the gradient vector at x. H(U) = W,(U) becomes a Banach space endowed with the
norm
∥u∥W,(U) = [∫U udx +
d
∑
i=∫U ( ∂u
∂xi
)

dx]
/
.
..
Minimizing in u with K Fixed
Let us assume first that K is fixed, as a closed subset of the open and bounded set Ω ⊂Rd,
and denote by
E(u) = μ∫Ω/K(u −g)dx + ∫Ω/K ∣∇u∣dx,
for u ∈W,(Ω/K), where Ω/K is open and bounded, and g ∈L(Ω/K). We have the
following classical results obtained as a consequence of the standard method of calculus of
variations.
Proposition 
There is a unique minimizer of the problem
inf
u∈W,(Ω/K) E(u).
(.)
Proof
[] First, we note that ≤inf E < +∞, since we can choose u≡and
E(u) = μ∫Ω/K g(x)dx < +∞. Thus, we can denote by m = inf u E(u) and let
{u j}j≥∈W,(Ω/K) be a minimizing sequence such that limj→∞E(u j) = m.
Recall that for u,v ∈L,
∥u + v

∥


+ ∥u −v

∥


= 
∥u∥

+ 
∥v∥

,
and so
∥u + v

∥


= 
∥u∥

+ 
∥v∥

−∥u −v

∥


.
(.)


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Let u,v ∈W,(Ω/K), thus E(u), E(v) < ∞, and apply (> .) to u −g and v −g, and
then to ∇u and ∇v; we obtain
E (u + v

) = 
E(u) + 
E(v) −μ
∫Ω/K ∣u −v∣dx −
∫Ω/K ∣∇(u −v)∣dx
= 
E(u) + 
E(v) −
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
μ
∥u −v∥
W,(Ω/K) + (−μ
)∥∇(u −v)∥
if 
≥μ


∥u −v∥
W,(Ω/K) + ( μ
−)∥u −v∥
if 
≤μ

.
(.)
If we choose u,v ∈W,(Ω/K), such that E(u), E(v) ≤m + є, then
m ≤E (u + v

) ≤m + є −
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
μ
∥u −v∥
W,(Ω/K) + (−μ
)∥∇(u −v)∥
if 
≥μ


∥u −v∥
W,(Ω/K) + ( μ
−)∥u −v∥
if 
≤μ

thus,
∥u −v∥
W,(Ω/K) ≤
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
є
μif 
≥μ

є if 
≤μ

.
(.)
We let wj = u j −u. From (> .), {wj} is a Cauchy sequence in W,(Ω/K); let w denote
its limit and set u= u+ w. Then
E(u) = μ∥u−g∥
+ ∥∇u∥
= μ∥(u−g) + w∥
+ ∥∇u+ ∇w∥

= lim
j→+∞[μ∥(u−g) + wj∥
+ ∥∇u+ ∇wj∥
]
= lim
j→+∞E(u j) = m,
by the continuity of L-norms. This shows the existence of minimizers. The uniqueness
follows from (> .) by taking є = .
∎
Proposition 
The unique solution u of (> .) is solution of the elliptic problem
∫Ω/K ∇u(x) ⋅∇v(x)dx = −μ∫Ω/K[u(x) −g(x)]v(x)dx, ∀v ∈W,(Ω/K),
(.)
or of
△u = μ(u −g)

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

in the sense of distributions in Ω/K, with associated boundary condition ∂u
∂⃗N = on ∂(Ω/K),
where ⃗N is the exterior unit normal to the boundary.
Proof
Indeed, let є ↦A(є) = E(u + єv) for є ∈R and arbitrary v ∈W,(Ω/K). Then A
is a quadratic function of є, given by
A(є) = μ∫Ω/K(u −g)dx + єμ∫Ω/K vdx + єμ∫Ω/K(u −g)vdx
+ ∫Ω/K ∣∇u∣dx + є∫Ω/K ∣∇v∣dx + є ∫Ω/K ∇u ⋅∇vdx,
and we have
A′(є) = єμ∫Ω/K vdx + μ∫Ω/K(u −g)vdx + є ∫Ω/K ∣∇v∣dx
+ ∫Ω/K ∇u ⋅∇vdx,
and
A′() = μ∫Ω/K(u −g)vdx + ∫Ω/K ∇u ⋅∇vdx.
Since we must have E(u) = A() ≤A(є) = E(u + єv) for all є ∈R and all v ∈W,(Ω/K),
we impose A′() = for all such v, which yields the weak formulation (> .).
If in addition u would be a strong classical solution of the problem, or if it would belong
to W,(Ω/K), then integrating by parts in the last relation we obtain
A′() = μ∫Ω/K(u −g)vdx −∫Ω/K(△u)vdx + ∫∂(Ω/K) ∇u ⋅⃗Nvds = .
Taking now v ∈C
(Ω/K) ⊂W,(Ω/K), we obtain
△u = μ(u −g) in Ω/K.
Using this and taking now v ∈C(Ω/K), we deduce the associated implicit boundary
condition ∇u ⋅⃗N =
∂u
∂⃗N = on the boundary of Ω/K (in other words, on the boundary
of Ω and of each Ωi).
∎
Assume now that g ∈L∞(Ω/K), which is not a restrictive assumption when g repre-
sents an image. We can deduce that the unique minimizer u of (> .) satisfies ∥u∥∞≤
∥g∥∞(as expected, due to the smoothing properties of the energy). To prove this, we first
state the following classical lemma (see e.g., ref.[], Chapter A).
Lemma 
If Ω/K is open, and if u ∈W,(Ω/K), then u+ = max(u,) also lies in
W,(Ω/K) and ∣∇u+(x)∣≤∣∇u(x)∣almost everywhere.
Now let u∗(x) = max{−∥g∥∞,min(∥g∥∞,u(x))} be the obvious truncation of u.
Lemma implies that u∗∈W,(Ω/K) and that ∫Ω/K ∣∇u∗(x)∣dx ≤∫Ω/K ∣∇u(x)∣dx. We
also obviously have ∫Ω/K(u∗−g)dx ≤∫Ω/K(u−g)dx, and we deduce that E(u∗) ≤E(u).
But u is the unique minimizer of E, thus u(x) = u∗(x) almost everywhere and we deduce
∥u∥∞≤∥g∥∞.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Remark 
Several classical regularity results for a weak solution u of (> .) can be
stated:
●
If g ∈L∞(Ω/K), then u ∈C
loc(Ω/K) (see e.g., ref.[], Chapter A).
●
If g ∈L(Ω/K), then u ∈W,
loc (Ω/K) = H
loc(Ω/K), which implies that u solves the
PDE (see e.g., ref.[], Chapter .).
△u = μ(u −g) a.e. in Ω/K.
..
Minimizing in K
We wish to formally compute here the first variation of E(u, K) with respect to K. Let us
assume that (u, K) is a minimizer of E from (> .), and we vary K. Let us assume that
locally, K ∩U is the graph of a regular function ϕ, where U is a small neighborhood near a
regular, simple point P of K. Without loss of generality, we can assume that U = D×I where
I is an interval in R and K ∩U = {(x, x, . . . , xd) ∈U = D × I : xd = ϕ(x, . . . , xd−)}. Let
u+ denote the restriction of u to
U+ = {(x, x, . . . , xd) : xd > ϕ(x, . . ., xd−)} ∩U,
and u−the restriction of u to
U−= {(x, x, . . . , xd) : xd < ϕ(x, . . ., xd−)} ∩U,
and choose Hextensions of u+ from U+ to U, and of u−from U−to U. For small є, define
a deformation Kє of K inside U as the graph of
xd = ϕ(x, . . . , xd−) + єψ(x, . . . , xd−),
such that ψ is regular and zero outside D, and Kє = K outside U. Define
uє(x) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
u(x)
if
x ∉U,
(extension of u+)(x)
if
x ∈U, x above Kє ∩U
(extension of u−)(x)
if
x ∈U, x below Kє ∩U.
Now, using z = (x, . . . , xd−),
E(uє, Kє) −E(u, K) = μ∫U [(uє −g)dx −(u −g)] dx
+ ∫U/Kє
∣∇uє∣dx −∫U/K ∣∇u∣dx +
[∣Kє ∩U∣−∣K ∩U∣]
= μ∫D (∫
ϕ(z)+єψ(z)
ϕ(z)
[(u−−g)−(u+ −g)] dxd)dz
+ ∫D (∫
ϕ(z)+єψ(z)
ϕ(z)
[∣∇u−∣−∣∇u+∣] dxd) dz
+
∫D [
√
+ ∣∇(ϕ + єψ)∣−
√
+ ∣∇ϕ∣] dz.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Thus,
lim
є→
E(uє, Kє) −E(u, K)
є
= μ∫D [(u−−g)−(u+ −g)]∣
xd=ϕ(z)ψ(z)dz
+ ∫D [∣∇u−∣−∣∇u+∣]∣
xd=ϕ(z)ψ(z)dz +
∫D
∇ϕ ⋅∇ψ
√
+ ∣∇ϕ∣dz = 
for all such ψ, since (u, K) is a minimizer. Integrating by parts, we formally obtain for
all ψ:
∫D
⎧⎪⎪⎨⎪⎪⎩
[(μ(u−−g)+ ∣∇u−∣) −(μ(u+ −g)+ ∣∇u+∣)]∣
xd=ϕ(z)
−div ⎛
⎝
∇ϕ
√
+ ∣∇ϕ∣
⎞
⎠
⎫⎪⎪⎬⎪⎪⎭
ψ(z)dz = ,
and we obtain the first variation with respect to K,
[μ(u−−g)+ ∣∇u−∣] −[μ(u+ −g)+ ∣∇u+∣] −div ⎛
⎝
∇ϕ
√
+ ∣∇ϕ∣
⎞
⎠= 
(.)
on K ∩U. Noticing that the last term represents the curvature of K ∩U, and if we write the
energy density as
e(u; x) = μ(u(x) −g(x))+ ∣∇u(x)∣,
we finally obtain
e(u+) −e(u−) + curv(K) = on K
(at regular points of K, provided that the traces of u and of ∣∇u∣on each side of K are taken
in the sense of Sobolev traces).
We conclude this section by stating another important result from [] regarding the
type of singular points of K, when (u, K) is a minimizer of E from (> .), in two
dimensions, d = . For the rather technical proof of this result, we refer the reader to the
instructive and inspiring constructions from [].
Theorem 
Let d = . If (u, K) is a minimizer of E(u, K) such that K is a union of simple
C,-curves Ki meeting ∂Ω and meeting each other only at their endpoints, then the only
vertices of K are:
() Points P on the boundary ∂Ω where one Ki meets ∂Ω perpendicularly
() Triple points P where three Ki meet with angles π/
() Crack-tips where a Ki ends and meets nothing.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
In the later sections we will discuss cases when the minimizer u is restricted to a specific
class of piecewise-constant or piecewise-smooth functions.
.
Mathematical Modeling and Analysis: The Weak
Formulation of the Mumford and Shah Functional
To better study the mathematical properties of the Mumford and Shah functional (> .),
it is necessary to define the measure of K as its d −-dimensional Hausdorff measure
Hd−(K), which is the most natural way to extend the notion of length to nonsmooth
sets. We recall the definition of the Hausdorff measure [, , ].
Deﬁnition 
For K ⊂Rd and n > , set
Hn(K) = sup
є>
Hn
є (K),
called the n-dimensional Hausdorff measure of the set K, where
Hn
є (K) = cn inf {
∞
∑
i=
(diamAi)n},
where the infimum is taken over all countable families {Ai}∞
i=of open sets Ai such that
K ⊂
∞
⋃
i=
Ai and diam Ai ≤є for all i.
Here, the constant cn is chosen so that Hn coincides with the Lebesgue measure on
n-planes.
Remark 
When n is an integer and K is contained in a C-surface of dimension n,
Hn(K) coincides with its n-dimensional surface measure.
We consider a first variant of the functional,
E(u, K) = μ∫Ω/K(u −g)dx + ∫Ω/K ∣∇u∣dx + Hd−(K).
(.)
In order to apply the direct method of calculus of variations for proving existence of
minimizers, it is necessary to find a topology for which the functional is lower semi-
continuous, while ensuring compactness of minimizing sequences. Unfortunately, the
last functional K ↦Hd−(K) is not lower semi-continuous with respect to any compact
topology [, , ].
To overcome this difficulty, the set K is substituted by the jump set Su of u, thus K is
eliminated, and the problem, called the weak formulation, becomes, in its second variant,
inf
u {F(u) = μ∫Ω/Su
(u −g)dx + ∫Ω/Su
∣∇u∣dx + Hd−(Su)}.
(.)

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

For illustration, we also give the weak formulation in one dimension, for signals. The
problem of reconstructing and segmenting a signal u from a degraded input g deriving
from a distorted transmission, can be modeled as finding the minimum
inf
u {μ∫
b
a (u −g)dt + ∫(a,b)/Su
∣u′∣dt + #(Su)},
where Ω = (a, b), Su denotes the set of discontinuity points of u in the interval (a, b), and
#(Su) = H(Su) denotes the counting measure of Su or its cardinal.
In order to show that (> .) has a solution, the following notion of special func-
tions of bounded variation and the following important lemma due to Ambrosio [, ] are
necessary.
Deﬁnition 
A function u ∈L(Ω) is a special function of bounded variation on Ω if its
distributional derivative can be written as
Du = ∇udx + (u+ −u−) ⃗
NuHd−∣Su
such that ∇u
∈
L(Ω), Su is of finite Hausdorff measure, (u+ −u−) ⃗
Nu χSu
∈
L(Ω,Hd−∣Su,Rd), where u+ and u−are the traces of u on each side of the jump part
Su, and ⃗
Nu is the unit normal to Su. The space of special functions of bounded variation is
denoted by SBV(Ω).
Lemma 
Let un ∈SBV(Ω) be a sequence of functions such that there exists a constant
C > with ∣un(x)∣≤C < ∞a.e. x ∈Ω and ∫Ω ∣∇un∣dx +Hd−(Sun) ≤C. Then there exists
a subsequence unk converging a.e. to a function u ∈SBV(Ω). Moreover, ∇unk converges
weakly in L(Ω)d to ∇u, and
Hd−(Su) ≤liminf
nk→∞Hd−(Sunk ).
Theorem 
Let g ∈L∞(Ω), with Ω ⊂Rd open, bounded, and connected. There is a
minimizer u ∈SBV(Ω) ∩L∞(Ω) of
F(u) = μ∫Ω/Su
(u −g)dx + ∫Ω/Su
∣∇u∣dx + Hd−(Su).
Proof
We notice that ≤inf SBV(Ω)∩L∞(Ω) F < ∞, because we can take u= ∈
SBV(Ω) ∩L∞(Ω) and using the fact that g ∈L∞(Ω) ⊂L(Ω), F(u) < ∞. Thus, there
is a minimizing sequence un ∈SBV(Ω) ∩L∞(Ω) satisfying limn→∞F(un) = inf F. We
also notice that, by the truncation argument from before, we can assume that ∥un∥∞≤
∥g∥∞< ∞. Since F(un) ≤C < ∞for all n ≥, and using g ∈L∞(Ω) ⊂L(Ω), we deduce
that ∥un∥≤C and ∫Ω/Sun ∣∇un∣dx + Hd−(Sun) < C for some positive real constant C.
Using these and Ambrosio’s compactness result, we deduce that there is a subsequence unk
of un, and u ∈SBV(Ω), such that unk ⇀u in L(Ω), ∇unk ⇀∇u in L(Ω)d. Therefore,
F(u) ≤liminf nk→∞F(unk) = inf F, and we can also deduce that ∥u∥∞≤∥g∥∞.
∎


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
For additional existence, regularity results and fine properties of minimizers, and for
the connections between problems (> .) and (> .), we refer the reader to Dal Maso
et al. [, ], the important monographs by Morel and Solimini [], Chambolle [],
by Ambrosio et al. [], by David [], and by Braides []. Existence and regularity of
minimizers for the piecewise-constant case can be found in [], Congedo and Tamanini
[, , , ], Larsen [], among other works.
.
Numerical Methods: Approximations to the
Mumford and Shah Functional
Since the original Mumford and Shah functional (> .) (or its weak formulation (> .))
is non-convex, it has an unknown set K of lower dimension, and it is not the lower-
semicontinuous envelope of a more amenable functional, it is difficult to find smooth
approximations and to solve the minimization in practice. Several approximations have
been proposed, including: the weak membrane model and the graduate non-convexity of
Blake and Zisserman [] (which can be seen as a discrete version of the Mumford and Shah
segmentation problem); discrete finite differences approximations starting with the work of
Chambolle [–] (also proving the Γ-convergence of Blake-Zisserman approximations
to the weak Mumford–Shah functional in one dimension); finite element approximations
by Chambolle and Dal Maso [] and by Chambolle and Bourdin [, ]; phase-field
elliptic approximations due to Ambrosio and Tortorelli [, ] (with generalizations pre-
sented by [] and extensions by Shah [], and Alicandro et al. []); region growing and
merging methods proposed by Koepfler et al. [], by Morel and Solimini [], by Dal
Maso et al. [, ] and level set approximations proposed by Chan and Vese [–, ],
by Samson et al. [], and by Tsai et al. []; approximations by nonlocal functionals by
Braides and Dal Maso [], among other approximations. We present in this section in
many more details the phase-field elliptic approximations and the level set approximations
together with their applications.
For proving the convergence of some of these approximations to the Mumford and Shah
functional, the notion of Γ-convergence is used, which is briefly recalled below. We refer
the interested reader to Dal Maso [] for a comprehensive introduction to Γ-convergence.
We would like to refer the reader to the monographs and textbooks by Braides [],
by Morel and Solimini [], and by Ambrosio et al. [] on detailed presentations of
approximations to the Mumford and Shah functional.
Deﬁnition 
Let X = (X, D) be a metric space. We say that a sequence Fj : X →
[−∞,+∞] Γ-converges to F : X →[−∞,+∞] (as j →∞) if for all u ∈X we have
() (liminf inequality) for every sequence (u j) ⊂X converging to u,
F(u) ≤liminf
j
Fj(u j)
(.)

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

() (existence of a recovery sequence) there exists a sequence (u j) ⊂X converging to
u such that
F(u) ≥limsup
j
Fj(u j),
or, equivalently by (> .),
F(u) = lim
j Fj(u j).
The function F is called the Γ-limit of (Fj) (with respect to D), and we write F = Γ-limj Fj.
The following fundamental theorem is essential in the convergence of some of the
approximations.
Theorem 
(Fundamental Theorem of Γ-convergence) Let us suppose that F = Γ-limj Fj,
and let a compact set C ⊂X exist such that inf X Fj = inf C Fj for all j. Then there is minimum
of F over X such that
min
X F = lim
j inf
X Fj,
and if (uj) ⊂X is a converging sequence such that limj Fj(u j) = limj inf X Fj, then its limit
is a minimum point of F.
..
Ambrosio and Tortorelli Phase-Field Elliptic
Approximations
A specific strategy, closer to the initial formulation of the Mumford–Shah problem in terms
of pairs (u, K = Su), is based on the approximation by functionals depending on two
variables (u,v), the second one related to the set K = Su.
...
Approximations of the Perimeter by Elliptic Functionals
The Modica–Mortola theorem [, ] enables the variational approximation of the
perimeter functional E ↦P(E, Ω) = ∫Ω ∣DχE∣< ∞of an open subset E of Ω by the
quadratic, elliptic functionals
MMє(v) = ∫Ω (є∣∇v∣+ W(v)
є
) dx,
v ∈W,(Ω),
where W(t) is a “double-well” potential. For instance, choosing W(t) = t(−t),
assuming that Ω is bounded with Lipschitz boundary and setting MMє(v) = ∞if v ∈
L(Ω)/W,(Ω), the functionals MMє(v) Γ-converge in L(Ω) to
F(v) = {

P(E, Ω)
if v = χE for some E ∈B(Ω),
∞
otherwise,
where B(Ω) denotes the σ-algebra of Borel subsets of Ω.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Minimizing the functional MMє(v) with respect to v yields the associated Euler–
Lagrange equation and boundary condition,
W′(v) = є△v in Ω,
∂v
∂⃗N
= on ∂Ω,
which can be easily solved in practice by finite differences.
...
Ambrosio-Tortorelli Approximations
In the Mumford and Shah functional the set K = Su is not necessarily the boundary of
an open and bounded domain, but a construction similar to MMє(v) can still be used,
with the potential W(t) = 
(−t)instead. Ambrosio and Tortorelli proposed two elliptic
approximations [, ] to the weak formulation of the Mumford and Shah problem. We
present the second one [], being simpler than the first one [], and commonly used in
practice.
Let X = L(Ω)and let us define
ATє(u,v) = ∫Ω(u −g)dx + β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx
(.)
if (u,v) ∈W,(Ω), ≤v ≤, and ATє(u,v) = +∞otherwise.
We also define the limiting Mumford–Shah functional,
F(u,v) = {∫Ω(u −g)dx + β ∫Ω ∣∇u∣+ αHd−(Su)
if u ∈SBV(Ω), v ≡,
+∞
otherwise.
Theorem 
ATє Γ-converges to F as є ↘in L(Ω). Moreover, ATє admits a minimizer
(uє,vє) such that up to subsequences, uє converges to some u ∈SBV(Ω) a minimizer of
F(u,) and inf ATє(uє,vє) →F(u,).
Interesting generalizations of this result are given and proved by Braides in [].
In practice, the Euler–Lagrange equations associated with the alternating minimization
of ATє with respect to u = uє and v = vє are used and discretized by finite differences. These
are
∂ATє(u,v)
∂u
= (u −g) −βdiv(v∇u) = 
∂ATє(u,v)
∂v
= βv∣∇u∣−αє △v + α
є(v −) = .
One of the finite differences approximations to compute u and v in two dimensions x =
(x, x) is as follows. We use a time-dependent scheme in u = u(x, x, t) and a stationary
semi-implicit fixed-point scheme in v = v(x, x). Let △x= △x= h be the step space, △t
be the time step, and gi,j, un
i,j, vn
i,j be the discrete versions of g, and of u and v at iteration
n ≥, for ≤i ≤M, ≤j ≤N. Initialize u= g and v= .

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

For n ≥, compute and repeat to steady state, for i = , . . ., M −and j = , . . ., N −
(combined with Neumann boundary conditions on ∂Ω):
∣∇un∣
i,j = (
un
i+,j −un
i,j
h
)

+ (
un
i,j+−un
i,j
h
)

,
= βvn+
i,j ∣∇un∣
i,j −αє
h(vn
i+,j + vn
i−,j + vn
i,j++ vn
i,j−
−vn+
i,j ) + α
є (vn+
i,j −),
un+
i,j −un
i,j
△t
= −(un
i,j −gi,j) + β
h[(vn+
i,j )
(un
i+,j −un
i,j) + (vn+
i,j )
(un
i,j+−un
i,j)
−(vn+
i−,j)
(un
i,j −un
i−,j) −(vn+
i,j−)
(un
i,j −un
i,j−)]
which is equivalent with
∣∇un∣
i,j = (
un
i+,j −un
i,j
h
)

+ (
un
i,j+−un
i,j
h
)

,
vn+
i,j =
α
є + αє
h(vn
i+,j + vn
i−,j + vn
i,j++ vn
i,j−)
α
є + β∣∇un∣
i,j + αє
h
,
un+
i,j = un
i,j + △t {−(un
i,j −gi,j) + β
h[(vn+
i,j )
(un
i+,j −un
i,j)
+ (vn+
i,j )
(un
i,j+−un
i,j) −(vn+
i−,j)
(un
i,j −un
i−,j)
−(vn+
i,j−)
(un
i,j −un
i,j−)] }.
We present experimental results obtained using the above Ambrosio–Tortorelli approx-
imations applied to the well-known Barbara image shown in
> Fig. -left. Segmented
images u are shown in
> Fig. -and the corresponding edge sets v are shown in
> Fig. -for varying coefficients α, β ∈{,,}. We notice that less regularization
(decreasing both α and β) gives more edges in v, as expected, thus u is closer to g. Fixed α
and increasing β gives smoother image u and fewer edges in v. Keeping fixed β but varying
α does not produce much variation in the results. We also show in
> Fig. -right the
numerical energy versus iterations for the case α = β = , є = ..
Applications of the Ambrosio–Tortorelli approximations to image restoration will be
presented in details in > Sect. ..
..
Level Set Formulations of the Mumford and Shah
Functional
We review in this section the level set formulations for minimizing the Mumford and
Shah functional, as proposed initially by Chan and Vese [–, ], and by Tsai et al. []


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
0
10
20
30
40
50
60
70
80
90
100
1
2
3
4
5
6
7
8
9
10
11 x 108
⊡Fig. -
Left: original image g. Right: numerical energy versus iterations for the Ambrosio–Tortorelli
approximations (α = β = , є = .)
(see also the related work by Samson et al. [] and Cohen et al. [, ]). These make the
link between curve evolution, active contours, and Mumford–Shah segmentation. These
models have been proposed by restricting the set of minimizers u to specific classes of func-
tions: piecewise constant, piecewise smooth, with the edge set K represented by a union
of curves or surfaces that are boundaries of open subsets of Ω. For example, if K is the
boundary of an open-bounded subset of Ω, then it can be represented implicitly, as the
zero-level line of a Lipschitz-continuous level set function. Thus the set K as an unknown
is substituted by an unknown function, that defines it implicitly, and the Euler–Lagrange
equations with respect to the unknowns can be easily computed and discretized.
Following the level set approach [, , , ], let ϕ : Ω →R be a Lipschitz continuous
function. We recall the variational level set terminology that will be useful to rewrite the
Mumford and Shah functional in terms of (u, ϕ), instead of (u, K). We are inspired by the
work of Zhao et al. [] for a variational level set approach for motion of triple junctions
in the plane.
We will use the one-dimensional (D) Heaviside function H, defined by
H(z) = {if z ≥
if z < ,
and its distributional derivative δ = H′ (in the weak sense). In practice, we may need to
work with smooth approximations of the Heaviside and δ functions. Here, we will use the
following C∞approximations as є →given by [, ],
Hє(z) = 
[+ 
π arctan (z
є )] ,
δє = H′
є.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

(1,1)
(1,5)
(1,10)
(5,1)
(5,5)
(5,10)
(10,10)
(10,5)
(10,1)
⊡Fig. -
Piecewise-smooth images u as minimizers of the Ambrosio–Tortorelli approximations for
є = .and various values of (α, β)
The area (or the volume) of the region {x ∈Ω : ϕ(x) > } is
A{x ∈Ω : ϕ(x) > } = ∫Ω H(ϕ(x))dx,
and for a level parameter l ∈R, the area (or volume) of the region {x ∈Ω : ϕ(x) > l} is
A{x ∈Ω : ϕ(x) > l} = ∫Ω H(ϕ(x) −l)dx.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
(1,1)
(1,5)
(5,5)
(1,10)
(5,10)
(5,1)
(10,1)
(10,5)
(10,10)
⊡Fig. -
Corresponding edge sets v as minimizers of the Ambrosio–Tortorelli approximations for
є = .and various values of (α, β)
The perimeter (or more generally the surface area) of the region {x ∈Ω : ϕ(x) > } is
given by
L{x ∈Ω : ϕ(x) > } = ∫Ω ∣DH(ϕ)∣,
which is the total variation of H(ϕ) in Ω, and the perimeter (or surface area) of {x ∈Ω :
ϕ(x) > l} is
L{x ∈Ω : ϕ(x) > l} = ∫Ω ∣DH(ϕ −l)∣.
Given the image data g ∈L∞(Ω) ⊂L(Ω) to be segmented, the averages of g over the
(nonempty) regions {x ∈Ω : ϕ(x) > } and {x ∈Ω : ϕ(x) < } respectively, are

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

∫Ω g(x)H(ϕ(x))dx
∫Ω H(ϕ(x))dx
and ∫Ω g(x)(−H(ϕ(x)))dx
∫Ω(−H(ϕ(x)))dx
= ∫Ω g(x)H(−ϕ(x))dx
∫Ω H(−ϕ(x))dx
.
More generally, for a given level parameter l ∈R, the averages of g over the corre-
sponding (nonempty) regions {x ∈Ω : ϕ(x) > l} and {x ∈Ω : ϕ(x) < l} respectively,
are
∫Ω g(x)H(ϕ(x) −l)dx
∫Ω H(ϕ(x) −l)dx
and ∫Ω g(x)H(l −ϕ(x))dx
∫Ω H(l −ϕ(x))dx
.
We prove next that if H and δ are substituted by the above C∞approximations Hє,
δє as є →, we obtain approximations of the area and length (perimeter) measures. We
obviously have that Hє(z) →H(z) for all z ∈R, as є →, and that the approximating area
term Aє(ϕ) = ∫Ω Hє(ϕ(x))dx converges to A(ϕ) = ∫Ω H(ϕ(x))dx.
Generalizing a result of Samson et al. [], we can show [] that our approximating
functional Lє(ϕ) = ∫Ω ∣DHє(ϕ)∣dx = ∫Ω δε(ϕ)∣∇ϕ∣dx converges to the length ∣K∣of the
zero-level line K = {x ∈Ω : ϕ(x) = }, under the assumption that ϕ : Ω →R is Lipschitz.
The same result holds for the case of any l-level curve of ϕ and not only for the -level
curve.
Theorem 
Let us define
Lє(ϕ) = ∫Ω ∣∇Hє(ϕ)∣dx = ∫Ω δє(ϕ)∣∇ϕ∣dx.
Then we have
lim
є→Lє(ϕ) = ∫{ϕ=} ds = ∣K∣,
where K = {x ∈Ω : ϕ(x) = }.
Proof
Using co-area formula [], we have:
Lє(ϕ) = ∫R [∫ϕ=ρ δє(ϕ(x))ds]dρ = ∫R [δє(ρ)∫ϕ=ρ ds]dρ.
By setting h(ρ) = ∫ϕ=ρ ds, we obtain
Lє(ϕ) = ∫R δє(ρ)h(ρ)dρ = ∫R

π
є
є+ ρh(ρ)dρ.
By the change of variable θ = ρ
є , we obtain
lim
є→Lє(ϕ) = lim
є→∫R

π
є
є+ єθh(θє)dθ = lim
є→∫R

π

+ θh(θє)dθ
= h()∫R

π

+ θdθ = h() 
π arctan θ∣+∞
−∞= h() = ∫ϕ=ds = ∣K∣,
which concludes the proof.
∎


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
In general, this convergence result is valid for any approximations Hє, δє, under the
assumptions
lim
є→Hє(z) = H(z) in R/{},
δє = H′
є, Hє ∈C(R), ∫
+∞
−∞δ(x)dx = .
...
Piecewise-Constant Mumford and Shah Segmentation
Using Level Sets
Our first formulation is for the case when the unknown set of edges K can be represented
by K = {x ∈Ω : ϕ(x) = } for some (unknown) Lipschitz function ϕ : Ω →R. In this case
we restrict the unknown minimizers u to functions taking two unknown values c, c, and
the corresponding Mumford–Shah minimization problem can be expressed as [, ]
inf
c,c,ϕ E(c, c, ϕ) = ∫Ω(g(x) −c)H(ϕ)dx + ∫Ω(g(x) −c)H(−ϕ)dx
+
∫Ω ∣DH(ϕ)∣.
(.)
The unknown minimizer u is expressed as
u(x) = cH(ϕ(x)) + c(−H(ϕ(x))) = cH(ϕ(x)) + cH(−ϕ(x)).
We substitute H by its C∞approximation Hє and we minimize instead
Eє(c, c, ϕ) = ∫Ω(g(x) −c)Hє(ϕ)dx + ∫Ω(g(x) −c)Hє(−ϕ)dx
+
∫Ω ∣∇Hє(ϕ)∣dx.
(.)
The associated Euler–Lagrange equations with respect to c, c, and ϕ are
c(ϕ) = ∫Ω g(x)Hє(ϕ(x))dx
∫Ω Hє(ϕ(x))dx
,
c(ϕ) = ∫Ω g(x)Hє(−ϕ(x))dx
∫Ω Hє(−ϕ(x))dx
,
and, after simplifications,
δє(ϕ)[(g(x) −c)−(g(x) −c)−div ( ∇ϕ
∣∇ϕ∣)] = in Ω,
(.)
with boundary conditions ∇ϕ ⋅⃗N = on ∂Ω. Since δє > as defined, the factor δє(ϕ)
can be removed from (> .), or substituted by ∣∇ϕ∣to obtain a more geometric motion
extended to all level lines of ϕ, as in the standard level set approach.
This approach has been generalized by Chung and Vese in [, ], where more
than one level line of the same level set function ϕ can be used to represent the edge set K.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Using m distinct real levels {l< l< . . . < lm}, the function ϕ partitions the domain Ω
into the following m+disjoint open regions, making up Ω, together with their boundaries:
Ω= {x ∈Ω : −∞< ϕ(x) < l},
Ω j = {x ∈Ω : l j < ϕ(x) < l j+},
≤j ≤m −
Ωm = {x ∈Ω : lm < ϕ(x) < +∞}.
The energy to minimize in this case, depending on c, c, . . ., cm, ϕ, will be
E(c, c, . . . , cm, ϕ) = ∫Ω ∣g(x) −c∣H(l−ϕ(x))dx +
m−
∑
j=∫Ω ∣g(x)
−c j∣H(ϕ(x) −l j)H(l j+−ϕ(x))dx + ∫Ω ∣g(x)
−cm∣H(ϕ(x) −lm)dx +

m
∑
j=∫Ω ∣DH(ϕ −l j)∣.
(.)
The segmented image will be given by
u(x) = cH(l−ϕ(x)) +
m−
∑
j=
c jH(ϕ(x) −l j)H(l j+−ϕ(x)) + cmH(ϕ(x) −lm).
As before, to minimize the above energy, we approximate and substitute the Heavi-
side function H by Hє, as є →. The Euler–Lagrange equations associated with the
corresponding minimization
inf
c,c,...,cm,ϕ Eє(c, c, . . . , cm, ϕ),
(.)
can be expressed as
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
c(ϕ) = ∫Ω g(x)Hє(l−ϕ(t,x))dx
∫Ω Hє(l−ϕ(t,x))dx
,
c j(ϕ) = ∫Ω g(x)Hє(ϕ(t,x)−l j)Hє(l j+−ϕ(t,x))dx
∫Ω Hє(ϕ(t,x)−l j)Hє(l j+−ϕ(t,x))dx
,
cm(ϕ) = ∫Ω g(x)Hє(ϕ(t,x)−lm)dx
∫Ω Hє(ϕ(t,x)−lm)dx
,
and
= ∣g −c∣δє(l−ϕ) +
m−
∑
j=
∣g −c j∣[δє(l j+−ϕ)Hє(ϕ −l j) −δє(ϕ −l j)Hє(l j+−ϕ)]
−∣g −cm∣δє(ϕ −lm) +

m
∑
j=
[δє(ϕ −l j)div ( ∇ϕ
∣∇ϕ∣)],
∂ϕ
∂⃗n∣
∂Ω = ,
(.)
where ⃗N is the exterior unit normal to the boundary ∂Ω.
We give here the details of the numerical algorithm for solving (> .) in two dimen-
sions (x, y), using gradient descent, in the case of one function ϕ with two levels l= ,


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
l= l > . Let h = △x = △y be the space steps, △t be the time step, and є = h. Let (xi, y j)
be the discrete points, for ≤i, j ≤M, and gi,j ≈g(xi, y j), ϕn
i,j ≈ϕ(n △t, xi, y j), with
n ≥. Recall the usual finite differences formulas
△x
+ϕi,j = ϕi+,j −ϕi,j, △x
−ϕi,j = ϕi,j −ϕi−,j,
△y
+ϕi,j = ϕi,j+−ϕi,j, △y
−ϕi,j = ϕi,j −ϕi,j−.
Set n = , and start with ϕ
i,j given (defining the initial set of curves). Then, for each n > 
until steady state:
() compute averages c(ϕn), c(ϕn), and c(ϕn).
() compute ϕn+
i,j , derived from the finite differences scheme:
ϕn+
i,j −ϕn
i,j
△t
= δє (ϕn
i,j)
⎡⎢⎢⎢⎢⎢⎣

h
⎛
⎜
⎝
△x
−
⎛
⎜
⎝
ϕn
i+,j −ϕn+
i,j
∣∇ϕn
i,j∣
⎞
⎟
⎠
+ △y
−
⎛
⎜
⎝
ϕn
i,j+−ϕn+
i,j
∣∇ϕn
i,j∣
⎞
⎟
⎠
⎞
⎟
⎠
+ ∣gi,j −c∣
−∣gi,j −c∣Hє (l −ϕn
i,j)
⎤⎥⎥⎥⎥⎥⎦
+ δє (ϕn
i,j −l)
⎡⎢⎢⎢⎢⎢⎣

h
⎛
⎜
⎝
△x
−
⎛
⎜
⎝
ϕn
i+,j −ϕn+
i,j
∣∇ϕn
i,j∣
⎞
⎟
⎠
+ △y
−
⎛
⎜
⎝
ϕn
i,j+−ϕn+
i,j
∣∇ϕn
i,j∣
⎞
⎟
⎠
⎞
⎟
⎠
−∣gi,j −c∣+ ∣gi,j −c∣Hє (ϕn
i,j)
⎤⎥⎥⎥⎥⎥⎦
,
where ∣∇ϕn
i,j∣=
√
(
ϕn
i+, j−ϕn
i, j
h
)

+ (
ϕn
i, j+−ϕn
i, j
h
)

. Let
C=

√
(
ϕn
i+, j−ϕn
i, j
h
)

+ (
ϕn
i, j+−ϕn
i, j
h
)
,
C=

√
(
ϕn
i, j−ϕn
i−, j
h
)

+ (
ϕn
i−, j+−ϕn
i−, j
h
)
,
C=

√
(
ϕn
i+, j−ϕn
i, j
h
)

+ (
ϕn
i, j+−ϕn
i, j
h
)
,
C=

√
(
ϕn
i+, j−−ϕn
i, j−
h
)

+ (
ϕn
i, j−ϕn
i, j−
h
)
.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Let m= △t
h(δє (ϕn
i,j) + δє (ϕn
i,j −l))
, C = +m(C+C+C+C). The main
update equation for ϕ becomes
ϕn+
i,j = 
C [ϕn
i,j + m(Cϕn
i+,j + Cϕn
i−,j + Cϕn
i,j++ Cϕn
i,j−)
+ △tδє (ϕn
i,j)(−(gi,j −c)(−Hє (ϕn
i,j −l))
+ (gi,j −c)) + △tδє (ϕn
i,j −l)(−(gi,j −c)+ (gi,j −c)Hє (ϕn
i,j))],
and repeat, until steady state is reached.
We conclude this section with several experimental results obtained using the models
presented here, that act as denoising, segmentation, and active contours. In
> Fig. -
we show an experimental result taken from [] obtained using the binary piecewise-
constant model (> .); we notice how interior contours can be automatically detected.
In > Fig. -, we show an experimental result using the multilayer model (> .), with
m = and two levels l, l, applied to the segmentation of a brain image.
The work in [, ] also shows how the previous Mumford–Shah level set approaches
can be extended to piecewise-constant segmentation of images with triple junctions, sev-
eral non-nested regions, or with other complex topologies, by using two or more level set
functions that form a perfect partition of the domain Ω.
⊡Fig. -
Detection of diﬀerent objects in a noisy image, with various convexities and with an interior
contour which is automatically detected, using only one initial curve. After a short time, an
interior contour appears inside the torus, and then it expands. Top: g and the evolving
contours. Bottom: the piecewise-constant approximations u of g over time, given by
u = cH(ϕ) + c(−H(ϕ))


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
⊡Fig. -
Segmentation of a brain image using one level set function with two levels. Parameters:
l= , l= , △t = .,
= .⋅, ,iterations

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

...
Piecewise-Smooth Mumford and Shah Segmentation
Using Level Sets
We first consider the corresponding two-dimensional case under the assumption that the
edges denoted by K in the image can be represented by one level set function ϕ, i.e., K =
{x ∈Ω∣ϕ(x) = }, and we follow the approaches developed in parallel by Chan and Vese
[, ] and by Tsai et al. [], in order to minimize the general Mumford and Shah model.
As in [], the link between the unknowns u and ϕ can be expressed by introducing two
functions u+ and u−(see > Fig. -) such that
u(x) = {u+(x) if ϕ(x) ≥,
u−(x) if ϕ(x) ≤.
We assume that u+ and u−are Hfunctions on ϕ ≥and on ϕ ≤, respectively (with
Sobolev traces up to all boundary points, i.e., up to the boundary {ϕ = }). We can write
the following minimization problem
inf
u+,u−,ϕ E(u+,u−, ϕ),
where
E(u+,u−, ϕ) = μ∫Ω ∣u+ −g∣H(ϕ)dx + μ∫Ω ∣u−−g∣(−H(ϕ))dx
+ ∫Ω ∣∇u+∣H(ϕ))dx + ∫Ω ∣∇u−∣(−H(ϕ))dx +
∫Ω ∣DH(ϕ)∣
is the Mumford–Shah functional restricted to u(x) = u+(x)H(ϕ(x)) + u−(x)(−
H(ϕ(x)).
Minimizing E(u+,u−, ϕ) with respect to u+, u−, and ϕ, we obtain the following
Euler–Lagrange equations (embedded in a time-dependent dynamical scheme for ϕ):
u = u–
u = u–
u = u+
u = u+
u = u +
φ > 0
u = u–
φ < 0
u = u –
φ < 0
φ > 0
φ < 0
φ > 0
⊡Fig. -
The functions u+, u−and the zero level lines of the level set function ϕ for piecewise-smooth
image partition


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
μ(u+ −g) = △u+ in {x : ϕ(t, x) > },
∂u+
∂⃗n = on {x : ϕ(t, x) = } ∪∂Ω, (.)
μ(u−−g) = △u−in {x : ϕ(t, x) < },
∂u−
∂⃗n = on {x : ϕ(t, x) = } ∪∂Ω, (.)
∂ϕ
∂t = δε(ϕ)[ ∇( ∇ϕ
∣∇ϕ∣) −μ∣u+ −g∣−∣∇u+∣+ μ∣u−−g∣+ ∣∇u−∣],
(.)
where ∂/∂⃗n denotes the partial derivative in the normal direction ⃗n at the corresponding
boundary. We also associate the boundary condition ∂ϕ
∂⃗n = on ∂Ω to > Eq. (.).
We show in
> Figs. -and
> -experimental results taken from [] obtained
with the piecewise-smooth two-phase model.
⊡Fig. -
Results on a noisy image, using the level set algorithm for the piecewise-smooth
Mumford–Shah model with one level set function. The algorithm performs as active
contours, denoising, and edge detection
⊡Fig. -
Numerical result using the piecewise-smooth Mumford–Shah level set algorithm with one
level set function, on a piecewise-smooth real galaxy image

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

There are cases when the boundaries K of regions forming a partition of the image
could not be represented by the boundary of an open domain. To overcome this, several
solutions have been proposed in this framework and we mention two of them: () in the
work by Tsai et al. [], the minimization of E(u+,u−, ϕ) is repeated inside each of the two
regions previously computed and () in the work of Chan and Vese [], two or more level
set functions are used. For example, in two dimensions, the problem can be solved using
only two level set functions, and we do not have to know a priori how many gray levels the
image has (or how many segments). The idea is based on the Four-Color Theorem. Based
on this observation, we can “color” all the regions in a partition using only four “colors,”
such that any two adjacent regions have different “colors.” Therefore, using two level set
functions, we can identify the four “colors” by the following (disjoint) sets: {ϕ> , ϕ> },
{ϕ< , ϕ< }, {ϕ< , ϕ> }, {ϕ< , ϕ< }. The boundaries of the regions forming
the partition will be given by {ϕ= } ∪{ϕ= }, and this will be the set of curves K.
Note that, in this particular multiphase formulation of the problem, we do not have the
problems of “overlapping” or “vacuum” (i.e., the phases are disjoint, and their union is the
entire domain Ω, by definition).
As before, the link between the function u and the four regions can be made by intro-
ducing four functions u++,u+−,u−+,u−−, which are in fact the restrictions of u to each of
the four phases, as follows (see > Fig. -):
u(x) =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
u++(x), if ϕ(x) > and ϕ(x) > ,
u+−(x), if ϕ(x) > and ϕ(x) < ,
u−+(x), if ϕ(x) < and ϕ(x) > ,
u−−(x), if ϕ(x) < and ϕ(x) < .
u = u++
u = u+–
u = –+
u = u++
u = u+–
u = u+–
u = u+–
f1 > 0
f2 > 0
f1 > 0
f2 < 0
f1 < 0
f2 > 0
f1 > 0
f2 > 0
f1 > 0
f2 < 0
⊡Fig. -
The functions u++, u+−, u−+, u−−, and the zero level lines of the level set functions ϕ, ϕfor
piecewise-smooth image partition


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Again, using the Heaviside function, the relation between u, the four functions u++,
u+−, u−+, u−−, and the level set functions ϕand ϕcan be expressed by:
u = u++H(ϕ)H(ϕ) + u+−H(ϕ)(−H(ϕ)) + u−+(−H(ϕ))H(ϕ) + u−−
(−H(ϕ))(−H(ϕ)).
We then introduce an energy in level set formulation based on the Mumford–Shah
functional:
E(u, ϕ, ϕ) = μ∫Ω ∣u++ −g∣H(ϕ)H(ϕ)dx
+ ∫Ω ∣∇u++∣H(ϕ)H(ϕ)dx
+ μ∫Ω ∣u+−−g∣H(ϕ)(−H(ϕ))dx
+ ∫Ω ∣∇u+−∣H(ϕ)(−H(ϕ))dx
+ μ∫Ω ∣u−+ −g∣(−H(ϕ))H(ϕ)dx
+ ∫Ω ∣∇u−+∣(−H(ϕ))H(ϕ)dx
+ μ∫Ω ∣u−−−g∣(−H(ϕ))(−H(ϕ))dx
+ ∫Ω ∣∇u−−∣(−H(ϕ))(−H(ϕ))dx
+
∫Ω ∣DH(ϕ)∣+
∫Ω ∣DH(ϕ)∣.
Note that the expression ∫Ω ∣DH(ϕ)∣+ ∫Ω ∣DH(ϕ)∣is not exactly the length term of
K = {x ∈Ω : ϕ(x) = and ϕ(x) = }, it is just an approximation and simplification.
In practice, satisfactory results using the above formula are obtained, and the associated
Euler–Lagrange equations are simplified.
We obtain the associated Euler–Lagrange equations as in the previous cases, embedded
in a dynamic scheme, assuming (t, x, y) ↦ϕi(t, x, y): minimizing the energy with respect
to the functions u++, u+−, u−+, u−−, we have, for each fixed t:
μ(u++ −g) = △u++ in {ϕ> , ϕ> },
∂u++
∂⃗n
= on {ϕ= , ϕ≥},{ϕ≥, ϕ= };
μ(u+−−g) = △u+−in {ϕ> , ϕ< },
∂u+−
∂⃗n
= on {ϕ= , ϕ≤},{ϕ≥, ϕ= };
μ(u−+ −g) = △u−+ in {ϕ< , ϕ> },
∂u−+
∂⃗n
= on {ϕ= , ϕ≥},{ϕ≤, ϕ= };
μ(u−−−g) = △u−−in {ϕ< , ϕ< },
∂u−−
∂⃗n
= on {ϕ= , ϕ≤},{ϕ≤, ϕ= }.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

The Euler–Lagrange equations evolving ϕand ϕ, embedded in a dynamic scheme,
are formally:
∂ϕ
∂t = δε(ϕ)[ ∇( ∇ϕ
∣∇ϕ∣) −μ∣u++ −g∣H(ϕ) −∣∇u++∣H(ϕ)
−μ∣u+−−g∣(−H(ϕ)) −∣∇u+−∣(−H(ϕ)) + μ∣u−+
−g∣H(ϕ) + ∣∇u−+∣H(ϕ) + μ∣u−−−g∣(−H(ϕ)) + ∣∇u−−∣(−H(ϕ))] = ,
∂ϕ
∂t = δε(ϕ)[ ∇( ∇ϕ
∣∇ϕ∣) −μ∣u++ −g∣H(ϕ) −∣∇u++∣H(ϕ)
+ μ∣u+−−g∣H(ϕ) + ∣∇u+−∣H(ϕ) −μ∣u−+ −g∣(−H(ϕ)) −∣∇u−+∣(−H(ϕ))
+ μ∣u−−−g∣(−H(ϕ)) + ∣∇u−−∣(−H(ϕ))] .
We can show, by standard techniques of the calculus of variations on the space SBV(Ω)
(special functions of bounded variations), and a compactness result due to Ambrosio [],
that the proposed minimization problems from this section, in the level set formulation,
have a minimizer. Finally, because there is no uniqueness of minimizers, and because the
problems are nonconvex, the numerical results may depend on the initial choice of the
curves, and we may compute a local minimum only. We think that, using the seed initial-
ization (see []) the algorithms have the tendency of computing a global minimum, most
of the times. Additional experimental results are shown in [].
...
Extension to Level Set Based Mumford–Shah
Segmentation with Open Edge Set K
We have mentioned in
> Sect. .., Theorem that in two dimensions, the Mumford–
Shah functional E from (> .) allows for minimizers (u, K) such that the set K could
include open curves or crack tips where a curve Ki of K ends and meets nothing. On the
other hand, the level set formulations presented in the previous sections allow only for
closed curves as pieces of K, an inherent property due to the implicit representation of
boundaries. In this section, we show how we can modify the level set representation of the
Mumford–Shah functional, so that images with edges made of open curves could also be
segmented. For more details we refer the reader to Mohieddine–Vese [].
The main idea is to use the open curve representation using level sets due to Smereka
[]. In [], by adding a “dual” level set function, a level set formulation for open curves
extending the standard methods is proposed. Given a level set function ϕ : Ω →R and
a “dual” level set function ψ : Ω →R (as Lipschitz continuous functions), an open curve
K can be defined as K = {x ∈Ω : ϕ(x) = ,ψ(x) > }. This is illustrated in
> Fig. -.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
f > 0
y < 0
f > 0
y > 0
f < 0
y > 0
f < 0
y < 0
K
⊡Fig. -
Representation of an open curve K = {ϕ = } ∩{ψ > }
The method in [] was applied to a curvature equation which models the dynamics of
spiral crystal growth, with velocity
v(t) = (−λκ)n,
(.)
where κ is the curvature and n is the unit normal of the open spiral curve, and λ is a
constant. After reformulating the equations with open level sets yields []:
∂ϕ
∂t + sign(ψ)[−λsign(ψ)κ(ϕ)]∣∇ϕ∣= ,
∂ψ
∂t + sign(ϕ)[−λsign(ϕ)κ(ψ)]∣∇ψ∣= .
In general, this method will give a symmetric system of the form
∂ϕ
∂t + F(ϕ,ψ) = ,
(.)
∂ψ
∂t + F(ψ, ϕ) = 
(.)
from where it is clear why these level set functions are called “dual” to each other. In the
general form, > Eqs. (.) and (> .) may or may not be derived from functional
minimization.
Here, we will use the idea of Smereka in the minimization of the Mumford–Shah model
for segmentation with open edge curves. We first define the following characteristic func-
tions over Ω: χ= H(ϕ), χ= H(ψ)(−H(ϕ)), χ= H(ψ), and χ= (−H(ψ))(−H(ϕ)).

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Then we propose the following open curve formulation of Mumford–Shah, in a particular
case: minimize
E(u,u, ϕ,ψ) = ∫Ω [∣u−g∣χ+ ∣u−g∣χ+ ∣u+ u

−g∣

χ]dx
+ μ ∫Ω [∣∇u∣(χ+ χ
) + μ∣∇u∣(χ+ χ
) + μ
∇u⋅∇uχ]
dx + λ ∫Ω χ∣∇χ∣.
The segmented image will be u = uχ+ uχ+ u+u

χ, the set K = {ϕ = } ∩{ψ > }
models the open jump set, and the length of K is ∣K∣= ∫Ω ∣∇H(ϕ)∣H(ψ) = ∫Ω χ∣∇χ∣. In
the above energy, the first term corresponds to the data filelity, the second term corresponds
to the regularization in u, while the third term is the length penalty. Thus, the functional
imposes that u ≈g over χ, χ, χ(thus over Ω), and that u is of class Hover the regions
whose characteristic functions are χ+ χand χ+ χ.
As in the previous sections, we first substitute the Heaviside function H by smooth
approximations Hє. Also, as in Theorem , it is possible to show that the approximating
term ∫Ω Hє(ψ)∣∇Hє(ϕ)∣converges, as є →, to the length of the open set K = {x ∈Ω :
ϕ(x) = ,ψ(x) > }.
The Euler–Lagrange equations associated with the minimization, expressed using the
Lgradient descent, formally are
∂u
∂t = μdiv [χ∇u+ χ
∇(u+ u)] −(u−g)χ−(u+ u

−g) χ
[χ∇u+ χ
∇(u+ u)] ⋅n = on ∂Ω
u(, x) = u−initial(x),
∂u
∂t = μdiv [χ∇u+ χ
∇(u+ u)] −(u−g)χ−(u+ u

−g) χ
[χ∇u+ χ
∇(u+ u)] ⋅n = on ∂Ω
u(, x) = u−initial(x),
∂ϕ
∂t = δ(ϕ)[λdiv (χ
∇ϕ
∣∇ϕ∣) −∣u−g∣+ χ∣u−g∣+ (−χ)
∣u+ u

−g∣

−μ∣∇u∣(
+ χ
)
+ μ∣∇u∣( 
+ χ
) + 
μ∇u⋅∇u(−χ)]
(χ
∇ϕ
∣∇ϕ∣) ⋅n = on ∂Ω
ϕ(, x) = ϕinitial(x),


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
dψ
dt = −δ(ψ)[λ∣∇χ∣+ (−χ)(∣u−g∣−∣u+ u

−g∣

−μ
∣∇u∣
+μ
∣∇u∣−μ
∇u⋅∇u)]
ψ(, x) = ψinitial(x).
Since we have doubled the amount of functions used to define one curve, we have also
increased the computational cost. Moreover, from a theoretical point of view, the system of
equations derived from the standard Lgradient decent may be ill posed. For illustration,
following Neuberger [] and Renka [], assume that we have the energy functional with a
potential F, i.e., E(ϕ) = ∫Ω F(Dϕ),to be minimized over H(Ω), where here D : H(Ω) →
H(Ω) × L(Ω) is the operator Dϕ = (ϕ,∇ϕ)T. We assume that ϕ ∈H(Ω) and for any
h ∈H
(Ω) we have the directional derivative:
(E′(ϕ), h) = ∫Ω F′(Dϕ)Dh = ⟨∇F(Dϕ), Dh⟩L= ⟨D∗∇F(Dϕ), h⟩L,
where D∗is the adjoint of D. We will call the first variation the Lgradient, ∇LE(ϕ) =
D∗∇F(Dϕ) and it defines the usual gradient descent method, ∂ϕ
∂t = −∇LE(ϕ). In the
semi-discrete case, we construct the sequence ϕn by ϕn+= ϕn −△t∇LE(ϕn), with
ϕ∈H(Ω), △t > , such that E(ϕn+) < E(Φn). In order to have ϕn+∈H(Ω) ⊂L(Ω),
this would require that ∇LE(ϕn) ∈H(Ω) ⊂L(Ω), in other words we would assume too
strong regularity for the solution ϕ, which may not hold. This is one of the reasons for
the small time steps necessary for stability when using Lgradient decent. Thus, the com-
bination of small time steps and increased amount of functions to represent open curves
can become problematic in practice. To avoid these issues, we derive an alternative decent
direction which is better posed. The next simplest direction to the Lgradient is the Sobolev
Hgradient direction. Denote the Hgradient as ∇HE(ϕ) and as before we will look at
the directional derivative. Equating the directional derivative with the Hinner product
yields the Sobolev gradient as follows:
(E′(ϕ), h) = ⟨∇LE(ϕ), h⟩L= ⟨∇HE(ϕ)), h⟩H.
So we have
⟨∇HE(ϕ), h⟩H= ⟨D(∇HE(ϕ)), Dh⟩L= ⟨D∗D(∇HE(ϕ)), h⟩L
and therefore ∇HE(ϕ) = (D∗D)−(∇LE(ϕ)) = (I −Δ)−(∇LE(ϕ)). One way to look
at this is applying gradient decent with respect to a different inner product. Numerically,
it can be viewed as a preconditioning of the regular gradient decent method []. This will
also have numerical benefits. For more details on the theory of Sobolev gradients, see [].
Here, the Sobolev Hgradient is used for all four equations in u,u, ϕ,ψ.
We present a few experimental results for the segmentation of a simple synthetic image
with noise. In > Fig. -we show a synthetic noisy image, the evolution of the unknown
open curve K over iterations, and the denoised image u over iterations.
> Figure -
shows the surface plot of the unknown u during the iterative procedure. The numerical

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
Segmentation of a synthetic noisy image with open curve discontinuity. Top, from left to
right: evolution of the unknown open curve K with iterations, superimposed over the noisy
data g. Bottom, from left to right: the initial noisy image g and the restored image u over
iterations
0
20
40
60
80
100
0
50
100
0
50
100
150
200
250
300
0
20
40
60
80
100
0
50
100
0
50
100
150
200
250
300
0
20
40
60
80
100
0
50
100
0
50
100
150
200
250
300
0
20
40
60
80
100
0
50
100
0
50
100
150
200
250
300
⊡Fig. -
The surface plot of the image u in > Fig. -over iterations


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
0
10
20
30
40
50
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5 x 106
x 106
0
100
200
300
400
500
600
700
800
⊡Fig. -
Numerical energy versus iterations (left, ﬁrst iterations; right, ﬁrst iterations)
energy versus iterations is presented in
> Fig. -, showing that the proposed numer-
ical algorithm [] is stable in practice. The boundary conditions for uand ucan be
simplified.
.
Case Examples: Variational Image Restoration with
Segmentation-Based Regularization
This section focuses on the challenging task of edge-preserving variational image restora-
tion. In this context, restoration is referred to as image deblurring and denoising, where
we deal with Gaussian and impulsive noise models. Terms from the Mumford–Shah seg-
mentation functional are used as regularizers, reflecting the model of piecewise-constant
or piecewise-smooth images.
In the standard model of degradation the underlying assumptions are the linearity and
shift invariance of the blur process and the additivity and normal distribution of the noise.
Formally, let Ω be an open-bounded subset of Rn. The observed image g : Ω →RN ∈L∞
is given by
g = h ∗u + n,
(.)
where g is normalized to the hypercube [,]N, h is the blur kernel such that h(x) > 
and ∫h(x)dx = , u : Ω →RN is the (“ideal”) original image, n ∼N(, σ ) stands for a
white Gaussian noise, and ∗denotes the convolution operator. The restoration problem is
the recovery of the original image u given > Eq. (.). Non-blind image restoration is
the problem whenever the blur kernel is known, while blind restoration refers to the case
of unknown kernel [, ]. The recovery process in the non-blind case is a typical inverse
problem where the image u is the minimizer of an objective functional of the form
F(u) = Φ(g −h ∗u) + J (∇u).
(.)

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

The functional consists of fidelity term and a regularizer. The fidelity term Φ forces the
smoothed image h ∗u to be close to the observed image g. The commonly used model of
a white Gaussian noise n ∼N(, σ ) leads by the maximum likelihood estimation to the
minimization of the Lnorm of the noise
ΦL= ∥g −h ∗u∥
L(Ω).
(.)
However, in the case of impulsive noise, some amount of pixels do not obey the Gaus-
sian noise model. Minimization of outlier effects can be accomplished by replacing the
quadratic form (> .) with a robust ρ-function [], e.g.,
ΦL= ∥g −h ∗u∥L(Ω).
(.)
The minimization of (> .) or (> .) alone with respect to u is an inverse problem
which is known to be ill posed: small perturbations in the data g may produce unbounded
variations in the solution. To alleviate this problem, a regularization term can be added.
The Tikhonov Lstabilizer []
JL= ∫Ω ∣∇u∣dx,
leads to over smoothing and loss of important edge information. A better edge preservation
regularizer, the Total Variation (TV) term, was introduced by Rudin et al. [, ], where
the Lnorm was replaced by the Lnorm of the image gradients
JL= ∫Ω ∣∇u∣dx.
Still, although the Total Variation regularization outperforms the Lnorm, the image
features – the edges, are not explicitly extracted. The edges are implicitly preserved only
by the image gradients.
An alternative regularizer is the one used in the Mumford–Shah functional [, ].
We recall that this is accomplished by searching for a pair (u, K) where K ⊂Ω denotes the
set of discontinuities of u, the unknown image, such that u ∈H(Ω/K), K ⊂Ω closed in
Ω, and
G(u, K) = β ∫Ω/K ∣∇u∣dx + αHn−(K) < ∞.
(.)
In our study, the regularizer to the restoration problem (> .) is given by
JMS = G(u, K),
its Lvariant [, ], and elliptic or level set approximations of these, as presented next.
This enables the explicit extraction and preservation of the image edges in the course of
the restoration process. We show the advantages of this regularizer in several applications
and noise models (Gaussian and impulsive).
As we have mentioned, Ambrosio and Tortorelli [] introduced an elliptic approxima-
tion Gє(u,v) to G(u, K), as є →+, that we recall here,
Gє(u,v) = β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
(.)


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Replacing the Mumford–Shah regularization term (> .) by Gє(u,v) yields the
proposed restoration model
Fє(u,v) = Φ(g −h ∗u) + β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
(.)
The functional (> .) can also be understood from a generalized robust statistics
viewpoint. This is beyond the scope of this chapter and the interested reader can find the
details in [].
In the rest of the chapter we consider the non-blind restoration problem presented
in [] and its generalizations to several more realistic situations. We consider the problem
of (semi) blind deconvolution, the case of impulsive noise, the color restoration problem
and the case of space-variant blur. We also consider the problem of restoration of piecewise-
constant images from noisy-blurry data using the level set form of the Mumford–Shah
regularizer and image restoration using nonlocal Mumford–Shah–Ambrosio–Tortorelli
regularizers.
..
Non-blind Restoration
We first address the restoration problem with a known blur kernel h and additive Gaussian
noise [, ]. In this case the fidelity term is the Lnorm of the noise (> .), and the
regularizer JMS = Gє(u,v) (> .). The objective functional is therefore
Fє(u,v) = 
∫Ω(g −h ∗u)dx + β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
(.)
The functional (> .) is strictly convex, bounded from below and coercive with respect
to the functions u and v if the other one is fixed. Following [], the alternate minimization
(AM) approach is applied: in each step of the iterative procedure we minimize with respect
to one function and keep the other one fixed. The minimization is carried out using the
Euler–Lagrange (E-L)equations with Neumann boundary conditions where u is initialized
as the blurred image g and v is initialized to .
δFє
δv = β v ∣∇u∣+ α v −
є
−є α △v = 
(.)
δFє
δu = (h ∗u −g) ∗h(−x,−y) −β ∇⋅(v∇u) = 
(.)
>Equation (.) is linear with respect tov and can be easily solved after discretization
by the Minimal Residual algorithm []. The integro-differential equation (> .) can be
solved by the conjugate-gradients method []. The iterative process is stopped whenever
some convergence criterion is satisfied (e.g., ∥un+−un∥< ε∥un∥). > Figure -demon-
strates the outcome of the algorithm. The top-left image is the blurred image g. The kernel
corresponds to horizontal motion blur. The top-right image is the reconstruction obtained

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
The case of a known (nine-pixel horizontal motion) blur kernel. Top-left: corrupted image.
Top-right: restoration using the TV method [, ]. Bottom-left: restoration using the MS
method. Bottom-right: Edge map produced by the MS method
using Total Variation (TV) regularization [, ]. The bottom-left image is the outcome
of the MS regularizer, with a known blur kernel. The bottom-right image shows the associ-
ated edge map v determined by the algorithm. Acceptable restoration is obtained with both
methods. Nevertheless, the MS method yields a sharper result and is almost free of “ghosts”
(white replications of notes) that can be seen in the top-right image (e.g., between the C
notes in the right part of the top stave). The algorithm can be also applied to D images as
shown in > Fig. -. In this example the blur kernel was anisotropic D Gaussian kernel.
..
Semi-Blind Restoration
Blind restoration refers to the case when the blur kernel h is not known in advance. In
addition to being ill posed with respect to the image, the blind restoration problem is ill
posed in the kernel as well. Blind image restoration with joint recovery of the image and


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
⊡Fig. -
D restoration of MRI image. Left: blurred (σx = ., σy = ., σz = .) image. Middle:
recovered image. Right: edge map
the kernel, and regularization of both, was presented by You and Kaveh [], followed by
Chan and Wong []. Chan and Wong suggested to minimize a functional consisting of
a fidelity term and Total Variation (Lnorm) regularization for both the image and the
kernel:
F(u, h) = 
∥h ∗u −g∥
L(Ω) + α∫Ω ∣∇u∣dx + α∫Ω ∣∇h∣dx.
(.)
By this approach the recovered kernel is highly dependent on the image characteristics. It
allows the distribution of edge directions in the image to have an influence on the shape of
the recovered kernel which may lead to inaccurate restoration []. Facing the ill-posedness
of blind restoration with a general kernel, two approaches can be taken. One is to add
relevant data; the other is to constrain the solution. In many practical situations, the blur-
ring kernel can be modeled by the physics/optics of the imaging device and the setup. The
blurring kernel can then be constrained and described as a member in a class of paramet-
ric functions. The blind restoration problem is then reduced to a semi-blind one. Let us
consider the case of isotropic Gaussian blur parameterized by the width σ,
hσ(x) =

πσ e−x
σ,
x ∈Rn.
The semi-blind objective functional then takes the form []
Fє(v,u, σ) = 
∫Ω(hσ ∗u −g)dx + Gє(u,v) + γ ∫Ω ∣∇hσ∣dx.
(.)
The last term in > Eq. (.) stands for the regularization of the kernel, necessary to
resolve the fundamental ambiguity in the division of the apparent blur between the recov-
ered image and the blur kernel. This means that we prefer to reject the hypothesis that the
blur originates from u, and assume that it is due to the convolution with the blur kernel.
From the range of possible kernels, we thus select a wide one. This preference is represented
by the kernel smoothness term: the width of the Gaussian corresponds to its smoothness,
measured by the Lnorm of its gradient. The optimization is carried out by using the alter-
nate minimization approach. The recovered image u is initialized with g, the edge indicator

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

function v is initialized with , and σ with a small number ε which reflects a delta func-
tion kernel. The Euler–Lagrange equations with respect to v and u are given by (> .)
and (> .) respectively. The parameter σ is the solution of
∂Fє
∂σ = ∫Ω [(hσ ∗u −g)(∂hσ
∂σ ∗u) + γ ∂
∂σ ∣∇hσ∣] dx = ,
(.)
which can be calculated by the bisection method. The functional (> .) is not gener-
ally convex. Nevertheless, in practical numerical simulations the algorithm converges to
visually appealing restoration results as can be seen in the second row of > Fig. -.
⊡Fig. -
Semi-blind restoration. Top row: blurred images. Second row: restoration using
the semi-blind method. Third row: original images. Bottom row: edge maps produced by the
semi-blind method


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
..
Image Restoration with Impulsive Noise
Consider an image that has been blurred with a known blur kernel h and contaminated
by impulsive noise. Salt and pepper noise, for instance, is a common model for the effects
of bit errors in transmission, malfunctioning pixels, and faulty memory locations. Image
deblurring algorithms that were designed for Gaussian noise produce inadequate results
with impulsive noise.
The left image of
> Fig. -is a blurred image contaminated by salt-and-pepper
noise, and the right image is the outcome of the Total Variation restoration method [].
A straight forward sequential approach is to first denoise the image, then to deblur it.
This two-stage method is however prone to failure, especially at high noise density. Image
denoising using median-type filtering creates distortion that depends on the neighborhood
size, this error can be strongly amplified by the deblurring process. This is illustrated in
> Fig. -. The top-left and top-right images are the blurred and blurred-noisy images,
respectively. The outcome of × median filtering followed by Total Variation deblur-
ring [] is shown bottom left. At this noise level, the ×neighborhood size of the median
filter is insufficient, the noise is not entirely removed, and the residual noise is greatly ampli-
fied by the deblurring process. If the neighborhood size of the median filter increases to
×, the noise is fully removed, but the distortion leads to inadequate deblurring (bottom
right).
In a unified variational framework, the “ideal” image u can be approximated as the
minimizer of the objective functional [, ]
Fє(u,v) = ∫Ω
√
(h ∗u −g)+ η dx + Gє(u,v).
(.)
⊡Fig. -
Current image deblurring algorithms fail in the presence of salt and pepper noise. Left:
blurred image with salt-and-pepper noise. Right: restoration using the TV method []

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
The failure of the two-stage approach to salt-and-pepper noise removal and image
deblurring. Top-left: blurred image. Top-right: blurred image contaminated by
salt-and-pepper noise. Bottom-left: the outcome of × median ﬁltering, followed by
deblurring. Bottom-right: the outcome of × median ﬁltering, followed by deblurring
The quadratic data-fidelity term is now replaced by the modified Lnorm [] which is
robust to outliers, i.e., to impulse noise. The parameter η ≪enforces the differentiabil-
ity of (> .) with respect to u. Optimization of the functional is carried out using the
Euler–Lagrange equations subject to Neumann boundary conditions:
δFє
δv = β v ∣∇u∣+ α (v −
є ) −є α △v = ,
(.)
δFє
δu =
(h ∗u −g)
√
(h ∗u −g)+ η
∗h(−x,−y) −β ∇⋅(v∇u) = .
(.)
The alternate minimization technique can be applied here as well since the func-
tional (> .) is convex, bounded from below and coercive with respect to either
function u or v if the other one is fixed. Equation (> .) is obviously linear with


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
respect to v. In contrast, (> .) is a nonlinear integro-differential equation. Lineariza-
tion of this equation is carried out using the fixed point iteration scheme as in [, ].
In this method, additional iteration index l serves as intermediate stage calculating un+.
We set u = ul in the denominator, and u = ul+elsewhere, where l is the current iteration
number. Equation (.) can thus be rewritten as
H(v,ul)ul+= G(ul),
l = ,, . . . .
(.)
where H is the linear integro-differential operator
H(v,ul)ul+=
h ∗ul+
√
(h ∗ul −g)+ η
∗h(−x,−y) −β ∇⋅(v∇ul+)
and
G(ul) =
g
√
(h ∗ul −g)+ η
∗h(−x,−y).
(.)
Note that (> .) is now a linear integro-differential equation in ul+.
The discretization of > Eqs. (.) and (> .) yields two systems of linear
algebraic equations. These systems are solved in alternation, leading to the following
iterative algorithm []:
Initialization: u= g,
v= .
. Solve for vn+
(β ∣∇un∣+ α
є −α є △) vn+= α
є.
(.)
. Set un+,= un and solve for un+(iterating on l)
H(vn+,un+,l )un+,l+= G(un+,l).
(.)
. If (∣∣un+−un∣∣L< ε∣∣un∣∣L) stop.
The convergence of the algorithm was proved in [].
> Figure -demonstrates the
performance of the algorithm. The top row shows the blurred images with increasing salt-
and-pepper noise level. The outcome of the restoration algorithm is shown in the bottom
row.
A variant of the Mumford–Shah functional in its Γ-convergence approximation was
suggested by Shah []. In this version the Lnorm of ∣∇u∣in (> .) was replaced by
its Lnorm in the first term of Gє
JMSTV (u,v) = β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
Alicandro et al. [] proved the Γ-convergence of this functional to
JMSTV (u) = β ∫Ω/K ∣∇u∣dx + α ∫K
∣u+ −u−∣
+ ∣u+ −u−∣dH+ ∣Dcu∣(Ω),

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
Top row: the Lena image blurred with a pill-box kernel of radius and contaminated by
salt-and-pepper noise. The noise density is (left to right) ., .and .. Bottom row: the
corresponding recovered images
where u+ and u−denote the image values on two sides of the edge set K, His the
one-dimensional Hausdorff measure and Dcu is the Cantor part of the measure-valued
derivative Du. The Mumford–Shah and Shah regularizers are compared in
> Fig. -.
The blurred and noisy images are shown in the left column. The results of the restora-
tion using the Mumford–Shah stabilizer (MS) are presented in the middle column and the
images recovered using the Shah regularizer (MSTV) are shown in the right column.
The recovery using both methods is satisfactory, but it can be clearly seen that while the
Mumford–Shah restoration performs better in the high-frequency image content (see the
shades for instance), the Shah restoration attracts the image toward the piecewise constant
or cartoon limit which yields images much closer to the “ideal.” This can be explained by
the fact that the Shah regularizer is more robust to image gradients and hence eliminates
high-frequency contributions.
The special case of pure impulse denoising (no blur) is demonstrated in
> Fig. -.
The image on the left shows the outcome of the algorithm of [] with Lnorm for both
the fidelity and regularization, while the recovery using the Lfidelity and MS regularizer
is shown on the right. It can be observed that the better robustness of the MS regularizer
leads to better performance in the presence of salt and pepper noise.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
⊡Fig. -
Left column: the Window image blurred with a pill-box kernel of radius and contaminated
by salt-and-pepper noise. The noise density is (top to bottom) ., and .. Middle column:
the corresponding recovered images with Mumford–Shah (MS) regularization. Right column:
the corresponding recovered images with Shah (MSTV) regularization
⊡Fig. -
Pure impulse denoising. Left column: restoration using the Lregularization []. Right
column: restoration using the MS regularizer
..
Color Image Restoration
We now extend the restoration problem to vector-valued images []. In the case of color
images, the image intensity is defined as u : Ω →[,]. Here g
denotes the observed
image at channel
∈{r, g, b} such that g
= h ∗u + n . The underlying assumption here

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

is that the blur kernel h is common to all of the channels. If the noise is randomly located
in a random color channel, the fidelity term can be modeled as
ΦL= ∫Ω ∑(h ∗u −g )dx
in the case of Gaussian noise, and
ΦL= ∫Ω ∑
√
(h ∗u −g )+ η dx,
η ≪,
(.)
in the case of impulsive noise. The TV regularization can be generalized to
JTV(u) = ∫Ω ∥∇u∥dx,
(.)
where
∥∇u∥=
√
∑
∈{r,g,b}
∣∇u ∣+ μ,
μ ≪.
(.)
The color MS regularizer thus takes the form
JMS(u,v) = β ∫Ω v∥∇u∥dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
(.)
Note that in this regularizer the edge map v is common for the three channels and provides
the necessary coupling between colors. In the same fashion the color MSTV regularizer is
given by
JMSTV (u,v) = β ∫Ω v∥∇u∥dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
(.)
Once again, the optimization technique is alternate minimization with respect to u
and
v []. > Figure -demonstrates the outcome of the different regularizers for an image
blurred by Gaussian kernel and corrupted by both Gaussian and salt-and-pepper noise.
The fidelity term in all cases was selected as ΦL(> .).
The methods based on Mumford–Shah regularizer are superior to the TV stabilizers,
where MSTV provides a result slightly closer to the “ideal” with little loss of details.
..
Space-Variant Restoration
The assumption of space-invariant blur kernel is sometimes inaccurate in real photo-
graphic images. For example, when multiple objects move at different velocities and in
different directions in a scene, one gets space-variant motion blur. Likewise, when a camera
lens is focused on one specific object, other objects nearer or farther away from the lens are
not as sharp. In such situations, different blur kernels degrade different areas of the image.
In some cases it can be assumed that the blur kernel is a piecewise space-variant function.
This means that every sub-domain in the image is blurred by a different kernel. In the full


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
MS
Blurred and noisy
TV
MSTV
a
b
c
d
⊡Fig. -
Recovery of the Lena image blurred by × out-of-focus kernel contaminated by mixture of
Gaussian and salt-and-pepper noise
blind restoration, several operations have to be simultaneously applied: () segmentation
of the subregions, () estimation of the blur kernels, and () recovery of the “ideal” image.
Here we present the simplest case where we assume that the subregions and blur kernels
are known in advance. The segmentation procedure in a semi-blind restoration problem
can be found in []. The non-blind space-variant restoration approach relies on the use of
a global regularizer, which eliminates the requirement of dealing with region boundaries.
As a result, the continuity of the gray levels in the recovered image is inherent. This method
does not limit the number of subregions, their geometrical shape, and the kernel support
size.
Let the open nonoverlapping subsets wi ⊂Ω denote regions that are blurred by kernels
hi, respectively. In addition, Ω/∪wi, denotes the background region blurred by the back-
ground kernel hb, and wi stands for the closure of wi. The region boundaries are denoted
by ∂wi. The recovered image u is the minimizer of the objective functional
F(u,v) = 
∑
i
ηi ∫wi
(hi ∗u−g)dx+ ηb
∫Ω/(∪wi )(hb ∗u−g)dx+JMS(u,v), (.)
where ηi and ηb are positive scalars and JMS(u,v) is the Mumford–Shah regular-
izer (> .). Following the formulation of Chan and Vese [], the domains wi can be
replaced by the Heaviside function H(ϕi), where

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

H(ϕi) =
⎧⎪⎪⎨⎪⎪⎩
, ϕi > ,
, ϕi ≤,
(.)
and ϕi : Ω →R is a level set function such that
∂wi = {x ∈Ω : ϕi(x) = }.
The functional then takes the form
F(u,v) = 
∑
i
ηi ∫Ω(hi ∗u −g)H(ϕi)dx +
ηb
∫Ω(hb ∗u −g)(−∑
i
H(ϕi)) dx + JMS(u,v).
(.)
> Figure -demonstrates the performance of the suggested algorithm. The two
images in the left column were synthetically blurred by different blur kernels within
the marked shapes. The corresponding recovered images are shown in the right col-
umn. Special handling of the region boundaries was not necessary because the MS
⊡Fig. -
Non-blind space-variant restoration. Left column: spatially variant motion blurred images.
Right column: the corresponding recovered images using the suggested method


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
regularizer was applied globally to the whole image, enforcing the piecewise smoothness
constraint. This means that the boundaries of the blurred regions were smoothed within
the restoration process while edges were preserved.
..
Level Set Formulations for Joint Restoration
and Segmentation
We present here other joint formulations for denoising, deblurring, and piecewise-
constant segmentation introduced in [] that can be seen as applications and modifica-
tions of the piecewise-constant Mumford–Shah model in level set formulation presented
in > Sect. .... For related work we refer the reader to [–, , ]. We use a min-
imization approach and we consider the gradient descent method. Let g = h ∗u + n
be a given blurred noisy image, where h is a known blurring kernel (such as the Gaus-
sian kernel) and n represents Gaussian additive noise of zero mean. We assume that the
contours or jumps in the image u can be represented by the m distinct levels {−∞=
l< l< l< ⋯< lm < lm+= ∞} of the same implicit (Lipschitz continuous)
function ϕ : Ω →R partitioning Ω into m + disjoint open regions R j = {x ∈Ω :
l j−< ϕ(x) < l j}, ≤j ≤m + . Thus, we can recover the denoised-deblurred image
u = cH(ϕ−lm)+∑m
j=c jH(ϕ −lm−j+)H(lm−j+−ϕ)+cm+H(l−ϕ) by minimizing the
following energy functional ( > ):
E(c, c, . . . , cm+, ϕ) = ∫Ω
___________
g −h ∗⎛
⎝cH(ϕ −lm) +
m
∑
j=
c jH(ϕ −lm−j+)H(lm−j+−ϕ)
+cm+H(l−ϕ)⎞
⎠
____________

dx +

m
∑
j=∫Ω ∣∇H(ϕ −l j)∣dx.
In the binary case (one level m = , l= ), we assume the degradation model
g = h ∗(cH(ϕ) + c(−H(ϕ))) + n, and we wish to recover u = cH(ϕ) + c(−
H(ϕ)) in Ω together with a segmentation of g. The modified binary segmentation model
incorporating the blur becomes:
inf
c,c,ϕ {E(c, c, ϕ) = ∫Ω ∣g −h ∗(cH(ϕ) + c(−H(ϕ)))∣dx
+ ∫Ω ∣∇H(ϕ)∣dx}.
(.)
We compute the Euler–Lagrange equations minimizing this energy with respect to c,
c, and ϕ. Using alternating minimization, keeping first ϕ fixed and minimizing the energy

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

with respect to the unknown constants cand c, we obtain the following linear system of
equations:
c∫Ω h
dx + c∫Ω hhdx = ∫ghdx,
c∫Ω hhdx + c∫Ω h
dx = ∫ghdx
with the notations h= h ∗H(ϕ) and h= h ∗(−H(ϕ)). Note that the linear system has
a unique solution because the determinant of the coefficient matrix is not zero due to the
Cauchy–Schwartz inequality (∫Ω hhdx)
≤∫Ω h
dx ∫Ω h
dx, where the equality holds
if and only if h= hfor a.e. x ∈Ω. But clearly, h= h ∗H(ϕ)) and h= h ∗(−H(ϕ))
are distinct, thus we have strict inequality.
Keeping now the constants cand cfixed and minimizing the energy with respect to ϕ,
we obtain the evolution equation by introducing an artificial time for the gradient descent
in ϕ(t, x), t > , x ∈Ω
∂ϕ
∂t (t, x) = δ(ϕ)[(˜h ∗g −c˜h ∗(h ∗H(ϕ)) −c˜h ∗(h ∗(−H(ϕ))))
(c−c) +
div ( ∇ϕ
∣∇ϕ∣)],
where ˜h(x) = h(−x).
We show in
> Fig. -a numerical result for joint denoising, deblurring and
segmentation of a synthetic image, in a binary level set approach.
In the case of two distinct levels l< lof the level set function ϕ (m = ), we
wish to recover a piecewise-constant image of the form u = cH(ϕ −l) + cH(l−ϕ)
H(ϕ −l) + cH(l−ϕ) and a segmentation of g, assuming the degradation model
g = h ∗(cH(ϕ −l) + cH(l−ϕ)H(ϕ −l) + cH(l−ϕ)) + n, by minimizing
inf
c,c,c,ϕ E(c, c, c, ϕ) = ∫Ω ∣g −h ∗(cH(ϕ −l) + cH(l−ϕ)H(ϕ −l)
+ cH(l−ϕ))∣dx +


∑
j=∫Ω ∣∇H(ϕ −l j)∣dx. (.)
Similar to the previous binary model with blur, for fixed ϕ, the unknown constants are
computed by solving the linear system of three equations:
c∫h
dx + c∫hhdx + c∫hhdx = ∫ghdx
c∫hhdx + c∫h
dx + c∫hhdx = ∫ghdx
c∫hhdx + c∫hhdx + c∫h
dx = ∫ghdx
where h= h ∗H(ϕ −l), h= h ∗(H(l−ϕ)H(ϕ −l)), and h= h ∗H(l−ϕ).


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
⊡Fig. -
Joint segmentation, denoising, and deblurring using the binary level set model. Top row:
(from left to right) degraded image g (blurred with motion blur kernel of length , oriented
at an angle θ = ○w.r.t. the horizon and contaminated by Gaussian noise with σn = ),
original image. Rows –: initial curves, curve evolution using (> .) at iterations , ,
with
= ⋅, and the restored image u (SNR = .). (c, c): original image
≈(., .), restored u, (., .)
For fixed c, c, and c, by minimizing the functional E with respect to ϕ, we obtain the
gradient descent for ϕ(t, x), t > , x ∈Ω:
∂ϕ
∂t (t, x) = ˜h ∗(g −h ∗(cH(ϕ −l) + cH(l−ϕ)H(ϕ −l)
+ cH(l−ϕ))(cδ(ϕ −l)
+ cH(l−ϕ)δ(ϕ −l) −cH(ϕ −l)δ(l−ϕ) −cδ(l−ϕ)))
+
div ( ∇ϕ
∣∇ϕ∣)(δ(ϕ −l) + δ(ϕ −l)).
(.)
We show in
> Figs. -and
> -a numerical result for joint denoising,
deblurring, and segmentation of the brain image in a multilayer level set approach.

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
Original image (left) and its noisy, blurry version (right) blurred with Gaussian kernel with
σb = and contaminated by Gaussian noise σn = 
..
Image Restoration by Nonlocal Mumford–Shah
Regularizers
The traditional regularization terms discussed in the previous sections (depending on the
image gradient) are based on local image operators, which denoise and preserve edges
very well, but may induce loss of fine structures like texture during the restoration process.
Recently, Buades et al. [] introduced the nonlocal means filter, which produces excel-
lent denoising results. Gilboa and Osher [, ] formulated the variational framework
of NL-means by proposing nonlocal regularizing functionals and the nonlocal operators
such as the nonlocal gradient and divergence. Following Jung et al. [], we present here
nonlocal versions of the Mumford–Shah–Ambrosio–Tortorelli regularizing functionals,
called NL/MSHand NL/MSTV, by applying the nonlocal operators proposed by Gilboa–
Osher to MSHand MSTV respectively, for image restoration in the presence of blur and
Gaussian or impulse noise. In addition, for the impulse noise model, we propose to use
a preprocessed image to compute the weights w (the weights w defined in the NL-means
filter are more appropriate for the additive Gaussian noise case).
We first recall the Ambrosio–Tortorelli regularizer,
ΨMSH
є
(u,v) = β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx,
where ≤v(x) ≤represents the edges: v(x) ≈if x ∈K and v(x) ≈otherwise, є is a
small positive constant, α, β are positive weights.
Shah [] suggested a modified version of the approximation (> .) to the MS
functional by replacing the norm square of ∣∇u∣by the norm in the first term:
ΨMSTV
є
(u,v) = β ∫Ω v∣∇u∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
)dx.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
⊡Fig. -
Curve evolution and restored u using (> .),
= .⋅, (c, c, c): original image
≈(., ., .), restored u ≈(., ., .)

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

This functional Γ converges to the other ΨMSTV functional []:
ΨMSTV(u) = β ∫Ω/K ∣∇u∣dx + α ∫K
∣u+ −u−∣
+ ∣u+ −u−∣dH+ ∣Dcu∣(Ω),
where u+ and u−denote the image values on two sides of the jump set K = Ju of u, and
Dcu is the Cantor part of the measure-valued derivative Du.
Nonlocal methods in image processing have been explored in many papers because
they are well adapted to texture denoising, while the standard denoising models working
with local image information seem to consider texture as noise, which results in losing
texture. Nonlocal methods are generalized from the neighborhood filters and patch-based
methods. The idea of neighborhood filter is to restore a pixel by averaging the values of
neighboring pixels with a similar gray level value.
Buades et al. [] generalized this idea by applying the patch-based methods, proposing
a famous neighborhood filter called nonlocal-means (or NL-means):
NLu(x) =

C(x) ∫Ω e−da(u(x),u(y))
h
u(y)dy
da(u(x),u(y)) = ∫RGa(t)∣u(x + t) −u(y + t)∣dt
where da is the patch distance, Ga is the Gaussian kernel with standard deviation a deter-
mining the patch size, C(x) = ∫Ω e−da(u(x),u(y))
h
dy is the normalization factor, and h is the
filtering parameter which corresponds to the noise level; usually we set it to be the standard
deviation of the noise. The NL-means not only compares the gray level at a single point but
the geometrical configuration in a whole neighborhood (patch). Thus, to denoise a pixel,
it is better to average the nearby pixels with similar structures rather than just with similar
intensities.
In practice, we use the search window Ωw = {y ∈Ω : ∣y −x∣≤r} instead of Ω
(semi-local) and the weight function at (x, y) ∈Ω×Ω depending on a function u : Ω →R
w(x, y) = exp (−da(u(x),u(y))
h
).
The weight function w(x, y) gives the similarity of image features between two pixels x
and y, which is normally computed based on the blurry noisy image g.
Based on the gradient and divergence definitions on graphs in the context of machine
learning, Gilboa and Osher [] derived the nonlocal operators. Let u : Ω →R be a
function, and w : Ω × Ω →R is a weight function assumed to be nonnegative and sym-
metric. The nonlocal gradient ∇wu : Ω × Ω →R is defined as the vector (∇wu)(x, y) :=
(u(y) −u(x))
√
w(x, y). Hence, the norm of the nonlocal gradient of u at x ∈Ω is
defined as
∣∇wu∣(x) =
√
∫Ω (u(y) −u(x))w(x, y)dy.


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
The nonlocal divergence divwa→v : Ω →R of the vector a→v : Ω × Ω →R is defined as the
adjoint of the nonlocal gradient
(divwa→v )(x) := ∫Ω (v(x, y) −v(y, x))
√
w(x, y)dy.
Based on these nonlocal operators, they introduced nonlocal regularizing functionals of
the general form
Ψ(u) = ∫Ω ϕ(∣∇wu∣)dx,
where ϕ(s) is a positive function, convex in √s with ϕ() = . Inspired by these ideas,
we present nonlocal versions of Ambrosio–Tortorelli and Shah approximations to the MS
regularizer for image denoising-deblurring. This is also continuation of work by Bar et al.
[–], as presented in the first part of this section.
We propose the following nonlocal approximated Mumford–Shah and Ambrosio–
Tortorelli regularizing functionals (NL/MS) by applying the nonlocal operators to the
approximations of the MS regularizer,
ΨNL/MS(u,v) = β ∫Ω vϕ(∣∇wu∣)dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx,
where ϕ(s) = s and ϕ(s) = √s correspond to the nonlocal version of MSHand MSTV
regularizers, called here NL/MSHand NL/MSTV, respectively:
ΨNL/MSH(u,v) = β ∫Ω v∣∇wu∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx
ΨNL/MSTV (u,v) = β ∫Ω v∣∇wu∣dx + α ∫Ω (є∣∇v∣+ (v −)
є
) dx.
In addition, we use these nonlocal regularizers to deblur images in the presence of Gaus-
sian or impulse noise. Thus, by incorporating the proper fidelity term depending on the
noise model, we design two types of total energies as
Gaussian noise model:
EG(u,v) = ∫Ω (g −h ∗u)dx + ΨNL/MS(u,v),
Impulse noise model:
EIm(u,v) = ∫Ω ∣g −h ∗u∣dx + ΨNL/MS(u,v).
Minimizing these functionals in u and v, we obtain the Euler–Lagrange equations:
Gaussian noise model:
∂EG
∂v = βvϕ(∣∇wu∣) −єα △v + α (v −
є ) = ,
∂EG
∂u = h∗∗(h ∗u −g) + LNL/MSu = .

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

Impulse noise model:
∂EIm
∂v
= βvϕ(∣∇wu∣) −єα △v + α (v −
є ) = ,
∂EIm
∂u
= h∗∗sign(h ∗u −g) + LNL/MSu = ,
where h∗(x) = h(−x) and
LNL/MSu = −∫Ω(u(y) −u(x))w(x, y)
[(v(y)ϕ′(∣∇w(u)∣(y))
+v(x)ϕ′(∣∇w(u)∣(x))] dy.
More specifically, the NL/MSHand NL/MSTV regularizers give
LNL/MSHu = −∇w ⋅(v(x)∇wu(x))
= −∫Ω(u(y) −u(x))w(x, y)
[v(y) + v(x)] dy,
LNL/MSTV u = −∇w ⋅(v(x) ∇wu(x)
∣∇wu(x)∣)
= −∫Ω (u(y) −u(x))w(x, y)[
v(y)
∣∇wu∣(y) +
v(x)
∣∇wu∣(x)]dy.
The energy functionals EG(u,v) and EIm(u,v) are convex in each variable and
bounded from below. Therefore, to solve two Euler–Lagrange equations simultaneously,
the alternate minimization (AM) approach is applied: in each step of the iterative proce-
dure, we minimize with respect to one function while keeping the other one fixed. Due to
its simplicity, we use the explicit scheme for u based on the gradient descent method and
the Gauss–Seidel scheme for v. Note that since both energy functionals are not convex in
the joint variable, we may compute only a local minimizer. However, this is not a drawback
in practice, since the initial guess for u in our algorithm is the data g.
⊡Fig. -
Original and noisy blurry images (noisy blurry image using the pill-box kernel of radius 
and Gaussian noise with σn = )


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
Furthermore, to extend the nonlocal methods to the impulse noise case, we need a
preprocessing step for the weight function w(x, y) since we cannot directly use the data
g to compute w. In other words, in the presence of impulse noise, the noisy pixels tend to
have larger weights than the other neighboring points, so it is likely to keep the noise value
at such pixel. Thus, we propose a simple algorithm to obtain first a preprocessed image
f , which removes the impulse noise (outliers) as well as preserves the textures as much
⊡Fig. -
Recovery of noisy blurry image from > Fig. -. Top row: recovered image u using MSTV
(SNR = .), MSH(SNR = .). Third row: recovered image u using NL/MSTV (SNR =
.), NL/MSH(SNR = .). Second, bottom rows: corresponding residuals g −h ∗u.
β = .(MSTV), .(NL/MSTV), .(MSH), .(NL/MSH), α = .,
є = .

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

as possible. Basically, we use the median filter, well known for removing impulse noise.
However, if we apply one step of the median filter, then the output may be too smoothed
out. In order to preserve the fine structures as well as to remove the noise properly, we use
the idea of Bregman iteration [, ], and we propose the following algorithm to obtain a
preprocessed image f that will be used only in the computation of the weight function:
Initialize : r= , f= .
do (iterate n = ,,, . . .)
fn+= median(g + rn,[a a])
rn+= rn + g −h ∗fn+
while ∥g −h ∗fn∥> ∥g −h ∗fn+∥
[Optional] fm = median(fm,[b b])
⊡Fig. -
Recovery of noisy blurry image with Gaussian kernel with σ = and salt-and-pepper noise with d = ..
Top row: original image, blurry image, noisy-blurry image. Middle row: recovered images using MSTV
(SNR = .), MSH(SNR = .). Bottom row: recovered images using NL/MSTV (SNR = .),
NL/MSH(SNR = .). Parameters: β = .(MSTV), .(NL/MSTV), α = ., є = .. Parameters:
β = (MSH), .(NL/MSH), α = ., є = .


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
where g is the given noisy blurry data, median(u,[a a]) is the median filter of size a × a
with input u; the optional step is needed in the case when the final fm still has some salt-
and-pepper-like noise. This algorithm is simple and requires a few iterations only, so it
takes less than s for a ×size image. The preprocessed image f will be used only in
the computation of the weights w, while keeping g in the data fidelity term, thus artifacts
are not introduced by the median filter.
We show in > Figs. -and > -an experimental result for image restoration of
a boat image degraded by the pill-box kernel blur of radius and additive Gaussian noise.
The nonlocal methods give better reconstruction.
We show in > Figs. -and > -an experimental result for image restoration of a
woman image degraded by Gaussian kernel blur and salt-and-pepper noise. > Figure -
shows the edge set v for the four results. The nonlocal methods give better reconstruction.
We show in
> Fig. -an experimental result for restoration of the Einstein image
degraded by motion kernel blur and random-valued impulse noise. The nonlocal methods
give better reconstruction.
⊡Fig. -
Edge map v using the MS regularizers in the recovery of the Lena image blurred with Gaussian blur
kernel with σb = and contaminated by salt-and-pepper noise with density d = .. Top: (left) MSTV,
(right) NL/MSTV. Bottom: (left) MSH, (right) NL/MSH

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

⊡Fig. -
Comparison between MSHand NL/MSHwith the image blurred and contaminated by high density
(d = .) of random-valued impulse noise. Top: noisy blurry image blurred with the motion blur in
recovered images using MSH(left, SNR = .) and NL/MSH(right, SNR = .). Bottom: noisy
blurry image blurred with the Gaussian blur in recovered images using MSH(left, SNR = .) and
NL/MSH(right, SNR = .). Top: β = .(MSH), .(NL/MSH), α = ., є = .. Bottom: β = .
(MSH), .(NL/MSH), α = .,є = .
.
Conclusion
We conclude this chapter by first summarizing its main results. The Mumford–Shah model
for image segmentation has been presented, together with its main properties. Several
approximations to the Mumford and Shah energy have been discussed, with an emphasis
on phase-field approximations and level set approximations. Several numerical results
for image segmentation by these methods have been presented. In the last section of
the chapter, several restoration problems were addressed in a variational framework. The
fidelity term was formulated according to the noise model (Gaussian, impulse, multi-
channel impulse). First, the a priori piecewise-smooth image model was mathematically
integrated into the functional as an approximation of the Mumford–Shah segmentation
elements by the Γ-convergence formulation. Comparative experimental results show the
superiority of this regularizer with respect to modern state-of-the-art restoration tech-
niques. Also, the piecewise-constant level set formulations of the Mumford–Shah energy
have been applied to image restoration (related to relevant work by Kim et al. []), joint
with segmentation. Finally, in the last section, the Ambrosio–Tortorelli approximations
and Bar et al. restoration models have been extended to nonlocal regularizers, inspired


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
by the work of Gilboa et al. These models produce much improved restoration results for
images with texture and fine details.
.
Recommended Reading
Many more topics on the Mumford–Shah model and its applications have been explored in
image processing, computer vision, and more generally in inverse problems. This chapter
contains only a small sample of results and methods. As mentioned before, we recommend
detailed monographs on the Mumford–Shah problem and related theoretical and applica-
tions topics by Blake and Zisserman [], by Morel and Solimini [], by Chambolle [],
by Ambrosio et al. [], by David [], and by Braides []. Also, the monographs by Aubert
and Kornprobst [] and by Chan and Shen [] contain chapters presenting the Mumford
and Shah problem and its main properties.
We would like to mention the work by Cohen et al. [, ] on using curve evolution
approach and the Mumford–Shah functional for detecting the boundary of a lake. The
work by Aubert et al. [] also proposes an interesting approximation of the Mumford–Shah
energy by a family of discrete edge-preserving functionals, with Γ-convergence result.
References and Further Reading
. Adams RA () Sobolev spaces. Academic,
New York
. Alicandro R, Braides A, Shah J () Free-
discontinuity problems via functionals involving
the L-norm of the gradient and their approxima-
tion. Interfaces Free Bound :–
. Ambrosio L () A compactness theorem for
a special class of functions of bounded variation.
Boll Un Mat Ital (B):–
. Ambrosio L, Fusco N, Pallara D () Func-
tions of bounded variation and free discontinuity
problems. Oxford University Press, New York
. Ambrosio L, Tortorelli VM () Approxima-
tion of functionals depending on jumps by elliptic
functionals via Γ-convergence. Comm Pure Appl
Math ():–
. Ambrosio L, Tortorelli VM () On the approx-
imation of free discontinuity problems. Boll Un
Mat Ital B():–
. Aubert G, Blanc-Féraud L, March R () An
approximation of the Mumford-Shah energy by
a family of discrete edge-preserving functionals.
Nonlinear Anal ():–
. Aubert G, Kornprobst P () Mathemati-
cal problems in image processing. Springer,
New York
. Bar L, Brook A, Sochen N, Kiryati N ()
Deblurring of color images corrupted by impul-
sive noise. IEEE Trans Image Process ():–

. Bar L, Sochen N, Kiryati N () Varia-
tional pairing of image segmentation and blind
restoration. In Proceedings of th European con-
ference on computer vision, vol of LNCS, pp
–
. Bar L, Sochen N, Kiryati N () Image deblur-
ring in the presence of salt-and-pepper noise. In
Proceedings of th international conference on
scale space and PDE methods in computer vision,
vol of LNCS, pp –
. Bar L, Sochen N, Kiryati N () Image deblur-
ring in the presence of impulsive noise. Int J
Comput Vis :–
. Bar L, Sochen N, Kiryati N () Semi-blind
image restoration via Mumford-Shah regulariza-
tion. IEEE Trans Image Process ():–

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

. Bar L, Sochen N, Kiryati N () Convergence of
an iterative method for variational deconvolution
and impulsive noise removal. SIAM J Multiscale
Model Simulat :–
. Bar L, Sochen N, Kiryati N () Restoration
of images with piecewise space-variant blur. In
Proceedings of st international conference on
scale space and variational methods in computer
vision, pp –
. Blake A, Zisserman A () Visual reconstruc-
tion. MIT Press, Cambridge
. Bourdin B () Image segmentation with a
finite element method. MAN Math Model
Numer Anal ():–
. Bourdin B, Chambolle A () Implementa-
tion of an adaptive finite-element approximation
of the Mumford-Shah functional. Numer Math
():–
. Braides
A
()
Approximation
of
free-
discontinuity problems, vol of Lecture notes
in mathematics. Springer, Berlin
. Braides A, Dal Maso G () Nonlocal approx-
imation of the Mumford-Shah functional. Calc
Var :–
. Bregman LM () The relaxation method for
finding common points of convex sets and its
application to the solution of problems in convex
programming. USSR Comp Math Phys :–
. Buades A, Coll B, Morel JM () A review
of image denoising algorithms, with a new one.
SIAM MMS ():–
. Chambolle
A
()
Un
théorème
de
γ-
convergence pour la segmentation des signaux.
C R Acad Sci Paris Sér. I Math ():–
. Chambolle A () Image segmentation by vari-
ational methods: Mumford and Shah functional,
and the discrete approximation. SIAM J Appl
Math :–
. Chambolle A () Finite-differences discretiza-
tions of the Mumford-Shah functional. MAN
Math Model Numer Anal ():–
. Chambolle A () Inverse problems in image
processing and image segmentation: some math-
ematical and numerical aspects. In: Chidume CE
(ed) ICTP Lecture notes series, vol . ICTP
. Chambolle A, Dal Maso G () Discrete
approximation of the Mumford-Shah functional
in dimension two. MAN Math Model Numer
Anal ():–
. Chan T, Vese L () An active contour model
without
edges.
Lecture
Notes
Comput
Sci
:–
. Chan T, Vese L () An efficient variational
multiphase motion for the Mumford-Shah seg-
mentationmodel.InthAsilomar conferenceon
signals,systems,andcomputers,vol ,pp –
. Chan T, Vese L () Active contours without
edges. IEEE Trans Image Process :–
. Chan T, Vese L () A level set algorithm
for minimizing the Mumford-Shah functional
in image processing. In IEEE/Computer Society
proceedings of the st IEEE workshop on varia-
tional and level set methods in computer vision,
pp –
. Chan TF, Shen J () Image processing and
analysis. Variational, PDE, wavelet, and stochastic
methods. SIAM, Philadelphia
. Chan TF, Wong CK () Total variation blind
deconvolution. IEEE Trans Image Process :
–
. Chung G, Vese LA () Energy minimization
based segmentation and denoising using a multi-
layer level set approach. Lecture Notes in Comput
Sci :–
. Chung G, Vese LA () Image segmentation
using a multilayer level-set approach. Computing
Visual Sci ():–
. Cohen L, Bardinet E, Ayache N () Surface
reconstruction using active contour models. In
SPIE ’conference on geometric methods in
computer vision, San Diego, July 
. Cohen
LD
()
Avoiding
local
minima
for deformable curves in image analysis. In:
Le Méhauté A, Rabut C, Schumaker LL (eds)
Curves and Surfaces with applications in CAGD,
pp –
. Dal Maso G () An introduction to Γ-
convergence. Progress in nonlinear differential
equations and their applications. Birkhäuser,
Boston
. David G () Singular sets of minimizers for
the Mumford-Shah functional. Birkhäuser Ver-
lag, Basel
. Evans LC () Partial differential equations.
American Mathematical
Society, Providence,
Rhode Island
. Evans LC, Gariepy RF () Measure theory and
fine properties of functions. CRC Press


Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration
. Geman S, Geman D () Stochastic relaxation,
Gibbs distributions, and the Bayesean restoration
of images. IEEE TPAMI :–
. Gilboa G, Osher S () Nonlocal linear image
regularization
and
supervised
segmentation.
SIAM MMS ():–
. Gilboa G, Osher S () Nonlocal operators
with applications to image processing. Multiscale
Model Simulat ():–
. Huber PJ () Robust statistics. Wiley, New York
. Jung M, Chung G, Sundaramoorthi G, Vese LA,
Yuille AL () Sobolev gradients and joint
variational image segmentation, denoising and
deblurring. In: IS&T/SPIE on electronic imag-
ing, vol of Computational imaging VII,
pp I––I–
. Jung M, Vese LA () Nonlocal variational
image deblurring models in the presence of gaus-
sian or impulse noise. In: International confer-
ence on Scale Space and Variational Methods in
Computer Vision (SSVM’ ), vol of LNCS,
pp –
. Kim J, Tsai A, Cetin M, Willsky AS () A curve
evolution-based variational approach to simulta-
neous image restoration and segmentation. In:
Proceedings of IEEE international conference on
image processing, vol , pp –
. Koepfler G, Lopez C, Morel JM () A multi-
scale algorithm for image segmentation by vari-
ational methods. SIAM J Numer Anal ():
–
. Kundur D, Hatzinakos D () Blind image
deconvolution. Signal Process Mag :–
. Kundur D, Hatzinakos D () Blind image
deconvolution revisited. Signal Process Mag
:–
. Larsen CJ () A new proof of regularity for
two-shaded image segmentations. Manuscripta
Math :–
. Leonardi GP, Tamanini I () On minimizing
partitions with infinitely many components. Ann
Univ Ferrara - Sez. VII - Sc. Mat XLIV:–
. Li C, Kao C-Y, Gore JC, Ding Z () Implicit
active contours driven by local binary fitting
energy. In IEEE conference on computer vision
and pattern recognition (CVPR), CVPR’
. Dal Maso G, Morel JM, Solimini S () Varia-
tional approach in image processing – existence
and approximation properties. C R Acad Sci Paris
Sér. I Math ():–
. Dal Maso G, Morel JM, Solimini S () A varia-
tional method in image segmentation - existence
and approximation properties. Acta Mathem
(–):–
. Massari U, Tamanini I () On the finiteness of
optimal partitions. Ann Univ Ferrara – Sez VII -
Sc Mat XXXIX:–
. Modica
L
()
The
gradient
theory
of
phase
transitions
and
the
minimal
inter-
face criterion. Arch Rational Mech Anal :
–
. Modica L, Mortola S () Un esempio di
γ-convergenza.
Boll
Un
Mat
Ital
B()():
–
. Mohieddine R, Vese LA () Open curve
level set formulations for the Mumford and
Shah segmentation model. UCLA C.A.M. Report,
pp –
. Morel J-M, Solimini S () Variational methods
in image segmentation. Birkhäuser, Boston
. Mumford D, Shah J () Boundary detection by
minimizing functionals. In Proceedings of IEEE
conference on computer vision and pattern recog-
nition, pp –
. Mumford D, Shah J () Boundary detection by
minimizing functionals. In: Ullman S, Richards
W (eds) Image understanding. Springer, Berlin,
pp –
. Mumford D, Shah J () Optimal approxima-
tions by piecewise smooth functions and asso-
ciated variational problems. Comm Pure Appl
Math :–
. Neuberger JW () Sobolev gradients and dif-
ferential equations. Springer lecture notes in
mathematics, vol 
. Nikolova M () Minimizers of cost-functions
involving nonsmooth data-fidelity terms: applica-
tion to the processing of outliers. SIAM J Numer
Anal :–
. Nikolova M () A variational approach to
remove outliers and impulse noise. J Math Imag-
ing Vis :–
. Osher S, Burger M, Goldfarb D, Xu J, Yin W
() An iterative regularization method for
total variation based image restoration. SIAM
MMS :–
. Osher S, Sethian JA () Fronts propagat-
ing with curvature-dependent speed: algorithms
based on Hamilton-Jacobi formulation. J Comput
Phys :–

Mumford and Shah Model and its Applications to Image Segmentation and Image Restoration 

. Osher SJ,FedkiwRP() Level setmethodsand
dynamic implicit surfaces. Springer, New York
. Renka RJ () A Simple Explanation of the
Sobolev Gradient Method. (online manuscript at
http: / /www.cse.unt.edu /∼renka/papers/sobolev.
pdf)
. Richardson WB () Sobolev gradient precon-
ditioning for image processing PDEs. Commun
Numer Meth Eng :–
. Rudin L, Osher S () Total variation based
image restoration with free local constraints. In:
Proceedings of IEEE international conference on
image processing, vol , Austin, pp –
. Rudin LI, Osher S, Fatemi E () Non linear
total variation based noise removal algorithms.
Physica D :–
. Samson C, Blanc-Féraud L, Aubert G, Zerubia J
() Multiphase evolution and variational
image classification. Technical
Report ,
INRIA Sophia Antipolis
. Sethian JA () Level set methods. Evolving
interfacesingeometry,fluidmechanics,computer
vision, and materials science. Cambridge Univer-
sity Press
. Sethian
JA
()
Level
set
methods
and
fast
marching
methods.
Evolving interfaces
in
computational
geometry,
fluid
mechan-
ics, computer vision, and materials science.
Cambridge University Press, Cambridge
. Shah J () A common framework for curve
evolution, segmentation and anisotropic diffu-
sion. In: Proceedings of IEEE conference on com-
puter vision and pattern recognition, pp –
. Smereka P () Spiral crystal growth. Physica
D :–
. Tamanini I () Optimal approximation by
piecewise constant functions. In: Progress in non-
linear differential equations and their applica-
tions, vol . Birkhäuser Verlag, Basel, pp –
. Tamanini I, Congedo G () Optimal segmen-
tation of unbounded functions. Rend Sem Mat
Univ Padova :–
. Tikhonov AN, Arsenin V () Solutions of ill-
posed problems. Winston, Washington
. Tsai A, Yezzi A, Willsky A () Curve evolution
implementation of the Mumford-Shah functional
for image segmentation, denoising, interpolation,
and magnification. IEEE Trans Image Process
():–
. Vese LA, Chan TF () A multiphase level
set framework for image segmentation using the
Mumford and Shah model. Int J Comput Vis
():–
. Vogel CR, Oman ME () Fast, robust total
variation-based reconstruction of noisy, blurred
images. IEEE Trans Image Process :–
. Weisstein
EW
Minimal
residual
method.
MathWorld–A Wolfram Web Resource. http://ma
thworld.wolfram.com/MinimalResidualMethod.
html.
. You Y, Kaveh M () A regularization approach
to joint blur identification and image restoration.
IEEE Trans Image Process :–
. Zhao HK, Chan T, Merriman B, Osher S ()
A variational level set approach to multiphase
motion. J Comput Phys :–


Local Smoothing
Neighborhood Filters
Jean-Michel Morel ⋅Antoni Buades ⋅Tomeu Coll
.
Introduction.....................................................................
.
Denoising........................................................................
..
Analysis of Neighborhood Filter as a Denoising Algorithm.....................
..
Neighborhood Filter Extension: The NL-Means Algorithm.....................
..
Extension to Movies....................................................................
.
Asymptotic.......................................................................
..
PDE Models and Local Smoothing Filters..........................................
..
Asymptotic Behavior of Neighborhood Filters (Dimension )..................
..
The Two-Dimensional Case..........................................................
..
A Regression Correction of the Neighborhood Filter.............................
..
The Vector-Valued Case...............................................................
...Interpretation...........................................................................
.
Variational and Linear Diffusion..............................................
..
Linear Diffusion: Seed Growing......................................................
..
Linear Diffusion: Histogram Concentration.......................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Local Smoothing Neighborhood Filters
.
Introduction
The neighborhood filter or sigma filter is attributed to J.S. Lee [] (in ) but goes back
to L. Yaroslavsky and the Sovietic image processing theory []. This filter is introduced in
a denoising framework for the removal of additive white noise:
v(x) = u(x) + n(x),
where x indicates a pixel site, v(x) is the noisy value, u(x) is the “true” value at pixel x, and
n(x) is the noise perturbation. When the noise values n(x) and n(y) at different pixels are
assumed to be independent random variables and independent of the image value u(x),
one talks about “white noise.” Generally, n(x) is supposed to follow a Gaussian distribution
of zero mean and standard deviation σ.
Lee and Yaroslavsky proposed to smooth the noisy image by averaging only those
neighboring pixels that have a similar intensity. Averaging is the principle of most denois-
ing methods. The variance law in probability theory ensures that if N noise values are
averaged, the noise standard deviation is divided by
√
N. Thus, one should, for example,
find for each pixel nine other pixels in the image with the same color (up to the fluctuations
due to noise) in order to reduce the noise by a factor . A first idea might be to chose the
closest ones. Now, the closest pixels have not necessarily the same color as illustrated in
> Fig. -. Look at the red pixel placed in the middle of
> Fig. -. This pixel has five
red neighbors and three blue ones. If the color of this pixel is replaced by the average of
the colors of its neighbors, it turns blue. The same process would likewise redden the blue
pixels of this figure. Thus, the red and blue border would be blurred. It is clear that in order
(219,96,85)
(225,107,124)
(228,101,126)
(194,62,55)
(185,71,85)
(195,67,63)
(147,174,219)
(135,166,216)
(144,185,226)
⊡Fig. -
The nine pixels in the baboon image on the right have been enlarged. They present a high
red-blue contrast. In the red pixels, the ﬁrst (red) component is stronger. In the blue pixels,
the third component, blue, dominates

Local Smoothing Neighborhood Filters 

to denoise the central red pixel, it is better to average the color of this pixel with the nearby
red pixels and only them, excluding the blue ones. This is exactly the technique proposed
by neighborhood filters.
The original sigma and neighborhood filter were proposed as an average of the spatially
close pixels with a gray level difference lower than a certain threshold h. Thus, for a certain
pixel x, the denoised value is the average of pixels in the spatial and intensity neighborhood:
{y ∈Ω ∣∥x −y∥< ρ and ∣u(x) −u(y)∣< h}.
However, in order to make it coherent with further extensions and facilitate the mathemat-
ical development of this chapter, we will write the filter in a continuous framework under a
weighted average form. We will denote the neighborhood or sigma filter by NF and define
it for a pixel x as
NFh,ρu(x) =

C(x) ∫Bρ(x) u(y)e−∣u(y)−u(x)∣
h
dy,
(.)
where Bρ(x) is a ball of center x and radius ρ > , h > is the filtering parameter,
and C(x) = ∫Bρ(x) e−∣u(y)−u(x)∣
h
dy is the normalization factor. The parameter h controls
the degree of color similarity needed to be taken into account in the average. This value
depends on the noise standard deviation σ, and it was set to .σ in [] and []. We will
justify the choice of this value for the h parameter in > Sect. ..
The Yaroslavsky and Lee’s filter (> .) is less known than more recent versions,
namely, the SUSAN filter [] and the Bilateral filter []. Both algorithms, instead of con-
sidering a fixed spatial neighborhood Bρ(x), weigh the distance to the reference pixel x:
SFh,ρu(x) =

C(x) ∫Ω u(y)e
−∣y−x∣
ρe−∣u(y)−u(x)∣
h
dy,
(.)
where C(x) = ∫Ω e
−∣y−x∣
ρe−∣u(y)−u(x)∣
h
dy is the normalization factor and ρ > is now a spatial
filtering parameter. Even if the SUSAN algorithm was previously introduced, the whole
literature refers to it as the Bilateral filter. Therefore, we shall call this filter by the latter
name in subsequent sections.
The only difference between the neighborhood filter and the Bilateral or SUSAN filter
is the way the spatial component is treated. While for the neighborhood filter all pixels
within a certain spatial distance are treated uniformly, for the Bilateral or SUSAN filter,
pixels closer to the reference one are considered more important. We display in > Fig. -
a denoising experience where a Gaussian white noise of standard deviation has been
added to a non-noisy image. We display the denoised image by both the neighborhood
and Bilateral filters. We observe that both filters avoid the excessive blurring caused by a
Gaussian convolution and preserve all contrasted edges in the image.
The above denoising experience was applied to color images. In order to clarify how the
neighborhood filters are implemented in this case, we remind that each pixel x is a triplet
of values u(x) = (u(x),u(x),u(x)), denoting the red, green, and blue components.


Local Smoothing Neighborhood Filters
⊡Fig. -
From left to right: noise image, Gaussian convolution, neighborhood ﬁlter, and Bilateral
ﬁlter. The neighborhood and Bilateral ﬁlters avoid the excessive blurring caused by a
Gaussian convolution and preserve all contrasted edges in the image
Then, the filter rewrites
NFh,ρui(x) =

C(x) ∫Bρ(x) ui(y)e−∣∣u(y)−u(x)∣∣
h
dy,
(.)
∣∣u(y) −u(x)∣∣being the average of the distances of the three channels:
∣∣u(y) −u(x)∣∣= 


∑
i=
∣ui(y) −ui(x)∣.
The same definition applies for the SUSAN or Bilateral filter by incorporating the spatial
weighting term. The above definition naturally extends to multispectral images with an
arbitrary number of channels. Bennett et al. [] applied it to multispectral data with an
infrared channel, and Peng et al. [] for general multispectral data.
The evaluation of the denoising performance of neighborhood filters and comparison
with state-of-the-art algorithms are postponed to
> Sect. .. In the same section, we
present a natural extension of the neighborhood filter, the NL-means algorithm, proposed
in []. This algorithm evaluates the similarity between two pixels x and y not only by the
intensity or color difference of x and y but by the difference of intensities in a whole spatial
neighborhood.
The Bilateral filter was also proposed as a filtering algorithm with a filtering scale
depending on both parameters h and ρ. Thus, taking several values for these parameters,
we obtain different filtered images and corresponding residuals in a multi-scale framework.
In
> Fig. -, we display several applications of the Bilateral filter for different values of
the parameters h and ρ. We also display the differences between the original and filtered
images in
> Fig. -. For moderated values of h, this residual contains details and tex-
ture, but it does not contain contrasted edges. This contrasted information is removed by
the bilateral filter only for large values of h. In that case, all pixels are judged as having a

Local Smoothing Neighborhood Filters 

⊡Fig. -
Several applications of the Bilateral ﬁlter for increasing values of parameters ρ and h. The
parameter ρ increases from top to bottom taking values {, , } and h increases from left
to right taking values {, , , }
similar intensity level and the weight is set taking into account only the spatial component.
It is well known that the residual by such an average is proportional to the Laplacian of the
image. In
> Sect. ., we will mathematically analyze the asymptotical expansion of the
neighborhood residual image.
This detail removal of the Bilateral while conserving very contrasted edges is the key in
many image and video processing algorithms. Durand and Dorsey [] use this property
in the context of tone mapping whose goal is to compress the intensity values of a high-
dynamic-range image. The authors isolate the details before compressing the range of the
image. Filtered details and texture are added back at the final stage. Similar approaches
for image editing are presented by Bae et al. [], which transfer the visual look of an artist
picture onto a casual photograph. Eisemann and Durand [] and Petschnigg et al. []
combine the filtered and residual image of a flash and non-flash image of the same scene.
These two last algorithms, in addition, compute the weight configuration in one image of
the pair and average the intensity values of the other image. As we will see in > Sect. .,


Local Smoothing Neighborhood Filters
⊡Fig. -
Residual diﬀerences between original and ﬁltered images in > Fig. -. For moderated
values of h this residual contains details and texture but it doesn’t contain contrasted edges.
These contrasted information is removed by the bilateral ﬁlter only for large values of h
this is a common feature with iterative versions of neighborhood filters. However, for these
applications, both images of the pair must be correctly and precisely registered.
The iteration of the neighborhood filter was not originally considered by the pioneer-
ing works of Lee and Yaroslavsky. However, recent applications have shown its interest. The
iteration of the filter as a local smoothing operator tends to piecewise constant images by
creating artificial discontinuities in regular zones. Barash et al. [] showed that an itera-
tion of the neighborhood filter was equivalent to a step of a certain numerical scheme of
the classical Perona–Malik equation []. A complete proof of the equivalence between
the neighborhood filter and the Perona–Malik equation was presented in [] including a
modification of the filter to avoid the creation of shocks inside regular parts of the image.
Another theoretical explanation of the shock effect of the neighborhood filters can be found
in Van de Weijer and van den Boomgaard [] and Comaniciu []. Both papers show that
the iteration of the neighborhood filter process makes points tend to the local modes of the
histogram but in a different framework: the first for images and the second for any dimen-
sional clouds of points. This discontinuity or shock creation in regular zones of the image

Local Smoothing Neighborhood Filters 

is not desirable for filtering or denoising applications. However, it can be used for image
or video editing as proposed by Winnemoller et al. [] in order to simplify video content
and achieve a cartoon look.
Even if it may seem paradoxical, linear schemes have showed to be more useful than
nonlinear ones for iterating the neighborhood filter, that is, the weight distribution for each
pixel is computed once and is maintained during the whole iteration process. We will show
in
> Sect. .that by computing the weights on an image and keeping them constant
during the iteration process, a histogram concentration phenomenon makes the filter a
powerful segmentation algorithm. The same iteration is useful to linearly diffuse or filter
any initial data or seeds as proposed by Grady et al. [] for medical image segmentation
or [] for colorization (see > Fig. -for an example). The main hypothesis for this seed
diffusion algorithm is that pixels having a similar gray level value should be related and are
likely to belong to the same object. Thus, pixels of different sites are related as in a graph
with a weight depending on the gray level distance. The iteration of the neighborhood filter
on the graph is equivalent to the solution of the heat equation on the graph by taking the
graph Laplacian. Eigenvalues and eigenvectors of such a graph Laplacian can be computed
allowing the design of Wiener and thresholding filters on the graph (see [] and [], []
for more details).
Both the neighborhood filter and the NL-means have been adapted and extended for
other types of data and other image processing tasks: for D data set points [], [],
[], [], [], and []; demosaicking, the operation which transforms the “R or G or B”
⊡Fig. -
Colorization experiment using the linear iteration of the neighborhood ﬁlter. Top left: input
image with original luminance and initial data on the chromatic components. Bottom right:
result image by applying the linear neighborhood scheme to the chromatic components
using the initial chromatic data as boundary conditions. Top middle and right: initial data on
the two chromatic components. Bottom middle and bottom right: ﬁnal interpolated
chromatic components


Local Smoothing Neighborhood Filters
raw image in each camera into an “R and G and B” image [], [], []; movie coloriza-
tion, [] and []; image inpainting by proposing a nonlocal image inpainting variational
framework with a unified treatment of geometry and texture [] (see also []) ; zoom-
ing by a fractal like technique where examples are taken from the image itself at different
scales []; movie flicker stabilization [], compensating spurious oscillations in the colors
of successive frames; super-resolution, an image zooming method by which several frames
from a video, or several low resolution photographs, can be fused into a larger image [].
The main point of this super-resolution technique is that it gives up an explicit estimate
of the motion, allowing actually for a multiple motion, since a block can look like several
other blocks in the same frame. The very same observation is made in [] for devising a
super-resolution algorithm, and in [], [].
.
Denoising
..
Analysis of Neighborhood Filter as a Denoising
Algorithm
In this section, we will further investigate the neighborhood filter behavior as a denois-
ing algorithm. We will consider the simplest neighborhood filter version which averages
spatially close pixels with an intensity difference lower than a certain threshold h. By clas-
sical probability theory, the average of N random and i.i.d values has a variance N times
smaller than the variance of the original values. However, this theoretical reduction is not
observed when applying neighborhood filters.
In order to evaluate the noise reduction capability of the neighborhood filter, we apply
it to a noise sample and evaluate the variance of the filtered sample. Let us suppose that we
observe the realization of a white noise at a pixel x, n(x) = a. The nearby pixels with an
intensity difference lower than h will be independent and identically distributed with the
probability distribution function the restriction of the Gaussian to the interval (a−h, a+h).
If the research zone is large enough, then the average value will tend to the expectation of
such a variable. Thus, the increase of the research zone and therefore of the number of
pixels being averaged does not increase the noise reduction capability of the filter. Such a
noise reduction factor is computed in the next result.
Theorem 
Assume that the n(i) are i.i.d. with zero mean and variance σ . Then, the
filtered noise by the neighborhood filter NFh satisfies the following:
(i)
The noise reduction depends only on the value of h,
Var NFh n(x) = f ( h
σ ) σ ,

Local Smoothing Neighborhood Filters 

where
f (x) =

(π)/∫R

β(a, x)(exa −)e(a+x)e
−a
da
is a decreasing function with f () = and limx−>∞f (x) = .
(ii)
The values NFhn(x) and NFhn(y) are uncorrelated for x ≠y.
The function f (x) is plotted in > Fig. -. The noise reduction increases as the ratio
h/σ also does. We see that f (x) is near zero for values of x over .or , that is, values
of h over .σ or σ, which justifies the values proposed in the original papers by Lee
and Yaroslavsky. However, for a Gaussian variable, the probability of observing values at
a distance of the average larger than .or times the standard deviation is very small.
Thus, by taking these values, we excessively increase the probability of mismatching pixels
of different objects. Thus, close objects with an intensity contrast lower than σ will not be
correctly denoised. This explains the decreasing performance of the neighborhood filter as
the noise standard deviation increases.
The previous theorem also tells us that the denoised noise values are still uncorrelated
once the filter has been applied. This is easily justified since we showed that as the size
ρ of the neighborhood increases, the filtered value tends to the expectation of the Gauss
distribution restricted to the interval (n(x) −h, n(x) + h). The filtered value is therefore
a deterministic function of n(x) and h. Independent random variables are mapped by a
deterministic function on independent variables.
This property may seem anecdotic since noise is what we wish to get rid of. Now, it is
impossible to totally remove noise. The question is how the remnants of noise look like. The
transformation of a white noise into any correlated signal creates structure and artifacts.
Only white noise is perceptually devoid of structure, as was pointed out by Attneave [].
1
2
3
4
5
0.2
0.4
0.6
0.8
1
⊡Fig. -
Noise reduction function f(x) given by Theorem 


Local Smoothing Neighborhood Filters
The only difference between the neighborhood filter and the Bilateral or SUSAN filter
is the way the spatial component is treated. While for the classical neighborhood all pix-
els within a certain distance are treated equally, for the Bilateral filter, pixels closer to the
reference pixel are more important. Even if this can seem a slight difference, this is crucial
from a qualitative point of view, that is, the creation of artifacts.
It is easily shown that introducing the weighting function on the intensity difference
instead of a non-weighted average does not modify the second property of Theorem , and
the denoised noise values are still uncorrelated if ρ is large enough. However, the introduc-
tion of the spatial kernel by the Bilateral or SUSAN filter affects this property. Indeed, the
introduction of a spatial decay of the weights makes denoised values at close positions to
be correlated.
There are two ways to show how denoising algorithms behave when they are
applied to a noise sample. One of them is to find a mathematical proof that the pix-
els remain independent (or at least uncorrelated) and identically distributed random
variables. The experimental device simply is to observe the effect of denoising on
the simulated realization of a white noise.
> Figure -displays the filtered noises
for the neighborhood filter, the Bilateral filter, and other state-of-the-art denoising
algorithms.
..
Neighborhood Filter Extension: The NL-Means
Algorithm
Now in a very general sense inspired by the neighborhood filter, one can define as “neigh-
borhood of a pixel x” any set of pixels y in the image such that a window around y looks
like a window around x. All pixels in that neighborhood can be used for predicting the
value at x, as was shown in [, ] for texture synthesis and in [, ] for inpainting
purposes. The fact that such a self-similarity exists is a regularity assumption, actually
more general and more accurate than all regularity assumptions we consider when deal-
ing with local smoothing filters, and it also generalizes a periodicity assumption of the
image.
Let v be the noisy image observation defined on a bounded domain Ω ⊂R, and let
x ∈Ω. The NL-means algorithm estimates the value of x as an average of the values of all
the pixels whose Gaussian neighborhood looks like the neighborhood of x:
NL(v)(x) =

C(x) ∫Ω e−(Ga ∗∣v(x+.)−v(y+.)∣)()
h
v(y) dy,
where Ga is a Gaussian kernel with standard deviation a, h acts as a filtering parameter,
and C(x) = ∫Ω e−(Ga∗∣v(x+.)−v(z+.)∣)()
h
dz is the normalizing factor. We recall that
(Ga ∗∣v(x + .) −v(y + .)∣)() = ∫RGa(t)∣v(x + t) −v(y + t)∣dt.

Local Smoothing Neighborhood Filters 

a
b
c
d
⊡Fig. -
Weight distribution of NL-means, the bilateral ﬁlter, and the anisotropic ﬁlter used to
estimate the central pixel in four detail images. On the two right-hand-side images of each
triplet, we display the weight distribution used to estimate the central pixel of the left image
by the neighborhood and the NL-means algorithm. (a) In straight edges, the weights are
distributed in the direction of the level line (as the mean curvature motion). (b) On curved
edges, the weights favor pixels belonging to the same contour or level line, which is a strong
improvement with respect to the mean curvature motion. In the cases of (c) and (d), the
weights are distributed across the more similar conﬁgurations, even though they are far
away from the observed pixel. This shows a behavior similar to a nonlocal neighborhood
ﬁlter or to an ideal Wiener ﬁlter
We will see that the use of an entire window around the compared points makes this
comparison more robust to noise. For the moment, we will compare the weighting dis-
tributions of both filters.
> Fig. -illustrates how the NL-means algorithm chooses in
each case a weight configuration adapted to the local geometry of the image. Then, the
NL-means algorithm seems to provide a feasible and rational method to automatically take
the best of all classical denoising algorithms, reducing for every possible geometric config-
uration the mismatched averaged pixels. It preserves flat zones as the Gaussian convolution
and straight edges as the anisotropic filtering while still restoring corners or curved edges
and texture.
Due to the nature of the algorithm, one of the most favorable cases is the textural case.
Texture images have a large redundancy. For each pixel, many similar samples can be found
in the image with a very similar configuration, leading to a noise reduction and a preserva-
tion of the original image. In > Fig. -, one can see an example with a Brodatz texture.
The Fourier transform of the noisy and restored images shows the ability of the algorithm
to preserve the main features even in the case of high frequencies.
The NL-means seems to naturally extend the Gaussian, anisotropic, and neighborhood
filtering. But it is not easily related to other state-of-the-art denoising methods as the total
variation minimization [], the wavelet thresholding [, ], or the local DCT empir-
ical Wiener filters []. For this reason, we compare these methods visually in artificial
denoising experiences (see [] for a more comprehensive comparison).


Local Smoothing Neighborhood Filters
⊡Fig. -
NL-means denoising experiment with a Brodatz texture image. Left: noisy image with
standard deviation . Right: NL-means restored image. The Fourier transforms of the noisy
and restored images show how main features are preserved even at high frequencies
⊡Fig. -
Denoising experience on a periodic image. From left to right and from top to bottom: noisy
image (standard deviation ), Gauss ﬁltering, total variation, neighborhood ﬁlter, Wiener
ﬁlter (ideal ﬁlter),TIHWT(translation invariant hard thresholding), DCT empirical Wiener
ﬁltering, and NL-means
> Figure -illustrates the fact that a nonlocal algorithm is needed for the correct
reconstruction of periodic images. Local smoothing filters, and Wiener and threshold-
ing methods are not able to reconstruct the wall pattern. Only NL-means and the global

Local Smoothing Neighborhood Filters 

Fourier–Wiener filter reconstruct the original texture. The Fourier–Wiener filter is based
on a global Fourier transform, which is able to capture the periodic structure of the image
in a few coefficients. But this only is an ideal filter: the Fourier transform of the origi-
nal image is being used.
> Fig. -d shows how NL-means chooses the correct weight
configuration and explains the correct reconstruction of the wall pattern.
The NL-means algorithm is not only able to restore periodic or texture images, natural
images also have enough redundancy to be restored. For example, in a flat zone, one can
find many pixels lying in the same region and with similar configurations. In a straight or
curved edge, a complete line of pixels with a similar configuration is found. In addition,
the redundancy of natural images allows us to find many similar configurations in faraway
pixels.
> Figure -shows that wavelet and DCT thresholding are well adapted to the recov-
ery of oscillatory patterns. Although some artifacts are noticeable in both solutions, the
stripes are well reconstructed. The DCT transform seems to be more adapted to this type
of texture, and stripes are a little better reconstructed. For a much more detailed compar-
ison between sliding window transform domain filtering methods and wavelet threshold
methods, we refer the reader to []. NL-means also performs well on this type of texture,
due to its high degree of redundancy.
⊡Fig. -
Denoising experience on a natural image. From left to right and from top to bottom: noisy
image (standard deviation ), total variation, neighborhood ﬁlter, translation invariant
hard thresholding (TIHWT), empirical Wiener, and NL-means


Local Smoothing Neighborhood Filters
⊡Fig. -
The noise to noise criterion. From left to right and from top to bottom: original noise image
of standard deviation , Gaussian convolution, anisotropic ﬁltering, total variation, TIHWT,
DCT empirical Wiener ﬁlter, neighborhood ﬁlter, Bilateral ﬁlter, and the NL-means.
Parameters have been ﬁxed for each method so that the noise standard deviation is reduced
by a factor . The ﬁltered noise by the Gaussian ﬁlter and the total variation minimization
are quite similar, even if the ﬁrst one is totally blurred and the second one has created many
high frequency details. The ﬁltered noise by the hard wavelet thresholding looks like a
constant image with superposed wavelets. The ﬁltered noise by the neighborhood ﬁlter and
the NL-means algorithm looks like a white noise. This is not the case for the Bilateral ﬁlter,
where low frequencies of noise are enhanced because of the spatial decay
> Figure -displays the application of the denoising methods to a white noise. We
display the filtered noise.
..
Extension to Movies
Averaging filters are easily extended to the denoising of image sequences and video. The
denoising algorithms involve indiscriminately pixels not belonging only to the same frame
but also the previous and posterior ones.

Local Smoothing Neighborhood Filters 

In many cases, this straightforward extension cannot correctly deal with moving
objects. For that reason, state-of-the-art movie filters are motion compensated (see []
for a comprehensive review). The underlying idea is the existence of a “ground true” phys-
ical motion, which motion estimation algorithms should be able to estimate. Legitimate
information should exist only along these physical trajectories. The motion compensated
filters estimate explicitly the motion of the sequence by a motion estimation algorithm. The
motion compensated movie yields a new stationary data on which an averaging filter can be
applied. The motion compensation neighborhood filter was proposed by Ozkan et al. [].
We illustrate in > Fig. -the improvement obtained with the proposed compensation.
One of the major difficulties in motion estimation is the ambiguity of trajectories, the
so-called aperture problem. This problem is illustrated in > Fig. -. At most pixels, there
are several options for the displacement vector. All of these options have a similar gray level
value and a similar block around them. Now, motion estimators have to select one by some
additional criterion.
The above description of movie denoising algorithms and its juxtaposition to the
NL-means principle shows how the main problem, motion estimation, can be circum-
vented. In denoising, the more samples we have the happier we are. The aperture problem is
just a name for the fact that there are many blocks in the next frame similar to a given one in
the current frame. Thus, singling out one of them in the next frame to perform the motion
compensation is an unnecessary and probably harmful step. A much simpler strategy that
takes advantage of the aperture problem is to denoise a movie pixel by involving indis-
criminately spatial and temporal similarities (see [] for more details on this discussion).
The algorithm favors pixels with a similar local configuration, as the similar configurations
move, so do the weights. Thus, the algorithm is able to follow the similar configurations
when they move without any explicit motion computation (see > Fig. -).
⊡Fig. -
Aperture problem and the ambiguity of trajectories are the most diﬃcult problems in
motion estimation: There can be many good matches. The motion estimation algorithms
must pick one


Local Smoothing Neighborhood Filters
a
b
c
⊡Fig. -
Weight distribution of NL-means applied to a movie. In (a), (b), and (c), the ﬁrst row shows a
ﬁve frames image sequence. In the second row, the weight distribution used to estimate the
central pixel (in white) of the middle frame is shown. The weights are equally distributed
over the successive frames, including the current one. They actually involve all the
candidates for the motion estimation instead of picking just one per frame. The aperture
problem can be taken advantage of for a better denoising performance by involving more
pixels in the average

Local Smoothing Neighborhood Filters 

⊡Fig. -
Comparison of static ﬁlters, motion compensated ﬁlters, and NL-means applied to an image
sequence. Top: three frames of the sequence are displayed. Middle and left to right:
neighborhood ﬁlter, motion compensated neighborhood ﬁlter, and the NL-means. (AWA).
Bottom: the noise removed by each method (diﬀerence between the noisy and ﬁltered
frame). Motion compensation improves the static algorithms by better preserving the
details and creating less blur. We can read the titles of the books in the noise removed by
AWA. Therefore, that much information has been removed from the original. Finally, the
NL-means algorithm (bottom row) has almost no noticeable structure in its removed noise.
As a consequence, the ﬁltered sequence has kept more details and is less blurred
.
Asymptotic
..
PDE Models and Local Smoothing Filters
According to Shannon’s theory, a signal can be correctly represented by a discrete set of val-
ues, the “samples,” only if it has been previously smoothed. Let us start with uthe physical
image, a real function defined on a bounded domain Ω ⊂R. Then a blur optical kernel k
is applied, i.e., uis convolved with k to obtain an observable signal k∗u. Gabor remarked
in that the difference between the original and the blurred images is roughly propor-
tional to its Laplacian, Δu = uxx +uyy. In order to formalize this remark, we have to notice


Local Smoothing Neighborhood Filters
that k is spatially concentrated, and that we may introduce a scale parameter for k, namely,
kh(x) = h−k (h−
x). If, for instance, u is Cand bounded and if k is a radial function in
the Schwartz class, then
u∗kh(x) −u(x)
h
→cΔu(x).
Hence, when h gets smaller, the blur process looks more and more like the heat equation
ut = cΔu,
u() = u.
Thus, Gabor established a first relationship between local smoothing operators and PDEs.
The classical choice for k is the Gaussian kernel.
Remarking that the optical blur is equivalent to one step of the heat equation, Gabor
deduced that we can, to some extent, deblur an image by reversing the time in the heat
equation, ut = −Δu. Numerically, this amounts to subtracting the filtered version from the
original image:
u −Gh ∗u = −hΔu + o(h).
This leads to considering the reverse heat equation as an image restoration, ill-posed
though it is. The time-reversed heat equation was stabilized in the Osher–Rudin shock
filter [] who proposed
ut = −sign(L(u))∣Du∣,
(.)
where the propagation term ∣Du∣is tuned by the sign of an edge detector L(u). The func-
tion L(u) changes sign across the edges where the sharpening effect therefore occurs. In
practice, L(u) = Δu and the equation is related to a reverse heat equation.
The early Perona–Malik “anisotropic diffusion” [] is directly inspired from the Gabor
remark. It reads
ut = div(g(∣Du∣)Du),
(.)
where g : [,+∞) →[,+∞) is a smooth decreasing function satisfying g() = ,
lims→+∞g(s) = . This model is actually related to the preceding ones. Let us consider
the second derivatives of u in the directions of Du and Du⊥:
uηη = Du ( Du
∣Du∣, Du
∣Du∣),
uξξ = Du (Du⊥
∣Du∣, Du⊥
∣Du∣).
Then, > Eq. (.) can be rewritten as
ut = g(∣Du∣)uξξ + h(∣Du∣)uηη,
(.)
where h(s) = g(s) + sg′(s). Perona and Malik proposed the function g(s) =

+s/k . In
this case, the coefficient of the first term is always positive and this term therefore appears
as a one-dimensional diffusion term in the orthogonal direction to the gradient. The sign
of the second coefficient, however, depends on the value of the gradient. When ∣Du∣< k,
this second term appears as a one-dimensional diffusion in the gradient direction. It leads
to a reverse heat equation term when ∣Du∣> k.

Local Smoothing Neighborhood Filters 

The Perona–Malik model has got many variants and extensions. Tannenbaum and
Zucker [] proposed, endowed in a more general shape analysis framework, the simplest
equation of the list:
ut = ∣Du∣div ( Du
∣Du∣) = uξξ.
This equation had been proposed some time before in another context by Sethian [] as
a tool for front propagation algorithms. This equation is a “pure” diffusion in the direction
orthogonal to the gradient and is equivalent to the anisotropic filter AF []:
AFhu(x) = ∫Gh(t)u(x + tξ)dt,
where ξ = Du(x)⊥/∣Du(x)∣and Gh(t) denotes the one-dimensional Gauss function with
variance h.
This diffusion is also related to two models proposed in image restoration. The Rudin–
Osher–Fatemi [] total variation model leads to the minimization of the total variation
of the image TV(u) = ∫∣Du∣, subject to some constraints. The steepest descent of this
energy reads, at least formally,
∂u
∂t = div ( Du
∣Du∣)
(.)
which is related to the mean curvature motion and to the Perona–Malik equation when
g(∣Du∣) =

∣Du∣. This particular case, which is not considered in [], yields again (> .).
An existence and uniqueness theory is available for this equation [].
..
Asymptotic Behavior of Neighborhood Filters
(Dimension )
Let u denote a one-dimensional signal defined on an interval I ⊂R and consider the
neighborhood filter
NFh,ρu(x) =

C(x) ∫
x+ρ
x−ρ
u(y)e−∣u(y)−u(x)∣
h
dy,
(.)
where C(x) = ∫
x+ρ
x−ρ e−∣u(y)−u(x)∣
h
dy.
The following theorem describes the asymptotical behavior of the neighborhood filter
in D. The proof of this theorem and next ones in this section can be found in []. 
Theorem 
Suppose u ∈C(I), and let ρ, h, α > such that ρ, h →and h = O(ρα).
Consider the continuous function g(t) = te−t
E(t) , for t ≠, g() = 
, where E(t) = ∫
t
e−sds.
Let f be the continuous function
f (t) = g(t)
t
+ g(t) −
t,
f () = 
.
(.)


Local Smoothing Neighborhood Filters
Then, for x ∈R,
. If α < ,
NFh,ρu(x) −u(x) ≃u′′(x)

ρ.
. If α = ,
NFh,ρu(x) −u(x) ≃f ( ρ
h ∣u′(x)∣) u′′(x) ρ.
. If < α < 
,
NFh,ρu(x) −u(x) ≃g(ρ−α∣u′(x)∣)u′′(x) ρ.
According to Theorem , the neighborhood filter makes the signal evolve proportion-
ally to its second derivative. The equation ut = cu′′ acts as a smoothing or enhancing model
depending on the sign of c. Following the previous theorem, we can distinguish three cases
depending on the values of h and ρ. First, if h is much larger than ρ, the second derivative
is weighted by a positive constant and the signal is therefore filtered by a heat equation.
Second, if h and ρ have the same order, the sign and magnitude of the weight is given by
f ( ρ
h ∣u′(x)∣). As the function f takes positive and negative values (see
> Fig. -), the
filter behaves as a filtering/enhancing algorithm depending on the magnitude of ∣u′(x)∣.
If B denotes the zero of f , then a filtering model is applied wherever ∣u′∣< B h
ρ and an
enhancing model wherever ∣u′∣> B h
ρ . The intensity of the enhancement tends to zero
when the derivative tends to infinity. Thus, points x where ∣u′(x)∣is large are not altered.
The transition of the filtering to the enhancement model creates a singularity in the filtered
signal. In the last case, ρ is much larger than h and the sign and magnitude of the weight
is given by g ( ρ
h ∣u′(x)∣). Function g is positive and decreases to zero. If the derivative of u
is bounded, then ρ
h ∣u′(x)∣tends to infinity and the intensity of the filtering to zero. In this
case, the signal is hardly modified.
In summary, a neighborhood filter in dimension shows interesting behavior only if ρ
and h have the same order of magnitude, in which case the neighborhood filter behaves like
a Perona–Malik equation. It enhances edges with a gradient above a certain threshold and
smoothes the rest.
> Figure -illustrates the behavior of the one-dimensional neighborhood filter. The
algorithm is iterated until the steady state is attained on a sine signal for different values
of the ratio ρ/h. The results of the experiment corroborate the asymptotical expansion
of Theorem . In the first experiment, ρ/h = −and the neighborhood filter is equiva-
lent to a heat equation. The filtered signal tends to a constant. In the second experiment,
ρ/h = and the value g ( ρ
h∣u′∣) is nearly zero. As predicted by the theorem, the filtered
signal is nearly identical to the original one. The last two experiments illustrate the filter-
ing/enhancing behavior of the algorithm when h and ρ have similar values. As predicted,
an enhancing model is applied where the derivative is large. Many singularities are being
created because of the transition of the filtering to the enhancing model. Unfortunately, the
number of singularities and their position depend upon the value of ρ/h. This behavior is
explained by Theorem (). > Figure -illustrates the same effect in the D case.
The filtering/enhancing character of the neighborhood filter is very different from a
pure enhancing algorithm like the Osher–Rudin shock filter. > Figures -and > -
illustrate these differences. In
> Fig. -, the minimum and the maximum of the sig-
nal have been preserved by the shock filter, while these two values have been significantly
reduced by the neighborhood filter. This filtering/enhancing effect is optimal when the

Local Smoothing Neighborhood Filters 

1
0
–0.5
0.5
–1
0
200
400
1
0
–0.5
0.5
–1
0
200
400
1
0
–0.5
0.5
–1
0
200
400
1
0
–0.5
0.5
–1
0
200
400
1
0
–0.5
0.5
–1
0
200
400
⊡Fig. -
One-dimensional neighborhood ﬁlter experiment. The neighborhood ﬁlter is iterated until
the steady state is attained for diﬀerent values of the ratio ρ/h. Top: original sine signal.
Middle left: ﬁltered signal with ρ/h = −. Middle right: ﬁltered signal with ρ/h = . Bottom
left: ﬁltered signal with ρ/h = . Bottom right: ﬁltered signal with ρ/h = . The examples
corroborate the results of Theorem . If ρ/h tends to zero, the algorithm behaves like a heat
equation and the ﬁltered signal tends to a constant. If, instead, ρ/h tends to inﬁnity, the
signal is hardly modiﬁed. If ρ and h have the same order, the algorithm presents a
ﬁltering/enhancing dynamic. Singularities are created due to the transition of smoothing to
enhancement. The number of enhanced regions strongly depends upon the ratio ρ
h , as
illustrated in the bottom ﬁgures
signal is noisy. > Figure -shows how the shock filter creates artificial steps due to the
fluctuations of noise, while the neighborhood filter reduces the noise avoiding any spurious
shock. Parameter h has been chosen larger than the amplitude of noise in order to remove
it. Choosing an intermediate value of h, artificial steps could also be generated on points
where the noise amplitude is above this parameter value.


Local Smoothing Neighborhood Filters
1
0.5
0
–0.5
–1
1
0.5
0
–0.5
–1
1
0.5
0
–0.5
–1
0
200
400
0
200
400
0
200
400
⊡Fig. -
Comparison between the neighborhood ﬁlter and the shock ﬁlter. Top: original signal.
Bottom left: application of the neighborhood ﬁlter. Bottom right: application of the shock
ﬁlter. The minimum and the maximum of the signal have been preserved by the shock ﬁlter
and reduced by the neighborhood ﬁlter. This fact illustrates the ﬁltering/enhancing
character of the neighborhood ﬁlter compared with a pure enhancing ﬁlter
..
The Two-Dimensional Case
The following theorem extends the previous result to the two-dimensional case.
Theorem 
Suppose u ∈C(Ω), and let ρ, h, α > such that ρ, h →and h = O(ρα). Let
us consider the continuous function ˜g defined by ˜g(t) = 

te−t
E(t) , for t ≠, ˜g() = 
, where
E(t) = ∫
t
e−sds. Let ˜f be the continuous function defined by
˜f (t) = ˜g(t) + ˜g(t)
t
−
t,
˜f () = 
.
Then, for x ∈Ω,
. If α < ,
NFh,ρu(x) −u(x) ≃△u(x)

ρ.
. If α = ,
NFh,ρu(x) −u(x) ≃[˜g ( ρ
h ∣Du(x)∣) uξξ(x) + ˜f ( ρ
h ∣Du(x)∣) uηη(x)] ρ.

Local Smoothing Neighborhood Filters 

200
200
150
100
50
0
50
100
150
200
250
200
150
100
50
0
50
100
150
200
250
150
100
50
0
50
100
150
200
250
⊡Fig. -
Comparison between the neighborhood ﬁlter and the shock ﬁlter. Top: original signal.
Bottom left: application of the neighborhood ﬁlter. Bottom right: application of the shock
ﬁlter. The shock ﬁlter is sensitive to noise and creates spurious steps. The ﬁltering/
enhancing character of the neighborhood ﬁlter avoids this eﬀect
. If < α < 
,
NFh,ρu(x) −u(x) ≃˜g (ρ−α∣Du(x)∣)[ uξξ(x) + uηη(x)] ρ.
where ξ = Du(x)⊥/∣Du(x)∣and η = Du(x)/∣Du(x)∣.
According to Theorem , the two-dimensional neighborhood filter acts as an evolution
PDE with two terms. The first term is proportional to the second derivative of u in the direc-
tion ξ = Du(x)⊥/∣Du(x)∣, which is tangent to the level line passing through x. The second
term is proportional to the second derivative of u in the direction η = Du(x)/∣Du(x)∣,
which is orthogonal to the level line passing through x. Like in the one-dimensional case,
the evolution equations ut = cuξξ and ut = cuηη act as filtering or enhancing models
depending on the signs of cand c. Following the previous theorem, we can distinguish
three cases, depending on the values of h and ρ.
First, if h is much larger than ρ, both second derivatives are weighted by the same
positive constant. Thus, the sum of both terms is equivalent to the Laplacian of u, Δu,
and we get back to Gaussian filtering.
Second, if h and ρ have the same order of magnitude, the neighborhood filter behaves as
a filtering/enhancing algorithm. The coefficient of the diffusion in the tangential direction,


Local Smoothing Neighborhood Filters
2
4
6
8
–0.1
–0.05
0.05
0.1
0.15
0.2
2
4
6
8
–0.1
–0.05
0.05
0.1
0.15
0.2
⊡Fig. -
Weight functions of Theorems and when h and ρ have the same order. Left: function f of
Theorem . Right: functions
∼g (continuous line) and
∼
f (dashed line) of Theorem 
uξξ, is given by ˜g ( ρ
h ∣Du∣). The function ˜g is positive and decreasing. Thus, there is always
diffusion in that direction. The weight of the normal diffusion, uηη, is given by ˜f ( ρ
h∣Du∣).
As the function ˜f takes positive and negative values (see > Fig. -), the filter behaves as a
filtering/enhancing algorithm in the normal direction and depending on ∣Du∣. If ˜B denotes
the zero of ˜f , then a filtering model is applied wherever ∣Du∣< ˜B h
ρ and an enhancing
strategy wherever ∣Du∣> ˜B h
ρ . The intensity of the filtering in the tangent diffusion and the
enhancing in the normal diffusion tend to zero when the gradient tends to infinity. Thus,
points with a very large gradient are not altered.
Finally, if ρ is much larger than h, the value ρ
h tends to infinity and then the filtering
magnitude ˜g ( ρ
h ∣Du∣) tends to zero. Thus, the original image is hardly altered. Let us men-
tion that similar calculations were performed in a particular case for the neighborhood
median filter by Masnou [].
We observe that when ρ and h have the same order, the neighborhood filter asymptot-
ically behaves like a Perona–Malik model. Let us be more specific about this comparison.
Taking g(s) = ˜g (s

) in the Perona–Malik > Eq. (.), we obtain
ut = ˜g(∣Du∣)uξξ + ˜h(∣Du∣)uηη,
(.)
where ˜h(s) = ˜g(s) + s ˜g′(s). Thus, the Perona–Malik model and the neighborhood filter
can be decomposed in the same way and with exactly the same weight in the tangent direc-
tion. Then the function ˜h has the same behavior as ˜f (Theorem ), as can be observed in
> Fig. -. Thus, in this case, a neighborhood filter has the same qualitative behavior as
a Perona–Malik model, even if we cannot rewrite it exactly as such.
> Figure -displays a comparison of the neighborhood filter and the Perona–Malik
model. We display a natural image and the filtered images by both models. These solutions
have a similar visual quality and tend to display flat zones and artificial contours inside
the smooth regions.
> Figure -corroborates this visual impression. We display the
level lines of both filtered solutions. As expected from the above consistency theorems, for
both models the level lines of the original image tend to concentrate, thus creating large
flat zones separated by edges. The solutions are very close, up to the obvious very different

Local Smoothing Neighborhood Filters 

1
2
3
4
5
–0.2
–0.1
0.1
0.2
6
⊡Fig. -
Weight comparison of the neighborhood ﬁlter and the Perona–Malik equation. Magnitude
of the tangent diﬀusion (continuous line, identical for both models) and normal diﬀusion
(dashed line – –) of Theorem . Magnitude of the tangent diﬀusion (continuous line) and
normal diﬀusion (dashed line - - -) of the Perona–Malik model (> .). Both models show
nearly the same behavior
implementations. The neighborhood filter is implemented exactly as in its definition and
the Perona–Malik model by the explicit difference scheme proposed in the original paper.
..
A Regression Correction of the Neighborhood Filter
In the previous sections, we have shown the enhancing character of the neighborhood
filter. We have seen that the neighborhood filter, like the Perona–Malik model, can create
large flat zones and spurious contours inside smooth regions. This effect depends upon
a gradient threshold which is hard to fix in such a way as to always separate the visually
smooth regions from edge regions. In order to avoid this undesirable effect, let us analyze
in more detail what happens with the neighborhood filter in the one-dimensional case.
> Figure -shows a simple illustration of this effect. For each x in the convex part
of the signal, the filtered value is the average of the points y such that u(x) −h < u(y) <
u(x) + h for a certain threshold h. As it is illustrated in the figure, the number of points
satisfying u(x) −h < u(y) ≤u(x) is larger than the number of points satisfying u(x) ≤
u(y) < u(x) + h. Thus, the average value YNF(x) is smaller than u(x), enhancing this
part of the signal. A similar argument can be applied in the concave parts of the signal,
dealing with the same enhancing effect. Therefore, shocks will be created inside smooth
zones where concave and convex parts meet.
> Figure -also shows how the mean


Local Smoothing Neighborhood Filters
u (x) + h
x–
x
x+
YNF (x)
u (x)
u (x) – h
⊡Fig. -
Illustration of the shock eﬀect of the YNF on the convex of a signal. The number of points y
satisfying u(x) −h < u(y) ≤u(x) is larger than the number satisfying u(x) ≤u(y) < u(x) + h.
Thus, the average value YNF(x) is smaller than u(x), enhancing that part of the signal. The
regression line of u inside (x−, x+) better approximates the signal at x
is not a good estimate of u(x) in this case. In the same figure, we display the regression
line approximating u inside (u−(u(x) −h),u−(u(x) + h)). We see how the value of the
regression line at x better approximates the signal. In the sequel, we propose to correct the
neighborhood filter with this better estimate.
In the general case, this linear regression strategy amounts to finding for every point x
the plane locally approximating u in the following sense:
min
a,a∫Bρ(x) w(x,y)(u(y) −ay−a)dy,
w(x,y) = e−∣u(y)−u(x)∣
h
(.)
and then replacing u(x) by the filtered value ax+ a. The weights used to define the
minimization problem are the same as the ones used by the neighborhood filter. Thus, the
points with a gray level value close to u(x) will have a larger influence in the minimization
process than those with a further gray level value. We denote the above linear regression
correction by LNFh,ρ. Taking a= and then approximating u by a constant function, the
minimization (> .) goes back to the neighborhood filter.
This minimization was originally proposed by Cleveland [] with a weight family
not depending on the function u but only on the spatial distance of x and y. A similar
scheme incorporating u in the weight computation has been statistically studied in [].
The authors propose an iterative procedure that describes for every point the largest pos-
sible neighborhood in which the initial data can be well approximated by a parametric
function.

Local Smoothing Neighborhood Filters 

Another similar strategy is the interpolation by ENO schemes []. The goal of ENO
interpolation is to obtain a better adapted prediction near the singularities of the data. For
each point it selects different stencils of fixed size M, and for each stencil reconstructs the
associated interpolation polynomial of degree M. Then the least oscillatory polynomial is
selected by some prescribed numerical criterion. The selected stencils tend to escape from
large gradients and discontinuities.
The regression strategy also tends to select the right points in order to approximate the
function. Instead of choosing a certain interval, all the points are used in the polynomial
reconstruction, but weighted by the gray level differences.
As in the previous sections, let us analyze the asymptotic behavior of the linear regres-
sion correction. We compute the asymptotic expansion of the filter when < α ≤. We
showed that when α > , the signal is hardly modified.
For the sake of completeness, we first compute the asymptotic expansion in the one-
dimensional case.
Theorem 
Suppose u ∈C(I), and let ρ, h, α > such that ρ, h →and h = O(ρα). Let
˜f be the continuous function defined as ˜f () = 
,
˜f (t) =

t
⎛
⎝−t e−t
E(t)
⎞
⎠,
for t ≠, where E(t) = ∫
t
e−sds. Then, for x ∈R,
. If α < ,
LNFh,ρu(x) −u(x) ≃u′′(x)

ρ.
. If α = ,
NFh,ρu(x) −u(x) ≃˜f ( ρ
h ∣u′(x)∣) u′′(x) ρ.
Theorem shows that the LNFh,ρ filter lets the signal evolve proportionally to its second
derivative, as the neighborhood filter does. When h is larger than ρ, the filter is equivalent
to the original neighborhood filter and the signal is filtered by a heat equation. When ρ
and h have the same order, the sign and magnitude of the filtering process is given by
˜f ( ρ
h ∣u′(x)∣) (see
> Fig. -). This function is positive and quickly decreases to zero.
Thus, the signal is filtered by a heat equation of decreasing magnitude and is not altered
wherever the derivative is very large.
The same asymptotic expansion can be computed in the two-dimensional case.
Theorem 
Suppose u ∈C(Ω), and let ρ, h, α > such that ρ, h →and h = O(ρα).
Let ˜f be the continuous function defined as ˜f () = 
,
˜f (t) =

t
⎛
⎝−t e−t
E(t)
⎞
⎠,
for t ≠, where E(t) = ∫
t
e−sds. Then, for x ∈Ω,


Local Smoothing Neighborhood Filters
. If α < ,
LNFh,ρu(x) −u(x) ≃△u(x)

ρ.
. If α = ,
LNFh,ρu(x) −u(x) ≃[ ˜f ( ρ
h ∣Du(x)∣) uηη(x)(x) + 
uξξ(x)] ρ.
According to the previous theorem, the filter can be written as the sum of two diffusion
terms in the direction of ξ and η. When h is much larger than ρ, the linear regression
correction is equivalent to the heat equation like the original neighborhood filter. When ρ
and h have the same order, the behavior of the linear regression algorithm is very differ-
ent from the original neighborhood filter. The function weighting the tangent diffusion is
a positive constant. The function weighting the normal diffusion is positive and decreas-
ing (see > Fig. -), and therefore there is no enhancing effect. The algorithm combines
the tangent and normal diffusion wherever the gradient is small. Wherever the gradient
is larger, the normal diffusion is canceled and the image is filtered only in its tangent
direction. This subjacent PDE was already proposed as a diffusion equation in []. This
diffusion makes the level lines evolve proportionally to their curvature. In the Perona–
Malik model, the diffusion is stopped near the edges. In this case, the edges are filtered by
a mean curvature motion.
It may be asked whether the modified neighborhood filter still preserves signal dis-
continuities. The answer is yes. It is easily checked that for small enough h, all piecewise
affine functions with smooth jump curves are steady. Thus, the behavior is the same as for
the classical neighborhood filter. Our asymptotic analysis is of course not valid for such
functions, but only for smooth functions.
As a numerical scheme, the linear regression neighborhood filter allows the implemen-
tation of a mean curvature motion without the computation of gradients and orientations.
When the gradient is small, the linear regression filter naturally behaves like the heat
equation. This effect is introduced on typical schemes implementing the mean curvature
2
4
6
8
10
0.05
0.1
0.15
0.2
0.25
2
4
6
8
10
0.05
0.1
0.15
0.2
0.25
⊡Fig. -
Weighting functions of Theorems and . Left: function
∼
f of Theorem . Right: constant
function /(continuous line) and function
∼
f (dashed line) of Theorem 

Local Smoothing Neighborhood Filters 

⊡Fig. -
Comparison experiment. Top left: original image. Top right: Perona–Malik ﬁltered image.
Bottom left: ﬁltered image by the neighborhood ﬁlter. Bottom right: ﬁltered image by
the linear regression neighborhood ﬁlter. The neighborhood ﬁlter experiments are
performed by iterating the discrete version of deﬁnitions (> .) and (> .). Both the
neighborhood ﬁlter and its linear regression correction have been applied with the same
value of h and ρ. The displayed images have been attained within the same number of
iterations. The Perona–Malik equation is implemented by the explicit diﬀerence scheme
proposed in the original paper. The Perona–Malik model and the neighborhood ﬁlter create
artiﬁcial contours and ﬂat zones. This eﬀect is almost completely avoided by the linear
regression neighborhood ﬁlter
motion. In flat zones, the gradient is not well defined and some kind of isotropic diffu-
sion must be applied. Therefore, the linear regression neighborhood filter naturally extends
the mean curvature motion and yields a stable numerical scheme for its computation,
independent of gradient orientations.
> Figure -displays an experiment comparing the LNFh,ρ with the neighborhood
filter and the Perona–Malik equation. The linear correction does not create any contour or
flat zone inside the smooth regions. > Figure -displays the level lines of the previous
experiment. The level lines of the LNFh,ρ are filtered by a mean curvature motion, and they
do not get grouped creating flat zones.


Local Smoothing Neighborhood Filters
⊡Fig. -
Level lines of the images in > Fig. -. By the Perona–Malik ﬁlter and the neighborhood
ﬁlter, the level lines tend to group, creating ﬂat zones. The regression correction ﬁlters the
level lines by a curvature motion without creating any ﬂat zone
..
The Vector-Valued Case
Let u be a vector-valued function defined on a bounded domain Ω ⊂R, u : Ω →Rn.
The vector neighborhood filter can be written as
NFh,ρu(x) =

C(x) ∫Bρ(x) u(y)e−∣∣u(y)−u(x)∣∣
h
dy,
(.)
where ∣∣u(y) −u(x)∣∣is now the Euclidean vector norm and each component function ui
is filtered with the same weight distribution. The linear regression correction is defined as
in the scalar case, and each component is locally approximated by a plane with the same
weight distribution.
In order to compute the asymptotic expansion of the linear regression filter, we must
fix a coordinate system for R. In the scalar case, we used the reference system given by the
gradient of the image at x and its orthogonal direction. In addition, this reference allows
us to relate the obtained diffusion to the evolution of the level lines of the image and the
mean curvature motion. Now, we cannot use the same reference and we need to define
a new one. By analogy with the scalar case, we choose the directions of minimum and
maximum variation of the vector function.

Local Smoothing Neighborhood Filters 

⊡Fig. -
Comparison of the neighborhood ﬁlter and the linear regression correction. Top left: original
image. Top middle: ﬁltered image by the neighborhood ﬁlter. Top right: ﬁltered image by the
regression neighborhood ﬁlter. Bottom: level lines of a part of the images on the above line.
Both neighborhood ﬁlters have been performed with the same ﬁltering parameters and the
same number of iterations. The linear regression neighborhood algorithm has ﬁltered the
image while preserving the main boundaries as the original neighborhood ﬁlter. No
enhancing has been applied by the linear correction avoiding the shock eﬀect. The level
lines of the neighborhood ﬁlter tend to group and create large ﬂat zones. In addition, these
level lines oscillate, while those of the linear regression algorithm have been correctly
ﬁltered
Deﬁnition 
We define the normal direction η and the tangent direction ξ as the vectors
that respectively maximize and minimize the following variation:
n
∑
i=
∥∂ui
∂v (x)∥

under the constraint ∥v∥= .
It is easily seen that this constrained optimization leads to the computation of the
eigenvectors of the matrix
A =
⎛
⎜
⎝
∥∂u
∂x ∥

⟨∂u
∂x , ∂u
∂y ⟩
⟨∂u
∂x , ∂u
∂y ⟩
∥∂u
∂y ∥

⎞
⎟
⎠
,


Local Smoothing Neighborhood Filters
where ∂u
∂x = ( ∂u
∂x , . . . , ∂un
∂x ) and ∂u
∂y = ( ∂u
∂y , . . ., ∂un
∂y ). The two positive eigenvalues of A,
λ+ and λ−, are the maximum and the minimum of the vector norm associated to A and
the maximum and the minimum variations, as defined in Definition . The corresponding
eigenvectors are orthogonal leading to the above-defined normal and tangent directions.
This orthonormal system was first proposed for vector-valued image analysis in []. Many
PDE equations have been proposed for color image filtering using this system. We note the
Coherence Enhancing Diffusion [], the Beltrami Flow [], and an extension of the mean
curvature motion [].
Theorem 
Suppose u ∈C(Ω,Rn), and let ρ, h, α > such that ρ, h →and h = O(ρα).
Let ˜f be the continuous function defined as ˜f () = 
,
˜f (t) =

t
⎛
⎝−t e−t
E(t)
⎞
⎠,
for t ≠, where E(t) = ∫
t
e−sds. Then, for x ∈Ω,
. If α < ,
LNFh,ρu(x) −u(x) ≃△u(x)

ρ.
. If α = ,
LNFh,ρu(x)−u(x) ≃[ ˜f ( ρ
h ∥∂u
∂ξ (x)∥) Du(ξ, ξ)(x) + ˜f ( ρ
h ∥∂u
∂η(x)∥) Du(η, η)(x)] ρ
where Δu(x) = (Δui(x))≤i≤n and Du(v,v)(x) = (Dui(v,v)(x))≤i≤n for v ∈{η, ξ}.
...
Interpretation
When h is much larger than ρ, the linear regression neighborhood filter is equivalent to
the heat equation applied independently to each component. When h and ρ have the same
order, the subjacent PDE acts as an evolution equation with two terms. The first term is
proportional to the second derivative of u in the tangent direction ξ. The second term
is proportional to the second derivative of u in the normal direction η. The magnitude of
each diffusion term depends on the variation in the respective direction, λ−= ∥∂u
∂ξ (x)∥and
λ+ = ∥∂u
∂η (x)∥. The weighting function ˜f is positive and decreases to zero (see > Fig. -).
We can distinguish the following cases depending on the values of λ+ and λ−.
•
If λ+ ≃λ−≃, then there are very few variations of the vector image u around x. In
this case, the linear regression neighborhood filter behaves like a heat equation with
maximum diffusion coefficient ˜f ().
•
If λ+ >> λ−, then there are strong variations of u around x and the point may be located
on an edge. In this case, the magnitude ˜f ( ρ
h λ+) tends to zero and there is no diffusion

Local Smoothing Neighborhood Filters 

in the direction of maximal variation. If λ−>> , then x may be placed on an edge
with different orientations depending on each component and the magnitude of the
filtering in both directions tends to zero, so that the image is hardly altered. If λ−≃,
then the edges have similar orientations in all the components and the image is filtered
by a directional Laplacian in the direction of minimal variation.
•
If λ+ ≃λ−>> , then we may be located on a saddle point, and in this case the
image is hardly modified. When dealing with multivalued images, one can think of
the complementarity of the different channels leading to the perception of a corner.
In the scalar case, the theorem gives back the result studied in the previous sections.
The normal and tangent directions are, respectively, the gradient direction and the level
line direction. In this case, ∂u
∂ξ (x) = and ∂u
∂η (x) = ∣Du(x)∣, and we get back to
LNFh,ρu(x) −u(x) ≃[ 
Du(ξ, ξ)(x) + ˜f ( ρ
h∣Du(x)∣) Du(η, η)(x)] ρ.
.
Variational and Linear Diﬀusion
The relationship of neighborhood filters with classic local PDEs has been discussed in the
previous section. Yet, the main interest has shifted to defining nonlocal PDEs. The extension
of the neighborhood filter and the NL-means method to define nonlocal image-adapted
differential operators and nonlocal variational methods starts with [], which proposes
to perform denoising and deblurring by nonlocal functionals.
The general goal of this development is actually to give a variational to all neighbor-
hood filters, and to give a nonlocal form to the total variation as well. More precisely, the
neighborhood filters derive from the functional
J(u) = ∫Ω×Ω g (∣u(x) −u(y)∣
h
)w(∣x −y∣)dxdy,
where g and w have a Gaussian decay. In the same line, a functional yields a (variational)
interpretation to NL-means:
JNL(u) = ∫Ω×Ω (−e−Gσ ∗∣u(x−.)−u(y−.)∣()
h
)w(∣x −y∣)dxdy.
In a similar variational framework, Gilboa et al. [] consider the general kind of
quadratic nonlocal functional
J(u) := ∫Ω×Ω(u(x) −u(y))w(x,y)dxdy,
(.)
where w(x,y) is any fixed weight distribution, which in most applications writes as the
neighborhood or NL-means weight distribution. The resolution of the graph heat equation


Local Smoothing Neighborhood Filters
or the variational minimization (> .) is given by
un+(x) =

C(x) ∫Ω un(y)w(x,y)dy,
(.)
where C(x) = ∫Ω w(x,y)dy is a normalizing factor. The freedom of having a totally decou-
pled weight distribution makes this formulation a linear and powerful tool for image
processing. In fact, this formulation rewrites as the Dirichlet integral of the following non-
local gradient: ∇wu(x, y) = (u(x) −u(y))w(x, y). The whole process relates to a graph
Laplacian where each pixel is considered as the node of a weighted graph, and the weights
of the edge between two pixels x and y, respectively, are decreasing functions of the dis-
tances of patches around x and y, w(x, y). Then a graph Laplacian can be calculated on this
graph, seen as the sampling of a manifold, and the linear diffusion can be interpreted as the
heat equation on the set of blocks endowed with these weights. The eigenvalues and eigen-
vectors of such a Laplacian can be computed and used for designing spectral algorithms as
Wiener and thresholding methods (see [] and []).
The nonlocal term (> .) has shown to be very useful as a regularization term for
many image processing tasks. The nonlocal differential operators permit to define a total
variation or a Dirichlet integral. Several articles on deblurring have followed this vari-
ational line [], [], [] (for image segmentation), [] (in fluorescence microscopy),
[], again for nonlocal deconvolution, and [] for deconvolution and tomographic
reconstruction. In [], a paper dedicated to another notoriously ill-posed problem, the
super-resolution, the nonlocal variational principle is viewed as “an emerging powerful
family of regularization techniques,” and the paper “proposes to use the example-based
approach as a new regularising principle in ill-posed image processing problems such as
image super-resolution from several low resolution photographs.” For all these methods,
the weight distribution is computed in the first iteration and is maintained during the whole
iteration process.
In this section, we will concentrate on the last nonlocal functional as a linear diffusion
process and therefore as a heat equation is the associated graph to the image, that is, no
fidelity term will be added to the functional.
..
Linear Diﬀusion: Seed Growing
In [], [], a novel method was proposed for performing multi-label, semi-automated
medical image segmentation. The Grady segmentation method is a linearized sigma filter
applied to propagate seed regions.
Given a small number of pixels with user-defined labels which are called seeds, this
method computes the probability that a random walker starting at each unlabeled pixel
will first reach one of the pre-labeled pixels. By assigning each pixel to the label for which
the greatest probability is calculated, a high-quality image segmentation can be obtained.
With each unlabeled pixel, a K-tuple vector is assigned that represents the probability
that a random walker starting from this unlabeled pixel first reaches each of the K seed

Local Smoothing Neighborhood Filters 

points. A final segmentation may be derived from these K-tuples by selecting for each
pixel the most probable seed destination for a random walker. By biasing the random
walker to avoid crossing sharp intensity gradients, a quality segmentation is obtained that
respects object boundaries (including weak boundaries). The image (or volume) is treated
as a graph with a fixed number of vertices and edges. Each edge is assigned real-valued
weight corresponding to the likelihood that a random walker will cross that edge (e.g., a
weight of zero means that the walker may not move along that edge). By a classical result
the probability that a random walker first reaches a seed point exactly equals the solu-
tion to the heat equation [] with boundary Dirichlet conditions at the locations of the
seed points, the seed point in question being fixed to unity, while the other seeds are set
to zero.
This idea was not quite new. Region competition segmentation is an old concept [].
One can also refer to an algorithm developed for machine learning by Zhu et al. [], which
also finds clusters based upon harmonic functions, using boundary conditions set by a few
seed points. Ref. [] also involves weights in the image considered as a graph and takes
seed points. The method is also directly related to the recent image coloring method of
Sapiro et al. by diffusion from seeds [] (see also []).
Thus, the Grady segmentation method is a linearized sigma filter applied to propagate
seed regions.
> Figure -taken from [] illustrates the process on a two chamber
view of a cardiac image. The gray curves are user-defined seed regions roughly denoting
the ventricles in the image. In that case, one of the seed regions is put to and the other to .
A diffusion with sigma filter weights computed on the original image uis applied until a
steady state is attained. This gives at each pixel y a value p(y) between and , which is
interpreted as the probability for y to belong to the region of the first seed. In this binary
case, a single threshold at .gives the black curves separating the regions of both seeds.
⊡Fig. -
(Taken from [].) The Grady segmentation method is a linearized sigma ﬁlter applied to
propagate seed regions. The gray curves are user-deﬁned seed regions. A diﬀusion with
sigma ﬁlter weights computed on the original image uis applied until a steady state is
attained. A threshold gives the black curves separating the regions of initial seeds


Local Smoothing Neighborhood Filters
⊡Fig. -
Left and from top to bottom: initial chromatic data on the gray image, linear diﬀused seeds
by using neighborhood ﬁlter weights on the gray image, and the same for the NL-means
weights. Right: details of left-hand images. The neighborhood ﬁlter weights are not robust
since just a single point from diﬀerent objects can be easily confused and iteration may lead
to an incorrect colorization
Like the active contour method, this method is highly dependent on the initial seeds. It is,
however, much less sensitive to noise than the snakes method [] and permits to initial-
ize fairly far from the desired contours. We will see that by the histogram concentration
phenomenon, one can get similar or better results without any initialization.
The very same process as illustrated allows to diffuse initial chromatic information
on an initial gray image as we exposed in the introduction.
> Figure -illustrates
this application and compares the obtained solution by using the NL-means and the
neighborhood filter.
..
Linear Diﬀusion: Histogram Concentration
The segmentation process can be accomplished by iterating the neighborhood filter and
computing the weight distribution in the initial image, as displayed in > Fig. -. The top
image shows one slice of a D CT image with interest area surrounded by a parallelepiped.
The next row shows several slices of this area of interest. It can be appreciated, first, that
the background of arteries has a lot of oscillating clutter and, second, that the gray level
value in arteries varies a lot, thus making an automatic threshold problematic. The best

Local Smoothing Neighborhood Filters 

⊡Fig. -
Comparative behavior of discussed methods in D. Application to a D angiograhy CT image
of the head where blood vessels should be segmented. Top: one slice image of the CT
volume data with marked interested area. Middle: display of interest area for several slices of
the D image. Second row: ﬁltered slices by using median ﬁlter. Third row: sigma ﬁlter. Fourth
row: D nonlocal heat equation. Bottom: ﬁltered slices by using the linear method with D
NL-means weights. The whole sequence has been treated as a D image with a weight
support of (× × ) and a comparison window of × × . The background is ﬂattened and
blood vessels are enhanced. Thus, a better segmentation is possible by a simple threshold,
as justiﬁed by > Fig. -
way actually to convince oneself that even in this small area a direct threshold would
not do the job is to refer to the histograms of
> Fig. -. The first histogram that is
Gaussian-like and poorly concentrated corresponds to the background. The background
mode decreases slowly. On the far right part of the histogram, one can see a small pick
corresponding to very white arteries. The fixing of an accurate threshold in the slowly
decreasing background mode is problematic. The top right histogram shows what happens
after the application of a median iterative filtering (the mean curvature motion). The his-
togram does not concentrate at all. The bottom left histogram is obtained after applying
the linearized neighborhood filter. The bottom right histogram is the one obtained by


Local Smoothing Neighborhood Filters
1
0.8
0.6
0.4
z
z
z
0.2
0
1
1
0.8
0.6
0.4
z
0.2
0
1
1
0.8
0.6
0.4
0.2
0
1
1
0.8
0.6
0.4
0.2
0
1
0.5
0.5
y
y
y
y
x
x
x
x
0 0
0 0
0 0
0 0
0.5
0.5
1
1
0.5
0.5
0.5
0.5
⊡Fig. -
From top to bottom and left to right: original iso-surface of the D image, same iso-surface
ﬁltered by iterative median ﬁlter, by linear sigma ﬁlter, and by linear NL-means. The
iso-surface extracted from the original image presents many irregularities due to noise. The
median ﬁlter makes them disappear, but makes important parts disappear and some
vessels disconnect or fuse. Linear NL-means keeps most vessels and maintains the topology
the linearized NL-means described in the same section. In both cases, one observes that
the background mode of the histogram is strongly concentrated on a few gray level val-
ues. An automatic threshold is easily fixed by taking the first local minimum after the
main histogram peak. This histogram concentration is very similar to the obtained by
the mean-shift approach [] where the neighborhood filter is nonlinearly iterated. In
that case, the authors show that clusters tend to its mean, yielding piecewise constant
image.

Local Smoothing Neighborhood Filters 

100,000
80,000
60,000
40,000
20,000
0
0
50
100
150
200
250
100,000
80,000
60,000
40,000
20,000
0
0
50
100
150
200
250
100,000
80,000
60,000
40,000
20,000
0
0
50
100
150
200
250
100,000
80,000
60,000
40,000
20,000
0
0
50
100
150
200
250
⊡Fig. -
Gray level histogram of D areas of interest. Top left: original D image before. Top right:
after median ﬁltering. Bottom left: after proposed method with sigma ﬁlter weights. Bottom
right: proposed method with NL-means weights. The background is now represented by a
few gray level values when the volume is ﬁltered by the proposed method. A threshold can
therefore be more easily and automatically applied
The histogram concentration phenomenon is actually visible in the comparative evolu-
tion of some slices under the various considered filters, as shown in > Fig. -. The first
row shows these slices picked in the interest area. The topology killing effect of the median
filter (mean curvature motion): small arteries tend to vanish and larger ones shrink and
become circular as shown in the third slice showing an artery section. The third row is
dedicated to the linear sigma filter, which corresponds to Grady’s method applied directly
to the image instead of using seeds. It is quite apparent that well-contrasted objects are
well maintained and the contrast augmented, in agreement with the consistency of this
recursive filter with the Perona–Malik equation. However, the less contrasted objects tend
to vanish because, on them, the evolution becomes similar to an isotropic heat equation.
The fourth row is the result of applying the D nonlocal linear heat equation, where the
Laplacian coefficients are computed from the original image. The whole sequence has been
treated as a D image with a weight support of (× × ) and a comparison window of
× × . Clearly the background is flattened and blood vessels are enhanced on this back-
ground. A threshold just above the homogeneously made background level should give


Local Smoothing Neighborhood Filters
back arteries, and this indeed occurs. Thus, in that case, the D visualization of objects
with complex topology like the cerebral arteries can be achieved by an automatic threshold.
The exact segmentation of the artery is a more difficult problem. Even if the histogram is
concentrated, a different choice of the visualization threshold can produce slightly different
surfaces.
References and Further Reading
. Andreu F, Ballester C, Caselles V, Mazon JM
() Minimizing total variation flow. Comptes
Rendus de l’ Academie des Sciences Series I
Mathematics ():–
. Arias P, Caselles V, Sappiro G () A varia-
tional framework for non-local image inpaint-
ing. In: Proceedings of the EMMCVPR. Springer,
Heidelberg
. Attneave F () Some informational aspects
of
visual
perception.
Psychol
Rev
():
–
. Aubert G, Kornprobst P () Mathematical
problems in image processing: partial differential
equations and the calculus of variations. Springer,
New York
. Bae S, Paris S, Durand F () Two-scale tone
management for photographic look. ACM Trans
Graphic (TOG) ():
. Barash D () A fundamental relationship
between bilateral filtering, adaptive smoothing,
and the nonlinear diffusion equation. IEEE Trans
Pattern Anal Mach Intell :–
. Bennett EP, Mason JL, McMillan L () Multi-
spectral bilateral video fusion. IEEE Trans Image
Process ():
. Boulanger J, Sibarita JB, Kervrann C, Bouthemy P
() Nonparametric regression for patch-
based fluorescence microscopy image sequence
denoising. In: th IEEE international symposium
on biomedical imaging: from nano to macro,
. ISBI , pp –
. Boykov Y, Jolly MP () Interactive graph cuts
for optimal boundary and region segmentation
of objects in ND images. Int Conf Comput Vis
:–
. Brailean
JC,
Kleihorst
RP,
Efstratiadis
S,
Katsaggelos AK, Lagendijk RL () Noise
reduction filters for dynamic image sequences: a
review. Proc IEEE ():–
. Buades A, Coll B, Lisani J, Sbert C () Con-
ditional image diffusion. Inverse Probl Imaging
():
. Buades A, Coll B, Morel JM () A review
of image denoising algorithms, with a new one.
Multiscale Model Simul ():–
. Buades A, Coll B, Morel JM () Neighbor-
hood filters and PDE’s. Numer Math ():–
. Buades A, Coll B, Morel JM () Nonlocal
image and movie denoising. Int J Comput Vision
():–
. Buades A, Coll B, Morel JM, Sbert C () Self-
similarity driven color demosaicking. IEEE Trans
Image Process ():–
. Caselles V, Kimmel R, Sapiro G () Geodesic
active contours. Int J Comput Vision ():–
. Choudhury P, Tumblin J () The trilateral
filter for high contrast images and meshes. In:
ACM SIGGRAPH courses, ACM, p 
. Cleveland WS () Robust locally weighted
regression and smoothing scatterplots. J Am Stat
Assoc ():–
. Coifman RR, Donoho DL () Translation-
invariant de-noising. Lecture notes in statistics,
pp –
. Comaniciu D, Meer P () Mean shift: a robust
approach toward feature space analysis. IEEE
Trans Pattern Anal Mach Intell ():–
. Criminisi A, Pérez P, Toyama K () Region
filling and object removal by exemplar-based
image inpainting. IEEE Trans Image Process
():–
. Danielyan A, Foi A, Katkovnik V, Egiazarian K
() Image and video super-resolution via spa-
tially adaptive block-matching filtering. In: Pro-
ceedings of international workshop on local and
non-local approximation in image processing
. De Bonet JS () Multiresolution sampling
procedure for analysis and synthesis of texture

Local Smoothing Neighborhood Filters 

images. In: Proceedings of the th annual con-
ference on Computer graphics and interactive
techniques, ACM Press/Addison-Wesley, p 
. Delon J, Desolneux A () Flicker stabilization
in image sequences. hal.archives-ouvertes.fr
. Di Zenzo S () A note on the gradient
of a multi-image. Comput Vision Graph ():
–
. Dong B, Ye J, Osher S, Dinov I () Level
set based nonlocal surface restoration. Multiscale
Model Simul :
. Donoho
DL
()
De-noising
by
soft-
thresholding.
IEEE
Trans
Inf
Theory
():–
. Durand F, Dorsey J () Fast bilateral filter-
ing for the display of highdynamic-range images.
In: Proceedings of the th annual conference on
Computer graphics and interactive techniques,
ACM New York, pp –
. Ebrahimi M, Vrscay ER () Solving the
inverse problem of image zooming using “Self-
Examples”. Lecture notes in computer science,
vol , p 
. Ebrahimi M, Vrscay ER () Multi-frame
super-resolution with no explicit motion esti-
mation. In: Proceedings of the interna-
tional conference on image processing, computer
vision, and pattern recognition
. Efros AA, Leung TK () Texture synthesis by
non-parametric sampling. In: International con-
ference on computer vision, vol , Corful, Greece,
pp –
. Eisemann E, Durand F () Flash photography
enhancement via intrinsic relighting. ACM Trans
Graphic (TOG) ():–
. Elad M, Datsenko D () Example-based reg-
ularization deployed to superresolution recon-
struction of a single image. Compu J :–
. Elmoataz A,LezorayO,BougleuxS,TaVT ()
Unifying local and nonlocal processing with par-
tial difference operators on weighted graphs. In:
International workshop on local and non-local
approximation in image processing
. Fleishman S, Drori I, Cohen-Or D () Bilat-
eral mesh denoising. ACM Trans Graphic (TOG)
():–
. Gilboa G, Osher S () Nonlocal linear
image regularization and supervised segmenta-
tion. Multiscale Model Simul ():–
. Grady L () Random walks for image seg-
mentation. IEEE Trans Pattern Anal Mach Intel
():
. Grady L, Funka-Lea G () Multi-label image
segmentation for medical applications based on
graph-theoretic electrical potentials. In: Com-
puter vision and mathematical methods in med-
ical and biomedical image analysis, ECCV, pp
–
. Grady LJ () Space-variant computer vision:
a graph-theoretic approach. PhD thesis, Boston
University
. Guichard F, Morel JM, Ryan R Contrast invariant
image analysis and PDEs. Book in preparation
. Harten A, Engquist B, Osher S, Chakravarthy
SR () Uniformly high order accurate essen-
tiallynon-oscillatoryschemes,III.J ComputPhys
():–
. Huhle B, Schairer T, Jenke P, Straßer W ()
Robust non-local denoising of colored depth
data. In: IEEE computer society conference on
computer vision and pattern recognition work-
shops, pp –
. Jones TR, Durand F, Desbrun M () Non-
iterative, feature-preserving mesh smoothing.
ACM Trans Graphic ():–
. Jung M, Vese LA () Nonlocal variational
image deblurring models in the presence of
Gaussian or impulse noise
. Kimia BB, Tannenbaum A, Zucker SW () On
the evolution of curves via a function of cur-
vature, I: the classical case. J Math Anal Appl
():–
. Kimmel R, Malladi R, Sochen N () Images as
embedded maps and minimal surfaces: movies,
color,texture, and volumetric medical images. Int
J Comput Vision ():–
. Kindermann S, Osher S, Jones PW () Debl-
urring and denoising of images by nonlocal func-
tionals. Multiscale Model Simul ():–
. Lee JS () Digital image smoothing and
the sigma filter. Computer Vision Graph ():
–
. Lezoray O, Ta VT, Elmoataz A () Nonlocal
graph regularization for image colorization. In:
International conference on pattern recognition
. Lou Y, Zhang X, Osher S, Bertozzi A () Image
recovery via nonlocal operators. J Sci Comput
():–


Local Smoothing Neighborhood Filters
. Mairal J, Elad M, Sapiro G et al () Sparse
representation for color image restoration. IEEE
Trans Image Process ():
. Masnou
S
()
Filtrage
et
désocclusion
d’images
par
méthodes
d’ensembles
de
niveau. PhD thesis, Ceremade, Universit e
Paris-Dauphine
. Mignotte M () A non-local regularization
strategy for image deconvolution. Pattern Recog-
nit Lett ():–
. Osher S, Rudin LI () Feature-oriented image
enhancement using shock filters. SIAM J Numer
Anal ():–
. Ozkan MK, Sezan MI, Tekalp AM () Adap-
tive motion-compensated filtering of noisy image
sequences. IEEE Trans Circuits Syst Video Tech-
nol ():–
. Peng H, Rao R, Messinger DW Spatio-spectral
bilateral filters for hyperspectral imaging
. Perona P, Malik J () Scale-space and edge
detection using anisotropic diffusion. IEEE Tran
PAMI ():–
. Petschnigg G, Szeliski R, Agrawala M, Cohen M,
Hoppe H, Toyama K () Digital photography
with flash and no-flash image pairs. ACM Trans
Graphic (TOG) ():–
. Peyré G () Manifold models for signals
and images. Computer Vis Image Underst ():
–
. Peyré G () Sparse modeling of textures. J
Math Imaging Vis ():–
. Polzehl J, Spokoiny V () Varying coefficient
regressionmodelingbyadaptiveweightssmooth-
ing. Preprint 
. Protter M, Elad M, Takeda H, Milanfar P ()
Generalizing
the
nonlocal-means
to super-
resolution reconstruction. IEEE Trans Image
Process ():–
. Ramanath R, Snyder WE () Adaptive demo-
saicking. J Electron Imaging :
. Rudin L, Osher S, Fatemi E () Nonlinear total
variation based noise removal algorithms. Phys D
(–):–
. Sapiro G, Ringach DL () Anisotropic dif-
fusion of multivalued images with applications
to color filtering. IEEE Trans Image Process
():–
. Sapiro G, Ringach DL () Anisotropic dif-
fusion of multivalued images with applications
tocolor filtering. IEEE Trans Image Process
():–
. Sethian JA () Curvature and the evolution of
fronts. Commun Math Phys ():–
. Shi J, Malik J () Normalized cuts and image
segmentation. IEEE Trans Pattern Anal Mach
Intell ():–
. Smith SM, Brady JM () SUSAN: a new
approach to low level image processing. Int J
Comput Vision ():–
. Szlam AD, Maggioni M, Coifman RR ()
A general framework for adaptive regularization
based on diffusion processes on graphs. Yale tech-
nichal report
. Tomasi C, Manduchi R () Bilateral filtering
for gray and color images. In: Proceedings of
the sixth international conference on computer
vision, , pp –
. van den Boomgaard R, van de Weijer J ()
On the equivalence of localmode finding, robust
estimation and mean-shift analysis as used in
early vision tasks. In: International conference
on pattern recognition, vol , Citeseer, pp
–
. Weickert J () Anisotropic diffusion in image
processing. Citeseer
. Winnemoller H, Olsen SC, Gooch B ()
Real-time video abstraction. ACM Trans Graphic
(TOG) ():
. Wong A, Orchard J () A nonlocal-means
approach to exemplar-based inpainting. In: th
IEEE international conference on image process-
ing, , pp –
. Yaroslavsky LP () Digital picture processing.
Springer Secaucus
. Yaroslavsky LP () Local adaptive image
restorationandenhancementwith theuseof DFT
and DCT in a running window. In: Proceedings
of SPIE, vol , p 
. Yaroslavsky LP, Egiazarian KO, Astola JT ()
Transform domain image restoration methods:
review, comparison, and interpretation. In: Pro-
ceedings of SPIE, vol , p 
. Yatziv L, Sapiro G () Fast image and video
colorization using chrominance blending. IEEE
Trans Image Process ():–
. Yoshizawa S, Belyaev A, Seidel HP ()
Smoothing by example: Mesh denoising by aver-
aging with similarity-based weights. In: IEEE

Local Smoothing Neighborhood Filters 

international conference on shape modeling and
applications, pp –
. Zhang D, Wang Z () Image information
restoration based on longrange correlation. IEEE
Trans Circuits Syst Video Technol ():–
. Zhang X, Burger M, Bresson X, Osher S ()
Bregmanized nonlocal regularization for decon-
volution and sparse reconstruction. UCLA CAM
report –
. Zhu SC, Yuille A () Region competition: uni-
fying snakes, region growing, and bayes/MDL
for multiband image segmentation. IEEE Trans
Pattern Anal Mach Intell ():–
. Zhu X, Lafferty J, Ghahramani Z () Semi-
supervised learning: from Gaussian fields to
gaussian processes. School of Computer Science,
Carnegie Mellon University


Neighborhood Filters and
the Recovery of D
Information
Julie Digne ⋅Mariella Dimiccoli ⋅Philippe Salembier ⋅
Neus Sabater
.
Introduction....................................................................
.
Bilateral Filters Processing Meshed D Surfaces .............................
..
Bilateral Filter Definitions. ...........................................................
..
Trilateral Filters........................................................................
..
Similarity Filters........................................................................
..
Summary of D Mesh Bilateral Filter Definitions.................................
..
Comparison of Bilateral Filter and Mean Curvature Motion Filter on
Artificial Shapes.........................................................................
..
Comparison of the Bilateral Filter and the Mean Curvature Motion Filter
on Real Shapes..........................................................................
.
Depth-Oriented Applications..................................................
..
Bilateral Filter for Improving the Depth Map Provided by Stereo Matching
Algorithms...............................................................................
..
Bilateral Filter for Enhancing the Resolution of Low-Quality
Range Images...........................................................................
..
Bilateral Filter for the Global Integration of Local Depth Information. .......
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Neighborhood Filters and the Recovery of D Information
Abstract: Following their success in image processing (see > Chap. ), neighborhood
filters have been extended to D surface processing. This adaptation is not straightforward.
It has led to several variants for surfaces depending on whether the surface is defined as a
mesh, or as a raw data point set. The image gray level in the bilateral similarity measure is
replaced by a geometric information such as the normal or the curvature. The first section
of this chapter reviews the variants of D mesh bilateral filters and compares them to the
simplest possible isotropic filter, the mean curvature motion.
In a second part, this chapter reviews applications of the bilateral filter to a data com-
posed of a sparse depth map (or of depth cues) and of the image on which they have
been computed. Such sparse depth cues can be obtained by stereo vision or by psy-
chophysical techniques. The underlying assumption to these applications is that pixels
with similar intensity around a region are likely to have similar depths. Therefore, when
diffusing depth information with a bilateral filter based on locality and color similarity,
the discontinuities in depth are assured to be consistent with the color discontinuities,
which is generally a desirable property. In the reviewed applications, this ends up with
the reconstruction of a dense perceptual depth map from the joint data of an image and of
depth cues.
.
Introduction
The idea of processing a pixel relatively to its similar-looking neighbors proved to be very
powerful and was adapted to solve a huge variety of problems. Since its primary goal is
to denoise data and since the same denoising problem appeared for three-dimensional
surfaces, the idea of a D bilateral filter was only natural. Nevertheless, we shall see that this
extension is far from straightforward. Multiple adaptations have in fact been introduced,
experimental results show that it is far better for denoising a shape while preserving edges
than an isotropic filter (as one could expect).
The bilateral filter could be used not only to filter images but also to diffuse information
across an image: in numerous applications, some information (e.g., depth value) is given
only at some point positions. The problem is then to extrapolate the information for all
pixels in the image. This can be used to improve the quality of disparity maps obtained by
stereoscopy or to diffuse depth cues in images.
In the present chapter, the different applications will be reviewed and tested exper-
imentally.
> Section .reviews bilateral filters applied to D data point sets, often
organized in a triangulation (a mesh). It ends up with comparative simulations illus-
trating the advantage of bilateral filters on isotropic filtering.
> Section .considers
the various cases where, in an image, depth values or depth cues are available and
shows that the bilateral filter used as a diffusion tool performs well in restoring a dense
depth map.
A previous review by Paris et al. [] discusses the bilateral filter and its implementation.
It also provides an overview of numerous applications.

Neighborhood Filters and the Recovery of D Information 

.
Bilateral Filters Processing Meshed D Surfaces
This section proceeds by first examining the various adaptations of bilateral filtering on
meshes (triangulated D surfaces) and discussing their implementation, which can depend
on the surface triangulation. Finally several comparative experiments on synthetic and
real meshes will be performed. Since a common notation is needed for all methods,
this section starts with a small glossary and notation summary to which the reader may
refer.
Glossary and notation
•
M: The mesh, namely a set of triangles
•
v: Current mesh vertex to be denoised
•
N(v): Neighborhood of vertex v (this neighborhood excludes v)
•
nv, np, etc.: Normals at vertex v or point p, etc. to the underlying surface
•
w(∣∣p−v∣∣), w(< nv, p−v >), etc.: D centered Gaussians with various variances, used
as weighting functions applied to the distance of neighbors to the current vertex and to
the distance along the normal direction at v
•
Hv, Hp, etc.: Curvatures of the underlying surface at v, p, etc.
•
f : Triangle of a mesh
•
a f : Area of triangle f
•
c f : Barycenter of triangle f
•
n f : Normal to triangle f
•
Π f : Projection on the plane containing triangle f
•
V: Voxel containing points of the data set
•
s′, v′, p′, n′
v: Processed versions of s, v, p, nv, . . .
•
∥p −q∥: Euclidean distance between points p and q
The neighborhood filter or sigma filter is attributed to Lee [] in but goes
back to Yaroslavsky and the Sovietic image processing theory (see the book summariz-
ing these works []) in D image analysis. A recent variant by Tomasi and Manduchi
names it bilateral filter []. The bilateral filter denoises a pixel by using a weighted
mean of its similar neighbors’ gray levels. In the original article, the similarity mea-
sure was the difference of pixel gray levels, yielding for a pixel v of an image I with
neighborhood N(v):
ˆI(v) =

C(v)
∑
p∈N (v)
w(∥p −v∥)w(∣I(v) −I(p)∣)I(p),
where wet ware decreasing functions on R+ (e.g., Gaussian) and C(v) is a nor-
malizing coefficient: C(v) = ∑p∈N (v) w(∥p −v∥)w(∣I(v) −I(p)∣). Thus, ˆI(v) is an
average of pixel values for pixels that are similar in position but also in value. Hence the
“bilaterality.”


Neighborhood Filters and the Recovery of D Information
..
Bilateral Filter Deﬁnitions
Filtering without losing the sharp features is as critical for surfaces as it is for images, and a
first adaptation of the bilateral filter to surface meshes was proposed by Fleishman, Drori,
and Cohen-Or in []. Consider a meshed surface M with known normals nv at each
vertex position v. Let N(v) be the one-ring neighborhood of v (i.e., the set of vertices
sharing an edge with v). Then the filtered position of v writes v′ = v + δv ⋅nv, where
δv =

C(v)
∑
p∈N (v)
w(∥p −v∥)w(< nv, p −v >) < nv, p −v >,
(.)
where the weight normalization factor is C(v) = ∑p∈N (v) w(∥p−v∥)w(< nv, p−v >)∣). In
a nutshell, this means that the normal component of the vertex v is moved by a weighted
average of the normal components of its neighboring points which are also close to the
plane tangent to the surface at v. The distance to the tangent plane plays for meshes, the
role that was played for images by the distance between gray levels. If v belongs to a sharp
edge, then the only points close to the tangent plane at v are the points on the edge. Thus,
the edge sharpness will not be smoothed away. One of the drawbacks of the above filter
is clearly the use of a mesh-dependent neighborhood. In case of a mesh with fixed length
edges, using the one-ring neighborhood is the same as using a fixed size neighborhood.
Yet in most cases, mesh edges do not have the same length. The one-ring neighborhood is
then very dependent on the mesh representation and not on the shape itself. This is easily
fixed by defining an intrinsic Euclidean neighborhood.
Another adaptation of the D bilateral filter to surface meshes is introduced by Jones,
Durand, and Desbrun in []. This approach considers the bilateral filtering problem as a
robust estimation problem for the vertex position. A set of surface predictors are linked to
the mesh M: For each triangle f , the position estimator Π f projects a point to the plane
defined by f . Let a f be the surface area and c f be the center of f . Then, for each vertex v,
the denoised vertex is
v′ =

C(v) ∑
f ∈M
Π f (v)af w(∥c f −v∥)w(∥Π f (v) −v∥),
(.)
where C(v) = ∑f ∈M a f w(∥c f −v∥)w(∥Π f (v)−v∥) is the weight normalizing factor and
wand ware two Gaussians.
Thus, the weight w(∣∣c f −v∣∣) is small if the triangle f is close to v. This term is the
classic locality-in-space term of the bilateral. Similarly, w(∥Π f (v)−v∥) measures how far
point v is from its projection onto the plane of the triangle. This weight favors the triangles
f whose plane is coherent with v.
Since the projection on the tangent planes operator Π f depends on the normals to
f , these normals must be robustly estimated. Normals being first-order derivatives, they
are more subject to noise than vertex positions. Hence the method starts by denoising

Neighborhood Filters and the Recovery of D Information 

the normal field. To do so, the mesh is first smoothed using the same formula as above
without the influence weight wand with Π f (v) = c f , namely an updated position:
v′ =

C(v) ∑
f ∈M
c f a f w(∥c f −v∥),
where C(v) = ∑f ∈M a f w(∥c f −v∥). The normal for each face in the denoised mesh is then
computed and assigned to the corresponding face of the original noisy mesh. It is with this
robust normal field that the bilateral filter of > Eq. (.) is applied in a second step.
The idea of filtering normals instead of point positions is crucial in point rendering
applications, as was pointed out by Jones, Durand, and Zwicker in []. Indeed, when ren-
dering a point set, removing noise from normal is more important that removing noise
from point position, since normal variations are in fact what is perceived by observers.
More precisely the eye perceives a dot product of the illumination and the normal, which
makes it very sensitive to noisy normal orientations. The bilateral filter of [] is seen as
a deformation F of the points: v′ = F(v). Then, the update of normal nv can be obtained
through the transposed inverse of the Jacobian J(v) of F(v):
n′
v = J−T(v)nv, where Ji(v) = ∂F
∂vi
(v),
where Ji is the ith column of J and vi is the ith component of v. nv must then be renor-
malized. The rendering of the point set with smoothed normal is better than without any
smoothing.
In [], Wang introduces a related bilateral approach which denoises feature-
insensitively sampled meshes. Feature-insensitively means that the mesh sampling is
independent of the features of the underlying surface like, e.g., uniform sampling. The algo-
rithm proceeds as follows: it detects the shape geometry (namely sharp regions), denoises
the points, and finally optimizes the mesh by removing thin triangles. The bilateral filter is
defined in a manner similar to [], with the difference that only triangles inside a given
neighborhood are used on this definition. Let v be a mesh vertex, N(v) be the set of trian-
gles within a given range of v, and n f , a f , c f be respectively the normal, area, and center
of a facet f (a triangle). Denote by Π f (v) the projection of v onto the plane of f , then the
denoised vertex is defined by
v′ =

C(v)
∑
f ∈N (v)
Π f (v)a f w(∥c f −v∥)w(∥Π f (v) −v∥),
where C(v) = ∑f ∈N (v) a f w(∥c f −v∥)w(∥Π f (v) −v∥) (weight normalizing factor).
The first step is to detect sharp regions. Several steps of bilateral filtering (as defined
in []) are applied, then a smoothness index is computed by measuring the infimum
of angles between normals of faces adjacent to v. By thresholding this measurement, the
sharp vertices are selected. Triangles whose three vertices are sharp and whose size does not
increase during the bilateral iterations are marked as sharp. This detection done, points are
restored to their original positions. Then the bilateral filtering formula is applied to sharp
vertices only, and the geometry sharpness is encoded into a data collection containing


Neighborhood Filters and the Recovery of D Information
normals, centers, and areas of filtered triangles. Points are then restored to their origi-
nal position. Each sharp vertex is moved using the bilateral filtering over the neighboring
stored data units, and thin vertices are removed from the mesh (these last two steps are
iterated a certain number of times). Finally, a post-filtering step consists in applying one
step of bilateral filtering on all non sharp edges.
In [], a two-step denoising method combines the fuzzy C-means clustering method
(see Dunn’s article on fuzzy means []) with a bilateral filtering approach. Fuzzy C-means
is a clustering technique that allows a piece of data to belong to two different clusters.
Each point p gets a parameter μp,k which measures the degree of membership of p to a
cluster k. Let mp be the number of points in the spherical neighborhood of a point p. If
mp < threshold the point is deleted. Otherwise, a fuzzy C-means clustering center cp is
associated with p. The normal at point cp is computed as the normal to the regression plane
of the data set in a spherical neighborhood of p. Fleishman’s bilateral filter [] is used to
filter ci which yields the denoised point. This hybrid and complex method is doubly bilat-
eral. Indeed, the previous C-means clustering selects an adapted neighborhood for each
point and replaces it by an average which is by itself the result of a first bilateral filter in the
wide sense of neighborhood filter. Indeed, the used neighborhood for each point depends
on the point. The second part of the method therefore applies a second classical bilateral
method to a cloud that has been filtered by a first bilateral filter.
The bilateral filtering idea was also used as a part of a surface reconstruction process.
In [], for example, Miropolsky and Fischer introduced a method for reducing position
and sampling noise in point cloud data while reconstructing the surface. A D geomet-
ric bilateral filter method for edge preserving and data reduction is introduced. Starting
from a point cloud, the points are classified in an octree, whose leaf cells are called voxels.
The voxel centers are filtered, representative surface points are defined, and the mesh is
finally reconstructed. A key point is that the denoising depends on the voxel decomposi-
tion. Indeed, the filter outputs a result for each voxel. For a voxel V, call v its centroid with
normal nv. Let wand ube two functions weighting respectively ∥p −v∥, the distance
between a point p position and the centroid location and δ(p,v) = ⟨np, nv⟩, the scalar
product of the normal at p and the normal at the centroid. Then the output of the filter for
voxel V is
v′ =

C(v) ∑
p∈V
w(∥p −v∥)u(δ(p,v))p,
where C(v) = ∑p∈V w(∥p −v∥)u(δ(p,v)). Here wis typically a Gaussian and uis
an increasing function on [,]. But this filter proves unable to recover sharp edges, so a
modification is introduced: prior to any filtering for each voxel V, points of V are projected
onto a sphere centered at the centroid v. Each mapped point is given a normal ˜np which
has direction p −v and is normalized. The geometric filtering is reduced to:
v′ =

C(v) ∑
p∈V
u(δ(˜np, nv))p with C(v) = ∑
p∈V
u(δ(˜np, nv)).
Although only the similarity of normals is taken into account in the above formula, the
filter is bilateral because the average is localized in the voxel.

Neighborhood Filters and the Recovery of D Information 

In [], Liu et al. interpreted the bilateral filter as the association to each vertex v of a
weighted average
v′ =

C(v)
∑
p∈N (v)
w(∥p −v∥)w(∥Πp(v) −v∥)Πp(v),
where C(v) = ∑p∈N (v) w(∥p −v∥)w(∥Πp(v) −v∥) (normalizing factor) and Πp(v) is
a predictor which defines a “denoised position of v due to p,” namely the projection of v
on the plane passing by p and having the normal nv. For example, the bilateral predictor
used in [] is Πp(v) = v + ((p −v) ⋅nv)nv and in [], the used predictor is Πp(v) =
v + ((p −v) ⋅np)np which is the projection of v on the tangent plane passing by p. With
this last predictor the corners are less smoothed out, yet there is a tangential drift due to
the fact that the motion is not in the normal direction nv but in an averaged direction of
the np for p ∈N(v). Therefore a new predictor is introduced:
Πp(v) = v + (p −v) ⋅np
nv ⋅np
nv.
This predictor tends to preserve better the edges than all other bilateral filters.
The question of choosing automatically the parameters for the bilateral filter was raised
by Hou, Bai and Wang in []. It was proposed to choose adaptive parameters. The adaptive
bilateral normal smoothing process starts by searching for the set of triangles (Ti)i whose
barycenters are within a given distance of a center triangle T. (But this keeps a distance
parameter anyway.) Then the influence weight parameter σs is computed as the standard
deviation of the distance between normals ∥n(Ti) −n(T)∥. The spatial weight parameter
is estimated using a minimum length descriptor criterion (for various scales). The esti-
mated parameters are then used to get the smoothed normal. This result is finally used for
rebuilding the mesh using the smoothed normals by Ohtake, Belyaev, and Seidel’s method
described in [].
The bilateral filter has proved to be very efficient to denoise a mesh while preserving
sharp features. The trilateral filter is then a natural extension which takes into account still
more geometric information.
..
Trilateral Filters
Choudhury and Tumblin [] propose an extension of the trilateral image filter to oriented
meshes. It is a two-pass filter: a first pass filters the normals and a second pass filters the
vertex positions. Starting from an oriented mesh, a first pass denoises bilaterally the vertex
normals using the following update:
n′
v =

C(nv)
∑
p∈N (v)
npw(∥p −v∥)w(∥np −nv∥),
where C(nv) = ∑p∈N (v) w(∥p−v∥)w(∥np−nv∥). Then, an adaptive neighborhood N(v)
is found by iteratively adding faces near v until the normals n f of face f differ too much


Neighborhood Filters and the Recovery of D Information
from n′
v. A function F measuring the similarity between normals is built using a given
threshold R,
F(v, f ) = if ∥n′
v −n f ∥< R; otherwise.
The trilateral filter for normals filters a difference between normals. Define nΔ(f ) =
n f −n′
v. Then the trilaterally filtered normal nv is
n′′
v = n′
v +

C(v)
∑
f ∈N (v)
nΔ(f )w(∥c f −v∥)w(nΔ(f ))F(v, f ),
where C(v) = ∑f ∈N (v) w(∥c f −v∥)w(nΔ(f ))F(v, f ). Finally, the same trilateral filter
can be applied to vertices. Call Pv the plane passing through v and orthogonal to n′
v. Call
˜c f the projection of c f onto Pv and cΔ(f ) = ∥˜c f −c f ∥. Then the trilateral filter for vertices,
using the trilaterally filtered normal n′′
v writes
v′ = v + n′′
v

C(v)
∑
p∈N (v)
cΔ(f )w(∥˜c f −v∥)w(nΔ(f ))F(v, f ),
where C(v) = ∑p∈N (v) w(∥˜c f −v∥)w(cΔ(f ))F(v, f ).
The results are similar to [] though slightly better. They are comparable to the results
of [] since both methods use the distance to the tangent plane as a similarity between
points.
..
Similarity Filters
In [], Wang et al. proposed a trilateral filter with slightly different principles. A geometric
intensity of each sampled point is first defined as depending on the neighborhood of the
point
δ(p) =

C(p)
∑
q∈N (p)
wpq < np, q −p >
with
wpq = w(∥q −p∥)w(∥< np, q −p > ∥)wh(∥Hq −Hp∥).
and
C(p) =
∑
q∈N (p)
wpq.
This type of filter is a trilateral filter, which means that it depends on three variables:
distance between the point p and its neighbors q, distance along the normal np between
the point p and its neighbors q, and the difference of their mean curvatures Hp and Hq.
At each point, a local grid is built on the local tangent plane (obtained by local
covariance analysis), and at each point of this grid, the geometry intensity is defined by
interpolation. Thus, neighborhoods of same geometry are defined for each pair of dis-
tinct points and the similarity can be computed as a decreasing function of the Ldistance
between these neighborhoods.

Neighborhood Filters and the Recovery of D Information 

Since the goal is to denoise one point with similar points, the algorithm proposes to
cluster the points into various classes by the mean shift algorithm. To denoise a point, only
points of the same class are used. This gives a denoised geometry intensity δ′(p) and the
final denoised position p′ = p + δ′(p)np.
More recently the NL-means [] method which proved very powerful in image denois-
ing was adapted to meshes and point clouds by Yoshizawa, Belyaev, and Seidel []. Recall
that for an image I, the NL-means filter computes a filtered value J(x) of pixel x as:
J(x) =

C(x) ∫Ω w(x, y)I(y)dy,
an adaptive average with weights
w(x, y) = exp −
h∫Ga(∣t∣)∣I(x −t) −I(y −t)∣dt
and C(x) = ∫Ω w(x, y)dy.
Here Ga is a Gaussian or a compactly supported function, so that it defines a patch.
Thus, the denoised point is a mean of pixel values with weights measuring the local image
similarity of patches around other pixels with the patch around the current pixel.
Consider now the adaptation to a mesh M. Let Ωσ(v) = {y ∈M∣
∣v −y∣≤σ}. The
smoothing is done by changing v at each step: vn+= vn + k(vn)nn
v with nv the normal to
M at v. Let Sy be the surface associated to vertex y. The following definitions are directly
adapted from the image case (a continuous formalism is adopted here for clarity):
k(v) =

C(v) ∫Ωσ
w(v, y)I(y)dSy
C(v) = ∫Ωσ
w(v, y)dSy
I(y) = < nv, y −v >
w(v, y) = exp −D(v, y)
h
.
The problem is to define the similarity kernel D. Let σbe the half radius of the neigh-
borhood used to define the geometric similarity between two points and σbe the half
radius of the domain where similar points are looked for, with σ< σ. The local tangent
plane at y is parametrized by tand t. For all z of Ωσ(y) the translation t is defined as
t = −(< t, z −y >,< t, z −y >) = −(uz,vz), where (uz,vz,wz) are the coordinates of
vertex z in the local coordinate system (y, t, t, ny).
A local approximation Fv(u,v) by Radial Basis Functions (RBF) is built around each
vertex v, and the similarity kernel finally yields:
D(v, y) = ∫Ωσ(y) Gσ(∣t∣)∣Fv(uz,vz) −I(y −t)∣dt
with I(y −t) =< nv, z −v > and Gσa Gaussian kernel with variance σ.


Neighborhood Filters and the Recovery of D Information
Thus each vertex is compared with vertices in a limited domain around it and the
weighted mean over all these nodes yields the denoised position. This results in a bet-
ter feature preserving mesh denoising method, but at the cost of a considerably higher
computation time.
To improve the computation time when denoising data using neighborhood filters,
bilateral approximations were introduced by Paris and Durand among others in [],
where a signal processing interpretation of the D bilateral filter is given, yielding an effi-
cient approximation. Another efficient method is the Gaussian k-d trees introduced by
Adams et al. in []. The proposed method was designed to compute efficiently a class of
n-dimensional filters which replace a pixel value by a linear combination of other pixel
values. The basic idea is to consider those filters as nearest neighbors search in a higher
dimensional space, for example (r, g, b, x, y) in case of a D color image and a bilateral
filter. To accelerate this neighbor search, a Gaussian k-d tree is introduced. Consider the
non local means filter which has, in its naive implementation, a O(nf ) complexity for n
pixels and f × f patches. To apply Gaussian k-d tree, the position of a pixel is set to be the
patch and the value is set to be the color value of the pixel. A simple Principle Component
Analysis (PCA) on patches helps to capture the dimensions that best describe the patches.
The Gaussian k-d tree is also used to perform D NL-means on meshes or point clouds. To
produce a meaningful value to describe geometry, the idea of spin images is used. At each
point sample p, a regression plane is estimated and the coordinates of the neighboring
points in the local coordinate system are used to build a histogram of cylindrical coor-
dinates around (p, np) (the spin image). This gives the position vector. The value of p is
then set to be the difference d = p′ −p between p and the laplacian filtered position p′
expressed in the local coordinate system. This gives the input for building the gaussian k-d
tree yielding good results for mesh denoising.
..
Summary of D Mesh Bilateral Filter Deﬁnitions
The filters reviewed in this section are almost all defined for meshes. Yet, with very little
effort almost all of them can be adapted to unstructured point clouds by simply redefining
the neighborhoods as the set of points within a given distance from the center point (spher-
ical neighborhood). Several classic variants of bilateral filters were examined, but their
main principle is to perform an average of neighboring vertices pondered by the distance
of these vertices to an estimated tangent plane of the current vertex. This distance takes
the role played by the gray level in image bilateral filters. It can be implemented in several
ways by either projecting the current vertex to the neighboring triangles or by projecting
the neighboring vertices on the current triangle or by using an estimate of the normal at
the current vertex which has been itself previously filtered. An interesting and simple pos-
sibility is to directly combine distance of vertices and of their normals or even distances of
vertices, normals, and curvatures (but this requires a previous smoothing to get denoised
normals and curvatures). Notice that position, normal, and curvature characterize the
cloud shape in a larger neighborhood. Thus, at this point, the obvious generalization of

Neighborhood Filters and the Recovery of D Information 

bilateral filters is NL-means, which directly compares pointwise, the shape of the neighbor-
hood of a vertex with the overall shape of the neighborhoods of others before performing
an average of the most similar neighborhoods to deliver a filtered neighborhood.
Sticking to the simplicity of comparisons and to the essentials of bilateral filter, we
shall be contented in the comparative section to illustrate the gains of the bilateral filter
with respect to a (good) implementation of its unilateral counterpart, the mean curvature
motion, performed by projection of each vertex on a local regression plane. The remainder
of this section is divided as follows:
> Sect. ..presents experiments and comparisons
on artificial shapes and > Sect. ..presents results on some real shapes.
..
Comparison of Bilateral Filter and Mean Curvature
Motion Filter on Artiﬁcial Shapes
In the following experiments, the denoising of the bilateral filter as introduced in [] will
be compared with the mean curvature motion (MCM). Recall that [] defined the update
of a point as:
δv =

C(v)
∑
p∈N (v)
w(∥p −v∥)w(< nv, p −v >) < nv, p −v >
with
C(v) =
∑
p∈N (v)
w(∥p −v∥)w(< nv, p −v >)∣)
(see > Eq. (.) for notations).
The mean curvature motion used here is the projection on the regression plane: a vertex
v with normal nv and spherical neighborhood N(v) is projected on the regression plane
of N(v). In [], Digne et al. showed that this operator was an approximation of the mean
curvature motion:
∂v
∂t = Hnv.
The effects of bilateral denoising are first shown on some artificial shapes. A cube with
sidelength is created with added gaussian noise with standard deviation .(> Fig. -).
> Figures -and > -show all points of the D cloud seen from one side of the cube.
Obviously, the edges seem to have some width due to the noise.
The experiments of
> Fig. -a–c show the denoising power of the bilateral filter
in term of preserving edges and should be compared with the standard mean curvature
motion filter (> Fig. -d–f). The comparison is particularly interesting in the corner
areas. The bilateral filter implies an anisotropic curvature motion leading to a diffusion
only in smooth parts while preserving the sharp areas. Let us now see how those filters
perform in case of a sharp edge. An estimation of the noise for each of the denoising meth-
ods is shown on > Fig. -. This estimation was obtained as follows: an edge was created
by sampling two intersecting half planes and adding Gaussian noise, the obtained edge
was then denoised by bilateral filtering an mean curvature motion. Finally, the root mean
square error (RMSE) to the underlying model is computed.
> Figure -tends to prove


Neighborhood Filters and the Recovery of D Information
⊡Fig. -
A noisy cube with Gaussian noise
5.5
5
4.5
4
5.5
5
4.5
4
5.5
5
4.5
4
0.5
1
a  One bilateral iteration
b  Two bilateral iterations
c  Five bilateral iterations
1.5
2
0.5
1
1.5
2
0.5
1
1.5
2
d  One MCM iteration
e  Two MCM iterations
f  Five MCM iterations
5.5
5
4.5
40.5
1
1.5
2
5.5
5
4.5
40.5
1
1.5
2
5.5
5
4.5
40.5
1
1.5
2
⊡Fig. -
Bilateral and MCM iterations on the cube corner. Notice how the sharpness is much better
preserved by the bilateral ﬁlter than by the mean curvature equation
Input
Iteration 1
Iteration 2
Iteration 5
RMSE (bilateral)
0.01
0.0031
0.0019
0.0035
RMSE (mcm)
0.01
0.0051
0.0085
0.0164
⊡Fig. -
Noise estimation for the sharp edge denoising

Neighborhood Filters and the Recovery of D Information 

that Mean Curvature Motion, although it smoothes well the noisy flat parts also smoothes
away the sharpness, whereas the bilateral filter tends to preserve the sharp edges better.
With few iterations, the noisy parts are smoothed out decreasing the root mean square
error. Then, when iterating the operator, the sharpness tends to be smoothed, increasing
the RMSE again. This phenomenon is of course far quicker with the mean curvature motion
since this filter does not preserve edges at all.
..
Comparison of the Bilateral Filter and the Mean
Curvature Motion Filter on Real Shapes
This section starts with running some experiments on the Michelangelo’s David point
cloud. At each step, an interpolating mesh was built for visualization.
On
> Fig. -, denoising artifacts created by the bilateral filter can be seen. They
appear as oscillations, for example, on David’s cheek. These artifacts can be explained by
the fact that the bilateral filter enhances structures. Added noise structures can thus be ran-
domly enhanced by the bilateral filter. > Figure -shows that some noise remains after
one iteration of bilateral denoising. The bilateral filter is therefore iterated with the same
parameters. Then, obviously, the remaining noise disappears at the cost of some sharpness
loss (see > Figs. -and > -). This can also be seen on a noisy simple scan of a screw
nut driver (> Fig. -) and on a fragment of the Stanford Forma Urbis Romae Project
(> Fig. -).
.
Depth-Oriented Applications
This section focuses on the applications of the bilateral filter and its generalized version
to depth-oriented image processing tasks. The common idea to all these applications is to
constrain the diffusion of depth information to the intensity similarity between pixels. The
underlying assumption is that pixels with similar intensity around a region are likely to have
similar depths. Therefore, when diffusing depth information based on intensity similarity,
the discontinuities in depth are assured to be consistent with the color discontinuities. This
is often a desirable property, as it was noticed by Gamble and Poggio [] and Kellman and
Shipley [].
The remainder of this section is organized as follows. > Section ..reviews the appli-
cations of the bilateral filter to stereo matching algorithms, while > Sect. ..describes
an application to the resolution enhancement of range images.
> Section ..reviews
applications to the estimation of depth in single images.
..
Bilateral Filter for Improving the Depth Map
Provided by Stereo Matching Algorithms
Stereo matching algorithms address the problem of recovering the depth map of a D scene
from two images captured from different viewpoints. This is achieved by finding a set of


Neighborhood Filters and the Recovery of D Information
a  Original of David
c  Bilateral denoising
d  MCM
b  Noisy David
⊡Fig. -
Denoising of David’s face
a  Initial noisy David
b  Bilateral denoising
c  MCM
⊡Fig. -
Denoising of David (back)

Neighborhood Filters and the Recovery of D Information 

a  Iteration 1
b  Iteration 2
⊡Fig. -
Iterating the bilateral ﬁlter on David (back)
a  Bilateral filtering
b  MCM
⊡Fig. -
Detail of David
a  Initial scan
b  Bilateral denoising
c  MCM
⊡Fig. -
Denoising of a screw nut driver scan
points in one image which can be identified in the other one. In fact, the point-to-point
correspondences allow to compute the relative disparities, which are directly related to the
distance of the scene point to the image plane.
The matching process is based on a similarity measure between pixels of both images.
Due to the presence of noise and repetitive texture, these correspondences are extremely


Neighborhood Filters and the Recovery of D Information
a  Initial fragment with added gaussian noise
b  Bilateral denoising
c  MCM
⊡Fig. -
Denoising of fragment “u”of Stanford Forma Urbis Romae, see Koller et al. [] for an
explanation of the data
difficult to find without global reasoning. In addition, occluded and textureless regions are
ambiguous. Indeed, local image matching is not enough to find reliable disparities in the
whole image. Because of all these reasons, the matching process yields either low-accuracy
dense disparity maps or high-accuracy sparse ones.
Improvements can be obtained through filtering or interpolation, by using median or
morphological filters for instance. However, their ability to do so is limited. Yin and Coop-
erstock have proposed [] a post-processing step to improve dense depth maps produced
by any stereo matching algorithm. The proposed method consists in applying an iterated
bilateral filter, which diffuses the depth values. This diffusion relies on the original image
gradient instead of the one of the depth image. This allows to incorporate edge informa-
tion into the depth map, assuring discontinuities in depth to be consistent with intensity
discontinuities.
The color-weighted correlation idea underlying the bilateral filter has been exploited
by Yoon and Kweon [] to reduce the ambiguity of the correspondence search problem.
Classically, this problem has been addressed by area-based methods relying on the use of

Neighborhood Filters and the Recovery of D Information 

local support windows. In this approach, all pixels in a window are assumed to have similar
depth in the scene and, therefore, similar disparities. Accordingly, pixels in homogeneous
regions get assigned the disparities inferred from the disparities of neighboring pixels.
However, when the windows are located on depth discontinuities, the same dis-
parity is assigned to pixels having different depths, resulting in a foreground-fattening
phenomenon. This phenomenon was studied by Delon and Rougé []. To obtain accu-
rate results, an appropriate window should be selected for each pixel adaptively. This
problem is addressed by Yoon and Kweon [] by weighting the pixels in a given win-
dow taking into account their color similarity and geometric proximity to the reference
pixel.
The similarity between two pixels is then measured using the support weights in both
windows, taking into account the edge information into the disparity map. Experimental
results show that the use of adaptive support weights produces accurate piecewise smooth
disparity maps, while preserving depth discontinuities.
The idea of exploiting the color-weighted correlation to reduce the ambiguity of the cor-
respondence problem has been implemented in a parallel architecture, allowing its use in
real-time applications and more complex stereo systems. See Yang et al. [] and Wang et al.
[] papers which achieved a good rank in the Middlebury benchmark (Evaluation of the
Middlebury stereo website http://vision.middlebury.edu/stereo/) proposed by Scharstein
and Szeliski [].
The bilateral filter averages the pixel colors, based on both theilr geometric closeness
and their photometric similarity, preferring near values in space and color to distant ones.
Ansar, Castano, and Matthies [], Yoon and Kweon [] and more recently, Mattoccia,
Giardino, and Gambin [] have used the bilateral filter to weight the correlation windows
before the stereo correspondence search. On the other hand, Gehrig and Franke [] have
applied the bilateral filter to obtain an improved and smoother disparity map.
The interpolation of disparity maps and in particular of Digital Elevation Models
(DEMs) has been considered in several recent works. Facciolo and Caselles [] propose to
interpolate unknown areas by constraining a diffusion anisotropic process to the geometry
imposed by a reference image and coupling the process with a data fitting term which tries
to adjust the reconstructed surface to the known data. More recently, Facciolo et al. []
have proposed a new interpolation method which defines a geodesic neighborhood and
fits an affine model at each point. The geodesic distance is used to find the set of points that
are used to interpolate a piecewise affine model in the current sample. This interpolation
is refined by merging the obtained affine patches with a Mumford-Shah-like algorithm.
The a contrario methodology has been used in this merging procedure. In the urban con-
text, Lafarge et al. [] use a dictionary of complex building models to fit the disparity map.
However, the applicability of such a method is less evident because of the initial delineation
of buildings by a rectangle fitting.
We shall illustrate the bilateral interpolation process with experiments from Sabater’s
Ph.D thesis [] where the bilateral filter is used to interpolate a sparse disparity map. Let
q be a point in the image I. Consider Lq ⊂I the subimage where the weight is learned. For
each p ∈Lq the weight due to color similarity and proximity are computed.


Neighborhood Filters and the Recovery of D Information
Color similarity: The following color distance is considered
dc(uq,up) = ((Ru(q) −Ru(p))+ (Gu(q) −Gu(p))+ (Bu(q) −Bu(p)))
/,
where Ru, Gu, and Bu are the red, green, and blue channels of u. Then the weight
corresponding to the color similarity between p and q is
wc(p, q) = exp ⎛
⎝−dc(uq,up)
h

⎞
⎠.
Proximity: The Euclidean distance between the point positions in the image plane is used
d(q, p) = ((q−p)+ (q−p))
/,
where p = (p, p) and q = (q, q). Then the weight corresponding to proximity is
wd(p, q) = exp (−d(q, p)
h

).
Therefore, the total associated weight between the two points q and p is
W(p, q) = 
Zq
wc(p, q)wd(p, q) = 
Zq
exp (−(dc(uq,up)
h

+ d(q, p)
h

)),
where Zq is the normalizing factor Zq = ∑
p∈Lq
wc(p, q)wd(p, q) . The interpolated disparity
map μI is computed via an iterative scheme
μI(q, k) = ∑
p∈Lq
W(p, q)μI(p, k −),
where k is the current iteration and the initialization μI(⋅,) = μ(⋅) is the sparse disparity
to be interpolated.
> Figures -and > -show the interpolated Middlebury results (% density).
The experiments demonstrate that, starting from a disparity map which is very sparse near
image boundaries, the bilateral diffusion process can recover a reasonable depth map.
..
Bilateral Filter for Enhancing the Resolution of
Low-Quality Range Images
Contrary to intensity images, each pixel of a range image expresses the distance between a
known reference frame and a visible point in the scene. Range images are acquired by range
sensors that, when acquired at video rate, are either very expensive or very limited in terms

Neighborhood Filters and the Recovery of D Information 

⊡Fig. -
Tsukuba and Venus results. For each couple of images: stereo pair of images, output
of a sparse algorithm retaining only sure points, points (in red) are the rejected
correspondences, interpolated version of these results and ground truth
of resolution. To increase the resolution of low-quality range images acquired at video rate,
Yang et al. [] have proposed a post-processing step relying on an iterated bilateral filter.
The filter diffuses the depth values of the low-quality range image, steering the diffusion by
the color information provided by a registered high-quality camera image.
The input low-resolution range image is upsampled to the camera image resolution.
Then an iterative refinement process is applied. The up-sampled range image Dis used as


Neighborhood Filters and the Recovery of D Information
⊡Fig. -
Teddy and Cones results. For each couple of images: stereo pair of images, output
of a sparse algorithm retaining only sure points, points (in red) are the rejected
correspondences, interpolated version of these results and ground truth
the initial depth map to build an initial D cost volume c. The D cost volume ci(x,y, d)
associated to the current depth map Di at the i-th iteration is given by:
ci(x,y, d) = min (μL,(d −Di(x,y))),
(.)
where d is the depth candidate, L is the search range controlled by constant μ, and Di(x,y)
is the current depth estimate. To each depth candidate d in the search range corresponds a

Neighborhood Filters and the Recovery of D Information 

single slice (disparity image) of the current cost volume. At each iteration, a bilateral filter
is applied on each slice of the current cost volume ci. This allows to smooth each slice
image while preserving the edges. A new cost volume cBF
i
is therefore generated. Based on
this new cost volume, a refined depth map Di+is obtained by selecting for each (x,y) the
lowest cost candidate d.
..
Bilateral Filter for the Global Integration of Local
Depth Information
Following the phenomenological approach of gestaltists [], the perception of depth in
single images results from the global integration of a set of monocular depth cues. How-
ever, all methods proposed in the computer vision literature to estimate depth in single
real images rely on the use of prior experience about objects and their relationships to
the environment. As a consequence, these methods generally rely on strong assumptions
on the image structure [, ], for instance, that the world is made of ground/horizon-
tal planes and vertical walls; or assumptions on the image content [] such as the prior
knowledge of the class of objects being involved.
In contrast to the state of the art, Dimiccoli, Morel, and Salembier [] proposed a gen-
eral low-level approach for estimating depth in single real images. In this approach, the
global depth interpretation is directly inferred from a set of monocular depth cues, with-
out relying on any previously learned contextual information nor on any strong assumption
on the image structure. In particular, the set of initial local depth hypothesis derived from
different monocular depth cues is used to initialize and constrain a diffusion process. This
diffusion is based on an iterated neighborhood filter.
With this strategy, the occlusion boundaries and the relative distances from the view-
point of depicted objects are simultaneously recovered from local depth information,
without the need of any explicit segmentation. Possible conflicting depth relationships are
automatically solved by the diffusion process itself.
Once monocular depth cues are detected, each region involved in a depth relationship
is marked by one or few points, called source points (see > Fig. -a). Source points mark-
ing the regions closer to the viewpoint are called Foreground Source Points (FSPs), whereas
source points marking the regions more distant to the viewpoint are called Background
Source Points (BSPs). In case of occlusion three source points are marked (see white circle
in > Fig. -a). A single FSP marks the region representing the occluding object and two
corresponding BSPs mark the partially occluded object and the background. In case of con-
vexity, there is a single FSP and its corresponding BSP (see black circle in
> Fig. -a).
The depth image z is initialized by assigning a positive value Δ to FSPs and value to BSPs.
The rest of the image is initialized with value (see > Fig. -b). The diffusion process is
applied to the depth image z by using the gradient of the original image u rather than the
one of the depth image. Doing so, the edge information is incorporated into the depth map,
ensuring that depth discontinuities are consistent with gray level (color) discontinuities.


Neighborhood Filters and the Recovery of D Information
a
b
d
c
⊡Fig. -
Example of depth diﬀusion using > Eq. (.). (a) Gray level image, where BSPs and FSPs are
marked in white and black, respectively. (b) Depth image, where points corresponding to
FSPs are initialized with a positive value (marked in white) and the rest of the image with
value zero. (c) and (d) Depth images after an increasing number of iterations of the DDF
The depth diffusion filter (DDF) proposed in [] by Dimiccoli, Morel, and Salembier is
DDFh,rz(x) =

C(x) ∫Sr(x) z(y)e
−∣u(x)−u(y)∣
h
dy,
(.)
where Sr(x) is a square of center x and side r, h is the filtering parameter which controls
the decay of the exponential function, and
C(x) = ∫Sr(x) e
−∣u(x)−u(y)∣
h
dy
(.)
is the normalization factor. In practice, the parameters are r = and h = .
> Equation (.) is applied iteratively until the stability is attained. In the discrete
case, after each iteration, the values of FSPs and BSPs are updated. More precisely, if the
difference between the values of a FSP and the corresponding BSP becomes smaller than Δ,
then Δ is added to the value of the FSP. In the continuous case, the neighborhood filter can
be seen as a partial differential equation []. With this interpretation, the depth difference
constraints in the discrete case can be understood as the Dirichlet boundary conditions.
Furthermore, they allow to handle multiple depth layers.
> Figure -is an example of the diffusion through the DDF. Using
> Eq. (.) a
very large number of iterations is needed to attain the stability. To make the diffusion faster,

Neighborhood Filters and the Recovery of D Information 

a
c
d
e
f
g
h
b
⊡Fig. -
Example of depth diﬀusion using > Eq. (.) to speed up the diﬀusion. (a) Depth image,
where FSPs have been initialized with a positive value (marked in gray) and the rest of the
image with value zero. From (b) to (g) depth images correspond to an increasing number of
iterations. After each iteration, the depth diﬀerence between corresponding FSPs and BSPs
is forced to be at least equal to the initial depth diﬀerence ∆, by adding ∆to FSPs when the
diﬀerence between corresponding FSPs and BSPs becomes less than ∆. (h) Final depth
image obtained using > Eq. (.) on image (g)
the following equation is used as initialization
DDFh,rz(x) =
sup
y∈Sr(x)
z(y)e
−∣u(x)−u(y)∣
h
,
(.)
while > Eq. (.) is used only in the last iterations (see > Fig. -).


Neighborhood Filters and the Recovery of D Information
a
b
c
⊡Fig. -
(a) Original image; (b) local depth cues are represented through vectors that point to the
region closer to the viewpoint; (c) depth image
Experimental results on real images (see []) proved that this simple formulation turns
out to be very effective for the integration of several monocular depth cues. In particular,
contradictory information given by conflicting depth cues is dealt with the bilateral dif-
fusion mechanism, which allows two regions to invert harmoniously their depths, in full
agreement with the phenomenology. On > Fig. -is shown some experimental results
involving occlusion and convexity. For each experiment three images are shown.
First, the original image (> Fig. -a) is shown. Then, on the second image, the initial
depth gradient at depth cue points is represented by vectors pointing to the region closer
to the viewpoint (red vectors arise from T-junctions, green vectors arise from local con-
vexity) (> Fig. -b). Finally, the third image is the final result of the bilateral diffusion
method (> Fig. -c). In this depth map high values indicate regions that are close to
the camera. First and second rows of
> Fig. -show examples of indoor scenes, for

Neighborhood Filters and the Recovery of D Information 

which a proper solution is obtained. On the third row, there is an example of an outdoor
scene involving a conflict. The T-junction detected on the back of the horse is due to a
reflectance discontinuity and its local depth interpretation is incorrect. However, on the
depth map, the shape of the horse appears clearly on the foreground since the diffusion
process allowed to overcome the local inconsistency. On the last row, there is an example
involving self-occlusion: occluding contours have different depth relationships at different
points along its continuum. However, the bilateral diffusion method performs also well in
this ambiguous situation.
Acknowledgments
The David raw point set is courtesy of the Digital Michelangelo Project, Stanford University.
Fragment “u” of the Stanford Fragment Urbis Romae, Stanford University, the Screw Nut
point set, is provided by the AIM@SHAPE repository and is courtesy of Laurent Saboret,
INRIA. The research is partially financed by Institut Farman, ENS Cachan, the Centre
National d’Etudes Spatiales (MISS Project), the European Research Council (advanced
grant Twelve Labours), and the Office of Naval research (grant N---).
References and Further Reading
. Adams A, Gelfand N, Dolson J, Levoy M ()
Gaussian kd-trees for fast high-dimensional fil-
tering. ACM Trans Graph ():–
. Ansar A, Castano A, Matthies L () Enhanced
real-time stereo using bilateral filtering. In:
DPVT ’: Proceedings of the D data process-
ing, visualization, and transmission, nd inter-
national symposium. IEEE Computer Society,
Washington, pp –
. Buades A, Coll B, Morel JM () A review
of image denoising algorithms, with a new one.
Multiscale Model Simul ():–
. Buades T, Coll B, Morel J-M () Neighbor-
hood filters and pde’s. Numerische Mathematik
():–
. Choudhury P, Tumblin J () The trilateral
filter for high contrast images and meshes.
In: SIGGRAPH ’: ACM SIGGRAPH 
Courses. ACM, New York, p 
. DelageE,LeeH,NgY() A dynamicbayesian
network model for autonomous d reconstruc-
tion from a single indoor image. In: Interna-
tional conference on computer vision and pattern
recognition (CVPR), pp –
. Delon J, Rougé B () Small baseline stereovi-
sion. J Math Imaging Vis ():–
. Digne J, Morel J-M, Mehdi-Souzani C, Lartigue
C (October ) Scale space meshing of raw
data point sets. Preprint CMLA -- ENS
Cachan
. Dimiccoli M () Monocular depth estima-
tion for image segmentation and filtering. Ph.D
thesis, Technical University of Catalonia (UPC),
Catalonia
. Dimiccoli M, Morel JM, Salembier P (December
) Monocular depth by nonlinear diffusion.
In: Indian conference on computer vision,
graphics
and
image
processing
(ICVGIP),
Bhubaneswar
. Dunn JC () A fuzzy relative of the isodata
process and its use in detecting compact well-
separated clusters. J Cybernetics ():–
. Facciolo G,CasellesV () Geodesicneighbor-
hoods for piecewise affine interpolation of sparse
data. In: International conference on image
processing
. Facciolo G, Lecumberry F, Almansa A, Pardo
A, Caselles V, Rougé B () Constrained


Neighborhood Filters and the Recovery of D Information
anisotropic diffusion and some applications. In:
British machine vision conference
. Fleishman S, Drori I, Cohen-Or D () Bilat-
eral mesh denoising. ACM Trans Graph ():
–
. Gamble E, Poggio T () Visual integration and
detection of discontinuities: the key role of inten-
sitiy edges. Technical Report , MIT AI Lab
Memo
. Gehrig SK, Franke U () Improving stereo
sub-pixel accuracy for long range stereo. In: Pro-
ceedings of the th international journal of com-
puter vision, pp –
. Hoiem D, Stein AN, Efros AA, Hebert M ()
Recovering occlusion boundaries from a single
image. In: Proceedings of international confer-
ence on computer vision (ICCV), pp –
. HouQ,Bai L,WangY() Mesh smoothing via
adaptivebilateral filtering.In:Springer (ed) Com-
putational science - ICCS . Springer, Berlin,
pp –
. Jones TR, Durand F, Zwicker M () Normal
improvement for point rendering. IEEE Comput
Graph Appl ():–
. Jones TR, Durand F, Desbrun M () Non-
iterative, feature preserving mesh smoothing. In:
SIGGRAPH ’: ACM SIGGRAPH papers.
ACM, New York, pp –
. Kellman PJ, Shipley TF () Visual interpo-
lation in object perception. Curr Directions in
Psychol Sci ():–
. Koller D, Trimble J, Najbjerg T, Gelfand N,
Levoy M () Fragments of the city: tanford’s
digital forma urbis romae project. In: Proceed-
ings of rd Williams symposium on classical
architecture. J Roman Archaeol (Suppl ), pp
–
. Lafarge F, Descombes X, Zerubia J, Pierrot-
Deseilligny M () Automatic building extrac-
tion from dems using an object approach and
application to the d-city modeling. J Pho-
togramm Remote Sens ():–
. Lee J-S () Digital image smoothing and the
sigma filter. Comput Vision Graph Imag Process
():–
. Liu Y-S, Yu P-Q, Yong J-H, Zhang H, Sun J-G
() Bilateral filter for meshes using new pre-
dictor. In: Springer (ed) Lecture notes in com-
puter science, computational and information
science, vol /. Springer, Heidelberg, pp
–
. Mattoccia S, Giardino S, Gambin A () Accu-
rate and efficient cost aggregation strategy for
stereo correspondence based on approximated
joint bilateral filtering. In: Asian conference on
computer vision (ACCV)
. Metzger W () Gesetze des Sehens. Waldemar
Kramer, Frankfurt
. Miropolsky A, Fischer A () Reconstruction
with d geometric bilateral filter. In: SM ’: Pro-
ceedings of the th ACM symposium on solid
modeling and applications. Eurographics Associ-
ation, Aire-la-Ville, Switzerland, pp –
. Ohtake Y, Belyaev AG, Seidel H-P () Mesh
smoothing by adaptive and anisotropic gaussian
filter applied to mesh normals. In: VMV, pp
–
. Paris S, Durand F () A fast approximation
of the bilateral filter using a signal processing
approach. Int J Comput Vision ():–
. Paris S, Kornprobst P, Tumblin J, Durand F
() Bilateral filtering: theory and applications,
vol . Foundations and Trends® in Computer
Graphics and Vision. IEEE Transactions on Visu-
alization and Computer Graphics
. Rother D, Sapiro G () d reconstruction
from a single image (submitted to IEEE trans-
actions on pattern analysis and machine intelli-
gence IMA Prepr International
. Sabater N () Reliability and accuracy in
stereovision. Application to aerial and satellite
high resolution image. Ph.D. thesis, ENS Cachan
. Scharstein D, Szeliski R () A taxonomy and
evaluation of dense two-frame stereo correspon-
dence algorithms. Int J Computer Vis (–):
–
. Tomasi C, Manduchi R () Bilateral filter-
ing for gray and color images. In: ICCV ’:
Proceedings of the th international conference
on computer vision, IEEE Computer Society,
Washington, p 
. Wang C () Bilateral recovering of sharp
edges on feature-insensitive sampled meshes.
IEEE
Trans
Visual
Comput
Graph
():
–
. Wang L, Liao M, Gong M, Yang R, Nistér D
() High-quality real-time stereo using adap-
tive cost aggregation and dynamic programming.

Neighborhood Filters and the Recovery of D Information 

In: rd international symposium on d data pro-
cessing, visualization and transmission, DPVT
. Wang L, Yuan B, Chen J () Robust fuzzy
c-means and bilateral point clouds denoising. In:
th international conference on signal pro-
cessing, vol , pp –
. Wang
R-F, Zhang
S-Y, Zhang
Y, Ye X-Z
(June
)
Similarity-based
denoising
of
point-sampled
surfaces.
J
Zhejiang
Univ
():–
. Yang Q, Wang L, Yang R, Stewénius H, Nistér D
() Stereo matching with color-weighted
correlation, hierarchical belief propagation and
occlusion handling. IEEE Trans Pattern Anal
Mach Intell (PAMI) ():–
. Yang Q, Yang R, Davis J, Nistér D ()
Spatial-depth super resolution for range images.
In: International conference on computer vision
and pattern recognition (CVPR)
. Yaroslavsky LP () Digital picture processing.
An introduction, vol of Springer series in infor-
mation sciences. Springer, Berlin
. Yin J, Cooperstock JR () Improving depth
maps by nonlinear diffusion. In: Proceedings of
th international conference in Central Europe
on computer graphics, visualization and com-
puter vision (WSCG), pp –
. Yoon K-J, Kweon S () Adaptive support-
weight approach for correspondence search.
IEEE Trans Pattern Anal Mach Intell ():
–
. Yoshizawa S, Belyaev A, Seidel H-P ()
Smoothing by example: mesh denoising by aver-
aging with similarity-based weights. In: SMI’:
Proceedings of the IEEE international conference
on shape modeling and applications . IEEE
Computer Society, Washington, p 


Splines and Multiresolution
Analysis
Brigitte Forster
.
Introduction.....................................................................
.
Historical Background.........................................................
.
Mathematical Modeling and Application.....................................
..
Mathematical Foundations. ..........................................................
...Regularity and Decay Under the Fourier Transform.............................
...Criteria for Riesz Sequences and Multiresolution Analyses.....................
...Regularity of Multiresolution Analysis..............................................
...Order of Approximation...............................................................
...Wavelets.................................................................................
..
B-Splines.................................................................................
..
Polyharmonic B-Splines...............................................................
.
Survey on Mathematical Analysis of Methods................................
..
Schoenberg’s B-Splines for Image Analysis – the Tensor Product
Approach.................................................................................
..
Fractional and Complex B-Splines..................................................
..
Polyharmonic B-Splines and Variants..............................................
..
Splines on Other Lattices.............................................................
...Splines on the Quincunx Lattice.....................................................
...Splines on the Hexagonal Lattice....................................................
.
Numerical Methods.............................................................
.
Open Questions.................................................................
.
Conclusion......................................................................
.
Cross-References................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Splines and Multiresolution Analysis
Abstract: Splines and multiresolution are two independent concepts, which – considered
together – yield a vast variety of bases for image processing and image analysis. The idea
of a multiresolution analysis is to construct a ladder of nested spaces that operate as some
sort of mathematical looking glass. It allows to separate coarse parts in a signal or in an
image from the details of various sizes. Spline functions are piecewise or domainwise poly-
nomials in one dimension (D) resp. nD. There is a variety of spline functions that generate
multiresolution analyses.
The viewpoint in this chapter is the modeling of such spline functions in frequency
domain via Fourier decay to generate functions with specified smoothness in time domain
resp. space domain. The mathematical foundations are presented and illustrated at the
example of cardinal B-splines as generators of multiresolution analyses. Other spline mod-
els such as complex B-splines, polyharmonic splines, hexagonal splines, and others are
considered. For all these spline families exist fast and stable multiresolution algorithms
which can be elegantly implemented in frequency domain. The chapter closes with a look
on open problems in the field.
AMS Subject Classification (): ASpline approximation
DNumerical analysis – Splines
UComputing methodologies and applications – Image processing
TNumerical methods in Fourier analysis
Keywords Cardinal B-splines ⋅image processing ⋅multiresolution analysis ⋅order of
approximation ⋅polyharmonic B-splines ⋅riesz basis ⋅scaling function two-scale relation
.
Introduction
This chapter deals with two originally independent concepts, which were recently com-
bined, and since together have a strong impact on signal and image analysis: the concept
of splines, i.e., of piecewise polynomials, and the concept of multiresolution analysis, i.e.,
splitting functions – or more general data – into coarse approximations and details of var-
ious sizes. Eversince the combination of the two concepts, they have led to a load of new
applications in, e.g., signal and image analysis, as well as in signal and image reconstruction,
computer vision, numerics of partial differential equations, and other numerical fields. An
impression of the vast area of applications can be gained in, e.g., [, , , ].
Already the spline functions alone proved to be very useful for mathematical analysis as
well as for signal and image processing, analysis and representation, for computer graphics
and many more, see, e.g., [, , , , , , ]. An example for a family of spline functions
are I. J. Schoenberg’s polynomial splines with uniform knots [, ]:
βm(t) = 
m!
m+
∑
k=
(−)k (m + 
k
)(t −k)m
+ ,
m ∈N.
(.)
Here, tm
+ denotes the one-sided power function, i.e., tm
+ = for t < and tm
+ = tm for t ≥.

Splines and Multiresolution Analysis 

1
0.8
0.6
0.4
0.2
–1
1
3
5
2
4
⊡Fig. -
Cardinal B-splines of degree m = , . . . , 
The B-splines βm can be easily generated by an iterative process. Let β(t) = χ[,)(t)
be the characteristic function of the interval [,). Then the B-spline of degree m is derived
by the convolution product
βm = β∗βm−= β∗. . . ∗β
	




















































m+-times
for m ∈N,
(.)
where
β∗βm−(x) = ∫R β(y)βm−(x −y) dy = ∫

βm−(x −y) dy.
For an illustration of the cardinal B-splines, see > Fig. -.
Splines had their second breakthrough as Battle [] and Lemarié [] discovered that
B-splines generate multiresolution analyses. The simple form of the B-splines and their
compact support, in particular, were convenient for designing multiresolution algorithms
and fast implementations.
Deﬁnition 
Let A : Rn →Rn be a linear mapping that leaves Zn invariant, i.e., A(Zn) ⊂
Zn and that has (real or complex) eigenvalues with absolute values greater than .
A multiresolution analysis associated with the dilation matrix A is a sequence of nested
subspaces (Vj)j∈Z of L(Rn) such that the following conditions hold:
(i)
. . . ⊂V−⊂V⊂V⊂. . .,
(ii)
⋂j∈Z Vj = {},
(iii)
Span ⋃j∈Z Vj is dense in L(Rn),
(iv)
f ∈Vj
⇐⇒
f (A−j●) ∈V,
(v)
f ∈V
⇐⇒
f (●−k) ∈Vfor all k ∈Zn.
(vi)
There exists a so-called scaling function φ ∈Vsuch that the family {φ(●−k)}k∈Zn
of translates of φ forms a Riesz basis of V.
Here, L(Rn) denotes the vector space of square-integrable functions f : Rn →C with
norm
∥f ∥= (∫Rn ∣f (x)∣dx)




Splines and Multiresolution Analysis
and corresponding inner product
⟨f , g⟩= ∫Rn f (x) g(x) dx,
where g denotes the complex conjugate of g. The elements in L(Rn) are also called
functions of finite energy.
Riesz bases are a slightly more general concept than orthonormal bases. In fact, Riesz
bases are equivalent to orthonormal bases and therefore can be generated by applying a
topological isomorphism on some orthonormal basis.
Deﬁnition
A sequence of functions {fn}n∈Z in a Hilbert space V is called a Riesz sequence
if there exist positive constants A and B, the Riesz bounds, such that
A∥c∥l ≤∥∑
k∈Zn
ck fk∥
V
≤B∥c∥l 
for all scalar sequences C = (ck)k∈Zn ⊂l (Zn).
A Riesz sequence is called a Riesz basis if it additionally spans the space V.
A good introduction to Riesz bases, their properties, and their relation to orthonormal
bases is given in the monography by Young []. Multiresolution constructions with
splines are treated in numerous sources. As a starting point, there are, e.g., the books by
Christensen [, ] and Wojtaszczyk [].
The mathematical properties in Definition have intuitive interpretations. A function
f ∈L(Rn), which is projected orthogonally on Vj, is approximated with the so-called
resolution Aj. In fact, let
Pj : L(Rn) →Vj
denote the orthogonal projection operator. Then (ii) gives that by going to lower resolu-
tions, all details are lost:
lim
j→−∞∥Pj f ∥= .
Whereas when the resolution is increased, j →∞, more and more details are added. By
(iii), the projection then converges to the original function f :
lim
j→∞∥f −Pj f ∥= .
Hereby, the rate of convergence depends on the regularity of f .
The approximation spaces Vj are nested, which allows for computing coarser approxi-
mations in Vk for k < j for functions f ∈Vj. The scaling Ak enlarges details. Property (iv)
shows that the approximation spaces have a similar structure over the scales and emanate
from one another. The translation invariance (v) ensures that the analysis of a function in
Vj is independent of the starting time or location.
And property (vi) finally ensures the beautiful and mighty property that the whole
sequence of nested approximation spaces can be generated by translates and scalings of one

Splines and Multiresolution Analysis 

single function – the scaling function. In fact, (vi) together with (iv) yields that
{φ(Aj ●−k), k ∈Zn}
is a Riesz basis for Vj.
While moving from the coarser approximation space Vj to the finer, larger space Vj+,
information has to be added. In fact, there is an orthogonal complement Wj, j ∈Z, such
that
Vj+= Vj ⊕Wj.
These spaces are called detail spaces or wavelet spaces. It is well known that these
spaces also possess a Riesz basis spanned by shifts of ∣det A∣−generators, the wavelets
ψ, . . . ,ψ∣det A∣−. Here, A is the dilation matrix in Definition . The wavelets can be con-
structed from the scaling function. As a consequence, the knowledge of just the single
function φ allows for the construction of the approximation spaces Vj and for the wavelet
spaces Wj. Detailed information on the generation of wavelets and their properties can be
found in various books, e.g., [, , , , ].
Example 
A simple example for a multiresolution analysis on L(R) is given by piecewise
constant functions. Consider the characteristic function φ = χ[,) of the interval semi-
open interval [,). Then φ generates a dyadic multiresolution analysis, i.e., for A = . The
approximation spaces are
Vj = span {χ[,)(j ●−k)}k∈Z
L(R).
They consist of functions constant on intervals of the form [k−j,(k + )−j). The spaces are
obviously nested and separate L(R) in the sense of Definition (ii). Since piecewise con-
stant functions are dense in L(R), (iii) holds. (iv) – (vi) hold by construction. In fact, this
multiresolution analysis is generated by the B-spline βas scaling function. The B-spline basis
operates as mean-value operator over the support interval. The corresponding wavelet extracts
the details, i.e., the deviation from the mean value. To this end, it operates as a difference oper-
ator. > Figure -shows the scaling function βand the corresponding wavelet, the so-called
Haar wavelet. In > Fig. -, an example of a multiresolution is given.
0.2
0.4
0.6
0.8
–1
–0.5
0.5
1
1
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
⊡Fig. -
Left: The B-spline βis a scaling function and operates as mean value. Right: The
corresponding wavelet, the Haar wavelet, operates as a local diﬀerence operator


Splines and Multiresolution Analysis
1
0.75
0.5
0.25
–0.25
–0.5
–0.75
–1
1
0.75
0.5
0.25
–0.25
–0.5
–0.75
–1
1
0.8
0.6
0.4
0.2
1
0.8
0.6
0.4
0.2
2
4
6
8
1
0.8
0.6
0.4
0.2
2
Original step function
4
6
8
a
c
d
First iteration: approximation and details
b
1
0.8
0.6
0.4
0.2
2
4
6
8
1
0.75
0.5
0.25
–0.25
–0.5
–0.75
–1
2
4
6
8
2
4
6
8
2
4
6
8
2
4
6
8
Second iteration: approximation and details
Third iteration: approximation and details
⊡Fig. -
Multiresolution decomposition of the step function (a) with βas scaling function and with
the Haar wavelet. The approximations (left column) are further iterated and decomposed
into a coarser approximation and details (right column), until the coarsest approximation
step, here the mean value, is reached. The sum of the coarsest approximation in the third
iteration and of all details yields the original function (a). No information is lost in a
multiresolution decomposition.

Splines and Multiresolution Analysis 

.
Historical Background
The idea of piecewise polynomial functions and splines goes back to Schoenberg [, ].
In the s, when computer power started to be used for numerical procedures such
as data fitting, interpolation, solving differential equations or computer aided geometric
design, splines experienced an extreme upsurge. Schoenberg invented and strongly con-
tributed to the concept of cardinal splines, which have equidistant nodes on the integers,
see, e.g., [, , ] and many more.
As a parallel development, in the s, the adaption of signal resolution to only process
relevant details for a particular task evolved. For example, for computer vision, a multires-
olution pyramid was introduced by Burt and Adelson []. It allowed to process an image
first on a low resolution level and then selectively increase the resolution and add more
detailed information when needed. The definition of a dyadic multiresolution analysis, i.e.,
A = Id, was contributed by Mallat [] and Meyer []. An interesting and in some parts
historical collection on the most important articles in multiresolution and wavelet theory
was assembled by Heil and Walnut [].
The concepts of splines and multiresolution where joined by Lemarié [] and Battle
[], when they showed that cardinal B-splines are scaling functions for multiresolution
analyses. This led to many more developments of piecewise polynomial scaling functions
for various settings and also multidimensions [], as, e.g., polyharmonic B-splines [, ]
and other functions inspired from radial basis functions [].
In , S. Mallat published his famous algorithm for multiresolution and wavelet
analysis []. He had developed an efficient numerical method such that multiresolution
decompositions could be calculated in a fast way. For the splines, M. Unser et al. proposed a
fast implementation [, , ] which strongly contributed to the breakthrough of splines
for signal and image analysis. In the last years, periodic, fractional, and complex versions
of splines for multiresolution were developed, e.g., [, , , , ]. Many of them use a
Fourier domain filter algorithm which allows for infinite impulse response filters. The for-
mer important feature of compact support of the cardinal B-splines and other functions is
no longer a limiting criterion. Therefore, it can be expected that many new contributions
on splines will still be made in future by modeling signal and image features in Fourier
domain.
.
Mathematical Modeling and Application
..
Mathematical Foundations
...
Regularity and Decay Under the Fourier Transform
An important idea behind splines and multiresolution is the relation between regularity in
time domain and decay in frequency domain, respectively between decay in time domain
and regularity in frequency domain. To illustrate this, the notion of the Schwartz space is
very useful [, , , ].


Splines and Multiresolution Analysis
Deﬁnition 
The subspace of functions f ∈C∞(Rn) with
sup
∣α∣≤N
sup
x∈Rn(+ ∥x∥)N∣Dα f (x)∣< ∞
for all N = ,,, . . .
is calledthe space of rapidly decreasing functions or Schwartz space S(Rn). The norms induce
a Fréchet space topology, i.e., the space S is complete and metrizable.
Here, Dα = ( ∂
∂x)
α⋯( ∂
∂xn )
αn for every multi-index α = (α, . . . , αn) ∈Nn
o.
The dual space S′(Rn), endowed with the weak-∗-topology, is called space of tempered
distributions.
The following famous linear transform relates the view points of the space domain and of
the frequency domain:
Deﬁnition 
The Fourier transform, defined by
F f (ω) := ̂f (ω) := ∫Rn f (x)e−i⟨ω,x⟩dx,
ω ∈Rn,
is a topological isomorphism on L(Rn) and on S(Rn). Its inverse is given by
f (x) =

(π)n ∫Rn
̂f (ω)ei⟨ω,x⟩dω
in L(Rn) resp. in S(Rn).
The Fourier transform can be extended to the space of tempered distributions. For T ∈S′(Rn)
the Fourier transform is defined in a weak sense as
FT(φ) := ̂T(φ) := T(̂φ)
for all φ ∈S(Rn).
Also on S′(Rn), the Fourier transform is a topological isomorphism.
The Fourier transform has the nice property to relate polynomials and differential
operators.
Theorem 
(i)
Let f ∈S(R). Then for all k ∈N
F (f (k))(ω) = (iω)k ̂f (ω),
and
̂f (k)(ω) = F ((−i ●)k f )(ω).
(ii)
Let P be an algebraic polynomial in Rn, say P(x) = ∑α cαxα
. . . xαn
n , and let f ∈
S(Rn). Then
F (P (
i D) f ) = P ̂f
and
̂
P f = P(iD)̂f,
where P(iD) = ∑α cαi∣α∣Dα.
(iii)
Part (ii) also holds for f ∈S′(Rn).

Splines and Multiresolution Analysis 

Example 
The Fourier transform of the polynomial xk is the tempered distribution
ik d k
dxk δ, k ∈N.
For the construction of a multiresolution analysis, the scaling function can be used as a
starting point. The idea is to choose a scaling function of a certain regularity, such that the
generated multiresolution analysis inherits the smoothness properties. In particular, for
the splines the idea is to model the regularity via decay in Fourier domain. The following
theorem gives a motivation for this. The result can be deduced from the considerations
above, and the fact that S(Rn) is dense in L(Rn):
Theorem 
Let f ∈L(Rn) and its Fourier transform decay as
∣̂f (ω)∣≤C(+ ∥ω∥)−N−ε
for some ε > . Then all partial derivatives of order ≤N −n are continuous and in L(Rn).
These results allow to construct scaling function with explicit regularity and decay prop-
erties, in space and in frequency domain. However, some criteria are needed to verify that
the constructed function generates a multiresolution analysis.
...
Criteria for Riesz Sequences and Multiresolution Analyses
The following is an explicit criterion to verify whether some function φ is a scaling
function.
Theorem 
Let A be a dilation matrix and let φ ∈L(Rn) be some function satisfying the
following properties:
(i)
{φ(●−k)}k∈Zn is a Riesz sequence in L(Rn).
(ii)
φ satisfies a scaling relation. I.e., there is a sequence of coefficients (ak)k∈Zn such that
φ(A−x) = ∑
k∈Zn
akφ(x + k)
in L(Rn).
(.)
(iii)
∣̂φ∣is continuous at and ̂φ() ≠.
Then the spaces
Vj = span {φ(Aj ●−k)}k∈Zn,
j ∈Z,
form a multiresolution analysis of L(Rn) with respect to the dilation matrix A.
Proof
See, e.g., [, Theorem .] for the D case.
∎
For particular applications, the Riesz basis property (i) of {φ(●−k)}k∈Zn in Vis not
enough, but an orthonormal basis is needed. An example for such an application is the


Splines and Multiresolution Analysis
denoising of signals contaminated with Gaussian white noise [, Chap. X, Sect. ..].
However, there is an elegant mathematical method to orthonormalize Riesz bases gener-
ated by shifts of a single function.
Theorem 
Let φ ∈L(Rn). Then the following holds:
(i)
{φ(●−k)}k∈Zn is a Riesz sequence in L(Rn) if and only if there are constants c and
C, such that
< c ≤∑
k∈Zn
∣̂φ(ω + πk)∣≤C < ∞
almost everywhere.
I.e., the autocorrelation filter M(ω) := ∑k∈Zn ∣̂φ(ω + πk)∣is strictly positive and
bounded from above.
(ii)
{φ(●−k)}k∈Zn is an orthonormal sequence if and only if
∑
k∈Zn
∣̂φ(ω + πk)∣= 
almost everywhere.
(iii)
If {φ(●−k)}k∈Zn is Riesz basis of a subspace X of L(Rn), then there exists a function
Φ ∈L(Rn), namely
̂Φ(ω) =
̂φ(ω)
√
∑k∈Zn ∣̂φ(ω + πk)∣
(.)
such that {Φ(●−k)}k∈Zn is an orthonormal basis of X.
Proof
See, e.g., [] and [, Chap. VII].
∎
Due to this theorem, every scaling function can be orthonormalized. Let φ ∈L(Rn) be
some scaling function that generates a multiresolution analysis {Vj}j∈Z of L(Rn). Then
the family {Φ j,k}k∈Zn with
Φ j,k(x) = −j/Φ(−j(x −k)),
and Φ as defined in (> .) is an orthonormal basis of the space Vj, j ∈Z.
Example 
A simple possibility to construct a dyadic multiresolution analysis in L(Rn)
is the tensor product approach. Let (Vj)j∈Z be a dyadic (i.e., A = ) multiresolution analy-
sis of L(R) with scaling function φ. Then (V j)j∈Z with V j = Vj ⊗. . . ⊗Vj
	
























































n-times
together with
the scaling function φ(x, . . . , xn) = φ(x) ⋅. . . ⋅φ(xn) forms a multiresolution analysis of
L(Rn) and dilation matrix Id.
In the same way, the scaling function φ(x, . . . , xn) = φ(x) ⋅. . . ⋅φn(xn) generates a
multiresolution analysis of L(Rn), if every φk, k = , . . . , n, is a scaling function of some D
multiresolution analysis with dilation factor a ∈N/{}.

Splines and Multiresolution Analysis 

...
Regularity of Multiresolution Analysis
In signal and image analysis the choice of an appropriate analysis basis is crucial. Here,
appropriate means that the features of the basis such as smoothness should be in accor-
dance with the properties of the functions to analyze. For example, analyzing a smooth
signal or image with a fractal basis in general yields results that are difficult to interpret
and to work with in practice. In this case, the signal resp. the image model does not match
the model of the basis.
The next section will show that the family of spline bases helps to avoid such difficulties,
because the splines allow for a good adjustment due to their regularity parameter m, cf.
(> .) and (> .). The following definition specifies the term “regular.” (See [].)
Deﬁnition 
Denote Cr the class of r-times continuously differentiable functions in Rn, C
the class of continuous functions, and C−the class of measurable functions.
(i)
Let r = −,,, . . . . A function f : Rn →C is called r-regular, if f ∈Cr and
∣∂α
∂xα f (x)∣≤
Ak
(+ ∥x∥)k
for every k ∈N, every multi-index α with ∣α∣≤max(r,) and constants Ak.
(ii)
A multiresolution analysis of L(Rn) is called r-regular if it generated by an r-regular
scaling function.
It is important to note that the orthonormalization procedure (> .) does not affect the
regularity of the corresponding basis. For the orthonormalized scaling function Φ of a
multiresolution analysis, the same regularity properties hold.
Proposition 
Let φ ∈L(Rn) be an r-regular function, such that {φ(●−k)}k∈Zn forms
a Riesz sequence. Then the via (> .) orthonormalized function Φ is also r-regular [].
...
Order of Approximation
Having found a scaling function that generates a multiresolution analysis, how good do
the corresponding approximation spaces Vj approximate some function f ∈L(Rn) of a
certain regularity? Let
Hk(Rn) = {f ∈L(Rn) : ∥f ∥Hk :=

(π)n ∥(+ ∥●∥Rn)k ̂f ∥L< ∞},
k ∈N,
denote the Sobolev spaces. The following criterion for the order of approximation turns
out to be easy to verify for splines.


Splines and Multiresolution Analysis
Theorem 
Let φ ∈L(Rn) satisfy the following properties [, Theorem .]:
(i)
/̂φ is bounded on some neighborhood of the origin.
(ii)
Let Bε be some open ball centered at the origin and let E := Bε + (πZn/{}). For
some α > k + n/, all derivatives of ̂φ of order ≤α are in L(E).
(iii)
Dγ̂φ(ω) = for all ∣γ∣< k and all ω ∈πZd/{}.
Then V= span {φ(●−k)}k∈Zn provides approximation order k:
For f ∈Hk(Rn),
min{∥f −s(●/h)∥L, s ∈V} ≤const. hk∥f ∥Hk
for all h > .
...
Wavelets
For the step from a coarser approximation space Vj to a finder one Vj+information has
to be added. It is contained in the wavelet or detail space Wj, which is the orthonormal
complement of Vj in Vj+:
Vj+= Vj ⊕Wj.
It follows that Vj+m = Vj ⊕⊕k−
l=Wj+l, and hence
L(Rn) = ⊕
j∈Z
Wj
(.)
can be decomposed in a direct sum of mutually orthogonal detail spaces. Moreover, the
detail spaces Wj inherit the scaling property from Definition (iv) for the approximation
spaces Vj. For all j ∈Z,
f ∈Wj
⇐⇒
f (A−j●) ∈W.
The question now is whether there is also a simple basis generated by the shifts of one or
few functions, the wavelets. The following definition is motivated from > Eq. (.).
Deﬁnition
Let Abe a dilation matrix, and let {ψl }l=, ... ,s be a set of functions in L(Rn),
such that the family
{∣det A∣j/ψl(Aj ●−k) ∣l = , . . . , s, j ∈Z, k ∈Zn}
forms an orthonormal basis of L(Rn). Then {ψl}l=, ... ,s is called wavelet set associated
with A.
What qualitative properties do the wavelets have? The approximation spaces Vj are gen-
erated by the scaling function, which operates as a low pass filter. This can be seen from
Theorem (iii) ̂φ() ≠, resp. from Theorem (i): /̂φ is bounded in some neighborhood
of the origin. Therefore, the added details and thus the wavelets have to carry the high-
frequency information. In addition, the wavelets ψ in Ware elements of Vand therefore
have the form
ψ(A−x) = ∑
k∈Zn
akφ(x −k)
(.)

Splines and Multiresolution Analysis 

in L-norm, where {ak}k∈Zn are the Fourier coefficients of a certain πZn-periodic
function.
Proposition 
Let (Vj)j∈Z be a multiresolution analysis of L(Rn) with respect to the
dilation matrix A and with scaling function φ. Then for a function f ∈Vif and only if
̂f (ATω) = m f (ω)̂φ(ω)
almost everywhere.
Here, m f ∈L([,π]n) and
∥m f ∥
L([ ,π]n) =

∣det A∣∥f ∥
L(Rn).
For a proof, see, e.g., [, ]. Note that for a wavelet ψ as in > Eq. (.) there holds
mψ(ω) =

∣det A∣∑
k∈Zn
akei⟨ω,k⟩.
How many wavelets, i.e., generators of W, are needed to span the space? The parameter
s in Definition is yet unspecified. In fact, s depends on the scaling matrix. A leaves the
lattice Zn invariant, AZn ⊂Zn. The number of cosets is ∣det A∣= ∣Zn/AZn∣(see [, Propo-
sition .]). It turns out that q = ∣det A∣−wavelets are needed to generate the space W.
To motivate this, for a start, let f ∈Vbe an arbitrary function. Denote γ, . . . ,γq repre-
sentatives of the q + cosets of AZn in Zn. Then each coset can be written as γm + AZn,
m = , . . . , q. The function f has the representation

∣det A∣/f (A−x) = ∑
k∈Zn
ck(f )φ(x −k),
(.)
or in Fourier domain
̂f (ATω) =

∣det A∣/c f (ω)̂φ(ω),
(.)
in L-sense and with an appropriate πZn-periodic function c f (ω) with Fourier coeffi-
cients (ck(f ))k∈Zn. Then c f (ω) can be decomposed with respect to the cosets:
c f (ω) = ∑
k∈Zn
ck(f )ei⟨ω,k⟩=
q
∑
m=
∑
k∈γm+AZn
ck(f )ei⟨ω,k⟩
=
q
∑
m=
ei⟨ω,γm⟩∑
k∈AZn
ck+γm(f )ei⟨ω,k⟩=
q
∑
m=
cm
f (ω),
where
cm
f (ω) = ei⟨ω,γm⟩∑
k∈AZn
ck+γm(f )ei⟨ω,k⟩= ei⟨ω,γm⟩∑
k∈Zn
cAk+γm(f )ei⟨ω, Ak⟩
= ei⟨ω,γm⟩∑
k∈Zn
cAk+γm(f )ei⟨AT ω,k⟩= ei⟨ω,γm⟩κm
f (ATω).


Splines and Multiresolution Analysis
This representation exists for all functions V, in particular for φ and the wavelets. The
following theorem indicates, how many wavelets are needed to generate the space W, such
that W⊕V= V.
Theorem 
Let φ ∈Vbe a scaling function and let ψ, . . . ,ψq ∈V. Then the family
{φ(●−k)}k∈Zn is an orthonormal system if and only if
q
∑
m=
∣κm
φ (ω)∣
= 
almost everywhere.
(.)
The system {φ(●−k)}k∈Zn ∪⋃
q
m={ψm(●−k)}k∈Zn is an orthonormal basis in Vif and only
if the so-called polyphase matrix
⎛
⎜⎜
⎝
κ
φ(ω)
κ
ψ(ω)
⋯
κ
ψq(ω)
⋮
⋮
⋱
⋮
κq
φ(ω)
κq
ψ(ω)
⋯
κq
ψq(ω)
⎞
⎟⎟
⎠
is unitary for almost all ω ∈Rn.
The proof for a more general version of this theorem is given in [, Sect. .].
A summary and a condition for r-regular wavelets yields in the following theorem.
Theorem 
Consider a multiresolution analysis on Rn associated with a dilation
matrix A.
(i)
Then there exists an associated wavelet set consisting of q = ∣det A∣−functions.
(ii)
If the multiresolution analysis is r-regular, and in addition q + > n, then there exists
an associated wavelet set consisting of q r-regular functions.
The idea of the proof is that for an r-regular function φ on Rn and a πZn-periodic C∞-
function η(ω) the convolution ψ defined by ̂ψ(ω) = η(ω)̂φ(ω) is an r-regular function.
For an explicit proof, see again [].
Example 
As a continuation of Example , the wavelet function corresponding to
φ = χ[,) is derived. To this end, consider the space L(R) and the dilation A = . Then
q = det A −= ; thus γ= and γ= are representatives of the cosets of A. That is, there is
only a single wavelet needed to generate W. > Equation (.) yields

√

φ (x
) =

√

φ(x) +

√

φ(x −)
for the normalized generator of V. Thus c(φ) =

√
and c(φ) =

√
eiω. This implies
κ
φ ( ω
) = κ
φ ( ω
) =

√
. Then by
> Eq. (.) of Theorem the family {φ(●−k)}k∈Z is

Splines and Multiresolution Analysis 

orthogonal, since ∣κ
φ(ω)∣
+ ∣κ
φ(ω)∣
= . The polyphase matrix
(κ
φ(ω)
κ
ψ(ω)
κ
φ(ω)
κ
ψ(ω))
can be completed to a unitary matrix by choosing κ
ψ(ω) =

√
= −κ
ψ(ω). The corresponding
wavelet then has the representation

√

ψ (x
) =

√

φ(x) −

√

φ(x −),
corresponding to (> .). This yields the Haar wavelet ψ as illustrated in > Fig. -.
..
B-Splines
Several of the criteria for scaling functions and multiresolution analyses given in the
previous section are based on the Fourier representation of the scaling function, e.g.,
the Riesz sequence criterion and the orthonormalization trick in Theorem , as well as
the criterion for the order of approximation in Theorem . For this reason, the mod-
eling of a scaling function in Fourier domain to achieve certain specific properties is
promising.
Aiming at constructing a scaling function φ ∈L(R) of regularity r = −,,, . . ., this
property is considered in Fourier domain: It is a decay property of the Fourier transform
̂φ (compare with > Sect. ...):
̂φ(ω) = O (

∥ω∥r+)
for ∥ω∥→∞.
Taking into account Theorem a first model for the scaling function in Fourier domain is
̂φ(ω) =
(ω)
ωr+,
ω ∈R,
(.)
where the function
still has to be specified. Since scaling functions satisfy a scaling
relation (> .)
φ (x
) = ∑
k∈Z
hkφ(x −k)
in L(R),
the Fourier transform of this equation yields
̂φ(ω) = H(ω)̂φ(ω),
where (hk)k∈Z is the sequence of Fourier coefficients of the π-periodic function H. For
the ansatz (> .),
H(ω) = 
̂
φ(ω)
̂φ(ω) = 
(ω)
(ω)r+
ωr+
(ω) =

r+
(ω)
(ω) .
(.)


Splines and Multiresolution Analysis
This gives criteria for the choice of the function :
(i)
vanishes at the origin and there has a zero of order r+. This ensures that ̂φ ∈L(R)
and that Theorem (iii) is satisfied.
(ii)
(ω) is a trigonometric function, to ensure that H(ω), the so-called scaling filter,
is π-periodic.
(iii)
has no other zeros in [−π, π ], except at the origin. Otherwise, the autocorrelation
filter A(ω) = ∑k∈Z ∣̂φ(ω + πk)∣would vanish somewhere, and the shifts of the
function φ would fail to generate a Riesz sequence, see Theorem (i).
A simple function ensuring all three requirements (i), (ii), and (iii) is
(ω) = (sin(ω/)θ(ω/))r+,
where θ is a π-periodic phase factor such that ∣θ∣= , i.e., a shift in time domain. Choosing
θ(ω) = e−iω yields the cardinal B-splines as given in (> .) resp. (> .):
̂β(ω) = ∫

e−iωt dt = −e−iω
iω
= sin(ω/)
ω/
e−iω/.
Since βhas compact support, ̂β ∈C∞[]. Due to the convolution formula (> .),
̂βm(ω) = (−e−iω
iω
)
m+
= (sin(ω/)
ω/
e−iω/)
m+
.
(.)
The βm are scaling functions of regularity r = m −, as the verification of the criteria in
Theorem shows. In fact, the following holds. Let A = .
Integrability: Since by (> .) the functions ̂βm are L-integrable, so are the βm, m ∈N.
Riesz sequence property: The shifted characteristic functions β(x −k) = χ[,)(x −k),
k ∈Z, are clearly orthonormal. Theorem (ii) thus yields
∑
k∈Z
∣̂β (ω + πk)∣= 
almost everywhere.
To verify the Riesz sequence property for βm, the autocorrelation filter must be bounded
with strictly positive constants from above and from below. It is
∑
k∈Z
∣̂βm(ω + πk)∣= ∑
k∈Z
∣̂β (ω + πk)∣m+.
In [−π, π ], ∣̂β ∣is clearly positive (cf. > Fig. -), which gives ∣̂β (π)∣= /π as a positive
bound from below. There is a constant c, such that
< c = (/π)m+< ∣̂β (ω)∣m+≤∑
k∈Z
∣̂βm(ω + πk)∣m+.

Splines and Multiresolution Analysis 

–4p –3p –2p
3p
4p
2p
–p
p
0.2
0.4
0.6
0.8
1
⊡Fig. -
The function ∣
∧
β∣is strictly positive in the interval [−π, π ]
Since the sequence (∣̂β (ω + πk)∣)k∈Z ∈l (Z) for all ω ∈R, the same is true for the
sequence ∣̂βm(ω + πk)∣= ∣̂β (ω + πk)∣m+. This yields the existence of the requested
upper bound c< ∞. Thus {̂βm(●−k)}k∈Z forms a Riesz sequence in L(R).
Scaling relation: The scaling filter (> .)
H(ω) = −m (−e−iω)m+
(−e−iω)m+= −m(+ e−iω)m+= −m
m+
∑
k=
(m + 
k
)e−iωk
is obviously π-periodic and has Fourier coefficients (−m(m+
k ))k∈Z ∈l (Z). Hence, the
B-splines satisfy the scaling relation (> .)
βm(x/) =
m+
∑
k=
−m(m + 
k
)βm(x + k).
For βthis equation reads
β(x/) = β(x) + β(x + ),
which is true since β(x/) = χ[,)(x/) = χ[,)(x). This equation and examples for
scaling relations of other B-splines are illustrated in > Fig. -.
Continuity and positivity of ̂φ at the origin: From > Eq. (.),
∣̂
βm(ω)∣= ∣sin(ω/)
ω/
∣
m+
,
which has a continuous continuation at the origin, and ̂
βm() = . Thus we have proved
the following conclusion:
Theorem 
The cardinal B-spline βm, m ∈N, is a scaling function of an m −-regular
multiresolution analysis with dilation . The order of approximation is m + .


Splines and Multiresolution Analysis
2
4
6
8
0.1
0.2
0.3
0.4
0.5
0.6
2
4
6
0.1
0.2
0.3
0.4
0.5
0.6
0.7
–1
1
m = 0
m = 1
m = 2
m = 3
2
3
0.2
0.4
0.6
0.8
1
–1
1
2
3
4
5
0.2
0.4
0.6
0.8
1
⊡Fig. -
Scaling relation for B-splines βm, m = , . . . , . The B-spline versions βm(x/) ∈V−are
displayed with solid lines, the scaled translates in Vare depicted dashed. The sum of the
dashed functions gives the B-spline at the lower scale βm(x/)
Note that the cardinal B-splines βm with Fourier transform of the form (> .) are scaling
functions, but they are not yet orthonormalized: The family {βm(●−k)}k∈Z spans Vand
is a Riesz basis, but it is not an orthonormal basis of V. Orthonormality can be achieved
with Theorem and > Eq. (.):
̂Bm(ω) :=
̂βm(ω)
√
∑k∈Z ∣̂βm(ω + πk)∣
.
> Figure -shows some orthonormalized B-spline scaling functions and the corre-
sponding wavelets.
..
Polyharmonic B-Splines
The same approach to model scaling functions in Fourier domain can be done in higher
dimensions. We aim at constructing a scaling function for a multiresolution analysis of

Splines and Multiresolution Analysis 

m = 1
m = 2
m = 3
–4
–2
2
4
–0.2
0.2
0.4
0.6
0.8
1
–4
–2
2
4
–1
–0.5
0.5
1
–4
–2
2
4
–0.2
0.2
0.4
0.6
0.8
1
1.2
–4
–2
2
4
–1.5
–1
–0.5
0.5
–4
–2
2
4
–1
–0.5
0.5
–4
–2
2
4
–0.2
0.2
0.4
0.6
0.8
1
⊡Fig. -
Orthonormalized B-splines Bm (left column) and corresponding wavelets (right column) for
m = , , in time domain. Note that the orthonormalized B-splines and wavelets do not
have compact support. Due to the orthonormalization procedure (> .) the
orthonormalized B-spline is an inﬁnite series of shifted compactly supported B-splines
L(Rn) of the form
̂φ(ω) =
(ω)
∥ω∥r ,
r ∈N,
r > n/,
x ∈Rn.
With an appropriate trigonometric polynomial
(ω) = (
n
∑
k=
sin(ωk/))
r
,
ω = (ω, . . . , ωn),


Splines and Multiresolution Analysis
̂φ is a nonseparable scaling function for a multiresolution analysis of L(Rn) with respect to
dilation matrices A that are scaled rotations. The corresponding function in space domain
φ is called elementary r-harmonic cardinal B-spline, or short polyharmonic B-spline Pr.
This terminology can be justified as follows. The Fourier transform in the sense of tempered
distributions of the function /∥ω∥r is indeed a polynomial – up to a logarithmic factor
for r −n even. In fact, in S′(Rn),
F−(/∥●∥r)(x) = ∥x∥r−n(A(n, r)ln ∥x∥+ B(n, r)) =: ρ(x),
with constants A(n, r), B(n, r) as given in [, Chap. , Sect. ], and A(n, r) = except
for r −n even. (Note that for r > n/on the right-hand side the final parts have to be
considered.) The term polyharmonic comes from the fact that ρ is the Green function of
the r-iterated Laplace operator Δr. However, with these considerations
φ(x) = Pr(x) = ∑
k∈Z
kρ(x + k)
almost everywhere
becomes an nD spline. Here ( k)k∈Z is the sequence of Fourier coefficients of . Due to the
decay in Fourier domain, the polyharmonic B-spline Pr has continuous derivatives Dβ for
multi-indices ∣β∣< r −n. In the same way as for the B-splines, it can be shown with the
theorems given in > Sect. ..that φ forms indeed a scaling function with approximation
order r [, , ].
> Figure -shows the polyharmonic B-spline scaling function in
space domain and in frequency domain.
2
1.5
1
0.5
0
–2
–2
0
2
0
2
1
0.8
0.6
0.4
0.2
2
1
0
–1
–2
–2
0
2
⊡Fig. -
Polyharmonic B-spline for r = in space domain (left) and frequency domain (right)

Splines and Multiresolution Analysis 

.
Survey on Mathematical Analysis of Methods
There are many function families that consist of piecewise polynomials and that are called
splines, which in addition fulfil a multiresolution condition in the one or the other sense.
These families can be classified by various aspects, e.g., by their dimensionality, by the lat-
tice which is invariant under the corresponding dilation matrix, by the geometries they are
defined on, or whether they provide phase information or not, and so on. The following
sections list some of these spline approaches and illustrates their mathematical properties
and features.
..
Schoenberg’s B-Splines for Image Analysis – the
Tensor Product Approach
As mentioned in Example , multiresolution analyses for L(Rn) and dilation matrix Id
can be generated from tensor products of D dyadic multiresolution analyses. To analyze
images with B-splines, the tensor product βm(x)βm(y) of B-splines is a scaling function
for L(R) and the dilation matrix A = Id. Since in D the determinant det A = , the
corresponding details space Wis spanned by three wavelet functions:
ψ(x)βm(y),
βm(x)ψ(y),
ψ(x)ψ(y),
x, y ∈R.
(.)
A drawback of this approach is the fact that these wavelets prefer horizontal, vertical, and
diagonal directional features and are not sensitive to other directions, see
> Fig. -.
For the analysis of images with many isotropic features, the use of isotropic or steerable
wavelets is recommended. However, the tensor approach is a simple and widely used
wavelet approach. For an illustration of the respective image decomposition in coarse
approximations and details of various sizes, see > Fig. -.
⊡Fig. -
The three B-spline tensor wavelets (> .) show a preference for horizontal, vertical, and
diagonal directions. Here, m = . Minimal resp. maximal function values are given in black
resp. white


Splines and Multiresolution Analysis
b
a
c
d
⊡Fig. -
Decomposition of an image [, Part of IM.tif] into coarse approximations and details of
various sizes. (a) Original image. (b) Matrix of the absolute values of the multiresolution
coeﬃcients. Large coeﬃcients are white. The wavelet coeﬃcients are depicted in the lower
two and the upper right band of each scale; the approximation coeﬃcients in the upper left
band. (c) Two steps of themultiresolution decomposition. From left to right: Finest details
and second ﬁnest details, remaining approximation of the original image. (d) Second ﬁnest
details (c, center) split into the contribution of the three wavelets. From left to right: The
decomposition into horizontal, vertical and diagonal details
..
Fractional and Complex B-Splines
The B-splines as described up to now have a discrete order of smoothness, i.e., they are
Cn-functions with n ∈{−,,,, . . .}. For some applications, e.g., in medical imaging,
where the order of smoothness of certain image classes is fractional, it would be favorable

Splines and Multiresolution Analysis 

to have a spline and wavelet basis that is adaptable with respect to this regularity [, , ].
A first step in this direction was done by T. Blu and M. Unser, who proposed B-splines
and wavelets of fractional orders []. They defined two variants of fractional B-splines, the
causal ones and the symmetrical ones.
The causal fractional B-spline is generated by applying the (α+)th fractional difference
operator to the one-sided power function tα
+:
βα
+(t) :=

Γ(α + )Δα+
+ tα
+ =

Γ(α + ) ∑
k≥
(−)k(α + 
k )(t −k)α
+.
The Fourier-transform representation is similar to the one of the classical B-splines (cf.
> Eq. .):
̂βα
+(ω) = (−e−iω
iω
)
α+
.
Here again, the smoothness property βα
+ ∈Cm,γ(R) in time domain is gained by the frac-
tional polynomial decay of order O(∣ω∣∣α+) in frequency domain. Note that Cm,γ(R)
denotes the Hölder space with exponent m = ⌊α + ⌋and γ = α + −m, i.e., the space
of m-times continuously differentiable functions f that are Hölder-regular with exponent
< γ ≤such that there is a constant C > with
∣Dm f (t) −Dm f (s)∣≤C∣t −s∣
∀s, t ∈R.
Although the fractional B-splines are not compactly supported, they decay in the order
O(∣t∣−(α+)) as t →∞. They are elements of L(R) for α > , of L(R) for α > −
and of
the Sobolev spaces Wr
(R) for r < α + 
. They share many properties with their classical
B-spline relatives, such as the convolution property, their relation to difference operators,
i.e., they are integral kernels for fractional difference operators [], and they are scaling
functions for dyadic multiresolution analyses. This can be verified by the procedure given
in > Sect. ...
The causal fractional B-spline is not symmetric. Since for some signal and image anal-
ysis tasks symmetrical bases are preferred, in [] the symmetrical fractional B-splines βα
∗
are proposed. They are defined in Fourier domain as follows:
̂βα
∗(ω) = (−e−iω
iω
)
α+

(+ eiω
−iω )
α+

= ∣sin(ω/)
ω/
∣
α+
,
(.)
and therefore obviously are symmetrical in time domain. The same regularity and decay
properties apply as for the causal fractional B-splines. The symmetrical fractional B-splines
are also piecewise polynomials, as long as α ∉N. For even integer degrees, the singular-
ity introduced through the absolute value in
> Eq. (.) causes that βm
∗
is a sum of
integer shifts of the logarithmic term ∣t∣m ln(t) for m ∈N. For the explicit time-domain
representation and further details on these splines cf. [].
In [], Blu and Unser defined another variant, the generalized fractional B-spline or
(α, τ)-fractional spline βα
τ with a parameter τ ∈R. Also these splines are defined via their


Splines and Multiresolution Analysis
Fourier domain representation:
̂βα
τ (ω) = (−e−iω
iω
)
α+
+τ
(−eiω
−iω )
α+
−τ
.
As above, the parameter α > controls the regularity of the splines. The parameter τ, in
contrast, controls the position of the splines with respect to the grid Z. This can be justified
by the following fact. All variants of the B-splines considered in this section converge to
optimally time-frequency localized functions in the sense of Heisenberg, i.e., to Gaussians
or Gabor functions, if the degree α becomes large. For a proof for the classical cardinal
B-splines, see []. In the case of the (α, τ)-fractional splines [],
βα
τ (t) = O (e−

α+(t−τ))
for α →∞.
This explains the notion “shift parameter” for τ. Moreover, the parameter τ allows to
interpolate the spline family between the two “knots,” the symmetrical ones (τ =) and
the causal ones (τ = α+
), see > Fig. -. Both parameters α and τ can be tuned
independently and therefore allow for an individual adjustment of the analyzing basis.
Another generalization are the complex B-splines []. There are two variants, both
defined via their Fourier domain representation. Let z = α + iγ ∈C, α > −
, γ ∈R and
y ∈C. Then
̂βz(ω) = (−e−iω
iω
)
z+
,
̂βz
y(ω) = (−e−iω
iω
)
z+
−y
(−eiω
−iω )
z+
+y
are complex B-splines of complex degree z. The functions are well defined, because the
function Ω(ω) = −e−iω
iω
never touches the negative real axis such that Ω(ω)z is uniquely
0
1
0.5
–0.5
–2
–1
0
1
2
3
4
0
1
0.5
–0.5
–2
–1
0
1
2
3
4
⊡Fig. -
The fractional (α, τ)-splines interpolate the families of the causal and the symmetric
fractional splines. τ = α+
k for k = , 
, 
, 
, from the most right (causal) to the most left
(symmetrical) function in each image. Right: α = .. Left: α = 

Splines and Multiresolution Analysis 

defined. βz and βz
y are elements of the Sobolev spaces Wr
(R) for r < α + 
. βz has the
time-domain representation
βz(t) =

Γ(z + ) ∑
k≥
(−)k(z + 
k )(t −k)z
+,
i.e., βz is a piecewise polynomial of complex degree. For more details on the properties of
these families of complex splines cf. [].
The idea behind the complex degree is as follows: The real part Re z = α operates as reg-
ularity and decay parameter in the same way as for the fractional B-splines. The imaginary
part, however, causes an enhancement resp. damping of positive or negative frequencies.
In fact,
̂βz(ω) = ̂βα
+(ω)e−iγ ln ∣Ω(ω)∣eγ arg Ω(ω).
The imaginary part γ of the complex degree introduces a phase and a scaling factor in
frequency domain. The frequency components on the negative and positive real axis are
enhanced with different sign, because arg Ω(ω) ≥for negative ω and arg Ω(ω) ≤for
positive ω. > Figure -illustrates this effect.
With real-valued functions, only symmetric spectra can be analyzed. The complex
B-splines, however, allow for an approximate analysis of the positive or the negative
frequencies, because the respective symmetric bands are damped due to the complex
exponent. However, the complex B-splines inherit many properties of their classical and
fractional relatives.
All of the generalized B-spline families mentioned in this section have in common that
they are scaling functions of dyadic multiresolution analyses. They are one-dimensional
functions, but with the tensor approach mentioned in Exampleand > Sect. ..they are
also suitable for image processing tasks. Although the fractional and the complex splines, in
general, to not have compact support, they allow for fast analysis and synthesis algorithms.
Due to their closed form in Fourier domain, they invite for an implementation of these
algorithms in Fourier domain.
–15
–10
–5
5
10
15
0.2
0.4
0.6
0.8
1
–15
–10
–5
5
10
15
0.25
0.5
0.75
1
1.25
1.5
–15
–10
–5
5
10
15
1
2
3
4
5
6
⊡Fig. -
The frequency spectrum ∣
∧
βz∣for z = + iγ, γ = , , (from left to right). The spectrum of
β= β
+ is symmetric (right), whereas the spectra of β+i (center) and β+i(left) show an
enhancement of the positive frequency axis


Splines and Multiresolution Analysis
..
Polyharmonic B-Splines and Variants
In
> Sect. ..the so-called polyharmonic B-splines in Rn were introduced. They are
defined in Fourier domain by the representation
̂Pr(ω) = (∑n
k=sin(ωk/)
∑n
k=ω
k
)
r
,
r > n/, ω = (ω, . . . , ωn).
These polyharmonic B-splines satisfy many properties of the classical Schoenberg splines;
e.g. they are piecewise polynomial functions, they satisfy a convolution relation Pr+r=
Pr∗Pr, they are positive functions, etc. However, they do not share the property that they
converge to optimally space-frequency localized Gaussians as r increases []. This is due
to the fact that the trigonometric polynomial in the numerator regularizes insufficiently at
the origin: The second order derivative of
∑n
k=sin(ωk/)
∑n
k=ω
k
is not continuous. Van De Ville et al. [] therefore proposed another localizing trigono-
metric polynomial:
μ(ω) = 
n
∑
k=
sin(ωk/) −

n−
∑
k=
n
∑
l=k+
sin(ωk/)sin(ωl/).
(.)
A new function family then is defined in Fourier domain via
̂
Qr(ω) = ( μ(ω)
∥ω∥)
r
,
r > n
.
(.)
Qr is called isotropic polyharmonic B-spline. The function is piecewise polynomial (except
for r −n even, where a logarithmic factor has to be added, see below), and shares with
the polyharmonic splines their decay properties in Fourier domain and their regularity
properties in space domain. Qr converges to a Gaussian as r increases, which makes the
function family better suitable for image analysis than Pr, because of the better space-
frequency localization. This effect is due to higher order rotation invariance or isotropy of
the localizing trigonometric polynomial (> .):
(ω) = + O(∥ω∥)
vs.
μ(ω) = + 
∥ω∥+ O(∥ω∥)
as ∥ω∥→.
This causes that ̂
Qr has a second order moment, and thus the central limit theorem can be
applied to proof the convergence to the Gaussian function. In addition, ̂
Qr has a higher
regularity than ̂Pr; therefore Qr decays faster. For a complete proof of the localization
property, see []. An example of the isotropic polyharmonic spline is given in > Fig. -.
The polyharmonic B-splines and the isotropic polyharmonic B-splines both are real-
valued functions. The isotropic B-spline is approximately rotation-invariant and there-
fore is suited for image analysis of isotropic features. A complex-valued variant of these
B-splines in D was introduced in []. The idea is to design a spline scaling function that

Splines and Multiresolution Analysis 

2
1.5
1
0.5
0
–2
–2
0
2
0
2
⊡Fig. -
The isotropic polyharmonic spline Q. Compare with > Fig. -of the classical
polyharmonic B-spline P
is approximately rotation covariant, instead of rotation invariant. Rotation covariant here
means that the function intertwines with rotations up to a phase factor.
Again, the design of the scaling function is done in Fourier domain, now using a
perfectly rotation-covariant function
̂ρr,N(ω, ω) =

(ω
+ ω
)r (ω−iω)N ,
where r ≥and N ∈N. In fact, for some rotation matrix Rθ ∈GL(,R), ̂ρr,N(Rθω) =
e−iNθ ̂ρr,N(ω). For localizing the function ̂ρr,N, the same trigonometric polynomials
and
μ as above can be used. The corresponding complex polyharmonic B-splines are then
defined in frequency domain as
̂
Rr,N(ω, ω) =
( (ω, ω))r+ N

(ω
+ ω
)r (ω−iω)N ,
or as
̂
Rr,N
μ (ω, ω) =
(μ(ω, ω))r+ N

(ω
+ ω
)
r (ω−iω)N .
(.)
The case N = yields the real-valued polyharmonic splines.
There are also other trigonometric polynomials that are suitable as localizing numera-
tor for the real and the complex polyharmonic B-splines. With an appropriate choice the
features of the polyharmonic splines can be tuned []. However, for both the real and
the complex variant, the localizing multiplier has to fulfil moderate conditions to make
the respective polyharmonic B-splines being a scaling function. In D, the following result
holds (cf. []):


Splines and Multiresolution Analysis
Theorem 
Let r > and N ∈N. Let η(ω, ω) be a bounded, πZ-periodic function,
such that
∣
(η(ω, ω))r+ N

(ω
+ ω
)r (ω−iω)N ∣
is bounded in a neighborhood of the origin, and such that η(ω, ω) ≠for all (ω, ω) ∈
[−π, π]/{(,)}.
Then ̂φ = ηr+ N
⋅̂ρ is the Fourier transform of a scaling function φ which generates a
multiresolution analysis . . . V−⊂V⊂V. . . of L(R) with dilation matrix A = ( a
b
−b
a),
a, b ∈Z:
Vj = span {∣det A∣j/φ(Aj ●−k), k ∈Z}.
From the Fourier domain representation immediately follows that ̂φ ∈L(R) for r+ N
> 

and that ̂φ decays as ∣̂φ(ω, ω)∣= O(∥(ω, ω)∥−r−N) when ∥(ω, ω)∥→∞.
As a result, for all three variants of the polyharmonic B-splines, the classical ones Pr,
the isotropic ones Qr, and the complex ones Rr,N, the following properties hold: They are
scaling functions for multiresolution analysis. Their smoothness parameter r can be chosen
fractional and must fulfill r + N
> n/for integrability reasons. Then the scaling function
in space domain is element of the Sobolov space φ ∈Ws
(R) for all s < r + N −. The
explicit space domain representation is
φ(x) = ∑
k∈Z
ηkρr,N(x + k)
for almost all x ∈R. Here (ηk)k∈Zdenotes the Fourier coefficients of ηr+ N
. ρr,N is
the inverse Fourier transform of the Hadamard final part P f (̂ρr,N) ∈S′(R). In fact, for
r ∉N,
ρr,N(x, x) = c(x
+ x
)
r−(x+ ix)N
and for r ∈N,
ρr,N(x, x) = c(x
+ x
)
r−(x+ ix)N (ln π
√
x
+ x
) + c)
with appropriate constants c, c, c∈C. This justifies the notion spline for the function
families. They all have a closed form in frequency domain. As in the case of the D cardi-
nal B-splines there is a fast analysis and synthesis algorithm using the frequency domain
representation and the fast Fourier transform, cf. > Sect. ..
..
Splines on Other Lattices
...
Splines on the Quincunx Lattice
The tensor product of two D dyadic multiresolution analyses yields a D multiresolution
analysis with dilation matrix A = Id, cf. Example . As a consequence, the scaling factor
while moving from one approximation space Vto the next coarser space Vis ∣det A∣= .

Splines and Multiresolution Analysis 

1
2
3
4
5
6
1
2
3
4
5
6
c
1
2
3
4
5
6
1
2
3
4
5
6
b
1
2
3
4
5
6
1
2
3
4
5
6
a
⊡Fig. -
Three iterations of the quincunx subsampling scheme. (a) Z, (b) AqZ, (c) A
qZ. The thinning
of the Z-lattice using the dilation matrix Aq is ﬁner than dilation with the dyadic matrix
A = Id, which in one step leads from (a) to (c)
For some image processing applications, especially in medical imaging, this scaling step
size is too large. A step size of as in the D case would be preferred. Moreover, the decom-
position of the wavelet space into three subspaces then would be avoided, and the eventual
problematic of the directionality of the three involved wavelets would not arise. An example
of a dilation matrix satisfying these requirement is the scaled rotation matrix
Aq = ( 

−
)
with det Aq = . It leads to the so-called quincunx lattice. This lattice is generated by
applying Aq to the cartesian lattice. It holds AqZ⊂Z, see > Fig. -.
Since Aq falls into the class of scaled rotations, the polyharmonic B-Spline construc-
tion including all variants are applicable for this case, cf. Theorem . Note that the tensor
product approach in general is not suitable for the quincunx subsampling scheme.
...
Splines on the Hexagonal Lattice
Images as D objects are normally processed on the cartesian lattice, i.e., the image pixels
are arranged on a rectangular grid. For image processing this arrangement has the draw-
back that not all neighbors of a pixel have the same relation: The centers of the diagonal
neighbors have a larger distance to the center pixel than the adjacent ones. A higher degree
of symmetry has the hexagonal lattice. It is therefore ideal for isotropic feature represen-
tation. The hexagonal lattice gives an optimal tesselation of Rin the sense of the classical
honeycomb conjecture, which says that any partition of the plane into regions of equal
area has a perimeter at least that of regular hexagonal tiling []. The better isotropy of the
hexagonal lattice is attractive for image analysis and has led to a series of articles on image
processing methods (e.g., [, , ]) as well as on applications (e.g., [, , , ]).


Splines and Multiresolution Analysis
The hexagonal lattice is generated by applying the matrix
Rh =
√

√

(
/

√
/)
on the cartesian lattice Λh = RhZ. A scaling function of a multiresolution analysis of
L(R) in the hexagonal lattice fulfils all properties of Definition , but the last two. They
change to
(v)
f ∈V
⇐⇒
f (●−Rhk) ∈Vfor all k ∈Z.
(vi)
There exists a scaling function φ ∈V, such that the family {φ(●−Rhk)}k∈Zof
translates of φ forms a Riesz basis of V.
Let A be a dilation matrix which leaves the hexagonal lattice invariant AΛh ⊂Λh. Then A
is of the form []
A = RhBR−
h
with B ∈GL(,R) having only integer entries and with eigenvalues strictly larger than
one.
> Fig. -gives an example of two subsampling steps on the hexagonal
lattice.
There are several possible approaches to define spline functions on the hexagonal
lattice. Sablonnière and Sbibih [] proposed to convolve piecewise linear pyramids to
generate higher order B-splines. Van De Ville et al. [] started with the characteristic
function of one hexagon and also used an iterative convolution procedure to construct
B-splines of higher degree. However, both approaches lead to discrete order hexagonal B-
splines. If A is a scaled rotation, then fractional and complex B-splines on the hexagonal
lattice can be defined in an analog way as in
> Sect. ..for polyharmonic splines and
1
2
3
4
5
6
1
2
3
4
5
6
1
2
3
4
5
6
1
2
3
4
5
6
1
2
3
4
5
6
1
2
3
4
5
6
c
b
a
⊡Fig. -
Three iterations of subsampling of the hexagonal grid with A = RhBR−
h , where B = ( 
−). (a)
Λh, (b) AΛh = RhBR−
h Λh = RhBZ, (c) AΛh

Splines and Multiresolution Analysis 

their (complex) variants []. Consider again the perfectly rotation-covariant (or for N = 
rotation-invariant) function
̂ρr,N(ω, ω) =

(ω
+ ω
)r (ω−iω)N ,
where r > and N ∈N. The idea is now to use a hexagonal-periodic trigonometric poly-
nomial for localizing this function and to eliminate the singularity at the origin. Condat
et al. [] proposed
ηh(ω, ω) =

√

(−(cos (/(−ω/
√
+ ω)/
√
)
+cos (/(ω/
√
+ ω)/
√
) + cos (−/√
ω))),
and defined the elementary polyharmonic hexagonal rotation-covariant B-spline via its
frequency domain representation as
̂
Rr,N
h (ω, ω) =
(ηh(ω, ω))r+ N

(ω
+ ω
)r (ω−iω)N .
The B-spline in space domain then has the representation
Rr,N
h (x) = ∑
k∈Z
ηh,kρr,N(x −Rhk).
Here, (ηh,k)k∈Zdenotes the sequence of Fourier coefficients of η
r+ N

h
.
> Figure -
shows the localizing trigonometric polynomial η and the Fourier spectra of two hexagonal
splines.
For N = , the functions are the elementary polyharmonic hexagonal rotation-invariant
B-splines. They are real-valued functions that converge to a Gaussian, as r →∞, and
therefore are well localized in the space domain as well as in the frequency domain. For
N ∈N, the splines are complex-valued functions and approximately rotation covariant in
a neighborhood of the origin:
̂
Rr,N
h (ω) = eiN arg(ω) (+ C∥ω∥+ O(∥ω∥))
for ω →,
where ω = (ω, ω) and C ∈R a constant.
The translates of the complex B-spline Rr,N
h
form a Riesz basis of the approximation
spaces
Vj = span {Rr,N
h (Ajx −Rhk) : k ∈Z}
L(R)
,
j ∈Z.
The ladder of spaces (Vj)j∈Z generates a multiresolution analysis of L(R) for the
hexagonal grid and for scaled rotations A. Also in this case, the implementation of
the analysis and synthesis algorithm can be elegantly performed in frequency domain
[, ].


Splines and Multiresolution Analysis
–10
0
5
a
b
c
10 –10
–5
0
5
10
0
2
4
–5
–10
–5
0
5
10 –10
–5
0
5
10
0
0.25
0.5
0.75
1
–10
–5
0
5
10 –10
–5
0
5
10
0
0.25
0.5
0.75
1
⊡Fig. -
Localization of ∧ρ with (a) the hexagonal-periodic trigonometric polynomial ηh yields the
elementary polyharmonic hexagonal rotation-covariant B-spline. Frequency spectrum of
∧
R
r,N
h (or equivalently of ∧
R
r+N/,
h
), (b) for r + N
= , and (c) for r + N
= .
.
Numerical Methods
For the illustration of the numerical method, we focus on the quincunx dilation matrix
A = ( 

−
),
(.)
and consider the polyharmonic spline variants in D as defined in
> Sect. ... Since
det A = , the generators of the multiresolution space and the corresponding wavelet space
are two functions: The scaling function (here the variant of the polyharmonic spline), and
the associated wavelet.
We consider the scaling function φ(x) = Qr
μ(x) resp. φ(x) = Rr,N
μ (x). It spans the
ladder of nested approximation spaces {Vj}j∈Z via
Vj = span {∣det A∣j/φ(Aj ●−k), k ∈Z}
L(R)
,
j ∈Z.

Splines and Multiresolution Analysis 

Denote
M(ω) := ∑
k∈Z
∣̂φ(ω + πk)∣
(.)
the autocorrelation filter. It is bounded < M(ω) ≤C for some positive constant C [].
The scaling functions can be orthonormalized applying the procedure given in Theorem 
(iii):
̂Φ(ω) =
̂φ(ω)
√
M(ω)
.
The scaled shifts of Φ span the same spaces {Vj}j∈Z.
The B-splines as scaling functions φ satisfy a refinement relation
φ(A−x) = ∑
k∈Z
hkφ(x −k)
almost everywhere and in L(R).
This relation in fact is a discrete convolution. The Fourier transform yields a πZ-periodic
function H ∈L(T) of the form
H(eiω) = ∣det A∣⋅̂φ(ATω)
̂φ(ω)
= ∣det A∣⋅̂ρr,N(ATω)ζ(ATω)
̂ρr,N(ω)ζ(ω)
= ζ(ATω)
ζ(ω)
⋅

(a+ b)r−(a −ib)N ,
ω ∈R.
Here, ζ(ω) = (μ(ω))r+ N
is the localizing multiplier for the isotropic polyharmonic B-
spline in the case N = , and for the rotation-covariant polyharmonic B-splines in the case
N ∈N, cf. (> .), (> .), and (> .).
The wavelet function ψ spanning a Riesz basis for the orthogonal complement
Wj = span {j/ψ(Aj ●−k), k ∈Z}
L(R)
in Vj+= Wj⊕Vj can also be gained in frequency domain. For the quincunx dilation matrix
A as in (> .) a wavelet (or sometimes called a prewavelet, since the functions are not
yet orthonormalized) is given by
̂ψ(ω) = G(eiω)̂φ(ω) = e−iωH(ω + (π, π)T)M(ω + (π, π)T)̂φ(ω),
compare with
> Sect. .... The (pre-)wavelet Riesz basis for Wj is then given by the
family
{ψj,k = j/ψ(Aj ●−k), k ∈Z}.
This basis in general is not orthonormal: ⟨ψj,k,ψ j,l⟩≠δk,l. A function f ∈L(R) can then
be represented by the series
f =
∑
j∈Z,k∈Z
⟨f , ̃ψj,k⟩ψ j,k =
∑
j∈Z,k∈Z
⟨f , ̃ψj,k⟩ψ j,k,
where {̃ψj,k}k∈Zdenotes the dual basis for each j ∈Z: ⟨̃ψj,k,ψ j,l⟩= δk,l. Its generator in
frequency domain is


Splines and Multiresolution Analysis
̂̃ψ(ω) = e−iωH(ω + (π, π)T)M(ω + (π, π)T)
M(ATω)
̂φ(ω)
M(ω).
In contrast, the formula
̂Ψ(ω) =
M
N
N
T M(ω + (π, π)T)
M(ATω)
̂ψ(ω)
generates an orthonormal wavelet basis. It corresponds to the orthonormal basis of V
generated by the integer shifts of the orthonormalized scaling function Φ. These con-
siderations show that there are three variants of a multiresolution implementation: An
“orthonormal” one with respect to the orthonormalized scaling functions and correspond-
ing orthonormal wavelets, one with the B-splines on the analysis side,
f = ∑
k∈Z
⟨f , φ j,k⟩̃φ j,k + ∑
k∈Z
⟨f ,ψj,k⟩̃ψj,k
for f ∈Vj+,
and finally one with the B-splines on the synthesis side:
f = ∑
k∈Z
⟨f , ̃φ j,k⟩φ j,k + ∑
k∈Z
⟨f , ̃ψj,k⟩ψ j,k
for f ∈Vj+.
Both, the scaling filters H(eiω) and the wavelet filters
G(eiω) = e−iωH(ω + (π, π)T)M(ω + (π, π)T)
as well as their orthogonal and dual variants in our case are nonseparable and infinitely sup-
ported. Therefore, a spatial implementation of the decomposition would lead to truncation
errors due to the necessary restriction to a finite number of samples. However, because of
the closed form of H and therefore of G, the corresponding multiresolution decomposition
or wavelet transform can be efficiently implemented in frequency domain. The respective
image first undergoes an FFT, then is filtered in frequency domain by multiplication with
the scaling filter H and the wavelet filter G. This method automatically imposes periodic
boundary conditions.
The coefficients resulting from the high pass filtering with G are the detail coeffi-
cients. They are stored, whereas the coefficients resulting from the low pass filtering H
are reconsidered for the next iteration step.
∑
k∈Z
⟨f , φ j+,k⟩̃φ j+,k = ∑
k∈Z
⟨f , φ j,k⟩̃φ j,k + ∑
k∈Z
⟨f ,ψj,k⟩̃ψj,k.
For details and tricks of the frequency domain implementation, cf. [, , , ].
> Figure -shows the multiresolution decomposition for the scaling function
φ = R,
μ . There it was assumed that the image is bandlimited and projected on the space
V, which has the advantage that the coefficients do not depend on the chosen flavour of
the scaling function, i.e., orthogonal, B-spline or dual. Qualitatively the transform is very
similar to a multiscale gradient with the real part corresponding to the x-derivative and
the imaginary part corresponding to the x-derivative [].

Splines and Multiresolution Analysis 

b
a
d
c
⊡Fig. -
Decomposition of an image [, Part of IM.tif] into approximation and wavelet
coeﬃcients. (a) Original image. (b) Matrix of the absolute values of the multiresolution
coeﬃcients. Large coeﬃcients are white. The approximation coeﬃcients in the upper left
band, the other bands are wavelet coeﬃcients on six scales. (c) Real part of the coeﬃcient
matrix, (d) imaginary part of the decomposition matrix for φ = R,
µ and the corresponding
wavelets. The coeﬃcients had their intensity rescaled for better contrast
.
Open Questions
In this chapter a method for the construction of spline multiresolution bases was described.
It yields a nice variety of new bases with several parameters for adaption and tuning. In the
last decade, the notion of compressive sampling or compressed sensing arose, which is
footing on the existence of well-adaptable bases. In fact, the idea behind compressed sens-
ing is that certain functions have a sparse representation, if the underlying basis is smartly
chosen. In this case, the function can be reconstructed from very few samples because of


Splines and Multiresolution Analysis
the prior knowledge of sparsity in this underlying basis. As a consequence, the knowledge
on sparsity allows to sample such a signal at a rate significantly under the Nyquist rate. (The
Shannon–Nyquist sampling theorem says that a signal must be sampled at least two times
faster than the signal’s bandwidth to avoid loss of information.)
In the last years, and virtually explosively in the last years, many important theo-
retical results were proven in this field, in particular by D. Donoho, E. Candès, J. Romberg,
and T. Tao. For an introduction and references on compressed sensing, see e.g., [, ] and
the website [].
Compressed sensing is based upon two fundamental concepts: that of incoherence and
that of sparsity. Let {xi}i=, ... ,N be an orthonormal basis of the vector space V. Let f =
∑N
i=sixi with si = ⟨f , xi⟩. The signal f is called k-sparse, if only k of the coefficients are
nonzero, k ∈N.
A general linear measurement process for signals consists in computing M < N inner
products y j = ⟨f , y j⟩for a collection of vectors {y j}j. In matrix form,
g = Y f = YXs,
where Y and X are the matrixes with {yi}i and {x j}j as columns, and YX is an M ×
N-matrix. If the function families Y and X are incoherent, i.e., if the incoherence measure
μ(Y, X) =
√
N max
≤i,j≤N ∣⟨yi, xj⟩∣∈[,
√
N]
is close to one, then under mild additional conditions the k-sparse signal f can be
reconstructed from M > Cμ(Y, X)k ln N samples with overwhelming probability.
Wavelet bases have proven to be very suitable for compressed sensing. It is an open
question to classify the signals from certain applications, and to estimate in which appro-
priate B-spline basis they have a k-sparse representation. Then adequate bases and function
families incoherent with the spline bases have to be identified.
In the last years, the concept of sparsity entered image processing. It has proven to
help immensely to accelerate the solution of inverse problems and reconstruction algo-
rithms, e.g., in medical imaging, such as in magnetic resonance imaging [], computed
tomography [], photo-acoustic tomography [], tomosynthesis [], and others. In
this area, as well as in other fields of imaging, it can be expected that the combina-
tion of splines – due to their easy modeling and the fast frequency domain algorithms –
multiresolution and wavelets, and sparsity will lead to novel impressing fast algorithms for
image reconstruction.
.
Conclusion
In the design procedure for scaling functions of multiresolution analyses, regularity and
decay features, as well as symmetry properties can be tuned by an appropriate modeling
in frequency domain. The idea is to start in frequency domain with a polynomial function
P that fulfils the required symmetry features, and that has a degree, such that /P decays

Splines and Multiresolution Analysis 

sufficiently fast. This assures that the resulting scaling function has the desired regularity.
However, /P in general is not an L-function and has to be multiplied with a localiz-
ing trigonometric polynomial
that eliminates the zeros in the denominator such that P
becomes square integrable. The choice of this trigonometric polynomial has to be taken
carefully to be compatible with the required features modeled in /P. Then under mild
additional conditions the fraction
̂φ = P
is the scaling function of a multiresolution analysis. This construction can be performed for
D and higher dimensional spaces likewise. In time domain, the resulting scaling function
is a piecewise polynomial, thus a spline. This design procedure for scaling functions unites
the concepts of splines and of multiresolution.
Interestingly, the polynomial in the denominator can be of a fractional or a complex
degree and therefore allows a fine tuning of the scaling function’s properties. However, the
scaling function then becomes an infinite series of shifted (truncated) polynomials. The
numerical calculation with the approximating basis of the multiresolution analysis in time
domain would cause truncation errors, which is unfavorable. But due to the construction
of φ in frequency domain, and due to the closed form there, the implementation in fre-
quency domain with periodic boundary conditions yields a fast and stable multiresolution
algorithm suitable for image analysis tasks.
.
Cross-References
> Astronomy
> Compressive Sensing
> Gabor Analysis for Imaging
> Neighborhood Filters and Local D Recovery
> Sampling Methods
References and Further Reading
. Aldroubi A, Unser MA (eds) () Wavelets in
medicine and biology. CRC Press, Boca Raton
. Baraniuk R () Compressive sensing. IEEE
Signal Process Mag ():–, 
. Bartels RH, Bealty JC, Beatty JC () An intro-
duction to splines for use in computer graphics
and geometric modeling. Morgan Kaufman, Los
Altos
. Battle G () A block spin construction of
ondelettes. Part : Lemarié functions. Commun
Math Phys :–
. Blu T, Unser M () The fractional spline
wavelet transform: definition and implementa-
tion. In: proceedings of the th IEEE Inter-
national Conference on Acoustics, Speech, and
Signal Processing (ICASSP’), vol , Istanbul,
Turkey, –June, , pp –
. Blu T, Unser M (). A complete family of
scaling functions: the (α, τ)-fractional splines.
In: Proceedings of the th International Con-
ference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP’), vol , Hong Kong SAR,


Splines and Multiresolution Analysis
People’s Republic of China, April –, , pp
–
. Buhmann MD () Radial basis functions:
theory and implementations. Cambridge mono-
graphs on applied and computational mathemat-
ics. Cambridge University Press, Cambridge
. de Boor C, Höllig K, Riemenschneider S ()
Box splines, vol of Applied mathematical sci-
ences. Springer, New York
. de Boor C, Devore RA, Ron A () Approxi-
mation from shiftinvariant subspaces of L(Rd).
Trans Am Math Soc ():–
. Burt PJ, Adelson EH (Apr ) The Laplacian
pyramid as a compact image code. IEEE Trans
Commun ():–
. Candès EJ, Wakin MB () An introduction to
compressive sampling. IEEE Signal Process Mag
():–
. Champeney DC () A handbook of Fourier
theorems. Cambridge University Press, Cam-
bridge
. Chen H-() Complex harmonic splines,
periodic quasi-wavelets, theory and applications.
Kluwer Academic, Dordrecht
. Choi JY, Kim MW, Seong W, Ye JC () Com-
pressed sensing metal artifact removal in dental
ct. In: Proceedings of IEEE International Sym-
posium on Biomedical Imaging (ISBI), June–
July , Boston, pp –
. Christensen O () An introduction to frames
and riesz bases. Birkhäuser, Boston
. Christensen O () Frames and bases: an intro-
ductory course (applied and numerical harmonic
analysis). Birkhäuser, Boston
. Chui CK () Multivariate splines. SIAM,
Philadelphia
. Chui C () Wavelets—a tutorial in theory and
practice. Academic, San Diego
. Chui CK (ed) () Wavelets: a tutorial in theory
and applications. Academic, Boston
. Condat L, Forster-Heinlein B, Van De Ville
D () A new family of rotation-covariant
wavelets on the hexagonal lattice. In: SPIE
Wavelets XII, Aug , San Diego
. Condat
L
()
Image
database.
Online
Ressource.
http://www.greyc.ensicaen.fr/_
lcondat/imagebase.html (Version of Apr )
. Dahmen
W,
Kurdila
A,
Oswald
P
(eds)
() Multiscale wavelet methods for partial
differential
equations,
vol

of
Wavelet
analysis and its applications. Academic, San
Diego
. Daubechies I () Ten lectures on wavelets.
Society for Industrial and Applied Mathematics,
Philadelphia
. Dierckx P () Curve and surface fitting with
splines. McGraw-Hill, New York
. Feilner M, Van De Ville D, Unser M ()
An orthogonal family of quincunx wavelets with
continuously adjustable order. IEEE Trans Image
Process ():–
. Forster B, Blu T, Unser M () Complex
B-splines. Appl Comput Harmon Anal ():
–
. Forster B, Blu T, Van De Ville D, Unser M ()
Shiftinvariant spaces from rotation-covariant
functions. Appl Comput Harmon Anal ():
–
. Forster B, Massopust P () Statistical encoun-
ters with complex B-splines. Constr Approx
():–
. Frikel J () A new framework for sparse reg-
ularization in limited angle x-ray tomography.
In IEEE international symposium on biomedical
imaging, Rotterdam
. Giles RC, Kotiuga PR, Mansuripur M () Par-
allel micromagnetic simulations using Fourier
methods on a regular hexagonal lattice. IEEE
Trans Magn ():–
. Grigoryan
AM
()
Efficient
algorithms
for
computing
the
-D
hexagonal
Fourier
transforms. IEEE Trans Signal Process ():
–
. Hales TC () The honeycomb conjecture.
Discr Comput Geom :–
. Heil C, Walnut DF () Fundamental papers
in wavelet theory. Princeton University Press,
Princeton. New edition
. Jones DS () Generalised functions. McGraw-
Hill, London
. Lai M-J, Schumaker LL () Spline functions
on triangulations. Cambridge University Press,
Cambridge
. Laine AF, Schuler S, Fan J, Huda W () Mam-
mographic feature enhancement by multiscale
analysis. IEEE Trans Med Imaging ():–
. Legrand P () Local regularity and multifrac-
tal methods for image and signal analysis. In:
Abry P, Gonçalves P, Véhel L (eds) Scaling, frac-
tals and wavelets, chap . Wiley-ISTE, London

Splines and Multiresolution Analysis 

. Lemarié P-G () Ondelettes a localisation
exponentielle. J Math pures et Appl :–
. LesageF,ProvostJ () Theapplicationof com-
pressed sensing for photo-acoustic tomography.
IEEE Trans Med Imaging ():–
. Lipow PR, Schoenberg IJ () Cardinal inter-
polation and spline functions. III: Cardinal
hermite interpolation. Linear Algebra Appl :
–
. Louis AK, Maaß P, Rieder A () Wavelets:
theory and applications. Wiley, New York
. Lustig M, Donoho D, Pauly JM () Sparse
MRI: the application of compressed sensing for
rapid MR imaging. Magn Reson Med ():
–
. Mallat S () Multiresolution approximations
and wavelet orthonormal bases of L(R). Trans
Am Math Soc :–
. Mallat SG () A wavelet tour of signal process-
ing. Academic, San Diego
. Mersereau RM () The processing of hexag-
onally sampled two-dimensioanl signals. Proc
IEEE ():–
. Meyer Y () Wavelets and operators. Cam-
bridge University Press, Cambridge
. Middleton L, Sivaswamy J () Hexagonal
image processing: a practical approach. Advances
in pattern recognition. Springer, Berlin
. Nicolier F, Laligant O, Truchetet F () B-spline
quincunx wavelet transforms and implementa-
tion in Fourier domain. Proc SPIE :–
. Nicolier F,LaligantO,TruchetetF() Discrete
wavelet transform implementation in Fourier
domain for multidimensional signal. J Electron
Imaging :–
. Nürnberger G () Approximation by Spline
functions. Springer, Berlin
. Plonka G, Tasche M () On the computa-
tion of periodic splione wavelets. Appl Comput
Harmon Anal :–
. Püschel M, Rötteler M () Algebraic signal
processing theory: D spatail hexagonal lattice.
IEEE Trans Image Proc ():–
. Rabut C (a) Elementary m-harmonic cardi-
nal B-splines. Numer Algorithms :–
. Rabut C (b) High level m-harmonic cardinal
B-splines. Numer Algorithms :–
. Rice
University
()
Compressive
sens-
ing resources. Online Ressource. http://dsp.
rice.edu/cs (Version of Apr )
. Rudin W () Functional analysis. Interna-
tional series in pure and applied mathematics.
McGraw-Hill, New York
. Sablonnière P, Sbibih D () B-splines with
hexagonal support on a uniform three-direction
mesh of the plane. C R Acad Sci Paris Série I
:–
. Schempp W () Complex contour integral
representation of cardinal spline functions, vol 
of Contemporary mathematics. American Math-
ematical Society, Providence
. Schoenberg IJ () Contributions to the prob-
lem of approximation of equidistant data by
analytic functions. Part a.–on the problem of
osculatory interpolation. A second class of ana-
lytic approximation formulae. Quart Appl Math
:–
. Schoenberg IJ () Contributions to the prob-
lem of approximation of equidistant data by
analytic functions. Part a.–on the problem of
smoothing or graduation. A first class of ana-
lytic approximation formulae. Quart Appl Math
:–
. Schoenberg IJ () Cardinal interpolation and
spline functions. J Approx Theory :–
. Schoenberg IJ () Cardinal interpolation and
spline functions. II: Interpolation of data of
power growth. J Approx Theory :–
. Schwartz L () Théorie des distributions. Her-
mann, Paris
. Unser M () Splines: A perfect fit for medi-
cal imaging. In: Sonka M., Fitzpatrick JM (eds)
Progress in biomedical optics and imaging, vol ,
no. , vol , Part I of Proceedings of the
SPIE international symposium on medical imag-
ing: image processing (MI’), San Diego, –
Feb, pp –
. Unser M, Blu T (Mar ) Fractional splines and
wavelets. SIAM Rev ():–
. Unser M, Aldroubi A, Eden M (Mar ) Fast
B-spline transforms for continuous image repre-
sentation and interpolation. IEEE Trans Pattern
Anal Mach Intell ():–
. Unser M, Aldroubi A, Eden M (Mar ) On
the asymptotic convergence of B-spline wavelets
to gabor functions. IEEE Trans Info Theory :
–
. Unser M, Aldroubi A, Eden M (Feb a) B-
spline signal processing: Part I—Theory. IEEE
Trans Signal Process ():–


Splines and Multiresolution Analysis
. Unser M, Aldroubi A, Eden M (Feb b) B-
spline signal processing: Part II—Efficient design
and applications. IEEE Trans Signal Process
():–
. Van De Ville D, Blu T, Unser M, Philips W,
Lemahieu I, Van de Walle R () Hex-splines:
a novel spline family for hexagonal lattices. IEEE
Trans Image Process ():–
. Van De Ville D, Blu T, Unser M (Nov )
Isotropic polyharmonic B-splines: scaling func-
tions and wavelets. IEEE Trans Image Process
():–
. Watson AB, Ahumuda AJ, Jr () Hexago-
nal orthogonal-oriented pyramid as a model of
image representation in visual cortex. IEEE Trans
Biomed Eng ():–
. Wendt H, Roux SG, Jaffard S, Abry P ()
Wavelet leaders and bootstrap for multifractal
analysis of images. Signal Process :–
. Wojtaszczyk P () A mathematical introduc-
tion to wavelets, vol of London mathemati-
cal society student texts. Cambridge University
Press, Cambridge
. Young R () An introduction to nonharmonic
Fourier series. Academic, New York (revised first
edition )

Gabor Analysis for Imaging
Ole Christensen ⋅Hans G. Feichtinger⋅Stephan Paukner
.
Introduction.....................................................................
.
Tools from Functional Analysis................................................
..
The Pseudo-Inverse Operator........................................................
..
Bessel Sequences in Hilbert Spaces..................................................
..
General Bases and Orthonormal Bases.............................................
..
Frames and Their Properties.........................................................
.
Operators........................................................................
..
The Fourier Transform................................................................
..
Translation and Modulation..........................................................
..
Convolution, Involution and Reflection............................................
..
The Short-Time Fourier Transform.................................................
.
Gabor Frames in L(Rd).......................................................
.
Discrete Gabor Systems........................................................
..
Gabor Frames in ℓ(Z)...............................................................
..
Finite Discrete Periodic Signals......................................................
..
Frames and Gabor Frames in CL....................................................
.
Image Representation by Gabor Expansion...................................
..
D Gabor Expansions..................................................................
..
Separable Atoms on Fully Separable Lattices......................................
..
Efficient Gabor Expansion by Sampled STFT.....................................
..
Visualizing a Sampled STFT of an Image...........................................
..
Non-Separable Atoms on Fully Separable Lattices................................
.
Historical Notes and Hint to the Literature...................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Gabor Analysis for Imaging
.
Introduction
In contrast to classical Fourier analysis, time-frequency analysis is concerned with local-
ized Fourier transforms. Gabor analysis is an important branch of time-frequency analysis.
Although significantly different, it shares with the wavelet transform methods the ability
to describe the smoothness of a given function in a location-dependent way.
The main tool is the sliding window Fourier transform or short-time Fourier transform
(STFT) in the context of audio signals. It describes the correlation of a signal with the time-
frequency shifted copies of a fixed function (or window, or atom). Thus, it characterizes a
function by its transform over phase space, which is the time-frequency plane (TF-plane) in
a musical context, or the location-wavenumber-domain in the context of image processing.
Since the transition from the signal domain to the phase space domain introduces an
enormous amount of data redundancy, suitable subsampling of the continuous transform
allows for complete recovery of the signal from the sampled STFT. The knowledge about
appropriate choices of windows and sampling lattices has increased significantly during
the last decades. Since the suggestion goes back to the idea of D. Gabor (, []), this
branch of TF-analysis is called Gabor analysis. Gabor expansions are not only of interest
due to their very natural interpretation, but also algorithmically convenient due to a good
understanding of algebraic and analytic properties of Gabor families.
In this chapter, we describe some of the generalities relevant for an understanding of
Gabor analysis of functions on Rd. We pay special attention to the case d = , which is the
most important case for image processing and image analysis applications.
The chapter is organized as follows.
> Section .presents central tools from func-
tional analysis in Hilbert spaces, e.g., the pseudo-inverse of a bounded operator and the
central facts from frame theory. In
> Sect. ., we introduce several operators that play
important roles in Gabor analysis. Gabor frames on L(Rd) are introduced in > Sect. .,
and their discrete counterpart are treated in > Sect. .. Finally, the application of Gabor
expansions to image representation is considered in > Sect. ..
.
Tools from Functional Analysis
In this section we recall basic facts from functional analysis. Unless another reference is
given, a proof can be found in []. In the entire section, H denotes a separable Hilbert
space with inner product ⟨⋅,⋅⟩.
..
The Pseudo-Inverse Operator
It is well known that an arbitrary matrix has a pseudo-inverse, which can be used to find
the minimal-norm least squares solution of a linear system. In the case of an operator on an
infinite dimensional Hilbert spaces one has to restrict the attention to linear operators with
closed range in order to obtain a pseudo-inverse. Observe that a bounded operator (We will

Gabor Analysis for Imaging 

always assume linearity!) U on a Hilbert space H is invertible if and only if it is injective
and surjective, while injectivity combined with a dense range is not sufficient in the infinite
dimensional case. However, if the range of U is closed, there exists a “right-inverse operator”
U† in the following sense:
Lemma 
Let H,K be Hilbert spaces, and suppose that U : K →H is a bounded operator
with closed range RU. Then there exists a bounded operator U† : H →K for which
UU†x = x, ∀x ∈RU.
(.)
Proof
Consider the operator the obtained by taking the restriction of U to the orthogonal
complement of the kernel of U, i.e., let
˜U := U∣N 
U : N 
U →H.
Obviously, ˜U is linear and bounded. ˜U is also injective: if ˜Ux = , it follows that x ∈
N 
U ∩NU = {}. We prove next that the range of ˜U equals the range of U. Given y ∈RU,
there exists x ∈K such that Ux = y. By writing x = x+ x, where x∈N 
U, x∈NU, we
obtain that
˜Ux= Ux= U(x+ x) = Ux = y.
It follows from Banach’s theorem that ˜U has a bounded inverse
˜U−: RU →N 
U.
Extending ˜U−by zero on the orthogonal complement of RU we obtain a bounded operator
U† : H →K for which UU†x = x for all x ∈RU .
∎
The operator U† constructed in the proof of Lemma is called the pseudo-inverse of U. In
the literature, one will often see the pseudo-inverse of an operator U defined as the unique
operator U† satisfying that
NU† = R
U, RU† = N 
U, and UU†x = x, x ∈RU;
(.)
this definition is equivalent to the above construction. We collect some properties of U†
and its relationship to U.
Lemma 
Let U : K →H be a bounded operator with closed range. Then the following
holds:
(i)
The orthogonal projection of H onto RU is given by UU†.
(ii)
The orthogonal projection of K onto RU† is given by U†U.
(iii)
U∗has closed range, and (U∗)† = (U†)∗.
(iv)
On RU, the operator U† is given explicitly by
U† = U∗(UU∗)−.
(.)


Gabor Analysis for Imaging
..
Bessel Sequences in Hilbert Spaces
When we deal with infinite-dimensional vector spaces, we need to consider expansions in
terms of infinite series. The purpose of this section is to introduce a condition that ensures
that the relevant infinite series actually converge. When speaking about a sequence {fk}∞
k=
in H, we mean an ordered set, i.e., {fk}∞
k== {f, f, . . . }. That we have chosen to index the
sequence by the natural numbers is just for convenience.
Deﬁnition 
A sequence {fk}∞
k=in H is called a Bessel sequence if there exists a constant
B > such that
∞
∑
k=
∣⟨f , fk⟩∣≤B ∣∣f ∣∣, ∀f ∈H.
(.)
Any number B satisfying (> .) is called a Bessel bound for {fk}∞
k=. The optimal bound for
a given Bessel sequence {fk}∞
k=is the smallest possible value of B > satisfying (> .).
Except for the case fk = , ∀k ∈N, the optimal bound always exists.
Theorem 
Let {fk}∞
k=be a sequence in H and B > be given. Then {fk}∞
k=is a Bessel
sequence with Bessel bound B if and only if
T : {ck}∞
k=→
∞
∑
k=
ck fk
defines a bounded operator from ℓ(N) into H and ∣∣T∣∣≤
√
B.
The operator T is called the synthesis operator. The adjoint T∗is called the analysis operator,
and is given by
T∗: H →ℓ(N), T∗f = {⟨f , fk⟩}∞
k=.
These operators play key roles in the theory of frames, to be considered in > Sect. ...
The Bessel condition (> .) remains the same, regardless of how the elements {fk}∞
k=
are numbered. This leads to a very important consequence of Theorem :
Corollary 
If {fk}∞
k=is a Bessel sequence in H, then ∑∞
k=ck fk converges unconditionally
for all {ck}∞
k=∈ℓ(N), i.e., the series is convergent, irrespective of how and in which order
the summation is realized.
Thus a reordering of the elements in {fk}∞
k=will not affect the series ∑∞
k=ck fk when
{ck}∞
k=is reordered the same way: the series will converge toward the same element as
before. For this reason, we can choose an arbitrary indexing of the elements in the Bessel
sequence; in particular, it is not a restriction that we present all results with the natural
numbers as index set. As we will see in the sequel, all orthonormal bases and frames are
Bessel sequences.

Gabor Analysis for Imaging 

..
General Bases and Orthonormal Bases
We will now briefly consider bases in Hilbert spaces. In particular, we will discuss orthonor-
mal bases, which are the infinite-dimensional counterparts of the canonical bases in Cn.
Orthonormal bases are widely used in mathematics as well as physics, signal processing,
and many other areas where one needs to represent functions in terms of “elementary
building blocks.”
Deﬁnition 
Consider a sequence {ek}∞
k=of vectors in H.
(i)
The sequence {ek}∞
k=is a (Schauder) basis for H if for each f ∈H there exist unique
scalar coefficients {ck(f )}∞
k=such that
f =
∞
∑
k=
ck(f )ek.
(.)
(ii)
A basis {ek}∞
k=is an unconditional basis if the series (> .) converges uncondition-
ally for each f ∈H.
(iii)
A basis {ek}∞
k=is an orthonormal basis if {ek}∞
k=is an orthonormal system, i.e., if
⟨ek, e j⟩= δk,j =
⎧⎪⎪⎨⎪⎪⎩

if
k = j,

if
k ≠j.
An orthonormal basis leads to an expansion of the type (> .) with an explicit expression
for the coefficients ck(f ):
Theorem 
If {ek}∞
k=is an orthonormal basis, then each f ∈H has an unconditionally
convergent expansion
f =
∞
∑
k=
⟨f , ek⟩ek.
(.)
In practice, orthonormal bases are certainly the most convenient bases to use: for other
types of bases, the representation (> .) has to be replaced by a more complicated expres-
sion. Unfortunately, the conditions for {ek}∞
k=being an orthonormal basis are strong,
and often it is impossible to construct orthonormal bases satisfying extra conditions. We
discuss this in more detail later. Note also that it is not always a good idea to use the Gram–
Schmidt orthonormalization procedure to construct an orthonormal basis from a given
basis: it might destroy special properties of the basis at hand. For example, the special
structure of a Gabor basis (to be discussed later) will be lost.


Gabor Analysis for Imaging
..
Frames and Their Properties
We are now ready to introduce one of the central subjects:
Deﬁnition 
A sequence {fk}∞
k=of elements in H is a frame for H if there exist constants
A, B > such that
A ∣∣f ∣∣≤
∞
∑
k=
∣⟨f , fk⟩∣≤B ∣∣f ∣∣,
∀f ∈H.
(.)
The numbers A and B are called frame bounds. A special role is played by frames for which
the optimal frame bounds coincide:
Deﬁnition 
A sequence {fk}∞
k=in H is a tight frame if there exists a number A > such
that
∞
∑
k=
∣⟨f , fk⟩∣= A∣∣f ∣∣,
∀f ∈H.
The number A is called the frame bound.
Since a frame {fk}∞
k=is a Bessel sequence, the operator
T : ℓ(N) →H, T{ck}∞
k==
∞
∑
k=
ck fk
(.)
is bounded by Theorem . Composing T and T∗, we obtain the frame operator
S : H →H, S f = TT∗f =
∞
∑
k=
⟨f , fk⟩fk.
(.)
The frame decomposition, stated in (> .) below, is the most important frame result.
It shows that if {fk}∞
k=is a frame for H, then every element in H has a representation as
an infinite linear combination of the frame elements. Thus it is natural to view a frame as
a “generalized basis.”
Theorem 
Let {fk}∞
k=be a frame with frame operator S. Then
f =
∞
∑
k=
⟨f , S−fk⟩fk,
∀f ∈H,
(.)
and
f =
∞
∑
k=
⟨f , fk⟩S−fk,
∀f ∈H.
(.)
Both series converge unconditionally for all f ∈H.
Theorem shows that all information about a given vector f
∈H is contained in
the sequence {⟨f , S−fk⟩}∞
k=. The numbers ⟨f , S−fk⟩are called frame coefficients. The
sequence {S−fk}∞
k=is also a frame; it is called the canonical dual frame of {fk}∞
k=.

Gabor Analysis for Imaging 

Theorem also immediately reveals one of the main difficulties in frame theory. In fact,
in order for the expansions (> .) and (> .) to be applicable in practice, we need to
be able to find the operator S−, or at least to calculate its action on all fk, k ∈N. In general,
this is a major problem. One way of circumventing the problem is to consider only tight
frames:
Corollary 
If {fk}∞
k=is a tight frame with frame bound A, then the canonical dual frame
is {A−fk}∞
k=, and
f = 
A
∞
∑
k=
⟨f , fk⟩fk,
∀f ∈H.
(.)
By a suitable scaling of the vectors {fk}∞
k=in a tight frame, we can always obtain that A = ;
in that case, (> .) has exactly the same form as the representation via an orthonormal
basis, see (> .). Thus, such frames can be used without any additional computational
effort compared with the use of orthonormal bases; however, the family does not have to
be linear independent now.
Tight frames have other advantages. For the design of frames with prescribed proper-
ties, it is essential to control the behavior of the canonical dual frame, but the complicated
structure of the frame operator and its inverse makes this difficult. If, e.g., we consider
a frame {fk}∞
k=for L(R) consisting of functions with exponential decay, nothing guar-
antees that the functions in the canonical dual frame {S−fk}∞
k=have exponential decay.
However, for tight frames, questions of this type trivially have satisfactory answers, because
the dual frame equals the original one. Also, for a tight frame, the canonical dual frame
automatically has the same structure as the frame itself: if the frame has Gabor structure
(to be described in > Sect. .), the same is the case for the canonical dual frame.
There is another way to avoid the problem of inverting the frame operator S. A frame
that is not a basis is said to be overcomplete; in the literature, the term redundant frame
is also used. For frames {fk}∞
k=that are not bases, one can replace the canonical dual
{S−fk}∞
k=by other frames:
Theorem 
Assume that {fk}∞
k=is an overcomplete frame. Then there exist frames
{gk}∞
k=≠{S−fk}∞
k=for which
f =
∞
∑
k=
⟨f , gk⟩fk,
∀f ∈H.
(.)
A frame {gk}∞
k=satisfying (> .) is called a dual frame of {fk}∞
k=. The hope is to find
dual frames that are easier to calculate or have better properties than the canonical dual.
Examples of this type can be found in [].
.
Operators
In this section we introduce several operators that play key roles in Gabor analysis. In par-
ticular, we will need the basic properties of the localized Fourier transform, which is called


Gabor Analysis for Imaging
the STFT (short-time Fourier transform). It is natural for us to start with the Fourier trans-
form, which is defined as an integral transform on the space of all (Lebesgue) integrable
functions, denoted by L(Rd).
..
The Fourier Transform
Deﬁnition 
For f ∈L(Rd), the Fourier transform is defined as
ˆf (ω) ∶= (F f )(ω) ∶= ∫Rd f (x) e−πix⋅ω dx,
(.)
where x ⋅ω = ∑d
k=xkωk is the usual scalar product of vectors in Rd.
Lemma (Riemann–Lebesgue)
If f
∈L(Rd), then ˆf is uniformly continuous and
lim∣ω∣→∞∣ˆf (ω)∣= .
The Fourier transform yields a continuous bijection from the Schwartz space S(Rd) to
S(Rd). This follows from the fact that it turns analytic operations (differentiation) into
multiplication with polynomials and vice versa:
F(Dα f ) = (πi)∣α∣Xα(F f )
(.)
and
Dα(F f ) = (−πi)∣α∣F(Xα f ),
(.)
with a multi-index α = (α, . . . , αd) ∈Nd
, ∣α∣∶= ∑d
i=αi, Dα as differential operator
Dα f (x) ∶=
∂α⋯∂αd
∂xα
⋯∂xαd
d
f (x, . . . , xd)
and Xα as multiplication operator (Xα f )(x) ∶= xα
⋯xαd
d f (x, . . . , xd). It follows from the
definition that S(Rd) is invariant under these operations, i.e.,
Xα f ∈S(Rd)
and
Dα f ∈S(Rd)
∀α ∈Nd

∀f ∈S(Rd).
Using the reflection operator (I f )(x) ∶= f (−x), one can show that F = I and so F =
IdS(Rd). This yields
F−= IF
(.)
and we can give an inversion formula explicitly:
Theorem (Inversion Formula)
The Fourier transform is a bijection from S(Rd) to
S(Rd) and the inverse operator is given by
(F−f )(x) = ∫Rd f (ω) eπix⋅ω dω
∀x ∈Rd.
(.)
Furthermore,
⟨F f ,F g⟩L= ⟨f , g⟩L
∀f , g ∈S(Rd).

Gabor Analysis for Imaging 

We can extend the Fourier transform to an isometric operator on all of L(Rd). We will
use the same symbol F although the Fourier transform on L(Rd) is not defined by a
Lebesgue integral (> .) anymore if f ∈L/L(Rd), but rather by means of summability
methods. Moreover, F f should be viewed as an equivalence class of functions, rather than
a pointwise given function.
Theorem (Plancherel)
If f ∈L∩L(Rd), then
∥f ∥L= ∥F f ∥L.
(.)
As a consequence, F extends in a unique way to a unitary operator on L(Rd) that satisfies
Parseval’s formula
⟨f , g⟩L= ⟨F f ,F g⟩L
∀f , g ∈L(Rd).
(.)
In signal analysis, the isometry of the Fourier transform has the interpretation that it pre-
serves the energy of a signal. For more details on the role of the Schwartz class for the
Fourier transform see [, V].
..
Translation and Modulation
Deﬁnition 
For x, ω ∈Rd we define the translation operator Tx by
(Tx f )(t) ∶= f (t −x)
(.)
and the modulation operator Mω by
(Mω f )(t) ∶= eπiω⋅t f (t).
(.)
One has T−
x
= T−x and M−
ω = M−ω. The operator Tx is called a time shift, and Mω a
frequency shift. Operators of the form TxMω or MωTx are called time-frequency shifts (TF-
shifts). They satisfy the commutation relations
Tx Mω = e−πix⋅ωMωTx.
(.)
Time-frequency shifts are isometries on Lp for all ≤p ≤∞, i.e.,
∥TxMω f ∥Lp = ∥f ∥Lp.
The interplay of TF-shifts with the Fourier transform is as follows:
̂
Tx f = M−x ˆf
or
FTx = M−xF
(.)
and
̂
Mω f = Tω ˆf
or
FMω = TωF.
(.)
> Equation (.) explains why modulations are also called frequency shifts: modulations
become translations on the Fourier transform side. Altogether, we have
̂
TxMω f = M−xTω ˆf = e−πix⋅ωTωM−x ˆf .


Gabor Analysis for Imaging
..
Convolution, Involution and Reﬂection
Deﬁnition 
The convolution of two functions f , g ∈L(Rd) is the function f ∗g defined
by
(f ∗g)(x) ∶= ∫Rd f (y) g(x −y) dy.
(.)
It satisfies
∥f ∗g∥L≤∥f ∥L∥g∥L
and
̂
f ∗g = ˆf ⋅ˆg.
One may view f ∗g as f being “smeared” by g and vice versa. One can thus smoothen a
function by convolving it with a narrow bump function.
Deﬁnition 
The involution of a function is defined by
f ∗(x) ∶= f (−x).
(.)
It follows that
̂f ∗= ¯ˆf
and
̂
I f = I ˆf .
Finally, let us mention that convolution corresponds to pointwise multiplication (and
conversely), i.e., the so-called convolution theorem is valid:
̂
g ∗f = ˆf ⋅ˆg.
(.)
..
The Short-Time Fourier Transform
The Fourier transform as described in > Sect. ..provides only global frequency infor-
mation of a signal f . This is useful for signals that do not vary during the time, e.g., for
analyzing the spectrum of a violin tone. However, dynamic signals such as a melody have
to be split into short time-intervals over which it can be well approximated by a linear com-
bination of few pure frequencies. Since sharp cut-offs would introduce discontinuities in
the localized signal and therefore leaking of the frequency spectrum, a smooth window
function g is usually used in the definition of the short-time Fourier transform.
In image processing, one has plane waves instead of pure frequencies, thus the global
Fourier transform is only well suited to stripe-like patterns. Again, a localized version of the
Fourier transform allows to determine dominant plane waves locally, and one can recon-
struct an image from such a redundant transform. Gabor analysis deals with the question
of how one can reconstruct an image from only somewhat overlapping local pieces, which
are stored only in the form of a sampled (local) D Fourier transform.
Deﬁnition 
Fix a window function g ∈L(Rd)/{}. The short-time Fourier transform
(STFT), also called (continuous) Gabor transform of a function f ∈L(Rd) with respect to
g is defined as
(Vg f )(x, ω) ∶= ∫Rd f (t) g(t −x) e−πit⋅ω dt
for x, ω ∈Rd.
(.)

Gabor Analysis for Imaging 

0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
Time (s)
Amplitude
Signal 1: Concurrent frequencies
a
0
0.33
0.67
1
−1
−0.5
0
0.5
1
Time (s)
Amplitude
Signal 2: Consecutive frequencies
b
12
27
45
0
50
100
150
200
Frequency (Hz)
Power
Fourier power spectrum 1
c
Time (s)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
12
27
45
STFT 1 with wide window
e
Time (s)
Frequency (Hz)
0
0.2
0.4
0.6
0.8
1
12
27
45
STFT 1 with narrow window
g
Time (s)
Frequency (Hz)
0
0.33
0.67
1
12
27
45
STFT 2 with narrow window
h
Time (s)
Frequency (Hz)
0
0.33
0.67
1
12
27
45
STFT 2 with wide window
f
12
27
45
0
50
100
150
200
Frequency (Hz)
Power
Fourier power spectrum 2
d
⊡Fig. -
Two signals and their (short-time) Fourier transforms
For f , g ∈L(Rd) the STFT Vg f is uniformly continuous (by Riemann-Lebesgue) on Rd
and can be written as
(Vg f )(x, ω) = ̂
f ⋅Tx ¯g(ω)
(.)
= ⟨f , MωTx g⟩L
(.)
= e−πix⋅ω(f ∗Mωg∗)(x).
(.)


Gabor Analysis for Imaging
The STFT as a function in x and ω seems to provide the possibility to obtain infor-
mation about the occurrence of arbitrary frequencies ω at arbitrary locations x as desired.
However, the uncertainty principle (cf.[]) implies that there is a limitation concerning the
joint resolution. In fact, the STFT has limitations in its time-frequency resolution capabil-
ity: Low frequencies can hardly be located with narrow windows, and similarly, short pulses
remain invisible for wide windows. The choice of the analyzing window is therefore crucial.
Just like the Fourier transform, the STFT is a kind of time-frequency representation
of a signal. This again raises the question of how to reconstruct the signal from its time-
frequency representation. To approach this, we need the orthogonality relations of the
STFT, which corresponds to Parseval’s formula (> .) for the Fourier transform:
Theorem (Orthogonality relations for STFT)
Let f, f, g, g∈L(Rd). Then Vg j fj ∈
L(Rd) for j ∈{,}, and
⟨Vgf,Vgf⟩L(Rd) = ⟨f, f⟩L⟨g, g⟩L.
Corollary 
If f , g ∈L(Rd), then
∥Vg f ∥L(Rd) = ∥f ∥L∥g∥L.
In the case of ∥g∥L= we have
∥f ∥L= ∥Vg f ∥L(Rd)
∀f ∈L(Rd),
(.)
i.e., the STFT as an isometry from L(Rd) into L(Rd).
Formula (> .) shows that the STFT preserves the energy of a signal; it corresponds
to (> .) which shows the same property for the Fourier transform. Therefore, f is
completely determined by Vg f and the inversion is given by a vector-valued integral (for
good functions valid in the pointwise sense):
Corollary (Inversion formula for the STFT)
Let g,γ ∈L(Rd) and ⟨g,γ⟩≠. Then
f (x) =

⟨γ, g⟩L∬Rd Vg f (x, ω) MωTxγ(x) dω dx
∀f ∈L(Rd).
(.)
Obviously, γ = g is a natural choice here. The time-frequency analysis of signals is usually
done by three subsequent steps:
(i) Analysis: Using the STFT, the signal is transformed into a joint time-frequency
representation.
(ii) Processing: The obtained signal representation is then manipulated in a certain way,
e.g., by restriction to a part of the signal yielding the relevant information.
(iii) Synthesis: The inverse STFT is applied to the processed representation, thus creating
a new signal.

Gabor Analysis for Imaging 

A function is completely represented by its STFT, but in a highly redundant way. To mini-
mize the influence of the uncertainty principle the analyzing window g should be chosen
such that g and its Fourier transform ˆg both decay rapidly, e.g., as Schwartz functions.
A computational implementation can only be obtained by a discretization of both the
functions and the STFT. Therefore, only sampled versions of the STFT are possible and
only certain locations and frequencies are used for analyzing a given signal. The chal-
lenge is to find the appropriate lattice constants in time and frequency and to obtain good
time-frequency resolution.
.
Gabor Frames in L(Rd)
By formula (> .), the STFT analyzes a function f ∈L(Rd) into coefficients
⟨f , MωTx g⟩L
using modulations and translations of a single window function
g ∈L(Rd)/{}. One problem we noticed was that these TF-shifts are infinitesimal and
overlap largely, making the STFT a highly redundant time-frequency representation. An
idea to overcome this is to restrict to discrete choices of time-positions x and frequencies
ω such that this redundancy is decreased while leaving enough information in the coeffi-
cients about the time-frequency behavior of f . This is the very essence of Gabor analysis:
It is sought to expand functions in L(Rd) into an absolutely convergent series of modula-
tions and translations of a window function g. Therefore it is interesting to find necessary
and sufficient conditions on g and a discrete set Λ ⊆Rd × Rd such that
{gx,ω}(x,ω)∈Λ ∶= {MωTx g}(x,ω)∈Λ
forms a frame for L(Rd). The question arises how the sampling set Λ should be structured.
It turns out to be very convenient to have this set closed under the addition operation,
urging Λ to be a subgroup of the time-frequency plane, i.e., Λ ⊴Rd × Rd. Dennis Gabor
(Actually Dénes Gábor.) suggested in his Theory of Communication [], , to use fixed
step-sizes α, β > for time and frequency and use the set {αk}k∈Zd for the time-positions
and {βn}n∈Zd for the frequencies, yielding the functions
gk,n(x) ∶= MβnTαk g(x) = eπiβn⋅x g(x −αk)
as analyzing elements. This is the approach that is usually presented in the literature,
although there is also a more general group-theoretical setting possible where Λ is an arbi-
trary (discrete) subgroup. This subgroup is also called a time-frequency lattice, although it
doesn’t have to be of such a “rectangular” shape in general.
Deﬁnition 
A lattice Λ ⊆Rd is a (discrete) subgroup of Rd of the form Λ = AZd, where
A is an invertible d × d-matrix over R. Lattices in Rd can be described as
Λ = {(x, y) ∈Rd ∣(x, y) = (Ak + Bℓ, Ck + Dℓ), (k, ℓ) ∈Zd}


Gabor Analysis for Imaging
with A, B, C, D ∈Cd×d and
A = (A
B
C
D).
A lattice Λ = αZd × βZd ⊴Rd for α, β > is called a separable lattice, a product lattice, or
a grid.
In the following, our lattice will be of the separable type for fixed lattice parameters α, β > .
Deﬁnition 
For a nonzero window function g ∈L(Rd) and lattice parameters α, β > ,
the set of time-frequency shifts
G(g, α, β) ∶= {MβnTαk g}k,n∈Zd
is called a Gabor system. If G(g, α, β) is a frame for L(Rd), it is called a Gabor frame or
Weyl–Heisenberg frame. The associated frame operator is the Gabor frame operator and takes
the form
S f = ∑∑
k,n∈Zd
⟨f , MβnTαk g⟩LMβnTαk g
(.)
= ∑∑
k,n∈Zd
Vg f (αk, βn) MβnTαk g
for all f ∈L(Rd). The window g is also called the Gabor atom.
According to the general frame theory, {S−gk,n}k,n∈Zd yields the canonical dual frame. So
we would have to compute S−and apply it to all modulated and translated versions of the
Gabor atom g. A direct computation shows that for arbitrary fixed indices ℓ, m ∈Zd,
SMβmTαℓ= MβmTαℓS.
(.)
Consequently, also S−commutes with time-frequency shifts, which gives the following
fundamental result for (regular) Gabor analysis:
Theorem 
If the given Gabor system G(g, α, β) is a frame for L(Rd), then all of the
following hold:
(a) There exists a dual window γ ∈L(Rd) such that the dual frame is given by the Gabor
frame G(γ, α, β).
(b) Every f ∈L(Rd) has an expansion of the form
f = ∑∑
k,n∈Zd
⟨f , MβnTαk g⟩LMβnTαkγ
(.)
= ∑∑
k,n∈Zd
⟨f , MβnTαkγ⟩LMβnTαk g
with unconditional convergence in L(Rd).

Gabor Analysis for Imaging 

(c) The canonical dual frame is given by the Gabor frame {MβnTαkS−g}k,n∈Zd built from
the canonical dual window γ○∶= S−g.
(d) The inverse frame operator S−is just the frame operator for the Gabor system G(γ○, α, β)
and
S−f = ∑∑
k,n∈Zd
⟨f , MβnTαkγ○⟩LMβnTαkγ○.
(.)
We note that if the function g is compactly supported and the modulation parameter β is
sufficiently small, it is easy to verify whether G(g, α, β) is a frame, and to find the canonical
dual window in the affirmative case; see [, .] or [, .].
One can show [, ..] that all dual windows γ of a Gabor frame G(g, α, β) are within
an affine subspace of L(Rd), namely, γ ∈γ○+ K⊥, where K is the closed linear span of
G (g, 
β , 
α ) and therefore
K⊥= {h ∈L(Rd) : ⟨h, Mn/αTk/βg⟩L= ∀k, n ∈Zd}.
(.)
Hence, we have γ = γ○+ h for a certain h ∈K⊥, and as γ○∈K, the canonical dual window
possesses the smallest L-norm among all dual windows and is most similar to the original
window g. However, there might be reasons not to choose the canonical dual window,
but one of the others in γ○+ K⊥, if, e.g., one wants the dual window to have a smaller
essential support, or if the window should be as smooth as possible. Explicit constructions
of alternative dual windows can be found in [].
A key result in Gabor analysis states a necessary condition for a Gabor system to form
a frame:
Theorem 
Let g ∈L(Rd)/{} and α, β > . If G(g, α, β) is a frame, then:
(a) αβ ≤.
(b) G(g, α, β) is a basis if and only if αβ = .
Unfortunately, having αβ ≤is not sufficient for a Gabor system to form a frame. Suffi-
cient conditions are presented, e.g., in [, .]. A special result is known for the Gaussian
function:
Theorem 
Consider the normalized Gaussian φ(x) ∶= d/e−πx. Then G(φ, α, β) is a
frame for L(Rd) if and only if αβ < .
In signal analysis, it is customary to call the case
αβ < 
oversampling
αβ = 
critical sampling and
αβ > 
undersampling
In the case of the Gaussian window, oversampling guarantees an excellent time-
frequency localization. But for Gabor frame theory in L(Rd), it is quite delicate to find


Gabor Analysis for Imaging
appropriate windows for given αβ ≤. The case αβ = is problematic from the point of
view of time frequency analysis, as the Balian–Low Theorem demonstrates:
Theorem (Balian–Low)
Let g ∈L(Rd) be a nonzero window and α, β > with αβ = .
If g has good TF-concentration in the sense of
∥Xg∥L∥X ˆg∥L< ∞,
then G(g, α, β) cannot constitute a frame.
Combining Theorem and Theorem shows that it is impossible for a Gabor basis to be
well localized in both the time domain and the frequency domain. This motivates the study
of redundant Gabor systems: As demonstrated by Theorem , redundant Gabor frames
exist for any αβ < .
.
Discrete Gabor Systems
For practical implementations of Gabor analysis, it is essential to develop discrete versions
of the theory for Gabor frames.
..
Gabor Frames in ℓ(Z)
Classically, most signals were considered as “continuous waves”. Indeed, the technology for
signal processing originally was of the continuous-time analog type before digital com-
puters came into our everyday life. Nowadays digital signal processing is used almost
exclusively, forcing us to change our function model to a time-discrete one. It is therefore
natural to switch from L(R) to ℓ(Z).
Gabor frame theory in ℓ(Z) is very similar to that in L(R) and will therefore only be
discussed briefly in this section. The main differences concern the time shifts and frequency
shifts. Time shifts are given as multiples of integer translates, i.e.,
Tk f (j) = f (j −k)
(.)
for k ∈Z and f ∈ℓ(Z). A shift parameter α > for Gabor frames in ℓ(Z) can only be
given as α = N ∈N.
For fixed L ∈N and corresponding to the modulation parameter /L, we define the
modulation operator Mℓby
Mℓf (j) = eπi jℓ/L f (j)
(.)
for ℓ∈Z. Modulations are now periodic with period L, i.e., Mℓ+nL = Mℓ
∀n ∈Z,
implying that one needs only the modulations M, . . . , ML−.

Gabor Analysis for Imaging 

The discrete Gabor system generated by the sequence g ∈ℓ(Z), shift parameters N,
and modulation parameter /L is now the family of sequences {gk,ℓ}k∈Z,ℓ∈⟨L⟩where
gk,ℓ(j) ∶= MℓTkN g(j) = eπi jℓ/L g(j −kN)
and ⟨L⟩∶= {, . . . , L −} ⊆Z.
If a Gabor system satisfies the frame inequalities for f ∈ℓ(Z) the dual frame is again
a Gabor frame built from a dual window γ ∈ℓ(Z). The frame expansion takes the form
f =
∞
∑
k=−∞
L−
∑
ℓ=
⟨f , MℓTkNγ⟩MℓTkN g
for
f ∈ℓ(Z).
Many results and conditions for Gabor systems in ℓ(Z) can mutatis mutandis be taken
over from L(R), e.g., a necessary condition for the mentioned Gabor system to be a frame
for ℓ(Z) is that αβ = N/L ≤.
We note that there is a natural way of constructing Gabor frames in ℓ(Z) from Gabor
frames in L(R) through sampling; see the paper [] by Janssen.
The step from L(R) to ℓ(Z) is the first one toward computational realization of Gabor
analysis. However, since in finite time only finitely many elements can be considered, only
vectors of finite length and finite sums can be computed. Therefore, we turn to signals of
finite length next.
..
Finite Discrete Periodic Signals
In practice, one has to resort to finite, discrete sequences. We will consider signals f ∈CL,
i.e., signals of length L ∈N and write f = (f (), . . . , f (L −)), defined (for convenience)
over the domain ⟨L⟩∶= {, . . . , L−} ⊆Z. This way of indexing suggests in a natural way to
view them as functions over the group of unit roots of order L, or equivalently as periodic
sequences with
f (j + nL) ∶= f (j)
∀n ∈Z, j ∈⟨L⟩.
The discrete modulation Mℓdefined in (> .) can still be applied, the translation Tk
defined in (> .) can be taken from the range ≤k ≤L −.
The discrete Fourier transform (DFT) of f ∈CL is defined as
ˆf (j) ∶= (F f )(j) ∶=
L−
∑
k=
f (k) e−πi jk/L,
j ∈ZL,
(.)
which is – up to a constant – a unitary mapping on CL. Its inverse is
(F−f )(j) ∶= 
L
L−
∑
k=
f (k) eπi jk/L,
j ∈ZL.
(.)
The unitary version CL →CL has the factor /
√
L in front. A well-known and very efficient
implementation of the DFT is the Fast Fourier Transform (FFT).


Gabor Analysis for Imaging
The discrete STFT of f ∈CL with respect to the discrete window g ∈CL is given as
(Vg f )(k, ℓ) = ⟨f , MℓTkg⟩CL.
The actions of time- and frequency shifts are in more detail given as
Tk f = Tk(f (), . . . , f (L −)) = (f (−k), . . ., f (L −−k))
and
Mℓf = Mℓ(f (), . . . , f (L −))
= (f (), eπiℓ/L f (), eπiℓ/L f (), . . . , eπi(L−)ℓ/L f (L −)).
The actions of the TF-shifts can be described as matrices that operate on the vector
f = (f (), . . . , f (L −))T. The time-shift matrix Tk is given as the permutation matrix
with ones on the (periodized) k-th subdiagonal, whereas the modulation matrix has its
exponential entries positioned at the main diagonal. It is obvious that the composition of
arbitrary TF-shifts need not be commutative, since
TkMℓ= eπikℓ/LMℓTk,
k, ℓ∈ZL
To get a more compact notation for TF-shifts, we write
π(λ) ∶= π(k, ℓ) ∶= MℓTk
with
λ = (k, ℓ) ∈ZL × ZL,
where ZL × ZL is the discrete time-frequency plane. The commutation relations imply for
λ = (r, m) and μ = (s, n)
π(λ) π(μ) = π(λ + μ) eπirn/L
(.)
= π(μ) π(λ) eπi(rn−sm)/L.
(.)
..
Frames and Gabor Frames in CL
The general frame definitions and results can easily be carried over to the case of finite
discrete signals. The conditions for the finite sequence {g, . . . , gN−} of elements g j ∈CL
to be a frame for the finite-dimensional Hilbert space CL are that there exist A, B > such
that
A
L−
∑
k=
∣f (k)∣≤
N−
∑
j=
∣⟨f , g j⟩CL∣
≤B
L−
∑
k=
∣f (k)∣
∀f ∈CL
or
A∥f ∥
≤∥C f ∥
≤B∥f ∥

∀f ∈CL,
where C is the analysis operator. It is obvious that the sequence {g j}N−
j=has to span all of
CL, i.e., span{g j}N−
j== CL, hence N ≥L in a Hilbert space with dimension L. Also the
converse is true: Every spanning set in CL is a frame for CL.

Gabor Analysis for Imaging 

The action of the linear analysis operator C on the vector f is given as the vector C f =
(⟨f , g j⟩)
N−
j=, indicating that its j-th entry is
(C f )j = ⟨f , g j⟩=
L−
∑
k=
f (k) g j(k) .
Letting g∗= ¯gT, the matrix form of C ∈CN×L is
C =
⎛
⎜
⎝
g∗

⋮
g∗
N−
⎞
⎟
⎠
=
⎛
⎜⎜
⎝
g()
⋯
g(L −)
⋮
⋮
⋮
gN−()
⋯
gN−(L −)
⎞
⎟⎟
⎠
.
A family {g j}j∈⟨N⟩is a frame for CL if and only if the corresponding analysis operator C
has full rank, and every matrix with full rank uniquely represents a frame.
The frame operator S = C∗C becomes an L × L-matrix that also has full rank, and it is
therefore invertible. Its condition number equals the ratio between its largest and smallest
eigenvalue; letting A denote the largest lower frame bound and B the smallest upper frame
bound, this is equal to the ratio B/A.
If we translate the discrete frame expansion
f = C∗c = (g, . . . , gN−)
⎛
⎜
⎝
c()
⋮
c(N −)
⎞
⎟
⎠
=
⎛
⎜⎜
⎝
∑N−
j=c(j) g j()
⋮
∑N−
j=c(j) g j(L −)
⎞
⎟⎟
⎠
for a given f ∈CL, we see from a linear algebra point of view that we are looking for N
unknown coordinates of c ∈CN, using L ≤N equations. Clearly the solution cannot be
unique if L < N. Considering that
f = SS−f = C∗C(C∗C)−f ,
we see that one solution for c could be given as
c = C(C∗C)−f = (C∗)† f
in terms of the pseudoinverse of the synthesis operator C∗. This also provides the matrix
form of the canonical dual frame that is given by
(S−g, . . . , S−gN−)
∗= (S−C∗)
∗= CS−= (C∗)†.
We will now proceed to the special case of Gabor frames. They are given as a sequence
of TF-shifts of a single window function g ∈CL, i.e., a Gabor frame for CL is a sequence
{gλ}λ∈Λ ∶= {π(λ)g}λ∈Λ for a certain discrete subset Λ ⊆ZL × ZL. We write Cg for the
Gabor analysis operator to indicate the dependence on g and use it synonymously for the
Gabor frame itself. It is clear that it is necessary to have N ≥L elements to span all of CL,
but this is of course not sufficient for validating a frame. The ratio between N and L is also
called the redundancy of the frame,
redC ∶= N
L .


Gabor Analysis for Imaging
For any subgroup Λ ⊴ZL×ZL, the Gabor frame operator Sg = C∗
g Cg commutes with all
TF-shifts π(λ) for λ ∈Λ. This can be shown in a similar way as in > Sect. .. Therefore,
the dual frame is once again a Gabor frame, built by the same TF-shifts of a single dual
window γ ∈CL. The canonical dual frame consists of elements
S−
g π(λ)g = π(λ)S−
g g = π(λ)γ○,
and the computation of the canonical dual window reduces to finding a solution for the
linear equation
Sgγ○= g.
(.)
Therefore, the discrete Gabor expansion of an f ∈CL is given as
f = ∑
λ∈Λ
⟨f , π(λ)g⟩CL π(λ)γ○= ∑
λ∈Λ
⟨f , π(λ)γ○⟩CL π(λ)g,
where the Gabor coefficients belong to ℓ(Λ) ≅CN.
A special case for a lattice is a so-called separable lattice Λ = αZL × βZL with α, β ∈N
being divisors of L. The elements of such a Gabor frame take the form
MβℓTαk g(j) = eπiβℓj/Lg(j −αk)
with k ∈⟨L
α ⟩and ℓ∈⟨L
β ⟩. The number of elements is N = L
α ⋅L
β = L
αβ , and it is necessary to
have L
αβ ≥L elements to have a frame. The oversampled case is therefore given for αβ < L,
and the undersampled case for αβ > L. Critical sampling is given for αβ = L.
.
Image Representation by Gabor Expansion
We have seen that Gabor analysis can be considered as a localized Fourier analysis, where
the main design freedom is the choice of (a) the time-frequency lattice and (b) the window
function. The type of sampling lattice can be distinguished into a separable or non-
separable case, where the first one can be described by the choice of lattice constants
α, β > .
It turns out that in the twofold-separable case, i.e., where the d-dimensional analysis
window is a tensor product of d one-dimensional functions
g = g⊗⋯⊗gd ,
with
g⊗⋯⊗gd(x, . . . , xd) = g(x) . . . gd(xd),
and the sampling lattice Λ is a product Λ = ∏d
i=αiZLi × ∏d
i=βiZLi, the dual Gabor win-
dow γ is given as a product γ = γ⊗⋯⊗γd as well. Thus the computation is reduced
to finding the D duals γi of the D atoms gi with respect to the corresponding D
time-frequency lattices Λi = αiZLi × βiZLi.
Our aim here is to show how the results can be applied to the case of image signals.
Gabor expansions of finite discrete D signals (i.e., digital images) are similar to those of

Gabor Analysis for Imaging 

0.06
0.04
0.02
–0.02
–0.04
–0.06
0
⊡Fig. -
Typical D Gabor atoms
finite discrete D signals and in a more general notation, there is no difference at all. We
are going to describe it next.
..
D Gabor Expansions
The key point for the development of efficient algorithms is to interpret an image of size
L× Las a real or complex-valued function on the additive Abelian group G = ZL× ZL.
The position-frequency space is
G × ̂G = ZL× ZL×
̂
ZL× ZL.
A Gabor system G(g, Λ) consists of TF-shifts MlTkg of a window g ∈CL×L, where
(k,l) are elements of a sampling subgroup Λ ⊴ZL× ZL×
̂
ZL× ZL. The Gabor
coefficients of the image f ∈CL×Lare defined as
ck,l ∶= ⟨f , MlTkg⟩F,
(k,l) ∈Λ.
(.)
Here we use the subscript F in order to recall that for matrices (this is how images are usu-
ally stored) one takes the scalar product and the corresponding norm just as the Euclidian
one in CN, with N = LL, usually denoted as Frobenius norm.
The Gabor system is a frame if for < A ≤B < ∞one has
A∥f ∥
F ≤
∑
(k,l)∈Λ
∣⟨f , MlTkg⟩F∣≤B∥f ∥
F
∀f ∈CL×L.
For dimensionality reasons, it is clear that the frame condition is only possible if the
number of elements in Λ has to be at least equal to the dimension of the signal space and,
therefore, we need LL≤∣Λ∣≤(LL). The redundancy of the Gabor frame is
redΛ ∶= ∣Λ∣
LL
≥.


Gabor Analysis for Imaging
As in the one-dimensional case, the Gabor frame operator
Sg f ∶=
∑
(k,l)∈Λ
⟨f , MlTkg⟩F MlTkg
commutes with TF-shifts determined by Λ, and the minimal resp. maximal eigenvalue
are equal to the maximal lower frame bound A and minimal upper frame bound B,
respectively.
Again, the dual Gabor frame has a similar structure as the Gabor frame itself: Using
the same TF-shifts, now applied to a dual window γ ∈CL×Lone has the expansion
f =
∑
(k,l)∈Λ
⟨f , MlTkg⟩F MlTkγ =
∑
(k,l)∈Λ
⟨f , MlTkγ⟩F MlTkg
for all f ∈CL×L. The existence of the dual atom is guaranteed by the theory of frames, and
the calculation of the dual Gabor frame is done by the methods developed there. Recent
results guarantee that good TF-concentration of the atom g implies a similar quality for
the dual Gabor atom. Typically, the condition number of the frame operator depends on
the geometric density (hence to some extent on the redundancy) of the lattice. However
it is worth mentioning that even for low redundancy factors, relatively good condition
numbers can be expected for suitably chosen atoms, and that perfect reconstruction can
be achieved in a stable way in a computationally efficient way even if the discretization of
the continuous representation formula is far from satisfactory. Expressed differently, the
frame operator may be far away from the identity operator but still stably invertible.
The optimal method and effective computational cost for obtaining Gabor expan-
sions of an image depends on the structure of the D sampling lattice. A (fully) separable
position-frequency lattice (PF-lattice) can be described by parameters α, α, β, β> 
such that the constants αi and βi describing the position and frequency shift parameters
are divisors of Li, respectively. The set Λ itself is given as
Λ = {(k,l) = (k, k, ℓ, ℓ) = (αu, αu, βv, βv)∣ui ∈⟨Li
αi ⟩,vi ∈⟨Li
βi ⟩},
i.e., it is a product group: Λ = Λ× Λwith Λi = αiZLi × βi ̂
ZLi.
Full separability may be violated in different ways. Assume that Λ = Λ× Λbut with
non-separable D-lattices Λi. There are at least two natural choices, whose usefulness may
depend on the concrete application. The first and probably more relevant choice is a lattice
Λin position space and Λ, another lattice, in the wave-number domain. For the case of
radial symmetric windows, g one may choose a hexagonal packing in both the spatial and
the wave-number domain.
Another flavor of separability comes in by choosing lattices within CL
and CL
,
respectively, describing the first and the second pair of phase space variables.
In passing, we note that there are also fully non-separable subgroups. They will not be
discussed here, because it is not clear whether the increased level of technicality is worth
the effort.

Gabor Analysis for Imaging 

..
Separable Atoms on Fully Separable Lattices
In this section, we will show why the case of a D separable window g = g⊗gand a fully
separable PF-lattice
Λ = Λ× Λ= αZL× β̂
ZL× αZL× β̂
ZL
allows for very efficient Gabor expansions at decent redundancy. It is crucial to observe
that in this case, it is enough to find a dual D window γfor the D window gon the
TF-lattice Λ⊴ZL× ̂
ZLand a dual D window γfor the D window gon the TF-lattice
Λ⊴ZL× ̂
ZLin order to obtain a dual D window γ for g for the lattice Λ, simply as
γ ∶= γ⊗γ. In short, the D Gabor frame on the product space CL⊗CLis obtained
by combining via tensorization the Gabor frames for the signal spaces CLand CL. The
abstract result in the background can be summarized as follows:
Lemma 
If {em}m∈⟨N⟩⊆CLand {fn}n∈⟨N⟩⊆CLare frames for CLand CL,
respectively, then the sequence {em ⊗fn}(m,n)∈⟨N⟩×⟨N⟩is a frame for CL⊗CL, where
(g ⊗h)(j, k) ∶= g(j) h(k) for g ∈CLand h ∈CL. The joint redundancy is NN
LL≥.
As our image space is a tensor product, we define D Gabor windows g ∈CL×Lby g =
g⊗gfor gi ∈CLi. As we are looking at the case where Λ = Λ× Λ, we take two Gabor
frames {g(i)
ki,ℓi}
(ki,ℓi)∈Λi ∶= {MℓiTki gi}(ki,ℓi)∈Λi ⊆CLi with frame operators Si and use the
set of products {g()
k,ℓ⊗g()
k,ℓ}
(k,l)∈Λ ⊆CL⊗CLas frame for the image space with frame
operator S⊗S.
In order to ensure the fact that this is a D Gabor family, one just has to verify that the
translation by some element in a product group, applied to a tensor product can be split
into the action of each component to the corresponding factor. Finally, the exponential law
implies that a similar splitting is valid for the modulation operators, in fact, plane waves
are themselves tensor products of pure frequencies. We thus have altogether
MℓTkg⊗MℓTkg= M(ℓ,ℓ)T(k,k)(g⊗g)
∀(k, k),(ℓ, ℓ) ∈ZL× ZL
as building blocks for our D Gabor frame.
The canonical dual of g with respect to that frame is given as
γ○= S−g = S−
g⊗S−
g= γ○
⊗γ○
.
The calculation of D dual windows for separable TF-lattices has been efficiently imple-
mented in MATLAB available from the NuHAG web-page (http://www.univie.ac.at/nuhag-
php/mmodule/resp. by Peter Søndergaards LTFAT Toolbox (linked with the above page).)
Next, let us check out how we can efficiently obtain the Gabor coefficients of an image
f ∈CL×Las given by (> .). How does the Gabor matrix Cg look likeif it is to be applied


Gabor Analysis for Imaging
to an image f ∈CL×Lstored as an L× L-matrix? For sure, f must be seen as a vector in
CLLand Cg as an NN× LL-matrix if the number of elements in the D frame is NN
and the coefficient vector is c ∈CNN. In general, f cannot be assumed to be separable,
thus the only thing simplifying our computation is the structure
ck,l = ⟨f , MℓTkg⊗MℓTkg⟩F.
If we think of the D case with some f ∈CL and a general frame {g j}j∈⟨N⟩⊆CL, the
coefficients are obtained by
c = C f = (⟨f , g j⟩)j∈⟨N⟩= (c j)j∈⟨N⟩,
and for Gabor frames, c = (ck,ℓ)(k,ℓ)∈Λ with Λ ⊴ZL × ̂
ZL is actually a coefficient matrix
in CL×L with ∣Λ∣= N ≤Lnonzero entries. But due to simply stacking the vectors
{gk,ℓ}(k,ℓ)∈Λ = {g j}j∈⟨N⟩⊆CL in the coefficient matrix
C =
⎛
⎜
⎝
g∗

⋮
g∗
N−
⎞
⎟
⎠
∈CN×L,
(.)
one just gets a “flat” c ∈CN. In our D case, the Gabor coefficient even consists of entries
ck,l = ck,k,ℓ,ℓ. We also want to take the approach by using general frames {gm}m∈⟨N⟩⊆
CLand {hn}n∈⟨N⟩⊆CL, and look at the product frame {gm ⊗hn}m,n for CL⊗CL. We
also reduce the coefficient c = (c(m, n))m,n ∈CNNto a vector of form
c = (c(,), c(,), . . ., c(, N−), c(,), . . ., c(, N−), . . .
. . ., c(N−,), . . . , c(N−, N−))
T
such that we can try to find the corresponding coefficient matrix C ∈CNN×LLthat can
be applied to f ∈CLL, where
f = (f (,), . . . , f (, L−), f (,), . . . , f (L−, L−))
T.
(.)
Now we can look at the (m, n)-th, or rather, (mN+ n)-th entry of the coefficient:
(C f )m,n = c(m, n) = ⟨f , gm ⊗hn⟩CLL
=
L−
∑
u=
L−
∑
v=
f (u,v)(gm ⊗hn)(u,v)
=
L−
∑
u=
L−
∑
v=
f (u,v) gm(u) hn(v).
(.)
Since we are now able to split the indices u and v for the frame elements, we can consider
the order in (> .) and get
(C f )m,n = (gm() h∗
n
gm() h∗
n
⋯
gm(L−) h∗
n) f = (C)m,n f ,
where (C)m,n is the (m, n)-th or (mN+ n)-th line of C and contains LLentries. The
line vectors {h∗
n}n∈⟨N⟩form the frame matrix C∈CN×Llike in (> .). If we look at

Gabor Analysis for Imaging 

the range of Nlines {(m,), . . .,(m, N−)}, we are able to express the corresponding
segment of C as
(C)m;n∈⟨N⟩= (gm() C
gm() C
⋯
gm(L−) C).
This shows that the frame matrix of the product frame is the Kronecker product of the
partial frame operators Ci ∈CNi×Li, i = ,:
C = C⊗C∈CNN×LL.
Nevertheless, we want to see whether we can compute c = (C⊗C)f in a cheaper way
by applying the frame matrices Ci without computing their Kronecker product. As images
are not stored as vectors f ∈CLLbut rather as matrices f ∈CL×Lin numerical software
like MATLAB or Octave, we could try to get the coefficient c = (c(m, n))m,n ∈CN×N
more directly.
Proposition 
Given two frames {gm}m∈⟨N⟩⊆CLand {hn}n∈⟨N⟩⊆CLwith frame
matrices Ci ∈CNi×Li, then the frame coefficient c ∈CN×Nfor the image f ∈CL×Lwith
respect to the product frame {gm ⊗hn}(m,n)is given by matrix multiplication as follows:
c = C∗f ∗CT
=
(
g()
⋯
g(L−)
⋮
⋮
⋮
gN−() ⋯gN−(L−)
)(
f (,)
⋯
f (,L−)
⋮
⋯
⋮
f (L−,) ⋯f (L−,L−))(
h()
⋯
hN−()
⋮
⋯
⋮
h(L−) ⋯hN−(L−)
)
(.)
Note that similar thoughts reveal the fact that the D-DFT of an image f ∈CL×Lcan
be obtained by the matrix multiplication
Ff = FL∗f ∗FL∈CL×L,
(.)
where FLi ∈CLi×Li are the (symmetric) Fourier matrices of order Li.
If the synthesis operation is to be done by f = C∗c for given f ∈CL and a frame
C ∈CN×L, one solution is obtained by c = (C∗)† f with a right-inverse for C∗such that IL =
SS−= C∗C(C∗C)−= C∗(C∗)†, making the pseudo-inverse of the synthesis operator
the matching analysis operator. C∗(C∗)† is the orthogonal projection onto the range of
the desired synthesis operator. One notices that due to (C∗)† = (C†)∗we already have
IL = (C†C)∗= C†C, the orthogonal projection onto the range of ran C†. Thus, the role
of the operators can be interchanged, meaning that C† is the matching synthesis operator
for the analysis operator C.
If we again interpret signals f ∈CL⊗CLas f ∈CLLand take a product frame
{gm ⊗hn}m,n with analysis operator C⊗C, we get ILL= C†(C⊗C) and ILL= IL⊗
IL= (C†
C) ⊗(C†
C), yielding that the matching synthesis operator is C† = C†
⊗C†
.
Due to Proposition , we can thus reconstruct f ∈CL×Lby
f = (C†
C)f (C†
C)
T
= C†
c (C†
)
T
(.)
because c = Cf CT
is in the range of the corresponding analysis operator.


Gabor Analysis for Imaging
These results were derived for products of general frames and therefore also hold
for products of Gabor frames. Given two Gabor frames {Mℓi Tki gi} on subgroups Λi ⊴
ZLi × ̂
ZLi and with analysis operators Cgi, we get their synthesis operators by C†
gi = C∗
γ○
i
with γ○
i ∶= S−
gi gi. The product of those two frames is the Gabor frame {MlTkg}(k,l)∈Λ×Λ
consisting of PF-shifts of the window g = g⊗g∈CL×Lon the lattice Λ = Λ× Λ. The
dual window to g is given by γ○∶= γ○
⊗γ○
. Due to (> .) and (> .), the D Gabor
analysis operation for the image f ∈CL×Lis obtained by
c = Cgf CT
g
(.)
and a possible reconstructing synthesis operation by
f = C∗
γ○
c (C∗
γ○
)
T
= Cγ○

T c Cγ○
,
(.)
yielding that it is enough to obtain the two duals γ○
i . > Figure -shows the construction
and look of the separable dual D window of a D Gaussian window on a fully separable
PF-lattice.
..
Eﬃcient Gabor Expansion by Sampled STFT
In the case of a separable D atom and a fully separable PF-lattice, we can make use of
any fast D STFT implementation (cf. the NuHAG software page, or the LTFAT toolbox
by Peter Søndergaard) to obtain the Gabor analysis coefficient c = Cgf CT
gand the Gabor
reconstruction f = C∗
γ○
c (C∗
γ○
)
T
for a given image f ∈CL×L. These matrix multiplications
from the left and right could still be rather expensive, so one can obtain the set of Gabor
coefficients c by calculating a finite number of sampled D STFTs, with the sampling points
determined by shift parameters α, αand modulation parameters β, β.
If we remember the D case, the Gabor frame Cg for CL by a window g ∈CL involves
a separable lattice Λ = αZL × βZL with ∣Λ∣= N = L
αβ , and for arbitrary f ∈CL we have
(Cg f )k,ℓ= ck,ℓ= ⟨f , MβℓTαk g⟩CL =
L−
∑
u=
f (u) MβℓTαk g(u) = Vg f (αk, βℓ)
for k ∈⟨L
α ⟩and ℓ∈⟨L
β⟩, which can be viewed as a vector of length N if the frame is
seen as a matrix Cg ∈CN×L. In the D case, if we consider f = (f, . . . , fL−) with fj ∶=
(f (, j), . . . , f (L−, j))T, then bj = Cgfj acts as the Gabor analysis operation for all
fj ∈CLwith coefficients bj ∈CNfor all j ∈⟨L⟩. The operation b = Cgf collects these
in a matrix b = (b, . . . , bL−). If we express its k-th line as a line vector qT
k ∶= (b)k =
(b(k), . . ., bL−(k)), we get
Cgf = b = qT =
⎛
⎜
⎝
qT

⋮
qT
N−
⎞
⎟
⎠
∈CN×L.

Gabor Analysis for Imaging 

0
20
a
c
40
60
red1 = 1.3333
80
100
120
0
20
40
60
80
100
120
lat1 = lattp (120,15,6)
0
20
40
60
80
100
120
140
160
0
20
40
60
80
100
120
140
160
red2 = 1.25
lat2 = lattp (160,8,16)
−60
−40
−20
0
20
40
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
g1
real(gd1)
imag(gd1)
−80
−60
−40
−20
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
g = g1.’ ∗ g2 
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
2D window g = g1Ä g2
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
Lattice
= 15
1
120
120
6
´
b
Lattice
= 8
2
160
160
16
´
Guassian g1 and its dual γ° on
1
1
f
2D dual window γ° = γ° Ä γ° on
1
1
2
2
e
gd = gd1.’ ∗ gd2 
d
g2
real(gd2)
imag(gd2)
Guassian g2 and its dual γ° on
2
2
´
⊡Fig. -
D separable window and its dual on a fully separable lattice


Gabor Analysis for Imaging
The complete D Gabor analysis operation is thus c = qTCT
g= (Cgq)T, and this is just the
Gabor analysis operation of the vectors qk ∈CLfor k ∈⟨N⟩with respect to the Gabor
frame Cg.
All in all, the D Gabor analysis operation in the twofold-separable case can be obtained
by first computing LD STFT-operations of output length Nusing the parameters α, β
followed by ND STFT-operations of output length Nusing the parameters α, β.
As the reconstruction (D Gabor expansion) is just a multiplication of the dual Gabor
matrices C∗
γ○
i from the left and right of c, this task can be seen as a sequence of D Gabor
expansions and can thus be obtained by a sequence of inverse D STFT-operations as well.
There are again two ways: The first one is to do Ninverse operations with output length L
using the parameters α, βfollowed by Noperations with output length Lusing α, β.
The second way exchanges Li and Ni correspondingly.
..
Visualizing a Sampled STFT of an Image
So far we have visualized the full STFT of an image as a large block image, where either each
block fully represents the frequency domain and the position of the blocks the position
domain, or vice versa. As such an image would become rather huge, we prefer to visual-
ize only a sampled STFT instead. In the case of a separable atom, this can be realized by
obtaining the discrete D Gabor transform by (> .), where the two involved matrices
Cgi consider a special order of their Gabor frame elements Mℓi Tki gi.
For a Gabor frame {MℓTkg}(k,ℓ)∈Λ ⊆CL given by a D window g ∈CL on a separable
lattice Λ = αZL × βZL with N = ∣Λ∣= L
αβ elements, we say that the Gabor frame elements
are ordered by modulation priority if the frame matrix Cg ∈CN×L is of the form
Cg =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜
⎝
MTg∗
MβTg∗
⋮
ML/β−Tg∗
MTg∗
⋮
ML/β−Tg∗
⋮
ML/β−TL/α−g∗
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟
⎠

Gabor Analysis for Imaging 

We call it ordered by translation priority if is of the form
̃Cg =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜
⎝
MTg∗
MTα g∗
⋮
MTL/α−g∗
MTg∗
⋮
MTL/α−g∗
⋮
ML/β−TL/α−g∗
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟
⎠
Obviously ̃Cg = PCg for a suitable permutation matrix P ∈CN×N.
If we take an image f ∈CL×Land two Gabor frames {Mℓi Tki gi}, (ki, ℓi) ∈Λi, on
separable lattices Λi = αiZLi × βiZLi, we can take their product Gabor frame for CL×L
and obtain the mentioned two possibilities for an STFT block image by either considering
the frame matrices Cgi or ̃Cgi. The matrices Cgi are ordered by modulation priority, and if
c = Cgf CT
g, then c consists of L
β× L
β-blocks
Xk,k∶= (⟨f , M(ℓ,ℓ)T(k,k)g⟩)ℓ,ℓ
such that
c =
⎛
⎜
⎝
X,
⋯
X,L/α−
⋮
⋯
⋮
XL/α−,
⋯
XL/α−,L/α−
⎞
⎟
⎠
.
The blocks Xk,kequal the part (Vg f (k, k, ℓ, ℓ))ℓ,ℓof the sampled STFT and thus con-
tain the whole (sampled) set of frequency shifts for a certain position shift of the window
g = g⊗g. The (sampled) frequency domain is therefore spanned in each of the blocks
Xk,k, and their positions in c span the (sampled) position domain. Each Xk,kcould be
seen as a sampled “Fourier image” of the discrete Fourier transform̂f ⋅T(k,k)¯g.
In the other case, where we have ˜c = ̃Cgf ̃CT
g, the Gabor coefficient consists of L
α×
L
α-blocks
Yℓ,ℓ∶= (⟨f , M(ℓ,ℓ)T(k,k)g⟩)k,k
such that
˜c =
⎛
⎜
⎝
Y,
⋯
Y,L/β−
⋮
⋯
⋯
YL/β−,
⋯
YL/β−,L/β−
⎞
⎟
⎠
.
Here, the blocks Yℓ,ℓequal the part (Vg f (k, k, ℓ, ℓ))k,kof the sampled STFT and con-
tain the corresponding set of position shifts for a certain frequency-shift of g. The position
domain is spanned in each of the blocks Yℓ,ℓ, and their positions in ˜c span the frequency
domain.


Gabor Analysis for Imaging
> Figures -and
> -show examples for both cases using the zebra test image.
As it is a square image, we can take g= gand thus Cg= Cg. The first figure composes
the Gabor transform coefficient matrix as blocks of Fourier images. Clearly, the overall
image reflects the shape of the zebra. The “pixels” of that image contain “Fourier jets”
that are orthogonal to the edges at the corresponding position in the original zebra image.
Thus, the “jets” are oriented horizontally where, e.g., the body of the animal shows vertical
line patterns. The second figure shows blocks of zebra images that have been convolved
with modulated Gaussians. The absolute values show the peaks as black spots within the
respective image blocks.
500
1000
1500
2000
2500
3000
3500
500
1000
1500
2000
2500
3000
3500
⊡Fig. -
Discrete D Gabor transform of a zebra, modulation priority. The picture shows the absolute
values of c = CgfCT
g, where g is the D Gaussian of length and Cg is the Gabor matrix for
the lattice Λ = Z× Z, whose entries were ordered with modulation priority

Gabor Analysis for Imaging 

−1500
−1000
−500
0
500
1000
1500
−1500
−1000
−500
0
500
1000
1500
⊡Fig. -
Discrete D Gabor transform of a zebra, translation priority. The picture shows the absolute
values of
∼c =
∼
Cgf
∼
CT
g, where g is the D Gaussian of length and
∼
Cg is the Gabor matrix for
the lattice Λ = Z× Z, whose entries were ordered with translation priority. The
Gaussian blurred image in the middle has been scaled into the colormap individually
..
Non-Separable Atoms on Fully Separable Lattices
Non-separable windows are those that can only be defined considering the complete image
domain ZL× ZL, and not ZLand ZLseparately. These cannot be described as a tensor
product g⊗gwith gi ∈CLi anymore, but only generally as g ∈CL×L. With this case
we lose the ability to consider two (D) frames independently for each dimension and we
cannot apply two frame matrices independently to an image. It appears that we have to stick
to the known factorizations of Gabor matrices on (fully) separable lattices with parameters
αi, βi, and we thus cannot make use of the equidistantly sampled D STFT. However, under
certain conditions, this case can be completely referred to a D case, as we will see below.


Gabor Analysis for Imaging
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
Non-separable window g
a
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
Dual at vertically critical redundancy
d
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
b
1
Dual γ° on
´
2
−80 −60 −40 −20
0
20
40
60
80
−60
−40
−20
0
20
40
60
c
parameters
Dual on
with exchange
1 ´
2
¢
¢
⊡Fig. -
A non-separable window and some duals on fully separable lattices. The lattices Λi are that
of > Fig. -. The lattices Λ′
i exchange αi with βi. The last lattice has vertical redundancy 
and horizontal redundancy .
> Figure -indicates an important thing about the redundancy. Sure, a redundancy
of ∣Λ∣
LL≥is only a necessary condition, but it seems to be important to consider the redun-
dancy in each dimension. The involved window is a D Gaussian window g ∈C×,
stretched vertically by 
, shrunken horizontally by 
, then rotated (counter-clockwise) by

π. > Subfigure -d shows its dual on a fully separable D PF-lattice with overall redun-
dancy .. It was computed in the work of P. Prinz which makes use of the Gabor matrix
factorizations ([]). But although the redundancy value gives the impression to be safe,
it hides the fact that the involved lattice is actually Λ = Z× Z× Z× Z,
yielding the redundancy as

⋅⋅
⋅= ⋅.. This shows that the vertical redundancy
is critical, and the dual has a bad localization in the vertical dimension. It is therefore
necessary to make sure that the redundancy is reasonably distributed among the dimen-
sions. In this sense, fully separable D lattices can always be considered as a product of two
D TF-lattices with independent redundancies, no matter what structure the D window
possesses.

Gabor Analysis for Imaging 

.
Historical Notes and Hint to the Literature
Nonorthogonal expansions as proposed by D. Gabor in his seminal work [] of were
ignored for a long time by the mathematical community. The question, to which extent
the claims made by D. Gabor could be realized in the context of generalized functions
was carefully analyzed by A.J.E.M. Janssen in []). Around the same time M. Basti-
aans explored the connections between Gabor theory and optics ([–]). In the critically
sampled case, he suggested to use the biorthogonal function γ in order to calculate Gabor
coefficients. The connection to the biorthogonality relations for dual Gabor windows was
pointed out in two papers in ([, ]) and brought to the multidimensional case in
[, , ].
Two early papers in the field, authored by J. Daugman and Y.Y. Zeevi and his
coauthors established a connection between a D version of Gabor analysis and early
vision ([, , , , ]), Various subsequent papers emphasized that a Gabor fam-
ily is not an orthogonal system, and that, therefore, computation of coefficients has
to be computationally expensive. We know by now that while linear independence is
indeed lost, the rich covariance structure of the Gabor problems actually lead to efficient
algorithms.
The mathematical theory of Gabor expansions was promoted in various directions in
the last decades. Although a lot of Gabor analysis is naturally valid in the context of
general locally compact Abelian groups, a substantial body of references only covers the
standard case, for D signals and separable lattices.
Of course, the theory underlying image processing is formally covered by the theory
of Gabor analysis over finite Abelian groups as described in []. Some basic facts in the
general LCA context are given in [] and some further results generalize to this setting,
applying standard facts from abstract harmonic analysis ([]).
Multidimensional, non-separable lattices are discussed in [], and [] deals with sit-
uations where the isomorphism of D groups with certain D groups helps to use D Gabor
code to calculate D dual Gabor windows.
Numerical methods for Gabor representations have been discussed since the first and
pioneering papers (see e.g., [, , ]). There are also hints how to perform parallel ver-
sions of the Gabor transform ([]). A partial comparison of algorithms is in [] and in
the toolbox of P. Søndergaard. It can be expected to provide further implementations and
more details concerning numerical issues in the near future.
One of the most natural applications (based on the interpretation of Gabor coefficients)
are space-variant filters. Given the Gabor transform one can multiply them with a /func-
tion over the coefficient domain, passing through, e.g., higher frequencies within regions
of interest, whereas otherwise only low frequencies are stored, thus representing foveated
images (with somewhat blurred parts outside the region of interest).
Since different textures in different regions of an image might also be detected using
Gabor coefficients, natural applications are texture segmentation (see e.g., [, ]), image


Gabor Analysis for Imaging
restoration ([, ]), and image fusion ([]). The extraction of directional features in
images has been considered recently in []. Other contributions to texture analysis are
found in []. Other applications are pattern recognition ([]), face identification as
described in [], and face detection ([]).
Some of the material presented in this paper can be found in an extended form in the
master thesis of the last named author ([]).
References and Further Reading
. Ali ST,AntoineJ-P,Murenzi R,Vandergheynst P
() Two-dimensional wavelets and their rel-
atives. Cambridge University Press, Cambridge
. Assaleh K, Zeevi Y, Gertner I () on the real-
ization of Zak-Gabor representation of images.
SPIE :–
. Bastiaans M () Application of the Wigner
distribution function to partially coherent light.
J Opt Soc Am ():–
. Bastiaans MJ () Gabor’s expansion of a sig-
nal into Gaussian elementary signals. Proc IEEE
():–
. Bastiaans MJ () A sampling theorem for the
complex spectrogram and Gabor’s expansion of
a signal in Gaussian elementary signals. Opt Eng
():–
. Bastiaans MJ () On the sliding-window rep-
resentation in digital signal processing. IEEE
Trans Acoust Speech Signal Process ():
–
. Bastiaans MJ () Gabor’s signal expansion
in optics. In: Feichtinger HG, Strohmer T (eds)
Gabor analysis and algorithms: theory and
applications. Birkhäuser, Boston, pp –,
Appl. Numer. Harmon. Anal
. Bastiaans MJ, van Leest AJ () From the rect-
angular to the quincunx Gabor lattice via frac-
tional Fourier transformation. IEEE Signal Proc
Lett ():–
. Bastiaans MJ, van Leest AJ () Product forms
in Gabor analysis for a quincunx-type sampling
geometry. In: Veen J (ed) Proceedings of the
CSSP-, ProRISC/IEEE workshop on circuits,
systems and signal processing, Mierlo, –
November . STW, Technology Foundation,
Utrecht, pp –
. Battle G () Heisenberg proof of the Balian-
Low theorem. Lett Math Phys ():–
. Ben Arie J, Rao KR () Nonorthogonal signal
representation by Gaussians and Gabor func-
tions. IEEE Trans Circuits-II ():–
. Ben Arie J, Wang Z () Gabor kernels
for
affine-invariant
object
recognition.
In:
Feichtinger HG, Strohmer T (eds) Gabor anal-
ysis and algorithms: theory and applications.
Birkhauser, Boston
. Bölcskei H, Feichtinger HG, Gröchenig K,
Hlawatsch
F
()
Discrete-time
multi-
window Wilson expansions: pseudo frames,
filter banks, and lapped transforms. In: Proceed-
ings of the IEEE-SP international symposium
on time-frequency and time-scale analysis,
Paris, pp –
. Bölcskei
H,
Gröchenig
K,
Hlawatsch
F,
Feichtinger HG () Oversampled Wilson
expansions. IEEE Signal Proc Lett ():–
. Bölcskei H, Janssen AJEM() Gabor frames,
unimodularity, and window decay. J Fourier
Anal Appl ():–
. ChristensenO () Anintroductionto frames
and Riesz bases. Applied and numerical har-
monic analysis. Birkhäuser, Boston
. Christensen O () Frames and bases: an
introductory course. Applied and numerical
harmonic analysis. Birkhäuser, Basel
. Coifman RR, Matviyenko G, Meyer Y ()
Modulated Malvar-Wilson bases. Appl Comput
Harmon Anal ():–
. G. Cristobal and R. Navarro. Blind and adaptive
image restoration in the framework of a multi-
scale Gabor representation. In Time-frequency
and time-scale analysis, ., Proceedings of
theIEEE-SPInternational Symposiumon,pages
–, Oct .
. Cristobal G, Navarro R () Space and fre-
quency variant image enhancement based on

Gabor Analysis for Imaging 

a Gabor representation. Pattern Recognit Lett
():–
. Cvetkovic Z, Vetterli M () Oversam-
pled filter banks. IEEE Trans Signal Process
():–
. Daubechies I, Grossmann A, Meyer Y ()
Painless nonorthogonal expansions. J Math
Phys ():–
. Daubechies I () Time-frequency local-
ization operators: a geometric phase space
approach. IEEE Trans Inf Theory ():–
. Daubechies I () The wavelet transform,
time-frequency localization and signal analysis.
IEEE Trans Inf Theory ():–
. Daubechies I, Jaffard S, Journé JL () A sim-
ple Wilson orthonormal basis with exponential
decay. SIAM J Math Anal :–
. Daubechies I, Landau HJ, Landau Z ()
Gabor time-frequency lattices and the Wexler-
Raz identity. J Fourier Anal Appl ():–
. Daugman JG () Complete discrete -D
Gabor transforms by neural networks for image
analysis and compression. IEEE Trans Acoust
Speech Signal Process ():–
. Dubiner Z, Porat () Position-variant fil-
tering in the positionfrequency space: perfor-
mance analysis and filter design. pp –
. Dufaux F, Ebrahimi T, Geurtz A, Kunt M ()
Coding of digital TV by motion-compensated
Gabor decomposition. In: Tescher AG (ed)
Applications of Digital Image Processing XIV,
Image Compression, Proc. SPIE, July , vol
. SPIE, pp –
. Dufaux F, Ebrahimi T, Kunt M () Massively
parallel implementation for real-time Gabor
decomposition. In: Tzou K-H, Koga T (eds)
Visual communications and image process-
ing ’: image processing, Boston, vol of
VLSI implementation and hardware architec-
tures. SPIE, pp –
. Dunn D, Higgins WE () Optimal Gabor fil-
ters for texture segmentation. IEEE Trans Image
Process ():–
. Ebrahimi T, Kunt M () Image compres-
sion by Gabor expansion. Opt Eng ():
–
. Ebrahimi T, Reed TR, Kunt M () Video cod-
ing using a pyramidal gabor expansion. In: Pro-
ceedings of visual communications and image
processing ’, vol . SPIE, pp –
. Feichtinger HG () Modulation spaces:
looking back and ahead. Sampl Theory Signal
Image Process ():–
. Feichtinger HG, Gröchenig K () Theory and
practice of irregular sampling. In: Benedetto
J, Frazier M (eds) Wavelets: mathematics and
applications, studies in advanced mathematics.
CRC Press, Boca Raton, pp –
. Feichtinger HG, Gröchenig K () Banach
spaces related to integrable group represen-
tations and their atomic decompositions, I. J
Funct Anal :–
. Feichtinger HG,GröchenigK,Walnut DF ()
Wilson bases and modulation spaces. Math
Nachr :–
. Feichtinger HG, Kaiblinger N () D-Gabor
analysis based on D algorithms. In: Proceed-
ings of the OEAGM-, Hallstatt
. Feichtinger HG, Kozek W, Prinz P, Strohmer
T () On multidimensional non-separable
Gabor expansions. In: Proceedings of the SPIE:
wavelet applications in signal and image pro-
cessing IV
. Feichtinger HG, Kozek W () Quantization
of TF lattice-invariant operators on elementary
LCA groups. In: Feichtinger HG, Strohmer T
(eds) Gabor analysis and algorithms. Theory
and applications. Applied and numerical har-
monic analysis. Birkhäuser, Boston, pp –,
–
. Feichtinger HG, Kozek W, Luef F () Gabor
analysis over finite Abelian groups. Appl Com-
put Harmon Anal :–
. Feichtinger HG, Luef F, Werther T () A
guided tour from linear algebra to the founda-
tions of Gabor analysis. In: Gabor and wavelet
frames, vol of Lecture notes series, Insti-
tute for Mathematical Sciences, National Uni-
versity of Singapore. World Scientific, Hacken-
sack, pp –
. Feichtinger HG, Strohmer T, Christensen O
() A grouptheoretical approach to Gabor
analysis. Opt Eng :–
. Folland GB () Harmonic analysis in phase
space. Princeton University Press, Princeton
. Gabor D () Theory Commun J IEE
():–
. Gertner I, Zeevi YY () Image representa-
tion with position-frequency localization. In:
Acoustics, speech, and signal processing, .


Gabor Analysis for Imaging
ICASSP-, international conference on, vol ,
pp –
. Golub G, van Loan CF () Matrix computa-
tions, rd edn. Johns Hopkins University Press,
Baltimore
. Grafakos L, Sansing C () Gabor frames and
directional time frequency analysis. Appl Com-
put Harmon Anal ():–
. Grigorescu S, Petkov N, Kruizinga P ()
Comparison of texture features based on Gabor
filters.IEEETransImageProcess():–
. Gröchenig K () Aspects of Gabor anal-
ysis on locally compact abelian groups. In:
Feichtinger HG, Strohmer T (eds) Gabor anal-
ysis and algorithms: theory and applications.
Birkhäuser, Boston, pp –
. Gröchenig K () Foundations of time-
frequency analysis. Birkhäuser, Boston, Appl.
Numer. Harmon. Anal
. Hoffmann U, Naruniec J, Yazdani A, Ebrahimi
T () Face detection using discrete Gabor
jets and a probabilistic model of colored image
patches. In: Filipe J, Obaidat MS (eds) E-
business and telecommunications, ICETE ,
–July, revised selected papers, vol of
communications in computer and information
science. pp –
. Janssen AJEM () Gabor representation of
generalized functions. J Math Anal Appl :
–
. Janssen AJEM () Duality and biorthogonal-
ity for Weyl-Heisenberg frames. J Fourier Anal
Appl ():–
. Janssen AJEM () From continuous to dis-
crete Weyl-Heisenberg frames through sam-
pling. J Fourier Anal Appl ():–
. G. Kutyniok and T. Strohmer. Wilson bases for
general time-frequency lattices. SIAM J. Math.
Anal., ():–(electronic), .
. Lee TS () Image representation using D
Gabor wavelets. IEEE Trans Pattern Anal Mach
Intell ():–
. Li S () Discrete multi-Gabor expansions.
IEEE Trans Inf Theory ():–
. Lu Y, Morris J () Fast computation of
Gabor functions. Signal Processing Letters,
IEEE ():–
. Malvar HS () Lapped transforms for effi-
cient transform/subband coding. IEEE Trans
Acoust Speech Signal Process ():–
. Navarro R, Portilla J, Tabernero A () Dual-
ity between oveatization and multiscale local
spectrum estimation. In: Rogowitz BE, Pappas
TN (eds) Human vision and electronic imaging
III, San Jose, January , vol of Proc.
SPIE. SPIE, Bellingham, pp –
. Nestares O, Navarro R, Portilla J, Tabernero A
() Efficient spatial-domain implementation
of a multiscale image representation based on
Gabor functions. J Electron Imaging ():–

. Paukner S () Foundations of Gabor analysis
for image processing. Master’s thesis, University
of Vienna
. Porat M, Zeevi Y () The generalized Gabor
scheme of image representation in biological
and machine vision. IEEE Trans Pattern Anal
Mach Intell ():–
. Porat
M,
Zeevi
YY
()
Gram-Gabor
approach to optimal image representation. In:
Kunt M (ed) Visual communications and image
processing ’: fifth in a series, Proc SPIE,
Lausanne, vol . SPIE, pp –
. Prinz P () Calculating the dual Gabor win-
dow for general sampling sets. IEEE Trans Sig-
nal Process ():–
. Redding N, Newsam G () Efficient calcu-
lation of finite Gabor transforms. IEEE Trans
Signal Process ():–
. Redondo R, Sroubek F, Fischer S, Cristobal G
() Multifocus image fusion using the log-
Gabor transformandaMultisizeWindowstech-
nique. Inf Fusion ():–
. Ron A, Shen Z () Weyl-Heisenberg frames
and Riesz bases in L(Rd). Duke Math J
():–
. Shen L, Bai L, Fairhurst M () Gabor
wavelets and general discriminant analysis for
face identification and verification. Image Vis
Comput ():–
. Søndergaard PL () Finite discrete Gabor
analysis. PhD thesis, Technical University of
Denmark
. Strohmer T () Numerical algorithms for
discrete Gabor expansions. In: Feichtinger HG,
Strohmer T (eds) Gabor analysis and algo-
rithms: theory and applications. Birkhäuser,
Boston, pp –
. Subbanna NK, Zeevi YY () Image rep-
resentation using noncanonical discrete multi-

Gabor Analysis for Imaging 

window Gabor frames. In: Visual information
engineering, VIE . IET International con-
ference on publication, pp –
. Urieli S, Porat M, Cohen N () Optimal
reconstruction of images from localized phase.
IEEE Trans Image Process ():–
. Vargas A, Campos J, Navarro R () An appli-
cation of the Gabor multiscale decomposition
of an image to pattern recognition. SPIE :
–
. van Leest AJ, Bastiaans MJ () Gabor’s signal
expansion and the Gabor transform on a non-
separable time-frequency lattice. J Franklin Inst
():–
. Weldon T, Higgins W, Dunn D () Efficient
Gabor filter design for texture segmentation.
Pattern Recognit ():–
. Werner D () Funktionalanalysis. (Func-
tional Analysis) ., Überarb. Au. Springer, Berlin
. Wojdyllo P () Modified Wilson orthonor-
mal bases. Sampl Theory Signal Image Process
():–
. Wojdyllo P () Characterization of Wilson
systems for general lattices. Int J Wavelets Mul-
tiresolut Inf Process ():–
. Wojtaszczyk P () Stability and instance
optimality for Gaussian measurements in com-
pressed sensing. Found Comput Math :–
. Zeevi YY () Multiwindow Gabor-type rep-
resentations and signal representation by partial
information. In: Byrnes JS (ed) Twentieth
century Harmonic analysis – a celebration
proceedings of the NATO Advanced Study
Institute, II Ciocco, –July , vol of
NATO Sci Ser II. Math Phys Chem, Kluwer,
Dordrecht, pp –
. Zeevi YY, Zibulski M, Porat M () Multi-
window Gabor schemes in signal and image
representations. In: Feichtinger HG, Strohmer
T (eds) Gabor analysis and algorithms: theory
and applications. Birkhäuser, Boston, pp –
, Appl. Numer. Harmon. Anal
. Yang J, Liu L, Jiang T, Fan Y () A mod-
ified Gabor filter design method for finger-
print image enhancement. Pattern Recognit Lett
():–
. Zibulski M, Zeevi Y () Matrix algebra
approach to Gabortype image representation.
In: Haskell BG, Hang H-M (eds) Visual com-
munications and image processing ’, wavelet,
Proc. SPIE, vol , November , SPIE.
pp –
. Zibulski M, Zeevi YY () Frame analysis of
the discrete Gaborscheme. IEEE Trans Signal
Process ():–
. Zibulski M, Zeevi YY () Analysis of multi-
windowGabor-typeschemesbyframemethods.
Appl Comput Harmon Anal ():–
. Zibulski M, Zeevi YY () Discrete multiwin-
dow Gabor-type transforms. IEEE Trans Signal
Process ():–
. Zibulski M, Zeevi YY () The generalized
Gabor scheme and its application in signal and
image representation. In: Signal and image rep-
resentation in combined spaces, vol of wavelet
Anal Appl. Academic, San Diego, pp –


Shape Spaces
Alain Trouvé ⋅Laurent Younes
.
Introduction.....................................................................
.
Background......................................................................
.
Mathematical Modeling and Analysis.........................................
..
Some Notation..........................................................................
..
A Riemannian Manifold of Deformable Landmarks..............................
...Interpolating Splines and RKHSs....................................................
...Riemannian Structure.................................................................
...Geodesic Equation......................................................................
...Metric Distortion and Curvature.....................................................
...Invariance................................................................................
..
Hamiltonian Point of View...........................................................
...General Principles......................................................................
...Application to Geodesics in a Riemannian Manifold.............................
...Momentum Map and Conserved Quantities.......................................
...Euler–Poincaré Equation..............................................................
...A Note on Left Actions................................................................
...Application to the Group of Diffeomorphisms....................................
...Reduction via a Submersion..........................................................
...Reduction: Quotient Spaces..........................................................
...Reduction: Transitive Group Action.................................................
..
Spaces of Plane Curves................................................................
...Introduction and Notation............................................................
...Some Simple Distances................................................................
...Riemannian Metrics on Curves......................................................
...Projecting the Action of D Diffeomorphisms....................................
..
Extension to More General Shape Spaces..........................................
..
Applications to Statistics on Shape Spaces.........................................
.
Numerical Methods and Case Examples......................................
..
Landmark Matching via Shooting....................................................
..
Landmark Matching via Path Optimization.......................................
..
Computing Geodesics Between Curves............................................
..
Inexact Matching and Optimal Control Formulation............................
...Inexact Matching.......................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Shape Spaces
...Optimal Control Formulation........................................................
...Gradient w.r.t. the Control............................................................
...Application to the Landmark Case..................................................
.
Conclusion......................................................................
.
Cross-References................................................................

Shape Spaces 

Abstract: This chapter describes a selection of models that have been used to build Rie-
mannian spaces of shapes. It starts with a discussion of the finite dimensional space of
point sets (or landmarks) and then provides an introduction to the more challenging issue
of building spaces of shapes represented as plane curves. A special attention is devoted
to constructions involving quotient spaces, since they are involved in the definition of
shape spaces via the action of groups of diffeomorphisms and in the process of identify-
ing shapes that can be related by a Euclidean transformation. The resulting structure is first
described via the geometric concept of a Riemannian submersion and then reinterpreted in
a Hamiltonian and optimal control framework, via momentum maps. These developments
are followed by the description of algorithms and illustrated by numerical experiments.
.
Introduction
The analysis of shapes as mathematical objects have constituted a significant area of
interest in the past few decades motivated by the development of image acquisition meth-
ods and segmentation algorithms, in which shapes could be extracted as isolated objects.
Shape analysis is a framework, in which a given shape is considered as a single (typically
infinite dimensional) variable, requiring the development of new techniques for their rep-
resentation and statistical interpretation. This framework has found applications in several
fields, including object recognition in computer vision and computational anatomy.
The example in
> Fig. -can help framing the kind of problems that are being
addressed, and serve as a motivation. These shapes are fairly easily recognizable for the
human eye. They do however exhibit large variations, and a description in simple terms of
how they vary, and of how they can be compared is a much harder task. It is clear that a naive
representation, like a list of points, cannot be used directly, because the discretized curves
may have different numbers of points, and no correspondence is available between them.
Coming up with quantitative and reliable descriptors that can be, for example, analyzed in
a rigorous statistical study is, however, of main importance for the many applications, and
the goal of this chapter is to provide a framework in which such a task can be performed
in a reliable well-posed way.
⊡Fig. -
Examples of shapes (taken from the MPEG-shape database)


Shape Spaces
.
Background
During the past decades, several essential contributions have been made, using rigorous
mathematical concepts and methods, to address this problem and others of similar nature.
This collection of efforts has progressively defined a new discipline that can be called
mathematical shape theory.
Probably, the first milestone in the development of the theory is Kendall’s construc-
tion of a space of shapes, defined as a quotient of the space of disjoint points in Rd by
the action of translation, rotation, and scaling []. Kendall’s theory has been the starting
point of a huge literature [,,] and allowed for new approaches for studying datasets
in which the group of similitudes was a nuisance factor (for such data as human skulls,
prehistoric jewelry, etc.). One can argue that, as a candidate for a shape space, Kendall’s
model suffers from two main limitations. First, it relies on the representation of a shape by
a finite number of labeled points, or landmarks. These landmarks need to have been iden-
tified on each shape, and shapes with different numbers of landmarks belong to different
spaces. From a practical point of view, landmarks are most of the time manually selected,
the indexation of large datasets being time consuming and prone to user-dependent errors.
The second limitation is that the metric on shapes is obtained by quotienting out the stan-
dard Euclidean metric on point sets, using a standard “Riemannian submersion” process
that we will discuss later in this chapter. The Euclidean metric ignores a desirable property
of shape comparison, which states that shapes that are smooth deformations of one another
should be considered more similar than those for which the points in correspondence are
randomly displaced, even if the total point displacement is the same.
This important issue, related to smoothness, was partially addressed by another impor-
tant contribution to the theory, which is Bookstein’s use of the thin plate splines originally
developed by Duchon and Meinguet [, , ]. Splines interpolate between landmark
displacements to obtain a smooth, dense, displacement field (or vector field). It can be
addressed with the generic point of view of Reproducing Kernel Hilbert Spaces [, ],
which will also be reviewed later in this chapter.
This work had a tremendous influence on shape analysis based on landmarks, in par-
ticular for medical studies. It suffers, however, from two major drawbacks. The first one
is that the interpolated displacement can be ambiguous, with several points moved to
the same position. This is an important limitation, since inferring unobserved correspon-
dences is one of the objectives of this method. The second drawback, in relation with the
subject of this chapter, is that the linear construction associated to splines fails to provide
a metric structure on the nonlinear space of shapes. The spline deformation energy pro-
vides in fact a first-order approximation of a non-constant Riemannian metric on point
sets, which provides an interesting version of a manifold of landmarks, as introduced
in [,,].
After point sets, plane curves is certainly the shape representation in which the most
significant advances have been observed over the last few years. Several important metrics
have been discussed in publications like [, , , , –]. They have been cataloged,

Shape Spaces 

among many other metrics, in a quasiencyclopedic effort by D. Mumford and P. Michor
[]. We will return to some of these metrics in the > Sect. ...
Grenander’s theory of deformable templates [] is another seminal work for shape
spaces. In a nutshell, Grenander’s basic idea, which can be traced back to D’Arcy Thomson’s
work on biological shapes in the beginning of last century [], is to introduce suitable
group actions as generative engines for visual object models, with the natural use of the
group of diffeomorphisms for shapes. While the first developments in this context use lin-
ear approximations of diffeomorphisms [, , ], a first computational breakthrough in
the nonlinear estimation of diffeomorphisms was provided in [] with the introduction
of flows associated to ordinary differential equations. This idea was further developed in
a fully metric approach of diffeomorphisms and shape spaces, in a framework that was
introduced in [, , ] and further developed in [, , , , , ]. The approach also
led to important developments in medical imaging, notably via the establishment of a new
discipline, called computational anatomy, dedicated to the study of datasets of anatomical
shapes [,,,].
.
Mathematical Modeling and Analysis
..
Some Notation
The following notation will be used in this chapter. The Euclidean norm of vectors a ∈Rd
will be denoted using single bars and the dot product between a and b as a ⋅b or explicitly
as aTb, where aT is the transpose of a. So
∣a∣= a ⋅a = aTa
for a ∈Rd.
Other norms (either Riemannian metrics or norms on infinite dimensional spaces)
will be denoted with double bars, generally with a subscript indicating the corresponding
space, or relevant point in the manifold. We will use angles for the corresponding inner
product, with the same index, so that, for a Hilbert space V, the notation for the inner
product between v and w in V will be ⟨v , w⟩V with
∥v∥
V = ⟨v , v⟩V.
When f is a function that depends on a variable t, its derivative with respect to t com-
puted at some point twill be denoted either ∂t f (t) or ˙ft(t), depending on which form
gives the most readable formula. Primes are never used to denote derivative, that is, f ′ is
not the derivative of f , but just another function. The differential at x of a function of sev-
eral variables F is denoted DF(x). If F is scalar valued, its gradient is denoted ∇F(x). The
divergence of a vector field v : Rd →Rd is denoted ∇⋅v.


Shape Spaces
If M is a differential manifold, the tangent space to M at x ∈M will be denoted TxM
and its cotangent space (dual of the former) T∗
x M. The tangent bundle (disjoint union of
the tangent spaces) is denoted TM and the cotangent bundle T∗M.
When μ is a linear form on a vector space V (i.e., a scalar-valued linear transformation),
the natural pairing between μ and v ∈V will be denoted (μ∣v), that is,
(μ∣v) = μ(v).
..
A Riemannian Manifold of Deformable Landmarks
...
Interpolating Splines and RKHSs
Let us start with some preliminary facts on Hilbert spaces of functions or vector fields and
their relation with interpolating splines. A Hilbert space is a possibly infinite dimensional
vector space equipped with an inner product which induces a complete topology. Letting
V be such a space, with norm and inner product respectively denoted ∥⋅∥V and ⟨⋅, ⋅⟩V,
a linear form on V is a continuous linear transformation μ : V ↦R. The set of such
transformations is called the dual space of V and denoted V ∗. An element μ in V∗being
continuous by definition, there exists a constant C such that
∀v ∈V, μ(v) ≤C∥v∥V.
The smaller number C for which this assertion is true is called the operator norm of μ and
denoted ∥μ∥V∗.
Instead of μ(v) like above, the notation (μ∣v) will be used to represent the result of μ
applied to v. The Riesz representation theorem implies that V ∗is in one-to-one correspon-
dence with V, so that for any μ in V ∗there exists a unique element v = KV μ ∈V such that,
for any w ∈V,
(μ∣w) = ⟨KV μ , w⟩V;
KV and its inverse LV = K−
V are called the duality operators of V. They provide an
isometric identification between V and V∗, with, in particular, ∥μ∥
V∗= (μ∣KV μ) =
∥KV μ∥
V.
Of particular interest is the case when V is a space of vector fields in d dimensions, that
is of functions v : Rd →Rd (or from Ω →Rd where Ω is an open subset of Rd), and when
the norm in V is such that the evaluation functionals a⊗δx belong to V ∗for any a, x ∈Rd,
where
(a ⊗δx∣v) = aTv(x),v ∈V.
(.)
In this case, the vector field KV(a ⊗δx) is well defined and linear in a. One can define the
matrix-valued function (y, x) ↦˜KV(y, x) by
˜KV(y, x)a = (KV(a ⊗δx))(y);

Shape Spaces 

˜KV is the kernel of the space V. In the following, we will write KV (x, y) instead of ˜KV(x, y),
with the customary abuse of notation of identifying the kernel and the operator that it
defines.
One can easily deduce from its definition that KV satisfies the reproducing property
∀a, b ∈Rd,⟨KV(⋅, x)a , KV(⋅, y)b⟩V = aTKV(x, y)b,
which also implies the symmetry property KV(x, y) = KV(y, x)T. Unless otherwise speci-
fied, it will always be assumed that V is a space of vector fields that vanish at infinity, which
implies the same property for the kernel (one variable tending to infinity and the other
remaining fixed).
A space V as considered above is called a reproducing kernel Hilbert space (RKHS) of
vector fields. Fixing such a space, one can consider the spline interpolation problem, which
is to find v ∈V with minimal norm such that v(xi) = ci, where x, . . ., xN are points in Rd
and c, . . . , cN are d-dimensional vectors. It is quite easy to prove that the solution takes
the form
v(y) =
N
∑
i=
KV(y, xi)αi,
(.)
where α, . . . , αN are identified by solving the dN-dimensional system
N
∑
i=
KV(xj, xi)αi = c j, for j = , . . . , N.
(.)
Let SV(x) (where x = (x, . . ., xN)) denote the dN by dN block matrix
SV(x) = (KV(xi, xj))i,j=,...,N.
Stacking c, . . ., cN and α, . . . , αN in dN-dimensional column vectors c and α, one can
show that, for the optimal v:
∥v∥
V = αTSV(x)α = cTS(x)−
V c,
(.)
each term representing this spline deformation energy for the considered interpolation
problem.
How one uses this interpolation method now depends on how one interprets the vector
field v. One possibility is to consider it as a displacement field, in the sense that a particle
at position x in space is moved to position x + v(x), therefore involving the space trans-
formation φv := id + v. In this view, the interpolation problem can be rephrased as finding
the smoothest (in the V-norm sense) full space interpolation of given landmark displace-
ments. The deformation energy in (> .) can then be interpreted as some kind of “elastic”
energy that evaluates the total stress involved in the transformation φv. This (with some
variants, including allowing for some no-cost affine, or polynomial, transformations) is the
framework of interpolation based on thin plates, or radial basis functions, as introduced
in [,,,,] for example. As discussed in the introduction, this approach does not lead
to a nice mathematical notion of a shape space of landmarks; moreover, in the presence


Shape Spaces
of large displacements, the interpolated transformation φv may fail to be one to one and
therefore to provide a well-defined dense correspondence.
The other way to interpret v is as a velocity field, so that v(x) is the speed of a particle at
x at a given time. The interpolation problem is then to obtain a smooth velocity field given
the speeds c, . . . , cN of particles x, . . ., xN. This point of view has the double advantage
of providing a diffeomorphic displacement when the velocity field is integrated over time,
and to allow for the interpretation of the deformation energy as a kinetic energy, directly
related to a Riemannian metric on the space of landmarks.
...
Riemannian Structure
Let LmkN denote the submanifold of RdN consisting of all ordered collections of N distinct
points in Rd,
LmkN = {x = (x, . . . , xN) ∈(Rd)N, xi ≠x j if i ≠j}.
The tangent space to LmkN at x can be identified to the space of all families of
d-dimensional vectors c = (c, . . ., cN), and one defines (with the same notation as in the
previous section) the Riemannian metric on LmkN
∥c∥
x = cTSV(x)−c.
As already pointed out, ∥c∥
x is the minimum of ∥v∥
V among all v in V such that v(xi) = ci,
i = , . . . , N. This minimum is attained at
vc(⋅) =
N
∑
i=
K(⋅, xj)αj
with α = SV(x)−c.
Now, given any differentiable curve t ↦x(t) in LmkN, one can build an optimal time-
dependent velocity field
v(t,⋅) = vc(t)(⋅)
with c = ∂tx. One can then define the flow associated to this time-dependent velocity,
namely the time-dependent diffeomorphism φv such that φv(, x) = x and
∂tφv(t, x) = v(t, φv(t, x))
which is, by construction, such that φv(t, xi()) = xi(t) for i = , . . . , N. So, this construc-
tion provides a diffeomorphic extrapolation of any curve in LmkN, which is optimal in the
sense that its velocity has minimal V norms, given the induced constraints. The metric that
has been defined on LmkN is the projection of the V norm via the infinitesimal action of
velocity fields on LmkN, which is defined by
v ⋅(x, . . . , xN) = (v(x), . . . ,v(xN)).
This concept will be extensively discussed later on in this chapter.

Shape Spaces 

...
Geodesic Equation
Geodesics on LmkN are curves that locally minimize the energy, that is, they are curves
t ↦x(t) such that, for any t, there exists h > such that
∫
t+h
t−h ∥˙xu(u)∥
x(u)du
is minimal over all possible curves in LmkN that connect x(t −h) and x(t + h). The
geodesic, or Riemannian, distance between xand xis defined as the minimizer of the
square root of the geodesic energy
∫

∥˙xu∥
x(u)du
over all curves in LmkN that connect xand x.
Geodesics are characterized by a second-order equation, called the geodesic equation.
If one denotes GV(x) = SV(x)−, with coefficients g(k,i),(l,j) for k, l = , . . . , N and i, j =
, . . . , d, the classical expression of this equation is
¨xk,i +
N
∑
l,l′=′
d
∑
j,j′=
Γ(k,i)
(l,j),(l′,j′) ˙xl,j ˙xl′,j′ = ,
where Γ(k,i)
(l,j),(l′,j′) are the Christoffel symbols, given by
Γ(k,i)
(l,j),(l′,j′) = 
(∂xl′, j′ g(k,i),(l,j) + ∂xl, j g(k,i),(l′,j′) −∂xk,i g(l,j),(l′,j′)).
In these formulae, the two indices that describe the coordinates in LmkN, xk,i were made
explicit, representing the ith coordinate of the kth landmark. Solutions of this equation are
unique as soon as x() and ˙x() are specified.
Equations put in this form become rapidly intractable when the number of landmarks
becomes large. The inversion of the matrix SV(x), or even simply its storage can be compu-
tationally impossible when N gets larger than a few thousands. It is much more efficient,
and analytically simpler as well, to use the Hamiltonian form of the geodesic equation,
which is (see []),
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
∂tx = SV(x)α
∂tα = −
∂x(αTSV(x)α)
(.)
This equation will be justified in
> Sect. ..., in which the optimality conditions for
geodesics will be retrieved as a particular case of general problems in calculus of variations
and optimal control. Its solution is uniquely defined as soon as x() and α() are specified.
The time-dependent collection of vectors t ↦α(t) is called the momentum of the motion.
It is related to the velocity c(t) = ˙x(t) by the identity c = SV(x)α.


Shape Spaces
Introducing KV and letting Ki j
V (x, y) denote the i, j entry of KV(x, y), this geodesic
equation can be rewritten in the following even more explicit form:
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
∂txk =
N
∑
l=
KV(xk, xl)αl,
k = , . . . , N,
∂tαk = −
N
∑
l=
d
∑
i,j=
∇Ki j
V (xk, xl)αk,iαl,j,
k = , . . . , N,
(.)
where ∇Ki j
V denotes the gradient of the i, j entry of KV with respect to its first variable.
The geodesic equation defines the Riemannian exponential map as follows. Fix x∈
LmkN. The exponential map at xis the transformation c ↦expx(c) defined over all tan-
gent vectors c to LmkN at x(which are identified to all families of d-dimensional vectors,
c = (c, . . ., cN)), such that expx(c) is the solution at time t = of the geodesic equation
initialized at x() = xand ˙x() = c. Alternatively, one can define the exponential chart
in Hamiltonian form that will also be called the momentum representation in LmkN by the
transformation
α↦exp♭
x(α),
where exp♭
x(α) is the solution at time of system (> .) initialized at (x, α).
For the metric that is considered here, one can prove that the exponential map at
x(resp. the momentum representation) is defined for any vector c (resp. α); this also
implies that they both are onto, so that any landmark configuration y can be written as
y = exp♭
x(α) for some α∈(Rd)N. The representation is not one to one, because
geodesics may intersect, but it is so if restricted to a small-enough neighborhood of . More
precisely, there exists an open subset U ⊂TxLmkN over which expxis a diffeomorphism.
This provides the so-called exponential chart at x on the manifold.
...
Metric Distortion and Curvature
Exponential charts are often used for data analysis on a manifold, because they provide, in
a neighborhood of a reference point x, a vector-space representation which has no radial
metric distortion, in the sense that, in the chart, the geodesic distance between xand
expx(c) is equal to ∥c∥x. The representation does distort the metric in the other directions.
One way to measure this is by comparing (see > Fig. -), for given cand cwith ∥c∥x=
∥c∥x= , the points expx(tc) and expx(t(c+sc)). Let F(t, s) denote the last term (so
that the first one is F(t,)). One can write
dist(F(t, s), F(t,)) = s∥∂sF(t,)∥F(t,) + o(s).
Without metric distortion, this distance would be given by st∥c∥x= st. However, it turns
out that []
∥∂sF(t,)∥F(t,) = t −ρx(c,c) t
+ o(t),

Shape Spaces 

exp(c0 + sc1)
c1
c0 + sc1
0
c0
x0
exp(c0)
⊡Fig. -
Metric distortion for the exponential chart
where ρx(c,c) is the sectional curvature of the plane generated by cand cin TxLmkN.
So, this sectional curvature directly measures (among many other things) the first order of
metric distortion in the manifold and is therefore an important indication of this distortion
of the exponential charts.
The usual explicit formula for the computation of the curvature involves the second
derivatives of the metric tensor matrix GV(x), which, as we have seen, is intractable for
large values of N. In a recent work, Micheli [] introduced an interesting new formula for
the computation of the curvature in terms of the inverse tensor, SV(x).
...
Invariance
The previous landmark space ignored the important facts that two shapes are usually
considered as identical when one can be deduced from the other by an Euclidean transfor-
mation, which is a combination of a rotation and a translation (scale invariance is another
important aspect that will not be discussed in this section). To take this into account, we
need to “mod out” these transformations, that is, to consider the quotient space of LmkN
by the Euclidean group.
One can pass from the metric discussed in the previous section to a metric on the
quotient space via the mechanism of Riemannian submersion (> Fig. -). The scheme
is relatively simple, and we describe it and set up notation in a generic framework before
taking the special case of the landmark manifold. So, let Q be a Riemannian manifold and
π : Q →M be a submersion, that is, a smooth surjection from Q to another manifold M
such that its differential Dπ has full rank everywhere. This implies that, for m ∈M, the set
π−(m) is a submanifold of Q, called the fiber at m. If q ∈Q and m = π(q), the tangent
space TqQ can be decomposed into the direct sum of the tangent space to π−(m) and
the space perpendicular to it. We will refer to the former as the space of vertical vectors
at q, and denote it Vq and to the latter as the space of horizontal vectors, denoted Hq. We
therefore have
TqQ = Vq ⊥Hq.


Shape Spaces
[x0]
[x1]
G.x0
G.x1
Horizontal geodesic
Projected geodesic
x0
x1
⊡Fig. -
Riemannian submersion (geodesics in the quotient space)
The differential of π at q, Dπ(q), vanishes on Vq (since π is constant on π−(m)) and is
an isomorphism between Hq and TmM. Let us make the abuse of notation of still denot-
ing Dπ(q) the restriction of Dπ(q) to Hq. Then, if q, q′ ∈π−(m), the map pq′, q :=
Dπ(q)−○Dπ(q′) is an isomorphism between Hq′ and Hq. One says that π is a Rieman-
nian submersion if and only if the maps pq′, q are in fact isometries between Hq′ and Hq
whenever q and q′ belong in the same fiber, that is, if one has, for all v′ ∈Hq′
∥pq′, qv′∥q = ∥v′∥q′.
Another way to phrase this property is
π(q) = π(q′),v ∈Hq,v′ ∈Hq′, Dπ(q)v = Dπ(q′)v′ ⇒∥v∥q = ∥v′∥q′.
A Riemannian submersion naturally induces a Riemannian metric on M, simply defining,
for m ∈M and h ∈TmM
∥h∥m = ∥Dπ(q)−h∥q
for any q ∈π−(m), the definition being independent of q by assumption. This is the
Riemannian projection of the metric on Q via the Riemannian submersion π.
Let us now return to the landmark case, and consider the action of rotations and trans-
lations, that is of the special Euclidean group of Rd, which is traditionally denoted SE(Rd ).
The action of a transformation g ∈SE(Rd) on a landmark configuration x = (x, . . . , xN) ∈
LmkN is
g ⋅x = (g(x), . . ., g(xN)).

Shape Spaces 

We want to use a Riemannian projection to deduce a metric on the quotient space M =
LmkN/SE(Rd) from the metric that has been defined on LmkN, the surjection π being the
projection π : LmkN →M, which assigns to a landmark configuration x its equivalence
class, or orbit under the action of SE(Rd), defined by
[x] = {g ⋅x, g ∈SE(Rd)} ∈M.
To make sure that M is a manifold, one needs to restrict to affinely independent landmark
configurations, which form an open subset of LmkN and therefore let Q be this space and
restrict π to Q. In this context, one can show that a sufficient condition for π to be a Rie-
mannian submersion is that the action of SE(Rd) is isometric, that is, for all g ∈SE(Rd),
the operation ag : x ↦g ⋅x is such that, for all u,v ∈TxQ,
⟨Dag(x)u , Dag(x)v⟩g⋅x = ⟨u , v⟩x.
This property can be translated into equivalent properties on the metric. For translations,
for example, it says that, for every x ∈Q and τ ∈Rd, one must have
SV(x + τ) = SV(x)
which is in turn equivalent to the requirement that, for all x, y, τ ∈Rd, KV(x + τ, y + τ) =
KV(x, y), so that KV only depends on x −y. With rotations, one needs
diag(R)TSV(Rx)diag(R) = SV(x),
which again translates into a similar property for the kernel, namely
RTKV(Rx, Ry)R = KV(x, y).
Here, R is an arbitrary d dimensional rotation, and diag(R) is the dN by dN block-diagonal
matrix with R repeated N times.
Kernels that satisfy these properties can be characterized in explicit forms. These
kernels include all positive radial kernels, that is, all kernels taking the form
KV(x, y) = γ(∣x −y∣)IdRd,
where γ : [,+∞) →R is the Laplace transform of some positive measure μ, that is,
γ(t) = ∫
∞

e−tydμ(y).
Such functions include Gaussians:
γ(t) = exp(−t/σ ),
(.)
Cauchy:
γ(t) =

+ t/σ ,
(.)
or Laplacian kernels, defined for any integer c ≥by
γc(t) = (
c
∑
l=
ρ(c, l) tl
σ l )exp(−t/σ)
(.)
with ρ(c, l) = l−c(c −l)⋯(c + −l)/l!.


Shape Spaces
One can also use non-diagonal kernels. One simple construction of such kernels is to
start with a scalar kernel, for example, associated to a radial function γ as above, and, for
some parameter λ ≥, to implicitly define KV via the identity, valid for all pairs of smooth
compactly supported vector fields v and w,
∫Rd ∫Rd v(x)TKV(x, y)w(y)dxdy = ∫Rd ∫Rd γ (∣x −y∣)(v(x)Tw(y))dxdy
+ λ
∫Rd ∫Rd γ (∣x −y∣)(∇⋅v(x))(∇⋅w(y))dxdy,
where (∇⋅) denotes the divergence operator. The explicit form of the kernel can be deduced
after a double application of the divergence theorem yielding
KV(x, y) = (γ(r) −λ˙γ(r))IdRd −λ¨γ(r)(x −y)(x −y)T
with r = ∣x −y∣.
Assume that one of these choices has been made for KV, so that one can use a Rie-
mannian submersion to define a metric on the quotient space Q/SE(Rd). One of the
appealing properties of this construction is that geodesics in the quotient space are given
by (equivalent classes of) geodesics in the original space, provided that they are initialized
with horizontal velocities. Another interesting feature is that the horizontality condition
is very simply expressed in terms of the momenta, which provides another advantage of
the momentum representation in
> Eq. (.). Take translations, for example. A vertical
tangent vector for their action at any point x ∈M is a vector of the form (τ, . . . , τ), where
τ is a d-dimensional vector repeated N times. A momentum, or covector, α is horizontal
if and only if it vanishes when applied to any such vertical vector, which yields
N
∑
k=
αk = .
(.)
A similar analysis for rotations yields the horizontality condition
N
∑
k=
(αkxT
k −xkαT
k ) = .
(.)
These two conditions provide the d(d + )/constraints that must be imposed to the
momentum representation on M to obtain a momentum representation on M/SE(Rd).
..
Hamiltonian Point of View
...
General Principles
This section presents an alternate formulation in which the accent is made on variational
principles rather than on geometric concepts. Although the results obtained using the
Hamiltonian approach that is presented here will be partially redundant with the ones that

Shape Spaces 

were obtained using the Riemannian point of view, there is a genuine benefit in under-
standing and being able to connect the two of them. As will be seen below, working with
the Hamiltonian formulation brings new, relatively simple concepts, especially when deal-
ing with invariance and symmetries. It is also often the best way to handle numerical
implementations.
To lighten the conceptual burden, the presentation will remain within the elementary
formulation that uses a state variable q and a momentum p, rather than the more general
symplectic formulation. On a manifold, this implies that the presentation is made with
variables restricted to a local chart.
An optimal control problem in Lagrangian form is associated to a real-valued cost func-
tion (or Lagrangian) (q,u) ↦L(q,u) defined on Q × U, where Q is a manifold and U is
the space of controls, and to a function (q,u) ↦f (q,u) ∈TqQ. The resulting variational
problem consists in the minimization of
∫
t f
ti
L(q,u)dt
(.)
subject to the constraint ˙qt = f (q,u) and some boundary conditions for q(ti) and q(t f ).
The simplest situation is the classical problem in the calculus of variations for which
f (q,u) = u and the problem is to minimize ∫
t f
ti L(q, ˙qt)dt. Here, [ti, t f ] is a fixed finite
interval. The values ti = and t f = will be assumed in the following.
The general situation in (> .) can be formally addressed by introducing Lagrange
multipliers, denoted p(t), associated to the constraint ∂tq = f (q,u) at time t; p is called
the costate in the optimal control setting. One then looks for critical paths of
J(q, p,u) ≐∫

(L(q,u) + (p∣˙qt −f (q,u))) dt,
where the paths p, u, and q vary now freely as far as q() and q() remain fixed. The costate
is here a linear form on Q, that is, an element of T∗
q Q.
Introduce the Hamiltonian
H(q, p,u) ≐(p∣f (q,u)) −L(q,u)
for which
J= ∫

((p∣˙qt) −H(q, p,u)) dt.
Writing the conditions for criticality, δJ/δu = δJ/δq = δJ/δp = , directly leads to the
Hamiltonian equation:
∂tq = ∂pH, ∂tp = −∂qH, ∂uH = .
(.)
The above derivation is only formal. A rigorous derivation in various finite dimensional
as well as infinite dimensional situations is the central object of Pontryagin Maximum
Principle (PMP) theorems which state that along a solution (q∗, p∗,u∗), one has
H(q∗(t), p∗(t),u∗(t)) = max
u
H(q∗(t), p∗(t),u).


Shape Spaces
Introducing ˜H(q, p) ≐maxu H(q, p,u), one gets the usual Hamiltonian equation:
∂tp = −∂q ˜H, ∂tq = ∂p ˜H.
(.)
One can notice that, in the classical case f (q,u) = u, ˜H(q, p) coincides with the Hamil-
tonian obtained via the Legendre transformation in which a function u(p, q) is defined via
the equation p = ∂uL and
˜H(p, q) = (p∣u(q, p)) −L(q,u(q, p)).
...
Application to Geodesics in a Riemannian Manifold
Let Q be a Riemannian manifold with metric at q denoted ⟨⋅, ⋅⟩q. The computation of
geodesics in Q can be seen as a particular case of the previous framework in at least two
(equivalent) ways. The first one is to take
L(x,u) = ∥u∥
q/and f (x,u) = u,
which gives a variational problem in standard form. For the other choice, introduce the
duality operator Kq : T∗
q Q →TqQ defined by
(α∣ξ) = ⟨Kqα , ξ⟩q,
α ∈T∗
q Q, ξ ∈TqQ, and let, denoting the control by α,
L(q, α) = (α∣Kqα)/and f (q, α) = Kqα.
The Hamiltonian equation in this case yields p = α and
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
∂tq = Kqα,
∂tα = −
∂q((α∣Kqα)).
(.)
This equation obviously reduces to (> .) with q = x, Kqα = SV(x)α.
...
Momentum Map and Conserved Quantities
A central aspect of the Hamiltonian formulation is its ability to turn symmetries into
conserved quantities. This directly relates to the Riemannian submersion discussed in
> Sect. ....
Consider a Lie group G acting on the state variable q ∈Q, assuming, for the rest of
this section and the next one, an action on the right denoted (g, q) →q ⋅g. Notice that
results obtained with a right action immediately translate to left actions, by transforming
a left action (g, q) ↦g ⋅q into the right action (g, q) ↦g−⋅q. In fact, both right and left
actions are encountered in this chapter. The standard notation TidG = G will be used in
the following to represent the Lie algebra of G.

Shape Spaces 

By differentiation in the q variable, the action can be extended to the tangent bundle,
with notation (g,v) →v ⋅g for v ∈TQ. By duality, this induces an action on the costate
variable through the equality (p ⋅g∣v) ≐(p∣v ⋅g−). Differentiating again in g at g = idG
gives the infinitesimal actions on the tangent and cotangent bundles, defined for any ξ ∈
G ≐TidGG by (ξ,v) →v ⋅ξ and for any (ξ, p) →p ⋅ξ such that (p ⋅ξ∣v) + (p∣v ⋅ξ) = ,
for all v ∈TQ and p ∈T∗Q.
Now, assume that H is G-invariant, that is, H(q ⋅g, p ⋅g) = H(q, p) for any g ∈G, and
define the momentum map (q, p) →m(q, p) ∈G∗by
(m(q, p)∣ξ) = (p∣q ⋅ξ).
(.)
Then, one has, along a Hamiltonian trajectory,
∂tm(p, q) = ,
(.)
that is, the momentum map is a conserved (vectorial) quantity along the Hamiltonian flow.
This result is proved as follows. First notice that if g(t) is a curve in G with g() = idG and
˙gt() = ξ, then, if H is G-invariant,
= ∂tH(q ⋅g, p ⋅g) = (∂qH∣q ⋅ξ) + (p ⋅ξ∣∂pH).
On the other hand, from the definitions of the actions, one has,
(∂tm(q, p)∣ξ) = ∂t(p∣q ⋅ξ) = (∂tp∣q ⋅ξ) −(p ⋅ξ∣∂tq),
so that, if (q, p) is a Hamiltonian trajectory,
(∂tm(q, p)∣ξ) = −(∂qH∣q ⋅ξ) −(p ⋅ξ∣∂pH) = 
which gives (> .).
Notice that the momentum map has an interesting equivariance property:
(m(q ⋅g, p ⋅g)∣ξ) = (p ⋅g∣(q ⋅g) ⋅ξ)
= (p ⋅g∣q ⋅(gξ))
= (p∣(q ⋅(gξ)) ⋅g−)
= (p∣q ⋅((gξ)g−))
where gξ denotes the derivative of h ↦gh in h at h = idG along the direction ξ and
(gξ)g−the derivative of h ↦hg−in h at h = g along the direction gξ. The map ξ ↦
(gξ)g−defined on G is called the adjoint representation and usually denoted v ↦Adgξ.
One therefore gets
(m(q ⋅g, p ⋅g)∣ξ) = (p∣q ⋅Adg(ξ)) = (m(q, p)∣Adg(ξ)) = (Ad∗
g(m(q, p))∣ξ),
where Ad∗
g is the conjugate of Adg. Hence
m(q ⋅g, p ⋅g) = Ad∗
g(m(q, p)),
(.)
that is, m is Ad∗-equivariant.


Shape Spaces
...
Euler–Poincaré Equation
Consider the particular case in which Q = G and G acts on itself. In this case,
(m(idG, p)∣v) = (p∣v),
so that m(idG, p) = p and one gets from > Eq. (.)
pg−= m(idG, pg−) = Ad∗
g−(m(g, p)).
Hence, along a trajectory starting from g() = idG of a G-invariant Hamiltonian H, one
has (denoting ρ = pg−∈G∗and using the fact that the momentum map is conserved over
time)
ρ(t) ≐p(t)g(t)−= Ad∗
g−(t)(m(g(t), p(t)))
= Ad∗
g−(t)(m(idG, p())) = Ad∗
g−(t)(ρ()).
(.)
This is the integrated version of the so-called Euler–Poincaré equation on G∗[,],
∂tρ + ad∗
v(ρ)(ρ) = ,
(.)
where v(ρ) = ˙gg−= ∂pH(idG, pg−) = ∂pH(idG, ρ) and ad is the differential at location
g = idG of Adg.
A special case of this, which will be important later, is when the Hamiltonian corre-
sponds to a right-invariant Riemannian metric on G. There is a large literature on invariant
metrics on Lie groups, which can be shown to be related to important finite and infi-
nite dimensional mechanical models, including the Euler equation for perfect fluids. The
interested reader can refer to [,,,,,].
Such a metric is characterized by an inner product ⟨⋅, ⋅⟩V on G, and defined by
⟨v , w⟩g = ⟨vg−, wg−⟩G.
(.)
If one lets KG be the duality operator on G so that
(ρ∣v) = ⟨KGρ , v⟩G,
the issue of finding minimizing geodesics can be rephrased as an optimal control problem
like in the case of landmarks, with Lagrangian L(g, μ) = (μ∣Kg μ)/, f (g, μ) = Kgμ and
Kgμ = (KG(μg−))g.
(.)
The Hamiltonian equations are then directly given by (> .), namely
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
∂tg = Kgμ,
∂tμ = −
∂g((μ∣Kgμ)).
(.)

Shape Spaces 

This equation is equivalent to the one obtained from the conservation of the momen-
tum map, which is (with ρ = μg−)
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
∂tg = vg,
v = KGρ,
∂tρ = −ad∗
v ρ.
(.)
...
A Note on Left Actions
Invariance with respect to left actions is handled in a symmetrical way to right actions. If
G is acting on the left on G, define the momentum map by
(m(p, q)∣v) = (p∣v ⋅q)
which is conserved along Hamiltonian trajectories. Working out the equivariance property
gives
m(g ⋅p, g ⋅q) = Ad∗
g−m(p, q).
When G acts on itself on the left, the Euler–Poincaré equation reads
ρ(t) = Ad∗
g(ρ())
or
∂tρ −ad∗
v(ρ)ρ = 
with ρ = g−p and v(ρ) = g−˙gt.
...
Application to the Group of Diﬀeomorphisms
Let G ⊂Diff(Rd) be a group of smooth diffeomorphisms of Rd (which, say, smoothly
converge to the identity at infinity). Elements of the tangent space to G, which are deriva-
tives of curves t ↦φ(t,⋅) where φ(t,⋅) ∈G for all t, can be identified to vector fields
x ↦v(x) ∈Rd, x ∈Rd.
To define a right-invariant metric on G, introduce a Hilbert space V of vector fields
on Rd with inner product ⟨⋅, ⋅⟩V. Like in
> Sect. .., let LV and KV = L−
V denote
the duality operators on V, with ⟨v , w⟩V = (LVv∣w) and ⟨μ , ν⟩V∗= (μ∣KVν); KV is
furthermore identified with a matrix-valued kernel KV(x, y) acting on vector fields.
The application of the formulae derived for Hamiltonian systems and of the Euler–
Poincaré equation will remain in the following of this section at a highly formal level, just
computing the expression assumed in the case of diffeomorphisms by the general quan-
tities introduced in the previous section. There will be no attempt at proving that these
formulae are indeed valid in this infinite dimensional context, which is out of the scope of
this chapter. As an example of the difficulties that can be encountered, let us mention the


Shape Spaces
dilemma that is involved in the mere choice of the group G. On the first hand, G can be
chosen as a group of infinitely differentiable diffeomorphisms that coincide with the iden-
tity outside a compact set. This would provide a rather nicely behaved manifold with a Lie
group structure in the sense of [,]. The problem with such a choice is that the struc-
ture would be much stronger than what Riemannian metrics of interest would induce, and
that geodesics would typically spring out of the group. One can, on the other hand, try to
place the emphasis on the Riemannian and variational aspects so that the computation of
geodesics in G, for example, remains well posed. This leads to a solution, introduced in []
(see also []), in which G is completed in a way which depends on the metric ⟨⋅, ⋅⟩V, so
that the resulting group (denote it GV) is complete for the geodesic distance. This exten-
sion, however, comes with the cost of losing the nice features of infinitely differentiable
transformations, resulting in GV not being a Lie group, for example.
This being acknowledged, first consider the transcription of (> .) to the case of
diffeomorphisms. This equation will involve a time-evolving diffeomorphism φ(t,⋅), and
a time-evolving covector, denoted μ(t), which is a linear form over vector fields (it takes a
vector field x ↦v(x) and returns a number that has so far been denoted (μ(t)∣v)). It will
be useful to apply μ(t) to vector-valued functions of several variables, say v(x, y) defined
for x, y ∈R, by letting one of the variables fixed and considering v as a function of the
other. This will be denoted by adding a subscript representing the effective variable, so that
(μ(t)∣v(x, y))x
is the number, dependent of y, obtained by applying μ(t) to the vector field x ↦v(x, y).
One first need to identify the operator Kφ in > Eq. (.), defined by
Kφμ = (KV(μφ−))φ = (KV(μφ−)) ○φ
since right translation here coincides with composition. Now, for any vector a ∈Rd and
y ∈Rd, one has,
aT(Kφμ)(y) = aT(KV(μφ−))(φ(y))
= (a ⊗δφ(y)∣KV(μφ−))
= (μφ−∣KV(a ⊗δφ(y)))
= (μ∣KV(a ⊗δφ(y)) ○φ)
= (μ∣KV(φ(x), φ(y))a)x.
So, letting e, . . ., ed denote the canonical basis of Rd, one has
(Kφμ)(y) =
d
∑
i=
eT
i (Kφμ)(y) ei =
d
∑
i=
(μ∣Ki
V(φ(x), φ(y)))x ei,
where Ki
V is the ith column of KV. Therefore
(μ∣Kφμ) =
d
∑
i=
(μ∣(μ∣Ki
V(φ(x), φ(y)))xei)y

Shape Spaces 

and (using the symmetry of KV)
(∂φ(μ∣Kφμ)∣w) = 
d
∑
i=
(μ∣(μ∣DKi
V(φ(x), φ(y))w(y))x ei)y,
where DKi
V is the derivative of KV with respect to its second variable. These computations
directly give the transcription of (> .) for diffeomorphisms, namely
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∂tφ(t, y) =
d
∑
i=
(μ(t)∣Ki
V(φ(t, x), φ(t, y)))x ei
∀w : (∂tμ(t)∣w) = −
d
∑
i=
(μ(t)∣(μ(t)∣DKi
V(φ(t, x), φ(t, y))w(y))x ei)y.
(.)
To transcribe > Eq. (.) to diffeomorphisms one only needs to work out the expres-
sions of Adφ and adv in this context. Recall that Adφw was defined by (φw)φ−; φw being
the differential of the left translation (i.e., ∂t(φ○ψ(t))()with ψ() = id and ∂tψ() = w),
one finds φw = Dφ w, and since right translation is just composition,
Adφw = (Dφ w) ○φ−.
Now, since advw is the differential of Adφw in φ (at φ = id), a quick computation shows
that
advw = Dv w −Dw v.
So, > Eq. (.) provides
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
∂tφ(t, y) = v(t, φ(t, y))
v(t, x) = KV ρ(t)(x)
∀w ∈V,(∂tρ∣w) = −(ρ(t)∣Dv w −Dw v)
(.)
with the last equation being equivalent to
(ρ(t)∣w) = (ρ()∣Dφ−(w ○φ)).
Note that μ and ρ in (> .) and (> .) are related via μ = ρφ or
(μ∣w) = (ρ∣w ○φ−).
Solving
> Eq. (.) (or (> .)) between times and provides the momentum
representation in G, denoted
φ(,⋅) = exp♭
φ(,⋅)(μ()).
Equivalently, the initial velocity being KV μ(), this is, in the exponential chart:
φ(,⋅) = expφ(,⋅)(KV μ()).


Shape Spaces
...
Reduction via a Submersion
This section, which can be put in parallel with the discussion on Riemannian submersions
in > Sect. ..., discusses how submersions from a manifold Q onto another manifold
M allow for the transfer of a Hamiltonian system on Q to a Hamiltonian system on M,
given some invariance properties satisfied by the Hamiltonian.
Let π be a submersion from a manifold Q onto a manifold M so that for any q ∈Q,
Dπq : TqQ →Tπ(q)M is a surjective mapping. For any q ∈Q, Vq ≐Dπ(q)−() is the
previously mentioned vertical space so that V ≐∪q∈QVq will be called the vertical bundle.
In the previous Riemannian setting, a metric on TQ induces the definition of a horizontal
space Hq at any location q ∈Q such that TqQ = Vq +Hq. In the Hamiltonian approach, the
horizontal space is defined in the cotangent space T∗
q Q without any reference to a particular
metric as the set of conormal covectors to the vertical space, that is,
H∗
q ≐{p ∈T∗
q Q ∣(p∣v) = ∀v ∈Vq}.
(.)
An elementary argument in linear algebra (which is left to the reader) shows that if one
introduces the one-to-one adjoint mapping Dπ∗(q) : T∗
π(q)M →T∗
q Q, one has H∗
q =
Dπ(π(q))∗(T∗
π(q)M). In other terms, a covector p is horizontal at q if and only if there
exists a covector α ∈T∗
π(q)M such that Dπ(π(q))∗α = p. Therefore, H∗≐∪q∈QH∗
q can
be seen as a sub-bundle of the cotangent bundle T∗Q for which there exist a surjective
mapping
̃π : H∗→T∗M
defined by ̃π(q, p) = (π(q),(Dπ∗(q))−(p)).
The main idea of this Hamiltonian (one should say symplectic or better Poisson) point
of view is that H∗is the natural image in T∗Q of the dynamic space (phase space) T∗M
on M. Now, assume that a Hamiltonian HQ is given on Q. One says that HQ is π-reducible
if there exists an Hamiltonian HM on T∗M such that
HQ∣H∗= HM ○̃π
(.)
or equivalently
HM(m, α) = HQ(q, Dπ(q)∗α)
(.)
for q ∈π−(m).
Hamiltonian trajectories in both spaces are related as follows. Assume that(q, p) is HQ -
Hamiltonian (i.e., ∂tq = ∂pHQ and ∂tp = −∂qHQ) with (q(), p()) ∈H∗and consider in a
similar way a HM-Hamiltonian trajectory (m, α) such that (m(), α()) = ̃π(q(), p()),

Shape Spaces 

then for any t ≥, one has (q(t), p(t)) ∈H∗and (m(t), α(t)) = ̃π(q(t), p(t)).
Equivalently, one has the commutative diagram
H∗
ΦQ(.,.,t)
....→
H∗
///0
̃π
///0
̃π
T∗M
ΦM(.,.,t)
....→T∗M,
(.)
where ΦH and ΦQ are the associated Hamiltonian flows (in particular H∗is ΦQ invariant).
To prove this fact, first notice that from the definition of HM, which can be rewritten as
HM(π(q), α) = HQ(q, Dπ(q)∗α),
one gets
(ρ∣∂αHM) = (Dπ(q)∗ρ∣∂pHQ)
(.)
and (∂mHM∣Dπ(q)ξ) = (∂qHQ∣ξ) + (α∣Dπ(q)(ξ, ∂pHQ))
(.)
(as usual, computations are assumed to be done within a chart and the second derivative
of π defined according to this chart).
Define x(t) ≐(m(t), α(t)), y(t) ≐(q(t), p(t))), z = (x, y) and the transformation
ψ(z) = (ψM(z),ψQ(z)) = (m −π(q), p −Dπ∗(q)α).
One needs to prove that ψ(z(t)) ≡.
If Z(z) ≐(∂αHM,−∂mHM, ∂pHQ,−∂qHQ) is the vector field governing the joint
Hamiltonian flows (by construction ∂tz = Z(z)), one has
Dψ(z)Z = if ψ(z) = .
(.)
Notice that this fact implies that Z is everywhere tangent to the set ψ = , which is locally
a submanifold because Dψ(z) has full rank, as can easily be seen. This implies that ψ = 
is invariant by the flow associated to Z.
To prove (> .), assume ψ(z) = and notice that the statement is equivalent to
(ζ M∣DψM(z)Z(z)) + (DψQ(z)Z(z)∣ζQ) = for any ζ = (ζ M, ζQ). One has
(ζ M∣DψM(z)Z(z)) = (ζ M∣∂αHM −Dπ(q)∂pHQ)
and
(DψQ(z)Z(z)∣ζQ)) = (−∂qHQ + Dπ∗(q)∂mHM∣ζQ) −(α∣Dπ(q)(ζQ, ∂pH))
= (∂mHM∣Dπ(q)ζQ) −(∂qHQ∣ζQ) −(α∣Dπ(q)(∂pH, ζQ))
and the result is a direct consequence of (> .) and (> .).
Let us review how this concept of reduction via a submersion property generalizes the
Riemannian submersion idea. When Q and M are Riemannian with M equipped with the
projected metric, one has by construction
⟨ξ , η⟩q = ⟨Dπ(q)ξ,
Dπ(q)η⟩π(q′)
(.)


Shape Spaces
for any q ∈M and ξ ∈Hq horizontal at q. Let Kq : T∗
q Q →TqQ be the duality operator for
the metric at q (such that (p∣ξ) = ⟨Kqp , ξ⟩q), and Km be the same operator for the metric
on M. From (> .), one gets
Hq = KqH∗
q .
If one expresses (> .) for ξ = KqDπ(q)∗α and η = KqDπ(q)∗β and identifies the
terms, one gets an equivalent version of (> .) in terms of the duality operators, namely
Km = Dπ(q)KqDπ(q)∗,
(.)
the invariance assumption being that the right-hand term does not depend on q ∈π−(m).
Now, the Hamiltonians associated to the metrics respectively are HQ(q, p) = (p∣Kq p)/
and HM(m, α) = (α∣Kmα)/and it is now straightforward to see that the condition
HM(m, α) = HQ(q, Dπ(q)∗α) if π(q) = m, that is, condition (> .), is also equivalent
to (> .).
...
Reduction: Quotient Spaces
A fundamental special case of the previous situation is when π is the projection onto a
quotient space M = Q/Gs where Gs is a group of symmetries, acting on Q. A left action
is assumed in the following, a right action being handled in a symmetrical way. Introduce
the canonical projection π : Q →M which associates the orbit Gs ⋅q to an element q of
Q. Let us first work out conditions that ensure that a Hamiltonian HQ is π-reducible. One
needs:
HQ(q, p) = HQ(q′, p′)
whenever π(q) = π(q′) and there exists α ∈T∗
π(q)M such that p = Dπ(q)∗α and p′ =
Dπ(q′)∗α. Notice that π(q) = π(q′) implies that there exists a g ∈G such that q′ = g ⋅q.
From the relation Dπ(q′)(g ⋅ξ) = Dπ(q)ξ which derives from π(g ⋅q) = π(q), one gets
(p∣ξ) = (α∣Dπ(q)ξ) = (α∣Dπ(q′)(g ⋅ξ)) = (p′∣g ⋅ξ)
which implies that p′ = g ⋅p (this condition obviously implying that they correspond to the
same α if they both are horizontal). So HQ is π-reducible if and only if HQ is G-invariant,
namely
HQ(g ⋅q, g ⋅p) = HQ(q, p)
(.)
For the construction made in the previous section to be useful in practice, one needs to
provide a simple description of the cotangent bundle to M, T∗M. This will be done using
the momentum map ms for the action of Gs, and in particular the set
m−
s () = {(q, p) ∈T∗Q : ∀ξ ∈Gs,(p∣ξ ⋅q) = } = H∗.
Given this notation, one has the identification:
m−
s ()/Gs ≅T∗M.
(.)

Shape Spaces 

First notice that the right-hand term is meaningful, since, by the equivariance of the
momentum map, m−
s () is invariant by Gs. To prove (> .), recall the transforma-
tion ̃π : H∗= m−
s () →T∗M by ̃π(q, p) = (m, α) with m = π(α) and p = Dπ(q)∗α. The
last identity means that
(α∣Dπ(q)v) = (p∣v)
and the condition ms(q, p) = implies that this definition is not ambiguous, since
Dπ(q)v = implies that v = ξ ⋅q for some ξ, and therefore that (p∣v) = (ms(q, p)∣ξ) = .
(The definition does define (α∣ρ) for all ρ because Dπ(q) has full rank, since π is a
submersion.)
The next remark is that ̃π induces a map [̃π] on the quotient space m−
s ()/Gs, defined
by
[̃π ](Gs ⋅(q, p)) = ̃π(q, p).
Again, one must make sure that the definition makes sense by proving that ̃π(g ⋅q, g ⋅p) =
̃π(p, q) but this is an immediate consequence of the definition of the extended action of
Gs on T∗Q. Finally, [̃π] is one to one, since, as shown above if π(q) = π(q′) = m and
p = Dπ∗(q)α and p′ = Dπ∗(q′)α, then there exists g ∈Gs such that (q′, p′) = (g ⋅q, g ⋅p).
This proves the identification (> .).
As an example, consider the reduction of the Hamiltonian
H(x, α) = 
αTSV(x)α
in the landmark case (Q = LmkN) and the invariance by the group Gs = SE(Rd). With
(α∣ξ) = ∑N
k=αT
k ξk, the momentum map for this action is
(ms(x, α)∣(A, τ)) =
N
∑
k=
αT
k (Axk + τ)
defined for all skew-symmetric matrix A and vector τ ∈Rd, and the conditions for
ms(x, α) = are exactly those given in (> .) and (> .).
Note that condition (> .) on the duality operator directly correspond to the
invariance conditions associated to the kernel KV in > Sect. ....
...
Reduction: Transitive Group Action
Consider the situation of a left group action G × M →M of a group G on a manifold M.
The important example in this chapter is when G is a group of diffeomorphisms and M
be a set of “shapes” (for instance M = LmkN). Assume that the action is transitive, that is,
G ⋅m= M so that π : G →M defined by π(g) = g ⋅mis a smooth surjection, that will be
assumed to be a submersion. The situation here is on how to project a Hamiltonian system
on G onto a reduced one on M.
Let
G= {g ∈G ∣g ⋅m= m} = π−(m)


Shape Spaces
be the isotropy group of m. Then condition (> .) for a Hamiltonian HG on G is
equivalent to the invariance of HG to the right action of Gon G, namely HG(gh, ρh) =
HG(g, ρ) for h ∈G.
Although it is often more convenient to apply the reduction directly to π as defined
above, since the structure of T∗M is generally easily defined in this context, it is interesting
to notice that this reduction also comes as an application of the previous construction on
quotient spaces via the well-known identification [] M ≅G/G. This identity extends to
cotangent spaces as above, with
m−
G ()/G≅T∗M,
(.)
where mG is the momentum map associated with G.
One can interpret the construction of the Riemannian metric for landmarks within
this framework. Take M = LmkN, G a group of diffeomorphisms and m= x. If α =
(α, . . . , αN) ∈TxM∗, one can identify p = Dπ∗(φ)α as
p =
N
∑
i=
αi ⊗δx,i,
since for any v ∈TφG,
(p∣v) =
N
∑
i=
(αi∣v(x,i)).
and Dπ(φ)v = (v(x,), . . . ,v(x,N)). Assume that a Riemannian metric is defined on
G such that ⟨v , w⟩φ is associated with a duality operator Kφ that can be identified with
a reproducing kernel also denoted Kφ (without assuming right invariance yet). With this
assumption, one has
HG(x, α) = H(φ, p) = 
(p∣Kφ p) = 

N
∑
i=
αT
i Kφ(x,i, x,j)αj
The invariance assumption is now clear: one needs that Kφ(x,i, x,j) only depends on
x = φ ⋅x. This is in particular implied by the full right-invariance assumption discussed
in > Sect. ...for which Kφ(x,i, x,j) = KV(xi, xj), yielding in this case
HM(x, α) = 

N
∑
i=
αT
i KV(xi, xj)αj
in the G-invariant case. As an alternative, one could, for example, also use the less restrictive
assumption Kφ(x,i, x,j) = Kx(xi, xj) where Kx is still a kernel, like in (> .), (> .),
or (> .), in which the scale parameter σ is chosen dependent of x (e.g., increasing as a
function of ∣x −x∣).
The situation of a fully G-invariant Hamiltonian HG can be studied in the general set-
ting. Indeed, since G acts on M, one can consider the associated momentum map mM on
T∗M defined by
(mM(m, α)∣ξ) = (α∣ξ ⋅m).

Shape Spaces 

If p = Dπ∗(g)(α) then pg−= mM(m, α). Indeed,
(pg−∣ξ) = (p∣ξg) = (α∣Dπ(g)ξg) = (α∣ξ ⋅m).
Hence,
HM(m, α) = HG(g, p) = HG(idG, pg−) = HG(idG,mM(m, α)).
In the case of an invariant Riemannian metric H(idG, p) = 
(p∣KV p) = 
∥p∥
G∗where
∥∥G∗denotes the dual norm, this gives
HM(m, α) = 
∥mM(m, α)∥
G∗.
(.)
..
Spaces of Plane Curves
...
Introduction and Notation
We now consider two-dimensional shapes represented by their contours and address the
case of spaces of plane curves. Compared to the space of landmarks, two new issues signif-
icantly complicate the theory. The first one is that curves are infinite-dimensional objects,
which will place us in the framework of infinite dimensional Riemannian manifolds. The
second one is that curves are rarely labeled, which will require the analysis to be invariant
by a change of parameterization.
Let us first start with a few definitions regarding plane curves. Parameterized plane
curves can be seen as functions x : S→R, where Sis the unit circle in R. For simplicity,
they will be assumed to be smooth (infinitely differentiable), unless specified otherwise.
Smooth curves over the unit circle can equivalently be seen as infinitely differentiable π-
periodic functions with periodic derivatives defined on the real line. It will be convenient
to use both representations in the following.
One says that x : S→Ris an immersion (or an immersed curve) if its first dif-
ferential never vanishes (one often also says that x is a regular curve). We let I denote
the space of immersed curves. Immersed curves, which are easily characterized by their
non-vanishing first derivative, are a convenient but a relatively imperfect representation of
two-dimensional shapes, since they may include curves that self-intersect. A more restric-
tive class is the space of embedded curves, that contains immersed curves that coincide, in
the neighborhood of any point, and after a suitable rotation, with the graph of a smooth
function. But because being embedded is a global statement about the curve, and there-
fore harder to handle than being immersed which is just local, this discussion will primarily
focus on the space I.
We let τ(u) = ˙x(u)/∣˙x(u)∣be the unit tangent at u (or x(u)) to x, ν(u) be the unit
normal, obtained by rotating τ(u) of π/, and κ(u) the curvature, given by
κ = (∂sτ)Tν = ˙τT
u ν/∣˙xu∣
where, following [], we let ∂s denote the operator ∂u/∣˙xu∣.


Shape Spaces
A change of parameter (or reparameterization) for a curve is a smooth diffeomorphism
u ↦ψ(u) of S, or, alternatively a smooth increasing diffeomorphism of the real line such
that, for all u ∈R,
ψ(u + π) = ψ(u) + π.
Changes of parameter act on parameterized curves on the right via
(ψ,x) ↦x ○ψ.
A normalized arc-length parameterization of x is a change of parameter taking the form
s(u) = s+ π
L ∫
u

∣˙xu(˜u)∣d ˜u
(.)
and L is the length of x with
length(x) = ∫
π

∣˙xu(˜u)∣d ˜u.
The scalar number sintervening in the arc-length parameterization can be assumed to
be between and π without loss of generality, and will be referred to as the offset of the
parameterization.
The quotient space of immersed curves by reparameterization is the space of geometric
curves, denoted B. This space can in turn be quotiented out by the actions of rotations,
translations, and scaling, which act on the left and commute with changes of parameter,
in the sense that the result of applying a similitude and a change of parameters does not
depend on the order with which these operations are performed.
The goal in this section is to discuss shape spaces of curves obtained by putting a Rie-
mannian structure on B, possibly quotiented by Euclidean transformations and/or scaling.
But before this discussion, it will be interesting to list a few of the basic distances that can
be defined on this set without using a Riemannian construction.
...
Some Simple Distances
We here consider some simple parameterization-free distances between curves based on
the images of the curves (the set x(R)).
A very simple example is to use standard norms (like Lp or Sobolev norms) computed
on the difference between two curves parameterized with their normalized arc length.
Take, for example, the Lnorm, and define, for two curves x and ˜x parameterized with
normalized arc length
dL(x, ˜x) = inf
s(∫
π

∣x(s + s) −˜x(s)∣ds)
(.)
the infimum being taken over all possible offsets as defined in > Eq. (.).
One must apply some care when defining distances like (> .) which involves some
optimization over some parameters that affect the curves. The following statement (the

Shape Spaces 

proof of which is left to the reader) is a key for this to be a valid way of building distances
on quotient spaces.
Lemma 
Let M be a metric space, with distance d : (x,x′) ↦d(x,x′). Let G be a group
acting on M (with, say, a left action). Assume that d is G-invariant, which means that, for all
x,x′ ∈M and all g ∈G,
d(g ⋅x, g ⋅x′) = d(x,x′).
Then the distance ¯d defined on the quotient space M/G by
¯d([x],[x′]) = inf
g,g′∈G d(g ⋅x, g′ ⋅x′)
(.)
is symmetric and satisfies the triangle inequality.
Notice that, because of the G-invariance, ¯d is also given by
¯d([x],[x′]) = inf
g∈G d(g ⋅x,x′).
(.)
A sufficient condition ensuring that
¯d is a distance (the missing property being
d([x],[x′]) = ⇒[x] = [x′]) is that the orbits [x] = G ⋅x are closed subsets of M for
all x ∈M. The invariance condition can be placed in parallel with the invariance condition
that arose in our discussion of the Riemannian submersion, the latter being an infinitesimal
version of the former in the case of Riemannian metrics.
Returning to (> .), it is easy to see that a change of offset provides a group action
on the left on curves, and that the Ldistance is invariant to this action. It is not too hard
to prove that the action has closed orbits so that (> .) does provide a valid distance
in B. Since the Ldistance is also invariant by the left action of rotations and translations,
one can also define
¯dL(x, ˜x) = inf
s,θ,b (∫
π

∣gθx(s + s) + b −˜x(s)∣ds),
(.)
where gθ is the rotation of angle θ and b ∈R.
A variant of this distance directly compares the derivative of the curves, which provides
a translation-invariant representation, defining
¯dH(x, ˜x) = inf
s,θ (∫
π

∣gθ∂sx(s + s) −∂s˜x(s)∣ds).
(.)
This distance has been introduced for curve comparison in [], with a very efficient
computation algorithm based on Fourier transforms.
When a curve x is simple (i.e., without self intersection), it can be considered as the
boundary of a bounded set (its interior) that will be denoted Ωx. A simple distance com-
paring two such curves, say x and x′, is the area of the symmetric difference between Ωx
and Ωx′, that is,
dsym(x,x′) = area(Ωx ∪Ωx′) −area(Ωx ∩Ωx′).


Shape Spaces
A more advanced notion, the Hausdorff distance, is defined by
dH(x,x′) = inf {ε > ,x ⊂Bε(x′) and x′ ⊂Bε(x)},
where Bε(x) is the set of points at distance less than ε from x (and similarly for Bε(x′)).
The same distance can be used with Ωx and Ωx′ instead of x and x′ for simple closed curves,
the Hausdorff distance being in fact a distance between closed subsets of R.
Instead of comparing curves that are already parameterized with arc length, one can
start with distances that are invariant by reparameterization and quotient out this action
as described in Lemma . It is not easy to come up with explicit formulae for such invariant
distances, but here is an important example.
Start with the supremum norm between the curves, namely
d∞(x,x′) = sup
u
∣x(u) −x′(u)∣,
which is obviously invariant by changes of parameter. The distance obtained after reduction
is called the Fréchet distance and is therefore defined by
dF(x,x′) = inf
ψ d∞(x ○ψ,x′).
Note that, if, for some reparameterization ψ, one has d∞(x ○ψ,x′) ≤ε, then x ⊂Bε(x′)
and x′ ⊂Bε(x). This implies the relation
ε > dF(x,x′) ⇒ε > dH(x,x′)
which implies dH ≤dF. As a consequence, dF(x,x′) = is only possible when x = x′ up to
reparameterization, which completes Lemma in ensuring that dF is a distance.
Another interesting point of view that leads to parameterization-invariant distances is
to include plane curves in a suitable Hilbert space. We have already seen an example of this
with the Ldistance based on the arc length parameterization, although this one required
an extra one-dimensional optimization to get rid of the offset. An interesting alternate
option (two of them, in fact) can be obtained by considering curves as linear forms instead
of functions.
One can first identify a curve to a measure, which is a linear form on continuous
functions, defined by, for a curve x, and for a function f : R→R,
(μx∣f ) = ∫
π

f (x(u))∣˙xu(u)∣du.
This is clearly parameterization independent, and more precisely, μx = μy if and only if
x = y up to reparameterization or change of orientation.
Another point of view is to identify a curve to a current [], or equivalently in this
case, to a vector measure which is a linear form on vector fields. For this, simply define, for
f : R→R:
(νx∣f ) = ∫
π

˙xu(u)T f (x(u))du.

Shape Spaces 

This is also parameterization independent, with νx = νy if and only if x = y up to
reparameterization.
Both (signed) measures and vector measures form linear spaces, even if not all of them
correspond to curves. Nonetheless any norm on these spaces directly induces a parame-
terization invariant distance between curves. Hilbert norms are specially attracting for this
purpose because of the numerical convenience of being associated to a dot product. One
way to build such norms is to start with a Hilbert space of functions on R(resp. vector
fields) for which μx (resp. νx) is continuous, and then use the corresponding norm on the
dual space [–,].
Start with the case of scalar functions and consider a Hilbert space W of functions
f : R→R such that the evaluation functionals x ↦f (x) are continuous (so that W is
a reproducing kernel Hilbert spaces of scalar functions). Denote by LW : W →W∗and
KW : W∗→W the duality operators on W, similarly to what has been introduced in
> Sect. ..with LV and KV, so that, for f ∈W and μ ∈W∗,
∥f ∥
W = (LW f ∣f ) and ∥μ∥
W∗= (μ∣KW μ).
Like in
> Sect. .., KW is a kernel operator, and there exists a scalar-valued function
(x, y) ↦KW(x, y) such that, for a measure μ
(KW μ)(x) = ∫RKW(x, y)dμ(y).
This implies
∥μ∥
W∗= ∫R×RKW(x, y)dμ(x)dμ(y)
and directly leads to a distance between curves, namely
d(x,x′)= ∥μx −μx′∥
W∗
(.)
= ∫
π

∫
π

KW(x(u),x(u′))∣˙x(u)∣∣˙x(u′)∣dudu′
−∫
π

∫
π

KW(x(u),x′(u′))∣˙x(u)∣∣˙x′(u′)∣dudu′
+ ∫
π

∫
π

KW(x′(u),x′(u′))∣˙x′(u)∣∣˙x′(u′)∣dudu′.
The construction associated to vector measures is similar. The space W being this time
a space of vector fields, the discussion is identical to the one holding for V in > Sect. ..,
with a kernel KW which is matrix valued. Other than this, the resulting norm in the dual
space is formally the same, yielding
d(x,x′)= ∥νx −νx′∥
W∗
(.)
= ∫
π

∫
π

˙xT
u KW(x(u),x(u′))˙xu(u′)dudu′
−∫
π

∫
π

˙xT
u KW(x(u),x′(u′))˙x′
u(u′)dudu′
+ ∫
π

∫
π

˙x
′ T
u KW(x′(u),x′(u′))˙x′
u(u′)dudu′.


Shape Spaces
...
Riemannian Metrics on Curves
We now pass to the specific problem of designing Riemannian metrics on spaces of curves.
The first issue we have to deal with is that we are now handling infinite dimensional mani-
folds, which is significantly more complex than the finite dimensional space of landmarks.
Since there is more than one type of infinite dimensional vector spaces, there is more than
one type of infinite dimensional manifolds, and the one which is appropriate when deal-
ing with spaces of infinitely differentiable curves, is the class of Fréchet manifolds []. It is
not our intent, here, to handle the related issues with the appropriate scrutiny, the reader
being invited to refer to [,] for a more rigorous presentation. We will here simply state
intuitively plausible facts on the structures that are defined.
The space I of immersed curves is open in the Fréchet space C∞(S,R) of infinitely
differentiable functions from Sto R(in which a sequence of curves xn converges to x if
all its derivatives converge for the supremum norm). If x ∈I, a tangent vector ξ ∈TmI is
an element of C∞(S,R), that can also be considered as a smooth vector field along x. A
Riemannian metric on I will therefore be a norm on this space, namely
ξ ↦∥ξ∥x
associated to an inner product ⟨⋅, ⋅⟩x that depends on x ∈I.
We will consider norms that allow for Riemannian projections when quotienting out
the action of changes of parameters, as well the action of the usual transformation groups,
SE(R) possibly combined with scaling. Starting with changes of parameters, the differ-
ential of the map x ↦x ○ψ simply is ξ ↦ξ ○ψ, which yields the first requirement
∥ξ ○ψ∥x○ψ = ∥ξ∥x
(.)
for all x ∈I, ξ ∈C∞(S,R) and smooth reparameterization ψ. A simple way to ensure
parameterization invariance is to define the norm for curves that are parameterized with
normalized arc length, simply ensuring that the norm is invariant by a change of offset.
Invariance with respect to translations, rotations and scaling respectively requires:
∥ξ∥x+b = ∥ξ∥x, b ∈R
(.)
∥gξ∥gx = ∥ξ∥x, g ∈SO(R)
(.)
λ∥ξ∥λx = ∥ξ∥x, λ ∈(,+∞).
(.)
A very simple norm, which satisfies (> .)–(> .), is the Lnorm of ξ relative
to the curve arc length, which is
∥ξ∥
x = ∫
π

∣ξ(u)∣∣˙xu∣du.
(.)
This norm has been studied in [, ], and shown to provide degenerate Riemannian
metrics, in the sense that the projected Riemannian distance between any two curves is
zero.
Before elaborating on this fact, consider vertical vectors for the projection of I onto
the space B of curves modulo reparameterization. They are described as follows. Tangent

Shape Spaces 

vectors at x to the orbit of x under the action of changes of parameters are obtained as
ξ = (∂ε(x ○ψ))(,u), where ε ↦ψ(ε,u) is a reparameterization in u which smoothly
depends on ε. This yields ξ = ∂εψ(,u)˙xu ○ψ(,u), which implies that vertical vectors
ξ ∈Vm are such that all ξ(u) are tangent to x.
Horizontal vectors at x for the metric in (> .) are therefore given by vector-valued
functions ξ ↦ξ(u) that are everywhere normal to x. It follows that if [x] and [x′] are
two equivalent classes of curves modulo reparameterization, their geodesic “distance” is
given by
d(x,x′)= inf {∫

∫
π

∣˙yt∣∣˙yu∣dudt,y(,.) = x,[y(,.)] = x′, ˙yT
u ˙yt = }.
(.)
As written above, one has the following theorem.
Theorem (Mumford–Michor)
The distance defined in (> .) vanishes between any
pair of smooth curves x and x′.
A proof of this result can be found in [, ]. It relies on the remark that one can grow
thin protrusions (“teeth”) on a curve at a cost which is negligible compared to the size of
the tooth. To get the basic idea underlying this result, one can understand how open seg-
ments can be translated at arbitrary small geodesic cost. First consider a path that starts
with a horizontal segment; progressively grow an isosceles triangle of width ε and height
t (at time t) somewhere on the segment until t = . A quick computation shows that the
associated geodesic length is o(ε) (in fact, O(εln ε)). This implies that one can cover the
horizontal segment with O(/ε) thin non-overlapping teeth at cost O(ε ln ε). With a sim-
ilar construction and the same cost, one can pull up the triangles pointing downward to
obtain a translated segment. The total cost of the operation being arbitrarily small when
ε →, the geodesic distance between parallel segments is zero. This can in fact be extended
to any pair of close or open curves, yielding the result stated in Theorem .
Quite interestingly, small variations in the definition of the metric are sufficient to
address this issue. Take, for example, the distance associated with
∥ξ∥
x = length(x)∫
π

∣ξ(u)∣∣˙xu∣du,
(.)
introduced in [,]. Looking back at the previous “tooth example,” the length of a teeth
being approximately , we see that the length term penalizes the geodesic energy when
growing O(/ε) teeth by an extra (/ε) factor, and the total energy is not negligible any-
more. In fact, the associated distance is not degenerate, as shown in [], in which the
geodesic length is proved to correspond to the total area swept by the time-dependent
curve.
Another way to control degeneracy is to penalize high curvature points, using for
example,
∥ξ∥
x = ∫
π

(+ aκx(u))∣ξ(u)∣∣˙xu∣du.
(.)


Shape Spaces
This metric has been studied in [], where it is shown (among other results) that the
distance between distinct curves is positive.
All the previous metrics could be put in the form
∥ξ∥
x = ∫
π

ρx(u)∣ξ(u)∣∣˙xu∣du,
(.)
where ρx > is invariant by reparameterization, in the sense that
ρx○ψ ○ψ = ρx.
More generally, one can consider metrics associated to positive symmetric linear operators
ξ ↦Axξ which associate to a smooth vector u ↦ξ(u) along x another smooth vector,
u ↦(Axξ)(u), with the properties that
∫
π

(η)T(Axξ)∣˙xu∣du = ∫
π

(Axη)T ξ∣˙xu∣du
and
Ax○ψ(ξ ○ψ) = (Axξ) ○ψ.
The geodesic equation associated to such a metric can be derived by computing the
first variation of the geodesic energy. The computation is straightforward if one makes
the following assumption on the variations of the operator Ax. Assume that there exists a
bilinear operator D′Ax that takes as input two vector fields along x, say ξ(⋅) and η(⋅), and
return a new vector D′Ax(ξ, η)(⋅) such that
∂ε ∫
π

(Ax+εζ ξ)Tη∣˙xu∣du = ∫
π

(D′Ax(ξ, η))Tζ∣˙xu∣du,
where the derivative in the left-hand side is evaluated at ε = . With this notation, the
geodesic equation is
∂t(Ax˙xt) + (∂s˙xt)Tτ Ax˙xt + 
∂s((Ax˙xt)T˙xt τ) = 
D′Ax(˙xt, ˙xt)
(.)
with ∂s = ∂u/∣˙xu∣as above.
This class of metrics includes the so-called Sobolev metrics [,] for which
∫
π

(Axξ)Tξdu =
p
∑
k=
ak(x)∫
π

∣∂k
s ξ∣
du
with positive coefficients ak(x), typically depending on the length of x. Let us take one
simple example that has interesting developments: define
∫
π

(Axξ)T ξdu = length(x)−∫
π

∣∂sξ∣du
(.)
or Axξ = −length(x)−∂
s ξ. The metric associated to Ax is degenerate, since it vanishes over
constants. But it provides a metric on curves modulo translations. It satisfies the invariance

Shape Spaces 

properties described above, characterized in (> .), (> .) and (> .). This met-
ric was first introduced in [] and further studied in [, , ]. A direct computation
shows that, in this case,
D′Ax(ξ, η) = length(x)−∂s((∂sξ)T∂sητ) −length(x)−⟨ξ , η⟩xκν.
The study of this metric is, however, much simpler than replacing the expression of
D′Ax into (> .) would make believe. The simplification comes after the following
transformation of the curve representation. Consider the transformation, defined over
pairs of real-valued functions u ↦(a(u),b(u)) by
x(u) = ( 
∫
u
(a−b)d ˜u , ∫
u

abd ˜u), u ∈[,π],
(.)
so that
˙xu = ((a−b)/, ab).
With the notation above, we have ∣˙xu∣= (a+ b)/. This generate a curve in R, with
length
length(x) = 
∫
π

(a+ b)du = 
(∥a∥
+ ∥b∥
).
Denoting by x = T(a,b) the transformation in > Eq. (.), one can write the differential
of T as
DT(a,b)(α, β) : u ↦(∫
u
(aα −bβ)d ˜u , ∫
u
(bα + aβ)d ˜u)
and a direct computation yields
∥DT(a,b)(α, β)∥T(a,b) = 
√
∥α∥
+ ∥β∥

√
∥a∥
+ ∥b∥

.
Restricting to closed curves with unit length implies the conditions
∥a∥= ∥b∥= and ⟨a, b⟩= 
which means that (a,b) forms an orthonormal two-frame in the space L(S), that is, an
element of the Stieffel manifold St(L,). Up to the factor two, the mapping T is then an
isometry between St(L,), equipped with its standard metric, and the subset of I con-
sisting of unit-length curves. If one furthermore makes the reduction of quotienting out
rotations for curves, one finds that the isometry becomes with the Grassmannian mani-
fold Gr(L,) of two-dimensional subspaces of L. This identification can be exploited to
obtain explicit geodesics in the considered shape space (see []). It is important to notice
that the restriction to curves with unit length is equivalent to making the Riemannian
projection on the quotient space modulo scalings. This is because horizontal vectors for
the scale action can easily be shown to satisfy ∫(∂sξ)Tτ = , which, if ξ = ˙xt, directly
implies that ∂t(∫∣˙xu∣) = . Therefore, length is conserved along horizontal geodesics,
which justifies the choice of unit length curves. Some numerical issues associated to this
metric are studied in [] and [] in the simpler case in which it is applied to open curves.
An example of geodesic obtained using this metric is provided in > Fig. -.


Shape Spaces
⊡Fig. -
An example of a geodesic connecting a circle to a star-shaped curve for the metric deﬁned in
(> .). The evolving curves are superimposed with progressively reduced size to
facilitate visualization (the compared curves having both length originally)
A parameterized variant of this metric, applied to closed curves with unit length, has
been proposed in [], in the form:
∥ξ∥
x = ∫
π

((∂s)ξTτ)∣˙xu∣du + c ∫
π

((∂s)ξTν)∣˙xu∣du,
the previous metric corresponding to c = . When c ≠, the unit length constraint is not
induced by a Riemannian projection, but the metric can be studied on this space anyway.
One can analyze this metric in the following way. Let x(t,u) be a time-dependent curve.
Define λ = ∣˙xu∣so that
∂s˙xt = λ−∂t(λτ) = ∂t(log λ)τ + ∂tτ.
Since the two terms in the sum are perpendicular, this gives
∥˙xt∥
x = ∫
π

((∂t log λ)+ c∣∂tτ∣)∣˙xu∣du.
(.)
The first term measures the logarithmic variation of the arc length and the second is the
instantaneous rotation of the tangents. Interestingly, another change of variable akin to the
one discussed for c = can also simplify this metric in the case c = . Take, in this case,
x = T(a, b) := u ↦(∫
u

a
√
a+ bdu′ , ∫
u

b
√
a+ bdu′);
one has this time
∥DT(a, b)(α, β)∥
T(a,b) = ∫
π

(α+ β)du
which provides an identification of the space of open curves with unit length with an
infinite-dimensional sphere. This identification has the important property to carry over
to higher dimensional curves []. There is, however, no “nice” representation for closed
curves in this case.

Shape Spaces 

Notice that the two identifications that were just discussed apply to parameterized
curves. In both cases, the geodesic distance must be optimized with respect to reparame-
terization to obtain a metric between geometric curves.
Another important contribution to the theory of spaces of plane curves was made in
[], in which simple closed domains in Rare represented via the correspondence maps
between the conformal mapping of their interior and of their exterior to the unit disc. This
induces an almost one-to-one representation of simple curves by diffeomorphisms of the
unit circle. In fact, this representation has to come modulo Möbius transformations on
the circle, which are very simply accommodated by an invariant metric, called the Weil–
Peterson metric, on such diffeomorphisms. The reader is referred to the cited work for
more details.
...
Projecting the Action of D Diﬀeomorphisms
At the exception of the one just mentioned, the previously discussed metrics were all
defined based on the parameterizations of the curves. This provided reasonably simple
definitions, exploiting in particular the invariance property of the arc length. Because they
relied on local properties of the curves, these metrics were not able to penalize singularities
that occur globally, like the intersection of two remote parts.
One way to handle global constraints is to use an approach similar to the one that has
been used to define the landmark manifold, based on the action of two-dimensional diffeo-
morphisms on curves. This will therefore be based on the projection paradigm discussed
in > Sect. ....
So, let G ⊂Diff(R) be a group of smooth diffeomorphisms of R(which, say, smoothly
converge to the identity at infinity), and let xbe a reference curve, or template. Consider
the set M = G ⋅x, the orbit of xunder the action of G, the latter being simply defined by
(φ ⋅x)(u) = φ(x(u)).
This implies that Dπ(φ)v = v ○xand a horizontal covector at φ ∈G for the projection
takes the form
(p∣v) = (ρ∣v ○x)
for some ρ ∈Tφ(x)M∗.
Let’s make this explicit for ρ belonging to an important class of linear forms on TxM,
associated to vector measures, that is,
(ρ∣ξ) = ∫
π

ξTadμ
where μ is a measure on the unit circle and a is a vector-valued function. The associated
horizontal covector is then
(p∣v) = ∫
π

v(x(u))Ta(u)dμ(u)
(.)
and the reduced Hamiltonian computed on this covector is (denoting as in > Sect. ...
Kφ the duality operator on TφG, still assumed to be associated to a reproducing kernel)


Shape Spaces
HM(x, ρ) = 
∫
π

∫
π

a(u)TKφ(x(u),x(u′))a(u′)dμ(u′)dμ(u)
(.)
with x = φ ⋅x. As in
> Sect. ..., the invariance requirement boils down to
Kφ(x(u),x(u′)) only depending on φ ⋅x, with the simplest choice associated to a
right-invariant metric on G, yielding
HM(x, ρ) = 
∫
π

∫
π

a(u)TKV(x(u),x(u′))a(u′)dμ(u′)dμ(u)
(.)
for a fixed kernel KV. An important fact is that measure covectors remain so during the
evolution, the Hamiltonian (or geodesic) equations are simply written as
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∂tx(t,u) = ∫
π

KV(x(t,u),x(t, ˜u))a(t, ˜u)dμ(˜u)
∂ta(t,u) = −

∑
i,j=∫
π

ai(t,u)aj(t, ˜u)∇Ki j(x(t,u),x(t, ˜u))dμ(˜u)
(.)
Another interesting fact is that (> .) exactly provides (> .) in the case when
μ is a weighted sum of Dirac measures. This is because these equations are, as proved
in
> Sect. ..., all particular instances of the Hamiltonian system (or geodesic
equation) obtained on the acting group of diffeomorphisms, namely (> .).
This was the first step downward, from diffeomorphisms to parameterized plane
curves. It remains to discuss the additional steps, which are the reduction for the required
invariance, by reparametrization and Euclidean transformation.
Consider the action of reparameterization, which is a right action. The action of change
of parameters on vector measures like in (> .) is
(p ⋅ψ∣ξ) = ∫
π

aT ξ ○ψ−dμ(u) = ∫
π

(a ○ψ)Tξd(ψ−μ)(u),
where ψ ⋅μ is the image of μ by ψ. Using this, the invariance requirement applied to a
Hamiltonian taking the form
HM(x, ρ) = 
∫
π

∫
π

a(u)TKx(u,u′)a(u′)dμ(u′)dμ(u)
(.)
can be seen to reduce to the constraint that
Kx○ψ(ψ−(u),ψ−(u)) = Kx(u,u′)
and this property is satisfied for Kx(u,u) = KV(x(u),x(u′)).
The momentum map associated to changes of parameters is
(m(x, p)∣v) = ∫
π

a(u)T˙xuv(u)dμ(u),
so that horizontal vector measures simply are those for which a is normal to the curve,
that is, a(u) = α(u)ν(u), where α is scalar valued and ν is the normal to x. The evolution
equations then become

Shape Spaces 

⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∂tx(t,u) = ∫
π

KV(x(t,u),x(t, ˜u))α(t, ˜u)ν(t, ˜u)d ˜u
∂tα(t,u) = −α(t,u)

∑
i,j=∫
π

α(t, ˜u)νi(t,u)νj(t, ˜u)∇Ki j(x(t,u),x(t, ˜u))d ˜u.
(.)
If KV is furthermore invariant by rotation and translation, quotienting out these operations
results in additional conditions on α. Invariance by translation requires
∫
π

α(u)ν(u)du = ,
and the constraint associated to rotations is
∫
π

α(u)(ν(u)x(u)T −x(u)ν(u)T)du = .
..
Extension to More General Shape Spaces
The construction based on the Riemannian submersion from groups of diffeomorphisms
can be reproduced in a large variety of contexts, essentially for any class of objects that
can be deformed by diffeomorphisms. This can be applied to provide metrics on space of
surfaces, spaces of images, of vector fields, of measures, etc.
Let us consider, for example, the case of images, that we will take as differentiable
functions I : Rd →R. Define the left action of a diffeomorphism φ on an image I to
be
φ ⋅I = I ○φ−.
From this, one sees that the infinitesimal action of a vector field v on I is
v ⋅I = −vT∇I.
(This is why we assumed that the images are differentiable. For non differentiable images,
v ⋅I is not a function, but a distribution, with, if ρ is a smooth function,
(v ⋅I∣ρ) = ∫Rd I∇⋅(ρv)dx,
where ∇⋅is the divergence operator. The reader is referred to [,] for the analysis of the
inexact matching approach in the more general case of images with bounded variations.)
Fix a reference image Iand consider the space
M = {φ ⋅I, φ ∈G},
the surjection being as usual π(φ) = φ ⋅I. Consider covectors on M that are associated to
measures, namely
(ρ∣ξ) = ∫Rd ξ(x)dρ(x),


Shape Spaces
where ξ is a real-valued function (which represents a tangent vector to M). The differential
of π(φ) = I○φ−is (letting ψ = φ−)
Dπ(φ)v = −(∇I○φ−)D(φ−)v ○φ−= −∇ITv ○φ−
with I = φ ⋅I, so that the horizontal covector at φ ∈G associated to a measure ρ is p =
Dπ(φ)∗ρ defined by
(p∣v) = −∫Rd v(φ−(x))T∇I(x)dρ(x),
where I = φ ⋅I. Starting from a Hamiltonian associated to a right-invariant metric on G
yields the reduced Hamiltonian
HM(I, ρ) = 
∫Rd ∫Rd ∇I(x)TKφ(φ−(x), φ−(y))∇I(y)dρ(y)dρ(x)
= 
∫Rd ∫Rd ∇I(x)TKV(x, y)∇I(y)dρ(y)dρ(x)
with Kφ(x, y) = KV(φ(x), φ(y)). The associated Hamiltonian equations are
{∂tI(x) = ∫Rd ∇I(x)TKV(x, y)∇I(y)dρ(y)dy
∂tα = ∇⋅(αKV(∇Iρ))
A limitation in the image case is that two given images are very rarely connected by
diffeomorphisms, so that working with images that are deformations of a reference image
is a strong restriction. This issue can be addressed by extending the projection to a larger
set that the sole group of diffeomorphisms. One can use a simple construction for this: call
M the space of all smooth images (instead of just an orbit, as it was defined before), and still
let G denote a group of smooth diffeomorphisms. Consider the surjection π : G × M →M
defined by
π(φ, I) = φ ⋅I.
(This is obviously a surjection since I = π(idRd, I).)
Letting J = I ○φ−, one has
Dπ(φ, I)(v, ξ) = −∇JTv ○φ−+ ξ ○φ−,
so that the horizontal covector at (φ, I) associated to a measure ρ on M is ¯p = Dπ(φ, I)∗ρ
such that
(¯p∣(v, ξ)) = ∫Rd(−∇JTv ○φ−+ ξ ○φ−)dρ
with J = I ○φ−. Thinking of a covector ¯p ∈T(φ,I)(G × M)∗as a pair (p, η) with p ∈TφG∗
and η ∈TIM∗, one can identify (p, η) = Dπ(φ, I)∗ρ as
(p∣v) = −∫Rd ∇JTv ○φ−dρ and (η∣ξ) = ∫Rd ξ ○φ−dρ.
If dρ = ρdx is absolutely continuous with respect to Lebesgue’s measure, the second term
gives η = ρ ○φ detDφ dx.

Shape Spaces 

If one starts with a Hamiltonian on G × M for which
H((φ, I),(p, ηdx)) = 
(p∣Kφ p) + λ
∫Rd η(x)(detDφ(x))−dx
with Kφ as above, the resulting reduced Hamiltonian on M is
HM(J, ρdx) = 
∫Rd ∫Rd ρ(x)∇J(x)TKV(x, y)∇J(y)ρ(y)dxdy + λ
∫Rd ρ(x)dx.
The corresponding evolution equations then are
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∂tJ = ∇JTKV(ρ∇J) + λρ
∂tρ = ∇⋅(ρKV(ρ∇J)).
This is a particular instance of the theory of metamorphosis applied to images (the
interested reader can refer to [,] for further developments).
..
Applications to Statistics on Shape Spaces
An important situation in which the previously discussed concepts are relevant is for the
analysis of shape samples, that is, families x, . . . ,xn, in which each xj is a shape, possi-
bly represented as a collection of landmarks or a plane curve (or an other representation,
like surfaces, images, etc…), and interpreted as a point in a manifold M. A simple and
commonly used approach to analyze such samples is to “normalize” them using the expo-
nential or momentum representation relative to a fixed template ¯x. Each shape xk is then
transformed into a tangent or cotangent vector, say ξk ∈T¯xM so that xk = exp¯x(ξk).
The problem is then reduced to the well-explored context of data analysis in a linear
space, and how it is analyzed afterward depends of the specific problem at hand and is out
of the scope of the present discussion. An important thing that one should remember is that
this reduction can be accompanied with significant metric distortion, related to curvature
as described in > Sect. ...(in spaces with positive curvature, the representation may
even fail to be one-to-one). The approach has however proved to be a powerful analysis
tool in several applications [,], including the analysis of medical data [].
This distortion being larger when the distances between the represented shapes and
the template are large, it is natural to select the template in a way that minimizes these
distances, the most widespread approach being to define it as a Karcher (or geometric)
mean, that is, as a minimizer of
U(x) =
n
∑
k=
dM(x,xk).
(.)
This well-posedness of this definition is also related to the curvature. The function U is
convex and the minimum is unique if M has negative curvature []. Negative curvature
is unfortunately difficult to obtain in shape spaces because the reduction process always


Shape Spaces
increases the sectional curvature [] (notice however that the representation in [] has
negative curvature, but it seems to be the only such example). The sectional curvature on
the landmark manifold, as shown in [], can be both positive and negative. As proved
in [], a sufficient condition ensuring the convexity of U (> .) is that the diameter
of the sample set (the largest geodesic distance between two of the points) is smaller than
π/(√smax), where smax is a positive upper bound of the sectional curvature (U is always
convex with negative curvature). Interestingly, in that case, the optimality condition of the
Karcher mean is that it constitutes a sample average in the exponential representation, that
is, xk = exp¯x(ξk) with ∑n
k=ξk = . This leads to an algorithm for the computation of
the mean, which can be proved to converge under similar curvature conditions [, ]:
start with an initial guess for ¯x and compute the exponential representation ξk over the
sample set. Compute ¯ξ = ∑n
k=ξk/n, replace ¯x by exp¯x(−¯ξ), and iterate until stabilization. A
variation of this algorithm has been proposed in []. One can also mention the interesting
algorithm proposed in [] in which kernel regression is generalized to shape manifolds.
.
Numerical Methods and Case Examples
The most important numerical method on the previously discussed shape spaces are related
to the computation of geodesics (i.e., solving the geodesic equation), and, most importantly
in practice, to the computation of the representation in the exponential chart, or of the
momentum representation.
This section will focus on the latter problem (which anyway includes the first one as a
subproblem) that will first be addressed in the simpler case of the landmark manifold.
To compute exponential coordinates around some object x, one needs to solve, for
some target object y, the equation
expx(ξ) = y
(.)
or, if the momentum representation is more convenient,
exp♭
x(α) = y.
(.)
Since these representations are defined by nonlinear evolution equations, this is a highly
nonlinear problem, in which the function to be inverted cannot be written in closed form.
Also, in the case of curves, the problem is infinite dimensional, and must therefore be prop-
erly discretized. Another non-negligible issue is that, even if the equation has a solution
(which is often the case in the discussed framework), this solution is not necessarily unique
unless y is close enough to x. For this reason, it may be impossible to represent a generic
shape dataset using only one of these charts, but this may be achievable for a more focused
one (like, say, shapes of fish, or leaves, of fixed anatomical organs).
There are mainly two options to address the computation. The first one is to directly
solve the equation (using zero-finding methods, like Newton’s algorithm). The second one

Shape Spaces 

is to return to the definition of geodesics as curves with minimal energy, and to solve the
variational problem of finding minimal energy paths between xand y.
..
Landmark Matching via Shooting
Let us start with the first approach. Recall that given some differentiable function F : Rn →
Rn, Newton’s method to solve the equation F(z) = iterates (starting with a good guess of
the solution, z)
zk+= zk −DF(zk)−F(zk).
This scheme can be directly applied to the solution of (> .) in the landmark case,
with F(α) = exp♭
x(α) −y, since it is finite dimensional; one needs for this to compute
the differential of the momentum representation, which is only described in (> .) via
the solution of a differential equation. As a result, the differential of F, which is also the
differential of exp♭
x, must also be computed by solving a differential equation. Noting that
(> .) takes the form
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∂tx = Q(x, α)
∂tα = R(x, α, α)
(.)
with Q linear and R quadratic in α, and that x(t) = exp♭
x(tα), we have, denoting
J(t) = Dexp♭
x(tα)
(.)
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∂tJβ = ∂Q(x, α)Jβ + Q(x, Hβ)
∂tHβ = ∂R(x, α, α)Jβ + R(x, α, Hβ)
(.)
in which H is an auxiliary operator that represents the variation in α (and ∂is the differ-
ential with respect to the first variable). Solving (> .) and (> .) up to time t = 
provides x() and J(), and the newton step is given by
αk+

= αk
−J()−(x() −y).
Making explicit the expressions of ∂Q and ∂R is not difficult, but rather lengthy, and these
expressions will not be provided here (the interested reader can refer to [] for more details).
An important limitation for the feasibility of this kind of approach is the cost involved in
the computation of the full matrix J(t). With N landmarks in d dimensions, the size of x
and α is n = Nd and the size of J is n. The computation of the right-hand size of (> .)
requires an order of noperations if one takes advantages of the special structure of the
operator Q(x,⋅) (it would be notherwise). Even with this reduction, a computation cost
which is cubic in the number of landmarks rapidly becomes unfeasible, and it is difficult
to run this algorithm with, say, more than a few hundred landmarks. On the other hand,
convergence (when it happens) can require a very small number of steps.


Shape Spaces
Another limitation of Newton’s method is the fact that it is not guaranteed to converge,
unless the starting point (α
with our notation) is close enough to the solution, in a way
which is generally impossible to quantify a priori. For this reason, the method is often
usefully complemented (and possibly replaced if the number of landmarks is too large) by
simple gradient descent in which the minimized function is
F(α) = (exp♭
x(α) −y)
T (exp♭
x(α) −y).
The first variation of F is, with the previous notation,
∂εF(α+ εβ)∣[ε=] = (exp♭
x(α) −y)
T J()β.
It is natural to define gradients relative to the Riemannian metric at x, as defined in
> Eq. (.). When working with momenta as done here, the gradient should be identified
using
βTSV(x)∇F(α) = (exp♭
x(α) −y)
T J()β
yielding
∇F(α) = SV(x)−(J()T (exp♭
x(α) −y)).
(.)
The computation, for a given vector z, of J(t)Tz can be done by solving backward in time
the system
⎧⎪⎪⎪⎨⎪⎪⎪⎩
∂tξ = −(∂Q(x, α))Tξ −(∂R(x, α, α))Ta
∂ta = −Q(x)Tξ −R(x, α)Ta
(.)
initialized with (ξ(),a()) = (z,), with the notation Q(x)β = Q(x, β) and R(x, α)β =
R(x, α, β). One then has J()Tz = a(). The proof of this statement derives from
elementary computations on linear dynamical systems.
This implies that the term J()T (exp♭
x(α) −y) can be computed by solving an ODE
which has the same dimension as the geodesic
> Eq. (.). Notice, however, that
(> .) requires using the solution of (> .) with a backward time evolution (from
t = to t = ). This implies that the solution of (> .) must be first computed and stored
with a fine enough time discretization to allow for an accurate solution of (> .). This
may cause memory issues for high dimensional models. An example of trajectories and
deformations estimated using this algorithm is provided in > Fig. -.
The above discussion only addressed the computation of geodesics in landmark shape
space without quotienting out rotations and translations. Recall that this operation, when
done starting from a metric for which the projection on the quotient space is a Rieman-
nian submersion, only requires to constrain the momentum representation with a finite
number of linear relations. The associated reduction in the number of degrees of freedom
is balanced by the reduced requirement of connecting the reference shape to some element
of the orbit of the target under the quotiented out group action, instead of the target itself.

Shape Spaces 

⊡Fig. -
Example of geodesics between two landmark conﬁgurations; Left: trajectories (diamonds
move onto circles); Right: resulting diﬀeomorphism
More explicitly, the equations that need to be solved to compute the momentum represen-
tation of y relative to xare
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
exp♭
x(α) −g ⋅y = 
N
∑
k=
α,k = 
N
∑
k=
(α,kxT
,k −x,kαT
,k) = ,
(.)
where g ∈SE(Rd). A transformation g in this space is represented by a rotation part, R and
a translation part, b, and classically parameterized in the form
⎛
⎝
R
b
⋯

⎞
⎠= exp ⎛
⎝
A
ω
⋯

⎞
⎠
with A skew symmetric and ω ∈Rd. System (> .) therefore has Nd + d(d + )/
equations and variables, and can be solved as above, using Newton iterations when fea-
sible, or gradient descent. Since the exponential is the solution of a differential equation
(∂t exp(tU) = U exp(tU)), optimization in A and ω above can be treated exactly like the
optimization in α. Another option is to directly use the formula
∂ε exp(U + εh)∣ε== ∫

exp(tU)h exp(−tU)dt.


Shape Spaces
..
Landmark Matching via Path Optimization
The other option, in order to compute the momentum representation, is to solve the
shortest path problem between xand y, that is, to minimize
E(x(⋅)) = ∫

∥˙xt∥
x(t)dt,
with the constraints x() = xand x() = y, using gradient descent on the space of all
trajectories t ↦x(t). Letting P(x, ξ) = ∥ξ∥
x, one has
∂εE(x(⋅) + εξ(⋅))∣ε== ∫

(⟨˙xt , ˙ξt⟩x(t) + ∂P(x, ˙xt)T ξ) dt.
Using gradient descent requires selecting an appropriate metric on the space of all time-
dependent objects, and interesting developments arise when selecting a metric for which
the constraints are continuous functionals [, ]. Consider, as an example, the inner
product
⟨ξ(⋅), η(⋅)⟩= ∫


˙ξ
T
t ˙ηtdt
that we restrict to the space of time-dependent ξ and η that vanish at t = and t = . One
can check that, defining
ηx(t) = ∫
t
SV(x(u))˙xt(u)du −∫
t
∫
u

∂P(x(˜u), ˙xt(˜u))d ˜udu
+ (+ t)∫
t
∂P(x(u), ˙xt(u))du,
one has
∂εE(x(⋅) + εξ(⋅))∣ε== ⟨ξ , ∇E(x)⟩
with
∇E(x)(t) = ηx(t) −tηx().
One can therefore use gradient descent to minimize the geodesic energy, in the form
x(n+)(t) = x(n)(t) −ε(ηx(n)(t) −tηx(n)()).
..
Computing Geodesics Between Curves
We now discuss whether, and how, the previous methods extend to the computation of
minimizing geodesics in Riemannian spaces of curves. We start with the metric associ-
ated with the projection from D diffeomorphisms, since it belongs to the same family
as the one discussed with landmarks. In fact, there is a simple way to discretize a curve
matching problem so that it boils down to a landmark matching problem. Assume that
a reference curve xand a target curve y, are given, but that they are only observable in

Shape Spaces 

discrete versions, as sequences of points xdisc

= (x,, . . . , x,N) and ydisc = (y, . . . , yN).
Then, as we have remarked, > Eq. (.) when restricted to discrete momenta of the form
μ =
N
∑
k=
ak ⊗δxk,
boils down to
> Eq. (.), and one can now solve the problem of finding a solution of
this equation that transports x,k to yk exactly as in the previous sections.
Unfortunately, such an approach has little practical use, given that it is very unlikely
that two discrete curves are observed such that the points that constitute them are exactly
homologous. This means that one should not require a given x,k to transform exactly into
yk, but maybe to another yl, or in between two of them. Most of the time, anyway, the
curves are given with different number of points.
This issue is obviously the discrete form of the parameterization invariance that has
been discussed in > Sect. ... We know that the horizontality condition for parameter-
ization invariance induces the constraint that ak is perpendicular to the reference curve. In
this context, the problem in the continuum is formulated as: given xand y, find an initial
momentum μwhich is horizontal at xand such that the solution of (> .) trans-
forms the curve xinto a deformed curve φ(,x) which coincides with y up to a change
of parameterization.
A change of parameter being a diffeomorphism of S, it can be generated with an
equation like (> .). Roughly speaking, this change of parameter can be generated by
momenta that are scalar functions on the unit circle. Horizontal geodesics in spaces of
curves (still roughly speaking) are generated by momenta that are normal to the reference
curve, which can also represented as scalar functions on the unit disc. So, one needs to
find two scalar functions (one for the reparameterization and one for the deformation)
that bring the reference curve xto the target y; the target being also characterized by two
scalar functions (its coordinates), one sees that the dimensions match and that an approach
based on zero finding is possible, at least in principle (there has been no attempt so far in
the literature to solve the curve comparison problem in this way). The problem needs to
be properly discretized, using, for example, the same number of points to represent x, y,
the reparameterization momentum and the deformation momentum.
One can also use a variational approach in the initial momentum, using an objective
function like
E(a) = d(exp♭
x(a),y)

(.)
where d is a reparameterization-invariant distance, like the ones in
> Eqs. (.) and
(> .), which are, since they derive from Hilbert norms, well amenable to variational
computations. The initial momentum acan be discretized as
a=
N
∑
k=
a,k ⊗δuk,


Shape Spaces
where u, . . . ,uN is a discretization of the unit disc, which, as already noticed, lead to
geodesic equations identical to the ones considered with landmarks in (> .), the ini-
tial “landmark positions” being x,k = x(uk). This implies that the variational methods
discussed in > Sect. ..directly apply, simply changing the objective function.
In fact, the same point of view can also be used with other metrics on curves, with the
correct version of the exponential chart or of the momentum representation (whichever is
more convenient). Notice that enforcing the fact that the initial momentum is horizontal
for reparameterization is optional for these methods, as long as the objective function (the
distance d) is parameterization invariant. Disregarding discretization issues, the optimal
solution will always be horizontal, so one does not need to make an exact count of the min-
imal number of degrees of freedom, as was required by zero-finding methods. In practice,
the computational efficiency resulting from the reduction of the number of variables can
be counter balanced by the additional flexibility in moving in the space of solutions which
is offered by over-parameterized formulations, the choice between the two options being
problem dependent.
Finally, notice that path-minimizing methods are also available for curve matching
(an approach similar to the one discussed for landmarks in the previous section has been
proposed in []).
..
Inexact Matching and Optimal Control Formulation
...
Inexact Matching
In many cases, requiring an exact representation of the target y in the exponential chart
is not needed, and even undesirable. In most instances, indeed, there is an inherent inac-
curacy in the way objects are acquired. Landmarks, whether manually or automatically
selected, are rarely well defined and the process can lead to significant variability. The same
holds for curves, or surfaces, which are generally extracted using segmentation algorithms,
sometimes applied to noisy data, with results that cannot be assumed to be perfect.
Formulations in which geodesics are only required to provide a good approximation
of the target then make sense and have a large range of applications. They are akin to the
variational methods that were discussed for exact representation, in that they minimize
an appropriate distance between the end-point of a geodesic and the target, but they also
include a penalty term on the length or the energy of the geodesic. In other terms, instead
of minimizing d (exp♭
x(a),y)
like in (> .), for example, one would minimize
E(a) = d (exp♭
x(a),y)
+ σ ∥a∥
x
(in the momentum representation, the norm is for the dual metric in the cotangent space
at x). If it is more convenient to use an exponential chart instead of the momentum
representation, just minimize, over all tangent vectors ξat x,
E(ξ) = d(expx(ξ),y)
+ σ ∥ξ∥
x

Shape Spaces 

⊡Fig. -
Two results of inexact matching with the vector-measure distance between curves as error
term. The lower right curve (light gray) is the target. The ﬁrst curves provide the geodesic
evolution
Since this formulation only adds the term σ a(or σ ξ) to the gradient of the
objective function that was used for exact representation (i.e., with σ = ), the methods
that were described in the previous paragraphs can be adapted with minor changes, and
yield, for example, results like those provided in > Fig. -. Interestingly, in this context,
additional methods, deriving from optimal control theory, become available too.
...
Optimal Control Formulation
Let us first return to the general principles discussed in
> Sect. ...and consider an
optimal control problem with an additional end-point cost E:
minimize ∫

L(q,u)dt + E(q()) subject to ˙q = f (q,u) and q() fixed.
Notice that here q() is free, but this situation is handled quite similarly to the one with
fixed q(). Introduce
JE(q, p,u) = J(q, p,u) + E(q())
= ∫

(L(q,u) + (p∣˙qt −f (q,u))) dt + E(q()).
The only change in the analysis arises when working out the variation in q which now gives
the extra end-point condition
p() + DE(q()) = ,
(.)
which come in addition to the previously obtained (> .).
The conservation of the momentum map can be extended to this case when a group
G acts on Q and the Hamiltonian H is G-invariant. If, in addition, E is also G-invariant,
one deduces from E(qg) = E(q) for all g the fact that (DE(q)∣ξg) = for all ξ ∈G
which is exactly m(q, DE(q)) = where the momentum map m is defined in (> .).


Shape Spaces
Therefore, (> .) implies that m(q(), p()) = , which, combined with the conserva-
tion of momentum, implies that
m(q(t), p(t)) = .
for any t ∈[,]. Thus, the momentum map is not only invariant, but vanishes along solu-
tions of the optimal control problem. In the context of the reduction discussed in > Sect.
..., this says that the momentum associated to a solution is horizontal.
...
Gradient w.r.t. the Control
One can compute the variations with respect to u of
C ≐∫

L(q,u)dt + E(q())
subject to the constraint ˙q = f (q,u) and q() fixed.
Taking the variation with respect to this constraint yields
∂tδq = ∂q f δq + ∂u f δu.
Introduce the semigroup Ps,t solution of ∂tPs,t = ∂q f Ps,t with Ps,s = id, so that
δqt = ∫
t
Ps,t(∂u f )sδusds.
One can write
δC = ∫

(((∂qL)t∣∫
t
Ps,t(∂u f )sδusds) + (∂uL∣δu(t))) dt
+ (DE(q())∣∫

Ps,(∂u f )sδusds).
Interverting integrals in s and t yields
δC = ∫

(∂uL −(∂u f )∗
s p(s)∣δu(s))ds = ∫

(−∂uH∣δu(s))ds
(.)
with
p(s) ≐−(∫

s
P∗
s,t(∂qL)tdt + P∗
s,DE(q()))
which is characterized by p() + DE(q()) = and ∂tp = ∂qL −∂q f ∗p = −∂qH(q, p,u).
The last two conditions are precisely δJE/δq = for
JE = ∫

((p∣˙q) −H(q, p,u))dt + E(q())

Shape Spaces 

as above. Since ˙qt = f (q,u) is δJE/δp = , one gets from (> .) that δC/δu = δJE/δu
for δJE/δp = δJE/δq = .
...
Application to the Landmark Case
In the landmark case u = α, q = x, L(x, α) = αTSV(x)α/and ˙x = f (x, α) = SV(x)α, so
that ∂uH(x, p, α) = SV(x)(p −α) and
δC = ∫

⟨α −p , δα(s)⟩xds
The gradient of C is therefore particularly simple to compute if one chooses along the path
the natural metric given on the α’s by the matrix SV(x) (cf.
> Sect. ... This gives
the updating rule (see []) : αn+= αn −Δt(α −pn), ˙qn+
t
= f (qn+, αn+), where pn is
computed by the backward integration of the ode ˙pn
t = −∂qH(qn, pn, αn) with end-point
condition pn() + E(qn()) = .
.
Conclusion
Even if would be impossible to provide a comprehensive description of every method that
has been devised in this domain, this chapter provides an introduction to many of the
mathematical constructions of spaces of shapes. The combined description of the Rie-
mannian and of the Hamiltonian point of views, which are complementary, should help
the reader to a more thorough understanding of the range of available methods, whether
they were described in this chapter or elsewhere in the literature. The described numerical
methods are basic components that can also be found in most of the contributions that
were not directly addressed here.
Mathematical shape analysis remains a domain of intensive research, with open prob-
lems arising both for fundamental aspects (e.g., with buiding spaces of three-dimensional
shapes) and for numerical issues and their connections with applications. It is however
likely that the concepts introduced here will remain relevant and serve as foundations for
future work.
.
Cross-References
> Large-Scale Inverse Problems
> Manifold Intrinsic Similarity
> Variational Approach in Image Analysis
> Variational Methods and Shape Spaces


Shape Spaces
References and Further Reading
. Allassonniere S, Trouve A, Younes L ()
Geodesic shooting and diffeomorphic match-
ing via textured meshes. In: Proceedings of
EMMCVPR,
vol

of
LNCS.
Springer,
Berlin/Heidelberg
. Amit Y, Piccioni P () A non-homogeneous
markov process for the estimation of gaussian
random fields with non-linear observations. Ann
Probab :–
. Arad N, Dyn N, Reisfeld D, Yeshurun Y ()
Image warping by radial basis functions: applica-
tion to facial expressions. CVGIP: Graph Models
Image Process ():–
. Arad N, Reisfeld D (). Image warping using
few anchor points and radial functions. Comput
Graph Forum :–
. Arnold VI () Sur un principe variationnel
pour les ecoulements stationnaires des liquides
parfaits et ses applications aux problemesde stan-
bilite non lineaires. J Mecanique :–
. Arnold VI () Mathematical methods of clas-
sical mechanics. Springer, , New York. Sec-
ond edition 
. Aronszajn N () Theory of reproducing ker-
nels. Trans Am Math Soc :–
. Beg MF, Miller MI, Trouve A, Younes L ()
Computing large deformation metric mappings
via geodesic flows of diffeomorphisms. Int J
Comp Vis ():–
. Bookstein FL () Principal warps: thin plate
splines and the decomposition of deformations.
IEEE Trans PAMI ():–
. Bookstein FL () Morphometric tools for
landmark data; geometry and biology. Cam-
bridge University Press, Cambridge
. Camion V, Younes L () Geodesic interpo-
lating splines. In: Figueiredo M, Zerubia J, Jain
K (eds) EMMCVPR , vol of Lecture
notes in computer sciences. Springer, Berlin, pp
–
. Christensen GE, Rabbitt RD, Miller MI ()
Deformable templates using large deforma-
tion kinematics. IEEE Trans Image Process
():–
. Davis BC, Fletcher PT, Bullitt E, Joshi S (Decem-
ber ) Population shape regression from
random design data. In: IEEE th international
conference on computer vision (ICCV), pp –
. Do Carmo MP () Riemannian geometry.
Birkäuser, Boston
. Dryden IL, Mardia KV () Statistical shape
analysis. Wiley, New York
. Duchon J () Interpolation des fonctions de
deux variables suivant le principe de la exion des
plaques minces. R.A.I.R.O. Analyse Numerique
:–
. Dupuis P, Grenander U, Miller M () Varia-
tional problems on flows of diffeomorphisms for
image matching. Quart Appl Math :–
. Dyn N () Interpolation and approximation
by radial and related functions. In: Chui CK, Shu-
maker LL, Ward JD (eds) Approximation theory
VI, vol . Academic, San Diego, pp –
. Federer H () Geometric measure theory.
Springer, New York
. Fletcher PT, Lu C, Pizer M, Joshi S () Prin-
cipal geodesic analysis for the study of nonlin-
ear statistics of shape. IEEE Trans Med Imaging
():–
. Fletcher PT, Venkatasubramanian S, Joshi S
() Robust statistics on Riemannian mani-
folds via the geometric median. In: Computer
vision and pattern recognition. CVPR . IEEE
conference on computer vision, pp –
. Glaunes J () Transport par difféomor-
phismes de points, de mesures et de courants
pour la comparaison de formes et l’anatomie
numérique. Ph.D. thesis, University of Paris ,
Paris (in French)
. Glaunes J, Qiu A, Miller MI, Younes L ()
Large deformation diffeomorphic curve match-
ing. Int J Comput Vis ():–
. Glaunes J, Trouve A, Younes L () Dif-
feomorphic
matching
of
distributions:
a
new approach for unlabelled point-sets and
sub-manifolds matching. In: Proceedings of
CVPR’
. Glaunes J, Trouve A, Younes L () Model-
ing planar shape variation via Hamiltonian flows
of curves. In: Krim H, Yezzi A (eds) Statistics
and analysis of shapes. Springer Birkhauser, pp
–
. Glaunes J, Vaillant M, Miller MI () Land-
mark matching via large deformation diffeomor-
phisms on the sphere. J Math Imag Vis :
–

Shape Spaces 

. Grenander U () General pattern theory.
Oxford Science Publications, Oxford
. Grenander U, Chow Y, Keenan DM () Hands:
a pattern theoretic study of biological shapes.
Springer, New York
. Grenander U, Keenan DM () On the shape
of plane images. Siam J Appl Math ():
–
. Grenander U, Miller MI () Computational
anatomy: an emerging discipline. Quart Appl
Math LVI():–
. Hamilton RS () The inverse function theo-
rem of Nash and Moser. Bull Am Math Soc (N.S.)
():–
. Helgason S () Differential geometry, lie
groups
and
symmetric
spaces.
Academic,
New York
. Holm DD () Geometric mechanics. Imperial
College Press, London
. Holm DD, Marsden JE, Ratiu TS () The
Euler–Poincaré equations and semidirect prod-
ucts with applications to continuum theories.
Adv Math :–
. Holm DR, Trouvé A, Younes L () The Euler–
Poincaré theory of metamorphosis. Quart Appl
Math :–.
. Joshi S, Miller M () Landmark matching via
large deformation diffeomorphisms. IEEE Trans
Image Process ():–
. Joshi SH, Klassen E, Srivastava A, Jermyn I ()
A novel representation for Riemannian analy-
sis of elastic curves in Rn. In: Proceedings of
CVPR’
. Jost J () Riemannian geometry and geomet-
ric analysis, nd edn. Springer, Berlin
. Karcher H () Riemannian center of mass
and mollifier smoothing. Comm Pure Appl Math
():–
. Kendall DG () Shape manifolds, Procrustean
metrics and complex projective spaces. Bull Lond
Math Soc :–
. Kendall DG, Barden D, Carne TK, Le H ()
Shape and shape theory. Wiley, New York
. Klassen E, Srivastava A, Mio W, Joshi S ()
Analysis of planar shapes using geodesic paths on
shape spaces. IEEE Trans PAMI :–
. Klassen E, Srivastava A, Mio W, Joshi SH ()
Analysis of planar shapes using geodesic paths
on shape spaces. IEEE Trans Pattern Anal Mach
Intell ():–
. Kriegl A, Michor PW () The convenient set-
ting of global analysis. Mathematical Surveys and
Monographs . AMS, Providence
. Kriegl A, Michor PW () Regular infinite
dimensional lie groups. J Lie Theory ():–
. Le H () Mean size-and-shapes and mean
shapes: a geometric point of view. Adv Appl Probl
:–
. Le
H
()
Estimation
of
Riemannian
barycentres. Lond Math Soc J Comput Math :
–
. Marques
JA,
Abrantes
AJ
()
Shape
alignment-optimal
initial
point
and
pose
estimation. Pattern Recogn Lett :–
. Marsden
JE
()
Lectures
on
geometric
mechanics.
Cambridge
University
Press,
New York
. Marsden JE, Ratiu TS () Introduction to
mechanics and symmetry. Springer, Berlin
. Meinguet J () Multivariate interpolation at
arbitrary points made simple. J Appl Math Phys
:–
. Mennucci
A,
Yezzi
A
()
Metrics
in
the
space
of
curves.
Technical
report,
arXiv:mathDG/v
. Micheli M () The differential geometry of
landmark shape manifolds: metrics, geodesics,
and curvature. Ph.D. thesis, Brown University,
Providence
. Michor PW, Mumford D () Vanishing
geodesic distance on spaces of submanifolds and
diffeomorphisms. Documenta Math :–
. Michor PW, Mumford D () Riemannian
geometries on spaces of plane curves. J Eur Math
Soc :–
. Michor PW, Mumford D () An overview
of the Riemannian metrics on spaces of curves
using the Hamiltonian approach. Appl Comput
Harmonic Anal ():–
. Miller MI, Trouvé A, Younes L () Geodesic
shooting for computational anatomy. J Math
Image Vis ():–
. Miller MI, Younes L () Group action,
diffeomorphism and matching: a general frame-
work. Int J Comp Vis :–(Originally
published
in
electronic
form
in:
Proceed-
ing
of
SCTV
,
http://www.cis.ohiostate.
edu/szhu/SCTV.html)
. O’Neill B () The fundamental equations of a
submersion. Michigan Math J :–


Shape Spaces
. Qiu A, Younes L, Miller MI () Intrinsic
and extrinsic analysis in computational anatomy.
Neuroimage ():–
. Qiu A, Younes L, Wang L, Ratnanather JT,
Gillepsie SK, Kaplan K, Csernansky J, Miller MI
() Combining anatomical manifold infor-
mation via diffeomorphic metric mappings for
studying cortical thinning of the cingulate gyrus
in schizophrenia. NeuroImage ():–
. Shah J () Htype Riemannian metrics on
the space of planar curves. Quart Appl Math :
–
. Sharon E, Mumford D () d-shape analy-
sis using conformal mapping. Int J Comput Vis
():–
. Small C () The statistical theory of shape.
Springer, New York
. Wendworth D () Thompson on growth and
form. Dover Publications, , Mineola, Revised
edition 
. Trouvé A () Infinite dimensional group
action and pattern recognition. Technical report.
DMI, Ecole Normale Supérieure (unpublished)
. Trouvé A () Diffeomorphism groups and
pattern matching in image analysis. Int J Comp
Vis ():–
. Trouvé A, Younes L () Diffeomorphic
matching in d: designing and minimizing
matching
functionals.
In:
Vernon
D
(ed)
Proceedings of ECCV 
. Trouvé A, Younes L () On a class of optimal
matching problems in dimension. Siam J Contr
Opt ():–
. Trouvé A, Younes L () Local geometry
of deformable templates. SIAM J Math Anal
():–
. Trouvé A, Younes L () Metamorphoses
through lie group action. Found Comp Math
pp –
. Trouvé A () Action de groupe de dimension
infinie et reconnaissance de formes. C R Acad Sci
Paris Ser I Math ():–
. Twinings C, Marsland S, Taylor C () Measur-
ing geodesic distances on the space of bounded
diffeomorphisms. In: British machine vision
conference
. Vaillant M, Glaunés J () Surface matching via
currents. In: Springer (ed), Proceedings of infor-
mation processing in medical imaging (IPMI
), No. in Lecture notes in computer
science
. Vaillant M, Miller MI, Trouvé A, Younes L
() Statistics on diffeomorphisms via tan-
gent space representations. Neuroimage (S):
S–S
. Vialard F-X () Hamiltonian approach to
shape spaces in a diffeomorphic framework: from
the discontinuous image matching problem to a
stochastic growth model.Ph.D. thesis, Ecole Nor-
male Supérieure de Cachan. http://tel.archives-
ouvertes.fr/tel-/fr/
. Vialard F-X, Santambrogio F () Extension
to bv functions of the large deformation diffeo-
morphisms matching approach. C R Math 
(–):–
. Wahba G () Spline models for observational
data. SIAM, Philadelphia
. Wang L, Beg MF, Ratnanather JT, Ceritoglu C,
Younes L, Morris J, Csernansky J, Miller MI
() Large deformation diffeomorphism and
momentum based hippocampal shape discrimi-
nation in dementia of the Alzheimer type. IEEE
Trans Med Imaging : –
. Younes L () Computable elastic distances
between shapes. SIAM J Appl Math ():
–
. Younes L () Optimal matching between
shapes via elastic deformations. Image Vis Com-
put :–
. Younes L, Michor P, Shah J, Mumford D ()
A metric on shape spaces with explicit geodesics.
Rend Lincei Mat Appl :–

Variational Methods
in Shape Analysis
Martin Rumpf ⋅Benedikt Wirth
.
Introduction....................................................................
.
Background.....................................................................
.
Mathematical Modeling and Analysis.........................................
..
Recalling the Finite-Dimensional Case.............................................
..
Path-Based Viscous Dissipation Versus State-Based Elastic Deformation
for Non-rigid Objects..................................................................
...
Path-Based, Viscous Riemannian Setup.............................................
...State-Based, Path-Independent Elastic Setup......................................
...Conceptual Differences Between the Path- and State-Based Dissimilarity
Measures.................................................................................
.
Numerical Methods and Case Examples......................................
..
Elasticity-Based Shape Space.........................................................
...
Elastic Shape Averaging...............................................................
...Elasticity-Based PCA..................................................................
..
Viscous Fluid-Based Shape Space...................................................
..
A Collection of Computational Tools...............................................
...Shapes Described by Level Set Functions..........................................
...Shapes Described via Phase Fields...................................................
...Multi-Scale Finite Element Approximation........................................
.
Conclusion......................................................................
.
Cross-References................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Variational Methods in Shape Analysis
Abstract: The concept of a shape space is linked both to concepts from geometry and
from physics. On one hand, a path-based viscous flow approach leads to Riemannian dis-
tances between shapes, where shapes are boundaries of objects that mainly behave like
fluids. On the other hand, a state-based elasticity approach induces a (by construction)
non-Riemannian dissimilarity measure between shapes, which is given by the stored elastic
energy of deformations matching the corresponding objects. The two approaches are both
based on variational principles. They are analyzed with regard to different alpplications, and
a detailed comparison is given.
.
Introduction
The analysis of shapes as elements in a frequently infinite-dimensional space of shapes
has attracted increasing attention over the last decade. There are pioneering contribu-
tions in the theoretical foundation of shape space as a Riemannian manifold as well as
path-breaking applications to quantitative shape comparison, shape recognition, and shape
statistics. The aim of this chapter is to adopt a primarily physical perspective on the space
of shapes and to relate this to the prevailing geometric perspective. Indeed, we here con-
sider shapes given as boundary contours of volumetric objects, which consist either of a
viscous fluid or an elastic solid.
In the first case, shapes are transformed into each other via viscous transport of fluid
material, and the flow naturally generates a connecting path in the space of shapes. The vis-
cous dissipation rate – the rate at which energy is converted into heat due to friction – can
be defined as a metric on an associated Riemannian manifold. Hence, via the computation
of shortest transport paths one defines a distance measure between shapes.
In the second case, shapes are transformed via elastic deformations, where the asso-
ciated elastic energy only depends on the final state of the deformation and not on the
path along which the deformation is generated. The minimal elastic energy required to
deform an object into another one can be considered as a dissimilarity measure between
the corresponding shapes.
In what follows we discuss and extensively compare the path-based and the state-based
approach. As applications of the elastic shape model, we consider shape averages and a
principal component analysis of shapes. The viscous flow model is used to exemplarily
cluster D and D shapes and to construct a flow type nonlinear interpolation scheme.
Furthermore, we show how to approximate the viscous, path-based approach with a time-
discrete sequence of state-based variational problems.
.
Background
The structure of shape spaces and statistical analyses of shapes have been examined in
various settings, and applications range from the computation of priors for segmentation
[, , ] and shape classification [, , , ] to the construction of standardized

Variational Methods in Shape Analysis 

anatomical atlases [, , ]. Among all existing approaches, a number of different
concepts of a shape are employed, including landmark vectors [, ], planar curves
[, , ], surfaces in
 [, , ], boundary contours of objects [, , ],
multiphase objects [] as well as the morphologies of images [].
The analysis of a shape space is typically based on a notion of a distance or dissimilar-
ity measure d(⋅,⋅) between shapes [, , , , , ], whose definition frequently takes
a variational form. This distance can be used to define an average [, ] or a median
[, ] S of given shapes S, . . . ,Sn according to S = argmin ˜S ∑n
i=d( ˜S,Si)p for p = 
and p = , respectively (cf. > Sect. ...). Likewise, shape variations can be obtained
by a principal component analysis (PCA, cf. > Sect. ...) or a more general covari-
ance analysis in a way which is consistent with the dissimilarity measure between shapes
[, , , ]. From the conceptional point of view, one can distinguish two types of these
dissimilarity or distance measures which may be characterized as rather state based or path
based, respectively. While the first approach is independent of the notion of paths of shapes,
the latter distance definition requires the computation of an optimal, connecting path in
shape space. In some cases, both concepts coincide: The Euclidean distance between two
points, e.g., can equivalently be interpreted in a state-based manner as the norm of the dif-
ference vector or as the length of the shortest connecting path (we shall provide a physical
interpretation for each case in > Sect. ..).
The notion of a shape space was already introduced by Kendall in [], who con-
siders shapes as k-tuples of points in
 d, endowed with the quotient metric of
 kd with
respect to similarity transforms. Often, however, a shape space is just modeled as a linear
vector space which is not invariant with respect to shift or rotation a priori. In the simplest
case, such a shape space is made up of vectors of landmark positions, and distances between
shapes can be evaluated in a state-based manner as the Euclidean norm of their difference.
Chen and Parent [] investigated averages of D contours already in . Cootes et al.
perform a PCA on training shapes with consistently placed landmarks to obtain priors for
edge-based image segmentation []. Hafner et al. use a PCA of position vectors cover-
ing the proximal tibia to reconstruct the tibia surface just from six dominant modes [].
Perperidis et al. automatically assign consistent landmarks to training shapes by a non-
rigid registration as a preprocessing step for a PCA of the cardiac anatomy []. Söhn et al.
compute dominant eigenmodes of landmark displacement on human organs, also using
registration for preprocessing [].
As an infinite-dimensional vector space, the Lebesgue-space Lhas served as shape
space, where again shape alignment is a necessary preprocessing step. Leventon et al. iden-
tify shapes with their signed distance functions and impose the Hilbert space structure
of Lon them to compute an average and dominant modes of variation []. Tsai et al.
apply the same technique to D prostate images []. Dambreville et al. also compute shape
priors, but using characteristic instead of signed distance functions [].
A more sophisticated state-based shape space is obtained by considering shapes as
subsets of an ambient space with a metric d(⋅,⋅) and endowing them with the Hausdorff
distance
dH(S,S) = max{sup
x∈S
inf
y∈Sd(x, y), sup
y∈S
inf
x∈Sd(x, y)}


Variational Methods in Shape Analysis
between any two shapes S,S. Charpiat et al. employ smooth approximations of the
Hausdorff distance based on a comparison of the signed distance functions of shapes [].
For a given set of shapes, the gradient of the shape distance functional at the average shape
is regarded as shape variation of the average and used to analyze its dominant modes
of variation []. Frame indifference is mimicked by an inner product that weights rota-
tions, shifts, scalings, and the orthogonal complement to these transformations differently.
Charpiat et al. also consider gradient flow morphing from one shape onto another one
which can be regarded as a means to obtain meaningful paths even in shape spaces with
state-based distance measures.
An isometrically invariant distance measure between shapes (or more general metric
spaces) that is also not based on connecting paths is provided by the Gromov–Hausdorff
distance, which can be defined variationally as
dGH(S,S) = 

inf
ϕ:S→S
ψ:S→S
sup
yi=ϕ(xi)
ψ(yi)=xi
∣dS(x, x) −dS(y, y)∣,
where dSi(⋅,⋅) is a distance measure between points in Si . The Gromov–Hausdorff distance
represents a global, supremum-type measure of the lack of isometry between two shapes.
Memoli and Sapiro use this distance for clustering shapes described by point clouds,
and they discuss efficient numerical algorithms to compute Gromov–Hausdorff distances
based on a robust notion of intrinsic distances dS(⋅,⋅) on the shapes []. Bronstein et al.
incorporate the Gromov–Hausdorff distance concept in various classification and mod-
eling approaches in geometry processing []. Memoli investigates the relation between
the Gromov–Hausdorff distance and the Hausdorff distance under the action of Euclidean
isometries as well as Lp-type variants of the Gromov–Hausdorff distance [].
In [], Manay et al. define shape distances via integral invariants of shapes and
demonstrate the robustness of this approach with respect to noise.
Another distance or dissimilarity measure which also measures the lack of isometry
between shapes can be obtained by interpreting shapes as boundaries of physical objects
and measuring the (possibly nonlinear) deformation energy of an elastic matching defor-
mation ϕ between two objects [, ]. Since, by the axiom of elasticity, this energy solely
depends on the original and the final configuration of the deformed object but not on the
deformation path, the elastic dissimilarity measure can clearly be classified as state based
(as will be detailed in > Sect. ...). This physical approach comes along with a nat-
ural linearization of shapes via boundary stresses to perform a covariance analysis []
and will be presented in > Sect. ... Pennec et al. define a nonlinear elastic energy as
the integral over the ambient space of an energy density that depends on the logarithm of
the Cauchy–Green strain tensor DϕTDϕ [, ], which induces a symmetric state-based
distance.
Typical path-based shape spaces have the structure of a Riemannian manifold. Here,
the strength of a shape variation is measured by a Riemannian metric, and the square root
of the Riemannian metric evaluated on the temporal shape variation is integrated along a
path of shapes to yield the path length. The length of the shortest path between two shapes

Variational Methods in Shape Analysis 

represents their geodesic distance d(⋅,⋅). Averages are obtained via the Fréchet mean [],
which was further analyzed by Karcher []. There is also a natural linear representation
of shapes in the tangent space at the Fréchet mean via the logarithmic map, which enables
a PCA.
A Riemannian shape space which might still be regarded as rather state- than path-
oriented is given by the space of polygonal medial axis representations, where each shape
is described by a polygonal lattice and spheres around each vertex []: Here, the Lie group
structure of the medial representation space can be exploited to approximate the Fréchet
mean as exponential map of the average of the logarithmic maps of the input. Fletcher
et al. perform a PCA on these log-maps to obtain the dominant geometric variations of
kidney shapes [] and brain ventricles []. Fuchs and Scherzer use the PCA on log-
maps to obtain the covariance of medial representations, and they use a covariance-based
Mahalanobis distance to impose a new metric on the shape manifold. This metric is
employed to obtain priors for edge-based image segmentation [, ].
Kilian et al. compute and extrapolate geodesics between triangulated surfaces of fixed
mesh topology, using isometry invariant Riemannian metrics that measure the local distor-
tion of the grid []. Eckstein et al. employ different metrics in combination with a smooth
approximation to the Hausdorff distance to perform gradient flows for shape matching
[]. Liu et al. use a discrete exterior calculus approach on simplicial complexes to com-
pute geodesics and geodesic distances in the space of triangulated shapes, in particular
taking care of higher genus surfaces [].
An infinite-dimensional Riemannian shape space has been developed for planar
curves. Klassen et al. propose to use as a Riemannian metric, the L-metric on variations of
the direction or curvature functions of arclength-parameterized curves. They implement
a shooting method to find geodesics [], while Schmidt and Cremers present an alterna-
tive variational approach []. Srivastava et al. assign different weights to the L-metric on
stretching and on bending variations and obtain an elastic model of curves []. Michor
and Mumford examine Riemannian metrics on the manifold of smooth regular curves [].
They show the standard L-metric in tangent space, leading to arbitrarily short geodesics
and hence employ a curvature-weighted L-metric instead. Yezzi and Mennucci resolved
the problem taking into account the conformal factor in the metric []. Sundaramoorthi
et al. use Sobolev metrics in the tangent space of planar curves to perform gradient flows
for image segmentation via active contours []. Michor et al. discuss a specific metric
on planar curves, for which geodesics can be described explicitly []. In particular, they
demonstrate that the sectional curvature on the underlying shape space is bounded from
below by zero, which points out a close relation to conjugate points in shape space and thus
to only locally shortest geodesics. Finally, Younes considers a left-invariant Riemannian
distance between planar curves by identifying shapes with elements of a Lie group acting
on one reference shape [].
When warping objects bounded by shapes in
 d, a shape tube in
 d+is formed.
Delfour and Zolésio [] rigorously develop the notion of a Courant metric in this con-
text. A further generalization to classes of non-smooth shapes and the derivation of the
Euler–Lagrange equations for a geodesic in terms of a shortest shape tube is investigated
by Zolésio in [].


Variational Methods in Shape Analysis
Dupuis et al. [] and Miller et al. [, ] define the distance between shapes based
on a flow formulation in the embedding space. They exploit the fact that in case of suf-
ficient Sobelev regularity for the motion field v on the whole surrounding domain Ω,
the induced flow consists of a family of diffeomorphisms. This regularity is ensured by
a functional ∫

∫Ω Lv ⋅v dx dt, where L is a higher-order elliptic operator [, ]. Geo-
metrically, ∫Ω Lv ⋅v dx is the underlying Riemannian metric, and we will discuss related,
path-based concepts in > Sect. .... Under sufficient smoothness assumptions, Beg
et al. derive the Euler–Lagrange equations for the diffeomorphic flow field []. To com-
pute geodesics between hypersurfaces in the flow of diffeomorphism framework, a penalty
functional measures the distance between the transported initial shape and the given
end shape. Vaillant and Glaunès [] identify hypersurfaces with naturally associated two
forms and used the Hilbert space structures on the space of these forms to define a mis-
match functional. The case of planar curves is investigated under the same perspective by
Glaunès et al. in []. To enable the statistical analysis of shape structures, parallel transport
along geodesics is proposed by Younes et al. [] as the suitable tool to transfer structural
information from subject-dependent shape representations to a single template shape.
In most applications, shapes represent boundary contours of physical objects. Fletcher
and Whitaker adopt this viewpoint to develop a model for geodesics in shape space which
avoids overfolding []. Fuchs et al. [] propose a Riemannian metric on a space of shape
contours, motivated by linearized elasticity. This metric can be interpreted as the rate of
physical dissipation during the deformation of a viscous liquid object [, ] and will be
elaborated in > Sect. ...
Finally, a shape space is sometimes understood as a manifold, learnt from train-
ing shapes and embedded in a higher-dimensional (often linear) space. Many related
approaches are based on kernel density estimation in feature space. Here, the manifold is
described by a probability distribution in the embedding space, which is computed by map-
ping points of the embedding space into a higher-dimensional feature space and assuming
a Gaussian distribution there. In general, points in feature space have no exact preimage
in shape space, so that approximate preimages have to be obtained via a variational for-
mulation []. Cremers et al. use this technique to obtain D silhouettes of D objects as
priors for image segmentation []. Rathi et al. provide a comparison between kernel PCA,
local linear embedding (LLE), and kernel LLE (kernel PCA only on the nearest neighbors)
[]. Thorstensen et al. approximate the shape manifold using weighted Karcher means of
nearest neighbor shapes obtained by diffusion maps [].
.
Mathematical Modeling and Analysis
..
Recalling the Finite-Dimensional Case
At first, let us investigate distances and their relation to concepts from physics in the simple
case of Euclidian space. In Euclidean space, shortest paths are straight lines, and they are

Variational Methods in Shape Analysis 

unique, so that the distance computation involves only the states of the two end points:
The geodesic distance between any two points x, x∈
 d is given by the norm of the
difference, ∥x−x∥, which implies the equivalence of the state-based and the path-based
perspective. A corresponding physical view might be the following. Considering that –
by Hooke’s law – the stored elastic energy of an elastic spring extended from xto xis
given by W = 
C∥x−x∥
for the spring constant C, the distance can be interpreted in a
state-based manner as the square root of the elastic spring energy (> Fig. -). Likewise,
from a path-based point of view, the minimum dissipated energy of a dashpot which is
extended from xto xat constant speed within the fixed time interval [,] reads Diss
= ∫

μ ∥v∥
dt = μ ∥x−x∥
, where μ is the dashpot parameter and the velocity is
given by v = x−x. Using this physical interpretation, we can express for instance the
arithmetic mean x = 
n ∑n
i=xi = argmin ˜x ∑n
i=∥xi −˜x∥
of a given set of points x, . . . , xn ∈
 d either as the minimizer of the total elastic deformation energy in a system, where the
average x is connected to each xi by elastic springs or as the minimizer of the total viscous
dissipation when extending dashpots from xi to x.
Before we investigate the same concepts on more general Riemannian manifolds, let
us briefly recall some basic notation. A Riemannian manifold is a set M that is locally
diffeomorphic to Euclidean space. Given a smooth path x(t) ∈M, t ∈[,], we can
define its derivative ˙x(t) at time t as a tangent vector to M at x(t). The vector space
of all such tangent vectors makes up the tangent space Tx(t)M, and it is equipped with
the metric gx(t)(⋅,⋅) as the inner product. The length of a path x(t) ∈M, t ∈[,],
is defined as ∫


√
gx(t)(˙x(t), ˙x(t))dt, and locally shortest paths are denoted geodesics.
They can be shown to minimize ∫

gx(t)(˙x(t), ˙x(t))dt [, Lemma .]. Let us empha-
size that a general geodesic is only locally the shortest curve. In particular, there might be
multiple geodesics of different length connecting the same end points. The geodesic dis-
tance between two points is the length of the shortest connecting path. Finally, for a given
x ∈M there is a bijection expx : TxM →M of a neighborhood of ∈TxM into a
neighborhood of x ∈M that assigns to each tangent vector v ∈TxM the end point of the
geodesic emanating from x with initial velocity v and running over the time interval [,]
[, Theorem ..] or [, Chap. , Theorem ].
F = C(x2 − x1)
F = 2µv
= 2µ (x2 − x1)
x1
x2
x1
x2
⊡Fig. -
The force F of an elastic spring between xand xis proportional to (x−x), as well as the
force F of a dashpot which is extended from xto xwithin time at constant velocity v. The
spring energy reads W = ∫F dx = 
C∥x−x∥
and the dashpot dissipation
Diss = ∫F ⋅v dt = µ ∥x−x∥



Variational Methods in Shape Analysis
x2
x3
x3
x1
x1
x
logx
logx
logx
x 2
⊡Fig. -
The logarithmic map assigns each point xi on the manifold M a vector in the tangent space
TxM, which may be seen as a linear representative
We can now define the (possibly non-unique, cf. > Sect. .) mean x of a number of
n points x, . . . , xn ∈M in analogy to the Euclidian case as x = argmin˜x ∑n
i=d(xi, ˜x),
where d(⋅,⋅) is the Riemannian distance on M. This average is uniquely defined as long
as the geodesics involved in the distance computation are unique, and it has been investi-
gated in differential geometry by Karcher []. Furthermore, on a Riemannian manifold
M, the inverse exponential map logx = exp−
x provides a method to obtain representa-
tives logx(xi) ∈TxM of given input points xi ∈M in the (linear) vector space TxM
(> Fig. -). On these, we can perform a PCA, which is by definition a linear
statistical tool.
In a Riemannian space M, the path-based approach can immediately be applied by
exploiting the Riemannian structure, and ∫

gx(t)(˙x(t), ˙x(t))dt can be considered as the
energy dissipation spent to move a point from x() to x() along a geodesic. The loga-
rithms logx(xi) in this model correspond to the initial velocities of the transport process
leading from x to xi. When applying the state-based elastic model in M, however, there
is no mechanically motivated notion of paths and thus also no logarithmic map. Only if
we suppose that the Riemannian structure of the space M is not induced by changes in
the inner structure of our objects, the physical model based on elastic springs still coin-
cides with the viscous model: We consider elastic springs stretched on the surface M
and connecting the points x and xi with a stored energy 
Cd(x, xi). Then, as before
in the Euclidian case, a state-based average x of input points x, . . . , xn can be defined.
Furthermore, interpreting spring forces acting on x and pointing toward xi as linear
representatives of the input points xi, one can run a PCA on these forces as well. However,
for any reasonable (even finite-dimensional) model of shape space, objects are not rigid,
and the inner relation between points as subunits (such as the vertex points of polygo-
nal shapes) essentially defines the Riemannian (and thus the path-based) structure of the
space M: The rate of dissipation along a path in shape space depends on the interaction of
object points. Physically, the corresponding point interaction energy is converted into ther-
mal energy via friction. This dissipation depends significantly on the path in shape space
traversed from one shape to the other. In contrast, when applying the state-based approach
to the same shape space, we directly compare the inner relations between the subunits, i.e.,
we have no history of these relations. This comparison can be quantified based on a stored
(elastic) interaction energy which is then a quantitative measure of the dissimilarity of the
two objects but in general no metric distance.

Variational Methods in Shape Analysis 

..
Path-Based Viscous Dissipation Versus State-Based
Elastic Deformation for Non-rigid Objects
In the following, we will especially consider two different physically motivated perspectives
on a shape space of non-rigid volumetric objects in more detail. In the first case, we will
adopt a path-based view, motivated by the theory of viscous fluids, while the second, state-
based approach will be motivated by elasticity.
We will regard shapes S as boundaries S = ∂O of domains O ⊂
 d which will be
interpreted as physical objects. The resulting shape space structure depends on the par-
ticular type of physical objects O: An interpretation of O as a blob of a viscous fluid will
yield an actually Riemannian, path-based shape space, while the interpretation as an elas-
tic solid results in a state-based perspective, which will turn out to be non-Riemannian by
construction.
...
Path-Based, Viscous Riemannian Setup
Shapes will be modeled as the boundary contour of a physical object that is made of a
viscous fluid. The object might be surrounded by a different fluid (e.g., with much lower vis-
cosity and compression modulus), nevertheless, without any restriction we will assume
void outside the object in the derivation of our model. Here, viscosity describes the inter-
nal resistance in a fluid and is a macroscopic measure of the friction between fluid particles,
e.g., the viscosity of honey is significantly larger than that of water. The friction is described
in terms of the stress tensor σ = (σi j)i j=,...d, whose entries describe a force per area ele-
ment. By definition, σi j is the force component along the ith coordinate direction acting on
the area element with a normal pointing in the jth coordinate direction. Hence, the diago-
nal entries of the stress tensor σ refer to normal stresses, e.g., due to compression, and the
off-diagonal entries represent tangential (shear) stresses. The Cauchy stress law states that
due to the preservation of angular momentum, the stress tensor σ is symmetric [].
In a Newtonian fluid, the stress tensor is assumed to depend linearly on the gradient
Dv := ( ∂vi
∂x j )
i j=,...d of the velocity v. In case of a rigid body motion the stress vanishes.
A rotational component of the local motion is generated by the antisymmetric part 

(Dv −(Dv)T) of the velocity gradient, and it has the local rotation axis ∇× v and local
angular velocity ∣∇× v∣[]. Thus, as rotations are rigid body motions, the stress only
depends on the symmetric part є[v] := 
(Dv + (Dv)T) of the velocity gradient. For an
isotropic Newtonian fluid we get σi j = λδi j ∑k(є[v])kk +μ (є[v])i j, or in matrix notation
σ = λtr(є[v]) 1+μє[v], where 1 is the identity matrix. The parameter λ is denoted Lamé’s
first coefficient. The local rate of viscous dissipation – the rate at which mechanical energy
is locally converted into heat due to friction – can now be computed as
diss[v] = λ
(trє[v])+ μtr (є[v]) .
(.)


Variational Methods in Shape Analysis
This is in direct correspondence to the mechanical definition of the stress tensor σ
as the first variation of the local dissipation rate with respect to the velocity gradient,
i.e., σ = δDvdiss. Indeed, by a straightforward computation we obtain δ(Dv)i jdiss =
λ trє[v] δi j + μ (є[v])i j = σi j . Here, tr(є[v]) measures the averaged local change of
length and (trє[v])the local change of volume induced by the transport. Obviously
divv = tr(є[v])=characterizes an incompressible fluid.
Now, let us consider a path (O(t))t∈[,] of objects connecting O() with O() and
generated by a time-continuous deformation. If each point x ∈O(t) of the object O(t) at
time t ∈[,] moves in an Eulerian framework at the velocity v(t, x) (˙x = v(t, x)), so that
the total deformation of O() into O(t) can be obtained by integrating the velocity field v
in time, then the accumulated global dissipation of the motion field v in the time interval
[,] takes the form
Diss [(v(t),O(t))t∈[,]] = ∫

∫O(t) diss[v]dx dt .
(.)
This is the same concept as employed by Dupuis et al. [] and Miller et al. [] in their
pioneering diffeomorphism approach. They minimize a dissipation functional under the
simplifying assumption that the material behaves equally viscous inside and outside the
object. Also, diss[v] =
λ
(trє[v])+ μtr(є[v]) is replaced by a higher-order quadratic
form Lv ⋅v which plays the role of the local rate of dissipation in a multipolar fluid model
[]. Multipolar fluids are characterized by the fact that the stresses depend on higher spa-
tial derivatives of the velocity. If the quadratic form associated with L acts only on є[v] and
is symmetric, then rigid body motion invariance is incorporated in the multipolar fluid
model (cf. > Sect. ..). In contrast to this approach, we here measure the rate of dissi-
pation differently inside and outside the object and rely on classical (monopolar) material
laws from fluid mechanics.
On this physical background we will now derive a Riemannian structure on the space
of shapes S in an admissible class of shapes S. The associated metric GS on the (infinite-
dimensional) manifold S is in abstract terms a bilinear mapping that assigns each element
S ∈S an inner product on variations δS of S (cf. > Sect. ..above). The associ-
ated length of a tangent vector δS is given by ∥δS∥=
√
GS(δS, δS). Furthermore, as
we have already seen above the length of a differentiable curve S : [,] →S is then
defined by L[S] = ∫

∥˙S(t)∥dt = ∫


√
GS(t) ( ˙S(t), ˙S(t))dt, where ˙S(t) is the tem-
poral variation of S at time t. The Riemannian distance between two shapes SA and SB
on S is given as the minimal length taken over all curves with S() = SA and S() = SB
or equivalently (cf. > Sect. ..above) as the length of a minimizer of the functional
∫

GS(t) ( ˙S(t), ˙S(t)) dt. For shapes S ∈S an infinitesimal variation δS of a shape S = ∂O
is associated with a transport field v : O →
 d. This transport field is obviously not unique.
Indeed, given any vector field w on O with w(x) ∈TxS for all x ∈S = ∂O (where
TxS denotes the (d −)-dimensional tangent space to S at x), the transport field v + w
is another possible representation of the shape variation δS. Let us denote by V(δS) the
affine space of all these representations. As a geometric condition for v ∈V(δS) we obtain

Variational Methods in Shape Analysis 

v(x) ⋅n[S](x) = δS(x) ⋅n[S](x) for all x ∈S, where n[S](x) ∈
 d denotes the outer
normal to S ⊂
 d in x ∈S. Given all possible representations we are interested in the
optimal transport, i.e., the transport leading to the least dissipation. Thus, using definition
(> .) of the local dissipation rate we finally define the metric GS(δS, δS) as the minimal
dissipation rate on motion fields v which are consistent with the variation of the shape δS,
GS(δS, δS) := min
v∈V(δS) ∫O diss[v]dx = min
v∈V(δS) ∫O
λ
(trє[v])+ μ tr (є[v]) dx .
(.)
Let us remark that we distinguish explicitly between the metric g(v,v) := ∫O diss[v]dx
on motion fields and the metric GS(δS, δS) on shape variations. Finally, integration
in time leads to the total dissipation (> .) to be invested in the transport along a
path (S(t))t∈[,] in the shape space S. This implies the following definition of a time-
continuous geodesic path in shape shape:
Deﬁnition (Geodesic path)
Given two shapes SA and SB in a shape space S, a geodesic
path between SA and SB is a curve (S(t))t∈[,] ⊂S with S() = SA and S() = SB which
is a local solution of
min
v(t)∈V( ˙S(t))Diss [(v(t),O(t))t∈[,]]
among all differentiable paths in S.
The Riemannian distance between two shapes SA and SB induced by this definition is given
by the length of the shortest (geodesic) path S(t) between the two shapes, i.e.,
dviscous(SA,SB) = L[(S(t))t∈[,]] .
> Figure -shows two different paths between the same pair of shapes, one of them being
a (numerically approximated) geodesic. Note that the chosen dissipation model combines
the control of infinitesimal length changes via tr (є[v]), and the control of compression
via tr (є[v]). > Figure -evaluates the impact of these two terms on the shapes along a
geodesic path.
⊡Fig. -
A geodesic (top, path length L = .and total dissipation Diss = .) and a
non-geodesic path (bottom, L = ., Diss = .) between an A and a B. The
intermediate shapes of the bottom row are obtained via linear interpolation between the
signed distance functions of the end shapes. The local dissipation rate is color coded as


Variational Methods in Shape Analysis
⊡Fig. -
Two geodesic paths between dumbbell shapes varying in the size of the ends. In the top
example the ratio λ/µ between the dissipation parameters is .(leading to rather
independent compression and expansion of the ends since the associated change of
volume implies relatively low dissipation), and in the bottom row (now mass is actually
transported from one end to the other). The underlying texture on the objects is aligned to
the transport direction, and the absolute value of the velocity v is color coded as
...
State-Based, Path-Independent Elastic Setup
Now, objects bounded by a shape contour S are no longer composed of a viscous fluid but
are considered to be elastic solids. To describe object deformations, we aim for an elas-
tic energy which is not restricted to small displacements and which is consistent with first
principles. Alongside the shape space modeling, we will recall some background from elas-
ticity. For details we refer to the comprehensive introductions in the books by Ciarlet []
and Marsden and Hughes [].
For two objects OA and OB with shapes SA = ∂OA and SB = ∂OB, we assume a defor-
mation ϕ to be defined on OA and constrained by the assumption ϕ(SA) = SB. For practi-
cal reasons one might consider OA to be embedded in a very soft elastic material occupying
Ω /OA for some computational domain Ω. There is an elastic energy Wdeform[ϕ,OA] asso-
ciated with the deformation ϕ : Ω →
 d. By definition, elastic means that this energy
solely depends on the state and not on the path along which the deformation proceeds in
time. More precisely, for so-called hyper-elastic materials, Wdeform[ϕ,OA] is the integral of
an energy density W depending solely on the Jacobian Dϕ of the deformation ϕ, i.e.,
Wdeform[ϕ,OA] = ∫OA
W(Dϕ)dx .
(.)
This elastic energy is considered as a dissimilarity measure between the shapes SA
and SB. As a fundamental requirement one postulates the invariance of the deformation
energy with respect to rigid body motions, Wdeform[Q ○ϕ + b,SA] = Wdeform[ϕ,SA] for any
orthogonal matrix Q ∈SO(d) and translation vector b ∈
 d (the axiom of frame indif-
ference in continuum mechanics). From this, one deduces that the energy density only

Variational Methods in Shape Analysis 

depends on the right Cauchy–Green deformation tensor DϕTDϕ. Hence, there is a func-
tion ̃
W :
 d,d →
  such that the energy density W satisfies W(F) = ̃
W(FTF) for
all F ∈
 d,d. The Cauchy–Green deformation tensor geometrically represents the met-
ric measuring the deformed length in the undeformed reference configuration. For an
isotropic material and for d = , the energy density W can be further rewritten as a
function ˆW(I, I, I) solely depending on the principal invariants of the Cauchy–Green
tensor, namely I= tr(DϕTDϕ), controlling the local average change of length, I=
tr (cof(DϕTDϕ)) (cofF := det F F−T), reflecting the local average change of area, and
I= det(DϕTDϕ), which controls the local change of volume. For a detailed discussion
we refer to [, ]. We shall furthermore assume that the energy density is polyconvex [],
i.e., a convex function of Dϕ, cofDϕ, and detDϕ, and that isometries, i.e., deformations
with DϕT(x)Dϕ(x) = 1, are local minimizers with W(Dϕ) = ̃
W(1) = []. Typical
energy densities in this class are of the form
ˆW(I, I, I) = aI
p

+ aI
q

+ Γ(I)
(.)
for a, a> and a convex function Γ : [,∞) →
  with Γ(I) →∞for I→and
I→∞. In nonlinear elasticity such material laws have been proposed by Ogden [],
and for p = q = (the case considered in our computations) we obtain the Mooney–
Rivlin model []. The built-in penalization of volume shrinkage, i.e., ˆW(I, I, I)
I→
→∞,
enables us to control local injectivity (cf. []).
Incorporation of such a nonlinear elastic energy allows to describe large deformations
with strong material and geometric nonlinearities, which cannot be treated by a linear elas-
tic approach (cf. Hong et al. []). Furthermore, it balances in an intrinsic way expansion
and collapse of the elastic objects and hence frees us from imposing artificial boundary
conditions or constraints.
As in the previous section, the local force per area, induced by the deformation, is
described at a point ϕ(x) ∈ϕ(O) by the Cauchy stress tensor σ. It is related to the first
Piola–Kirchhoff stress tensor σ ref = W,F(Dϕ) := ∂W(F)
∂F
∣F=Dϕ, which measures the force
density in the undeformed reference configuration, by σ ref = σ ○ϕ cofDϕ.
Based on these concepts from nonlinear elasticity, we can now define a dissimilarity
measure on shapes
delast(SA,SB) :=
min
ϕ,ϕ(SA)=SB
√
Wdeform[ϕ,OA] .
(.)
> Figure -shows some applications of this measure. Obviously, the elastic energy
is in general not symmetric so that delast(SA,SB) ≠delast(SB,SA). Indeed, by construc-
tion delast(⋅,⋅) does not impose a metric structure on the space of shapes (we refer to
> Sect. ...for a detailed discussion). Nevertheless, it can be applied to develop phys-
ically sound statistical tools for shapes such as shape averaging and a PCA on shapes, as
outlined below in > Sect. ...
Let us make a brief remark on the mathematical relation between the two different
concepts of elasticity and viscous fluids. If we assume the Hessian of the energy density W
at the identity to be given by W,FF(1)(G,G) = λ(trG)+ μ
tr((G + GT)) (which can be


Variational Methods in Shape Analysis
Ö
Ö
Ö
Ö
Ö
Ö
deform = 0.19945
= 0.19696
= 0.23346
= 0.34599
= 0.16861
= 0. 20534
deform
deform
deform
deform
deform
⊡Fig. -
Example of elastic dissimilarities between diﬀerent shapes. The arrows indicate the
direction of the deformation, the color coding represents the local deformation energy
density (in the reference as well as the deformed state)
realized in (> .) for a particular choice of a, a, and Γ, depending on the exponents
p and q), then by the ansatz ϕ(x) = x + τv(x) and a second-order Taylor expansion we
obtain
W(Dϕ) = W(1) + τW,F(1)(Dv) + τ
W,FF(1)(Dv,Dv) + O(τ)
= + + τ( λ
(trDv)+ μ
tr ((Dv + (Dv)T)
)) + O(τ).
(.)
In effect, the Hessian of the nonlinear elastic energy leads to the energy density in
linearized, isotropic elasticity
W
lin(Du) = λ
(trє[u])+ μ tr (є[u])
(.)
for displacements u with ϕ(x) = x +u(x). This energy density, acting on displacements u,
formally coincides with the local dissipation rate diss[v], acting on velocity fields v, in the
viscous flow approach.
Finally, let us deal with the hard constraint ϕ(SA) = SB, which is often inadequate in
applications. Due to local shape fluctuations or noise in the shape acquisition, the shape SA
frequently contains details that are not present in SB and vice versa. These defects would
imply high energies in a strict -matching approach. Hence, we have to relax the constraint
and introduce some penalty functional. Here, we either measure the symmetric difference
of the input shapes SA and the pullback ϕ−(SB) of the shape SB given by
F[SA, ϕ,SB] = Hd−(SA △ϕ−(SB)) ,
(.)
where A△B = A/ B ∪B / A, or alternatively the volume mismatch
F[SA, ϕ,SB] = vol (OA △ϕ−(OB)) .
(.)

Variational Methods in Shape Analysis 

...
Conceptual Diﬀerences Between the Path- and State-Based
Dissimilarity Measures
The concept of the state-based, elastic approach to dissimilarity measurement between
shapes differs significantly from the path-based viscous flow approach. In the elastic setup,
the axiom of elasticity implies that the energy at the deformed configuration SB = ϕ(SA)
is independent of the path from shape SA to shape SB along which the deformation is
generated in time. Hence, there is no notion of shortest paths if we consider a purely elas-
tic shape model, and different from a path-based approach there might not even exist an
intermediate shape SC with delast(SA,SB) = delast(SA,SC) + delast(SC,SB).
Unlike in the elasticity model, in the Newtonian model of viscous fluids the rate of
dissipation and the induced stresses solely depend on the gradient of the motion field v.
Even though the dissipation functional (> .) looks like the deformation energy from
linearized elasticity as outlined above, the underlying physics is only related in the sense
that an infinetisimal displacement in the fluid leads to stresses caused by viscous friction,
and these stresses are immediately absorbed via dissipation.
Surely, every (path-based) Riemannian space is metrizable (and in that sense state-
based), and for many sufficiently regular (state-based) metric spaces we can devise a
corresponding (path-based) Riemannian metric. However, from our mechanical perspec-
tive, the conceptual difference between the path-based, viscous and the state-based elastic
approach is striking. In the path-based approach, the structure of the space is too com-
plicated for a closed formula of the geodesic distance, so that the actual computation of
a path is required. In the state-based approach, there is either no underlying path (i.e., no
S(t)t∈[,] such that for any ≤t≤t≤t≤we have d(S(t),S(t)) = d (S(t),S(t))+
d (S(t),S(t))), or the shape space structure is simple enough to allow for a closed
formula of the geodesic distance as in Euclidean space.
Mathematically, the path-based nature of the viscous flow approach and the fact that
an inversion of the motion field v →−v leads to a path from shape SB to SA in shape space
with the same dissipation and length, i.e.,
Diss [(v(t),O(t))t∈[,]] = Diss [(−v(−t),O(−t))t∈[,]]
ensures that the associated distance dviscous is actually a metric. In particular, the symme-
try condition dviscous(SA,SB) = dviscous(SB,SA) and the triangle inequality dviscous(SA,SC) ≤
dviscous(SA,SB) + dviscous(SB,SC) hold. As we have already seen, the symmetry condition
does not hold for the elastic dissimilarity measure. Also, the triangle inequality cannot be
expected to hold. Indeed, if a deformation ϕA,B maps OA onto OB and a deformation ϕB,C
maps OB onto OC, then ϕA,C := ϕB,C ○ϕA,B deforms OA onto OC. However, based on
our elastic model, OB is considered to be stress free when applying the deformation ϕB,C
(although it is actually obtained as the image of object OA under the deformation ϕA,B).


Variational Methods in Shape Analysis
L = 0.18527
L = 0.09894
L = 0.08700
 = 0.19016
 = 0.09976
 = 0.08737
⊡Fig. -
Left: viscosity-based (time-discrete) geodesics between the shapes at the corners (the
shapes are taken from []). The triangle inequality holds. Right: elastic dissimilarities
delast(⋅, ⋅) =
√
W ≡
√
Wdeform between the same shapes, where the arrows point from the
reference to the deformed conﬁguration. The triangle inequality does not hold
 = 0.24311
= 0.20460
⊡Fig. -
The state-based elastic dissimilarity measure delast is not symmetric (as opposed to the
path-based, viscous distance dviscous): In this example, it costs much more energy to drag out
the protrusion than to push it in. The color coding represents the local deformation energy
density in the reference and the deformed conﬁguration
Hence, the “history” of the deformation ϕA,B is lost when measuring the energy of ϕB,C.
In addition, the energy density is highly nonlinear. As a consequence, in general we cannot
expect delast(SA,SC) ≤delast(SA,SB) + delast(SB,SC). Indeed, > Fig. -gives an exam-
ple where the triangle inequality holds in the viscous, path-based and fails in the elastic,
state-based approach. Furthermore, > Fig. -depicts another example for the lack of
symmetry already apparent in > Fig. -with a particularly pronounced mechanical
difference of the two dissimilarity measures.
.
Numerical Methods and Case Examples
..
Elasticity-Based Shape Space
In this section we will perform a statistical analysis on shapes up to the second moment,
i.e., we will consider shape averaging and a principal component analysis on shapes as two
exemplary applications of the state-based elastic shape space.

Variational Methods in Shape Analysis 

...
Elastic Shape Averaging
As usual, we consider objects O as open sets in
 d with the object shape given as S := ∂O.
Given n sufficiently regular shapes Si = ∂Oi, i = , . . ., n, we are interested in an average
shape which reflects the geometric characteristics of the input shapes in a physically intu-
itive manner. Suppose S = ∂O ⊂
 d denotes a candidate for this unknown shape. As it is
characteristic for the elastic approach, the similarity of the input shapes Si to S is measured
by taking into account optimal elastic deformations ϕi : Oi →
 d with ϕi(Si) = S. The
elastic energy Wdeform[ϕi,Oi] of these deformations has the interpretation of a dissimilarity
measure (cf. > Sect. ...), so that we obtain a natural definition of an average shape as
the minimizer of the sum of these terms (cf. > Sect. .).
Deﬁnition (Elastic shape average)
Given shapes S, . . . ,Sn in some shape space S, the
elastic shape average S is the minimizer of
n
∑
i=
delast(Si,S)=
n
∑
i=
inf
ϕi:Oi→ d,ϕi(Si)=S
Wdeform[ϕi,Oi].
If the input objects Oi have Lipschitz boundary and the integrand of the deforma-
tion energy Wdeform[ϕi,Oi] = ∫Oi W(Dϕi)dx is polyconvex and bounded below by
C∥Dϕi∥p −Cfor p > d, C, C> , the existence of a Hölder-continuous elastic
shape average and deformations ϕi ∈W,p(Oi) which realize the above infimum is
guaranteed [].
An example of a shape average is provided in > Fig. -. Obviously, the process of
shape averaging is a constrained variational problem in which we simultaneously have
to minimize over n deformations ϕi and the unknown shape S under the n constraints
ϕi(Si) = S.
The necessary conditions for a set of minimizing deformations are the corresponding
Euler–Lagrange equations. As usual, inner variations of one of the deformations lead to the
classical system of PDEs div W,F(Dϕi) = for every deformation ϕi on Oi /Si, meaning
a divergence-free, equilibrized stress field (cf. > Sect. ...). Furthermore, the coupling
between the deformations via the constraints (ϕi(Si) = S)i=,...,n allows to derive a stress
balance relation on S: Consistent variation of all deformations ϕi and the average S by
some displacement u : O →
 d via (1+δu)○ϕi and (1+δu)(S) results in the optimality
condition d
dδ ∑n
i=Wdeform [(1 + δu) ○ϕi,Oi]∣δ== , which after integration by parts leads
to ∑n
i=∫Si W,F(Dϕi)(u ○ϕi) ⋅[Si]da[Si] = for the outer normal
[Si] to Si. We
have here exploited divW,F(Dϕi) = on Oi /Si. Now, we consider displacements u with
local support and let this support collapse at some point x on S. This yields the pointwise
condition
=
n
∑
i=
(σ
ref
i
[Si]da[Si]) (ϕ−
i (x))
and thus
=
n
∑
i=
(σi [S])(x)
(.)


Variational Methods in Shape Analysis
⊡Fig. -
Elastic shape average (bottom right) of ﬁve human silhouettes. For the computation, all
shapes have actually been described as phase ﬁelds, and the elastic deformations are
extended outside the input objects Oi (cf. > Sect. ...). The objects Oi are depicted along
with their deformations ϕi (acting on a checkerboard) and the distribution of local length
change

√
∥Dϕi∥and volume change det(Dϕi) (range [., .] color coded as
)
1
2
3
1
2
3
−1
1 (x)
−1
2 (x)
−1
3 (x)
2
ref
1
ref
ref
2
ref
ref
3
ref
x
n
f
f
f
f
f
f
s
s
s n
1
s n
3
s
s
n
n
n
⊡Fig. -
Sketch of the pointwise stress balance relation on the averaged shape
for x ∈S, where we have used the relation
(σ
ref
i
[Si]da[Si]) (ϕ−
i (x)) = (σi [S]da[S])(x)
between first Piola–Kirchhoff stress σ ref
i =W,F(Dϕi) and Cauchy stress σi= (σ ref
i (cofDϕi)−)○
ϕ−
i . Hence, the shape average can be interpreted as that stable shape at which the boundary
stresses of all deformed input shapes balance each other (> Fig. -). Obviously, there is
a straightforward generalization involving jumps of normal stresses on interior interfaces
in case of multi-component objects.
In order to ensure a certain regularity of the average shape S, in addition to the sum
of deformation energies in Definition one can consider a further energy contribution
which acts as a prior on S in the variational approach. In the exemplary computations
shown (> Figs. -–-), the (d −)-dimensional Hausdorff measure L[S] = Hd−(S)
has been employed as regularization.

Variational Methods in Shape Analysis 

⊡Fig. -
Average of hand silhouettes (Taken from [])
⊡Fig. -
Five segmented kidneys and their average (right). For the ﬁrst two input kidneys the
distribution of

√
∥Dϕi∥,

√
∥cof(Dϕi)∥, and det(Dϕi) is shown on sagittal cross-sections
(the range [., .] is color coded as
). While the ﬁrst kidney is dilated toward
the average, the second is compressed
⊡Fig. -
Twenty-four given foot shapes (Courtesy of adidas), textured with the distance to the
surface of the average foot (bottom-right). Values range from mm inside the average foot
to mm outside, color coded as
...
Elasticity-Based PCA
As already explained in > Sect. .., a principal component analysis (PCA) is a linear
statistical tool which decomposes a vector space into the direct sum of orthogonal sub-
spaces. These subspaces are ordered according to the strength of variation which occurs
along each subspace within a random set of sample vectors. We would like to interpret a
given set of input shapes S, . . . ,Sn as such a random sample and perform a corresponding
PCA, however, due to the linearity of a PCA we first have to identify linear representatives
for each shape on which a PCA can then be performed. For a Riemannian shape space, we
have outlined in > Sect. ..that such linear representatives are given by the logarithmic
map of the input shapes, but we have also learnt in > Sect. ...that a state-based elastic
shape space is incompatible with a Riemannian structure.
To prepare the definition of appropriate linear representatives of shapes in an elastic
shape space, let us briefly review the physical concept of boundary stresses. By the Cauchy


Variational Methods in Shape Analysis
stress principle, each deformation ϕk : Ok →O is characterized by pointwise boundary
stresses on S = ∂O in the deformed configuration. The stress at some point x on S is given
by the application of the Cauchy stress tensor σk to the outer normal
on S. The result-
ing stress σk
is a force density acting on a local surface element of S. The shape S is in
an equilibrium configuration if the opposite force is applied as an external surface load
(cf. > Fig. -). Otherwise, by the axiom of elasticity, releasing the object O, the elastic
body will snap back to the original reference configuration Ok. Let us assume the relation
between the energetically favorable deformation and its induced stresses to be one-to-one,
so that the average shape S can be described in terms of the input shape Sk and the bound-
ary stress σk , and we write S = Sk[σk ]. Upon scaling the stress with a weight t ∈[,],
we obtain a one-parameter family of shapes S(t) = Sk[tσk ], connecting Sk = S() with
S = S(). Thus, we can regard σk
as a representative of shape Sk in the linear space of
vector fields on S.
Physically, it is more intuitive to identify a displacement uk instead of the normal stress
σk as the representative of an input shape Sk. Hence, let us study how the average shape S
varies if we increase the impact of a particular input shape Sk for some k ∈{, . . . , n}. For
this purpose, we apply the Cauchy stress σk
to the average shape S, scaled with a small
constant δ. This additional boundary stress δσk
may be seen as a first Piola–Kirchhoff
stress acting on the (reference) configuration S. The elastic response is given by a corre-
spondingly scaled displacement uk : O →
 d. Here, to properly incorporate the nonlinear
nature of the second moment analysis, O should be interpreted as the compound object
which is composed of all deformed and thus prestressed input objects ϕi(Oi). This inter-
pretation is reflected by the elastic material law employed to compute the displacements uk.
In detail, uk is obtained as the minimizer of the free mechanical energy
Ek[δ,u]= 
n
n
∑
i=
Wdeform[(1+δu)○ϕi,Oi]−δ∫Sσk ⋅u da
(.)
under the constraints ∫O uk dx = and ∫O x × uk dx = of zero average translation and
rotation. These displacements uk are considered as representatives of the variation of the
average shape S with respect to the input shape Sk, on which a PCA will be performed.
As long as F ↦W(F) is not quadratic in F, uk still solves a nonlinear elastic problem.
The advantage of this nonlinear variational formulation is that it is of the same type as the
one for shape averaging, and it encodes in a natural way the compound elasticity configu-
ration of the averaged shape domain O. However, for the linearization of shape variations
we are actually only interested in the displacements δuk for small δ. Therefore, we consider
the limit of the Euler–Lagrange equations for δ →and after a little algebra obtain uk as
the solution of the linearized elasticity problem
div (Cє[u]) = in O ,
Cє[u]
= σk
on S
(.)
for the symmetrized displacement gradient є[u] = 
(Du + DuT) under the constraints
∫O u dx = and ∫O x × u dx = , where the in general inhomogeneous and anisotropic
elasticity tensor C reads
C = 
n
n
∑
i=
(

detDϕi
DϕiW,FF(Dϕi)DϕT
i ) ○ϕ−
i .

Variational Methods in Shape Analysis 

Next, for a PCA on the linearized shape variations uk we select a suitable inner
product (metric) g(u, ˜u) on displacements u, ˜u : O →
 d. Note that g induces a metric
˜g(σ , ˜σ ) := g(u, ˜u) on the associated boundary stresses so that instead of analyzing the
uk the covariance analysis can equivalently be performed directly on the boundary stresses
σ, . . . , σn , which we originally derived as linear shape representatives. Indeed, the solv-
ability condition ∫O div(C∇u)dx = ∫S C∇u da[S] is fulfilled, and thus the solution uk
for given boundary stress σk
= C∇u is uniquely determined up to a linearized rigid body
motion (i.e., an affine displacement with skew-symmetric matrix representation), which is
fixed by the conditions of zero mean displacement and angular momentum for u. Then,
due to the linearity of the operator σ
↦u, the metric ˜g is bilinear and symmetric as well,
and its positive definiteness follows from the positive definiteness of g and the injectivity
of the map σ
↦u.
We consider two different inner products on displacements u : O →
 d:
•
The L-product. Given two square integrable displacements u, ˜u we define
g(u, ˜u) := ∫O u ⋅˜u dx.
This product weights local displacements equally on the whole object O.
•
The Hessian of the energy as inner product. Different from the L-metric, we now
measure displacement gradients in a non-homogeneous way. We define
g(u, ˜u) := ∫O Cє[u] : є[˜u]dx
for displacements u, ˜u with square integrable gradients. Hence, the contribution to the
inner product is larger in areas of the compound object which are in a significantly
stressed configuration.
Given an inner product, we can define the covariance operator Cov by
Covu := 
n
n
∑
k=
g(u,uk)uk
(note that the stresses σk
and thus also the displacements uk have zero mean due to
(> .)). Obviously, Cov is symmetric positive definite on span(u, . . . ,un). Hence, we
can diagonalize Cov on this finite-dimensional space and obtain a set of g-orthonormal
eigenfunctions wk : O →
 d and eigenvalues λk > with Cov wk = λkwk. These eigen-
functions can be considered as the principal modes of variation of the average object O
and hence of the average shape S, given the n sample shapes S, . . . ,Sn. Their eigenvalues
encode the variation strength. The diagonalization of Cov can be performed by diagonal-
izing the symmetric matrix 
n (g(ui,u j))i j = OΛOT, where Λ = diag(λ, λ, . . .) and O is
orthogonal. The eigenfunctions are then obtained as wk =

√
λk ∑n
j=O jku j.
Being displacements on O, the modes of variation wk can easily be visualized via
a scalar modulation δwk for varying δ (cf. the vizualization in > Figs. -–-or
the red lines in > Figs. -and > -). If an amplified visualization of the modes is


Variational Methods in Shape Analysis
⊡Fig. -
First three dominant modes of variation for six input shapes (left), based on diﬀerent
metrics. Left: L-metric on displacements of a non-prestressed object (modes wk with ratios
λk
λof , ., .). Middle: L-metric on displacements of the compound object ( λk
λ= , .,
.). Right: energy Hessian-based metric on displacements of the compound object ( λk
λ= ,
., .).
required, it is preferable to depict displacements wk
δ which are defined as minimizers of
the nonlinear variational energy 
n ∑n
i=Wdeform[(1 + w) ○ϕi,Oi] −δ∫S C∇wk
⋅w da
(cf. (> .)).
Let us underline that this covariance analysis properly takes into account the usually
strong geometric nonlinearity in shape analysis via the transfer of geometric shape varia-
tion to elastic stresses on the average shape, based on paradigms from nonlinear elasticity.
Displacements or stresses are interpreted as the proper linearization of shapes. In abstract
terms, either the space of displacements or stresses can be considered as the tangent space
of shape space at the average shape, where the identification of displacements and stresses
via (> .) provides a suitable physical interpretation of stresses as shape variations.
The impact of the chosen metric. Naturally, the modes of variation depend on the chosen
inner product. We have already mentioned that in order to be physically meaningful, the
inner product should act on displacements uk of the compound object (which is composed
of all deformed input shapes). If instead the uk were obtained by applying the boundary
stresses σk
to an object which just looks like the average shape but does not contain the
information how strongly the input shapes had to be deformed to arrive at the average, we
obtain a different result (> Fig. -, left): If the prestressed state of some object regions
is neglected, it becomes easier to deform them which causes the prediction of stronger
variations. > Figure -also hints at the differences between the employed metrics: The
L-metric pronounces shape variations with large displacements even though they are
energetically cheap (e.g., a rotation of some structure around a joint), while the Hessian
of the elastic energy measures distances between displacements solely based on the asso-
ciated change of elastic energy. Thus, displacements are weighted strongly in regions and
directions which are significantly loaded.

Variational Methods in Shape Analysis 

⊡Fig. -
First three modes of variation for eight dumbbell shapes, left for a times stronger
penalization of length than of volume changes (with ratios λi
λof , ., .), right for the
reverse ( λi
λ= , ., .). Each row represents the variation of the average (middle shape)
by δwk for the mode wk and varying δ
The impact of the nonlinear elasticity model. Likewise, the particular choice of the
nonlinear elastic energy density has a considerable effect on the average shape and its
modes of variation.
> Figure -has been obtained using W(Dϕ) =
μ
∥Dϕ∥+
λ
det Dϕ−(μ + λ
)log detDϕ −μ −λ
, where μ and λ are the coefficients of length and
volume change penalization, respectively. A low penalization of volume changes appar-
ently leads to independent compression and inflation at the dumbbell ends (left), while for
deformations with a strong volume change penalization (right), material is squeezed from
one end to the other. Here, the underlying metric is the based on the Hessian of the energy.
> Figures -–-show the dominant modes of variation for the examples from
the previous section. A statistical analysis of the hand shapes in > Fig. -has also been
performed in [] and [], where the shapes are represented as vectors of landmark posi-
tions. The average and the modes of variation are quite similar, representing different kinds
of spreading the fingers. The dominant modes of variation for a set of three-dimensional
kidney shapes is depicted in > Fig. -, where for all modes wk we show the average (mid-
dle) and its variation according to δwk for varying δ. Local structures seem to be quite well
represented and preserved during the averaging process and the subsequent covariance
analysis compared to, e.g., the PCA on kidney shapes in [] where a medial representation
is used.
The PCA of the foot shapes from > Fig. -is shown in > Fig. -and is much
more intuitive than the color coding in > Fig. -. The first mode apparently represents
changing foot lengths, the second and third mode belong to different variants of combined
width and length variation, and the fourth to sixth mode correspond to variations in rel-
ative heel position, ankle thickness, and instep height. Finally, > Fig. -shows that the
approach also works for image morphologies instead of shapes, using thorax CT scans as
input. Here, the image edge set is considered as the corresponding shape, which is typically
quite complex and characterized by nested contours. The first mode of variation represents


Variational Methods in Shape Analysis
⊡Fig. -
First four modes of variation with ratios λi
λof , ., ., and .for the hand
silhouettes from > Fig. -
⊡Fig. -
Forty-eight input kidneys (Courtesy of Werner Bautz, radiology department at the
University Hospital Erlangen, Germany) and their ﬁrst four modes of variation with ratios λi
λ
of , ., ., and .
λ1/ λ1 = 1
λ2/λ1 = 0.010
λ3/ λ1 = 0.010
λ4/ λ1 = 0.003
λ5/λ1 = 0.001
λ6/ λ1 = 0.0008
⊡Fig. -
The ﬁrst six dominant modes of variation for the feet from > Fig. -
a variation in chest size, the next mode corresponds to a change of heart and scapula shape,
while the third mode mostly concerns the rib position.
..
Viscous Fluid-Based Shape Space
As explained in > Sect. ..., the viscous fluid shape space is by construction a (infinite-
dimensional) Riemannian manifold and as such is based on the computation of shape paths

Variational Methods in Shape Analysis 

⊡Fig. -
thorax CT scans from diﬀerent patients (courtesy of Bruno Wirth, urology department at
the Hospital zum hl. Geist, Kempen, Germany) and their ﬁrst three modes of variation with
ratios λi
λof , ., and .. Note that the thin lines which can be seen left of the heart
correspond to contours of the liver, which are only visible in the ﬁrst and last input image
as opposed to state-based approaches like the elastic shape space from the previous section.
In the elastic, state-based approach, we have to find for each pair of shapes SA = ∂OA
and SB = ∂OB one single optimal matching deformation ϕ : OA →
 d via which the
similarity between SA and SB is determined. In contrast, here we require more information
to measure the distance between the two shapes, namely an optimal velocity field v(t) :
O(t) →
 d at each time t within the given time interval [,]. In effect, this implies an
increase of the dimension of the variational problem by the time component.
The two qualitatively different types of coordinates, the space coordinates (that span the
space in which the shapes lie) and the time coordinate, are intuitively treated in different
ways. One possibility is to regard the variational problem of computing a geodesic as a
classical elliptic boundary value problem in time, in which each shape on a path seeks to
be in equilibrium with its local neighborhood on the path. The equilibrizing force can be
interpreted as an acceleration acting on the velocity field v. In this setting, it seems most
natural to discretize first the time variable and approximate geodesics in shape space as
discrete sequences S, . . . ,SK of shapes, where each shape is connected to and equilibrates
with its neighbors and the path length along the discrete path S, . . . ,SK is approximated
as a sum ∑K
k=˜d(Sk−,Sk) of approximations ˜d(Sk−,Sk) of the geodesic distance between
neighboring shapes. The distance ˜d can be based on a matching deformation energy which
will be elaborated on further down.
An alternative view starts from the underlying velocity field which generates the
geodesic. Dupuis et al. [] and Beg et al. [] consider shapes (or rather images) embed-
ded in a domain Ω ⊂
 d. These shapes deform according to smooth, compactly sup-
ported velocity fields v ∈L([,]; W n,

(Ω;
 d)) with n >+ 
d . The regularity of the
velocity fields is ensured by defining the path dissipation as ∫

∫Ω Lv ⋅v dx dt and the


Variational Methods in Shape Analysis
path length as ∫


√
∫Ω Lv ⋅v dx dt for a differential operator L of sufficiently high order
(cf. >Sect. ...). The corresponding shape deformation ϕ which is induced by the veloc-
ity field is obtained as the solution ϕ = ϕof the pointwise, Lagrangian ordinary differential
equation d
dt ϕt(x) = v(ϕt(x), t).
In the first approach, the computation of a geodesic was seen as the concatenation of
a number of local subproblems each of which represents the approximation of a geodesic
segment between two intermediate shapes and each of which thus inherits the constraint
that one shape is transferred exactly into the other. In contrast, in the second approach
we have one single constraint, acting at the end of the geodesic and expressing that the
accumulated flow ϕ deforms the starting shape SA into the final shape SB, ϕ(SA) =
SB.
Let us now focus on the first approach in which a geodesic path will be approximated
via a finite sequence of shapes S, . . . ,SK, connected by deformations ϕk : Ok−→
 d
which are optimal in a variational sense and fulfil the constraint ϕk(Sk−) = Sk.
Given two shapes SA, SB in some given space of shapes S, we define a discrete path
of shapes as a sequence of shapes S, . . . ,SK ∈S with S= SA and SK = SB. For the
time step τ =

K the shape Sk is supposed to be an approximation of S(tk) with tk=kτ,
where (S(t))t∈[,] is a continuous path connecting SA = S() and SB = S(). For
each pair of consecutive shapes Sk−and Sk we now consider a matching deformation
ϕk : Ok−→
 d which satisfies ϕk(Sk−) = Sk. With each deformation ϕk we asso-
ciate a deformation energy Wdeform[ϕk,Ok−] = ∫Ok−W(Dϕk)dx of the same type as
described in > Sect. .... If appropriately chosen, this energy will ensure sufficient reg-
ularity and a -matching property for deformations ϕk with finite energy. As in elasticity,
the energy is assumed to depend only on the local deformation, reflected by the Jaco-
bian Dϕ. Yet, different from elasticity, we suppose the material to relax instantaneously
so that object Ok is again in a stress-free configuration when applying ϕk+at the next
time step. Let us also emphasize that the stored energy does not depend on the deforma-
tion history as in most plasticity models in engineering. This energy is now employed to
define time-discrete counterparts to the dissipation and length of continuous paths from
> Sect. ....
Deﬁnition (Discrete dissipation and discrete path length)
Given a discrete path
S, . . . ,SK ∈S, its dissipation is defined as
Dissτ(S, . . . ,SK) :=
K
∑
k=

τWdeform[ϕk,Ok−],
where ϕk : Ok−→
 d is a minimizer of the deformation energy Wdeform[ϕk,⋅] under the
constraint ϕk(Sk−) = Sk. Furthermore, the discrete path length is defined as
Lτ(S, . . . ,SK) :=
K
∑
k=
√
Wdeform[ϕk,Ok−] .

Variational Methods in Shape Analysis 

Let us make a brief remark on the proper scaling factors. The deformation energy
Wdeform[ϕk,Ok−] is expected to scale like τ(cf. (> .)). Hence, the factor 
τ ensures
the discrete dissipation measure to be conceptually independent of the time step size. The
same holds for the discrete length measure Lτ(S, . . . ,SK).
To ensure that the above-defined dissipation and length of discrete paths in shape space
are well defined, a minimizing deformation ϕk of the elastic energy Wdeform[⋅,Ok−] with
ϕk(Sk−) = Sk has to exist. In fact, this holds for objects Ok−and Ok with Lipschitz
boundaries Sk−and Sk for which there exists at least one bi-Lipschitz deformation ˆϕk of
Ok−into Ok for k = , . . . , K [].
With the notion of dissipation at hand, we can define a discrete geodesic path following
the standard paradigms in differential geometry.
Deﬁnition (Discrete geodesic path)
A discrete path S,S. . . ,SK in a set of admissible
shapes S connecting two shapes SA = Sand SB = SK is a discrete geodesic if there exists an
associated family of deformations (ϕk)k=, …,K such that (ϕk,Sk)k=, …,K minimize the total
energy ∑K
k=

τ Wdeform[ ˜ϕk, ˜Ok−] over all intermediate shapes ˜S= ∂˜O, . . . , ˜SK−= ∂˜OK−∈
S and all possible matching deformations ˜ϕ, . . . , ˜ϕK with ˜ϕk( ˜Sk−) = ˜Sk for k = , . . . , K.
Examples of discrete geodesics are provided in > Figs. -and > -. Apparently, the
frame indifference and the (local) injectivity property of the matching deformations, which
are ensured by the nonlinear deformation energy Wdeform, allow the computation of rea-
sonable discrete geodesics with only few intermediate shapes. Under sufficient growth
conditions on the integrand of the deformation energy Wdeform, the existence of discrete
geodesics is guaranteed at least for certain compact sets S of admissible shapes, e.g., shapes
S which can be described by spline curves with a finite set of control points from some
compact domain and which satisfy a uniform cone condition in the sense that each x ∈S
is the tip of two cones with fixed height and opening angle which lie completely on either
side of S []. Such requirements on S are necessary since the known regularity theory for
deformation energies of the employed type does not allow to prove Lipschitz-regularity of
optimal deformations so that the intermediate shapes might degenerate.
The discrete dissipation as the sum of matching deformation energies indeed represents
an approximation to the time-continuous dissipation of a velocity field from > Sect. ....
If a smooth path in shape space is considered which is interpolated at discrete times tk = kτ,
k = , . . ., K, and if for t ∈[tk−, tk), vτ(t) = (ϕk−1)
τ
○( tk−t
τ
1 + t−tk−
τ
ϕk)
−denotes the
velocity field which generates the associated matching deformations ϕk, then as the time
step size τ =

K decreases and vτ converges against a smooth velocity field v, the discrete
dissipation converges against the time-continuous dissipation (> .) induced by v (cf.
[] for details).
Within this framework of geodesics in shape space, the strict constraints that one shape
is deformed exactly into another one are often inadequate in applications as has already
been discussed in > Sect. ...for the state-based, elastic setup. For the computation
of an elastic dissimilarity measure, the single matching constraint could be relaxed as a


Variational Methods in Shape Analysis
⊡Fig. -
Discrete geodesics between a straight and a rolled up bar, from ﬁrst row to fourth row based
on one, two, four, and eight time steps. The light gray shapes in the ﬁrst, second, and third
row show a linear interpolation of the deformations connecting the dark gray shapes. The
shapes from the ﬁnest time discretization are overlayed over the others as thin black lines.
In the last row the rate of viscous dissipation is rendered on the shape domains O, . . . , O
from the previous row, color coded as
mismatch penalty. In the Riemannian, viscous setting we pursue the same concept, how-
ever, the particular form of the employed constraints depends on the chosen view on
shape geodesics. In the framework of geodesics as paths of diffeomorphisms, which we
introduced at the beginning of this section, there is the single constraint ϕ(SA) = SB,
meaning that the induced diffeomorphism ϕ maps the initial shape SA onto the final
shape SB. This constraint can be relaxed in the same manner as in > Sect. ...via a
penalty measuring the mismatch of the shapes or of the corresponding objects. For the
time-discrete geodesic setting we have a sequence of matching constraints ϕk(Sk−) = Sk,
k = , …, K, each of which can again be relaxed by the same means. In fact, we add to
the discrete dissipation of a set (ϕk)k=, …, K of deformations a sum of mismatch penalties
∑K
k=vol (Ok−△ϕ−
k (Ok)). In the limit for vanishing time step size τ =

K and under
the same conditions as above, this sum can be shown to converge against the optical
flow type functional ∫T ∣(,v(t)) ⋅n [t,S(t)]∣da for the unit outward normal n[t,S(t)]
to the space time shape tube T = ⋃t∈[,]{t} × S(t). Furthermore, ∑K
k=τL[Sk] with
L[Sk] = Hd−(Sk) has been employed as regularization, which in the limit for τ →
converges against the integral ∫

Hd−(S(t))dt.
Real-world objects are most often not only characterized by their outer contour but
also contain internal structures that have to be matched properly when computing the
similarity between two objects. As an example, consider the straight and the folded rod in
> Fig. -. The rods consist of three distinct components, which imposes a constraint on

Variational Methods in Shape Analysis 

⊡Fig. -
Discrete geodesic between a cat and a lion and between the hand shapes mand m
from the Princeton Shape Benchmark []. For both examples, the local dissipation is color
coded on slices through the shapes as
reasonable connecting paths: Each component is to be mapped onto its correct counter-
part. A shortest path under this constraint obviously differs significantly from the geodesic
which just matches the outer contours (cf. > Fig. -).
This observation calls for a generalization of shapes, an example of which we have
already seen in the context of an elastic shape space in > Fig. -, where the edge set
of an image was considered as a shape. Here, let us adopt a slightly different approach and
regard shapes as being composed of a number of subcomponents. In detail, instead of a
geodesic between just two shapes SA = ∂OA and SB = ∂OB, we now seek a geodesic path
(S i(t))i=, …, m with S i(t) = ∂Oi(t) for t ∈[,], between two collections of m separate


Variational Methods in Shape Analysis
⊡Fig. -
Discrete geodesic between the straight and the folded bar from > Fig. -, where the black
region of the initial shape is constrained to be matched to the black region of the ﬁnal
shape. The bottom row shows a color coding of the corresponding viscous dissipation. Due
to the strong change in relative position of the black region, the intermediate shapes exhibit
a strong asymmetry and high dissipation near the bar ends
shapes, (S i
A)i=, …, m with S i
A(t) = ∂Oi
A(t) and (S i
B)i=, …, m with S i
B(t) = ∂Oi
B(t). The
geodesic path is supposed to be generated by a joint motion field v(t) : ⋃m
i=Oi(t) →
 d.
The single objects Oi(t) can then be regarded as the subcomponents of an overall object
⋃m
i=Oi(t). The total dissipation along the path is measured exactly as before by
Diss[(v(t),(Oi(t))i=, …, m)t∈[,]] = ∫

∫
⋃m
i=Oi(t)
λ
(trє[v])+ μ tr (є[v]) dx dt .
This naturally translates to the discrete dissipation of a path with K + intermediate shape
collections (S i
k)i=, …, m, k = , . . . , K,
K
∑
k=
Wdeform [ϕk,(Oi
k−)i=, …, m] :=
K
∑
k=∫
⋃m
i=Oi
k−
W(Dϕk)dx,
where the deformations ϕk satisfy the constraints ϕk (S i
k−) = S i
k for k = , …, K,
i = , …, m, and S i
= S i
A, S i
K = S i
B, i = , …, m.
The different object components can of course be assigned different material properties.
> Figure -shows frames from a real video sequence of moving white and red blood
cells (top) as well as a discrete geodesic between the first and last frame (middle) for which
the material parameters of the white blood cell were chosen twenty times weaker than for
the red blood cells. The result is a nonlinear interpolation between distant frames which is
in good agreement with the actually observed motion. Once geodesic distances between
shapes are defined, one can statistically analyze ensembles of shapes and cluster them in
groups based on the geodesic distance as a reliable measure for the similarity of shapes. Two
exemplary examples are provided by the evaluation of geodesic distances between different
D letters (> Fig. -, left) and between six different D foot shapes (> Fig. -, right).
In the D example, we clearly identify three distinct clusters (Bs, Xs, and M).

Variational Methods in Shape Analysis 

⊡Fig. -
Top: frames from a real video sequence of a white blood cell among a number of red ones
(courtesy Robert A. Freitas, Institute for Molecular Manufacturing, California, USA). Middle:
computed discrete geodesic between the segmented shapes in the ﬁrst and the last frame.
Bottom: pushforward of the initial (ﬁrst four shapes) and pullback of the ﬁnal frame (last ﬁve
shapes) according to the geodesic ﬂow
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
3.8
12.1 25.9 24.7 37.7
10.1 25.5 24.4 37.7
23.5 23.6 32.3
13.6 31.2
40.4
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
12.70
17.17
21.48 24.55 34.98
18.80
20.65 21.53 32.96
27.41 25.73 35.79
17.30 22.14
17.69
⊡Fig. -
Left: pairwise geodesic distances between (also topologically) diﬀerent letter shapes. Right:
pairwise geodesic distances between diﬀerent scanned D feet. The feet have volumes
., ., ., ., , and cm, respectively
..
A Collection of Computational Tools
So far, we have investigated some of the many aspects on mathematical models in shape
space without any discussion of the corresponding computational tools and numerical
algorithms. Hence, let us at least briefly mention some fundamental computational aspects
to effectively deal with general classes of shapes as boundary contours of volumetric
objects.
At first, we replace the strict separation between material inside the object and void
outside by substituting the void with a material which is several orders of magnitude


Variational Methods in Shape Analysis
softer than inside the object. This relaxation is important with respect to the exis-
tence analysis and the stabilization of the computational method. In fact, we replace
the deformation energy Wdeform[ϕ,O] = ∫O W(Dϕ)dx by the energy W η
deform[ϕ,O] =
∫Ω ((−η)χO + η) W(Dϕ)dx for a small constant η. In the implementation which
underlies the above applications, for η = −one observes no significant qualitative impact
of this regularization on the solution. Furthermore, as mentioned above, to ensure regu-
larity of the shape contour S, we take into account the area functional L[S] = ∫S da as a
prior, weighted with a small factor.
Compared to a parametric description of shapes, e.g., as a polygonal line or a trian-
gulated surface, an implicit description has several advantages. In particular, it does not
require a remeshing even in case of large deformations, it allows for topological transitions
without any extra handling of the associated singularities, and it can be combined with
multi-scale relaxation schemes for an efficient minimization of the involved functionals.
In what follows, we consider a level set and a phase field description of shapes and
outline the general framework of a multi-scale method based on finite element calculus.
In fact, the phase field model has been used in the examples for the elastic shape averaging
and the PCA, whereas the level set method has served as a numerical building block for
the computation of time-discrete shape geodesics.
...
Shapes Described by Level Set Functions
The level set method first presented by Osher and Sethian [] has been used for a wide
range of applications [, ]. Burger and Osher gave an overview in the context of shape
optimization []. To numerically solve variational problems in shape space, we assume a
shape S to be represented by the zero level set {x ∈Ω : u(x) = } of a scalar function
u : Ω →
  on a computational domain Ω ⊂
 d. Furthermore, the zero super level
set {x ∈Ω : u(x) > } defines the corresponding object domain O. This shape descrip-
tion can be incorporated in a variational approach following the approximation proposed
by Chan and Vese []. In fact, the partition of the domain Ω into object and back-
ground is encoded via a regularized Heaviside function Hε ○u. As in [] we consider
the function Hε(x) :=

+ 
π arctan ( x
ε ), where ε is a scale parameter representing the
width of the smeared-out shape contour. Then, a deformation energy W η
deform[ϕ,O] =
∫Ω ((−η)χO + η) W(Dϕ)dx is approximated by
W ε,η
deform[ϕ,u] = ∫Ω ((−η)Hε(u) + η) W(Dϕ)dx.
Furthermore, the energy F[SA, ϕ,SB] = vol (OA △ϕ−(OB)) measuring the volumetric
mismatch between an object OA and the pullback of an object OB under a deformation ϕ
can be approximated by
F ε[uA, ϕ,uB] = ∫Ω (Hε(uB ○ϕ) −Hε(uA))dx,

Variational Methods in Shape Analysis 

where uA, uB are level set representations of the shapes SA and SB, respectively. Finally,
the surface area of a shape S, which appears as a prior, is replaced by the total variation of
Hε ○u, and we obtain
Lε[u] = ∫Ω ∣∇Hε(u)∣dx.
Let us emphasize that in the actual energy minimization algorithm, the guidance of an
initial zero level set towards the final shape relies on the nonlocal support of the derivative
of the regularized Heaviside function (cf. []).
...
Shapes Described via Phase Fields
An alternative to a level set description of shapes is a phase field representation. Physically,
the phase field approach is inspired by the observation that interfaces are usually not sharp
but characterized by a diffusive transition. Mathematically, there are two basic types of such
phase field representations, a single phase approach as the one presented by Ambrosio and
Tortorelli [] for the approximation of the Mumford–Shah model [] and the double phase
approach by Modica and Mortola [] used to approximate surface integrals. In the shape
context studied here, let us focus on the single phase model. Thus, a shape S is encoded by
a continuous, piecewise-smooth phase field function u : Ω →
  which is zero on S, but
close to one everywhere else. The specific profile of the phase field function u for a shape
S is determined via the phase field approximation
Lε[u] = 
∫Ω ε∣∇u∣+ 
ε(u −)dx
of the involved surface area ∫S da. As in the above level set model the phase field parameter
ε determines the width of the diffusive interface. Different from the level set model by Chan
and Vese, the interface profile is not explicitly prescribed but implicitly encoded in the
variational approach as the profile attained by minimizers of the functional. Based on this
phase field model the penalty functional F[SA, ϕ,SB] = Hd−(SA △ϕ−(SB)) measuring
the area mismatch between a shape SA and the pullback of a shape SB under a deformation
ϕ can be approximated by
F ε[uA, ϕ,uB] = 
ε ∫Ω(uB ○ϕ)(−uA)+ u
A(−uB ○ϕ)dx,


Variational Methods in Shape Analysis
where uA, uB are phase fields representing the shapes SA and SB, respectively. In this type
of models the deformation energy W η
deform[ϕ,O] cannot be realized based on a phase field
function u due to the fact that a single phase model allows to identify the shape itself
but does not distinguish its inside and outside. Therefore, in the presented applications of
elastic shape averaging and the elastic PCA the input objects and thus their characteristic
functions χO were given a priori.
...
Multi-Scale Finite Element Approximation
For the spatial discretization of the functionals in the above variational approaches the
finite element method can be applied. Hence, the level set function or the phase field u,
representing a (unknown) shape S, and the different components of the deformations ϕ are
represented by continuous, piecewise multilinear (trilinear in D and bilinear in D) finite
element functions U and Φ on a regular grid superimposed on the domain Ω = [,]d. For
the ease of implementation a dyadic grid resolution with L + vertices in each direction
and a grid size h = −L is chosen.
Descent algorithm. The functionals depend nonlinearly both on the discrete deformations
Φ (due to the concatenation U ○Φ and the nonlinear integrand W(⋅) of the deformation
energy) as well as on the discrete level set or phase field functions U (e.g., due to the con-
catenation of the level set function with the regularized Heaviside function Hε(⋅)). In our
energy relaxation algorithm for fixed grid size, we employ a gradient descent approach. We
constantly alternate between performing a single gradient descent step for all deformations
and the level set or phase field functions.
Numerical quadrature. Integral evaluations in the descent algorithm are performed by
Gaussian quadrature of third order on each grid cell. For various terms we have to evaluate
pullbacks U ○Φ of a discretized level set function U or a test function under a discretized
deformation Φ. Let us emphasize that quadrature based on nodal interpolation of U ○Φ
would lead to artificial displacements near the shape edges accompanied by strong artifi-
cial tension. Hence, in our algorithm, if Φ(x) lies inside Ω for a quadrature point x, then
the pullback is evaluated exactly at x. Otherwise, we project Φ(x) back onto the boundary
of Ω and evaluate U at that projection point.
Cascadic multi-scale algorithm. The variational problem considered here is highly non-
linear, and for fixed time step size the proposed scheme is expected to have very slow
convergence; also it might end up in some nearby local minimum. Here, a multilevel
approach (initial optimization on a coarse scale and successive refinement) turns out to
be indispensable in order to accelerate convergence and not to be trapped in undesirable
local minima. Due to our assumption of a dyadic resolution L+in each grid direction, we
are able to build a hierarchy of grids with l +nodes in each direction for l = L, . . .,. Via a
simple restriction operation we project every finite element function to any of these coarse
grid spaces. Starting the optimization on a coarse grid, the results from coarse scales are

Variational Methods in Shape Analysis 

successively prolongated onto the next grid level for a refinement of the solution []. Hence,
the construction of a grid hierarchy allows to solve coarse scale problems in our multi-scale
approach on coarse grids. Since the width ε of the diffusive shape representation should
naturally scale with the grid width h, we choose ε = h.
.
Conclusion
Let us close with a comparison of path- and state-based shape space. Already in
> Sect. ...we have studied the difference between the state-based dissimilarity mea-
sure delast and the path-based distance dviscous. Based on the applications considered in the
previous sections let us compare the underlying concepts now more on a conceptual level
of the geometry of shape space:
•
Non-uniqueness of shape averages. Due to the nonlinearity of the elastic variational
problem, local minimizers of the elastic energy might be non-unique. There might even
exist different minimizing deformations with the same elastic energy. Mechanically, this
non-uniqueness is frequently associated with different buckling modes, which occur in
case of large, geometrically nonlinear deformations. Hence, the shape average need not
be uniquely defined, except in the small displacement case, where a linear elastic model
(> .) applies. In case of the path-based approach, (shortest) geodesics do not have
to be unique either. Indeed, a geodesic is the unique shortest path only until the first
conjugate point. Hence, the shape average is in a strict sense not well-defined if the
distances are sufficiently large.
•
Different physical interpretation of the PCA. In the Riemannian setup with the met-
ric being the rate of viscous dissipation, the logS Sk corresponds to the initial velocity
vk : S →
 d in the (optimal transport) flow of O associated with shape S into
Ok associated with the kth input shape Sk. In the elastic model, the boundary stress
σk
: ∂O →
 d results from the deformation ϕk of Ok onto the average object O
and effectively is the restoring force acting on the average shape S. Via the linearized
elasticity problem in the prestressed compound configuration of the average object O,
these restoring forces are identified with displacements uk. Depending on the model,
either the flow velocities vk or the linear elastic displacements uk form the basis of a
covariance analysis in the linear vector space of mappings O →
 d. The outcome are
principal shape variations of the average shape, either generated by motion fields or
displacements, respectively.
•
Quantitative shape analysis. The Riemannian metric given by the rate of viscous dissi-
pation in the path-based viscous fluid approach allows direct comparison of multiple
ensembles of shapes via pairwise distance computations. Due to the lack of a triangle
inequality this is possible only in a restricted sense in the state-based elastic approach,
where dissimilarity measures for one fixed shape and a set of varying shapes can be
computed.


Variational Methods in Shape Analysis
•
The method of choice depends on the specific application. If shapes are considered
as boundaries of objects with a viscous fluid inside then the path-based approach would
be more appropriate. The state-based elastic approach is favorable for objects which
behave more like deformable solids.
.
Cross-References
> Level Set Methods Including Fast Marching Methods
> Mumford Shah, Phase Field Models
> Numerical Methods for Variational Approach in Image Analysis
> Shape Spaces
> Variational Approach in Image Analysis
Acknowledgments
The model proposed in > Sect. ..has been developed in cooperation with Leah Bar
and Guillermo Sapiro from the University of Minnesota. Benedikt Wirth has been funded
by the Bonn International Graduate School in Mathematics. Furthermore, the work was
supported by the Deutsche Forschungsgemeinschaft, SPP “Optimization with Partial
Differential Equations.” Part of > Figs. -–-, and > -–-have been taken from
[], the results from > Figs. -, > -, and > -–-stem from [, ].
References and Further Reading
. Ambrosio
L,
Tortorelli
VM
()
On
the
approximation
of
free
discontinuity
problems. B UNIONE MAT ITAL B ():
–
. Ball J () Global invertibility of Sobolev func-
tions and the interpenetration of matter. Proc Roy
Soc Edinburgh A:–
. Beg MF, Miller MI, Trouvé A, Younes L (February
) Computing large deformation metric map-
pings via geodesic flows of diffeomorphisms. Int J
Comput Vis ():–
. Berkels B, Linkmann G, Rumpf M () An
SL() invariant shape median (submitted)
. Bornemann F, Deuflhard P () The cascadic
multigrid method for elliptic problems. Numer
Math ():–
. Bronstein A, Bronstein M, Kimmel R ()
Numerical
Geometry
of
Non-Rigid
Shapes.
Monographs in computer science. Springer, New
York
. Burger M, Osher SJ () A survey on level
set methods for inverse problems and optimal
design. Eur J Appl Math ():–
. Caselles V, Kimmel R, Sapiro G () Geodesic
active contours. Int J Comput Vis ():–
. Chan TF, Vese LA () Active contours with-
out edges. IEEE Trans Image Process ():
–
. Charpiat G, Faugeras O, Keriven R ()
Approximations of shape metrics and application
to shape warping and empirical shape statistics.
Foundations Comput Math ():–

Variational Methods in Shape Analysis 

. Charpiat G, Faugeras O, Keriven R, Maurel P
() Distance-based shape statistics. In: Pro-
ceedings of the IEEE international conference on
acoustics, speech and signal processing (ICASSP
), vol , pp –
. Chen SE, Parent RE () Shape averaging and
its applications to industrial design. IEEE Comput
Graphics Appl ():–
. Chorin AJ, Marsden JE () A Mathematical
introduction to fluid mechanics, vol of Texts in
applied mathematics. Springer, New York
. Christensen GE, Rabbitt RD, Miller MI () D
brainmappingusingadeformableneuroanatomy.
Phys Med Biol ():–
. Ciarlet PG () Three-dimensional elasticity.
Elsevier Science B.V., New York
. Cootes TF, Taylor CJ, Cooper DH, Graham J
()
Active
shape
models—their
training
and application. Comput Vis Image Underst
():–
. CremersD,Kohlberger T,Schnörr C() Shape
statistics in kernel space for variational image
segmentation. Pattern Recogn :–
. Dacorogna B () Direct methods in the calcu-
lus of variations. Springer, New York
. Dambreville S, Rathi Y, Tannenbaum A () A
shape-based approach to robust image segmenta-
tion. In: Campilho A, Kamel M (eds) IEEE com-
puter society conference on computer vision and
pattern recognition, vol of LNCS, pp –
. Delfour MC, Zolésio J () Geometries and
shapes: analysis, differential calculus and opti-
mization. Advance in design and control . SIAM,
Philadelphia
. do Carmo MP () Riemannian geometry.
Birkhäuser, Boston
. Droske M, Rumpf M () Multi scale joint
segmentation and registration of image morphol-
ogy. IEEE Trans Pattern Recogn Mach Intell
():–
. Dupuis D, Grenander U, Miller M () Vari-
ational problems on flows of diffeomorphisms
for image matching. Quart Appl Math :
–
. Eckstein I, Pons JP, Tong Y, Kuo CC, Desbrun M
() Generalized surface flows for mesh pro-
cessing. In: Eurographics symposium on geome-
try processing
. Elad (Elbaz) A, Kimmel R () On bending
invariant signatures for surfaces. IEEE Trans Pat-
tern Anal Mach Intell ():–
. Fletcher P, Lu C, Pizer S, Joshi S () Prin-
cipal geodesic analysis for the study of nonlin-
ear statistics of shape. IEEE Trans Med Imaging
():–
. Fletcher PT, Lu C, Joshi S () Statistics of shape
via principal geodesic analysis on Lie groups. In:
IEEE computer society conference on computer
vision and pattern recognition (CVPR), vol . Los
Alamitos, CA, pp –
. Fletcher T, Venkatasubramanian S, Joshi S ()
Robust statistics on Riemannian manifolds via
the geometric median. In: IEEE conference on
computer vision and pattern recognition (CVPR)
. Fletcher P, Whitaker R () Riemannian met-
rics on the space of solid shapes. In: Medical
image computing and computer assisted inter-
vention – MICCAI 
. Fréchet M () Les éléments aléatoires de
nature quelconque dans un espace distancié. Ann
Inst H Poincaré :–
. Fuchs M, Jüttler B, Scherzer O, Yang H ()
Shape metrics based on elastic deformations.
J Math Imaging Vis ():–
. Fuchs M, Scherzer O (May ) Segmentation of
biologic image data with a-priori knowledge. FSP
Report, Forschungsschwerpunkt S, Univer-
sität Innsbruck, Austria
. Fuchs M, Scherzer O () Regularized recon-
struction of shapes with statistical a priori knowl-
edge. Int J Comput Vis ():–
. Glaunès J, Qiu A, Miller MI, Younes L ()
Large deformation diffeomorphic metric curve
mapping. Int J Comput Vis ():–
. Hafner B, Zachariah S, Sanders J () Charac-
terisation of three-dimensional anatomic shapes
using principal components: application to the
proximal tibia. Med Biol Eng Comput :–
. Hong BW, Soatto S, Vese L () Enforcing local
context into shape statistics. Adv Comput Math
(online first)
. Joshi S, Davis B, Jomier M, Gerig G ()
Unbiased
diffeomorphic
atlas
construction
for computational
anatomy.
NeuroImage 
(Suppl ):–
. Karcher H () Riemannian center of mass and
mollifier smoothing. Commun Pure Appl Math
():–


Variational Methods in Shape Analysis
. Kendall DG () Shape manifolds, procrustean
metrics, and complex projective spaces. Bull Lond
Math Soc :–
. Kilian M, Mitra NJ, Pottmann H () Geo-
metric modeling in shape space. In: ACM Trans
Graph ():–
. Klassen E, Srivastava A, Mio W, Joshi SH ()
Analysis of planar shapes using geodesic paths
on shape spaces. IEEE Trans Pattern Anal Mach
Intell ():–
. Klingenberg WPA () Riemannian geometry.
Walter de Gruyter, Berlin
. Leventon ME, Grimson WEL, Faugeras O ()
Statistical shape influence in geodesic active con-
tours. In: th IEEE EMBS international summer
school on biomedical imaging
. Ling H, Jacobs DW () Shape classification
using the inner-distance. IEEE Trans Pattern Anal
Mach Intell ():–
. Liu X, Shi Y, Dinov I, Mio W () A compu-
tational model of multidimensional shape. Int J
Comput Vis (online first)
. Manay S, Cremers D, Hong BW, Yezzi AJ, Soatto
S () Integral invariants for shape match-
ing. IEEE Trans Pattern Anal Mach Intell ():
–
. Marsden JE, Hughes TJR () Mathemat-
ical
foundations
of
elasticity.
Prentice-Hall,
Englewood Cliffs
. McNeill G, Vijayakumar S () d shape classi-
fication and retrieval. In: Proceedings of the th
international joint conference on artificial intelli-
gence, pp –
. Mémoli F () Gromov-Hausdorff distances
in euclidean spaces. In: Workshop on non-rigid
shape analysis and deformable image alignment
(CVPR workshop, NORDIA’)
. Mémoli F, Sapiro G () A theoretical and
computational framework for isometry invari-
ant recognition of point cloud data. Foundations
Comput Math :–
. Michor PW, Mumford D () Riemannian
geometries on spaces of plane curves. J Eur Math
Soc :–
. Michor PW, Mumford D, Shah J, Younes L ()
A metric on shape space with explicit geodesics.
Rend Lincei Mat Appl :–
. Miller M, Trouvé A, Younes L () On the
metrics and Euler-Lagrange equations of com-
putational anatomy. Annu Rev Biomed Engg
:–
. Miller MI, Younes L () Group actions, home-
omorphisms, and matching: a general framework.
Int J Comput Vis (–):–
. Modica L, Mortola S () Un esempio di-
Γ−-convergenza. Boll Un Mat Ital B () ():
–
. Mumford D, Shah J () Optimal approxima-
tion by piecewise smooth functions and associ-
ated variational problems. Commun Pure Appl
Math :–
. Neˇcas J, ˇCsilhavý M () Multipolar viscous
fluids. Quart Appl Math ():–
. Ogden RW () Non-linear elastic deforma-
tions. Wiley, New York
. Osher S, Fedkiw R () Level set methods
and dynamic implicit surfaces, vol of Applied
mathematical sciences. Springer, New York
. Osher S, Sethian JA () Fronts propagat-
ing with curvature dependent speed: algorithms
based on Hamilton–Jacobi formulations. J Com-
put Phys ():–
. Pennec X () Left-invariant Riemannian elas-
ticity: a distance on shape diffeomorphisms?
In: Mathematical foundations of computational
anatomy - MFCA , pp –
. Pennec X, Stefanescu R, Arsigny V, Fillard P,
Ayache N () Riemannian elasticity: a sta-
tistical regularization framework for non-linear
registration. In: Medical image computing and
computer-assisted intervention – MICCAI .
LNCS, Palm Springs, pp –
. Perperidis D, Mohiaddin R, Rueckert D ()
Construction of a d statistical atlas of the car-
diac anatomy and its use in classification. In:
Duncan J, Gerig G (eds) Medical image comput-
ing and computer assisted intervention, vol 
of LNCS, pp –
. Rathi Y, Dambreville S, Tannenbaum A ()
Statistical shape analysis using kernel PCA. In:
Proceedings of SPIE, vol , pp –
. Rathi Y, Dambreville S, Tannenbaum A ()
Comparativeanalysisof kernel methodsfor statis-
tical shape learning. In: Beichel R, Sonka M (eds)
Computer vision approaches to medical image
analysis, vol of LNCS, pp –

Variational Methods in Shape Analysis 

. Rueckert D, Frangi AF, Schnabel JA () Auto-
matic construction of -d statistical deformation
models of the brain using nonrigid registration.
IEEE Trans Med Imaging ():–
. Rumpf M, Wirth B () A nonlinear elastic
shape averaging approach. SIAM J Imaging Sci
():–
. Rumpf M, Wirth B () An elasticity approach
to principal modes of shape variation. In: Pro-
ceedings of the second international conference
on scale space methods and variational meth-
ods in computer vision (SSVM ), vol of
LNCS, pp –
. Rumpf M, Wirth B () An elasticity-based
covariance analysis of shapes. Int J Comput Vis
(accepted)
. Schmidt FR, Clausen M, Cremers D () Shape
matching by variational computation of geodesics
on a manifold. In: Pattern recognition, vol of
LNCS. Springer, Berlin, pp –
. Sethian JA () Level set methods and fast
marching methods. Cambridge University Press,
Cambridge
. Shilane P, Min P, Kazhdan M, Funkhouser T
() The Princeton shape benchmark. In: Pro-
ceedings of the shape modeling international,
, Genova, pp –
. Söhn M, Birkner M, Yan D, Alber M ()
Modelling individual geometric variation based
on dominant eigenmodes of organ deformation:
implementation and evaluation. Phys Med Biol
:–
. Spivak M () A comprehensive introduction
to differential geometry, vol . Publish or Perish,
Boston
. Srivastava A, Jain A, Joshi S, Kaziska D ()
Statistical shape models using elastic-string repre-
sentations. In Narayanan P (ed) Asian conference
on computer vision, vol of LNCS. Springer,
Heidelberg, pp –
. Sundaramoorthi G, Yezzi A, Mennucci A ()
Sobolev active contours. Int J Comput Vis
():–
. Thorstensen N, Segonne F, Keriven R () Pre-
image as karcher mean using diffusion maps:
application to shape and
image
denoising.
In: Proceedings of the second international
conference
on
scale
space
methods
and
variational
methods
in
computer
vision
(SSVM ), vol of LNCS, pp –
. Truesdell C, Noll W () The non-linear field
theories of mechanics. Springer, Berlin
. Tsai A, Yezzi A, Wells W, Tempany C, Tucker
D, Fan A, Grimson WE, Willsky A ()
A shape-based approach to the segmentation of
medical imagery using level sets. IEEE Trans Med
Imaging ():–
. Vaillant M, Glaunès J () Surface matching via
currents. In: IPMI : Information processing
in medical imaging, vol of LNCS. Springer,
Glenwood Springs, pp –
. Wirth B () Variational methods in shape
space. Dissertation, University Bonn, Bonn
. Wirth B, Bar L, Rumpf M, Sapiro G ()
Geodesics in shape space via variational time dis-
cretization. In: Proceedings of the th interna-
tional conference on energy minimization meth-
ods in computer vision and pattern recognition
(EMMCVPR’), vol of LNCS, pp –
. Wirth B, Bar L, Rumpf M, Sapiro G () A
continuum mechanical approach to geodesics in
shape space (submitted to IJCV)
. Yezzi AJ, Mennucci A () Conformal metrics
and true “gradient flows” for curves. In: ICCV
: Proceedings of the th IEEE international
conference on computer vision, pp –
. Younes L (April ) Computable elastic dis-
tances between shapes. SIAM J Appl Math
():–
. Younes L, Qiu A, Winslow RL, Miller MI ()
Transport of relational structures in groups of
diffeomorphisms. J Math Imaging Vis ():–
. Yushkevich P, Fletcher PT, Joshi S, Thalla A,
Pizer SM () Continuous medial representa-
tions for geometric object modeling in d and d.
Image Vis Comput ():–
. Zolésio JP () Shape topology by tube
geodesic. In: IFIP conference on system modeling
and optimization No , pp –


Manifold Intrinsic Similarity
Alexander M. Bronstein ⋅Michael M. Bronstein
.
Introduction....................................................................
..
Problems................................................................................
..
Methods.................................................................................
..
Chapter Outline........................................................................
.
Shapes as Metric Spaces........................................................
..
Basic Notions...........................................................................
...
Topological Spaces.....................................................................
...Metric Spaces...........................................................................
...Isometries...............................................................................
..
Euclidean Geometry...................................................................
..
Riemannian Geometry................................................................
...Manifolds................................................................................
...Differential Structures.................................................................
...Geodesics.................................................................................
...Embedded Manifolds...................................................................
...Rigidity...................................................................................
..
Diffusion Geometry....................................................................
...Diffusion Operators....................................................................
..
Diffusion Distances....................................................................
.
Shape Discretization............................................................
..
Sampling.................................................................................
...
Farthest Point Sampling...............................................................
...Centroidal Voronoi Sampling.........................................................
..
Shape Representation..................................................................
...Simplicial Complexes..................................................................
...Parametric Surfaces....................................................................
...Implicit Surfaces........................................................................
.
Metric Discretization...........................................................
..
Shortest Paths on Graphs..............................................................
...Dijkstra’s Algorithm....................................................................
...Metrication Errors and Sampling Theorem........................................
..
Fast Marching..........................................................................
...Eikonal Equation.......................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Manifold Intrinsic Similarity
...Triangular Meshes.....................................................................
...Parametric Surfaces....................................................................
...Parallel Marching......................................................................
...Implicit Surfaces and Point Clouds..................................................
..
Diffusion Distance.....................................................................
...Discretized Laplace–Beltrami Operator............................................
...Computation of Eigenfunctions and Eigenvalues.................................
...Discretization of Diffusion Distances...............................................
.
Invariant Shape Similarity.....................................................
..
Rigid Similarity.........................................................................
...
Hausdorff Distance....................................................................
...Iterative Closest Point Algorithms...................................................
...Shape Distributions....................................................................
...Wasserstein Distances.................................................................
..
Canonical Forms........................................................................
...Multidimensional Scaling.............................................................
...Eigenmaps...............................................................................
..
Gromov–Hausdorff Distance.........................................................
...Generalized Multidimensional Scaling.............................................
..
Graph-Based Methods................................................................
...Probabilistic Gromov–Hausdorff Distance.........................................
..
Gromov–Wasserstein Distances.....................................................
...Numerical Computation..............................................................
..
Shape DNA..............................................................................
.
Partial Similarity...............................................................
..
Significance.............................................................................
..
Regularity...............................................................................
..
Partial Similarity Criterion...........................................................
..
Computational Considerations.......................................................
.
Self-Similarity and Symmetry..................................................
..
Rigid Symmetry........................................................................
..
Intrinsic Symmetry....................................................................
..
Spectral Symmetry.....................................................................
..
Partial Symmetry.......................................................................
..
Repeating Structure....................................................................
.
Feature-Based Methods........................................................
..
Feature Descriptors....................................................................
...Feature Detection......................................................................

Manifold Intrinsic Similarity 

...Feature Description....................................................................
...Heat Kernel Signatures................................................................
...Scale-Invariant Heat Kernel Signatures.............................................
..
Bags of Features........................................................................
..
Combining Global and Local Information.........................................
.
Concluding Remarks...........................................................


Manifold Intrinsic Similarity
Abstract: Non-rigid shapes are ubiquitous in Nature and are encountered at all levels
of life, from macro to nano. The need to model such shapes and understand their behav-
ior arises in many applications in imaging sciences, pattern recognition, computer vision,
and computer graphics. Of particular importance is understanding which properties of the
shape are attributed to deformations and which are invariant, i.e., remain unchanged. This
chapter presents an approach to non-rigid shapes from the point of view of metric geom-
etry. Modeling shapes as metric spaces, one can pose the problem of shape similarity as
the similarity of metric spaces and harness tools from theoretical metric geometry for the
computation of such a similarity.
.
Introduction
Those who played the game Rock, Paper, and Scissors in their childhood certainly remem-
ber the three gestures used in the game: Rock, represented by a clenched fist; Paper,
represented by an open hand; and Scissors, represented by the extended index and middle
fingers. These gestures are a toy example of the non-rigid shape similarity problem, which is
the central topic of this chapter. No matter how one bends the fingers, he will immediately
recognize the underlying object: the human hand.
More generally, the problem of determining the similarity of shapes undergoing certain
class of transformations is termed invariant shape similarity. A similarity criterion is said
to be invariant if it is not influenced by the transformation (> Fig. -). Different classes of
transformations prescribe different similarity criteria based on geometric shape properties
that are invariant under such transformations. The wider is the class, the less properties
are preserved, and as a thumb rule, the more difficult is the problem. Specifically, in this
chapter, we will consider rigid, inelastic, topology-changing, and scaling transformations.
In many cases, such transformations are a good approximation of real transformations that
natural objects may undergo.
..
Problems
Since non-rigid shapes are ubiquitous in the world and are encountered at all scales from
macro to nano, non-rigid shape similarity plays a key role in many applications in imag-
ing sciences, pattern recognition, computer vision, and computer graphics. Two archetype
problems in shape analysis considered in this chapter areinvariant similarity and correspon-
dence. As will be discussed in the following, these two problems are inter-related: finding
best correspondence between two shapes also allows quantifying their similarity.
A good example of shape similarity is the problem of face recognition [, , ]. As the
crudest approximation, one can think of faces as rigid surfaces and compare them using
similarity criteria invariant under rigid transformations. However, such an approach does
not account for surface deformations due to facial expressions, which can be approximated

Manifold Intrinsic Similarity 

⊡Fig. -
Invariant shape similarity
by inelastic deformations. Accounting for such deformations requires different similarity
criteria. Yet, even elastic deformations are not enough to model the behavior of human
faces: many facial expresses involve elastic deformations that change the facial shape topol-
ogy (think of open and closed mouth). This extension of the model will require revisiting
the similarity criterion once again.
The problem of correspondence is often encountered in shape synthesis applications
such as morphing. In order to morph one shape into the other, one needs to know which
point on the first shape will be transformed into a point on the second shape, in other
words, establishing a correspondence between the shapes.
..
Methods
Many different approaches to shape similarity and correspondence can be considered as
instances of the minimum distortion correspondence problem, in which two shapes are
endowed with certain structure, and one attempts to find the best (least distorting) match-
ing between these structures. Such structures can be local (e.g., multiscale heat kernel
signatures [], local photometric properties [, ], or conformal factor []) or global
(e.g., geodesic [, , ], diffusion [], and commute time []) distances. The distor-
tion of the best possible correspondence can be used as a criterion of shape similarity. By
defining a structure invariant under certain class of transformations, it is possible to obtain
invariant correspondence or similarity.
Local structures can be regarded as feature descriptors. As a model for global structures,
metric spaces are used.


Manifold Intrinsic Similarity
..
Chapter Outline
This chapter tries to present a unifying view on the archetypical problems in shape analysis.
The first part presents a metric view on the problem of non-rigid shape similarity and cor-
respondence, a common denominator allowing to deal with different types of invariance.
According to this model, shapes are represented as metric spaces. The mathematical foun-
dations of this model are provided in > Sect. .. > Sections .and
> .deal with
discrete representation of shapes, which is of essence in practical numerical computations.
> Sect. .provides a rigorous formulation of the invariant shape similarity problem and
reviews different algorithms for its computation. > Section .deals with an extension of
invariant similarity to shapes which are partially similar, and > Sect. .deals with a par-
ticular case of self-similarity and symmetry. Local feature-based methods and their use to
create global shape descriptors are presented in > Sect. .. Finally, concluding remarks
in > Sect. .end the chapter. This chapter is based in part on the book [], to which the
reader is referred for further discussion and details.
.
Shapes as Metric Spaces
Elad and Kimmel [, ], Mémoli and Sapiro [], and Bronstein et al. [, ] suggested
to model shapes as metric spaces. The key idea of this model is that it allows to compare
shapes as metric spaces. Since the model allows arbitrarity in the definition of the metric,
desired invariance considerations guide the choice of the metric.
This section introduces the mathematical formalism and notation of this model
and shows the construction of three different types of metric geometries: Euclidean,
Riemannian, and diffusion.
..
Basic Notions
...
Topological Spaces
Given a set X, a topology T on X is a collection of subsets of X satisfying (Ti) X,∈T; (Tii)
⋃α Uα ∈T for Uα ∈T; (Tiii) ⋂N
i=Ui ∈T for Ui ∈T. X together with T is called a topological
space. By convention, sets in T are referred to as open sets and their complements as closed
sets.
A neighborhood N(x) of x is a set containing an open set U ∈T such that x ∈U. Points
with neighborhood are called interior.
A topological space is called Hausdorff if distinct points in it have disjoint neighbor-
hoods.
Two topological spaces X and Y are homeomorphic if there exists bijection
α : X →Y which is continuous and has continuous inverse α−. Since homeomorphisms
copy topologies, homeomorphic spaces are topologically equivalent [].

Manifold Intrinsic Similarity 

...
Metric Spaces
A function d : X × X →R which is (Mi) positive-definite (d(x, y) > for all x ≠y and
d(x, y) = for x = y) and (Mii) subadditive (d(x, z) ≤d(x, y) + d(y, z) for all x, y, z) is
called a metric on X. The metric is an abstraction of the notion of distance between pairs
of points on X. Property (Mii) is called triangle inequality and generalizes the known fact:
the sum of the lengths of two edges of a triangle is greater or equal to the length of the third
edge. The pair (X, d) is called a metric space.
A metric induces topology through the definition of open metric ball Br(x) = {x′ ∈
X : d(x, x′) < r}. A neighborhood of x in a metric space is a set containing a metric ball
Br(x) [].
...
Isometries
Given two metric spaces (X, d) and (Y, δ), the set C ⊂X × Y of pairs such that for every
x ∈X there exists at least one y ∈Y such that (x, y) ∈C, and similarly for every y ∈Y there
exists an x ∈X such that (x, y) ∈C is called a correspondence between X and Y. Note that
a correspondence C is not necessarily a function. The correspondence is called bijective if
every point in X has a unique corresponding point in Y and vice versa.
The discrepancy of the metrics d and δ between the corresponding points is called the
distortion of the correspondence,
dis(C) =
sup
(x,y),(x′,y′)∈C
∣d(x, x′) −δ(y, y′)∣.
Metric spaces (X, d) and (Y, δ) are said to be є-isometric if there exists a correspon-
dence C with dis(C) ≤є. Such a C is called an є-isometry.
A particular case of a -isometry is called an isometry. In this case, the correspondence
is a bijection and X and Y are called isometric.
..
Euclidean Geometry
Euclidean space Rm (hereinafter also denoted as E) with the Euclidean metric dE(x, x′) =
∥x−x′∥is the simplest example of a metric space. Given as a subset X of E, we can measure
the distances between points x and x′ on X using the restricted Euclidean metric,
dE∣X×X(x, x′) = dE(x, x′)
for all x, x′ in X.
The restricted Euclidean metric dE∣X×X is invariant under Euclidean transforma-
tions of X, which include translation, rotation, and reflection in E. In other words, X
and its Euclidean transformation i(X) are isometric in the sense of the Euclidean met-
ric. Euclidean isometries are called congruences and two subsets of E differing up to a
Euclidean isometry are said to be congruent.


Manifold Intrinsic Similarity
..
Riemannian Geometry
...
Manifolds
A Hausdorff space X which is locally homeomorphic to Rn (i.e., for every x in X there
exists a neighborhood U and a homeomorphism α : U →Rn) is called an n-manifold or
an n-dimensional manifold. The function α is called a chart. A collection of neighborhoods
that cover X together with their charts is called an atlas on X. Given two charts α and β with
overlapping domains U and V, the map βα−: α(U ∩V) →β(U ∩V) is called a transition
function. A manifold whose transition functions are all differentiable is called a differen-
tiable manifold. More generally a Ck-manifold has all transition maps k-times continuously
differentiable. A C∞-manifold is called smooth.
A manifold with boundary is not a manifold in the strict sense of the above definition.
Its interior points are locally homeomorphic to Rn, and every point on the boundary ∂X is
homeomorphic to [,∞) × Rn−.
Of particular interest for the discussion in this chapter are two-dimensional (n = )
manifolds, which model boundaries of physical objects in the world surrounding us. Such
manifolds are also called surfaces. In the following, when referring to shapes and objects,
the terms manifold, surface, and shape will be used synonymously.
...
Diﬀerential Structures
Locally, a manifold can be represented as a linear space, in the following way. Let
α : U →Rn be a chart on a neighborhood of x and γ : (−,) :→X be a differentiable curve
passing through x = γ(). The derivative of the curve d
dt (α○γ)() is called a tangent vector
at x. The set of all equivalence classes of tangent vectors at x forming an n-dimensional real
vector space is called the tangent space Tx X at x.
A family of inner products ⟨⋅,⋅⟩x : Tx X × Tx X →R depending smoothly on x is
called Riemannian metric tensor. A manifold X with a Riemannian metric tensor is called
a Riemannian manifold.
The Riemannian metric allows to define local length structures and differential calculus
on the manifold. Given a differentiable scalar-valued function f : X →R, the exterior
derivative (differential) is a form d f = ⟨∇f ,⋅⟩on the tangent space TX. For a tangent
vector v ∈Tx X, d f (x)v = ⟨∇f (x),v⟩x. ∇f is called the gradient of f at x and is a natural
generalization of the notion of the gradient in vector spaces to manifolds. Similarly to the
definition of Laplacian satisfying
∫X⟨∇f ,∇h⟩xdμ(x) = ∫X hΔX f dμ(x)
for differentiable scalar-valued functions f and h, the operator ΔX is called the Laplace–
Beltrami operator, a generalization of the Laplacian. Here μ denotes the measure associated
with the n-dimensional volume element (area element for n = ). The Laplace–Beltrami
operator is (Li) symmetric (∫X hΔX f dμ(x) = ∫X f ΔXxdμ(x)), (Lii) of local action

Manifold Intrinsic Similarity 

(ΔX f (x) is independent of the value of f (x′) for x′ ≠x), (Liii) positive semi-definite
(∫X f ΔX f dμ(x) ≥), (In many references, the Laplace–Beltrami is defined as a negative
semi-definite operator.) and (Liv) coincides with the Laplacian on Euclidean domains, such
that ΔX f = if f is a linear function and X is Euclidean.
...
Geodesics
Another important use of the Riemannian metric tensor is to measure the length of paths
on the manifold. Given a continuously differentiable curve γ : [a, b] →X, its length is
given by
ℓ(γ) = ∫
b
a ⟨γ′(t),γ′(t)⟩/
γ(t)dt.
For the set of all continuously differentiable curves Γ(x, x′) between the points x, x′,
dX(x, x′) =
inf
γ∈Γ(x,x′) ℓ(γ)
(.)
defines a metric on X referred to as length or geodesic metric. If the manifold is compact,
for any pair of points x and x′ there exists a curve γ ∈Γ(x, x′) called a minimum geodesic
such that ℓ(γ) = dX(x, x′).
...
Embedded Manifolds
A particular realization of a Riemannian manifold called embedded manifold (or embed-
ded surface for n = ) is a smooth submanifold of Rm (m > n). In this case, the tangent
space is an n-dimensional hyperplane in Rm, and the Riemannian metric is defined as the
restriction of the Euclidean inner product to the tangent space, ⟨⋅,⋅⟩Rm∣T X.
The length of a curve γ : [a, b] →X ⊂Rm on an embedded manifold is expressed
through the Euclidean metric,
ℓ(γ) = ∫
b
a (⟨γ′(t),γ′(t)⟩Rm∣Tγ(t) X)
/dt,= ∫
b
a ∥γ′(t)∥Rmdt
(.)
and the geodesic metric dX defined according to (> .) is said to be induced by dRm.
(Repeating the process, one obtains that the metric induced by dX is equal to dX. For this
reason, dX is referred to as intrinsic metric [].)
Though apparently embedded manifolds are a particular case of a more general notion
of Riemannian manifolds, it appears that any Riemannian manifold can be realized as an
embedded manifold. This is a consequence of the Nash embedding theorem [], showing
that a Ck(k ≥) Riemannian manifold can be isometrically embedded in a Euclidean space
of dimension m = n+ n + . In other words, any smooth Riemannian manifold can be
defined as a metric space which is isometric to a smooth submanifold of a Euclidean space
with the induced metric.


Manifold Intrinsic Similarity
...
Rigidity
Riemannian manifolds do not have a unique realization as embedded manifolds. One
obvious degree of freedom is the set of all Euclidean isometries: two congruent embed-
ded manifolds are isometric and thus are realizations of the same Riemannian manifold.
However, a Riemannian manifold may have two realizations which are isometric but
incongruent. Such manifolds are called non-rigid. If, on the other hand, a manifold’s only
isometries are congruences, it is called rigid.
..
Diﬀusion Geometry
Another type of metric geometry arises from the analysis of heat propagation on manifolds.
This geometry is called diffusion and is also intrinsic. We start by reviewing properties of
diffusion operators.
...
Diﬀusion Operators
A function k : X × X →R is called a diffusion kernel if it satisfies the following
properties: (Ki) non-negativity: k(x, x) ≥; (Kii) symmetry: k(x, y) = k(y, x); (Kiii)
positive-semidefiniteness: for every bounded f ,
∫∫k(x, y)f (x)f (y)dμ(x)dμ(y) ≥;
(Kiv) square integrability: ∫∫k(x, y)dμ(x)dμ(y) < ∞; and (Kv) conservation:
∫k(x, y)dμ(y) = . The value of k(x, y) can be interpreted as a transition probability
from x to y by one step of a random walk on X.
Diffusion kernel defines a linear operator
Kf = ∫k(x, y)f (y)dμ(y),
(.)
which is known to be self-adjoint. Because of (Kiv), K has a finite Hilbert norm and there-
fore is compact. As the result, it admits a discrete eigendecomposition Kψi = αiψi with
some eigenfunctions {ψi}∞
i=and eigenvalues {αi}∞
i=. αi ≥by virtue of property (Kiii),
and αi ≤by virtue of (Kv) and consequence of the Perron–Frobenis theorem.
By the spectral theorem, the diffusion kernel can be presented as k(x, y)
=
∞
∑
i=
αiψi(x)ψi(y). Since {ψi}∞
i=form an orthonormal basis of L(X),
∫∫k(x, y)dμ(x)dμ(y) =
∞
∑
i=
α
i ,
(.)

Manifold Intrinsic Similarity 

a fact sometimes referred to as Parseval’s theorem. Using these results, properties (Kiii–Kv)
can be rewritten in the spectral form as ≤αi ≤and
∞
∑
i=
α
i < ∞.
An important property of diffusion operators is the fact that for every t ≥, the operator
Kt is also a diffusion operator with the eigenbasis of K and corresponding eigenvalues
{αt
i}
∞
i=. The kernel of Kt expresses the transition probability by random walk of t steps.
This allows to define a scale space of kernels, {kt(x, y)}t∈T, with the scale parameter t.
There exists a large variety of possibilities to define a diffusion kernel and the related
diffusion operator. Here, we restrict our attention to operators describing heat diffusion.
Heat diffusion on surfaces is governed by the heat equation,
(ΔX + ∂
∂t)u(x, t) = ;
u(x,) = u(x),
(.)
where u(x, t) is the distribution of heat on the surface at point x in time t, uis the initial
heat distribution, and ΔX is the positive-semidefinite Laplace-Beltrami operator, a gener-
alization of the second-order Laplacian differential operator Δ to non-Euclidean domains.
(If X has a boundary, boundary conditions should be added.)
On Euclidean domains (X = Rm), the classical approach to the solution of the heat
equation is by representing the solution as a product of temporal and spatial components.
The spatial component is expressed in the Fourier domain, based on the observation that
the Fourier basis is the eigenbasis of the Laplacian Δ, and the corresponding eigenvalues
are the frequencies of the Fourier harmonics. A particular solution for a point initial heat
distribution u(x) = δ(x −y) is called the heat kernel ht(x −y) =

(πt)m/e−∥x−y∥/t,
which is shift-invariant in the Euclidean case. A general solution for any initial condition
uis given by convolution Htu= ∫Rm ht(x −y)u(y)dy, where Ht is referred to as heat
operator.
In the non-Euclidean case, the eigenfunctions of the Laplace–Beltrami operator ΔX ϕi =
λiϕi can be regarded as a “Fourier basis,” and the eigenvalues given the “frequency”
interpretation. The heat kernel is not shift-invariant but can be expressed as ht(x, y) =
∑∞
i=e−tλi ϕi(x)ϕi(y).
It can be shown that the heat operator is related to the Laplace–Beltrami operator as
Ht = e−tΔ, and as a result, it has the same eigenfunctions ϕi and corresponding eigenval-
ues e−tλi. It can be thus seen as a particular instance of a more general family of diffusion
operators K diagonalized by the eigenbasis of the Laplace–Beltrami operator, namely K’s
as defined in the previous section but restricted to have the eigenfunctions ψi = ϕi.
The corresponding diffusion kernels can be expressed as
k(x, y) =
∞
∑
i=
K(λi)ϕi(x)ϕi(y),
(.)
where K(λ) is some function such that αi = K(λi) (in the case of Ht, K(λ) = e−tλ). Since
the Laplace–Beltrami eigenvalues can be interpreted as frequency, K(λ) can be thought
of as the transfer function of a low-pass filter. Using this signal processing analogy, the
kernel k(x, y) can be interpreted as the point spread function at a point y, and the action


Manifold Intrinsic Similarity
of the diffusion operator Kf on a function f on X can be thought of as the application
of the point spread function by means of a non shift-invariant version of convolution. The
transfer function of the diffusion operator K t is Kt(λ), which can be interpreted as multiple
applications of the filter K(λ). Such multiple applications decrease the effective bandwidth
of the filter and, consequently, increase its effective support in space. Because of this duality,
both k(x, y) and K(λ) are often referred to as diffusion kernels.
..
Diﬀusion Distances
Since a diffusion kernel k(x, y) measures the degree of proximity between x and y, it can
be used to define a metric
d(x, y) = ∥k(x,⋅) −k(y,⋅)∥
L(X),
(.)
on X, which was first constructed by Berard et al. in [*] and dubbed as the diffusion distance
by Coifman and Lafon []. Another way to interpret the latter distance is by consid-
ering the embedding Ψ : x ↦L(X) by which each point x on X is mapped to the
function Ψ(x) = k(x,⋅). The embedding Ψ is an isometry between X equipped with dif-
fusion distance and L(X) equipped with the standard Lmetric, since d(x, y) = ∥Ψ(x)−
Ψ(y)∥L(X). Because of spectral duality, the diffusion distance can also be written as
d(x, y) =
∞
∑
i=
K(λi)(ϕi(x) −ϕi(y)).
(.)
Here as well we can define an isometric embedding Φ :
x
↦
ℓwith Φ(x) =
{K(λi)ϕi(x)}∞
i=, termed as the diffusion map by Lafon. The diffusion distance can be
cast as d(x, y) = ∥Φ(x) −Φ(y)∥ℓ.
The same way a diffusion operator Kt defines a scale space, a family of diffusion metrics
can be defined for t ≥as
d
t (x, y) = ∥Φt(x) −Φt(y)∥
ℓ
(.)
=
∞
∑
i=
Kt(λi)(ϕi(x) −ϕi(y)),
where Φt(x) = {Kt(λi)ϕi(x)}∞
i=. Interpreting diffusion processes as random walks, dt
can be related to the “connectivity” of points x and y by walks of length t (the more such
walks exist, the smaller is the distance).
The described framework is very generic, leading to numerous potentially useful
diffusion geometries parametrized by the selection of the transfer function K(λ). Two par-
ticular choices are frequent in shape analysis, the first one being the heat kernel, Kt(λ) =
e−tλ, and the second one being the commute time kernel, K(λ) =

√
λ , resulting in the
heat diffusion and commute time metrics, respectively. While the former kernel involves
a scale parameter, typically tuned by hand, the latter one is scale-invariant, meaning that
neither the kernel, nor the diffusion metric it induces changes under uniform scaling of
the embedding coordinates of the shape.

Manifold Intrinsic Similarity 

.
Shape Discretization
In order to allow storage and processing of a shape by a digital computer, it has to
be discretized. This section reviews different notions in the discrete representation of
shapes.
..
Sampling
Sampling is the reduction of the continuous surface X representing a shape into a finite
discrete set of representative points ˆX = {x, . . ., xN}. The number of points ∣ˆX∣= N is
called the size of the sampling. The radius of the sampling refers to the smallest positive
scalar r for which ˆX is an r-covering of X, i.e.,
r( ˆX) = max
x∈X min
xi∈ˆX
dX(x, xi).
(.)
The sampling is called s-separated if dX(xi, xj) ≥s for every distinct xi, xj ∈ˆX.
Sampling partitions the continuous surface into a set of disjoint regions,
Vi = {x ∈X : dX(x, xi) < dX(x, xj), x j≠i ∈ˆX},
(.)
called the Voronoi regions [] (> Fig. -). A Voronoi region Vi contains all the points on
X that are closer to xi than to any other x j. That the sampling is said to induce a Voronoi
tessellation (Unlike in the Euclidean case where every sampling induces a valid tessellation
(cell complex), samplings of curved surfaces may result in Voronoi regions that are not
⊡Fig. -
Voronoi decomposition of a surface with a non-Euclidean metric


Manifold Intrinsic Similarity
valid cells, i.e., are not homeomorphic to a disc. In [], Leibon and Letscher showed that an
r-separated sampling of radius r with r smaller than 
of the convexity radius of the shape
is guaranteed to induce a valid tessellation.) which we denote by V( ˆX) = {Vi, . . . , Vn}.
Sampling can be regarded as a quantization process in which a point x on the contin-
uous surface is represented by the closest xi in the sampling []. Such a process can be
expressed as a function mapping each Vi to the corresponding (Points on the boundary
of the Voronoi regions are equidistant from at least two sample points and therefore can
be mapped arbitrarily to any of them.) sample xi. Intuitively, the smaller are the Voronoi
regions, the better is the sampling. Sampling quality is quantified using an error function.
For example,
є∞( ˆX) = max
x∈X dX(x, ˆX) = max
x∈X min
xi∈ˆX
dX(x, xi)
(.)
determines the maximum size of the Voronoi regions. If the shape is further equipped with
a measure (e.g., the standard area measure), other error functions can be defined, e.g.,
єp( ˆX) = ∑
i ∫Vi
d p
X(x, xi)dμ(x).
(.)
In what follows, we will show sampling procedures optimal or nearly-optimal in terms
of these criteria.
...
Farthest Point Sampling
Farthest point sampling (FPS) is a greedy procedure constructing a sequence of samplings
ˆX, ˆX, . . . . A sampling ˆXN+is constructed from ˆXN by adding the farthest point
xN+= arg max
x∈X dX(x, ˆXN) = arg max
x∈X min
xi∈ˆXN
dX(x, xi).
(.)
The sequence {rN} of the sampling radii associated with { ˆXN} is non-increasing and,
furthermore, each ˆXN is also rN-separated. The starting point xis usually picked up at
random, and the stopping condition can be either the sampling size or radius.
Though FPS does not strictly minimize any of the error criteria defined in the previous
section, in terms of є∞it is no more than twice inferior to the optimal sampling of the same
size []. In other words, for ˆX produced using FPS,
є∞( ˆX) ≤min
∣ˆX′∣=∣ˆX∣
є∞( ˆX′).
(.)
This result is remarkable, as finding the optimal sampling is known to be an NP-hard
problem.

Manifold Intrinsic Similarity 

...
Centroidal Voronoi Sampling
In order for a sampling to be є-optimal, each sample xi has to minimize
∫Vi
d
X(x, xi)dμ(x).
(.)
A point minimizing the latter quantity is referred to as the centroid of Vi. Therefore, an є-
optimal sampling induces a so-called centroidal Voronoi tessellation (CVT), in which the
centroid of each Voronoi region coincides with the sample point inducing it [, ]. Such
a tessellation and the corresponding centrodial Voronoi sampling are generally not unique.
A numerical procedure for the computation of a CVT of a shape is known as the Lloyd-
Max algorithm [, ]. Given some initial sampling ˆXof size N (produced, e.g., using
FPS), the Voronoi tessellation induced by it is computed. The centroids of each Voronoi
region are computed, yielding a new sampling ˆXof size N. The process is repeated
iteratively until the change of ˆXk becomes insignificant. While producing high-quality
samplings in practice, the Lloyd–Max procedure is guaranteed to converge only to a local
minimum of є. For computational aspects of CVTs on meshes, the reader is referred to
[].
..
Shape Representation
Once the shape is sampled, it has to be represented in a way allowing computation of
discrete geometric quantities associate with it.
...
Simplicial Complexes
The simplest representation of a shape is obtained by considering the points of the sampling
as points in the ambient Euclidean space. Such a representation is usually referred to as a
point cloud. Points in the cloud are called vertices and denoted by X = {x, . . . , xN}. The
notion of a point cloud can be generalized using the formalism of simplical complexes. For
our purpose, an abstract k-simplex is a set of cardinality k+. A subset of a simplex is called
a face. A set K of simplices is said to be an abstract simplical complex if any face of σ ∈K is
also in K, and the intersection of any two simplices σ, σ∈K is a face of both σand σ. A
simplicial k-complex is a simplicial complex in which the largest dimension of any simplex
is k. A simplicial k-complex is said to be homogeneous if every simplex of dimension less
than k is the face of some k-simplex. A topological realization ¯K of a simplicial complex K
maps K to a simplicial complex in Rn, in which vertices are identified with the canonical
basis of Rn and each simplex in K is represented as the convex hull of the corresponding
points {ei}. A geometric realization ϕX( ¯K) is a map of the simplicial complex ¯K to R
defined by associating the standard basis vectors ei ∈Rn with the vertex positions xi.
In this terminology, a point cloud is a simplicial -complex having a discrete topol-
ogy. Introducing the notion of neighborhood, we can define a sub-set E ⊂X × X of pairs


Manifold Intrinsic Similarity
of vertices that are adjacent. Pairs of adjacent vertices are called edges, and the simplicial
-complex X ∪E has a graph topology, i.e., the set of vertices X forms an undirected graph
with the set of edges E. A simplicial -complex consisting of vertices, edges, and triangular
faces built upon triples of vertices and edges is called a triangular mesh. The mesh is called
topologically valid if it is homeomorphic to the underlying continuous surface X. This usu-
ally implies that the mesh has to be a two-manifold. A mesh is called geometrically valid
if it does not contain self-intersecting triangles, which happens if an only if the geometric
realization ϕX( ¯K) of the mesh is bijective. Consequently, any point x on a geometrically
valid mesh can be uniquely represented as x = φX(u). The vector u is called the barycentric
coordinates of x, and has at most three non-zero elements. If the point coincides with a ver-
tex, u is a canonical basis vector; if the point lies on an edge, u has two non-zero elements;
otherwise, u has three non-zero elements and x lies on a triangular face face.
A particular way of constructing a triangular mesh stems from the Voronoi tessellation
induced by the sampling. We define the simplicial -complex as
X ∪{(xi,xj) : ∂Vi ∩∂Vj ≠/} ∪{(xi,xj,xk) : ∂Vi ∩∂Vj ∩∂Vk ≠/},
(.)
in which a pair of vertices spans an edge and a triple of vertices spans a face if the corre-
sponding Voronoi regions are adjacent. A mesh defined in this way is called a Delaunay
mesh. (Unlike in the Euclidean case where every sampling induces a valid Delaunay trian-
gulation, an invalid Voronoi tessellation results in a topologically invalid Delaunay mesh.
In [], Leibon and Letscher showed that under the same conditions sufficient for the
existence of a valid Voronoi tessellation, the Delaunay mesh is also topologically valid.)
...
Parametric Surfaces
Shapes homeomorphic to a disc can be parametrized using a single global chart, e.g., on
the unit square, x : [,]→R. (Manifolds with more complex topology can still be
parametrized in this way by introducing cuts that open the shape into a topological disc.)
Such surfaces are called parametric and can be sampled directly in the parametrization
domain. For example, if the parametrization domain is sampled on a regular Cartesian grid,
the shape can be represented as three N × N arrays of x, y, and z values. Such a completely
regular structure is called a geometry image [, , ] and can be thought indeed as a
three-channel image that can undergo standard image processing such as compression.
Geometry images are ideally suitable for processing by vector and parallel hardware.
...
Implicit Surfaces
Another way of representing a shape is by considering the isosurfaces {x : Φ(x) = }
of some function Φ defined on a region of R. Such a representation is called implicit

Manifold Intrinsic Similarity 

and it often arises in medical imaging applications, where shapes are two dimensional
boundaries created by discontinuities in volumetric data. Implicit representation can
be naturally processed using level-set based algorithms and it easily handles arbitrary
topology. A disadvantage is the bigger amount of storage commonly required for such
representations.
.
Metric Discretization
Next step in the discrete representation of shapes is the discretization of the metric.
..
Shortest Paths on Graphs
The most straightforward approach to metric discretization arises from considering the
shape as a graph in which neighbor vertices are connected. A path in the graph between
vertices xi, xj is an ordered set of connected edges
Γ(xi, xj) = {(xi, xi),(xi, xi),...,(xik, xik+)} ⊂E,
(.)
where xi= xi and xik+= x j. The length of path Γ is the sum of its constituent edge lengths,
L(Γ(xi, xj)) =
k
∑
n=
∥xik −xik+∥.
(.)
A minimum geodesic in a graph is the shortest path between the vertices,
Γ∗(xi, xj) = arg min
Γ(xi ,x j) L(Γ(xi, xj)).
(.)
We can use dL(xi, xj) = L(Γ∗(xi, xj)) as a discrete approximation to the geodesic metric
dX(xi, xj).
According to the Bellman optimality principle [], given Γ∗(xi, xj) a shortest path
between xi and x j and xk a point on the path, the sub-paths Γ∗(xi, xk) and Γ∗(xk, xj)
are the shortest paths between xi, xk and xk, xj, respectively. The length of the shortest
path in the graph can be thus expressed by the following recursive equation:
dL(xi, xj) =
min
xk :(xk,x j)∈E {dL(xi, xk) + ∥xk −x j∥}.
(.)
...
Dijkstra’s Algorithm
A famous algorithm for the solution of the recursion (> .) was proposed by Dijkstra.
Dijkstra’s algorithm measures the distance map d(xk) = dL(xi, xk) from the source vertex
xi to all the vertices in the graph.


Manifold Intrinsic Similarity
Initialize d(xi) = , d(xk) = ∞for all k ≠i; queue of unprocessed vertices
Q = {x,..., xN}.
while Q is non-empty do
Find vertex with smallest value of d in the queue
x = arg min
x∈Q d(x)
for all unprocessed adjacent vertices x′ ∈Q : (x, x′) ∈E do
d(x′) = min {d(x′), d(x) + ∥x −x′∥}
end for
Remove x from Q.
end while
Every vertex in Dijkstra’s algorithm is processed exactly once, hence Nn outer itera-
tions are performed. Extraction of vertex with smallest d is straightforward with O(N)
complexity and can be reduced to O(log N) using efficient data structures such as Fib-
bonaci heap. In the inner loop, updating adjacent vertices in our case, since the graph is
sparsely connected, is O(). The resulting overall complexity is O(N log N).
...
Metrication Errors and Sampling Theorem
Unfortunately, the graph distance dL is an inconsistent approximation of dX, in the sense
that dL usually does not converge to dX when the sampling becomes infinitely dense. This
phenomenon is called metrication error, and the reason is that the graph induces a metric
inconsistent with dX (> Fig. -). While metrication errors make in general the use of dL
an approximation of dX disadvantageous, Bernstein–de Silva–Langford–Tenenbaum the-
orem [] states that under certain conditions the graph metric dL can be made as close as
desired to the geodesic metric dX. The theorem is formulated as a bound of the form
−λ≤dL
dX
≤+ λ,
(.)
where λ, λdepend on shape properties, sampling quality, and graph connectivity. In
order for dL to represent dX accurately, the sampling must be sufficiently dense, length
of edges in the graph bounded, and sufficiently close vertices must be connected, usually
in a non-regular manner.
..
Fast Marching
...
Eikonal Equation
An alternative to computation of a discrete metric on a discretized surface is the discretiza-
tion of the metric itself. The distance map d(x) = dX(x, x) (> Fig. -) on the manifold

Manifold Intrinsic Similarity 

⊡Fig. -
Shortest paths measured by Dijkstra’s algorithm (solid bold lines) do not converge to the true
shortest path (dashed diagonal), no matter how much the grid is reﬁned. Reproduced
from []
⊡Fig. -
Distance map measured on a curved surface. Equi-distant contours from the source located
at the right hand are shown. Reproduced from []


Manifold Intrinsic Similarity
can be associated with the time of arrival of a propagating front traveling with unit speed
(illustratively, imagine a fire starting at point xat time t = and propagating from the
source). Such a propagation obeys the Fermat principle of least action (the propagating
front chooses the quickest path to travel, which coincides with the definition of the geodesic
distance) and is governed by the eikonal equation
∥∇Xd∥= ,
(.)
where ∇X is the intrinsic gradient on the surface X. Eikonal equation is a hyperbolic PDE
with boundary conditions d(x) = ; minimum geodesics are its characteristics. Prop-
agation direction is the direction of the steepest increase of d and is perpendicular to
geodesics.
Since the distance map is not everywhere differentiable (in particular, at the source
point), no solution to the eikonal equation exists in the classical sense, while there exist
many non Cfunctions satisfying the equation and the boundary conditions. Among such
functions, the largest d satisfying the boundary conditions and the inequality
∥∇Xd∥≤
(.)
at every point where ∇Xd exists is called the viscosity solution []. The viscosity solution
of the eikonal equation always exists, is unique, and its value at a point x coincides with
dX(x, x). It is known to be monotonous, i.e., not having local maxima.
...
Triangular Meshes
A family of algorithms for finding the viscosity solution of the discretized eikonal equa-
tion by simulated wavefront propagation is called fast marching methods [, , ]. Fast
marching algorithms can be thought of as continuous variants of the Dijkstra algorithm,
with the notable difference that they consistently approximate the geodesic metric dX on
the surface.
Initialize d(x) = and mark xas processed; for all k ≠set d(xk) = ∞and mark xk
as unprocessed.
while there exist unprocessed vertices do
Mark unprocessed neighbors of processed vertices as interface.
for all interface vertices x and all incident triangles (x, x, x) with x, x≠unprocessed
do
Update d(x) from d(x) and d(x).
end for
Mark interface vertex with the smallest value of d as processed.
end while
The general structure of fast marching closely resembles that of Dijkstra’s algorithm
with the main difference lying in the update step. Unlike the graph case where shortest
paths are restricted to pass through the graph edges, the continuous approximation allows

Manifold Intrinsic Similarity 

d1
n
x1
d3
x3
d2
x2
x1
d1
x2
d2
d3
x3
n
⊡Fig. -
Fast marching updates the triangle (x,x,x) by estimating the planar wavefront direction n
and origin p based on dat xand dat x, and propagating it further to x. dhas two
possible solutions: the one shown on the left is inconsistent, since d< d, d. The solution
on the right is consistent, since d> d, d. Geometrically, in order to be consistent, the
update direction n has to form obtuse angles with the triangle edges (x, x) and (x, x).
Reproduced from []
paths passing anywhere in the simplicial complex. For that reason, the value of d(x) has
to be computed from the values of the distance map at two other vertices forming a trian-
gle with x. In order to guarantee consistency of the solution, all such triangles must have
an acute angle at x. Obtuse triangles are split at a preprocessing stage by adding virtual
connections to non-adjacent vertices.
Given a triangle (x, x, x) with known values of d(x) and d(x), the goal of the update
step is to compute d(x). The majority of fast marching algorithms do so by simulating the
propagation of a planar wavefront in the triangle. The wavefront arrival time to xand xis
set to d(x) and d(x), from which the parameters of the wave source are estimated. Gener-
ally, there exist two solutions for d(x) consistent with the input, the smallest corresponding
to the wavefront first arriving to x and then to xand x, and the largest corresponding
to the inverse situation. In order to guarantee monotonicity of the solution, the largest
solution is always chosen (> Fig. -).
Computationally, fast marching has the O(N log N) complexity of Dijkstra, perhaps
with a slightly larger constant.
...
Parametric Surfaces
For surfaces admitting a global parametrization x : U →R, the eikonal equation can be
expressed entirely in the parametrization domain as []
∇TdG−∇d = ,
(.)
where d(u) is the distance map in the parametrization domain, ∇d is its gradient with
respect to the standard basis in R, and G are the coefficients of the first fundamental form


Manifold Intrinsic Similarity
in parametrization coordinates. The fast marching update step can be therefore performed
on U. Moreover, since only G is involved in the equation, the knowledge of the actual
vertex coordinates is not required. This property is useful when the surface is reconstructed
from some indirect measurements, e.g., normals or gradients, as it allows to avoid surface
reconstruction for metric computation.
...
Parallel Marching
The main disadvantage of all Dijkstra-type algorithms based on a heap structure in general
and fast marching in particular is the fact that they are inherently sequential. Moreover, as
the order in which the vertices are visited is unknown in advance, they typically suffer
from inefficient access to memory. Working with well-structured parametric surfaces such
as geometry images allows to circumvent these disadvantages by replacing the heap-based
update by the regular raster scan update. Such family of algorithms is usually called parallel
marching [] or fast sweeping [, ].
In parallel marching, vertices of the geometry image are visited in a raster scan
order, and for each vertex the standard fast marching update is applied using already
updated (causal) vertices as the supporting vertices for the following update. Four raster
scans in alternating left-to-right top-to-bottom, right-to-left top-to-bottom, left-to-right
bottom-to-top, and right-to-left bottom-to-top directions are applied (in practice, it is
advantageous to rotate the scan directions by ○, as shown in > Fig. -). For a Euclidean
domain, such four scans are sufficient to consistently approximate the metric; for non-
Euclidean shapes, several repetitions of the four scans are required. The algorithm stops
when the distance map stops changing significantly from one repetition to another. The
exact number of repetitions required depends on the metric and the parametrization, but
is practically very small.
Parallel marching algorithms map well on modern vector and parallel architectures and
in particular on graphics hardware [].
...
Implicit Surfaces and Point Clouds
Two-dimensional manifolds represented in the implicit form X = {Φ(x) = } ⊂Rcan
be approximated with arbitrary precision as the union of Euclidean balls of radius h > 
around X,
Bh(X) = ⋃
x∈X
BR
h (x).
(.)
Bh(X) is a three-dimensional Euclidean sub-manifold, which for h < /max κhas a
smooth boundary. For every x, x′ ∈X, the shortest path in Bh(X) is no longer than
the corresponding shortest path on X. Mémoli and Sapiro [] showed that as h →,
shortest paths in Bh(X) converge to those on X and the corresponding geodesic dis-
tances dBh(X)∣X×X converge uniformly to dX. This result allows to cast the computation of a

Manifold Intrinsic Similarity 

⊡Fig. -
Raster scan grid traversals rotated by ○. Reproduced from []
distance map on a curved two-dimensional space as the computation of a distance map on
a three-dimensional Euclidean submanifold. The latter can be done using fast marching or
parallel marching on orthogonal grid restricted to a narrow band around X [].
A similar methodology can be used for the computation of distance maps on point
clouds []. The union of Euclidean balls centered at each vertex of the cloud create a
three-dimensional Euclidean manifold, on which the distance map is computed using fast
marching or parallel marching.
..
Diﬀusion Distance
The diffusion distance is expressed through the spectral decomposition of the Laplace–
Beltrami operator, and its discretization involves the discretization of the Laplace–Beltrami
operator and the computation of its eigenfunctions.


Manifold Intrinsic Similarity
...
Discretized Laplace–Beltrami Operator
A discrete approximation of the Laplace-Beltrami on the mesh ˆX has the following generic
form
(Δ ˆX f )i = 
ai ∑
j
wi j(fi −fj),
(.)
where f = (f, . . . , fN) is a scalar function defined on the mesh ˆX, wi j are weights, and ai
are normalization coefficients. In matrix notation,
> Eq. (.) can be written as
Δ ˆX f = A−L f ,
(.)
where A = diag(ai) and L = diag(∑l≠i wil ) −(wi j).
Different discretizations of the Laplace–Beltrami operator lead to different choice
of A and W. In general, it is common to distinguish between discrete and discretized
Laplace–Beltrami operator; the former being a combinatorial construction and the latter a
discretization trying to preserve some of the properties (Li)–(Liv) of the continuous coun-
terpart. In addition to these properties, it is important that the discrete Laplace–Beltrami
operator converges to the continuous one, in the sense that the solution of the continuous
heat equation with ΔX converges to the discrete solution of the discrete heat equation with
Δ ˆX as the number of samples grows to infinity.
Purely combinatorial approximations such as the umbrella operator (wi j = if xi and
xj are connected by an edge and zero otherwise) [] and the Tutte Laplacian (wi j = d−
i ,
where di is the valence of vertex xi) [] are not geometric, violate property (Liv), and do
not converge to the continuous Laplace–Beltrami operator. One of the most widely used
discretizations is the cotangent weight scheme [] and its variants [] (wi j = cot αi j +
cot βi j if xi and xj are connected, where αi j and βi j are the two angles opposite to the edge
between vertices xi and xj in the two triangles sharing the edge, and ai is proportional to
the sum of the areas of the triangles sharing xi). It preserves properties (Li)–(Liv) as well
as satisfies the convergence property under certain mild conditions [].
...
Computation of Eigenfunctions and Eigenvalues
By solving the generalized eigendecomposition problem []
AΦ = ΛLΦ,
where Φ is an N × (k + ) matrix whose columns are discretized eigenfunctions
ϕ,..., ϕk and Λ is the diagonal matrix of the corresponding eigenvalues λ,..., λk of the
discretized Laplace–Beltrami operator are computed. ϕil approximates the value of the lth
eigenfunction at the point xi.
A different approach to the computation of eigenfunction is based on the finite ele-
ments method (FEM). Using the Green formula, the Laplace–Beltrami eigenvalue problem

Manifold Intrinsic Similarity 

ΔXϕ = λϕ can be expressed in the weak form as
⟨ΔXϕ, α⟩L(X) = λ⟨ϕ, α⟩L(X)
(.)
for any smooth α. Given a finite basis {α, . . ., αK} spanning a subspace of L(X), the
solution ϕ can be expanded as ϕ(x) ≈uα(x)+⋅⋅⋅+uKαK(x). Substituting this expansion
into (> .) results in a system of equations
K
∑
j=
u j⟨ΔXαj, αk⟩L(X) = λ
K
∑
j=
u j⟨αj, αk⟩L(X), k = , . . . , K,
which, in turn, is posed as a generalized eigenvalue problem
Au = λBu.
(.)
(here A and B are K × K matrices with elements akj = ⟨ΔXαj, αk⟩L(X) and bkj =
⟨αj, αk⟩L(X)). Solution of (> .) gives eigenvalues λ and eigenfunctions ϕ = uα+
⋅⋅⋅+ uKαK of ΔX.
As the basis, linear, quadratic, or cubic polynomials defined on the mesh can be used.
Since the inner products are computed on the surface, the method is less sensitive to shape
discretization compared to the direct approach based on the discretization of the Laplace–
Beltrami operator. This is confirmed by numerical studies performed by Reuter et al. [].
...
Discretization of Diﬀusion Distances
Using the discretized eigenfunctions, a discrete diffusion kernel is approximated as
K(xi, xj) ≈
k
∑
l=
K(λl)ϕil ϕjl,
and can be represented as an N × N matrix. The corresponding diffusion distance is
approximated as
dX,t(xi, xj) ≈(
k
∑
l=
K(λi)(ϕil −ϕjl))
/
.
.
Invariant Shape Similarity
Let us denote by X the space of all shapes equipped with some metric, i.e., a point in X is a
metric space (X, dX). Let T be a group of shape transformations, i.e., a collection of oper-
ators τ : X →X with the function composition. Two shapes differing by a transformation
τ ∈T are said to be equivalent up to T . The equivalence relation induces the quotient space
X/T in which each point is an equivalence class of shapes that differ by a transformation
in T . A particular instance of T is the group of isometries, i.e., such transformations that
acting on X leave dX unchanged. The exact structure of such the isometry group depends


Manifold Intrinsic Similarity
on the the choice of the metric with which the shapes are equipped. For example, if the
Euclidean metric dX = dE∣X×X is used, the isometry group coincides with the group of
Euclidean congruences (rotations, translations, and reflections).
A function d : X × X →R that associates a pair of shapes with a non-negative scalar is
called a distance or dissimilarity function. We will say that the dissimilarity d is T -invariant
if it defines a metric on the quotient space X/T . In particular, this means that d(X, τ(X)) =
and d(τ(X)×σ(Y)) = d(X, Y) for every τ, σ ∈T and X, Y ∈X. In particular, for T being
the isometry group, a T -invariant dissimilarity is an isometry invariant metric between
shapes. The exact type of invariance depends on the structure of the isometry group and,
hence, again on the choice of the metric with which the shapes are equipped.
As a consequence from the metric axioms, an isometry invariant dissimilarity d(X, Y)
between two shapes X and Y equals zeros if and only if X and Y are isometric. However,
since exact isometry is an ideal rather than practical notion, it is desirable to extend this
property to similar (almost isometric) rather than strictly equivalent (isometric) shapes.
We will therefore require that (Ii) two є-isometric shapes X and Y satisfy d(X, Y) ≤cє+b;
and vice versa (Iii) if d(X, Y) ≤є, then X and Y are (cє+b)-isometric, where c, c, band
bare some non-negative constants. In what follows, we will focus on the construction of
such dissimilarities and their approximation, and show how different choices of the metric
yield different classes of invariance.
..
Rigid Similarity
Equipping shapes with the restriction of the Euclidean metric in E allows to consider them
as subsets of a bigger common metric space, E equipped with the standard Euclidean
metric. We will therefore examine dissimilarity functions allowing to compare between
two subsets of the same metric space.
...
HausdorﬀDistance
For two sets X and Y, a subset R ⊆X × Y is said to be a correspondence between X and Y
if () for every x ∈X there exists at least one y ∈Y such that (x, y) ∈R and () for every
y ∈Y there exists at least one x ∈X such that (x, y) ∈R. We will denote by R(X, Y) the
set of all possible correspondences between X and Y.
Using the notion of correspondence, we can define the Hausdorff distance [] between
the two subsets of some metric space (Z, dZ) as
dZ
H(X, Y) =
min
R∈R(X,Y) max
(x,y)∈R dZ(x, y).
(.)

Manifold Intrinsic Similarity 

In other words, Hausdorff distance is the smallest non-negative radius r for which Br(X) =
⋃x∈X Br(x) ⊆Y and Br(Y) ⊆X, i.e.,
dZ
H(X, Y) = max{max
x∈X min
y∈Y dZ(x, y),max
y∈Y min
x∈X dZ(x, y)}.
(.)
Hausdorff distance is a metric on the set of all compact non-empty sets in Z. However,
it is not isometry invariant, i.e., for a non-trivial τ ∈Iso(Z), generally dZ
H(X, τ(X)) ≠.
The isometry invariant Hausdorff metric is constructed as the minimum of dZ
H over all
isometries in Z,
dZ/Iso(Z)
H
(X, Y) =
min
τ∈Iso(Z)dZ
H(X, τ(Y)).
(.)
In the particular case of (Z, dZ) being (E, dE), the isometry invariant Hausdorff metric
can be used to quantify similarity between rigid shapes measuring to which extent they are
congruent (isometric in the Euclidean sense) to each other. The metric assumes the form
dE/Iso(E)
H
(X, Y) = min
R,t dE
H(X, RY + t),
(.)
where an orthogonal (rotation, or sometimes, rotation and reflection) matrix R and a
translation vector t ∈E are used to parametrize the Euclidean isometry group.
...
Iterative Closest Point Algorithms
Denoting by
cpY(x) = min
y∈Y dE(x, y)
(.)
the closest point to x in Y, the Euclidean isometry invariant Hausdorff metric can be
expressed as
dE/Iso(E)
H
(X, Y) = max {min
R,t max
x∈X dE(x, RY + t),min
R,t max
y∈Y dE(y, RX + t)}
= min
R,t max {max
x∈X ∥x −cpRY+t(x)∥,max
y∈Y ∥y −cpR−(X−t)(y)∥}. (.)
Such a formulation lends itself to numerical computation. A family of algorithms referred
to as iterative closest point (ICP) [, ] first established the closest point correspondences
between X and Y; once the correspondence is available, the Euclidean isometry (R, t)min-
imizing maxx∈X ∥x −cpRY+t(x)∥and maxy∈Y ∥y −cpR−(X−t)(y)∥is found and applied
to Y. This, however, is likely to change the correspondence, so the process is repeated until
convergence. For practical reasons, more robust variants of the Hausdorff distance are
used [].


Manifold Intrinsic Similarity
...
Shape Distributions
A disadvantage of the ICP algorithms is that the underlying optimization problem
becomes computationally intractable in high-dimensional spaces. A different approach for
isometry-invariant comparison of rigid shapes, proposed by Osada et al. [] and referred
to as shape distribution, compares the distributions (histograms) of distances defined on
the shape. Two isometric shapes obviously have identical shape distributions, which makes
the approach isometry-invariant. Shape distributions can be computed in a space of any
dimension, are computationally efficient, and not limited to a specific metric. A notable
disadvantage of shape distribution distance is that it does not satisfy our axioms (Ii)–(Iii),
as there may be two non-isometric shapes with equal shape distributions, therefore, it is
not a metric.
...
Wasserstein Distances
Let the sets X and Y be further equipped with measures μX and μY, respectively. (It is
required that supp(μX) = X and supp(μY) = Y.) We will say that a measure μ on
X × Y is a coupling of μX and μY if (i) μ(X′ × Y) = μX(X′) and (ii) μ(X × Y′) =
μY(Y′) for all Borel sets X′ ⊆X and Y′ ⊆Y. We will denote by M(μX, μY) the
set of all possible couplings of μX and μY. The support supp(μ) of the measure μ is
the minimum closed subset R ⊂X × Y such that μ(Rc) = . The support of each
μ ∈M(μX, μY) defines a correspondence; measure coupling can be therefore interpreted
as a “soft” or “fuzzy” correspondence between two sets.
The family of distances
dZ
W,p(μX, μY) =
min
μ∈M(μX,μY)(∫X×Y d p
Z(x, y)dμ(x, y))

p
(.)
for ≤p < ∞, and
dZ
W,∞(μX, μY) =
min
μ∈M(μX,μY)
max
(x,y)∈supp(μ)dZ(x, y)
(.)
for p = ∞is called the Wasserstein or Earth mover’s distances []. Wasserstein distances
are metrics on the space of distributions (finite measures) on Z. For convenience, we will
sometimes write dZ
W,p(X, Y) implying dZ
W,p(μX, μY).
Exactly like in the case of the Hausdorff distance, Wasserstein distances can be trans-
formed into isometry invariant metrics by considering the quotient with all isometries
of Z,
dZ/Iso(Z)
W,p
(X, Y) =
min
τ∈Iso(Z)dZ
W,p(X, τ(Y)).
(.)
Wasserstein distances are intimately related to Monge–Kantorovich optimal transporta-
tion problems. Informally, if the measures μX and μY are interpreted as two ways of piling

Manifold Intrinsic Similarity 

up a certain amount of dirt over the regions X and Y, respectively, and the cost of trans-
porting dirt from point x to point y is quantified by d p
Z(x, y), then the Wasserstein distance
dZ
W,p expresses the minimum cost of turning one pile into the other. On discrete domains,
the Wasserstein distance can be cast as an optimal assignment problem and solved using the
Hungarian algorithm or linear programming []. Several approximations have also been
proposed in [, ].
..
Canonical Forms
The Hausdorff distance allows comparing shapes equipped with the restricted Euclidean
metric, i.e., considered as subsets of the Euclidean space. If other metrics are used, we have
a more difficult problem of comparison of two different metric spaces. Elad and Kimmel
[, ] proposed an approximate solution to this problem, reducing it to the comparison
of Euclidean sub-spaces by means of minimum distortion embedding. Given a shape X with
some metric d (e.g., geodesic or diffusion), it can be represented as a subset of the Euclidean
space by means of an embedding φ : X →Rm. If the embedding is isometric (dE∣φ(X)×φ(X)○
φ = d), the Euclidean representation (φ(X), dE∣φ(X)×φ(X)) called the canonical form of X
can be used equivalently instead of (X, d) (> Fig. -). Given a Euclidean isometry i ∈E,
if φ is isometric, then φ○i is also isometric. In other words, the canonical form is defined up
to an isometry. In a more general setting, an arbitrary metric space (Z, dZ) is used instead
of E for the computation of the canonical form.
The advantage of using canonical forms is that it brings the problem of shape
comparison to the comparison of subsets of the Euclidean space, using, e.g., the
Hausdorff distance. Given two shapes (X, d) and (Y, δ), their canonical forms φ(X)
and ψ(Y) in Z are computed. In order to compensate for ambiguity in the definition
⊡Fig. -
Nearly-isometric deformations of a shape (top row) and their canonical forms in R(bottom
row)


Manifold Intrinsic Similarity
of the canonical forms, an isometry-invariance distance between subsets of Z must be
used, e.g., dZ/Iso(Z)
H
(φ(X),ψ(Y)). In the particular case of Euclidean canonical forms,
dE/Iso(E)
H
(φ(X),ψ(Y)) can be computed using ICP.
...
Multidimensional Scaling
Unfortunately, in most cases there exists no isometric embedding of X into some pre-
defined metric space. The right choice of Z can decrease the embedding distortion, but
not make it zero [, ]. Instead, one can find an embedding with minimal distortion,
min
φ:(X,d)→(Z,dZ)dis(φ).
In this case, dZ∣φ(X)×φ(X) ○φ ≈d, and thus the canonical form is only an approximate
representation of the shape X in the space Z.
In the discrete setting and Z = Rm, given the discretized shape {x,..., xN} with the
discretized metric di j = d(xi, xj), the minimum-distortion embedding can be computed
by solving the multidimensional scaling (MDS) problem [, ],
min
{z,...,zN }⊂Rm
max
i,j=,...,N ∣di j −∥zi −z j∥∣,
(.)
where zi = φ(xi).
In practical applications, other norms (e.g., L) are used in the MDS problem (> .).
The MDS objective function is usually referred to as stress in MDS literature. For the L
MDS problem (also known as least squares or LS-MDS), an efficient algorithm based on
iterative majorization (commonly referred to as scaling by majorizing a complicated function
or SMACOF) exists []. Denoting by Z the N ×m matrix of the embedding coordinates of
{x,⋯, xN} in Rm, the SMACOF algorithm can be summarized as follows:
Initialize embedding coordinate Z().
for k = ,,⋯do
Perform multiplicative update
Z(k) = 
N B(Z(k−))Z(k−),
where B(Z) is an N × N matrix-valued function with elements
bi j(Z) =
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
dX(xi,x j)
∥zi−z j∥
i ≠j and ∥zi −z j∥≠,

i ≠j and ∥zi −z j∥= ,
−∑k≠i bik
i = j.
end for
SMACOF iteration is equivalent to a weighted steepest descent with constant step size
[], but due to a special structure of the problem, it guarantees monotonous decrease of the

Manifold Intrinsic Similarity 

stress function []. Other Lp formulations can be solved using iteratively reweighted least-
squares (IRLS) techniques []. Acceleration of convergence is possible using multiscale
and multigrid methods [] as well as vector extrapolation techniques [].
...
Eigenmaps
In the specific case when the shape is equipped with the diffusion distance (d = dX,t),
the canonical form can be computed by observing the fact that the map ΦX,t(x) =
(e−λtϕ(x), e−λtϕ(x), . . .) defined by the eigenvalues and eigenvectors of Laplace–
Beltrami operator ΔX satisfies dX,t(x, x′) = ∥Φt(x) −Φt(x′)∥. In other words, ΦX,t
is an isometric embedding of (X, dX,t) into an infinite dimensional Euclidean space, and
can be thought of as an infinite-dimensional canonical form [, ]. ΦX,t is termed
Laplacian eigenmap [] or diffusion map []. Another eigenmap given by ΨX(x) =
(λ−/

ϕ(x), λ−/

ϕ(x), . . .), referred to as the global point signature (GPS) [], is an
isometric embedding of the commute time metric cX.
Unlike Elad–Kimmel canonical forms computed by MDS, the eigenmap is uniquely
defined (i.e., there are no degrees of freedom related to the isometry in the embedding
space) if the Laplace–Beltrami operator has no eigenvalues of multiplicity greater than one.
Otherwise, the ambiguity in the definition of the eigenmap is up to switching between
the eigenfunction corresponding to the eigenvalues with multiplicity and changes in their
signs. More ambiguities arise in cases of symmetric shapes []. In general, two eigenmaps
may differ by a permutation of coordinates corresponding to simple eigenvalues, or by a
roto-reflection in the eigensubspaces corresponding to eigenvalues with multiplicities.
For practical comparison of eigenmaps, a finite number k of eigenvectors is used, ˜ΦX,t =
(e−λtϕ, . . . , e−λk tϕk). The Euclidean distance on the eigenmap ˜ΦX,t is thus a numerical
approximation to the diffusion metric dX,t using k eigenfunctions of the Laplace–Beltrami
operator (similarly, ˜ΨX approximates the commute time). For small k, eigenmaps can be
compared using ICP. The problem of coordinate permutations must be addressed if eigen-
values of multiplicity greater than one are present. Such an approach is impractical for
k >> .
As an alternative, Rustamov [] proposed using shape distributions to compare
eigenmaps. This method overcomes the aforementioned problem, but lacks the metric
properties of a true isometry-invariant metric.
..
Gromov–HausdorﬀDistance
The source of inaccuracy of Elad–Kimmel canonical forms is that it is generally impossi-
ble to select a common metric space (Z, dZ) in which the geometry of any shape can be
accurately represented. However, for given two shapes X and Y, the space (Z, dZ) can be
selected in such a way that (X, d) and (Y, δ) can be isometrically embedded into it, the
simplest example being the disjoint union Z = X⊔Y of X and Y, with the metric dZ∣X×X = d


Manifold Intrinsic Similarity
and dZ∣Y×Y = δ. dZ∣X×Y is defined to minimize the Hausdorff distance between X and Y
in (Z, dZ), resulting in a distance,
dGH(X, Y) = inf
dZ dZ
H(X, Y),
(.)
called the Gromov–Hausdorff distance. The Gromov–Hausdorff distance was first proposed
by Gromov [] as a distance between metric spaces and a generalization of the Hausdorff
distance and brought into shape recognition by Mémoli and Sapiro [].
The Gromov–Hausdorff distance satisfies axioms (Ii)–(Iii) with c= c= and b=
b= , such that dGH(X, Y) = if and only if X and Y are isometric. More generally, if
dGH(X, Y) ≤є, then X and Y are є-isometric and conversely, if X and Y are є-isometric,
then dGH(X, Y) ≤є [].
The Gromov–Hausdorff distance is a generic distance between metric spaces, and in
particular, can be used to measure similarity between subsets of the Euclidean metric space,
(X, dE∣X×X) and (Y, dE∣Y×Y). In [] Mémoli showed that the Gromov–Hausdorff distance
in the Euclidean space is equivalent to the ICP distance,
c ⋅dE/Iso(E)
H
(X, Y) ≤dGH((X, dE∣X×X),(Y, dE∣Y×Y)) ≤dE/Iso(E)
H
(X, Y),
in the sense of equivalence of metrics (c > is a constant). (Metric equivalence should
not be confused with equality: for example, Land Lmetrics are equivalent but not
equal.) Consequently, () if dE/Iso(E)
H
(X, Y) ≤є, then (X, dE∣X×X) and (Y, dE∣Y×Y) are є-
isometric; and () if (X, dE∣X×X) and (Y, dE∣Y×Y) are є-isometric, then dE/Iso(E)
H
(X, Y) ≤
c√є.
Using the Gromov–Hausdorff distance to compare shapes equipped with diffusion
metric allows to benefit from the advantage of the diffusion metric over geodesic one, such
as lesser sensitivity to topological noise [].
...
Generalized Multidimensional Scaling
For compact metric spaces, the Gromov–Hausdorff distance can also be expressed as
dGH(X, Y) = 
inf
C dis (C),
(.)
where the infimum is taken over all correspondence C and dis(C). The two expres-
sions (> .) and (> .) are equivalent [].
The advantage of this formulation is that it allows to reduce the computation of the
Gromov–Hausdorff distance to finding a minimum-distortion embedding, similarly to the
computation of canonical forms by means of MDS. In the discrete setting, given two tri-
angular meshes ˆX and ˆY representing the shapes X, Y, let us fix two sufficiently dense
finite samplings P = {p,..., pm} and Q = {q,..., qn} of ˆX and ˆY, respectively. A dis-
crete correspondence between the shapes is defined as C = (P × Q′) ∪(Q × P′), where
P′ = {p′
,..., p′
n} and Q′ = {q′
,..., q′
m}are some (different) sets of samples on ˆX and ˆY

Manifold Intrinsic Similarity 

corresponding to Q and P, respectively. One can represent C as the union of the graphs of
two discrete functions φ : P →ˆY and ψ : Q →ˆX, parametrizing the class of all discrete
correspondences.
Given two sets P and P′ on ˆX, we can construct an m × n distance matrix D(P, P′),
whose elements are the distances ˆd ˆX (pi, p′
j) (e.g., geodesic or diffusion). In these terms,
the distortion of the correspondence C can be written as
dis (C) = ∥( D(P, P)
D(P, P′)
D(P, P′)T
D(P′, P′)) −(D(Q′, Q′)
D(Q′, Q)
D(Q′, Q)T
D(Q, Q))∥,
where ∥⋅∥is some norm on the space of (m + n) × (m + n) matrices. The selection of
the infinity norm ∥D∥∞= maxi,j ∣di j∣is consistent with the Gromov-Hausdorff distance;
however, in practice more robust norms like the Frobenius norm ∥D∥
F = trace(DDT) are
often preferable (see [, , ] for discussions on the regularization of the infinity norm
in the Gromov–Hausdorff framework by other lp norms).
The discretization of dis(C) leads directly to a discretized approximation of the
Gromov–Hausdorff distance between shapes, which can be expressed as
ˆdGH( ˆX, ˆY) := 
min
P′,Q′ dis (C).
Note that only P′ and Q′ participate as continuous minimization variables, while P and Q
are constants (given samples on the respective shapes). The above minimization problem
is solved using a numerical procedure resembling MDS, first introduced in [, ] under
the name generalized MDS (GMDS).
We use barycentric coordinates to represent points on ˆX and ˆY. In these coordinates,
a point pi lying in a triangle ti on ˆX is represented as a convex combination of the triangle
vertices (corresponding to the indices t
i, t
i , and t
i ) with the weights ui = (u
i,u
i ,u
i )
T.
We will denote by T = (t, . . . , tm)T the vector of triangle indices and by U = (u, . . . ,um)
the × m matrix of coordinates corresponding to the sampling P. Similarly, the samplings
P′, Q, and Q′ are represented as (T′,U′), (S, V), and (S′, V ′). For the sake of notation
simplicity, we are going to use these interchangeably.
It was shown in [] that a first-order approximation of a geodesic distance between p′
i
and p′
j on ˆX can be expressed as the quadratic form
Di j(P′, P′) ≈u′T
i
⎛
⎜⎜⎜
⎝
Dt
i,t
j(P, P)
Dt
i,t
j (P, P)
Dt
i,t
j (P, P)
Dt
i ,t
j(P, P)
Dt
i ,t
j (P, P)
Dt
i ,t
j (P, P)
Dt
i ,t
j(P, P)
Dt
i ,t
j (P, P)
Dt
i ,t
j(P, P)
⎞
⎟⎟⎟
⎠
u′
j.
Other distance terms are expressed similarly. Using tensor notation, we can write
dis(C) ≈∥(U,U′)D ˆX(T, T′)(U,U′) −(V, V ′)D ˆY(S, S′)(V, V ′)∥

F ,
where D ˆX(T, T′) is a rank four tensor whose ij-th elements are defined as the ×distance
matrices above, and D ˆY(S, S′) is defined in a similar way.


Manifold Intrinsic Similarity
The resulting objective function dis (C) is a fourth-order polynomial with respect to the
continuous coordinates U′, V′, also depending on the discrete index variables T′ and S′.
However, when all indices and all coordinate vectors except one, say, u′
i, are fixed, the
function becomes convex and quadratic with respect to u′
i. A closed-form minimizer
of dis (u′
i) is found under the constraints u′
i ≥and u′
i + u′
i + u′
i
= , guarantee-
ing that the point p′
i remains within the triangle t′
i. The GMDS minimization algorithm
proceeds iteratively by selecting u′
i or v′
i corresponding to the largest gradient of the
objective function, updating it according to the closed-form minimizer, and updating
the corresponding triangle index to a neighboring one in case the solution is found on
the boundary of the triangle. The reader is referred to [] for further implementation
details.
..
Graph-Based Methods
The minimum-distortion correspondence problem can be formulated as a binary labeling
problem with uniqueness constraints [] in a graph with vertices defined as pairs of points
and edges defined as quadruplets. Let V = {(x, y) : x ∈X, y ∈Y} = X × Y be the set of
pairs of points from X and Y, and let E = {((x, y),(x′, y′)) ∈V ×V and (x, y) ≠(x′, y′)}.
A correspondence C ⊂X ×Y can be represented as binary labeling u ∈{,}V of the graph
(V,E), as follows: ux,y = iff (x, y) ∈C and otherwise. When using Ldistortions, the
correspondence problem can be reformulated as
min
u∈{,}V
∑
((x,y),(x′,y′))∈E
ux,yux′,y′∣dX(x, x′) −dY(y, y′)∣
s.t.∑
y
ux,y ≤∀x ∈X;
∑
x
ux,y ≤∀y ∈Y.
(.)
In general, optimization of this energy is NP-hard []. One possible approxima-
tion of (> .) is by relaxing the labels to be in [,]. This formulation leads to a
non-convex quadratic program with linear constraints [, ]. Alternatively, instead of
minimizing directly the energy (> .), it is possible to maximize a lower bound on it
by solving the dual to the linear programming (LP) relaxation of (> .), a technique
known as dual decomposition []. This approaches demonstrate good global convergence
behavior [].
...
Probabilistic Gromov–HausdorﬀDistance
The Gromov–Hausdorff framework can be extended to a setting in which pairwise
distances are replaced by distributions of distances, modeling the intra-class variability
shapes (e.g., the fact that different humans have legs of different length) []. The pairwise
metric difference terms in the correspondence distortion are replaced by probabilities, and
the problem is posed as likelihood maximization.

Manifold Intrinsic Similarity 

..
Gromov–Wasserstein Distances
Same way as the Gromov–Hausdorff extends the Hausdorff distance by taking a minimum
over all possible metric spaces, dGH = mindZ dZ
H, an extension for the Wasserstein distance
of the form
dGW,p(X, Y) = min
dZ dZ
W,p(X, Y)
(.)
= min
dZ
min
μ∈M(μX,μY)(∫X×Y d p
Z(x, y)dμ(x, y))

p
,
referred to as Gromov–Wasserstein distance, was proposed by Mémoli []. Here, it is
assumed that X and Y are metric measure spaces with metrics dX , dY and measures μX, μY.
The analogy between the Gromov–Hausdorff and the Gromov–Wasserstein distances is
very close: the Hausdorff distance is a distance between subsets of a metric measure space,
and the Gromov–Hausdorff distance is a distance between metric spaces. The Wasserstein
distance is a distance between subsets of a metric space, and the Gromov–Wasserstein
distance is a distance between metric measure spaces.
...
Numerical Computation
In [], Mémoli showed that (> .) can be alternatively formulated as
dGW,p(X, Y) =
(.)
min
μ∈M(μX,μY)(∫X×Y ∫X×Y ∣dX(x, x′) −dY(y, y′)∣pdμ(x, y)dμ(x′, y′))

p
.
This formulation has an advantage in numerical implementation. Given discrete surfaces
{x,..., xN} and {y,..., yM} with discretized metrics dX(xi, xi′), dY(y j, y j′) and measures
μX(xi), μY(y j) (for i, i′ = ,..., N and j = ,..., M), problem (> .) can be posed as an
optimization problem with NM variables and N + M linear constraints:
min
μ
N
∑
i,i′=
M
∑
j,j′=
μi jμi′ j′∣dX(xi, xi′) −dY(y j, y j′)∣p
s.t. μi j ∈[,]
N
∑
i=
μi j = μY(y j)
M
∑
j=
μi j = μX(xi).


Manifold Intrinsic Similarity
..
Shape DNA
Reuter et al. [] proposed using the Laplace–Beltrami spectrum (i.e., eigenvalues λ, λ,...
of ΔX) as shape descriptors, referred to as shape DNA. Laplace–Beltrami spectrum is
isometry-invariant; however, there may exist shapes which are isospectral (have equal
eigenvalues) but non-isometric. This fact was first conjectured by Kac [] and shown by
example in []. Thus, the equivalence class of isospectral shapes to which the shape DNA
approach is invariant is wider than the class of isometries. The exact relations between these
classes are currently unknown.
.
Partial Similarity
In many situations, it happens that, while two objects are not similar, some of their
parts are. Such a situation is common, for example, in the face recognition application,
where the quality of facial images (or surfaces in the case of D face recognition) can
be degraded by acquisition imperfections, occlusions, and the presence of facial hair.
Semantically, we can say that two objects are partially similar if they have significant
similar parts. If one is able to detect such parts, the degree of partial similarity can be
evaluated.
We define a part of a shape (X, dX) simply as its subset X′ ⊂X equipped with the
restricted metric dX∣X′×X′. According to this definition, every part of a shape is also a shape.
We will denote by Σ(X) ⊂X the set of all admissible parts, satisfying () Σ(X) is non-
empty; () Σ(X) is closed under complement, i.e., if X′ ∈Σ(X), then X ∖X′ ∈Σ(X); and
() Σ(X) is closed under countable unions, i.e., any countable union of parts from Σ(X)
is also an admissible part in Σ(X). Formally, the set of all parts of X is a σ-algebra. An
equivalent representation of a part is by means of a binary indicator function, p : X →
{,}, assuming the value of one for each x ∈X′ and zero otherwise. We will see the utility
of such a definition in the sequel.
..
Signiﬁcance
The significance of a part is a function on Σ(X) assigning each part a number quantifying
its “importance.” We denote significance by σ and demand that () σ is non-negative; ()
σ(/) = ; and () σ is countably additive, i.e., σ (⋃i X′
i) = ∑i σ (X′
i) for every countable
union of parts in Σ(X). Formally, significance is a finite measure on X. As in the case of
similarity, the notion of significance is application-dependent. The most straightforward
way to define significance is by identifying it with the area
σ(X′) = ∫X′ da

Manifold Intrinsic Similarity 

or the normalized area
σ(X′) =
∫X′ da
∫X da
.
of the part. However, such a definition might deem equally important a large flat region
and a region rich in features if they have the same area, while it is clear that the latter
one would usually be more informative. A better approach is to interpret significance as
the amount of information about the entire shape contained in its part, quantified, e.g.,
as the ability to discriminate the shape from a given corpus of other shapes given only its
part. Such a definition leads to a weighted area measure, where the weighting reflects the
discriminativity density of each point and is constructed similarly to the term frequency-
inverse document frequency (TF-IDF) weighting commonly used in text retrieval [].
..
Regularity
Another quantity characterizing the importance of a part is its regularity, which we model
as a scalar function ρ : Σ(X) →R [, ]. In general, we would like the part to be sim-
ple, i.e., if two parts contain the same amount of information (are equally significant), we
would prefer the simpler one, following Ockham’s pluralitas non est ponenda sine necessitate
principle. What is exactly meant by “regular” and “simple” is again application-dependent.
In many applications, an acceptable definition of regularity is the deviation of a shape
from some perfectly regular one. For example, in image processing and computer vision,
regularity is commonly expressed using the shape factor
ρ(X′) =
π∫X′ da
(∫∂X′ ds)
,
or the ratio between the area of X′ and the squared length of its boundary. Because of the
isoperimetric inequality in the plane, this ratio is always less or equal to one, with the equal-
ity achieved by a circle, which is arguably a very regular shape. Shape factor can be readily
extended to non-Euclidean shapes, where, however, there is no straightforward analogy of
the isoperimetric inequality. Consequently, two equally regular shapes might have com-
pletely different topology, e.g., one might have numerous disconnected components while
the other having only one (> Fig. -).
A remedy can be in regarding regularity as a purely topological property, counting for
example the number of disconnected components of a part. Topological regularity can
be expressed in terms of the Euler characteristic, which using the Gauss–Bonnet identity
becomes
ρ(X′) = πχ(X′) = ∫X′ Kda + ∫∂X′ kgds,
where K is the Gaussian curvature of X and kg is the geodesic curvature of ∂X′.


Manifold Intrinsic Similarity
⊡Fig. -
Large shape factor does not necessarily imply regularity in non-Euclidean shapes. Here, the
upper body of the dog and the four legs have the same area and the same boundary length
(red contours) and, hence, the same shape factor. However, the upper body is arguably more
regular than the four disconnected legs. Reproduced from []
..
Partial Similarity Criterion
In this terminology, the problem of partial similarity of two shapes X and Y can be thought
finding two parts X′ ∈Σ(X) and Y′ ∈Σ(Y) simultaneously maximizing regularity,
significance, and similarity. Since a part of a shape is also a shape, the latter can be quanti-
fied using any shape similarity (Since we use dissimilarity, we will maximize −d(X′, Y′).)
criterion appropriate for the application, e.g., the Gromov–Hausdorff distance. This can be
written as the following multi-criterion optimization problem [, , ]
max
X′∈Σ(X)
Y′∈Σ(Y)
(ρ(X′), ρ(Y′), σ(X′), σ(Y′),−d(X′, Y′)),
where maximum is understood as a point in the criterion space, such that no other point
has all the criteria larger simultaneously. Such a maximum is said to be Pareto-efficient and
is not unique. The solution of this multi-criterion maximization problem can be interpreted
as a set-valued partial similarity criterion. Since such criteria are not mutually comparable,
the problem should be converted into a scalar maximization problem
max
X′∈Σ(X)
Y′∈Σ(Y)
λr(ρ(X′) + ρ(Y′)) + λs(σ(X′) + σ(Y′)) −d(X′, Y′),
(.)

Manifold Intrinsic Similarity 

where λr and λs are positive scalars reflecting the tradeoff between regularity, significance,
and dissimilarity.
..
Computational Considerations
Direct solution of problem (> .) involves searching over the space of all parts of X
and Y, which has combinatorial complexity. However, the problem can be relaxed to max-
imization in continuous variables if binary parts are allowed to be fuzzy. Formally, a fuzzy
part is obtained by letting the binary indicator functions assume values on the interval
[,]. Such functions are called membership functions in the fuzzy set theory terminology.
The optimization problem becomes []
max
p:X→[,]
q:Y→[,]
λr(ρ(p) + ρ(q)) + λs(σ(p) + σ(q)) −d(p, q),
where ρ(p), σ(p) and d(p, q) are the fuzzy counterparts of the regularity, significance, and
dissimilarity terms. The significance of a fuzzy part p is simply
σ(p) = ∫X p dσ.
The regularity term is somewhat more involved as it involves integration along the part
boundary, which does not exist in case of a fuzzy part. However, the following relaxation
is available []
ρ(p) =
π∫X p da
(∫X ∥∇p∥δ (p −
) da)
,
with δ being the Dirac delta function. This fuzzy version of the shape factor converges to
the original definition when p approaches a binary indicator function. The dissimilarity
term needs to be modified to involve the membership function. The most straightforward
way to do so is by defining a weighted dissimilarity between the entire shapes X and Y
with p and q serving as the weights. For example, using p(x)da(x) and q(y)da(y) as the
respective measures on X and Y, the Wasserstein distance incorporates the weights in a
natural way.
.
Self-Similarity and Symmetry
An important particular case of shape similarity is the similarity of shape with itself, which
is commonly referred to as symmetry. The latter notion is intimately related with that of
invariance.


Manifold Intrinsic Similarity
..
Rigid Symmetry
Computation of exact and approximate symmetries has been extensively studied in the
Euclidean sense [, , , ]. A shape X is said to be symmetric if there exists a non-trivial
Euclidean isometry f ∈Iso(R) to which it is invariant, i.e., f (X) = X. Such an isometry is
called a symmetry of X. True symmetries, like true isometries, are a mere idealization not
existing in practice. In real applications, we might still find approximate symmetries. The
degree of asymmetry of a Euclidean isometry f can be quantified as a distance between X
and f (X) in R, e.g.,
asym(f ) = dR
H (X, f (X)).
..
Intrinsic Symmetry
A symmetry f restricted to X defines a self-isometry of X, i.e., f ∣X ∈Iso(X). Therefore,
an alternative definition of an approximate symmetry could be an є-isometry, with the
distortion quantifying the degree of asymmetry. Such a definition requires approximate
symmetries to be automorphisms of X, yet its main advantage is the fact that it can be
extended beyond the Euclidean case (> Fig. -). In fact, identifying the symmetry group
with the isometry group Iso(X, dX) of the shape X with some intrinsic (e.g., geodesic
or diffusion) metric dX, a non-rigid equivalent of symmetries is defined, while setting
dX = dE∣X×X the standard Euclidean symmetries are obtained []. Approximate symme-
tries with respect to any metric can be computed as local minima of the distortion function
in embedding X into itself. Computationally, the process can be carried out using GMDS.
Extrinsic symmetry
dE = dE o (g × g)
Intrinsic symmetry
dX = dX o (g × g)
g
g
⊡Fig. -
Symmetry deﬁned as a metric-preserving automorphism (self-isometry) of X allows
extending the standard notion of Euclidean symmetry to non-rigid shapes. Reproduced
from []

Manifold Intrinsic Similarity 

..
Spectral Symmetry
An alternative to this potentially heavy computation is due to Ovsjanikov et al. [],
and is based on the elegant observation that for any simple (A simple eigenfunction
is one corresponding an eigenvalues with multiplicity one.) eigenfunction ϕi of the
Laplace–Beltrami operator, a reflection symmetry f satisfies ϕi ○f = ±ϕi. This allows
parametrize reflection symmetries by sign sequences s = {s, s, . . .}, si ∈{±}, such that
ϕi ○f = siϕi.
Given a sign sequence, the eigenmap Φs(x) = {sλ−/

ϕ(x), sλ−/

ϕ(x), . . . } is
defined. Symmetries are detected by evaluating the asymmetry
asym(s) = max
x∈X min
x′∈X ∥Φs(x′) −Φ(x)∥
of different sign sequences, and keeping those having asym ≤є. The symmetry itself
corresponding to a sequence s is recovered as
f (x) = arg min
x′∈X ∥Φs(x′) −Φ(x)∥,
and is anє self-isometry of X in the sense of the commute time metric. While it can be made
relatively computationally simple, this method is limited to global reflection symmetries
only.
..
Partial Symmetry
In many cases, a shape does not have symmetries as a whole, yet possesses parts that
are symmetric. Adopting the notion of partial similarity defined in > Sect. ., one can
think of a part X′ ⊂X and a partial symmetry f : X′ × X′ as of a Pareto-efficient trade-
off between asymmetry asym(f ), part significance σ(X′), and regularity ρ(X′). Partial
symmetries are found similarly to the computation of partial similarity of two distinct
shapes.
..
Repeating Structure
Another important particular case of self-similarity is repeating regular structure. Shapes
possessing regular structure can be divided into self-similar patches (structural elements)
forming some regular patterns, e.g., a grid. State-of-the-art methods [?, , ] can detect
structured repetitions in extrinsic geometry if the Euclidean transformations between
repeated patches exhibit group-like behavior. In case of non-rigid and deformable shapes,
however, the problem is challenging since no apparent structure is visible to simple
Euclidean probes in the absence of repetitive Euclidean transformations to describe the
shape. A general solution for the detection of intrinsic regular structure is still missing,
though particular cases have been recently addressed in [].


Manifold Intrinsic Similarity
.
Feature-Based Methods
Another class of methods, referred to as feature-based, uses local information to describe
the shape, perform matching, or compute similarity. The popularity of these methods has
increased following the success of the scale-invariant feature transform (SIFT) [] and
similar algorithms [, ] in image analysis and computer vision application.
..
Feature Descriptors
In essence, feature-based methods try to represent the shape as a collection of local
feature descriptors. This is typically done in two steps first, selecting robust and representa-
tive points ( feature detection), and computing the local shape representation at these points
( feature description).
...
Feature Detection
One of the main requirements on a feature detector is that the points it selects are ()
repeatable, i.e., in two instances of a shape, ideally the same set of corresponding points
is detected; and () informative, i.e., the information contained in these points is sufficient
to, e.g., distinguish the shape from others.
In the most trivial case, no feature detection is performed and the feature descriptor
is computed at all the points of the shape or at some regularly sampled subset thereof.
The descriptor in this case is usually termed dense. Dense descriptors bypass the problem
of repeatability at the price of increased computational cost and potentially introducing
many unimportant points that clutter the shape representation.
Many geometric feature detection paradigms come from the image analysis commu-
nity, such as finding points with high derivatives (e.g., the Harris operator [, , ]) or
local maxima in a scale-space (e.g., difference of Gaussians (DOG) [] or local maxima
of the heat kernel []).
...
Feature Description
Given a set of feature points (or, in the case of a dense descriptor, all the points on the
shape), a local descriptor is then computed. An ideal feature descriptor should be () invari-
ant under the class of transformations a shape can undergo and () informative. One of the
most known feature descriptors is spin image [, , ], describing the neighborhood of a
point by fitting an oriented coordinate system at the point. Belongie and Malik introduced
the shape context descriptor [], describing the structure of the shape as relations between
a point to the rest of the point. Given the coordinates of a point x on the shape, the shape
context descriptor is constructed as a histogram of the direction vectors from x to the rest

Manifold Intrinsic Similarity 

of the point, y −x. Typically, a log-polar histogram is used. Because of dependence on the
embedding coordinates, such a descriptor is not deformation-invariant. Other descriptors
exist based on local patches [], local moments [] and volume descriptors [], spherical
harmonics [], and contour and edge structures [, ]. Zaharescu et al. [] proposed
using as a local descriptor the histogram of gradients of a function (e.g., Gaussian curva-
ture) defined in a neighborhood of a point, similarly to the histogram of gradients (HOG)
[] and SIFT [] techniques used in computer vision.
Because considering local geometry, feature descriptors are usually not very susceptible
to non-rigid deformations of the shape. Nevertheless, there exist several geometric descrip-
tors which are invariant to isometric deformations by construction. Examples include
descriptors based on histograms of local geodesic distances [?, ], conformal factors [],
and heat kernels [], described in the following in more details.
...
Heat Kernel Signatures
Sun et al. [] proposed the heat kernel signature (HKS), defined as the diagonal of the heat
kernel. Given some fixed time values t, . . . , tn, for each point x on the shape, the HKS is
an n-dimensional descriptor vector
p(x) = (Kt(x, x), . . . , Ktn(x, x)).
(.)
The HKS descriptor is deformation-invariant, captures local geometric information at mul-
tiple scales, insensitive to topological noise, informative (if the Laplace–Beltrami operator
of a shape is non-degenerate, then any continuous map that preserves the HKS at every
point must be an isometry), and is easily computed across different shape representations
solving the eigenproblem described in > Sect. ...
...
Scale-Invariant Heat Kernel Signatures
A disadvantage of the HKS is its dependence on the global scale of the shape. If X is glob-
ally scaled by β, the corresponding HKS is β−Kβ−t(x, x). In some cases, it is possible
to remove this dependence by global normalization of the shape. A scale-invariant HKS
(SI-HKS) based on local normalization was proposed in []. By using a logarithmic scale-
space t = ατ, the scaling of X by β results in HKS amplitude scaling and shift by logα β.
This effect is undone by the following sequence of transformations,
pdi f (x) = (log Kατ(x, x) −log Kατ(x, x), . . . ,log Kατm (x, x) −log Kατm−(x, x)),
ˆp(x) = ∣(F pdi f (x))(ω, . . . , ωn)∣,
(.)
where F is the discrete Fourier transform, and ω, . . . , ωn denotes a set of frequencies at
which the transformed vector is sampled. Taking differences of logarithms removes the
scaling constant, and the Fourier transform converts the scale-space shift into a complex
phase, which is removed by taking the absolute value.


Manifold Intrinsic Similarity
..
Bags of Features
One of the notable advantages of feature-based approaches is the possibility of representing
a shape as a collection of primitive elements (“geometric words”), and using the well-
developed methods from text search such as the bag of features (BOF) (or bag of words)
paradigm [, ]. Such approaches are widely used in image retrieval, and have been
introduced more recently to shape analysis [, ]. The bag of features representation
is usually compact, easy to store and compare, which makes such approaches suitable for
large-scale shape retrieval.
The construction of a bag of features is usually performed in a few steps, depicted in
> Fig. -. First, the shape is represented as a collection of local feature descriptors
(either dense or computed at a set of stable points following an optional stage of feature
detection). Second, the descriptors are represented by geometric words from a geometric
vocabulary using vector quantization. The geometric vocabulary is a set of representative
descriptors, precomputed in advance. This way, each descriptor is replaced by the index of
the closest geometric word in the vocabulary. Computing the histogram of the frequency of
occurrence of geometric words gives the bag of features. Alternatively, a two-dimensional
histogram of co-occurrences of pairs of geometric words (geometric expressions) can be
used []. Shape similarity is computed as a distance between the corresponding bags of
features.
..
Combining Global and Local Information
Another use of local descriptors is in combination with global (metric) information, in
an extension of the Gromov–Hausdorff framework. Given two shapes X, Y with metrics
dX, dY and descriptors pX, pY, the quality of correspondence C ⊆X ×Y is measured using
global geometric distortion as well as local matching of descriptors,
dis(C) =
sup
(x,y),(x′,y′)∈C
∣dX(x, x′) −dY(y, y′)∣+ β sup
(x,y)∈C
∥pX(x) −pY(y)∥,
Feature
detection
Feature
description
Vector
quantization
Bag of
words
Bag of
expressions
⊡Fig. -
Feature-based shape analysis algorithm. Reproduced from []

Manifold Intrinsic Similarity 

where β > is some parameter. This L∞formulation can be replaced by a more robust
Lversion. As the descriptors, texture [, ] or geometric information [, ] can be
used.
The minimum-distortion correspondence can be found by an extension of the GMDS
algorithm described in > Sect. ...[] or graph labeling [, , ] described
in > Sect. ... The probabilistic extension of the Gromov-Hausdorff distance can be
applied to this formulation as well [].
.
Concluding Remarks
In this chapter, the problem of invariant shape similarity was presented through the prism
of metric geometry. It was shown that by representing shapes as metric spaces allows to
reduce the similarity problem to isometry-invariant comparison of metric spaces. The
particular choice of the metric results in different isometry groups and, hence, different
invariance classes. The construction of Euclidean, geodesic, and diffusion metrics were
presented, and their theoretical properties were highlighted in > Sect. .. Based on these
notions, different shape similarity criteria and distances were presented in > Sect. ., fit-
ting well under the metric umbrella. Computational aspects related to shape and metric
discretization were discussed in > Sects. .and > ., and computation of full and par-
tial similarity were discussed in > Sects. .and > .. In > Sect. ., feature-based
methods were discussed. For further detailed discussion of these and related subjects, the
reader is referred to the book [].
References and Further Reading
. Adams CC, Franzosa R () Introduction
to topology: pure and applied, Prentice-Hall,
Harlow
. Aizawa A () An information-theoretic per-
spective of tf–idf measures. Inform Process
Manage ():–
. Alt H, Mehlhorn K, Wagener H, Welzl E
() Congruence, similarity, and symmetries
of geometric objects. Discrete Comput Geom :
–
. Andreetto M, Brusco N, Cortelazzo GM ()
AutomaticDmodelingoftexturedculturalher-
itage objects. Trans Image Process ():–
. Assfalg J, Bertini M, Pala P, Del Bimbo A ()
Content-based retrieval of d objects using
spin image signatures. Trans Multimedia ():
–
. Atallah MJ () On symmetry detection. IEEE
Trans Comput c-():–
. Aurenhammer F () Voronoi diagramsa sur-
vey of a fundamental geometric data structure.
ACM Comput Surv ():–
. Bay H, Tuytelaars T, Van Gool L () SURF:
speeded up robust features. Proceedings of
ECCV, pp –
. Belkin M, Niyogi P () Laplacian eigenmaps
for dimensionality reduction and data represen-
tation. Neural Comput :–, Introduc-
tion of Laplacian embeddings
. Bellman RE () Dynamic programming.
Dover, New York
. Belongie S, Malik J, Puzicha J () Shape
matching and object recognition using shape
contexts. Trans PAMI :–


Manifold Intrinsic Similarity
. Ben-Chen M, Weber O, Gotsman C ()
Characterizing shape using conformal factors.
Proceedings of DOR
. Bérard P, Besson G, Gallot S () Embed-
ding Riemannian manifolds by their heat kernel.
Geom Funct Anal ():–
. Besl PJ, McKay ND () A method for regis-
tration of D shapes, IEEE Trans Pattern Anal
Mach Intell (PAMI) ():–, Introduc-
tion of ICP
. Bjorck AA () Numerical methods for least
squares problems. Society for Industrial Mathe-
matics, Philadelphia
. Bernstein M, de Silva V, Langford JC, Tenen-
baum JB () Graph approximations to
geodesics on embedded manifolds, Technical
report
. Borg I, Groenen P () Modern multidimen-
sional scaling - theory and applications. Com-
prehensive overview of MDS problems and their
numerical solution. Springer, New York
. Bronstein AM, Bronstein MM () Not
only size matters: regularized partial match-
ing of nonrigid shapes. IEEE computer soci-
ety conference on computer vision and pattern
recognition workshops, CVPR Workshops

. Bronstein AM, Bronstein MM () Regular-
ized partial matching of rigid shapes. Proceed-
ings of European conference on computer vision
(ECCV), pp –
. Bronstein AM, Bronstein MM, Kimmel R
() Expression-invariant D face recogni-
tion. Proceedings of audio and video-based bio-
metric person authentication. Lecture notes in
computer science, vol , D face recog-
nition using metric model. Springer, Berlin,
pp –
. Bronstein AM, Bronstein MM, Kimmel R
() On isometric embedding of facial sur-
faces into S() Proceedings of interna-
tional conference scale space and pde methods
in computer vision. Lecture notes in computer
science, vol , MDS with spherical geometry.
Springer, New York, pp –
. Bronstein AM, Bronstein MM, Kimmel R
() Three-dimensional face recognition. Int
J Comput Vis (IJCV) ():–, D face recog-
nition using metric model
. Bronstein AM, Bronstein MM, Kimmel R
() Efficient computation
of isometry-
invariant distances between surfaces. SIAM
J
Sci
Comput
():–,
computa-
tion
of
the
Gromov-Hausdorff
distance
using GMDS
. Bronstein AM, Bronstein MM, Kimmel R
() Generalized multidimensional scaling: a
framework for isometry-invariant partial sur-
face matching. Proc Natl Acad Sci (PNAS)
():–, Introduction of generalized
MDS
. Bronstein AM, Bronstein MM, Kimmel R
() Robust expression-invariant face recog-
nition from partially missing data. Proceedings
of European Conference on Computer Vision
(ECCV), D face recognition with partially
missing data, pp –
. Bronstein AM, Bronstein MM, Kimmel R
()
Numerical
geometry
of
non-rigid
shapes. Springer, New York, first systematic
treatment of non-rigid shapes
. Bronstein AM, Bronstein MM, Kimmel R,
Mahmoudi M, Sapiro G () A Gromov-
Hausdorff framework with diffusion geometry
for topologically-robust non-rigid shape match-
ing. Int J Comput Vis (IJCV) :–
. Bronstein AM, Bronstein MM, Ovsjanikov M,
Guibas LJ () Shape google: a computer
vision approach to invariant shape retrieval.
Proceedings of NORDIA
. Bronstein AM, Bronstein MM, Bruckstein AM,
Kimmel R () Partial similarity of objects,
or how to compare a centaur to a horse. Int J
Comput Vis ():–
. Bronstein AM, Bronstein MM, Bustos B, Castel-
lani U, Crisani M, Falcidieno B, Guibas LJ,
Isipiran I, Kokkinos I, Murino V, Ovsjanikov M,
Patané G, Spagnuolo M, Sun J () Robust
feature detection and description benchmark.
Proceedings of DOR
. Bronstein AM, Bronstein MM, Kimmel R,
Mahmoudi M, Sapiro G () A Gromov-
Hausdorff framework with diffusion geometry
for topologically-robust non-rigid shape match-
ing. IJCV (–):–
. Bronstein MM, Bronstein AM () Shape
recognition with spectral Distances. Trans
PAMI (in press)

Manifold Intrinsic Similarity 

. Bronstein MM, Bronstein AM, Kimmel R,
Yavneh I () Multigrid multidimensional
scaling. Num Linear Algebra Appl (–):
–, Multigrid solver for MDS problems
. Bronstein
MM,
Kokkinos I () Scale-
invariant heat kernel signatures for non-rigid
shape recognition. Proceedings of CVPR
. Burago D, Burago Y, Ivanov S () A course
in metric geometry. Graduate studies in mathe-
matics, vol , Systematic introduction to metric
geometry. AMS, Providence
. Chan TF, Vese LA () A level set algorithm
for minimizing the Mumford-Shah functional
in image processing. IEEE workshop on varia-
tional and level set methods, pp –
. Chen Y, Medioni G () Object modeling by
registration of multiple range images. Proceed-
ings of conference on robotics and automation,
Introduction of ICP
. Chum O, Philbin J, SivicJ, Isard M, Zisserman A
() Total recall: automatic query expan-
sion with a generative feature model for object
retrieval. Proceedings of ICCV
. Clarenz U, Rumpf M, Telea A () Robust
feature detection and local classification for sur-
faces based on moment analysis. Trans Visual
Comput Graphics ():–
. Coifman RR, Lafon S () Diffusion maps.
Appl Comput Harmon Anal ():–, Defini-
tion of diffusion distance
. Coifman RR, Lafon S, Lee AB, Maggioni M,
Nadler B, Warner F, Zucker SW () Geo-
metric diffusions as a tool for harmonic analysis
and structure definition of data: diffusion maps.
Proc Natl Acad Sci (PNAS) ():–,
Introduction of diffusion maps and diffusion
distances
. Cox TF, Cox MAA () Multidimensional
scaling. Chapman & Hall, London
. Crandal MG, Lions P-L () Viscosity solu-
tionsof Hamilton–Jacobi Equations.TransAMS
:–
. Dalai N, Triggs B () Histograms of oriented
gradients for human Detection. Proceedings of
CVPR
. De Leeuw J () Recent developments in
statistics, ch Applications of convex analysis
to multidimensional scaling. North-Holland,
Amsterdam, pp –
. Du Q, Faber V, Gunzburger M () Cen-
troidal Voronoi tessellations: applications and
algorithms. SIAM Rev ():–
. Dubrovina A, Kimmel R () Matching
shapes by eigendecomposition of the Laplace-
Beltrami operator. Proceedings of DPVT
. Elad A, Kimmel R () Bending invariant
representations
for
surfaces.
Proceedings
on computer vision and pattern recognition
(CVPR), Introduction
of canonical
forms,
pp –
. Elad A, Kimmel R () On bending invariant
signatures for surfaces. IEEE Trans Pattern Anal
Mach Intell (PAMI) ():–, Introduc-
tion of canonical forms
. Gebal K, Bærentzen JA, Aanæs H, Larsen R
() Shape analysis using the auto dif-
fusion function. Computer Graphics Forum
():–
. Gelfand N, Mitra NJ, Guibas LJ, Pottmann H
() Robust global registration. Proceedings
of SGP
. Gersho A, Gray RM () Vector quantization
and signal compression. Kluwer, Boston
. Glomb P (May ) Detection of inter-
est points on D data: extending the harris
Operator. Computer recognition systems .
Advances in soft computing, vol . Springer,
Berlin/Heidelberg, pp –
. Gold S, Rangarajan A () A graduated
assignment algorithm for graph matching.
Trans PAMI :–
. Gordon C, Webb DL, Wolpert S () One
cannot hear the shape of the drum. Bull AMS
():–, Example of isospectral but non-
isometric shapes
. Gromov M () Structures Métriques Pour
les Variétés Riemanniennes. Textes Mathéma-
tiques, vol , Introduction of the Gromov-
Hausdorff distance
. Gu X, Gortler S, Hoppe H () Geom-
etry
images.
Proceedings
of
SIGGRAPH,
pp –
. Harris C, Stephens M () A combined corner
and edge detection. Proceedings of fourth Alvey
vision conference, pp –
. Hausdorff F () Grundzüge der Mengen-
lehre, Definition of the Hausdorff distance.
Verlag Veit & Co, Leipzig,


Manifold Intrinsic Similarity
. Hochbaum DS, Shmoys DB () A best pos-
sible heuristic for the k-center problem. Math
Oper Res :–
. Indyk P, Thaper N () Fast image retrieval
via embeddings. rd International workshop on
statistical and computational theories of vision
. Johnson AE,Hebert M () Using spin images
for efficient object recognition in cluttered D
scenes. Trans PAMI ():–
. Kac M () Can one hear the shape of a drum?
Am Math Mon :–, Kac’s conjecture about
isospectral but non-isometric shapes
. Kimmel R, Sethian JA () Computing
geodesic paths on manifolds. Proc Natl Acad
Sci (PNAS) ():–
. Kolomenkin M, Shimshoni I, Tal A ()
On edge detection on surfaces. Proceedings of
CVPR
. Komodakis N, Paragios N, Tziritas G ()
MRF
optimization
via
dual
decomposi-
tion: message-passing revisited. Proceedings
of ICCV
. Leibon G, Letscher D () Delaunay triangu-
lations and Voronoi diagrams for Riemannian
manifolds. Proceedings of symposium on com-
putational geometry, pp –
. Lévy B () Laplace-Beltrami eigenfunctions
towards an algorithm that “understands” geom-
etry. International conference on shape model-
ing and applications, The use of Laplace-Belrami
operator for shape analysis and synthesis
. Lloyd SP () Least squares quantization in
PCM. Bell telephone laboratories paper
. Losasso F, Hoppe H, Schaefer S, Warren J
() Smooth geometry Images. Proceedings
of symposium on geometry processing (SGP),
pp –
. Lowe D () Distinctive image features from
scale-invariant keypoint. Int J Comput Vis
:–
. Matas J, Chum O, Urban M, Pajdla T ()
Robust wide-baseline stereo from maximally
stable extremal regions. Image Vis Comput
():–
. Mateus D, Horaud RP, Knossow D, Cuzzolin F,
Boyer E () Articulated shape matching
using
Laplacian eigenfunctions
and unsu-
pervised
point
registration.
Proceedings
of CVPR
. Max J () Quantizing for minimum distor-
tion. IEEE Trans Inform Theory ():–
. Mémoli F () On the use of Gromov-
Hausdorff distances for shape Comparison.
Proceedings of point based graphics, Prague,
Definition of the Gromov-Wasserstein distance
. Mémoli F () Gromov-Hausdorff distances
in Euclidean spaces. Proceedings of non-rigid
shapes and deformable image alignment (NOR-
DIA), Relation of Gromov-Hausdorff distances
in Euclidean spaces to Hausdorff and ICP
distances
. Mémoli F, Sapiro G () Fast computation
of weighted distance functions and geodesics
on implicit hyper-surfaces. J Comput Phys
():–
. Memoli F, Sapiro G () Distance functions
and geodesics on submanifolds of rd and point
clouds. SIAM J Appl Math ():
. Mémoli F, Sapiro G () A theoretical
and computational framework for isometry
invariant recognition of point cloud data.
Found Comput Math :–, First use
of the Gromov-Hausdorff distance in shape
recognition
. Meyer M, Desbrun M, Schroder P, Barr AH
() Discrete differential-geometry opera-
tors for triangulated -manifolds. Visual Math
III:–, Cotangent weights discretization of
the Laplace-Beltrami operator
. Mitra NJ, Bronstein AM, Bronstein MM ()
Intrinsic regularity detection in D geometry,
Proc. ECCV
. Mitra NJ, Gelfand N, Pottmann H, Guibas L
() Registration of point cloud data from
a geometric optimization perspective. Proceed-
ings of Eurographics symposium on geometry
processing, pp –, Analysis of ICP algo-
rithms from optimization standpoint
. Mitra NJ, Guibas LJ, Giesen J, Pauly M ()
Probabilistic fingerprints for shapes. Proceed-
ings of SGP
. Mitra NJ, Guibas LJ, Pauly M () Par-
tial and approximate symmetry detection for
D geometry. ACM Trans Graphics ():
–
. Nash J () The imbedding problem for
Riemannian manifolds. Ann Math :–,
Nash embedding theorem

Manifold Intrinsic Similarity 

. Osada R, Funkhouser T, Chazelle B, Dobkin D
() Shape distributions. ACM Trans Graph-
ics (TOG) ():–, Introduction of the
shape distributions method for rigid shapes
. Ovsjanikov M, Sun J, Guibas L () Global
intrinsic symmetries
of Shapes. Computer
graphics forum, vol . Spectral method for
non-rigid symmetry detection, pp –
. Ovsjanikov M, Sun J, Guibas LJ () Global
intrinsic symmetries of shapes. Proceedings of
SGP, pp –
. Pauly M, Keiser R, Gross M () Multi-
scale
feature
extraction
on
point-sampled
surfaces. Computer graphics forum, vol ,
pp –
. Pauly M, Mitra NJ, Wallner J, Pottmann H,
Guibas LJ () Discovering structural reg-
ularity in D geometry, ACM trans. Graphics
()
. Peyre G, Cohen L () Surface segmenta-
tion using geodesic centroidal Tesselation. Pro-
ceedings of international symposium on D
data processing visualiztion transmission, pp
–
. Pinkall U, Polthier K () Computing dis-
crete minimal surfaces and their conjugates. Exp
Math ():–, Cotangent weights discretiza-
tion of the Laplace-Beltrami operator
. Raviv D,BronsteinAM,BronsteinMM,Kimmel
R () Symmetries of non-rigid shapes, Pro-
ceedings of workshop on non-rigid registration
and tracking through learning (NRTL)
. Raviv D,BronsteinAM,BronsteinMM,Kimmel
R () Full and partial symmetries of non-
rigid shapes. Intl J Comput Vis (IJCV) ():
–
. Reuter M, Biasotti S, Giorgi D, Patanè G,
Spagnuolo M () Discrete Laplace-Beltrami
operators for shape analysis and segmentation.
Comput Graphics :–, FEM approxima-
tion of the Laplace-Beltrami operator
. Reuter M, Wolter F-E, Peinecke N ()
Laplace-beltrami spectra as “shape-DNA” of
surfaces and solids. Comput Aided Design
():–,
Shape
recognition
using
Laplace-Beltrami spectrum
. Rosman G, Bronstein AM, Bronstein MM,
Sidi A, Kimmel R () Fast multidimensional
scaling using vector extrapolation. Technical
report CIS--, Department
of
Com-
puter Science, Technion, Israel, Introduction
of vector extrapolation methods for MDS
problems
. Rubner Y, Guibas LJ, Tomasi C () The earth
movers distance, multi-dimensional scaling,
and color-based image retrieval. Proceedings
of the ARPA image understanding workshop,
pp –
. Rustamov RM () Laplace-Beltrami eigen-
functions for deformation invariant shape rep-
resentation.Proceedingsof SGP, Introductionof
GPS embedding, pp –
. Sander P, Wood Z, Gortler S, Snyder J, Hoppe H
() Multichart geometry images. Proceed-
ings of Symposium on geometry processing
(SGP), pp –
. Sethian JA () A fast marching level set
method for monotonically advancing fronts.
Proc Natl Acad Sci (PNAS) ():–
. Shilane P, Funkhauser T () Selecting dis-
tinctive D shape descriptors for similarity
retrieval. Proceedings of Shape Modelling and
Applications
. Shirdhonkar S, Jacobs DW () Approximate
earth movers distance in linear time. IEEE Con-
ference on Computer Vision and Pattern Recog-
nition, . CVPR 
. Sivic J, Zisserman A () Video google: a text
retrieval approach to object matching in videos.
Proceedings of CVPR
. Spira A, Kimmel R () An efficient solu-
tion to the eikonal equation on parametric
manifolds. Interfaces Free Boundaries ():
–
. Starck J, Hilton A () Correspondence
labelling for widetimeframe free-form surface
matching. Proceedings of ICCV
. SunJ,Ovsjanikov M,GuibasLJ()Aconcise
and provably informative multi-scale signature
based on heat diffusion. Proceedings of SGP
. Thorstensen N, Keriven R () Non-rigid
shape matching using geometry and photome-
try. Proceedings of CVPR
. Thrun S, Wegbreit B () Shape from symme-
try. Procedings of ICCV
. Toldo R, Castellani U, Fusiello A () Visual
vocabulary signature for D object retrieval and
partial matching. Proceedings of DOR


Manifold Intrinsic Similarity
. Torresani L, Kolmogorov V, Rother C ()
Feature correspondence via graph matching:
Models and global optimization. Proceedings of
ECCV, pp –
. Tsai YR, Cheng LT, Osher S, Zhao HK
() Fast sweeping algorithms for a class of
Hamilton-Jacobi equations. SIAM J Num Anal
(SINUM) ():–
. Tsitsiklis JN () Efficient algorithms for glob-
ally optimal trajectories. IEEE Trans Automatic
Control ():–
. Tutte WT () How to draw a graph. Proc
Lond Math Soc ():–, Tutte Laplacian
operator
. Walter J, Ritter H () On interactive visu-
alization of high-dimensional data using the
hyperbolic plane. Proceedings of international
conference on knowledge discovery and data
mining (KDD), MDS with hyperbolic geometry,
pp –
. Wang C, Bronstein MM, Paragios N ()
Discrete minimum distortion correspondence
problems
for
non-rigid
shape
matching,
Research report , INRIA
. Wardetzky M, Mathur S, Kälberer F, Grinspun E
() Discrete Laplace operators: no free
lunch. Conference on computer graphics and
interactive techniques, Analysis of different dis-
cretizations of the Laplace-Beltrami operator
. Weber
O,
Devir
YS,
Bronstein
AM,
Bronstein MM,
Kimmel
R
()
Parallel
algorithms for approximation of distancelm
maps on parametric surfaces. ACM Trans
Graph ():–
. Wolter JD, Woo TC, Volz RA () Optimal
algorithms for symmetry detection in two and
three dimensions. Visual Comput :–
. Zaharescu A, Boyer E, Varanasi K, Horaud R
() Surface feature detection and description
with applications to mesh matching. Proceed-
ings of CVPR
. Zhang
H
()
Discrete
combinatorial
Laplacian
operators
for
digital
geometry
processing. SIAM Conference on Geomet-
ric Design. Combinatorial Laplace-Beltrami
operator, pp –
. Zhao HK () Fast sweeping method for
Eikonal equations. Math Comput :–

Image Segmentation with
Shape Priors: Explicit
Versus Implicit
Representations
Daniel Cremers
.
Introduction....................................................................
..
Image Analysis and Prior Knowledge...............................................
..
Explicit Versus Implicit Shape Representation. ....................................
.
Image Segmentation via Bayesian Inference..................................
.
Statistical Shape Priors for Parametric Shape Representations.............
..
Linear Gaussian Shape Priors........................................................
..
Nonlinear Statistical Shape Priors...................................................
.
Statistical Priors for Level Set Representations...............................
..
Shape Distances for Level Sets.......................................................
..
Invariance by Intrinsic Alignment...................................................
...Translation Invariance by Intrinsic Alignment....................................
...Translation and Scale Invariance via Alignment..................................
..
Kernel Density Estimation in the Level Set Domain.............................
..
Gradient Descent Evolution for the Kernel Density Estimator..................
..
Nonlinear Shape Priors for Tracking a Walking Person..........................
.
Dynamical Shape Priors for Implicit Shapes..................................
..
Capturing the Temporal Evolution of Shape.......................................
..
Level Set Based Tracking via Bayesian Inference..................................
..
Linear Dynamical Models for Implicit Shapes.....................................
..
Variational Segmentation with Dynamical Shape Priors.........................
.
Parametric Representations Revisited: Combinatorial Solutions
for Segmentation with Shape Priors...........................................
.
Conclusion......................................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
.
Introduction
..
Image Analysis and Prior Knowledge
Image segmentation is among the most studied problems in image understanding and
computer vision. The goal of image segmentation is to partition the image plane into
a set of meaningful regions. Here meaningful typically refers to a semantic partition-
ing where the computed regions correspond to individual objects in the observed scene.
Unfortunately, generic purely low-level segmentation algorithms often do not provide the
desired segmentation results, because the traditional low level assumptions like intensity
or texture homogeneity and strong edge contrast are not sufficient to separate objects in a
scene.
To overcome these limitations, researchers have proposed to impose prior knowl-
edge into low-level segmentation methods. In the following, we will review methods
which allow to impose knowledge about the shape of objects of interest into segmentation
processes.
In the literature there exist various definitions of the term shape, from the very broad
notion of shape of Kendall [] and Bookstein [] where shape is whatever remains of an
object when similiarity transformations are factored out (i.e., a geometrically normalized
version of a gray value image) to more specific notions of shape referring to the geometric
outline of an object in D or D. In this work, we will adopt the latter view and refer to an
object’s silhouette or boundary as its shape. Intentionally we will leave the exact mathemat-
ical definition until later, as different representations of geometry actually imply different
definitions of the term shape.
One can distinguish various kinds of shape knowledge:
•
Low-level shape priors which typically simply favor shorter boundary length, i.e.,
curves with shorter boundary have lower shape energy, where boundary length can
be measured in various ways [, , , , ].
•
Mid-level shape priors which favor for example thin and elongated structures, thereby
facilitating the segmentation of roads in satellite imagery or of blood vessles in medical
imagery [, , ].
•
High-level shape priors which favor similarity to previously observed shapes, such as
hand shapes [, , ], silhouettes of humans [, ], or medical organs like the heart,
the prostate, the lungs, or the cerebellum [, , , ].
There exists a wealth of works on shape priors for image segmentation. It is beyond the
scope of this article to provide a complete overview of existing work. Instead we will present
a range of representative works – with many of the examples taken from the author’s own
work – discuss their advantages and shortcomings. Some of these works are formulated in
a probabilistic setting where the challenge is to infer the most likely shape given an image
and a set of training shapes. Typically the segmentation is formulated as an optimization
problem.

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

One can distinguish two important challenges:
. The modeling challenge: How do we formalize distances between shapes? What prob-
ability distributions do we impose? What energies should we minimize?
. The algorithmic challenge: How do we minimize the arising cost function? Are the
computed solutions globally optimal? If they are not globally optimal, how sensitive
are solutions with respect to the initialization?
..
Explicit Versus Implicit Shape Representation
A central question in the modeling of shape similarity is that of how to represent a shape.
Typically one can distinguish between explicit and implicit representations. In the former
case, the boundary of the shape is represented explicitly – in a spatially continuous set-
ting this could be a polygon or a spline curve. In a spatially discrete setting this could be
a set of edgles (edge elements) forming a regular grid. Alternatively, shapes can be repre-
sented implicitly in the sense that one labels all points in space as being part of the interior
or the exterior of the object. In the spatially continuous setting, the optimization of such
implicit shape representations is solved by means of partial differential equations. Among
the most popular representatives are the level set method [, ] or alternative convex
relaxation techniques []. In the spatially discrete setting, implicit representations have
become popular through the graph cut methods [, ]. More recently, researchers have
also advocated hybrid representations where objects are represented both explicitly and
implicitly []. > Table -provides an overview of a few representative works on image
segmentation based on explicit and implicit representations of shape.
> Figure -shows examples of shape representations using an explicit parametric
representation by spline curves (spline control points are marked as black boxes), implicit
representations by a signed distance function or a binary indicator function, and an explicit
discrete representation (th image).
As we shall see in the following, the choice of shape representation has important con-
sequences on the class of objects that can be modeled, the type of energy that can be
⊡Table -
Shapes can be represented explicitly or implicitly, in a spatially continuous or a spatially dis-
crete setting. More recently, researchers have adopted hybrid representations [], where
objects are represented both in terms of their interior (implicitly) and in terms of their
boundary (explicitly)
Spatially continuous
Spatially discrete
Explicit
Polygons [, ], splines
[, , ]
Edgel labeling & dyn. progr.
[, , , , ]
Hybrid repres. &
LP relaxation []
Implicit
Level set methods [, ],
convex relaxation [, ]
Graph cut methods [, ]


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
2
3
4
6
5
7
8
9
0
1
⊡Fig. -
Examples of shape representations by means of a parametric spline curve (st image), a
signed distance function (nd image), a binary indicator function (rd image), and an
explicit discrete representation (th image)
minimized, and the optimality guarantees that can be obtained. Among the goals of this
article is to put in contrast various shape representations and discuss their advantages and
limitations. In general one observes that:
•
Implicit representations are easily generalized to shapes in arbitrary dimension.
Respective algorithms (level set methods, graph cuts, convex relaxation techniques)
straight-forwardly extend to three or more dimensions. Instead, the extension of
explicit shape representations to higher dimensions is by no means straightforward:
The notion of arc-length parameterization of curves does not extend to surfaces. More-
over, the discrete polynomial-time shortest-path algorithms [, , ] which allow to
optimally identify pairwise correspondence of points on either shape do not directly
extend to minimal-surface algorithms.
•
Implicit representations are easily generalized to arbitrary shape topology. Since the
implicit representation merely relies on a labeling of space (as being inside or outside
the object), the topology of the shape is not constrained. Both level set and graph cut
algorithms can therefore easily handle objects of arbitrary topology. Instead, for spa-
tially continuous parametric curves, modeling the transition from a single closed curve
to a multiply connected object boundary requires sophisticated splitting and merging
techniques [, , , ]. Similarly, discrete polynomial-time algorithms are typically
constrained to finding open [, , ] or closed curves [, ].
•
Explicit boundary representations allow to capture the notion of point correspondence
[, , ]. The correspondence between points on either of two shapes and the under-
lying correspondence of semantic parts is of central importance to human notions of
shape similarity. The determination of optimal point correspondences, however, is an
important combinatorial challenge, especially in higher dimensions.
•
For explicit representations, the modeling of shape similarity is often more straight-
forward and intuitive. For example, for two shapes parameterized as spline curves, the
linear interpolation of these shapes also gives rise to a spline curve and often cap-
tures the human intuition of an intermediate shape. Instead, the linear interpolation
of implicit representations is generally not straight forward: Convex combintations
of binary-valued functions are no longer binary-valued. And convex combinations

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

⊡Fig. -
The linear interpolation of spline-based curves (shown here along the ﬁrst three
eigenmodes of a shape distribution) gives rise to a families of intermediate shapes
⊡Fig. -
This ﬁgure shows the linear interpolation of the signed distance functions associated with
two human silhouettes. The interpolation gives rise to intermediate shapes and allows
changes of the shape topology. Yet, the linear combination of two signed distance functions
is generally no longer a signed distance function
of signed distance functions are generally no longer a signed distance function.
> Figure -shows examples of a linear interpolations of spline curves and a linear
interpolations of signed distance functions. Note that the linear interpolation of signed
distance functions may give rise to intermediate silhouettes of varying topology.
In the following, we will give an overview over some of the developments in the
domain of shape priors for image segmentation. In > Sect. ., we will review a for-
mulation of image segmentation by means of Bayesian inference which allows the
fusion of input data and shape knowledge in a single energy minimization framework
(> Fig. -). In > Sect. ., we will discuss a framework to impose statistical shape
priors in a spatially continuous parametric representation. In > Sect. ., we discuss
methods to impose statistical shape priors in level set based image segmentation. In
> Sect. ., we discuss statistical models which allow to represent the temporal evolu-
tion of shapes and can serve as dynamical priors for image sequence segmentation. And
lastly, in > Sect. ., we will present recent developments to impose elastic shape priors
in a manner which allows to compute globally optimal shape-consistent segmentations in
polynomial time.


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
.
Image Segmentation via Bayesian Inference
Over the last decades Bayesian inference has become an established paradigm to tackle
data analysis problems – see [, ] for example. Given an input image I : Ω →R on a
domain Ω ⊂R, a segmentation C of the image plane Ω can be computed by maximizing
the posterior probability:
P(C ∣I) = P(I ∣C) P(C)
P(I)
,
(.)
where P(I ∣C) denotes the data likelihood for a given segmentation C, and P(C) denotes
the prior probability which allows to impose knowledge about which segmentations are
a priori more or less likely.
Maximizing the posterior distribution can be performed equivalently by minimizing
the negative logarithm of (> .) which gives rise to an energy or cost function of the
form:
E(C) = Edata(C) + Eshape(C),
(.)
where Edata(C) = −logP(I ∣C) and Eshape(C) = −logP(C) are typically referred to as
data fidelity term and regularizer or shape prior. By maximizing the posterior, one aims
at computing the most likely solution given data and prior. Of course there exist alterna-
tive strategies of either computing solutions corresponding to the mean of the distribution
rather than its mode, or of retaining the entire posterior distribution in order to propagate
multiple hypotheses over time, as done for example in the context of particle filtering [].
Over the years various data terms have been proposed. In the following, we will simply
use a piecewise-constant approximation of the input intensity I []:
Edata(C) =
k
∑
i=∫Ωi
(I(x) −μi)
dx,
(.)
where the regions Ω, . . . , Ωk are pairwise disjoint regions separated by the boundary C
and μi denotes the average of I over the region Ωi:
μi =

∣Ωi∣∫Ωi
I(x) dx.
(.)
More sophisticated data terms based on color likelihoods [, , ] or texture likelihoods
[, ] are conceivable.
A glance into the literature indicates that the most prominent image segmentation
methods rely on a rather simple geometric shape prior Eshape which energetically favors
shapes with shorter boundary length [, , ], a penalizer which – in a spatially discrete
setting – dates back at least as far as the Ising model for ferromagnetism []. There are sev-
eral reasons for the popularity of length constraints in image segmentation. Firstly, solid
objects in our world indeed tend to be spatially compact. Secondly, such length constraints
are mathematically well-studied. They give rise to well-behaved models and algorithms –
mean curvature motion in a continuous setting and low-order Markov random fields and
submodular cost functions in the discrete setting.

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

Nevertheless, the preference for a shorter boundary is clearly a very simplistic shape
prior. In many applications the user may have a more specific knowledge about what kinds
of shapes are likely to arise in a given segmentation task. For example, in biology one may
want to segment cells that all have a rather specific size and shape. In medical imaging
one may want to segment organs that all have a rather unique shape – up to a certain
variability – and preserve a specific spatial relationship with respect to other organs. In
satellite imagery one may be most interested in segmenting thin and elongated roads, or in
the analysis of traffic scenes from a driving vehicle, the predominant objects may be cars
and pedestrians. In the following sections, we will discuss ways to impose such higher-level
shape knowledge into image segmentation methods.
.
Statistical Shape Priors for Parametric Shape
Representations
Among the most straight forward ways to represent a shape is to model its outline as a
parametric curve. An example is a simple closed spline curve C ∈Ck(S, Ω) of the form:
C(s) =
n
∑
i=
pi Bi(s),
(.)
where pi ∈Rdenote a set of spline control points and Bi a set of spline basis functions
of degree k [, , , ]. In the special case of linear basis functions, we simply have a
polygonal shape, used for example in []. With increasing number of control points, we
obtain a more and more detailed shape representation – see > Fig. -. It shows one of the
nice properties of parametric shape representations: The representation is quite compact in
the sense that very detailed silhouettes can be represented by a few real-valued variables.
Given a family of m shapes, each represented by a spline curve of a fixed number of
n control points, we can think of these training shapes as a set {z, . . . , zm} of control point
vectors:
zi = (pi, . . . , pin) ∈Rn,
(.)
where we assume that all control point vectors are normalized with respect to translation,
rotation and scaling [].
Input image
20 points
40 points
70 points
100 points
⊡Fig. -
Spline representation of a hand shape (left) with increasing resolution


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
With this contour representation, the image segmentation problem boils down to
computing an optimal spline control point vector z ∈Rn for a given image. The segmen-
tation process can be constrained to familiar shapes by imposing a statistical shape prior
computed from the set of training shapes.
..
Linear Gaussian Shape Priors
Among the most popular shape prior is based on the assumption that the training shapes
are Gaussian distributed – see for example [, , ]. There are several reasons for the
popularity of Gaussian distributions. Firstly, according to the central limit theorem the
average of a large number of i.i.d. random variables is approximately Gaussian distributed –
so if the observed variations of shape were created by independent processes, then one
could expect the overall distribution to be approximately Gaussian. Secondly, the Gaussian
distribution can be seen as a second-order approximation of the true distribution. And
thirdly, the Gaussian distribution gives rise to a convex quadratic cost function that allows
for easy minimization.
In practice, the number of training shapes m is often much smaller than the number
of dimensions n. Therefore, the estimated covariance matrix Σ is degenerate with many
zero eigenvalues and thus not invertible. As introduced in [], a regularized covariance
matrix is given by:
Σ = Σ + λ (I −V V t),
(.)
where V is the matrix of eigenvectors of Σ. In this way, we replace all zero eigenvalues of the
sample covariance matrix Σ by a constant λ ∈[, λr], where λr denotes the smallest non-
zero eigenvalue of Σ. (Note that the inverse Σ−
 of the regularized covariance matrix defined
in (> .) fundamentally differs from the pseudoinverse, the former scaling components
in degenerate directions by λ−
 while the latter scales them by .) In [] it was shown
that λ can be computed from the true covariance matrix by minimizing the Kullback-
Leibler divergence between the exact and the approximated distribution. Yet, since we do
not have the exact covariance matrix but merely a sample covariance matrix, the reasoning
for determining λ suggested in [] is not justified.
The Gaussian shape prior is then given by:
P(z) =

∣πΣ∣/exp (−
(z −¯z)t Σ−
 (z −¯z)),
(.)
where ¯z denotes the mean control point vector.
Based on the Gaussian shape prior, we can define a shape energy that is invariant to
similarity transformations (translation, rotation and scaling) by:
Eshape(z) = −log P (ˆz),
(.)
where ˆz is the shape vector upon similarity alignment with respect to the training shapes:
ˆz = R (z −z)
∣R (z −z)∣,
(.)

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

Initial curve
Step 1
Step 2
Step 3
Step 4
Step 5
Final
Training shapes
⊡Fig. -
Evolution of a parametric spline curve during gradient descent on the energy (> .)
combining the piecewise constant intensity model (> .) with a Gaussian shape prior
constructed from a set of sample hand shapes (lower right). Note that the shape prior is by
construction invariant to similiarity transformations. As a consequence, the contour easily
undergoes translation, rotation, and scaling as these do not aﬀect the energy
where the optimal translation zand rotation R can be written as functions of z []. As a
consequence, we can minimize the overall energy
E(z) = Edata(C(z)) + Eshape(z)
(.)
using gradient descent in z. For details on the numerical minimization we refer to [, ].
> Figure -shows several intermediate steps in a gradient descent evolution on the
energy (> .) combining the piecewise constant intensity model (> .) with a Gaussian
shape prior constructed from a set of sample hand shapes. Note how the similarity-
invariant shape prior (> .) constrains the evolving contour to hand-like shapes without
constraining its translation, rotation, or scaling.
> Figure -shows the gradient descent evolution with the same shape prior for
an input image of a partially occluded hand. Here the missing part of the silhouette is
recovered through the statistical shape prior. These evolutions demonstrate that the curve
converges to the correct segmentation over rather large spatial distance, an aspect which is
characteristic for region-based cost functions like (> .).
..
Nonlinear Statistical Shape Priors
The shape prior (> .) was based on the assumption that the training shapes are Gaussian
distributed. For collections of real-world shapes this is generally not the case. For example,


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
⊡Fig. -
Gradient descent evolution of a parametric curve from initial to ﬁnal with similarity
invariant shape prior. The statistical shape prior permits a reconstruction of the hand
silhouette in places where it is occluded
the various silhouettes of a rigid D object obviously form a three-dimensional manifold
(given that there are only three degrees of freedom in the observation process). Similarly,
the various silhouettes of a walking person essentially correspond to a one-dimensional
manifold (up to small fluctuations). Furthermore, the manifold of shapes representing
deformable objects like human persons are typically very low-dimensional, given that the
observed D structure only has a small number of joints.
Rather than learning the underlying low-dimensional representation (using principal
surfaces or other manifold learning techniques), we can simply estimate arbitrary shape
distributions by reverting to nonlinear density estimators – nonlinear in the sense that
the permissible shapes are not simply given by a weighted sum of eigenmodes. Classical
approaches for estimating nonlinear distributions are the Gaussian mixture model or the
Parzen–Rosenblatt kernel density estimator – see > Sect. ..
An alternative technique is to adapt recent kernel learning methods to the problem of
density estimation []. To this end, we approximate the training shapes by a Gaussian
distribution, not in the input space but rather upon transformation ψ : Rn →Y to some
generally higher-dimensional feature space Y:
Pψ(z) ∝exp (−
(ψ(z) −ψ)t Σ−
ψ (ψ(z) −ψ)).
(.)
As before, we can define the corresponding shape energy as:
E(z) = −logPψ (ˆz),
(.)
with ˆz being the similarity-normalized shape given in (> .). Here ψand Σψ denote
the mean and covariance matrix computed for the transformed shapes:
ψ= 
m
m
∑
i=
ψ(zi),
Σψ = 
m
m
∑
i=
(ψ(zi) −ψ)(ψ(zi) −ψ)⊺,
(.)
where Σψ is again regularized as in (> .).
As shown in [], the energy E(z) in (> .) can be evaluated without explicitly spec-
ifying the nonlinear transformation ψ. It suffices to define the corresponding Mercer kernel
[, ]:
k(x, y) := ⟨ψ(x),ψ(y)⟩,
∀x, y ∈Rn,
(.)

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

representing the scalar product of pairs of transformed points ψ(x) and ψ(y). In the
following, we simply chose a Gaussian kernel function of width σ:
k(x, y) =

(πσ )
n
exp (−∣∣x −y∣∣
σ 
).
(.)
It was shown in [] that the resulting energy can be seen as a generalization of the classical
Parzen–Rosenblatt estimators. In particular, the Gaussian distribution in feature space Y is
fundamentally different from the previously presented Gaussian distribution in the input
space Rn. > Figure -shows the level lines of constant shape energy computed from a
set of left and right hand silhouettes, displayed in a projection onto the first two eigenmodes
of the distribution. While the linear Gaussian model gives rise to elliptical level lines, the
Gaussian mixture and the nonlinear Gaussian allow for more general non-elliptical level
lines. In contrast to the mixture model, however, the nonlinear Gaussian does not require
an interative parameter estimation process, nor does it require or assume a specific number
of Gaussians.
> Figure -shows screenshots of contours computed for an image sequence by gra-
dient descent on the energy (> .) with the nonlinear shape energy (> .) computed
from a set of training silhouettes. Throughout the entire sequence, the object of interest
was occluded by an artificially introduced rectangle. Again, the shape prior allows to cope
with spurious background clutter and to restore the missing parts of the object’s silhou-
ette. Two-dimensional projections of the training data and evolving contour onto the first
principal components, shown in > Fig. -, demonstrate how the nonlinear shape energy
constrains the evolving shape to remain close to the training shapes.
Aligned contours
Simple gaussian
Mixture model
Feature space
gaussian
⊡Fig. -
Model comparison. Density estimates for a set of left (●) and right (+) hands, projected onto
the ﬁrst two principal components. From left to right: Aligned contours, simple Gaussian,
mixture of Gaussians, and Gaussian in feature space (> .). In contrast to the mixture
model, the Gaussian in feature space does not require an iterative (sometimes suboptimal)
ﬁtting procedure


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
Initial contour
No prior
With prior
Sample segmentations of subsequent frames
⊡Fig. -
Tracking a familiar object over a long image sequence with a nonlinear statistical shape
prior. A single shape prior constructed from a set of sample silhouettes allows the
emergence of a multitude of familiar shapes, permitting the segmentation process to cope
with background clutter and partial occlusions
Projection onto1st and 2nd
principal component
Projection onto 2nd and 4th
principal component
⊡Fig. -
Tracking sequence from > Fig. -visualized. Training data (●), estimated energy density
(shaded), and the contour evolution (white curve) in appropriate D projections. The
evolving contour – see > Fig.-– is constrained to the domains of low energy induced by
the training data

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

.
Statistical Priors for Level Set Representations
Parametric representations of shape as those presented above have numerous favorable
properties, in particular, they allow to represent rather complex shapes with few a param-
eters, resulting in low memory requirements and low computation time. Nevertheless, the
explicit representation of shape has several drawbacks:
•
The representation of explicit shapes typically depends on a specific choice of represen-
tation. To factor out this dependency in the representation and in respective algorithms
gives rise to computationally challenging problems. Determining point correspon-
dences, for example, becomes particularly difficult for shapes in higher dimensions
(surfaces in D for example).
•
In particular, the evolution of explicit shape representations requires sophisticated
numerical regridding procedures to assure an equidistant spacing of control points and
prevent control point overlap.
•
Parametric representations are difficult to adapt to varying topology of the represented
shape. Numerically, topology changes require sophisticated splitting and remerging
procedures.
•
A number of recent publications [, , ] indicate that in contrast to explicit shape
representations, the implicit representation of shape allows to compute globally optimal
solutions to shape inference for large classes of commonly used energy functionals.
A mathematical representation of shape which is independent of parameterization was
pioneered in the analysis of random shapes by Fréchet [] and in the school of mathemat-
ical morphology founded by Matheron and Serra [, ]. The level set method [, ]
provides a means of propagating contours C (independent of parameterization) by evolv-
ing associated embedding functions ϕ via partial differential equations – see
> Fig. -
⊡Fig. -
The level set method is based on representing shapes implicitly as the zero level set of a
higher-dimensional embedding function


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
for a visualization of the level set function associated with a human silhouette. It has been
adapted to segment images based on numerous low-level criteria such as edge consistency
[, , ], intensity homogeneity [, ], texture information [, , , ] and motion
information [].
In this section, we will give a brief insight into shape modeling and shape priors for
implicit level set representations. Parts of the following text were adopted from [, , ].
..
Shape Distances for Level Sets
The first step in deriving a shape prior is to define a distance or dissimilarity measure for
two shapes encoded by the level set functions ϕand ϕ. We shall briefly discuss three
solutions to this problem. In order to guarantee a unique correspondence between a given
shape and its embedding function ϕ, we will in the following assume that ϕ is a signed
distance function, i.e., ϕ > inside the shape, ϕ < outside and ∣∇ϕ∣= almost every-
where. A method to project a given embedding function onto the space of signed distance
functions was introduced in [].
Given two shapes encoded by their signed distance functions ϕand ϕ, a simple
measure of their dissimilarity is given by their L-distance in Ω []:
∫
Ω
(ϕ−ϕ)dx.
(.)
This measure has the drawback that it depends on the domain of integration Ω. The shape
dissimilarity will generally grow if the image domain is increased – even if the relative
position of the two shapes remains the same. Various remedies to this problem have been
proposed. We refer to [] for a detailed discussion.
An alternative dissimilarity measure between two implicitly represented shapes rep-
resented by the embedding functions ϕand ϕis given by the area of the symmetric
difference [, , ]:
d(ϕ, ϕ) = ∫
Ω
(Hϕ(x) −Hϕ(x))dx.
(.)
In the present work, we will define the distance between two shapes based on this measure,
because it has several favorable properties. Beyond being independent of the image size Ω,
measure (> .) defines a distance on the set of shapes: it is non-negative, symmetric, and
fulfills the triangle inequality. Moreover, it is more consistent with the philosophy of the
level set method in that it only depends on the sign of the embedding function. In practice,
this means that one does not need to constrain the two level set functions to the space
of signed distance functions. It can be shown [] that L∞and W,norms on the signed
distance functions induce equivalent topologies as the metric (> .).
Since the distance (> .) is not differentiable, we will in practice consider an approx-
imation of the Heaviside function H by a smooth (differentiable) version Hє. Moreover,
we will only consider gradients of energies with respect to the L–norm on the level set

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

function, because they are easy to compute and because variations in the signed distance
function correspond to respective variations of the implicitly represented curve. In general,
however, these do not coincide with so-called shape gradients – see [] for a recent work
on this topic.
..
Invariance by Intrinsic Alignment
One can make use of the shape distance (> .) in a segmentation process by adding it
as a shape prior Eshape(ϕ) = d(ϕ, ϕ) in a weighted sum to the data term, which we will
assume to be the two-phase version of (> .) introduced in []:
Edata(ϕ) =∫
Ω
(I −u+)Hϕ(x)dx +∫
Ω
(I −u−)(−Hϕ(x))dx + ∫
Ω
∣∇Hϕ∣dx, (.)
Minimizing the total energy
Etotal (ϕ) = Edata(ϕ) + α Eshape(ϕ) = Edata(ϕ) + α d(ϕ, ϕ),
(.)
with a weight α > , induces an additional driving term which aims at maximizing the
similarity of the evolving shape with a given template shape encoded by the function ϕ.
By construction this shape prior is not invariant with respect to certain transformations
such as translation, rotation, and scaling of the shape represented by ϕ.
A common approach to introduce invariance (cf. [, , ]) is to enhance the prior
by a set of explicit parameters to account for translation by μ, rotation by an angle θ, and
scaling by σ of the shape:
d(ϕ, ϕ, μ, θ, σ) = ∫
Ω
(H (ϕ (σRθ(x −μ))) −Hϕ(x))dx.
(.)
This approach to estimate the appropriate transformation parameters has several draw-
backs:
•
Optimization of the shape energy (> .) is done by local gradient descent. In partic-
ular, this implies that one needs to determine an appropriate time step for each parame-
ter, chosen so as to guarantee stability of resulting evolution. In numerical experiments,
we found that balancing these parameters requires a careful tuning process.
•
The optimization of μ, θ, σ, and ϕ is done simultaneously. In practice, however, it is
unclear how to alternate between the updates of the respective parameters. How often
should one iterate one or the other gradient descent equation? In experiments, we found
that the final solution depends on the selected scheme of optimization.
•
The optimal values for the transformation parameters will depend on the embed-
ding function ϕ. An accurate shape gradient should therefore take into account this
dependency. In other words, the gradient of (> .) with respect to ϕ should take
into account how the optimal transformation parameters μ(ϕ), σ(ϕ), and θ(ϕ) vary
with ϕ.


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
Inspired by the normalization for explicit representations introducing in (> .), we
can elimiate these difficulties associated with the local optimization of explicit transfor-
mation parameters by introducing an intrinsic registration process. We will detail this for
the cases of translation and scaling. Extensions to rotation and other transformations are
conceivable but will not be pursued here.
...
Translation Invariance by Intrinsic Alignment
Assume that the template shape represented by ϕis aligned with respect to the shape’s
centroid. Then we define a shape energy by:
Eshape(ϕ) = d(ϕ, ϕ) = ∫
Ω
(Hϕ(x + μϕ) −Hϕ(x))
dx,
(.)
where the function ϕ is evaluated in coordinates relative to its center of gravity μϕ
given by:
μϕ = ∫x hϕ dx,
with hϕ ≡
Hϕ
∫Ω Hϕ dx .
(.)
This intrinsic alignment guarantees that the distance (> .) is invariant to the location
of the shape ϕ. In contrast to the shape energy (> .), we no longer need to iteratively
update an estimate of the location of the object of interest.
...
Translation and Scale Invariance via Alignment
Given a template shape (represented by ϕ) which is normalized with respect to translation
and scaling, one can extend the above approach to a shape energy which is invariant to
translation and scaling:
Eshape(ϕ) = d(ϕ, ϕ) = ∫
Ω
(Hϕ(σϕ x + μϕ) −Hϕ(x))
dx,
(.)
where the level set function ϕ is evaluated in coordinates relative to its center of gravity μϕ
and in units given by its intrinsic scale σϕ defined as:
σϕ = (∫(x −μ)hϕ dx)


,
where hϕ =
Hϕ
∫Ω Hϕ dx .
(.)
In the following, we will show that functional (> .) is invariant with respect to
translation and scaling of the shape represented by ϕ. Let ϕ be a level set function repre-
senting a shape which is centered and normalized such that μϕ = and σϕ = . Let ˜ϕ be an
(arbitrary) level set function encoding the same shape after scaling by σ ∈R and shifting
by μ ∈R:
H ˜ϕ(x) = Hϕ (x −μ
σ
).

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

Indeed, center and intrinsic scale of the transformed shape are given by:
μ ˜ϕ = ∫xH ˜ϕdx
∫H ˜ϕdx = ∫xHϕ ( x−μ
σ ) dx
∫Hϕ ( x−μ
σ ) dx = ∫(σx′ + μ)Hϕ(x′) σdx′
∫Hϕ(x′) σdx′
= σ μϕ + μ = μ,
σ ˜ϕ = ⎛
⎝
∫(x −μ ˜ϕ)H ˜ϕdx
∫H ˜ϕdx
⎞
⎠


= ⎛
⎝
∫(x −μ)Hϕ ( x−μ
σ ) dx
∫Hϕ ( x−μ
σ )dx
⎞
⎠


= (∫(σx′)Hϕ(x′)dx′
∫Hϕ(x′)dx′
)


= σ.
The shape energy (> .) evaluated for ˜ϕ is given by:
Eshape( ˜ϕ) = ∫
Ω
(H ˜ϕ(σ ˜ϕ x + μ ˜ϕ) −Hϕ(x))

dx = ∫
Ω
(H ˜ϕ(σ x + μ) −Hϕ(x))
dx
= ∫
Ω
(Hϕ(x) −Hϕ(x))dx = Eshape(ϕ).
Therefore, the above shape dissimilarity measure is invariant with respect to translation
and scaling.
Note, however, that while this analytical solution guarantees an invariant shape dis-
tance, the transformation parameters μϕ and σϕ are not necessarily the ones which
minimize the shape distance (> .). Extensions of this approach to a larger class of
invariance are conceivable. For example, one could generate invariance with respect to
rotation by rotational alignment with respect to the (oriented) principal axis of the shape
encoded by ϕ. We will not pursue this here.
..
Kernel Density Estimation in the Level Set Domain
In the previous sections, we have introduced a translation and scale invariant shape energy
and demonstrated its effect on the reconstruction of a corrupted version of a single familiar
silhouette the pose of which was unknown. In many practical problems, however, we do
not have the exact silhouette of the object of interest. There may be several reasons for this:
•
The object of interest may be three-dimensional. Rather than trying to reconstruct the
three dimensional object (which generally requires multiple images and the estima-
tion of correspondence), one may learn the two dimensional appearance from a set
of sample views. A meaningful shape dissimilarity measure should then measure the
dissimilarity with respect to this set of projections – see the example in > Fig. -.
•
The object of interest may be one object out of a class of similar objects (the class of cars
or the class of tree leafs). Given a limited number of training shapes sampled from the
class, a useful shape energy should provide the dissimilarity of a particular silhouette
with respect to this class.
•
Even a single object, observed from a single viewpoint, may exhibit strong shape defor-
mation – the deformation of a gesticulating hand or the deformation which a human
silhouette undergoes while walking. In the following, we will assume that one can


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
Selected sample shapes from a set of a walking silhouettes
⊡Fig. -
Density estimated using a kernel density estimator for a projection of silhouettes of a
walking person (see above) onto the ﬁrst three principal components
merely generate a set of stills corresponding to various (randomly sampled) views of
the object of interest for different deformations – see > Fig. -. In the following, we
will demonstrate that – without constructing a dynamical model of the walking pro-
cess – one can exploit this set of sample views in order to improve the segmentation of
a walking person.
In the above cases, the construction of appropriate shape dissimilarity measures
amounts to a problem of density estimation. In the case of explicitly represented bound-
aries, this has been addressed by modeling the space of familiar shapes by linear subspaces
(PCA) [] and the related Gaussian distribution [], by mixture models [] or nonlinear
(multi-modal) representations via simple models in appropriate feature spaces [, ].
For level set based shape representations, it was suggested [, , ] to fit a linear
sub-space to the sampled signed distance functions. Alternatively, it was suggested to rep-
resent familiar shapes by the level set function encoding the mean shape and a (spatially
independent) Gaussian fluctuation at each image location []. These approaches were
shown to capture some shape variability. Yet, they exhibit two limitations: Firstly, they
rely on the assumption of a Gaussian distribution which is not well suited to approximate
shape distributions encoding more complex shape variation. Secondly, they work under
the assumption that shapes are represented by signed distance functions. Yet, the space of
signed distance functions is not a linear space. Therefore, in general, neither the mean nor
the linear combination of a set of signed distance functions will correspond to a signed
distance function.

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

In the following, we will propose an alternative approach to generate a statistical shape
dissimilarity measure for level set based shape representations. It is based on classical
methods of (so-called non-parametric) kernel density estimation and overcomes the above
limitations.
Given a set of training shapes {ϕi}i=...N – such as those shown in
> Fig. -– we
define a probability density on the space of signed distance functions by integrating the
shape distances (> .) or (> .) in a Parzen–Rosenblatt kernel density estimator
[, ]:
P(ϕ) ∝
N
N
∑
i=
exp (−
σ d(Hϕ, Hϕi)).
(.)
The kernel density estimator is among the theoretically most studied density estimation
methods. It was shown (under fairly mild assumptions) to converge to the true distribution
in the limit of infinite samples (and σ →), the asymptotic convergence rate was studied
for different choices of kernel functions.
It should be pointed out that the theory of classical nonparametric density estima-
tion was developed for the case of finite-dimensional data. It is beyond the scope of this
work to develop a general theory of probability distributions and density estimation on
infinite-dimensional spaces (including issues of integrability and measurable sets). For a
general formalism to model probability densities on infinite-dimensional spaces, we refer
the reader to the theory of Gaussian processes []. In our case, an extension to infinite-
dimensional objects such as level set surfaces ϕ : Ω →R could be tackled by considering
discrete (finite-dimensional) approximations {ϕi j ∈R}i=,...,N,j=,...,M of these surfaces
at increasing levels of spatial resolution and studying the limit of infinitesimal grid size
(i.e., N, M →∞). Alternatively, given a finite number of samples, one can apply classical
density estimation techniques efficiently in the finite-dimensional subspace spanned by the
training data [].
Similarly respective metrics on the space of curves give rise to different kinds of gra-
dient descent flows. Recently researchers have developed rather sophisticated metrics to
favor smooth transformations or rigid body motions. We refer the reader to [, ] for
promising advances in this direction. In the following we will typically limit ourselves to
Lgradients.
There exist extensive studies on how to optimally choose the kernel width σ based on
asymptotic expansions such as the parametric method [], heuristic estimates [, ],
or maximum likelihood optimization by cross validation [, ]. We refer to [, ] for
a detailed discussion. For this work, we simply fix σ to be the mean squared nearest-
neighbor distance:
σ = 
N
N
∑
i=
min
j≠i d(Hϕi, Hϕj).
(.)
The intuition behind this choice is that the width of the Gaussians is chosen such that on
the average the next training shape is within one standard deviation.


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
Reverting to kernel density estimation resolves the drawbacks of existing approaches
to shape models for level set segmentation discussed above. In particular:
•
The silhouettes of a rigid D object or a deformable object with few degrees of freedom
can be expected to form fairly low-dimensional manifolds. The kernel density estimator
can capture these without imposing the restrictive assumption of a Gaussian distribu-
tion.
> Figure -shows a D approximation of our method: We simply projected
the embedding functions of silhouettes of a walking person onto the first three
eigenmodes of the distribution. The projected silhouette data and the kernel density
estimate computed in the D subspace indicate that the underlying distribution is not
Gaussian. The estimated distribution (indicated by an isosurface) shows a closed loop
which stems from the fact that the silhouettes were drawn from an essentially periodic
process.
•
Kernel density estimators were shown to converge to the true distribution in the limit
of infinite (independent and identically distributed) training samples [, ]. In the
context of shape representations, this implies that our approach is capable of accurately
representing arbitrarily complex shape deformations.
•
By not imposing a linear subspace, we circumvent the problem that the space of shapes
(and signed distance functions) is not a linear space. In other words: Kernel density
estimation allows to estimate distributions on non-linear (curved) manifolds. In the
limit of infinite samples and kernel width σ going to zero, the estimated distribution is
more and more constrained to the manifold defined by the shapes.
..
Gradient Descent Evolution for the Kernel Density
Estimator
In the following, we will detail how the statistical distribution (> .) can be used to
enhance level set based segmentation process. As for the case of parametric curves, we for-
mulate level set segmentation as a problem of Bayesian inference, where the segmentation
is obtained by maximizing the conditional probability:
P(ϕ ∣I) = P(I ∣ϕ) P(ϕ)
P(I)
,
(.)
with respect to the level set function ϕ, given the input image I. For a given image, this
is equivalent to minimizing the negative log-likelihood which is given by a sum of two
energies:
E(ϕ) = Edata(ϕ) + Eshape(ϕ),
(.)
with
Eshape(ϕ) = −logP(ϕ).
(.)
Minimizing the energy (> .) generates a segmentation process which simultane-
ously aims at maximizing intensity homogeneity in the separated phases and a similarity
of the evolving shape with respect to all the training shapes encoded through the statistical
estimator (> .).

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

Gradient descent with respect to the embedding function amounts to the evolution:
∂ϕ
∂t = −
α
∂Edata
∂ϕ
−∂Eshape
∂ϕ
,
(.)
where the knowledge-driven component is given by:
∂Eshape
∂ϕ
=
∑αi ∂
∂ϕ d(Hϕ, Hϕi)
σ ∑αi
,
(.)
which simply induces a force in direction of each training shape ϕi weighted by the factor:
αi = exp (−
σ d(Hϕ, Hϕi)),
(.)
which decays exponentially with the distance from the training shape ϕi.
..
Nonlinear Shape Priors for Tracking a Walking
Person
In the following, we apply the above shape prior to the segmentation of a partially occluded
walking person. To this end, a sequence of a walking figure was partially occluded by an
artificial bar. Subsequently we minimized energy (> .), segmenting each frame of the
sequence using the previous segmentation as initialization.
> Figure -shows that this
⊡Fig. -
Purely intensity-based segmentation. Various frames show the segmentation of a partially
occluded walking person generated by minimizing the Chan-Vese energy (> .). The
walking person cannot be separated from the occlusion and darker areas of the background
such as the person’s shadow


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
purely image-driven segmentation scheme is not capable of separating the object of inter-
est from the occluding bar and similarly shaded background regions such as the object’s
shadow on the floor.
In a second experiment, we manually binarized the images corresponding to the first
half of the original sequence (frames through ) and aligned them to their respective
center of gravity to obtain a set of training shape – see
> Fig. -. Then we ran the seg-
mentation process (> .) with the shape prior (> .). Apart from adding the shape
prior we kept the other parameters constant for comparability.
> Figure -shows several frames from this knowledge-driven segmentation. A
comparison to the corresponding frames in > Fig. -demonstrates several properties:
•
The shape prior permits to accurately reconstruct an entire set of fairly different shapes.
Since the shape prior is defined on the level set function ϕ – rather than on the
boundary C (cf. []) – it can easily handle changing topology.
•
The shape prior is invariant to translation such that the object silhouette can be
reconstructed in arbitrary locations of the image.
⊡Fig. -
Segmentation with nonparametric invariant shape prior. Segmentation generated by
minimizing energy (> .) combining intensity information with the shape prior (> .).
For every frame in the sequence, the gradient descent equation was iterated (with ﬁxed
parameters), using the previous segmentation as initialization. The shape prior permits to
separate the walking person from the occlusion and darker areas of the background such as
the shadow. The shapes in the second half of the sequence were not part of the training set

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

•
The statistical nature of the prior allows to also reconstruct silhouettes which were not
part of the training set – corresponding to the second half of the images shown (beyond
frame ).
.
Dynamical Shape Priors for Implicit Shapes
..
Capturing the Temporal Evolution of Shape
In the above works, statistically learned shape information was shown to cope for miss-
ing or misleading information in the input images due to noise, clutter, and occlusion.
The shape priors were developed to segment objects of familiar shape in a given image.
Although they can be applied to tracking objects in image sequences, they are not well-
suited for this task, because they neglect the temporal coherence of silhouettes which
characterizes many deforming shapes.
When tracking a deformable object over time, clearly not all shapes are equally likely at
a given time instance. Regularly sampled images of a walking person, for example, exhibit
a typical pattern of consecutive silhouettes. Similarly, the projections of a rigid D object
rotating at constant speed are generally not independent samples from a statistical shape
distribution. Instead, the resulting set of silhouettes can be expected to contain strong
temporal correlations.
In the following, we will present temporal statistical shape models for implicitly rep-
resented shapes that were first introduced in []. In particular, the shape probability at a
given time depends on the shapes observed at previous time steps. The integration of such
dynamical shape models into the segmentation process can be elegantly formulated within
a Bayesian framework for level set based image sequence segmentation. The resulting opti-
mization by gradient descent induces an evolution of the level set function which is driven
both by the intensity information of the current image as well as by a dynamical shape
prior which relies on the segmentations obtained on the preceding frames. Experimental
evaluation demonstrates that the resulting segmentations are not only similar to previ-
ously learned shapes, but they are also consistent with the temporal correlations estimated
from sample sequences. The resulting segmentation process can cope with large amounts
of noise and occlusion because it exploits prior knowledge about temporal shape consis-
tency and because it aggregates information from the input images over time (rather than
treating each image independently).
..
Level Set Based Tracking via Bayesian Inference
Statistical models can be estimated more reliably if the dimensionality of the model and the
data are low. We will therefore cast the Bayesian inference in a low-dimensional formula-
tion within the subspace spanned by the largest principal eigenmodes of a set of sample


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
shapes. We exploit the training sequence in a twofold way: Firstly, it serves to define a low-
dimensional subspace in which to perform estimation. And secondly, within this subspace
we use it to learn dynamical models for implicit shapes. For static shape priors this concept
was already used in [].
Let {ϕ, . . . , ϕN} be a temporal sequence of training shapes. (We assume that all
training shapes ϕi are signed distance functions. Yet an arbitrary linear combination of
eigenmodes will in general not generate a signed distance function. While the discussed
statistical shape models favor shapes which are close to the training shapes (and therefore
close to the set of signed distance functions), not all shapes sampled in the considered sub-
space will correspond to signed distance functions.) Let ϕdenote the mean shape and
ψ, . . . ,ψn the n largest eigenmodes with n ≪N. We will then approximate each training
shape as:
ϕi(x) = ϕ(x) +
n
∑
j=
αi j ψj(x),
(.)
where
αi j = ⟨ϕi −ϕ,ψ j⟩≡∫(ϕi −ϕ)ψj dx.
(.)
Such PCA based representations of level set functions have been successfully applied for
the construction of statistical shape priors in [, , , ]. In the following, we will
denote the vector of the first n eigenmodes as ψ = (ψ, . . .,ψn). Each sample shape ϕi is
therefore approximated by the n-dimensional shape vector αi = (αi, . . . , αin). Similarly,
an arbitrary shape ϕ can be approximated by a shape vector of the form:
αϕ = ⟨ϕ −ϕ,ψ⟩.
(.)
In addition to the deformation parameters α, we introduce transformation parameters
θ, and we introduce the notation:
ϕα,θ(x) = ϕ(Tθx) + α⊺ψ(Tθx),
(.)
to denote the embedding function of a shape generated with deformation parameters α
and transformed with parameters θ. The transformations Tθ can be translation, rotation,
and scaling (depending on the application).
With this notation, the goal of image sequence segmentation within this subspace can
be stated as follows: Given consecutive images It : Ω →R from an image sequence, and
given the segmentations ˆα:t−and transformations ˆθ:t−obtained on the previous images
I:t−, compute the most likely deformation ˆαtand transformation ˆθt by maximizing the
conditional probability:
P(αt, θt ∣It, ˆα:t−, ˆθ:t−) = P(It ∣αt, θt) P(αt, θt ∣ˆα:t−, ˆθ:t−)
P(It ∣ˆα:t−, ˆθ:t−)
.
(.)
The key challenge, addressed in the following, is to model the conditional probability:
P(αt, θt ∣ˆα:t−, ˆθ:t−),
(.)

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

which constitutes the probability for observing a particular shape αt and a particu-
lar transformation θt at time t, conditioned on the parameter estimates for shape and
transformation obtained on previous images.
..
Linear Dynamical Models for Implicit Shapes
For realistic deformable objects, one can expect the deformation parameters αt and the
transformation parameters θt to be tightly coupled. Yet, we want to learn dynamical shape
models which are invariant to the absolute translation, rotation, etc. To this end, we can
make use of the fact that the transformations form a group which implies that the trans-
formation θt at time t can be obtained from the previous transformation θt−by applying
an incremental transformation △θt: Tθ tx = T△θ tTθ t−x. Instead of learning models of the
absolute transformation θt, we can simply learn models of the update transformations △θt
(e.g., the changes in translation and rotation). By construction, such models are invariant
with respect to the global pose or location of the modeled shape.
To jointly model transformation and deformation, we simply obtain for each train-
ing shape in the learning sequence the deformation parameters αi and the transformation
changes △θi, and define the extended shape vector:
βt := ( αt
△θt
).
(.)
We will then impose a linear dynamical model of order k to approximate the temporal
evolution of the extended shape vector:
P(βt ∣ˆβ:t−) ∝exp (−
v⊺Σ−v),
(.)
where
v ≡βt −µ −Aˆβt−−Aˆβt−. . . −Ak ˆβt−k.
(.)
Various methods have been proposed in the literature to estimate the model parameters
given by the mean µ and the transition and noise matrices A, . . . , Ak, Σ. We applied a
stepwise least squares algorithm proposed in []. Using dynamical models up to an order
of , we found that according to Schwarz’s Bayesian criterion [], our training sequences
were best approximated by an autoregressive model of second order (k = ).
> Figure -shows a sequence of statistically synthesized embedding functions and
the induced contours given by the zero level line of the respective surfaces – for easier
visualization, the transformational degrees are neglected. In particular, the implicit repre-
sentation allows to synthesize shapes of varying topology. The silhouette on the bottom left
of > Fig. -, for example, consists of two contours.


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
⊡Fig. -
Synthesis of implicit dynamical shapes. Statistically generated embedding surfaces
obtained by sampling from a second order autoregressive model, and the contours given by
the zero level lines of the synthesized surfaces. The implicit representation allows the
embedded contour to change topology (bottom left image)
..
Variational Segmentation with Dynamical
Shape Priors
Given an image It from an image sequence and given a set of previously segmented shapes
with shape parameters ˆα:t−and transformation parameters ˆθ:t−, the goal of tracking is
to maximize the conditional probability (> .) with respect to shape αt and transfor-
mation θt. This can be performed by minimizing its negative logarithm, which is – up to a
constant – given by an energy of the form:
E(αt, θt) = Edata(αt, θt) + Eshape(αt, θt).
(.)
For the data term we use the model in (> .) with independent intensity variances:
Edata(αt, θt) = ∫((It−μ)
σ 

+log σ) Hϕαt,θ t + ((It−μ)
σ 

+log σ)(−Hϕαt,θ t) dx.
(.)
Using the autoregressive model (> .), the shape energy is given by:
Eshape(αt, θt) = 
v⊺Σ−v,
(.)
with v defined in (> .).
The total energy (> .) is easily minimized by gradient descent. For details we refer
to [].

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

>Figure -shows images from a sequence that was degraded by increasing amounts
of noise.
> Figure -shows segmentation results obtained by minimizing (> .) as pre-
sented above. Despite prominent amounts of noise, the segmentation process provides
reliable segmentations where human observers fail.
>Figure -shows the segmentation of an image sequence showing a walking person
that was corrupted by noise and an occlusion which completely covers the walking person
for several frames. The dynamical shape prior allows for reliable segmentations despite
noise and occlusion. For more details and quantitative evaluations we refer to [].
25% noise
50% noise
75% noise
90% noise
⊡Fig. -
Images from a sequence with increasing amount of noise
Segmentation results for 75% noise
Segmentation results for 90% noise
⊡Fig. -
Variational image sequence segmentation with a dynamical shape prior for various
amounts of noise. % noise means that nine out of ten intensity values were replaced by a
random intensity from a uniform distribution. The statistically learned dynamical model
allows for reliable segmentation results despite prominent amounts of noise
⊡Fig. -
Tracking in the presence of occlusion. The dynamical shape prior allows to reliably segment
the walking person despite noise and occlusion


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
.
Parametric Representations Revisited:
Combinatorial Solutions for Segmentation with
Shape Priors
In previous sections we saw that shape priors allow to improve the segmentation and
tracking of familiar deformable objects, biasing the segmentation process to favor famil-
iar shapes or even familiar shape evolution. Unfortunately, these approaches are based
on locally minimizing respective energies via gradient descent. Since these energies
are generally non-convex, respective solutions are bound to be locally optimal only.
As a consequence, they depend on an initialization and are likely to be suboptimal
in practice. One exception based on implicit shape representations as binary indicator
functions and convex relaxation techniques was proposed in []. Yet, the linear interpo-
lation of shapes represented by binary indicator functions does not give rise to plausible
intermediate shapes such that respective algorithms require a large number of training
shapes.
Moreover, while implicit representations like the level set method circumvent the
problem of computing correspondences between points on either of two shapes, it is well-
known that the aspect of point correspondences plays a vital role in human notions of shape
similarity. For matching planar shapes there is abundant literature on how to solve the aris-
ing correspondence problem in polynomial time using dynamic programming techniques
[, , ].
Similar concepts of dynamic programming can be employed to localize deformed
template curves in images. Coughlan et al. [] detected open boundaries by shortest
path algorithms in higher-dimensional graphs. And Felzenszwalb et al. used dynamic
programming in chordal graphs to localize shapes, albeit not on a pixel level.
Polynomial-time solutions for localizing deformable closed template curves in images
using minimum ratio cycles or shortest circular paths were proposed in [], with a further
generalization presented in []. There the problem of determining a segmentation of an
image I : Ω →R that is elastically similar to an observed template cc : S→Rby
computing minimum ratio cycles
Γ : S→Ω × S
(.)
in the product space spanned by the image domain Ω and template domain S. See
> Fig. -for a schematic visualization. All points along this circular path provide a pair
of corresponding template point and image pixel. In this manner, the matching of template
points to image pixels is equivalent to the estimation of orientation-preserving cyclic paths,
which can be solved in polynomial time using dynamic programming techniques such as
ratio cycles [] or shortest circular paths [].
> Figure -shows an example result obtained with this approach: The algorithm
determines a deformed version (right) of a template curve (left) in an image (center) in
globally optimal manner. An initialization is no longer required and the best conceivable
solution is determined in polynomial time.

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

⊡Fig. -
A polynomial-time solution for matching shapes to images: Matching a template curve
C : S→R(left) to the image plane Ω ⊂Ris equivalent to computing an
orientation-preserving cyclic path Γ : S→Ω × S(blue curve) in the product space spanned
by the image domain and the template domain. The latter problem can be solved in
polynomial time – see [] for details
Optimal segmentation
Close-up of input image
Template curve
⊡Fig. -
Segmentation with a single template: despite signiﬁcant deformation and translation, the
initial template curve (red) is accurately matched to the low-contrast input image. The
globally optimal correspondence between template points and image pixels is computed in
polynomial time by dynamic programming techniques []
> Figure -shows further examples of tracking objects: Over long sequences
of hundreds of frames, the objects of interest are tracked reliably – despite low con-
trast, camera shake, bad visibility, and illumination changes. For further details we refer
to [].


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
Frame 1
Frame 10
Frame 80
Frame 110
Frame 1
Frame 100
Frame 125
Frame 200
⊡Fig. -
Tracking of various objects in challenging real-world sequences. []. Despite bad visibility,
camera shake, and substantial lighting changes, the polynomial-time algorithm allows to
reliably track objects over hundreds of frames. Image data taken from []
.
Conclusion
In the previous sections, we have discussed various ways to impose statistical shape priors
into image segmentation methods. We have made several observations:
•
By imposing statistically learnt shape information one can generate segmentation pro-
cesses which favor the emergence of familiar shapes – where familiarity is based on one
or several training shapes.
•
Statistical shape information can be elegantly combined with the input image data in the
framework of Bayesian maximum a posteriori estimation. Maximizing the posterior
distribution is equivalent to minimizing a sum of two energies representing the data
term and the shape prior. A further generalization allows to impose dynamical shape
priors so as to favor familiar deformations of shape in image sequence segmentation.
•
While linear Gaussian shape priors are quite popular, the silhouettes of typical objects in
our environment are generally not Gaussian distributed. In contrast to linear Gaussian
priors, nonlinear statistical shape priors based on Parzen–Rosenblatt kernel density
estimators or based on Gaussian distributions in appropriate feature spaces [] allow
to encode a large variety of rather distinct shapes in a single shape energy.
•
Shapes can be represented explicitly (as points on the object’s boundary or surface)
or implicitly (as the indicator function of the interior of the object). They can be
represented in a spatially discrete or a spatially continuous setting.
•
The choice of shape representation has important consequences regarding the question
which optimization algorithms are employed and whether respective energies can be

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

minimized locally or globally. Moreover, different shape representations give rise to dif-
ferent notions of shape similarity and shape interpolation. As a result, there is no single
ideal representation of shape. Ultimately one may favor hybrid representations such
as the one proposed in []. It combines explicit and implicit representations allowing
cost functions which represent properties of both the object’s interior and its boundary.
Subsequent LP relaxation provides minimizers of bounded optimality.
References and Further Reading
. Amini AA, Weymouth TE, Jain RC () Using
dynamic programming for solving variational
problems in vision. IEEE Trans Pattern Anal
Mach Intell ():–
. Awate SP, Tasdizen T, Whitaker RT ()
Unsupervised texture segmentation with non-
parametric neighborhood statistics. In: Euro-
pean conference on computer vision (ECCV).
Springer, Graz, pp –
. Blake A, Isard M () Active contours.
Springer, London
. Blake A, Zisserman A () Visual reconstruc-
tion. MIT Press, Cambridge
. Bookstein FL () The measurement of bio-
logical shape and shape change, vol of lecture
notes in Biomath. Springer, New York
. Boykov Y, Kolmogorov V () Computing
geodesics and minimal surfaces via graph cuts.
In: IEEE international conference on computer
vision, Nice, pp –
. Boykov Y,Kolmogorov V () Anexperimen-
tal comparison of min-cut/max-ow algorithms
for energy minimization in vision. IEEE Trans
Pattern Anal Mach Intell ():–
. Brox T, Rousson M, Deriche R, Weickert J
() Unsupervised segmentation incorporat-
ing colour, texture, and motion. In: Petkov N,
Westenberg MA (eds) Computer analysis of
imagesandpatterns,vol of LNCS.Springer,
Groningen, pp –
. Brox T, Weickert J () A TV flow based
local scale measure for texture discrimination.
In: Pajdla T, Hlavac V (eds) European confer-
ence on computer vision, vol of LNCS.
Springer, Prague, pp –
. Caselles V, Kimmel R, Sapiro G () Geodesic
active contours. In: Proceedings of the IEEE
International Conference on Computer Vision,
Boston, pp –
. Chan T, Esedoglu S, Nikolova M ()
Algorithms for finding global minimizers of
image segmentation and denoising models.
SIAM Journal on Applied Mathematics ():
–
. Chan T and Zhu W () Level set based shape
prior segmentation. Technical report -,
Computational Applied Mathematics, UCLA,
Los Angeles
. Chan TF, Vese LA () Active contours with-
out edges. IEEE Trans Image Process ():
–
. Chan TF, Vese LA () A level set algorithm
for minimizing the Mumford–Shah functional
in image processing. In: IEEE workshop on
variational and level set methods, Vancouver,
pp –
. Charpiat G, Faugeras O, Keriven R ()
Approximations of shape metrics and appli-
cation to shape warping and empirical shape
statistics. J Found Comput Math (): –
. Charpiat G, Faugeras O, Pons J-P, Keriven R
() Generalized gradients: priors on min-
imization flows. Int J Comput Vision ():
–
. Chen Y, Tagare H, Thiruvenkadam S, Huang F,
Wilson D, Gopinath KS, Briggs RW, Geiser E
() Using shape priors in geometric active
contours in a variational framework. Int J Com-
put Vision ():–
. Chow YS, Geman S, Wu LD () Consis-
tent cross-validated density estimation. Ann
Stat :–
. Cipolla R, Blake A () The dynamic analy-
sis of apparent contours. In: IEEE international


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
conference
on
computer
vision.
Springer,
Osaka, pp –
. Cohen L, Kimmel R () Global minimum
for active contour models: a minimal path
approach. Int J Comput Vision ():–
. Cootes TF, Taylor CJ, Cooper DM, Graham J
() Active shape models – their training
and application. Computer Vision and Image
Understanding ():–
. Cootes TF, Taylor CJ () A mixture model for
representing shape variation. Image and Vision
Computing ():–
. Coughlan J, Yuille A, English C, Snow D
() Efficient deformable template detection
and localization without user initialization.
Computer Vision and Image Understanding
():–
. Courant R, Hilbert D () Methods of mathe-
matical physics, vol . Interscience, New York
. Cremers D () Statistical shape knowledge
in variational image segmentation. PhD the-
sis, Department of Mathematics and Computer
Science, University of Mannheim, Germany
. Cremers D () Dynamical statistical shape
priors for level set based tracking. IEEE Trans
Pattern Anal Mach Intell ():–
. Cremers D, Kohlberger T, Schnörr C ()
Nonlinear shape statistics in Mumford–Shah
based segmentation. In: Heyden A et al (eds)
European conference
on computer
vision,
vol of LNCS. Springer, Copenhagen,
pp –
. Cremers D, Kohlberger T, Schnörr C ()
Shape statistics in kernel space for varia-
tional image segmentation. Pattern Recognition
():–
. Cremers D, Osher SJ, Soatto S () Kernel
density estimation and intrinsic alignment for
shape priors in level set segmentation. Int J
Comput Vision ():–
. Cremers D, Rousson M, Deriche R () A
review of statistical approaches to level set seg-
mentation: integrating color, texture, motion
and shape. Int J Comput Vision ():–
. Cremers D, Schmidt FR, Barthel F ()
Shape priors in variational image segmentation:
Convexity, lipschitz continuity and globally
optimal solutions. In: IEEE conference on com-
puter vision and pattern recognition (CVPR),
Anchorage
. Cremers D, Soatto S () A pseudo-distance
for shape priors in level set segmentation. In:
Paragios N (ed) IEEE nd international work-
shop on variational, geometric and level set
methods, Nice, pp –
. Cremers D, Soatto S () Motion compe-
tition: a variational framework for piecewise
parametric motion segmentation. Int J Comput
Vision ():–
. Cremers D, Sochen N, Schnörr C () A
multiphase dynamic labeling model for varia-
tional recognition-driven image segmentation.
In: Pajdla T, Hlavac V (eds) European confer-
ence on computer vision, vol of LNCS.
Springer, pp –
. Cremers D, Sochen N, Schnörr C () A mul-
tiphase dynamic labeling model for variational
recognition-driven image segmentation. Int J
Comput Vision ():–
. Cremers D, Tischhäuser F, Weickert J, Schnörr
C () Diffusion snakes: introducing statis-
tical shape knowledge into the Mumford–Shah
functional. Int J Comput Vision ():–
. Deheuvels
P
()
Estimation
non
paramétrique de la densité par histogrammes
généralisés. Revue de Statistique Appliquée
:–
. Delingette H, Montagnat J () New algo-
rithms for controlling active contours shape and
topology. In: Vernon D (ed) Proceedings of
the European conference on computer vision,
vol of LNCS. Springer, pp –
. Dervieux A, Thomasset F () A finite ele-
ment method for the simulation of Raleigh–
Taylor instability. Springer Lect Notes Math :
–
. Devroye L, Györfi L () Nonparametric den-
sity estimation: the Lview. Wiley, New York
. Dryden IL, Mardia KV () Statistical shape
analysis. Wiley, Chichester
. Duin RPW () On the choice of smoothing
parameters for Parzen estimators of probabil-
ity density functions. IEEE Trans Comput :
–
. Farin
G
()
Curves
and
surfaces
for
computer–aided geometric design. Academic,
San Diego
. Franchini E,Morigi S,Sgallari F() Segmen-
tation of D tubular structures by a PDE-based
anisotropic diffusion model. In: International

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

conference on scale space and variational meth-
ods, vol of LNCS. Springer, pp –
. Fréchet M () Les courbes aléatoires. Bull Int
Stat Inst :–
. Fundana K, Overgaard NC, Heyden A ()
Variational segmentation of image sequences
using
region-based
active
contours
and
deformable shape priors. Int J Comput Vision
():–
. Gdalyahu Y, Weinshall D () Flexible syn-
tactic matching of curves and its application
to automatic hierarchical classication of sil-
houettes. IEEE Trans Pattern Anal Mach Intell
():–
. Geiger D, Gupta A, Costa LA, Vlontzos J ()
Dynamic programming for detecting, tracking
and matching deformable contours. IEEE Trans
Pattern Anal Mach Intell ():–
. Greig DM, Porteous BT, Seheult AH ()
Exact maximum a posteriori estimation for
binary images. J Roy Stat Soc B ():–
. Grenander U, Chow Y, Keenan DM ()
Hands: a pattern theoretic study of biological
shapes. Springer, New York
. Heiler M, Schnörr C () Natural image
statistics for natural image segmentation. In:
IEEE international conference on computer
vision, Nice, pp –
. Ising E () Beitrag zur Theorie des Ferromag-
netismus. Zeitschrift für Physik :–
. Kass M, Witkin A, Terzopoulos D () Snakes:
active contour models. Int J Comput Vision
():–
. Kendall DG () The diffusion of shape. Adv
Appl Probab :–
. Kervrann
C,
Heitz
F
()
Statistical
deformable
model-based
segmentation
of
image motion. IEEE Trans Image Process
:–
. Kichenassamy
S,
Kumar
A,
Olver
PJ,
Tannenbaum A, Yezzi AJ () Gradient ows
and geometric active contour models. In: IEEE
international conference on computer vision, pp
–
. Kim J, Fisher JW, Yezzi A, Cetin M, Willsky A
() Nonparametric methods for image seg-
mentation using information theory and curve
evolution.In:International conferenceonimage
processing, vol . Rochester, pp –
. Kohlberger
T,
Cremers
D,
Rousson
M,
Ramaraj R () D shape priors for level set
segmentation of the left myocardium in SPECT
sequences. In: Medical image computing and
computer assisted intervention, vol of
LNCS. Springer, Heidelberg, pp –
. Kolev K, Klodt M, Brox T, Cremers D ()
Continuous global optimization in multview D
reconstruction. International Journal of Com-
puter Vision :–
. Lachaud J-O, Montanvert A () Deformable
meshes
with
automated
topology changes
for
coarse-to-fine
three-dimensional
sur-
face
extraction.
Medical
Image
Analysis
():–
. Leitner F, Cinquin P () Complex topol-
ogy D objects segmentation. In: SPIE confer-
ence on advances in intelligent robotics systems,
vol . Boston
. Leventon M, Grimson W, Faugeras O ()
Statistical shape inuence in geodesic active con-
tours. In: International conference on computer
vision and pattern recognition, vol . Hilton
Head Island, pp –
. Malladi R, Sethian JA, Vemuri BC () Shape
modeling with front propagation: a level set
approach. IEEE Trans Pattern Anal Mach Intell
():–
. Matheron G () Random sets and integral
geometry. Wiley, New York
. McInerney T, Terzopoulos D () Topologi-
callyadaptablesnakes.In:Proceedings of theth
international conference on computer vision.
IEEE Computer Society Press, Los Alamitos,
–June , pp –
. Menet S, Saint-Marc P, Medioni G ()
B–snakes: implementation and application to
stereo. In: Proceedings of the DARPA image
understanding workshop, Pittsburgh, –April
, pp –
. Mercer J () Functions of positive and neg-
ative type and their connection with the theory
of integral equations. Philos Trans R Soc Lond
A :–
. Moghaddam B, Pentland A () Probabilistic
visual learning for object representation. IEEE
Trans Pattern Anal Mach Intell ():–
. Mumford D, Shah J () Optimal approx-
imations by piecewise smooth functions and


Image Segmentation with Shape Priors: Explicit Versus Implicit Representations
associated variational problems. Commun Pure
Appl Math :–
. Nain D, Yezzi A, Turk G () Vessel segmen-
tation using a shape driven flow. In: MICCAI.
pp –
. Neumaier A, Schneider T () Estimation
of parameters and eigenmodes of multivariate
autoregressive models. ACM Trans Math Softw
():–
. Osher SJ, Sethian JA () Fronts propaga-
tion with curvature dependent speed: algo-
rithms based on Hamilton–Jacobi formulations.
J Comput Phys :–
. Paragios N, Deriche R () Geodesic active
regions and level set methods for supervised
texture segmentation. Int J Comput Vision
():–
. Parent P, Zucker SW () Trace inference, cur-
vature consistency, and curve detection. IEEE
Trans Pattern Anal Mach Intell ():–
. Parzen E () On the estimation of a proba-
bility density function and the mode. Ann Math
Stat :–
. Rasmussen C-E, Williams CKI () Gaus-
sian processes for machine learning. MIT Press,
Cambridge
. Riklin-Raviv T, Kiryati N, Sochen N ()
Unlevel sets: geometry and prior-based segmen-
tation. In: Pajdla T, Hlavac V (eds) European
conference on computer vision, vol of
LNCS. Springer, Prague, pp –
. Rochery M, Jermyn I, Zerubia J () Higher
order active contours. Int J Comput Vision
:–
. Rosenblatt F () Remarks on some nonpara-
metric estimates of a density function. Annof
Math Stat :–
. Rousson M, Brox T, Deriche R () Active
unsupervised texture segmentation on a diffu-
sion based feature space. In: Proceedings of the
IEEE conference on computer vision and pat-
tern recognition, Madison, pp –
. Rousson M, Cremers D () Efficient kernel
density estimation of shape and intensity priors
for level set segmentation. In: MICCAI, vol ,
pp –
. Rousson M, Paragios N () Shape priors for
level set representations. In: Heyden A et al (eds)
European conference on computer vision, vol
of LNCS. Springer, pp –
. Rousson M, Paragios N, Deriche R ()
Implicit active shape models for D segmenta-
tion in MRI imaging. In: MICCAI, vol of
LNCS. Springer, pp –
. Rosenfeld A, Zucker SW, Hummel RA ()
An application of relaxation labeling to line
and curve enhancement. IEEE Trans Comput
():–
. Schmidt FR, Farin D, Cremers D () Fast
matching of planar shapes in sub-cubic runtime.
In: IEEE international conference on computer
vision, Rio de Janeiro
. Schoenemann T, Cremers D () Globally
optimal image segmentation with an elastic
shape prior. In: IEEE international conference
on computer vision, Rio de Janeiro
. Schoenemann T, Cremers D () Introducing
curvature into globally optimal image segmen-
tation:minimumratio cycleson product graphs.
In: IEEE international conference on computer
vision, Rio de Janeiro
. Schoenemann T, Cremers D () Matching
non-rigidly deformable shapes across images:
a globally optimal solution. In: IEEE confer-
ence on computer vision and pattern recogni-
tion (CVPR), Anchorage
. Schoenemann T, Cremers D () A com-
binatorial solution for model-based image
segmentation and real-time tracking IEEE
Trans. Pattern Anal and Mach Intell ():
–
. Schoenemann T, Kahl F, Cremers D ()
Curvature regularity for region-based image
segmentation and inpainting: a linear program-
ming relaxation. In: IEEE international confer-
ence on computer vision, Kyoto
. Schoenemann T, Schmidt FR, Cremers D ()
Image segmentation with elastic shape priors via
global geodesics in product spaces. In: British
machine vision conference, Leeds
. Schwarz G () Estimating the dimension of a
model. Ann Stat :–
. Sebastian T, Klein P, Kimia B () On align-
ing curves. IEEE Trans Pattern Anal Mach Intell
():–
. Serra J () Image analysis and mathematical
morophology. Academic, London
. Silverman BW () Choosing the window
width when estimating a density. Biometrika
:–

Image Segmentation with Shape Priors: Explicit Versus Implicit Representations 

. Silverman BW () Density estimation for
statistics and data analysis. Chapman and Hall,
London
. Sundaramoorthi G,Yezzi A,Mennucci A,Sapiro
G () New possibilities with sobolev active
contours. Int J Comput Vision ():–
. Sussman M, Smereka P, Osher SJ () A level
set approach for computing solutions to incom-
pressible twophase flow. J Comput Phys :
–
. Tsai A, Wells W, Warfield SK, Willsky A ()
Level set methods in an EM framework for
shape classification and estimation. In: MICCAI
. Tsai A, Yezzi A, Wells W, Tempany C, Tucker D,
Fan A, Grimson E, Willsky A () Model–
based curve evolution technique for image seg-
mentation. In: IEEE conference on computer
vision pattern recognition, Kauai, pp –
. Tsai A, Yezzi AJ, Willsky AS () Curve evo-
lution implementation of the Mumford–Shah
functional for image segmentation, denoising,
interpolation, and magnification. IEEE Trans
Image Process ():–
. Unal G, Krim H, Yezzi AY () Information-
theoretic active polygons for unsupervised
texture segmentation. Int J Comput Vision
():–
. Unger M, Pock T, Cremers D, Bischof H ()
TVSeg – interactive total variation based image
segmentation.In:Britishmachinevisionconfer-
ence (BMVC), Leeds
. Wagner TJ () Nonparametric estimates of
probability densities. IEEE Trans Inf Theory
:–
. Zhu SC, Yuille A () Region competi-
tion: unifying snakes, region growing, and
Bayes/MDL for multiband image segmentation.
IEEE Trans Pattern Anal Mach Intell ():
–


Starlet Transform in
Astronomical Data
Processing
Jean-Luc Starck ⋅Fionn Murtagh ⋅Mario Bertero
.
Introduction.....................................................................
..
Source Detection.......................................................................
.
Standard Approaches to Source Detection....................................
..
The Traditional Data Model..........................................................
..
PSF Estimation.........................................................................
..
Background Estimation...............................................................
..
Convolution.............................................................................
..
Detection................................................................................
..
Deblending/Merging..................................................................
..
Photometry and Classification.......................................................
...Photometry.............................................................................
...Star–Galaxy Separation...............................................................
...Galaxy Morphology Classification..................................................
.
Mathematical Modeling........................................................
..
Sparsity Data Model...................................................................
..
The Starlet Transform.................................................................
..
The Starlet Reconstruction............................................................
..
Starlet Transform: Second Generation..............................................
..
Sparse Modeling of Astronomical Images..........................................
...Selection of Significant Coefficients Through Noise Modeling.................
..
Sparse Positive Decomposition......................................................
...Example : Sparse positive decomposition of NGC.........................
...Example : Sparse positive starlet decomposition of a simulated image.......
.
Source Detection Using a Sparsity Model......................................
..
Detection Through Wavelet Denoising..............................................
..
The Multiscale Vision Model.........................................................
...Introduction.............................................................................
...Multiscale Vision Model Definition.................................................
...From Wavelet Coefficients to Object Identification...............................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Starlet Transform in Astronomical Data Processing
...Source Reconstruction.................................................................
..
Examples.................................................................................
...Band Extraction.........................................................................
...Star Extraction in NGC..........................................................
...Galaxy Nucleus Extraction............................................................
.
Deconvolution...................................................................
..
Statistical Approach to Deconvolution..............................................
..
The Richardson–Lucy Algorithm....................................................
..
Deconvolution with a Sparsity Prior................................................
...Constraints in the Object or Image Domains......................................
...Example..................................................................................
..
Detection and Deconvolution........................................................
...Object Reconstruction Using the PSF...............................................
...The Algorithm..........................................................................
...Space-Variant PSF......................................................................
...Undersampled Point Spread Function..............................................
...Example: Application to Abell ISOCAM Data..............................
.
Conclusion......................................................................
.
Cross-References................................................................

Starlet Transform in Astronomical Data Processing 

Abstract: We begin with traditional source detection algorithms in astronomy. We then
introduce the sparsity data model. The starlet wavelet transform serves as our main focus in
this chapter. Sparse modeling, and noise modeling, are described. Applications to object
detection and characterization, and to image filtering and deconvolution, are discussed.
The multiscale vision model is a further development of this work, which can allow for
image reconstruction when the point spread function is not known, or not known well.
Bayesian and other algorithms are described for image restoration. A range of examples is
used to illustrate the algorithms.
.
Introduction
Data analysis is becoming more and more important in astronomy. This can be explained
by detector evolution, which concerns all wavelengths. In the s, charge coupled device
(CCD) images had a typical size of × pixels, while astronomers now have CCD
mosaics with ,× ,pixels or even more. At the same time, methods of analy-
sis have become much more complex, and the human and financial efforts to create and
process the data can sometimes be of the same order as for the construction of the instru-
ment itself. As an example, for the ISOCAM camera of the Infrared Space Observatory
(ISO), the command software of the instrument, and the online and offline data processing,
required altogether person years of development, while person years were necessary
for the construction of the camera. The data analysis effort for the PLANCK project is even
larger. Furthermore, the quantity of outputs requires the use of databases, and in parallel,
sophisticated tools are needed to extract ancillary astrophysical information, generally now
through the web. From the current knowledge, new questions emerge, and it is necessary
to proceed to new observations of a given object or a part of the sky. The acquired data
need to be calibrated prior to useful information for the scientific project being extracted.
Data analysis acts during the calibration, the scientific information extraction process,
and the database manipulation. The calibration phase consists of correcting various instru-
mental effects, such as the dark current (i.e., in the absence of any light, the camera does
not return zero values, and the measured image is called the dark image, and needs to
be subtracted from any observation), or the flat field correction (i.e., for uniform light,
the detector does not return the same value for each pixel, and a normalization needs to be
performed by dividing the observed image by the “flat” image). Hence, it is very important
to know well the parameters of the detector (flat field image, dark image, etc.), because any
error on the these parameters will propagate to the measurements. Other effects can also
be corrected during this phase, such as the removal of the cosmic ray impacts or the field
distortion (the pixel surface for each pixel does not correspond to the same surface on the
sky). Depending on the knowledge of the instrument, each of these tasks may be more or
less difficult.
Once the data are calibrated, the analysis phase can start. Following the scientific
objectives, several kinds of information can be extracted from the data, such as, e.g., the


Starlet Transform in Astronomical Data Processing
detection of stars and galaxies, the measurement of their position, intensity, and various
morphological parameters. The results can be compared to existing catalogs, obtained from
previous observations. It is obviously impossible to cite all operations we may want to carry
through on an astronomical image, and we have just mentioned the most common. In
order to extract the information, it is necessary to take into account noise and point spread
function. Noise is the random fluctuation which is added to the CCD data and comes par-
tially from the detector and partially from the data. In addition to the errors induced by
the noise on the different measurements, noise also limits the detection of objects and can
be responsible for false detections. The point spread function is manifested in how the
image of a star, e.g., is generally spread out on several pixels, caused by the atmosphere’s
effect on the light path. The main effect is a loss of resolution, because two sufficiently
close objects cannot be separated. Once information has been extracted, such details can
be compared to our existing knowledge. This comparison allows us to validate or reject our
understanding of the universe.
In this chapter, we will discuss in detail how to detect objects in astronomical
images and how to take into account the point spread function though the deconvolution
processing.
..
Source Detection
As explained above, source (i.e., object) extraction from images is a fundamental step for
astronomers. For example, to build catalogs, stars and galaxies must be identified and their
position and photometry must be estimated with good accuracy. Catologs comprise a key
result of astronomical research. Various methods have been proposed to support the con-
struction of catalogs. One of the now most widely used software packages is SExtractor []
that is capable of handling very large images. A standard source detection approach, such
as in SExtractor, consists of the following steps:
. Background estimation
. Convolution with a mask
. Detection
. Deblending/merging
. Photometry
. Classification
These different steps are described in the next section. Astronomical images contain
typically a large set of point-like sources (the stars), some quasi point-like objects (faint
galaxies, double stars), and some complex and diffuse structures (galaxies, nebulae, plan-
etary stars, clusters, etc.). These objects are often hierarchically organized: a star in a small
nebula, itself embedded in a galaxy arm, itself included in a galaxy, and so on.
The standard approach, which is presented in detail in > Sect. ., presents some
limits, when we are looking for faint extended objects embedded in noise.
> Figure -

Starlet Transform in Astronomical Data Processing 

3 Sigma
Point source
Extended source
Background level
Detection level
Flux
⊡Fig. -
Example of astronomical data: a point source and an extended source are shown, with noise
and background. The extended object, which can be detected by eye, is undetected by a
standard detection approach
shows a typical example where a faint extended object is under the detection limit. In order
to detect such objects, more complex data modeling needs to be defined. > Section .
presents another approach to model and represent astronomical data, by using a sparse
model in a wavelet dictionary. A specific wavelet transform, called the starlet transform or
the isotropic undecimated wavelet transform, is presented. Based on this new modeling,
several approaches are proposed in > Sects. .and > ..
.
Standard Approaches to Source Detection
We describe here the most popular way to create a catalog of galaxies from astronomical
images.
..
The Traditional Data Model
The observed data Y can be decomposed into two parts, the signal X and the noise N:
Y[k, l] = X[k, l] + N[k, l]
(.)


Starlet Transform in Astronomical Data Processing
The imaging system can also be considered. If it is linear, the relation between the data and
the image in the same coordinate frame is a convolution:
Y[k, l] = (HX)[k, l] + N[k, l]
(.)
where H is the matrix related to the Point Spread Function (PSF) of the imaging system.
In most cases, objects of interest are superimposed on a relatively flat signal B, called
background signal. The model becomes:
Y[k, l] = (HX)[k, l] + B[k, l] + N[k, l]
(.)
..
PSF Estimation
The PSF H can be estimated from the data, or from an optical model of the imaging
telescope. In astronomical images, the data may contain stars, or one can point toward
a reference star in order to reconstruct a PSF. The drawback is the “degradation” of this
PSF because of unavoidable noise or spurious instrument signatures in the data. So, when
reconstructing a PSF from experimental data, one has to reduce the images used very care-
fully (background removal, for instance). Another problem arises when the PSF is highly
variable with time, as is the case for adaptive optics (AO) images. This means usually that
the PSF estimated when observing a reference star, after or before the observation of the
scientific target, has small differences from the perfectly correct PSF.
Another approach consists of constructing a synthetic PSF. Various studies [, , , ]
have suggested a radially symmetric approximation to the PSF:
P(r) ∝(+ r
R)
−β
(.)
The parameters β and R are obtained by fitting the model with stars contained in the data.
In the case of AO systems, this model can be used for the tail of the PSF (the so-called
seeing contribution), while in the central region the system provides an approximation of
the diffraction-limited PSF. The quality of the approximation is measured by the Strehl
ratio (SR), which is defined as the ratio of the observed peak intensity in the image of a
point source to the theoretical peak intensity of a perfect imaging system.
..
Background Estimation
The background must be accurately estimated, otherwise it will introduce bias in flux esti-
mation. In [, ], the image is partitioned into blocks, and the local sky level in each block
is estimated from its histogram. The pixel intensity histogram p(Y) is modeled using three

Starlet Transform in Astronomical Data Processing 

parameters, the true sky level B, the RMS (root mean square) noise σ, and a parameter
describing the asymmetry in p(Y) due to the presence of objects, and is defined by []:
p(Y) = 
a exp(σ /a)exp [−(Y −B)/a]erfc (σ
a −(Y −B)
σ
)
(.)
Median filtering can be applied to the D array of background measurements in order
to correct for spurious background values. Finally, the background map is obtained by a
bilinear or a cubic interpolation of the D array. The blocksize is a crucial parameter. If it is
too small, the background estimation map will be affected by the presence of objects, and
if too large, it will not take into account real background variations.
In [, ], the local sky level is calculated differently. A three-sigma clipping around the
median is performed in each block. If the standard deviation is changed by less than % in
the clipping iterations, the block is uncrowded, and the background level is considered to
be equal to the mean of the clipped histogram. Otherwise, it is calculated by c×median −
c× mean, where c= , c= in [], and c= ., c= .in []. This approach has been
preferred to histogram fitting for two reasons: it is more efficient from the computation
point of view and more robust with small sample size.
..
Convolution
In order to optimize the detection, the image must be convolved with a filter. The shape of
this filter optimizes the detection of objects with the same shape. Therefore, for star detec-
tion, the optimal filter is the PSF. For extended objects, a larger filter size is recommended.
In order to have optimal detection for any object size, the detection must be repeated
several times with different filter sizes, leading to a kind of multiscale approach.
..
Detection
Once the image is convolved, all pixels Y[k, l] at location (k, l) with a value larger than
T[k, l] are considered as significant, i.e., belonging to an object. T[k, l] is generally chosen
as B[k, l] + Kσ, where B[k, l] is the background estimate at the same position, σ is the
noise standard deviation, and K is a given constant (typically chosen between and ). The
thresholded image is then segmented, i.e., a label is assigned to each group of connected
pixels. The next step is to separate the blended objects which are connected and have the
same label.
An alternative to the thresholding/segmentation procedure is to find peaks. This is only
well suited to star detection and not to extended objects. In this case, the next step is to
merge the pixels belonging to the same object.


Starlet Transform in Astronomical Data Processing
..
Deblending/Merging
This is the most delicate step. Extended objects must be considered as single objects, while
multiple objects must be well separated. In SExtractor, each group of connected pixels is
analyzed at different intensity levels, starting from the highest down to the lowest level.
The pixel group can be seen as a surface, with mountains and valleys. At the beginning
(highest level), only the highest peak is visible. When the level decreases several other peaks
may become visible, defining therefore several structures. At a given level, two structures
may become connected, and the decision whether they form only one (i.e., merging) or
several objects (i.e., deblending) must be taken. This is done by comparing the integrated
intensities inside the peaks. If the ratio between them is too low, then the two structures
must be merged.
..
Photometry and Classiﬁcation
...
Photometry
Several methods can be used to derive the photometry of a detected object [, ]. Adaptive
aperture photometry uses the first image moment to determine the elliptical aperture from
which the object flux is integrated. Kron [] proposed an aperture size of twice the radius
of the first image moment radius r, which leads to recovery of most of the flux (>%).
In [], the value of .ris discussed, leading to loss of less than % of the total flux. Assum-
ing that the intensity profiles of the faint objects are Gaussian, flux estimates can be refined
[, ]. When the image contains only stars, specific methods can be developed which take
the PSF into account [, ].
...
Star–Galaxy Separation
In the case of star–galaxy classification, following the scanning of digitized images, Kurtz
[] lists the following parameters which have been used:
. Mean surface brightness
. Maximum intensity, area
. Maximum intensity, intensity gradient
. Normalized density gradient
. Areal profile
. Radial profile
. Maximum intensity, second- and fourth-order moments, ellipticity
. The fit of galaxy and star models
. Contrast versus smoothness ratio
. The fit of a Gaussian model

Starlet Transform in Astronomical Data Processing 

. Moment invariants
. Standard deviation of brightness
. Second-order moment
. Inverse effective squared radius
. Maximum intensity, intensity weighted radius
. Second- and third-order moments, number of local maxima, maximum intensity
References for all of these may be found in the cited work. Clearly, there is room for
differing views on parameters to be chosen for what is essentially the same problem. It is
of course the case also that aspects such as the following will help to orientate us toward a
particular set of parameters in a particular case: the quality of the data; the computational
ease of measuring certain parameters; the relevance and importance of the parame-
ters measured relative to the data analysis output (e.g., the classification or the planar
graphics); and, similarly, the importance of the parameters relative to theoretical models
under investigation.
...
Galaxy Morphology Classiﬁcation
The inherent difficulty of characterizing spiral galaxies especially when not face-on has
meant that most work focuses on ellipticity in the galaxies under study. This points to an
inherent bias in the potential multivariate statistical procedures. In the following, it will
not be attempted to address problems of galaxy photometry per se [, ], but rather to
draw some conclusions on what types of parameters or features have been used in practice.
From the point of view of multivariate statistical algorithms, a reasonably homogeneous
set of parameters is required. Given this fact, and the available literature on quantitative
galaxy morphological classification, two approaches to parameter selection appear to be
strongly represented:
. The luminosity profile along the major axis of the object is determined at discrete inter-
vals. This may be done by the fitting of elliptical contours, followed by the integrating of
light in elliptical annuli []. A similar approach was used in the ESO-Uppsala survey.
Noisiness and faintness require attention to robustness in measurement: the radial pro-
file may be determined taking into account the assumption of a face-on optically thin
axisymmetric galaxy and may be further adjusted to yield values for circles of given
radius []. Alternatively, isophotal contours may determine the discrete radial values
for which the profile is determined [].
. Specific morphology-related parameters may be derived instead of the profile. The inte-
grated magnitude within the limiting surface brightness of or mag. arcsec−in the
visual is popular [, ]. The logarithmic diameter (D) is also supported by Okamura
[]. It may be interesting to fit to galaxies under consideration model bulges and disks
using, respectively, r

or exponential laws [], in order to define further parameters.
Some catering for the asymmetry of spirals may be carried out by decomposing the


Starlet Transform in Astronomical Data Processing
object into octants; furthermore, the taking of a Fourier transform of the intensity may
indicate aspects of the spiral structure [].
The following remarks can be made relating to image data and reduced data.
•
The range of parameters to be used should be linked to the subsequent use to which
they might be put, such as to underlying physical aspects.
•
Parameters can be derived from a carefully constructed luminosity profile, rather than
it being possible to derive a profile from any given set of parameters.
•
The presence of both partially reduced data such as luminosity profiles, and more fully
reduced features such as integrated flux in a range of octants, is of course not a hin-
drance to analysis. However, it is more useful if the analysis is carried out on both types
of data separately.
Parameter data can be analyzed by clustering algorithms, by principal components
analysis or by methods for discriminant analysis. Profile data can be sampled at suitable
intervals and thus analyzed also by the foregoing procedures. It may be more convenient
in practice to create dissimilarities between profiles and analyze these dissimilarities: this
can be done using clustering algorithms with dissimilarity input.
.
Mathematical Modeling
Different models may be considered to represent the data. One of the most effective is
certainly the sparsity model, especially when a specific wavelet dictionary is chosen to rep-
resent the data. We introduce here the sparsity concept, as well as the wavelet transform
decomposition, which is the most used in astronomy.
..
Sparsity Data Model
A signal X, X = [x, . . . , xN]T, is sparse if most of its entries are equal to zero. For instance,
a k-sparse signal is a signal where only k samples have a nonzero value. A less strict defini-
tion is to consider a signal as weakly sparse or compressible when only a few of its entries
have a large magnitude, while most of them are close to zero.
If a signal is not sparse, it may be sparsified using a given data representation. For
instance, if X is a sine, it is clearly not sparse but its Fourier transform is extremely sparse
(i.e., -sparse). Hence, we say that a signal X is sparse in the Fourier domain if its Fourier
coefficients ˆX[u], ˆX[u] =

N ∑+∞
k=−∞X[k]eiπ uk
N are sparse. More generally, we can model
a vector signal X ∈RN as the linear combination of T elementary waveforms, also called
signal atoms: X = Φα = ∑T
i=α[i]ϕi, where α[i] = ⟨X, ϕi⟩are called the decomposition

Starlet Transform in Astronomical Data Processing 

coefficients of X in the dictionary Φ = [ϕ, . . . , ϕT] (the N × T matrix whose columns are
the atoms normalized to a unit ℓ-norm, i.e. ∀i ∈[, T],∥ϕi∥ℓ= ).
Therefore, to get a sparse representation of our data we need first to define the
dictionary Φ and then to compute the coefficients α. x is sparse in Φ if the sorted coeffi-
cients in decreasing magnitude have fast decay; i.e., most coefficients α vanish except for a
few.
The best dictionary is the one which leads to the sparsest representation. Hence we
could imagine having a huge overcomplete dictionary (i.e., T ≫N), but we would be faced
with prohibitive computation time cost for calculating the α coefficients. Therefore, there
is a trade-off between the complexity of our analysis step (i.e., the size of the dictionary)
and the computation time. Some specific dictionaries have the advantage of having fast
operators and are very good candidates for analyzing the data.
The Isotropic Undecimated Wavelet Transform (IUWT), also called starlet wavelet
transform, is well known in the astronomical domain because it is well adapted to astro-
nomical data, where objects are more or less isotropic in most cases [, ]. For more
astronomical images, the starlet dictionary is very well adapted.
..
The Starlet Transform
The starlet wavelet transform [] decomposes an n × n image cinto a coefficient set
W = {w, . . . ,wJ, cJ}, as a superposition of the form
c[k, l] = cJ[k, l] +
J
∑
j=
wj[k, l],
where cJ is a coarse or smooth version of the original image cand wj represents the details
of cat scale −j (see [, , , ] for more information). Thus, the algorithm outputs
J + sub-band arrays of size N × N. (The present indexing is such that j = corresponds
to the finest scale or high frequencies.)
The decomposition is achieved using the filter bank (hD, gD = δ −hD, ˜hD = δ,
˜gD = δ) where hD is the tensor product of two one-dimensional (D) filters hD and δ is
the Dirac function. The passage from one resolution to the next one is obtained using the
“à trous” (“with holes”) algorithm []
c j+[k, l] = ∑
m
∑
n
hD[m]hD[n]c j[k + jm, l + jn],
wj+[k, l] = c j[k, l] −c j+[k, l],
(.)
If we choose a B-spline for the scaling function:
ϕ(x) = B(x) = 
(∣x −∣−∣x −∣+∣x ∣−∣x + ∣+ ∣x + ∣) (.)
the coefficients of the convolution mask in one dimension are hD = { 
, 
, 
, 
, 
} and
in two dimensions,


Starlet Transform in Astronomical Data Processing
0.8
0.6
0.4
0.2
0.0-2
-1
0
1
2
0.4
0.2
0.0
-0.2-2
-1
0
1
2
⊡Fig. -
Left, the cubic spline function ϕ and right, the wavelet ψ
hD = (/
/
/
/
/)
⎛
⎜⎜⎜⎜⎜⎜
⎝






⎞
⎟⎟⎟⎟⎟⎟
⎠
=
⎛
⎜⎜⎜⎜⎜⎜
⎝


















































⎞
⎟⎟⎟⎟⎟⎟
⎠
> Figure -shows the scaling function and the wavelet function when a cubic spline
function is chosen as the scaling function ϕ.
The most general way to handle the boundaries is to consider that c[k + N] =
c[N −k] (“mirror”). But other methods can be used such as periodicity (c[k +N] = c[N]),
or continuity (c[k + N] = c[k]).
The starlet transform algorithm is:
. We initialize j to and we start with the data c j[k, l].
. We carry out a discrete convolution of the data c j[k, l] using the filter (hD), using the
separability in the two-dimensional case. In the case of the B-spline, this leads to a row-
by-row convolution with ( 
, 
, 
, 
, 
), followed by column-by-column convolution.
The distance between the central pixel and the adjacent ones is j.
. After this smoothing, we obtain the discrete wavelet transform from the difference
c j[k, l] −c j+[k, l].
. If j is less than the number J of resolutions we want to compute, we increment j, and
then go to step .
. The set α = {w, . . . ,wJ, cJ} represents the wavelet transform of the data.
This starlet transform is very well adapted to the detection of isotropic fea-
tures, and this explains its success for astronomical image processing, where the data
contain mostly isotropic or quasi-isotropic objects, such as stars, galaxies or galaxy
clusters.

Starlet Transform in Astronomical Data Processing 

⊡Fig. -
Galaxy NGC 
> Figure -shows the starlet transform of the galaxy NGC displayed in
> Fig. -. Five wavelet scales are shown and the final is a smoothed plane (lower right).
The original image is given exactly by the sum of these six images.
..
The Starlet Reconstruction
The reconstruction is straightforward. A simple co-addition of all wavelet scales repro-
duces the original map: c[k, l] = cJ[k, l] + ∑J
j=wj[k, l]. But because the transform is
non-subsampled, there are many ways to reconstruct the original image from its wavelet
transform []. For a given wavelet filter bank (h,g), associated with a scaling function
ϕ and a wavelet function ψ, any synthesis filter bank (˜h, ˜g), which satisfies the following
reconstruction condition
ˆh∗( )ˆ˜h( ) + ˆg∗( )ˆ˜g( ) = ,
(.)
leads to exact reconstruction. For instance, for isotropic h, if we choose ˜h = h (the synthesis
scaling function ˜ϕ = ϕ) we obtain a filter ˜g defined by []:
˜g = δ + h.


Starlet Transform in Astronomical Data Processing
⊡Fig. -
Wavelet transform of NGC by the IUWT. The co-addition of these six images reproduces
exactly the original image
If h is a positive filter, then g is also positive. For instance, if hD = [,,,,]/, then
˜gD = [,,,,]/. That is, ˜gD is positive. This means that ˜g is no longer related to a
wavelet function. The D detail synthesis function related to ˜gD is defined by:


˜ψD ( t
) = ϕD(t) + 
ϕD ( t
).
(.)
Note that by choosing ˜ϕD = ϕD, any synthesis function ˜ψD which satisfies
ˆ˜ψD() ˆψD() = ˆϕ
D( ) −ˆϕ
D()
(.)
leads to an exact reconstruction [] and ˆ˜ψD() can take any value. The synthesis function
˜ψD does not need to verify the admissibility condition (i.e., to have a zero mean).
> Figure -shows the two functions ˜ϕD (= ϕD) and ˜ψD used in the reconstruction
in D, corresponding to the synthesis filters ˜hD = hD and ˜gD = δ + hD. More details can
be found in [].

Starlet Transform in Astronomical Data Processing 

0.8
0.6
0.4
0.2
0.0-2
-1
0
1
2
1.0
0.8
0.6
0.4
0.2
0.0-4
-2
0
2
4
⊡Fig. -
Left:
∼ϕD, the D synthesis scaling function and right: ∼ψD, the D detail synthesis function
..
Starlet Transform: Second Generation
A particular case is obtained when ˆ˜ϕD = ˆϕD and ˆψD() =
ˆϕ
D( )−ˆϕ
D()
ˆϕD( )
, which leads
to a filter gD equal to δ −hD ⋆hD. In this case, the synthesis function ˜ψD is defined by

˜ψD ( t
) = ϕD(t) and the filter ˜gD = δ is the solution to (> .).
We end up with a synthesis scheme, where only the smooth part is convolved during
the reconstruction.
Deriving h from a spline scaling function, for instance, B(h= [,,]/) or B(h=
[,,,,]/) (note that h= h⋆h), since hD is even symmetric (i.e., H(z) = H(z−)),
the z-transform of gD is then,
G(z) = −H(z) = −z(+ z−

)

=

(−z−z−z−z + −z−−z−−z−−z−), (.)
which is the z-transform of the filter
gD = [−,−,−,−, ,−,−,−,−]/.
We get the following filter bank:
hD = h= ˜h = [,,,,]/
gD = δ −h ⋆h = [−,−,−,−,,−,−,−,−]/
˜gD = δ.
(.)
The second-generation starlet transform algorithm is:
. We initialize j to and we start with the data c j[k].
. We carry out a discrete convolution of the data c j[k] using the filter hD. The distance
between the central pixel and the adjacent ones is j. We obtain c j+[k].


Starlet Transform in Astronomical Data Processing
. We do exactly the same convolution on c j+[k], and we obtain c
′
j+[k].
. After this two-step smoothing, we obtain the discrete starlet wavelet transform from
the difference wj+[k] = c j[k] −c
′
j+[k].
. If j is less than the number J of resolutions we want to compute, we increment j and
then go to step .
. The set α = {w, . . . ,wJ, cJ} represents the starlet wavelet transform of the data.
As in the standard starlet transform, extension to D is trivial. We just replace the convo-
lution with hD by a convolution with the filter hD, which is performed efficiently by using
the separability.
With this specific filter bank, there is no convolution with the filter ˜gD during the
reconstruction. Only the low-pass synthesis filter ˜hD is used.
The reconstruction formula is
c j[l] = (h(j)
D ⋆c j+)[l] + wj+[l],
(.)
and denoting L j = h() ⋆⋅⋅⋅⋆h(j−) and L= δ, we have
c[l] = (LJ ⋆cJ)[l] +
J
∑
j=
(L j−⋆wj)[l].
(.)
Each wavelet scale is convolved with a low-pass filter.
The second-generation starlet reconstruction algorithm is:
. The set α = {w, . . . ,wJ, cJ} represents the input starlet wavelet transform of the data.
. We initialize j to J −and we start with the coefficients c j[k].
. We carry out a discrete convolution of the data c j+[k] using the filter (hD). The
distance between the central pixel and the adjacent ones is j. We obtain c′
j+[k].
. Compute c j[k] = c′
j+[k] + wj+[k].
. If j is larger than , j = j −and then go to step .
. ccontains the reconstructed data.
As for the transformation, the D extension consists just in replacing the convolution by
hD with a convolution by hD.
> Figure -shows the analysis scaling and wavelet functions. The synthesis func-
tions ˜ϕD and ˜ψD are the same as those in
> Fig. -. As both are positive, we have a
decomposition of an image X on positive scaling functions ˜ϕD and ˜ψD, but the coeffi-
cients α are obtained with the starlet wavelet transform, and have a zero mean (except for
cJ), as a regular wavelet transform.
In D, similarly, the second-generation starlet transform leads to the representation of
an image X[k, l]:
X[k, l] = ∑
m,n
ϕ()
j,k,l (m, n)cJ[m, n] +
J
∑
j=
∑
m,n
ϕ()
j,k,l (m, n)wj[m, n],
(.)

Starlet Transform in Astronomical Data Processing 

0.8
0.50
0.40
0.30
0.20
0.10
0.00
-0.10
-0.20
0.6
0.4
0.2
0.0-2
-1
0
1
2
-4
-2
0
2
4
⊡Fig. -
Left, the ϕD analysis scaling function and right, the ψD analysis wavelet function.
The synthesis functions ˜ϕD and ˜ψD are the same as those in > Fig. -
where ϕ()
j,k,l(m, n)= −j ˜ϕD(−j(k −m)) ˜ϕD(−j(l −n)), and ϕ()
j,k,l (m, n)= −j ˜ψD
(−j(k −m)) ˜ψD(−j(l −n)).
ϕ() and ϕ() are positive, and wj are zero mean D wavelet coefficients.
The advantage of the second-generation starlet transform will be seen in > Sect. ...
..
Sparse Modeling of Astronomical Images
Using the sparse modeling, we now consider that the observed signal X can be considered
as a linear combination of a few atoms of the wavelet dictionary Φ = [ϕ, . . . , ϕT]. The
model of > Eq. .is then replaced by the following:
Y = HΦα + N + B
(.)
and X = Φα, and α = {w, . . . ,wJ, cJ}. Furthermore, most of the coefficients α will be equal
to zero. Positions and scales of active coefficients are unknown, but they can be estimated
directly from the data Y. We define the multiresolution support M of an image Y by:
M j[k, l] = {
if wj[k, l] is significant

if wj[k, l] is not significant
(.)
where wj[k, l] is the wavelet coefficient of Y at scale j and at position (k, l). Hence, M
describes the set of active atoms in Y. If H is compact and not too extended, then M


Starlet Transform in Astronomical Data Processing
describes also well the active set of X. This is true because the background B is gener-
ally very smooth, and therefore a wavelet coefficient wj[k, l] of Y, which does not belong
to the coarsest scale is only dependent on X and N (the term < ϕi, B > being equal to zero).
...
Selection of Signiﬁcant Coeﬃcients Through Noise
Modeling
We need now to determine when a wavelet coefficient is significant. Wavelet coefficients of
Y are corrupted by noise, which follows in many cases a Gaussian distribution, a Poisson
distribution, or a combination of both. It is important to detect the wavelet coefficients
which are “significant,” i.e., the wavelet coefficients which have an absolute value too large
to be due to noise.
For Gaussian noise, it is easy to derive an estimation of the noise standard deviation σj
at scale j from the noise standard deviation, which can be evaluated with good accuracy in
an automated way []. To detect the significant wavelet coefficients, it suffices to compare
the wavelet coefficients wj[k, l] to a threshold level tj. tj is generally taken equal to Kσj,
and K, as noted in > Sect. ., is chosen between and . The value of corresponds to
a probability of false detection of .%. If wj[k, l] is small, then it is not significant and
could be due to noise. If wj[k, l] is large, it is significant:
if ∣wj[k, l] ∣≥tj
then wj[k, l] is significant
if ∣wj[k, l] ∣< tj
then wj[k, l] is not significant
(.)
When the noise is not Gaussian, other strategies may be used:
•
Poisson noise: If the noise in the data Y is Poisson, the transformation [] A(Y) =

√
Y + 
acts as if the data arose from a Gaussian white noise model, with σ = , under
the assumption that the mean value of Y is sufficiently large. However, this transform
has some limits, and it has been shown that it cannot be applied for data with less than
counts (due to photons) per pixel. So for X-ray or gamma ray data, other solutions
have to be chosen, which manage the case of a reduced number of events or photons
under assumptions of Poisson statistics.
•
Gaussian + Poisson noise: The generalization of variance stabilization [] is:
G(Y[k, l]) = 
α
√
αY[k, l] + 
α+ σ −αg,
where α is the gain of the detector, and g and σ are the mean and the standard deviation
of the read-out noise.
•
Poisson noise with few events using the MS-VST: For images with very few pho-
tons, one solution consists in using the Multi-Scale Variance Stabilization Transform
(MS-VST) []. The MS-VST combines both the Anscombe transform and the starlet
transform in order to produce stabilized wavelet coefficients, i.e., coefficients corrupted

Starlet Transform in Astronomical Data Processing 

by a Gaussian noise with a standard deviation equal to . In this framework, wavelet
coefficients are now calculated by:
Starlet
+
MS-VST
⎧⎪⎪⎪⎨⎪⎪⎪⎩
c j = ∑m ∑n hD[m]hD[n]
c j−[k + j−m, l + j−n]
wj = Aj−(c j−) −Aj(c j)
(.)
where Aj is the VST operator at scale j defined by:
Aj(c j) = b(j)√
∣c j + e(j)∣,
(.)
where the variance stabilization constants b(j) and e(j) only depend on the filter hD
and the scale level j. They can all be precomputed once for any given hD []. The mul-
tiresolution support is computed from the MS-VST coefficients, considering a Gaussian
noise with a standard deviation equal to . This stabilization procedure is also invertible
as we have:
c= A−

⎡⎢⎢⎢⎢⎣
AJ(aJ) +
J
∑
j=
wj
⎤⎥⎥⎥⎥⎦
(.)
For other kinds of noise (correlated noise, nonstationary noise, etc.), other solutions have
been proposed to derive the multiresolution support [].
..
Sparse Positive Decomposition
Many astronomical images can be modeled as a sum of positive features, like stars and
galaxies, which are more or less isotropic. The previous representation, based on the starlet
transform, is well adapted to the representation of isotropic objects but does not introduce
any prior relative to the positivity of the features contained in our image. A positive and
sparse modeling of astronomical images is similar to > Eq. .:
Y = HΦα + N + B
(.)
or
Y = Φα + N + B
(.)
if we do not take into account the point spread function. All coefficients in α are now
positive, and all atoms in the dictionary Φ are positive functions. Such a decomposition
normally requires computationally intensive algorithms such as Matching Pursuit []. The
second-generation starlet transform offers us a new way to perform such a decomposition.
Indeed, we have seen in > Sect. ..that, using a specific filter bank, we can decompose
an image Y on a positive dictionary Φ (see
> Fig. -) and obtain a set of coefficients
α(Y), where α(Y) = WY = {w, . . . ,wJ, cJ}, W being the starlet wavelet transform oper-
ator. α coefficients are positive and negative and are obtained using the standard starlet
wavelet transform algorithm. Hence, by thresholding all negative (respectively, positive)


Starlet Transform in Astronomical Data Processing
coefficients, the reconstruction is always positive (respectively, negative), since Φ contains
only positive atoms.
Hence, we would like to have a sparse set of positive coefficients ˜α which verify Φ˜α = Y.
But in order to take into account the background and the noise, we need to define the
constraint in the wavelet space (i.e., WΦ˜α = WY = α(Y)), and this constraint must be
applied only to the subset of coefficients in α(Y) which are larger than the detection level.
Therefore, to get a sparse positive decomposition on Φ, we need to minimize:
˜α = min
α
∥α ∥
s.t. MWΦα = Mα(Y),
(.)
where M is the multiresolution support defined in the previous section (i.e., M j[k, l] = 
is a significant coefficient is detected at scale j and at position (k, l), and zero otherwise).
To remove the background, we have to set MJ+[k, l] = for all (k, l).
It was shown that such optimization problems can be efficiently solved through an iter-
ative soft thresholding (IST) algorithm [, , ]. The following algorithm, based on the
IST, allows to take into account the noise modeling through the multiresolution support
and force the coefficients to be all positive.
. Take the second-generation starlet wavelet transform of the data Y, we obtain α(Y).
. From a given noise model, determine the multiresolution support M.
. Set the number of iterations Niter, the first threshold, λ() = MAX (α(Y)), and the
solution ˜α() = .
. For = , Niter do
•
Reconstruct the image ˜Y(i) from ˜α(i): ˜Y(i) = Φ˜α(i).
•
Take the second-generation starlet wavelet transform of the data ˜Y(i), we obtain
α ˜Y(i) = WΦ˜α(i).
•
Compute the significant residual r(i):
r(i) = M (α(Y) −α
˜Y(i)) = M (α(Y) −WΦ˜α(i))
(.)
•
Calculate the value λ(i) = λ()(−i/Niter).
•
Update the solution, by adding the residual, applying a soft thresholding on positive
coefficients using the threshold level λ(i), and setting all negative coefficients to zero.
˜α(i+) = (˜α(i) + r(i) −λ(i))+
= (˜α(i) + M (α(Y) −WΦ˜α(i)) −λ(i))+
(.)
•
i = i + .
. The set ˜α = ˜α(Niter) represents the sparse positive decomposition of the data.

Starlet Transform in Astronomical Data Processing 

⊡Fig. -
Positive starlet decomposition of the galaxy NGCwith six scales
The threshold parameter λ(i) decreases with the iteration number and it plays a role similar
to the cooling parameter of the simulated annealing techniques, i.e., it allows the solution
to escape from local minima.
...
Example : Sparse positive decomposition of NGC
> Figure -shows the position starlet decomposition, using iterations and can be
compared to > Fig. -
...
Example : Sparse positive starlet decomposition of a
simulated image
The next example compares the standard starlet transform to the positive starlet decom-
position (PSD) on a simulated image.
> Figure -shows respectively from top to bottom and left to right, (a) the origi-
nal simulated image, (b) the noisy data, (c) the reconstruction from the PSD coefficients,
and (d) the residual between the noisy data and the PSD reconstructed image (i.e.,
image b −image c). Hence, the PSD reconstructed image gives a very good approximation
of the original image. No structures can be seen in the residual, and all sources are well
detected.


Starlet Transform in Astronomical Data Processing
⊡Fig. -
Top left and right, original simulated image and the same image contaminated by a Gaussian
noise. Bottom left and right, reconstructed image for the positive starlet coeﬃcients of the
noisy image using iterations and residual (i.e., noisy image – reconstructed image)
The first PSD scale does not contain any nonzero coefficient. The top of
> Fig. -
shows the four first scales of the wavelet transform, and > Fig. -bottom, the four first
scales of the PSD.
.
Source Detection Using a Sparsity Model
As described in the previous section, the wavelet coefficients of Y, which do not belong to
the coarsest scale cJ are not dependent on the background. This is a serious disadvantage,
since the background estimation can be sometimes very problematic.
Two approaches have been proposed to detect sources, assuming the signal is sparse
in the wavelet domain. The first consists in first removing the noise and the back-
ground, and then applying the standard approach described in > Sect. .. It has been
used for many years for X-ray source detection [, ]. The second approach, called

Starlet Transform in Astronomical Data Processing 

⊡Fig. -
Top, starlet transform and bottom, positive starlet decomposition of a simulated
astronomical image
Multiscale Vision Model [], attempts to define directly an astronomical object in the
wavelet space.
..
Detection Through Wavelet Denoising
The most commonly used filtering method is hard thresholding, which consists of setting
to all wavelet coefficients of Y which have an absolute value lower than a threshold tj:
˜wj[k, l] = {wj[k, l]
if ∣wj[k, l] ∣> tj

otherwise
(.)
More generally, for a given sparse representation (i.e., wavelet) with its associated fast trans-
form W and fast reconstruction R, we can derive a hard threshold denoising solution X
from the data Y, by first estimating the multiresolution support M using a given noise
model, and then calculating:
X = RMWY.
(.)
We transform the data, multiply the coefficients by the support, and reconstruct the
solution.
The solution can however be improved by considering the following optimization prob-
lem, minX ∥M(WY −WX) ∥
, where M is the multiresolution support of Y. A solution
can be obtained using the Landweber iterative scheme [, ]:
Xn+= Xn + RM [WY −WXn]
(.)


Starlet Transform in Astronomical Data Processing
If the solution is known to be positive, the positivity constraint can be introduced using
the following equation:
Xn+= P+ (Xn + RM [WY −WXn]),
(.)
where P+ is the projection on the cone of nonnegative images.
This algorithm allows us to constrain the residual to have a zero value within the mul-
tiresolution support []. For astronomical image filtering, iterating improves the results
significantly, especially for the photometry (i.e., the integrated number of photons in a
given object).
Removing the background in the solution is straightforward. The algorithm does not
need to be modified. We only need to set the coefficients related to the coarsest scale in the
multiresolution support to zero: ∀k MJ[k, l] = .
..
The Multiscale Vision Model
...
Introduction
The wavelet transform of an image Y by the starlet transform produces at each scale j a
set {wj}. This has the same number of pixels as the image. The original image I can be
expressed as the sum of all the wavelet scales and the smoothed array cJ by the expression
Y[k, l] = cJ[k, l] +
J
∑
j=
wj[k, l].
(.)
Hence, we have a multiscale pixel representation, i.e., each pixel of the input image is asso-
ciated with a set of pixels of the multiscale transform. A further step is to consider a mul-
tiscale object representation, which would associate with an object contained in the data a
volume in the multiscale transform. Such a representation obviously depends on the kind
of image we need to analyze, and we present here a model that has been developed for
astronomical data. It may however be used for other kinds of data to the extent that such
data are similar to astronomical data. We assume that an image Y can be decomposed into
a set of components:
Y[k, l] =
No
∑
i=
Xi[k, l] + B[k, l] + N[k, l],
(.)
where No is the number of components, Xi are the components contained in the data (stars,
galaxies, etc.), B is the background image, and N is the noise.
To perform such a decomposition, we have to detect, to extract, to measure, and to
recognize the significant structures. This is done by first computing the multiresolution
support of the image (i.e., the set of significant active coefficients) and then by applying
a segmentation scale by scale. The wavelet space of a D direct space is a D volume.
An object, associated to a component, has to be defined in this space. A general idea for
object definition lies in the connectivity property. An object occupies a physical region,

Starlet Transform in Astronomical Data Processing 

and in this region we can join any pixel to other pixels based on significant adjacency.
Connectivity in direct space has to be transported into wavelet transform space. In order
to define the objects, we have to identify the wavelet transform space pixels we can attribute
to the objects. We describe in this section the different steps of this method.
...
Multiscale Vision Model Deﬁnition
The multiscale vision model, MVM [], described an object as a hierarchical set of
structures. It uses the following definitions:
•
Significant wavelet coefficient: a wavelet coefficient is significant when its absolute value
is above a given detection limit. The detection limit depends on the noise model
(Gaussian noise, Poisson noise, and so on). See > Sect. ..for more details.
•
Structure: a structure S j,k is a set of significant connected wavelet coefficients at the
same scale j:
S j,k = {wj[k, l],wj[k, l], . . . ,wj[kp, lp]}
(.)
where p is the number of significant coefficients included in the structure S j,k, and
wj,xi ,yi is a wavelet coefficient at scale i and at position (xi, yi).
•
Object: an object is a set of structures:
Ol = {S j,k, . . . , S jn,kn}
(.)
We define also the operator L, which indicates to which object a given structure
belongs: L(S j,k) = l is S j,k ∈Ol, and L(S j,k) = otherwise.
•
Object scale: the scale of an object is given by the scale of the maximum of its wavelet
coefficients.
•
Interscale relation: the criterion allowing us to connect two structures into a single
object is called the “interscale relation.”
•
Sub-object: a sub-object is a part of an object. It appears when an object has a local
wavelet maximum. Hence, an object can be composed of several sub-objects. Each sub-
object can also be analyzed.
...
From Wavelet Coeﬃcients to Object Identiﬁcation
Multiresolution Support Segmentation
Once the multiresolution support has been calculated, we have at each scale a boolean
image (i.e., pixel intensity equals when a significant coefficient has been detected, and
otherwise). The segmentation consists of labeling the boolean scales. Each group of con-
nected pixels having a “” value gets a label value between and Lmax, Lmax being the
number of groups. This process is repeated at each scale of the multiresolution support.
We define a “structure” S j,i as the group of connected significant pixels which has the label
i at a given scale j.


Starlet Transform in Astronomical Data Processing
Interscale Connectivity Graph
An object is described as a hierarchical set of structures. The rule which allows us to con-
nect two structures into a single object is called “interscale relation.” > Figure -shows
how several structures at different scales are linked together, and form objects. We have
now to define the interscale relation. Let us consider two structures at two successive scales,
S j,k and S j+,l. Each structure is located in one of the individual images of the decompo-
sition and corresponds to a region in this image where the signal is significant. Denoting
(xm, ym) the pixel position of the maximum wavelet coefficient value of S j,k, S j,k is said to
be connected to S j+,l if S j+,l contains the pixel position (xm, ym) (i.e., the pixel position
of the maximum wavelet coefficient of the structure S j,k must also be contained in the
structure S j+,l). Several structures appearing in successive wavelet coefficient images can
be connected in such a way, which we call an object in the interscale connectivity graph.
Hence, we identify no objects in the wavelet space, each object Oi being defined by a set of
structures, and we can assign to each structure a label i, with i ∈[, no]: L(S j,k) = i if the
structure S j,k belongs to the ith object.
⊡Fig. -
Example of connectivity in wavelet space: contiguous signiﬁcant wavelet coeﬃcients form a
structure, and following an interscale relation, a set of structures forms an object. Two
structures Sj, Sj+at two successive scales belong to the same object if the position pixel of
the maximum wavelet coeﬃcient value of Sj is included in Sj+

Starlet Transform in Astronomical Data Processing 

Filtering
Statistically, some significant structures can be due to the noise. They contain very few pix-
els and are generally isolated, i.e., connected to no field at upper and lower scales. So, to
avoid false detection, the isolated fields can be removed from the initial interscale connec-
tion graph. Structures at the border of the images may also have been detected because of
the border problem, and can be removed.
Merging/Deblending
As in the standard approach, true objects which are too close may generate a set of con-
nected structures, initially associated with the same object, and a decision must be taken
whether to consider such a case as one or two objects. Several cases may be distinguished:
•
Two (or more) close objects, approximately of the same size, generate a set of structures.
At a given scale j, two separate structures S j,and S j,are detected while at the scale
j + , only one structure is detected S j+,, which is connected to the S j,and S j,.
•
Two (or more) close objects of different sizes generate a set of structures, from scale j
to scale k (k > j).
In the wavelet space, the merging/deblending decision will be based on the local
maxima values of the different structures belonging to this object. A new object (i.e.,
deblending) is derived from the structure S j,k if there exists at least one other structure
at the same scale belonging to the same object (i.e., there exists one structure S j+,a and
at least one structure S j,b such that L(S j+,a) = L(S j,b) = L(S j,k)), and if the following
relationship is verified: wm
j > wm
j−and wm
j > wm
j+, where:
•
wm
j is the maximum wavelet coefficient of the structure S j,k: wm
j = Max(S j,k).
−wm
j−= if S j,k is not connected to any structure at scale j −.
−wm
j−is the maximum wavelet coefficient of the structure S j−,l, where S j−,l is such
that L(S j−,l) = L(S j,k) and the position of its highest wavelet coefficient is the
closest to the position of the maximum of S j,k.
•
wm
j+= Max{wj+,x,y, . . . ,wj+,xn ,yn}, where all wavelet coefficients wj+,x,y are at a
position which belongs also to S j,k (i.e., wj,x,y ∈S j,k).
When these conditions are verified, S j,k and all structures at smaller scales which are
directly or indirectly connected to S j,k will define a new object.
Object Identiﬁcation
We can now summarize this method allowing us to identify all the objects in a given
image Y:
. We compute the wavelet transform with the starlet algorithm, which leads to a set
α = WY = {w, . . . ,wJ, cJ}. Each scale wj has the same size as the input image.
. We determine the noise standard deviation in w.


Starlet Transform in Astronomical Data Processing
. We deduce the thresholds at each scale from the noise modeling.
. We threshold scale by scale and we do an image labeling.
. We determine the interscale relations.
. We identify all the wavelet coefficient maxima of the wavelet transform space.
. We extract all the connected trees resulting from each wavelet transform space
maximum.
...
Source Reconstruction
Partial Reconstruction as an Inverse Problem
A set of structures Si (Si ={S j,k, . . . , S j′,k′}) defines an object Oi which can be
reconstructed separately from other objects, in order to provide the components Xi.
The coaddition of all reconstructed objects is a filtered version of the input data. We will
denote αi the set of wavelet coefficients belonging to the object Oi. Therefore, αi is a subset
of the wavelet transform of Xi, ˜αi = WXi. Indeed, the last scale of ˜αi is unknown, as well as
many wavelet coefficients which have not been detected. Then the reconstruction problem
consists of searching for an image Xi such that its wavelet transform reproduces the coeffi-
cients αi (i.e., they are the same as those of Si, the detected structures). If W describes the
wavelet transform operator, and Pw, the projection operator in the subspace of the detected
coefficients (i.e., having set to zero all coefficients at scales and positions where nothing was
detected), the solution is found by the minimization of:
min
Xi ∥αi −Pw (WXi) ∥
(.)
The size of the restored image Xi is arbitrary and it can be easily set greater than the number
of known coefficients. It is certain that there exists at least one image Xi , which gives exactly
αi, i.e., the original one. But generally we have an infinity of solutions, and we have to
choose among them the one which is considered as correct. An image is always a positive
function, which leads us to constrain the solution, but this is not sufficient to get a unique
solution. More details on the reconstruction algorithm can be found in [, ].
..
Examples
...
Band Extraction
We simulated a spectrum which contains an emission band at .μm and nonstationary
noise superimposed on a smooth continuum. The band is a Gaussian of width FWHM =
.μm (FWHM = full width at half-maximum), and normalized such that its maximum
value equals ten times the local noise standard deviation.
> Figure -(top) contains the simulated spectrum. The wavelet analysis results in
the detection of an emission band at .μm above σ. > Figure -(middle) shows the

Starlet Transform in Astronomical Data Processing 

4
15
10
5
30
20
10
0
0
2
0
–2
20
30
10
Flux [10–13W m–2 mm–1]
Flux [10–13W m–2 mm–1]
Flux [10–13W m–2 mm–1]
Wavelength [mm]
0
3.30
3.40
3.50
3.60
3.70
3.30
3.40
3.50
3.60
3.70
3.30
3.40
3.50
3.60
3.70
3.30
3.40
3.50
3.60
3.70
⊡Fig. -
Top, simulated spectrum. Middle, reconstructed simulated band (full line) and original band
(dashed line). Bottom, simulated spectrum minus the reconstructed band
reconstruction of the detected band in the simulated spectrum. The real feature is over-
plotted as a dashed line. > Figure -(bottom) contains the original simulation with the
reconstructed band subtracted. It can be seen that there are no strong residuals near the
location of the band, which indicates that the band is well reconstructed. The center posi-
tion of the band, its FWHM, and its maximum, can then be estimated via a Gaussian fit.
More details about the use of MVM for spectral analysis can be found in [].


Starlet Transform in Astronomical Data Processing
...
Star Extraction in NGC
We applied MVM to the galaxy NGC(> Fig. -, top left). Two images were created
by coadding objects detected from scales and , and from scales –. They are displayed
respectively in > Fig. -, top right, and bottom left. > Figure -, bottom right, shows
the difference between the input data and the image which contained the objects from
scales and . As we can see, all small objects have been removed, and the galaxy can be
better analyzed.
...
Galaxy Nucleus Extraction
> Figure -shows the extracted nucleus of NGCusing the MVM method and the
difference between the galaxy image and the nucleus image.
⊡Fig. -
(a) Galaxy NGC, (b) objects detected from scales and , (c) objects detected from
scales –, and (d) diﬀerence between (a) and (b)

Starlet Transform in Astronomical Data Processing 

⊡Fig. -
Upper left, galaxy NGC. Upper right, extracted nucleus. Bottom, diﬀerence between the
two previous images
.
Deconvolution
Up to now, the PSF H has not been considered in the source detection. This means that all
morphological parameters (size, ellipticity, etc.) derived from the detected objects need to
be corrected from the PSF. Very close objects may also be seen as a single object because
H acts as a blurring operator on the data. A solution may consist in deconvolving first the
data, and carrying out the source detection afterwards.
The problem of image deconvolution is ill posed [] and, as a consequence, the matrix
H modeling the imaging system is ill-conditioned. If Y is the observed image and X the
unknown object, the equation HX = Y has not a unique and stable solution. Therefore, one
must look for approximate solutions of this equation that are also physically meaningful.
One approach is Tikhonov regularization theory []; however, a more general approach


Starlet Transform in Astronomical Data Processing
is provided by the so-called Bayes paradigm [], even if it is applicable only to discrete
problems. In this framework, one can both take into account statistical properties of the
data (Tikhonov regularization is obtained by assuming additive Gaussian noise) and also
introduce a priori information on the unknown object.
..
Statistical Approach to Deconvolution
We assume that the detected image Y is the realization of a multi-valued random vari-
able I corresponding to the (unknown) value X of another multi-valued random variable,
the object O. Moreover, we assume that the conditional probability distribution pI(Y∣X) is
known. Since the unknown object appears as a set of unknown parameters, the problem of
image deconvolution can be considered as a classical problem of parameter estimation. The
standard approach is the maximum likelihood(ML) method. In our specific application, for
a given detected image Y, this consists of introducing the likelihood function defined by
LY(X) = pI(Y∣X).
(.)
Then the ML estimate of the unknown object is any maximizer X∗of the likelihood
function
X∗= arg max
X∈Rn LY(X),
(.)
if it exists.
In our applications, the likelihood function is the product of a very large number of
terms (the data components are assumed to be statistically independent), so that it is conve-
nient to take the logarithm of this function; moreover, if we consider the negative logarithm
(the so-called neglog), the maximization problem is transformed into a minimization one.
Let us consider the function
J(X; Y) = −A ln LY(X) + B,
(.)
where A, B are suitable constants. They are introduced in order to obtain a function which
has a simpler expression and is also nonnegative since, in our applications, the neglog
of the likelihood is bounded from below. Then, it is easy to verify that the problem of
> Eq. (.) is equivalent to the following one:
X∗= arg min
X∈Rn J(X; Y).
(.)
We consider now the model of > Eq. (.) with three different examples of noise.
Example
In the case of additive white Gaussian noise, by a suitable choice of the constants
A, B, we obtain (we assume here that the background B is not subtracted even if it must be
estimated)
J(X; Y) = ∣∣HX + B −Y∣∣,
(.)
and therefore the ML approach coincides with the well-known least-squares (LS) approach.
It is also well known that the function of > Eq. (.) is convex, and strictly convex if and

Starlet Transform in Astronomical Data Processing 

only if the equation HX = has only the solution X = . Moreover, it has always absolute
minimizers, i.e., the LS-problem has always a solution; but the problem is ill conditioned
because it is equivalent to the solution of the Euler equation:
HTH X = HT(Y −B).
(.)
We remark that the ill-posedness of the LS-problem is the starting point of Tikhonov regu-
larization theory (see, for instance, [, ]), and therefore this theory is based on the tacit
assumption that the noise affecting the data is additive and Gaussian.
We remark that, in the case of object reconstruction, since objects are nonnegative, we
should consider the minimization of the function of
> Eq. (.) on the nonnegative
orthant. With such a constraint the problem is not treatable in the standard framework of
regularization theory.
Example 
In the case of Poisson noise, if we introduce the so-called generalized Kullback-
Leibler (KL) divergence of a vector Z from a vector Y, defined by
DKL(Y, Z) =
m
∑
i=
{Yi ln Yi
Zi
+ Zi −Yi},
(.)
then, with a suitable choice of the constants A, B, the function J(X; Y) is given by
J(X; Y) = DKL(Y; HX + B) =
(.)
=
m
∑
i=
{Yi ln
Yi
(HX + B)i
+ (HX + B)i −Yi}.
It is quite natural to take the nonnegative orthant as the domain of this function. Moreover,
it is well known that it is convex (strictly convex if the equation HX = has only the solution
X = ), non-negative, and coercive. Therefore it has absolute minimizers. However, these
minimizers are strongly affected by noise and the specific effect of the noise in this problem is
known as checkerboard effect [], since many components of the minimizers are zero.
Example 
In the case of Gauss + Poisson noise, the function J(X; Y) is given by a much
more complex form. This function is also convex (strictly convex if the equation Hx = has
the unique solution x = ), nonnegative and coercive ([, Proposition ]). Therefore, it also
has absolute minimizer on the nonnegative orthant.
The previous examples demonstrate that, in the case of image reconstruction, ML
problems are ill posed or ill conditioned. That means that one is not interested in comput-
ing the minimum points X∗of the functions corresponding to the different noise models
because they do not provide sensible estimates ¯X of the unknown object.
The previous remark is not surprising in the framework of inverse problem theory.
Indeed it is generally accepted that, if the formulation of the problem does not use some
additional information on the object, then the resulting problem is ill posed. This is what
happens in the maximum likelihood approach because we only use information about the
noise with, possibly, the addition of the constraint of nonnegativity.


Starlet Transform in Astronomical Data Processing
The additional information may consist, for instance, of prescribed bounds on the solu-
tion and/or its derivatives up to a certain order (in general not greater than two). These
prescribed bounds can be introduced in the problem as additional constraints in the vari-
ational formulation provided by ML. However, in a quite natural probabilistic approach,
called the Bayesian approach, the additional information is given in the form of statistical
properties of the object [].
In other words, one assumes that the unknown object X is a realization of a vector-
valued random variable O, and that the probability distribution of O, the so-called prior
denoted by pO(X), is also known or can be deduced from known properties of the object.
The most frequently used priors are Markov random fields or, equivalently, Gibbs random
fields, i.e., they have the following form:
pO(X) = 
Z e−μΩ(X),
(.)
where Z is a normalization constant, μ is a positive parameter (a hyperparameter in statis-
tical language, a regularization parameter in the language of regularization theory), while
Ω(X) is a function, possibly convex.
The previous assumptions imply that the joint probability density of the random
variables O, I is given by
pOI(X, Y) = pI(Y∣X)pO(X).
(.)
If we introduce the marginal probability density of the image I
pI(Y) = ∫pOI(X, Y) dX,
(.)
from Bayes’ formula we obtain the conditional probability density of O for a given value Y
of I:
pO(X∣Y) = pOI(X, Y)
pI(Y)
= pI(Y∣X)pO(X)
pI(Y)
.
(.)
If in this equation we insert the detected value Y of the image, we obtain the a posteriori
probability density of X:
PY(X) = pO(X∣Y) = LY(X) pO(X)
pI(Y) .
(.)
Then, a maximum a posteriori (MAP) estimate of the unknown object is defined as any
object X∗that maximizes the a posteriori probability density:
X∗= arg max
X∈Rn PY(X).
(.)
As in the case of the likelihood it is convenient to consider the neglog of PY(X). If we
assume a Gibbs prior as that given in > Eq. (.) and we take into account the definition
of > Eq. (.), we can introduce the following function
J(X; Y) = −A lnPY(X) + B −A ln Z −
(.)
−A ln pI(Y) = J(X; Y) + μJR(X),

Starlet Transform in Astronomical Data Processing 

where JR(X) = AΩ(X). Therefore, the MAP estimates are also given by
X∗= arg min
X∈Rn J(X; Y)
(.)
and again one must look for the minimizers satisfying the nonnegativity constraint.
..
The Richardson–Lucy Algorithm
One of the most frequently used methods for image deconvolution in astronomy is an
iterative algorithm known as the Richardson–Lucy (RL) algorithm [, ]. In emission
tomography it is also denoted as expectation maximization (EM) because, as shown in [],
it can be obtained by applying to the ML problem with Poisson noise a general EM method
introduced in [] for obtaining ML estimates.
In [] it is shown that, if the iteration converges, then the limit is just a ML estimate
in the case of Poisson data. Subsequently the convergence of the algorithm was proved by
several authors in the case B = . An account can be found in [].
The iteration is as follows: it is initialized with a positive image X(a constant array, in
general); then, given Xn, Xn+is computed by
Xn+= XnHT
Y
HXn + B.
(.)
This algorithm has some nice features. First, the result of each iteration is automatically
a positive array; second, in the case B = , the result of each iteration has the same
flux of the detected image Y, and this property is interesting from the photometric point
of view.
The limit of the RL iteration is, in general, very noisy (see the remark at the end of
Example ), but a reasonable solution can be obtained by a suitable stopping of the algo-
rithm before convergence. This can be seen as a kind of regularization []. An example of
RL-reconstruction is shown in > Fig. -(lower left panel).
Several iterative methods, modeled on RL, have been introduced for computing MAP
estimates corresponding to different kinds of priors. A recent account can be found in [].
..
Deconvolution with a Sparsity Prior
Another approach is to use the sparsity to model the data. A sparse model can be inter-
preted from a Bayesian standpoint, by assuming the coefficients α of the solution in the


Starlet Transform in Astronomical Data Processing
⊡Fig. -
Simulated Hubble Space Telescope Wide Field Camera image of a distant cluster of galaxies.
Upper left, original, unaberrated, and noise free. Upper right, input, aberrated, noise added.
Lower left, restoration, Richardson–Lucy. Lower right, restoration starlet deconvolution
dictionary Φ follow a leptokurtic PDF with heavy tails such as the generalized Gaussian
distribution form:
pdfα(α, . . ., αK) ∝
K
∏
k=
exp (−λ ∥αi∥p
p)
≤p < .
(.)
Between all possible solutions, we want the one which has the sparsest representation in
the dictionary Φ. Putting together the log-likelihood function in the case of Gaussian noise
σ and the priors on α, the MAP estimator leads to the following optimization problem:
min
α,...,αK

σ ∥Y −Φα∥+ λ
K
∑
k=
∥αk∥p
p ,≤p < .
(.)

Starlet Transform in Astronomical Data Processing 

The sparsity can be measured through the ∥α∥norm (i.e., p = ). This counts in fact
the number of nonzero elements in the sequence. It was also proposed to convexify the
constraint by substituting the convex ∥α∥norm for the ∥α∥norm []. Depending on the
H operator, there are several ways to obtain the solution of this equation.
A first iterative thresholding deconvolution method was proposed in [] which
consists of the following iterative scheme:
X(n+) = P+ (X(n) + HT (WDenM(n) (Y −HX(n)))),
(.)
where P+ is the projection on the cone of nonnegative images. and WDen is an operator
which performs a wavelet thresholding, i.e., applies the wavelet transform of the residual
R(n) (i.e., R(n) = Y −HX(n)), thresholds some wavelet coefficients, and applies the inverse
wavelet transform. Only coefficients that belong to the multiresolution support M(n) []
are kept, while the others are set to zero. At each iteration, the multiresolution support
M(n) is updated by selecting new coefficients in the wavelet transform of the residual which
have an absolute value larger than a given threshold. The threshold is automatically derived
assuming a given noise distribution such as Gaussian or Poisson noise.
More recently, it was shown [, , ] that a solution of
> Eq. .for p = can be
obtained through a thresholded Landweber iteration:
X(n+) = P+ (WDenλ (X(n) + HT (Y −HX(n)))),
(.)
with ∥H∥= . In the framework of monotone operator splitting theory, it was shown that
for frame dictionaries, a slight modification of this algorithm converges to the solution [].
Extension to constrained nonlinear deconvolution is proposed in [].
...
Constraints in the Object or Image Domains
Let us define the object domain O as the space in which the solution belongs, and the image
domain I as the space in which the observed data belongs (i.e., if X ∈O then HX ∈I).
The constraint in (> .) was applied in the image domain, while in (> .) we have
considered constraints on the solution. Hence, two different wavelet based strategies can
be chosen in order to regularize the deconvolution problem. The constraint in the image
domain through the multiresolution support leads to a very robust way to control the noise.
Indeed, whatever the nature of the noise, we can always derive robust detection levels in the
wavelet space and determine scales and positions of the important coefficients. A drawback
of the image constraints is that there is no guarantee that the solution is free of artifacts such
as ringing around point sources. A second drawback is that image constraints can be used
only if the point spread function is relatively compact, i.e., does not smear the information
over the whole image.
The property of introducing robust noise modeling is lost when applying the constraint
in the object domain. For example, in the case of Poisson noise, there is no way (except
using time consuming Monte Carlo techniques) to estimate the level of the noise in the
solution and to adjust properly the thresholds. The second problem with this approach is


Starlet Transform in Astronomical Data Processing
that, in fact, we try to solve two problems simultaneously (noise amplification and artifact
control in the solution) with one parameter (i.e., λ). The choice of this parameter is crucial,
while such a parameter is implicit when using the multiresolution support.
Ideally, constraints should be added in both the object and image domains in order
to better control the noise by using the multiresolution support and avoid artifact such as
ringing.
...
Example
A simulated Hubble Space Telescope Wide Field Camera image of a distant cluster of galax-
ies is shown in
> Fig. -, upper left. The simulated data are shown in
> Fig. -,
upper right. The Richardson–Lucy and the wavelet solutions are shown respectively in
> Fig. -, lower left and right. The Richardson–Lucy method amplifies the noise, which
implies that the faintest objects disappear in the deconvolved image, while the wavelet
starlet solution is stable for any kind of PSF, and any kind of noise modeling can be
considered.
..
Detection and Deconvolution
The PSF is not needed with MVM. This is an advantage when the PSF is unknown, or
difficult to estimate, which happens relatively often when it is space variant. However, when
the PSF is well determined, it becomes a drawback because known information is not used
for the object reconstruction. This can lead to systematic errors in the photometry, which
depends on the PSF and on the source signal-to-noise ratio. In order to preempt such a
bias, a kind of calibration must be performed using simulations []. This section shows
how the PSF can be used in the MVM, leading to a deconvolution.
...
Object Reconstruction Using the PSF
A reconstructed and deconvolved object Xi can be obtained by searching for a sig-
nal Xi such that the wavelet coefficients of HXi are the same as those of the detected
structures αi. If W describes the wavelet transform operator, and Pw the projection
operator in the subspace of the detected coefficients, the solution is found by minimiza-
tion of
min
Xi ∥αi −Pw (WHXi) ∥,
(.)
where αi represents the detected wavelet coefficients of the object Oi, and H is the PSF. In
this approach, each object is deconvolved separately. The flux related to the extent of the
PSF will be taken into account. For point sources, the solution will be close to that obtained
by PSF fitting. This problem is also different from the global deconvolution in the sense

Starlet Transform in Astronomical Data Processing 

that it is well constrained. Except for the positivity of the solution which is always true and
must be used, no other constraint is needed. This is due to the fact that the reconstruction
is performed from a small set of wavelet coefficients (those above a detection limit). The
number of objects is the same as those obtained by the MVM, but the photometry and the
morphology are different. The astrometry may also be affected.
...
The Algorithm
Any minimizing method can be used to obtain the solution Xi. Since there is no problem of
convergence, noise amplification, or ringing effect, the Van Cittert method was proposed
on the grounds of its simplicity []. It leads to the following iterative scheme:
X(n+)
i
= X(n)
i
+ R(αi −Pw (WHX(n)
i
)) ,
(.)
where R is the inverse wavelet transform, and the algorithm is:
. Set n to .
. Find the initial estimation X(n)
i
by applying an inverse wavelet transform to the set αi
corresponding to the detected wavelet coefficients in the data.
. Convolve X(n)
i
with the PSF H: Y(n)
i
= HX(n)
i
.
. Determine the wavelet transform α(Y(n)
i
) of Y(n)
i
.
. Threshold all wavelet coefficients in α(Y(n)
i
) at position and scales where nothing has
been detected (i.e., Pw operator). We get α
(Y(n)
i
)
t
.
. Determine the residual αr = αi −α
(Y(n)
i
)
t
.
. Reconstruct the residual image R(n) by applying an inverse wavelet transform.
. Add the residual to the solution: X(n+)
i
= X(n)
i
+ R(n).
. Threshold negative values in X(n+)
i
.
. If σ(R(n))/σ (X()
i
) < є then n = n + and go to step .
. X(n+)
i
contains the deconvolved reconstructed object.
In practice, convergence is very fast (less than iterations). The reconstructed image (not
deconvolved) can also be obtained just by reconvolving the solution with the PSF.
...
Space-Variant PSF
Deconvolution methods generally do not take into account the case of a space-variant PSF.
The standard approach when the PSF varies is to decompose the image into blocks, and
to consider the PSF constant inside a given block. Blocks which are too small lead to a
problem of computation time (the FFT cannot be used), while blocks which are too large
introduce errors due to the use of an incorrect PSF. Blocking artifacts may also appear.


Starlet Transform in Astronomical Data Processing
Combining source detection and deconvolution opens up an elegant way for deconvolution
with a space-variant PSF. Indeed, a straightforward method is derived by just replacing the
constant PSF at step of the algorithm with the PSF at the center of the object. This means
that it is not the image which is deconvolved, but its constituent objects.
...
Undersampled Point Spread Function
If the PSF is undersampled, it can be used in the same way, but results may not be optimal
due to the fact that the sampled PSF varies depending on the position of the source. If
an oversampled PSF is available, resulting from theoretical calculation or from a set of
observations, it should be used to improve the solution. In this case, each reconstructed
object will be oversampled.
> Equation (.) must be replaced by
min
Xi ∥αi −Pw (WDl HXi) ∥,
(.)
where Dl is the averaging-decimation operator, consisting of averaging the data in the
window of size l × l, and keeping only one average pixel for each l × l block.
...
Example: Application to Abell ISOCAM Data
> Figure -(left) shows the detections (isophotes) obtained using the MVM method
without deconvolution on ISOCAM data. The data were collected using the arcsecond
ISOCAM
+2
0
Arc minutes
-2
-2
0
Arc minutes
+2
ISOCAM
+2
0
Arc minutes
-2
-2
0
Arc minutes
+2
⊡Fig. -
Abell : left, ISOCAM source detection (isophotes) overplotted on an optical image (NTT,
band V). The ISOCAM image is a raster observation at µm. Right, ISOCAM source detection
using the PSF (isophotes) overplotted on the optical image. Compared to the left panel, it is
clearly easier to identify the detected infrared sources in the optical image

Starlet Transform in Astronomical Data Processing 

lens at .μm. This was a raster observation with s integration time, raster positions,
and frames per raster position. The noise is nonstationary, and the detection of the sig-
nificant wavelet coefficients was carried out using the root mean square error map Rσ(x, y)
by the method described in []. The isophotes are overplotted on an optical image (NTT,
band V) in order to identify the infrared source.
> Figure -(right) shows the same
treatment but using the MVM method with deconvolution. The objects are the same, but
the photometry is improved, and it is clearly easier to identify the optical counterpart of the
infrared sources.
.
Conclusion
In this chapter, we have used the sparsity principle that now occupies a very central role in
signal processing. We have discussed the vision models within which the sparsity principle
is applied. Finally, we have reviewed the use of the starlet wavelet transform as a prime
technique in order to apply the sparsity principle in the context of vision models in various
application domains.
Among the latter are object detection coupled with denoising, deconvolution, and fil-
tering generally. Issues of algorithmic optimization and of statistical modeling entered into
our discussion on various occasions. Many examples and case studies were used to demon-
strate the powerfulness of the approaches described for astronomical data analysis and
processing.
.
Cross-References
> EM Algorithms
> Iterative Solution Methods
> Large Scale Inverse Problems
> Linear Inverse Problems
> Numerical Methods for Variational Approach in Image Analysis
> Spline and Multiresolution Analysis
References and Further Reading
. Anscombe FJ () The transformation of
Poisson, binomial and negative-binomial data.
Biometrika :–
. Benvenuto
F,
La
Camera
A,
Theys
C,
Ferrari
A,
Lantéri
H,
Bertero
M
()
The study of an iterative method for the
reconstruction
of
images
corrupted
by
Poisson and Gaussian
noise. Inverse Probl
()
. Bertero M, Boccacci P () Introduction to
inverse problems in imaging. Institute of Physics,
Bristol
. Bertero M, Boccacci P, Desiderá G, Vicidomini G
() Image deblurring with Poisson data: from
cells to galaxies. Inverse Probl :
. Bertin E, Arnouts S (June ) Extractor:
software for source extraction. Astron Astrophys
Suppl Ser :–


Starlet Transform in Astronomical Data Processing
. Bijaoui A (Apr ) Sky background estimation
and application. Astron Astrophys :–
. Bijaoui A, Rué F () A multiscale vision model
adapted to astronomical images. Signal Process
:–
. Buonanno R, Buscema G, Corsi CE, Ferraro I,
Iannicola G (Oct ) Automated photographic
photometry of stars in globular clusters. Astron
Astrophys :–
. Chen SS, Donoho DL, Saunders MA ()
Atomic decomposition by basis pursuit. SIAM J
Sci Comput ():–
. Combettes PL, Wajs VR () Signal recovery by
proximal forward-backward splitting. Multiscale
Model Simulat ():–
. Da GS () Costa basic photometry techniques.
In: Howel SB (ed) ASP conference series ,
Astronomical CCD Observing and Reduction
Techniques, vol . Astronical Society of the
Pacific, p 
. Daubechies I, Defrise M, De Mol C () An
iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Commun
Pure Appl Math :–
. Davoust E, Pence WD () Detailed bibliogra-
phy on the surface photometry of galaxies. Astron
Astrophys Suppl Seri :–
. Debray B, Llebaria A, Dubout-Crillon R, Petit M
(Jan ) CAPELLA: software for stellar pho-
tometry in dense fields with an irregular back-
ground. Astron Astrophys :–
. Dempster A, Laird N, Rubin D () Maximum
likelihood from incomplete data via the EM algo-
rithm. J Roy Stat Soc B ():–
. Djorgovski S (Dec ) Modelling of seeing
effects in extragalactic astronomy and cosmology.
J Astrophys Astron :–
. DupéF-X,Fadili MJ,StarckJ-L () Aproximal
iteration for deconvolving Poisson noisy images
using sparse representations. IEEE Trans Image
Process ():–
. Engl HW, Hanke M, Neubauer A () Regular-
ization of inverse problems, vol of Mathemat-
ics and its applications. Kuwer Academic
. Figueiredo MA, Nowak R () An EM algo-
rithm for wavelet-based image restoration. IEEE
Trans Image Process ():–
. Geman S, Geman D () Stochastic relaxation,
Gibbs distributions and the Bayesian restoration
of images. IEEE Trans Pattern Anal Mach Intell
:–
. Irwin MJ (June ) Automatic analysis of
crowded fields. Monthly Notices Roy Astron Soc
:–
. Kron RG (June ) Photometry of a complete
sample of faint galaxies. Astrophys J Suppl Ser
:–
. Kurtz MJ () Classification methods: an intro-
ductory survey. In: Statistical methods in astron-
omy. European Space Agency Special Publication
, pp –
. Lefèvre O, Bijaoui A, Mathez G, Picat JP, Lelièvre
G () Electronographic BV photometry of
three distant clusters of galaxies. Astron Astro-
phys :–
. Lucy LB () An iteration technique for the
rectification of observed distributions. Astron J
:–
. Maddox SJ, Efstathiou G, Sutherland WJ (Oct
) The APM galaxy survey – Part Two –
Photometric corrections. Monthly Notices Roy
Astron Soc :
. Mallat S () A wavelet tour of signal pro-
cessing, the sparse way, rd edn. Academic,
New York
. Mallat S, Zhang Z () Matching pursuits with
time-frequency dictionaries. IEEE Trans Signal
Process ():–
. Moffat AFJ (Dec ) A theoretical investigation
of focal stellar images in the photographic emul-
sion and application to photographic photometry.
Astron Astrophys :
. Molina R, Ripley BD, Molina A, Moreno F,
Ortiz JL (Oct ) Bayesian deconvolution with
prior knowledge of object location – applications
to ground-based planetary images. Astrophys J
:–
. Murtagh F, Starck J-L, Bijaoui A () Image
restoration with noise suppression using a mul-
tiresolution support. Astron Astrophys Suppl Ser
:–
. Natterer F, Wûbbeling F () Mathemati-
cal methods in image reconstruction. SIAM,
Philadelphia
. Naylor T (May ) An optimal extraction algo-
rithm for imaging photometry. Monthly Notices
Roy Astron Soc :–

Starlet Transform in Astronomical Data Processing 

. Okamura S () Global structure of Virgo clus-
ter galaxies.In:ESO workshop onthevirgo cluster
of galaxies, pp –
. Pence WD, Davoust E () Supplement to
the detailed bibliography on the surface pho-
tometry of galaxies. Astron Astrophys Suppl Ser
:–
. Pierre M, Valtchanov I, Altieri B, Andreon S,
Bolzonella M, Bremer M, Disseau L, Dos Santos
S, Gandhi P, Jean C, Pacaud F, Read A, Refregier
A, Willis J, Adami C, Alloin D, Birkinshaw M,
Chiappetti L, Cohen A, Detal A, Duc P, Gosset
E, Hjorth J, Jones L, LeFevre O, Lonsdale C,
Maccagni D, Mazure A, McBreen B, McCracken
H, Mellier Y, Ponman T, Quintana H, Rottgering
H, Smette A, Surdej J, Starck J, Vigroux L, White S
(Sept ) The XMM-LSS survey. Survey design
and first results. J Cosmol Astropart Phys :
. Richardson WH () Bayesian-based iterative
method of image restoration. J Opt Soc Am
:–
. Shepp LA, Vardi Y () Maximum likelihood
reconstruction for emission tomography. IEEE
Trans Med Imaging MI-:–
. Starck J-L, Aussel H, Elbaz D, Fadda D, Cesarsky
C () Faint source detection in ISOCAM
images. Astron Astrophys Suppl Ser :–
. Starck J-L, Bijaoui A, Murtagh F () Mul-
tiresolution support applied to image filtering
and deconvolution. CVGIP: Graph Models Image
Process :–
. Starck J-L, Elad M, Donoho DL () Redun-
dant multiscale transforms and their applica-
tion for morphological component analysis. Adv
Imaging Electron Phys :–
. Starck J-L, Fadili J, Murtagh F () The undeci-
mated wavelet decomposition and its reconstruc-
tion. IEEE Trans Image Process :–
. Starck J-L, Murtagh F () Image restoration
with noise suppression using the wavelet trans-
form. Astron Astrophys :–
. Starck J-L,MurtaghF() Automaticnoiseesti-
mation from the multiresolution support. Publ
Astron Soc Pacific :–
. Starck J-L, Murtagh F () Astronomical image
and data analysis. Springer, New York
. Starck J-L, Murtagh F () Astronomical image
and data analysis, nd edn. Springer, Berlin
. Starck J-L, Murtagh F, Bijaoui A () Image
processing and data analysis: the multiscale
approach. Cambridge University Press, New York
. Starck J-L, Pierre M () Structure detection
in low intensity X-ray images. Astron Astrophys
Suppl Ser :–
. Starck J-L, Siebenmorgen R, Gredel R ()
Spectral analysis by the wavelet transform. Astro-
phys J :–
. Starck J-L, Murtagh F, Fadili J () Sparse image
& signal processing. Cambridge University Press,
Cambridge (UK)
. Takase B, Kodaira K, Okamura S () An atlas
of selected galaxies. University of Tokyo Press,
Tokyo
. Thonnat M () INRIA Rapport de Recherche.
Centre Sophia Antipolis, No. Automatic mor-
phological description of galaxies and classifica-
tion by an expert system
. Tikhonov AN, Goncharski AV, Stepanov VV,
Kochikov IV () Ill-posed image processing
problems. Soviet Phys Doklady :–
. Watanabe M, Kodaira K, Okamura S () Digi-
tal surface photometry of galaxies toward a quan-
titative classification. I. galaxies in the Virgo
cluster. Astron Astrophys Suppl Ser :–
. Zhang B, Fadili MJ, Starck J-L () Wavelets,
ridgelets and curvelets for Poisson noise removal.
IEEE Trans Image Process ():–


Diﬀerential Methods for
Multi-Dimensional Visual
Data Analysis
Werner Benger ⋅René Heinzl ⋅Dietmar Hildenbrand ⋅
Tino Weinkauf ⋅Holger Theisel ⋅David Tschumperl´e
.
Introduction.....................................................................
.
Modeling Data via Fiber Bundles..............................................
..
Differential Geometry: Manifolds, Tangential Spaces, and Vector Spaces.....
...
Tangential Vectors......................................................................
...Co-vectors...............................................................................
...Tensors...................................................................................
...Exterior Product........................................................................
...Visualizing Exterior Products........................................................
...Geometric Algebra.....................................................................
...Vector and Fiber Bundles.............................................................
..
Topology: Discretized Manifolds....................................................
..
Ontological Scheme and Seven-Level Hierarchy..................................
...Field Properties.........................................................................
...Topological Skeletons..................................................................
...Non-topological Representations. ...................................................
.
Differential Forms and Topology..............................................
..
Differential Forms......................................................................
...
Chains....................................................................................
...Cochains.................................................................................
...Duality between Chains and Cochains.............................................
..
Homology and Cohomology.........................................................
..
Topology................................................................................
.
Geometric Algebra Computing................................................
..
Benefits of Geometric Algebra ......................................................
...Unification of Mathematical Systems...............................................
...Uniform Handling of Different Geometric Primitives. ..........................
...Simplified Geometric Operations...................................................
...More Efficient Implementations. ....................................................
Otmar Scherzer (ed.), Handbook of Mathematical Methods in Imaging, DOI ./---_,
© Springer Science+Business Media LLC 


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
..
Conformal Geometric Algebra.......................................................
..
Computational Efficiency of Geometric Algebra Using Gaalop ................
.
Feature-based Vector Field Visualization.....................................
..
Characteristic Curves of Vector Fields..............................................
..
Derived Measures of Vector Fields..................................................
..
Topology of Vector Fields.............................................................
...Critical Points...........................................................................
...Separatrices. ............................................................................
...Application..............................................................................
.
Anisotropic Diffusion PDE’s for Image Regularization and
Visualization....................................................................
..
Regularization PDE’s : A review......................................................
...Local Multi-valued Geometry and Diffusion Tensors............................
...Divergence-based PDE’s..............................................................
...Trace-based PDE’s.....................................................................
...Curvature-Preserving PDE’s..........................................................
..
Applications.............................................................................
...Color Image Denoising...............................................................
...Color Image Inpainting................................................................
...Visualization of Vector and Tensor Fields..........................................

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Abstract: Images in scientific visualization are the end-product of data processing. Start-
ing from higher-dimensional datasets, such as scalar-, vector-, tensor- fields given on D,
D, D domains, the objective is to reduce this complexity to two-dimensional images
comprehensible to the human visual system. Various mathematical fields such as in partic-
ular differential geometry, topology (theory of discretized manifolds), differential topology,
linear algebra, Geometric Algebra, vectorfield and tensor analysis, and partial differential
equations contribute to the data filtering and transformation algorithms used in scientific
visualization. The application of differential methods is core to all these fields. The fol-
lowing chapter will provide examples from current research on the application of these
mathematical domains to scientific visualization and ultimately generating of images for
analysis of multi-dimensional datasets.
.
Introduction
▷
Scientists need an alternative to numbers. The use of images is a technical reality nowadays
and tomorrow it will be an essential requisite for knowledge. The ability of scientists to visu-
alize calculations and complex simulations is absolutely essential to ensure the integrity of
analyses, to promote scrutiny in depth and to communicate the result of such scrutiny to
others... The purpose of scientiﬁc calculation is looking, not enumerating. It is estimated that
% of the brain’s neurons are associated with vision. Visualization in a scientiﬁc calculation
is aimed at putting this neurological machinery to work [].
Since this visionary quote from an article in , scientific visualization, benefiting from
the affordable graphics hardware driven by the computer gaming industry, has grown
rapidly. Beyond academic research interests it has become also a consumer market with
practical applicability in industry and medicine. Still there are yet many gaps that are left
open due to the unequal evolution velocities in different fields. Once, there is the human
mind that is not able to keep up with the deluge of visual information which can be pro-
duced with modern technology. Many scientists still prefer looking at numbers instead of
utilizing modern display technology. At the same time, data can be produced by modern
supercomputers that is far beyond the ability of even high-end graphics engines to be pro-
cessed. Data sets originating from numerical simulations of physical processes will usually
be three-dimensional or four-dimensional, with images just the final result of the process
of scientific visualization. In this context images are the means to analyze data set of higher
dimensions.
Reducing numerical datasets to images is known as the concept of the visualiza-
tion pipeline. In its simplest form it consists of a data source (n-dimensional), a data
filter (an algorithmic operation), and a data-sink (an image). Data filters need to under-
stand the structure and meaning of the multi-dimensional input data and to operate
efficiently on them. This involves various mathematical fields such as in particular dif-
ferential geometry, topology (theory of discretized manifolds), differential topology, linear


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
algebra, Geometric Algebra, vectorfield and tensor analysis, and partial differential equa-
tions. Within a scientific visualization process all these mathematical fields will work
together, with more or less weighting. We subsume this set of mathematical domains as
“differential methods” in this chapter as the concept of differentiation is fundamental to
their approach of data analysis. The following sections will demonstrate the application
of the respective mathematical fields to visual analysis by virtue of examples of ongoing
research.
In > Sect. .we discuss the general issue of how to lay out data to model the structure
of space and time, as we know it from mathematics as foundation for further operations.
Frequently visualization algorithms are implemented ad hoc, given the problem, inventing
the solution with highest performance. This allegedly reasonable approach comes with an
unfortunate downside: incompatibility among independently developed solutions, which
impacts data exchange and interfacing complementary implementations. However, when
keeping a common data model in mind right from the earliest steps of conceiving some
algorithm, interoperability can be achieved at no cost with same performance as solitary
solutions.
Given a solid foundation for data structures, > Sect. .demonstrates how to for-
mulate differential operators using the concepts of chains, cochains, homology, and co-
homology. Since in computer graphics and visualization we have to deal with discretized
spaces, we arrive in the mathematical field of topology, as an essential descriptive tool for
meshes and all non-trivial grid structures.
When considering mathematics as a language unifying computer science, we need
to even more think about a common denominator within mathematics itself. Geometric
Algebra is a relatively new – or rather, re-discovered – branch of mathematics that is very
promising. It is extraordinarily visually intuitive, while covering the abstractions of Clifford
Algebra as used in quantum mechanics equally well as the formulations of curved space in
general relativity. However, even independent of such physics-oriented applications, Geo-
metric Algebra has found its merits within computer graphics itself. > Section .will
talk about the elegant usage of five-dimensional projective conformal Geometric Alge-
bra to handle primitives in computer graphics, and eventually implement the raytracing
algorithm with a few, well-defined algebraic operations.
The general goal of visualization is to give insight into large and complex data sets.
Due to the sheer size of the data sets alone, it is favorable if not necessary to automate
at least parts of the analysis. A way to achieve this is by extracting features. Features can
either be certain quantities derived from a data set or a mathematically well-defined, geo-
metric object (point, line, surface, . . .) with its definition and interpretation depending
on the underlying application, but usually it represents important structures (e.g., vortex,
stagnation point) or changes to such structures (events, bifurcations). A feature-based visu-
alization aims at the reduction of information to guide a user to the most interesting parts
of a data set. In > Sect. .we describe some important approaches to feature-based visu-
alization of vector fields. These include investigation of derived quantities such as vortices
(> Sect. ..) and the topology of vector fields (> Sect. ..). These approaches have
become a standard tool for the analysis of vector fields.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Finally, in > Sect. .we explore the capabilities of partial differential equations for
the filtering and regularization of image data sets. Applications are enhancing image qual-
ity by reducing noise or similar artifacts, as well as the visualization of vector and tensor
fields.
.
Modeling Data via Fiber Bundles
Purely numerical algorithms in C++ can be abstracted from concrete data structures using
programmings techniques such as generic programming []. However, generic algo-
rithms still need to make certain assumptions about the data they operate on. The question
remains what these concepts are that describe “data”: What properties should be expected
by some algorithm from any kind of data provided for scientific visualization? Moreover,
consistency among concepts shared by independent algorithms is also required to achieve
interoperability among algorithms and eventually (independently developed) applications.
While any particular problem can be addressed by some particular solution, a common
concept allows to build a framework instead of just a collection of tools. Tools are what
an end-user needs to solve a particular problem with a known solution. However, when a
problem is not yet clearly defined and a solution unknown, then a framework is required
that allows exploration of various approaches, and eventually adaption toward a specific
direction that does not exist a priori.
The concept how to layout data to perform visualization operations in a common
framework constitutes adata model for visualization. Many visualization applications are to
a greater or lesser extent a collection of tools, even when bundled together within the same
software library or binary. Consequently, interoperability between different applications
and their corresponding file formats is hard or impossible. Only very few implementations
adhere to the vision of a common data model across the various data types for visualiza-
tion. The idea of a common data model is frequently undervalued or even disregarded as
being impossible. However, as D. Butler said, “The proper abstractions for scientific data
are known. We just have to use them” [].
D. Butler was following the mathematical concepts of fiber bundles [], or more spe-
cific, vector bundles [], to model data. The IBM Data Explorer, one of the earliest visual-
ization applications, now Open Source and known as “OpenDX (http://www.opendx.org),”
implemented this concept successfully []. These ideas have been revived and expanded
by [] leading to a hierarchical data structure consisting of a non-cyclic graph in seven
levels. It can be seen as largely keyword-free, hierarchical version of the OpenDX model,
seeking to cast the information and relationships provided in original model into a
grouping structure. This data model will be reviewed in the following, together with its
mathematical background. > Section ..will review the basic mathematical structures
that are used to describe space and time. > Section ..will introduce the mathematical
formulation of discretized space. Based on this background, > Sect. ..will present a
scheme that is able to cover the described mathematical structures.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
..
Diﬀerential Geometry: Manifolds, Tangential Spaces,
and Vector Spaces
Space and time in physics is modeled via the concept of a differentiable manifold. As sci-
entific visualization deals with data given in space and time, following these concepts is
reasonable. In short, a manifold is a topological space that is locally homeomorphic to Rn.
However, not all data occurring in scientific visualization are manifolds. The more general
case of topological spaces will be discussed in > Sects. ..and > ...
A vector space over a field F (such as R) is a set V together with two binary operations
vector addition + : V × V →V and scalar multiplication ○: F × V →V. The mathematical
concept of a vector is defined as an element v ∈V. A vector space is closed under the
operations + and ○, i.e., for all elements u,v ∈V and all elements λ ∈F there is u+v ∈V and
λ ○u ∈V (vector space axioms). The vector space axioms allow computing the differences
of vectors and therefore defining the derivative of a vector-valued function v(s) : R →V as
d
dsv(s) := lim
ds→
v(s + ds) −v(s)
ds
(.)
A manifold in general is not a vector space. However, a differentiable manifold M allows
to define a tangential space TP(M) at each point P which has vector space properties.
...
Tangential Vectors
In differential geometry, a tangential vector on a manifold M is the operator d
ds that com-
putes the derivative along a curve q(s) : R →M for an arbitrary scalar-valued function
f : M →R:
d
ds f ∣
q(s)
:= d f (q(s))
ds
(.)
Tangential vectors fulfill the vector space axioms and can therefore be expressed as linear
combinations of derivatives along the n coordinate functions x μ : M →R with μ = . . .
n−, which define a basis of the tangential space Tq(s)(M) on the n-dimensional manifold
M at each point q(s) ∈M:
d
ds f =
n−
∑
μ=
dx μ (q(s))
ds
∂
∂x μ f =:
n−
∑
μ=
˙qμ∂μ f
(.)
where ˙qμ are the components of the tangential vector
d
ds in the chart {x μ} and {∂μ} are
the basis vectors of the tangential space in this chart. In the following text the Einstein
sum convention is used, which assumes implicit summation over indices occurring on the
same side of an equation. Often tangential vectors are used synonymous with the term
“vectors” in computer graphics when a direction vector from point A to point B is meant.
A tangential vector on an n-dimensional manifold is represented by n numbers in a chart.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

...
Co-vectors
The set of operations d f : T(M) →R that map tangential vectors v ∈T(M) to a scalar
value v(f ) for any function f : M →R defines another vector space which is dual to the
tangential vectors. Its elements are called co-vectors:
< d f ,v >= d f (v) := v(f ) = vμ∂μ f = vμ ∂f
∂x μ
(.)
Co-vectors fulfill the vector space axioms and can be written as linear combination of co-
vector basis functions dx μ:
d f =: ∂f
∂x μ dx μ
(.)
whereby the dual basis vectors fulfill the duality relation
< dx , ∂μ >=
⎧⎪⎪⎨⎪⎪⎩
μ =
:

μ /=
:

(.)
The space of co-vectors is called the co-tangential space T∗
P (M). A co-vector on an
n-dimensional manifold is represented by n numbers in a chart, same as a tangential vector.
However, co-vectors transform inverse to tangential vectors when changing coordinate sys-
tems, as is directly obvious from > Eq. (.) in the one-dimensional case: As <dx, ∂> = 
must be sustained under coordinate transformation, dxmust shrink by the same amount
as ∂grows when another coordinate scale is used to represent these vectors. In higher
dimensions this is expressed by an inverse transformation matrix.
In Euclidean three-dimensional space, a plane is equivalently described by a “normal
vector,” which is orthogonal to the plane. While “normal vectors” are frequently symbol-
ized by an arrow, similar to tangential vectors, they are not the same, rather they are dual
to tangential vectors. It is more appropriate to visually symbolize them as a plane. This
visual is also supported by (> .), which can be interpreted as the total differential of a
function f : A co-vector describes the change of a function f along a direction as speci-
fied by a tangential vector ⃗v. A co-vector V can thus be visually imagined as a sequence
of coplanar (locally flat) planes at distances given by the magnitude the co-vector, that
count the number of planes which are crossed by a vector ⃗w. This number is V(w). For
instance, for the Cartesian coordinate function x the co-vector dx “measures” the “cross-
ing rate” of a vector w in the direction along the coordinate line x, see
> Figs. -and
> -. On an n-dimensional manifold a co-vector is correspondingly symbolized by a
(n −)-dimensional subspace.
...
Tensors
A tensor Tm
n of rank n × m is a multi-linear map of n vectors and m co-vectors to a scalar
Tm
n : T(M) × ...T(M)

n
× T∗(M) × ...T∗(M)

m
→R
(.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
Vector ﬁeld ¶z
a
b
c
Duality relationship among 
¶z and dz 
Co-vector field dz 
⊡Fig. -
The (trivial) constant vector ﬁeld along the z-axis viewed as vector ﬁeld ∂z and as co-vector
ﬁeld dz
Radial ﬁeld dr, ¶r
Azimuthal ﬁeld dj, ¶j,
view of the equatorial
plane (z-axis towards eye)
Altitudal ﬁeld du, ¶u, 
slice along the z-axis
a
b
c
⊡Fig. -
The basis vector and co-vector ﬁelds induced by the polar coordinates {r, ϑ, φ}
Tensors are elements of a vector space themselves and form the tensor algebra. They are
represented relative to a coordinate system by a set of kn+m numbers for a k-dimensional
manifold. Tensors of rank may be represented using matrix notation. Tensors of type T

are equivalent to co-vectors and called co-variant, in matrix notation (relative to a chart)
they correspond to rows. Tensors of type T
are equivalent to a tangential vector and are
called contra-variant, corresponding to columns in matrix notation. The duality relation-
ship between vectors and co-vectors then corresponds to the matrix multiplication of a
× n row with a n × column, yielding a single number
< a, b >=< aμ∂μ, bμdx μ > ≡(aa. . . an)
⎛
⎜⎜⎜⎜
⎝
b
b
⋯
bn
⎞
⎟⎟⎟⎟
⎠
(.)

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

By virtue of the duality relationship (> .), the contraction of lower and upper indices is
defined as the interior product ι of tensors, which reduces the dimensionality of the tensor:
ι : Tm
n × T l
k →Tm−k
n−l
: u,v ↦ιuv
(.)
The interior product can be understood (visually) as a generalization of some “projection”
of a tensor onto another one.
Of special importance are symmetric tensors of rank two g ∈T
with g : T(M) ×
T(M) →R : u,v ↦g(u,v) , g(u,v) = g(v,u), as they can be used to define a metric or
inner product on the tangential vectors. Its inverse, defined by operating on the co-vectors,
is called the co-metric. A metric, same as the co-metric, is represented as a symmetric n×n
matrix in a chart for a n-dimensional manifold.
Given a metric tensor, one can define equivalence relationships between tangential
vectors and co-vectors, which allow to map one into each other. These maps are called
the “musical isomorphisms,” ♭and ♯, as they raise or lower an index in the coordinate
representation:
♭:T(M) →T∗(M) : vμ∂μ ↦vμgμ dx
(.)
♯:T∗(M) →T(M) : Vμdx μ ↦Vμg μ ∂
(.)
As an example application, the “gradient” of a scalar function is given by ∇f = ♯d f using
this notation. In Euclidean space, the metric is represented by the identity matrix and
the components of vectors are identical to the components of co-vectors. As computer
graphics usually is considered in Euclidean space, this justifies the usual negligence of dis-
tinction among vectors and co-vectors; consequently graphics software only knows about
one type of vectors which is uniquely identified by its number of components. However,
when dealing with coordinate transformations or curvilinear mesh types, then distinguish-
ing between tangential vectors and co-vectors is unavoidable. Treating them both as the
same type within a computer program leads to confusions and is not safe.
...
Exterior Product
The exterior product ∧: V×V →Λ(V) is an algebraic construction generating vector space
elements of higher dimensions from elements of a vector space V. The new vector space is
denoted Λ(V). It is alternating, fulfilling the property v ∧u = −u ∧v ∀u,v ∈V (which
results in v ∧v = ∀v ∈V). The exterior product defines an algebra on its elements, the
exterior algebra (or Grassman algebra). It is a sub-algebra of the Tensor algebra consisting
of the anti-symmetric tensors. The exterior algebra is defined intrinsically by the vector
space and does not require a metric. For a given n −dimensional vector space V, there
can at most be n-th power of an exterior product, consisting of n different basis vectors.
The (n + )th power must vanish, because at least one basis vector would occur twice, and
there is exactly one basis vector in Λn(V).
Elements v ∈Λk(V) are called k-vectors, whereby two-vectors are also called bi-
vectors and three-vectors tri-vectors. The number of components of an k-vector of an


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
n-dimensional vector space is given by the binomial coefficient (n
k). For n = there are
two one-vectors and one bi-vector, for n = there are three one-vectors, three bi-vectors,
and one tri-vector. These relationships are depicted by the Pascal’s triangle, with the row
representing the dimensionality of the underlying base space and the column the vector
type:















(.)
As can be easily read off, for a four-dimensional vector space there will be four one-vectors,
six bi-vectors, four tri-vectors and one four-vector. The n-vector of a n-dimensional vector
space is also called a pseudo-scalar, the (n −) vector a pseudo-vector.
...
Visualizing Exterior Products
An exterior algebra is defined on both the tangential vectors and co-vectors on a manifold.
A bi-vector v formed from tangential vectors is written in chart as
v = vμ ∂μ ∧∂
(.)
a bi-covector U formed from co-vectors is written in chart as
U = Uμ dx μ ∧dx
(.)
They both have (n
) independent components, due to vμ
= −v
μ and Uμ
= −U
μ (three
components in D, six components in D). A bi-tangential vector (> .) can be under-
stood visually as an (oriented, i.e., signed) plane that is spun by the two defining tangential
vectors, independently of the dimensionality of the underlying base space. A bi-co-vector
(> .) corresponds to the subspace of an n-dimensional hyperspace where a plane is
“cut out.” In three dimensions these visualizations overlap: both a bi-tangential vector and
a co-vector correspond to a plane, and both a tangential vector and a bi-co-vector corre-
spond to one-dimensional direction (“arrow”). In four dimensions, these visuals are more
distinct but still overlap: a co-vector corresponds to a three-dimensional volume, but a bi-
tangential vector is represented by a plane same as a bi-co-vector, since cutting out a D
plane from four-dimensional space yields a D plane again. Only in higher dimensions
these symbolic representations become unique. However, in any case, a co-vector and a
pseudo-vector will have the same appearance as an n −dimensional hyperspace, same as
a tangential vector corresponds to an pseudo-co-vector:
Vμdx μ ⇔vαα...αn−∂α∧∂α∧. . . ∂αn−
(.)
vμ∂μ ⇔Vαα...αn−dxα∧dxα∧. . . dxαn−
(.)

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

A tangential vector – lhs of (> .) – can be understood as one specific direction, but
equivalently as well as “cutting off” all but one n −-dimensional hyperspaces from an
n-dimensional hyperspace – rhs of (> .). This equivalence is expressed via the interior
product of a tangential vector v with an pseudo-co-scalar Ω yielding a pseudo-co-vector
V (> .), similarly the interior product of a pseudo-vector with an pseudo-co-scalar
yielding a tangential vector (> .):
ιΩ : T(M) →(T∗)(n−)(M) : V ↦ιΩv
(.)
ιΩ : T(n−)(M) →T∗(M) : V ↦ιΩv
(.)
Pseudo-scalars and pseudo-co-scalars will always be scalar multiples of the basis vectors
∂α∧∂α∧. . . ∂αn and dxα∧dxα∧. . . dxαn. However, when inversing a coordinate x μ →
−x μ they flip sign, whereas a “true” scalar does not. An example known from Euclidean
vector algebra is the allegedly scalar value constructed from the dot and cross product
of three vectors V(u,v,w) = u⋅(v × w) which is the negative of when its arguments are
flipped:
V(u,v,w) = −V(−u,−v,−w) = −u⋅(−v × −w)
(.)
which is actually more obvious when (> .) is written as exterior product:
V(u,v,w) = u ∧v ∧w = V∂∧∂∧∂
(.)
The result (> .) actually describes the multiple of a volume element span by the basis
tangential vectors ∂μ – any volume must be a scalar multiple of this basis volume element,
but can flip sign if another convention on the basis vectors is used. This convention depends
on the choice of a right-handed versus left-handed coordinate system, and is expressed by
the orientation tensor Ω = ±∂∧∂∧∂. In computer graphics, both left-handed and
right-handed coordinate systems occur, which may lead to lots of confusions.
By combining (> .) and (> .) – requiring a metric – we get a map from pseudo-
vectors to vectors and reverse. This map is known as the Hodge star operator “∗”:
∗: T(n−)(M) →T(M) : V $→♯ιΩV
(.)
The same operation can be applied to the co-vectors accordingly, and generalized to all
vector elements of the exterior algebra on a vector space, establishing a correspondence
between k −vectors and n −k-vectors. The Hodge star operator allows to identify vectors
and pseudo-vectors, similarly to how a metric allows to identify vectors and co-vectors.
The Hodge star operator requires a metric and an orientation Ω.
A prominent application in physics using the hodge star operator are the Maxwell equa-
tions, which, when written based on the four-dimensional potential A = Vdx+ Akdxk
(Vthe electrostatic, Ak the magnetic vector potential), take the form
d ∗dA = J
(.)
with J the electric current and magnetic flow, which is zero in vacuum. The combination
d ∗d is equivalent to the Laplace operator “2,” which indicates that (> .) describes
electromagnetic waves in vacuum.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
...
Geometric Algebra
Geometric Algebra is motivated by the intention to find a closed algebra on a vector space
with respect to multiplication, which includes existence of an inverse operation. There is no
concept of dividing vectors in “standard” vector algebra. Neither the inner or outer product
have provide vectors of the same dimensionality as their arguments, so they do not provide
a closed algebra on the vector space.
Geometric Algebra postulates a product on elements of a vector space u,v,w ∈V that
is associative, (uv)w = u(vw), left-distributive u(v + w) = uv + uw, right-distributive
(u + v)w = uw + vw, and reduces to the inner product as defined by the metric v=
g(v,v). It can be shown that the sum of the outer product and the inner product fulfill
these requirements; this defines the geometric product as the sum of both:
uv := u ∧v + u ⋅v
(.)
Since u ∧v and u ⋅v are of different dimensionality ((n
) and (n
), respectively), the result
must be in a higher dimensional vector space of dimensionality (n
) + (n
). This space is
formed by the linear combination of k-vectors, its elements are called multivectors. Its
dimensionality is ∑n−
k=(n
k) ≡n.
For instance, in two dimensions, the dimension of the space of multivectors is = .
A multivector V, constructed from tangential-vectors on a two-dimensional manifold, is
written as
V = V + V ∂+ V ∂+ V ∂∧∂
(.)
with V μ the four components of the multivector in a chart. For a three-dimensional man-
ifold, a multivector on its tangential space has = components and is written as
V =V +
V ∂+ V ∂+ V ∂+
V ∂∧∂+ V ∂∧∂+ V ∂∧∂+
V ∂∧∂∧∂
(.)
with V μ the eight components of the multivector in a chart. The components of a mul-
tivector have a direct visual interpretation, which is one of the key features of Geometric
Algebra. In D, a multivector is the sum of a scalar value, three directions, three planes,
and one volume. These basis elements span the entire space of multivectors. Geometric
Algebra provides intrinsic graphical insight to the algebraic operations. Its application for
computer graphics will be discussed in > Sect. ..
...
Vector and Fiber Bundles
The concept of a fiber bundle data model is inspired by its mathematical correspondence.
In short, a fiber bundle is a topological space that looks locally like a product space B × F
of a base space B and a fiber space F.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

The fibers of a function f : X &→Y are the pre-images or inverse images of the points
y ∈Y, i.e., the sets of all elements x ∈X with f (x) = y:
f −(y) = {x ∈X ∣f (x) = y}
is a fiber of f (at the point y). A fiber can also be the empty set. The union set of all fibers of
a function is called the total space. The definition of a fiber bundle makes use of a projection
map pr, which is a function that maps each element of a product space to the element of
the first space:
pr: X × Y &→X
(x, y) $→x
Let E, B be topological spaces and f : E &→B a continuous map. (E, B, f ) is called
a (fiber) bundle if there exists a space F such that the union of fibers of a neighborhood
Ub ⊂B of each point b ∈B are homeomorphic to Ub × F such that the projection prof
Ub × F is Ub again:
(E, B, f : E →B) bundle ⇐⇒∃F : ∀b ∈B : ∃Ub : f −(Ub)
hom
≃Ub × F
and
pr(Ub × F) = Ub
E is called the total space E, B is called the base space, and f : E &→B the projection map.
The space F is called the fiber type of the bundle or simply the fiber of the bundle. In other
words, the total space can be written locally as a product space of the base space with some
space F. The notation F(B) = (E, B, f ) will be used to denote a fiber bundle over the base
space B. It is also said that the space F fibers over the base space B.
An important case is the tangent bundle, which is the union of all tangent spaces Tp(M)
on a manifold M together with the manifold T (M) := {(p,v) : p ∈M,v ∈Tp(M)}.
Every differentiable manifold possesses a tangent bundle T (M). The dimension of T (M)
is twice the dimension of the underlying manifold M, its elements are points plus tangential
vectors. Tp(M) is the fiber of the tangent bundle over the point p.
If a fiber bundle over a space B with fiber F can be written as B × F globally, then it is
called a trivial bundle (B×F, B, pr). In scientific visualization, usually only trivial bundles
occur. A well known example for a non-trivial fiber bundles is the Möbius strip.
..
Topology: Discretized Manifolds
For computational purposes, a topological space is modeled by a finite set of points. Such
a set of points intrinsically carries a discrete topology by itself, but one usually consid-
ers embeddings in a space that is homeomorphic to Euclidean space to define various
structures describing their spatial relationships.
A subset c ⊂X of a Hausdorff space X is a k-cell if it is homeomorphic to an open
k-dimensional ball in Rn. The dimension of the cell is k. zero-cells are called vertices,


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
one-cells are edges, two-cells are faces or polygons, three-cells are polyhedra – see also
> Sect. .... An n-cell within an n-dimensional space is just called a “cell.” (n −)-cells
are sometimes called “facets” and (n −)-cells are known as “ridges.” For k-cells of arbi-
trary dimension, incidence and adjacency relationships are defined as follows: Two cells
c, care incident if c⊆∂c, where ∂cdenotes the border of the cell c. Two cells of the
same dimension can never be incident because dim(c) ≠dim(c) for two incident cells
c, c. cis a side of cif dim(c) < dim(c), which may be written as c< c. The special
case dim(c) = dim(c) −may be denoted by c≺c. Two k−cells c, cwith k > are
called adjacent if they have a common side, i.e.,
cell c, cadjacent ⇐⇒∃cell f : f < c, f < c
For k = , two zero-cells (i.e., vertices) v,vare said to be adjacent if there exists a one-
cell (edge) e which contains both, i.e., v< e and v< e. Incidence relationships form an
incidence graph. A path within an incidence graph is a cell-tuple: A cell-tuple C within an
n-dimensional Hausdorff space is an ordered sequence of k-cells (cn, cn−, . . . , c, c) of
decreasing dimensions such that ∀< i ≤n : ci−≺ci. These relationships allow to deter-
mine topological neighborhoods: Adjacent cells are called neighbors. The set of all k+cells
which are incident to a k-cell forms a neighborhood of the k-cell. The cells of a Hausdorff
space X constitute a topological base, leading to the following definition: A (“closure-finite,
weak-topology”) CW-complex C, also called a decomposition of a Hausdorff space X, is a
hierarchical system of spaces X(−) ⊆X() ⊆X() ⊆⋅⋅⋅⊆X(n), constructed by pairwise
disjoint open cells c ⊂X with the Hausdorff topology ⋃c∈C c, such that X(n) is obtained
from X(n−) by attaching adjacent n-cells to each (n −)-cell and X(−) = /. The respective
subspaces X(n) are called the n-skeletons of X. A CW-complex can be understood as a set
of cells which is glued together at their subcells. It generalizes the concept of a graph by
adding cells of dimension greater than .
Up to now, the definition of a cell was just based on a homeomorphism of the underly-
ing space X and Rn. Note that a cell does not need to be “straight,” such that e.g. a two-cell
may be constructed from a single vertex and an edge connecting the vertex to itself, as, e.g.,
illustrated by J. Hart []. Alternative approaches toward the definition of cells are more
restrictively based on isometry to Euclidean space, defining the notion of “convexity” first.
However, it is recommendable to avoid the assumption of Euclidean space, and treating
the topological properties of a mesh purely based on its combinatorial relationships.
..
Ontological Scheme and Seven-Level Hierarchy
The concept of the fiber bundle data model builds on the paradigm that numerical data
sets occurring for scientific visualization can be formulated as trivial fiber bundles (see
> Sect. ...). Hence, data sets may be distinguished by their properties in the base
space and the fiber space. At each point of the – discretized – base space, there are some

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

data in the fiber space attached. Basically a fiber bundle is a set of points with neighborhood
information attached to each of them. An n-dimensional array is a very simple case of a
fiber bundle with neighborhood information given implicitly.
The structure of the base space is described as a CW-complex, which categorizes the
topological structure of an n-dimensional base space by a sequence of k-dimensional
skeletons, with < k < n. These skeletons carry certain properties of the data set: the zero-
skeleton describes vertices, the one-skeleton refers to edges, two-skeleton to the faces, etc.,
of some mesh (a triangulation of the base space). Structured grids are triangulations with
implicitly given topological properties. For instance, a regular n-dimensional grid is one
where each point has n neighbors.
The structure of the fiber space is (usually) not discrete and given by the properties of
the geometrical object residing there, such as a scalar, vector, co-vector, and tensor. Same as
the base space, the fiber space has a specific dimensionality, though the dimensionality of
the base space and fiber space is independent. > Figure -demonstrates example images
from scientific visualization classified via their fiber bundle structure. If the fiber space has
vector space properties, then the fiber bundle is a vector bundle and vector operations can
be performed on the fiber space, such as addition, multiplication, and derivation.
The distinction between base space and fiber space is not common use in computer
graphics, where topological properties (base space) are frequently intermixed with geomet-
rical properties (coordinate representations). Operations in the fiber space can, however,
be formulated independently from the base space, which leads to a more reusable design
of software components. Coordinate information, formally part of the base space, can as
well be considered as fiber, leading to further generalization. The data sets describing a
fiber are ideally stored as contiguous arrays in memory or disk, which allows for opti-
mized array and vector operations. Such a storage layout turns out to be particularly useful
for communicating data with the GPU using vertex buffer objects: the base space is given
by vertex arrays (e.g., OpenGL glVertexPointer), fibers are attribute arrays (e.g.,
OpenGL glVertexAttribPointer), in the notation of computer graphics. While the
process of hardware rendering in its early times had been based on procedural descriptions
(cached in display lists), vertex buffer objects are much faster in state-of-the-art technol-
ogy. Efficient rendering routines are thus implemented as maps from fiber bundles in RAM
to fiber bundles in GPU memory (eventually equipped with a GPU shader program).
A complex data structure (such as some color-coded time-dependent geometry) will
be built from many data arrays. The main question that needs to be answered by a data
model is how to assign a semantic meaning to each of these data arrays – what do the
numerical values actually mean? It is always possible to introduce a set of keywords with
semantics attached to them. However, the choice of keywords is arbitrary and requires
agreements about the used conventions, besides that keywords also pollute the name space
of identifiers. The approach followed in the data model presented in [] is to avoid use of
keywords as much as possible. Instead, it assigns the semantics of an element of the data
structure into the placement of this element. The objective is to describe all data types that
occur in an algorithm (including file reader and rendering routines) within this model.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
It is formulated as a graph of up to seven levels (two of them optional). Each level represents
a certain property of the entire data set, the “Bundle.” These levels are called
. Slice
. Grid
. Skeleton
. Representation
. Field
. (Fragment)
. (Compound Elements)
Actual data arrays are stored only below the “Field” level. Given one hierarchy level, the
next one is accessed via some identifier. The type of this identifier differs for each level:
Hierarchy object
Identifier type
Identifier semantic
Bundle
Floating point number
Time value
Slice
String
Grid name
Grid
Integer set
Topological properties
Skeleton
Reference
Relationship map
Representation
String
Field name
Field
Multidimensional index
Array index
Numerical values within a Skeleton level are grouped into Representation objects,
which hold all information that is relative to a certain “representer.” Such a representer may
be a coordinate object that for instance refers to some Cartesian or polar chart, or it may
well be another Skeleton object, either within the same Grid object or even within
another one. An actual data set is described through the existence of entries in each level.
Only two of these hierarchy levels are exposed to the end-user, these are the “Grid” and
“Field” levels. Their corresponding textual identifiers are arbitrary names specified by
the user.
A Grid is subset of data within the Bundle that refers to a specific geometrical entity.
A Grid might be a mesh carrying data such as a triangular surface, a data cube, a set of
data blocks from a parallel computation, or many other data types. A Field is the col-
lection of data sets given as numbers on a specific topological component of a Grid, for
instance floating point values describing pressure or temperature on a Grid’s vertices. All
other levels of the data model describe the properties of the Bundle as construction blocks.
The usage of these construction blocks constitutes a certain language to describe data sets.
A Slice is identified by a single floating point number representing time (generalization to
arbitrary-dimensional parameter spaces is possible). A Skeleton is identified by its dimen-
sionality, index depth (relationship to the vertices of a Grid), and refinement level. This will
be explained in more detail in > Sect. .... The scheme also extends to cases beyond the
purely mathematical basis to also cover data sets that occur in praxis, which is described

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

in > Sect. .... A Representation is identified via some reference object, which may
be some coordinate system or another Skeleton. The lowest levels of Fragments and Com-
pounds describe the internal memory layout of a Field data set and are optional, some
examples are described in [, ].
...
Field Properties
A specific Field identifier may occur in multiple locations. All these locations together
define the properties of a field. The following four properties are expressible in the data
model:
() Hierarchical ordering: For a certain point in space, there exist multiple data values, one
for each refinement level. This property describes the topological structure of the base
space.
() Multiple coordinate systems: One spatial point may have multiple data representa-
tions relating to different coordinate systems. This property describes the geometrical
structure of the base space.
() Fragmentation: Data may stem from multiple sources, such as a distributed multipro-
cess simulation. The field then consists of multiple data blocks, each of them covering
a subdomain of the field’s base space. Such field fragments may also overlap, known
as “ghostzones.”
() Separated Compounds: A compound data type, such as a vector or tensor, may be
stored in different data layouts since applications have their own preferences. An
array of tensors may also be stored as a tensor of arrays, e.g., XYZXYZXYZXYZ
as XXXXYYYYZZZZ. This property describes the internal structure of the fiber
space.
All of these properties are optional. In the most simple case, a field is just represented by an
array of native data types; however, in the most general case (which the visualization algo-
rithm must always support), the data are distributed over several such property elements
and built from many arrays. With respect to quick transfer to the GPU, only the ability to
handle multiple arrays per data set is of relevance.
> Figure -illustrates the organization of these four properties within the four last
levels of the data model, Skeleton, Representation, fragmentation, and compound
components. The ordering of these levels is done merely based on their semantic impor-
tance, with the uppermost level () embracing multiple resolutions of the spatial domain
being the most visible one to the end-user. Each of these resolution levels may come with
different topological properties, but all arrays within the same resolution are required to be
topologically compatible (i.e., share the same number of points). There might still be mul-
tiple coordinate representations required for each resolution, which constitutes the second
hierarchy level () of multiple coordinate patches. Data per patch may well be distributed
over various fragments (), which is considered an internal structure of each patch, due


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
Refinement level 0
Refinement level 1
Refinement level 2
Refinement level 2
Fragment 0
Fragment 1
Fragment 2
Scalar-, vector-, Tensorfield
Patch 0
S
S
S
S
S
S
S
X
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZyZ
XX Xy
yy
ZX
ZZyZ
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZ yZ
XX Xy
yy
ZX
ZZ yZ
Y
Z
X Y
Z
X Y
Z
X Y
Z
X Y
Z
X Y
Z
X Y
Z
X Y
Z
X Y
Z
S
S
Patch 1
Patch 2
⊡Fig. -
Hierarchical structure of the data layout of the concept of a ﬁeld in computer memory:
() Organization by multiple resolutions for same spatial domain; () multiple coordinate
systems covering diﬀerent spatial domains (arbitrary overlap possible); () fragmentation of
ﬁelds into blocks (recombination from parallel data sources); () layout of compound ﬁelds
as components for performance reasons, indicated as S (scalar ﬁeld), {x, y, z} for vector
ﬁelds and {xx, xy, yy, yz, zz, zx} for tensor ﬁelds
to parallelization or numerical issues, but not fundamental to the physical setup. Last not
least fields of multiple components such as vector or tensor fields may be separated into
distinct arrays themselves []. This property, merely a performance issue of in-memory
data representation, is not what that the end-user usually does not want to be bothered
with, and is thus set as the lowest level in among these four entries.
...
Topological Skeletons
The Skeleton level of the fiber bundle hierarchy describes a certain topological prop-
erty. This can be the vertices, the cells, the edges, etc. Its primary purpose is to describe
the skeletons of a cw-complex, but they may also be used to specify mesh refinement levels
and agglomerations of certain elements. All data fields that are stored within a Skeleton
level provide the same number of elements. In other words share their index space (a data
space in HDFterminology). Each Topology object within a Grid object is uniquely
identified via a set of integers, which are the dimension (e.g., the dimension of a k-cell),
index depth (how many dereferences are required to access coordinate information in the
underlying manifold), and refinement level (a multidimensional index, in general). Ver-
tices – index depth – of a topological space of dimension n define a Skeleton of type
(n,). Edges are one-dimensional sets of vertex indices, therefore of index depth and

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Zero-cells, 0D
a
Zero-cells, 1D
b
Zero-cells, 3D
c
Zero-cells, 6D
d
One-cells, 0D
e
One-cells, 1D
f
One-cells, 3D
g
One-cells, 6D
h
Two-cells, 0D
i
Two-cells, 1D
j
Two-cells, 3D
k
Two-cells, 6D
l
Three-cells, 0D
m
Three-cells, 1D
n
Three-cells, 3D
o
Three-cells, 6D
p
⊡Fig. -
Fiber-bundle classiﬁcation scheme for visualization methods: dimensionality of the
base-space (involving the k-skeleton of the discretized manifold) and dimensionality of the
ﬁber-space (involving the number of ﬁeld quantities per element, zero referring to display
of the mere topological structure)
define Skeleton type (,). Faces are two-dimensional sets of vertex indices, hence Skele-
ton type (,). Cells – such as a tetraedron or hexaeder – are described by a Skeleton
type (,). All the Skeleton objects of index depth build the k−Skeletons of a manifold’s
triangulation.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
T=9,600
(slice)
T=1,290
ADCRIC
(slice)
(grid)
(grid)
(topology)
(topology)
(topology)
(representation)
(representation)
(representation)
MM5
Points
Points
Points
Cartesian
174
174
135
135
174
600k
Connectivity
Elevation
Points
Pressure
Temperature
Wind
600k
1.2M
135
43
Cartesian
Connectivity
MM5 Atmospheric model
ADCIRC storm-surge model
⊡Fig. -
The ﬁve-level organization scheme used for atmospheric data (MMmodel data) and surge
data (ADCIRC simulation model), built upon common topological property descriptions
with additional ﬁelds (From Venkataraman et al., )
Higher index depths describe sets of k-cells. For instance, a set of edges describes a
line – a path along vertices in a Grid. Such a collection of edges will fit into a Skeleton of
dimension and depth , i.e., type (,). It is a one-dimensional object of indices that refer
to edges that refer to vertices.
...
Non-topological Representations
Polynomial coordinates, information on field fragments, histograms, and color maps can
be formulated in the fiber bundle model as well. These quantities are no longer direct cor-
respondences of the mathematical background, but they may still be cast into the given
context.
Coordinates may be given procedurally, such as a via some polynomial expression. The
data for such expressions may be stored in a Skeleton of negative index depth – as these
data are required to compute the vertex coordinates and more fundamental than these in
this case.
A fragment of a Field given on vertices – the (n,)-Skeleton of a Grid – defines an
n-dimensional subset of the Grid, defined by the hull of the vertices corresponding to

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

the fragments. These may be expressed as a (n,)-Skeleton, where the positions field
(represented relative to the vertices) refers to the (global) vertex indices of the respec-
tive fragments. The representation in coordinates corresponds to its range, known as the
bounding box. Similarly, a field given on the vertices will correspond to the field’s numerical
minimum/maximum range within this fragment.
A histogram is the representation of a field’s vertex complex in a “chart” describing the
required discretization, depending on the min/max range and a number count. A color map
(transfer function) can be interpreted as a chart object itself. It has no intrinsically geomet-
rical meaning, but provides means to transform some data. For instance, some scalar value
will be transformed to some RGB tripel using some colormap. A scalar field represented
in a certain color map is therefore of type RGB values, and could be stored as an array of
RGB values for each vertex. In practice, this will not be done since such transformation is
performed in realtime by modern graphics hardware. However, this interpretation of a col-
ormap as a chart object tells how colormaps may be stored in the fiber bundle data model.
.
Diﬀerential Forms and Topology
This section introduces not only the concepts of differential forms and their discrete coun-
terparts, but also illustrates that similar concepts are applied in several separate areas of
scientific visualization. Since the available resources are discrete and finite, concepts mir-
roring these characteristics have to be applied to visualize complex data sets. The most
distinguished algebraic structure is described by exterior algebra (or Grassmann algebra,
see also > Sect. ...), which comes with two operations, the exterior product (or wedge
product) and the exterior derivative.
..
Diﬀerential Forms
Manifolds can be seen as a precursor to model physical quantities of space. Charts on
a manifold provide coordinates, which allows using concepts which are already well
established. Furthermore they are crucial for the field of visualization, as they are key
components to obtain depictable expressions of abstract entities. Tangential vectors were
already introduced in > Sect. ...as derivatives along a curve. Then a one-form α is
defined as a linear mapping which assigns a value to each tangential vector v from the
tangent space TP(M), i.e., α : TP(M) →R. They are commonly called co-variant vec-
tors, co-vectors (see > Sect. ...), or Pfaff-forms. The set of one-forms generates the
dual vector space or co-tangent space T∗
P (M). It is important to highlight that the tangent
vectors v ∈TP(M) are not contained in the manifold itself, so the differential forms also
generate an additional space over P ∈M. In the following, these one-forms are generalized
to (alternating) differential forms.
An alternative point of view treats a tangential vector v as a linear mapping which
assigns a scalar to each one-form α by < α,v >∈R. By omitting one of the arguments


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
of the obtained mappings, < α,. > or α(v), and < .,v > or v(α), linear objects are defined.
Multi-linear mappings depending on multiple vectors or co-vectors appear as an extension
of this concept and are commonly called tensors
γ : T∗m × Tn →R
(.)
where n and m are natural numbers, Tn and T∗m represent the n and m powered Cartesian
product of the tangential space or the dual vector space (co-tangential space). A tensor γ is
called an (n, m)-tensor which assigns a scalar value to a set of m co-vectors and n vectors.
All tensors of a fixed type (n, m) generate a tensor space attached at the point P ∈M. The
union of all tensor spaces at the points P ∈M is called a tensor bundle. The tangential and
co-tangential bundles are specialized cases for (,) and (,) tensor bundles, respectively.
Fully anti-symmetric tensors of type (, m) may be identified with differential forms of
degree m. For m > dim(M), where dim(M) represents the dimension of the manifold,
differential forms vanish.
The exterior derivative or Cartan derivative of differential forms generates a p + -form
df from a p-form f and conforms to the following requirements:
. Compatibility with the wedge product (product rule):
d(α ∧β) = dα ∧β + (−)mα ∧dβ
. Nilpotency of the operation d, d ○d = , depicted in > Fig. -
. Linearity
A subset of one-forms is obtained as a differential df of zero-forms (functions) f at
P and are called exact differential forms. For an n-dimensional manifold M, a one-form
can be depicted by drawing (n −)-dimensional surfaces, e.g., for the three-dimensional
space, > Fig. -depicts a possible graphical representation of a one-form attached to M.
This depiction also enables a graphical representation how to integrate differential forms,
where only the number of surfaces which are intersected by the integration domain have
to be counted:
< d f ,v >= d f (v) = α(v)
(.)
A consequence of being exact includes the closeness property dα = . Furthermore
the integral ∫Cp d f with Cp representing an integration domain, e.g., an interval xand x,
results in the same value f (x) −f (x). In the general case, a p-form is not always the
exterior derivative of a p-one-form, therefore the integration of p-forms is not indepen-
dent of the integration domain. An example is given by the exterior derivative of a p-form
β resulting in a p + -form γ = dβ. The structure of such a generated differential form can
be depicted by a tube-like structure such as in
> Fig. -. While the wedge product of
an r-form and an s-form results in a r + s-form, this resulting form is not necessarily rep-
resentable as a derivative.
> Figure -depicts a two-form which is not constructed by
the exterior derivative, but instead by α ∧β, where α and β are one-forms. In the gen-
eral case, a p-form attached on an n-dimensional manifold M is represented by using
(n −p)-dimensional surfaces.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

⊡Fig. -
Possible graphical representation of the topological structure of one-forms in three
dimensions. Note that the graphical display of diﬀerential forms varies in diﬀerent
dimension and does not depend on the selected basis elements
By sequentially applying the operation d to (, m) for ≤m ≤dim(M), the
deRham complex is obtained, which enables the investigation of the relation of closed and
exact forms. The deRham complex enables the transition from the continuous differential
forms to the discrete counterpart, so called cochains. The already briefly mentioned topic
of integration of differential forms is now mapped onto the integration of these cochains.
To complete the description, the notion of chains, also modeled by multivectors (as used in
Geometric Algebra, see > Sects. ...and > .) or fully anti-symmetric (n,)-tensors,
as description of integration domains is presented, where a chain is a collection of n-cells.
The connection between chains and cochains is investigated in algebraic topology
under the name of homology theory, where chains and cochains are collected in additive
Abelian groups Cp(M).
...
Chains
As the deRham complex collects cochains, a cell complex aggregates chain elements, cells.
To use these elements, e.g., all edges, in a computational manner, a mapping of the n-cells
onto an algebraic structure is needed. An algebraic representation of the assembly of cells,


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
⊡Fig. -
Possible graphical representation of a general two-form generated by α ∧β, where α and β
are one-forms. The topologically tube-like structure of the two-forms is enclosed by the
depicted planes
an n-chain, over a cell complex K and a vector space V can be written by
cn =
j
∑
i=
wiτi
n
τi
n ∈K,wi ∈V
which is closed under reversal of the orientation:
∀τi
n ∈cn there is −τi
n ∈cn
The different topological elements are called cells, and the dimensionality is expressed
by adding the dimension such as a three-cell for a volume, a two-cell for surface ele-
ments, a one-cell for lines, and a zero-cell for vertices. If the coefficients are restricted to
{−,,}∈Z, the following classification for elements of a cell complex is obtained:
•
: if the cell is not in the complex
•
: if the unchanged cell is in the complex
•
−: if the orientation is changed
A structure-relating map between sets of chains Cp, called boundary operator, on a
cell complex K and τi
p ∈K, τi
p = {k, k,.., kp} a cell can be written by the boundary

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

A
a
b
c
C
d
e
B
C
D
E
F
D
c3
c4
c2
c1
n
s
Partial S
A
B
⊡Fig. -
Representation of a one-chain τi
with zero-chain boundary τ j
(left) and a two-chain τwith
one-chain boundary τk
(right)
homomorphism, which defines a (p −)-chain in terms of a p-chain, ∂p : Cp(K) →
Cp−(K):
∂pτi
p = ∑
i
(−)i[k, k,.., ˜ki,...kn]
(.)
where ˜ki indicates that ki is deleted from the sequence. This map is compatible with
the additive and the external multiplicative structure of chains and builds a linear
transformation:
Cp
∂p
&→Cp−
(.)
Therefore, the boundary operator is linear
∂(∑
i
wiτi
p) = ∑
i
wi (∂τi
p)
(.)
which means that the boundary operator can be applied separately to each cell of a chain.
Using the boundary operator on a sequence of chains of different dimensions results in a
chain complex C∗= {Cp, ∂p} such that the complex property
∂p−∂p = 
(.)
is given. Homological concepts are visible here for the first time, as homology examines the
connectivity between two immediately neighboring dimensions. > Figure -depicts two
examples of one-chains, two-chains, and an example of the boundary operator.
Applying the appropriate boundary operator to the two-chain example reads
∂τ= τ
+ τ
+ τ
+ τ

(.)
∂(τ
+ τ
+ τ
+ τ
) = τ
+ τ
−τ
+ τ
−τ
+ τ
−τ
−τ
= 
(.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
A different view on chain complices presents itself when the main focus is placed on the
cells within a chain. To cover even the most abstract cases, a cell is defined as a subset c ⊂X
of a Hausdorff space X if it is homeomorphic to the interior of the open n-dimensional
ball Dn = {x ∈Rn : ∣x∣< }. The number n is unique due to the invariance of domain
theorem [], and is called the dimension of c whereas homeomorphic means that two or
more spaces share the same topological characteristics. The following list assigns terms
corresponding to other areas of scientific computing:
•
-cell: point
•
-cell: edge
•
-cell: facet
•
n-cell: cell
A cell complex K (see also > Sect. ..) can be described by a set of cells that satisfy
the following properties:
•
The boundary of each p-cell τi
p is a finite union of (p −)-cells in K : ∂pτi
p = ⋃m τm
p−.
•
The intersection of any two cells τi
p,τ j
p in K is either empty, or is a unique cell in K.
The result of these operations are subspaces X(n) which are called the n-skeletons of the
cell complex. Incidence and adjacence relations are then available. Examples for incidence
can be given by vertex on edge relation, and for adjacency by vertex to vertex relations.
This cell complex with the underlying topological space guarantees that all interdimen-
sional objects are connected in an appropriate manner. Although there are various possible
attachments of cells, only one process results in a cell complex, see > Fig. -.
...
Cochains
In addition to chain and cell complices, scientific visualization requires the notation and
access mechanisms to global quantities related to macroscopic n-dimensional space-time
⊡Fig. -
Examples of violations of correct cell attachment. Left: missing zero-cell. Middle: cells do not
intersect at vertices. Right: intersection of cells

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

domains. The differential forms which are necessary concepts to handle physical prop-
erties can also be projected onto discrete counterparts, which are called cochains. This
collection of possible quantities, which can be measured, can then be called a section of
a fiberbundle, which permits the modeling of these measurements as a function that can
be integrated on arbitrary n-dimensional (sub)domains or multivectors. This function can
then be seen as the abstracted process of measurement of this quantity [, ]. The con-
cept of cochains allows the association of numbers not only to single cells, as chains do, but
also to assemblies of cells. Briefly, the necessary requirements are that this mapping is not
only orientation-dependent, but also linear with respect to the assembly of cells. A cochain
representation is now the global quantity association with subdomains of a cell complex,
which can be arbitrarily built to discretize a domain.
A linear transformation σ of the n-chains into the field R of real numbers forms a vector
space cn
σ&→R and is called a vector valued m-dimensional cochain or short m-cochain.
The co-boundary δ of a m-cochain is a (m + )-cochain defined as
δcm = ∑
i
viτi,
where
vi =
∑
b ∈faces(τi)
σ(b, τi)cm(b)
(.)
Thus, the coboundary operator assigns non-zero coefficients only to those (m + ) cells
that have cm as a face. As can be seen, δcm depends not only on cm but on how cm lies
in the complex K. This is a fundamental difference between the two operators ∂and δ.
An example is given in
> Fig. -where the coboundary operator is used on a one-
cell. The right part δ ○δK of
> Fig. -is also depicted for the continuous differential
forms in > Fig. -. The coboundary of a m-cochain is a m + cochain which assigns to
each (m + ) cell the sum of the values that the m-cochains assigns to the m-cells which
form the boundary of the (m + ) cell. Each quantity appears in the sum multiplied by
the corresponding incidence number. Cochain complices [, ] are similarly to chain
complices except that the arrows are reversed, so a cochain complex C∗= {Cm, δm} is a
sequence of modules Cm and homomorphisms:
δm : Cm →Cm+
(.)
such that
δm+δm = 
(.)
Then, the following sequence with δ ○δ = is generated:

δ&→Cδ&→Cδ&→C
δ&→C
δ&→
(.)
Cochains are the algebraic equivalent of alternating differential forms, while the
coboundary process is the algebraic equivalent of the external derivative and can therefore
be considered as the discrete counterpart of the differential operators:
•
grad(.)
•
curl(.)
•
div (.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
+ / −
+ / −
⊡Fig. -
Cochain complex with the corresponding coboundary operator: K
δ→δK
δ→δ ○δK= .
Proceeding from left to right, a one-cochain represented by a line segment, a two-cochain
generated by the product of two one-forms, and a three-cochain depicted by volume
objects are illustrated
It indeed satisfies the property δ ○δ ≡corresponding to
•
curl(grad (.)) ≡
•
div (curl(.)) ≡
...
Duality between Chains and Cochains
Furthermore, a definition of the adjoint nature of ∂, δ : C p →C p+can be given:
⟨c p, ∂cp+⟩= ⟨δc p, cp+⟩
(.)
The concepts of chains and cochains coincide on finite complices []. Geometrically,
however, Cp and C p are distinct [] despite an isomorphism. An element of Cp is a for-
mal sum of p-cells, where an element of C p is a linear function that maps elements of
Cp into a field. Chains are dimensionless multiplicities of aggregated cells, whereas those
associated with cochains may be interpreted as physical quantities []. The extension of
cochains from single cell weights to quantities associated with assemblies of cells is not
trivial and makes cochains very different from chains, even on finite cell complices. Nev-
ertheless, there is an important duality between p-chains and p-cochains. The first part of
the deRham (cohomology group) complex, depicted in
> Fig. -on the left, is the set
of closed one-forms modulo the set of exact one-forms denoted by
H= Z/B
(.)
This group is therefore trivial (only the zero element) if all closed one-forms are exact. If
the corresponding space is multiply connected, then there are closed one-chains that are
not themselves boundaries, and there are closed one-forms that are not themselves exact.
For a chain cp ∈Cp(K,R) and a cochain c p ∈C p(K,R), the integral of c p over cp is
denoted by ∫cp c p, and integration can be regarded as a mapping, where D represents the
corresponding dimension:
∫
: Cp(K) × C p(K) →R,
for ≤p ≤D
(.)

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

C1
0
Z1
C0 = Z0
B0
0
0
0
⊡Fig. -
A graphical representation of closed and exact forms. The forms Z, B, Z, and Bare closed
forms, while only the forms Band Bare exact forms. The nilpotency of the operation d
forces the exact forms to vanish
Integration in the context of cochains is a linear operation: given a, a∈R, c p,c p,∈
C p(K) and cp ∈Cp(K), reads
∫cp
ac p,+ ac p,= a∫cp
c p,+ a∫cp
c p,
(.)
Reversing the orientation of a chain means that integrals over that chain acquire the
opposite sign
∫−cp
c p = −∫cp
c p
(.)
using the set of p-chains with vector space properties Cp(K,R), e.g., linear combinations of
p-chains with coefficients in the field R. For coefficients in R, the operation of integration
can be regarded as a bilinear pairing between p-chains and p-cochains. Furthermore, for
reasonable p-chains and p-cochains, this bilinear pairing for integration is non-degenerate,
if
∫cp
c p = 
∀cp ∈Cp(K),
then c p = 
(.)
and
if
∫cp
c p = 
∀c p ∈C p(K),
then cp = 
(.)
The integration domain can be described by, using Geometric Algebra notation, the
exterior product applied to multivectors. An example is then given by the generalized
Stokes theorem:
∫cp
d f = ∫∂cp
f
(.)
or
< d f , cp >=< f , ∂cp >
(.)
The generalized stokes theorem combines two important concepts, the integration
domain and the form to be integrated.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
..
Homology and Cohomology
The concepts of chains can also be used to characterize properties of spaces, the homol-
ogy and cohomology, where it is only necessary to use Cp(K,Z). The algebraic structure of
chains is an important concept, e.g., to detect a p-dimensional hole that is not the bound-
ary of a p + -chain, which is called a p-cycle. For short, a cycle is a chain whose boundary
is ∂pcp = , a closed chain. The introduced boundary operator can also be related to homo-
logical terms. A boundary is a chain bp for which there is a chain cp such that ∂pcp = bp.
Since ∂○∂= , Bn ⊂Zn is obtained. The homology is then defined by Hn = Zn/Bn.
The homology of a space is a sequence of vector spaces. The topological classification of
homology is defined by
Bp = im ∂p+
and
Zp = ker ∂p
so that Bp ⊂Zp and
Hp = Zp/Bp
where βp = Rank Hp. Here im is the image and ker is the kernel of the mapping.
For cohomology
Bp = im d p+
and
Z p = ker d p
so that Bp ⊂Z p and
H p = Z p/Bp
where βp = Rank H p. An important property of these vector spaces is given by β, which
corresponds to the dimension of the vector spaces H and is called the Betti number [, ].
Betti numbers identify the number of non-homologous cycles which are not boundaries:
•
βcounts the number of connected components.
•
βcounts the number of tunnels (topological holes).
•
βcounts the number of enclosed cavities.
The number of connected components gives the number of distinct entities of a given
object, whereas tunnels describe the number of separated parts of space. In contrast to a
tunnel, the enclosed cavities are completely bounded by the object.
Examples for the Betti numbers of various geometrical objects are stated by:
•
Cylinder: β= , β= , βn = 
∀n ≥. The cylinder consists of one connected
component, which forms a single separation of space. Therefore no enclosed cavitiy is
present.
•
Sphere: β= , β= , β= , βn = 
∀n ≥. If βand βare switched, a sphere
is obtained by contracting the separation by generating an enclosed cavity from the
tunnel.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

•
Torus : β= , β= , β= , βn = 
∀n ≥. Closing a cylinder onto itself results
in a torus which not only generates an enclosed cavity, but also maintains the cylin-
der’s tunnel. An additional tunnel is introduced due to the closing procedure which is
depicted in > Fig. -as the central hole.
The Euler characteristics, which is an invariant, can be derived from the Betti numbers
by: ξ = β−β+ β.
> Figure -depicts the homology of a three-dimensional chain complex with the
respective images and kernels, where the chain complex of K is defined by im ∂p+⊆ker ∂p.
As can be seen, the boundary operator expression yields ∂p ○∂p+= . To give an example,
the first homology group is the set of closed one-chains (curves) modulo the closed one-
chains which are also boundaries. This group is denoted by H= Z/B, where Zare cycles
⊡Fig. -
Topologically a torus is the product of two circles. The partially shaded circle is span around
the fully drawn circle which can be interpreted as the closure of a cylinder onto itself
C3
C2
C1
C0=Z0
Z3
Z2
Z1
B2
B2
B1
B0
0
0
0
0
0
0
⊡Fig. -
A graphical representation of (co)homology for a three-dimensional cell complex


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
A
C
B
⊡Fig. -
Illustration of cycles A, B, C and a boundary C. A, B are not boundaries
or closed one-chains and Bare one-boundaries. Another example is given in > Fig. -,
where A, B, C are cycles and a boundary C, but A, B are not boundaries.
..
Topology
Conceptual consistency in scientific visualization is provided by topology. Cell complices
convey topology in a computationally treatable manner and can therefore be introduced
by much simpler definitions. A topological space (X,T ) is the collection of sets T that
include:
•
The space itself X and the empty set /
•
The union of any of these sets
•
The finite intersection of any of the sets
The family T is called a topology on X, and the members of T are called open sets. As an
example a basic set X = {a, b, c} and a topology is given:
(X,T ) = {/,
{a},{b},{c},
{a, b},{a, c},{b, c},
{a, b, c}}
The general definition for a topological space is very abstract and allows several topological
spaces which are not useful in scientific visualization, e.g., a topological space (X,T ) with
a trivial topology T = {/, X}. So basic mechanisms of separation within a topological
space are required, e.g., the Hausdorff property. A topological space (X,T ) is said to be
Hausdorff if, given x, y ∈X with x ≠y, there exist open sets U,Usuch that x ∈U, y ∈U
and U⋂U= /. But the question remains, what topology actually is. A brief explanation
is given by the study of properties of an object that do not change under deformation. To
describe this deformation process, abstract rules can be stated and if they are true, then an

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

⊡Fig. -
Topologically a torus and a coﬀee mug are equivalent and so have the same Betti numbers
object A can be transformed into an object B without change. The two objects A, B are then
called homeomorphic:
•
All points of A ↔all points of B
•
−correspondence (no overlap)
•
Bicontinous (continuous both ways)
•
Cannot tear, join, poke/seal holes
The deformation is −if each point of A maps to a single point on B, and there is no
overlap. If this deformation is continuous, A cannot be teared, joined, disrupted, or sealed
up. If two objects are homeomorphic, then they are topologically equivalent. > Figure -
illustrates an example of a torus and coffee mug which are a prominent example for topo-
logically equivalence. The torus can be continuously deformed, without tearing, joining,
disrupting, or sealing up, into a cup. The hole in the torus becomes the handle of the cup.
But why should anybody in visualization be concerned about how objects can be
deformed? Topology is much more than the illustrated properties, it can be much better
described by the study of connectedness:
•
Understanding of space properties: how connectivity happens.
•
Analysis of space properties: how connectivity can be determined.
•
Articulation of space properties: how connectivity can be described.
•
Control about space properties: how connectivity can be enforced.
Topology studies properties of sets that do not change under well-behaved trans-
formations (homeomorphisms). These properties include completeness and compact-
ness. In visualization, one property is of significance: connectedness. Especially, how
many disjoint components can be distinguished and how many holes (or tunnels) are in
these components. Geometric configuration is another interesting aspect in visualization


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
because it is important to know which of these components have how many holes, and
where the holes are relative to each other. Several operations in scientific visualization can
be summarized:
•
Simplification: reduction of data complexity.
If objects are described with fewer properties, important properties such as compo-
nents or holes should be retained or removed, if these properties become insignificant,
unnecessary, or imperceptible.
•
Compression: reduction of data storage.
It is important that each operation does not alter important features (interaction of
geometrical and topological features).
•
Texturing: visualization context elements.
How can a texture kept consistent if an object, e.g., a torus, is transformed into another
object, e.g., a coffee cup.
•
Morphing: transforming one object into another.
If an object is morphed into another, topological features have to remain, e.g., the torus
hole has to become the coffee cup handle hole.
.
Geometric Algebra Computing
Geometric Algebra as a general mathematical system unites many mathematical concepts
such as vector algebra, quaternions, Plücker coordinates, and projective geometry, and it
easily deals with geometric objects, operations, and transformations. A lot of applications
in computer graphics, computer vision, and other engineering areas can benefit from these
properties. In a ray tracing application, for instance, the intersection of a ray and a bound-
ing sphere is needed. According to > Fig. -, this can be easily expressed with the help
of the outer product of these two geometric entities.
Geometric Algebra is based on the work of Hermann Grassmann (see the conference
[] celebrating his th birthday in ) and William Clifford [, ]. Pioneering work
ray R
bounding sphere S
Inter section = R Ù S
⊡Fig. -
Spheres and lines are basic entities of Geometric Algebra to compute with. Operations like
the intersection of them are easily expressed with the help of their outer product. The result
of the intersection of a ray and a (bounding) sphere is another geometric entity, the point
pair of the two points of the line intersecting the sphere. The sign of the square of the point
pair easily indicates whether there is a real intersection or not

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

has been done by David Hestenes, who first applied Geometric Algebra to problems in
mechanics and physics [, ].
The first time Geometric Algebra was introduced to a wider computer graphics audi-
ence was through a couple of courses at the SIGGRAPH conferences and (see
[]) and later at the Eurographics []. Researchers at the University of Cambridge, UK,
have applied Geometric Algebra to a number of graphics related projects. Geomerics []
is a start-up company in Cambridge specializing in simulation software for physics and
lighting, which presented its new technology allowing real-time radiosity in videogames
utilizing commodity graphics processing hardware. The technology is based on Geometric
Algebra wavelet technology. Researchers at the University of Amsterdam, the Nether-
lands, are applying their fundamental research on Geometric Algebra to D computer
vision, to ray tracing, and on the efficient software implementation of Geometric Alge-
bra. Researchers from Guadalajara, Mexico, are primarily dealing with the application of
Geometric Algebra in the field of computer vision, robot vision, and kinematics. They are
using Geometric Algebra for instance for tasks like visual guided grasping, camera self-
localization and reconstruction of shape and motion. Their methods for geometric neural
computing are used for tasks like pattern recognition []. Registration, the task of finding
correspondences between two point sets, is solved based on Geometric Algebra methods in
[]. Some of their kinematics algorithms are dealing with inverse kinematics, fixation, and
grasping as well as with kinematics and differential kinematics of binocular robot heads.
At the University of Kiel, Germany, researchers are applying Geometric Algebra to robot
vision and pose estimation []. They also do some interesting research dealing for instance
with neural networks based on Geometric Algebra []. In addition to these examples there
are many other applications like Geometric Algebra Fourier transforms for the visualiza-
tion and analysis of vector fields [] or classification and clustering of spatial patterns with
Geometric Algebra [] showing the wide area of possibilities of advantageously using this
mathematical system in engineering applications.
..
Beneﬁts of Geometric Algebra
As follows, we highlight some of the properties of Geometric Algebra that make it
advantageous for a lot of engineering applications.
...
Uniﬁcation of Mathematical Systems
In the wide range of engineering applications, many different mathematical systems are
currently used. One notable advantage of Geometric Algebra is that it subsumes mathe-
matical systems like vector algebra, complex analysis, quaternions, or Plücker coordinates.
> Table -, for instance, describes how complex numbers can be identified within the
D Geometric Algebra. This algebra does not only contain the two basis vectors eand e,


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
but also basis elements of grade (dimension) and representing the scalar and imaginary
part of complex numbers.
Other examples are Plücker coordinates based on the description of lines in conformal
Geometric Algebra (see > Sect. ..) or quaternions as to be identified in
> Fig. -
with their imaginary units.
...
Uniform Handling of Diﬀerent Geometric Primitives
Conformal Geometric Algebra, the Geometric Algebra of conformal space we focus on,
is able to easily treat different geometric objects. > Table -presents the representation
of points, lines, circles, spheres, and planes as the same entities algebraically. Consider the
spheres of > Fig. -, for instance. These spheres are simply represented by
S = P −
re∞
(.)
based on their center point P, their radius r and the basis vector e∞which represents the
point at infinity. The circle of intersection of the spheres is then easily computed using the
outer product to operate on the spheres as simply as if they were vectors:
Z = S∧S
(.)
This way of computing with Geometric Algebra clearly benefits computer graphics
applications.
⊡Table -
Multiplication table of the D Geometric Algebra. This algebra consists of basic algebraic
objects of grade (dimension) , the scalar, of grade , the two basis vectors eand eand of
grade , the bi-vector e∧e, which can be identiﬁed with the imaginary number i squaring
to −

e
e
e∧e


e
e
e∧e
e
e

e∧e
e
e
e
−e∧e

−e
e∧e
e∧e
−e
e
−
Sphere S1
Sphere S2
Circle = S1 Ù S2
⊡Fig. -
Spheres and circles are basic entities of Geometric Algebra. Operations like the intersection
of two spheres are easily expressed

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

R
p
p
p R
—
⊡Fig. -
The ray R is reﬂected from the plane π computing −πR
π
⊡Table -
List of the basic geometric primitives provided by the D conformal Geometric Algebra. The
bold characters represent D entities (x is a D point, n is a D normal vector and xis the
scalar product of the D vector x). The two additional basis vectors eand e∞represent the
origin and inﬁnity. Based on the outer product, circles and lines can be described as inter-
sections of two spheres, respectively two planes. The parameter r represents the radius of
the sphere and the parameter d the distance of the plane to the origin
Entity
Representation
Point
P = x + 
xe∞+ e
Sphere
S = P −
r e∞
Plane
π = n + de∞
Circle
Z = S∧S
Line
L = π∧π
...
Simpliﬁed Geometric Operations
Geometric operations like rotations, translations (see []) and reflections can be easily
treated within the algebra. There is no need to change the way of describing them with other
approaches (vector algebra, for instance, additionally needs matrices in order to describe
transformations).
> Figure -visualizes the reflection of the ray R from one plane
π = n + de∞
(.)
(see > Table -). The reflected line, drawn in magenta,
Rreflected = −πR
π
(.)
is computed with the help of the reflection operation including the reflection object as well
as the object to be reflected.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
...
More Eﬃcient Implementations
Geometric Algebra as a mathematical language suggests a clearer structure and greater ele-
gance in understanding methods and formulae. But, what about the runtime performance
for derived algorithms? Geometric Algebra inherently has a large potential for creating
optimizations leading to more highly efficient implementations especially for parallel plat-
forms. Gaalop [], as presented in > Sect. .., is an approach offering dramatically
improved optimizations.
..
Conformal Geometric Algebra
Conformal Geometric Algebra is a D Geometric Algebra based on the D basis vectors
e, e, and eas well as on the two additional base vectors erepresenting the origin and
e∞representing infinity.
Blades are the basic computational elements and the basic geometric entities of Geo-
metric Algebras. The D conformal Geometric Algebra consists of blades with grades
(dimension) , , , , , and , whereby a scalar is a -blade (blade of grade ). The ele-
ment of grade five is called the pseudoscalar. A linear combination of blades is called a
k-vector. So a bi-vector is a linear combination of blades with grade . Other k-vectors
are vectors (grade ), tri-vectors (grade ), and quadvectors (grade ). Furthermore, a lin-
ear combination of blades of different grades is called a multivector. Multivectors are the
general elements of a Geometric Algebra. > Table -lists all the blades of conformal
Geometric Algebra. The indices indicate : scalar, . . .: vector, . . .: bi-vector, . . .:
tri-vector, . . . : quadvector, : pseudoscalar.
A point P = xe+ xe+ xe+ 
xe∞+ e(see > Table -), for instance, can be
written in terms of a multivector as the following linear combination of blades b[i]:
P = x∗b[] + x∗b[] + x∗b[] + 
x∗b[] + b[]
(.)
with multivector indices according to > Table -.
> Figure -describes some interpretations of the basis blades of conformal Geo-
metric Algebra. Scalars like the number π are grade entities. They can be combined with
the blade representing the imaginary unit i to complex numbers or with the blades repre-
senting the imaginary units i, j, k to quaternions. Since quaternions describe rotations, this
kind of transformation can be handled within the algebra. Geometric objects like spheres,
planes, circles, and lines can be represented as vectors and bi-vectors.
> Table -lists the two representations of the conformal geometric entities. The inner
product null space (IPNS) and the outer product null space (OPNS) [] are dual to each
other. While > Table -already presented the IPNS representation of spheres and planes,
they can be described also with the outer product of four points being part of them. In the

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Grade Term
Blades
3.1416
i, j, k
cos a
cos a
–sin a 0
0
0
0
1
sin a
0
1
b
a
a
a
h
l
b
c
1
1
nr.
5
10
10
5
1
2
3
4
5
Scalar
e1, e2, e3, e0, e¥
e1 Ù e2,  e1 Ù e3,
Vector
Bivector
Trivector
Quadvector
Pseudoscalar
e2 Ù e3,
e1 Ù e¥,
e2 Ù e¥, e3 Ù e¥,
e1 Ù e0,
e1 Ù e2 Ù e3 Ù e¥,
e1 Ù e2 Ù e3 Ù e0,
e1 Ù e2 Ù e0 Ù e¥,
e1 Ù e3 Ù e0 Ù e¥,
e2 Ù e3 Ù e0 Ù e¥,
e1 Ù e2 Ù e3 Ù e0 Ù e¥
e2 Ù e0, e3 Ù e0,
...
e0 Ù e¥,
⊡Fig. -
The blades of conformal Geometric Algebra. Spheres and planes, for instance, are vectors.
Lines and circles can be represented as bi-vectors. Other mathematical systems like complex
numbers or quaternions can be identiﬁed based on their imaginary units i, j, k. This is why
also transformations like rotations can be handled within the algebra
⊡Table -
The extended list of the two representations of the conformal geometric entities. The IPNS
representations as described in > Table -have also an OPNS representation, which are
dual to each other (indicated by the star symbol). In the OPNS representation the geometric
objects are described with the help of the outer product of conformal points that are part of
the objects, for instance lines as the outer product of two points and the point at inﬁnity
Entity
IPNS representation
OPNS representation
Point
P = x + 
xe∞+ e
Sphere
S = P −
re∞
S∗= P∧P∧P∧P
Plane
π = n + de∞
π∗= P∧P∧P∧e∞
Circle
Z = S∧S
Z∗= P∧P∧P
Line
L = π∧π
L∗= P∧P∧e∞
Point pair
Pp = S∧S∧S
Pp∗= P∧P
case of a plane one of these four points is the point at infinity e∞. Circles can be described
with the help of the outer product of three conformal points lying on the circle or as the
intersection of two spheres.
Lines can be described with the help of the outer product of two points and the point
at infinity e∞or with the help of the outer product of two planes (i.e., intersection in IPNS
representation). An alternative expression is
L = ue+ m ∧e∞
(.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
Ly
Lx
Lz
m
b u
a
⊡Fig. -
The line L through the D points a, b and the visualization of its D Plücker parameters
based on the two D vectors u and m of > Eq. (.)
with the D pseudoscalar e= e∧e∧e∞, the two D points a, b on the line, u = b −a
as D direction vector, and m = a × b as the D moment vector (relative to origin). The
corresponding six Plücker coordinates (components of u and m) are (see > Fig. -)
(u : m) = (u: u: u: m: m: m)
(.)
..
Computational Eﬃciency of Geometric Algebra
Using Gaalop
Because of its generality, Geometric Algebra needs some optimizations for efficient
implementations.
Gaigen [] is a Geometric Algebra code generator developed at the university of
Amsterdam (see [, ]). The philosophy behind Gaigen is based on two ideas: generative
programming and specializing for the structure of Geometric Algebra. Please find some
benchmarks comparing Gaigen with other pure software solutions as well as comparing
five models of D Euclidean geometry for a ray tracing application in [, ].
Gaalop [] combines the advantages of software optimizations and the adaptability on
different parallel platforms. As an example, an inverse kinematics algorithm of a computer
animation application was investigated []. With the optimization approach of Gaalop,
the software implementation became three times faster and with a hardware implemen-
tation about times faster [] (three times by software optimization and times by
additional hardware optimization) than the conventional software implementation.
> Figure -shows an overview over the architecture of Gaalop. Its input is a Geo-
metric Algebra algorithm written in CLUCalc [], a system for the visual development of
Geometric Algebra algorithms. Via symbolic simplification it is transformed into an inter-
mediate representation (IR) that can be used for the generation of different output formats.
Gaalop supports sequential platforms with the automatic generation of C and JAVA code

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Geometric algebra algorithm
Symbolic simplification
IR (intermediate representation)
Sequential platforms
Parallel platforms
C
Java
CUDA
FPGA
...
...
⊡Fig. -
Architecture of Gaalop
while its main focus is on supporting parallel platforms like reconfigurable hardware as
well as modern accelerating GPUs.
Gaalop uses the symbolic computation functionality of Maple (using the Open Maple
interface and a library for Geometric Algebras []) in order to optimize a Geometric
Algebra algorithm. It computes the coefficients of the desired multivector symbolically,
returning an efficient implementation depending just on the input variables.
As an example, the following CLUCalc code computes the intersection circle C of two
spheres Sand Saccording to > Fig. -:
P1 = x1*e1 +x2*e2 +x3*e3
+1/2*(x1*x1+x2*x2+x3*x3)*einf +e0;
P2 = y1*e1 +y2*e2 +y3*e3
+1/2*(y1*y1+y2*y2+y3*y3)*einf +e0;
S1 = P1 - 1/2 * r1*r1 * einf;
S2 = P2 - 1/2 * r2*r2 * einf;
?C = S1 ˆ S2;
See > Table -for the computation of the conformal points Pand P, the spheres
Sand S, as well as the resulting circle based on the outer product of the two spheres.
The resulting C code generated by Gaalop for the intersection circle C is as follows and
depends only on the variables x, x, x, y, y, y, rand rfor the D center points and
radii:
float C [32] = {0.0};
C[7] = x1*y2-x2*y1; C[8] = x1*y3-x3*y1;
C[9] = -0.5*y1*x1*x1-0.5*y1*x2*x2
-0.5*y1*x3*x3+0.5*y1*r1*r1
+0.5*x1*y1*y1+0.5*x1*y2*y2
+0.5*x1*y3*y3-0.5*x1*r2*r2;


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
C[10] = -y1+x1;
C[11] = -x3*y2+x2*y3;
C[12] = -0.5*y2*x1*x1-0.5*y2*x2*x2
-0.5*y2*x3*x3+0.5*y2*r1*r1
+0.5*x2*y1*y1+0.5*x2*y2*y2
+0.5*x2*y3*y3-0.5*x2*r2*r2;
C[13] = -y2+x2;
C[14] = -0.5*y3*x1*x1-0.5*y3*x2*x2
-0.5*y3*x3*x3+0.5*y3*r1*r1
+0.5*x3*y1*y1+0.5*x3*y2*y2
+0.5*x3*y3*y3-0.5*x3*r2*r2;
C[15] = -y3+x3;
C[16] = -0.5*y3*y3+0.5*x3*x3
+0.5*x2*x2+0.5*r2*r2
-0.5*y1*y1-0.5*y2*y2
+0.5*x1*x1-0.5*r1*r1;
In a nutshell, Gaalop always computes optimized -dimensional multivectors. Since a cir-
cle is described with the help of a bi-vector, only the blades to (see > Table -) are
used. As you can see, all the corresponding coefficients of this multivector are very simple
expressions with basic arithmetic operations.
.
Feature-based Vector Field Visualization
We will identify derived quantities that describe flow features such as vortices
(> Sect. ..) and we discuss the topology of vector fields (> Sect. ..). However,
not all feature-based visualization approaches can be covered here. The reader is referred
to [] for further information on this topic. We start with a description of integral curves
in vector fields, which are the basis for most feature-based visualization approaches.
..
Characteristic Curves of Vector Fields
A curve q : R →M (see > Sect. ...) is called a tangent curve of a vector field v(x), if
for all points x ∈q the tangent vector ˙q of q coincides with v(x). Tangent curves are the

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

⊡Table -
The blades of the D conformal Geometric Algebra
Index
Blade
Grade




e


e


e


e∞


e


e∧e


e∧e


e∧e∞


e∧e


e∧e


e∧e∞


e∧e


e∧e∞


e∧e


e∞∧e


e∧e∧e


e∧e∧e∞


e∧e∧e


e∧e∧e∞


e∧e∧e


e∧e∞∧e


e∧e∧e∞


e∧e∧e


e∧e∞∧e


e∧e∞∧e


e∧e∧e∧e∞


e∧e∧e∧e


e∧e∧e∞∧e


e∧e∧e∞∧e


e∧e∧e∞∧e


e∧e∧e∧e∞∧e

solutions of the autonomous ODE system
d
dτ x(τ) = v(x(τ))
with x() = x
(.)
For all points x ∈M with v(x) ≠, there is one and only one tangent curve through it.
Tangent curves do not intersect or join each other. Hence, tangent curves uniquely describe
the directional information and are therefore an important tool for visualizing vector fields.
The tangent curves of a parameter-independent vector field v(x) are called stream lines.
A stream line describes the path of a massless particle in v.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
In a one-parameter-dependent vector field v(x, t), there are four types of characteristic
curves: stream lines, path lines, streak lines, and time lines. To ease the explanation, we
consider v(x, t) as a time-dependent vector field in the following: In a space-time point
(x, t) we can start a stream line (staying in time slice t = t) by integrating
d
dτ x(τ) = v(x(τ), t)
with x() = x
(.)
or a path line by integrating
d
dt x(t) = v(x(t), t)
with x(t) = x
(.)
Path lines describe the trajectories of massless particles in time-dependent vector fields.
The ODE system (> .) can be rewritten as an autonomous system at the expense of an
increase in dimension by one, if time is included as an explicit state variable:
d
dt (x
t) = (v(x(t), t)

)
with (x
t)() = (x
t
)
(.)
In this formulation space and time are dealt with on equal footing – facilitating the analysis
of spatio-temporal features. Path lines of the original vector field v in ordinary space now
appear as tangent curves of the vector field
p(x, t) = (v(x, t)

)
(.)
in space-time. To treat stream lines of v, one may simply use
s(x, t) = (v(x, t)

)
(.)
> Figure -illustrates s and p for a simple example vector field v. It is obtained by a
linear interpolation over time of two bilinear vector fields.
A streak line is the connection of all particles set out at different times but the same
point location. In an experiment, one can observe these structures by constantly releasing
dye into the flow from a fixed position. The resulting streak line consists of all particles
which have been at this fixed position sometime in the past. Considering the vector field p
introduced above, streak lines can be obtained in the following way: apply a stream surface
integration in p where the seeding curve is a straight line segment parallel to the t-axis, a
streak line is the intersection of this stream surface with a hyperplane perpendicular to the
t-axis ( > Fig. -c).
A time line is the connection of all particles set out at the same time but different loca-
tions, i.e., a line which gets advected by the flow. An analogon in the real world is a yarn
or wire thrown into a river, which gets transported and deformed by the flow. However, in
contrast to the yarn, a time line can get shorter and longer. It can be obtained by applying
a stream surface integration in p starting at a line with t = const., and intersecting it with
a hyperplane perpendicular to the t-axis ( > Fig. -d).

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

v (x, y, t) = (1 − t)
+ t
(a) Tangent curves of s
correspond to the stream
lines in v. See (.)
(b) Tangent curves of p
correspond to the path
lines in v. See (.)
(c) Streak lines of v as
intersections with a
stream surface
(d) Streak lines of v as
intersections with a
stream surface
⊡Fig. -
Characteristic curves of a simple D time-dependent vector ﬁeld. Stream and path lines are
shown as illuminated ﬁeld lines. Streak and time lines are shown as thick cylindrical lines,
while their seeding curves and resulting stream surfaces are colored red. The red/green
coordinate axes denote the (x, y)-domain, the blue axis shows time
Streak lines and time lines cannot be described as tangent curves in the spatio-temporal
domain. Both types of lines fail to have a property of stream and path lines: they are not
locally unique, i.e., for a particular location and time there is more than one streak and time
line passing through. However, stream, path, and streak lines coincide for steady vector
fields v(x, t) = v(x, t) and are described by (> .) in this setting. Time lines do not fit
into this.
..
Derived Measures of Vector Fields
A number of measures can be derived from a vector field v and its derivatives. These mea-
sures indicate certain properties or features and can be helpful when visualizing flows.
The following text assumes the underlying manifold M where the vector field is given to
be Euclidean space, i.e., the manifold is three-dimensional and Cartesian coordinates are
used where the metric (see > Sect. ...) is representable as the unit matrix.
The magnitude of v is then given as
∣v∣=
√
u+ v+ w
(.)
The divergence of a flow field is given as
div(v) = ∇⋅v = trace(J) = ux + vy + wz
(.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
and denotes the gain or loss of mass density at a certain point of the vector field: given a
volume element in a flow, a certain amount of mass is entering and exiting it. Divergence is
the net flux of this at the limit of a point. A flow field with div(v) = is called divergence-
free, which is a common case in fluid dynamics since a number of fluids are incompressible.
The vorticity or curl of a flow field is given as
ω =
⎛
⎜
⎝
ω
ω
ω
⎞
⎟
⎠
= ∇× v =
⎛
⎜
⎝
wy −vz
uz −wx
vx −uy
⎞
⎟
⎠
(.)
This vector is the axis of locally strongest rotation, i.e., it is perpendicular to the plane in
which the locally highest amount of circulation takes place. The vorticity magnitude ∣ω∣
gives the strength of rotation and is often used to identify regions of high vortical activity.
A vector field with ω = is called irrotational or curl-free, with the important subclass of
conservative vector fields, i.e., vector fields which are the gradient of a scalar field. Note that
Geometric Algebra, see > Sect. ...and > ., treats > Eqs. (.) and (> .) as
an entity, called the geometric derivative.
The identification of vortices is a major subject in fluid dynamics. The most widely
used quantities for detecting vortices are based on a decomposition of the Jacobian matrix
J = S + Ω into its symmetric part, the strain tensor
S = 
(J + JT)
(.)
and its antisymmetric part, the vorticity tensor
Ω = 
(J −JT) =
⎛
⎜
⎝

−ω
ω
ω

−ω
−ω
ω

⎞
⎟
⎠
(.)
with ωi being the components of vorticity (> .). While Ω assesses vortical activity,
the strain tensor S measures the amount of stretching and folding which drives mixing to
occur.
Inherent to the decomposition of the flow field gradient J into S and Ω is the follow-
ing duality: vortical activity is high in regions where Ω dominates S, whereas strain is
characterized by S dominating Ω.
In order to identify vortical activity, Jeong et al. used this decomposition in [] to
derive the vortex region quantity λas the second largest eigenvalue of the symmetric
tensor S+ Ω. Vortex regions are identified by λ< , whereas λ> lacks physical
interpretation. λdoes not capture stretching and folding of fluid particles and hence does
not capture the vorticity–strain duality.
The Q-criterion of Hunt [], also known as the Okubo-Weiss criterion, is defined by
Q = 
(∥Ω∥−∥S∥) = ∥ω∥−
∥S∥
(.)

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

where Q is positive, the vorticity magnitude dominates the rate of strain. Hence it is natural
to define vortex regions as regions where Q > . Unlike λ, Q has a physical meaning also
where Q < . Here the rate of strain dominates the vorticity magnitude.
..
Topology of Vector Fields
In this section we collect the first order topological properties of steady D and D vec-
tor fields. The extraction of these topological structures has become a standard tool in
visualization for the feature-based analysis of vector fields.
...
Critical Points
Considering a steady vector field v(x), an isolated critical point xis given by
v(x) = 
with
v(x±єєє) ≠
(.)
This means that v is zero at the critical point, but non-zero in a certain neighborhood.
Every critical point can be assigned anindex. For a D vector field it denotes the number
of counterclockwise revolutions of the vectors of v while traveling counterclockwise on a
closed curve around the critical point (For D vector fields, it is therefore often called the
winding number.). Similarly, the index of a D critical point measures the number of times
the vectors of v cover the area of an enclosing sphere. The index is always an integer and
it may be positive or negative. For a curve/sphere enclosing an arbitrary part of a vector
field, the index of the enclosed area/volume is the sum of the indices of the enclosed critical
points. Mann et al. show in [] how to compute the index of a region using Geometric
Algebra. A detailed discussion of index theory can be found in [, , ].
Critical points are characterized and classified by the behavior of the tangent curves
around it. Here we concentrate on first order critical points, i.e., critical points with
det(J(x)) ≠. As shown in [, ], a first order Taylor expansion of the flow around
xsuffices to completely classify it. This is done by an eigenvalue/eigenvector analysis
of J(x). Let λi be the eigenvalues of J(x) ordered according to their real parts, i.e.,
Re(λi−) ≤Re(λi). Furthermore, let ei be the corresponding eigenvectors, and let fi be
the corresponding eigenvectors of the transposed Jacobian (J(x))T (Note that J and JT
have the same eigenvalues but not necessarily the same eigenvectors.). The sign of the real
part of an eigenvalue λi denotes – together with the corresponding eigenvector ei – the
flow direction: positive values represent an outflow and negative values an inflow behav-
ior. Based on this we give the classification of D and D first-order critical points in the
following.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
D Vector Fields
Based on the flow direction, first order critical points in D vector fields are classified into:
Sources:

<Re(λ)≤Re(λ)
Saddles:
Re(λ)<

<Re(λ)
Sinks:
Re(λ)≤Re(λ)<

Thus, sources and sinks consist of complete outflow/inflow, while saddles have a mixture
of both.
Sources and sinks can be further divided into two stable subclasses by deciding whether
or not imaginary parts are present, i.e., whether or not λ, λis a pair of conjugate complex
eigenvalues:
Foci:
Im(λ) = −Im(λ) ≠
Nodes:
Im(λ) = Im(λ) = 
There is another important class of critical points in D: a center. Here, we have a pair
of conjugate complex eigenvalues with Re(λ) = Re(λ) = . This type is common in
incompressible (divergence-free) flows, but unstable in general vector fields since a small
perturbation of v changes the center to either a sink or a source. > Figure -shows the
phase portraits of the different types of first order critical points following [].
The index of a saddle point is −, while the index of a source, sink, or center is +. It
turns out that this coincides with the sign of det(J(x)): a negative determinant denotes a
saddle, a positive determinant a source, sink, or center. This already shows that the index
of a critical point cannot be used to distinguish or classify them completely, since different
types like sources and sinks have assigned the same index.
An iconic representation is an appropriate visualization for critical points, since vector
fields usually contain a finite number of them. We will display them as spheres colored
according to their classification: sources will be colored in red, sinks in blue, saddles in
yellow, and centers in green.
Saddle point
R1 <0, R2 > 0
I1 = I2 = 0
Repelling node
R1 , R2 > 0
I1 = I2 = 0
Repelling focus
R1 = R2 > 0
I1 = –I2 ≠ 0
Center
R1 = R2 = 0
I1 = –I2 ≠ 0
Attracting focus
R1 = R2 < 0
I1 = –I2 ≠ 0
Attracting node
R1, R2 < 0
I1 = I2  = 0
⊡Fig. -
Classiﬁcation of ﬁrst order critical points. R, Rdenote the real parts of the eigenvalues of
the Jacobian matrix while I, Idenote their imaginary parts (From [])

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

D Vector Fields
Depending on the sign of Re(λi) we get the following classification of first-order critical
points in D vector fields:
Sources:

<Re(λ)≤Re(λ)≤Re(λ)
Repelling saddles:
Re(λ)<

<Re(λ)≤Re(λ)
Attracting saddles:
Re(λ)≤Re(λ)<

<Re(λ)
Sinks:
Re(λ)≤Re(λ)≤Re(λ)<

Again, sources and sinks consist of complete outflow/inflow, while saddles have a mixture
of both. A repelling saddle has one direction of inflow behavior (called inflow direction)
and a plane in which a D outflow behavior occurs (called outflow plane). Similar to this,
an attracting saddle consists of an outflow direction and an inflow plane.
Each of the four classes above can be further divided into two stable subclasses by decid-
ing whether or not imaginary parts in two of the eigenvalues are present (λ, λ, λare not
ordered):
Foci:
Im(λ) = 
and
Im(λ) = −Im(λ) ≠
Nodes:
Im(λ) = Im(λ) = Im(λ) = 
As argued in [], the index of a first order critical point is given as the sign of the
product of the eigenvalues of J(x). This yields an index of +for sources and attracting
saddles, and an index of −for sinks and repelling saddles.
In order to depict D critical points, several icons have been proposed in the literature,
see [, , , ]. Basically, we follow the design approach of [, ] and color the icons
depending on the flow behavior: Attracting parts (inflow) are colored blue, while repelling
parts (outflow) are colored red ( > Fig. -).
e1
f1
f3
e3
e1
e2
e1
e3
e2
e1
f1
e1
f1
e1
e2
e3
e1
e2
e3
a
b
c
d
a
b
c
e
f
g
h
e3
d
e
f
g
h
Sources and sinks; (a) repelling node and
(b) its icon; (c) repelling focus and (d) its icon;
(e) attracting node and (f) its icon; (g)
attracting focus and (h) its icon
Repelling and attracting saddles; (a) repelling
node saddle and (b) its icon; (c) repelling focus
saddle and (d) its icon; (e) attracting node
saddle and (f) its icon; (g) attracting focus saddle
and (h) its icon
⊡Fig. -
Flow behavior around critical points of D vector ﬁelds and corresponding iconic
representation


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
...
Separatrices
Separatrices are stream lines or stream surfaces which separate regions of different flow
behavior. Here we concentrate on separatrices that emanate from critical points. Due to
the homogeneous flow behavior around sources and sinks (either a complete outflow or
inflow), they do not contribute to separatrices. Each saddle point creates two separatrices:
one in forward and one in backward integration into the directions of the eigenvectors. For
a D saddle point this gives two separation lines ( > Fig. -a). Considering a repelling
saddle xR of a D vector field, it creates one separation curve (which is a stream line starting
in xR in the inflow direction by backward integration) and a separation surface (which is a
stream surface starting in the outflow plane by forward integration). > Figure -b gives
an illustration. A similar statement holds for attracting saddles.
Other kinds of separatrices are possible as well: They can emanate from boundary
switch curves [], attachment and detachment lines [], or they are closed separatrices
without a specific emanating structure [].
...
Application
In the following, we exemplify the topological concepts described above by applying them
to a D vector field. First, we extract the critical points by searching for zeros in the vector
field. Based on an eigenvalue/eigenvector analysis we identify the different types of the
critical points. Starting from the saddles, we integrate the separatrices into the directions
of the eigenvectors.
(a) Separatrices from D saddle points
(yellow points) are stream lines ending in
sources/sinks or leaving the domain
(b) The separatrices of a D repelling node
saddle are D and D manifolds obtained by
integration
⊡Fig. -
Separatrices are stream lines or surfaces starting from saddle points into the direction of the
eigenvectors

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

a
b
c
⊡Fig. -
Topological representations of the benzene data set with critical points. (a) Iconic
representation. (b) Due to the shown separation surfaces, the topological skeleton of the
vector ﬁeld looks visually cluttered. (c) Visualization of the topological skeleton using saddle
connectors
> Figure -visualizes the electrostatic field around a benzene molecule. This data
set was calculated on a regular grid using the fractional charges method described
in []. It consists of first order critical points depicted in
> Fig. -a. The sep-
aration surfaces shown in
> Fig. -b emanate from attracting and repelling
saddles. Note how they hide each other as well as the critical points. Even rendering the
surfaces in a semi-transparent style does not reduce the visual clutter to an acceptable
degree. This is one of the major challenges for the topological visualization of D vector
fields.
> Figure -c shows a possible solution to this problem by showing the saddle
connectors that we found in this data set. Saddle connectors are the intersection curves of
repelling and attracting separation surfaces and have been introduced to the visualization
community in []. Despite the fact that saddle connectors can only indicate the approxi-
mate run of the separation surfaces, the resulting visualization gives more insight into the
symmetry and three-dimensionality of the data set. Saddle connectors are a useful compro-
mise between the amount of coded information and the expressiveness of the visualization
for complex topological skeletons.
.
Anisotropic Diﬀusion PDE’s for Image Regularization
and Visualization
..
Regularization PDE’s : A review
We consider a D multi-valued image I : Ω →Rn (n = for color images) defined on a
domain Ω ⊂R, and denote by Ii : Ω →R, the scalar channel i of I : ∀X = (x, y)∈Ω,
I(X) = (I(X) I(X) ... In(X))
T.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
...
Local Multi-valued Geometry and Diﬀusion Tensors
PDE-based regularization can be often seen as the local smoothing of an image I along
defined directions depending themselves on the local configuration of the pixel intensities,
i.e., one wants basically to smooth I in parallel to the image discontinuities. Naturally, this
means that one has first to retrieve the local geometry of the image I. It consists in the
definition of these important features at each image point X = (x, y) ∈Ω :
•
Two orthogonal directions θ+
(X) , θ−
(X) ∈Salong the local maximum and minimum
variations of image intensities at X. θ−is then considered to be parallel to the local
edge, when there is one.
•
Two corresponding positive values λ+
(X) , λ−
(X) measuring the effective variations of
the image intensities along θ+
(X) and θ−
(X) respectively. λ−, λ+ are related to the local
contrast of an edge.
For scalar images I : Ω →R, this local geometry { λ+/−, θ+/−∣X ∈Ω} is usually retrieved
by the computation of the smoothed gradient field ∇Iσ = ∇I ∗Gσ where Gσ is a D Gaus-
sian kernel with standard deviation σ. Then, λ+ = ∥∇Iσ∥is a possible measure of the local
contrast of the contours, while θ−= ∇I
σ/∥∇Iσ∥gives the contours direction. Such a local
geometry { λ+/−, θ+/−∣X ∈Ω} can be represented in a more convenient form by a field
G : Ω →P() of second-order tensors (× symmetric and semi-positive matrices) :
∀X ∈Ω,
G(X) = λ−θ−θ−T + λ+ θ+θ+T.
Eigenvalues of G are indeed λ−and λ+ and corresponding eigenvectors are θ−and
θ+. The local geometry of scalar-valued images I can be then modeled by the tensor field
G(X) = ∇Iσ(X)∇IT
σ(X).
For multi-valued images I : Ω →Rn, the local geometry can be retrieved in a similar
way, by the computation of the field G of the smoothed structure tensors. As explained in
[, ], this is a nice extension of the gradient for multi-valued images :
∀X ∈Ω,
Gσ(X) = (
n
∑
i=
∇Iiα(X)∇IT
iα(X)) ∗Gσ
where
∇Iiα =
⎛
⎜⎜
⎝
∂Ii
∂x
∂Ii
∂y
⎞
⎟⎟
⎠
∗Gα (.)
Gσ(X) is a very good estimator of the local multi-valued geometry of I at X : its spectral
elements give at the same time the vector-valued variations (by the eigenvalues λ−, λ+ of
Gσ) and the orientations (edges) of the local image structures (by the eigenvectors θ−Dθ+
of Gσ), σ being proportional to the so-called noise scale.
Once the local geometry Gσ of I has been determined, the way the regularization pro-
cess is achieved is defined by another field T : Ω →P() of diffusion tensors, which specifies
the local smoothing geometry that should drive the PDE flow. Of course, T depends on
the targeted application, and most of the time it is constructed from the local geometry Gσ
of I. It is thus defined from the spectral elements λ−, λ+ and θ−, θ+ of Gσ. In [, ], the
following expression is proposed for image regularization :
∀X ∈Ω,
T(X) = f −
(λ+,λ−) θ−θ−+ f +
(λ+,λ−) θ+θ+T
(.)

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

where
f −
(λ+,λ−) =

(+ λ+ + λ−)p
and
f +
(λ+,λ−) =

(+ λ+ + λ−)p
with p< p
are the two functions which set the strengths of the desired smoothing along the respective
directions θ−,θ+. This latest choice basically says that if a pixel X is located on an image
contour (λ+
(X) is high), the smoothing on X would be performed mostly along the contour
direction θ−
(X) (since f +
(.,.) << f −
(.,.)). Conversely, if a pixel X is located on a homogeneous
region (λ+
(X) is low), the smoothing on X would be performed in all possible directions
(isotropic smoothing), since f +
(.,.) ≃f −
(.,.) (and then T ≃Id). Pre-defining the smoothing
geometry T of each applied PDE iteration is the first stage of most of the PDE-based regu-
larization algorithms. Most of the differences between existing regularization methods (as
in [, , , , , , –, –]) lie first on the definition of T, but also on the kind
of the diffusion PDE that will be used indeed to perform the desired smoothing.
...
Divergence-based PDE’s
One of the common choice to smooth a corrupted multi-valued image I : Ω →Rn
following a local smoothing geometry T : Ω →P() is to use the divergence PDE :
∀i = ,.., n,
∂Ii
∂t = div (T∇Ii)
(.)
The general form of this now classical PDE for image regularization has been introduced
by Weickert in [], and adapted for color/multivalued images in []. In this latter case,
the tensor field T is chosen the same for all image channels Ii, ensuring that channels
are smoothed with a coherent multi-valued geometry which takes the correlation between
channels into account (since T depends on G). > Equation (.) unifies a lot of exist-
ing scalar or multi-valued regularization approaches and proposes at the same time two
interpretation levels of the regularization process :
•
Local interpretation: > Equation (.) may be seen as the physical law describing
local diffusion processes of the pixels individually regarded as temperatures or chemical
concentrations in an anisotropic environment which is locally described by T.
•
Global interpretation: The problem of image regularization can be regarded as the mini-
mization of the energy functional E(I) by a gradient descent (i.e., a PDE), coming from
the Euler-Lagrange equations of E(I) [, , , , ]:
E(I) = ∫Ω ψ(λ+, λ−) dΩ
where ψ : R→R
(.)
It results in a particular case of the PDE (> .), with T = ∂Ψ
∂λ−θ−θ−T + ∂Ψ
∂λ+ θ+θ+T,
where λ+, λ−are the two positive eigenvalues of the non-smoothed structure tensor field
G = ∑i ∇Ii∇IT
i and θ+, θ−are the corresponding eigenvectors.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
Unfortunately, there are local configurations where the PDE (> .) does not fully
respect the geometry T and where the smoothing is performed in unexpected directions.
For instance, considering (> .) with tensor fields T(X) = ( ∇I
∥∇I∥) ( ∇I
∥∇I∥)
T
(purely
anisotropic), and T(X) = Id (purely isotropic) lead both to the heat equation ∂I
∂t = ΔI
which has obviously an isotropic smoothing behavior. Different tensors fields T with dif-
ferent shapes (isotropic or anisotropic) may define the same regularization behavior. This
is due to the fact that the divergence implicitly introduces a dependance on the spa-
tial variations of the tensor field T, so it hampers the design of a pointwise smoothing
behavior.
...
Trace-based PDE’s
Alternative PDE-based regularization approaches have been proposed in [, , , , ]
in order to smooth an image directed by a local smoothing geometry. They are inspirit very
similar to the divergence equation (> .), but based on a trace operator:
∀i = ,.., n,
∂Ii
∂t = trace (THi)
with Hi =
⎛
⎜⎜⎜
⎝
∂Ii
∂x
∂Ii
∂x∂y
∂Ii
∂x∂y
∂Ii
∂y
⎞
⎟⎟⎟
⎠
(.)
Hi stands for the Hessian of Ii. The > Eq. (.) is in fact nothing more than a tensor-
based expression of the PDE ∂I
∂t = f −
(λ−,λ+) Iθ−θ−+ f +
(λ−,λ+) Iθ+θ+ where Iθ−θ−=
∂I
∂θ−. This
PDE can be viewed as a simultaneous combination of two orthogonally-oriented and
weighted D Laplacians. In case of multi-valued images, each channel Ii of I is here also
coherently smoothed with the same tensor field T. As demonstrated in [], the evolution
of > Eq. (.) has a geometric meaning in terms of local linear filtering: It may be seen
locally as the application of very small convolutions around each point X with a Gaussian
mask GT
t oriented by the tensor T(X):
GT
t (X) =

πt exp (−XT T−X
t
)
This ensures that the smoothing performed by (> .) is indeed oriented along the
pre-defined smoothing geometry T. As the trace is not a differential operator, the spa-
tial variation of T does not trouble the diffusion directions here and two different tensor
fields will necessarily lead to different smoothing behaviors. Under certain conditions, the
divergence PDE (> .) may be also developed as a trace formulation (> .). But
in this case, the tensors inside the trace and the divergence are not the same []. Note
that trace-based > Eq. (.) are more hardly connected to functional minimizations,
especially when considering the multi-valued case. For scalar-valued images (n = ), some
correspondences are known anyway [, , ].

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

...
Curvature-Preserving PDE’s
Basically, the divergence and trace > Eqs. (.) and (> .) locally behave as oriented
Gaussian smoothing whose strengths and orientations are directly related to the tensors
T(X). But on curved structures (like corners), this behavior is not desirable : In case of high
variations of the edge orientation θ−, such a smoothing will tend to round corners, even
by conducting it only along θ−(an oriented Gaussian is not curved by itself). To avoid
this over-smoothing effect, regularization PDE’s may try to stop their action on corners
(by vanishing tensors T(X) there, i.e f −= f + = ), but this implies the detection of curved
structures on images that are themselves noisy or corrupted. This is generally a hard task.
To overcome this problem, curvature-preserving regularization PDE’s have been intro-
duced in []. We illustrate the general idea of these equations by considering the simplest
case of image smoothing along a single direction, i.e., a vector field w : Ω →Rinstead of a
tensor-valued one T. The two spatial components of w are denoted w(X) = (u(X) v(X))T.
The curvature-preserving regularization PDE that smoothes I along w is defined as
∀i = , . . . , n,
∂Ii
∂t = trace (wwT Hi) + ∇IT
i Jww
with Jw =
⎛
⎜⎜
⎝
∂u
∂x
∂u
∂y
∂v
∂x
∂v
∂y
⎞
⎟⎟
⎠
(.)
where Jw stands for the Jacobian of w. > Eq. (.) simply adds a term ∇IT
i Jww to the
corresponding trace-based PDE (> .) that would smooth I along w. This term naturally
depends on the variation of the vector field w. Actually, it has been demonstrated in []
that > Eq. (.) is equivalent to the application of this one-dimensional PDE flow:
∂Ii(C(a))
∂t
= ∂Ii(C(a))
∂a
with
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
CX
()
=
X
∂CX
(a)
∂a
=
w (CX
(a))
(.)
where CX
(a) is the streamline curve of w, starting from X and parameterized by a ∈R.
Thus, > Eq. (.) is nothing more than the one-dimensional heat flow constrained on the
streamline curve C. This is indeed very different from a heat-flow oriented by w, as in the for-
mulation ∂Ii
∂t = ∂Ii
∂wsince the curvatures of the streamline of w are now implicitly taken into
account. In particular, > Eq. (.) has the interesting property to vanish when the image
intensities are constant on the streamline CX, whatever the curvature of CX is. So, defining
a field w that is tangent everywhere to the image structures allows the preservation of these
structures during the regularization process, even if they are curved (such as corners).
Moreover, as > Eq. (.) is a D heat flow on a streamline CX, its solution at time
dt can be estimated by convolving the image signal lying on the streamline CX by a D
Gaussian kernel []:
∀X ∈Ω,
I[dt]
(X) = ∫
+∞
−∞I[t=] (CX
(p)) Gdt(p) dp
(.)


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
a>0
a=0
a
b
a<0
x
⊡Fig. -
Streamline CX of various vector ﬁelds w : Ω →R. (a) Streamline of a general ﬁeld w (b)
Example of streamlines when w is the lowest eigenvector of the smoothed structure tensor
Gσ (one block is one color pixel)
This formulation is very close to the Line Integral Convolutions (LIC) framework [],
which has been introduced as a visualization technique to render a textured image repre-
senting a D vector field w. As we are considering diffusion equations here, the weighting
function in > Eq. (.) is naturally Gaussian. This geometric interpretation particularly
allows to implement curvature-preserving PDE’s (> .) using Runge–Kutta estimations
of the streamline geometries, leading to sub-pixel precision of the smoothing process.
This single-direction smoothing PDE (> .) can be easily extended to deal with a
tensor-valued geometry T : Ω →P(), in order to be able to represent both anisotropic or
isotropic regularization behaviors. This is done by decomposing the tensor field T as the
sum of several single-directional tensors, i.e., T = 
π ∫
π
α=(
√
Taα)(
√
Taα)
T dα, where
aα = (cos α
sin α)T. This naturally suggests to decompose a tensor-driven regularization
process into a sum of single direction smoothing processes, each of them being expressed
as a curvature-preserving PDE. As a result, the corresponding curvature-preserving PDE
directed by a tensor field T is
∀i = , . . . , n,
∂Ii
∂t = trace(THi) + 
π ∇IT
i ∫
π
α=J√
Taα
√
Taα dα
(.)
When T is locally isotropic (on homogeneous region), then > Eq. (.) is similar to a
D heat-flow, while when T is locally anistropic (on an image contour), it behaves as a
D heat-flow on the streamline curve following the contour path, thus taking care of its
curvature.

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

Noisy color image (left), denoised image (right) by 
curvature-preserving PDE (50.75)
Image of a ﬁngerprint
After several iterations 
of trace-based PDE (50.71)
After several iterations of
curvature-preserving PDE
(50.75) (with same tensor
field T) 
⊡Fig. -
Using PDE-based smoothing to regularize color and grayscale images
..
Applications
Some application results are presented here, mainly based on the use of the curvature-
preserving PDE’s (> .). A specific diffusion tensor field T has been used to adapt the
smoothing behavior to each targeted application.


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
Original color image (left), image with 50% pixel removed (middle), reconstructed 
using PDE (50 .75) (right)
Original color image (left), reconstructed using PDE (50.75) (right)
(the in painting mask covers the cage)
⊡Fig. -
Image inpainting using PDE-based regularization techniques
...
Color Image Denoising
Image denoising is a direct application of regularization methods. Sensor inaccuracies, dig-
ital quantifications, or compression artefacts are indeed some of the various noise sources
that can affect a digital image, and suppressing them is a desirable goal.
> Figure -
illustrates how curvature-preserving PDE’s (> .) can be successfully applied to remove
such noise artefacts while preserving the thin structures of the processed images. The
tensor field T is chosen as in > Eq. (.).

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

...
Color Image Inpainting
Image inpainting consists in filling-in missing (user-defined) image regions by guessing
pixel values such that the reconstructed image still looks natural. Basically, the user pro-
vides one color image I : Ω →R, and one mask image M : Ω →{,}. The inpainting
algorithm must fill-in the regions where M(X) = , by the mean of some intelligent
interpolations. Image inpainting using diffusion PDE’s has been proposed for instance in
[, , ]. Inpainting is a direct application of our proposed curvature-preserving PDE
(> .), where the diffusion equation is applied only on the regions to inpaint, allowing
the neighbor pixels to diffuse inside these regions in an anisotropic way ( > Fig. -).
Vector ﬁeld visualization
with arrows
Visualization using PDE
(50.75) (after 5 iter)
Visualization using PDE
(50.75) (after 15 iter)
Tensor ﬁeld displayed with ellipsoids
(left) and tracked ﬁbers (right)
Tensor ﬁeld rendered using a PDE approach
(50.75)
⊡Fig. -
Visualization of vector and tensor ﬁelds using PDE’s


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
...
Visualization of Vector and Tensor Fields
Regularization PDE’s such as (> .), (> .), and (> .) can be also used to visu-
alize a vector field w : Ω →Ror a tensor field G : Ω →P(), see also > Sect. .. The
idea is to smooth an originally pure noisy image using a diffusion tensor field T which is
chosen to be T = wwT or T = G, or other variations as long as the smoothing geometry is
indeed directed by the field we want to visualize. Whereas the PDE evolution time t goes
by, more global structures of the considered fields appear, i.e., a visualization scale-space
is constructed. The same PDE-based visualization technique allows to display interesting
global rendering of DT-MRI volumes (medical imaging) displaying “stuffed” views of the
fibers map. ( > Fig. -).
References and Further Reading
. The homepage of geomerics ltd. http://www.
geomerics.com.
. Abłamowicz
R,
Fauser
B
()
Clifford/
bigebra, a maple package for Clifford (co)algebra
computations. Available at http://www.math.
tntech.edu/rafal/. © –, RA&BF
. Bayro-Corrochano E, Vallejo R, Arana-Daniel N
() Geometric preprocessing, geometric feed-
forward neural networks and Clifford support
vector machines for visual learning. Special issue
of Journal Neurocomputing :–
. Benger W () Visualization of general rela-
tivistic tensor fields via a fiber bundle data model.
PhD thesis, FU Berlin
. Benger W () Colliding galaxies, rotating
neutron stars and merging black holes – visu-
alising high dimensional data sets on arbitrary
meshes. N J Phys . http://stacks.iop.org/-
//
. Benger W, Ritter, M, Acharya S, Roy S, Jijao F
() Fiberbundle-based visualization of a stir
tank fluid. In WSCG , Plzen
. Bochev P, Hyman M () Principles of compat-
ible discretizations. In: Proceedings of IMA Hot
Topics Workshop on Compatible Discretizations.
Springer, vol IMA , pp –
. Brouwer
L
()
Zur
Invarianz
des
n-
dimensionalen
Gebiets.
Mathematische
Annalen, 
. Buchholz S, Hitzer EMS, Tachibana K ()
Optimal learning rates for Clifford neurons. In:
International Conference on Artificial Neural
Networks,Porto,Portugal.vol ,pp –,–
. Butler DM, Bryson S () Vector bundle classes
form a powerful tool for scientific visualization.
Comput Phys :–
. Butler DM, Pendley MH () A visualization
model based on the mathematics of fiber bundles.
Comput Phys ():–
. Clifford WK (a) Applications of grass-
mann’s extensive algebra. In: Tucker R (ed)
Mathematical
Papers.
Macmillian,
London,
pp –
. Clifford WK (b) On the classification of geo-
metric algebras. In: Tucker R (ed) Mathematical
Papers. Macmillian, London, pp –
. Dorst L, Fontijne D, Mann S () Geometric
algebra for computer science, an object-oriented
approach to geometry. Morgan Kaufman, San
Mateo
. Ebling J () Clifford fourier transform on
vector fields. IEEE Trans Visual Comput Gr
():–.
IEEE
member
Scheuermann,
Gerik
. Firby P, Gardiner C () Surface topology,
Chap . Ellis Horwood, Vector Fields on Surfaces,
pp –
. Fontijne D () Efficient implementation of
geometric algebra. PhD thesis, University of
Amsterdam
. Fontijne D, Bouma T, Dorst L () Gaigen:
a geometric algebra implementation generator.
http://www.science.uva.nl/ga/gaigen
. Fontijne D, Dorst L () Modeling D
euclidean geometry. IEEE Comput Graph Appl
():–

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

. Garth C, Tricoche X, Scheuermann G ()
Tracking of vector field singularities in unstruc-
tured D time-dependent datasets. In: Proceed-
ings of the IEEE Visualization, pp –
. Globus A, Levit C, Lasinski T () A tool for
visualizing the topologyof threedimensional vec-
tor fields. In: Proceedings of the IEEE Visualiza-
tion ’, pp –
. Gottlieb DH () Vector fields and classical
theorems of topology. Rendiconti del Seminario
Matematico e Fisico, Milano, 
. Gottlieb DH () All the way with gauss-
bonnet and the sociology of mathematics.
The American Mathematical Monthly ():
–
. Gross P, Kotiuga PR () Electromagnetic the-
ory and computation: a topological approach.
Cambridge University Press, Cambridge
. Hart J () Using the cw-complex to repre-
sent the topological structure of implicit surfaces
and solids. In: Implicit Surfaces ’, Eurograph-
ics/SIGGRAPH, pp –. http://basalt.cs.uiuc.
edu/∼jch/papers/cw.pdf
. Hatcher A () Algebraic topology. Cambridge
University Press, Cambridge
. Hauser H, Gröller E () Thorough insights
by enhanced visualization of flow topology. In:
th International Symposium on Flow Visual-
ization,
http://www.cg.tuwien.ac.at/research/
publications//Hauser--Tho/
. Helman J, Hesselink L () Representation and
display of vector field topology in fluid flow data
sets. IEEE Computer ():–
. Helman J, Hesselink L () Visualizing vector
field topologyin fluid flows. IEEE Comput Graph
Appl :–
. Hestenes D () New foundations for classical
mechanics. Reidel, Dordrecht
. Hestenes D, Sobczyk G () Clifford algebra to
geometric calculus: a unified language for math-
ematics and physics. Dordrecht, Reidel
. Hildenbrand D, Fontijne D, Perwass C, Dorst L
() Tutorial geometric algebra and its appli-
cation to computer graphics. In: Eurographics
Conference Grenoble
. Hildenbrand D, Fontijne D, Wang Y, Alexa M,
Dorst L () Competitive runtime perfor-
mance for inverse kinematics algorithms using
conformal geometric algebra. In Eurographics
conference Vienna.
. Hildenbrand D, Lange H, Stock F, Koch A ()
Efficient inverse kinematics algorithm based on
conformal geometric algebra using reconfig-
urable hardware. In GRAPP conference Madeira
. Hildenbrand D, Pitt J () The Gaalop home
page. http://www.gaalop.de
. Hocking J, Young G () Topology. Addison-
Wesley, Dover, New York
. Hunt J () Vorticity and vortex dynamics in
complex turbulent flows. Proceedings of CAN-
CAM, Transactions of the Canadian Society for
Mechanical Engineering, :
. Jeong J, Hussain F () On the identification of
a vortex. J Fluid Mech :–
. Kenwright D, Henze C, Levit C () Feature
extraction of separation and attachment lines.
IEEE Trans Vis Comput Graph ():–
. Löffelmann H, Doleisch H, Gröller E () Visu-
alizing dynamical systems near critical points. In:
Spring Conference on Computer Graphics and its
Applications. Budmerice, Slovakia, pp –
. Mann S, Rockwood A () Computing singu-
larities of D vector fields with geometric alge-
bra. In: Proceedings of the IEEE Visualization, pp
–
. Mattiussi C () The geometry of time-
stepping. In: Teixeira FL (ed) Geometric meth-
ods in computational electromagnetics, PIER .
EMW, Cambridge, pp –
. McCormick B, DeFanti T, Brown M () Visu-
alization in scientific computing. Comput Gr
()
. Naeve A, Rockwood A () Course geomet-
ric algebra. In: Siggraph conference Los Angeles.
. Perwass
C
()
The
CLU
home
page.
http://www.clucalc.info
. Perwass C () Geometric algebra with appli-
cations in engineering. Springer, Berlin
. Petsche H-J () The Grassmann Bicenten-
nial
Conference
home
page.http://www.uni-
potsdam.de/ u/ philosophie/ grassmann/ Papers.
htm
. Pham MT, Tachibana K, Hitzer EMS, Yoshikawa
T, Furuhashi T () Classification and cluster-
ing of spatial patterns with geometric algebra. In:
AGACSE conference Leipzig.
. Reyes-Lozano L, Medioni G, Bayro-Corrochano
E () Registration of d points using geo-
metric algebra and tensor voting. J Comput Vis
():–


Diﬀerential Methods for Multi-Dimensional Visual Data Analysis
. Rosenhahn B, Sommer G () Pose estimation
in conformal geometric algebra. J Math Imaging
Vis :–
. Stalling D, Steinke T () Visualization of
vector fields in quantum chemistry. Technical
Report, ZIB Preprint SC--
. Theisel H, Weinkauf T, Hege H-C, Seidel H-P
() Saddle connectors – an approach to visu-
alizing the topological skeleton of complex D
vector fields. In Proceedings of the IEEE Visual-
ization, pp –
. Theisel H, Weinkauf T, Hege H-C, Seidel H-P
() Grid-independent detection of closed
stream lines in D vector fields. In Proceedings
of the Vision, Modeling and Visualization ,
November –, USA, pp –, http://
www.courant.nyu.edu/∼weinkauf/publications/
bibtex/theiselb.bib
. Tonti E (/) The Reason for Analogies
between Physical Theories. Appl Math Model
():–
. Treinish LA () Data explorer data model.
http://www.research.ibm.com/people/l/lloydt/
dm/dx/dx_dm.htm.
. Veldhuizen
T
()
Using
C++
template
metaprograms.
C++
Report
():–.
Reprinted in C++ Gems, ed. Stanley Lippman
. Venkataraman
S,
Benger
W,
Long
A,
Byungil Jeong LR () Visualizing hurri-
cane katrina – large data management, rendering
and display challenges. In: GRAPHITE ,

November–
December,
Kuala
Lumpur,
Malaysia
. Weinkauf T () Extraction of topological
structures in D and D vector fields. PhD the-
sis, University Magdeburg. http://tinoweinkauf.
net/
. Weinkauf T, Theisel H, Hege H-C, Seidel H-P
() Boundary switch connectors for topolog-
ical visualization of complex D vector fields.
In: Data Visualization . Proceedings of
the VisSym , May –, Konstanz, Ger-
many, pp –, http://www.courant.nyu.edu/
∼weinkauf/publications/bibtex/weinkaufa.bib
. Zomorodian AJ () Topology for comput-
ing. In: Cambridge Monographs on Applied and
Computational Mathematics
. Alvarez L, Guichard F, Lions PL, Morel JM
() Axioms and fundamental equations of
image processing. Arch Ration Mech Anal ():
–
. Aubert G, Kornprobst P () Mathematical
problems in image processing: partial differential
equations and the calculus of variations, applied
mathematical sciences, vol . Springer, January
. Barash D () A fundamental relationship
between bilateral filtering, adaptive smoothing
and the nonlinear diffusion equation. IEEE Trans
Pattern Anal Mach Intell ():
. Becker J, Preusser T, Rumpf M () PDE meth-
ods in flow simulation post processing. Comput
Vis Sci ():–
. Bertalmio M, Sapiro G, Caselles V, Ballester C
() Image inpainting. ACM SIGGRAPH, Int
Conf Comp Gr Interact Tech pp –
. Black MJ, Sapiro G, Marimont DH, Heeger D
() Robust anisotropic diffusion. IEEE Trans
Image Process ():–
. Cabral B, Leedom LC () Imaging vector fields
using line integral convolution. SIGGRAPH’,
in Computer Graphics Vol., pp –
. Chan T, Shen J () Non-texture inpaintings
by curvature-driven diffusions. J Vis Commun
Image Represent ():–
. Charbonnier P, Blanc-Féraud L, Aubert G, Bar-
laud M () Deterministic edge-preserving
regularization in computed imaging. IEEE Trans
Image Process ():–
. Di Zenzo S () A note on the gradient of a
multi-image. Comput Vision Gr Image Process
:–
. Kimmel R, Malladi R, Sochen N () Images as
embedded maps and minimal surfaces: movies,
color,texture, and volumetric medical images. Int
J Comput Vision ():–
. Koenderink JJ () The structure of images.
Biol Cybern :–
. Kornprobst P, Deriche R, Aubert G () Non-
linear operatorsin image restoration. In Proceed-
ings of the Conference on Computer Vision
and Pattern Recognition (CVPR ’) (June –,
). CVPR. IEEE Computer Society, Washing-
ton, DC, pp 
. Lindeberg T () Scale-space theory in com-
puter vision. Kluwer Academic, Dordrecht
. Nielsen M, Florack L, Deriche R () Regular-
ization, scale-space and edge detection filters. J
Math Imaging Vis ():–

Diﬀerential Methods for Multi-Dimensional Visual Data Analysis 

. Perona P, Malik J () Scale-space and edge
detection using anisotropic diffusion. IEEE Trans
Pattern Anal Mach Intell ():–
. Preußer T, Rumpf M () Anisotropic non-
linear diffusion in flow visualization. In Pro-
ceedings of the Conference on Visualization ’:
Celebrating Ten Years (San Francisco, Califor-
nia, United States). IEEE Visualization. IEEE
Computer Society Press, Los Alamitos, CA,
pp –
. Rudin L, Osher S, Fatemi E () Nonlinear
total variation based noise removal algorithms.
Physica D :–
. Sapiro G () Geometric partial differential
equations and image analysis. Cambridge Uni-
versity Press, Cambridge
. Sapiro G, Ringach DL () Anisotropic dif-
fusion of multi-valued images with applications
to color filtering. IEEE Trans Image Process
():–
. Tomasi C, Manduchi R () Bilateral Filter-
ing for Gray and Color Images. In Proceed-
ings of the Sixth international Conference on
Computer Vision (January –, ). ICCV.
IEEE
Computer
Society,
Washington,
DC,
pp 
. Tschumperlé D, Deriche R () Vector-valued
image regularization with PDE’s: a common
framework for different applications. IEEE Trans
Pattern Anal Mach Intell ()
. Tschumperlé D () Fast anisotropic smooth-
ing of multi-valued images using curvature-
reserving PDE’s. Int J Comput Vis ():–,
ISSN: -
. Vemuri BC, Chen Y, Rao M, McGraw T,
Wang Z, Mareci T () Fiber Tract Mapping
from Diffusion Tensor MRI. In Proceedings
of the IEEE Workshop on Variational and
Level Set Methods (Vlsm’) (July –, ).
VLSM. IEEE Computer Society, Washington,
DC, pp 
. Weickert J () Anisotropic diffusion in image
processing. Teubner-Verlag, Stuttgart
. Weickert J () Coherence-enhancing diffu-
sion of colour images. Image Vis Comput :
–


Index
A
Abel transform, , , 
Accelerated EM algorithms, , , –
Acoustically inhomogeneous medium, , 
Acoustic attenuation, , –, 
Acousto-electric imaging, , , 
Active contours, , –, , , ,
, 
Adjoint field method, –
Algebraic reconstruction technique (ART), , ,
, –, , , , , –
Alternating projection theorem, –, 
Analysis of minimizers, , 
Anisotropic, , , , , , , , , ,
, –, , , , , , , ,
, , , , 
–conductivity, , , , 
–diffusion, –
–medium, , , 
–total variation, , –, , , 
Anomaly detection, , , –, , 
Aperture problem, , 
Approximation errors, , , , , –,
, , 
a priori choices, , , –, 
ART. See Algebraic reconstruction technique
ART, MART and SMART methods, , , , ,
–, , , , –
Asymptotic expansions, , , –, , ,
, , , , , , , , , ,
, , 
Asymptotics of neighborhood filters, –
Autocorrelation function, , , 
B
Backpropagation, –, 
Bag of features (BOF), 
Banach space, , , –, , , , , ,
, , –, , –, , , , ,
, , , , –, , , , 
Band-limited signal, –, 
Basis pursuit, , , , , 
Bayes estimation, , –, , , , 
Bayesian, , , , , , –, –, ,
, , , , , , –, –,
–
–estimate, , , , , , , 
–formulation, –
–inference, , , –, , –
Bilateral filter, –, , , –,
–
Blob, –, –, , 
Blur, , , , , –, , , , , , ,
, , , , , , , –,
, , –, , , , , 
BOF. See Bag of features
Born approximation, , , , , –, ,
–, –, –, , 
Boundary control method, –, , 
Boundary detection, 
Boundary distance function, , , , , ,
–, 
Boundary measurements, , , , , ,
–, –, , , , , , , ,
, , 
Bounded variation (BV), , , , –,
–, , , , , , 
function, , , , , , 
Bregman distance, , –, , , 
Bregman iteration, –, , –, 
BV. See Bounded variation
C
Calderón’s problem, –, , 
Canonical form, , –
Cartesian lattice, , , 
CEM. See Complete electrode model
CG method. See Conjugate gradient (CG) method
CGO. See Complex geometric optics
Characterization, , , , , , , , , ,
, –, , , , , , , , ,
, , , , 
Cheeger set, , , 
Classification
–morphology, –
–object, , 


Index
Coherence, , –, , , , , ,
, , 
Colour level set, –
Combinatorial optimization, 
Compact operator, –, –, , , , , ,
–, , , , , –
Complete electrode model (CEM), , , ,
–, 
Complex geometric optics (CGO), , –,
, 
Compressive sampling, , 
Compressive sensing, , –,
, 
Computational anatomy, , 
Computed tomography (CT), , , , , , ,
, –, , , , , 
Conditional stability, , , –, , 
Conjugate gradient (CG) method, , , , , ,
, , , , –, ,
, , , , 
Convergence, EM Algorithms, , –
Convergence rate, , , , , –, , –,
–, , , , , , , , –,
, , , , , –, , , ,
, , 
Convex energy, , , , , 
Convex penalty term, –, 
Crack, , , –, –, , , –,
, 
Crack detection, –
Critical point, –
CT. See Computed tomography
Curvature, , , , , , –, ,
, , –
Curvelet, , , –, –
Curve matching, , 
D
Data access ordering, , , , 
Data model, –, –, , ,
–, 
Data redundancy, , 
D-bar (bar/partial) method, , , ,
, 
DCT. See Discrete cosine transform
Deblending, , , 
Decay, , , , , , , , , , ,
–, , , , , , 
Decomposition methods, , , –, , ,
, , , 
Deconvolution, , –, , –, , –,
, , –, –, –, , , ,
, , , , –
Degree of nonlinearity, –
Demosaicking, 
Denoising, , , , –, , , , ,
, –, , –, , , ,
, , , , , –, , ,
, , –, –, 
Density estimation, –, , , ,
–
Detection
–feature, , , 
–object, , , , , , , ,
, 
Determination, shape of boundary,
–, 
Diffeomorphism, –, –, 
Differential equations, , –, , 
Differential geometry, , , –
Diffuse optical tomography (DOT), , , , ,
, , , 
Diffusion approximation, 
Diffusion distance, , –
Diffusion geometry, –
Dipole potential, , , , , 
Direct regularization methods, –, , –,
, 
Dirichlet to Neumann map, , , , ,
–, –, , , , , , 
Discrepancy, –, , , , , , , , , ,
, , , , , , , , , , ,
, , 
Discrepancy principle, , , , , , , , ,
, , , –, , , , , , 
Discrete cosine transform (DCT), , –,
–
Discrete Expectation Maximization algorithm,

Discrete Gabor system, –
Discretization, , , , , , , , , ,
, , , , , , , , , –,
, , –, , , , , ,
–, , , , , , 
Distance measure
–morphological, –
–multi modal, 
–pixel based, –, 
–statistical, , –

Index

Distortion, , , –, , , , ,
, –, , , , , ,
–, , , 
Divergence (f -divergence)
Kullback-Leibler divergence, , ,
–, –, , , , , ,
, 
Shannon divergence, , 
DOT. See Diffuse optical tomography
Dual frame, , , , , 
Dual methods, –
Dynamical shape priors, , , , ,
, 
E
ECT. See Electrical capacitance tomography
Edge restoration, , , , , , , ,
, , , , 
Eigenfunction expansion, –, –, 
Eikonal equation, , –, –
EIT. See Electrical impedance tomography
Elastic deformation, , , , , , ,
, 
Electrical capacitance tomography (ECT), , ,
, , –
Electrical impedance tomography (EIT), –,
, , , , , , –, , , ,
, , , , –, , 
Electrical resistance/resistivity tomography (ERT),
, , , , , , –, 
Electromagnetic waves, , , , , , ,
, , , , , , , 
Electron microscopy, , , , –, , ,

Elliptic and hyperbolic PDEs, , , , , ,
, , , 
Elliptic approximations, , , –,

EM algorithm. See Expectation-maximization
algorithm
Embedding, , , , , , , –, ,
, , , , –, , , , ,
, , , , , , , , ,
–, , , , , , 
Emission tomography, , , –, –, ,
, , , –, , , 
Energy minimization, , , , , 
ERT. See Electrical resistance/resistivity tomography
Euler-Poincaré manifold, –
Evidence, –, , , , , , 
Expectation-maximization (EM) algorithm, –,
–, , , , , , , , –,
, , , , –, , , –,
, 
F
Factorization method, –, , , , , ,
, , , –, , , , , , ,
–, , 
Far field operator, , , –, , –,
, , , , , , , –, –
Far field pattern, , , , –, –, ,
, , , , , , , , , ,
–, –, –, –, –
Fast Fourier transform (FFT), , –, , ,
, , , , 
Fast marching, , , –
FBP. See Filtered backprojection algorithm
Feature descriptor, , –
Feature extraction, , , , 
FEM. See Finite element method
Fenchel conjugate
–compressive sensing, 
–convex function, 
–deconvolution, –
–denoising, 
–directional derivative, , , , , , 
–Fenchel duality
–linear constraints, , 
–Fredholm integral equations, –, –
–Hahn-Banach theorem, , 
–inverse scattering, –, –
–Lagrange multiplier, 
–linear inverse problems, –
–lower semi-continuous, 
–norm, , , 
–subdifferential, 
–sublinear function, 
–support function, , 
–total variation, , 
–variational principle, –
FFT. See Fast Fourier transform
Fiber bundles, –
Figure of merit (FOM), –
Filtered backprojection (FBP) algorithm, –,
, , , , , , , , 
Finite differences approximations, 
Finite-dimensional object representation, ,
–, 
Finite discrete Gabor system, 


Index
Finite element approximation, –
Finite element method (FEM), , , , , ,
–, , 
First kind integral equation, , , , , –,
, , , , 
Fisher information, –, 
FOM. See Figure of merit
Fourier transform, , , , , , , , ,
, , , , , , , , , , ,
, , , , , , , , , , ,
, , –, –, –, ,
, , , , , , , , 
Frequency domain, –, 
Functional photoacoustic tomography, , , ,
, 
G
Gabor analysis, , , , –, , 
Gabor expansion, , , , 
Gabor frame, , –, –, , ,
, , 
Galaxy
–Abell cluster, –
–NGC , , , 
Gaussian beam, –
Gaussian noise, , , , , , , , ,
, , , , , , , –, ,
, , , , , , , , ,
, , , , 
Gauss-Newton method, –, , , , ,
–, , 
GCV. See Generalized cross validation
Gelfand width, , –
Generalized cross validation (GCV), , –, 
Generalized Radon transform, , 
Geodesic, , , , , , , ,
, 
–distance, , , , , , , , ,
, , , –, , , ,
, , , , , 
–path, , , , , , –
Geometric algebra, , , , –,
, 
Geometrical optics, , , , 
Gibbs smoothing, –, –, –
Golub-Kahan bidiagonalization (GKB), –, 
Gradient descent, , , –, , , , ,
, , , , , , , ,
–, –, , –, ,
–, , , 
Gradient descent method, , , , , , ,
, , , 
Gradient methods, , , , , , –, ,
, , , , –, , –
Graph-cut methods, , –, , , 
Graph-cuts algorithm, 
Greedy algorithm, , , , 
Green’s function, , , , , , , ,
, , , , –, –, , , ,
, , , , , 
Gromov-Hausdorff distance, , –, 
Group action, , 
H
Haar basis, , , 
Half-time reconstruction, –
Hamiltonian system, , , , 
Hard margin classification, , –,
–
Hausdorff measure, –
Heat equation, , , , , , , , ,
, , , , , –, , ,
, , 
Heat kernel, , , , 
Helmholtz equation, , , , , , , ,
, , , , , , , , , –,
, , , , , –, , , –,
, , , , 
Hexagonal lattice, –
Hierarchical model, –, 
High resolution image reconstruction, 
Hilbert space, –, , , , , , –, –,
, , , , , , , , , , ,
, –, , –, , , ,
–, , , 
Histogram concentration, , –
Homology, , , , , ,
–
Homotopy method, –
Huygens’ principle, , , , –, , ,
, 
Hybrid model, , 
Hyperbolic equation, , , , 
Hyperprior, –, 
I
IACT. See Integrated autocorrelation time
Identification
–coefficients in wave equations, –
–of potential, –
–of wave source, –

Index

Ill-posed, , , , –, , , , , ,
, , , , , , , –, , ,
, , , , , 
–problem, , , , –, , , –, , ,
, , , , , 
Ill-posedness and local ill-posedness, –, 
Image deblurring, , –, , , , , ,
, , , , , , 
Image denoising, , , –, , ,
–, , , , , , ,
, 
Image filtering, 
Image inpainting, , , , 
Image reconstruction, , , , , , ,
–, , , , , , , , , ,
, , , , , , , , , ,
, , , 
Image registration, , , , , , , 
Image representation, , 
Image restoration, , , , , , , , ,
–, , , , –, , ,
, , 
Image segmentation, –, , , –,
, , , , , , , , ,
–, 
Images with interfaces, , , , , , 
Impedance tomography, , , , , , ,
, –, , , , , , 
Implicit shape representation, , , , ,
, , 
Impulsive noise, –, , 
Incomplete data, , , , , , , ,
, , , –, –
Inexact Newton method, , –, 
Infinite dimension manifold, 
Information (f -information)
–Helling information, –
–Mutual information, , –, , 
Inhomogeneous medium, , , , , ,
–, , –, , , –, 
Integral equation with analytic kernel,
–, 
Integrated autocorrelation time (IACT), , 
Intercept, , , , , , , –,
, 
Internal measurements, , , , , 
Interpolation, , , –, , , , ,
, , , , , , , , ,
, –, , , , 
Intrinsic alignment, , 
Invariance, , , –, , , , ,
, , , –, , , –,
–, , , , , , , ,
–, 
Invariant correspondence, , 
Invariant distance, , , 
Invariant similarity, –, , , –
Inverse, , –, , , , , , , , , , ,
, , , , –,, , , , 
–conductivity problem, , 
–medium scattering, , 
–obstacle scattering, –, , , , 
–problems, –, , , , , , , , , , ,
, , , –, , , , , , , , ,
, , , , , , , , , –,
–, , , , , , , , ,
, , , , , , , , , ,
, , , , , , , , , ,
, , , , , 
–scattering, –, –, , , , ,
, , , , –, , –, ,
, , , 
–scattering problem, , , , , , ,
, , –, , , , 
Inverse Synthetic-Aperture Radar (ISAR), ,
–, , , , , 
Inversion formula, , , , , , , , ,
, , –, –, –, , 
IRLS. See Iteratively re-weighted least squares
ISAR. See Inverse Synthetic-Aperture Radar
Isotropic Undecimated Wavelet Transform (IUWT),
, , 
Iterated Tikhonov regularization, , , 
Iterative image reconstruction, , , , ,
–
Iteratively regularized Gauss-Newton method, ,
, , , –
Iteratively re-weighted least squares (IRLS), ,
–, 
Iterative regularization methods, , –, ,
, 
Iterative step, , , , , , , 
J
Jump set, , , , 
K
Kaczmarz-type methods, , , , , 
Karcher Mean, , , 
Kernel, , , , , , –, , , , ,
, , , , , , , , –,


Index
, , , , –, , , ,
, , , , , , , , , ,
, , , , , , , , ,
, 
Kernel density estimation, –, , ,
–, 
Kirchhoff approximation, , , 
Kirchhoff imaging, –, , , 
KL divergence. See Kullback-Leibler (KL) divergence
Kriging, –
Kuhn-Tucker conditions, , , , , ,
, , 
Kullback-Leibler (KL) divergence, , , –,
, , , , , , , 
L
Lagrangian, , , –, –, , ,
, , , , –, , , 
Landmark matching, , 
Landweber-Kacmarz method, , 
Laplace-Beltrami operator, , , , ,
–, , –, , 
Laplace equation, , , , , 
LBFGS quasi-Newton method, 
Least squares (LS) classification, , , –,
–, –
Least squares (LS) regression, , , –,
–
Level sets, , , , , , –, , ,
, , , , , , –,
, 
–functions, , –, , , , ,
–, , , , , , ,
–, , , , , , –
–method, –, , , , , , ,
, , , 
Levenberg-Marquardt method, , , , ,
, 
Likelihood, , , , , , , , , –,
–, , , –, , , –, ,
, , , , , , , , –, ,
, , , –, –, , , , ,
, , , , , 
Linear programming (LP), , , , , 
Linear sampling method, , , , , –,
, , –, , 
Linear shape prior, –, 
Line detector, , , 
Lippmann-Schwinger integral equation, , , ,
, –, –
L-minimization, –, –, –, , ,
–, , –
Local D recovery, 
Loss function, , , –, , , , ,
–, 
LP. See Linear programming
LS classification. See Least squares (LS) classification
LS regression. See Least squares (LS) regression
M
Magnetic resonance elastography (MRE), , ,
, , 
Magneto-acoustic imaging, , , , –
MAP. See Maximum a posteriori
Margin, , , , , , , –, , ,
, 
Markov chain Monte Carlo (MCMC), , ,
–, , 
Matching pursuit, , , , 
Maximum a posteriori (MAP), , , , –,
, , –, –, , , –
Maximum flow methods, –
Maximum likelihood (ML), , , –, , ,
, , –, , –, , , ,
, , , , , , , , –,
–, , , 
Maximum likelihood estimator (MLE), , , ,
, –
Maxwell’s equations, , , –, , , ,
–, –, , , 
MCM. See Mean curvature motion
MDS. See Multidimensional scaling
Mean curvature motion (MCM), , , –,
, , , , –, 
Measurement matrix, , , , –, , 
Mesh fairing, , , , , , 
Mesh non-local means, 
Metric, , –, , , , , , , ,
, , , , , , , , ,
, , , , , , , , ,
, 
–geometry, , , 
Microlocal analysis, , , 
Microwave breast screening, , , , , ,
, 
Minimizer function, , –, , –,
–
Misfit functional, –
ML. See Maximum likelihood
MLE. See Maximum likelihood estimator

Index

Model reduction, –, , 
Monocular depth estimation, , 
Moore-Penrose inverse, –, 
Movie colorization, 
Movie denoising, 
MRE. See Magnetic resonance elastography
Multidimensional scaling (MDS), ,
–
Multi-label problems, –
Multiple signal classification (MUSIC), , , ,
–, , , , , , 
Multiplicative iterative algorithms, –
Multiresolution analysis, , , , , ,
, 
Multiscale vision model, –
Multi-task learning, , , –, 
Multi-wave imaging, 
MUSIC. See Multiple signal classification
MUSIC imaging, –, , 
N
N-D map. See Neumann-to-Dirichlet (N-D) map
Near infrared (NIR), , , , 
Neighborhood filter, , –, , –,
, –, 
Neumann-Dirichlet operator, , , –,
, 
Neumann-to-Dirichlet (N-D) map, , , ,
, –, –, , –, ,
–
Newton’s method, , –, , ,
, 
Newton type methods, , , , , , , 
NIR. See Near infrared
Non-convex analysis, –, –, , , 
Nonlinear ill-posed problem, , , 
Nonlinear Landweber regularization, , , 
Nonlocal-means, , , , , –, ,
–, –
Nonlinear operator equation, , , –, , ,
, –, 
Nonlinear shape prior, , , , 
Nonlocal operators, , , , , 
Non-local total variation, –, 
Nonnegative least squares, , –
Non-smooth analysis, , , –, –,
, 
Nonstationary, , –, , , , ,
, , 
Null space property (NSP), , , –, 
Numerical methods
–dual methods, –
–primal-dual methods, –, ,
–
O
Observation surface, , , , , , , 
Optical imaging, , , , –, , ,
, 
Optimal control, , , 
Optimization, , , , , , , , , , ,
, –, , , , , , , ,
, , , , , , , , ,
, , , 
Optoacoustic tomography, 
Order of approximation, –, , 
Orthonormal basis, , , , , , , ,
, , , , , 
P
Parameter identification, , 
Parametric shape representation, –, ,
, 
Parametrix, –, 
Partial data, , , , –, , –
Partial differential equations (PDE), , , , ,
, , , , , , , , , , ,
, , , , , , , , ,
, , , –
Partial random Fourier matrix, , , –,

PAT. See Photoacoustic tomography
PCA. See Principal component analysis
Penalty terms, –, , , , , , , ,
, , , , , 
Perimeter, , , , , , –,
–, , , , 
Perona-Malik, , –, , , –,

Perturbation analysis, , , , , , , ,
, , –
PET. See Positron transmission tomography
Phase field, –, , –
Phase-field approximations, 
Photo-acoustic imaging, , , , , 
Photoacoustics, , , , , 
Photoacoustic tomography (PAT), , , ,
, 
Photometry, , , , , , , ,
, , , , 
Picard criterion, , , 


Index
Picture distance measure, , , , –
Piecewise constant media, 
Pixel, , , , , , , , , , , , ,
, , , , , , , , , , ,
, , , , , , , , ,
, , , , , , , , 
Planar detector, 
Plane incident wave, , , , , , 
Point spread function (PSF), –, , –, –,
, , , , , , , –, ,
–, , , –
Poisson noise, , , , , , , , 
Polarization tensor, –, , , , , ,
, , 
Polyphase matrix, , 
Positron transmission tomography (PET), –, ,
, , , , , , 
Posteriori choices, , , , 
Primal-dual methods, , , , –,
, , 
Principal component analysis (PCA), , , ,
, , , , , , –, ,
, , , , 
Prior, –, –, , , , , , ,
, , –, , –, –, , ,
–, , , , , , , ,
, 
Prior model, , , –, , , –,
, , , 
Probe method, , , –
Proximal analysis, , , , , , 
PSF. See Point spread function
Q
Quadratic misfit, –
Quincunx lattice, –
R
Radar, , , , , –, –, –,
, , , , –, –
Radial basis function (RBF), –, , , ,
, 
Radiative transfer, –, , , , 
Radon transform, , , , , , , , ,
, , , , , , , , , , ,
, , , , , 
Random matrix, , , 
Range, , , , , , , , , , , ,
, , , –, –, , –,
, , , , 
Range image enhancement, , –
Ratio cycle, 
RBF. See Radial basis function
Reciprocity gap functional, 
Regularity, –, , , –, ,
–, , , , , , , ,
, –
Regularization, , , , –, , –, , , ,
, , , , –, , , , , , ,
, , , , , , , , , , ,
, , , , , , , , , ,
, , , , , 
–in emission tomography, –
–methods, , , , , , , , , , , ,
–, , , –, , , , ,
, , , , , , , , , ,
, , 
Relaxation parameter, –, , , , , 
Rellich’s lemma, , , , , , , ,
, 
Representation error, , , , –
Reproducing Kernel Hilbert Space (RKHS), ,
, –, , –, –, ,
–, 
Reservoir characterization, –, –
Response operator, , , , , , , ,
, –
Restoration, , , , , , , , , ,
–, , , , , 
Restricted isometry property (RIP), , –,
–, 
Reverse diffusion, –, 
Richardson-Lucy (RL) algorithm, , , 
Riemannian distance, , , , , ,
, 
Riemannian geometry, , –
Riemannian manifold, , , , , , ,
, , –, , , , , , ,
, , –, –
Riemannian submersion, , –, , ,
, , , 
Riesz Basis, –, , , , ,
, 
RIP. See Restricted isometry property
Risk, , , , , , 
RKHS. See Reproducing Kernel Hilbert Space
RL algorithm. See Richardson-Lucy (RL) algorithm
ROF model. See Rudin-Osher-Fatemi (ROF) model
Rudin-Osher-Fatemi (ROF) model, , , ,
, , , –, , –, ,
–, 

Index

S
Saddle connector, 
SAR. See Synthetic-Aperture Radar
Scaling function, , , , , ,
, 
Scattering, –, –, –, , , ,
, , –, , , , , –,
–, , –, , , , ,
–, , , , , , , –, ,
–, , , , 
–relation, , –, , –, –
–theory, , , , , –, , , ,
, , –
–transform, , 
Scientific visualization, –, , –,
, , , 
Second dyadic decomposition, –
Seed growing, –
Segmentation, , , , , , , , ,
, , , , –, , , ,
, , , , –, –, ,
, 
Separable lattice, , , , –,
–
Separable nonlinear least squares problem, –, 
Separatrix, 
Series expansion method, , , 
Set of finite perimeter, –, , , ,
, , 
Shape, , , , , , , , , , , ,
, , , , , , , , , ,
, , , , , , , 
–average, , , , , –, 
–evolution, , , , , , , , ,
, , , –, , , 
–gradient, , 
–optimization, , –, , 
–priors, –, , , –,
, 
–sensitivity analysis, , , –
–space, , , –, , , ,
–, , –, , , ,
, –, , , –, ,
, , 
Shepp-Vardi EM algorithm, –, –, ,
, , , , 
Short time Fourier transform (STFT), ,
–, –
Signal and image processing, , , , , ,
, 
Signal restoration, , , , 
Silver-Müller radiation condition, , , ,
–
Similarity filters, –
Single-scattering approximation, ,
–, 
Singular sources method, –
Singular value decomposition (SVD), , , , ,
, –, , , –, , , , , 
Singular values, , , , , –, , , ,
, , , , , , , 
Small-scene approximation, , 
Sobolev functions, , , , , , , 
Soft margin classification, , –,
–, 
Soft thresholding, , 
Sommerfeld radiation condition, , , , ,
, , , , 
Source conditions, , , , , –, –, ,
, –, , , , –, 
Sparse recovery, , , , , , , , ,
, , 
Sparse representation, , , , , , ,
, , , , , , 
Sparse signal, , , , , 
Sparse vector, , , , , , , , ,
–, , –
Sparsity, , , , –, –, , , , ,
, , , , , , , , , , ,
, , , , 
–constraints, , 
Special functions of bounded variation, , ,
, , 
Spectral factorization, 
Speed method, –
Speed-of-sound heterogeneities, –, 
Spherical mean transform/operator, , , ,
–, , , 
Spherical Radon transform, , , , , ,
, , 
Spiral CT, , 
Spline curve, , –, –
Splines, , , , , –, 
–cardinal B-splines, , , , ,
, 
–complex splines, 
–fractional splines, , 
–isotropic splines, 
–polyharmonic splines, , , , 
–tensor splines, –, , , 


Index
Stability, , , , , , , , , , –
–analysis, , , , , –, , ,
–, , , –, –, ,
, 
Starlet wavelet transform, , , , , 
Statistical hypothesis testing, , 
Statistics, , , , , , , , , , ,
, , , , , , , , , , ,
, , , , , –, , ,
, 
Steepest descent and minimal error method, ,
, 
Stereo matching, –
STFT. See Short time Fourier transform
Stopping rules, , –, , , , , ,
, , , , , , 
Strehl ratio, , 
Stripmap, –
Structural inversion, , 
Structured media, –
Substitution algorithms, –, –
Super-resolution, , , , , –, , 
Support vector classification, , , , –,

Support vector machines (SVM), , , , ,
, , 
Support vector regression, , , –
Support vectors, , , , , , , ,
, 
Surface impedance, , , 
SUSAN filter, , , 
SVD. See Singular value decomposition
SVM. See Support vector machines
Synthetic aperture, , , , , , ,
, 
Synthetic-Aperture Radar (SAR), –, –,
–, , , –
T
Tangent PCA, 
Thermoacoustics, , 
Thermoacoustic tomography, , , 
Thermography, 
Thin shapes, , –, –, , ,
, 
Tight frame, , , , , 
Tikhonov regularization, –, , –, , –,
, –, , , , , , , , –,
, , , –, , , , , , ,
, , , , –
Tikhonov regularization in hilbert spaces, –, 
Time discretization, , , , , 
Time-frequency analysis, , , 
Time resolved, –
Time-reversal, –, , , , –, ,
–, , , 
Tomography, , , , , , , , , ,
, , , , 
Tomosynthesis, –, –, 
Topological derivative, –, 
Topology, , , , , , , ,
–, –, –, , –,
, –
Total variation (TV), , , , , , , , ,
, , , , , , , , , ,
–, , , –, , , –,
, , , , , –, , ,
, , , , , , , –,
, , , –, , , 
Total variation regularization, , , , , ,
, , , , 
Tracking, , , , , , , , ,
–, 
Transform method, , 
Transmission eigenvalues, , , ,
–, 
Transport equation, , , , , , 
Trapping, , , –, –, , , , ,
, , , , , 
Travel time, , , , –, , , –,
, 
TV. See Total variation
U
Ultrasound imaging, , –, , 
Unbounded linear operator, , , , 
Uniqueness, , , , , , , , –, ,
, , , , , , , , , , ,
, , , , , , , , , , ,
, , , , , , , , 
Uniqueness theorems in inverse scattering, 
V
Variable projection method, –
Variance stabilization, , 
Variational inequality, –
Variational methods, , –, , , , ,
, , –
Variational problem, , , , , , , ,
–, , , , , , , ,
, 

Index

Variational regularization, –, , , , , ,
, 
Variational regularization in banach spaces, –
Vector field analysis, –
Vector field topology, –
Velocity flow, –, 
Viscous dissipation, , , , , , 
Visibility condition, , 
Visible singularity, , 
Visualization, , , , , , , ,
, –, , –, , , ,
, –, –
W
Wave equation, –, , , –, –,
, , , , –, , , , –,
, , , , , –, –, –,
, , , , , –
Wave front, , , , , , 
Wavelets, , , , , , , , ,
, , –, –, , , ,
, , , , , , 
Wave packet, , 
Weak convergence, , , , , , , , , ,
, , 
Weak formulation, , , , , , –,

Well-posed problem, , , , , , , , ,
, , , , , 
X
x* minimum norm solution, –, 
Y
Yaroslavsky filter, , , , 
Z
Zooming, , , , 


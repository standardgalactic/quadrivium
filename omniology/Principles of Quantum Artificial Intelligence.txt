
QUANTUM 
PRiNCiPLES OF 
ARTiFiCiAL 
iNTELLiGENCE

This page intentionally left blank
This page intentionally left blank

NEW JERSEY  •  LONDON  •  SINGAPORE  •  BEIJING  •  SHANGHAI  •  HONG KONG  •  TAIPEI  •  CHENNAI  
World Scientific
Instituto Superior Técnico - Universidade de Lisboa, Portugal
QUANTUM 
PRiNCiPLES OF 
ARTiFiCiAL 
iNTELLiGENCE
Andreas Wichert

Published by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office:  27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office:  57 Shelton Street, Covent Garden, London WC2H 9HE
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
PRINCIPLES OF QUANTUM ARTIFICIAL INTELLIGENCE
Copyright © 2014 by World Scientific Publishing Co. Pte. Ltd.
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means,
electronic or mechanical, including photocopying, recording or any information storage and retrieval
system now known or to be invented, without written permission from the publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright
Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to
photocopy is not required from the publisher.
ISBN 978-981-4566-74-2
Printed in Singapore

for Andr´e

This page intentionally left blank
This page intentionally left blank

Preface
Artiﬁcial intelligence and quantum computation divide the subject into
many major areas. Each of these areas are now so extensive and huge,
that a major understanding of the core concepts that unite them is ex-
tremely diﬃcult. This book is about the core ideas of artiﬁcial intelligence
and quantum computation. They are united in new subarea of artiﬁcial
intelligence: “Quantum Artiﬁcial Intelligence”.
The book is composed of two sections: the ﬁrst is on classical com-
putation and the second section is on quantum computation. In the ﬁrst
section, we introduce the basic principles of computation, representation
and problem solving. In the second section, we introduce the principles
of quantum computation and their relation to the core ideas of artiﬁcial
intelligence, such as search and problem solving. We illustrate their use
with several examples.
The notes on which the book is based evolved in the course “Informa-
tion and Computation for Artiﬁcial Intelligence” in the years 2008 −2012
at Department of Computer Science and Engineering, Instituto Superior
T´ecnico, Technical University of Lisbon. Thanks to Technical University of
Lisbon for rewarding me a sabbatical leave in the 2012-2013 academic year,
which has given me the time to ﬁnish this book. My research in recent
years has beneﬁted from many discussions with Ana Paiva, Lu´ıs Tarrat-
aca, ˆAngelo Cardoso, Jo˜ao Sacramento and Catarina Moreira. Especially
I would like to thank Lu´ıs Tarrataca and oﬀer all of him deepest grati-
tude. The chapter about “Quantum Problem-Solving” is mainly based on
his work. Finally, I would like to thank my loving wife Manuela, without
her encouragement the book would be never ﬁnished.
Andreas Wichert
vii

This page intentionally left blank
This page intentionally left blank

Contents
Preface
vii
1.
Introduction
1
1.1
Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . .
1
1.2
Motivation and Goals
. . . . . . . . . . . . . . . . . . . .
2
1.3
Guide to the Reader . . . . . . . . . . . . . . . . . . . . .
4
1.4
Content . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4.1
Classical computation . . . . . . . . . . . . . . . .
4
1.4.2
Quantum computation
. . . . . . . . . . . . . . .
6
2.
Computation
9
2.1
Entscheidungsproblem . . . . . . . . . . . . . . . . . . . .
9
2.1.1
Cantor’s diagonal argument
. . . . . . . . . . . .
11
2.1.2
Reductio ad absurdum
. . . . . . . . . . . . . . .
11
2.2
Complexity Theory . . . . . . . . . . . . . . . . . . . . . .
12
2.2.1
Decision problems . . . . . . . . . . . . . . . . . .
13
2.2.2
P and NP
. . . . . . . . . . . . . . . . . . . . . .
13
2.3
Church–Turing Thesis . . . . . . . . . . . . . . . . . . . .
14
2.3.1
Church–Turing–Deutsch principle . . . . . . . . .
15
2.4
Computers
. . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4.1
Analog computers . . . . . . . . . . . . . . . . . .
15
2.4.2
Digital computers . . . . . . . . . . . . . . . . . .
16
2.4.3
Von Neumann architecture . . . . . . . . . . . . .
16
3.
Problem Solving
19
3.1
Knowledge Representation . . . . . . . . . . . . . . . . . .
19
ix

x
Principles of Quantum Artiﬁcial Intelligence
3.1.1
Rules . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.1.2
Logic-based operators . . . . . . . . . . . . . . . .
20
3.1.3
Frames . . . . . . . . . . . . . . . . . . . . . . . .
23
3.1.4
Categorial representation . . . . . . . . . . . . . .
23
3.1.5
Binary vector representation . . . . . . . . . . . .
24
3.2
Production System . . . . . . . . . . . . . . . . . . . . . .
25
3.2.1
Deduction systems . . . . . . . . . . . . . . . . . .
26
3.2.2
Reaction systems
. . . . . . . . . . . . . . . . . .
28
3.2.3
Conﬂict resolution . . . . . . . . . . . . . . . . . .
28
3.2.4
Human problem-solving . . . . . . . . . . . . . . .
29
3.2.5
Example . . . . . . . . . . . . . . . . . . . . . . .
29
3.3
Sub-Symbolic Models of Problem-Solving
. . . . . . . . .
30
3.3.1
Proto logic . . . . . . . . . . . . . . . . . . . . . .
31
3.3.2
Binding problem . . . . . . . . . . . . . . . . . . .
32
3.3.3
Icons . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.3.4
Euclidian geometry of the world . . . . . . . . . .
35
4.
Information
37
4.1
Information and Thermodynamics
. . . . . . . . . . . . .
37
4.1.1
Dice model . . . . . . . . . . . . . . . . . . . . . .
39
4.1.2
Entropy . . . . . . . . . . . . . . . . . . . . . . . .
40
4.1.3
Maxwell paradox and information . . . . . . . . .
41
4.1.4
Information theory
. . . . . . . . . . . . . . . . .
42
4.2
Hierarchical Structures . . . . . . . . . . . . . . . . . . . .
47
4.2.1
Example of a taxonomy . . . . . . . . . . . . . . .
49
4.3
Information and Measurement
. . . . . . . . . . . . . . .
50
4.3.1
Information measure I
. . . . . . . . . . . . . . .
52
4.3.2
Nature of information measure . . . . . . . . . . .
55
4.3.3
Measurement of angle . . . . . . . . . . . . . . . .
55
4.3.4
Information and contour
. . . . . . . . . . . . . .
56
4.4
Information and Memory
. . . . . . . . . . . . . . . . . .
57
4.5
Sparse code for Sub-symbols . . . . . . . . . . . . . . . . .
67
4.5.1
Sparsiﬁcation based on unary sub-vectors . . . . .
68
4.6
Deduction Systems and Associative Memory . . . . . . . .
68
4.6.1
Taxonomic knowledge organization
. . . . . . . .
74
5.
Reversible Algorithms
75
5.1
Reversible Computation . . . . . . . . . . . . . . . . . . .
75

Contents
xi
5.2
Reversible Circuits . . . . . . . . . . . . . . . . . . . . . .
76
5.2.1
Boolean gates
. . . . . . . . . . . . . . . . . . . .
76
5.2.2
Reversible Boolean gates . . . . . . . . . . . . . .
76
5.2.3
Toﬀoli gate . . . . . . . . . . . . . . . . . . . . . .
77
5.2.4
Circuit . . . . . . . . . . . . . . . . . . . . . . . .
78
6.
Probability
79
6.1
Kolmogorovs Probabilities . . . . . . . . . . . . . . . . . .
79
6.1.1
Conditional probability . . . . . . . . . . . . . . .
80
6.1.2
Bayes’s rule
. . . . . . . . . . . . . . . . . . . . .
81
6.1.3
Joint distribution
. . . . . . . . . . . . . . . . . .
82
6.1.4
Na¨ıve Bayes and counting
. . . . . . . . . . . . .
84
6.1.5
Counting and categorization . . . . . . . . . . . .
85
6.1.6
Bayesian networks . . . . . . . . . . . . . . . . . .
86
6.2
Mixed Distribution . . . . . . . . . . . . . . . . . . . . . .
90
6.3
Markov Chains . . . . . . . . . . . . . . . . . . . . . . . .
91
7.
Introduction to Quantum Physics
95
7.1
Unitary Evolution
. . . . . . . . . . . . . . . . . . . . . .
95
7.1.1
Schr¨odinger’s cat paradox . . . . . . . . . . . . . .
96
7.1.2
Interpretations of quantum mechanics . . . . . . .
96
7.2
Quantum Mechanics . . . . . . . . . . . . . . . . . . . . .
97
7.2.1
Stochastic Markov evolution and unitary evolution
98
7.3
Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . .
99
7.3.1
Spectral representation∗
. . . . . . . . . . . . . .
101
7.4
Quantum Time Evolution . . . . . . . . . . . . . . . . . .
103
7.5
Compound Systems
. . . . . . . . . . . . . . . . . . . . .
105
7.6
Von Neumann Entropy . . . . . . . . . . . . . . . . . . . .
108
7.7
Measurement . . . . . . . . . . . . . . . . . . . . . . . . .
109
7.7.1
Observables
. . . . . . . . . . . . . . . . . . . . .
110
7.7.2
Measuring a compound system . . . . . . . . . . .
111
7.7.3
Heisenberg’s uncertainty principle∗
. . . . . . . .
112
7.8
Randomness . . . . . . . . . . . . . . . . . . . . . . . . . .
114
7.8.1
Deterministic chaos . . . . . . . . . . . . . . . . .
114
7.8.2
Kolmogorov complexity . . . . . . . . . . . . . . .
114
7.8.3
Humans and random numbers . . . . . . . . . . .
116
7.8.4
Randomness in quantum physics . . . . . . . . . .
116
8.
Computation with Qubits
119

xii
Principles of Quantum Artiﬁcial Intelligence
8.1
Computation with one Qubit . . . . . . . . . . . . . . . .
119
8.2
Computation with m Qubit . . . . . . . . . . . . . . . . .
121
8.3
Matrix Representation of Serial and Parallel Operations .
123
8.4
Entanglement . . . . . . . . . . . . . . . . . . . . . . . . .
125
8.5
Quantum Boolean Circuits
. . . . . . . . . . . . . . . . .
127
8.6
Deutsch Algorithm . . . . . . . . . . . . . . . . . . . . . .
130
8.7
Deutsch Jozsa Algorithm
. . . . . . . . . . . . . . . . . .
132
8.8
Amplitude Distribution
. . . . . . . . . . . . . . . . . . .
135
8.8.1
Cloning . . . . . . . . . . . . . . . . . . . . . . . .
136
8.8.2
Teleportation . . . . . . . . . . . . . . . . . . . . .
137
8.9
Geometric Operations . . . . . . . . . . . . . . . . . . . .
139
9.
Periodicity
145
9.1
Fourier Transform
. . . . . . . . . . . . . . . . . . . . . .
145
9.2
Discrete Fourier Transform
. . . . . . . . . . . . . . . . .
147
9.2.1
Example . . . . . . . . . . . . . . . . . . . . . . .
149
9.3
Quantum Fourier Transform . . . . . . . . . . . . . . . . .
150
9.4
FFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
9.5
QFT Decomposition . . . . . . . . . . . . . . . . . . . . .
154
9.5.1
QFT quantum circuit∗
. . . . . . . . . . . . . . .
155
9.6
QFT Properties . . . . . . . . . . . . . . . . . . . . . . . .
159
9.7
The QFT Period Algorithm . . . . . . . . . . . . . . . . .
161
9.8
Factorization . . . . . . . . . . . . . . . . . . . . . . . . .
164
9.8.1
Example . . . . . . . . . . . . . . . . . . . . . . .
164
9.9
Kitaev’s Phase Estimation Algorithm∗. . . . . . . . . . .
168
9.9.1
Order ﬁnding . . . . . . . . . . . . . . . . . . . . .
170
9.10
Unitary Transforms
. . . . . . . . . . . . . . . . . . . . .
171
10.
Search
173
10.1
Search and Quantum Oracle . . . . . . . . . . . . . . . . .
173
10.2
Lower Bound Ω(√n) for Uf-based Search∗. . . . . . . . .
175
10.2.1
Lower bound of at . . . . . . . . . . . . . . . . . .
176
10.2.2
Upper bound of at . . . . . . . . . . . . . . . . . .
178
10.2.3
Ω(√n) . . . . . . . . . . . . . . . . . . . . . . . .
179
10.3
Grover’s Ampliﬁcation . . . . . . . . . . . . . . . . . . . .
180
10.3.1
Householder reﬂection . . . . . . . . . . . . . . . .
180
10.3.2
Householder reﬂection and the mean value . . . .
181
10.3.3
Ampliﬁcation
. . . . . . . . . . . . . . . . . . . .
182

Contents
xiii
10.3.4
Iterative ampliﬁcation . . . . . . . . . . . . . . . .
184
10.3.5
Number of iterations
. . . . . . . . . . . . . . . .
191
10.3.6
Quantum counting
. . . . . . . . . . . . . . . . .
192
10.4
Circuit Representation . . . . . . . . . . . . . . . . . . . .
194
10.5
Speeding up the Traveling Salesman Problem . . . . . . .
195
10.6
The Generate-and-Test Method . . . . . . . . . . . . . . .
196
11.
Quantum Problem-Solving
199
11.1
Symbols and Quantum Reality . . . . . . . . . . . . . . .
199
11.2
Uninformed Tree Search . . . . . . . . . . . . . . . . . . .
200
11.3
Heuristic Search
. . . . . . . . . . . . . . . . . . . . . . .
203
11.3.1
Heuristic functions
. . . . . . . . . . . . . . . . .
205
11.3.2
Invention of heuristic functions . . . . . . . . . . .
205
11.3.3
Quality of heuristic . . . . . . . . . . . . . . . . .
207
11.4
Quantum Tree Search
. . . . . . . . . . . . . . . . . . . .
208
11.4.1
Principles of quantum tree search . . . . . . . . .
208
11.4.2
Iterative quantum tree search
. . . . . . . . . . .
210
11.4.3
No constant branching factor . . . . . . . . . . . .
211
11.5
Quantum Production System . . . . . . . . . . . . . . . .
212
11.6
Tarrataca’s Quantum Production System
. . . . . . . . .
213
11.6.1
3-puzzle . . . . . . . . . . . . . . . . . . . . . . . .
213
11.6.2
Extending for any n-puzzle . . . . . . . . . . . . .
217
11.6.3
Pure production system . . . . . . . . . . . . . . .
218
11.6.4
Unitary control strategy
. . . . . . . . . . . . . .
219
11.7
A General Model of a Quantum Computer . . . . . . . . .
220
11.7.1
Cognitive architecture . . . . . . . . . . . . . . . .
220
11.7.2
Representation . . . . . . . . . . . . . . . . . . . .
221
12.
Quantum Cognition
223
12.1
Quantum Probability . . . . . . . . . . . . . . . . . . . . .
223
12.2
Decision Making
. . . . . . . . . . . . . . . . . . . . . . .
226
12.2.1
Interference
. . . . . . . . . . . . . . . . . . . . .
231
12.3
Unpacking Eﬀects
. . . . . . . . . . . . . . . . . . . . . .
232
12.4
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . .
233
13.
Related Approaches
235
13.1
Quantum Walk . . . . . . . . . . . . . . . . . . . . . . . .
235
13.1.1
Random walk
. . . . . . . . . . . . . . . . . . . .
235

xiv
Principles of Quantum Artiﬁcial Intelligence
13.1.2
Quantum insect . . . . . . . . . . . . . . . . . . .
235
13.1.3
Quantum walk on a graph
. . . . . . . . . . . . .
237
13.1.4
Quantum walk on one dimensional lattice . . . . .
238
13.1.5
Quantum walk and search
. . . . . . . . . . . . .
239
13.1.6
Quantum walk for formula evaluation . . . . . . .
239
13.2
Adiabatic Computation
. . . . . . . . . . . . . . . . . . .
240
13.2.1
Quantum annealing . . . . . . . . . . . . . . . . .
241
13.3
Quantum Neural Computation
. . . . . . . . . . . . . . .
243
13.4
Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
Bibliography
247
Index
259

Chapter 1
Introduction
Symbolical artiﬁcial intelligence is a ﬁeld of computer science that is highly
related to quantum computation. At ﬁrst glance, this statement appears
to be a contradiction. However, the artiﬁcial intelligence framework, such
as search and production system theory, allows an elegant description of a
quantum computer model that is capable of quickly executing programs.
1.1
Artiﬁcial Intelligence
Artiﬁcial intelligence (AI) is a subﬁeld of computer science that models the
mechanisms of intelligent human behavior (intelligence).
This approach
is accomplished via simulation with the help of artiﬁcial artifacts, typi-
cally with computer programs on a machine that performs calculations. It
should be noted that the machine does not need to be electronic. Indeed,
Charles Babbage (1791-1871) sketched the ﬁrst mechanical machine (a dif-
ference engine) for the calculation of certain values of polynomial functions
[Hyman (1985)]. With the goal of mechanizing calculation steps, Babbage
sketched the ﬁrst model of a mechanical universal computer and called it
an analytical engine. At the same time, Lady Ada Lovelance (1815-1852)
thought about the computing power of such a machine. She argued that
such a machine could only perform what it was told to do; such a machine
could not generate new knowledge.
The term “artiﬁcial intelligence” itself was invented by the American
computer scientist John McCarthy. It was used in the title of a confer-
ence that took place in the year 1956 at Dartmonth College in the USA.
During this meeting, programs were presented that played chess and check-
ers, proved theorems and interpreted texts. The programs were thought
to simulate human intelligent behavior. However, the terms “intelligence”
1

2
Principles of Quantum Artiﬁcial Intelligence
and “intelligent human behavior” are not very well deﬁned and understood.
The deﬁnition of artiﬁcial intelligence leads to the paradox of a discipline
whose principal purpose is its own deﬁnition.
A.M. Turing (1912-1954), in 1950, wrote the essay “Computing Machin-
ery and Intelligence”, in which he poses the question of how to determine
whether a program is intelligent or not [Turing (1950)]. He deﬁnes intel-
ligence as the reaction of an intelligent being to certain questions. This
behavior can be tested by the so-called Turing test. A subject communi-
cates over a computer terminal with two non-visible partners, a program
and a human. If the subject cannot diﬀerentiate between the human and
the program, the program is called intelligent. The questions posed can
originate from any domain. However, if the domain is restricted, then the
test is called a restricted Turing test. A restricted domain could be, for
example, a medical diagnosis or the game of chess.
Human problem-solving algorithms are studied in Artiﬁcial Intelligence.
The key idea behind these algorithms is the symbolic representation of the
domain in which the problems are solved. Symbols are used to denote or
refer to something other than themselves, namely other things in the world
(according to the, pioneering work of Tarski [Tarski (1944, 1956, 1995)]).
They are deﬁned by their occurrence in a structure and by a formal lan-
guage which manipulates these structures [Simon (1991); Newell (1990)]
(see Figure 1.1). In this context, symbols do not, by themselves, represent
any utilizable knowledge. For example, they cannot be used for a deﬁnition
of similarity criteria between themselves. The use of symbols in algorithms
which imitate human intelligent behavior led to the famous physical symbol
system hypothesis by Newell and Simon (1976) [Newell and Simon (1976)]:
“The necessary and suﬃcient condition for a physical system to exhibit in-
telligence is that it be a physical symbol system.” Symbols are not present
in the world; they are the constructs of a human mind and simplify the
process of representation used in communication and problem solving.
1.2
Motivation and Goals
Traditional AI is built around abstract algorithms and data structures that
manipulate symbols. One of the important algorithms is the tree or graph
search. Common forms of knowledge representation are symbolic rules and
semantic nets. Traditional AI attempts to imitate human behavior without

Introduction
3
Objects
Relations
On-relation
Predicates
Object symbols
On(B,A)
B
A
Imaginable world
Logic
Fig. 1.1
Object represented by symbols and relation represented by predicate.
any relationship to physical reality, for example, in biological hardware.
Sub-Symbolical processing, on the other hand, belongs to biology-inspired
AI, which involves methods such as neural networks or behavioral systems.
Could the physical nature, as described by quantum physics, also lead to
algorithms that imitate human behavior? What are the possibilities for
the realization of artiﬁcial intelligence by means of quantum computation?
Computational algorithms that are inspired by this physical reality are de-
scribed by quantum computation. We will answer questions such as why
and how to use quantum algorithms in artiﬁcial intelligence.
Questions that appear to be quite simple, such as: what are random num-
bers and how can we generate them, cannot be answered by traditional
computer science. The widely used pseudo random generators are based
on deterministic procedures and do not generate randomness; instead, they
generate pseudo-randomness. Pseudo random generators are related to de-
terministic chaos sequences, which are described by mathematical chaos
theory. Chaotic patterns can arise from very simple mathematics. While
the results can be similar to true randomness, the output patterns are
generated by deterministic rules.
Chaotic patterns diﬀer from most de-
terministic systems because any small change made to their variables can
result in unpredictable changes to the system behavior.

4
Principles of Quantum Artiﬁcial Intelligence
Recently, quantum algorithms for AI were proposed, including a quan-
tum tree search algorithm and a quantum production system [Tarrataca
and Wichert (2011b,a, 2012b, 2013b)]. In this book, we introduce quantum
computation and its application to AI. Based on information science, we
will illustrate the general principles that govern information processing and
information structures.
1.3
Guide to the Reader
This book is about some core ideas of artiﬁcial intelligence and quantum
computation and is composed of two sections: the ﬁrst is on classical com-
putation and the second section is on quantum computation. In the ﬁrst
section, we introduce the basic principles of computation, representation
and problem solving.
Quantum physics is based on information theory
and probability theory. We present both theories and indicate their re-
lationships to artiﬁcial intelligence by associative memory and Bayesian
networks. In the second section, we introduce the principles of quantum
computation and its mathematical framework. We present two principles
on which quantum computation is based, the discrete Fourier transform
and Grover’s algorithm. Based on these principles, we introduce the it-
erative quantum tree search algorithm that speeds up the search. In the
next step we introduce a quantum production system on which a universal
quantum computer model is based. Finally, related topics such as quantum
cognition and quantum random walk are presented. Readers who want to
develop a general understanding of the quantum computation mathemati-
cal framework should read the second section, beginning with the chapter
“Introduction to Quantum Physics”. Sections that go more into detail are
marked by a star “∗” and can be skipped on the ﬁrst reading.
1.4
Content
1.4.1
Classical computation
Computation - Chapter 2
The Entscheidungsproblem is presented,
and the Turing machine is introduced. The proof of the Entscheidungsprob-
lem is based on Cantor’s diagonal argument and G¨odelization. The Uni-
versal Turing machine is an abstract model of a computer. Computational
complexity theory addresses questions regarding which problems can be

Introduction
5
solved in a ﬁnite amount of time on a computer. The Church–Turing thesis
states that any algorithmic process can be simulated on a Turing machine.
Two classes of practical computers are presented: analog and digital com-
puters.
Problem Solving - Chapter 3
In the ﬁrst step, the knowledge rep-
resentation framework is introduced: rules, logic-based operators, frames
and categorical representations. In the next step, production systems are
introduced. A production system is a model of human problem solving.
It is composed of long-term memory and working memory, which is also
called short-term memory. We distinguish between deduction systems and
reaction systems. Planning can be performed more easily by reaction sys-
tems in which the premise speciﬁes the conditions that must be satisﬁed;
in this way, the condition that speciﬁes an action can be undertaken. An
8-puzzle example is presented. There is an assumption that the distance
between states in the problem space is related to the distance between the
sub-symbols that represent the states in sub-symbolical problem-solving.
Sub-symbolical problem-solving takes advantage of the geometric nature of
the world.
Information - Chapter 4
Information theory is highly related to math-
ematical probability theory and thermodynamics. Entropy is a measure of
the disorder of the conﬁguration of states and can be described by a dice
model. The Maxwell paradox identiﬁes information with a negative mea-
sure of entropy. The ideal entropy represents the minimal number of opti-
mal questions that must be addressed to know the result of an experiment.
We will indicate the relationships between information and hierarchical
structures and measurement. In the section on information and memory,
we will introduce a biologically inspired model of associative memory. The
Information and storage capacity of the model is high, given that the binary
representation is sparse. Finally, a deduction system based on associative
memory is presented.
Reversible Algorithms - Chapter 5
Bennett (1973) showed that ir-
reversible overwriting of one bit causes at least k · T · log2 joules of energy
dissipation, where k is Boltzmann’s constant and T is the absolute temper-
ature. Bennett also indicated that this lower bound can be ignored when
using reversible computation. Reversible computing is a model of comput-
ing in which the computational process is reversible. Reversible Boolean
gates and circuits are described.

6
Principles of Quantum Artiﬁcial Intelligence
Probability - Chapter 6
Probability theory is built around Kol-
mogorov’s axioms. For a joint distribution of n possible variables, the expo-
nential growth of combinations being true or false becomes an intractable
problem for large n. There are two possible solutions to this problem, Na¨ıve
Bayes and Bayesian networks. Na¨ıve Bayes is related to counting and cate-
gorization. For the unobservable variables, the Bayesian networks are based
on the law of total probability. A Markov chain is a mathematical system
that undergoes transitions described by a stochastic matrix. A stochastic
matrix evolution that occurs when describing the evolution of a physical
system is usually non-reversible.
1.4.2
Quantum computation
Introduction to Quantum Physics - Chapter 7
The unitary deter-
ministic evolution represented by the Schr¨odinger equation and two inter-
pretations of quantum mechanics are presented. We indicate the diﬀerence
between stochastic Markov evolution and unitary evolution. The mathe-
matical framework of quantum theory is based on linear algebra in Hilbert
space.
The relationships between the unitary operators represented by
unitary matrices and the Schr¨odinger equation and the Hamiltonian are
described by a spectral representation. A 2-state quantum system is de-
scribed by a two-dimensional Hilbert space. Such a 2-state quantum system
corresponds to a qubit. A register is composed of several qubits and is de-
ﬁned by the tensor product. The von Neumann entropy of a superposition
of qubits measures the distribution of the probabilities. It describes the
departure of the state from a pure state. For a pure state, there is no un-
certainty during the measurement. The higher the entropy is, the higher
the uncertainty during the measurement.
Computation with Qubits - Chapter 8
A unitary operator on a
qubit is called a unary quantum gate. An operation on several qubits can
be represented by unitary matrices. The matrix representation of serial and
parallel operations is composed of either a product or a tensor operation
between the matrices. A reversible circuit of m bits corresponds to a unitary
mapping. A reversible circuit can be represented by a unitary permutation
matrix or by quantum Boolean gates.
The Deutsch algorithm exploits
the superposition of qubits that are generated by Hadamard gates; the
Deutsch algorithm is more powerful than any other classical algorithm and
determines whether an unknown function of one bit is constant or not by

Introduction
7
calling the function one time. A classical algorithm requires two calls. The
Deutsch Jozsa algorithm generalizes the working principle even to a more
powerful algorithm for a function of m bits.
Periodicity - Chapter 9
The Fourier transform changes a signal from
the time domain to the frequency domain. The discrete Fourier transform
changes discrete time-based or space-based data into frequency-based data.
The discrete Fourier transform (DFT) can be seen as a linear transform
represented by a unitary matrix F.
This unitary matrix F also deﬁnes
the quantum Fourier transform (QFT). The decomposition of the matrix
is described by the fast Fourier transform (FFT). The QFT decomposition
is equivalent to the FFT. The QFT period algorithm determines the pe-
riod of a periodic function in polynomial time and is the basis of Shor’s
Algorithm for the factorization of numbers in polynomial time. An alter-
native approach is described in Kitaev’s phase estimation algorithm, which
determines the eigenvalue for a unitary operator and an eigenvector.
Search - Chapter 10
We want to ﬁnd x for which f(x) = 1, x = ξ. This
task is equivalent to a decision problem with a binary answer 1 = yes and
0 = no and the instance x. There is a lower bound for a quantum search
on a quantum computer using a quantum oracle. The possible speedup is
quadratic; NP −complete problems remain NP −complete. The search
is described by Grover’s algorithm and is based on the Householder reﬂec-
tion. In the case, the number of the solutions is unknown, we can apply
a quantum counting algorithm. The generate-and-test method is a simple
AI paradigm that can directly beneﬁt from Grover’s algorithm.
Quantum Problem-Solving - Chapter 11
Problem-solving can be
modeled by a production system that implements a search algorithm. The
search deﬁnes a problem space and can be represented as a tree. In an
uninformed search, no additional information about the states is given. A
heuristic search is based on a heuristic function h(ν) that estimates the
cheapest cost from the node ν to the goal. However, inventing heuristic
functions is diﬃcult. An alternative approach is that of the quantum tree
search algorithm. Using Grover’s algorithm, we search through all possi-
ble paths and verify, for each path, whether it leads to the goal state. We
present the iterative quantum tree search, which is the basis of the quantum
production system. We explain the principles of Tarrataca’s quantum pro-
duction system on a trivial example, the 3-puzzle. Finally, we present a uni-
versal quantum computer model that is capable of more quickly executing

8
Principles of Quantum Artiﬁcial Intelligence
programs. The corresponding principles can be integrated into the uniﬁed
theories of human cognition.
Quantum cognition - Chapter 12
Quantum cognition uses mathemat-
ical quantum theory to model cognitive phenomena. It is assumed that the
computation itself is performed on a classical computer and not on a quan-
tum computer. The brain is considered a classical computer in a quantum
world. The quantum probabilities, when observed, correspond to classical
probability theory. If not observed, the state of a system is described by
a complex vector with a length of one. Two equivalent states represent
the same state when a measurement is performed, but they can behave
diﬀerently during the unitary evolution. Humans, when making decisions,
violate the law of total probability. The violation can be explained as a
quantum interference.
Related approaches - Chapter 13
Quantum random walks correspond
to Grover’s algorithm and the quantum tree search algorithm. A quantum
random walk corresponds to the random walk. We will introduce a quantum
insect and demonstrate the principles of quantum walk by a discrete walk
on a line. Adiabatic quantum computation is an alternative approach to
quantum computation and is based on the evolution time of a quantum
system. The energy of a system can be described by a function. In quantum
annealing, the quantum ﬂuctuation parameter replaces a local minimum
state by a randomly selected neighboring state in a ﬁxed radius. Quantum
annealing can speed up some machine learning tasks that are based on the
gradient descent method.

Chapter 2
Computation
2.1
Entscheidungsproblem
David Hilbert, one of the most famous German mathematicians, attended
a banquet in 1934, and he was seated next to the new minister of edu-
cation, Bernhard Rust [Reid (1996)]. Rust asked, “How is mathematics
in G¨ottingen now that it has been freed of the Jewish inﬂuence?” Hilbert
replied, “Mathematics in G¨ottingen? There is really none any more.” David
Hilbert died in 1943. On his tombstone, at G¨ottingen, one can read his epi-
taph:
• Wir m¨ussen wissen (We have to know)
• Wir werden wissen (We shall know!)
At the International Congress of Mathematicians in Paris in 1900, he put
forth an inﬂuential list of 23 unsolved problems in mathematics. Hilbert’s
23rd problem became known as the Entscheidungsproblem.
Hilbert’s
Entscheidungsproblem is formulated as the following question:
• Is there a general algorithm to determine whether a mathematical
conjecture is true or false?
It was commonly believed that there was no such thing as an unsolvable
problem. However, Alonzo Church and Alan Turing discovered indepen-
dently, around 1936, that a general solution to the Entscheidungsproblem
is impossible. They showed that it is impossible to decide algorithmically
whether statements in arithmetic are true or false. This result is now known
as Church’s Theorem or the Theorem Church-Turing Theorem.
Alonzo Church created the method for deﬁning recursive functions λ-
calculus [Church (1936a)], [Church (1936b)], [Church (1941)] and Alan Tur-
9

10
Principles of Quantum Artiﬁcial Intelligence
ing created a simple model called the Turing machine [Turing (1936)] (see
Figure 2.1). The Turing machine constitutes an inﬁnitely long tape that
is divided into a sequence of cells. In each cell, a certain symbol can be
written and later read by a head. The head can move along the tape and
exist in one state of a ﬁnite set of internal states. A set of rules speciﬁes a
new state given the current state and the symbol being read. The new state
determines in which direction the head must move and if it must write or
read a symbol. For each state, only one rule describes one action. Turing
Fig. 2.1
The Turing machine constitutes an inﬁnitely long tape that is divided into a
sequence of cells. In each cell, a certain symbol can be written and later read by a head.
The head can move along the tape and exist in one state of a ﬁnite set of internal states.
A set of rules speciﬁes a new state given the current state and the symbol being read.
The new state determines in which direction the head must move and if it must write or
read a symbol. For each state, only one rule describes one action.
realized that one could encode the transformation rules of any speciﬁc Tur-
ing machine T as some pattern of symbols on the tape that fed into a special
Turing machine U. U had the eﬀect of reading in the pattern, specifying
the transformation rules for T and then simulating T. Any algorithmic
process can be simulated on a Turing machine U. Entscheidungsproblem
corresponds to the halting problem. Whether a program will halt or run
forever can be determined and is based on the given program with a ﬁnite

Computation
11
input. We will sketch the proof, assuming a common computer model and
not the Turing machine, to make the explanation easier.
The proof uses a technique called reduction ad absurdum, in which we
assume the truth of the opposite of what we want to prove and then derive
a logical contradiction. The proof is based on Cantor’s diagonal argument
and a form of coding in which a function or a program can be represented
by a number. This form of coding is called G¨odelization. It allows self-
reference, which means that the code of the program that is represented
by a number can form an input to the program. The program can make
statements about itself.
2.1.1
Cantor’s diagonal argument
Cantor’s diagonal argument indicates that there is no bijection between in-
ﬁnite sets and natural numbers [Lewis and Papadimitriou (1981)]. Natural
numbers are countably inﬁnite, and inﬁnite sets are uncountably inﬁnite.
Suppose that inﬁnite sets are represented by binary inﬁnite strings, and we
list all of the sets in a table. This list is inﬁnitely long, and we can write
each entry as an inﬁnitely long binary string. It is possible to build a new
inﬁnite binary string in such a way that its ﬁrst element is diﬀerent from
the ﬁrst element of the ﬁrst inﬁnite binary string in the list, its second ele-
ment is diﬀerent from the second element of the second inﬁnite binary sting
in the list, and so on. In general, its nth element is diﬀerent from the nth
element of the nth inﬁnite binary string in the list. The new inﬁnite binary
string corresponds to the diagonal elements of the list in which its element
is zero if the diagonal element is one and is zero if the diagonal element is
one. The procedure can be repeated many times; the inﬁnite list of inﬁnite
sets is never complete, and the set of all inﬁnite sets is uncountable. It
should be noted that to deny the Cantor’s diagonal argument implicates
the rejection of inﬁnity. Because the size of the power set is 2n, it follows
from Cantor’s diagonal argument
lim
n→∞n < lim
n→∞2n.
2.1.2
Reductio ad absurdum
Using G¨odelization, we can represent a program with binary numbers. A
binary number can be easily mapped to a binary string. A program can

12
Principles of Quantum Artiﬁcial Intelligence
be inﬁnite because the Turing machine has an inﬁnite tape [Lewis and
Papadimitriou (1981)]. As previously observed, we cannot list all of the
inﬁnite programs. Suppose that the program is ﬁnite. The program and
the ﬁnite input are encoded as a pair of positive binary numbers. Is there
a program that solves the halting problem? Let us suppose that such a
program exists and that it is using a self-referent function halt(x). The
binary number x is its representation using the G¨odelization principle. The
function halt(x) returns a one if the corresponding program represented by
the binary number x with the input x halts and otherwise returns a zero.
halt(x) =
 1 program x halts on input x
0 otherwise
Using the modiﬁed diagonal argument we deﬁne the program with the name
“Diagonal”
Diagonal(x)
{
if halt(x)=0 then halt;
else loop forever;
}
The program “Diagonal” is represented by the binary number u. Does the
program Diagonal(u) halt?
If yes, then we have a contradiction to the
deﬁnition of halt(x). If it does not halt, then there is also a contradiction
to the deﬁnition of halt(x). From the contradiction, it would follow that
there is no program that solves the halting problem.
2.2
Complexity Theory
The elegant way of modeling a computer by a Turing machine leads us to
computational complexity theory. Computational complexity theory ad-
dresses the questions of which problems can be solved in a ﬁnite amount
of time on a computer. Time is the most important resource during com-
putation besides space and energy. Space and energy are negligible when
using the Turing machine because the Turing machine itself is composed
of inﬁnitely long tape and does not require any energy resources [Lewis
and Papadimitriou (1981)]. To simplify the analysis of the correspondence
to time, special computational problems are investigated, namely decision
problems.

Computation
13
2.2.1
Decision problems
A decision problem is a computational problem with instances formulated
as a question with a binary “yes” or “no” answer.
An example is the
question of whether a certain number n is a prime number. Most problems
can be converted into a decision problem. A problem is easy if a certain
Turing machine can determine the instances related to the input for the
answer “yes” in polynomial time. Otherwise, we state that the problem
is hard. The relationship to the size of the input accounts for the reading
time of the input. It should not inﬂuence the time complexity. A number
is usually represented by the base B > 1, which means that k digits can
represent Bk diﬀerent numbers. In other words, the hierarchical organized
structure of the numbers exponentially speeds up the reading time of a
deterministic Turing machine. This relationship is not valid for a unary
representation in which the input size is equivalent to the numbers that
can be represented.
2.2.2
P and NP
The formal deﬁnition for easy problems represented by P is as follows:
The set of all decision problems that have instances that are solvable in
polynomial time using a deterministic Turing machine. In a deterministic
Turing machine, all of the transitions are described by some ﬁxed rules
[Lewis and Papadimitriou (1981)].
An example for an easy problem is
multiplication x × b = c, where x is the instance and the values b and c
are given; determine x so that the answer to the question is x × 7 = 28
is yes. On the other hand, it is thought that the factoring determines the
integers for which the product is equal to for a given number d; for example,
integers a, b with a× b = d is not in P, which indicates that it is hard. The
class NP is the set of all decision problems that have instances that are
solvable in polynomial time using a non-deterministic Turing machine. In
a non-deterministic Turing machine, in contrast to a deterministic Turing
machine, for each state, several rules with diﬀerent actions can be applied.
Non-deterministic Turing machine
branches into many copies that
are represented by a computational tree in which there are diﬀerent com-
putational paths. The class NP corresponds to a non-deterministic Turing
machine that guesses the computational path that represents the solution.
By doing so, it guesses the instances of the decision problem. In the second
step, a deterministic Turing machine veriﬁes whether the guessed instance

14
Principles of Quantum Artiﬁcial Intelligence
leads to a “yes” answer. It is easy to verify whether a solution is valid or
not. This statement does not mean that ﬁnding a solution is easy. Clearly,
the class P ⊆NP is known, and it follows that NP ̸= P or NP = P;
however, other relationships are not known. The class NP −complete is
present if the problem is in NP and every other problem in NP can be
reduced to the class NP −complete.
It was not obvious that an NP −complete problem exists. Cook-Levin
described the ﬁrst example of an NP −complete problem, the satisﬁability
problem. Until recently, thousands of other problems are known to be NP −
complete, including the well-known traveling salesman and Hamiltonian
cycle problem [Cormen et al. (2001)].
The structure of NP −complete problems
is equivalent to a com-
putational tree of a non-deterministic Turing machine in which all diﬀerent
computational paths must be searched by a deterministic Turing machine.
A simple algorithm for solving NP −complete problems by a deterministic
Turing machine is to perform an iterative search for all possible instances.
• The formal deﬁnition for NP is as follows: a deterministic Tur-
ing machine veriﬁes whether an instance (ticket) leads to a “yes”
answer in polynomial time.
• The formal deﬁnition of an NP −complete problem is as follows:
The problem is NP and the problem is NP −hard. NP −hard
means that every other problem in NP can be reduced to it in
polynomial time.
2.3
Church–Turing Thesis
The deﬁnition of P and NP should not depend upon the currently used
computational model. The following is stated in the Church–Turing the-
sis: Any algorithmic process can be simulated on a Turing machine. The
extended Church–Turing thesis, which is also called the strong Church–
Turing thesis, states that everything that can be computed in a certain
amount of time on any physical computer can be also be computed on a
Turing machine with a polynomial slowdown. In other words, any reason-
able algorithmic process can be simulated on a Turing machine, with the
possibility of a polynomial slowdown, in the number of steps required to
run the simulation.
The problems in P are precisely those for which a
polynomial-time solution is the best possible, in any physically reasonable
model of computation.

Computation
15
The hypothesis that the universe is equivalent to a Turing machine,
which is related to the Church–Turing thesis, is similar to that stated in dig-
ital physics. However, Richard Feynman observed in the early eighties that
it did not appear possible for a Turing machine to simulate certain quantum
physical processes without incurring an exponential slowdown. This fact
would contradict the strong Church–Turing thesis, which led Feynman to
ask whether a quantum system can be simulated on an imaginary quantum
computer.
2.3.1
Church–Turing–Deutsch principle
In 1985, David Deutsch [Deutsch (1985)] reformulated the Church–Turing
thesis based on the observation of Richard Feynman in physical terms:
“Every ﬁnitely realizable physical system can be perfectly simulated by
universal computing machine operating by ﬁnite means.” The Turing ma-
chine was replaced by the universal computing machine which operates by
ﬁnite means.
2.4
Computers
The Turing machine is a theoretical mathematical model, not a practical
engineering model of a computer. There are two distinct classes of practical
computers, analog and digital computers.
2.4.1
Analog computers
An analog computer represents information by analog means, such as volt-
age.
In such a computer, information is represented by a voltage wave
and the algorithm is represented by an electrical circuit. Such a circuit is
composed of resistors and capacitors that are connected together. An al-
gorithm represents a mathematical model of a physical system, which can
be described, for example, by speciﬁc diﬀerential equations. In the ﬁrst
step, the mathematical model is determined. Then, a block diagram that
models the analogous system is developed. This model deﬁnes the elec-
trical components that specify the computation. The input and output of
the computation are voltage waves that can be observed by an oscilloscope.
The represented values are usually less accurate than digitally represented
values. The results of each computation can vary due to external inﬂuences.

16
Principles of Quantum Artiﬁcial Intelligence
For this reason, each result of the computation is unique. The exact value
cannot be reproduced without an error. This type of noise, which results
from an external inﬂuence, makes it impossible to recompute the output
of chaotic deterministic functions. Even making the smallest change to the
initial condition can cause the results to greatly diverge. It is important to
note that analog computers are not covered by the Church–Turing thesis
because they cannot be simulated by a Turing machine; however, analog
computers are covered by the Church–Turing–Deutsch principle because
they correspond to a computing machine that operates by ﬁnite means.
Analog computers were popular in the 1950s. However, analog computers
fell into decline with the advent of the development of the microprocessor,
which led to the development of digital computers.
2.4.2
Digital computers
A digital computer is a device that processes information that is represented
in discrete means such as symbols. Usually, the symbols are represented
in binary form. Modern digital computers are based on digital circuits. In
a digital circuit, the information is represented by binary digits. Due to a
digital representation, the exact values of each computation can be repro-
duced without any error. The computation can be repeated, and the result
remains the same (this scenario is not the case with analog computers).
Binary digits are represented by the minimal unit of information, the bit.
The binary information is manipulated by Boolean digital circuits. Emil
Post has proven the complete sets of truth functions. It follows that they
can be computed using Boolean circuits, which are composed of Boolean
gates and represent Boolean logic, which operates on bits [Cormen et al.
(2001)]. An algorithm can be described by a circuit. The construction of a
circuit requires the exact knowledge of the values that must be computed.
For this reason, a circuit is not an algorithmic device; by itself, it does not
correspond to a universal Turing machine. The Halting problem cannot by
represented by a circuit.
2.4.3
Von Neumann architecture
The ENIAC (Electronic Numerical Integrator And Computer) was one of
the ﬁrst Turing-complete universal digital computers that was capable of
being programmed.
John von Neumann learned in 1945 of the ENIAC
Project and described this model in a technical report called “First Draft

Computation
17
of a Report on the EDVAC” [von Neumann (1945)]. The model become
known as the Van Neumann architecture [Aspray (1990)]. This technology
is composed of ﬁve main concepts:
• An arithmetic logic unit (ALU) unit that is capable of performing
both arithmetic and logic operations on the data.
• A control unit (CU) that interprets an instruction retrieved from
the memory and that selects alternative courses of action based on
the results of the previous operations.
• Main memory stores both data and instructions and read-write
random-access memory (RAM).
• Secondary memory represents the external mass storage.
• Input and output mechanisms.
Fig. 2.2
The Van Neumann architecture. In modern computers, the ALU and the CU
are parts of the central processing unit (CPU) that are represented by a single silicon
chip called a microprocessor. The subsystem called a bus transfers the data between the
random-access memory, the CPU, and the input and output.
In modern computers, the ALU and the CU are parts of the central pro-
cessing unit (CPU) that are represented by a single silicon chip called a

18
Principles of Quantum Artiﬁcial Intelligence
microprocessor. The subsystem called a bus transfers the data between the
random-access memory, the CPU, and the input and output (see Figure
2.2). Most modern computers are based on the von Neumann architecture.

Chapter 3
Problem Solving
It is quite certain for the cognitive psychologist that the human computa-
tional model is not at all based on the Van Neumann architecture. Instead,
they propose that the human computational model is based on production
systems.
A production system is a mathematical as well as a practical
model that can be realized as a computing machine. Production systems
are closely related to the approach taken by Markov algorithms [Markov
(1954)], and similar to these approaches, production systems are equiva-
lent in power to a Turing machine [Turing (1936)]. A Turing machine can
also be easily simulated by a production system. The production system
is a model of actual human problem-solving behavior [Newell and Simon
(1972); Anderson (1983); Klahr and Waterman (1986); Newell (1990)].
3.1
Knowledge Representation
Production systems are composed of if-then rules that are also called pro-
ductions. A rule contains several “if” patterns and one or more “then”
patterns.
A pattern in the context of rules is an individual predicate,
which can be negated together with arguments.
A rule can establish a
new assertion by the “then” part (its conclusion) whenever the “if” part
(its premise) is true. Productions can be represented by rules or logic-based
operators. Alternative sub-symbolical representation is based on categorial
representation. Binary vector representation is the basis of the quantum
production systems.
3.1.1
Rules
A rule [Winston (1992); Russell and Norvig (1995); Luger and Stubbleﬁeld
(1998)] contains several “if” patterns and one or more “then” patterns.
19

20
Principles of Quantum Artiﬁcial Intelligence
A pattern in the context of rules is an individual predicate which can be
negated together with arguments. The rule can establish a new assertion
by the “then” part, the conclusion whenever the “if” part, the premise,
is true. When variables become identiﬁed with values they are bound to
these values. Whenever the variables in a pattern are replaced by values,
the pattern is said to be instantiationed. Here is an example of rules with
a variable x:
• If (ﬂies(x) ∨feathes(x)) ∧lays eggs(x)
|
{z
}
premise
then bird(x)
| {z }
conclusion
• If bird(x) ∧swims(x) then penguin(x)
• If bird(x) ∧sings(x) then nightinagle(x)
The following assertions are present:
• feathers(Pit)
• lays eggs(Pit)
• swims(Pit)
• ﬂies(Airbus)
Pit is a bird because the premise of the ﬁrst rule is true when x is bound
to Pit. Because bird(Pit), the premise of the second rule is true and Pit is
a penguin.
3.1.2
Logic-based operators
Logical representation is motivated by philosophy and mathematics
[Kurzweil (1990); Tarski (1995); Luger and Stubbleﬁeld (1998)].
Predi-
cates are functions that map objects’ arguments into true or false values.
They describe the relation between objects in a world which is represented
by symbols. Whenever a relation holds with respect to some objects, the
corresponding predicate is true when applied to the corresponding object
symbols.
Predicates can be negated by the function ¬ (not) and combined by the
logical connectives ∨(disjunction), ∧( conjunction) and the implies (→)
operator. ¬, ∨, ∧, and →determine the predicate’s value. To signal that
an expression is universally true, the universal quantiﬁer and a variable
standing for possible objects is used.

Problem Solving
21
∀x[Feathers(x) →Bird(x)].
An Object having feathers is a bird.
Some expressions are true only for some objects. This is represented by an
existential quantiﬁer and a variable.
∃x[Bird(x)].
There is at least one object which is a bird.
An interpretation is an accounting of the correspondence between ob-
jects and object symbols and between relations and predicates. An inter-
pretation can be only either true or false. These are some basic ideas about
representation in predicate calculus, which is a subset of formal logic. A
world state can be described including properties and relations using pred-
icate calculus. This kind of description can be used to deﬁne operators
like those used in the STRIPS computer science approach (see Figure 3.1)
[Fikes and Nilsson (1971); Nilsson (1982); Givan and Dean (1997)].
ontable(A).
ontable(C).
on(B,A).
clear(B).
clear(C).
gripping().
A
B
C
Fig. 3.1
ABC block world.

22
Principles of Quantum Artiﬁcial Intelligence
Using the block examples, four operations “pickup”, “putdown”,
“stack” and “unstack” can be deﬁned [Nilsson (1982)]: 1
pickup(x)



P : gripping() ∧clear(x) ∧ontable(x)
A : gripping(x)
D : ontable(x) ∧gripping()
putdown(x)



P : gripping(x)
A : ontable(x) ∧gripping() ∧clear(x)
D : gripping(x)
stack(x, y)



P : gripping(x) ∧clear(x)
A : on(x, y) ∧gripping() ∧clear(x)
D : clear(y) ∧gripping(x)
unstack(x, y)



P : gripping() ∧clear(x) ∧on(x, y)
A : gripping(x) ∧clear(y)
D : on(x, y) ∧gripping()
Each of the operators is represented as triples of description. The ﬁrst ele-
ment is the precondition, the world state that must be met for an operator
to be applied. It can be true or false when variables become identiﬁed with
the values, which describe the state. The second element is the additions
to the state description that are a result of applying the operator. The last
element is the items that are removed from the state description to create
a new state when the operator is applied. These operators obey the frame
axiom since they specify what is true in one state of the world and what
exactly has changed by performing some action by an operator. The prob-
lem of specifying which part of the description should change and which
should not is called the frame problem [Winston (1992)].
ontable(A).
clear(A)
ontable(C).
clear(C).
gripping(B).
1The expressions are always universally true, and therefore the universal quantiﬁer is
omitted.

Problem Solving
23
The state after the operator pickup(B) was applied to the state of Figure
3.1 (see Figure 3.2).
A
C
B
Fig. 3.2
The state after the operator pickup(B) was applied to the state of Figure 3.1.
3.1.3
Frames
Frames describe individual objects and entire classes [Minsky (1975, 1986);
Winston (1992)] and can be used to represent states of the world. They are
composed of slots which can be either attributes, which describe the classes
or object, or links to other frames. With the aid of links, a hierarchy can be
represented in which classes or objects are parts of more general classes. In
this taxonomic representation, frames inherit attributes of the more general
classes (see Figure 3.3). Frames can be viewed as generalization of semantic
nets. They are psychologically motivated and were popularized in computer
science by Marvin Minsky. One important result of the frame theory is the
object-oriented approach in programming.
3.1.4
Categorial representation
Humans divide the world into categories so that they can make sense of
it [Smith (1995)]. The categorization task consists of the determination
if an object belongs to a category [Osherson (1995); Lakeoﬀ(1987)]. Ob-
jects can be described by a set of discrete features, such as red, round and

24
Principles of Quantum Artiﬁcial Intelligence
mammal
is-a
animal
gives
milk
bird
is-a
does
lay egg
sing
does 
is-a
nightingale
dolphin
swim
is-a
does
penguin
is-a
does
swim
giraffe
long neck
is-a
has
Fig. 3.3
Taxonomic frame representation of some animals.
sweet [Tversky (1977); McClelland and Rumelhart (1985)]. The similarity
between them can be deﬁned as a function of the features they have in
common [Osherson (1995); Sun (1995); Goldstone (1999); Gilovich (1999)].
The contrast model of Tversky [Tversky (1977)] is one well known model
in cognitive psychology [Smith (1995); Opwis and Pl¨otzner (1996)] which
describes the similarity between two objects which are described by their
features. Rather than relying on prototypical features, picture categoriza-
tion often relies on detailed shape representation [Smith et al. (1978); Mur-
phy and Brownell (1985); Smith (1995)]. Objects and scenes can be rep-
resented by binary pictures which are normalized for size and orientation
[Feldman (1985)]. Similarity between the objects or scenes is measured by
the amount of shared area between the overlaid patterns [Biederman and
Ju (1988); Kurbat et al. (1994); Smith and Sloman (1994)].
Categorial
representation represents the basis of sub-symbolical representation.
3.1.5
Binary vector representation
Binary vectors can represent discrete features. A one represents a discrete
feature at the corresponding position of a binary vector, its absence is de-
noted by a zero. The feature set A, B, C, D, E, F, G, H, I, J, K is represented

Problem Solving
25
by a binary vector of dimension 11. The presence of features C and E is
represented by the binary vector [0 0 1 0 1 0 0 0 0 0 0]. Binary vectors
can represent transitions between states. The ﬁrst binary vector describes
the state which should be present before the transition (the premise). The
second binary vector describes the world state after the transition (the
conclusion).
3.2
Production System
Human problem solving can be described by a problem-behavior graph con-
structed from a protocol of the person talking aloud, mentioning consid-
ered moves and aspects of the situation. According to the resulting theory,
searching whose state includes the initial situation and the desired situation
(goal) in a problem space [Newell (1990); Anderson (1995b)]. This process
can be described by the production system theory. The production system
in the context of classical Artiﬁcial Intelligence and Cognitive Psychology
is one of the most successful computer models of human problem solving.
The production system theory describes how to form a sequence of actions,
which lead to a goal, and oﬀers a computational theory of how humans
solve problems [Anderson (1995b)]. A production system is composed of
[Brownston et al. (1985); Luger and Stubbleﬁeld (1998)] (see Figure 3.4):
• Set of rules. These rule are also called productions. The set of
rules models the human long-term memory.
• Working memory. This memory contains a description of the state
in a problem solving process. The state is described using predi-
cate calculus and is simply called a pattern. Whenever a premise is
true, the conclusions of the rules change the contents of the work-
ing memory. The working memory models the human short-term
memory.
• Recognize-act cycle. The current state of the problem-solving pro-
cess is maintained as a set of patterns in the working memory.
Working memory is initialized with the initial state description.
The patterns in working memory are matched against the premise
of the rules. The premise of the rules that match the patterns in
working memory produces a set, which is called the conﬂict set.
One of the rules of this set is chosen and the conclusion of the
rule changes the content of the working memory. This process is
denoted as ﬁring of the rule. This recognize-cycle is repeated on

26
Principles of Quantum Artiﬁcial Intelligence
the modiﬁed working memory until a desired state is reached or no
rules can be ﬁred. The recognize-act cycle models the current focus
of attention triggering one of the set of permanent skills. This, in
turn, changes the focus of attention.
Conﬂict resolution chooses a rule from the conﬂict set for ﬁring. There are
diﬀerent conﬂict resolution strategies, such as choosing a random rule from
the set, or selecting a rule by some certain function. In a pure production
system which was proposed as a formal theory of computation [Post (1943)]
the system halts if no production can ﬁre in a state.
Fig. 3.4
A production system is composed of the long term memory and the working
memory also called short term memory. This recognize-cycle is repeated on the modiﬁed
working memory until a desired state is reached or no rules can be ﬁred.
3.2.1
Deduction systems
Problems without side eﬀects of actions can be described by deduction sys-
tems which are a subgroup of production systems [Winston (1992)]. In de-
duction systems the premise speciﬁes combinations of assertions, by which
a new assertion of the conclusion is directly deduced. This new assertion is
added to the working memory. Deduction systems do not need strategies
for conﬂict resolution because every rule presumably produces reasonable
assertions and there is no harm in ﬁring all triggered rules. Deduction sys-
tems may chain together rules in a forward direction, from assertions to
conclusions, or backward from hypotheses to premises. During backward
chaining it is ensured that all features are properly focused.
Backward

Problem Solving
27
chaining is used if no features are present. If all features are given, forward
chaining is used to prevent wasting of time pursuing hypotheses, which
are not speciﬁed by the features. The chained rules describe the complete
problem space which can be represented by a graph [Quillian (1968); Shas-
tri (1988)]. Examples of deduction systems are semantic nets, diagnostic
systems and expert systems. Simple if then rules can be represented by a
knowledge base. We present a compendium of six rules concerning problems
with the oil of a car expert system:
(1) oil lamp lights during driving round a bend or oil lamp lights during
braking then cable of the oil pressure lamp is loose
(2) driving round a bend and problems with oil then oil lamp lights during
driving round a bend
(3) braking and problems with oil then oil lamp lights during braking
(4) oil lamp goes out after some time and problems with oil and during
idling then oil pressure too low
(5) problems with oil and during idling then oil level too low
(6) oil lamp lights up then problems with oil
For clarity we can replace the names of features and categories by sym-
bols, each symbol representing a name, for example B representing “oil
lamp lights during driving round a bend”:
(1) B ∨C then A
(2) D ∧F then B
(3) E ∧F then C
(4) H ∧F ∧I then G
(5) F ∧I then J
(6) K then F
The representation of the logical relationship deﬁned by these rules re-
quires an extension to the basic graph model known as AND/OR graph
[Luger and Stubbleﬁeld (1998)]. We can represent the set of rules (for ex-
ample describing an ontology) by a directed AND/OR graph. In Figure 3.5
we see the representation of the six rules. During the deduction a subgraph
is constructed. The deduction system is a simple model. It is not practical
therefore to use it for planning, as planning is mostly described by fewer
rules, but it characterizes a much bigger problem space. Dynamical rep-
resentation of the problem space is suitable instead of static for reaction
systems.

28
Principles of Quantum Artiﬁcial Intelligence
A
B
C
D
G
I
F
H
K
E
J
Fig. 3.5
Representation of six rules by a directed AND/OR graph. The ‘and’ rules are
indicated by an arc between the links connecting the nodes indicating the manifestations.
3.2.2
Reaction systems
Problems with side eﬀects of actions like planning can be resolved by reac-
tion systems [Winston (1992)]. Reaction systems are a subgroup of produc-
tion systems. The premise speciﬁes the conditions that must be true before
the action described in the conclusion can be taken. Reaction systems need
strategies for conﬂict resolution. We will call the reaction systems simply
“production system” if no confusion with a deduction system is possible.
3.2.3
Conﬂict resolution
Conﬂict resolution strategies are often speciﬁed by general provisions [Jack-
son (1999)]:
• chose randomly a rule;
• a rule should be not allowed to ﬁre more than once on the same data;
• rules that have used more recent data are preferred;
• rules that have a greater number of patterns in the premise are pre-
ferred.
Rules also can be evaluated by a heuristic function. There are two diﬀerent
kinds of heuristic functions:
• the probability that the function is on the best path;
• the distance or diﬀerence between a given state and the desired state.

Problem Solving
29
It is diﬃcult to deﬁne heuristic functions, though frequently features can
be picked out which describe the distance to the goal [Russell and Norvig
(1995)]. The other possibility is the reuse of solutions to solved problems
to indicate which rule to use.
3.2.4
Human problem-solving
In systems which model human behavior and in practical applications, back-
tracking to a previous state of working memory is allowed. By allowing
backtracking and the exclusion of loops, a search from the initial state to
the desired state is executed. The search deﬁnes a problem space and can
be represented as a tree. However, it may not reach the desired goal either
because the branches are inﬁnite, or because after backtracking to the ini-
tial state no rule can ﬁre. A problem is described by the productions in
the long term memory, by the initial state, and by the desired state. The
solution to the problem is represented by a set of the productions which
successively change the state from the initial state to the desired state.
One of the best-known cognitive models, based on the production system,
is SOAR. The SOAR state, operator and result model was developed to
explain human problem-solving behavior [Newell (1990)]. It is a hierarchi-
cal production system in which the conﬂict-resolution strategy is treated as
another problem to be solved. All satisﬁed instances of rules are executed
in parallel in a “temporary” mode. After the temporary execution, the
best rule is chosen to take action. The decision takes place in the context
of a stack of earlier decisions. Those decisions are rated utilizing prefer-
ences and added to the stack by chosen rules. Preferences are determined
together with the rules by an observer using knowledge about a problem.
3.2.5
Example
The 8-puzzle is composed of eight numbered movable tiles in a 3 × 3 frame.
One cell of the frame is empty; as a result, tiles can be moved around to
form diﬀerent patterns. The goal is to ﬁnd a series of moves of tiles into
the blank space that changes the board from the initial conﬁguration to a
desired conﬁguration (see Figure 3.6). The production of long-term memory
can be speciﬁed by four productions [Luger and Stubbleﬁeld (1998)]:
• If the empty cell is not on the top edge, then move the empty cell up;
• If the empty cell is not on the left edge, then move the empty cell left;
• If the empty cell is not on the right edge, then move the empty cell

30
Principles of Quantum Artiﬁcial Intelligence
8
8
8
1
4
7
7
5
4
7
5
5
8
3
8
3
7
6
6
7
3
8
6
7
7
7
7
7
8
6
4
8
6
4
5
8
6
3
5
3
8
6
3
5
8
7
1
2
3
5
1
2
3
2
1
4
5
4
1
2
6
2
4
1
8
7
1
5
6
4
2
3
5
1
2
4
1
2
4
5
2
4
1
8
5
2
3
6
4
7
1
1
2
3
4
6
1
2
3
6
2
3
6
5
3
2
1
7
8
6
5
4
Fig. 3.6
The ﬁrst pattern (upper left) represents the initial conﬁguration and the last
(low right) the desired conﬁguration. The series of moves describe the solution to the
problem.
right;
• If the empty cell is not on the bottom edge, then move the empty cell
down.
The control strategy for the search would be
• halt when goal is in the working memory;
• chose a random production;
• do not allow loops.
3.3
Sub-Symbolic Models of Problem-Solving
Perception-oriented representation is an example of sub-symbolical repre-
sentation, such as the representation of numbers by the Oksapmin tribe of
Papua New Guinea. The Oksapmin tribe of Papua New Guinea counts by
associating a number with the position of the body [Lancy (1983)]. The
sub-symbolical representation often corresponds to a pattern that mirrors
the way the biological sense organs describe the world. Vectors represent
patterns. A vector is only a sub-symbol if there is a relationship between

Problem Solving
31
the vector and the corresponding similarity of the represented object or
state in the real world through sensors or biological senses. Feature-based
representation is an example of sub-symbolical representation.
Living organisms experience the world as a simple Euclidian geometrical
world. The actual perception of the world and manipulation in the world by
living organisms lead to the invention or recreation of an experience that, at
least in some respects, resembles the experience of actually perceiving and
manipulating objects in the absence of direct sensory stimulation [Wichert
(2009)].
This kind of representation is called sub-symbolic. Sub-symbolic rep-
resentation implies heuristic functions. The assumption that the distance
between states in the problem space is related to the similarity between the
sub-symbols representing the states is only valid in simple cases. However,
simple cases represent the majority of exiting problems in domain. Sense
organs sense the world by receptors which a part of the sensory system and
the nervous system.
According to the production system theory (reaction systems), we can
deﬁne a geometrically-based problem-solving model as a production system
operating on vectors of ﬁxed dimensions. Instead of rules, we use associ-
ations and vectors represent the states. Our goal is to form a sequence of
associations, which lead to a desired state represented by a vector, from an
initial state represented by a vector. Each association changes some parts
of the vector. In each state, several possible associations can be executed,
but only one has to be chosen. Otherwise, conﬂicts in the representation
of the state would occur. To perform these operations, we divided a vec-
tor representing a state into sub-vectors. An association recognizes some
sub-vectors in the vector and exchanges them for diﬀerent sub-vectors. It is
composed of a precondition of ﬁxed arranged α sub-vectors and a conclusion
[Wichert (2009)], [Wichert (2013)].
3.3.1
Proto logic
The manipulation of the states is described by simple proto logic, which
veriﬁes if a subset of symbols is present in a certain set of symbols. Suppose
a vector is divided into α sub-vectors with α > β. A production recognizes
β diﬀerent sub-vectors and exchanges them for β diﬀerent sub-vectors. Let
α = 7 objects that were recognized in the visual scene.
The seven vi-
sual objects are indicated at a certain position of the scene by symbols
A, B, C, D, E, F and G. The task of proto logic is to identify a precondi-

32
Principles of Quantum Artiﬁcial Intelligence
tion formed by visual objects represented by the set B, C, G, β = 3. Each
of the symbols B, C, G is checked for presence in the set that represents the
scene. It is veriﬁed if a set representing a precondition is a subset of the
set representing a scene. Proto logic operates on sets. It veriﬁes whether a
subset is present in a certain set [Wichert (2013)]. The task of proto logic
is trivial when working with sets. For an associative memory the direct
access to the stored information is not present, a solution to this problem
is described in [Wichert (2011)].
3.3.2
Binding problem
The “binding problem” determines how to connect together all physically
separated fragments of a complex object so that they can be processed
as a whole in sub-symbolical representation [Miikkulainen (1993); Kurfess
(1997); Wennekers (1999); Hummel (1999)]. For example, a red block is ob-
viously a diﬀerent object then a blue block. The fragments in this example
are the form and the color. Sub-vectors representing diﬀerent fragments can
be concatenated to a sub-vector representing the object [Wichert (2011)].
3.3.3
Icons
Out of several possible associations, we chose the one, which modiﬁes the
state in such a way that it becomes more similar to the desired state ac-
cording to the Euclidean distance [Wichert (2009)] (see Figure 3.7).
(a)
(b)
(c)
Fig. 3.7
The Euclidean distance between the corresponding vectors can compute the
distance between the icons. The distance between the states in the problem space is
actually related to the distance between the icons representing the states.
Euclidian
distance of state (a) and state (c) is smaller than the distance between state (b) and (c).
The distance in the problem space as well between state (a) and state (c) is smaller than
the distance between state (b) and (c).

Problem Solving
33
Fig. 3.8
The simplest method corresponds to a random choice, and does not oﬀer any
advantage over simple symbolical representation. An example of visual planning of the
tower building task of three blocks using the random choice is shown. The upper left
pattern represents the initial state; the bottom right pattern, the desired state.
With the aid of this heuristic hill climbing is performed. Each element
represents an object. Objects are represented by some dimensions of the
space and form a sub-space by themselves.
The example of Figure 3.8 and Figure 3.9 consists of the task of building
a tower from a collection of blocks [Nilsson (1982)]. A robot arm can stack,
unstack, and move the blocks within a plane on three diﬀerent positions
at a table. There are two diﬀerent classes of blocks: cubes and pyramids.
While additional blocks may be stacked on top of a cube, no other blocks
may be placed on top of a pyramid. The robot arm, which is represented
in the upper right corner, has a gripper that can grasp any available block.
It can move the block to eight diﬀerent positions on the tabletop or place
it on top of another cube [Wichert (2001, 2005a)].
The computation can be improved by a simple and universal heuristics
function, which takes into account the relationship between the vector and
the corresponding similarity of the represented states (see Figure 3.8 and
Figure 3.9). The heuristics function makes a simple assumption that the
distance between the states in the problem space is related to the similarity

34
Principles of Quantum Artiﬁcial Intelligence
Fig. 3.9
An example of visual planning of the tower building task of three blocks using
hill climbing shown. The upper left pattern represents the initial state; the bottom right
pattern, the desired state..
Fig. 3.10
An example of visual planning of the tower building task of eleven blocks using
hill climbing shown. The upper left pattern represents the initial state; the bottom right
pattern, the desired state.
of the vectors representing the states. This becomes more obvious with the
growth of objects that can be manipulated (see Figure 3.10).

Problem Solving
35
3.3.4
Euclidian geometry of the world
The similarity between the corresponding vectors can indicate the distance
between the sub-symbols representing the state.
Empirical experiments
in popular problem-solving domains of Artiﬁcial Intelligence, like robot in
a maze, block world or 8-puzzle indicated that the distance between the
states in the problem space is actually related to the similarity between
the images representing the states [Wichert (2001); Wichert et al. (2008);
Wichert (2009)].
Sub-symbolical problem solving takes advantage of the geometric nature
of the world. The assumption that the distance between states in the prob-
lem space is related to the distance between the sub-symbols representing
the states is only valid in simple cases. For example, our simple heuristics
and the resulting hill climbing cannot overcome problems in which one can-
not perform either of the necessary ﬁrst actions without undoing them at a
later stage. These kinds of problems are very often called anomalies, such
as the “Sussman anomaly” [Sussman (1975)].

This page intentionally left blank
This page intentionally left blank

Chapter 4
Information
4.1
Information and Thermodynamics
Information can be presented in a variety of forms that diﬀer from one
another: natural language, symbols, acoustic speech and pictures [Resnikoﬀ
(1989)] (see Figure 4.1).
Fig. 4.1
Information can be presented in a variety of forms which diﬀer from one an-
other: natural language, symbols, acoustic speech, pictures.
Information appeared as a unifying scientiﬁc concept before the inven-
tion of the ﬁrst computers. Information does not concern the substance
and the forces of the physical world [Stonier (1990)]. Information is what
remains after one abstracts from the material aspects of physical reality. In-
37

38
Principles of Quantum Artiﬁcial Intelligence
formation theory is highly related to mathematical probability theory and
thermodynamics. A thermodynamic description similar to information is
not a description of a physical property and relationships, but rather, this
description is a fundamental property. Thermodynamic relations might re-
main valid for all physical theories that can describe an aspect of reality.
They link physical entities to their organization and to their informational
structure. Thermodynamics is the study of the collective behavior of en-
tities on a macroscopic scale and uses statistics to describe microscopic
states of entities. The microscopic behavior corresponds to the motion of
the entities; on a macroscopic scale, this behavior is represented by the
temperature, pressure, and volume of physical systems such as a gas or a
ﬂuid. The statistical description gives us freedom of abstraction [Maxwell
(2001)]. It was James Clerk Maxwell, (1831 −1879), a Scottish mathe-
matician, who was one of the ﬁrst to recognize the connection between
thermodynamic quantities that are associated with gas, such as tempera-
ture, and the statistical descriptions of the molecules. We do not need to
specify each entity as a molecule or specify its exact speed or position. It
is suﬃcient to describe the statistical behavior of speciﬁc molecules. The
three laws of thermodynamics describe the processes that are involved in
the transport of heat. They are some of the most important laws of physics.
• Conservation of energy (ﬁrst law of thermodynamics): The change in
the internal energy of a closed thermodynamic system is equal to the
sum of the amount of heat energy supplied to the system and the work
performed on the system.
• The second law deals with entropy. Entropy is a measure of disorder
of the conﬁguration of the states of the atoms or other particles, which
make up the system. The total entropy of any isolated thermodynamic
system tends to increase over time, approaching a maximum value.
• Third law of thermodynamics: absolute zero temperature As a system
asymptotically approaches the absolute zero of the temperature, all
processes virtually cease and the entropy of the system asymptotically
approaches a minimum value. The entropy of all systems and of all
states of a system is zero at absolute zero.
“A physical system that is made up of many, many tiny parts will have
microscopic details to its physical behavior that are not easy to observe.
There are various microscopic states the system can have, each of which is
deﬁned by the state of motion of every one of its atoms, for instance.” a

Information
39
Table 4.1
Each of the two die can have one out of six microstates.
macrostate
(die1, die2)
number of microstates
2
1,1
1
3
1,2;2,1
2
4
1,3; 2,2; 3,1
3
5
1,4; 2,3; 3,2; 4,1
4
6
1,5; 2,4; 3,3; 4,2; 5,1
5
7
1,6; 2,5; 3,4; 4,3; 5;2, 5,1
6
8
2,6; 3,5; 4,4; 5,3; 6,2
5
9
3,6; 4,5; 5,4; 6,3
4
10
4,6; 5,5; 6,4
3
11
5,6; 6,5
2
12
6,6
1
quote from Matt McIrvin.1 We can choose to measure a physical system at
the macroscopic level. Macroscopic properties such as density or pressure
are the result of microscopic properties. A macroscopic property can be
realized by diﬀerent microscopic states. Macroscopic states are not static,
but they continuously change corresponding to the motion of atoms or
molecules.
Statistical mechanics describes this motion by some random
parameters, whereby each atom is moving randomly from a macroscopic
viewpoint. A set of atoms is described by a distribution of random variables
that express the random movements.
4.1.1
Dice model
Suppose that we have two dice; the macrostate is the total of the two dice,
and the microstate corresponds to the number on each of the die. There
are six ways to get a total of 7 from the microstates of the two dice but
only one way to get a total of 2 or 12. For this reason, throwing two dice
to sum a total of seven is more likely to occur; a sum of seven is six times
more likely to occur than two or twelve. Each die can have one out of six
microstates. The number of microstates of the whole system of two dice
is 6 × 6 = 36, the number of possible microstates is multiplied together.
The number of diﬀerent macrostates adds, for example, (6 + 6) −1 = 11.
We must subtract a one because the smallest macrostate is two and not
one (see Table 4.1). For four dice the number of microstates is already
6 × 6 × 6 × 6 = 1296.
In general, when several systems are combined
into a larger system, the number of possible microstates of each system
is multiplied, and the number of macrostates is added. For n fair dice,
1Matt McIrvin physics- group posting.

40
Principles of Quantum Artiﬁcial Intelligence
the total number of possible microstates is at a maximum at a macrostate
value of 3.5 × n. The value corresponds to the median of the Gaussian
distribution. The median of a ﬁnite list of microstates can be found by
arranging all of the microstates from the lowest value that represents a
macrostate to the highest value of a macrostate and picking the middle
value (see Table 4.1). The value of 3.5 × n can be derived for n dice, as
follows; the median is also represented as follows,
min + max
2
(4.1)
for n dice it follows:
n + n · 6
2
= n · 3.5.
(4.2)
If we shake n dice in a bag and measure the uppermost faces, and we
repeat the experiment, the dice will converge rapidly on the value (the
“macrostate”) for which the number of ways to make the value from indi-
vidual dice (“microstates”) is at a maximum. By performing this action,
we model an isolated system, the second law of thermodynamics. We model
the thermal ﬂuctuations by “shaking” the bag. The macroscopically mea-
surable quantities converge to the values that have the largest number of
microstates. It appears that the basis of the time direction, as expressed
by the second law of thermodynamics, is a type of Gaussian randomness.
4.1.2
Entropy
When several systems are combined into a larger system, the number of
possible microstates of each system are multiplied. This operation leads to
a very large number, which becomes intractable very quickly. A solution to
this problem is to use the logarithms of numbers; in this case, the log of the
product is the sum of the logs. In statistical thermodynamics, Boltzmann’s
equation relates the entropy S of an ideal gas to the number of microstates,
where W corresponds to a given macrostate [Boltzman (1995)]. We add
the number of microstates when we put systems together by taking the
logarithm. The result indicates the relationship between entropy and the
number of ways that the atoms or molecules of a thermodynamic system
can be arranged.
Entropy corresponds to the number of ways that the
microstate can rearrange itself without aﬀecting the macrostate. In the

Information
41
equation, k is Boltzmann’s constant, which is equal to 1.38062 × 10−23
joule/kelvin.
S = k · log ·W
(4.3)
The Second Law states that isolated systems tend toward an equilibrium
macrostate with as large total entropy as possible corresponding to the
largest number of microstates. For example, atoms moving around in a
gas possess a large number of possible microstates. This number is less for
a crystalline structure and, ﬁnally, it is one at 0K, where all microstates
become identical.
4.1.3
Maxwell paradox and information
In 1929, Le´o Szil´ard explained the thermodynamic Maxwell paradox by
identifying information with the negative measure of entropy [Szil´ard
(1929)]. Suppose that we have two chambers that are separated by a com-
mon partition, which could be removed to permit the objects in one to
move freely to the other. One chamber contains gas and the other chamber
contains nothing; on the removal of the partition, the gas will rapidly dif-
fuse and ﬁll the empty chamber (see Figure 4.2). Reverse evidence of this
Fig. 4.2
Suppose that we have two chambers that are separated by a common partition,
which could be removed to permit the objects in one to move freely to the other. One
chamber contains gas and the other chamber contains nothing; on the removal of the
partition, the gas will rapidly diﬀuse and ﬁll the empty chamber.
sequence mostly does not occur because an isolated systems tends toward
an equilibrium macrostate that corresponds to the largest number of mi-
crostates that are present when the gas is diﬀused over the two chambers.
Thinking in terms of the dice model, the probability of reverse evidence

42
Principles of Quantum Artiﬁcial Intelligence
approaches zero but is never equal to zero. In the paradox, there is a de-
mon that observes the gas molecules. Between the chambers, there is a
small door. The demon can open and close the small door, passing only
one molecule into a chamber. When a molecule in its random motion heads
toward the door of the chamber, it opens the door, brieﬂy permitting the
molecule to pass into the other chamber. Soon there will be more molecules
in one chamber than the other. Because the demon requires almost no en-
ergy to operate the door, the process decreases the entropy, which is a
contradiction to the second law of thermodynamics. Szilard’s explanation
is the following: to perform this task, the demon must be very well informed
about the position and velocity of the molecules that approached the door.
Only with this information can he judge when and for how long the door
should be opened to pass a molecule through and into the chamber without
allowing any molecules to pass in the opposite direction. As the demon’s
information about the distribution of the gas increases, the entropy of the
gas decreases.
4.1.4
Information theory
Instead of a demon that operates a door between two chambers, let us
imagine a simple experiment, for example, throwing a fair coin [Topsoe
(1974)]. Before we perform the experiment, we do not know what will be the
result; we are uncertain about the outcome. We measure the uncertainty by
the entropy of the experiment. The experiment starts at t0 and ends at t1.
At t0, we have no information about the results of the experiment, and at
t1, we have all of the information, so that the entropy of the experiment is
0. We can describe an experiment by probabilities. For the outcome of the
ﬂip of an honest coin, the probability for a head or tail is 0.5, p = (0.5, 0.5).
How can we deﬁne entropy? A person A knows the outcome, but person
B does not. Person B could ask A about the outcome of the experiment.
If the question is of the most basic nature, then we could measure the
minimal number of optimally required questions B must pose to know the
result of the experiment. A most basic question corresponds to the smallest
information unit that could correspond to a yes or no answer. The smallest
information unit is called a binary digit, or bit. For a fair coin, we pose
just one question, for example, is it a tail?
For a card game, to determine if a card is either red, clubs or spades,
we have a diﬀerent number of possible questions. If the card is red, then
we need only one question. However, in the case in which the card is not

Information
43
red, we need another question to determine whether it is a spade or a
club. The probability of being red is 0.5, of clubs 0.25 and spades 0.25,
p = (0.5, 0.25, 0.25). If the card is red, then we need only one question
(with probability 0.5). For clubs and spades, we need two questions. In the
meantime, we must ask 1 · 0.5 + 2 · 0.25 + 2 · 0.25 questions, which would
result in 1.5 questions. Thus, we must measure the mean number of op-
timal questions. For four cards, of which one is the joker, the probability
of a joker is 0.25 and of the other cards 1 −0.25 = 0.75, p = (0.25, 0.75).
In the meantime, we must ask 1 · 0.25 + 1 · 0.75 questions to determine if
the card is a joker or not. This approach results in one question. Given n
cards, of which one is the joker, the probability of a joker is 1/n and of the
other cards is 1 −1/n. In the meantime, we must ask 1 · 1/n + 1 · (1 −1/n)
questions to determine if the card is a joker or not. This approach results
in one question that is independent of the size of n. How could it be that
the result is independent of the size of n? It appears that something is
missing in our deﬁnition. Our result is correct for one independent exper-
iment; however, for several experiments, the mean number of questions is
lower. We deﬁne the real entropy for one experiment as H0(F 1), for two
experiments as H0(F 2), and for k experiments as H0(F k). Here, we have
the mean number of questions for the ﬁrst experiment, the second, and
the third to the kth experiment. The mean number of questions for one
experiment in the sequence of k experiments is 1/k · H0(F k).
For four cards of which one is the joker the probability of a joker is 0.25
and of other cards 1 −0.25 = 0.75 The real entropy for one experiment is
H0(F 1):
H0(F 1) = 1 · 0.75 + 1 · 0.25 = 1
H0(F 1)
1
= 1
The binary search tree corresponding to one experiment is represented in
the Figure 4.3.
The hierarchy of the probabilities for two experiments is shown in Table
4.2 and the resulting binary search tree is represented in the Figure 4.4. The
real entropy for two experiments is H0(F 2):
H0(F 2) = 1 · 0.75 · 0.75 + 2 · 0.75 · 0.25 + 3 · 0.25 · 0.75 + 3 · 0.25 · 0.25
H0(F 2) = 1.6875

44
Principles of Quantum Artiﬁcial Intelligence
Fig. 4.3
For four cards of which one is the joker the probability of a joker is 0.25 and of
other cards 1−0.25 = 0.75, p = (0.25, 0.75). In the mean we have to ask 1· 0.25+ 1· 0.75
questions to determine to determine if the card is a joker or not. The real entropy for
one experiment as H0(F 1)
Table 4.2
Hierarchy of the
probabilities for two experi-
ments.
results
probability
card, card
0.75 · 0.75
joker, card
0.25 · 0.75
card, joker
0.75 · 0.25
joker, joker
0.25 · 0.25
H0(F 2)
2
= 0.84375
The hierarchy of the probabilities for three experiments is shown in
Table 4.3 and the resulting binary search tree is represented in the Figure
4.5. The real entropy for two experiments is H0(F 3):
H0(F 3) = 1 · 0.42188 + 3 · 0.14062 + 3 · 0.14062 + 3 · 0.14062+
+5 · 0.046875 + 5 · 0.046875 + 5 · 0.046875 + 5 · 0.015625
H0(F 3) = 2.4688
H0(F 3)
3
= 0.82292
Does the sequence hk := H0(F k)
k
, with the values {1, 0.84375, 0.82292, ...}
for k = 1, 2, 3, .. have a limit for limk→∞hk?
It has. The limit is deﬁned as
H(F) := lim
k→∞
H0(F k)
k
≤H0(F)
(4.4)

Information
45
Fig. 4.4
The real entropy for two experiments as H0(F 2)
Table 4.3
Hierarchy of the probabilities
for three experiments.
results
probability
card, card, card
0.75 · 0.75 · 0.75
card, card, joker
0.75 · 0.75 · 0.25
card, joker, card
0.75 · 0.25 · 0.75
joker. card card
0.25 · 0.75 · 0.75
joker, joker, card
0.25 · 0.25 · 0.75
joker, card, joker
0.25 · 0.75 · 0.25
card, joker, joker
0.75 · 0.25 · 0.25
joker, joker, joker
0.25 · 0.25 · 0.25
it is called the ideal entropy, it converges to [Shannon (1948)]
H(F) = −
X
i
pi log2 pi.
(4.5)
In our case it is simply
H(F) = −0.25 · log2 0.25 −0.75 · log2 0.75 = 0.81128.
The ideal entropy indicates the minimal number of optimal questions that
B must pose to know the result of the experiment on A [Shannon (1948)],
[Topsoe (1974)]. Suppose that A repeated the experiment an inﬁnite num-
ber of times. The ideal entropy is the essential information obtained by
taking out the redundant information that corresponds to the ideal distri-
bution to which the results converge. An experiment is described by the

46
Principles of Quantum Artiﬁcial Intelligence
Fig. 4.5
The real entropy for two experiments as H0(F 3)
probabilities p = (p1, p2, ..., pn). Does the distribution of these probabilities
have an eﬀect on the ideal entropy? It turns out that the ideal entropy is
maximal in the case in which all probabilities are equal, which means that
p = (1/n, 1/n..., 1/n). In this case, the maximal ideal entropy is
H(F) = −
X
i
pi log2 pi = −log2 1/n = log2 n
(4.6)
in which n describes the number of states. For 216 = 65536 states that
present with the equal probability 1/65536, the optimal number of questions
is 16, which corresponds to 16 bits. If we suppose the letters of the alphabet
(26 of them) occur with equal probability in a message, then the average
information content per letter in a message is
H(F) −
26
X
i
1/26 · log2 1/26 = log2 26 = 4.7004 bit.
A word of ﬁve letters has an average information content of 5 · log2 6 =
23.502 bits.
However, in a real human language, the average informa-
tion content per letter is much lower because the distribution is not equal
[Topsoe (1974)]. Some letters are more frequent than others; an e is more
frequent than an x. We say that events that seldom happen, for example,
the letter x in a message, have a higher surprise. Surprise is inversely re-
lated to probability. The larger the probability that we receive a character,
the less surprised we are. A message over events that are not equally dis-
tributed has less information than a message over events that are equally
distributed, but that message allows a higher surprise.

Information
47
The equation H(F) = log2 n is very similar to Boltzmann’s equation,
in which W number of microstates corresponds to a given macrostate. It
follows, then, that the number of microstates is evenly distributed, and each
microstate has the same probability of appearance. There are, however,
two diﬀerences between the two equations. Boltzmann’s equation includes
Boltzmann’s constant, and it uses log instead of log2. Instead of measuring
the information in bits (yes/no questions) it measures the information in
nepit (nat), which is based on Euler’s number e = 2.7182818... (sometimes
also called Napier’s constant). Euler’s number is irrational and cannot be
attributed to any questions. However, in the next section, we indicate that
Euler’s number is the ideal number that minimizes the depth of an idealistic
search tree.
4.2
Hierarchical Structures
The principles of hierarchical organization appear in nature, for example,
the structure of matter itself is hierarchically organized, including elemen-
tary particles, atomic nuclei, atoms, and molecules [Resnikoﬀ(1989)]. The
idea of hierarchical structures is based on the decomposition of a hierarchy
into simpler parts. One of the greatest inventions of human civilization is
the possibility of representing numbers by a hierarchically organized struc-
ture, which was begun approximately 5000 years ago by the Sumerian and
Akkadian civilizations and later popularized by the Babylonians. An an-
cient way to denote a positive whole number is a sequence of marks used
for counting. A requirement for the sequence is to provide a way to write
down the next mark. An example is the arrangement of a number as a
set of similar strokes; however, this representation takes up a large amount
of space. This type of representation is called the unary numeral system.
Positional notation is itself a hierarchical organized structure in which the
whole number is mapped to positional notation to a certain base. For a
unary numeral system, the base is one, and no hierarchy is present. For
base B > 1 (a whole number), the hierarchy for a number N corresponds to
determining the largest integer k such that Bk ≤N, where k indicates the
number of digits that correspond to the depth of the hierarchy for the basis
B. Bk represents the number of possible numbers that can be represented
by the corresponding hierarchy. In the next step, k questions for each layer
of the hierarchy must be answered to indicate which combination out of
the Bk possible numbers is present. Each question has B possible answers,

48
Principles of Quantum Artiﬁcial Intelligence
which correspond to the digits B −1, B −2, .., 0. The ﬁrst question is: what
is the leading digit nk of the base B representing N, nk · Bk ≤N. The
next question asks about the digit,
nk−1 · Bk−1 ≤N −nk · Bk.
(4.7)
The following questions ask about the digit
nk−2 · Bk−2 ≤N −nk · Bk −nk−1 · Bk−1
(4.8)
and so forth.
For a binary base, the hierarchical structure corresponds to the ideal
Entropy in which all of the probabilities of occurrences of the numbers are
equal. Each question receives a reply of either yes or no. For each base, each
question has B possible answers. This representation is related to a tree
in computer science that has a constant branching factor of B. However,
there are some diﬀerences; the nodes are organized in levels, the root of the
tree corresponds to the level 0, and each node at each level has B children,
which means that, at each level k, there are Bk nodes. A has the highest
level L, and there are BL = N leaves that correspond to the N represented
objects. For N objects and the number of levels L, the branching factor B
is equal to
B = N
1
L .
(4.9)
For each level k there are
N(K) = Bk = N
1
L
(4.10)
nodes with N(L) = L. For a search of an object out of N at each level, B
questions must be answered. The costs are B · log(N)/(log(B). Is there a
B for which the cost becomes minimal? If we suppose that the tree is an
ideal tree in which the branching factor can be an irrational number, then
the solution is easy
cost = B · log(N)
log(B)
(4.11)
0 = ∂cost
∂B
= log(N)
log(B) −log(N)
log(B)2
with the solution
B = e = 2.7182818...
which corresponds to Euler’s number. The closest whole number is three,
followed by two. Euler’s number minimizes the cost of an idealistic search

Information
49
tree; it is the minimal mean number of questions that must be answered.
The minimal value of the number of questions corresponds to the essential
information represented by nepit (nat) in the Boltzmann’s equation. We
assumed that we can answer all of the questions. Suppose that we cannot.
In this case, we do not know which path to follow, and we must perform
a search. Either we chose the path (answer to a question) randomly, or
we perform a blind search, or we get a hint of which path (which answer)
is right. In computer science, the hint is expressed by a heuristic function
that rates the value’s diﬀerent paths according to how far the paths are
from the goal object.
4.2.1
Example of a taxonomy
One of the most eﬀective ways to structure knowledge is the taxonomic ar-
rangement of the information that represents it [Resnikoﬀ(1989)]. In 1887
Professor Harry Govier Seeley grouped all dinosaurs into the Saurischia
and Ornithischia groups according to their hip design [Haines (1999)]. In
Figure 4.6 and we 4.7 can see some examples of the two categories [Wichert
(2000)].
Fig. 4.6
Stenonychosaurus is an example of the category Saurischia.
Fig. 4.7
Stegosaurus is an example of the category Ornithischia.

50
Principles of Quantum Artiﬁcial Intelligence
The saurischian were divided later into two subgroups:
the car-
nivorous, bipedal theropods and the plant-eating, mostly quadruped
sauropodomorphs.
The ornithischians were divided into the subgroups
birdlike ornithopods, armored thyreophorans, and margginoncephalia. The
subgroups can be divided into suborders and then into families and ﬁnally
into genus. The genus includes the species. It must be noted that in this
taxonomy many relations are only guesswork, and many paleontologists
have diﬀerent ideas about how the taxonomy should look [Lambert (1983,
1993)] (see Figure 4.8).
Order
Ornthischian
Ornithopods
Four Legged
Ornithischian
Other
Saurischian
Strange Killers
Sauropodomorphs
Sauropods
Coelurosaurs
Carnosaurs
Therodopods
5
14
Prosauropods
Assorted Sauropods
Staurikosaurids
1
10
Fig. 4.8
Taxonomy of dinosauria.
The number written below the rectangular boxes
represents the categories which are not divided, the species. Uncertain categories are
represented by dotted arrows.
4.3
Information and Measurement
Numbers are used in two fundamentally diﬀerent ways. Either they can
describe measurements of observed phenomena or they are used for mathe-
matical calculations. Numbers that are attached to an observed magnitude
represent the gained information. A real number a will be represented as
decimal expansions of the form
a = a−N · · · a−1a0.a1a2 · · ·
(4.12)
it is a shorthand of notation of
a = a−N10N + · · · + a−110 + a0100 + a110−1 + a210−2 + · · · .
(4.13)
Increasingly precise variants of the measurement yield to least signif-
icant digit (right one) in the decimal expansion.
How much additional

Information
51
information is provided by increasing the process of measurement? The
measurement of a quantity Ξ is made. Its numerical value is h with in-
ﬁnitely many digits. The measurement provides a range of values in which
h will be contained. Suppose the measurement provides numbers x1 and
y1 with [Resnikoﬀ(1989)]
x1 < h < y1,
(4.14)
(see Figure 4.9).
In terms of decimal expansion, the measurement has
Fig. 4.9
The measurement provides numbers x1 and y1 with x1 < h < y1.
determined a certain ﬁxed numbers of the expansion of h.
The second
measurement is done with a greater precision x2 and y2 with
x1 < x2 < h < y2 < y1,
(4.15)
(see Figure 4.10). The gain of information after the second measurement
Fig. 4.10
The second measurement is done with a greater precision x2 and y2 with
x1 < x2 < h < y2 < y1.
can be expressed by
I10 = log10
y1 −x1
y2 −x2

= log10(y1 −x1) −log10(y2 −x2)
(4.16)
with log10(y2 −x2) being approximately number of digits in decimal
expansion. The information can be measured in diﬀerent units, for example
binary digit expansion is
I2 = log2
y1 −x1
y2 −x2

= log2(y1 −x1) −log2(y2 −x2).
(4.17)
The Equation 4.16 represents the information, which depends only on
the endpoints x and y of the measurement of the interval.
It does not
depend on

52
Principles of Quantum Artiﬁcial Intelligence
• the choice of the zero point
I(x + b, y + b) = I(x, y)
• nor the scale
I(x · a, y · a) = I(x, y).
4.3.1
Information measure I
We determine the function I which satisﬁes both conditions [Resnikoﬀ
(1989)]. With b = −x we get
I(x, y) = I(x + b, y + b) = I(0, y −x)
(4.18)
and with a = 1
x we get
I(x, y) = I(a · x, a · y) = I

1, y
x

.
(4.19)
If we apply a =
1
y−x to Equation 4.18 we get
I(x, y) = I(0, y −x) = I(a · 0, a · (y −x)) = I(0, 1) = constant
(4.20)
One measurement results always in a constant gain. Information gain does
not depend on the results of one measurement described by x and y. Sup-
pose that I is a function of 3 variables x, y, z which were the results of a
measurement (see Figure 4.11). The function does not depend on
Fig. 4.11
I is a function of 3 variables x, y, z which were the results of a measurement.
• the choice of the zero point
I(x + b, y + b, z + b) = I(x, y, z)
• nor the scale
I(x · a, y · a, z · a) = I(x, y, z).
Setting b = −x and a = 1
y we get
I(x, y, z) = I(x + b, y + b, z + b) = I(0, y −x, z −x)
(4.21)
I(x, y, z) = I(a · x, a · y, a · z) = I
x
y , 1, z
y

(4.22)

Information
53
Combining this two results together we get
I(x, y, z) = I

0, 1, z −x
y −x

.
(4.23)
Information measurement which yields the numbers x, y, z can be expressed
as a function of the ratio of the diﬀerence z −x to the diﬀerence y −x. This
ratio corresponds to two measurements for h with x < y < z (see Figure
4.11):
• ﬁrst with the bounds x < h < z,
• second with the bounds x < h < y.
We can preform a third measurement h with x < w < y < z, see Figure
4.12. The information content of the second measurement relative to the
Fig. 4.12
x, y, z, w describe two measurements for h with x < w < y < z.
ﬁrst one is
I(x, y, z) = I

0, 1, z −x
y −x

(4.24)
and the information content of the third measurement relative to the second
one is
I(x, y, z) = I

0, 1, y −x
w −x

.
(4.25)
The information content of the third measurement relative to the ﬁrst
should remain unchanged
I

0, 1, z −x
w −x

= I

0, 1, z −x
y −x

+ I

0, 1, y −x
w −x

.
(4.26)
We simplify
I
 z −x
w −x

= I
z −x
y −x

+ I
 y −x
w −x

(4.27)
with
t := z −x,
u := y −x,
v := w −x

54
Principles of Quantum Artiﬁcial Intelligence
I
 t
v

= I
 t
u

+ I
u
v

(4.28)
and with
s := 1
v ,
u := 1
I(s · t) = I(t) + I(s).
(4.29)
I must be a Logarithm with respect to chosen base, for log2
I(s) = log2 s
(4.30)
and
I
 z −x
w −x

= log2
 z −x
w −x

(4.31)
represent the gain of information in bits. The general case is the one of the
two measurements, with the ﬁrst measurement x < h < z and second one
is a more precise measurement u < h < v with x < u < v (see Figure 4.13).
Additivity of the information function I provides
Fig. 4.13
The general case is the one of the two measurements, with the ﬁrst mea-
surement x < h < z and second one is a more precise measurement u < h < v with
x < u < v.
I
z −x
z −u

+ I
z −u
v −u

= I
z −x
v −u

(4.32)
• First term is the information increment obtained by narrowing the es-
timate x < h < z to u < h < z where x < u.
• Second term is the information increment gained by narrowing the es-
timate u < h < z to u < h < v where v < z.
• Sum as in formation gained by passing from the estimate x < h < z to
the more precise estimate u < h < v.

Information
55
4.3.2
Nature of information measure
Information measure is relative [Resnikoﬀ(1989)]:
• The information remains unchanged when each of the variables is in-
creased by a value or multiplied by a constant.
• A single measurement does not provide information.
Information gained from the measurement of an interval must always be
considered relative to some prior measurement. This prior measurement can
be represented by a pre deﬁned scale representing the ﬁrst measurement,
like deﬁned by the metric system.
4.3.3
Measurement of angle
In a circle a measurement is represented by an angle α and a more exact
one by the angle β with α > β (see Figure 4.14). The information gain is
Fig. 4.14
In a circle a measurement is represented by two angles.
I
α
β

= log2
α
β

(4.33)
bits. The angle measure of a direction must lie between 0 and 2 · π With a
priori knowledge α = 2 · π only one measurement β is required [Resnikoﬀ
(1989)]
I
2 · π
β

= log2
2 · π
β

.
(4.34)
For a straight line β = π and the information content is
I(π) = log2
2 · π
π

= 1 bit.
(4.35)

56
Principles of Quantum Artiﬁcial Intelligence
The observed direction lies in one half-of the planes. Smaller angles corre-
spond to greater information, I(π/2) = 2 bits,
I(π/4) = 3 bits.
4.3.4
Information and contour
A contour is subdivided into short segments of equal length. Each seg-
menting point can be thought as the vertex of an angle formed with two
neighboring points [Resnikoﬀ(1989)].
Associated with that angle is its
measure of information gain. In a simple example there are three points P,
Q, R (see Figure 4.15). We move on a line from P to Q. The corresponding
information is 1 bit. The information gain passing from one straight line
to the next is
I
π
π

= 0 bits
since the angle remains unchanged. When the right angle at vertex Q is
reached, there is a positive gain of information
I
 π
π/2

= 1 bits.
At the next step, passing from right angle to the straight angle there is an
information loss
I
π/2
π

= −1 bits.
In the example the right angle is where the contour changes its direction.
Fig. 4.15
There are three points P , Q, R.
Corners yield the greatest information, more strongly curved points yield
more information.

Information
57
4.4
Information and Memory
“Human memory is based on associations with the memories it contains.
Just a snatch of well-known tune is enough to bring the whole thing back to
mind. A forgotten joke is suddenly completely remembered when the next-
door neighbor starts to tell it again. This type of memory has previously
been termed content-addressable, which means that one small part of the
particular memory is linked - associated -with the rest.” Cited from [Brunak
and Lautrup (1990)], page 104.
Associative memory models human memory [Palm (1990); Churchland
and Sejnowski (1994); Fuster (1995); Squire and Kandel (1999)]. The as-
sociative memory and sub-symbolic distributed representation incorporate
the following abilities in a natural way [Palm (1982); Hertz et al. (1991);
Anderson (1995b); Kohonen (1989)]:
• The ability to correct faults if false information is given.
• The ability to complete information if some parts are missing.
• The ability to interpolate information. In other words, if a sub-symbol
is not currently stored the most similar stored sub-symbol is deter-
mined.
The Lernmatrix, also simply called “associative memory”, was devel-
oped by Steinbuch in 1958 as a biologically inspired model from the eﬀort
to explain the psychological phenomenon of conditioning [Steinbuch (1961,
1971)]. Later this model was studied under biological and mathematical as-
pects mainly by Willshaw [Willshaw et al. (1969)] and Palm [Palm (1982,
1990)].
Associative memory is composed of a cluster of units. Each unit rep-
resents a simple model of a real biological neuron. The Lernmatrix was
invented by Steinbuch, whose goal was to produce a network that could use
a binary version of Hebbian learning to form associations between pairs of
binary vectors, for example each one representing a cognitive entity. Each
unit is composed of binary weights, which correspond to the synapses and
dendrites in a real neuron (see Figure. 4.16).
They are described by wij ∈{0, 1} in Figure 4.17. T is the threshold
of the unit.
The Lernmatrix is simply called associative memory if no
confusion with other models is possible [Anderson (1995a); Ballard (1997)].
The patterns, which are stored in the Lernmatrix, are represented by
binary vectors. The presence of a feature is indicated by a ‘one’ component
of the vector, its absence through a ‘zero’ component of the vector. A pair of

58
Principles of Quantum Artiﬁcial Intelligence
dendrites
synapse
soma
axon
unit
neuron
Fig. 4.16
A unit is an abstract model of a biological neuron [McClelland and Kawamoto
(1986); Palm (1990); Hertz et al. (1991); OFTA (1991); Schwenker (1996)].
y
y
y
y
x
x
x
T
T
T
T
w
w
w
w
w
w
w
w
w
w
w
1
2
n
1
2
3
m
11
21
31
wm1
m2
mn
12
22
23
1n
2n
3n
Fig. 4.17
The Lernmatrix is composed of a set of units which represent a simple model
of a real biological neuron. The unit is composed of weights, which correspond to the
synapses and dendrites in the real neuron. In this Figure they are described by wij ∈
{0, 1} where 1 ≤i ≤m and 1 ≤j ≤n. T is the threshold of the unit.
these vectors is associated and this process of association is called learning.
The ﬁrst of the two vectors is called the question vector and the second,
the answer vector. After learning, the question vector is presented to the
associative memory and the answer vector is determined by the retrieval
rule.
Learning
Initially, no information is stored in the associative memory.
Because the information is represented in weights, all unit weights are ini-
tially set to zero. In the learning phase, pairs of binary vector are associated.

Information
59
Let ⃗x be the question vector and ⃗y the answer vector, the learning rule is:
wnew
ij
1
if yi · xj = 1
wold
ij
otherwise.
(4.36)
This rule is called the binary Hebbian rule [Palm (1982)]. Every time a
pair of binary vectors is stored, this rule is used.
Retrieval
In the one-step retrieval phase of the associative memory, a
fault tolerant answering mechanism recalls the appropriate answer vector
for a question vector ⃗x.
The retrieval rule for the determination of the
answer vector ⃗y is:
yi =
 1 Pn
j=1 wijxj = T
0 otherwise
(4.37)
where T is the threshold of the unit. The threshold T is set to the number
of “one” components in the question vector ⃗x, T := |⃗x|. It is quite possible
that no answer vector is determined (zero answer vector). This happens
when the question vector has a subset of components that was not correlated
with the answer vector. A solution to this problem is the soft threshold
strategy. In this strategy, the threshold is set to the maximum sum:
T :=max i
n
X
j=1
δ(wijxj)
(4.38)
with
δ(x) =
1 if x > 0
0 if x = 0
(4.39)
and the retrieval rule for the determination of the answer vector ⃗y is:
yi =
 1 Pn
j=1 wijxj ≥T
0 otherwise.
(4.40)
This retrieval is called:
• association provided that the answer vector represents the reconstruc-
tion of the disturbed question vector;
• hetero-assocation if both vectors are diﬀerent.

60
Principles of Quantum Artiﬁcial Intelligence
Example
In Figure 4.18 the vector pair ⃗x1 = (1, 0, 0, 0, 1) and ⃗y1 =
(0, 1, 1, 1, 0) is learned. The corresponding binary weights of the associ-
ated pair are indicated by a black square. In the next step the vector pair
⃗x2 = (0, 1, 1, 0, 1) and ⃗y2 = (1, 1, 0, 0, 1) is learned. The corresponding bi-
nary weights of the associated pair are indicated by a black circle. In third
step the retrieval phase is preformed. The question vector ⃗x∗= (0, 1, 0, 0, 1)
diﬀers by one bit to the learned question vector ⃗x2 = (0, 1, 1, 0, 1). The
threshold T is set to the number of “one” components in the question vec-
tor ⃗x∗, T = 2. The retrieved vector is the vector ⃗y2 = (1, 1, 0, 0, 1) that
was stored. A backward projection can be preformed in the fourth step.
The synaptic matrix is a transpose of the matrix W, which is used for the
forward projection. The similarity between the stored vector pair and the
presented can be computed.
Fig. 4.18
The vector pair ⃗x1 = (1, 0, 0, 0, 1) and ⃗y1 = (0, 1, 1, 1, 0) is learned.
The
corresponding binary weights of the associated pair are indicated by a black square. In
the next step the vector pair ⃗x2 = (0, 1, 1, 0, 1) and ⃗y2 = (1, 1, 0, 0, 1) is learned. The
corresponding binary weights of the associated pair are indicated by a black circle. In
third step the retrieval phase is preformed. The question vector ⃗x∗= (0, 1, 0, 0, 1) diﬀers
by one bit to the learned question vector ⃗x2 = (0, 1, 1, 0, 1). The threshold T is set to
the number of “one” components in the question vector ⃗x∗, T = 2. The retrieved vector
is the vector ⃗y2 = (1, 1, 0, 0, 1) that was stored. A backward projection can be preformed
in the fourth step. The synaptic matrix is a transpose of the matrix W , which is used for
the forward projection. The similarity between the stored vector pair and the presented
can be computed.

Information
61
Storage capacity
For an estimation of the asymptotic number L of vec-
tor pairs (⃗x, ⃗y) that can be stored in an associative memory before it begins
to make mistakes in the retrieval phase, it is assumed that both vectors have
the same dimension n. It is also assumed that both vectors are composed
of k ones, which are equally likely to be in any coordinate of the vector. In
this case it was shown [Palm (1982); Hecht-Nielsen (1989); Sommer (1993)]
that the optimum value for k is approximately
k .= log2(n/4).
(4.41)
For example for a vector of the dimension n=1000000 only k = 18 ones
should be used to code a pattern according to the Equation 4.41.
For
an optimal value for k according to the Equation 4.41 with ones equally
distributed over the coordinates of the vectors, approximately L vector
pairs can be stored in the associative memory [Palm (1982); Hecht-Nielsen
(1989)]. L is approximately
L .= (ln 2)(n2/k2).
(4.42)
This value is much greater than n. The estimate of L is very rough because
Equation 4.42 is only valid for very large networks. Equation 4.42 does
not apply for networks of reasonable size, however the capacity increase is
still considerable. For realistic values please consult Table 2 in [Knoblauch
et al. (2010)]. Small deviation from the logarithmic sparseness reduces the
network capacity. It is very diﬃcult to ﬁnd coding schemas that represent
the information by logarithmic sparse codes [Knoblauch et al. (2010)].
It should be noted that the Lernmatrix system allows high capacity and
fast access when working in parallel, each unit represents a neuron that
performs calculations. On a conventional Von Neumann architecture, com-
pressed look-up tables are more eﬃcient [Knoblauch et al. (2010)]. However
a Von Neuman architecture is not biologically plausible.
Information and storage capacity
We indicate a sketch of a proof for
Equation 4.41 and 4.42 [Hecht-Nielsen (1989)]. There are C(n, k) diﬀerent
binary vectors of the dimension n with k ones,
C(n, k) =
n!
(n −k)! · k!
(4.43)
for example
C(3, 2) =
3!
(1)! · 2! = 3,
(1, 1, 0),
(1, 0, 1),
(0, 1, 1).

62
Principles of Quantum Artiﬁcial Intelligence
We can determine for each vector the probability the presence of a one
pC(n,k) =
1
C(n, k).
(4.44)
The entropy of L vectors is
H(F) = −
X
i
pi log2 pi = −L · pC(n,k) log2 pC(n,k)
(4.45)
and we can rewrite it as
H(F) = L ·
1
C(n, k) log2 C(n, k).
(4.46)
The self-information or the information content associated with the out-
come of a random variable ωi is deﬁned as
I(ωi) = −log2 P(ωi).
(4.47)
The information content of L vectors is
I = −L · log2 pC(n,k) = L · log2 C(n, k) = L · log2

n!
(n −k)! · k!

. (4.48)
We want to maximize the information I in correspondence to the size of
the associative memory n,
Maximize −→I
n2 .
(4.49)
Depending on the size n, we have to ﬁnd optimal values for k and L.
• We need to determine the probability p after storing L binary vectors
in the associative memory, that a weight wij at a certain position (ij)
is one.
• We need to determine the probability p−1 after storing L binary vectors
in the associative memory, that a weight wij at a certain position (ij)
is zero.
The probability p depends on the probability 1 −p. For L pairs the prob-
ability that a weight is zero is
(1 −p) =
n2 −k2
n2
L
(4.50)
since the probability of a sequence of n independent events is its product.
For L pairs the probability that a weight is one is
p = 1 −

1 −k2
n2
L
.
(4.51)

Information
63
We try to determine the probability of obtaining an extra 1 during recall of
⃗yq. We know that the vector ⃗xq has k ones and the probability of a weight
being 1 is p. Let us demand that the number of wrong ones on each ⃗yq
vector recall be 1. The product of (n −k), the number of 0 in ⃗yq and the
probability of each 0 being wrongly set to one (pk) will be equal to one
1 = (n −k) · pk.
(4.52)
Combining the Equations 4.51 and 4.52 yields
(n −k) ·
 
1 −

1 −k2
n2
L!k
= 1,
(4.53)
1 −

1 −k2
n2
L
= (n −k)−1
k ,
(4.54)

1 −k2
n2
L
= 1 −(n −k)−1
k ,
(4.55)
L · log

1 −k2
n2

= log

1 −(n −k)−1
k

,
(4.56)
L =
log

1 −(n −k)−1
k

log
 1 −k2
n2

.
(4.57)
Combining the Equations 4.48
I = L · log2 C(n, k) =
log

1 −(n −k)−1
k

log
 1 −k2
n2

· log2

n!
(n −k)! · k!

(4.58)
and
log2

n!
(n −k)! · k!

= log2(n!) −log2((n −k)!) −log2(k!).
(4.59)
We can use the logarithmic version of Sterling‘s formula. The Sterling‘s
formula is given by
log(n!) = n + 1
2
· log(n) −n + 1
2 · log(2 · π),
(4.60)
log2(n!) = n + 1
2
· log2(n) · log(2) −n + 1
2 · log2(2 · π) · log(2),
log2(n!) = n + 1
2
· log2(n) · log(2) −n + 0.92.
(4.61)

64
Principles of Quantum Artiﬁcial Intelligence
All together yields
I = L · log2 C(n, k) =
1
log(2) ·


log

1 −(n −k)−1
k

log
 1 −k2
n2


·

n + 1
2 · (log(n) −log(n −k)) + k · log(n −k) −log(k) ·

k + 1
2

−0.92

(4.62)
Using computer simulation we can determine the corresponding values k
that maximize I
Maximize −→I
n2
depending on n, n = 102, 103, · · · , 10100, see Figure 4.19.
We ﬁnd the
20000
40000
60000
80000
100000
8
10
12
14
Fig. 4.19
Values k that maximizes I for n = 102, 103, 104, 105.
optimum value for k to be
k .= log2(n) −2 = log2(n/4)
(4.63)
and
L .= (ln 2)(n2/k2).
Substituting Equation 4.63 into Equation 4.62 we get the upper bound for
large n
I = n2 log 2 = n2 · 0.693
(4.64)
the asymptotic capacity is 69.31 percent with allowed error rate of one
additional one per ⃗yq as expressed by Equation 4.52. This capacity is only
valid for sparse equally distributed ones [Palm (1982)].

Information
65
Weight matrix diagram
The diagram of the weight matrix illustrates
the weight distribution which results from the distribution of the stored
patterns [Marcinowski (1987); Freeman (1994)]. Useful associative proper-
ties result from equally distributed weights over the whole weight matrix.
Clusters in the diagram indicate strong correlation between parts of stored
patterns. The load of the associative memory is indicated by the percent-
age of weights which are not zero. A high percentage indicates an overload
and the loss of its associative properties. Figure 4.20 represents a diagram
of a high loaded matrix with equally distributed weights.
Structure of weight matrix
The structure of the weight matrix indi-
cates the elementary blocks which compose an associative memory. It is
represented by the frequency of diﬀerent sum values of the weights of rows
or columns [Marcinowski (1987)]. The sum over column i is,
µi =
n
X
j=1
wij
The µi are sorted with a new index π = ι(i),
µ1 ≤µ2 ≤µ3 ≤µπ ≤. . . ≤. . . ≤µm.
The number ζ of groups with diﬀerent µ values and the number of their
elements is determined,
µ1 = µ2 = µ3
|
{z
}
τ1=3
< µπ = . . .
|
{z
}
τ2
< . . . = µm.
|
{z
}
τζ
This can be represented as a procedure:
Φ = 1
τΦ = 1
FOR π = 1 TO m-1 STEP 1
DO
IF µπ = µπ+1 THEN τΦ = τΦ + 1
ELSE DO Φ = Φ + 1; τΦ = 1 OD
OD
ζ = Φ.

66
Principles of Quantum Artiﬁcial Intelligence
The ζ sorted diﬀerent τΦ values are represented by a diagram.
The x
axis represents the Φ ∈[1, 2, . . . , ζ] values and y axis the corresponding
frequency of sum values τΦ. The relationship between the x axis ordinate
and corresponding value µπ is represented additionally, for example, by an
additional plot. If the associative memory performs hetroassocative recalls,
the associative matrix is not symmetric and the diagrams for the sum of
rows and columns are diﬀerent. The sum over row j is
µj =
n
X
i=1
wij.
There are n µj values (see Figure 4.17). In Figure 4.21 the structure of
the weight matrix of Figure 4.20 is represented. The plot illustrates that the
weight matrix is composed of approximately 300 elementary blocks which
represent a nearly gausian correlation between the stored pattern parts.
Figure 4.20 shows the distribution results of the ten randomly set ones in
the 2000 dimensional, 20000 learned vector pairs.
200
400
600
800
1000
1200
1400
1600
1800
2000
200
400
600
800
1000
1200
1400
1600
1800
2000
Fig. 4.20
The weight matrix after learning of 20000 test patterns, in which ten ones were
randomly set in a 2000 dimensional vector represents a high loaded matrix with equally
distributed weights.
This example shows that weight matrix diagram often contains
nearly no information. Information about the weight matrix can be extracted by the
structure of weight matrix. (White color represents weights.)

Information
67
(a)
0
50
100
150
200
250
0
5
10
15
20
(b)
0
50
100
150
200
250
600
700
800
900
(c)
0
50
100
150
200
250
300
0
5
10
15
20
(d)
0
50
100
150
200
250
300
600
700
800
900
Fig. 4.21
38% of synapses of associative memory are not zero. (a) The frequency of
diﬀerent sum values of columns. (b) The corresponding sum values of columns. (c) The
frequency of diﬀerent sum values of rows. (d) The corresponding sum values of rows.
4.5
Sparse code for Sub-symbols
Usually suboptimal sparse codes are used. An example of a suboptimal
sparse code is the representation of words by context-sensitive letter units
[Wickelgren (1969, 1977); Rumelhart and McClelland (1986); Bentz et al.
(1989)]. The ideas for the used robust mechanism come from psychology
and biology [Wickelgren (1969, 1977); Rumelhart and McClelland (1986);
Bentz et al. (1989)]. Each letter in a word is represented as a triple, which
consists of the letter itself, its predecessor, and its successor. For example,
six context-sensitive letters encode the word desert, namely:
de, des, ese,
ser, ert, rt .
The character “ ” marks the word beginning and ending.
Because the alphabet is composed of 26+1 characters, 273 diﬀerent context-
sensitive letters exist. In the 273 dimensional binary vector each position
corresponds to a possible context-sensitive letter, and a word is represented
by indication of the actually present context-sensitive letters.
A set of features can be represented by a binary vector and represent a
category. A position in the corresponding vector corresponds to a feature.
To be sparse, the set of features that describes a category compared to the
dimension of the vector has to be suﬃciently small. This is because, of all
possible features, only some should deﬁne categories. This can be achieved
by sparsiﬁcation based on unary sub-vector representation.

68
Principles of Quantum Artiﬁcial Intelligence
4.5.1
Sparsiﬁcation based on unary sub-vectors
A binary representation of a number h would require a vector of length
d = ⌊log2h+1⌋. However if we represent the number h in unary, we requireh
positions. One unary representation of h ̸= 0 is a string of h −1 zeros with
a one at h-th postion [Wichert (2013)]. A binary number of length d is
represented by a unary number of 2d positions, which is exponential in
the size of input. A binary vector ⃗x of dimension t is split into f distinct
sub vectors of dimension p = dim(t/f). The binary sub vectors ui(⃗x) of
dimension p = dim(t/f) are represented as unary vectors of dimension 2p :
⃗x = x1, , x2, · · · , xp
|
{z
}
u1(⃗x)
, · · · , xm−p+1, · · · , xm
|
{z
}
uf(⃗x)
(4.65)
The resulting binary vector is composed out of the unary vectors and has
the dimension f ·2p. In the following example a binary vector of dimension
6 is split into 2 distinct sub vectors of dimension 3. The binary sub vectors
ui(⃗x) of dimension 3 are represented as unary vectors of dimension 23 :
⃗x = 1, 0, 1
| {z }
u1(1,0,1)
, 0, 0, 1
| {z }
u2(0,0,1)
(4.66)
u1(1, 0, 1) = (0, 0, 0, 0, 1, 0, 0, 0);
(h = 5)
u2(0, 0, 1) = (1, 0, 0, 0, 0, 0, 0, 0);
(h = 1)
u(⃗x) = (0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0),
(4.67)
resulting in a new vector of dimension 16 = 2∗23 with 2 ones. The resulting
vector is more sparse, however some information related to correlation is
lost [Wichert (2013)]. These ideas are related to the Wilshaw model of
associative memory with local inhibition [Shim et al. (1990)] and [Kropﬀ
and Treves (2005)].
4.6
Deduction Systems and Associative Memory
In this section we will present a straightforward transformation from sym-
bolic rules into a representation by associative memory. We will indicate
that deduction systems may be represented by an associative memory with
feedback connections [Wichert (2005b)], [Wichert (2006)], [Wichert (2012)].

Information
69
Binary vectors can represent features. A ‘one’ represents a feature at the
corresponding position of a binary vector; its absence is denoted by a ‘zero’.
The feature set A, B, C, D, E, F, G, H, I, J, K (deﬁned in the preceding sec-
tion) is represented by a binary vector of dimension 11, no distinction be-
tween categories and features is made. The presence of features C and E
is represented by the binary vector [0 0 1 0 1 0 0 0 0 0 0].
The associative memory represents the long-term memory of our de-
duction system in which the rules are stored. In the initialization phase
of the associative memory, no information is stored.
Because the infor-
mation is represented in the weights, they are all initially set to zero. In
the learning phase, binary vector pairs are associated. In the ﬁrst vector
⃗x we store the feature set; the category itself is indicated by the second
vector ⃗y. For example, the rule D ∧F then B is represented in the vectors
x = [0 0 0 1 0 1 0 0 0 0 0] and y = [0 1 0 0 0 0 0 0 0 0 0].
We demonstrate this procedure with the example that was introduced in
the section about the deduction systems (see Section 3.2.1). For clarity we
can replace the names of features and categories by symbols, each symbol
representing a name, for example B representing “oil lamp lights during
driving round a bend”:
(1) B ∨C then A
(2) D ∧F then B
(3) E ∧F then C
(4) H ∧F ∧I then G
(5) F ∧I then J
(6) K then F
In Figure 4.22 we see the associative memory after learning of the six
rules. The ‘or’ rules are indicated by a one in the threshold. For example,
the ‘or’ rule B ∨C then A is represented by the ﬁrst unit, and the rule D
∧F then B is represented by the second unit (see Figure 4.22).
After learning the categories can be determined by the inference with the
aid of associative memory. The present features represented by the question
vector ⃗x are presented to the associative memory and the categories are
identiﬁed by the retrieval rule which determines the answer vector ⃗y with
the aid of the following adapted retrieval rule:
yi = µ(zi)
(4.68)

70
Principles of Quantum Artiﬁcial Intelligence
K
J
I
H
G
F
E
D
C
B
A
B
C
D
E
F
G
H
I
J
K
  

  

  

  

  

  

  

  

  

  

  

  

  

1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10 11
A
Fig. 4.22
The architecture of our inference system is composed of associative memory
with feedback connections [Palm (1990); Palm et al. (1992)]. The rule B ∨C then A is
represented by the ﬁrst unit; that it is an ‘or’ rule is indicated by a one in its threshold
(represented in this Figure by a black dot). The associative memory forms the long-term
memory; the short-term memory that is initialized with the initial state description is
represented by the row buﬀer on the left side of the associative memory. The features of
the short-term memory are presented to the associative memory, which determines the
categories by using the retrieval rule. The determined categories are transported from
the buﬀer below the units (column buﬀer) via the feedback connections to the short-term
memory.
with
zi =
Pn
j=1 wijxj −1
Pn
j=1 wij
(4.69)
and
µ(zi) =



1 if
zi > 0
for or rules
zi = 1 for and rules
0 else
(4.70)
The short-term memory, which is initialized with the initial state de-
scription, is represented by the row buﬀer (short-term memory) on the

Information
71
(i)
I
H
G
F
E
D
C
B
C
D
E
F
G
H
I
J
K
K
J
  

  

  

  

  

  

  

   


  

   


  

  

  

1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10 11
A
B
1
A
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(ii)
I
H
G
F
E
D
C
B
C
D
E
F
G
H
I
J
K
K
J
  

  

  

  

  

  

  

   


  

   


  

  

  

1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10 11
A
B
1
A
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
(iii)
I
H
G
F
E
D
C
B
C
D
E
F
G
H
I
J
K
K
J
  

   


  

  

  

  

  

  

  

  

  

  

  

1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10 11
A
B
1
A
1
0
0
0
0
1
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
(iv)
I
H
G
F
E
D
C
B
C
D
E
F
G
H
I
J
K
K
J
  

   


  

  

  

  

  

  

  

  

  

  

  

1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10 11
A
B
1
A
1
0
0
0
0
1
0
1
0
0
1
0
1
0
0
1
0
0
0
0
0
Fig. 4.23
(i) The features ‘oil lamp lights up’ and ‘braking’ are represented in our
symbol notation by K and E respectively. The short-term memory is initialized with
/K, E/.
(ii) In the ﬁrst inference step, F is deduced.
The activation of the units is
indicated by the buﬀer below the units and should be not confused with the ‘or’ rule
indication of a one in the threshold (represented by a black dot). iii) The short-term
memory is now /K, E, F/. The features of the short-term memory are presented to the
associative memory, and C and F are deduced in the following inference. (iv) The short-
term memory is now updated to /K, E, F, C/. In the next inference step, C, F and A are
deduced. A is deduced, because A ⇒B ∨C is an ‘or’ rule as indicated by the threshold
value one of the corresponding unit (represented by a black dot
left side of the associative memory, see Figure 4.22. The features of the
short-term memory are presented to the associative memory, which deter-
mines the categories by using the retrieval rule to perform an inference step.
The determined categories are transported from the buﬀer below the units
(column buﬀer) via the feedback connections to the short-term memory.
The short-term memory is updated and the procedure is repeated until the
short-term memory does not change, i.e. the number of features in it does
not grow.

72
Principles of Quantum Artiﬁcial Intelligence
I
C
D
E
F
G
H
A
G
B
F
G
E
A
B
C
I
H
D
D
C
A
B
E
A
T
K
L
P
Module 1
Module 3
Module 2
Module 4
Module 5
A
2
1
9
1
8
2
3
5
6
4
7
5
7
1
2
3
4
8
9
6
2
3
4
5
1
2
1
3
4
5
Fig. 4.24
Representation of 16 rules arranged in ﬁve modules by two directed AND/OR
graphs. A name and a number in the context of a module indicate each feature. Each
module is represented by an associative memory. The number of a feature indicates the
position in the corresponding vector.
A disorder that is deﬁned in a certain module
can cause a manifestation in other modules. This relation corresponds to connections
between modules. The ‘and’ rules are indicated by an arc between the links connecting
the nodes, which indicate the manifestations.
The features ‘oil lamp lights up’ and ‘braking’ are present (represented
in our symbol notation by K and E). The short-term memory is initialized
with /K, E/ (see Figure 4.23 (i)). In the ﬁrst inference step, F is deduced;
the short-term memory is now /K, E, F/ (see Figure 4.23 (ii)). The features
of the short-term memory are presented to the associative memory and, in
the following inference, C and F are deduced (see Figure 4.23 (iii)). The
short-term memory is then updated to /K, E, F, C/. In the next inference
steps, C, F and A are deduced. A is deduced, because A ⇒B ∨C is an
‘or’ rule as indicated by the threshold value of the corresponding unit (see
Figure 4.23 (iv)). The inference procedure is completed because no new

Information
73
features are determined in the following inference step.
In our example
the category A would correspond to “the cable of the oil pressure lamp is
loose.”
The representation of directed AND/OR graphs by associative memory
with feedback are summarized in the following points:
• The inference in the AND/OR graphs corresponds to breadth-ﬁrst
search.
• The number of cycles performed by the associative memory with
the feedback correspond to the maximum depth of the represented
graphs.
• The directed AND/OR graph can contain cycles, because only new,
not yet present disorders are added to the short term memory, and the
feedback is done until the number of features in the short term memory
does not grow.
1
2
3
4
5
6
7
8
9
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
1 2 3 4 5 6 7 8 9
1 2
1
2
1 2 3 4
1
2
3
4
5
5
1 2 3 4 5
1
2
3
4
5
Module 1
Module 2
Module 4
Module 5
Module 3
Fig. 4.25
The ﬁve modules are represented by ﬁve associative memories. By arranging
them diagonal, we compose a global associative memory.
An associative memory of
dimension 30 evolves. In this global context, connections between modules can be easily
indicated by weights depicted outside the modules.
Connection from the ﬁrst to the
second module is indicated by the weights in the column of the ﬁrst module and the row
of the second module. In this Figure we can see three connections between the modules.

74
Principles of Quantum Artiﬁcial Intelligence
• The number of represented disorders is correlated with the size of the
associative memory; this number is only limited by the size of the
expandable associative memory.
4.6.1
Taxonomic knowledge organization
For clarity, rules should be arranged in groups [Aikins (1986); Kahn et al.
(1987)] that deﬁne taxonomy.
A module could represent a group, each
feature is indicated by a name in the context of a module and each module
is represented by an associative memory [Wichert (2012)] (see Figure 4.24).
Each module (see Figure 4.24) could correspond to a diﬀerent area of
the human brain. How can we link this diﬀerent areas of the brain? For
example, 5 modules would be represented by 5 associative memories. From
the 5 associative memories, we can compose a global associative memory by
the arrangement on the diagonal. An associative memory of the dimension
30 evolves in which local feature addresses are translated into global feature
addresses (see Figure 4.25). In this global context, connections between
modules can be easily indicated by weights outside the modules, in the
column of the ﬁrst module and the row to the second module (connection
from the ﬁrst module to the second module). In Figure 4.25 we can see three
connections between the modules corresponding to our rule base [Wichert
(2012)].

Chapter 5
Reversible Algorithms
5.1
Reversible Computation
Energy is negligible when using the Turing machine model, because the
Turing machine itself constitutes an inﬁnitely long tape and does not re-
quire any energy resources. However, what is the relation between energy
and information that is processed by a Turing machine? In the thermody-
namic Maxwell paradox, there is a demon that observes the gas molecules.
Between the chambers, there is a small door. The demon can open and close
the small door, passing only one molecule into a chamber. The negative of
the demon’s entropy increments as the measure of the quantity of the infor-
mation that it has used. Information increments when entropy decrements.
The information must be stored in a demon’s memory. Given the fact that
the memory is ﬁnite, the demon must erase some information. The erasing
of information increases the entropy represented by energy. Bennett (1973)
showed [Bennett (1973)] that irreversible overwriting of one bit causes at
least k · T · log2 joules of energy dissipation, where k is Boltzmann’s con-
stant and T is the absolute temperature. Bennett also indicated that this
lower bound can be ignored when using reversible computation. Reversible
computing is a model of computing in which the computational process is
reversible. For example, a NOT gate is reversible in the sense that one
can infer the output from the input. However, neither the AND or OR
gates are reversible in the sense that one cannot infer the output from the
input. For example, (1 AND 0) = (0 AND 1) = (0 AND 0) = 0 [Ben-
nett (1982)]. Reversibility is obtained by Bennett by the storage of all of
the computational steps. A three-tape Turing machine is used, with an
input tape, history tape and output tape. The computation is performed
on the input tape, and all steps are stored on the history tape. Without
75

76
Principles of Quantum Artiﬁcial Intelligence
the history tape, the computational history would be forgotten. When the
computation stops, the results are copied to the output tape, and the com-
putation is run backward to erase the history tape for feature usage. Any
Turing machine can be simulated by a reversible Turing machine [Bennett
(1988)], [Bennett (1989)]. For a reversible production system, the sequence
of productions at each step must be remembered. This requirement is usu-
ally met for any production system. A computation of a production system
is speciﬁed by the initial and the goal state, the set of productions. The
solution is represented by the sequence of productions at each step.
5.2
Reversible Circuits
5.2.1
Boolean gates
The sets of truth functions can be computed using Boolean circuits, which
are composed of Boolean gates. An algorithm can be described by such a
circuit; however, the AND and OR gates are not reversible; each one of
them erases one bit and generates approximately k·T ·log2 joules of energy
dissipation [Landauer (1961)], [Landauer (1992)]. During the computation,
information is lost; however, at the same time, the entropy grows [Landauer
(1992)], [Bennett (2003)]. For this reason, processors generate waste heat
and must be cooled to keep them within permissible operating temperature
limits.
A solution to the generation of waste heat is reversible Boolean
gates. However, reversible Boolean gates require additional waste bits.
5.2.2
Reversible Boolean gates
To make a circuit reversible, we must make each of the gates reversible.
A necessary condition for a reversible gate is that of a bijective transition
function with m inputs and m outputs. No injective transition function
can be used, if n is the input and m the output; if n > m, then some
information is lost. Such a bijective function is a permutation of m inputs
and m outputs. For m bits, there are c = 2m diﬀerent combinations. For
c diﬀerent combinations, there are c! possible permutations between the
input and output. A reversible gate is such a permutation. For the NOT
gate, there are two possible combinations of one bit, c = 21. Either the
bit is 1 or 0. There are also two possible permutations, which means two
diﬀerent gates, which are both reversible. One gate is the identity gate,
and the other gate is the NOT gate. Both of them are reversible. For the

Reversible Algorithms
77
Table 5.1
Truth table of the Toﬀoli
gate.
inputs
outputs
dec.
x1, x2, x3
y1, y2, y3
dec.
0
0,0,0
0,0,0
0
1
0,0,1
0,0,1
1
2
0,1,0
0,1,0
2
3
0,1,1
0,1,1
3
4
1,0,0
1,0,0
4
5
1,0,1
1,0,1
5
6
1,1,0
1,1,1
7
7
1,1,1
1,1,0
6
identity gate, the input is equal to the output. For gates with two bits,
there are altogether four combinations of two bits, c = 22 and 24 = 4!
possible permutations, which means 24 diﬀerent gates. However, none of
the 24 gates solves the AND or OR problem.
5.2.3
Toﬀoli gate
The AND gate can be only computed by a reversible gate, which operates
on three bits. On there bits, c = 23 and 40320 = 8! possible permutations.
One of the possible permutations corresponds to the truth table of the
Toﬀoli gate [Toﬀoli (1980a)], [Toﬀoli (1980b)], [Toﬀoli (1980c)] Each value
of Table 5.1 corresponds to three input bits x1, x2, x3 and three output bits
y1, y2, y3. The permutation is deﬁned by the exchange of the values 1, 1, 0
with 1, 1, 1 (decimal 6, 7).
The Toﬀoli gate does not change the ﬁrst input bits x1 and x2. The op-
eration is described by the following mapping on three input bits x1, x2, x3
with B = {0, 1}
T : B3 →B3 :, T (x1, x2, x3) = (x1, x2, (x1 ∧x2) ⊕x3)
A Toﬀoli gate is a universal reversible gate it performs following operations.
• It computes the AND operation, the ancilla (ﬁxed) bit x3 is set to 0
T : B3 →B3 :, T (x1, x2, 0) = (x1, x2, x1 ∧x2)
• It computes the NOT operation on x3
T : B3 →B3 :, T (1, 1, x3) = (1, 1, ¬x3)
• It computes the NAND operation, the ancilla (ﬁxed) bit x3 is set to 1
T : B3 →B3 :, T (x1, x2, 1) = (x1, x2, ¬(x1 ∧x2)

78
Principles of Quantum Artiﬁcial Intelligence
• It computes the FANOUT operation (the value of bit x2 is copied into
x3)
T : B3 →B3 :, T (1, x2, 0) = (1, x2, x2)
The OR operation follows from the De Morgan’s laws
x1 ∨x2 = ¬(¬x1∧, ¬x2)
Because NAND and FANOUT are together universal, we can implement
any reversible circuit using the Toﬀoli gate.
5.2.4
Circuit
A reversible circuit can be built using Toﬀoli gates and NOT gates. This
construct represents a permutation on m bits, deﬁning an injective mapping
Bm →Bm. Out of m bits, several bits act as control bits (ancilla bits) and
others are not changed during the computation. For each AND operation,
one ancilla bit is required.
A reversible circuit performs a permutation
of the input bits. The output is a permutation of the input bits. Each
reversible circuit can be represented by a permutation matrix. As stated
before, a circuit is not an algorithmic device; by itself, it does not correspond
to a universal reversible Turing machine. This arrangement also applies to
any reversible circuit.

Chapter 6
Probability
6.1
Kolmogorovs Probabilities
Probability theory is built around Kolmogorov’s axioms (ﬁrst published in
1933, [Kolmogorov (1933)]). All probabilities are between 0 and 1. For any
proposition a,
0 ≤P(a) ≤1
and
P(true) = 1,
P(false) = 0.
To each sentence, a numerical degree of belief between 0 and 1 is assigned,
which provides a way of summarizing the uncertainty.
The last axiom
expresses the probability of disjunction and is given by
P(a ∨b) = P(a) + P(b) −P(a ∧b)
Where do these numerical degrees of belief come from?
• Humans can believe in a subjective viewpoint, which can be determined
by some empirical psychological experiments. This approach is a very
subjective way to determine the numerical degree of belief.
• A more objective method results from physical experiments or from
some databases describing market behavior: for any ﬁnite sample, we
can estimate the true fraction and also calculate how accurate our esti-
mation is likely to be. By using samples, as in most physical measure-
ments, we estimate the values. This approach is called frequentist. We
approach the true value by counting the frequency of an event but do
not reach the true value because we cannot access the whole population
of events.
79

80
Principles of Quantum Artiﬁcial Intelligence
• It appears that the true values can be determined from the true nature
of the universe, for example, for a fair coin, the probability of heads is
0.5. This approach is related to the Platonic world of ideas. However,
we can never verify whether a fair coin exists. To accomplish such a
veriﬁcation, we would have to follow the frequentist approach.
6.1.1
Conditional probability
The degree of belief P(a) is attached to a sentence a before any evidence
about the nature of the sentence is obtained; we call this probability the
prior (before) probability. Arising from the frequentist approach, one can
determine the probability of an event a by counting. If Ωis the set of all
possible events, P(Ω) = 1, then a ∈Ω. The cardinality determines the
number of elements of a set, card(Ω) is the number of elements of the set
Ω, card(a) is the number of elements of the set a and
P(a) = card(a)
card(Ω).
(6.1)
Now we can deﬁne the posterior probability, the probability of a after the
evidence b is obtained
P(a|b) = card(a ∧b)
card(b)
.
(6.2)
The posterior probability is also called the conditional probability. From
P(a ∧b) = card(a ∧b)
card(Ω)
(6.3)
and
P(b) = card(b)
card(Ω)
(6.4)
we get
P(a|b) = P(a ∧b)
P(b)
(6.5)
and
P(b|a) = P(a ∧b)
P(a)
.
(6.6)

Probability
81
6.1.2
Bayes’s rule
The Bayes’s rule follows from both equations
P(b|a) = P(a|b) · P(b)
P(a)
.
(6.7)
For mutually exclusive events b1, ..., bn with
n
X
i=1
P(bi) = 1
(6.8)
the law of total probability is represented by
P(a) =
n
X
i=1
P(a) ∧P(bi),
(6.9)
P(a) =
n
X
i=1
P(a|bi) · P(bi).
(6.10)
Bayes rule can be used to determine the prior total probability P(h) of
hypothesis h to given data D.
• P(D|h) is the probability that a hypothesis h generates the data D.
P(D|h) can be easily estimated. For example, what is the probability
that some illness generates some symptoms?
• The probability that an illness is present given certain symptoms, can
be then determined by the Bayes rule
P(h|D) = P(D|h) · P(h)
P(D)
.
(6.11)
The most probable hypothesis hi out of a set of possible hypothesis
h1, h2, · · · given some present data is according to the Bayes rule
P(hi|D) = P(D|hi) · P(hi)
P(D)
.
(6.12)
To determine the maximum posteriori hypothesis hMAP we maximize
hmap = argmaxhi
P(D|hi) · P(hi)
P(D)
.
(6.13)
The maximization is independent of P(D), it follows
hmap = argmaxhiP(D|hi) · P(hi).
(6.14)
Given the scores x and y
x = P(D|h) · P(h),
z = P(D|¬h) · P(¬h)
the probabilities P(h|D) and P(¬h|D) can be determined by normalization,
talking into account 1 = P(h|D) + P(¬h|D).

82
Principles of Quantum Artiﬁcial Intelligence
6.1.3
Joint distribution
The joint distribution for n possible variables is described by 2n possible
combinations. The probability distribution d1 × d2 × · · · × dn corresponds
to a vector of length 2n. For a joint distribution of n possible variables,
the exponential growth of combinations being true or false becomes an
intractable problem for large n. For
P(hi|d1, d2, d3, .., dn) = P(d1, d2, d3, ..., dn|hi) · P(hi)
P(d1, d2, d3, ..., dn)
(6.15)
all 2n −1 possible combinations must be known. There are two possible
solutions to this problem.
• The ﬁrst solution is the decomposition of large probabilistic domains
into weakly connected subsets via conditional independence,
P(d1, d2, d3, ..., dn|hi) =
n
Y
j=1
P(dj|hi).
(6.16)
This approach is known as the Na¨ıve Bayes assumption and is one
of the most important developments in the recent history of Artiﬁcial
Intelligence. It assumes that a single cause directly inﬂuences a number
of events, all of which are conditionally independent,
hmap = argmaxhi
n
Y
j=1
P(dj|hi) · P(hi).
(6.17)
However, this conditional independence is very restrictive. Often, it
is not present in real life events. Dependence between some events is
always present.
• Bayesian networks represent the second and more realistic solution.
Bayesian networks can describe a probability distribution of a set of
variables by combining conditional independence assumptions with con-
ditional probabilities. Unlike the Na¨ıve Bayes assumption, which states
that all of the variables are conditionally independent given the value
of the target variable, Bayesian networks enable these conditional inde-
pendence assumptions to be applied to subsets of variables, providing
a model with fewer constraints than the Bayes assumption [Mitchell
(1997)].
6.1.3.1
Example
Cancer screening aims to detect cancer before symptoms appear. This may
involve for example a blood tests. Suppose a result of one secure test is

Probability
83
positive [Mitchell (1997)]. The test is secure because in 99 percent of the
cases the test returns a correct positive result (= p) in which a rare form of
cancer is actually present. Should the doctor tell the patient, that he has
cancer?
He should not do it. It is quite probable that a false positive error occurs.
A consequence of the Bayesian inference is that such false positive errors
occur when the prior probability is very low. In our case it is the rare form
of cancer. The test has correct negative result (= n) in 99 percent of the
cases when rare form of cancer is not present. It is also known that 0.001
of the entire population have rare form of cancer (= c).
P(c) = 0.001,
P(¬c) = 0.999,
P(p|c) = 0.99,
P(n|c) = 0.01,
P(p|¬c) = 0.01,
P(n|¬c) = 0.99.
We determine hmap,
P(c|p) = α · P(p|c) · P(c) = α · 0.99 · 0.001 = α · 0.00099,
(6.18)
P(¬c|p) = α · P(p|¬c) · P(¬c) = α · 0.01 · 0.999 = α · 0.00999,
(6.19)
it follows
hmap = ¬c.
By normalization we get the actual probabilities
P(c|p) =
0.00099
0.00099 + 0.00999 = 0.0901639,
and
P(¬c|p) =
0.00999
0.00099 + 0.00999 = 0.909836.
However if we repeat the test again according to the the Na¨ıve Bayes as-
sumption
P(c|p) = α · P(p|c) · P(c) = α · 0.99 · 0.99 · 0.001 = α · 0.00098,
(6.20)
P(¬c|p) = α·P(p|¬c)·P(¬c) = α·0.01·0.01·0.999 = α·0.0000999, (6.21)
hmap = c.
To rule out false positive error the doctor has to repeat the test.

84
Principles of Quantum Artiﬁcial Intelligence
6.1.4
Na¨ıve Bayes and counting
The Na¨ıve Bayes approach is related to simple counting if we follow the
frequentist approach. For maximum a posteriori hypothesis hmap
hmap = argmaxhi
n
Y
j=1
P(dj|hi) · P(hi)
(6.22)
Ωis a set of all possible events
P(hi) = card(hi)
card(Ω)
(6.23)
and
P(dj|hi) = card(dj ∧hi)
card(hi)
,
(6.24)
hmap = argmaxhi
n
Y
j=1
card(dj ∧hi)
card(hi)
· card(hi)
card(Ω) .
(6.25)
Because Ωis a set of all possible events it does not play a role in the process
maximization of hmap
hmap = argmaxhi
n
Y
j=1
card(dj ∧hi)
(6.26)
we can apply log
hmap = argmaxhi log


n
Y
j=1
card(dj ∧hi)

,
(6.27)
hmap = argmaxhi
n
X
j=1
log (card(dj ∧hi)) .
(6.28)
For the process of maximization hmap we can simply write
hmap = argmaxhi
n
X
j=1
card(dj ∧hi).
(6.29)
The result is related to categorial representation based the contrast model
of Tversky [Tversky (1977)].

Probability
85
6.1.5
Counting and categorization
An object is judged to belong to a verbal to the extent that its features are
predicted by the verbal category [Osherson (1987)]. The sets of prototypical
features deﬁnes a category. If Ca is a category and B the features set, so
only Ca ∩B features are considered [Wichert (1998)], [Wichert (2000)].
Then
card(Ca ∧B) = |Ca ∩B|,
(6.30)
we normalize the result, in our case we normalize to the interval [−1, 1] and
deﬁne the Sim(Ca, B) function as
Sim(Ca, B) =
2
|Ca| · |Ca ∩B| −1
∈[−1, 1]
(6.31)
|Ca| is the number of the prototypical features that deﬁne the category Ca.
This function corresponds to the simpliﬁed normalized contrast model of
Tversky [Tversky (1977)]
Sim(Ca, B) = α|Ca ∩B| −β|Ca −B|
(6.32)
in which only Ca ∩B features are considered
Sim(Ca, B) = α|Ca ∩B| −β(|Ca −(Ca ∩B)|
(6.33)
it is supposed that the similarity value should be from the interval [−1, 1]
so α = 1/|Ca| and β = 1/|Ca| and
Sim(Ca, B) = |Ca ∩B|
|Ca|
−|(Ca −(Ca ∩B)|
|Ca|
.
(6.34)
The present features are counted and normalized so that the value can be
compared. For example, the category bird is deﬁned by the following fea-
tures: ﬂies, sings, lays eggs, nests in trees, eats insects. The category bat
is deﬁned by the following features: ﬂies, gives milk, eat insects. The fol-
lowing features are present: ﬂies and gives milk.
Simc(bird, presentfeatures) = 1
5 −4
5 = 2
5 · 1 −1 = −3
5,
(6.35)
Simc(bat, presentfeatures) = 2
3 −1
3 = 2
3 · 2 −1 = 1
3.
(6.36)
Features that discriminate among relevant facts should have a higher
salience than those that do not [Smith (1995)]. The features of an equal
salience have a unary representation, they can only be represented as ex-
istent or nonexistent. A category that is described as a set of features can

86
Principles of Quantum Artiﬁcial Intelligence
be present with diﬀerent grades of vagueness corresponding to the cardinal
number of the set. A set of features that describes a category can be some-
times divided into subsets that represent some subcategories. Each feature
can be also regarded as a kind of subcategory. If this subcategory cannot
be described by other features, but, nevertheless, should have the proper-
ties of variable salience and vagueness, it is described by invisible features.
To each feature a number of invisible features is assigned dependent on its
salience. An example of two old sayings from country folklore:
(1) If it is April and it snows much then probably the apple harvest will
be bad.
(2) If it is April and it rains a lot and it is very cold then the vintage will
be good.
The number of invisible features as determined by the observer:
• April is represented by one invisible feature, as it can be present or
absent.
• Snows much is described by two invisible features because the ob-
server thinks that it has a higher salience than April. It can be either
present, maybe present, or absent.
• The observer thinks that rains a lot has the same salience as snows
much.
• The observer thinks that very cold has the greatest salience, as it is
described by three invisible features. It can be either present, maybe
present, maybe absent or absent.
With this approach hierarchical categorization can be preformed in anal-
ogy to the Na¨ıve Bayes approach. Categories can be divided into subcat-
egories, so that a taxonomy can be constructed and represented by an
acyclic graph. The nodes in this graph correspond to categories and the
links indicate the “is a subcategory” relation between them. The process
of the hierarchical categorization can be performed by moving from more
general categories to more speciﬁc categories until the desired categories
are reached.
Several expert systems were build based on this approach
[Wichert (2000)], [Wichert (2004)].
6.1.6
Bayesian networks
Bayesian networks also provide a natural representation for (causally in-
duced) conditional independence. Bayesian networks also called belief net-

Probability
87
works because they represent our beliefs. They represent a set of condi-
tional independence assumptions, by the topology of an acyclic directed
graph and sets of conditional probabilities. In the network each variable
is represented by a node and the links between them represents the con-
ditional independence of the variable towards its non descendants and its
immediate predecessors. Bayesian networks represent for each variable a
conditional probability table which describes the probability distribution
of a speciﬁc variable given the values of its immediate predecessors.
A
conditional distribution for each node xi given its parents is
P(xi|Parent1(xi), Parent2(xi), .., Parentk(xi))
with k usually between 1 and 4 for nodes xi. Full joint distribution is given
by
P(x1, .., xn) =
n
Y
i=1
P(xi|Parent1(xi), Parent2(xi), .., Parentk(xi)). (6.37)
Given the x query variable which value has to be determined and e evidence
variable which is known and the remaining unobservable variables we pre-
form a summation over all possible y (all possible values of the unobservable
variables y according to the law of total probability)
P(x|e) = α
X
y
P(x, e, y).
(6.38)
The values P(x|e), P(¬x|e) can be determined by normalization
1 = α ·
 X
y
P(x, e, y) +
X
y
P(¬x, e, y)
!
.
Cooper [Cooper and Herskovits (1990)] has found that the exact inference
of probabilities is a NP −hard problem.
6.1.6.1
Example
The network topology reﬂects our belief in the associated causal knowledge.
Consider the well-known example of Judea Perl [Pearl (1989)], [Russell and
Norvig (2010)]. I am at work in Los Angeles, and neighbor John calls to
say that the alarm of my house is ringing, but neighbor Mary does not
call. Sometimes minor earthquakes set oﬀthe alarm. Is there a burglary?
Constructing a Bayesian network is diﬃcult because each variable should
be directly inﬂuenced by only a few other variables. In the example, there
are ﬁve variables, namely, Burglary(= b), Earthquake(= e), Alarm(= a),
JohnCalls(= j), and MaryCalls(= m). The corresponding network topol-
ogy is indicated in Figure 6.1 and reﬂects the following “causal” knowledge:

88
Principles of Quantum Artiﬁcial Intelligence
• A burglar can set the alarm oﬀ.
• An earthquake can set the alarm oﬀ.
• The alarm can cause Mary to call.
• The alarm can cause John to call.
Fig. 6.1
A Bayesian network and the corresponding conditional probability tables. Each
row requires one number p for P (X) = true (P (X) = false is 1 −p).
If all values are observed, for example we know that
j = true, m = true, a = true, b = false, e = false
then
P(j ∧m ∧a ∧¬b ∧¬e) = P(j, m, a, ¬b, ¬e)
P(j, m, a, ¬b, ¬e) = P(j|a) · P(m|a) · P(a|¬b ∧¬e) · P(¬b) · P(¬e)
P(j, m, a, ¬b, ¬e) = 0.9 · 0.7 · 0.001 · 0.999 · 0.998 = 0.00062.
There are three types of variables: x query variable, e evidence variables
and unobservable variables y.
An unobservable variable y is irrelevant
unless y is ancestor of x or e. We want to determine the probability of
Alarm if Burglary , Earthquake, JohnCalls, MaryCalls are unknown. In
this example JohnCalls and MaryCalls are irrelevant [Russell and Norvig
(2010)]. We want to determine the probability of
P(A|b, e, j, m) = α
X
b
X
e
X
j
X
m
P(A, b, e, j, m).
(6.39)

Probability
89
Small letters indicate instantiationed variables,
P(A|b, e, j, m)
= α
X
b
X
e

P(A|b, e) · P(b) · P(e) ·
X
j
P(j|A) ·
X
m
P(m|A)


(6.40)
JohnCalls and MaryCalls are irrelevant, because the sum over j and m
are oneX
j
P(j|a) =
X
j
P(j|¬a) =
X
m
P(m|a) =
X
m
P(m|¬a) = 1,
for example
X
j
P(j|a) = 0.9 + 0.1 = 1.
It follows that
P(A|b, e, j, m) = α ·
X
b
X
e
P(A|b, e) · P(b) · P(e) · 1 · 1
(6.41)
and
P(A|b, e, j, m)
= α ·
X
b
(P(A|b, e) · P(b) · P(¬e) + P(A|b, ¬e) · P(b) · P(¬e))
(6.42)
P(A|b, e, j, m) = α · (P(A|b, e) · P(b) · P(¬e) + P(A|b, ¬e) · P(b) · P(¬e)
+ P(A|¬b, e) · P(¬b) · P(¬e) + P(A|¬b, ¬e) · P(¬b) · P(¬e))
(6.43)
with the values from conditional probability table
P(a|b, e, j, m) = α(0.95 · 0.001 · 0.002 + 0.94 · 0.01 · 0.998
+ 0.29 · 0.999 · 0.002 + 0.001 · 0.999 · 0.998)
and
P(¬a|b, e, j, m) = α(0.05 · 0.001 · 0.002 + 0.06 · 0.01 · 0.998
+ 0.71 · 0.999 · 0.002 + 0.999 · 0.999 · 0.998),
P(a|b, e, j, m) = α · 0.010960,
P(¬a|b, e, j, m) = alpha · 0.99802, (6.44)
α =
1
0.010960 + 0.99802.
(6.45)
The probability of Alarm being present or not is
P(a|b, e, j, m) = 0.010862,
P(¬a|b, e, j, m) = 0.98914.
(6.46)

90
Principles of Quantum Artiﬁcial Intelligence
6.2
Mixed Distribution
Suppose that the variable d1 represents whether a person likes mathematics
or not. The truth-values are represented by 0 and 1, so d1 = 0 means that
the person does not like mathematics.
The next variable d2 represents
whether the person studies philosophy or not, and d3 represents if the
person knows how to play chess or not. If we introduce positional notation
for the variables, then d1, d2, d3 = 101 would mean that the person likes
mathematics and does not study philosophy and knows how to play chess.
The probability of P(d1 = 1, d2 = 0, d3 = 1) is written in short as p101. In
this case, the distribution d1 × d2 × d3 is represented by a vector of length
8 [Rieﬀel and Polak (2011)]:
⃗p = (p000, p001, p010, p011, p100, p101, p110, p111).
(6.47)
Suppose that we do not know anything about a person; in this case, the
probability distribution is a mixed distribution and is represented by the
vector
⃗p = (1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8).
The vector p represents the mixed distribution on uncertainty before know-
ing the facts about the person. Every possible combination has the same
probability. Before we know the facts, the distribution is mixed, with 23
states. After learning the fact that the person studies philosophy, d2 = 1,
we know for certain that four possible combinations are not present, namely
those combinations in which d2 = 0, which are p000, p001, p100 and p101. The
mixed distribution represented by the vector ⃗p must be re-normalized,
⃗p = (0, 0, 1/4, 1/4, 0, 0, 1/4, 1/4).
After learning the fact that the person does like mathematics, d1 = 1, the
mixed distribution represented by the vector ⃗p, must be re-normalized again
⃗p = (0, 0, 0, 0, 0, 0, 1/2, 1/2).
Finally, after learning that the person plays chess d3 = 1, the mixed distri-
bution becomes pure; in this case, there is no uncertainty that the vector
⃗p is a unary vector with a one at the position that corresponds to p111
⃗p = (0, 0, 0, 0, 0, 0, 0, 1).

Probability
91
6.3
Markov Chains
A physical system is described by the state of the system. The state of the
system can be represented by a vector ⃗x. A single object can be described
in classical mechanics by its position and momentum. Momentum is the
product of the mass and velocity of an object. The corresponding vector
is ⃗x = (x1, x2, x3, p1, p2, p3) ∈R6, where (x1, x2, x3) describes the posi-
tion and (p1, p2, p3) describes the momentum of the object. As the object
moves, the state of the system changes over time ⃗x(t). Classical mechan-
ics describes the time evolution of the state by the Hamiltonian equation
of motion represented by diﬀerential equations [Hirvensalo (2004)], [Ross
(2009)].
If we do not know the states of the system, we can attempt to describe
the probability distribution of the system. We know that a system is in
states x1, ..., xn with probabilities p1, ...., pn and that they sum up to 1.
The probability distribution over state xi is represented by the mixed state
p1[x1] + p2[x2] + ... + pn[xn].
(6.48)
Tossing a fair coin with a head and a tail is represented by the mixed state
0.5[h] + 0.5[t].
The time evolution of a system is a non-deterministic procedure. It develops
each state xi into a distribution
p1i[x1] + p2i[x2] + ... + pni[xn]
where pij is the probability that the system state xi evolves into xj. A
simpliﬁcation can be reached by using discrete time. Another simpliﬁcation
is the one-level dependence of the state, which is also called the Markov
property. Each state is dependent on the preceding state. The discrete
probabilistic time evolution represents a Markov chain that can be described
by linear mapping. Transition probabilities between states are represented
by conditional probabilities
pij = P(xj(t + 1)|xi(t)).
(6.49)
We represent
p1[x1] + p2[x2] + ... + pn[xn]
by a stochastic vector
⃗p = (p1, p2, .., pn).

92
Principles of Quantum Artiﬁcial Intelligence
For a state xi the mapping is represented by
p′
i = pi1p1 + pi2p2 + ... + pinipn.
(6.50)
The vector is deﬁned by an n × n stochastic matrix P = [pij] that is also
called a Markov matrix or a stochastic matrix, and it has the following
properties:
(i) pij ≥0 for 1 ≤i, j ≤n;
(ii)
n
X
i=1
pij = 1 for 1 ≤j ≤n.
The probabilities are related by the linear mapping using the stochastic
matrix P with





p′
1
p′
2
...
p′
n




=





p11 p12 · · · p1n
p21 p22 · · · p2n
...
...
...
...
pn1 pn2 · · · pnn




·





p1
p2
...
pn




.
(6.51)
Because all the columns sum to one we are guaranteed that
p′
1 + p′
2 + .. + p′
n = p1 + p2 + .. + pn.
(6.52)
A Markov chain is a mathematical system that undergoes transitions that
are described by a stochastic matrix that moves from one stochastic vector
to another [Markov (1954)]. The asymptotic behavior of Markov chains
either converges to a ﬁxed distribution or goes to a periodic sequence of
distributions.
Given that all of the entries of the stochastic matrix are
larger than zero, which means that each state can be reached from another
state, there is a ﬁxed distribution ⃗q.
The distribution ⃗q is independent
of the initial distribution ⃗p. A ﬁxed distribution is unchanged when it is
transformed by P. The ﬁxed distribution ⃗q is represented by the eigenvector
of P, with the eigenvalue equal to 1.





q1
q2
...
qn




=





p11 p12 · · · p1n
p21 p22 · · · p2n
...
...
...
...
pn1 pn2 · · · pnn




·





q1
q2
...
qn





(6.53)
because ⃗q is a probability vector, it follows that
n
X
i=1
qi = 1.

Probability
93
P describes the transition of the states of the weather, such as sunny, cloudy
and rainy (see Figure 6.2). Each state of the weather can be reached from
another state, and the prediction over several days is increasingly inaccu-
rate. The prediction tends toward a steady state vector that represents the
probabilities of sunny, cloudy and rainy, independent of the initial weather
state. The information about the initial weather is lost and cannot be re-
constructed. The evolution of a stochastic matrix does not lead to a loss of
Fig. 6.2
Transition of for the states of the weather, such as sunny, cloudy and rainy
represented by the symbol “sun”,“cloud” and “rain”. Each state of the weather can be
reached from another state.
information if the matrix is orthogonal. An example orthogonal stochastic
matrix is the permutation matrix. A permutation matrix is a square binary
matrix that has exactly one entry 1 in each row and each column and is 0
elsewhere. As a consequence, each state cannot be reached from another
state.

This page intentionally left blank
This page intentionally left blank

Chapter 7
Introduction to Quantum Physics
7.1
Unitary Evolution
Physical measurements have an intrinsically probabilistic character. Rep-
etitions of an observation using the same experimental apparatus with
the same initial conditions will generally yield diﬀerent measurements of
the observed variable. Statistical laws govern the totality of large obser-
vations.
An object can be described in classical mechanics by a vector
⃗x = (x1, x2, x3, p1, p2, p3) ∈R6, which describes its position (x1, x2, x3)
and its momentum (p1, p2, p3). The changes in the position and the mo-
mentum of the object over time are described by the Hamiltonian equation
of motion
d
dtxi =
∂
∂pi
H, d
dtpi =
∂
∂xi
H.
(7.1)
The state of the object is described by the Hamiltonian function
H = H(x1, x2, x3, p1, p2, p3).
The wavefunction in quantum mechanics, if unobservable, evolves in a
smooth and continuous way according to the Schr¨odinger equation, which
is related to the Hamiltonian equation of motion. This equation describes
a linear superposition of diﬀerent states at time t, which is represented by
the vector x(t),
i · h · d
dtx(t) = H · x(t)
(7.2)
with i = √−1 and h being the
Planck’s constant. For simpliﬁcation we
set h = 1. H is the Hamiltonian operator, which is related to the total
energy of the system. A general solution of the Schroedinger equation (for
the time-independent Hamiltonian) represents the unitary evolution with
x(t) = e−i·t·H · x(0) = Ut · x(0)
(7.3)
95

96
Principles of Quantum Artiﬁcial Intelligence
where Ut = e−i·t·H is the evolutionary operator, which can be represented
by a unitary matrix. Unitary evolution is deterministic and reversible. The
vector x(t) describes the probability of the presence of certain states. A
dimension represents each state, and the value of the vector is related to
the probability of the state being present. However, measurements always
ﬁnd the physical system to be in a deﬁnite state, which does something
to the wavefunction represented by the vector x(t). This something is not
explained by quantum theory.
7.1.1
Schr¨odinger’s cat paradox
The best known example of this type kind of this ‘something’ is the
Schr¨odinger’s cat paradox [Schr¨odinger (1935)].
“When I hear about
Schr¨odinger’s cat, I reach for my gun,” is a quote from Stephen Hawking.
In our description of the paradox we will replace the cat with a rabbit...
A rabbit is apparently evolving into a superposition of two states that can
be characterized as an alive rabbit and a dead rabbit. A Geiger counter
measures the decay of a radioactive substance.
There is a ﬁfty percent
chance that, in a given time frame, decay is measured. The Geiger counter
is connected to a device that kills the rabbit, if decay is measured. Because
the rabbit and the Geiger counter are in a closed room, we do not know
whether the rabbit is dead or alive. Each of these possibilities is associated
with a speciﬁc ﬁfty percent probability. The rabbit is in a mixed state,
and the two states are “really” present at the same time. A measurement
always ﬁnds either an alive rabbit or a dead rabbit with a probability of
ﬁfty percent.
7.1.2
Interpretations of quantum mechanics
As long as we make no measurements, there are no random eﬀects. The
behavior of the system is strictly deterministic. The randomness is only
present during the measurement. Randomness is an eﬀect of measurement.
Diﬀerent interpretations of quantum mechanics propose diﬀerent solutions
of the measurement problem. We present the two most inﬂuential explana-
tions:
• The most popular interpretation, the Copenhagen interpretation,
claims that quantum mechanics is a mathematical tool that is used

Introduction to Quantum Physics
97
in the calculation of probabilities and has no physical existence; all
other questions are metaphysical. This popular unscientiﬁc interpreta-
tion delayed for many years the development of quantum computation
[Heisenberg (1949)].
• The many-worlds theory led to the development of the ﬁrst quantum
algorithms; this approach is less popular due to some philosophical diﬃ-
culties. The many-worlds theory views reality as a many-branched tree
in which every possible quantum outcome is realized [Everett (1959)],
[Byrne (2007)]. The subjective appearance of the wave function col-
lapse is explained by the mechanism of quantum decoherence. Every
possible outcome to every event exists in its own world. In one world,
randomness exists, but not in the universe (multiverse) that describes
all possible worlds [Deutsch (1997)].
7.2
Quantum Mechanics
A bit can be represented by the state of a simple 2-state quantum system
such as the spin state of a particle. When measured, the spin is always in
one of two possible states: spin-up or spin-down. A quantum mechanical
description of a physical system is related to a probabilistic representation;
it is described by a vector in Hilbert space. This description extends the
two- or three-dimensional Euclidean space into spaces that have any ﬁnite
or inﬁnite number of dimensions. In such a space, the Euclidean norm is
induced by the inner product
∥x∥=
p
⟨x|x⟩.
(7.4)
A basis of n dimensional Hilbert space Hn is chosen. A 2-state system is
described by a two dimensional Hilbert space H2. For the basis
e1 =
 1
0

, e2 =
 0
1

(7.5)
the system is described by a vector x with complex numbers ω1, ω2 that
represent the amplitude of each dimension
x = ω1 · e1 + ω2 · e2 =
 ω1
ω2

.
(7.6)
The probabilities are real numbers between 0 and 1. The probability that
the system is in e1 and e2 is |ω1|2 and |ω2|2 . This is because the product
of complex number with is conjugate is always a real number
ω∗· ω = (x −y · i) · (x + y · i) = x2 + y2 = |ω|2.
(7.7)

98
Principles of Quantum Artiﬁcial Intelligence
The vector representing a state is normalized. Its length is one. The am-
plitudes correspond to the probability with
|ω1|2 + |ω2|2 = 1.
Paul Dirac introduced the following notation for a vector x describing a
state
|x⟩= ω1 · |e1⟩+ ω2 · |e2⟩= ω1 · |x1⟩+ ω2 · |x2⟩=
 ω1
ω2

(7.8)
with
|e1⟩= |x1⟩, |e2⟩= |x2⟩.
It is a shorthand notation for a column vector. Related to the scalar product
⟨x|x⟩row vector are ⟨x| “bra” and and column vectors are |x⟩“kets” from
bra(c)kets. A state vector is just a particular instance of a ket vector. It
is speciﬁed by a particular choice of basis and refers to observable that can
have some system properties.
Operators represented by a square matrix give mathematical description
how something changes in the quantum world.
For a 2-state quantum
system an operator that acts on the memory register would be represented
by a 2 × 2 dimensional unitary matrix. In Unitary matrices, its conjugate
transpose is equal to its inverse.
U ∗= U −1.
(7.9)
7.2.1
Stochastic Markov evolution and unitary evolution
To indicate the diﬀerence to a Markov chain we introduce the example of
a quantum coin, a system with two states 0 and 1 with
|0⟩=
1
0

, |1⟩=
 0
1

.
(7.10)
The mapping is represented as
|0⟩→
1
√
2 · |0⟩+ 1
√
2 · |1⟩
(7.11)
and
|1⟩→
1
√
2 · |0⟩−1
√
2 · |1⟩.
(7.12)
The corresponding operator is indicated by the following unitary matrix,
W =
 
1
√
2
1
√
2
1
√
2 −1
√
2
!
=
1
√
2
·
 1 1
1 −1

.
(7.13)

Introduction to Quantum Physics
99
If the system starts in state |0⟩and undergoes the time evolution, the
probability of observing 0 or 1 is
 1
√
2

2
= 1
2. If we do not preform a mea-
surement and repeat the mapping, the probability of observing 0 becomes
1 and observing 1 becomes zero. This is due to the fact, that the ampli-
tudes of |1⟩cancel each other. This eﬀect is called destructive interference
and cannot occur in the probability distribution since all its coeﬃcients are
non-negative real numbers.
Stochastic Markov evolution
The behavior of a fair coin can be mod-
eled by a Markov chain described by a stochastic matrix. The behavior of
the system is independent from initial distribution ⃗p.
The information
about the initial state is lost. The ﬁxed distribution is reached after one
step.
 1
2
1
2

=
 p1+p2
2
p1+p2
2

=
 1
2
1
2
1
2
1
2

·
p1
p2

.
(7.14)
If constantly observed the quantum coin has the same behavior as a fair
coin described by a stochastic matrix. The probability of being in one of the
states is 0.5. Each time the coin is tossed the “random” eﬀect is observed.
Unitary evolution
During unitary evolution of a (not observed) quan-
tum coin the information about the initial state is not lost, the system is
reversible and deterministic. Each of the two state is present. The loss of
information about its history occurs during the measurement.
In the next section we demonstrate the relation between the unitary ma-
trices and evolutionary operator Ut = e−i·t·H.
7.3
Hilbert Space
Amplitude distribution corresponds to a unit-length vector of a ﬁnite di-
mensional Hilbert space over complex numbers of dimension n denoted as
Hn. The inner product is deﬁned as by the inner product
⟨x|y⟩= ⟨y|x⟩∗
⟨x|x⟩≥0, ⟨x|x⟩= 0 ⇔x = 0
⟨x|c1 · y + c2 · z⟩= c1 · ⟨x|y⟩+ c2 · ⟨x|z⟩

100
Principles of Quantum Artiﬁcial Intelligence
with
x, y, z ∈Hn, c1, c2 ∈C.
With a ﬁxed basis we will use the the coordinate representation that will
induce the inner prouduct by
⟨x|y⟩= x∗
1 · y1 + x∗
2 · y2 + · · · + x∗
n · yn
and the Euclidean vector norm. Is is a vector space with and Euclidean
norm. If W is a subspace of a V, then the orthogonal complement of W is
also a subspace of V . The orthogonal complement W ⊥is the set of vectors
W ⊥= {y ∈V |⟨y|x⟩= 0 x ∈V }
(7.15)
and ach vector x ∈V can be represented as x = xW + xW ⊥with xW ∈W
and xW ⊥∈W ⊥. The mapping P · x = xW is an orthogonal projection.
Such projection is always a linear transformation and can be represented
by a projection matrix P. The matrix is self-adjoint P = P ∗with P = P 2.
An orthogonal projection can never increase a norm
∥P · x∥2 = ∥xW ∥2 ≤∥xW ∥2 + ∥xW ⊥∥2 = ∥xW + xW ⊥∥2 = ∥x∥2.
(7.16)
P can be generated by the normalized vector |x⟩indicating the direction
of the bisecting line
|x⟩=
1
√n · |x1⟩+
1
√n · |x2⟩+ · · · +
1
√n · |xn⟩=




1
√n
...
1
√n




(7.17)
and the row representation
⟨x| =
 1
√n, 1
√n, · · · , 1
√n

.
(7.18)
The projection matrix is
P = |x⟩⟨x| =





1
n
1
n · · ·
1
n
1
n
1
n · · ·
1
n
...
... ... ...
1
n
1
n · · ·
1
n




.
(7.19)
The projection P computes for each dimension described by the ﬁxed basis,
for example |xi⟩= |ei⟩, the mean value of all dimensions. An example of
such a projection in the two dimensional Hilbert space is the stochastic
matrix describing the behavior of a fair coin.

Introduction to Quantum Physics
101
7.3.1
Spectral representation∗
A linear mapping Hn →Hn is called an operator. An operator A is self-
adjoint if
⟨x |A · y⟩= ⟨A∗· x|y⟩.
(7.20)
For a ﬁxed basis a matrix can represent an operator. A self-adjoint op-
erator is represented by a Hermitian matrix A∗= A with aij = aji. For
real values the Hermitian matrix is a symmetrical matrix with AT = A.
Hermitian matrices have real eigenvalues and corresponding eigenvectors
are orthogonal. From the deﬁnition of eigenvalues and eigenvectors
A · x = λ · x
consequently
λ∗· ⟨x|x⟩= ⟨λ · x|x⟩= ⟨A · x|x⟩= ⟨x|A · x⟩= λ · ⟨x|x⟩.
The eigenvectors of diﬀerent eigenvalues λ1 ̸= λ2 are orthogonal,
λ1 · ⟨x1|x2⟩= ⟨A · x1|x2⟩= ⟨x1|A · x2⟩= λ2 · ⟨x1|x2⟩
hence the two diﬀerent eigenvectors are orthogonal
λ1 · ⟨x1|x2⟩= λ2 · ⟨x1|x2⟩⇒⟨x1|x2⟩= 0.
A Hermitian matrix can be represented by a sum of projections of its or-
thonormal eigenvectors (normalized eigenvectors) x1, · · · , xn weighted by
its eigenvalues. The projections are deﬁned by the matrix |xi⟩⟨xi| with
|xi⟩⟨xi| = (|xi⟩⟨xi|)2 . The set of the eigenvalues is called the spectrum and
the corresponding representation a spectral representation
A = λ1 · |x1⟩⟨x1| + λ2 · |x2⟩⟨x2| + · · · + λn · |xn⟩⟨xn|.
(7.21)
The function ei·A with a self-adjoint operator A is unitary
ei·A = ei·λ1 · |x1⟩⟨x1| + ei·λ2 · |x2⟩⟨x2| + · · · + ei·λn · |xn⟩⟨xn|.
(7.22)
An operator U is unitary, if U ∗= U −1,
 ei·A∗=
 ei·A−1 ,
(7.23)
 ei·A−1 = e−i·λ1 ·|x1⟩⟨x1|+e−i·λ2 ·|x2⟩⟨x2|+· · ·+e−i·λn ·|xn⟩⟨xn|. (7.24)
This representation is similar to the evolutionary operator Ut = e−i·t·H
for t = 1 and A = H is the Hamiltonian operator that is related to the
total energy of the system. Mathematical description in a quantum world

102
Principles of Quantum Artiﬁcial Intelligence
is given by operators represented by a unitary square matrix. For example
the unitary matrix that describes the quantum coin is
W =
1
√
2 ·
 1
1
1 −1

.
(7.25)
What is the relation between a unitary matrix and the evolutionary op-
erator?
For simplicity we omit the minus sign and write instead of U ∗
simply
W = U = ei·H.
What is the corresponding Hamiltonian operator? A unitary operator U
is not self-adjoint, the Matrix is unitary but not Hermitian. However it
can be decomposed into two self-adjoint operators. As shown before each
self-adjoint operators has a spectral representation
U = 1
2 · (R + i · I)
(7.26)
with
R = 1
2 · (U + U ∗),
I =
1
i · 2 · (U + U ∗).
R and I are Hermitian matrices and their spectral representation is
R = λ1 · |x1⟩⟨x1| + λ2 · |x2⟩⟨x2| + · · · + λn · |xn⟩⟨xn|
and
I = µ1 · |x1⟩⟨x1| + µ2 · |x2⟩⟨x2| + · · · + µn · |xn⟩⟨xn|.
U can be written as
U = (λ1 + i · µ1) · |x1⟩⟨x1| + · · · + (λn + i · µn) · |xn⟩⟨xn|.
(7.27)
How can we represent U as U = ei·H ?
For all eigenvalues of U
⟨x|x⟩= ⟨x|U · U ∗x⟩= ⟨U · x|U · x⟩= ⟨λ · x|λ · x = |λ|2 · ⟨x|x⟩
the absolute value is 1
1 = |λi + µi|
all values of λi and µi are real values from the interval [−1, 1]. Therefore
there exist a value θi for each i with
λi = cos θi,
µi = sin θi,
(7.28)
and because ei·x = cos x + i · sin x
λi + i · µi = cos θi + i · sin θi,
(7.29)
µi = ei·θi.
(7.30)
We can now represent any unitary operator U by the spectral representation
U = ei·H = ei·θ1 · |x1⟩⟨x1| + ei·θ2 · |x2⟩⟨x2| + · · · + ei·θn · |xn⟩⟨xn|.
(7.31)
with the corresponding Hamilton operator H
H = θ1 · |x1⟩⟨x1| + θ1 · |x2⟩⟨x2| + · · · θn · |xn⟩⟨xn|.
(7.32)

Introduction to Quantum Physics
103
Example
The unitary matrix that describes the quantum coin
W =
1
√
2 ·
1
1
1 −1

is as well Hermitian with W T = W. Because of this the two eigenvalues
are real, −1 and 1 . The corresponding eigenvectors are
|x1⟩=
1
p
4 −2 ·
√
2
·
 1 −
√
2
1

and
|x2⟩=
1
p
4 + 2 ·
√
2
·
1 +
√
2
1

.
The Hermitian matrix W can be represented by the spectral representation
W = 1 · |x1⟩⟨x1| −1 · |x2⟩⟨x2|
(7.33)
with
|x1⟩⟨x1| =
 
1−
√
2
4
−
√
2
4
−
√
2
4
1+
√
2
4
!
and
|x2⟩⟨x2| =
 
1+
√
2
4
√
2
4
√
2
4
1−
√
2
4
!
.
We can now represent unitary operator W as (evolutionary operator)
U = ei·H = ei·0 · |x1⟩⟨x1| + ei·π · |x2⟩⟨x2| = |x1⟩⟨x1| + ei·π · |x2⟩⟨x2| (7.34)
with the corresponding Hamilton operator H
H = 0 · |x1⟩⟨x1| + π · |x2⟩⟨x2|.
(7.35)
A unitary matrix can be represented by the evolutionary operator with the
Hamilton operator H.
7.4
Quantum Time Evolution
Quantum time Evolution is described by Schr¨odinger equation, its solution
represents the unitary evolution described by the unitary operator Ut. The
unitary operator is determined by the requirements for the time-evolution
mapping
x(t) = Ut · x(0).
Suppose Ut is a time-evolution mapping, then the following requirements
should be fulﬁlled [Hirvensalo (2004)]

104
Principles of Quantum Artiﬁcial Intelligence
• Ut should preserve the norm,
∥Ut · ⟨x|∥= ∥⟨x|∥.
The length of a vector is always one, before and after the mapping.
It determines the probability of the presence of certain states repre-
sented by its dimensions. The vector itself describes the probability
distribution.
• The mapping Ut is linear
Ut · (λ1 · |x1⟩+ · · · + λn · |xn⟩) = Ut · λ1 · |x1⟩+ · · · + Ut · λn · |xn⟩.
For the basis
|e1⟩= |x1⟩, |e2⟩= |x2⟩, · · · , |en⟩= |xn⟩
the linearity corresponds to simple matrix operation
U ·





λ1
λ2
...
λn




= U ·





λ1
0
...
0




+ U ·





0
λ2
...
0




+ · · · + U ·





0
0
...
λn




.
• For all t1 and t2
Ut2+t1 = Ut2Ut1.
• The time evolution must be smooth. Even if we are interested in the
state of the system a discrete time points, the evolution should be
smooth and continuous
lim
t→t0 Ut · x(0) = lim
t→t0 x(t) = x(t0).
Only a unitary operator can satisfy the ﬁrst three requirements. It can
be represented by a unitary matrix and by the evolutionary operator with
the Hamilton self-adjoint operator H. If Ut satisﬁes all four requirements,
there exists self-adjoint operator H with a relation to time t
Ut = e−i·t·H
and
x(t) = e−i·t·H · x(0) = Ut · x(0).
The time evolution is continuous and reversible, however we will represent
an algorithm as a sequence of one-length vectors in discrete time steps
t0 →t1 →t2 →t3 →· · ·
as
|x⟩→U1 · |x⟩→U2 · U1 · |x⟩→U3 · U2 · U1 · |x⟩→· · ·
This representation is motivated by the ﬁrst and second requirement.

Introduction to Quantum Physics
105
7.5
Compound Systems
A 2-state quantum system is described by a two dimensional Hilbert space
H2,
|x1⟩=
 1
0

, |x2⟩=
 0
1

(7.36)
is described by a vector |x⟩with complex numbers ω1, ω2 that represent the
amplitude of each dimension.
|x⟩= ω1 · |x1⟩+ ω2 · |x2⟩=
 ω1
ω2

.
(7.37)
Such a 2-state quantum system corresponds to a qubit with the basis
|0⟩= |x1⟩,
|1⟩= |x2⟩.
The qubit is described by a vector |x⟩with complex numbers ω1, ω2 that
represent the amplitude of each dimension
|x⟩= ω0 · |0⟩+ ω1 · |1⟩.
(7.38)
The vector has length one with
|ω0|2 + |ω1|2 = 1 →∥|x⟩∥= 1.
The unitary matrix W performs the following mapping in the ket notation
W · |0⟩= W · 1 · |0⟩+ W · 0 · |1⟩=
1
√
2 · |0⟩+ 1
√
2 · |1⟩
with the vector notation
1
√
2 ·
 1
1
1 −1

·
 1
0

=
 
1
√
2
1
√
2
!
.
Applying W again results in
W ·
 1
√
2 · |0⟩+ 1
√
2 · |1⟩

= W · 1
√
2 · |0⟩+ W · 1
√
2 · |1⟩= |0⟩
with the vector notation
1
√
2 ·
 1
1
1 −1

·
 
1
√
2
1
√
2
!
=
1
0

.
How can we represent a register composed of two qubits? Such a register
would represent 22 possible states and would be represented in a Hilbert

106
Principles of Quantum Artiﬁcial Intelligence
space H4. The ﬁrst qubit is represented by a two dimensional Hilbert space
H2,
|x⟩= ω0 · |0⟩+ ω1 · |1⟩=
ω0
ω1

and the second as
|y⟩= ω0 · |0⟩+ ω1 · |1⟩=
 ω0
ω1

.
The register of two qubits is represented as a direct product of |x⟩and |y⟩
|x⟩⊗|y⟩= |x⟩|y⟩= |xy⟩=
ω0
ω1

⊗
 ω0
ω1

=




ω0 · ω0
ω0 · ω1
ω1 · ω0
ω1 · ω1



=




ω0
ω1
ω2
ω3



(7.39)
or
|xy⟩= (ω0 · |0⟩+ ω1 · |1⟩) ⊗(ω0 · |0⟩+ ω1 · |1⟩)
|xy⟩= ω0 · |00⟩+ ω1 · |01⟩+ ω2 · |10⟩+ ω3 · |11⟩
(7.40)
with the new basis
|00⟩=




1
0
0
0



, |01⟩=




0
1
0
0



, |10⟩=




0
0
1
0



, |11⟩=




0
0
0
1



.
(7.41)
A register of three qubits represents 23 diﬀerent states represented in a
Hilbert space H8.
|xyz⟩= |x⟩⊗|y⟩⊗|z⟩=
ω0 · |00⟩+ ω1 · |001⟩+ ω2 · |010⟩+ ω3 · |011⟩+
+ω4 · |100⟩+ ω5 · |001⟩+ ω6 · |110⟩+ ω7 · |111⟩.
(7.42)
A quantum register of length m represents m qubits in a Hilbert space of
dimension n = 2m. A state in a n-dimensional Hilbert space Hn is deﬁned
by an orthonormal basis
|x1⟩, |x1⟩, · · · |xn⟩
and is represented as a unit-length vector
ω1 · |x1⟩+ ω2 · |x2⟩+ · · · + ωn · |xn⟩

Introduction to Quantum Physics
107
that determines the probability of distribution of the states. Each dimen-
sion correspond to a possible combination. The state is in a basis state |xi⟩
with a probability |ωi|2.
The compund system of the Hilbert space Hn and a w-dimensional Hilbert
space Hw deﬁned by a orthonormal basis |y1⟩, |y1⟩. · · · |yw⟩is deﬁned by the
tensor product
Hn·w = Hn ⊗Hw
(7.43)
According to this deﬁnition we can apply an operator on two qubits as
W · (ω0 · |0⟩+ ω1 · |1⟩) ⊗W · (ω0 · |0⟩+ ω1 · |1⟩) =

W ·
 ω0
ω1

⊗

W ·
ω0
ω1

(7.44)
(W ⊗W) · (ω0 · |0⟩+ ω1 · |1⟩) ⊗(ω0 · |0⟩+ ω1 · |1⟩) = (W ⊗W) ·




ω0
ω1
ω2
ω3




(7.45)
it follows

W ·
ω0
ω1

⊗

W ·
 ω0
ω1

= (W ⊗W) ·




ω0
ω1
ω2
ω3



.
(7.46)
The tensor product between matrix is deﬁned as
A ⊗B =
a11 · B a12 · B
a21 · B a22 · B

=




a11 · b11 a11 · b12 a12 · b11 a12 · b12
a11 · b21 a11 · b22 a12 · b21 a12 · b22
a21 · b11 a21 · b12 a22 · b11 a22 · b12
a21 · b21 a21 · b22 a22 · b21 a22 · b22



.
For example W ⊗W is
W ⊗W =
1
√
2 ·
1
1
1 −1

⊗1
√
2 ·
 1
1
1 −1

= 1
2 ·




1
1
1
1
1 −1
1 −1
1
1 −1 −1
1 −1 −1
1



.

108
Principles of Quantum Artiﬁcial Intelligence
7.6
Von Neumann Entropy
A state |x⟩(∥x∥= 1) in a Hilbert space Hn can be represented by the
corresponding density matrix P = |x⟩⟨x| with,
|x⟩⟨x| =





x1
x2
...
xn




· (x∗
1, x∗
2, · · · , x∗
n) =





|x1|2
x1 · x∗
2 · · · x1 · x∗
n
x2 · x∗
1
|x2|2 · · · x2 · x∗
n
...
... ...
...
xn · x∗
1 x1 · x∗
n · · ·
|xn|2




(7.47)
A density matrix P has following properties
• P is a Hermitian matrix with P ∗= P.
• P represents a self-adjoint operator P.
• P has a spectral representation.
• P is a projection with P = P 2, because |x⟩⟨x| = ||x⟩⟨x||2.
• P is a linear combination of one-dimensional projections.
• The trace of P, T r(P) = 1 with T r(P) = Pn
i=1 |xi|2.
For the density matrix P, the von Neumann entropy is deﬁned as
E(P) = −T r(P · log P),
(7.48)
with a logarithm of matrix Q = log P, where Q is a matrix with P = eQ.
With the spectral decomposition of the self-adjoint operator P the von
Neumann entropy can be easily computed without using logarithm of a
matrix,
P = λ1 · |x1⟩⟨x1| + λ2 · |x2⟩⟨x2| + · · · + λn · |xn⟩⟨xn|
with
E(P) = −
n
X
i=1
(λi · log λi)
(7.49)
and
1 =
n
X
i=1
λi.
The spectral representation of a state |x⟩in n-dimensional Hilbert space
Hn is unique . It is deﬁned by the orthonormal eigenvectors of the density
matrix P = |x⟩⟨x|. The representation of a state |x⟩by a vector of complex
amplitudes depends on the orthonormal basis. For the orthonormal basis
|x1⟩= e1, |x2⟩= e2, · · · , |xn⟩= en
|x⟩= ω1 · |x1⟩| + ω2 · |x2⟩+ · · · + ωn · |xn⟩

Introduction to Quantum Physics
109
we can assume that
E(P) = −
n
X
i=1
(|ωi|2 · log |ωi|2).
(7.50)
States equal to a basis are called pure states. A pure state corresponds to a
one-dimensional projection in a spectral representation with one eiganvalue
λi = 1. Otherwise the states are called superposition. The von Neumann
entropy of a pure state is zero, 0 = 1·log 1. The von Neumann entropy of a
superposition measures the distribution of the probabilities represented by
the eiganvalues. It describes the departure of the state from a pure state
with a maximal value when all eigenvalues are equal.
E(P) = −
n
X
i=1
λi · log λi = −
n
X
i=1
1
n · log 1
n = log n
(7.51)
We represent a state by a sequence of m qubits by a n = 2m dimensional
vector in Hn Hilbert space.
In this case the maximal value of the von
Neumann entropy is
E(P) = log n = log 2m = log 2 · log2 2m = log 2 · m.
(7.52)
The von Neumann entropy measure the information in nepit (nat). If the
von Neumann entropy is measured in bits, yes no questions, its maximal
value is just the number of present qubits.
Before the measurement of a state |x⟩we are uncertain about the outcome.
We measure the uncertainty by the entropy. After the measure the state
is pure, the von Neumann entropy is zero, 0 = 1 · log 1. The measurement
is a random process described by distributions of probabilities represented
by the eigenvalues λi.
7.7
Measurement
After a unitary information processing starting form a initial basis state
the result of the algorithm is determined by the measurement. The mea-
surement corresponds to the collapse of the state vector, a projection into
a basis state. The projection is not reversible and it is not consistent with
the unitary time evolution. For a state represented by a unit-length vector
ω1 · |x1⟩+ ω2 · |x2⟩+ · · · + ωn · |xn⟩
in a n-dimensional Hilbert space |xk⟩is observed. After the measurement
(observation) the state is in a pure state
1 · |xk⟩.

110
Principles of Quantum Artiﬁcial Intelligence
7.7.1
Observables
The measurement is preformed by an observable. A Hilbert space Hn can
be represented as a collection of orthogonal subspaces
Hn = E1 ⊕E2 ⊕. . . ⊕Ef
(7.53)
with f ≤n. A state |x⟩can be represented with |xi⟩∈Ei as
ω1 · |x1⟩+ ω2 · |x2⟩+ · · · + ωf · |xf⟩.
For one dimensional subspaces
Hn = E1 ⊕E2 ⊕. . . ⊕En
the value |xk⟩is observed with a probability ∥ωk · |xk⟩∥2 = |ωk|2.
Another description is through a projection into a subspace. A subspace
deﬁnes a projection PEk. Because a projection is self-adjoint PEk = P ∗
Ek
and PEk = P 2
Ek the probability of observing |xk⟩is
⟨x|PEk · x⟩= ⟨x|P 2
Ek · x⟩= ⟨PEk · x|PEk · x⟩= ∥ωk · |xk⟩∥2 = |ωk|2 (7.54)
and the resulting value is
1
p
⟨x|PEk · x⟩
· PEk · |x⟩= 1 · |xk⟩.
Finally a state can be represented by the corresponding density matrix
P with a corresponding spectral decomposition.
The projection in the
orthogonal subspace Ek is described by the projection matrix |xk⟩⟨xk| and
the corresponding probability is λk. We can represent this probability as
⟨x|xk⟩⟨xk|x⟩= λk
and the resulting value of the projection is
1
p
⟨x|xk⟩⟨xk|x⟩
|xk⟩⟨xk| · |x⟩= 1 · |xk⟩.
All three diﬀerent descriptions represent the same fact through diﬀerent
formalisms. An observation corresponds to a non-reversible projection onto
one or several basis states.

Introduction to Quantum Physics
111
7.7.2
Measuring a compound system
The state of the system is projected to the subspace that corresponds to the
observed state and the vector representing the state is renormalized to the
unit length. An observable describes a subspace of some dimensions with
a special case of one dimension. A part of the system can be observed by a
projection in a subspace with a dimension higher one. The compound sys-
tem of n-dimensional Hilbert space |x⟩∈Hn and a w-dimensional Hilbert
space |y⟩∈Hw deﬁned by a orthonormal basis |xy⟩∈Hn·w. A state of the
system is represented as
|xy⟩=
n
X
i=1
w
X
j=1
ωij|xi⟩|yj⟩.
(7.55)
For example
|xy⟩=
2
X
i=1
2
X
j=1
ωij|xi⟩|yj⟩=
= ω11 · |x1⟩|y1⟩+ ω12 · |x1⟩|y2⟩+ ω21 · |x2⟩|y1⟩+ ω22 · |x2⟩|y2⟩.
For simplicity we use the following notation for a qubit register
|xy⟩= ω0 · |00⟩+ ω1 · |01⟩+ ω2 · |10⟩+ ω3 · |11⟩
The probability of observing xk is Pw
j=1 |ωkj|2. If we observe xk, the system
after the observation is projected into
|xy⟩=
1
qPw
j=1 |ωkj|2
w
X
j=1
ωkj|xk⟩|yj⟩.
Suppose the two qubits are in the following state
√
0.25 · |00⟩+
√
0.25 · |01⟩+
√
0.25 · |10⟩+
√
0.25 · |11⟩=




1
2
1
2
1
2
1
2



.
The observed ﬁrst qubit is |0⟩. The probability of the observation is
|ω00|2 + |ω01|2 = |ω0|2 + |ω1|2 = |
√
0.25|2 + |
√
0.25|2 = 0.25 + 0.25 = 0.5
the system after the observation is projected into
√
0.25 · |00⟩+
√
0.25 · |01⟩
√
0.5
=
√
0.5 · |00⟩+
√
0.5 · |01⟩=






q
1
2
q
1
2
0
0






.

112
Principles of Quantum Artiﬁcial Intelligence
7.7.3
Heisenberg’s uncertainty principle∗
The commutator between two operators A and B is deﬁned as
[A, B] := A · B −B · A
(7.56)
so
[A, B] = 0 ⇐⇒A · B = B · A.
(7.57)
The anti-commutator between two operators A and B is deﬁned as
{A, B} := A · B + B · A
(7.58)
it follows that
A · B = [A, B] + {A, B}
2
.
(7.59)
The expected value of observable M in state x is
⟨M⟩= ⟨x|M · x⟩
(7.60)
and the standard deviation of observed values is
∆(M) =
p
⟨(M −⟨M⟩)2⟩=
p
⟨M 2⟩−⟨M⟩2.
(7.61)
What happens if we try to measure two observable G and K?
A and B are Hermitian, A∗= A, B∗= B and |x⟩is a quantum state.
If
⟨x|A · B · x⟩= a + ib
(7.62)
then
⟨x|[A, B] · x⟩= 2 · ib
(7.63)
and
⟨x|{A, B} · x⟩= 2 · a
(7.64)
It follows that
|⟨x|[A, B] · x⟩|2 + |⟨x|{A, B} · x⟩|2 = 4 · |⟨x|A · B · x⟩|2.
(7.65)
Because by the Cauchy-Schwarz inequality
|⟨x|A · B · x⟩|2 ≤⟨x|A2 · x⟩|⟨x|B2 · x⟩|
(7.66)
|⟨x|[A, B] · x⟩|2 + |⟨x|{A, B} · x⟩|2 ≤4 · ⟨x|A2 · x⟩|⟨x|B2 · x⟩|
(7.67)
and
|⟨x|[A, B] · x⟩|2+ ≤4 · ⟨x|A2 · x⟩|⟨x|B2 · x⟩|
(7.68)
With
A = G −⟨G⟩,
B = K −⟨K⟩
we obtain the Heisenberg’s uncertainty principle
∆(G)∆(K) ≥|⟨x|[G, K] · x⟩|
2
.
(7.69)

Introduction to Quantum Physics
113
Example
Suppose we have a large number of quantum systems in the
state |x⟩.
If we measure the observable G on some and the observable
K on others, then the standard deviation ∆(G) and ∆(K) will satisfy
the Equation 7.69. For example the observable G is [Nielsen and Chuang
(2000)]
G =
 0 1
1 0

(7.70)
and
K =
 0 −i
i 0

(7.71)
with
[G, K] =
 0 1
1 0

·
 0 −i
i 0

−
 0 −i
i 0

·
 0 1
1 0

[G, K] =
 2i
0
0 −2i

then
∆(G)∆(K) ≥
⟨0|
2i
0
0 −2i

· 0⟩

2
= 1
(7.72)
∆(G) and ∆(K) must be grater than 0.
Time-frequency information of a signal
The uncertainty principle
was originally applied to the momentum and location of moving particles.
The uncertainty principle can also be applied to the classical time-frequency
information of a signal. It is not possible to know the exact time-frequency
representation of a signal. In short-time Fourier transform (STFT), the
signal is divided into small segments. In each segment, the Fourier trans-
form determines the frequency representation of the signal. The size of
the window deﬁnes the time representation of the signal. The uncertainty
principle of STFT is related to the width of the window function that is
used.
• Narrow window: good time resolution, poor frequency resolution.
• Wide window: good frequency resolution, poor time resolution.
In classical physics, a possible solution exists. This solution is the multi
resolution analysis of the signal, as described by the wavelet transform.
The analysis is “repeated” several times. Each time, a diﬀerent size (scale)
of the window is used.

114
Principles of Quantum Artiﬁcial Intelligence
7.8
Randomness
Randomness can be deﬁned by numbers in a sequence [Williams and Clear-
watter (1997)]. A random number alone does not exist. A sequence of
random numbers must correspond to some distribution, and there should
be no correlation among the numbers in the sequence. For example, the
following sequence, which represents the toss of a fair coin 0, 1, is not ran-
dom,
0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1
because a correlation between 0 and 1 is present. Classical physics is deter-
ministic. Because of that, no randomness exists in its context. Some facts
could appear to be random. However, this arrangement is only the case
because some essential information is missing.
7.8.1
Deterministic chaos
A sequence could look random even though it is generated by a simple non-
linear deterministic equation. Such behavior is called deterministic chaos.
The logistic map is deﬁned by a dynamical nonlinear diﬀerence equation
xt+1 = r · xt · (1 −xt)
(7.73)
with with some constant r. The rule generates a sequence
x0, x,x2, x3, x4, · · ·
that depending on the value r.
For x0 = 0.1 and r = 3.2 the equation converges to a period (see Fig-
ure 7.1).
However, for values that are approximately in [3.5699, 4], the
sequence does not converge to any pattern. A minimal change of x0 leads
to a diﬀerent sequence (see the values x0 = 0.1 and r = 3.98 represented in
Figure 7.2 and the values x0 = 0.101 and r = 3.98 in Figure 7.3).
There is no randomness in classical physics and in the resulting equa-
tions that describe it, in spite of the fact that a behavior can be generated
that is highly unpredictable due to our lack of information. For example,
in the logistic equation, exact knowledge of the initial condition is required.
7.8.2
Kolmogorov complexity
Kolmogorov complexity is deﬁned as the shortest program that can pro-
duce its output. It is a measure of the amount of innate randomness of a

Introduction to Quantum Physics
115
20
40
60
80
100
0.4
0.5
0.6
0.7
Fig. 7.1
For x0 = 0.1 and r = 3.2 the logistic map converges to a period.
!"
#"
$"
%"
&""
"'!
"'#
"'$
"'%
&'"
Fig. 7.2
For x0 = 0.1 and r = 3.98 the logistic map behaves chaotically, it appears
random.
20
40
60
80
100
0.2
0.4
0.6
0.8
1.0
Fig. 7.3
The logistic map is sensitive to the initial condition.
For x0 = 0.101 and
r = 3.98 it behaves diferentlys as the sequence with initial condition or x0 = 0.1.

116
Principles of Quantum Artiﬁcial Intelligence
sequence. In the case of deterministic chaos, the randomness is low. It can
be described by a very short program, such as in the case of the logistic
map, by an equation together with the values of x0 and r. Randomness is
deﬁned by the Kolmogorov complexity. The larger the shortest program
that generates the sequence is, the more random it is. However, we cannot
determine whether a sequence can be represented by a short program. The
problem is related to the halting problem [Gardner (1979)]. We cannot
generate a truly random sequence using a Turing machine.
7.8.3
Humans and random numbers
Randomness could be nature’s way to avoid complexity. A decision could
be chosen randomly, where no knowledge is present and planning is not pos-
sible. How well can people generate random sequences? It was indicated
in one experiment that humans cannot generate true binary random se-
quences. Hagelbarger asked subjects to create a binary random sequence,
and the sequence was analyzed for correlations by a computer program.
The program attempted to predict the next symbol in the sequence and
achieved a 55 −60 percent accuracy. For a random sequence, the accuracy
should be approximately 50 percent [Shannon (1953)].
7.8.4
Randomness in quantum physics
Quantum physics is the only source of true randomness. Randomness can
be generated by a quantum computing device that simulates a quantum
coin. A quantum coin is deﬁned by the unitary matrix
W =
1
√
2 ·
1 1
1 −1

and by the mapping
W · |0⟩=
1
√
2 · |0⟩+ 1
√
2 · |1⟩
(7.74)
or
W · |1⟩=
1
√
2 · |0⟩−1
√
2 · |1⟩.
(7.75)
The corresponding operator is indicated by the following unitary matrix,
W =
 
1
√
2
1
√
2
1
√
2 −1
√
2
!
=
1
√
2
·
 1 1
1 −1

.
(7.76)

Introduction to Quantum Physics
117
After the mapping, a measurement is preformed. The probability of ob-
serving 0 or 1 is
 1
√
2

2
= 1
2. True randomness is present during the mea-
surement; it is an eﬀect of the measurement represented by the collapse.
The collapse itself is not explained by quantum theory. Using a quantum
coin, we can generate a true random binary sequence. It is easy to combine
the results from a quantum coin to generate a random integer in the range
0 to 2n−1. Later, we will see how to generate a true random dice using
QFT .

This page intentionally left blank
This page intentionally left blank

Chapter 8
Computation with Qubits
8.1
Computation with one Qubit
A unitary operator on a qubit is called an unary quantum gate.
It is
described by a unitary matrix of the dimension 2 × 2. For the qubit with
the basis
|0⟩=
 1
0

, |1⟩=
0
1

the quantum not gate M¬ does the not operation on a qubit
M¬|0⟩= |1⟩, M¬|1⟩= |0⟩
and is represented by the unitary matrix
M¬ =
 0 1
1 0

.
(8.1)
The not operation can be written using XOR = ⊕for x ∈B1
M¬|x⟩= |x ⊕1⟩
M¬|0⟩= |0 ⊕1⟩= |0⟩,
M¬|1⟩= |1 ⊕1⟩= |1⟩.
The square root of the not gate M¬ = √M¬ · √M¬ is represented by the
unitary matrix
p
M¬ =
 1+i
2
1−i
2
1−i
2
1+i
2

(8.2)
with
 1+i
2
1−i
2
1−i
2
1+i
2

·
 1+i
2
1−i
2
1−i
2
1+i
2

=
 0 1
1 0

(8.3)
119

120
Principles of Quantum Artiﬁcial Intelligence
and it is unitary because
 1+i
2
1−i
2
1−i
2
1+i
2

·
 1−i
2
1+i
2
1+i
2
1−i
2

=
1 0
0 1

(8.4)
with
M¬|0⟩= 1 + i
2
· |0⟩+ 1 −i
2
· |1⟩
and
M¬|1⟩= 1 −i
2
· |0⟩+ 1 + i
2
· |1⟩.
The probability of measuring |0⟩and |1⟩is 0.5, because

1 −i
2

2
=

1 + i
2

2
= 1
2.
−√M¬ has the same behavior with
M¬ = −
p
M¬ · −
p
M¬.
The identity gate preforms no operation on a qubit, it is deﬁned as the
identity matrix
I1 =
 1 0
0 1

.
(8.5)
The square root of the identity matrix is the identity I matrix is I and −I
−I1 =
 −1
0
0 −1

.
(8.6)
−I1 changes the sign of the amplitude but not the probabilities. The in-
troduced unitary matrix W maps a pure state in a superposition.
|0⟩→
1
√
2 · |0⟩+ 1
√
2 · |1⟩
|1⟩→
1
√
2 · |0⟩−1
√
2 · |1⟩
The probability of measuring |0⟩and |1⟩is 0.5. The matrix W is called
Walsh, Hadarmad or Hamarad Walsh, matrix.

Computation with Qubits
121
8.2
Computation with m Qubit
The register of m qubits is represented as a direct product of m qubits. It
deﬁnes n = 2m dimensional Hilbert space Hn with an orthonprmal basis
|x1⟩, |x1⟩, · · · |xn⟩. For example four qubits deﬁne a 16 dimensional Hilbert
space H16 with the basis
|0000⟩=






























1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






























, |0001⟩=




























0
1
0
0
0
0
0
0
0
0
0
0
0
0
0




























, |0010⟩=




























0
0
1
0
0
0
0
0
0
0
0
0
0
0
0




























, · · · , |1111⟩=




























0
0
0
0
0
0
0
0
0
0
0
0
0
0
1




























.
(8.7)
It is diﬃcult to simulate more than few of tens bits on an ordinary com-
puter because the dimension of the Hilbert space grows exponentially in
relation to the number of represented qubits. For example sixteen qubits
are represented by a 65536 dimensional Hilbert space H65536.
The Hadarmad matrix W on one qubit has the dimension 2 × 2 is also
called a Hadamard gate and is indicated as W1. A Hadarmad operator
for m qubits Wm is represented by a 2m × 2m dimensional matrix built by
a direct product of m W1 matrices. The complexity of the operator Wm
corresponds to m Hadamard gates W1.
Wm =
OmW1 = W1 ⊗W1 · · · ⊗W1
(8.8)
The Hadamard matrix is also called the Hadamard transform and can be
deﬁned recursively with W0 = 1 and
Wm =
1
√
2 ·
Wm−1
Wm−1
Wm−1 −Wm−1

(8.9)

122
Principles of Quantum Artiﬁcial Intelligence
with W3
W3 = W1 ⊗W1 ⊗W1
W3 =
1
√
2 ·
 W2
W2
W2 −W2

=
1
√
23 ·













1
1
1
1
1
1
1
1
1 −1
1 −1
1 −1
1 −1
1
1 −1 −1
1
1 −1 −1
1 −1 −1
1
1 −1 −1
1
1
1
1
1 −1 −1 −1 −1
1 −1
1 −1 −1 −1 −1
1
1
1 −1 −1 −1 −1
1
1
1 −1 −1
1 −1
1
1 −1













.
The Hadamard operator Wm maps m qubits |z⟩representing a pure state
in a Hilbert space H2m with z ∈Bm
Wm|z⟩=
1
√
2m
X
x∈Bm
(−1)⟨z|x⟩· |x⟩
(8.10)
with a scalar product (⟨z|x⟩) over the binary ﬁeld with two elements cor-
responding to the bits 0 and 1. The multiplication of two bits is equal to
the AND operation with
0 · 0 = 0 ∧0 = 0,
0 · 1 = 0 ∧1 = 0,
1 · 0 = 1 ∧0 = 0,
1 · 1 = 1 ∧1 = 1
and the addition is equal to the XOR operation ⊕
0 + 0 = 0 ⊕0 = 0,
0 + 1 = 0 ⊕1 = 1,
1 + 0 = 1 ⊕0 = 1,
1 + 1 = 1 ⊕1 = 0.
For the state zero represented by m qubits
|0⟩⊗m = |0⟩|0⟩|0⟩· · · |0⟩=







1
0
...
0
0







the Hadamard operator Wm maps a pure state into a superposition of all
possible states with no negative sign,
Wm|0⟩⊗m =
1
√
2m
X
x∈Bm
|x⟩.
(8.11)

Computation with Qubits
123
For example
W3|0⟩⊗3 = W3|000⟩=
1
√
23
X
x∈B3
|x⟩
W3|000⟩=
=
1
√
23 (|000⟩+ |001⟩+ |000⟩+ |010⟩+ |011⟩+ |100⟩+ |101⟩+ |111⟩) .
It can be expressed as
W3|000⟩= W1|0⟩⊗W1|0⟩⊗W1|0⟩=
|0⟩+ |1⟩
√
2

·
|0⟩+ |1⟩
√
2

·
|0⟩+ |1⟩
√
2

and
W3 · W3|000⟩= |000⟩.
This is because Wm = W ∗
m so that Im = Wm · Wm. The pure states |11⟩is
maped into
W2|11⟩= W1|1⟩⊗W1|1⟩=
|0⟩−|1⟩
√
2

·
|0⟩−|1⟩
√
2

W2|11⟩= 1
2 · (|00⟩−|01⟩−|10⟩+ |11⟩) .
A Walsh matrix is a square matrix of the dimension power of two.
It has the property that the scalar product of any two diﬀerent rows or
columns is zero. The vectors that represent the matrix are orthogonal. In
Hadamard transform the vectors that represent the matrix are orthonormal.
We indicate the matrix by Wm for m qubits (because of Walsh), not to be
confused with the Hilbert space Hn of the dimension n with the relation
n = 2m.
8.3
Matrix Representation of Serial and Parallel Opera-
tions
A serial computation corresponds to a multiplication of matrices that repre-
sent the gates. The multiplication of matrices is usually not commutative,
for example W1 · M¬ ̸= M¬W1
 
1
√
2
1
√
2
1
√
2 −1
√
2
!
·
0 1
1 0

̸=
 0 1
1 0

·
 
1
√
2
1
√
2
1
√
2 −1
√
2
!

124
Principles of Quantum Artiﬁcial Intelligence
 
1
√
2
1
√
2
−1
√
2
1
√
2
!
̸=
 
1
√
2 −1
√
2
1
√
2
1
√
2
!
it means
W1 · M¬ · |0⟩= W1 · |1⟩= |0⟩−|1⟩
√
2
and
M¬ · W1 · |0⟩= M¬ ·
|0⟩+ |1⟩
√
2

=
= M¬ · |0⟩+ M¬ · |1⟩
√
2
= |1⟩+ |0⟩
√
2
= |0⟩+ |1⟩
√
2
it follows
M¬ · W1 · |0⟩= W1 · |0⟩.
Only the multiplication with the identity matrix and the inverse matrix
are commutative operations. Parallel operations correspond to the direct
product, also called the tensor product or Kronecker product when dealing
with matrices. For example with
M¬ ⊗I1 ⊗W1 · |000⟩= (M¬ · |0⟩) ⊗(I1 · |0⟩) ⊗(W1 · |0⟩) =
|10⟩· |0⟩+ |1⟩
√
2
= |100⟩+ |101⟩
√
2
in vector representation as
 0 1
1 0

·
 1
0

⊗
1 0
0 1

·
1
0

⊗
  
1
√
2
1
√
2
1
√
2 −1
√
2
!
·
1
0
!
=















0
0
0
0
1
√
2
1
√
2
0
0
0
0
0
0
1
√
2 −1
√
2
0
0
0
0
0
0
0
0
1
√
2
1
√
2
0
0
0
0
0
0
1
√
2 −1
√
2
1
√
2
1
√
2
0
0
0
0
0
0
1
√
2 −1
√
2
0
0
0
0
0
0
0
0
1
√
2
1
√
2
0
0
0
0
0
0
1
√
2 −1
√
2
0
0
0
0















·













1
0
0
0
0
0
0
0













=














0
0
0
0
1
√
2
1
√
2
0
0














.
Matrices representing quantum operators can be decomposed, for example
W4 = W2 ⊗W2 = W1 ⊗W1 ⊗W1 ⊗W1.
There are however matrices representing quantum operators that can be
not decomposed with some serious consequences.

Computation with Qubits
125
8.4
Entanglement
The following operator MCNOT s unitary and defends an injective mapping
on two qubits that is reversible
MCNOT |00⟩= |00⟩,
MCNOT|01⟩= |01⟩,
MCNOT |10⟩= |11⟩,
MCNOT|11⟩= |10⟩.
The operator MCNOT is called a controlled not gate. The ﬁrst qubit count-
ing from the left is not changed. The second qubit is only ﬂipped in the
case that the ﬁrst qubit is 1. In this case a NOT operation on the second
qubit is executed. The control not gate can as well perform the fan-out
operation. For this operation the second qubit has to be zero. In this case
the value of the ﬁrst qubit is copied into the second one. The MCNOT can
be represented by a matrix
MCNOT =




1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0



.
(8.12)
MCNOT cannot be expressed as a tensor product of 2×2 matrices. Suppose
we start with the state |00⟩and map the ﬁrst qubit bit into the superposition
using the Hadamard gate
W1 ⊗I · |00⟩= (W1 · |0⟩) ⊗|0⟩= |0⟩+ |1⟩
√
2
⊗|0⟩= |00⟩+ |10⟩
√
2
.
To this state represented by the two qubit we apply MCNOT gate
MCNOT ·
|00⟩+ |10⟩
√
2

= MCNOT · |00⟩+ MCNOT · |10⟩
√
2
= |00⟩+ |11⟩
√
2
.
A register of two qubit is decomposable if it can be represented as a
direct product of two qubits. For example the state
|00⟩+ |01⟩+ |10⟩+ |11⟩
2
=
|0⟩+ |1⟩
√
2

⊗
|0⟩+ |1⟩
√
2

is decomposable. In vector notation it is represented as




1
21
21
2
1
2



=
 
1
√
2
1
√
2
!
⊗
 
1
√
2
1
√
2
!
.

126
Principles of Quantum Artiﬁcial Intelligence
However the state of two qubits
|00⟩+ |11⟩
√
2
(8.13)
is not decomposable.
We preform a proof by contradiction.
From the
assumption that the state is decomposable follows a contradiction,
|00⟩+ |11⟩
√
2
= (ao · |0⟩+ a1|1⟩) ⊗(bo · |0⟩+ b1|1⟩) =
(8.14)
= a0 · b0 · |00⟩+ a0 · b1 · |01⟩+ a1 · b0 · |10⟩+ a1 · b1 · |11⟩
→
a0 · b0 =
1
√
2
,
a0 · b1 = 0,
a1 · b0 = 0,
a1 · b1 =
1
√
2
that is a contradiction.
A state that is not decomposable is called entangled. If two qubits are
entangled in a state |00⟩+|11⟩
√
2
, then observing one of them will result in ei-
ther |0⟩or |1⟩with probability 1
2. However, it is not possible to observe
a diﬀerent value on the other, non-observed qubit. Both qubits behave as
one unit and are called an ebit. There are four known ebits:
|00⟩+ |11⟩
√
2
,
|00⟩−|11⟩
√
2
,
|01⟩+ |10⟩
√
2
,
|01⟩−|10⟩
√
2
.
(8.15)
Once either qubit of an ebit is measured, the states of both particles
become deﬁnite.
Experiments have shown that this correlation can re-
main even if the qubits are separated over a distance of several kilometers.
Quantum collapse during measurement is a non-local force. A non-local
interaction is not limited by the speed of light, and its strength is not me-
diated with distance. This arrangement conﬂicts with Einstein’s Theory of
Special Relativity, which states that nothing can travel faster than light.
The conﬂict is resolved by the fact that one cannot use an ebit to send any
information. If two qubits of an ebit are separated over a distance in two
places, A and B, and there are no other means of communication, then
measuring the qubit on place A determines the outcome on place B, but
at place B, the outcome is unknown. Measuring at place B is a random
process without the knowledge of the results of place A. More than two
qubits can be entangled. The GHZ (Greenberger–Horne–Zeilinger) state is
entangled over M qubits with M > 2
|0⟩⊗M + |1⟩⊗M
√
2
.
(8.16)

Computation with Qubits
127
For M = 3
|000⟩+ |111⟩
√
2
.
The entanglement can arise during a computation that uses quantum gates
due to the nature of the MCNOT gate. During a computation, a part of
the system can be observed by a projection, and the vector that represents
the state is renormalized to the unit length. However, if an entanglement is
present, then the observed part determines the rest of the state. In contrast
to the conventional register, several qubits in a quantum register can form
an entity, such as an ebit. Extra care must be taken when a part of the
system is measured. A register is used to indicate if the computation is ter-
minated, |1⟩for terminated and |0⟩for not terminated. If we measure |1⟩,
then we are done. If we measure |0⟩, then we must start all of the compu-
tation from the beginning due to entanglement. There is no entanglement
between two qubits of a state if
X
i
ωi · |xi⟩|yi⟩=
 X
i
ωi · |xi⟩
!
⊗|yi⟩
that is only valid if |yi⟩= |yj⟩for all i and j. The states
X
i
ωi · |xi⟩|yi⟩,
X
i
ωi · |xi⟩
behave diﬀerently.
8.5
Quantum Boolean Circuits
A reversible circuit that is composed of m bits corresponds to a unitary
mapping that represents a permutation on m bits, deﬁning an injective
mapping Bm →Bm.
A unitary permutation matrix can represent this
unitary mapping. A more elegant method is to map the reversible circuit
into the quantum Boolean gates. Such a mapping allows us to determine the
complexity of the circuit by the number of gates. The following quantum
gates are Boolean quantum gates: the identity gate I, the NOT gate and
the control not gate MCNOT . The control not gate performs the essential
fan-out operation.
What is missing are the AND and OR operations.
These operations can be represented by the universal reversible Toﬀoli gate
(see chapter “Reversible Algorithms”). A reversible Toﬀoli gate is a unitary

128
Principles of Quantum Artiﬁcial Intelligence
mapping. It deﬁnes a quantum gate on three qubits and can be represented
by a unitary matrix M in Hilbert space H8
M =




I1 0 0 0
0 I1 0 0
0 0 I1 0
0 0 0 M¬



=













1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0













.
(8.17)
The unitary matrix M can be decomposed into several ways using non
Boolean quantum gates. However each decomposition involves the MCNOT ,
indicating that an entanglement may arise when applying a quantum Toﬀoli
gate. With the basis of three qubits of the Hilbert space H8
|000⟩=













1
0
0
0
0
0
0
0













, |001⟩=













0
1
0
0
0
0
0
0













, |010⟩=













0
0
1
0
0
0
0
0













, |011⟩=













0
0
0
1
0
0
0
0













,
|100⟩=













0
0
0
0
1
0
0
0













, |101⟩=













0
0
0
0
0
1
0
0













, |110⟩=













0
0
0
0
0
0
1
0













, |111⟩=













0
0
0
0
0
0
0
1













the mapping of the reversible T (x1, x2, x3) = (x1, x2, (x1 ∧x2) ⊕x3) corre-
sponds to the unitary mapping
M · |xyz⟩= M · |x⟩|y⟩|z⟩= |x⟩|y⟩|(x ∧y) ⊕z⟩.
(8.18)
For the AND operation, the ancilla bit z is set to 0
M · |x⟩|y⟩|0⟩= |x⟩|y⟩|(x ∧y)⟩.
(8.19)

Computation with Qubits
129
The four bit conjunction x ∧y ∧z ∧v requires three quantum Toﬀoli gates
and three additional qubits that are zero. The input:
|x⟩|y⟩|0⟩|z⟩|0⟩|v⟩|0⟩.
First quantum Toﬀoli gate
(M · |x⟩|y⟩|0⟩) ⊗(I4| · |z⟩|0⟩|v⟩|0⟩) = |x⟩|y⟩|x ∧y⟩|z⟩|0⟩|v⟩|0⟩.
Second quantum Toﬀoli gate
(I2 · |x⟩|y⟩) ⊗(M · |x ∧y⟩|z⟩|0) ⊗(I2 · |v⟩|0⟩) =
= |x⟩|y⟩|x ∧y⟩|z⟩|x ∧y ∧z⟩|v⟩|0⟩.
Third quantum Toﬀoli gate
(I4 · |x⟩|y⟩|x ∧y⟩|z⟩) ⊗(M · |x ∧y ∧z⟩|v⟩|0⟩) =
= |x⟩|y⟩|x ∧y⟩|z⟩|x ∧y ∧z⟩|v⟩|x ∧y ∧z ∧v⟩.
The circuit corresponds to the following unitary mapping
((I4 ⊗M) (I2 ⊗M ⊗I2) · (M ⊗I4)) · |xy0z0v0⟩
with the result
|x⟩|y⟩|x ∧y⟩|z⟩|x ∧y ∧z⟩|v⟩|x ∧y ∧z ∧v⟩.
The third and the ﬁfth qubit are usually not required for further computa-
tion because the result is represented in the output qubit seven. However
they are entangled with the output qubit. It is not possible to reset them
to zero. Instead they are un-computed. Because M −1 = M we recompute
the ﬁrst and the second quantum Toﬀoli gate after determining the result.
The steps are reversed, it follows
((I2 ⊗M ⊗I2) · (M ⊗I4) · (I4 ⊗M) (I2 ⊗M ⊗I2) · (M ⊗I4)) · |xy0z0v0⟩
= |x⟩|y⟩|0⟩|z⟩|0⟩|v⟩|x ∧y ∧z ∧v⟩= |xy0z0v(x ∧y ∧z ∧v)⟩.
The OR operation is represented by the unitary mapping according to the
De Morgan’s laws
((I2 ⊗MNOT ) · M · (MNOT ⊗MNOT ⊗I1)) · |xy0⟩= xy(x ∨y)⟩.
For each quantum Boolean AND, OR operation a na¨ıve implementation
requires an ancilla bit. These bits can be reused for further computation
only by reversing the preceding steps. The complexity of the circuit corre-
sponds to the number of used quantum gates. A quantum circuit represents
a permutation in Hilbert space and is not an algorithmic device. The com-
putation does not alter the distribution of the amplitudes; the von Neumann
entropy remains unchanged during the execution of the quantum Boolean
gates. The probability of measuring certain states is the same before and
after the computation; by itself, it does not oﬀer any advantage over the
classical computation.

130
Principles of Quantum Artiﬁcial Intelligence
8.6
Deutsch Algorithm
The Hadamard gate changes the von Neumann entropy before and after
the computation; it performs an operation that goes beyond the Boolean
truth operations. It maps a state with zero von Neumann entropy to a
superposition with maximal entropy. The Deutsch algorithm [Deutsch and
Jozsa (1992)] exploits the superposition of qubits generated by Hadamard
gates and is more powerful than any classical algorithm. It determines if
an unknown function f : B1 →B1 : f(x) = y of one bit is constant or
not by calling the function one time. A classical algorithm requires two
calls. A constant function on one bit is either f(x) = 1 or f(x) = 0. A non
constant function is either the identity function f(0) = 0 and f(1) = 1 or
the ﬂip function f(0) = 1 and f(1) = 0. The condition of the function being
constant f(0) = f(1) implies that the XOR operation ⊕is f(0) ⊕f(1) = 0
is zero. On the other hand if the function is not constant f(0) ̸= f(1)
implies that the XOR operation ⊕is f(0)⊕f(1) = 1 is one. We can deﬁne
a unitary operator Uf that acts on the two qubits
Uf · |xy⟩= |x⟩|f(x) ⊕y⟩.
Uf can be implemented by a quantum Boolean circuit including CNOT gate.
There are four diﬀerent cases, for f(x) = 0 with the identity mapping
i) Uf|00⟩= |0⟩|0 ⊕0⟩= |00⟩,
Uf|01⟩= |01⟩,
Uf|10⟩= |10⟩,
Uf|11⟩= |11⟩
for f(x) = 1 with the permutation of all elements.
ii) Uf|00⟩= |0⟩|1 ⊕0⟩= |01⟩,
Uf|01⟩= |00⟩,
Uf|10⟩= |11⟩,
Uf|11⟩= |10⟩
and for a non-constant function, f(x) = x corresponds to a permutation of
two elements.
iii) Uf|00⟩= |0⟩|0 ⊕0⟩= |00⟩,
Uf|01⟩= |01⟩,
Uf|10⟩= |11⟩,
Uf|11⟩= |10⟩
and f(x) = ¬x with a permutation of two elements as well
vi) Uf|00⟩= |0⟩|1 ⊕0⟩= |01⟩,
Uf|01⟩= |00⟩,
Uf|10⟩= |10⟩,
Uf|11⟩= |11⟩.
There are two classes:
• No permutation i) or permutation of all elements ii) indicates that the
function is constant.
• Permutation of two elements iii), iv) indicates that the function is non
constant.

Computation with Qubits
131
The Algorithm
to determine if f(x) is constant or not is composed of
four steps. In the ﬁrst step of the algorithm we build a superposition of
two qubits
W2 · |01⟩= W1 · |0⟩⊗W1 · |1⟩=
|0⟩+ |1⟩
√
2

⊗
|0⟩−|1⟩
√
2

=
W2 · |01⟩= 1
2 · (|00⟩−|01⟩+ |10⟩−|11⟩) .
In the second step we apply the Uf, gate.
Uf · W2 · |01⟩= Uf
1
2 · (|00⟩−|01⟩+ |10⟩−|11⟩)

=
= 1
2 · (Uf · |00⟩−Uf · |01⟩+ Uf · |10⟩−Uf · |11⟩) .
There are four possible outcomes. For constant function
i)
= 1
2 · (|00⟩−|01⟩+ |10⟩−|11⟩) =
|0⟩+ |1⟩
√
2

⊗
|0⟩−|1⟩
√
2

,
ii)
= 1
2 · (|01⟩−|00⟩+ |11⟩−|10⟩) = 1
2 · (−|00⟩+ |01⟩−|10⟩+ |11⟩)
=
−|0⟩−|1⟩
√
2

⊗
|0⟩−|1⟩
√
2

= −
|0⟩+ |1⟩
√
2

⊗
|0⟩−|1⟩
√
2

,
and for non-constant function
iii)
= 1
2 · (|00⟩−|01⟩+ |11⟩−|10⟩) = 1
2 · (|00⟩−|01⟩|10⟩+ |11⟩)
=
|0⟩−|1⟩
√
2

⊗
|0⟩−|1⟩
√
2

,
iv)
= 1
2 · (|01⟩−|00⟩+ |10⟩−|11⟩) = 1
2 · (−|00⟩+ |01⟩+ |10⟩−|11⟩)
−|0⟩+ |1⟩
√
2

⊗
|0⟩−|1⟩
√
2

= −
|0⟩−|1⟩
√
2

⊗
|0⟩−|1⟩
√
2

.
In the third step a Hadamard gate is applied to the ﬁrst qubit
(W1 ⊗I1) · Uf · W2 · |01⟩.
(8.20)
There are four possible outcomes,
i) |0⟩⊗
|0⟩−|1⟩
√
2

,

132
Principles of Quantum Artiﬁcial Intelligence
ii) −|0⟩⊗
|0⟩−|1⟩
√
2

,
iii) |1⟩⊗
|0⟩−|1⟩
√
2

,
vi)
−|1⟩⊗
|0⟩−|1⟩
√
2

.
In the fourth step the ﬁrst qubit (that is in the pure state) is measured. It
is |0⟩if the function is constant, otherwise |1⟩.
Even the Deutsch algorithm is more powerful than any classical algorithm,
it determines a unknown function of one bit by calling it only one time, it
needs three serial steps before a measurement can take place. In the next
section we generalize the working principle of the algorithm even to a more
powerful algorithm.
8.7
Deutsch Jozsa Algorithm
It determines if a unknown function f : Bm →B1 : f(x) = y of m bit
is constant or a balanced function. In constant function of m bits for all
possible n = 2m inputs the output is either 0 or 1 [Deutsch and Jozsa
(1992)]. In a balanced function half of the n = 2m input values output 0
the other half output 1. A set of the input values x of the size 2m is mapped
into two subsets called 0 and 1 each of the size 2m/2 = 2m−1. Such a two
subsets could be the subset of even and odd numbers. A classical algorithm
has to call the function 2m−1 +1 times in the worst case, since in the worst
case the output is 2m−1 times 0. If in the next call it is 0 then the function
is constant, otherwise it is guaranteed to be balanced. The Deutsch Jozsa
algorithm needs three serial steps before a measurement can take place. We
deﬁne a unitary operator Uf that acts on the m + 1 qubits with x ∈Bm
and y ∈B1
Uf · |x⟩|y⟩= |x⟩|f(x) ⊕y⟩.
The Algorithm
to determine if f(x) is constant or balanced is composed
of four steps. In the ﬁrst step of the algorithm we build a superposition of
m + 1 qubits
Wm+1 · |0⊗m⟩|1⟩= Wm · |0⊗n⟩⊗W1 · |1⟩=
1
√
2n
X
x∈Bm
|x⟩⊗
|0⟩−|1⟩
√
2

.

Computation with Qubits
133
The ﬁrst m qubits represent a superposition over all possible states with
a positive amplitude, in the last qubit one amplitude of the two possible
states is negative. In the second step we apply the Uf, operator,
Uf · Wm+1 · |0⊗m⟩|1⟩= Uf ·
 
1
√
2m
X
x∈Bm
|x⟩⊗
|0⟩−|1⟩
√
2
!
= Uf ·
 
1
√
2m+1
X
x∈Bm
|x⟩⊗(|0⟩−|1⟩)
!
=
1
√
2m+1 · Uf ·
 X
x∈Bm
(|x⟩|0⟩−|x⟩|1⟩)
!
=
1
√
2n+1 ·
X
x∈Bm
Uf · (|x⟩|0⟩−|x⟩|1⟩)
=
1
√
2m+1 ·
X
x∈Bm
(Uf · |x⟩|0⟩−Uf · |x⟩|1⟩)
=
1
√
2m+1 ·
X
x∈Bm
Uf · |x⟩|0⟩−
1
√
2m+1 ·
X
x∈Bm
Uf · |x⟩|1⟩
=
1
√
2m+1 ·
X
x∈Bm
|x⟩|f(x) ⊕0⟩−
1
√
2m+1 ·
X
x∈Bm
|x⟩|f(x) ⊕1⟩.
There are three possible outcomes. For constant function
i)
1
√
2m+1 ·
X
x∈Bm
|x⟩|0 ⊕0⟩−
1
√
2m+1 ·
X
x∈Bm
|x⟩|0 ⊕1⟩=
=
1
√
2m
X
x∈Bm
|x⟩⊗
|0⟩−|1⟩
√
2

,
ii)
1
√
2m+1 ·
X
x∈Bm
|x⟩|1 ⊕0⟩−
1
√
2m+1 ·
X
x∈Bm
|x⟩|1 ⊕1⟩=
= −
1
√
2m
X
x∈Bm
|x⟩⊗
|0⟩−|1⟩
√
2

,

134
Principles of Quantum Artiﬁcial Intelligence
and for non-constant function
iii)
1
√
2n+1 · (
X
f(x)=0
|x⟩|0 ⊕0⟩−
X
f(x)=0
|x⟩|0 ⊕1⟩+
+
X
f(x)=1
|x⟩|1 ⊕0⟩−
X
f(x)=1
|x⟩|1 ⊕1⟩) =
=
1
√
2n ·

X
f(x)=0
|x⟩−
X
f(x)=1
|x⟩

⊕
|0⟩−|1⟩
√
2

=
=
1
√
2n
X
x∈Bm
(−1)f(x) · |x⟩⊗
|0⟩−|1⟩
√
2

.
The result i), ii) can be as well represented by iii). The representation
1
√
2m
X
x∈Bm
(−1)f(x) · |x⟩⊗
|0⟩−|1⟩
√
2

(8.21)
is one of the most used notations in quantum computation. The value of
the function f(x) is encoded by (−1)f(x), the sign of the amplitude. The
last qubit

|0⟩−|1⟩
√
2

is called auxiliary, or target bit and is ignored, so the
Equation 8.21 is written as
1
√
2m
X
x∈Bm
(−1)f(x) · |x⟩.
(8.22)
In the third step a Hadamard gate is applied to the ﬁrst n qubits, the target
qubit is ignored
(Wm ⊗I1) · Uf · Wm+1 · |0⊗n⟩|1⟩
(8.23)
there are four possible outcomes,
i) |0⊗n⟩⊗
|0⟩−|1⟩
√
2

ii) −|0⊗n⟩⊗
|0⟩−|1⟩
√
2

iii) Wm ·
1
√
2m
X
x∈Bm
(−1)f(x) · |x⟩⊗I1 ·
|0⟩−|1⟩
√
2

=
= Wm ·
 
1
√
2m
X
x∈Bn
(−1)⟨z|x⟩· |x⟩
!
⊗
|0⟩−|1⟩
√
2

= |z⟩⊗
|0⟩−|1⟩
√
2


Computation with Qubits
135
vi) Wm ·
1
√
2m
X
x∈Bm
(−1)f(x) · |x⟩⊗I1 ·
|0⟩−|1⟩
√
2

=
= Wm ·
 
−1
√
2m
X
x∈Bn
(−1)⟨z|x⟩· |x⟩
!
⊗
|0⟩−|1⟩
√
2

= −|z⟩⊗
|0⟩−|1⟩
√
2

.
The results i), ii), iii) and iv) can be represented as
1
2m
X
z∈Bm
X
x∈Bm
(−1)f(x) · |z⟩⊗
|0⟩−|1⟩
√
2

.
(8.24)
In the fourth step the ﬁrst m qubits are measured. They are |0⊗m⟩if the
function is constant, for a balanced function |z⟩̸= |0⊗m⟩. The algorithm
determines as well the shape of the function f(x). The shape is represented
by the z row or column of the Wm matrix, in which either 1 represents the
value zero of the function and −1 the value one or visa versa. Before the
measurement this information is represented by the minus sign of the am-
plitude. The ﬁrst n qubits are either |z⟩or −|z⟩. After the measurement
the m qubits are |z⟩, the information about of the amplitude is lost, |0⊗m⟩is
either the constant function f(x) = 0 or f(x) = 1. The Deutsch Jozsa algo-
rithm is build on three serial steps. It maps a state with zero von Neumann
entropy to a superposition with maximal entropy, does the computation on
this superposition and maps the result into a state with zero entropy. It
provides three most important principles of quantum computation:
• The function f(x) is represented by a quantum Boolean circuit.
• The properties of the function f(x) are determined using the super-
position principle and a generalized class of Fourier transform (The
Hadamard transform).
• The values of the function f(x) are encoded by (−1)f(x), the sign of
the amplitude.
This principles are the basis for the two most revolutionary quantum algo-
rithms, Shor’s algorithm and Grover’s algorithm. Before their introduction,
some limitations of quantum computation are highlighted.
8.8
Amplitude Distribution
The register of m qubits is represented as a direct product of m qubits. It
deﬁnes n = 2m dimensional Hilbert space Hn with an orthonormal basis

136
Principles of Quantum Artiﬁcial Intelligence
|x1⟩, |x1⟩· · · |xn⟩and a state is represented as a unit-length vector
|x⟩= ω1 · |x1⟩+ ω2 · |x2⟩+ · · · + ωn · |xn⟩.
After the measurement, observation the state |x⟩is projected into a pure
state
1 · |xk⟩.
All the information about the amplitude distribution ω1, · · · , ωn of |x⟩is
lost. Could we save this inform by coping the unit-length vector |x⟩to
another state? Could we clone a state?
8.8.1
Cloning
To preform this task we deﬁne a copy machine. We chose one orthonormal
basis state of the orthonormal basis, for example |x1⟩and deﬁne a unitary
copy operator that copies an state |x⟩∈Hn as
Ucopy(|x⟩, |x1⟩) = |x⟩|x⟩.
(8.25)
Does Ucopy exist? For pure states Ucopy is deﬁned. It can be realized for
example by MCNOT with |x1⟩= |0⟩and |x2⟩= |1⟩,
Ucopy(|x1⟩, |x1⟩) = |x1⟩|x1⟩,
Ucopy(|x2⟩, |x1⟩) = |x2⟩|x2⟩.
If the state is in a superposition
|x⟩= |x1⟩+ |x2⟩
√
2
it implies that
Ucopy(|x⟩, |x1⟩) = |x⟩|x⟩=
|x1⟩+ |x2⟩
√
2

⊗
|x1⟩+ |x2⟩
√
2

=
1
2 · (|x1⟩|x1⟩+ |x1⟩|x2⟩+ |x2⟩|x1⟩+ |x2⟩|x2⟩) .
Because of the linearity of Ucopy it follows,
Ucopy(|x⟩, |x1⟩) = Ucopy
|x1⟩+ |x2⟩
√
2
, |x1⟩

=
Ucopy(|x⟩, |x1⟩) = Ucopy
|x1⟩|x1⟩+ |x2⟩|x1⟩
√
2

=
Ucopy(|x1⟩|x1⟩) + Ucopy(|x2⟩|x1⟩)
√
2
=
1
√
2 · (|x1⟩|x1⟩+ |x2⟩|x2⟩)

Computation with Qubits
137
it leads to a contradiction. An operation that would produce a copy of
an arbitrary quantum state is not possible, we cannot copy an unknown
amplitude distribution of a state. For example we cannot copy an unknown
qubit α · |0⟩+ β · |1⟩. The amplitude distribution is speciﬁed by the values
of α and β. However we can copy the basis, α · |0⟩+ β · |1⟩into the basis
α · |00⟩+ β · |11⟩. The operator copy base Ucopy∗
Ucopy∗(α · |x1⟩+ β · |x2⟩, |x1⟩)
= Ucopy∗(α · |x1x1⟩+ β · |x2x1⟩) = α · |x1x1⟩+ β · |x2x2⟩
(8.26)
exist, it can be realized by MCNOT . Ucopy∗does not change the entropy of
the register, Ucopy would change it.
8.8.2
Teleportation
It is possible to teleport a qubit from one location to another using an ebit
[Bennett et al. (1993)]. The two qubits in an ebit behave as one unit, even if
the qubits are separated. This nonlocal interaction is not limited by speed
of light, not mediated by the distance. The qubit is transferred from one
point to another without traversing the physical space. Suppose we have
two qubits that are entangled in a state |00⟩+|11⟩
√
2
. We separate the two
qubits of the ebit over a distance on two places A and B.
|0A⟩|0B⟩+ |1A⟩|1B⟩
√
2
.
(8.27)
In the ﬁrst step of the teleportation of the qubit α · |0A⟩+ β · |1A⟩from the
place A to the place B we interact with the corresponding ebit
(α · |0A⟩+ β · |1A⟩) ⊗
|0A⟩|0B⟩+ |1A⟩|1B⟩
√
2

(8.28)
α · (|0A⟩|0A⟩|0B⟩+ |0A⟩|1A⟩|1B⟩) + β · (|1A⟩|0A⟩|0B⟩+ |1A⟩|1A⟩|1B⟩)
√
2
.
(8.29)
After the interaction there are two qubits on the location A and on the
location B. In the second step we apply the MCNOT quantum gate to the
ﬁrst two qubits at the location A and on the location B we do noting
(MCNOT ⊗I1) · 1
√
2 · (α · (|0A⟩|0A⟩|0B⟩+ |0A⟩|1A⟩|1B⟩)+
β · (|1A⟩|0A⟩|0B⟩+ |1A⟩|1A⟩|1B⟩)) =

138
Principles of Quantum Artiﬁcial Intelligence
α · (|0A⟩|0A⟩|0B⟩+ |0A⟩|1A⟩|1B⟩) + β · (|1A⟩|1A⟩|0B⟩+ |1A⟩|0A⟩|1B⟩)
√
2
.
(8.30)
This can be rewritten as
α · |0A⟩⊗(|0A⟩|0B⟩+ |1A⟩|1B⟩) + β · |1A⟩⊗(|1A⟩|0B⟩+ |0A⟩|1B⟩)
√
2
.
(8.31)
In the third step we apply the W1 quantum gate to the ﬁrst qubit at the
location A and on the location B we do noting.
(W1 ⊗I2) · 1
√
2 · (α · |0A⟩⊗(|0A⟩|0B⟩+ |1A⟩|1B⟩)+
β · |1A⟩⊗(|1A⟩|0B⟩+ |0A⟩|1B⟩)) =
1
2 · (α · (|0A⟩+ 1A⟩) ⊗(|0A⟩|0B⟩+ |1A⟩|1B⟩)+
+β · (|0A⟩−1A⟩) ⊗(|1A⟩|0B⟩+ |0A⟩|1B⟩)) =
1
2 · (α · |0A⟩0A⟩|0B⟩+ α · |0A|1A⟩|1B⟩+
α · |1A⟩0A⟩|0B⟩+ α · |1A|1A⟩|1B⟩+
β · |0A⟩|1A⟩|0B⟩+ β · |0A⟩0A⟩|1B⟩−β · |0A⟩1A⟩|0B⟩−β · |0A⟩|0A⟩|1B⟩)
after rewriting the equation we get the following representation
1
2 · (|0A⟩|0A⟩⊗(α · |0B⟩+ β · |1B⟩) + |0A⟩|1A⟩⊗(α · |1B⟩+ β · |0B⟩)+
|1A⟩|0A⟩⊗(α · |0B⟩−β · |1B⟩) + |1A⟩|1A⟩⊗(α · |1B⟩−β · |0B⟩)).
In the fourth step a measurement of the ﬁrst two qubits at the place A is
done. There are four possible results; each of them has an equal probability
of being measured.
|00⟩
is measured the state collapses at place B to
α · |0⟩+ β · |1⟩
at place B no correction is nesseascary, the qubit described by its amplitude
distribution was teleported.

Computation with Qubits
139
|01⟩
is measured the state collapses at place B to
α · |1⟩+ β · |0⟩
at place B a correction is necessary to reconstruct the teleported qubit.
MNOT gate is applied.
MNOT · (α · |1⟩+ β · |0⟩) = α · |0⟩+ β · |1⟩
|10⟩
is measured the state collapses at place B to
α · |0⟩−β · |1⟩
at place B a correction is necessary to reconstruct the teleported qubit. Z
gate is applied.
Z =
1
0
0 −1

(8.32)
Z · (α · |0⟩−β · |1⟩) = α · |0⟩+ β · |1⟩
|11⟩
is measured the state collapses at place B to
α · |1⟩−β · |0⟩
at place B a correction is necessary to reconstruct the teleported qubit.
MNOT gate and then the Z gate is applied.
Z · MNOT · (α · |1⟩−β · |0⟩) = α · |0⟩+ β · |1⟩.
This transformation is also called the Y gate
Y = Z · MNOT =
 1
0
0 −1

·
0 1
1 0

=

0 1
−1 0

.
(8.33)
For the teleportation of qubits classical communication is required. To
indicate how to reconstruct one qubit two bits have to be send over a clas-
sical channel, since one teleported qubit can take four diﬀerent superposi-
tions. It follows that an ebit cannot be used to send or teleport information,
additionally a classical channel is required.
8.9
Geometric Operations
A unitary operator performs a rotation or a reﬂection of a state represented
by a unit length vector in a Hilbert space. States may be equivalent if they
diﬀer only by the relative amplitudes, diﬀerent states when measured are
always equal.

140
Principles of Quantum Artiﬁcial Intelligence
Equivalent states
Two equivalent states represent the same state when
a measurement is preformed, but they can have behave diﬀerently during
the unitary evolution. Two states |x⟩and |y⟩are equivalent |x⟩≡|y⟩if
|x⟩= ei·θ · |y⟩
(8.34)
with
ei·θ = cos θ + i · sin θ.
(8.35)
For example |0⟩and −|0⟩are two equivalent states
|0⟩≡−|0⟩⇔|0⟩= −ei·π · |0⟩
(8.36)
for θ = π
ei·π = cos π + i · sin π = −1.
Other examples are
|0⟩≡i · |0⟩⇔|0⟩= i · ei·−π/2 · |0⟩
(8.37)
for θ = −π/2
ei·−π/2 = cos −π/2 + i · sin −π/2 = −i
and
|0⟩≡|0⟩+ i · |0⟩
√
2
⇔|0⟩= 1 + i
√
2 · ei·π/4 · |0⟩
(8.38)
for θ = −π/4
ei·(−π/4) = cos(−π/4) + i · sin(−π/4) = 1 −i
√
2
1 −i
√
2 · 1 + i
√
2 = 2
2 = 1.
However the following two state is not equal nor not equivalent
|0⟩+ |1⟩
√
2
̸= |0⟩−|1⟩
√
2
.
They are the reﬂection of each other.

Computation with Qubits
141
Reﬂection
An example of a reﬂection operator is the Z operator
Z =
 1
0
0 −1

.
It preforms a reﬂection on the basis deﬁned by |0⟩. The Z gate is a special
case of the phase gate
P =
 1 0
0 ei·θ

(8.39)
with θ = π. A phase gate alters the relative amplitudes but represents
the same state value when a measurement is preformed and can be used
together with the MNOT gate
 ei·θ
0
0 ei·θ

=
1 0
0 ei·θ

·
0 1
1 0

·
1
0
0 e−i·θ

.
(8.40)
Rotation
A rotation by an angle α is represented by the unitary operator
R
R =
cos α −sin α
sin α cos α

.
(8.41)
It can be shown that a unitary transformation is a rotation of n = 2m
Hilbert space [Rieﬀel and Polak (2011)].
Changing the basis
In data analysis the Karhunen-Lo`eve transforma-
tion rotates the coordinate system in such a way that the covariance matrix
is diagonal, means each dimension is uncorrelated. In quantum computa-
tion a unitary transformation is equivalent to a change of the basis.
Closure relation
For a basis an orthonormal basis
|x1⟩, |x1⟩. · · · |xn⟩
the identity operator is represented as
n
X
i=1
|xi⟩⟨xi|.
(8.42)
With the inner product
⟨x|xi⟩= ωi
an state |x⟩can be represented as
|x⟩= I · |x⟩=
 n
X
i=1
|xi⟩⟨xi|
!
|x⟩=
n
X
i=1
|xi⟩⟨xi|x⟩=
n
X
i=1
ωi · |xi⟩.
(8.43)

142
Principles of Quantum Artiﬁcial Intelligence
An operator A can be represented using the closure relation as
A = I · A · I =
 n
X
i=1
|xi⟩⟨xi|
!
· A ·


n
X
j=1
|xj⟩⟨xj|

=
X
i,j
⟨xi|A · |xj⟩· |xi⟩⟨xj|
(8.44)
with aij = ⟨xi|A · |xj⟩being the number of the operator matrix A at row
i and column j for the base |x1⟩, |x1⟩. · · · |xn⟩. For a diﬀerent orthonormal
basis
|y1⟩, |y1⟩. · · · |yn⟩
the operator A is represented as a′
ij = ⟨yi|A · |yj⟩
A′ =





⟨y1|A · |y1⟩⟨y1|A · |y2⟩· · · ⟨y1|A · |yn⟩
⟨y2|A · |y1⟩⟨y2|A · |y2⟩· · · ⟨y2|A · |yn⟩
...
... ...
...
⟨yn|A · |y1⟩⟨yn|A · |y2⟩· · · ⟨yn|A · |yn⟩




.
(8.45)
The change of the basis |xi⟩to yi⟩is represented by the operator U
U =





⟨y1|x1⟩⟨y1|x2⟩· · · ⟨y1|xn⟩
⟨y2|x1⟩⟨y2|x2⟩· · · ⟨y2|xn⟩
...
... ...
...
⟨yn|x1⟩⟨yn|x2⟩· · · ⟨yn|xn⟩




.
(8.46)
A vector |x⟩is changed to the basis |yi⟩by the basis change
|x′⟩= U · |x⟩
(8.47)
|x′⟩is the same vector as |x⟩represented in the basis |yi⟩. This method is
also called the unitary transformation. If we apply an operator A to |x⟩
and represent the result in the basis |yi⟩we do the following operation
|z⟩= U · A · |x⟩= U · A · U ∗· U · |x⟩.
(8.48)
The operator A is represented in the new basis as
A′ = U · A · U ∗.
(8.49)
In the following example in H2 we change from the basis
|0⟩=
 1
0

,
|1⟩=
0
1

to the the Hadarmad basis

Computation with Qubits
143
|+⟩=
 
1
√
2
1
√
2
!
,
|−⟩=
 
1
√
2
−1
√
2
!
.
The change of the basis |0⟩, |1⟩to |+⟩, |−⟩is represented by the operator U
U =
 ⟨+|0⟩⟨+|1⟩
⟨−|0⟩⟨−|1⟩

=
 
1
√
2
1
√
2
1
√
2
−1
√
2
!
= W1.
(8.50)
The MNOT gate
MNOT =
0 1
1 0

is represented in the basis |+⟩, |−⟩as
M ′
NOT = U · MNOT · U ∗=
 
1
√
2
1
√
2
1
√
2
−1
√
2
!
·
 0 1
1 0

·
 
1
√
2
1
√
2
1
√
2
−1
√
2
!
=
 1
0
0 −1

.
(8.51)
A base change corresponds to the unitary transformation.
In quantum physics there are two models:
• The Heisenberg picture, the state vectors are time-independent, the
basis change in time.
• The Schr¨odinger picture, the states evolve in time, the basis does not
change in time.
Both approaches are similar and depend on observer. Either he is inside
the coordinate system or outside. Either the sun is rotating around the
earth, or the earth is rotating around the sun. In quantum computation
the Schr¨odinge picture is used.

This page intentionally left blank
This page intentionally left blank

Chapter 9
Periodicity
9.1
Fourier Transform
A way to solve a problem is to transform it into some other problem, for
which a solution is known. Transformations are applied to signals to obtain
further information from the signal that is not readily available in the raw
signal. One such transformation is the Fourier transform. Many signals are
represented in the time domain, and some additional information is present
in the frequency content. A Fourier transform maps the signal from the
time domain to the frequency domain. The frequency is the number of
occurrences of a repeating event per unit time. The period is the duration
of one cycle of an event, and the period is the reciprocal of the frequency
f. For example, if we count 40 events in two seconds, then the frequency is
40
2 s = 20
1 s = 20 1
s = 20 hertz
then the period is
T = p = 1
20s.
A repeated event can be a rotation, oscillation, or a periodic wave. For
periodic waves, one period corresponds to the time in which a full cycle of
a wave passes. A cycle is represented by the wavelength. The velocity v
of the wave is represented by the wavelength λ divided by the period p.
Because the frequency f is the inverse of the period, we can represent the
velocity as
v = λ
p = λ · f
(9.1)
and the frequency as
f = 1
T = 1
p = v
λ.
(9.2)
145

146
Principles of Quantum Artiﬁcial Intelligence
If something changes rapidly, then we say that it has a high frequency. If it
does not change rapidly, i.e., it changes smoothly, we say that it has a low
frequency. The Fourier transform changes a signal from the time domain
x(t) ∈C to the frequency domain X(f) ∈C. The representation of the
signal x(t) in the frequency domain X(f) is the frequency spectrum. This
representation has the amplitude or phase plotted versus the frequency. In
a wave, the amplitude describes the magnitude of change and the phase
of the fraction of the wave cycle that has elapsed relative to the origin.
The complex number X(f) conveys both the amplitude and phase of the
frequency f. The absolute value |X(f)| represents the amplitude of the
frequency f. The phase is represented by the argument of X(f), arg(X(f)).
For a complex number
z = x + i · y = |z| · ei·θ
(9.3)
θ is the phase
θ = arg(z) = tan−1 y
x

(9.4)
and
|z| =
p
x2 + y2
(9.5)
phase is an angle (radians), and that negative phase corresponds to positive
time delay of the wave. For example if we shift the cosines function by the
angle θ
cos(x) →cos(x −θ)
the phase of the cosines wave is shifted. It follows as well that
sin(x) = cos(x −π/2).
(9.6)
The Fourier transform of x(t) is
X(f) =
Z ∞
−∞
x(t) · e−2·π·i·t·fdt
(9.7)
t stands for time and f for frequency. The signal x(t) is multiplied with an
exponential term at some certain frequency f, and then integrated over all
times. The frequency spectrum of a real valued signal is always symmetric,
since the symmetric part is exactly a mirror image of the ﬁrst part the
second part is usually not shown. The inverse Fourier transform of X(f) is
x(t) =
Z ∞
−∞
X(f) · e2·π·i·t·fdf.
(9.8)

Periodicity
147
9.2
Discrete Fourier Transform
The discrete Fourier transforms discrete time-based or space-based data
into the frequency sequency-based data. Given a seqience α
αt : [1, 2, · · · , n] →C.
(9.9)
The discrete Fourier transform produces a sequence ω:
ωf : [1, 2, · · · , n] →C.
(9.10)
The discrete Fourier transform of α(t) is
ωf =
1
√n ·
n
X
t=1
αt · e−2·π·i·(t−1)· (f−1)
n
(9.11)
its wave frequency is (f−1)
n
events per sample. The inverse discrete Fourier
transform of ωf is
αt =
1
√n ·
n
X
f=1
ωf · e2·π·i·(t−1)· (f−1)
n
.
(9.12)
Discrete Fourier transform (DFT) can be seen as a linear transform F
talking the column vector α to a column vector ω
ω = F · α
(9.13)





ω1
ω2
...
ωn




= F · α =
=
1
√n ·






e−2·π·i·(0)· (0)
n
e−2·π·i·(0)· (1)
n
· · · e−2·π·i·(0)· (n−1)
n
e−2·π·i·(1)· (0)
n
e−2·π·i·(1)· (1)
n
· · · e−2·π·i·(1)· (n−1)
n
...
...
...
...
e−2·π·i·(n−1)· (0)
n e−2·π·i·(n−1)· (1)
n · · · e−2·π·i·(n)· (n−1)
n






·





α1
α2
...
αn





(9.14)
and the inverse discrete Fourier transform (IDFT) can be seen as a linear
transform IF talking the column vector ω to a column vector α
α = IF · ω
(9.15)

148
Principles of Quantum Artiﬁcial Intelligence





α1
α2
...
αn




=
1
√n·






e2·π·i·(0)· (0)
n
e2·π·i·(0)· (1)
n
· · · e2·π·i·(0)· (n−1)
n
e2·π·i·(1)· (0)
n
e2·π·i·(1)· (1)
n
· · · e2·π·i·(1)· (n−1)
n
...
...
...
...
e2·π·i·(n−1)· (0)
n e2·π·i·(n−1)· (1)
n · · · e2·π·i·(n)· (n−1)
n






·





ω1
ω2
...
ωn




.
(9.16)
The the matrix F can be represented as a Vandermonde matrix using the
nth root of unity. An nth root of unity is a complex number ζ satisfying
the equation
ζn = 1
(9.17)
with n = 1, 2, 3, · · · , n −1 being a a positive integer, for example
ζn = e−2·π·i· 1
n = cos

2 · π · 1
n

−i · sin

2 · π · 1
n

(9.18)
with exponential of the complex number
ei·x = cos(x) + i · sin(x)
(9.19)
and
e−i·x = cos(x) −i · sin(x).
(9.20)
With ζn = e−2·π·i· 1
n the matrix F can be represented as
F =
1
√n ·






ζ(0)·(0)
n
ζ(0)·(1)
n
· · ·
ζ·(0)·(n−1)
n
ζ(1)·(0)
n
ζ(1)·(1)
n
· · ·
ζ·(1)·(n−1)
n
...
...
...
...
ζ(n−1)·(0)
n
ζ(n−1)·(1)
n
· · · ζ·(n−1)·(n−1)
n






.
(9.21)
A Vandermonde matrix V is a matrix with the terms of a geometric pro-
gression in each row
V =








1 γ1 γ2
1 γ3
1 · · · γ(n−1)
1
1 γ2 γ2
2 γ3
2 · · · γ(n−1)
2
1 γ3 γ2
3 γ3
3 · · · γ(n−1)
3
...
... ... ...
...
...
1 γn γ2
n γ3
n · · · γ(n−1)
n








.
(9.22)
F is a Vandermonde matrix, it can be represented as
F =
1
√n ·











1
1
1
1
· · ·
1
1
ζn
ζ2
n
ζ3
n
· · ·
ζ(n−1)
n
1
ζ2
n
ζ4
n
ζ6
n
· · ·
ζ2·(n−1)
n
1
ζ3
n
ζ6
n
ζ9
n
· · ·
ζ3·(n−1)
n
...
...
...
...
...
...
1 ζ(n−1)
n
ζ2·(n−1)
n
ζ3·(n−1)
n
· · · ζ(n−1)·(n−1)
n











.
(9.23)

Periodicity
149
The matrix F, also called DFT matrix is unitary
F −1 = F ∗= IF.
(9.24)
Because F is unitary it implies that the length of a vector is preserved as
stated in Parseval’s theorem
∥ω∥= ∥F · α∥= ∥α∥.
(9.25)
9.2.1
Example
We generates a list with 256 = 28 elements containing a periodic signal αt
with Gaussian random noise from the interval [−0.5, 0.5].
αt = sin
50 · t · 2 · ·π
256

+ noise.
The represented data looks random (see Figure 9.1).
50
100
150
200
250
!1.0
!0.5
0.5
1.0
Fig. 9.1
A periodic signal αt with with Gaussian random noise.
The discrete Fourier transform ωf of the real valued signal αt is sym-
metric. It shows a strong peak at 50+1 and a symmetric peak at 256−50+1
representing the frequency component of the signal αt (see Figure 9.2). The
zero frequency term represents the DC average and appears at position 1
instead at the position 0.
A ﬁlter that reduces Gaussian noise based on DFT removes frequencies
with low amplitude of ωf and performs inverse discrete Fourier transform.

150
Principles of Quantum Artiﬁcial Intelligence
50
100
150
200
250
2
4
6
8
Fig. 9.2
The discrete Fourier transform ωf . It shows a strong peak at 50 + 1 and a
symmetric peak at 256 −50 + 1 representing the frequency component of the signal αt.
The zero frequency term represents the DC average and appears at position 1 instead at
the position 0.
9.3
Quantum Fourier Transform
For n = 2m F performs a Quantum Fourier Transform (QFT) on a state
|x⟩of m qubits in a n-dimensional Hilbert space Hn
|x⟩= α1 · |x1⟩+ α2 · |x2⟩+ · · · + αn · |xn⟩.
The QFT is deﬁned as
|y⟩= Fm · |x⟩
(9.26)
with
|y⟩= ω1 · |x1⟩+ ω2 · |x2⟩+ · · · + ωn · |xn⟩
and inverse QFT is deﬁned as
|x⟩= IFm · |y⟩= F ∗
m · |y⟩.
(9.27)
For one qubit m = 1 , n = 2
ζ2 = e−2·π·i· 1
2 = e−π·i = eπ·i = −1
and the QFT F1 is
F1 =
1
√
2
·
 1 1
1 ζ2

=
1
√
2
·
1
1
1 −1

= W1.
(9.28)
F1 is just a Hadamard transform W1 of one qubit in Hilbert space H2.
A Hadamard transform of m qubits in Hilbert space Hn with n = 2m is
equivalent to a multidimensional two size discrete Fourier transforms F1
Wm =
OmW1 =
OmF1 = F1 ⊗F1 · · · ⊗F1.
(9.29)

Periodicity
151
For two qubits m = 2 , n = 4
ζ4 = e−2·π·i· 1
4 = e−·π·i· 1
2 = −i
and the QFT F2 is
F2 =
1
√
4
·




1 1 1 1
1 ζ4 ζ2
4 ζ3
4
1 ζ2
4 ζ4
4 ζ6
4
1 ζ3
4 ζ6
4 ζ9
4



= 1
2 ·




1
1
1
1
1 −i −1
i
1 −1
1 −1
1
i −1 −i




(9.30)
and the inverse QFT IF2 is
IF2 = F ∗
2 = 1
2 ·




1
1
1
1
1
i −1 −i
1 −1
1 −1
1 −i −1
i



.
(9.31)
For three qubits m = 3 , n = 8
ζ8 = e−2·π·i· 1
8 = e−π·i· 1
4 = 1 −i
√
2
F3 =
1
√
8 ·













1 1
1
1
1
1
1
1
1 ζ1
8 ζ2
8
ζ3
8
ζ4
8
ζ5
8
ζ6
8
ζ7
8
1 ζ2
8 ζ4
8
ζ6
8
ζ8
8 ζ10
8
ζ12
8
ζ14
8
1 ζ3
8 ζ6
8
ζ9
8 ζ12
8
ζ15
8
ζ18
8
ζ21
8
1 ζ4
8 ζ8
8 ζ12
8
ζ16
8
ζ20
8
ζ24
8
ζ28
8
1 ζ5
8 ζ10
8
ζ15
8
ζ20
8
ζ25
8
ζ30
8
ζ35
8
1 ζ6
8 ζ12
8
ζ18
8
ζ24
8
ζ30
8
ζ36
8
ζ42
8
1 ζ7
8 ζ14
8
ζ21
8
ζ28
8
ζ35
8
ζ42
8
ζ49
8













(9.32)
F3 =
1
√
8 ·













1
1
1
1
1
1
1
1
1 e−π·i· 1
4 −i e−π·i· 3
4 −1
eπ·i· 3
4
i
eπ·i· 1
4
1
−i −1
i
1
−i −1
i
1 e−π·i· 3
4
i e−π·i· 1
4 −1
eπ·i· 1
4
i
eπ·i· 3
4
1
−1
1
−1
1
−1
1
−1
1
eπ·i· 3
4 −i
eπ·i· 1
4 −1 e−π·i· 1
4
i e−π·i· 3
4
1
i −1
−i
1
i −1
−i
1
eπ·i· 1
4
i
eπ·i· 3
4 −1 e−π·i· 3
4 −i e−π·i· 1
4













.
(9.33)
The ﬁrst row of F3 is the DC average of the amplitude of the input state
when measured, the following rows represent the AC (diﬀerence) of the

152
Principles of Quantum Artiﬁcial Intelligence
input state amplitudes. The QFT operation on the state |x⟩is













ω1
ω2
ω3
ω4
ω5
ω6
ω7
ω8













=
1
√
8 ·














1
1
1
1
1
1
1
1
1
1−i
√
2 −i −1−i
√
2
−1 −1+i
√
2
i
1+i
√
2
1
−i −1
i
1
−i −1
i
1 −1−i
√
2
−i
1−i
√
2 −1
1+i
√
2
i −1+i
√
2
1
−1
1
−1
1
−1
1
−1
1 −1+i
√
2
−i
1+i
√
2 −1
1−i
√
2
i −1−i
√
2
1
i −1
−i
1
i −1
−i
1
1+i
√
2 −i −1+i
√
2
−1 −1−i
√
2
i
1−i
√
2














·













α1
α2
α3
α4
α5
α6
α7
α8













(9.34)
F3 can be represented as a sum of a real and imaginary matrix.
F3 =
1
√
8 ·














1
1
1
1
1
1
1
1
1
1
√
2
0 −1
√
2 −1 −1
√
2
0
1
√
2
1
0 −1
0
1
0 −1
0
1 −1
√
2
0
1
√
2 −1
1
√
2
0 −1
√
2
1 −1
1 −1
1 −1
1 −1
1 −1
√
2
0
1
√
2 −1
1
√
2
0 −1
√
2
1
0 −1
0
1
0 −1
0
1
1
√
2
0 −1
√
2 −1 −1
√
2
0
1
√
2














+
(9.35)
+ 1
√
8 ·














0
0
0
0 0
0 0
0
0 −i
√
2 −i −i
√
2 0
i
√
2 i
i
√
2
0 −i
0
i 0 −i 0
i
0 −i
√
2 −i −i
√
2 0
i
√
2 i −i
√
2
0
0
0
0 0
0 0
0
0
i
√
2 −i
i
√
2 0 −i
√
2 i −i
√
2
0
i
0 −i 0
i 0 −i
0
i
√
2 −i
i
√
2 0 −i
√
2 i −i
√
2














.
(9.36)
The ﬁrst row measures the DC, the second row fractional frequency of the
amplitude of the input state of 1/8, the third of 1/4 = 2/8, the fourth of
3/8, the ﬁfth of 1/2 = 4/8, the sixth of 5/8, the seventh of 3/4 = 6/8 and
the eighth of 7/8 or equivalently the fractional frequency of −1/8. The
resulting frequency of the amplitude vector ω of the state |y⟩for a real
valued amplitude vector α of the state |x⟩is symmetric, ω2 = ω6, ω3 = ω7
and ω4 = ω8.

Periodicity
153
9.4
FFT
The Hadamard transform Hm is composed of multidimensional two size
discrete Fourier transforms F1, it is a subset of DFT [Cormen et al. (2001)].
DFT Fm is related to the Hadamard transform but cannot be decomposed
as a tensor product of 2 × 2 matrices. Any operator can be represented by
single qubit gates together with MCNOT gates, however their number can
grow exponential in the number if qubits [Rieﬀel and Polak (2011)]. An
eﬃcient decomposition is represented by the fast Fourier transform (FFT).
Carl Friedrich Gauss invented the FFT algorithm around 1805. However
because the corresponding article was written in Latin it did not gain any
popularity. FFT was several times rediscovered and it was made popular by
J. W. Cooley and J. W. Tukey in 1965 [Cormen et al. (2001)]. The original
algorithm is limited to the DFT matrix of the size 2m × 2m, power of two.
Variants of the algorithm for the case in which the size of the matrix is not
power of two exist. The original algorithm decomposes Fm recursively.
Fm+1 =
1
√
2
·
 Im
Dm
Im −Dm

·
 Fm
0
0 Fm

· Rm+1
(9.37)
with the permutation matrix Rm given that n = 2m
Rm =



r11 · · · r1n
...
...
...
rn1 · · · rnn



(9.38)
with
rab =



1
if
2 · a −1 = b
1
if
2 · a −n = b
0
else
(9.39)
and the diagonal matrix with n · 2 = 2m+1
Dm =





ζ0
n·2
0
· · ·
0
0
ζ1
n·2 · · ·
0
...
...
...
...
0
0
· · · ζn−1
n·2




.
(9.40)
For example F2 is decomposed with
D1, ζ4 = e−2·π·i· 1
4 →D1 =
 
e−·π·i· 0
2
0
0
e−·π·i· 1
2
!
=
 1
0
0 −i

(9.41)

154
Principles of Quantum Artiﬁcial Intelligence
and
R2 =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1




(9.42)
it follows
F2 =
1
√
2 ·




1 0 1
0
0 1 0 −i
1 0 −1 0
0 1 0
i



· 1
√
2 ·




1
1 0
0
1 −1 0
0
0
0 1
1
0
0 1 −1



·




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1




(9.43)
F2 = 1
2 ·




1
1
1
1
1 −i −1
i
1 −1
1 −1
1
i −1 −i



.
(9.44)
The complexity of the FFT algorithm that decomposes Fm recursively is
O(n · m).
9.5
QFT Decomposition
The Hadamard transform Wm can be decomposed into a tensor product of
m 2 × 2 matrices representing W1
Wm =
OmW1 = W1 ⊗W1 · · · ⊗W1
so that the quantum complexity is O(m) [Rieﬀel and Polak (2011)]. Fm
cannot be decomposed as a tensor product of 2 × 2 matrices. Using the
decomposition of the FFT algorithm and decomposing the corresponding
matrices by the tensor product a quantum complexity of O(m2) = O(m·m)
can be achieved. The decomposition performs m steps. In each step is rep-
resented by a product of three unitary matrices. They can be decomposed
at the step k into k tensor products. It folows
m + (m −1) + (m −2) + · · · + 1 = m · (m −1)
2
= O(m2).
(9.45)
The permutation matrix Rm is unitary and can be decomposed into a tensor
product of m swap operators S preforming a swap operation on one qubit
states |x⟩and |y⟩,
S|xy⟩= |yx⟩
(9.46)

Periodicity
155
with
S|00⟩= |00⟩,
S|01⟩= |10⟩,
S|10⟩= |01⟩,
S|11⟩= |11⟩
and the matrix representation
S =




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



.
(9.47)
The following matrix can be recursively decomposed
 Fm
0
0 Fm

= I1 ⊗Fm
(9.48)
and
1
√
2 ·
 Im
Dm
Im −Dm

= (W1 · |0⟩⟨0|·) ⊗Im + (W1 · |1⟩⟨1|·) ⊗Dm
(9.49)
with
Dm = Dm−1 ⊗
 1
0
0 ζn·2

.
(9.50)
9.5.1
QFT quantum circuit∗
We can rewrite the recursive decomposition into a “kind” of tensor product
using the binary representation of |x⟩, |y⟩. This representation is popular,
however it not a real tensor decomposition. The QFT on a state |x⟩of m
qubits in a n-dimensional Hilbert space Hn = H2m can be represented as
[Kaye et al. (2007)]
|y⟩= Fm · |x⟩=
1
√n
X
y∈Bm
e−2·π·i· y
n ·x · |y⟩.
(9.51)
It is just the discrete Fourier transform of α(t) in the bra-ket notation
ωf =
1
√n ·
n
X
t=1
αt · e−2·π·i· (f−1)
n
·(t−1).
The binary representation of x of m bits is given by
x = xm · 2m−1 + xm−1 · 2m−2 + · · · + x2 · 21 + x1 · 20
(9.52)
and of y by
y = ym · 2m−1 + ym−1 · 2m−2 + · · · + y2 · 21 + y1 · 20.
(9.53)

156
Principles of Quantum Artiﬁcial Intelligence
We can represent the multiplication of
e
−2·π·i·y·x
n
= e
−2·π·i·y·x
2m
= e
−2·π·i·(ym·2m−1+···1+y1·20·x)··(xm·2m−1···+x2·21+x1·20)
2m
=
(9.54)
= e
−2·π·i·(ym·2m−1·(xm·2m−1···+x2·21+x1·20)+···+y1·20·(xm·2m−1···+x2·21+x1·20))
2m
.
(9.55)
Because
e−2·π·i·(a+b+c) = e(−2·π·i·a)+(−2·π·i·b)+(−2·π·i·c) =
= e(−2·π·i·a) · e(−2·π·i·b) · e(−2·π·i·c)
and e−2·π·i is a nth root of unity
e−2·π·i·n = 1,
n ∈N0 = {0, 1, 2, 3, · · ·}.
we can ignore in
e
−2·π·i·(ym·2m−1·(xm·2m−1···+x2·21+x1·20)+···+y1·20·(xm·2m−1···+x2·21+x1·20))
2m
the terms divisible by n = 2m. For example
e−2·π·i·(1+ 1
2 +2) = e(−2·π·i·1) · e(−2·π·i· 1
2 ) · e(−2·π·i·3) = 1 · e(−2·π·i· 1
2 ) · 1 = −1.
It follows that
e
−2·π·i·y·x
2m
=
= e−2·π·i·(ym· x1
21 +ym−1·(
x2
21 + x1
22 )+ym−2·(
x3
21 + x2
22 + x1
23 )+···+y1·( xm
21 +
xm−1
22
+···+ x1
2m ))
(9.56)
using the binary fraction notation for binary numbers
e
−2·π·i·y·x
2m
=
= e−2·π·i·(ym·0.x1+ym−1·0.x2x1+ym−2·0.x3x2x1+···+y1·0.xmxm 1xm−2···x2x1)
(9.57)
binary fractions are represented as
0.xmxm 1xm−2 · · · x2x1 = xm
21 + xm−1
22
+ · · · + x1
2m .
So the QFT can be factored into the tensor product of m single-qubit
operations,
|y⟩= Fm · |x⟩=
1
√n
X
y∈Bm
e−2·π·i· y
n ·x · |y⟩=

Periodicity
157
1
√n ·


X
ym∈{0,1}
e−2·π·i·ym·0.x1

·


X
ym−1∈{0,1}
e−2·π·i·ym−1·0.x2x1

· · ·
· · ·


X
y1∈{0,1}
e−2·π·i·y1·0.xm···x2x1


(9.58)
=
1
√n ·
 |0⟩+ e−2·π·i·0.x1 · |1⟩

⊗
 |0⟩+ e−2·π·i·0.x2x1 · |1⟩

⊗· · · ⊗
(9.59)
⊗
 |0⟩+ e−2·π·i·0.xm···x2x1 · |1⟩

.
The representation involeves the input in the tensor decomposition. For
example the equivalent decomposition of the the Hadamard matrix would
involve the input to determine the sign,
Wm =
1
√n · (|0⟩+ (−1)xm · |1⟩) ⊗(|0⟩+ (−1)xm−1 · |1⟩) ⊗· · ·
⊗(|0⟩+ (−1)x1 · |1⟩)
if the corresponding input is zero the sign is positive, if it is one the sign
is negative. The product of m single-qubit operations of the QFT allows
us to deﬁne a quantum circuit. The circuit will use a controlled phase gate
CRk that performs following mapping on two qubits
CRk|00⟩= |00⟩,
CRk|01⟩= |01⟩,
CRk|10⟩= |10⟩,
CRk|11⟩= e−2·π·i·/2k · |11⟩.
The general phase gate is
P =
1 0
0 ei·θ

.
The phase gate Rk is
Rk =
 
1
0
0 e−2·π·i·/2k
!
(9.60)
and the controlled phase gate CRk is
CRk =





1 0 0
0
0 1 0
0
0 0 1
0
0 0 0 e−2·π·i·/2k




.
(9.61)

158
Principles of Quantum Artiﬁcial Intelligence
We demonstrate the deﬁnition of the quantum circuit on F2
F2 =
1
√
4 ·
 |0⟩+ e−2·π·i·0.x1 · |1⟩

⊗
 |0⟩+ e−2·π·i·0.x2x1 · |1⟩

(9.62)
on the input |x2x1⟩. We deﬁne the circuit recursively from the back. Be-
cause
e−2·π·i·0.x1 = e−2·π·i· x1
2 = (−1)x1
it follows that
1
√
2 ·
 |0⟩+ e−2·π·i·0.x1 · |1⟩

=
1
√
2 · (|0⟩+ (−1)x1 · |1⟩)
can be represented by
(I1 ⊗W1) · |x2x1⟩.
The “ﬁrst” operation can be represented as
1
√
2 ·
 |0⟩+ e−2·π·i·0.x2x1 · |1⟩

=
1
√
2 ·

|0⟩+ e−2·π·i· x2
21 · e−2·π·i· x1
22 · |1⟩

and can be represented as
CR1 · (W1 ⊗I1) · |x2x1⟩.
Together we get
(I1 ⊗W1) · CR2 · (W1 ⊗I1) · |x2x1⟩=
=
1
√
4
·
 |0⟩+ e−2·π·i·0.x2x1 · |1⟩

⊗
 |0⟩+ e−2·π·i·0.x1 · |1⟩

(9.63)
The arrangement of the bits is is not correct. This is because the last qubit
in the result uses the ﬁrst input qubit and so on. To correct the order we
have to apply swap gate S as deﬁned for the FFT . The decomposition is
given by
F2 · |x2x1⟩= S · (I1 ⊗W1) · CR1 · (W1 ⊗I1) · |x2x1⟩.
(9.64)
or in matrix notation
F2 = 1
2 ·




1
1
1
1
1 −i −1
i
1 −1
1 −1
1
i −1 −i



=




1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1



·
1 0
0 1

⊗1
√
2 ·
1 1
1 −1

·
·




1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 −i



·
 1
√
2 ·
 1 1
1 −1

⊗
1 0
0 1

.

Periodicity
159
For F3 we need to deﬁne phase gate on three qubits |x3x2x1⟩and a swap
operation of the ﬁrst and last qubit. The swap operation is simply the swap
of the value of x1 with the value of x2, of the value of x2 with the value of
x3 and ﬁnally of the value of x1 with the value of x2,
(I1 ⊗S) · (S ⊗I1) · (I1 ⊗S).
The phase gate on third qubit controlled by the second qubit is simply
CRk ⊗I1.
The phase gate on third qubit controlled by the ﬁrst qubit is
(I1 ⊗S) · (CRk ⊗I1) · (I1 ⊗S)
F3 =
1
√
8 ·
 |0⟩+ e−2·π·i·0.x1 · |1⟩

⊗
 |0⟩+ e−2·π·i·0.x2x1 · |1⟩

⊗
⊗
 |0⟩+ e−2·π·i·0.x3x2x1 · |1⟩

(9.65)
the decomposition is given by
[(I1 ⊗S) · (S ⊗I1) · (I1 ⊗S)] · [(I2 ⊗W1)] · [(I1 ⊗CR1) · (I1 ⊗W1 ⊗I1)]
·[(I1 ⊗S) · (CR2 ⊗I1) · (I1 ⊗S) · (CR1 ⊗I1) · (W1 ⊗I2)] · |x3x2x1⟩.
The ﬁrst term requires one Hadamard gate, the second one requires a
Hadamard gate and a controlled phase gate. Each following term requires
an additional controlled phase gate. Summing up
1 + 2 + 3 + · · · (m −1) + m = m · (m −1)
2
= O(m2).
9.6
QFT Properties
QFT is just a simple DFT represented by the DFT matrix. However one
should take care of the fact that in quantum computing literature it is
common that the QFT is deﬁned as the inverse discrete Fourier transform
(IDFT) and the inverse QFT as the DFT. QFT decomposition is motivated
by FFT. The complexity of QFT is O(m2) that is exponentially less then
O(n · m) of the classical FFT. The saving results from the possible ten-
sor decomposition that can be computed in parallel. The main diﬀerence
between DFT and QFT beside the time complexity is quite obvious. In
QFT we cannot access the frequency domain of a signal represented by the

160
Principles of Quantum Artiﬁcial Intelligence
amplitude distribution of the state |y⟩. We can only gain some insight by
repeated experiments, measurements. DFT can be deﬁned for any dimen-
sion n, for example n = 3 or n = 6. With n = 6 we could deﬁne a quantum
dice that cannot be represented by qubits. The basis of a 6 dimensional
Hilbert space H6 is
|I⟩=









1
0
0
0
0
0









, |II⟩=









0
1
0
0
0
0









, |III⟩=









0
0
1
0
0
0









,
|IV ⟩=









0
0
0
1
0
0









, |V ⟩=









1
0
0
0
1
0









, |V I⟩=









0
0
0
0
0
1









.
(9.66)
The DFT for the basis in H6 is F 6
ζ6 = e−2·π·i· 1
6 = e−π·i· 1
3 = 1 −i ·
√
3
2
F 6 =
1
√
6 ·









1 1
1
1
1
1
1 ζ1
6 ζ2
6
ζ3
6
ζ4
6
ζ5
6
1 ζ2
6 ζ4
6
ζ6
6
ζ8
6 ζ10
6
1 ζ3
6 ζ6
6
ζ9
6 ζ12
6
ζ15
6
1 ζ4
6 ζ8
6 ζ12
6
ζ16
6
ζ20
6
1 ζ5
6 ζ10
6
ζ15
6
ζ20
6
ζ25
6









(9.67)
F 6 =
1
√
6 ·










1
1
1
1
1
1
1
1−i·
√
3
2
−1−i·
√
3
2
−1 −1+i·
√
3
2
1+i·
√
3
2
1 −1−i·
√
3
2
−1+i·
√
3
2
1
−1−i·
√
3
2
−1+i·
√
3
2
1
−1
1
−1
1
−1
1 −1+i·
√
3
2
−1−i·
√
3
2
1
−1+i·
√
3
2
−1−i·
√
3
2
1
1+i·
√
3
2
−1+i·
√
3
2
−1 −1−i·
√
3
2
1−i·
√
3
2










.
(9.68)
We can use F 6 to map the dice from a pure state in a superposition of
maximal entropy
F 6|I⟩=
1
√
6 ·|I⟩+| 1
√
6 ·II⟩+ 1
√
6 ·|III⟩+ 1
√
6 ·|IV ⟩+ 1
√
6 ·|V ⟩+ 1
√
6 ·|V I⟩.

Periodicity
161
We can deﬁne a register of m quantum dices using the tensor product, and
map a pure sate |I⟩in a superposition by
F 6
m =
Om
F 6 = F 6 ⊗F 6 · · · ⊗F 6.
(9.69)
We can deﬁne quantum computation on any base B besides qubits, base
B = 2.
The qubit representation as the bit representation is the most
popular one.
9.7
The QFT Period Algorithm
QFT and Fm is used equivalently as Wm in the Deutsch Jozsa algorithm to
determine the properties of the function f(x). In Deutsch Jozsa algorithm
the function must be balanced or constant.
In the algorithm based on
QFT the function f(x) must be periodic. The determined property is the
period of the function f(x). We cannot use QFT to determine if a function
is periodic or not. The QFT algorithm is built on three serial steps. It
maps a state with zero von Neumann entropy to a superposition with the
maximal entropy, does the computation in this superposition and maps the
result into a state with low entropy. It should be noted that the entropy is
not zero and consequently the algorithm is probabilistic. The algorithms is
build on three principles of quantum computation that are related to the
Deutsch Jozsa algorithm,
• The function f(x) is represented by a quantum Boolean circuit.
• The properties of the function f(x) are determined using the superpo-
sition principle and QFT.
• The values of the function f(x) are determined by measuring a com-
pound system.
We represent the function f(x) by a quantum Boolean circuit represented
by a unitary operator Uf that acts on two registers of m qubits,
Uf · |x⟩|0⊗m⟩= |x⟩|f(x)⟩
after the application of Uf the two registers are entangled. In the ﬁrst step
of the algorithm we build a superposition of m qubits
Wm · |0⊗m⟩|0⊗m⟩=
1
√
2m
X
x∈Bm
|x⟩|0⊗m⟩.
In the second step we apply the Uf operator
Uf
 
1
√
2m
X
x∈Bm
|x⟩|0⊗m⟩
!
=

162
Principles of Quantum Artiﬁcial Intelligence
=
1
√
2m
X
x∈Bm
Uf · |x⟩|0⊗m⟩=
1
√
2m
X
x∈Bm
|x⟩|f(x)⟩.
In the third step we measure the second register of the compound system
[Shor (1994)], [Shor (1995)]. The state of the system is projected to the
subspace that corresponds to the observed state and the vector representing
the state is renormalized to the unit length. Because the function f(x) is
periodic, the new amplitude distribution is normalized and has the same
period as f(x). Before the measurement the amplitude distribution is at
a constant value
1
√
2m , it corresponding to the maximum entropy.
The
measured value γ corresponds to all k xi values for which the periodic
function is γ = f(xi). The function α(x) after the measurement is deﬁned
as
α(x) =
(
1
√
k
if
γ = f(x)
0
else
.
(9.70)
After the measurement the state is represented as
X
x∈Bm
α(x) · |x⟩|γ⟩.
In the fourth step we apply QFT that computes the discrete Fourier trans-
form. The discrete Fourier transform of α(x) is ω(x). Fm s a linear trans-
form talking the column vector α to a column vector ω
Fm ·
X
x∈Bm
α(x) · |x⟩|γ⟩=
X
x∈Bm
ω(x) · |x⟩|γ⟩.
In the ﬁfth step we measure the ﬁrst register. The measurement gives us a
value v that is close to a multiple value of
n
period. There are three possible
cases:
Period r happens to be power of 2,
the discrete Fourier transform
gives exact multiplies
v = t · n
r = t · 2m
r .
(9.71)
In this case we can estimate r by several experiments if necessary
v
2m = t
r
(9.72)
where the lowest term of
v
2m will yield a fraction t
r whose denominator is
the period r.

Periodicity
163
Period r is not power of 2,
the discrete Fourier transform gives ap-
proximate multiples
v ≈t · n
r = t · 2m
r .
(9.73)
In this case we can estimate r by continued ﬁnite fraction expansion of
v
2m
resulting in a unique fraction p
q with r ≈q. For unique fraction p
q of
v
2m
with q < M

v
2m −p
q
 <
1
M 2 .
(9.74)
The fraction can be obtained by the following algorithm:
a0 =
j v
2m
k
,
ϵ0 = v
2m −a0,
p0 = a0,
q0 = 1
a1 =
 1
ϵ0

,
ϵ1 = 1
ϵ0
−a0,
p1 = a1 · a0 + 1,
q1 = a1
ai =
 1
ϵi−1

,
ϵi =
1
ϵi−1
−ai,
pi = ai · pi−1 + pi−2,
qi = ai · qi−1 + qi−2.
We stop the algorithm with the output pi
qi with r ≈qi if
qi < M ≤qi+1.
f(x) is a periodic block function,
the measured value γ in a block
function corresponds to all k xi values for which the periodic function is
γ = f(xi). The amplitude function α(x) after the measurement has less or
equal number of zeros with
n −k ≤k.
The
1
√
k dominates the amplitude distribution.
After the DFT the DC
average of the amplitude dominates the distribution. The measured value
will be with high probability v = 1, we cannot estimate the period. It is
important to remember that the ﬁrst row of DFT matrix Fm is the DC
average of the amplitude of the input state. However during data analysis
using mathematical software care has to to be taken because many DFT
plot functions omit the DC component.

164
Principles of Quantum Artiﬁcial Intelligence
9.8
Factorization
The application of QFT gained popularity by Shor’s algorithm for factor-
ization of numbers in polynomial time [Shor (1994)], [Shor (1995)]. The
widely used RSA-public key cryptography scheme is secure. Its security
corresponds to the diﬃculty in factoring large numbers on conventional
computers. Shor’s algorithm indicated how to do it on a quantum com-
puter in polynomial time.
Number theory relates the period of a particular function to the factor
of an integer. Given an integer number M to be factored, a function f(x)M
is deﬁned
fM(x) = ax mod M,
(9.75)
a is a randomly chosen coprime to M, means the greatest common divisor
of a and M is 1. f(x)M is periodic. For a value a the period of a modulo
M is r.
ar = 1 mod M,
(9.76)
if r is an even number (r is dependent on a, if r is not even, chose diﬀerent
a), then
 a
r
2 2 = 1 mod M
(9.77)
 a
r
2 2 −1 = 0 mod M ⇒
 a
r
2 2 −12 = 0 mod M
 a
r
2 −1

·
 a
r
2 + 1

= 1 mod M.
(9.78)
The product
 a
r
2 −1

·
 a
r
2 + 1

is some integer multiple of M, because if
we divide it by M the reminder is zero. A common factor between them can
be eﬃciently determined by the greatest common divisor (gcd) Euclidean
algorithm.
gcd
  a
r
2 −1

, M

,
gcd
  a
r
2 + 1

, M

.
9.8.1
Example
In this example we will factor the number M = 15. We chose a = 13.
f15(x) = 13x mod 15.
We apply the Uf15 operator
Uf15
 
1
√
28
X
x∈B8
|x⟩|0⊗8⟩
!
= 1
16
X
x∈B8
|x⟩|f(x)⟩.

Periodicity
165
50
100
150
200
250
0.2
0.4
0.6
0.8
Fig. 9.3
The periodic signal f15(x) = 13x mod 15.
It represents the superposition
described by the amplitude of the second register.
5
10
15
20
0.2
0.4
0.6
0.8
Fig. 9.4
The periodic signal f15(x) = 13x mod 15 in a higher resolution.
We can
recognize a period of four.
In Figure 9.3 and 9.4 a superposition is indicated that is described by the
amplitude of the second register.
We measure the second register of the
compound system. In our experiment the function α(x) of the ﬁrst register
after the measurement is deﬁned as
α(x) =
(
1
√
64
if
0.25 = f(x)
0
else
.
(9.79)
We indicate the periodic block function α(x) of the ﬁrst register in Figure
9.5 and 9.6.
We apply QFT that computes the discrete Fourier transform

166
Principles of Quantum Artiﬁcial Intelligence
50
100
150
200
250
0.02
0.04
0.06
0.08
0.10
0.12
Fig. 9.5
The periodic block function α(x) of the ﬁrst register.
5
10
15
20
0.02
0.04
0.06
0.08
0.10
0.12
Fig. 9.6
The periodic block function α(x) of the ﬁrst register in a higher resolution.
We can recognize a period of four.
in the ﬁrst register (see Figure 9.7 and 9.8). The measurement gives us a
value v that is close to a multiple value of
n
period.
We measure the ﬁrst
register. The measurement gives us a value 64+1 that is close to a multiple
value of
256
period. In our experiment the period r happens to be power of 2.
The zero frequency term represents the DC average and appears at position
1 instead at the position 0, so v = 64. It follows for the period r
r = 256
v
= 256
64 .

Periodicity
167
50
100
150
200
250
0.1
0.2
0.3
0.4
0.5
Fig. 9.7
DFT transform of the ﬁrst register. It shows a strong peak at 1 and 64 + 1,
2 · 64 + 1 = 129 and 3 · 64 + 1 = 193.
0
10
20
30
40
50
60
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Fig. 9.8
DFT transform of the ﬁrst register, higher resolution. The zero frequency term
represents the DC average and appears at position 1 instead at the position 0.
A common factor between them can be eﬃciently determined by the great-
est common divisor (gcd) Euclidean algorithm,
gcd

13
4
2 −1

, 15

= 3,
gcd

13
4
2 + 1

, 15

= 5.
The factors of 15 are 3 and 5. The algorithm is probabilistic and it can
fail. Suppose the measurement gives us the value 1, in this case we have to
repeat the whole procedure.

168
Principles of Quantum Artiﬁcial Intelligence
9.9
Kitaev’s Phase Estimation Algorithm∗
Given a unitary
operator U on m qubits with an eigenvector |u⟩with
an unknown eigenvalue e2·π·i·θ we want to determine the phase θ [Kitaev
(1996)], [Kaye et al. (2007)]. For a complex number
z = x + i · y = |z| · ei·θ
θ is the phase. if we apply U to |u⟩we get
U · |u⟩= e2·π·i·θ · |u⟩
(9.80)
if we apply U to |u⟩w times we get
U w · |u⟩= U w−1 ·
 e2·π·i·θ · |u⟩

=
 e2·π·i·θw · |u⟩= e2·π·i·θ·w · |u⟩. (9.81)
However we will not gain any information, because |u⟩and e2·π·i·θ·w · |u⟩
are equivalent states, they represent the same state when a measurement
is preformed. Instead of the unitary operator U w we use the controlled U w
operator cU w. If the control qubit is set then U w is applied to the target
qubits, otherwise not. The operator cU w is unitary and deﬁnes an injective
mapping on two qubits that is reversible
cU w · |0⟩|u⟩= |0⟩|u⟩,
cU w · |1⟩|u⟩= |1⟩
 e2·π·i·θ·w · |u⟩

= e2·π·i·θ·w · |1⟩|u⟩.
So with w = 2j
cU 2j ·
|0⟩+ |1⟩
√
2

· |u⟩

=
 
|0⟩+ e2·π·i·θ··2j|1⟩
√
2
!
· |u⟩.
The QFT is represented as a tensor product of m single-qubit operations.
The inverse QFT can be factored into the tensor product of m single-qubit
operations,
|y⟩= IFm · |x⟩=
1
√n
X
y∈Bm
e2·π·i· y
n ·x · |y⟩=
=
1
√n ·
 |0⟩+ e2·π·i·0.x1 · |1⟩

⊗
 |0⟩+ e2·π·i·0.x2x1 · |1⟩

⊗· · · ⊗
(9.82)
⊗
 |0⟩+ e2·π·i·0.xm···x2x1 · |1⟩

.
If we set θ = 0.xm · · · x2x1 we can rewrite the equation as
|y⟩= IFm · |x⟩=
1
√n
X
y∈Bm
e2·π·i·y·θ · |y⟩=

Periodicity
169
=
1
√n ·

|0⟩+ e2·π·i·(θ·2m−1) · |1⟩

⊗

|0⟩+ e2·π·i·(θ·2m−2) · |1⟩

⊗· · · ⊗
(9.83)
⊗

|0⟩+ e2·π·i·(θ·20) · |1⟩

.
For m control qubits we deﬁne cj+1U 2j in the following way.
For j ∈
{0, 1, 2, · · · , m −1} the control qubit j + 1 of the m qubits is set then
cj+1U 2j is applied to the target |u⟩, otherwise not. The initial state of the
algorithm is
|0⊗m⟩|u⟩
with u being the eigenvector of U. In the ﬁrst step of the algorithm we
build a superposition of m control qubits
Wm · |0⊗m⟩|u⟩=
1
√
2m
X
x∈Bm
|x⟩|u⟩=
=
1
√n · (|0⟩+ |1⟩) ⊗(|0⟩+ |1⟩) ⊗· · · ⊗(|0⟩+ |1⟩) |u⟩.
(9.84)
In the second step we apply m cj+1U 2j operators to the target |u⟩
m−1
Y
j=0
cj+1U 2j ·
 
1
√
2m
X
x∈Bm
|x⟩|u⟩
!
=
= IFm · |x⟩|u⟩=
1
√n
X
y∈Bm
e2·π·i·y·θ · |y⟩|u⟩=
=
1
√n ·

|0⟩+ e2·π·i·(θ·2m−1) · |1⟩

⊗

|0⟩+ e2·π·i·(θ·2m−2) · |1⟩

⊗· · · (9.85)
⊗

|0⟩+ e2·π·i·(θ·20) · |1⟩

· |u⟩
In the third step we apply QFT to the m control qubtis
Fm ·

1
√n
X
y∈Bm
e2·π·i·y·θ · |y⟩

· |u⟩=
= Fm ·

1
√n
X
y∈Bm
e2·π·i· y
n ·x · |y⟩

· |u⟩= |x⟩|u⟩.
(9.86)
In the fourth step we measure the ﬁrst register composed of m control
qubtis and estimate θ
θ = 0.xm · · · x2x1 = x
n = x
2m .
(9.87)

170
Principles of Quantum Artiﬁcial Intelligence
9.9.1
Order ﬁnding
It seem that with Kitaev’s phase estimation algorithm we do not need to
apply continued ﬁnite fraction expansion. But actually this is not the case.
One big problem when using Kitaev’s phase estimation algorithm is the
determination of the eigenvector |u⟩of the unitary operator U. Usually
this is non trivial and the computational costs are expensive. But for order
ﬁnding (period estimation), there is an elegant way around the problem.
For the unitary operator
Ua · |x⟩= |x · a mod M⟩, x ≤M
(9.88)
a is a coprime of M. Because ar = 1 mod M is a rth root of unity, it
follows
U r
a · |x⟩= |x · a mod M⟩= |x⟩
that U r
a is a rth root of unity operation, and Ua has the following eigenvector
|ut⟩=
1
√r
r−1
X
x=0
e−2·π·i· t
r ·x · |ax mod M⟩
(9.89)
in this case we can determine the eigenvalue
Ua · |ut⟩= e2·π·i· t
r · |ut⟩.
(9.90)
With the Kitaev’s phase estimation algorithm we could determine
t
r and
r ≈q as before. Without knowing r in advance we cannot determine |ut⟩.
However we can use the following relation
|ax mod M⟩= 1 ⇔x = 0 mod r
it follows
1
√r
r−1
X
t=0
|ut⟩= |1⟩
(9.91)
with this relation we can use Kitaev’s phase estimation. In the ﬁrst step
of the algorithm we build a superposition of m control qubits and with
|u⟩= |1⟩
Wm · |0⊗m⟩|1⟩=
1
√
2m
X
x∈Bm
|x⟩|u⟩=
=
1
√n · (|0⟩+ |1⟩) ⊗(|0⟩+ |1⟩) ⊗· · · ⊗(|0⟩+ |1⟩) |1⟩.
In the second step we apply m cj+1U 2j operators to the target |1⟩= |u⟩.
After applying QFT we get actually a superposition
1
√r
r−1
X
t=0

t
r

|ut⟩
(9.92)
after measuring the ﬁrst register we can estimate t
r and by continued ﬁnite
fraction expansion r ≈q.

Periodicity
171
9.10
Unitary Transforms
Other unitary transforms beside DFT can be applied in quantum computa-
tion. Examples of such transforms are the Discrete Cosine Transform DCT
and the Haar-Wavelet-Transform (HWT).
DCT
corresponds to the sum of cosine functions oscillating at diﬀerent
frequencies with no imaginary number representation.
HWT
The Haar transform analyzes the signal at diﬀerent frequencies
with diﬀerent resolutions. The Haar-Wavelet-Transform (HWT) is repre-
sented by the unitary matrix (HW). For n = 2m HWm performs a Quan-
tum Haar-Wavelet-Transform on a state |x⟩of m qubits in a n-dimensional
Hilbert space Hn.
HW1 = W1.
(9.93)
For three qubits HWT is,
HW3 =
1
√
8 ·













1
1
1
1
1
1
1
1
1
1
1
1 −1 −1
−1
−1
√
2
√
2 −
√
2 −
√
2
0
0
0
0
0
0
0
0
√
2
√
2 −
√
2 −
√
2
2 −2
0
0
0
0
0
0
0
0
2
−2
0
0
0
0
0
0
0
0
2 −2
0
0
0
0
0
0
0
0
2
−2













.
(9.94)
Interestingly The introduced unitary matrix W maps a pure state |000⟩in
a superposition,
HW3 · |000⟩=
1
2 ·
√
2 · |000⟩+
1
2 ·
√
2 · |001⟩+ 1
√
2 · |011⟩
(9.95)
where the distribution of the amplitudes does not corresponds to the max-
imal entropy.

This page intentionally left blank
This page intentionally left blank

Chapter 10
Search
10.1
Search and Quantum Oracle
For a function f(x)
fξ(x) =
 1
if
x = ξ
0
else
(10.1)
we want to ﬁnd x for which f(x) = 1, x = ξ.
The task is equivalent
to a decision problem with a binary answer 1 = yes and 0 = no and
the instance x.
We can describe by the function f(x) NP −complete
problems. A problem is NP −complete, if it is in NP and every other
problem in NP can be reduced to it. For NP a deterministic algorithm
veriﬁes if an instance (ticket) leads to a “yes” answer in polynomial time.
The veriﬁcation of an instance (ticket) is in polynomial time P and can be
represented by a uniformly polynomial circuit, a circuit with a polynomial
number of gates. If f(x) is NP −complete a quantum circuit Uf with a
polynomial number of quantum gates can verify for a given instance xticket
if f(xticket) = 1, xticket = ξ or f(xticket) = 0. The search for ξ is based on
the three principles of quantum computation:
• The function f(x) is represented by a quantum Boolean circuit Uf.
• The properties of the function f(x) are determined using the superpo-
sition principle and a some kind of unitary transform.
• The values of the function f(x) are encoded by (−1)f(x), the sign of
the amplitude.
Often f(x) is represented by an abstract black box function and not by a
quantum circuit. In this case Uf is a quantum oracle. The Uf quantum
oracle or quantum circuit is a unitary operator that acts on the m+1 qubits
173

174
Principles of Quantum Artiﬁcial Intelligence
with x ∈Bm and y ∈B1
Uf · |x⟩|y⟩= |x⟩|f(x) ⊕y⟩.
The function f(x), the solution is encoded by (−1)f(x), the sign of the
amplitude. In correspondence with the Deutsch Jozsa algorithm we build
a superposition of m + 1 qubits
Wm+1 · |0⊗m⟩|1⟩= Wm · |0⊗m⟩⊗W1 · |1⟩=
1
√
2m
X
x∈Bm
|x⟩⊗
|0⟩−|1⟩
√
2

and we apply the Uf, operator,
Uf · Wm+1 · |0⊗n⟩|1⟩=
=
1
√
2m+1 ·
X
x∈Bm
Uf · |x⟩|0⟩−
1
√
2m+1 ·
X
x∈Bm
Uf · |x⟩|1⟩
=
1
√
2m+1 ·
X
x∈Bm
|x⟩|f(x) ⊕0⟩−
1
√
2m+1 ·
X
x∈Bm
|x⟩|f(x) ⊕1⟩
=
1
√
2m+1 ·
 X
x∈Bm
|x⟩|f(x) ⊕0⟩−
X
x∈Bm
|x⟩|f(x) ⊕1⟩
!
.
There are four possible cases:
Uf · |x⟩|0⟩= |x⟩|f(x) ⊕0⟩= |x⟩|0⟩,
Uf · |x⟩|1⟩= |x⟩|f(x) ⊕1⟩= |x⟩|1⟩,
Uf · |ξ⟩|0⟩= |ξ⟩|f(ξ) ⊕0⟩= |ξ⟩|1⟩,
Uf · |ξ⟩|1⟩= |ξ⟩|f(ξ) ⊕1⟩= |ξ⟩|0⟩.
It follows that
=
1
√
2m+1 ·

X
x̸=ξ
|x⟩|0⟩+ |ξ⟩|1⟩−
X
x̸=ξ
|x⟩|1⟩−|ξ⟩|0⟩


=
1
√
2m+1 ·

X
x̸=ξ
|x⟩(|0⟩−|1⟩) + |ξ⟩(|1⟩−|0⟩)


=
1
√n
X
x∈Bm
(−1)f(x) · |x⟩⊗
|0⟩−|1⟩
√
2

.
(10.2)

Search
175
The value of the function f(x) is encoded by (−1)f(x), in most cases again
the auxiliary bit is ignored,
1
√
2m
X
x∈Bm
(−1)f(x) · |x⟩=
1
√n
X
x∈Bm
(−1)f(x) · |x⟩.
(10.3)
For m = 1 we could use the Deutsch algorithm to decide if ξ exist, for
m = 2 there exist the unitary operator described by the unitary matrix
S = 1
2 ·




−1
1
1
1
1 −1
1
1
1
1 −1
1
1
1
1 −1




(10.4)
with I = S · S∗. We can determine x = ξ by a simple approach. We apply
the Uf operator and then the operator S,
(S ⊗I1) · Uf · W2+1 · |00⟩|1⟩= S ·
 
1
√
4
X
x∈B2
(−1)f(x) · |x⟩
!
⊗
|0⟩−|1⟩
√
2

(10.5)
(S ⊗I1) · Uf · W2+1 · |00⟩|1⟩= |ξ⟩⊗
|0⟩−|1⟩
√
2

(10.6)
and measure the ﬁrst register and obtain |ξ⟩with probability 1. For example
if ξ = |10⟩= 3,
1
2 ·




−1
1
1
1
1 −1
1
1
1
1 −1
1
1
1
1 −1



· 1
2 ·




1
1
−1
1



=




0
0
1
0



.
(10.7)
Such a “spy” matrix S exist only for m = 2.
Is it possible to get an
exponential speed up for m ≫2? To determine ξ for we would require only
O(m) = O(log 2m) = O(log n) steps. This na¨ıve hope can not be fulﬁlled.
10.2
Lower Bound Ω(√n) for Uf-based Search∗
If the operator Uf is represented by O(m) gates, then we can get a speed
up of maximum O(√n) steps, in fact Ω(√n) is the lower bound. The proof
for lower bound Ω(√n) for Uf-based search is based on a sequence in a
Hilbert space and the Euclidean norm properties of Hilbert space as well as
the deﬁnition of the probabilities as normed squared amplitudes [Bennett

176
Principles of Quantum Artiﬁcial Intelligence
et al. (1997)], [Boyer et al. (1998)]. Suppose we start with a state |x⟩of m
qubits in a n-dimensional Hilbert space Hn
|x⟩= α1 · |x1⟩+ α2 · |x2⟩+ · · · + αn · |xn⟩
and we are searching for one solution represented by the pure state |ξ⟩
that corresponds to some basis of |x⟩. We apply Uf which performs a
phase shift to the amplitude of the corresponding basis, if |xi⟩= |ξ⟩then
−αi·|xi⟩. The Von Neumann Entropy is not changed by this operation. We
will apply t times Uf and some arbitrary unitary operation At that would
reduce the Von Neumann Entropy. The reduction is realized by changing
the amplitude that indicates the solution together with the amplitudes of
non solutions,
|xξ
t⟩= At · Uf · At−1 · Uf · · · · · A1 · Uf · |x⟩.
(10.8)
We will apply t times some arbitrary unitary operation At without indicat-
ing the solution by phase shift to the amplitude by Uf,
|xt⟩= At · At−1 · · · · · A1 · |x⟩.
(10.9)
We deﬁne a sequence at as the deviation after t steps between informed
and uninformed evaluation of unitary operators on the a state |x⟩
at = ∥|xξ
t⟩−|xt⟩∥2
(10.10)
with
||xt⟩∥2 = ⟨xt|xt⟩.
The norm of diﬀerence between informed and uninformed evaluation is
taken to the power of two, because the probabilities correspond to the
normed amplitudes power two.
10.2.1
Lower bound of at
We suppose that an observation yields to the solution of the search with
probability al least 0.5.
∥⟨ξ|xξ
t⟩∥2 ≥1
2.
(10.11)
We can rewrite at as
at = ∥|xξ
t⟩−|ξ⟩+ |ξ⟩−|xt⟩∥2 = ∥

|xξ
t⟩−|ξ⟩

+ (|ξ⟩−|xt⟩) ∥2.
We deﬁne two sequences, the deviation after t steps between informed eval-
uation of unitary operators and the solution |ξ⟩with
a+
t = ∥|xξ
t⟩−|ξ⟩∥2

Search
177
and the deviation after t steps between uninformed evaluation of unitary
operators and the solution |ξ⟩with
a−
t = ∥|ξ⟩−|xt⟩∥2
Because of the inequality
∥α + β∥2 ≥∥α∥2 −2 · ∥α∥· ∥β∥+ ∥β∥2
it follows that
at = ∥

|xξ
t⟩−|ξ⟩

+ (|ξ⟩−|xt⟩) ∥2 ≥a+
t −2 ·
q
a+
t ·
q
a−
t + a−
t . (10.12)
Using the two sequence we indicate a lower bound for at and t ≫1
at ≥a+
t −2 ·
q
a+
t ·
q
a−
t + a−
t =
q
a+
t −
q
a−
t
2
≥c.
(10.13)
With a+
t we indicate that in Hilbert space Hn a lower bound c > 0 exists
a+
t = ⟨xξ
t|xξ
t⟩−2 · ⟨ξ|xξ
t ⟩+ ⟨ξ|ξ⟩= 1 −2 · ⟨ξ|xξ
t ⟩+ 1.
(10.14)
The value of
2 · ⟨ξ|xξ
t⟩
can be estimated. In the ﬁrst step we notice that we can replace the state
⟨ξ| by an equivalent state
|ξ⟩= ei·θ · |ξ⟩.
Because of the Equation 10.11 for t ≫1
2 · ⟨ξ|xξ
t⟩= 2 · |⟨ξ|xξ
t ⟩| ≥
2
√
2 =
√
2
(10.15)
and
a+
t ≤2 −
√
2.
(10.16)
For
a−
t = ⟨xt|xt⟩−2 · ⟨ξ|xt⟩+ ⟨ξ|ξ⟩= 1 −2 · ⟨ξ|xξ
t⟩+ 1
(10.17)
with
2 · ⟨ξ|xt⟩= 2 · |⟨ξ|xt⟩| ≤
2
√n
(10.18)
it follows that
2 −
2
√n ≤a−
t ≤2
(10.19)
putting at+ and a−
t together we get
at ≥
q
a+
t −
q
a−
t
2
≥
q
2 −
√
2 −
√
2
2
= c = 0.421002.
(10.20)

178
Principles of Quantum Artiﬁcial Intelligence
10.2.2
Upper bound of at
The deviation cannot grow faster as O(t2), this can be proven by induction.
We prove that 4 · t2
n ≥at. For t = 0
|xξ
0⟩= |x⟩
because no unitary operators were applied. The induction step is if 4 · t2
n ≥
at, then 4 · (t+1)2
n
≥at+1 as well,
at+1 = ∥At+1 · Uf · |xξ
t⟩−At+1 · |xt⟩∥2
at+1 = ∥At+1 ·

Uf · |xξ
t⟩−|xt⟩

∥2 ≤∥|At+1∥2 · ∥Uf · |xξ
t⟩−|xt⟩∥2.
The norm for square matrices A in Hilbert space Hn is the Frobenius norm
∥A∥F =


n
X
i=1,j=1
|aij


1
2
=
p
tr(A · A∗)
(10.21)
∥A∥2 is also called the spectral norm. For unitary matrices A
∥A∥F = ∥A∥2 = 1.
(10.22)
Because the matrix At+1 is unitary it follows that
at+1 = ∥|At+1 ·

Uf · |xξ
t⟩−|xt⟩

∥2 = ∥Uf · |xξ
t⟩−|xt⟩∥2
at+1 = ∥|Uf ·

|xξ
t⟩−|xt⟩

+ (Uf −Im) · |xt⟩∥2.
Note that if a solution exist the amplitude of the corresponding basis of
|xt⟩that is equal to |ξ⟩is ⟨ξ|xt⟩, then
(Uf −Im) · |xt⟩= Uf · |xt⟩−|xt⟩= −2 · ⟨ξ|xt⟩· |ξ⟩
and
−2 · ∥⟨ξ|xt⟩· |ξ⟩∥= −2 · |⟨ξ|xt⟩|
also
∥Uf ·

|xξ
t⟩−|xt⟩

∥2 = ∥|xξ
t⟩−|xt⟩∥2
because
∥α + β∥2 ≤∥α∥2 + 2 · ∥α∥· ∥β∥+ ∥β∥2

Search
179
it follows
at+1 = ∥Uf ·

|xξ
t⟩−|xt⟩

−2 · ⟨ξ|xt⟩· |ξ⟩∥2
at+1 ≤∥|xξ
t⟩−|xt⟩∥2 + 4 · ∥|xξ
t⟩−|xt⟩t∥· |⟨ξ|xt⟩| + 4 · |⟨ξ|xt⟩|2
at+1 ≤at + 4 · √at · |⟨ξ|xt⟩| + 4 · |⟨ξ|xt⟩|2
(10.23)
with
⟨ξ|xt⟩= |⟨ξ|xt⟩| ≤
1
√n
(10.24)
it follows that
at+1 ≤4 · t2
n + 4 ·
r
4 · t2
n ·
1
√n + 4
n = 4 · t2
n + 4 ·
r
4 · t2
n2 + 4
n
(10.25)
at+1 ≤4 · t2
n + 8 · t
n + 4
n = 4 · (t + 1)2
n
(10.26)
and the induction step t to t + 1 is concluded.
10.2.3
Ω(√n)
With the lower and upper bound of at we can estimate the lower bound of
t. at grows quadratically,
4 · t2
n ≥at ≥c = 0.421002
it follows that
t ≥
r
n · c
4 = √n · 0.324423.
(10.27)
The speed up of O(√n) compared to O(n) is due to the deﬁnition of the
probabilities as normed squared amplitudes.
If the operator Uf is rep-
resented by O(m) gates, then we can get a speed up of maximum O(√n)
steps, in fact Ω(√n) is the lower bound [Bennett et al. (1997)], [Boyer et al.
(1998)], [Zalka (1999)]. This is the case when f(x) is a NP −complete
problem. It follows that for Uf-based search using a quantum computer
NP −complete problems remain NP −complete.
Despite the fact the
saving of O(2
m
2 ) compared to O(2m) is huge, 2m ̸= O(2
m
2 ) means
Θ(2
m
2 ) ̸= Θ(2m).
(10.28)
For 2m ̸= O(2
m
2 ) we assume there exist a constant c, that for certain value
m0 > 0
0 ≤2m ≤c · 2
m
2
∀m ≥m0.
However such a constant does not exist because from
2m = 2
m
2 · 2
m
2 ≤c · 2
m
2
follows the simple contradiction
2
m
2 ≤c.

180
Principles of Quantum Artiﬁcial Intelligence
10.3
Grover’s Ampliﬁcation
If the operator Uf is represented by O(m) gates, then Grover’s ampliﬁcation
algorithm implements exhaustive search in O(√n) steps in n-dimensional
Hilbert space Hn [Grover (1996)], [Grover (1997)], [Grover (1998a)], [Grover
(1998b)], [Grover (1996)], [Grover (1996)]. It is as good as any possible
quantum algorithm for exhaustive search due to the lower bound Ω(√n)
[Aharonov (1999)]. The algorithm is based on the Householder reﬂection
of state |x⟩of m qubits with n = 2m [Zalka (1999)].
10.3.1
Householder reﬂection
Beside rotation and permutation the Householder reﬂection is an important
class of unitary transformations (some times also called the elementary
reﬂector). The Householder reﬂection reﬂects one vector |x⟩∈Hn to its
negative and leaves invariant the orthogonal complement of this vectors. It
is described by the Householder matrix Qx With ∥|x⟩∥= 1 representing m
qubits with n = 2m
Qx = Im −2 · |x⟩⟨x|
(10.29)
Qx is unitary,
Qx · Q∗
x = (Im −2 · |x⟩⟨x|) · (Im −2 · |x⟩⟨x|)∗
Qx · Q∗
x = Im −2 · |x⟩⟨x| −2 · |x⟩⟨x| + 4 · |x⟩⟨x| · |x⟩⟨x|
Qx · Q∗
x = Im −4 · |x⟩⟨x| + 4 · ⟨x|x⟩· |x⟩⟨x| = Im
⟨x|x⟩= 1 because ∥|x⟩∥= 1. With
P = |x⟩⟨x|.
For example with
|x⟩= cos(α) · |x1⟩+ sin(α) · |x2⟩
we get
Qx =
 1 −2 · cos2 ·α
−2 · cos α · sin α
−2 · cos α · sin α
1 −2 · sin2 α

=
−cos 2 · α −sin 2 · α
−sin 2 · α cos 2 · α

.
(10.30)
It becomes clear with
Qx · |0⟩=
−cos 2 · α
−sin 2 · α


Search
181
and
Qx · |1⟩=
 −sin 2 · α
cos 2 · α

that Qx is not a rotation by the angle 2 · α
R2α =
 cos 2 · α −sin 2 · α
sin 2 · α cos 2 · α

.
It is a one dimensional reﬂection
S =
 −1 0
0 1

and a rotation by R2α
Qx = R2α · S.
10.3.2
Householder reﬂection and the mean value
Suppose Pm is generated by the normalized vector |x⟩indicating the direc-
tion of the bisecting line,
|x⟩=
1
√n · |x1⟩+
1
√n · |x2⟩+ · · · +
1
√n · |xn⟩=




1
√n
...
1
√n




(10.31)
or in qubit notation (binary)
|x⟩=
1
√
2m ·
X
y∈Bm
|y⟩
(10.32)
then the projection matrix Pm is
Pm = |x⟩⟨x| =





1
n
1
n · · ·
1
n
1
n
1
n · · ·
1
n
... ... ... ...
1
n
1
n · · ·
1
n





(10.33)
it computes for each dimension described by the ﬁxed basis he mean value
of all dimensions. 





Pn
i=1 xi
n
Pn
i=1 xi
n...
Pn
i=1 xi
n






=





1
n
1
n · · ·
1
n
1
n
1
n · · ·
1
n
... ... ... ...
1
n
1
n · · ·
1
n




·





x1
x2
...
xn





(10.34)

182
Principles of Quantum Artiﬁcial Intelligence
we get
Qx = Im −2 · Pm.
(10.35)
For each dimension the Householder reﬂection in the direction of the bi-
secting line computes the following mapping,
xi = xi −2 ·
Pn
i=1 xi
n
.
(10.36)
10.3.3
Ampliﬁcation
Grover’s ampliﬁcation is based on −Qx. It is a unitary operator with
Gm := −Qx = −Im + 2 · Pm = 2 · Pm −Im
(10.37)
the mapping is deﬁned as,
xi = 2 ·
Pn
i=1 xi
n
−xi.
(10.38)
Suppose only one amplitude of xj is negative and the other one are positive.
Then the corresponding amplitude grows with
xj = 2 ·
Pn
i=1 xj
n
+ xi
(10.39)
the other xi with i ̸= j diminish. With j = 2 we get






2 ·
Pn
i=1 xi
n
−x1
2 ·
Pn
i=1 xi
n
+ x2
...
2 ·
Pn
i=1 xi
n
−xn






=





2
n −1
2
n
· · ·
2
n
2
n
2
n −1 · · ·
2
n
...
...
...
...
2
n
2
n
· · ·
2
n −1




·





x1
−x2
...
xn




.
(10.40)
Amplitude ampliﬁcation is based on Gm and Uf. For a register of m qubits
in a Hilbert space Hn we apply the Uf operator and then the operator Gm,
(Gm ⊗I1) · Uf · Wm+1 · |0⊕m⟩|1⟩=
= Gm ·
 
1
√n
X
x∈Bm
(−1)f(x) · |x⟩
!
⊗
|0⟩−|1⟩
√
2

(10.41)
with Gm
Gm =





2
n −1
2
n
· · ·
2
n
2
n
2
n −1 · · ·
2
n
...
...
...
...
2
n
2
n
· · ·
2
n −1





(10.42)

Search
183
note that for m = 2, G2 is equal to the “spy” matrix S
G2 = S = 1
2 ·




−1
1
1
1
1 −1
1
1
1
1 −1
1
1
1
1 −1




(10.43)
the resulting state is |τ⟩
(Gm ⊗I1) · Uf · Wm+1 · |0⊕m⟩|1⟩= |τ⟩⊗
|0⟩−|1⟩
√
2

.
(10.44)
The state |τ⟩has an amplitude distribution with a lower entropy as the state
with the maximum entropy log n represented by the maximal superposition
|y⟩=
1
√n
X
x∈Bm
(−1)f(x) · |x⟩.
The new amplitude distribution is computed by
Gm · |y⟩= (2 · Pm −Im) · |y⟩.
First two times the average amplitude is computed. The amplitude values
for non solution are
1
√n and for one marked solution −1
√n, it follows
A = 2 · Pm · |y⟩= 2
n ·

n ·
1
√n −
1
√n −
1
√n

= 2
n ·

(n −1) ·
1
√n −
1
√n

(10.45)
A = 2 · Pm · |y⟩=
2
√n ·

1 −2
n

= 2 · n −4
n
3
2
.
(10.46)
The amplitude of the state |τ⟩indicating the solution in the dimension i is
τi = A +
1
√n = 3 · n −4
n
3
2
=
3
√n −
4
n · √n
(10.47)
and the non solution in the dimension j with j ̸= i
τj = A −
1
√n = n −4
n
3
2
=
1
√n −
4
n · √n.
(10.48)
For n = 4 τi = 1 and τj = 0, for n = 28 τi = 0.186523 and τj = 0.0615234.
The probability of measuring the solution depending on the size n is
∥|τi⟩∥2 =

3
√n −
4
n · √n

2
(10.49)
and non solution
∥|τj⟩∥2 =

1
√n −
4
n · √n

2
.
(10.50)
In the Figure 10.1 we indicated the probability of measuring the solution
and non solution for 4 ≤n ≤16 and in the Figure 10.2 for 28 ≤n ≤216.
For more than 4 qubits the probability of measuring a solution will be below
0.5. The probability can be increased by the iterative ampliﬁcation.

184
Principles of Quantum Artiﬁcial Intelligence
6
8
10
12
14
16
0.2
0.4
0.6
0.8
1.0
Fig. 10.1
The probability of measuring the solution and non solution for 4 ≤n ≤16.
The x-axis indicates n and the y-axis the probability. Top curve is the probability of the
solution, down curve the probability of the non-solution.
10000
20000
30000
40000
50000
60000
0.0002
0.0004
0.0006
0.0008
0.0010
Fig. 10.2
The probability of measuring the solution and non solution for 28 ≤n ≤216.
The x-axis indicates n and the y-axis the probability. Top curve is the probability of the
solution, down curve the probability of the non-solution.
10.3.4
Iterative ampliﬁcation
With the deﬁnition
Γm := (Gm ⊗I1) · Uf
(10.51)
the resulting state is |τ⟩. We describe several ampliﬁcations by
 r
Y
t=1
Γm
!
· Wm+1 · |0⊕m⟩|1⟩= Γm · Γm · · · · · Γm · Wm+1 · |0⊕m⟩|1⟩=

Search
185
= |τ⟩⊗
|0⟩−|1⟩
√
2

.
How many iterations do we need to preform, or what is the suitable value
for r? The probability of seeing one solution should be as close as possible
to 1 and r should be as small as possible. Suppose that the function f(x)
has k solutions with n ≫k ≥1,
fξ(x) =
 1
if
x = ξi i ∈{1, 2, · · · , k}
0
else
(10.52)
we want to ﬁnd x for which f(x) = 1. All the preceding steps can be easily
extended to this more general case. The amplitude for the solution at the
iteration t will be indicated by αt non-solution by βt. For t = 0 before the
ﬁrst iteration of Γm
α0 =
1
√n
(10.53)
β0 =
1
√n.
(10.54)
In the ﬁrst iteration Γm two times the average amplitude is computed, the
amplitude values for non-solution are
1
√n and for one marked solution −1
√n,
it follows
A = 2
n ·

n ·
1
√n −k
√n −k
√n

= 2
n ·

(n −k) ·
1
√n −k
√n

(10.55)
A = 2
n · ((n −k) · β0 −k · α0) .
(10.56)
The amplitude of the solution is
α1 = A +
1
√n = A + α0 = 2
n · ((n −k) · β0 −k · α0) + α0,
(10.57)
α1 = 1
n · (α0 · (n −2 · k) + β0 · (2 · n −2 · k),
α1 = α0 · (1 −2 · k
n ) + β0 · (2 −2 · k
n ),
(10.58)
and the non-solution
β1 = A −
1
√n = A −β0 = 2
n · ((n −k) · β0 −k · α0) −β0,
(10.59)
β1 = −α0 · 2 · k
n
+ β0 · (1 −2 · k
n ).
(10.60)

186
Principles of Quantum Artiﬁcial Intelligence
We can describe the evolution of the amplitudes in time t by two coupled
recurrence equations. They represent a discrete dynamical system of two
diﬀerence equations
αt+1 = αt ·

1 −2 · k
n

+ βt ·

2 −2 · k
n

(10.61)
βt+1 = −αt · 2 · k
n
+ βt ·

1 −2 · k
n

.
(10.62)
We can indicate the states of a system in three dimensional phase space
of αt, βt and t with the boundary condition of α0 = β0 =
1
√n (see Figure
10.3). It represents a periodic orbit in time. The projected orbit in the two
dimensional subspace αt, βt represents an ellipse (see Figure 10.4). The
ellipse is determined by the values of n and k (see Figure 10.5). The pro-
!1.0
!0.5
0.0
0.5
1.0
!0.05
0.00
0.05
0
50
100
150
200
Fig. 10.3
Phase space ofαt, βt and t with the boundary condition of α0 = β0 =
1
√n.
The values are n = 256, k = 1 and 1 ≤t ≤200. The x-axis indicates αt, the y-axis βt
and the z-axis t.
jected orbit in two dimensional subspace of amplitude and time t represents
two sine waves, periodic functions described by αt and βt (see Figure 10.6).
The period is determined by the value of k and n. With k = 1 the peak
amplitude is one (see Figure 10.7 and Figure 10.5). Such a linear and peri-
odic system is usually described by sine and cosine equations. It should be
noted that a cosine wave is a sine wave because of the phase-shift relation
cos(θ) = sin

θ + π
2

.

Search
187
The ellipse in a Cartesian system is represented by the equation
!1.0
!0.5
0.5
1.0
!0.06
!0.04
!0.02
0.02
0.04
0.06
Fig. 10.4
The projected orbit in the two dimensional subspace αt, βt with the boundary
condition of α0 = β0 =
1
√n . The values are n = 256, k = 1 and 1 ≤t ≤200. The x-axis
indicates αt and the y-axis βt.
!1.0
!0.5
0.5
1.0
!0.06
!0.04
!0.02
0.02
0.04
0.06
Fig. 10.5
The projected orbit in the two dimensional subspace αt, βt with the boundary
condition of α0 = β0 =
1
√n. The x-axis indicates αt, the y-axis βt. The outer ellipse
has the values n = 256, k = 1 as before, the two ellipses with diminishing x-axis radius
corresponds to increased k values k = 4 and k = 16. For all three ellipses with n = 256,
200 iterations were done. In the fourth ellipse k is one and n = 65536 = 216, y-axis
radius diminish. 1000 iterations were done.
x2
a2 + y2
b2 = 1.
(10.63)

188
Principles of Quantum Artiﬁcial Intelligence
50
100
150
200
!1.0
!0.5
0.5
1.0
Fig. 10.6
The projected orbit in the two dimensional subspace of amplitude and time t
represents a periodic function described by αt (the dotted curve) and βt (the continuous
curve). The x-axis indicates t and the y-axis the amplitude. The values are n = 256,
k = 1 and 1 ≤t ≤200.
200
400
600
800
1000
!1.0
!0.5
0.5
1.0
Fig. 10.7
The projected orbit in the two dimensional subspace of amplitude and time t.
The x-axis indicates t and the y-axis the amplitude. Four periodic functions represented
by αt with diﬀerent periods are shown. The period is determined by the value of k and
n. With k = 1 the peak amplitude is one. The greatest period corresponds to the values
k = 1 and n = 65536. For n = 256 the period diminishes, for k = 4 and k = 16 the peak
amplitude diminishes below one and the period diminishes even more. 1000 iterations
were done.
Because αt and βt are real, following equation representing an ellipse is
given by
k · α2
t + (n −k) · β2
t = 1.
(10.64)

Search
189
Using the the Pythagorean identity
sin2 θ + cos2 θ = 1
we can rewrite the equation representing the ellipse as
k ·
 r
1
k · sin θt
!2
+ (n −k) ·
 r
1
n −k · cos θt
!2
= 1
(10.65)
it follows that
αt =
r
1
k · sin θt
and
βt =
r
1
n −k · cos θt
(10.66)
and we can rewrite the the two coupled recurrence equations as
r
1
k · sin θt+1 =
r
1
k · sin θt ·

1 −2 · k
n

+
r
1
n −k · cos θt ·

2 −2 · k
n

r
1
n −k · cos θt+1 = −
r
1
k · sin θt · 2 · k
n
+
r
1
n −k · cos θt ·

1 −2 · k
n

simpliﬁed as
sin θt+1 = sin θt ·

1 −2 · k
n

+ cos θt · 2 ·
√
k ·
√
n −k
n
(10.67)
cos θt+1 = −sin θt · 2 ·
√
k ·
√
n −k
n
+ cos θt ·

1 −2 · k
n

.
(10.68)
Trigonometric simpliﬁcation∗
Because
−1 ≤

1 −2 · k
n

≤1
we can represent it as
cos ω = 1 −2 · k
n .
(10.69)
Because of the Pythagorean identity

1 −2 · k
n
2
+
 
2 ·
√
k ·
√
n −k
n
!2
= 1
it follows that
sin ω = 2 ·
√
k ·
√
n −k
n
(10.70)

190
Principles of Quantum Artiﬁcial Intelligence
and we can rewrite again the the two coupled recurrence equations as
sin θt+1 = sin θt · cos ω + cos θt · sin ω
(10.71)
cos θt+1 = −sin θt · sin ω + cos θt · cos ω
(10.72)
Because of the trigonometric identities, addition and subtraction theorem,
the two coupled recurrence equations with the boundary condition θ0 are
sin θt+1 = sin(θt + ω) = sin(θ0 + t · ω + ω)
(10.73)
cos θt+1 = cos(θt + ω) = cos(θ0 + t · ω + ω)
(10.74)
and we can rewrite the recurrence equations into two simple equations
αt =
1
√
k
· sin(θ0 + t · ω)
(10.75)
βt =
1
√
n −k · cos(θ0 + t · ω).
(10.76)
With the boundary condition of α0 = β0 =
1
√n and α0 =
q
1
k · sin θ0,
β0 =
1
√n−k · cos θ0 it follows that
sin2 θ0 = k
n
(10.77)
and
cos2 θ0 = n −k
n
= 1 −k
n
(10.78)
and because of the double angle formula
cos(2 · x) = 2 · cos2 x −1 = 1 −2 · sin2 x
and
sin(2 · x) = 2 · sin x · cos x
it follows that
cos ω = 1 −2 · k
n
= 1 −2 · sin2 θ0 = cos(2 · θ0).
(10.79)
sin ω = 2 ·
√
k ·
√
n −k
n
= 2 sin θ0 · cos0 θ0 = sin(2 · θ0).
(10.80)
Using the trigonometric identities
αt =
1
√
k
· (sin θ0 · cos(t · ω) + cos θ0 · sin(t · ω))
(10.81)

Search
191
βt =
1
√
n −k · (−sin θ0 · sin(t · ω) + cos θ0 · cos(t · ω))
(10.82)
the solution for the two diﬀerence equations representing the discrete dy-
namical with the boundary condition α0 = β0 =
1
√n is represented by the
two equations
αt =
1
√
k
· sin(θ0 + t · 2 · θ0) =
1
√
k
· sin(θ0 · (2 · t + 1))
(10.83)
βt =
1
√
n −k · cos(θ0 + t · 2 · θ0) =
1
√
n −k · cos(θ0 · (2 · t + 1))
(10.84)
with
θ0 = sin−1
 r
k
n
!
.
(10.85)
10.3.5
Number of iterations
The probability of seeing one solution should be as close as possible to 1
and the number of iterations r should be as small as possible. Because
there are k solution, the probability of measuring a state that represents a
solution is
k · α2
t = sin2(θ0 · (2 · t + 1)) = 1
(10.86)
θ0 · (2 · t + 1) = π
2
(10.87)
after t∗iterations the probability of measuring a solution is nearly one
t∗:= t =
π
4 · θ0
−1
2 = π
4 ·
rn
k −1
2 = π
4 ·
r
2m
k −1
2
(10.88)
t∗= 0.785398 ·
r
2m
k −0.5.
(10.89)
For k = 1 and more than two qubits (m > 2, n = 2m) the corresponding
value is above the possible lower bound for Uf based search
t∗= √n · 0.785398 −0.5 > √n · 0.324423.
(10.90)
The number of iterations r
 r
Y
t=1
Γm
!
· Wm+1 · |0⊕m⟩|1⟩

192
Principles of Quantum Artiﬁcial Intelligence
is the largest integer not greater than t∗,
r = ⌊t∗⌋=
$
π
4 ·
r
2m
k −1
2
%
.
(10.91)
The value of r depends on the relation of n versus k. For n = 4 and k = 1
we need only one rotation, we need as well only one rotation for
n
4 = k
to ﬁnd one of the k solutions. For 16 qubits and one solution, k = 1,
n = 65536 = 216 , t∗= 200.562. In this case we need two hundred rotations.
The probability of measuring a state that represents a solution is nearly one.
It is possible to adapt the iterations in such a way that the probability of
ﬁnding a solution is exactly one. One changes θ0 of the diﬀerence equations
either in the last step or continuously so that the r = t∗= ⌊t∗⌋as proposed
by Brassard [Brassard et al. (2000)], [Brassard et al. (1998)]. The resulting
speed up remains quadratic. The iterative ampliﬁcation algorithm requires
the value of k in order to determine the number of iterations.
We can
determine the value of k by the quantum counting algorithm.
10.3.6
Quantum counting
Quantum counting algorithm is based on the QFT period algorithm to esti-
mate the period of the sin wave period represented by the of the amplitude
αt or βt [Brassard et al. (1998)]. [Brassard et al. (2000)] (see Figure 10.6).
We deﬁne t iterations of ampliﬁcation as
Φt =
 tY
t=1
Γm
!
.
(10.92)
The state |τ⟩has two diﬀerent amplitudes representing solution and non-
solution. They deﬁne the two subspaces |τsolution⟩and |τnon⟩with
Φt · Wm+1|0⊗m⟩|1⟩= k · αt · |τsolution⟩+ (n −k) · βt · |τnon⟩⊗
|0⟩−|1⟩
√
2

(10.93)
ignoring the auxiliary qubt, or target qubit we can write
Φt · Wm|0⊗m⟩=
=
√
k·sin(θ0·(2·t+1))·|τsolution⟩+
p
(n −k)·cos(θ0·(2·t+1))·|τnon⟩. (10.94)

Search
193
We deﬁne a unitary operator UΦ that acts on two registers |t⟩and |x⟩and
one auxiliary qubit,
UΦ · |t⟩|x⟩⊗
|0⟩−|1⟩
√
2

= |t⟩|Φt|x⟩⊗
|0⟩−|1⟩
√
2

(10.95)
with
UΦ · |t⟩⊗
 
1
√
2m
X
x∈Bm
|x⟩
!
⊗
|0⟩−|1⟩
√
2

= |t⟩⊗(k · αt · |τsolution⟩+ (n −k) · βt · |τnon⟩) ⊗
|0⟩−|1⟩
√
2

(10.96)
after the application of UΨ the two registers and the auxiliary qubit are
entangled. In the ﬁrst step of the algorithm we build a superposition of m
qubits and µ qubits with T = 2µ taking into account the auxiliary bit. The
value of T can be estimated by the determination of the period of
sin(θ0 · (2 · t + 1))
assuming k = 1. It follows that
Wµ · Wm+1|0⊗µ⟩|0⊗m⟩|1⟩=
=
 
1
√
T
X
t∈Bµ
|t⟩
!
⊗
 
1
√n
X
x∈Bm
|x⟩
!
⊗
|0⟩−|1⟩
√
2

.
In the second step we apply the UΦ operator
UΦ
 
1
√
T
X
t∈Bµ
|t⟩
!
⊗
 
1
√n
X
x∈Bm
|x⟩
!
⊗
|0⟩−|1⟩
√
2

=
 
1
√
T
X
t∈Bµ
|t⟩
!
⊗(k · αt · |τsolution⟩+ (n −k) · βt · |τnon⟩) ⊗
|0⟩−|1⟩
√
2

.
(10.97)
In the third step we measure the second register of the compound system.
As the result the state of the system is projected to the subspace that
corresponds to the observed state and the vector representing the state is
renormalized to the unit length. The result of the second register is either
fsolution(t) = sin(θ0·(2·t+1)) or fnon(t) = cos(θ0·(2·t+1)). Both functions
have the same period and will be represented as f(t). The new amplitude
distribution is normalized and has the same periodic period as f(t). The
following steps correspond to the QFT period algorithm as before.

194
Principles of Quantum Artiﬁcial Intelligence
10.4
Circuit Representation
Grover’s ampliﬁcation is based on Gm = −Qx. and can be represented by
O(m) quantum gates. The unitary operator Λm reverse the sign of |0⟩
Λm · |0⟩= −|0⟩
and for |x⟩̸= |0⟩
Λm · |x⟩= |x⟩.
Λm can be implemented eﬃciently with f0(x)
f0(x) =
 1
if
x = 0
0
else
(10.98)
as
1
√n
X
x∈Bm
(−1)f0(x) · |x⟩⊗
|0⟩−|1⟩
√
2

.
(10.99)
We can write
Gm = −Qx = −Im + 2 · Pm = 2 · Pm −Im = −(Wm · Λm · Wm) (10.100)
with the auxiliary qbit for the Λm operator representation it becomes
Gm = −(Wm · Λm · Wm) ⊗
|0⟩−|1⟩
√
2

(10.101)
neglecting the auxiliary qbit it follows that
Wm · Λm · Wm =





1 −2
n
−2
n
· · ·
−2
n
−2
n
1 −2
n · · ·
−2
n
...
...
...
...
−2
n
−2
n
· · · 1 −2
n




.
(10.102)
The ﬁrst row of Wm which is positive and can be represented as
⟨w| =
X
x∈Bm
⟨x|
By the operator Λm it becomes negative. ⟨w| it multiplies with the ﬁrst
column of Wm
|w⟩=
X
x∈Bm
|x⟩
the ﬁrst column multiplied with the ﬁrst row results in
|w⟩⟨w| = Pm.

Search
195
This value is subtracted from
Im = Wm · Wm
and replaced by −|w⟩⟨w|
Im −|w⟩⟨w| −|w⟩⟨w| = Im −2 · |w⟩⟨w| = Im −2 · Pm.
(10.103)
Because Uf(0) that represents f0(x) can be represented by O(m) gates
and all other operators are based on Hadamard operator Wm built by a
direct product of m W1 matrices, Gm can be represented by O(m) quan-
tum gates. Given the fact that the operator Uf is represented by O(m)
gates, then Grover’s ampliﬁcation algorithm implements exhaustive search
in O(√n) steps in n-dimensional Hilbert space Hn.
10.5
Speeding up the Traveling Salesman Problem
A salesman must visit t cities; he must visit each city exactly once and
ﬁnish at the city where his tour started. We call such a tour a valid tour.
The costs of traveling from city i to city j are represented by cij. The costs
do not need to be symmetrical cij ̸= cji. The salesman wishes to conduct a
valid tour that costs at most k. The traveling salesman problem (TSP) is
the most popular NP −complete problem. An instance (ticket) leads to a
“yes” answer in polynomial time. Given a tour, we can verify whether the
tour is valid, and the costs are below k in polynomial time.
We search through all possible orderings of the cities and verify, for each
ordering, if the tour is valid and the costs are below k. A quantum circuit
UT SP with a polynomial number of quantum gates can verify, for a given
tour, whether it is a solution. Because we can not reset to the input state,
the circuit should recompute the output before applying the ampliﬁcation
step. For t cities, we can represent each city by ⌈log2 t⌉qubits, and a tour
by a register of
m∗= t · ⌈log2 t⌉
qubits. We will examine all of the possible orderings of t cities with rep-
etition for simplicity. Without repetitions, there are t! possible orderings;
with repetition,
tt = 2t log2 t = 2m
(10.104)
with
m = t · log2 t ≤m∗.

196
Principles of Quantum Artiﬁcial Intelligence
We apply the UT SP operator,
Γm := (Gm ⊗I1) · UT SP
(10.105)
determine the value of r by quantum counting
 r
Y
t=1
Γm
!
· Wm+1 · |0⊕m⟩|1⟩= |τ⟩⊗
|0⟩−|1⟩
√
2

(10.106)
and and determine the solution by the measurement of the register |τ⟩. The
computing costs are
O(t
t
2 ) = O
√
2m

(10.107)
with
t! ≫t
t
2 .
Any NP −complete problem can be solved in a similar way.
10.6
The Generate-and-Test Method
The generate-and-test method is a simple AI paradigm.
This approach
uses a generator and a tester. The generator produces all of the possible
solutions, and the tester evaluates each possible solution to see whether it
is the expected solution (see Figure 10.8).
Fig. 10.8
The generate-and-test method.

Search
197
This approach is mostly used to solve identiﬁcation problems.
The
generator produces hypotheses that are tested.
For some problems, the
possible solutions can be represented by some points in the problem space.
For other problems, the possible solutions can be represented by paths in
the problem space.
We can speed up the generate-and-test method by
mapping the generator into a superposition and the tester into an oracle
represented by a quantum circuit.

This page intentionally left blank
This page intentionally left blank

Chapter 11
Quantum Problem-Solving
11.1
Symbols and Quantum Reality
Problem-solving can be modeled by a production system that implements
a search algorithm. The search deﬁnes a problem space and can be rep-
resented as a tree. Because symbols do not by themselves represent any
utilizable knowledge, additional heuristic functions are used to speed up the
search. Without the use of heuristic functions, real-world problems become
intractable because of the exponential growth of the leaves in the tree. A
heuristic function is used that rates the value’s diﬀerent states according
to how far they are from the desired state. A best-ﬁrst search is that in
which the best rule (according to a heuristic function of the conﬂict set)
is chosen. The better the heuristic measure of the remaining distance to
the desired state is, the faster the best-ﬁrst search [Winston (1992)]. An
example for a simple heuristic function is the simple assumption that the
distance between the states in the problem space is related to the similarity
of the vectors that represent the states. A vector corresponds to a pattern
that mirrors the way that the biological sense organs describe the world and
is called a sub-symbol. The argument as presented before indicates that
the heuristic function results from the Euclidian geometry of the world as
experienced by humans. Could physical nature as described by quantum
physics also lead to a quantum heuristic? In the relation of sub-symbols to
symbols, do quantum-symbols exist?
In quantum computation, there are two known principles to speed up
the computation:
• The QFT can determine the period of a wave (periodic function) expo-
nentially faster than any known classical algorithm. QFT is based on
the unitary DFT matrix and FFT decomposition.
199

200
Principles of Quantum Artiﬁcial Intelligence
• Grover’s algorithm can speed up the search quadratically.
Grover’s
algorithm is based on the unitary Householder reﬂection, which is rep-
resented by a unitary matrix.
In a tree, we search for a leaf ξ. Using Grover’s algorithm, we represent the
corresponding search tree by a quantum Boolean circuit Utree. What is the
speedup in relation to the structure of the represented tree? We will in-
vestigate this question starting from classical tree search algorithms, which
will lead us to the general model of a quantum computer, the Tarrataca’s
quantum production system.
11.2
Uninformed Tree Search
In an
uninformed search, no additional information about the states is
given. The search represented by a search tree is performed from an initial
state through the following states until a goal state is reached. A search
tree is represented by nodes and edges. Each node represents a state, and
each edge represents a transition from one state to the following state. The
initial state deﬁnes the root of the tree. From each state ν, either Bν states
can be reached or the state is a leaf. Bν represents the branching factor
of the node v. A leaf represents either the goal of the computation or an
impasse when no valid transition to a succeeding state exists. In contrast to
a real tree in computer science, the root of a tree structure is at the top of
the tree and the leaves are at the bottom. Every node besides the root has a
unique node from which it was reached, called the parent. The parent is the
node above it and is connected by an edge. Each parent ν has Bν children.
The depth of a node ν is the number of edges to the root node. Nodes with
the same depth k deﬁne the level k. For a tree with a constant branching
factor B, each node at each level k has B children, and at each level k, there
are B · k nodes [Nilsson (1982)], [Luger and Stubbleﬁeld (1993)], [Russell
and Norvig (2010)].
Breadth-ﬁrst search
In a breadth-ﬁrst search the root node (level L =
0) is expanded ﬁrst. Then each children of the root at the level L = 1 are
expanded, they become the parents of the children at the level L = 2. Then
the procedure is repeated for the preceding levels until a goal is reached or
all nodes are impasse states (see Figure 11.1). Breadth-ﬁrst search performs
a level wise search. All nodes at level L have to be visited before visiting
a node at level L + 1. For a constant branching factor B Bm nodes are

Quantum Problem-Solving
201
Fig. 11.1
In a breadth-ﬁrst search the root node (level L = 0) is expanded ﬁrst. Then
each children of the root at the level L = 1 are expanded, they become the parents of
the children at the level L = 2.
expanded at level m with k = 0 being the root. The total number of nodes
at level m is represented by the geometric series
1 + B + B2 + B3 + · · · + Bm =
m
X
i=0
Bi = 1 −Bm+1
1 −B
= O(Bm).
(11.1)
Every node that is generated has to be represented in memory and the
number of nodes grows exponentially. The computing costs and the memory
requirements are in worst case O(Bm).
Depth-ﬁrst search
Progressive depending and local focusing leads to
a depth-ﬁrst search [Newell (1990)]. It always expands the deepest node
in the search tree until a goal is reached or all nodes are impasse states
(see Figure 11.2).
An impasse is present when no valid transition to a
Fig. 11.2
Depth-ﬁrst search always expands the deepest node in the search tree until a
goal is reached or all nodes are impasse states.

202
Principles of Quantum Artiﬁcial Intelligence
succeeding state exists. In this case backtracking to the previous state is
done. Another state can be chosen, if possible, or backtracking is repeated.
In worst case the same number of nodes are visited as in breadth-ﬁrst
search. Commonly this number is less and the memory requirements are
low compared to breadth-ﬁrst search. Only a path from the root to the
current node with the remaining unexpected sibling nodes for each node on
the path have to be represented. If m is the maximum depth of the search
tree and for a constant branching factor B the memory requirements are
B · m + 1
(11.2)
we add a one because the node at the depth m is a leaf or an impasse. The
computing costs are in the worst case O(Bm) and the memory requirements
B · m = O(m).
Depth-ﬁrst search ﬁrst search contrary to breadth-ﬁrst
search can fail in the case the maximum depth of the search tree converges
to m →∞. A solution to this problem is the combination of both methods
with the beneﬁts of both of them.
Iterative deepening search
In iterative deepening search we gradually
increase the limit of the search from one, to two, three, four and continue
to do it until a goal is found. For each limit mk a depth-ﬁrst search is
performed from the root with mk being the maximum depth of the search
tree. During iterative deepening states are generated multiple times [Korf
(1985)], [Russell and Norvig (2010)]. The time complexity of iterative deep-
ening search is of the same order of magnitude as breadth-ﬁrst search [Korf
(1985)], as explained by Richard E. Korf: “Since the number of nodes on
a given level of the tree grows exponentially with depth, almost all time is
spent in the deepest level, even through shallower levels are generated an
arithmetically increasing number of times.” The paradox can be explained
by the arithmetico-geometric sequence. The number of nodes for iterative
deepening for each level starting with level zero (for simplicity) is given by
1
1 + B
1 + B + B2
· · ·
to level m is given by
1 + B + B2 + B3 + · · · + Bm

Quantum Problem-Solving
203
the costs Cm (total number of visited nodes) are represented by the
arithmetic-geometric sequence that represent the sum of each iteration
Cm = (m+ 1)·1 + m·B +(m−1)·B2 +(m−2)·B3 + · · ·+1 ·Bm, (11.3)
Cm =
m
X
i=0
(m + 1 −i) · Bi.
(11.4)
With
Cm =
m+1
X
i=1
(m + 1 −(i −1)) · Bi−1.
(11.5)
According to Riley [Riley et al. (2006)]
Cm−Cm·B = Cm·(1−B) = m+1−(2·m+1)·Bm+1+ B · (1 −Bm)
(1 −B)
, (11.6)
Cm = m + 1
1 −B −(2 · m + 1) · Bm+1
1 −B
+ B · (1 −Bm)
(1 −B)2
,
(11.7)
by simplifying this equation we arrive at
Cm = 1 + m −B · m −2 · B1+m · (1 + m) + B2+m · (1 + 2 · m)
(B −1)2
= O(Bm).
(11.8)
The computing costs are in the worst case O(Bm) and the memory require-
ments B · m = O(m). It follows that we should use depth-ﬁrst search if
the depth of the solution is known, otherwise we should prefer iterative
deepening over breadth-ﬁrst search due to the low memory requirements.
Loops
To speed up the search the formation of loops should be prohib-
ited. By comparison with the sequence of carried out states loops can be
prevented.
11.3
Heuristic Search
Heuristic search is based on a heuristic function h(ν) that estimates the
cheapest cost from the node ν to the goal.

204
Principles of Quantum Artiﬁcial Intelligence
Greedy best-ﬁrst search
It expands the node ν that is closest to the
goal according to a heuristic function h(ν). Out of the B children the node
νi is chosen with
min
1≤i≤B(h(νi)).
(11.9)
Like depth-ﬁrst search it follows a single path to the goal. It always expands
the deepest node in the search tree according to h(ν) until a goal is reached
or all nodes are impasse states. An impasse is present when no valid transi-
tion to a succeeding state exists. In this case backtracking to the previous
state is done. Another state can be chosen that is closest to the goal, if
possible, or backtracking is repeated. The computing costs are in the worst
case O(Bm) and the memory requirements B · m = O(m). However with a
good heuristic function h(ν) the cost can be reduced considerably.
A search
It evaluates the nodes through a function f(ν) that estimates
the cheapest solution that passes through the node ν. The function f(ν)
is composed out of the heuristic function h(ν) and the function g(ν) that
indicates the cheapest costs of reaching the node ν from the root node
representing the initial state.
f(ν) = g(ν) + h(ν).
As breadth-ﬁrst search it keeps all evaluated nodes in memory. The com-
puting costs and the memory requirements are in the worst case O(Bm).
A∗search
A∗search is equivalent to the A search with the constraint
that the function h(ν) is an admissible heuristic, it never overestimates the
cost to reach the goal. If c(ν) are the true cost to reach the goal it follows
h(ν) ≤c(ν).
It follows that the triangle inequality is valid with c(ζ, ν) representing the
true cost from node ζ to node ν
h(ζ) ≤c(ζ, ν) + h(ν).
(11.10)
A∗search is complete and optimal. A solution is found if it exists. A∗
search is optimal, no other algorithm is guaranteed to expand fewer nodes.
However the number of expanded nodes can grow exponential unless the
error of estimates the cheapest cost from the node ν to the goal is extremely
small
c(ν) −h(ν) ≤O(log c(ν)).
(11.11)
The computing costs and the memory requirements are in worst case
O(Bm).

Quantum Problem-Solving
205
11.3.1
Heuristic functions
We will demonstrate the principles of heuristic function h(ν) on the 8-
puzzle example. Two common heuristics for this task are the number of
misplaced tiles, and the “city-block distance” [Nilsson (1982); Pearl (1984);
Luger and Stubbleﬁeld (1998)]. The ﬁrst heuristic counts the number of
misplaced tiles out of place in each state compared to the desired goal.
However this heuristic fails to take into account all available information
such as the distance the tiles must be moved. The “city-block distance”
sums all the distances by which the tiles are out of place, with one count
for each square a tile must be moved to reach a position of the desired
state. The “city-block distance”, also called the “Manhattan distance”, is
often better than the “number of misplaced tiles” (see Figure 11.3). Both
heuristic functions are admissible.
3
2
1
7
8
6
5
4
1
4
7
7
7
4
2
3
5
1
2
3
5
7
8
6
8
6
5
2
1
3
8
4
6
8
6
1
2
3
4
5
Fig. 11.3
The ﬁrst pattern (upper left) represents the initial conﬁguration and the last
(low right) the desired conﬁguration. The series of moves describe the solution to the
problem using the “city-block distance” heuristic function.
11.3.2
Invention of heuristic functions
Euclidean geometry of the world
The invention process can be in-
spired by the Euclidean geometry of the world as stated before. The Ok-
sapmin tribe of Papua New Guinea counts by associating a number with
the position of the body [Lancy (1983)]. This suggests a representation of
numbers by bars at certain positions which can overlap. A bar at a certain
position codes the magnitude of the number. The closeness or similarity of
diﬀerent numbers is determined by the overlap of the bar codes [Wichert
et al. (2008)] (see Figure 11.4). In the 8-puzzle, each tile is deﬁned by its

206
Principles of Quantum Artiﬁcial Intelligence
1
2
3
4
Fig. 11.4
A bar at a certain position codes the magnitude of the number. The closeness
or similarity of diﬀerent numbers is determined by the overlap of the bar codes.
corresponding coordinates. Two numbers can be represented by two bars
(see ﬁg 11.5). The amount of overlapping indicates the closeness of diﬀer-
ent tiles.
The resulting function is equivalent to the city-block distance.
4
5
6
1
2
3
7
8
Fig. 11.5
The desired state for the task 8-puzzle and its representation by bars. The
associative ﬁelds in which the objects are describe have a ﬁxed dimension of ten times
ten pixels. Because of this, excessive unused space is present.
The distance between a state and a desired state corresponds to the sum of
distance by which the tiles are out of place. The closeness or similarity of a
tile to the desired position of the tile is determined by the overlap of the two
bar codes representing the tile (see Figure 11.6). The overlap corresponds
to the distance by which the tile is out of place. The heuristic function
emerged by a reasonable sub-symbolical representation of the states in the
8-puzzle world [Wichert et al. (2008)].

Quantum Problem-Solving
207
Fig. 11.6
The tile “1” at the position of the tile “6” (shown by dotted bars).
The
value of the city-block distance is three. The hamming distance between the patterns
representing the tile “1” and “6” is also three.
Symbolical problems
Generally the invention of heuristic functions is
diﬃcult. In many applications there is no relation to the Euclidean ge-
ometry of the world. Examples are chemical structures or mathematical
expressions as used in symbolical integration. One way to is to approxi-
mate a problem by a relaxed problem with fewer restrictions. For example
in the relaxed version of the n-puzzle problem we assume we can move
each tile to its position independently of moving the other tiles. The re-
laxed problem represents an admissible heuristic function of the problem.
An optimal solution in the original problem c(ν) is also a solution in the
relaxed problem. By the abolition of present restrictions the cost of the
relaxed problem are less or equal to c(ν). Another way is to decompose
the problem into sub-problems and to store all exact solution in a database
and to use it to speed up the search using the solution of sub-problems or
to extract the heuristic function by machine learning.
11.3.3
Quality of heuristic
A frequent used measure is the eﬀective branching factor b. It is indepen-
dent of the length of the optimal solution. It is related to the costs Cm
represented by the number of generated nodes during A∗search [Nilsson
(1982)], [Russell and Norvig (2010)]. It is represented by the geometric
series (we do not omit level L=0 for simplicity)
Cm = 1 + b + b2 + b3 + · · · + bm =
m
X
i=0
bi = 1 −bm+1
1 −b
.
(11.12)
We cannot represent b as a function of m and Cm, however we can plot the
values of Cm in dependence of b and m (see Figure 11.7). The exponential

208
Principles of Quantum Artiﬁcial Intelligence
grow cost Cm can be only stopped in the case of fully informed search b = 1.
A heuristic reduces the value of b. By doing so it extends the horizon of
1
2
3
4
2
4
6
8
10
0
5000
10000
Fig. 11.7
Values of Cm in dependence of branching factor b (1-4) and depth m (1-10).
m of tractable Cm values. No classical universal heuristic function for all
domains exist, for each domain a heuristic function has to be invented. This
process in general is non-trivial.
11.4
Quantum Tree Search
11.4.1
Principles of quantum tree search
There is a simple relation between a tree search and information theory.
Suppose that we have a constant branching factor B and the depth of
the tree is m. In this case, there are Bm = n leaves. The goal of the
search is to visit all of the leaves. The ideal entropy indicates the minimum
number of optimal questions that describe the result of an experiment.
The experiment represents n leaves of a search tree with equal probabilities
p = (1/n, 1/n..., 1/n). The maximal ideal Entropy is
H(F) = m = −
X
i
pi logB pi = logB n
(11.13)

Quantum Problem-Solving
209
and corresponds to the depth of the search tree. In the case of B = 2,
each of the m questions has a reply of either “yes” or “no” and can be
represented by a bit (see Figure 11.8). The m answers are represented by
Fig. 11.8
Search tree for B = 2 and m = 2. Each question can be represented by a bit.
Each binary number (11, 10, 01, 00) represents a path from the root to the leaf.
a binary register of length m. There are n diﬀerent binary registers, which
represent all of the possible binary numbers of length m. Each binary num-
ber represents a path from the root to a leaf. A path could contain loops, or
an impasse could be present before a leaf is reached. However, for each goal,
a certain binary number indicates the solution. For a constant branching
factor B > 2, each question has B possible answers. The m answers can be
represented by a base-B number with m digits. For example, with B = 8,
the number is represented in an octal numeral system. Alternatively, the
m answers can be represented by a binary register of length m · ⌈log2 B⌉.
Using Grover’s algorithm, we search through all possible paths and ver-
ify, for each path, whether it leads to the goal state [Tarrataca and Wichert
(2011b)]. A quantum circuit Up with a polynomial number of quantum
gates can verify whether each path corresponds to a sequence of produc-
tions that lead from the initial state to the goal state. Because we can not

210
Principles of Quantum Artiﬁcial Intelligence
reset to the input state, the circuit should recompute the output before
applying the ampliﬁcation step. We apply the Up operator with,
µ = m · ⌈log2 B⌉
(11.14)
with the special case µ = m for constant branching factor B = 2, it follows
Γµ := (Gµ ⊗I1) · Up
(11.15)
determine the value of r by quantum counting
 r
Y
t=1
Γµ
!
· Wµ+1 · |0⊕µ⟩|1⟩= |τ⟩⊗
|0⟩−|1⟩
√
2

(11.16)
and and determine the solution by the measurement of the register |τ⟩.
Because
√
Bm ≈
p
2m·⌈log2 B⌉
(11.17)
it follows that the computing costs are
O
√
Bm

= O
√
2µ

= O
p
2m·⌈log2 B⌉

(11.18)
which is much less then uninformed tree search algorithms with
O
p
2m·⌈log2 B⌉

= O
√
Bm

≪O(Bm) = O

2m·⌈log2 B⌉
.
(11.19)
The eﬀective branching factor for quantum tree search bq is given by the
equation
Cm = 1 −bm+1
1 −b
≈B
m
2
(11.20)
it follows that
bq ≈
p
2⌈log2 B⌉≥
√
B.
(11.21)
11.4.2
Iterative quantum tree search
The presented algorithm is limited to a search of depth m. This constraint
can be overcome by the quantum iterative deepening search. A quantum
iterative deepening search is equivalent to the iterative deepening search
[Tarrataca and Wichert (2012a)], [Tarrataca and Wichert (2013b)].
We
gradually increase the limit of the search from one, to two, three, and four
and continue to search until the goal is found. For each limit m, a quantum
tree search is performed from the root, with m being the maximum depth of
the search tree. The possible solutions are determined by a measurement.

Quantum Problem-Solving
211
The time complexity of an iterative deepening search has the same order
of magnitude as the quantum tree search. With
β := B
1
2
the computing costs for each level starting with level zero (for simplicity)
are given by
O(1)
O(β) = O(B
1
2 )
O(β2) = O(B
2
2 )
· · ·
to level m is given by
O(βm) = O(B
m
2 )
the total costs of mk iterations with m measurments are O(B
m
2 ) =
O
√
Bm

O(1) + O(B
1
2 ) + O(B
2
2 ) + O(B
3
2 ) + · · · + O(B
m
2 ) = O(B
m
2 ) = O
√
Bm

(11.22)
O
√
Bm

= O
√
2µ

= O
p
2m·⌈log2 B⌉

the equation is based on the geometric series
1+β+β2+β3+· · ·+βm =
m
X
i=0
βi = 1 −βm+1
1 −β
= O(βm) = O(B
m
2 ) (11.23)
and the eﬀective branching factor is equal to bq as for quantum tree search.
11.4.3
No constant branching factor
Suppose the branching factor is not constant, in this case the tree search
can be described by the eﬀective branching factor. For uninformed tree
search for a large number of instances (diﬀerent initial and goal states)
the eﬀective branching factor converge to the averaged branching factor
for uninformed tree search [Tarrataca and Wichert (2011b)].
For a not
constant branching factor the quantum tree search the maximal branching
factor Bmax has to be used for the quantum tree search. For Bmax the

212
Principles of Quantum Artiﬁcial Intelligence
quantum algorithm using qubit representation is better then the classical
tree search described by the eﬀective branching factor b in the case
b > bq.
(11.24)
If Bmax is a power of two then
b > bq =
p
2⌈log2 Bmax⌉=
p
Bmax
(11.25)
otherwise
b > bq =
p
2⌈log2 Bmax⌉>
p
Bmax
(11.26)
for example Bmax = 9, then
bq = 4 =
√
24 > 3 =
√
9.
and base-9 instead of qubit representation would be more economical. In
production systems Bmax corresponds often to the number of productions.
In the 8-puzzle example there are four productions in the long term memory
and Bmax = 4 and bq = 2. For blind search the eﬀective branching factor is
≈2.8 [Russell and Norvig (2010)], the “city-block distance” heuristic eﬀec-
tive branching factor is ≈1.24. However one should keep in mind that the
invention of heuristic functions is diﬃcult and the 8-puzzle is a well stud-
ied problem in the AI community [Nilsson (1982); Pearl (1984); Luger and
Stubbleﬁeld (1998)]. Heuristic functions can fail. For example in instances
of a problem in which one cannot perform the ﬁrst necessary action without
undoing them at a later stage, also called “Sussman anomaly” [Sussman
(1975)]. Iterative quantum tree search never fails. This is because during
the iterative ampliﬁcations of the Grover’s algorithm one can adapt the
iterations in such a way that the probability of ﬁnding a solution is exactly
one.
11.5
Quantum Production System
The control structure of a production system (reaction system) can be de-
ﬁned in terms of an iterative quantum tree search [Tarrataca and Wichert
(2012b)], [Tarrataca and Wichert (2011a)]. In an iterative quantum tree
search, the limit is increased gradually in each step t. For each limit mt, a
quantum tree search is performed from the root, with mt being the max-
imum depth of the search tree. With a maximal branching factor Bmax
and using the qubit representation, there are nt diﬀerent binary registers,
which represent all of the possible binary numbers of length µt with
nt = 2µt = 2mt·⌈log2 Bmax⌉.
(11.27)

Quantum Problem-Solving
213
The register is called a path descriptor κt
i for the iteration step t with
i ∈{1, 2, · · · , nt}. Each path descriptor κt
i represents a possible path from
the root to the leaf. A path can be undeﬁned. However, for each goal state,
a corresponding κt
i exists. The quantum production system is deterministic
and reversible because the search is determined by the path descriptor. No
conﬂict resolution is needed.
The circuit U t
p veriﬁes whether each path
speciﬁed by the path descriptor κt
i corresponds to a sequence of productions
that leads from the initial state to the goal state.
The computation is
described by the initial state, the goal state and the long-term memory. The
long-term memory is composed of several productions. The productions
deﬁne the circuit U t
p. The result of the computation is represented by a path
descriptor κt
i. After each iterative step t, Grover’s algorithm is performed.
If no goal at step t was reached, then the resulting path descriptor κt
i is
randomly chosen. In a case in which the goal state can be reached by only
one path at iteration t, the path descriptor κt
i is chosen deterministically
by Grover’s algorithm. The number of possible paths to a goal state can
be determined by quantum counting.
11.6
Tarrataca’s Quantum Production System
A formal deﬁnition of Tarrataca’s quantum production system is based on
the pure production system [Tarrataca and Wichert (2012b)]. We explain
the principles of Tarrataca’s quantum production system on a trivial exam-
ple, the 3-puzzle. In the next step we generalize to n-puzzle and to general
quantum production systems. The description is a simpliﬁed version of the
one presented in [Tarrataca and Wichert (2011a)].
11.6.1
3-puzzle
The 3-puzzle is composed of three numbered movable tiles in a 2 × 2 frame
(see Figure 11.9).
Fig. 11.9
The desired conﬁguration of the 3-puzzle.

214
Principles of Quantum Artiﬁcial Intelligence
One cell of the frame is empty and because of this, tiles can be moved
around to form diﬀerent patterns. The goal is to ﬁnd a series of moves of
tiles into the blank space that changes the board from the initial conﬁgu-
ration to a desired conﬁguration). There are twelve possible conﬁgurations
(see Figure 11.10). For any of this conﬁguration only two movements are
possible. The movement of the empty cell are either a clockwise or counter-
clockwise movement.
Fig. 11.10
There are twelve possible conﬁgurations. For any of this conﬁguration only
two movements are possible. The movement of the empty cell are either a clockwise or
counter-clockwise movement.
The 3-puzzle is tractable and requires fewer qubits to encode. However
the model can be generalized to any n-puzzle.
The number of possible
solvable conﬁgurations of the n-puzzle is
n!
2 .
(11.28)
There are n! diﬀerent conﬁgurations, however only n!/2 are solvable by
moving the empty tile according to the rules. The problem of ﬁnding the
shortest solution is NP −complete.
In our example there are four diﬀerent objects: 3 cells and one empty
cell.
Each object can be coded by two qubits (22) and a conﬁguration
of the four objects can be represented by a register of eight qubits |x⟩.
The control function of the quantum production system needs to fulﬁll two
requirements [Tarrataca and Wichert (2011a)]:
• For a given board conﬁguration and a production rule determine the
new board conﬁguration.
• To determine if the conﬁguration is the goal conﬁguration.

Quantum Problem-Solving
215
The function g(x) determines if the conﬁguration is the goal conﬁguration
g(x) = g(x1, x2, x3, x4, x5, x6, x7, x8
|
{z
}
board configuration
) =
 1 if goal board conﬁguration
0 otherwise.
(11.29)
Function g(x) is represented by a unitary operator T . T acts on the 8 + 1
qubits with x ∈B8 and c ∈B1
T · |x⟩|c⟩= |x⟩|f(x) ⊕c⟩.
The new board conﬁguration is determined by productions that are
represented by the function p.
There are four possible positions of the
empty cell. The input of the function p is the current board conﬁguration
and a bit m that indicates whether the blank cell should perform a clockwise
(m = 1) or counter-clockwise movement (m = 0). Together, there are 8
possible mappings, which are represented by 8 productions. There are four
possible positions of the empty cell times two possible moves. For simplicity,
we will represent the mappings of the function p by a unitary permutation
matrix L(1). For each mapping, the empty tile can have three diﬀerent
neighbors. It follows that, in total, there are 24 = 8 × 3 instantiationed
rules. They correspond to permutations in the unitary permutation matrix
L(1). The matrix acts on the 8 + 1 qubits with m ∈B1 and x ∈B8
L(1) · |m⟩|x⟩= |m⟩|γ⟩.
The L(1) matrix represents the long-term memory of our production sys-
tem.
Decomposition
An important open question is whether the permutation
matrix L(1) of dimension 512 = 29 can be decomposed. It is possible to
determine if a permutation is tensor decomposable and to chose an eﬃcient
tensor decomposition if present [Kolda and Bader (2009)]. An alternative
less costly representation of the long-term memory can be realized by a
uniformly polynomial circuit. The circuit is based on the truth table that
describes the mappings of the function p.
In the case in which the system is not based on an iterative quantum tree
search but instead is based on a quantum tree search, we should apply only
the move of the blank cell if and only if the input board conﬁguration is
not a target board conﬁguration. This process could be performed by in-
cluding a reference to function g in the new function p’s deﬁnition and can
be described by a unitary operator.

216
Principles of Quantum Artiﬁcial Intelligence
The operator that describes the application of a single production rule for
the 3-puzzle and a test condition in order to determine if the ﬁnal board is
a target conﬁguration board is represented in B10 as
(I1 ⊗T ) · (L(1) ⊗I1) · |m, x1, x2, x3, x4, x5, x6, x7, x8, c⟩.
(11.30)
The operator that describes the application of a two production rules for
the 3-puzzle and a test condition in order to determine if the ﬁnal board is
a target conﬁguration board is represented with
L(2) · |m2, m1⟩|x⟩= |m2, m1⟩|γ⟩
as
(I2 ⊗T ) · (L(2) ⊗I1) · (L(2) ⊗I1) · |m2, m1, x1, x2, x3, x4, x5, x6, x7, x8, c⟩
(I2 ⊗T ) · (L(2) ⊗I1)2 · |m2, m1, x1, x2, x3, x4, x5, x6, x7, x8, c⟩.
(11.31)
The operator that describes the application of a t production rules for the
3-puzzle and a test condition in order to determine if the ﬁnal board is a
target conﬁguration board is represented with
L(t) · |mt, · · · , m1⟩|x⟩= |mt, · · · , m1⟩|γ⟩
and
|κt⟩= |mt, · · · , m1⟩
as
(It ⊗T ) · (L(t) ⊗I1)t · |κt, x1, x2, x3, x4, x5, x6, x7, x8, c⟩.
(11.32)
In
quantum
computation
it
is
not
possible
to
reset
the
register
x1, x2, x3, x4, x5, x6, x7, x8 to the pattern representing the initial state. In-
stead we un-compute the output back to the input before applying the
ampliﬁcation step of the Grover’s algorithm. Because of the unitary evolu-
tion it follows that
(It ⊗T )∗· ((L(t) ⊗I1)∗)t · (It ⊗T ) · (L(t) ⊗I1)t·
(11.33)
·|κt, x1, x2, x3, x4, x5, x6, x7, x8, c⟩= |κt, x1, x2, x3, x4, x5, x6, x7, x8, c⟩
the computation can be undone. The result indicated in the qubit c and can
saved into the qubit y by a controlled not gate MCNOT. The computation
is deﬁned on B10+t with
U3−puzzle := (It ⊗T ⊗I1)∗· ((L(t) ⊗I2)∗)t ·

Quantum Problem-Solving
217
·(I(9)+t ⊗MCNOT ) · (It ⊗T ⊗I1) · (L(t) ⊗I2)t
(11.34)
simpliﬁed as
U3−puzzle · |κt, x1, x2, x3, x4, x5, x6, x7, x8, c, y⟩= U3−puzzle · |κt, x, c, y⟩.
(11.35)
We apply the U3−puzzle operator,
ΓT := (Tt ⊗I10) · U3−puzzle
(11.36)
determine the value of r by quantum counting
 r
Y
t=1
ΓT
!
·
 (Wt · |0⊕t⟩) ⊗|x⟩⊗|0⟩⊗(W1 · |1⟩)

=
(11.37)
it follows
 r
Y
t=1
ΓT
!
·(Wt ⊗I10)·(It+9 ⊗W1)·|0⊕t⟩|x⟩|0⟩|1⟩= |κt
i⟩|x⟩|0⟩⊗
|0⟩−|1⟩
√
2

(11.38)
and and determine the solution by the measurement of the register |κt
i⟩that
represents the path descriptor. The 3-puzzle quantum production system
highlighted the principles of quantum production systems. It does not give
any true computational speed up due to the simplicity of the problem and
due to the na¨ıve implementation of the long term memory by a permutation
matrix.
11.6.2
Extending for any n-puzzle
For n-puzzle there are n + 1 diﬀerent objects: n cells and one empty cell.
Each object can be coded by ρ = ⌈log2 n + 1⌉qubits and a conﬁguration of
n + 1 objects can be represented by a register of z := ρ · (n + 1) qubits |x⟩.
The function g(x) is represented a unitary operator T . T acts on the
z + 1 qubits with x ∈Bz and c ∈B1
T · |x⟩|c⟩= |x⟩|f(x) ⊕c⟩.
The new board conﬁguration is determined by the function p. The input
of the function p is the current board conﬁguration and two bits m = m1, m2
indicating whether the blank cell should perform move up (m = 0 = |00⟩),
down (m = 1 = |01⟩), right (m = 2 = |10⟩) or left (m = 3 = |11⟩). The
mappings of the function p between states can be described by a truth table
and can represented as column permutations.

218
Principles of Quantum Artiﬁcial Intelligence
In the case the empty cell is in the corner only two movements are
possible, the other one are not valid. In the case of not valid movement a
halt bit ﬂag h represented by one bit is set and no further production is
applied. We move the blank cell if and only if the halt bit ﬂag h is not
set. The same applies for empty cell on the edge. In this case only three
movements are possible. For 8-puzzle Bmax = 4 and Baverage
Baverage = 4 · 1 + 2 · 4 + 3 · 4
9
= 2.7778.
(11.39)
With growing value n Baverage converges to Bmax.
There are maximal n + 1 possible positions of the empty cell.
The
empty cell can move either up, down, left or right or the halt bit ﬂag
is set.
Together there (4 + 1) actions and for n + 1 positions there are
(n + 1) · 5 possible mappings represented by n · 5 + 5 permutations. In each
combination the empty tile can have n diﬀerent neighbors. It follows that
in total there are (n + 1) · 5 · n = 5 · n2 + 5 · n instantiationed productions.
A 8-puzzle there would represented by 360 instantiationed productions.
The instantiationed productions are represented unitary permutation
matrix L(1). The matrix acts on the z + 3 qubits with m ∈B2 and x ∈Bz
and h ∈B1
L(1) · |m⟩|x⟩|h⟩= |m⟩|γ⟩|h′⟩.
11.6.3
Pure production system
The pure production system model has no mechanism for recovering from
an impasse [Post (1943)]. The system halts if no production can ﬁre. It is
composed of the set of productions L (the long term memory) and control
system C. A pure production system is a sextuple:
(Σ, L, W, γi, γg, C)
(11.40)
with
• Σ is a ﬁnite alphabet;
• W is the working memory. It represents a state γ ∈Σ.
• L is the long term memory. It is the set of B productions. A produc-
tion p has the form (precondition, conclusion) ∈Σ. The precondition
is matched against the contents of the working memory. If the precon-
dition is met then the conclusion is preformed and changes the contents
of the working memory;
• γi ∈Σ is the initial state. The working memory is initialized with the
initial state γi;

Quantum Problem-Solving
219
• γg ∈Σ is the goal state;
• δ is the control function of the form Σ →L × Σ × h. It chooses a
production and ﬁres it or halts h.
If C(γ) = (p, γ′, h), then the working memory contains symbol γ. It is
substituted by the the production p by γ′ or the computation halts h. The
computation halts if the goal state γg is reached or an an impasse is present
(no production can be applied).
11.6.4
Unitary control strategy
A pure production system can be converted into a quantum production
system by mapping the control strategy into a unitary control strategy
represented by a unitary operator L(t) on a register |v1⟩[Tarrataca and
Wichert (2012b)], [Tarrataca and Wichert (2013b)].
L(t) · |v1⟩= |v2⟩.
L(t) is used during the iterative quantum tree search to the limit t. The
register |v1⟩is determined by,
• µ = t · ⌈log2 Bmax⌉maximal branching factor represents the path de-
scriptor κt
i
• α = ⌈log2 |Σ⌉, represents the number of bits required to encode the
symbol set
• β = ⌈log2 B⌉, represents the number of bits required to encode each
one of the productions;
• η is a single bit used to encode h
The size of |v1⟩is
ℵ:= µ + α + β + η
qubits. 2ℵcombinations can be represented. A combination can be rep-
resented by a one at a certain position of the vector. The corresponding
unitary operator L(t) is represented by a matrix of the dimension 2ℵ× 2ℵ.
The matrix is sparse, it is populated primarily with zeros. It is possible to
determine if a permutation is tensor decomposable and to chose an eﬃcient
tensor decomposition if present [Kolda and Bader (2009)]. Additionally we
need a unitary operator T that determines if the state is the goal state
γg. As indicated in the 3-puzzle example, the circuit should recompute the
output before applying the ampliﬁcation step of the Grover’s algorithm.

220
Principles of Quantum Artiﬁcial Intelligence
11.7
A General Model of a Quantum Computer
In classical models such as the Turing machine, the end of a calculation
is indicated by a halt state.
An observer must check if the calculation
halted.
In a quantum Turing machine, a halt ﬂag can be non-trivially
implemented due to entanglement and the collapse of the halt qubit after
the measurement. A solution to this problem is a universal quantum Turing
machine in [Bernstein and Vazirani (1993)], which does not incorporate
into its deﬁnition the concept on non-termination. Myers in [Myers (1997)]
argues that the models presented in [Deutsch (1985)] and [Bernstein and
Vazirani (1993)] are not truly universal because they do not allow for non-
terminating computation.
A quantum production system represents a general model of computa-
tion. This type of system is an alternative approach to the quantum Turing
machine and allows an elegant description of the Halting problem through
the iterative quantum tree search. It is possible to simulate classical uni-
versal models of computation such as the universal Turing machine by a
quantum production system as shown in [Tarrataca and Wichert (2013b)].
This model can operate independently of whether the computation termi-
nates or not. The quantum production system also provides the maximal
speedup of √n in the case where the Turing machine simulation allows for
n multiple computational branches [Tarrataca and Wichert (2013b)].
A quantum computer based on a quantum production system would
involve classical artiﬁcial intelligence programing languages such as OPS5
[Brownston et al. (1985)]. OPS5 programs are executed by matching work-
ing memory elements with productions in long-term memory [Forgy (1981)].
Such a programmer does not need to contend with quantum gates, nor is
it required to address the principles of quantum computation. However, a
strong artiﬁcial intelligence background is essential.
11.7.1
Cognitive architecture
Uniﬁed theories of cognition is a theory that attempts to unify all of the
theories of the mind in a single framework.
Allen Newell proposed the
SOAR cognitive architecture [Laird et al. (1987)], [Newell (1990)], [Franklin
(1997)]. SOAR is an architecture of the mind: a ﬁxed structure underlying
the ﬂexible domain of cognitive processing as well as an architecture for
intelligent agents. All of the problem solving activity is formulated as the

Quantum Problem-Solving
221
selection and application of rules to a state to achieve a goal. In SOAR,
the domain knowledge is divided into two categories:
• basic problem space knowledge represents legal moves that are repre-
sented by productions;
• control knowledge, such as, for example, heuristic functions and other
control strategies.
With basic knowledge, SOAR can proceed to perform an unguided search
using depth ﬁrst. Resolving an impasse leads to learning. Impasses are
given, for example, when
• two or more productions are chosen to ﬁre;
• no production can ﬁre;
• one chosen production is rejected by another.
The learning mechanism is called chunking. Chunking collapses the results
of an impasse into a production that can then be ﬁred if the same, or similar
situation occurs again [Laird et al. (1986)]. Chunking is a psychological
phenomenon that involves the association of expressions and the production
of a new, single expression [Newell (1990)]. An extension of the proposed
SOAR cognitive architecture by a quantum production system would lead
to a hybrid architecture. The quantum production system would be invoked
if an impasse were present. Such a hybrid approach would speed up the
learning process without a need for domain-speciﬁc control knowledge (see
Figure 11.11).
11.7.2
Representation
The representation of the knowledge is the most important aspect. The uni-
tary operator L that represents the long-term memory with the productions
can be represented by a permutation matrix. It is possible to determine
whether an abstract permutation is tensor decomposable and to choose an
eﬃcient tensor decomposition if present [Kolda and Bader (2009)].
The question of how to decompose L representing a real world problem
is related to the question of how to decompose a problem into sub-problems.
A problem solving strategy is given by breaking a problem up into a set
of subproblems, solving each of the subproblems, and then composing a
solution to the original problem from the solutions of the subproblems.
How can we decompose a 15-puzzle into an 8-puzzle for example? This

222
Principles of Quantum Artiﬁcial Intelligence
Fig. 11.11
An extension of the proposed SOAR cognitive architecture by a quantum
production system would lead to a hybrid architecture. The quantum production system
would be invoked if an impasse is present. Such an hybrid approach would speed up the
learning process without the need of domain speciﬁc control knowledge.
problem corresponds to the deﬁnition of a tensor product and the eﬃcient
implementation of a quantum production system to be the decomposition
of L,
L =
 a11 · B a12 · B
a21 · B a22 · B

= A ⊗B.

Chapter 12
Quantum Cognition
The wave function
in quantum mechanics represents a superposition of
states. Suppose that an unobservable evolves smoothly and continuously;
however, during the measurement, it collapses into a deﬁnite state. Ac-
cording to most physical textbooks, the existence of the wave function and
its collapse is only present in the microscopic world and is not present
in the macroscopic world. More physical experiments indicate that wave
functions are present in the macroscopic world [Vedral (2011)]. Physical
experiments state that the size does not matter and that a very large num-
ber of atoms can be entangled [Ghosh et al. (2003)], [Amico et al. (2008)].
Clues from psychology indicate that human cognition is based on quantum
probability rather than the traditional probability theory as explained by
Kolmogorov’s axioms [Busemeyer et al. (2006)], [Busemeyer and Trueblood
(2009)], [Busemeyer et al. (2009)], [Busemeyer and Bruza (2012)]. This
approach would lead to the conclusion that a wave function can be present
at the macro scale of our daily life.
12.1
Quantum Probability
The quantum coin is a system with two states 0 and 1 with the mapping
|0⟩→
1
√
2 · |0⟩+ 1
√
2 · |1⟩
(12.1)
and
|1⟩→
1
√
2
· |0⟩−1
√
2
· |1⟩
(12.2)
represented by W1.
If the system starts in state |0⟩and undergoes the
time evolution of two steps the probability of observing 0 becomes 1 due
223

224
Principles of Quantum Artiﬁcial Intelligence
to the fact, of destructive interference. A Markov chain can model a fair
coin. In this case the information about the initial state is lost and a ﬁxed
distribution is reached. If constantly observed the quantum coin has the
same behavior as a fair coin described a Markov chain. Each time the coin
is tossed the “random” eﬀect is observed. During the evolution of a not
observed quantum coin the information about the initial state is not lost,
the system is fully deterministic. For a quantum coin the random eﬀect
corresponding to the loss of information occurs only during the measure-
ment. When observed the quantum probabilities correspond to the classical
probability theory. If not observed, a complex vector of a length one de-
scribes the state of a system. For two state the system is described buy two
complex amplitudes ω1, ω2 and the probability that the system is in one of
the two states is s |ω1|2 and |ω2|2 with |ω1|2 + |ω2|2 = 1. The product of
complex number with is conjugate is always a real number
ω∗· ω = (x −y · i) · (x + y · i) = x2 + y2 = |ω|2 = λ.
The quantum probabilities are also called von Neumann probabilities in
relation to the von Neumann entropy of a density matrix P
P = λ1 · |x1⟩⟨x1| + λ2 · |x2⟩⟨x2| + · · · + λn · |xn⟩⟨xn|
with the entropy of P
E(P) = −
n
X
i=1
(λi · log λi)
and with probabilities λi of the presence of a state. They all sum
1 =
n
X
i=1
λi
to one. Two equivalent states represent the same state when a measure-
ment is preformed, but they can have behave diﬀerently during the unitary
evolution. Two states |a⟩and |b⟩are equivalent if
|a⟩= ei·θ · |b⟩
(12.3)
with
ei·θ = cos θ + i · sin θ.
(12.4)
For example |a⟩and i · |a⟩are two equivalent states for θ = π/2. For θ = π
two equivalent states are |a⟩and −|a⟩. The value of θ correspond to an
angle. It can take inﬁnite many values corresponding a circle in the complex

Quantum Cognition
225
Im
Θ
sin Θ
cos Θ
"1.0
"0.5
0.5
1.0
"1.0
"0.5
0.5
1.0
Fig. 12.1
The value of θ correspond to an angle, can take inﬁnite many values describing
a circle in the complex number plane of the radius one described by the equation ei·θ =
cos θ + i · sin θ.
number plane. The radius is described by the equation ei·θ = cos θ+i·sin θ
(see Figure 12.1).
The circle can be extended to a sphere by adding additional third di-
mension that represents the state of one qubit. This sphere is called the
Bloch sphere, it represents the state as well as the amplitude of one qubit.
In the Bloch states |0⟩and |1⟩are represented by one dimension (see Figure
12.2).
Each proposition a is speciﬁed by a belief λ = P(a) with 0 ≤λ ≤1
and by the phase θ ∈[0, 2 · π). The state is either observable λ = P(a) or
unobservable. An unobservable state is in a superposition that is described
by the amplitude. The amplitude is the root of the belief multiplied with
the corresponding phase
√
λ · ei·θ.
(12.5)
As stated before the numerical degree of belief can result from either
from a frequentist approach or be determined from the nature of the uni-
verse, like for example the probability of throwing a six in a fair dice.
Alternatively it can be seen as a subjective viewpoint. On the other hand
it is diﬃcult to attribute any meaning to the phase speciﬁed by the angle
θ when a proposition with a known belief value a is not observable. The
unobservable propositions behave diﬀerently, the law of total probability is
not valid any more. For mutually exclusive events b1, ..., bn with
n
X
i=1
P(bi) = 1

226
Principles of Quantum Artiﬁcial Intelligence
!1.0
!0.5
0.0
0.5
1.0
!1.0
!0.5
0.0
0.5
1.0
!1.0
!0.5
0.0
0.5
1.0
Fig. 12.2
Bloch sphere represents the state as well as the amplitude of one qubit.
There are three axis, two represent its phase and one the two states |0⟩and |1⟩. The
representation is not faithful to reality: the states |0⟩and |1⟩are not orthogonal in the
Bloch sphere representation.
the law of total probability when events b1, ..., bn are not observable is not
valid,
P(a) ̸=

n
X
i=1
ei·θa|bi ·
p
P(a|bi) · ei·θbi ·
p
P(bi)

2
,
P(a) ̸=

n
X
i=1
ei·(θa|bi +θbi) ·
p
P(a|bi) ·
p
P(bi)

2
.
(12.6)
Humans when making decisions violates the law of total probability, yet
it can be explained as a quantum interference eﬀect in a manner similar to
the explanation for the results from two-hole experiments in physics.
12.2
Decision Making
Humans when making decisions violate the law of total probability. The
violation can be explained as a quantum interference resulting from the
phase represented by the angle θ. In an experiment, the violation of the
law of total probability was demonstrated by a categorization and decision
experiment [Busemeyer et al. (2009)]. 26 participants preformed 51 trials
per condition.
All together there were 26 · 51 = 1326 observation per
condition.
During the experiment the participants were shown pictures

Quantum Cognition
227
of faces. They should categorize the person represented on the pictures as
good or bad and in the next step decide to act friendly or aggressive. There
were two experimental conditions:
• Make a decision without reporting any categorization.
• Make a decision after categorizing a face.
In the second condition the conditional probabilities of categorization im-
ages of face were determined
P(good|face) = 0.17 P(bad|face) = 0.83
mostly the faces were categorized as being bad. Then the conditional prob-
abilities of acting friendly or aggressive were determined,
P(aggressive|good) = 0.42 P(friendly|good) = 0.58
and
P(aggressive|bad) = 0.63 P(friendly|bad) = 0.37.
We can represent the experiment by a simple graph with two nodes with
the following simpliﬁcation
P(good) = P(good|face) = 0.17
P(¬good) = P(bad|face) = 0.83
and
P(¬friendly|good) = 0.42 P(friendly|good) = 0.58
P(¬friendly|¬good) = 0.63 P(friendly|¬good) = 0.37,
see Figure 12.3.
For mutually exclusive events
P(good) + P(¬good) = 1
the law of total probability is
P(Friendly) =
X
good
P(Friendly|good) · P(good)
(12.7)
P(Friendly|Good) = P(good) · P(Friendly|good)+
+P(¬good) · P(Friendly|¬good)
(12.8)
P(friendly) = 0.58 · 0.17 + 0.37 · 0.83 = 0.4057,
(12.9)

228
Principles of Quantum Artiﬁcial Intelligence
Fig. 12.3
A graph with two nodes and the corresponding conditional probability tables.
Each row requires one number p for P (X) = true (P (X) = false is 1 −p).
P(¬friendly) = 0.42 · 0.17 + 0.63 · 0.83 = 0.5943.
(12.10)
In the ﬁrst condition of the experiment a decision without reporting any
categorization was determined. The value was
P(¬friendly) = P(aggressive|face) = 0.69.
The value is incompatible with the classical probability theory, it does not
correspond to the correct value as indicated by the law of total probability
0.69 >
X
good
P(¬friendly|good) · P(good) = 0.5943.
Why do human violate the law of total probability during decision making?
A possible explanation is given by the quantum probabilities. In quantum
probabilities the law of total probability is not valid
P(¬friendly) ̸=

2
X
i=1
ei·θfrendly|good ·
p
P(¬firendly|good) · ei·θgood ·
p
P(good)

2
.
(12.11)
We will indicate the solution for the Equation 12.11.
We simplify the
Equation 12.11 to the equivalent equation

2
X
i=1
ei·θαi · √αi · ei·θβi ·
p
βi

2
=
(12.12)
ei·θα1 · √α1 · ei·θβ1 ·
p
β1 + ei·θα2 · √α2 · ei·θβ2 ·
p
β2

2
=

Quantum Cognition
229
with the complex conjugate representation

ei·θα1 · √α1 · ei·θβ1 ·
p
β1 + ei·θα2 · √α2 · ei·θβ2 ·
p
β2

·

ei·θα1 · √α1 · ei·θβ1 ·
p
β1 + ei·θα2 · √α2 · ei·θβ2 ·
p
β2
∗
=
(12.13)

ei·θα1 · √α1 · ei·θβ1 ·
p
β1 + ei·θα2 · √α2 · ei·θβ2 ·
p
β2

·

e−i·θα1 · √α1 · e−i·θβ1 ·
p
β1 + e−i·θα2 · √α2 · e−i·θβ2 ·
p
β2

=
(12.14)
α1 · β1 + α2 · β2 + √α1 · √α2 ·
p
β1 ·
p
β2·
(ei·(θα1 −θα2+θβ1−θβ2) + e−i·(θα1−θα2+θβ1−θβ2)) =
α1·β1 +α2·β2+√α1 ·√α2 ·
p
β1·
p
β2 ·2·cos(θα1 −θα2 +θβ1 −θβ2) (12.15)
with
θ := θα1 −θα2 + θβ1 −θβ2
we can simplify to
α1 · β1 + α2 · β2 + 2 · √α1 · √α2 ·
p
β1 ·
p
β2 · cos(θ).
(12.16)
It should be noted that
−1 ≤cos(θ) ≤1
(12.17)
and
α1 · β1 + α2 · β
2
≥√α1 · √α2 ·
p
β1 ·
p
β2.
(12.18)
If
0 ≤α1 · β1 + α2 · β ≤1
(12.19)
then
0 ≤α1 · β1 + α2 · β2 + 2 · √α1 · √α2 ·
p
β1 ·
p
β2 · cos(θ) ≤2.
(12.20)
in the case the value of a query variable has to be determined and some
variables are unknown, the probabilities are determined by normalization
as described in the section about Bayesian networks.

230
Principles of Quantum Artiﬁcial Intelligence
The solution for the Equation 12.11 is

2
X
i=1
ei·θfrendly|good ·
p
P(¬firendly|good) · ei·θgood ·
p
P(good)

2
=
P(¬firendly|good) · P(good) + P(¬firendly|¬good) · P(¬good)+
2 ·
p
P(¬firendly|good) ·
p
P(¬firendly|¬good)
·
p
P(good) ·
p
P(¬good) · cos(θ).
(12.21)
The Equation 12.2 is the quantum interpretation of probability. Quantum
probabilities obey the law of total probability only in the case
cos(θ) = 0,
θ = π
2
in which the interference part is canceled out. In our example with
θ = θ¬friendly|good −θ¬friendly|¬good + θgood −θ¬good
the quantum probabilities obey the law of total probability in the case
π
2 = θ¬friendly|good −θ¬friendly|¬good + θgood −θ¬good.
The quantum interpretation of probability can explain the incompatibil-
ity with the classical probability theory by the Equation 12.2. The value
P(¬friendly) for a decision without reporting any categorization was 0.69.
By Equation 12.2 we can determine the corresponding θ. We add the clas-
sical probability 0.5943 the interference term
0.69 = 0.42·0.17+0.63·0.83+2·
√
0.42·
√
0.63·
√
0.17·
√
0.83·cos(θ) (12.22)
and the value of cos(θ) is
0.247642 = 0.69. −0.5943
0.386446
= cos(θ)
with
θ = 1.32055 ≈0.42 · π = 21
50 · π.

Quantum Cognition
231
!6
!4
!2
2
4
6
!1.0
!0.5
0.5
1.0
Fig. 12.4
The relation between cos(θ) and θ for −2 · π ≥θ ≥2 · π.
12.2.1
Interference
During hidden inference the interference plays an important part in the
quantum interpretation of probability. The interference is determined by
the value of cos(θ).
• For cos(θ) = 0 no interference is present.
• For 1 ≥cos(θ) > 0 positive interference is present.
• For −1 ≤cos(θ) < 0 negative interference is present.
The relation between cos(θ) and θ is shown in Figure 12.4 for −2 · π ≥θ ≥
2 · π. In general
θ := θα1 −θα2 + θβ1 −θβ2
and in our example
θ = θ¬friendly|good −θ¬friendly|¬good + θgood −θ¬good.
The interference depends on the phase of each possible state and the cor-
responding global θ.
• No interference is present in the case
θ = −3 · π
2 ,
θ = −π
2 ,
θ = π
2 ,
θ = 3 · π
2 .
• Positive interference is present for
θ ≤−3 · π
2 ,
−π
2 ≤θ ≤π
2 ,
3 · π
2
≤θ.
Maximal positive interference is present if the phase of each possible
state is equal
θα1 = θα2 = θβ1 = θβ2.

232
Principles of Quantum Artiﬁcial Intelligence
• Negative interference is present for
−3 · π
2
≤θ ≤−π
2 ,
π
2 ≤θ ≤3 · π
2 .
When performing hidden inference an additional free variable for each state
is present according to the quantum interpretation of probability. The free
variable corresponds to the phase speciﬁed by the angle θ. The probabil-
ities are determined by normalization as introduced in the section about
Bayesian networks. We presented an experiment in which positive interfer-
ence is present. In the next section we present the unpacking eﬀect and the
resulting negative interference.
12.3
Unpacking Eﬀects
An event can be described in more or less detail. The unpacking eﬀect is
present when the whole is less than the sum of its parts [Boven and Epley
(2003)]. With more details the unpacking eﬀects appear. In the experiment
by two conditions were present [Busemeyer et al. (2011)], [Busemeyer and
Bruza (2012)].
• What is the probability that some one died from natural causes? This
is the packed condition.
• What is the probability that some one died from cancer? After this
what is the probability that some one died from natural causes other
than cancer?
see Figure 12.5 for the graph representation.
According to classical
probability theory
Fig. 12.5
A graph with two nodes.

Quantum Cognition
233
P(Naturally) =
X
cancer
P(Naturally|cancer) · P(cancer).
(12.23)
It follows that
P(naturally) = P(naturally|cancer) · P(cancer)+
+P(naturally|¬cancer) · P(¬cancer).
(12.24)
However for packed condition
P(naturally) < P(naturally|cancer) · P(cancer)+
+P(naturally|¬cancer) · P(¬cancer).
(12.25)
According to the quantum interpretation of probability

2
X
i=1
ei·θnaturally|cancer ·
p
P(¬naturally|cancer) · ei·θcancer ·
p
P(cancer)

2
= P(¬naturally|cancer) · P(cancer) + P(¬naturally|¬cancer)·
·P(¬cancer) + 2 ·
p
P(¬naturally|cancer ·
p
P(¬naturally|¬cancer)
·
p
P(cancer) ·
p
P(¬cancer) · cos(θ).
(12.26)
with
cos(θ) < 0.
12.4
Conclusion
Quantum cognition uses mathematical quantum theory to model cognitive
phenomena. It is assumed that the computation itself is performed on a
classical computer and not on a quantum computer. The brain is considered
a classical computer in a quantum world. Because the wave function can
be present at the macro scale of our daily life, predictions such as, hidden
inferences, are based on von Neumann probabilities. Besides probability
judgment, other eﬀects, such as emotional judgments or order eﬀects, can
be modeled by quantum probabilities and unitary evolution. It is assumed
that evolution adapted to the quantum world on the macro scale. Accord-
ing to the quantum cognition assumption, humans violate the law of total
probabilities because, under certain conditions, this law is not valid in the
real world.

This page intentionally left blank
This page intentionally left blank

Chapter 13
Related Approaches
13.1
Quantum Walk
Related to Grover’s algorithm and the quantum tree search algorithm is
the quantum random walk on a graph ([Ambainis (2003)], [Kempe (2003)]
and [Ambainis (2004)]). Indeed, Grover’s algorithm can also be viewed as
a quantum walk algorithm. A quantum random walk is an analog to the
random walk. There are two types of random walk, namely, discrete- and
continuous-time. We describe the discrete-time random walk on a lattice
and the quantum random walk on a graph G = (V, E).
13.1.1
Random walk
Randomness could be nature’s way to avoid complexity when accomplish-
ing certain tasks. A ﬂy could choose the direction of its ﬂight randomly.
The insect ﬂies for a time in a randomly chosen direction. Then, it ran-
domly chooses another direction, and then, another random direction is
chosen, and so forth. This process is an example of a random walk in three
dimensions. We simplify the model by describing it as a discrete random
walk on a three-dimensional lattice [Gaylord and Wellin (1995)]. At each
point, the ﬂy can choose to ﬂy in six directions: up, down, north, south,
west or east. Each random choice is made after a constant amount of time;
the constant ﬂight path intervals are of equal length. In Figure 13.1, we see
a simulation of a lattice random walk in dimension three for 1000 steps. A
two-dimensional lattice walk is represented in Figure 13.2.
13.1.2
Quantum insect
In classical physics, randomness is not present. Suppose that a ﬂy gen-
erates randomness by some quantum computing device that simulates the
235

236
Principles of Quantum Artiﬁcial Intelligence
Fig. 13.1
Simulation of a lattice random walk in dimension three for 1000 steps.
Fig. 13.2
Simulation of a lattice random walk in dimension two for 100 steps.
behavior of a quantum coin. In this case, a true random walk is present.
Suppose that the ﬂy is in a closed room and we do not know its position.
The ﬂy is evolving into a superposition of several states that can be char-
acterized by its position. Each of the possible positions is associated with
a speciﬁc probability.
The “quantum ﬂy” is in a mixed state.
As long

Related Approaches
237
as we make no measurements, there are no random eﬀects. The behavior
of the system is strictly deterministic. The probability distribution repre-
sents the possible positions after several steps, and several measurements
of the quantum ﬂy and the classical ﬂy will diﬀer. This diﬀerence arises
because the randomness in the quantum walk is present only during the
measurement.
13.1.3
Quantum walk on a graph
The discrete-time quantum random walk on a graph G = (V, E) can be
described by a unitary operator U on Hilbert space
H = HS ⊗HC.
HS represents the vertex of the graph and HC describes the destination
choice. The destination choice HC is also called “coin space” [Shenvi et al.
(2003)] because the choice in one dimension can be described by a quantum
coin. U is composed of the operators S and C [Aharonov et al. (2001)],
U = S · C.
A step of the quantum walk is represented by two operations [Childs (2011)]:
• Build a superposition over the neighbor states by operator C;
• Move the state to the new target destination by operator S.
A register represents a state
|jk⟩= |j⟩|k⟩⇐⇒(j, k) ∈E.
The neighbors of j are represented in the register |k⟩by the by operator C
[Watrous (1998)], [Tarrataca and Wichert (2013a)],
C · |j⟩|k⟩= |j⟩
1
p
deg(j)
X
w:(j,w)∈E
|k ⊕w⟩
(13.1)
with deg(j) represents the degree of vertex j and P
m:(j,m)∈E
1
deg(j) = 1.
After applying the C operator the state of the system is moved from state
|j⟩to state |k⟩for example by a simple permutation
S · |j⟩|k⟩= |k⟩|j⟩
(13.2)
with the eﬀect that the quantum random walk takes places on the edges of
the graph.

238
Principles of Quantum Artiﬁcial Intelligence
13.1.4
Quantum walk on one dimensional lattice
The most simple graph is one dimensional lattice. The quantum walk is
called the discrete walk on a line. A state is represented by a register
|n⟩|0⟩,
|n⟩|1⟩
with n ∈Z being an integer representing the position on the line and |0⟩
and |1⟩being the state of the system.
A step of the quantum walk is
represented by two operations C and S [Ambainis (2003)]. The Hadamard
coin is represented by the following unitary matrix
C := W1 =
 
1
√
2
1
√
2
1
√
2 −1
√
2
!
=
1
√
2 ·
1 1
1 −1

.
(13.3)
The unitary S operator is deﬁned as
S · |n⟩|0⟩= |n −1⟩|0⟩,
S · |n⟩|1⟩= |n + 1⟩|1⟩.
(13.4)
Suppose we start in location |0⟩and the state |0⟩.
S · C · |0⟩|0⟩= U · |0⟩|0⟩= | −1⟩|0⟩+ |1⟩|1⟩
√
2
(13.5)
after one step the result is similar to classical random walk. After two steps
U · U · |0⟩|0⟩= | −2⟩|0⟩+ |0⟩1⟩+ |0⟩|0⟩−|2⟩|1⟩
2
(13.6)
the probability of n = 1 is zero contrary to classical random walk. After
three steps
U ·U ·U ·|0⟩|0⟩= | −3⟩|0⟩+ | −1⟩|1⟩+ 2 · | −1⟩|0⟩−|1⟩|0⟩+ |3⟩|1⟩
2 ·
√
2
(13.7)
the distribution is biased towards left because of the non-symmetric coin
operator W1. A symmetric unitary coin operator [Ambainis (2003)] would
be represented by
C :=
1
√
2 ·
1 i
i 1

.
(13.8)
In Figure 13.3 we see the comparison between the distribution of the classi-
cal random walk on a one dimensional lattice line and the quantum random
walk with a symmetric coin operator. The quantum random walk propa-
gates quadratically faster to the edges of the graph.

Related Approaches
239
Fig. 13.3
Comparison between the distribution of the classical random walk on a one
dimensional lattice and the quantum random walk with a symmetric coin operator after
50 steps [Hogg (2008)]. The walk starts at the origin. The classical walk has a peak near
the origin, it corresponds to Gaussian distribution. The probability distribution for the
quantum walk is approximately uniform near the origin and maximal near the edges of
the graph.
13.1.5
Quantum walk and search
In quantum random walk the goal states are marked through an oracle
operator as in the Grover’s algorithm [Shenvi et al. (2003)]. To obtain for
goal states with n = |V | the complexity is O(√n)
With the projection matrix Pm is
Pm = |x⟩⟨x| =





1
n
1
n · · ·
1
n
1
n
1
n · · ·
1
n
... ... ... ...
1
n
1
n · · ·
1
n





(13.9)
we get Grover’s ampliﬁcation also called Grover’s diﬀusion operator.
Gm = 2 · Pm −Im.
(13.10)
One can redeﬁne the coin operator in order to perform Grover’s diﬀusion
operator [Moore and Russell (2001)]. The reformulated coin operator C is
with m := deg(j)
C · |j⟩|k⟩= |j⟩·

2
√m ·
X
w:(j,w)∈E
|k ⊕w⟩⟨k| −Im

.
(13.11)
13.1.6
Quantum walk for formula evaluation
Quantum random walk can determine the properties of a graph by generat-
ing diﬀerent distributions. Quantum walk algorithms for Boolean formula
evaluation can evaluate a Boolean formula quadratically faster than any

240
Principles of Quantum Artiﬁcial Intelligence
known classical algorithm. For a Boolean formula f(x) to the input string
x1, ¬x1, · · · , xm, ¬xm. Boolean formula is f(x) = 1 otherwise f(x) = 0
[Farhi and Gutmann (1998)], [Childs et al. (2007)], [Ambainis (2007)], [Am-
bainis et al. (2007)]. However one should not neglect the cost of building
the coin operators that represent the instanced formula. For example to
speed up alpha-beta search by evaluating AND-OR formulas as used in
games [Farhi et al. (2008)], [Cleve et al. (2008)], the instantiationed graph
has to be determined dynamically during the performed search.
“We conclude by mentioning some open problems. Our algorithm needs
to know the full structure of the formula beforehand to determine the coin’s
bias at each internal vertex...”, cited from [Ambainis et al. (2007)].
13.2
Adiabatic Computation
Adiabatic quantum computation is an alternative approach to quantum
computation. It is based on time evolution of a quantum system. The en-
ergy of a system can be described by a function [Farhi et al. (2000)], [Farhi
et al. (2009)].
The lowest energy that the system can assume is called the ground state and
corresponds to the global minima of the function. If the energy is dependent
on two variables it can be represented by a two dimensional function (see
Figure 13.4). A ball that is left on the slope will descend until it reaches
the lowest point in the valley and will stay there. The corresponding point
is stationary. If the function describing the energy of the system is changed
very slowly the ball will be resting in a location which is the minimum point
of the new equation. If one starts with a solution to the ﬁrst simple func-
tion one will end up the process with a solution to a complicated function.
The Hamiltonian H, which is the operator that is responsible for the time
evolution of the state vector x(t). In quantum physics H is represented by
the Schr¨odinger equation. The equation describes a linear superposition of
diﬀerent states at time t represented by the vector x(t)
i · h · d
dtx(t) = H · x(t)
(13.12)
with i = √−1 and h being the Planck’s constant. The Hamiltonian opera-
tor H is related to the total energy of the system. The initial Hamiltonian
Hinit is in the ground state that corresponds to the solution of the initial
simple problem. Then the initial Hamiltonian is changed very slowly until

Related Approaches
241
!100
!50
0
50
100
!100
!50
0
50
100
0
5000
10000
15000
20000
Fig. 13.4
The energy of a system can be described by a function. The lowest energy that
the system can assume is called the ground state and corresponds to the global minima
of the function. If the energy is dependent on two variables it can be represented by a
two dimensional function.
it becomes the ﬁnal Hamiltonian Hfinal.
Hiniu
slowly
−→Hfinal.
(13.13)
As the Hamiltonian is slowly changed multiple qubits are close to a point
representing to the ground state.
Adding a slight amount of energy by
slowly changing the Hamiltonian keeps the system in the ground state. If we
change the Hamiltonian to fast the system could go out of the ground state.
It is not clear if the adiabatic computation is more or less eﬃcient than a
computation on a classical computer.
For some problems, its eﬃciency
could be better than the eﬃciency of classical computers.
13.2.1
Quantum annealing
Quantum annealing is a method for ﬁnding the global minimum of a func-
tion [Brooke et al. (1999)], [Johnson et al. (2011)]. A minimum of a function
can be determined by gradient descent. Gradient descent starts at a ran-
dom point of the function and moves down in the direction of steepest
descent. It continues until it cannot proceed downward anymore. Often,
a local (not necessarily global) minimum is found.
Gradient descent is
the basis of many learning and optimization algorithms, such as the back-
propagation algorithm [Hertz et al. (1991)]. Quantum annealing attempts
to avoid local minima by means of a quantum ﬂuctuation parameter that

242
Principles of Quantum Artiﬁcial Intelligence
replaces a state by a randomly selected neighboring state. In simulated an-
nealing [Hertz et al. (1991)], the temperature plays a role that is related to
quantum ﬂuctuation. In simulated annealing, the neighborhood stays the
same throughout the search. The temperature determines the probability
of moving out of a local minimum. At the beginning, the temperature is
high, and the probability of moving out of a minimum is high. Then, it is
slowly reduced until the probability of moving out of a minimum is zero.
In quantum annealing, the quantum ﬂuctuation parameter replaces a
local minimum state with a randomly selected neighboring state in some
ﬁxed radius. The neighborhood extends over the whole search space at the
beginning, and then, it is slowly reduced until the neighborhood shrinks to
those few states that diﬀer minimally from the current states. In a quantum
system, the quantum ﬂuctuation can be performed directly by an adiabatic
process rather than needing to be simulated. These processes are based on
quantum tunneling. The Heisenberg’s uncertainty principle is given by
∆(G)∆(K) ≥|⟨x|[G, K] · x⟩|
2
.
(13.14)
The principle is applied to the momentum and location of moving particles
and is represented by
∆(x)∆(p) ≥h
2
(13.15)
where x is the position and p the momentum of a particle and h the Planck
constant. It represents the relation between
∆(x)∆(p) ≈h
(13.16)
the position uncertainty times the momentum uncertainty. This relation is
also valid for energy E and time t
∆(E)∆(t) ≈h.
(13.17)
This arrangement contradicts the ﬁrst law of thermodynamics, which is the
conservation of energy, where the sum of the amount of energy of a system
remains constant. In quantum physics, there is uncertainty between the
energy E and the time t.
∆(E) ≈
h
∆(t).
(13.18)
This uncertainty means that some energy can be borrowed, to overcome
some mountain and go out of a minimum as long as we repay it in the time
interval [Hey and Walters (2003)]
∆(t) ≈
h
∆(E).
(13.19)

Related Approaches
243
Quantum tunneling is based on the Heisenberg uncertainty principle, as
shown, and the wave-particle duality of matter represented by the wave
propagation.
Quantum annealing can speed up some machine learning tasks that are
based on a gradient descent method, such as the back-propagation algo-
rithm that is used in artiﬁcial neural networks. It is an alternative to the
simulated annealing that is used in the learning and optimization tasks.
Adiabatic quantum computers based on quantum annealing do not corre-
spond to a universal Turing machine. Rather, they are related to analog
computers.
13.3
Quantum Neural Computation
Neuroimaging indicates how information processing is implemented in the
brain and when speciﬁc structures and processes are invoked. For example,
fMRI measures local properties of the cerebral blood ﬂow and is based on
blood oxygen level dependence. Changes of activity associated with various
stimulus conditions are correlated with brain activity. However: “It is un-
clear that we will come to a better understanding of mental processes simply
by observing which neural loci are active while subjects perform a task ”,
[Kosslyn (1999)]. In this section, we will describe the relationship between
three quantum computation principles that speed up the computation and
the human brain.
Human vision is based on information integration and is non-reversible.
Hubel and Wiesel’s discoveries provided a large amount of inﬂuence on the
ways that neuroscientists think about the brain [Hubel (1988)]. They have
inspired several models for pattern recognition. In these models, the neu-
ral units have a local view, unlike the common fully-connected networks
[Fukushima (1980)], [Wichert (1993)], [Riesenhuber and Poggio (1999)],
[Fukushima (1989)], [Cardoso and Wichert (2010)]. They gradually reduce
the information from the input layer through the output layer. This task
is accomplished by integrating local features into more global features in
sequential transformations. Its purpose is to classify topological data by
gradually reducing the information from the input layer through the output
layer. Each of these transformations is composed of two diﬀerent steps. The
ﬁrst step reduces the information by representing it with previously learned
templates. The second step blurs the information to allow positional shifts,

244
Principles of Quantum Artiﬁcial Intelligence
Fig. 13.5
Changes of activity associated with various stimulus conditions are corre-
lated with the brain activity. A cluster indicates a brain activity during an experiment
[Wichert et al. (2002)].
giving the model some invariance under shifts and distortions. A quantum
neural model should perform a reversible computation. Neurons do not
perform a reversible computation, neither do neural networks. Associative
memory, as introduced in this book, is non-reversible.
A quantum associative memory should be represented by an operator W
and the Grover’s algorithm to recall the patterns. For a question pattern,
a superposition of all possible stored answer patterns would be generated
by the operator W, which acts as an oracle [Tay et al. (2010)]. The op-
erator W indicates the solution. In the next step, the Grover’s algorithm
performs the phase ampliﬁcation. Quantum computation is based on two
principles that are the basis of quantum algorithms. The resulting quantum
algorithms are capable of performing a quadratic computation faster than
a classical algorithm. The corresponding principles can be integrated into
the uniﬁed theories of human cognition, such as SOAR.
A non-computable process involves the possibility of generating true ran-
domness [Penrose (1991)]. Classical physics leads to the paradox of free
will in a deterministic world. The question concerning randomness is highly
metaphysical. In classical physics, randomness does not exist; in quantum
physics, it cannot be explained. Besides randomness, quantum computation
cannot oﬀer any non-computable schema such as that claimed by the quan-
tum mind or the quantum consciousness hypothesis [Nunn et al. (1994)].

Related Approaches
245
On the other hand, quantum cognition assumes that the brain is a classical
computer in a quantum world. Because a wave function can be present at
the macro scale, predictions such as the hidden inference are based on von
Neumann probabilities and not on classical probability theory.
13.4
Epilogue
Quantum computation is based on two principles to speed up the compu-
tation:
• The QFT can determine the period of a wave.
• Grover’s algorithm can speed up the search quadratically for a given
number of possible solutions.
These two principles can be combined together in a quantum counting al-
gorithm to estimate the number of possible solutions. It appears that in
some domains of artiﬁcial intelligence, such as neural associative memories,
diagnostic reasoning or sub-symbolic problem solving quantum algorithms
other than quantum annealing are less useful. However, a symbolical ar-
tiﬁcial intelligence framework allows an elegant description of a possible
universal quantum computer model that is capable of faster execution of
programs.
The geometrical structure of the world can be used to speed up problem
solving. This structure is present in simple Euclidian geometry as expe-
rienced by humans. It is also present in the geometrical structure of the
world, as described by the quantum physics results with Grover’s algorithm,
which is based on the Householder reﬂection.

This page intentionally left blank
This page intentionally left blank

Bibliography
Aharonov, D. (1999). Noisy Quantum Computation, Ph.D. thesis, Hebrew Uni-
versity.
Aharonov, D., Ambainis, A., Kempe, J. and Vazirani, U. (2001). Quantum walks
on graphs, in Proceedings of ACM Symposium on Theory of Computation
(STOC’01), pp. 50–59.
Aikins, J. (1986). Prototypical knowledge for expert systems, Artiﬁcial Intelli-
gence 20, pp. 163–210.
Ambainis, A. (2003). Quantum walks and their algorithmic applications, Inter-
national Journal of Quantum Information 1, p. 507, URL http://www.
citebase.org/abstract?id=oai:arXiv.org:quant-ph/0403120.
Ambainis, A. (2004). Quantum search algorithms, SIGACT News 35, 2, pp. 22–
35, doi:http://doi.acm.org/10.1145/992287.992296.
Ambainis, A. (2007). A nearly optimal discrete query quantum algorithm for
evaluating NAND formulas, ArXiv e-prints .
Ambainis, A., Childs, A. and Reichardt, B. (2007). Any and-or formula of size n
can be evaluated in time n
1
2 +o(1) on a quantum computer, in Foundations
of Computer Science, 2007. FOCS ’07. 48th Annual IEEE Symposium on,
pp. 363 –372, doi:10.1109/FOCS.2007.57.
Amico, L., Fazio, R., Osterloh, A. and Vedral, V. (2008). Entanglement in many-
body systems, Reviews of Modern Physics 80, 2, pp. 517–576.
Anderson, J. (1983). The Architecture of Cognition (Harvard University Press).
Anderson, J. A. (1995a). An Introduction to Neural Networks (The MIT Press).
Anderson, J. R. (1995b). Cognitive Psychology and its Implications, 4th edn. (W.
H. Freeman and Company).
Aspray, W. (1990). John von Neumann and the origins of modern computing,
History of computing (MIT Press), ISBN 9780262011211.
Ballard, D. H. (1997). An Introduction to Natural Computation (The MIT Press).
Bennett, C. (1973). Logical reversibility of computation, IBM Journal of Research
and Development 17, pp. 525–532.
Bennett, C. H. (1982). The thermodynamics of computation–a review, Interna-
tional Journal of Theoretical Physics 21, 12, pp. 905–940.
Bennett, C. H. (1988). Notes on the history of reversible computation, IBM J.
Res. Dev. 32, 1, pp. 16–23.
247

248
Principles of Quantum Artiﬁcial Intelligence
Bennett, C. H. (1989). Time/space trade-oﬀs for reversible computation, SIAM
Journal on Computing 18, 4, pp. 766–776, doi:10.1137/0218053, URL
http://link.aip.org/link/?SMJ/18/766/1.
Bennett, C. H. (2003). Notes on landauer’s principle, reversible computation,
and maxwell’s demon, Studies In History and Philosophy of Science Part
B: Studies In History and Philosophy of Modern Physics 34, 3, pp. 501 –
510, doi:DOI:10.1016/S1355-2198(03)00039-X.
Bennett, C. H., Bernstein, E., Brassard, G. and Vazirani, U. (1997). Strengths
and weaknesses of quantum computing, URL http://www.citebase.org/
abstract?id=oai:arXiv.org:quant-ph/9701001.
Bennett, C. H., Brassard, G., Cr´epeau, C., Jozsa, R., Peres, A. and Wootters,
W. K. (1993). Teleporting an unknown quantum state via dual classical and
einstein-podolsky-rosen channels, Phys. Rev. Lett. 70, 13, pp. 1895–1899,
doi:10.1103/PhysRevLett.70.1895.
Bentz, H. J., Hagstroem, M. and Palm, G. (1989). Information storage and eﬀec-
tive data retrieval in sparse matrices, Neural Networks 2, 4, pp. 289–293.
Bernstein, E. and Vazirani, U. (1993). Quantum complexity theory, in STOC
’93: Proceedings of the twenty-ﬁfth annual ACM symposium on Theory of
computing (ACM, New York, NY, USA), ISBN 0-89791-591-7, pp. 11–20,
doi:http://doi.acm.org/10.1145/167088.167097.
Biederman, I. and Ju, G. (1988). Surface vs. edge-based determinants of visual
recognition, Cognitive Psychology 20, pp. 38–6.
Boltzman, L. (1995). Lectures on Gas Theory (Dover Books on Physics).
Boven, L. V. and Epley, N. (2003). The unpacking eﬀect in evaluative judgments:
When the whole is less than the sum of its parts, Journal of Experimental
Social Psychology 39, pp. 263–269.
Boyer, M., Brassard, G., Hoeyer, P. and Tapp, A. (1998). Tight bounds on
quantum searching, Fortschritte der Physik 46, p. 493, URL http://www.
citebase.org/abstract?id=oai:arXiv.org:quant-ph/9605034.
Brassard, G., Hoyer, P., Mosca, M. and Tapp, A. (2000). Quantum Amplitude
Ampliﬁcation and Estimation, eprint arXiv:quant-ph/0005055 .
Brassard, G., Hoyer, P. and Tapp, A. (1998). Quantum counting, URL http:
//www.citebase.org/abstract?id=oai:arXiv.org:quant-ph/9805082.
Brooke, J., Bitko, D., Rosenbaum, T. and Aeppli, G. (1999). Quantum annealing
of a disordered magnet, Science 284, 5415, pp. 779–781.
Brownston, L., Farell, R., Kant, E. and Martin, N. (1985). Programming Expert
Systems in OPS5: An Introduction to Rule-Based Programming (Addison-
Wesley).
Brunak, S. and Lautrup, B. (1990). Neural Networks Computers with Intuition
(World Scientiﬁc).
Busemeyer, J. R. and Bruza, P. D. (2012). Quantum Models of Cognition and
Decision (Cambridge University Press).
Busemeyer, J. R., R., E. M. P., Franco and Trueblood, J. S. (2011). A quan-
tum theoretical explanation for probability judgment errors, Psychological
Review 118, 2, pp. 193–218.
Busemeyer, J. R. and Trueblood, J. (2009). Comparison of quantum and bayesian
inference models, in P. Bruza, D. Sofge, W. Lawless, K. van Rijsbergen

Bibliography
249
and M. Klusch (eds.), Quantum Interaction, Lecture Notes in Computer
Science, Vol. 5494 (Springer Berlin / Heidelberg), pp. 29–43, URL http:
//dx.doi.org/10.1007/978-3-642-00834-4-5.
Busemeyer, J. R., Wang, Z. and Lambert-Mogiliansky, A. (2009). Empirical com-
parison of markov and quantum models of decision making, Journal of
Mathematical Psychology 53, 5, pp. 423 – 433, doi:DOI:10.1016/j.jmp.2009.
03.002.
Busemeyer, J. R., Wang, Z. and Townsend, J. T. (2006). Quantum dynamics of
human decision-making, Journal of Mathematical Psychology 50, 3, pp. 220
– 241, doi:DOI:10.1016/j.jmp.2006.01.003.
Byrne, P. (2007). The many worlds of hugh everett, Scientiﬁc American Magazine
, pp. 98–105.
Cardoso, A. and Wichert, A. (2010). Neocognitron and the map transformation
cascade, Neural Networks 23, 1, pp. 74–88.
Childs, A. M. (2011). LECTURE 14: Discrete-time quantum walk, University
of Waterloo, URL http://www.math.uwaterloo.ca/~amchilds/teaching/
w11/l14.pdf.
Childs, A. M., Reichardt, B. W., Spalek, R. and Zhang, S. (2007). Every NAND
formula of size N can be evaluated in time N
1
2 +o(1) on a quantum computer,
eprint arXiv:quant-ph/0703015 .
Church, A. (1936a). A note on the entscheidungsproblem, Journal of Symbolic
Logic 1, 1, pp. 40–41.
Church, A. (1936b). An unsolvable problem of elementary number theory, Amer-
ican Journal of Mathematics 58, 2, pp. 345–363.
Church, A. (1941). The Calculi of Lambda-Conversion, Annals of Mathematics
Studies (Princeton University Press, Princeton, New Jersey, USA).
Churchland, P. S. and Sejnowski, T. J. (1994). The Computational Brain (The
MIT Press).
Cleve, R., Gavinsky, D. and Yonge-Mallo, D. (2008). Quantum algorithms for
evaluating min-max trees, in Y. Kawano and M. Mosca (eds.), Proceedings
of Theory of Quantum Computation, Communication, and Cryptography
(TQC 2008), Vol. 5106 (Springer Berlin / Heidelberg), pp. 11–15.
Cooper, G. and Herskovits, E. (1990). Determination of the entropy of a belief
network is np-hard, Tech. rep., Medical Computer Science, Stanford Uni-
versity.
Cormen, T. H., Leiserson, C. E., Rivest, R. L. and Stein, C. (2001). Introduction
to Algorithms, 2/e (MIT Press).
Deutsch, D. (1985). Quantum theory, the church-turing principle and the uni-
versal quantum computer, in Proceedings of the Royal Society of London-
Series A, Mathematical and Physical Sciences, Vol. 400, pp. 97–117.
Deutsch, D. (1997). The Fabric of Reality (Penguin Group).
Deutsch, D. and Jozsa, R. (1992). Rapid Solution of Problems by Quantum Com-
putation, Royal Society of London Proceedings Series A 439, pp. 553–558.
Everett, H. (1959). “relative state” formulation of quantum mechanics, Reviews
of Modern Physics 29, pp. 454–462.

250
Principles of Quantum Artiﬁcial Intelligence
Farhi,
E.,
Goldstone,
J.,
Gosset,
D.,
Gutmann,
S.,
Meyer,
H. B. and
Shor, P. (2009). Quantum adiabatic algorithms, small gaps, and diﬀer-
ent paths, URL http://www.citebase.org/abstract?id=oai:arXiv.org:
0909.4766.
Farhi, E., Goldstone, J. and Gutmann, S. (2008). A quantum algorithm for the
hamiltonian nand tree, Theory of Computing 4, 1, pp. 169–190, doi:10.
4086/toc.2008.v004a008.
Farhi, E., Goldstone, J., Gutmann, S. and Sipser, M. (2000). Quantum Compu-
tation by Adiabatic Evolution, ArXiv Quantum Physics e-prints .
Farhi, E. and Gutmann, S. (1998). Quantum computation and decision trees,
Phys. Rev. A 58, 2, pp. 915–928, doi:10.1103/PhysRevA.58.915.
Feldman, J. (1985). Four frames suﬃce: A provisional model of vision and space,
Behavioral and Brain Sciences 8, pp. 265–289.
Fikes, R. E. and Nilsson, N. J. (1971). Strips: A new approach to the application
of theorem proving, Artiﬁcial Intelligence 2.
Forgy, C. (1981). Ops5 user’s manual cmu-cs-81-135, Tech. rep., Computer
Science Department, Carnegie-Mellon University, Pittsburgh, Pensilvania
USA.
Franklin, S. (1997). Artiﬁcial Minds (MIT Press).
Freeman, J. A. (1994). Simulating Neural Networks with Mathematica (Addison-
Wesley).
Fukushima, K. (1980). Neocognitron: a self organizing neural network model for
a mechanism of pattern recognition unaﬀected by shift in position. Biol
Cybern 36, 4, pp. 193–202.
Fukushima, K. (1989). Analisys of the process of visual pattern recognition by
the neocognitron, Neural Networks 2, pp. 413–420.
Fuster, J. (1995). Memory in the Cerebral Cortex (The MIT Press).
Gardner, M. (1979). Mathematical games: Mathematical games: The random
number omega bids fair to hold the mzsteries of the universe, Scientiﬁc
American , pp. 20–30.
Gaylord, R. J. and Wellin, P. R. (1995). Computer Simulations with Mathematica
(Spriner Verlag).
Ghosh, S., Rosenbaum, T. F., Aeppli, G. and Coppersmith, S. N. (2003). Entan-
gled quantum state of magnetic dipoles, Nature 425, pp. 48–51.
Gilovich, T. (1999). Tversky, in The MIT Encyclopedia of the Cognitive Sciences
(The MIT Press), pp. 849–850.
Givan, R. and Dean, T. (1997). Model minimization, regression, and proposi-
tional strips planning, in 15th International Joint Conference on Artiﬁcial
Intelligence, pp. 1163–8.
Goldstone, R. (1999). Similarity, in The MIT Encyclopedia of the Cognitive Sci-
ences (The MIT Press), pp. 763–765.
Grover, L. K. (1996). A fast quantum mechanical algorithm for database search,
in STOC ’96: Proceedings of the twenty-eighth annual ACM symposium on
Theory of computing (ACM, New York, NY, USA), ISBN 0-89791-785-5,
pp. 212–219, doi:http://doi.acm.org/10.1145/237814.237866.

Bibliography
251
Grover, L. K. (1997). Quantum mechanics helps in searching for a needle
in a haystack, Physical Review Letters 79, p. 325, URL doi:10.1103/
PhysRevLett.79.325.
Grover, L. K. (1998a). A framework for fast quantum mechanical algorithms, in
STOC ’98: Proceedings of the thirtieth annual ACM symposium on Theory
of computing (ACM, New York, NY, USA), ISBN 0-89791-962-9, pp. 53–62,
doi:http://doi.acm.org/10.1145/276698.276712.
Grover, L. K. (1998b). Quantum computers can search rapidly by using almost
any transformation, Phys. Rev. Lett. 80, 19, pp. 4329–4332, doi:10.1103/
PhysRevLett.80.4329.
Haines, T. (1999). Walking with Dinosaurs - a Natural History (BBC Worldwide
Limited).
Hecht-Nielsen, R. (1989). Neurocomputing (Addison-Wesley).
Heisenberg, W. (1949). The Physical Principles of the Quantum Theory (Courier
Dover Publications).
Hertz, J., Krogh, A. and Palmer, R. G. (1991). Introduction to the Theory of
Neural Computation (Addison-Wesley).
Hey, A. and Walters, P. (2003). The New Quantum Universe (Cambridge Uni-
versity Press).
Hirvensalo, M. (2004). Quantum Computing (Springer-Verlag, Berlin Heidelberg).
Hogg, T. (2008). Quantum random walk from the wolfram demonstrations
project, http://demonstrations.wolfram.com/QuantumRandomWalk/.
Hubel, D. H. (1988). Eye, Brain, and Vision (Scientiﬁc Ammerican Library, Ox-
ford, England).
Hummel, J. (1999). Binding problem, in R. A. Wilson and F. C. Keil (eds.), The
MIT Encyclopedia of the Cognitive Sciences (The MIT Press), pp. 85–86.
Hyman, A. (1985). Charles Babbage: Pioneer of the Computer (Princeton Uni-
versity Press).
Jackson, P. (1999). Introduction to Expert Systems, 3rd edn. (Addison-Wesley).
Johnson, M. W., Amin, M. H. S., Gildert, S., Lanting, T., Hamze, F., Dickson,
N., Harris, R., Berkley, A. J., Johansson, J., Bunyk, P., Chapple, E. M.,
Enderud, C., Hilton, J. P., Karimi, K., Ladizinsky, E., Ladizinsky, N.,
Oh, T., Perminov, I., Rich, C., Thom, M. C., Tolkacheva, E., Truncik, C.
J. S., Uchaikin, S., Wang, J., Wilson, B. and Rose, G. (2011). Quantum
annealing with manufactured spins, Nature 473, 7346, pp. 194–198, URL
http://dx.doi.org/10.1038/nature10012.
Kahn, G., Kepner, A. and Pepper, J. (1987). Test: a model-driven application
shell, in National Conference on Artiﬁcial Intelligence, pp. 814–18.
Kaye, P. R., Laﬂamme, R. and Mosca, M. (2007). An Introduction to Quantum
Computing (Oxford University Press, USA).
Kempe, J. (2003). Quantum random walks - an introductory overview, Contempo-
rary Physics 44, pp. 307–327, URL http://www.citebase.org/abstract?
id=oai:arXiv.org:quant-ph/0303081.
Kitaev, A. (1996). Quantum measurements and the abelian stabilizer problem,
Electronic Colloquium on Computational Complexity 3, TR96-003.

252
Principles of Quantum Artiﬁcial Intelligence
Klahr, P. and Waterman, D. (1986). Expert Systems: Techniques, Tools and Ap-
plications (Addison-Wesley).
Knoblauch, A., Palm, G. and Sommer, F. (2010). Memory capacities for synaptic
and structural plasticity, Neural Computation 22, pp. 289–341.
Kohonen,
T. (1989). Self-Organization and Associative Memory,
3rd edn.
(Springer-Verlag).
Kolda, T. G. and Bader, B. W. (2009). Tensor decompositions and applications,
SIAM Review 51, 3, pp. 455–500.
Kolmogorov,
A.
(1933).
Grundbegriﬀe
der
Wahrscheinlichkeitsrechnung
(Springer-Verlag).
Korf, R. E. (1985). Depth-ﬁrst iterative-deepening :
An optimal admissible
tree search, Artiﬁcial Intelligence 27, 1, pp. 97 – 109, doi:DOI:10.1016/
0004-3702(85)90084-0.
Kosslyn, S. M. (1999). If neuroimaging is the answer, what is the question? Philo-
sophical Transactions of the Royal Society of London B Biological Sciences
354, pp. 1283–1294.
Kropﬀ, E. and Treves, A. (2005). The storage capacity of potts models for se-
mantic memory retrieval, Journal of Statistical Mechanics: Theory and
Experiment .
Kurbat, M., Smith, E. and Medin, D. (1994). Categorization, typicality, and shape
similarity, in A. Ram and K. Eiselt (eds.), Proceedings of the Cognitive
Science Meeting (Atlanta, GA), pp. 520–530.
Kurfess, F. J. (1997). Neural networks and structured knowledge, in C. Hermann,
F. Reine and A. Strohmaier (eds.), Knowledge in Neural Networks (Logos
Verlag, Berlin), pp. 5–22.
Kurzweil, R. (1990). The Age of Intelligent Machines (The MIT Press).
Laird, J. E., Rosenbloom, P. S. and Newell, A. (1986). Chunking in soar: The
anatomy of a general learning mechanism, Machine Learning 1, 1, pp. 11–
46.
Laird, J. F., Newell, A. and Rosenbloom, P. S. (1987). SOAR: An architecture
for general intelligence, Artiﬁcial Intelligence 40.
Lakeoﬀ, G. (1987). Women, Fire, and Dangerous Things (The University of
Chicago Press).
Lambert, D. (1983). Collins Guide to Dinosaurs (Diagram Visual Information
Ltd).
Lambert, D. (1993). The Ultimate Dinosaur Book (Dorling Kindersley).
Lancy, D. (1983). Cross-Cultural Studies in Cognition and Mathematics (Aca-
demic Press, New York).
Landauer, R. (1961). Irreversibility and heat generation in the computing process,
IBM Journal of Research and Development 5, pp. 183–191.
Landauer, R. (1992). Information is physical, in Physics and Computation, 1992.
PhysComp ’92., Workshop on, pp. 1–4.
Lewis, H. R. and Papadimitriou, C. H. (1981). Elements of the Theory of
Computation (Prentice Hall PTR, Upper Saddle River, NJ, USA), ISBN
0132624788.

Bibliography
253
Luger, G. F. and Stubbleﬁeld, W. A. (1993). Artiﬁcial Intelligence: Structures
and Strategies for Complex Problem Solving: Second Edition (The Ben-
jamin/Cummings Publishing Company, Inc, Menlo Park, CA, USA).
Luger, G. F. and Stubbleﬁeld, W. A. (1998). Artiﬁcial Intelligence, Structures
and Strategies for Complex Problem Solving, 3rd edn. (Addison-Wesley).
Marcinowski, M. (1987). Codierungsprobleme beim Assoziativen Speichern, Mas-
ter’s thesis, Fakult¨at f¨ur Physik der Eberhard-Karls-Universit¨at T¨ubingen.
Markov, A. (1954). The theory of algorithms (National Academy of Sciences,
USSR).
Maxwell, J. C. (2001). Theory of Heat (9ed) (Courier Dover Publications).
McClelland, J. and Kawamoto, A. (1986). Mechanisms of sentence processing:
Assigning roles to constituents of sentences, in J. McClelland and D. Rumel-
hart (eds.), Parallel Distributed Processing (The MIT Press), pp. 272–325.
McClelland, J. and Rumelhart, D. (1985). Distributed memory and the represen-
tation of general and speciﬁc memory, Journal of Experimental Psychology:
General 114, pp. 159–188.
Miikkulainen, R. (1993). Subsymbolic Natural Language Processing: An Integrated
Model of Scripts, Lexicon and Memory (The MIT Press).
Minsky, M. (1975). A framework for representing knowledge, in P. Winston (ed.),
The Psychology of Computer Vision (McGraw-Hill, New York), pp. 211–77.
Minsky, M. (1986). The Society of Mind (Simon and Schuster, New York).
Mitchell, T. (1997). Machine Learning (McGraw-Hill).
Moore, C. and Russell, A. (2001). Quantum Walks on the Hypercube, eprint
arXiv:quant-ph/0104137 .
Murphy, G. and Brownell, H. (1985). Category diﬀerentiation in object recog-
nition: Typicality constraints on the basic category advantage, Journal of
Experimental Psychology 11, p. 70ﬀ.
Myers, J. M. (1997). Can a universal quantum computer be fully quantum? Phys.
Rev. Lett. 78, 9, pp. 1823–1824, doi:10.1103/PhysRevLett.78.1823.
Newell, A. (1990). Uniﬁed Theories of Cognition (Harvard University Press).
Newell, A. and Simon, H. (1972). Human Problem Solving (Prentice-Hall).
Newell, A. and Simon, H. (1976). Computer science as empirical inquiry: symbols
and search. Communication of the ACM 19, 3, pp. 113–126.
Nielsen, M. A. and Chuang, I. L. (2000). Quantum Computation and Quantum
Information (Cambridge University Press, Cambridge, MA, USA).
Nilsson, N. J. (1982). Principles of Artiﬁcial Intelligence (Springer-Verlag).
Nunn, C. M., Clarke, C. and Blott, B. (1994). Collapse of a quantum ﬁeld may
aﬀect brain function, Journal of Consciousness Studies 1, 1, pp. 127–139.
OFTA (1991). Les R´eseaux de Neurones (Masson).
Opwis, K. and Pl¨otzner, R. (1996). Kognitive Psychologie mit dem Computer
(Spektrum Akademischer Verlag, Heidelberg Berlin Oxford).
Osherson, D. N. (1987). New axioms for the contrast model of similarity, Journal
of Mathematical Psychology 31, pp. 93–103.
Osherson, D. N. (1995). Probability judgment, in E. E. Smith and D. N. Osherson
(eds.), Thinking, Vol. 3, 2nd edn., chap. two (MIT Press), pp. 35–75.
Palm, G. (1982). Neural Assemblies, an Alternative Approach to Artiﬁcial Intel-
ligence (Springer-Verlag).

254
Principles of Quantum Artiﬁcial Intelligence
Palm, G. (1990). Assoziatives Ged¨achtnis und Gehirntheorie, in Gehirn und Kog-
nition (Spektrum der Wissenschaft), pp. 164–174.
Palm, G., Schwenker, F. and Bibbig, A. (1992). Theorie Neuronaler Netze 1,
Skript zur Vorlesung, university of Ulm, Department of Neural Information
Processing.
Pearl, J. (1984). Heuristics: Intelligent Strategies for Computer Problem Solving
(Addison-Wesley).
Pearl, J. (1989). Probabilistic Reasoning in Intelligent Systems: Networks of Plau-
sible Inference (Morgan Kaufmann, Palo Alto, CA).
Penrose, R. (1991). The Emperor’s New Mind (Penguin).
Post, E. (1943). Formal reductions of the general combinatorial problem, Ameri-
can Journal of Mathematics 65, pp. 197–268.
Quillian, R. (1968). Semantic memory, in M. Minsky (ed.), Semantic Information
Processing (MIT Press), pp. 227–270.
Reid, C. (1996). Hilbert (Spriner Verlag).
Resnikoﬀ, H. L. (1989). The Illusion of Reality (Springer-Verlag).
Rieﬀel, E. and Polak, W. (2011). Quantum Computing - A Gentle Introduction
(The MIT Press).
Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object recognition
in cortex, Nature Neuroscience 2, pp. 1019–1025.
Riley, K. F., Hobson, M. P. and Bence, S. J. (2006). Mathematical methods for
physics and engineering (Cambridge University Press).
Ross, S. M. (2009). Introduction to probability and statistics for engineers and
scientists, 4th edn. (Academic Press).
Rumelhart, D. and McClelland (1986). On learning the past tense of english verbs,
in J. McClelland and D. Rumelhart (eds.), Parallel Distributed Processing
(MIT Press), pp. 216–271.
Russell,
S. and Norvig,
P. (2010). Artiﬁcial intelligence:
a modern ap-
proach, Prentice Hall series in artiﬁcial intelligence (Prentice Hall), ISBN
9780136042594, URL http://books.google.pt/books?id=8jZBksh-bUMC.
Russell, S. J. and Norvig, P. (1995). Artiﬁcial intelligemce: a modern approach
(Prentice-Hall).
Schr¨odinger, E. (1935). Die gegenw¨artige situation in der quantenmechanik,
Naturwissenschaften 23, 807.
Schwenker, F. (1996). K¨untliche Neuronale Netze: Ein ¨Uberblick ¨uber die theo-
retischen Grundlagen, in G. Bol, G. Nakhaeizadeh and K. Vollmer (eds.),
Finanzmarktanalyse und -prognose mit innovativen und quantitativen Ver-
fahren (Physica-Verlag), pp. 1–14.
Shannon, C. E. (1948). A mathematical theory of communication, Bell System
Technical Journal , pp. 1–54.
Shannon, C. E. (1953). Computers and automata, Proceedings of the I.R.E. 41,
pp. 1253–1241.
Shastri, L. (1988). Semantic Networks: An Evidential Formulation and its Con-
nectionistic Realization (Morgan Kaufmann, London).
Shenvi, N., Kempe, J. and Whaley, K. B. (2003). Quantum random-walk search
algorithm, Phys. Rev. A 67, 5, p. 052307, doi:10.1103/PhysRevA.67.
052307.

Bibliography
255
Shim, G. M., Kim, D. and Choi, M. Y. (1990). Statistical-mechanical formulation
of the wiiishaw model with local inhibition, Physical Review A A 43, 12,
pp. 7012–7018.
Shor, P. (1994). Algorithms for quantum computation: discrete logarithms and
factoring, in Proceedings 35th Annual Symposium on Foundations of Com-
puter Science, pp. 124–134, doi:10.1109/SFCS.1994.365700.
Shor, P. W. (1995). Polynomial-Time Algorithms for Prime Factorization and
Discrete Logarithms on a Quantum Computer, ArXiv Quantum Physics
e-prints .
Simon, H. A. (1991). Models of my Life (Basic Books, New York).
Smith, E., Balzano, G. and Walker, J. (1978). Nominal, perceptual, and semantic
codes in picture categorization, in J. Cotton and R. Klatzky (eds.), Seman-
tic factors in cognition (Hillsdale, NJ: Erlbaum Associates), pp. 137–168.
Smith, E. and Sloman, S. (1994). Similarity vs. rule-based categorization, Memory
Cognition 22, pp. 377–386.
Smith, E. E. (1995). Concepts and categorization, in E. E. Smith and D. N.
Osherson (eds.), Thinking, Vol. 3, 2nd edn., chap. one (MIT Press), pp.
3–33.
Sommer, F. T. (1993). Theorie neuronaler Assoziativspeicher, Ph.D. thesis,
Heinrich-Heine-Universit¨at D¨usseldorf, D¨usseldorf.
Squire, L. R. and Kandel, E. R. (1999). Memory. From Mind to Moleculus (Sci-
entiﬁc American Library).
Steinbuch, K. (1961). Die Lernmatrix, Kybernetik 1, pp. 36–45.
Steinbuch, K. (1971). Automat und Mensch, 4th edn. (Springer-Verlag).
Stonier, T. (1990). Information and the Internal Structure of Universe (Springer-
Verlag).
Sun, R. (1995). A two-level hybrid architecture for structuring knowledge for com-
monsense reasoning, in R. Sun and L. A. Bookman (eds.), Computational
Architectures Integrating Neural and Symbolic Processing, chap. 8 (Kluwer
Academic Publishers), pp. 247–182.
Sussman, G. (1975). A Computer Model of Skill Acquisition (MIT, Cambridge).
Szil´ard, L. (1929). ¨Uber die entropieverminderung in einem thermodynamischen
system bei eingriﬀen intelligenter wesen, Zeitschrift f¨ur Physik 53, pp. 840–
856.
Tarrataca, L. and Wichert, A. (2011a). Problem-solving and quantum computa-
tion, Cognitive Computation 3, pp. 510–524, URL http://dx.doi.org/10.
1007/s12559-011-9103-6.
Tarrataca, L. and Wichert, A. (2011b). Tree search and quantum computation,
Quantum Information Processing 10, 4, pp. 475–500, 10.1007/s11128-010-
0212-z.
Tarrataca, L. and Wichert, A. (2012a). Iterative quantum tree search, CiE 2012
- How the World Computes, 2012.
Tarrataca, L. and Wichert, A. (2012b). A quantum production model, Quantum
Information Processing 11, 1, pp. 189–209, URL http://dx.doi.org/10.
1007/s11128-011-0241-2, 10.1007/s11128-011-0231-4.

256
Principles of Quantum Artiﬁcial Intelligence
Tarrataca, L. and Wichert, A. (2013a). Intricacies of quantum computational
paths, Quantum Information Processing 12, pp. 1365–1378, doi:10.1007/
s11128-012-0475-7.
Tarrataca, L. and Wichert, A. (2013b). Quantum iterative deepening with an
application to the halting problem, PLOS ONE 8, 3.
Tarski, A. (1944). The semantic conception of truth and foundations of semantics,
Philos. and Phenom. Res. 4, pp. 241–376.
Tarski, A. (1956). Logic, Semantics,Metamathematics (Oxford University Press,
London).
Tarski, A. (1995). Pisma logiczno-ﬁlozoﬁczne. Prawda, Vol. 1 (Wydawnictwo
Naukowe PWN, Warszawa).
Tay, N., Loo, C. and Perus, M. (2010). Face recognition with quantum associative
networks using overcomplete gabor wavelet, Cognitive Computation , pp. 1–
6URL http://dx.doi.org/10.1007/s12559-010-9047-2, 10.1007/s12559-
010-9047-2.
Toﬀoli, T. (1980a). Reversible computing, in Proceedings of the 7th Colloquium on
Automata, Languages and Programming (Springer-Verlag, London, UK),
ISBN 3-540-10003-2, pp. 632–644.
Toﬀoli, T. (1980b). Reversible computing, Tech. rep., Massschusetts Institute of
Technology, Laboratory for Computer Science, Massachusetts, MA, USA.
Toﬀoli, T. (1980c). Reversible computing, in J. de Bakker and J. van Leeuwen
(eds.), Automata, Languages and Programming, Lecture Notes in Computer
Science, Vol. 85 (Springer Berlin / Heidelberg), pp. 632–644, URL http:
//dx.doi.org/10.1007/3-540-10003-2$_$104.
Topsoe, F. (1974). Informationstheorie (Teubner Sudienbucher).
Turing, A. (1936). On computable numbers, with an application to the entschei-
dungsproblem, in Proceedings of the London Mathematical Society, Vol. 2,
pp. 260–265.
Turing, A. M. (1950). Computing machinery and intelligence, Mind 59.
Tversky, A. (1977). Feature of similarity, Psychological Review 84, pp. 327–352.
Vedral, V. (2011). Living in a quantum world, Scientiﬁc American 304, 6, pp.
38–43.
von Neumann, J. (1945). First draft of a report on the edvac, Tech. rep., Univer-
sity of Pennsylvania.
Watrous, J. (1998). Quantum simulations of classical random walks and undi-
rected graph connectivity, CoRR cs.CC/9812012.
Wennekers, T. (1999). Synchronisation und Assoziation in Neuronalen Netzen
(Shaker Verlag, Aachen).
Wichert, A. (1993). MTCn-nets, in Proceedings World Congres on Neural Net-
works (Lawrence Erlbaum), pp. 59–62.
Wichert, A. (1998). Hierarchical categorization, in M. W. Evens (ed.), Ninth
Midwest Artiﬁcial Intelligence and Cognitive Science Conference (AAAI
Press), pp. 141–148.
Wichert, A. (2000). A categorical expert system “jurassic”, Expert Systems with
Application 19, 3, pp. 149–158.
Wichert, A. (2001). Pictorial reasoning with cell assemblies, Connection Science
13, 1.

Bibliography
257
Wichert, A. (2004). Categorial expert systems, Expert Systems 21, 1, pp. 34–47.
Wichert, A. (2005a). Associative computer: a hybrid connectionistic production
system, Cognitive Systems Research 6, 2, pp. 111–144.
Wichert, A. (2005b). Associative diagnose, Expert Systems 22, 1, pp. 26–39.
Wichert, A. (2006). Cell assemblies for diagnostic problem-solving, Neurocomput-
ing 69, 7-9, pp. 810–824.
Wichert, A. (2009). Sub-symbols and icons, Cognitive Computation 1, 4, pp.
342–347.
Wichert, A. (2011). The role of attention in the context of associative memory,
Cognitive Computation 3, 1.
Wichert, A. (2012). Inference, ontologies and the pump of though, in E. Dave-
laar (ed.), Connectionist Models of Neurocognition and Emergent Behavior
(World Scientiﬁc), pp. 227 –224.
Wichert, A. (2013). Proto logic and neural subsymbolic reasoning, Journal of
Logic and Computation 23, 3, pp. 627–643.
Wichert, A., Abler, B., Grothe, J., Walter, H. and Sommer, F. T. (2002). Ex-
ploratory analysis of event-related fmri demonstrated in a working memory
study, in F. Sommer and A. Wichert (eds.), Exploratory analysis and data
modeling in functional neuroimaging, chap. 5 (MIT Press, Boston, MA),
pp. 77–108.
Wichert, A., Pereira, J. D. and Carreira, P. (2008). Visual search light model for
mental problem solving, Neurocomputing 71, 13-15, pp. 2806–2822.
Wickelgren, W. A. (1969). Context-sensitive coding, associative memory, and
serial order in (speech)behavior, Psychological Review 76, pp. 1–15.
Wickelgren, W. A. (1977). Cognitive Psychology (Prentice-Hall).
Williams, C. P. and Clearwatter, S. H. (1997). Explorations in Quantum Com-
puting (Springer-Verlag).
Willshaw, D., Buneman, O. and Longuet-Higgins, H. (1969). Nonholgraphic as-
sociative memory, Nature 222, pp. 960–962.
Winston, P. H. (1992). Artiﬁcial Intelligence, 3rd edn. (Addison-Wesley).
Zalka, C. (1999). Grover’s quantum searching algorithm is optimal, Physical Re-
view A 60, p. 2746, URL http://www.citebase.org/abstract?id=oai:
arXiv.org:quant-ph/9711070.

This page intentionally left blank
This page intentionally left blank

Index
3-puzzle, 213
A search, 204
A∗search, 204
NP, 13, 173
NP −complete, 14, 173
NP −hard, 14
P, 13
λ-calculus, 9
n-puzzle, 207
nth root of unity, 148
8-puzzle, 29
AC diﬀerence, 151
adiabatic quantum computation, 240
admissible, 207
alpha-beta search, 240
amplitude, 182
analog computer, 15
ancilla bit, 77
AND-OR formulas, 240
AND/OR graph, 27, 73
associative memory, 57
auxiliary bit, 134, 193
Babbage, Charles, 1
backtracking, 29, 204
Bayes’s rule, 81
Bayesian networks, 86
belief networks, 87
Bennett, Charles H., 75
binary fraction, 156
binding problem, 32
bisecting line, 181
bit, 42
Bloch sphere, 225
Bloch, Felix, 225
block examples, 22
Boltzmann’s equation, 40
Boolean circuit, 16
Boolean formula evaluation, 239
Boolean gates, 76
branching factor, 200
Brassard, Gilles, 192
breadth-ﬁrst search, 200
Cantor’s diagonal argument, 11
categorization, 23
category, 85
Cauchy-Schwarz inequality, 112
changing the basis, 141
chaos theory, 3
children, 200
chunking, 221
Church, Alonzo, 9
Church–Turing thesis, 14
Church-Turing Theorem, 9
city-block distance, 205
cloning, 136
collapse, 109
conﬂict resolution, 26, 28
context-sensitive letter, 67
continued ﬁnite fraction, 163
contrast model, 84
controlled not gate, 125
259

260
Principles of Quantum Artiﬁcial Intelligence
Copenhagen interpretation, 96
copy machine, 136
cryptography, 164
David, Deutsch, 15
DC average, 151
decision making, 226
decision problem, 13, 173
decomposable, 125
deduction systems, 26, 68
dendrite, 57
density matrix, 224
depth-ﬁrst search, 201
deterministic chaos, 114
Deutsch algorithm, 130
Deutsch Jozsa algorithm, 132
DFT matrix, 153
diagnostic systems, 27
dice model, 39
digital computer, 16
Dirac, Paul, 98
direct product, 106
Discrete Cosine Transform, 171
discrete Fourier transform, 147
discrete-time random walk, 235
ebit, 126
edge, 200
eﬀective branching factor, 207, 211
ellipse, 188
entangled, 126
entropy, 40
Entscheidungsproblem, 10
equivalent states, 140
erasing of information, 75
Euclidean algorithm, 164, 167
Euclidean geometry, 205
Euclidian geometrical world, 31
Euler’s number, 47
Everett, Hugh, 97
exhaustive search, 180
expert systems, 27
factorization, 164
fan-out operation, 125
fast Fourier transform (FFT), 153
Feynman, Richard, 15
ﬁlter, 149
ﬁring, 25
fMRI, 243
formula evaluation, 239
Fourier transform, 146
fraction, 163
frame axiom, 22
free will, 244
frequency, 145
frequentist, 79
G¨odelization, 11
Gauss, Carl Friedrich, 153
generate-and-test, 196
geometric series, 201
geometrically-based problem-solving,
31
greatest common divisor, 164
Greedy best-ﬁrst search, 204
Grover’s ampliﬁcation, 180
Grover, Lov K., 180
Haar-Wavelet-Transform, 171
Hadamard transform, 153
Hadarmad gate, 121
Hadarmad matrix, 121
Hagelbarger, David, 116
halt ﬂag, 220
Halting problem, 16
halting problem, 10
Hamiltonian, 95, 101
Hamiltonian equation of motion, 95
Heisenberg’s uncertainty principle,
112, 242
Hermitian matrix, 101
heuristic, 199
heuristic functions, 31
heuristic search, 203
hidden inference, 231
hierarchy, 47
Hilbert space, 97
Hilbert, David, 9
hill climbing, 33
Householder reﬂection, 180
Householder, Alston Scott, 180

Index
261
ideal entropy, 45, 46
information, 37, 42
information theory, 38
instance, 14
instantiationed, 20, 89
inverse Fourier transform, 146
irrelevant, 88
iterative ampliﬁcation, 184
iterative deepening, 202
iterative quantum tree search, 212
Karhunen-Lo`eve transformation, 141
Kitaev’s phase, 168
Kitaev, Alexei, 168
Kolmogorov complexity, 114
Kolmogorov’s axioms, 79
Korf, Richard E., 202
Kronecker product, 124
law of total probability, 81, 226
laws of thermodynamics, 38
leaf, 200
learning, 58
Lernmatrix, 57
logistic map, 114
long-term memory, 25
Lovelance, Ada, 1
lower bound, 180
macroscopic, 39, 223
macrostate, 39
Markov chain, 92, 224
Markov matrix, 92
Markov propert, 91
maximum posteriori hypothesis, 81
Maxwell paradox, 41, 75
Maxwell, James Clerk, 38
McCarthy, John, 1
McIrvin, Matt, 39
measurement, 50
microscopic, 39
microstate, 39
mind, 220
multiverse, 97
nat, 47
negative interference, 231
nepit, 47
Neumann, John von, 16
neuron, 57
node, 200
non-computable schema, 244
non-reversible, 243
observable, 110
OPS5, 220
oracle, 173
orbit, 186
orthogonal projection, 100
parent, 200
Parseval’s theorem, 149
path descriptor, 213
period, 145, 186
Perl, Judea, 87
permutation matrix, 78
phase, 146, 168
phase space, 186
physical symbol system hypothesis, 2
Planck’s constant, 95
Planck, Max, 95
planning, 27
Platonic world, 80
positive interference, 231
Post, Emil, 16, 26
predicate calculus, 21
problem space, 27
problem-solving, 2, 25, 199
production system, 19, 25, 199
productions, 19, 25
proto logic, 31
pure production system, 218
pure state, 109, 120
Pythagorean identity, 189
QFT decomposition, 154
QFT period algorithm, 161
QFT quantum circuit, 155
quantum annealing, 241
quantum cognition, 223
quantum coin, 98
quantum computer, 15

262
Principles of Quantum Artiﬁcial Intelligence
quantum counting, 192
quantum ﬂuctuation, 242
Quantum Fourier Transform, 150
quantum insect, 235
quantum not gate, 119
quantum oracle, 173
quantum probabilities, 224
quantum random walk, 235
quantum tunneling, 242
quantum Turing machine, 220
quantum-symbols, 199
qubit, 105
random, 3
random walk, 235
randomness, 96, 114
reaction systems, 27
real entropy, 43
recurrence equations, 189
reﬂection, 141
register, 106, 121
relaxed problem, 207
retrieval, 59
reversible circuit, 78
reversible Turing machine, 76
root, 200
root of unity, 148
rotation, 141
RSA-public key, 164
Schr¨odinger equation, 95
Schr¨odinger, Erwin, 95
Schr¨odinger’s cat, 96
search, 200
self-adjoint, 101
sensors, 31
serial computation, 123
Shannon, Claude Elwood, 45
Shor’s algorithm, 164
Shor, Peter, 164
SOAR, 29, 220
sparse code, 67
spectral representation, 102
spy matrix, 175
SRIPS, 21
Steinbuch, Karl, 57
Sterling‘s formula, 63
sub-symbol, 3, 31
sub-symbolical representation, 30
superposition, 95
Sussman anomaly, 35
symbol, 2, 20
synapse, 57
Szil´ard, Le´o, 41
target bit, 134
Tarrataca’s quantum production
system, 213
Tarrataca, Lu´ıs, 213
taxonomy, 49
teleportation, 137
tensor product, 124
ticket, 14, 173
Toﬀoli gate, 77, 127
trace, 108
traveling salesman problem, 195
tree, 200
Turing machine, 10, 75, 220
Turing test, 2
Turing, Alan, 2, 9
unary quantum gate, 119
Uniﬁed theories of cognition, 220
uninformed search, 200
unitary control strategy, 219
unitary evolution, 99
unitary matrix, 98
universal Turing machine, 220
unobservable, 225
unpacking eﬀect, 232
Vandermonde matrix, 148
vision, 243
von Neumann entropy, 108, 135
von Neumann probabilities, 224
Walsh matrix, 123
wave function, 223
wavelength , 145
Wickelgren, Wayne, 67
Willshaw, David, 57
working memory, 25


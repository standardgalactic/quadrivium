 123
LNBIP 273
15th International Conference, IE 2016
Cluj-Napoca, Romania, June 2–3, 2016
Revised Selected Papers
Informatics in Economy
Gheorghe Cosmin Silaghi
Robert Andrei Buchmann
Catalin Boja (Eds.)

Lecture Notes
in Business Information Processing
273
Series Editors
Wil M. P. van der Aalst
Eindhoven University of Technology, Eindhoven, The Netherlands
John Mylopoulos
University of Trento, Trento, Italy
Michael Rosemann
Queensland University of Technology, Brisbane, QLD, Australia
Michael J. Shaw
University of Illinois, Urbana-Champaign, IL, USA
Clemens Szyperski
Microsoft Research, Redmond, WA, USA

More information about this series at http://www.springer.com/series/7911

Gheorghe Cosmin Silaghi
Robert Andrei Buchmann
• Catalin Boja (Eds.)
Informatics in Economy
15th International Conference, IE 2016
Cluj-Napoca, Romania, June 2–3, 2016
Revised Selected Papers
123

Editors
Gheorghe Cosmin Silaghi
Babeș-Bolyai University
Cluj-Napoca
Romania
Robert Andrei Buchmann
Babeș-Bolyai University
Cluj-Napoca
Romania
Catalin Boja
Bucharest University of Economic Studies
Bucharest
Romania
ISSN 1865-1348
ISSN 1865-1356
(electronic)
Lecture Notes in Business Information Processing
ISBN 978-3-319-73458-3
ISBN 978-3-319-73459-0
(eBook)
https://doi.org/10.1007/978-3-319-73459-0
Library of Congress Control Number: 2017962902
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The International Conference on Informatics in Economy (IE) is an established tradi-
tion having reached its 15th edition in 2016. It was initiated in 1993 by the Academy of
Economic Studies in Bucharest (ASE Bucharest), Romania, in collaboration with the
Institut National des Sciences Appliquées de Lyon (INSA de Lion), France, thus
becoming the ﬁrst scientiﬁc event to foster a scientiﬁc community in the area of
business information systems in Romania. The conference promotes research results in
business informatics and related computer science topics such as: cloud, distributed and
parallel computing, mobile-embedded and multimedia solutions, e-society, enterprise
and business solutions, databases and data warehouses, audit and project management,
quantitative economics, artiﬁcial intelligence, and data mining.
Starting with 2012, the conference has taken place annually in Bucharest and its
proceedings have been indexed by ISI Thomson. The 2016 edition of IE (http://www.
conferenceie.ase.ro/) was held in Cluj Napoca, Romania, and co-organized by Babeș-
Bolyai University – an occasion to also celebrate 25 years from the founding of the
Business Information Systems Department and line of studies at UBB Cluj Napoca. In
order to extend the international visibility of the event, this was the ﬁrst edition of IE to
have its proceedings published in Springer’s Lecture Notes in Business Information
Processing series – an ideal dissemination channel, considering the conference topics.
This volume includes extended versions of the best papers presented at the IE
conference during June 2–3, 2016. A total of 89 papers were presented, out of which 31
were submitted as extended revised versions for inclusion in this volume. The pro-
ceedings review process involved at least three reviewers for each submission and the
ﬁnal selection comprises ten full papers, three short papers and one invited keynote
paper.
The two keynote papers address the emerging paradigm of agile modeling method
engineering in business informatics (Prof. Dr. Dimitris Karagiannis, University of
Vienna, Austria) and mechanisms for next-generation smart grids (Dr. Valentin Robu,
Heriot-Watt University Edinburgh, UK). The rest of the table of contents is grouped
into ﬁve topical sections: “Distributed Systems,” “Information Systems Adoption,”
“Knowledge Representation and Processing,” “Domain-Speciﬁc Data Analysis,” and
Computational Models.
We take this opportunity to express our gratitude to the founders of the business
information systems studies and scientiﬁc community in Romania and in the univer-
sities co-organizing this event – Prof. Florin Gheorghe Filip - member of the Romanian
Academy (chair of the IE Conference),
(former rector of
Bucharest University of Economic Studies),
(former head
of the Business Information Systems Department at Babeş-Bolyai University of Cluj
Napoca), and Prof. Dan Racoviţan (former dean of Economic Studies at Babeş-Bolyai
University Cluj Napoca).

We thank the authors who submitted their work and addressed the suggestions for
improvement gathered both during the conference presentations and the proceedings
review process; we also thank the reviewers and members of the Program Committee,
who provided their expertise in selecting the best papers and for suggesting
improvements; we are grateful for the inspiration and research challenges raised and
discussed by the two invited keynote speakers; and, of course, this volume would not
have been possible without the extensive technical support and guidance provided by
the Springer team led by Ralf Gerstner.
For the conference organization, we acknowledge support from UEFISCDI under
project PN-II-PT-PCCA-2013-4-1644 and from NTT Data Romania, our ofﬁcial
partner.
February 2017
Gheorghe Cosmin Silaghi
Robert Andrei Buchmann
Cătălin Boja
VI
Preface

Organization
IE 2016 was hosted by the Faculty of Economic Sciences and Business Administration,
Babeş-Bolyai University of Cluj-Napoca and co-organized together with the Depart-
ment of Economic Informatics and Cybernetics, Faculty of Cybernetics, Statistics and
Economic Informatics from the Bucharest University of Economic Studies. The
conference was held during June 2–3, 2016 in Cluj-Napoca, Romania.
Organizing Committee
General Chair
Florin Gheorghe Filip
Bucharest University of Economic Studies, Romania
Program Co-chairs and Local Organizing Committee
Ion Smeureanu
Bucharest University of Economic Studies, Romania
Cătălin Boja
Bucharest University of Economic Studies, Romania
Gheorghe Cosmin Silaghi
Babeș-Bolyai University, Cluj-Napoca, Romania
Robert Andrei Buchmann
Babeș-Bolyai University, Cluj-Napoca, Romania
Program Committee
Frederique Biennier
INSA de Lion, France
Wladimir Bodrow
University of Applied Sciences Berlin, Germany
Ewa Bojar
Lublin University of Technology, Poland
Pino Caballero-Gil
University of La Laguna, Spain
Hans Czap
Trier University, Germany
Howard Duncan
Dublin City University, Ireland
Manfred Fischer
Wirtscahftsuniversität Wien, Austria
Janis Grundspenkis
Riga Technical University, Latvia
Timothy Hall
University of Limerick, Ireland
Luca Iandoli
University Federico II, Italy
Ivan Jelinek
Czech Technical University in Prague, Czech Republic
Jones Karl
Liverpool John Moores University, UK
Karlheinz Kautz
Copenhagen Business School, Denmark
Wong Wing Keung
National University of Singapore, Singapore
Yannis Manolopoulos
Aristotle University of Thessaloniki, Greece
Lynn Martin
University of Central England, UK
Antonio Jose Mendes
University of Coimbra, Portugal
Mihaela Muntean
West University of Timişoara, Romania
Peter Nijkamp
Free University of Amsterdam, The Netherlands
Maria Parlinska
Warsaw University of Life Sciences, Poland
Boris Rachev
Bulgarian Chapter of the ACM, Bulgaria

George Roussos
Birkbeck University of London, UK
Gheorghe Cosmin Silaghi
Babeş-Bolyai University, Romania
Frantz Rowe
University of Nantes, France
Doru E. Tiliute
Ştefan cel Mare University of Suceava, Romania
Eduardo Tome
Universidade Lusiada de Famalicao, Portugal
Michael Tschichholz
Fraunhofer eGovernment Center, Germany
Giuseppe Zollo
University Federico II, Italy
Additional Reviewers
Alvaro Arenas
IE Business School, Madrid, Spain
Benjamin Aziz
University of Portsmouth, UK
Costin Bădică
University of Craiova, Romania
Vasile Paul Breșfelean
Babeș-Bolyai University of Cluj Napoca, Romania
Robert Andrei Buchmann
Babeș-Bolyai University of Cluj Napoca, Romania
Darius Bufnea
Babeș-Bolyai University of Cluj Napoca, Romania
Anuța Buiga
Babeș-Bolyai University of Cluj Napoca, Romania
Cătălina Lucia Cocianu
Bucharest University of Economic Studies, Romania
Liviu Gabriel Crețu
European Commission, D. G. Informatics, Belgium
Luigi D’Ambra
University of Naples Federico II, Italy
Ana-Maria Ghiran
Babeș-Bolyai University of Cluj Napoca, Romania
Dorina Lazăr
Babeș-Bolyai University of Cluj Napoca, Romania
Cristia Litan
Babeș-Bolyai University of Cluj Napoca, Romania
Syed Naqvi
Birmingham City University, UK
Virginia Niculescu
Babeș-Bolyai University of Cluj Napoca, Romania
Ioan Petri
Cardiff University, UK
Răzvan Petruşel
Babeș-Bolyai University of Cluj Napoca, Romania
Mihai Daniel Roman
Bucharest University of Economic Studies, Romania
Monica Ioana Pop Silaghi
Babeș-Bolyai University of Cluj Napoca, Romania
Alexandru-Ioan Stan
Babeș-Bolyai University of Cluj Napoca, Romania
Adrian Sterca
Babeș-Bolyai University of Cluj Napoca, Romania
Alexandru Todea
Babeș-Bolyai University of Cluj Napoca, Romania
Claudiu Vințe
Bucharest University of Economic Studies, Romania
VIII
Organization

Keynote Abstracts

Designing Incentive Mechanisms
for Next-Generation Smart Grids
Valentin Robu
Mechanical, Process and Energy Engineering, Heriot-Watt University,
Edinburgh, UK
v.robu@hw.ac.uk
Abstract. This talk aims to give a broad overview of recent research in
multi-agent systems, algorithmic game theory and electronic markets and their
application to smart energy grids. It will cover a range of topics in this area, such
as using online mechanism design to coordinate the charging of multiple electric
vehicles while ensuring the capacity of distribution networks is to exceeded, the
use of scoring rules to elicit accurate predictions from renewable energy pro-
ducers, to demand side management through the formation of consumer coali-
tions. The talk will give a brief description of the key results obtained so far in
these areas and outline some potential directions for future work.

Conceptual Modelling Methods:
The AMME Agile Engineering Approach
Dimitris Karagiannis
Knowledge Engineering Research Group, Faculty of Computer Science,
University of Vienna, Vienna, Austria
dk@dke.univie.ac.at
Abstract. Current research in ﬁelds such as Business Process Management,
Enterprise Architecture Management, Knowledge Management and Software
Engineering raises a wide diversity of requirements for Conceptual Modelling,
typically satisﬁed by Design Science artefacts such as modelling methods. When
employed in the context of an Agile Enterprise, an underlying requirement for
Conceptual Modelling agility emerges - manifested not only on model contents
level, but also on modelling method level. Depending on the questions that must
be answered and the systems that must be supported with modelling means, the
need for agility may stem from the degree of domain-speciﬁcity, from gradual
understanding of modelling possibilities, from evolving model-driven systems
etc. The hereby proposed Agile Modelling Method Engineering (AMME)
approach thus becomes necessary to extend the traditional perspective of
“modelling through standards”; consequently, the beneﬁts of repeatability and
wide adoption are traded for responsiveness to dynamic needs identiﬁed within
an Agile Enterprise.

Contents
Keynote Paper
Conceptual Modelling Methods:
The AMME Agile Engineering Approach . . . . . . . . . . . . . . . . . . . . . . . . .
3
Dimitris Karagiannis
Distributed Systems
Optimizing Service Level Agreements in Peer-to-Peer Supply
Chain Model for Complex Projects Management. . . . . . . . . . . . . . . . . . . . .
23
Florina Livia Covaci
A Brief Overview of Semantic Interoperability for Enterprise
Information Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
Tarcisio Mendes de Farias, Ana Roxin, and Christophe Nicolle
Information Systems Adoption
Accepting Information Technology Changes
in Universities - A Research Framework . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Doina Danaiata, Ana-Maria Negovan,
and Luminita Hurbean
A Survey on Social Learning Analytics: Applications,
Challenges and Importance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
Maria-Iuliana Dascalu, Constanţa-Nicoleta Bodea,
Radu Ioan Mogos, Augustin Purnus, and Bianca Tesila
Students in Social Media: Behavior, Expectations and Views . . . . . . . . . . . .
84
Mircea Georgescu and Daniela Popescul
Knowledge Representation and Processing
Designing, Implementing and Testing the Acoustic Component
of a Text to Speech System for the Romanian Language . . . . . . . . . . . . . . .
101
Razvan Alin Boldizsar, Mihaela Ordean, and Corina Giurgea
Learning Style in Ontology-Based E-Learning System . . . . . . . . . . . . . . . . .
115
Lidia Băjenaru and Ion Smeureanu

Getting Meaning in the Online Environment of E-Commerce
by Using Semantic Web Technologies. . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
Sabina-Cristiana Necula
Modeling and Simulating a Call Center Activity . . . . . . . . . . . . . . . . . . . . .
138
Georgeta Soava and Adina Balan
Domain-Specific Data Analysis
Using Non-parametric Order-Alpha Hyperbolic Efficiency Estimators
to Assess Aspects of Melanoma in a Romanian Hospital . . . . . . . . . . . . . . .
149
Anamaria Aldea, Alexandra Limbău, Maria Daniela Tănăsescu,
Mircea Tampa, and Simona Roxana Georgescu
Forecasting Solutions for Photovoltaic Power Plants in Romania. . . . . . . . . .
160
Simona-Vasilica Oprea, Alexandru Pîrjan, Ion Lungu,
and Anca-Georgiana Fodor
Reflecting on Romanian Universities Ranking:
An Entropy-Based Approach to Evaluate Scientific Research . . . . . . . . . . . .
175
Luiza Bădin, Florentin Şerban, Anca-Teodora Şerban-Oprescu,
and Silvia Dedu
Computational Models
Insights of Adaptive Learning Approach to Modeling Expectations:
A Numerical Comparison with Adaptive Expectations
and Rational Expectations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
Raluca-Elena Pop
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
XIV
Contents

Keynote Paper

Conceptual Modelling Methods: The AMME Agile
Engineering Approach
Dimitris Karagiannis
(✉)
Knowledge Engineering Research Group, Faculty of Computer Science,
University of Vienna, Vienna, Austria
dk@dke.univie.ac.at
Abstract. Current research in ﬁelds such as Business Process Management,
Enterprise Architecture Management, Knowledge Management and Software
Engineering raises a wide diversity of requirements for Conceptual Modelling,
typically satisﬁed by Design Science artefacts such as modelling methods. When
employed in the context of an Agile Enterprise, an underlying requirement for
Conceptual Modelling agility emerges - manifested not only on model contents
level, but also on modelling method level. Depending on the questions that must
be answered and the systems that must be supported with modelling means, the
need for agility may stem from the degree of domain-speciﬁcity, from gradual
understanding of modelling possibilities, from evolving model-driven systems
etc. The hereby proposed Agile Modelling Method Engineering (AMME)
approach thus becomes necessary to extend the traditional perspective of “model‐
ling through standards”; consequently, the beneﬁts of repeatability and wide
adoption are traded for responsiveness to dynamic needs identiﬁed within an
Agile Enterprise.
Keywords: Agile Modelling Method Engineering · Metamodelling
Conceptual Modelling · Knowledge Management · Agile Enterprise
1
Introduction
A diﬀuse notion of Agile Enterprise has emerged in the literature, as an umbrella term
covering new challenges derived from increasingly dynamic needs that must be
addressed by evolving and responsive enterprise functions. Agility is generally deﬁned
in relation with change: “comprehensive response to the business challenges of proﬁting
from rapidly changing […] global markets” [1]; “[the agile enterprise is] built on policies
and processes that facilitate speed and change…” [2]. The requirement for agility is
raised both from a technical perspective (e.g., considering the high dynamics of para‐
digms such as Industry 4.0 [3] or the Internet of Things [4]) and from a managerial
perspective (e.g., Agile Manufacturing [5], Agile Knowledge Management [6]).
Consequently, speciﬁc challenges are also emerging for the paradigm of Conceptual
Modelling, considering the evolving nature of modelling needs with respect to various
functions within an Agile Enterprise. Modelling requirements reclaim ﬂexibility and
agility not only for model contents (already addressed in software engineering by the
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 3–19, 2018.
https://doi.org/10.1007/978-3-319-73459-0_1

Agile Modelling approach [7]), but also for the adopted modelling language, modelling
software and the encompassing modelling method (the relation between these will be
established in Sect. 3). A methodology and a new modelling paradigm are therefore
necessary to address the domain-speciﬁcity of the system to be modelled, as well as the
evolution of case-speciﬁc modelling requirements, for which standards may be insuﬃ‐
ciently ﬂexible.
The fields of Business Process Management (BPM), Enterprise Architecture Manage‐
ment (EAM), Model-driven Software Engineering (MDSE) and Knowledge Manage‐
ment (KM) – selected here as representative practices within an Agile Enterprise - have
traditionally relied on conceptual modelling standards for the benefits of repeatability and
reusability across domains. However, in the pursuit of the “Agile Enterprise” status, the
transformative effect of the Agile Manifesto [8] (originally advocated in the context of
MDSE) must also be considered for the practice of modelling method engineering in
general. Regardless whether a modelling method is subordinated to an Information
Systems engineering method or to various management and decision-making practices,
multiple factors may generate fluctuating requirements that should be addressed by agile
conceptualisation methodologies.
In support of this underlying need for agility, the framework of Agile Modelling
Method Engineering (AMME, initially outlined in [9]) is hereby proposed. In addition,
a community-oriented research environment - the Open Models Initiative Laboratory
(OMiLAB [10]) -, where the framework has been applied in several projects, will be
described. Two projects will be highlighted to emphasise the applicability of AMME:
(i) a research-oriented project addressing KM and EAM concerns (the ComVantage
method [11] and tool [12]) and (ii) an educational project for teaching MDSE and BPM
topics (the FCML method [13] deployed as the BEE-UP tool [14]).
The remainder of the paper is organised as follows: Sect. 2 will overview the key
motivating factors for modelling method agility, illustrated for the selected ﬁelds of
BPM, EAM, KM and MDSE. Section 3 will describe the key facets of modelling method
agility and the AMME framework. Section 4 will share experience and results with
applying AMME in projects that have been managed within the OMiLAB environment.
The paper ends with a summary and an outlook to future challenges for further consol‐
idating AMME as a method engineering paradigm.
2
Conceptual Modelling for the Agile Enterprise: A Selection
A selection of ﬁelds that are highly relevant for an Agile Enterprise are discussed here
as application areas for Conceptual Modelling, in order to motivate the relevance of
agile modelling methods with respect to their dynamic needs.
Conceptual Modelling for BPM is typically associated with popular languages such
as BPMN [15], EPC [16], UML activity diagrams [17] or various flowcharting predeces‐
sors that have emerged along the history of Enterprise Modelling. Petri Nets [18] became
a popular choice for formalisation concerns [19] (rather than a stakeholder-oriented
language). Figure 1 suggests a semantic spectrum that may be subject to evolving model‐
ling requirements: (i) at the “generic” end of the spectrum, UML activity diagrams may
4
D. Karagiannis

be used to describe any type of workflow (business processes, algorithms etc.), their
domain-specificity being commonly left to human interpretation; (ii) BPMN diagrams
narrow down semantics by fixing several concept specialisations (e.g., manual task, auto‐
mated task); (iii) at the right end of the spectrum, AMME was employed to semantically
enrich the Task concept with a “concept schema” comprising machine-readable proper‐
ties (e.g., different types of times, costs) that are relevant for decision-making or for simu‐
lation mechanisms required by stakeholders. Other BPM scenarios benefitting from
AMME include (i) notational heterogeneity - i.e., when multiple business process nota‐
tions co-exist and a semantic integration is required [20]; (ii) the extension of business
process models with conceptual patterns for semantic evaluations [21]; (iii) the customi‐
sation of processes for the specificity of product-service systems [22].
Fig. 1. A semantic spectrum for BPM concepts [13]
Conceptual Modelling for EAM also beneﬁts from various standards - e.g., Archi‐
mate [23], IDEF [24], or frameworks having a rather ontological scope without neces‐
sarily imposing diagrammatic designs (e.g., Zachman’s framework [25]). Typically,
EAM employs multi-perspective methods with viewpoints that can be instantiated in
various modelling views (see also ARIS [16, 26], BEN [27, 28] and MEMO [29, 30]
where the multi-perspective nature is emphasised). These may also be subjected to
modelling requirements that reclaim a gradual domain-speciﬁcity in the language or the
method itself (as shown in the case of BPM); another common requirement is for
semantic enablers to support decision-making mechanisms (commonly pertaining to
business-IT alignment challenges). For this, a minimal necessity is consistency manage‐
ment across viewpoints. Figure 2 shows a multi-view modelling tool for the SOM enter‐
prise modelling method [31], where changes in one model are required to propagate in
the others according to semantic overlapping and dependencies – AMME is called to
extend the method with consistency-preservation mechanisms that are tightly coupled
with the language vocabulary (diﬀerent approaches to multi-view modelling may also
be consulted in [32–35]).
Conceptual Modelling Methods
5

Fig. 2. Multi-view consistency challenges in enterprise modelling [36]
Conceptual Modelling for KM is less reliant on standard modelling languages, at
least when the focus is on management practices, rather than KM systems or knowledge
representation. The KM community is particularly concerned with knowledge processes
such as acquisition, externalisation and learning (also in the focus of an Agile KM
approach) and several key processes have been systematised in Nonaka’s seminal cycle
of knowledge conversion [37]. Figure 3 shows how this cycle may be extended when
employing Conceptual Modelling methods for knowledge representation. The following
phases are hereby proposed: (i) human-human socialisation corresponds to Nonaka’s
traditional “socialisation” phase; (ii) externalisation in raw form corresponds to Nona‐
ka’s traditional “externalisation” phase, if knowledge is captured in semi-structured
content (to be managed with content management system); (iii) externalisation in
diagrammatic-form is enabled by modelling methods that enable knowledge acquisition
through diagrammatic means (e.g., work procedures described in models rather than
natural language); (iv) combination corresponds to Nonaka’s traditional “combination”
phase, with additional opportunities for combining diagrammatic knowledge represen‐
tations; (v) internalisation at machine-level is enabled if the models are further exposed
as a knowledge base to model-driven systems; (vi) machine-to-human socialisation
would (potentially) be a socialisation variant where the “shared doing” involves a human
and a knowledge-driven system (e.g., robots). The challenge of AMME in this context
is to facilitate the knowledge acquisition with modelling means and tool support that are
adequate to the semantics deemed relevant for KM practices and systems. Other
approaches to the interplay between KM and modelling practices, based on business
process modelling as a facilitator, have been overviewed in [38].
6
D. Karagiannis

Fig. 3. An extended knowledge conversion cycle involving Conceptual Modelling
Conceptual Modelling for MDSE typically relies on modelling languages tailored
for software design and development – e.g., UML [17], ER [39]. A popular underlying
ambition is that of code generation, a task that depends on a ﬁxed and well-deﬁned
semantic space (hence an invariant modelling language amenable to standardisation).
Agile Modelling [7] is employed as a matter of quickly adapting model contents and
procedures rather than the governing language. AMME becomes relevant here by raising
the level of abstraction for MDSE agility, as it allows the propagation of change requests
to the language semantics and further to modelling functionality. This, of course, limits
the “modelling is programming” [40] possibilities (e.g., code generation); instead,
AMME is motivated by a “modelling is knowledge representation” perspective, with a
model base that drives “model-aware” run-time systems that are parameterised with
knowledge items (rather than generated). Figure 4 suggests an approach proposed by
the ComVantage project, where app orchestrations are derived from app requirements
captured in diagrammatic form, indicating the precedence of mobile app support along
a business process [41].
BPM, EAM, KM and MDSE are several ﬁelds that, under the hereby discussed
assumptions and driven by project-based requirements, have motivated the emergence
of AMME. The literature reports on several other approaches related to AMME in certain
aspects, however typically subordinated to MSDE goals and focusing on the domain
and case speciﬁcity aspect rather than the agility of the modelling method artefact – e.g.,
the notion of “Situational Methods” for Information Systems Engineering [42, 43], the
Domain-speciﬁc Modelling Language design methodology [44], extensibility mecha‐
nisms for standard languages [45]. Metamodelling environments such as [46–48] have
signiﬁcantly contributed to increasing the productivity of modelling tool implementa‐
tion, thus providing candidate environments for the rapid prototyping support needed
during an AMME deployment.
Conceptual Modelling Methods
7

3
The AMME Framework
The notion of Agile Enterprise opens a wider scope for agility than the one advocated
in agile software development and its conceptual dynamics must be captured in adequate
conceptualisation and engineering processes. A classiﬁcation of change drivers for an
Agile Enterprise is proposed here, as illustrated in Fig. 5:
– Changes in the business model and value proposition – e.g., shifting the value prop‐
osition towards the servitisation of existing products, a deeper specialization of prod‐
ucts reclaiming new domain-speciﬁc properties in design decisions;
– Changes in management strategy – e.g., shifting between diﬀerent KM approaches
or process improvement methods, reclaiming the inclusion of new properties in key
performance indicators;
– Changes in support technology and infrastructure – e.g., migration to a bring-your-
own-device strategy;
– Digitisation of assets – e.g., migration to new technological paradigms (Internet of
Things, Industry 4.0);
– Changes in the business context – e.g., market changes, reconﬁgurations of virtual
enterprises;
– Self-initiated changes – e.g., pro-active process re-engineering, adoption of a capa‐
bility-driven Enterprise Architecture [50];
– Normative changes – e.g., changes pertaining to legal or certiﬁcation compliance,
evolution of already adopted standards.
– Changes in the social eco-system – e.g., changes in user behaviour, in interactions
between users or between users and systems.
The enterprise performance, from an information and communication technology
viewpoint, is primarily supported by (i) Enterprise Information Systems (EIS) employed
at run-time (e.g., for enacting business processes and managing resources) and (ii) an
Enterprise Architecture (EA) supporting design-time decisions (e.g., business-IT
Fig. 4. Models for “model-aware information systems” (adapted from [49])
8
D. Karagiannis

alignment). Conceptual Modelling practices traditionally support both facets: they can
enable the deployment of model-based EIS as part of some IS engineering method; they
can also enable the accumulation of a Knowledge Base in conceptual model form. In
both cases, modelling activities must be supported by a modelling method and adequate
tooling – i.e., modelling software that supports communication, sense-making, the accu‐
mulation of knowledge assets or analytical system designs etc.
Fig. 5. The role of AMME in the Agile Enterprise
For this purpose, various model-based management and engineering practices typi‐
cally employ available standards or well-established languages and methods. These
bring inherent governance beneﬁts (e.g., repeatability, compatibility) – however, the
general assumption for adopting such methods is that modelling requirements are ﬁxed
and a standards-oriented modelling culture can be uniformly established within the
enterprise and for its application domain. The hereby discussed AMME framework is
motivated by the assumption that modelling requirements evolve due to one or more of
several factors:
– users become gradually familiar with modelling possibilities;
– richer semantics become necessary, either for design-time (e.g., decision-support) or
run-time use cases (e.g., model-driven systems);
Conceptual Modelling Methods
9

– stakeholders gain gradual insight and common understanding of the application
domain, of the properties that are relevant to the model abstractions.
Under these assumptions, the Agile Modelling Method Engineering (AMME)
approach (providing several qualities suggested in Fig. 5) becomes necessary and the
beneﬁts of standards may be traded for other beneﬁts - e.g., gradual domain-speciﬁc
enrichment of the modelling language, in-house evolution of model-aware systems.
Agility, as understood by AMME from an internal perspective, has two basic mani‐
festations: (i) artefact agility is enabled by the decomposition of a modelling method
into building blocks that deﬁne the backlog items to be managed through agile engi‐
neering eﬀorts; and (ii) methodological agility manifests in the engineering process
itself, taking the form of an incremental and iterative spiralling development.
Artefact agility is enabled by the deﬁnition of a modelling method. The artefact
created by AMME was originally deﬁned in [51] in terms of its building blocks (Fig. 6):
– A modelling language further decomposed in notation (graphical symbols corre‐
sponding to the language concepts), syntax (the language grammar and associated
syntactic constraints) and semantics (language vocabulary, machine-readable prop‐
erties of each concept, associated semantic constraints);
– Mechanisms and algorithms cover the model-based functionality to be made avail‐
able in a modelling tool – either generic (applicable to models of any type), speciﬁc
(applicable only to models of a speciﬁc type) or hybrid (applicable to a limited set
of model types that fulﬁl speciﬁc requirements);
– A modelling procedure consists of the modelling activities to be performed in order
to reach modelling goals; it may take the form of method documentation or may be
supported by mechanisms aiming to improve user experience (e.g., by automating
certain procedure steps).
Fig. 6. The modelling method building blocks [51]
Methodological agility is enabled by an iterative engineering process at the core of
the AMME framework and depicted in Fig. 7. This process is generically named the
10
D. Karagiannis

“Produce-Use” cycle, with two phases per iteration: (i) the Produce step will capture
domain knowledge (“models of concepts”), formalise it and deploy it in a modelling
tool; (ii) the Use step will employ this modelling tool to capture case knowledge that
instantiates the domain concepts (“models using concepts”) while also evaluating
acceptance and various quality criteria to feed back in the next iteration of the Produce
phase.
Fig. 7. The AMME framework (adapted from [9])
This cycle may be conveniently specialised for diﬀerent contexts and deployments.
The assumption is that diﬀerent instances will be necessary depending of the require‐
ments to the conceptualisation process. The “AMME Lifecycle” described in Fig. 8
shows how a concrete instance of the conceptualisation process is realised within the
Open Models Laboratory (OMiLAB), comprising several phases:
– Create: a mix of knowledge acquisition and requirements elicitation techniques;
– Design: the design of modelling method building blocks depicted in Fig. 6;
– Formalise: reﬁnements of the method design in terms of appropriate formalisms, to
supporting implementations across various platforms by removing ambiguities from
the method design speciﬁcation;
– Develop: the modelling tool development phase, typically beneﬁtting from rapid
prototyping environments (e.g., [46]);
– Deploy/Validate: the packaging and deployment of the tool with improved user
experience and an evaluation protocol that feeds back into the Create step of the next
iteration.
Conceptual Modelling Methods
11

Feedback loops occur both internally, between subsequent phases, and for the overall
cycle, as each deployment collects change requests for the next method increments.
The Produce-Use cycle interacts, at the method “front-end”, with (i) the enterprise
environment by assimilating requirements and domain knowledge; and, at the method
“back-end”, with (ii) an asset repository where lessons learned, method fragments and
various reusable assets are accumulated for future deployments.
Fig. 8. The AMME Lifecycle
4
Project Experience and Results
4.1
The Open Models Initiative Laboratory
The Open Models Initiative Laboratory (OMiLAB) [10] is a research environment (both
physical and virtual) that fosters a global community of researchers sharing a common
understanding of the concept of modelling method and of models value. OMiLAB may
be considered an instance deployment of AMME, providing speciﬁc enablers. A number
of domain-speciﬁc or hybrid modelling methods and their deployments (tools) have been
developed in projects of diﬀerent kinds (i) educational (e.g., modelling tools for didactic
purposes), (ii) research-oriented (i.e., results of metamodelling tasks in research
projects) and (iii) digitisation-oriented (i.e., typically follow-up developments of
research projects). A selection of such methods are presented in [52] - a ﬁrst volume in
a planned community-driven book series, reporting on projects that beneﬁt from the
OMiLAB enablers and its collaborative network.
Additionally, community-oriented events have established forums for dissemination
or knowledge transfer between academic research, industry and education. The most
prominent event is NEMO (Next-generation Enterprise Modelling) – an annual summer
school [53] where the principles and framework of AMME have been initially articulated
and students have received initial training with its application. Currently OMiLAB has
European and Asian “collaboratories”, as well as Associated Organisations fostering
localised communities. An organisational structure and related management policies
(e.g., for intellectual property rights) may be consulted in [54].
One key enabler provided by AMME is ADOxx - the rapid prototyping platform for
developing and deploying modelling tools [46]. Its meta-metamodel provides built-in
12
D. Karagiannis

facilities for developing the building blocks of a modelling method – e.g. a design envi‐
ronment for the language grammar and vocabulary, a vector graphics language for
dynamic notations, a scripting language for developing model-driven functionality. In
addition, a richness of plug-ins and ancillary development services and reusable items
are made available through the OMiLAB portal. Research is underway regarding MM-
DSL, a platform-independent declarative language for modelling method deﬁnitions –
an initial version was presented in [55].
4.2
The ComVantage Research Project
ComVantage, an FP7 European Project [56], proposed an IT architecture based on
mobile app ensembles consuming Linked Enterprise Data shared among organisations,
in support of collaborative business processes for various application areas (e.g.,
customised production, mobile maintenance) [57]. The run-time architecture was
complemented with design-time support in the form of the evolving ComVantage
modelling method, a process-centred enterprise modelling method tailored for the
domain-speciﬁcity of the project application areas, for the goal of establishing a knowl‐
edge repository in diagrammatic form (hence supporting KM and EAM).
Various semantic lifting approaches were applied to unify heterogeneous data
sources in a Linked Data cloud, from which front-end apps can retrieve them through
protocols established by the Linked Data technological space [58]. An RDFizer mech‐
anism was implemented to also expose the diagrammatic contents in Linked Data form
Fig. 9. Enterprise models for semantic lifting (as proposed by the ComVantage method)
Conceptual Modelling Methods
13

[59, 60], thus contributing to the knowledge processes proposed in Fig. 3 and opening
new opportunities of semantic lifting (as suggested in Fig. 9).
Consequently, requirements on client applications would inherently propagate to
requirements for the modelling method, reclaiming an AMME approach to evolve it
accordingly, and to ensure that a suﬃcient semantic space is available to clients.
By the end of the project, the modelling method reached a Zachman-style multi-
viewpoint structure address various aspects (perspectives) and scopes (levels of domain-
speciﬁcity) as reﬂected in Table 1. Multiple sources may be consulted for the method
documentation [11, 61–63]. Details on the method’s conceptual evolution with respect
to AMME are available in [49]. The modelling tool is hosted by OMiLAB at [12].
Table 1. Viewpoints of the ComVantage method [32]
Scopes
Aspects
Behavioural aspect
Structural aspect
Procedural Views
(procedural
knowledge captured in
the form of ﬂowcharts
with varying
semantics and
customised notation)
Collaborative Views
(the same kind of
knowledge, expressed
as interactions in
order to highlight
necessary interfaces)
Motivator Views
(structural
descriptions of the
commodities oﬀered
by the enterprise)
Participants Views
(structural
descriptions of
available or required
resources, liable
entities and their
capabilities)
Business/Enterprise
Models that describe a
business on its highest
level of business
process abstraction
(e.g., abstract value
creation processes)
Models that describe
value exchanges
between entities that
participate in the
business model or in
an enterprise-level
process
Models that describe
the values that are
created by the business
or by each process in
particular
Models that describe
actors and roles
involved in the
business model,
including their
business capabilities
Requirements
Models that describe
how work processes
are mapped on
requirements for
diﬀerent kinds of
resources
Models that describe
how diﬀerent
resources must
interact based on their
mappings on work
processes
Models that describe
required and available
resources
App execution
Models that describe
how mobile apps must
be “orchestrated”
(chained) according to
the ﬂow of the process
they must support
Models that describe
how mobile apps must
interact according to
the ﬂow of the process
they must support
Models that describe
mobile apps that are
required and must be
“orchestrated” to
support a process
App design
Models that describe
the ﬂow of interactions
between a user and
elements of an app’s
user interface
Models that describe a
navigational map
across required app
features
Models that describe
the features and data
requirements for a
mobile app
4.3
The FCML/BEE-UP Educational Project
FCML (Fundamental Conceptual Modelling Languages) is a teaching-oriented model‐
ling method providing a hybridisation of 5 well-known modelling languages: BPMN,
EPC, ER, UML and Petri Nets. Their initials form the acronym BEE-UP which is the
name of the modelling prototype made available through OMiLAB for teaching
14
D. Karagiannis

purposes, already adopted for teaching MDSE and BPM topics by several universities
associated with the OMiLAB collaboration network. Details on the FCML method can
be consulted in [13], only a brief overview is provided here:
FCML is not only a convenience tool that supports model types belonging to diﬀerent
languages. It also agilely assimilated semantic integration, extensions and functionality
to address modelling requirements for various teaching scenarios subordinated to BPM
(e.g., process path simulation, Petri Nets simulation) or MDSE (e.g., SQL code gener‐
ation):
– In the “mechanisms” building block, all three types of mechanisms are exempliﬁed:
(i) generic (e.g., model queries or diagram exports in the form of RDF knowledge
graphs), (ii) speciﬁc (e.g., SQL generation from ER diagrams, Petri Nets simulation/
stepping), (iii) hybrid (applicable to diﬀerent types of models complying to some
well-formedness requirements – e.g., process path analysis for models that correctly
use the basic workﬂow patterns, i.e. BPMN, EPC, UML activity diagrams – as
suggested in Fig. 10);
Fig. 10. Hybrid workload simulation mechanism on EPC and BPMN process models (adapted
from [13])
– In the “language” building block, semantic extensions are applied to support these
mechanisms: (i) at language level (e.g., an organigram model type to support work‐
load simulations); (ii) at model type level, (e.g., EPC extensions to support multiple
variants of EPC recommended in the literature, depending on their goal and required
rigor); (iii) at concept level (e.g., user-editable and machine-readable properties such
Conceptual Modelling Methods
15

as costs, times, probabilities to support process path simulations, SQL-speciﬁc
properties to support SQL code generation);
– The “modelling procedure” component is aligned accordingly to guide users in how
to create models for the diﬀerent scenarios.
FCML and its BEE-UP implementation enable a multi-purpose and multi-layered
modelling approach, providing on one hand notational alternatives for BPM and, on the
other hand, a complementary set of languages for teaching MDSE topics. The modelling
tool is hosted by OMiLAB at [14].
5
Summary and Future Challenges
The relevance of Conceptual Modelling to selected ﬁelds of research and management
practices – namely, Business Process Management, Enterprise Architecture Manage‐
ment, Knowledge Management (Systems) and Model-driven Software Engineering –
was hereby discussed. A common underlying requirement for modelling method agility
was highlighted and the AMME framework was proposed as a complement to standard
methods, which are typically considered invariants in agile development practices. Thus,
the work at hand raises the level of agility from that of software engineering to that of
modelling method engineering - even in the case of MDSE, where agility is advocated
here in relation to generic “conceptual model”-awareness concerns (rather than
standard-driven code generation). Experiences and results accumulated through the
Open Models Initiative Laboratory research environment validate the applicability of
AMME. The current experience is based on several project-based deployments and
further enablers must be developed, similarly to how the agile software development
practices have been emerging as a community-driven paradigm.
Several key enablers that must further consolidate the AMME vision are suggested
here as open challenges for which research is already under way: (i) an executable
declarative modelling language for coding modelling method deﬁnitions in a platform-
independent manner; (ii) interoperability mechanisms at meta2model level between the
popular metamodelling platforms; (iii) specialised issue tracking platforms considering
the speciﬁc characteristics of modelling methods as Design Science artefacts.
References
1. Goldman, S., Naegel, R., Preiss, K.: Agile Competitors and Virtual Organizations: Strategies
for Enriching the Customer. Wiley, New York (1994)
2. The Business Dictionary: The Agile Enterprise definition. http://www.businessdictionary.com/
definition/agile-enterprise.html
3. Schwab, K.: The Fourth Industrial Revolution. Crown Business, New York (2017)
4. Internet of Things Global Standard Initiative. http://www.itu.int/en/ITU-T/gsi/iot
5. Levy, M., Hazzan, O.: Agile knowledge management. In: Encyclopedia of Information
Science and Technology, pp. 112–117. IGI Global (2008)
6. Gunasekaran, A.: Agile Manufacturing: The 21st Century Competitive Strategy. Elsevier,
Amsterdam (2001)
16
D. Karagiannis

7. Ambler, S.W.: Agile Modeling: Eﬀective Practices for Extreme Programming and the Uniﬁed
Process. Wiley, New York (2002)
8. The Agile Manifesto. http://www.agilemanifesto.org
9. Karagiannis, D.: Agile modelling method engineering. In: Karanikolas, N., Akoumianakis,
D., Mara, N., Vergados, D., Michalis, X. (eds.) Proceedings of the 19th Panhellenic
Conference on Informatics, pp. 5–10. ACM (2015)
10. The Open Models Initiative Laboratory portal. http://omilab.org
11. Buchmann, R.A.: Modeling product-service systems for the Internet of Things: the
ComVantage method. In: Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-
Speciﬁc Conceptual Modelling, pp. 417–437. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-39417-6_19
12. The ComVantage project page in the OMiLAB portal. http://austria.omilab.org/psm/content/
comvantage
13. Karagiannis, D., Buchmann, R.A., Burzynski, P., Reimer, U., Walch, M.: Fundamental
conceptual modeling languages in OMiLAB. In: Karagiannis, D., Mayr, H.C., Mylopoulos,
J. (eds.) Domain-Speciﬁc Conceptual Modelling, pp. 3–30. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-39417-6_1
14. The Bee-Up project page in the OMiLAB portal. http://austria.omilab.org/psm/content/bee-
up/info
15. Object Management Group, The oﬃcial BPMN speciﬁcation. http://www.bpmn.org
16. Scheer, A.W.: ARIS-Vom Geschäftsprozess zum Anwendungssystem. Springer, Heidelberg
(2002). https://doi.org/10.1007/978-3-642-56300-3
17. Object Management Group, The oﬃcial UML resource page. http://www.uml.org
18. Petri, C.A., Reisig, W.: Petri net. Scholarpedia 3(4), 6477 (2008). https://doi.org/10.4249/
scholarpedia.6477
19. van der Aalst, W.M.P.: Formalization and veriﬁcation of event-driven process chains. Inf.
Softw. Technol. 41(10), 639–650 (1999)
20. Prackwieser, C., Buchmann, R., Grossmann, W., Karagiannis, D.: Overcoming heterogeneity
in business process modeling with rule-based semantic mappings. Int. J. Softw. Eng. Knowl.
Eng. 24(8), 1131–1158 (2014)
21. Fill, H.G.: Semantic evaluation of business processes using SeMFIS. In: Karagiannis, D.,
Mayr, H.C., Mylopoulos, J. (eds.) Domain-Speciﬁc Conceptual Modelling, pp. 149–170.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6_7
22. Boucher, X., Medini, Kh., Fill, H.G.: Product-service-system modeling method. In:
Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-Speciﬁc Conceptual Modelling,
pp. 455–484. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6_21
23. The Open Group, ArchiMate® 2.1 Speciﬁcation. http://www.opengroup.org/archimate
24. IEEE, IEEE Standard for Functional Modeling Language – syntax and semantics of IDEF0,
IEEE std. 1329.1-1998
25. Zachman, J.A.: A framework for information systems architecture. IBM Syst. J. 26(3), 276–
292 (1987)
26. Software AG, ARIS – the community page. http://www.ariscommunity.com
27. Aier, S., Kurpjuweit, S., Saat, J., Winter, R.: Business engineering navigator: a “Business to
IT” approach to enterprise architecture management. In: Bernard, S., Doucet, G., Gotze, J.,
Saha, P. (eds.) Coherency Management: Architecting the Enterprise for Alignment, Agility,
and Assurance, pp. 77–89. Author House, Bloomington (2009)
28. Winter, R.: Business Engineering Navigator. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-15913-8
Conceptual Modelling Methods
17

29. Frank, U.: Multi-perspective enterprise modeling (MEMO) – conceptual framework and
modeling languages. In: Proceedings of HICSS-35, pp. 1258–1267. IEEE (2002)
30. Bock, A., Frank, U.: Multi-perspective enterprise modeling – conceptual foundation and
implementation with ADOxx. In: Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-
Speciﬁc Conceptual Modelling, pp. 241–268. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-39417-6_11
31. Ferstl, O.K., Sinz, E.J.: Modelling of business systems using SOM. In: Bernus, P., Mertins,
K., Schmidt, G.J. (eds.) Handbook on Architectures of Information Systems, pp. 347–367.
Springer (2006)
32. Karagiannis, D., Buchmann, R.A., Bork, D.: Managing consistency in multi-view enterprise
models: an approach based on semantic queries. In: Paper 53, Proceedings of ECIS 2016.
Association for Information Systems (2016)
33. Bork, D.: Using conceptual modelling for designing multi-view modelling tools. In:
Proceedings of the AMCIS 2015. Association for Information Systems (2015)
34. Fertsl, O.K., Sinz, E.J., Bork, D.: Tool support for the semantic object model. In: Karagiannis,
D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-Speciﬁc Conceptual Modelling, pp. 291–312.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6_13
35. Jeusfeld, M.A.: SemCheck: checking constraints for multi-perspective modeling languages.
In: Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-Speciﬁc Conceptual
Modelling, pp. 31–54. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6_2
36. The SOM project page in the OMiLAB portal. http://www.omilab.org/web/som
37. Nonaka, I.: The knowledge-creating company. Harvard Bus. Rev. 69, 96–104 (1991)
38. Karagiannis, D., Woitsch, R.: Knowledge engineering in business process management. In:
vom Brocke, J., Rosemann, M. (eds.) Handbook on Business Process Management 2. IHIS,
pp. 623–648. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-642-45103-4_26
39. Chen, P.: The entity-relationship model - toward a uniﬁed view of data. ACM Trans. Database
Syst. 1(1), 9–36 (1976). ACM
40. Aquino, N., Vanderdonckt, J.I., Panach, J.I., Pastor, O.: Conceptual modelling of interaction.
In: Embley, D.W., Thalheim, B. (eds.) Handbook of Conceptual Modeling: Theory, Practice
and Research Challenges, pp. 335–358. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-15865-0_10
41. Ziegler, J., Graube, M., Pfeﬀer, J., Urbas, L.: Beyond app-chaining: mobile app orchestration
for eﬃcient model driven software generation. In: Cyganek, B., Nolte, Th. (eds.) Proceedings
of EFTA 2012, pp. 1–8. IEEE (2012)
42. Welke, R.J., Kumar, K.: Methodology engineering: a proposal for situation-speciﬁc
methodology construction. In: Cotterman, W., Senn, J. (eds.) Challenges and Strategies for
Research in Systems Development, pp. 257–269. Wiley, Chichester (1992)
43. Henderson-Seller, B., Ralyte, J., Agerfalk, P., Rossi, M.: Situational Method Engineering.
Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-41467-1
44. Frank, U.: Domain-Specific modelling languages: requirements analysis and design guidelines. In:
Reinhartz-Berger, I., Sturm, A., Clark, T., Cohen, S., Betin, J. (eds.) Domain Engineering, pp.
133–157. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36654-3_6
45. Object Management Group, UML Superstructure Speciﬁcation. http://www.omg.org/cgi-
bin/doc?formal/05-07-04
46. BOC GmbH: The ADOxx metamodelling platform – reference webpage. http://www.adoxx.org/
live
18
D. Karagiannis

47. Kelly, S., Lyytinen, K., Rossi, M.: MetaEdit+ a fully conﬁgurable multi-user and multi-tool
CASE and CAME environment. In: Bubenko, J., Krogstie, J., Pastor, O., Pernici, B., Rolland,
C., Solvberg, A. (eds.) Seminal Contributions to Information Systems Engineering, pp. 109–
129. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36926-1_9
48. Budinsky, F., Steinberg, D., Merks, E., Ellersick, R., Grose, T.J.: Eclipse Modeling
Framework. The Eclipse Series. Addison Wesley, Reading (2004)
49. Buchmann, R.A., Karagiannis, D.: Agile modelling method engineering: lessons learned in the
ComVantage research project. In: Ralyté, J., España, S., Pastor, Ó. (eds.) PoEM 2015. LNBIP,
vol. 235, pp. 356–373. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-25897-3_23
50. Loucopoulos, P., Kavakli, E.: Capability-oriented enterprise knowledge modeling: the CODEK
approach. In: Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.) Domain-Specific Conceptual
Modelling, pp. 197–216. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6_9
51. Karagiannis, D., Kühn, H.: Metamodelling platforms. In: Bauknecht, K., Tjoa, A.M.,
Quirchmayr, G. (eds.) EC-Web 2002. LNCS, vol. 2455, p. 182. Springer, Heidelberg (2002).
https://doi.org/10.1007/3-540-45705-4_19
52. Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.): Domain-Specific Conceptual Modelling:
Concepts. Methods and Tools. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-39417-6
53. Next-generation Enterprise Modelling (NEMO) Summer School series – oﬃcial website.
http://nemo.omilab.org
54. Gotzinger, D., Miron, E.T., Staﬀel, F.: OMiLAB: an open collaborative environment for
modeling method engineering. In: Karagiannis, D., Mayr, H.C., Mylopoulos, J. (eds.)
Domain-Speciﬁc Conceptual Modelling, pp. 55–78. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-39417-6_3
55. Visic, N., Fill, H.-G., Buchmann, R., Karagiannis, D.: A domain-speciﬁc language for
modelling method deﬁnition: from requirements to grammar. In: Rolland, C.,
Anagnostopoulos, D., Loucopoulos, P., Gonzalez-Perez, C. (eds.) Proceedings of RCIS 2015,
pp. 286–297. IEEE (2015)
56. ComVantage Consortium, Public project deliverables page. http://www.comvantage.eu/
results-publications/public-deriverables
57. Münch, T., Buchmann, R., Pfeﬀer, J., Ortiz, P., Christl, C., Hladik, J., Ziegler, J., Lazaro, O.,
Karagiannis, D., Urbas, L.: An innovative virtual enterprise approach to agile micro and SME-
based collaboration networks. In: Camarinha-Matos, L.M., Scherer, R.J. (eds.) PRO-VE
2013. IAICT, vol. 408, pp. 121–128. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-40543-3_13
58. Heath, T., Bizer, C.: Linked Data: Evolving the Web into a Global Data Space, 1st edn.
Morgan & Claypool, San Rafael (2011)
59. Karagiannis, D., Buchmann, R.A.: Linked open models: extending linked open data with
conceptual model information. Inf. Syst. 56, 174–197 (2016)
60. Buchmann, R.A., Karagiannis, D.: Enriching linked data with semantics from domain-
speciﬁc diagrammatic models. Bus. Inf. Syst. Eng. 58(5), 341–353 (2016)
61. Buchmann R.A.: Conceptual modeling for mobile maintenance: the ComVantage case. In:
Proceedings of HICSS-47, pp. 3390–3399. IEEE (2014)
62. Buchmann, R.A., Karagiannis, D.: Modelling mobile app requirements for semantic
traceability. Requir. Eng. 22, 41–75 (2015). https://doi.org/10.1007/s00766-015-0235-1
63. Buchmann, R.A., Karagiannis, D.: Domain-speciﬁc diagrammatic modelling: a source of
machine-readable semantics for the internet of things. Cluster Comput. 20, 895 (2016). https://
doi.org/10.1007/s10586-016-0695-1
Conceptual Modelling Methods
19

Distributed Systems

Optimizing Service Level Agreements
in Peer-to-Peer Supply Chain Model
for Complex Projects Management
Florina Livia Covaci(B)
Business Informations Systems Department, “Babes-Bolyai” University,
Cluj-Napoca, Romania
florina.covaci@ubbcluj.ro
Abstract. The focus of this paper is to ﬁnd appropriate approaches
to facilitate end-to-end SLA (Service Level Agreements) in complex
projects environments using Peer-to-Peer Supply Chain Model, to estab-
lish and enforce service levels between each pair of component con-
sumer/provider, so that the overall project requirements can be achieved
at the best utility value (SLA). The Supply Chain Formation problem is
described in terms of a directed acyclic graph where the nodes are rep-
resented by the component suppliers/consumers. Intelligent agents send
messages in the name of component suppliers/consumers on three con-
straints (scope, time, cost) giving raise to SLAs. The SLAs are expressed
as utility functions and it is concluded that in complex projects sce-
nario where the graph is always a tree the proposed model will converge
to the optimal solution and the best utility value will be propagated
autonomously across all component providers within the project envi-
ronment.
Keywords: Peer-to-Peer Supply Chain Model · Project management
Service Level Agreements
1
Introduction
In complex projects scenarios the project outcome may be the result of sev-
eral deployed components/services, each of which autonomously manages diverse
constraints which determine the quality criteria of the project outcome. In such
a scenario where the component availability is dynamic, guaranteeing speciﬁc
quality criteria of the ﬁnal outcome is a real challenge. The quality of the ﬁnal
service/product delivered to the customer is strongly aﬀected by those compo-
nents/services employed to compose it. If just one of the composing services vio-
lates the quality criteria, the global quality delivered to the customer might get
deﬁnitively compromised, to the detriment of the ﬁnal customer. There is need
for an automated mechanism that enables the negotiation of the project manage-
ment Iron triangle [1] constraints for complex cooperative projects. Our present
c
⃝Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 23–37, 2018.
https://doi.org/10.1007/978-3-319-73459-0_2

24
F. L. Covaci
work tackles the Supply Chain Formation (SCF) problem in the context of auto-
mated negotiation for complex projects environments, where a component can
be both a service provider and a service consumer in diﬀerent negotiation con-
texts at the same time. I am using the notion of Service Level Agreement (SLA)
in order to deﬁne and ensure the quality criteria between the customers and the
suppliers in the supply chain. A SLA represents a written agreement between
the customer and supplier with emphasis on a mutually beneﬁcial agreement and
deﬁnes the key quality indicators of the service provided [2]. SLAs are funda-
mental for any kind of supply chain and assume a central position in popular IT
service management standards such as ITIL (www.itil.co.uk). The Supply Chain
Formation (SCF) problem has been widely studied by the multi-agent systems
community. Numerous contributions can be found in the literature where par-
ticipants are represented by computational agents [6–13]. These computational
agents act in behalf of the participants during the SCF process. However, all
the previous work regarding Supply Chain Formation (SCF) problem is dealing
only with cost during SCF process. Our work proposes a multi-issue approach
that introduces utility functions in order to model the interdependence between
these issues. At each moment during the supply chain formation process, the
agent wants maximizing its expected utility function. Each participating agent
is sending messages under three constraints: scope, cost and time and is inter-
ested in obtaining those contract values that maximize their utility functions.
Negotiation ﬁnishes with a contract (the SLA) that is composed of the actual
quality criteria of the component provided by the supplier. In order to optimize
the overall SLA in a complex project environment so that the overall project
requirements can be achieved at the best utility value, we propose an automated
supply chain formation model based on the RB-Loopy Belief Propagation mes-
sage passing mechanism [3]. The present paper is structured as follows: Sect. 2
provides the background for SCF problem, Sect. 3 describes our proposed model
for optimizing SLA in a P2P supply chain for a complex project scenario, Sect. 4
evaluates the proposed model and ﬁnally Sect. 5 provides conclusions and future
work.
2
Background
The Supply Chain Formation (SCF) problem has been widely studied by the
multi-agent systems community. Numerous contributions can be found in the
literature where participants are represented by computational agents [6–13].
These computational agents act in behalf of the participants during the SCF
process [14]. By employing computational agents it is possible to form SCs in a
fraction of the time required by the manual approach [15].
The majority of the literature on SCF involve modeling the supply chain as
a network of auctions, with ﬁrst and second-price sealed bid auctions, double
auctions and combinatorial auctions among the most frequently-used methods.
SCF through auctions is a popular approach because auctions are frequently
used in real-world tendering and sales situations, they are often able to form

Optimizing Service Level Agreements in Supply Chains for Complex Projects
25
good solutions to the SCF problem, and some auctions are able to guarantee
various desirable game-theoretic properties.
Walsh and Wellman [9] proposed a market protocol with bidding restrictions
referred to as simultaneous ascending (M+1)st price with simple bidding (SAMP-
SB), which uses a series of simultaneous ascending double auctions. SAMP-SB was
shown to be capable of producing highly-valued solutions which maximize the dif-
ference between the costs of participating producers and the values obtained by
participating consumers over several network structures, although it frequently
struggled on networks where competitive equilibrium did not exist. The authors
also proposed a similar protocol, SAMP-SB-D, with the provision for decommit-
ment in order to remedy the inefciencies caused by solutions in which one or more
producers acquire an incomplete set of complementary input goods and are unable
to produce their output good, leading to negative utility. This use of a post alloca-
tion decommitment stage was recognized as an imperfect approach, however, due
to the possible problems created by rendering the results of auctions as nonbinding.
Although auctions and negotiations are by far the most commonly-employed
techniques in agent-based approaches to the SCF problem, there are some
approaches in the past years that make use of graphical models and inference
algorithms to tackle SCF and related problems. Next we will brieﬂy review them.
2.1
Max-Sum
Max-sum [16] is a message passing algorithm that can ﬁnd approximate solu-
tions to optimization problems and it translates the problem into a factor graph.
Max-sum has shown good empirical performance in a wide range of multi-agent
systems coordination scenarios [17–21]. Max-sum provides an approximate solu-
tion for the problem of maximizing a function that decomposes additively in
three steps. First, it maps the problem into a structure called local term graph.
Then it iteratively changes messages between vertices of that graph. Finally, it
determines the states of the variables.
Having X = {x1, ..., xn} be a sequence of variables, with each variable xi
taking states in a ﬁnite set Di known as its domain. The joint domain DX is
the cartesian product of the domain of each variable. xi refers to a possible state
of xi, that is xiϵDi. Moreover, X it is used to refer to a possible state for each
variable in X, that is XϵDX. Given a sequence of variables Xf ⊆X, a local
term f is a function
f : DXf −> R
(1)
Xf is the scope of f, and Xf is a possible state for each variable in Xf.
Finally, a term whose scope is a single variable is said to be a simple term, and
a term whose scope is two or more variables is said to be a composite term. A
function
g : DX−> R
(2)
is said to decompose additively if it can be broken as a sum of local terms. That is,
whenever there is a set of local terms F (referred to as the additive decomposition
F) such that g(X) = 
fϵF f(Xf) the problem of maximizing a function that

26
F. L. Covaci
decomposes additively can be expressed as follows: g(X) = 
fϵF f(Xf) subject
to: xiϵDi, ∀iϵ{1, ..., n}.
In order to maximize a function g that decomposes additively max-sum is map-
ping an additive decomposition F of g into a graph which is called the local term
graph. The local term graph LTG is simply a specialization of the local domain
graph deﬁned in [22]. Each vertex of the LTG is, as its name suggests, a local term
in an additive decomposition F of the objective function g. An edge between two
terms in the LTG means that these two terms share one variable and that they
are willing to exchange information about the variable they share. For each vertex
vϵ LTG, fv represents its associated term. Vertices associated with simple terms
are refered as simple vertices vertices associated to composite terms are refered as
composite vertices. In general, the local term graph used by max-sum is built from
F as follows. First, for each variable xi, a simple vertex is created, associating with
it a term (fxi) that is the addition of every simple term in F whose scope is xi.
Second, for each composite term fj, a composite vertex is created. The simple ver-
tex is labeled xi associated to variable xi and fj the composite vertex associated
to composite term fj. Finally, each composite vertex is connected to the simple
vertex of each of the variables in its scope. Notice that, since composite vertices
are only connected to simple ones and vice versa, any pair of connected vertices
in this graph share a single variable, the one corresponding to the simple vertex.
The factor graph is a bipartite graph with two disjoint sets corresponding to the
set of simple vertices and the set of composite vertices. After an additive decompo-
sition of the function to maximize has been mapped into a local term graph, max-
sum proceeds to the second step by iteratively exchanging messages over that local
term graph. Each vertex of the local term graph is in charge of receiving messages
from its neighbors, composing new messages and sending them to its neighbors.
Therefore, there will be messages exchanged from simple vertices xi to composite
vertices fj and vice versa. The message exchanged between a pair of vertices is a
vector of real numbers, one for each possible state of the variable shared by both
vertices. The exchange of messages continues until a convergence criterion is met.
Initially, each vertex v will initialize the message from each of its neighbors w to
zeros. After that, for each neighbor w, it will assess message μw
v send the message to
the corresponding neighbor, and receive the message μv
w from vertex w. The pro-
cedure above, is repeated until convergence or a maximum number of iterations is
reached. Max-sum is said to have converged after none of the messages change from
one iteration to another [23]. A slightly less stringent criterion for convergence is
to stop max-sum after the preferred states for the variables do not change from
one iteration to another [17]. This second criterion is useful for instances in which
the preferred state of the variables converges but the messages marginally change
at each iteration. After the message exchange ends, the step three determines the
states of the variables.
2.2
Loopy Belief Propagation (LBP)
LBP is the ﬁrst peer to peer approach that has been used to solve the SCF
problem in a decentralized manner [4,12,13,24]. In [4], an LBP-based approach

Optimizing Service Level Agreements in Supply Chains for Complex Projects
27
was applied to the SCF problem, noting that the passing of messages in LBP
is comparable to the placing of bids in standard auction-based approaches. The
decentralized and distributed nature of LBP also allows for the avoidance of
the scalability issues present in centralized approaches such as combinatorial
auctions. LBP is a decentralized and distributed approximate inference scheme
involving the application of Pearls belief propagation algorithm [5] to graphical
models containing cycles. It uses iterative stages of message passing as a means
for estimating the marginal probabilities of nodes being in given states: at each
iteration, each node in the graph sends a message to each of its neighbors giving
an estimation of the senders beliefs about the likelihoods of the recipient being
in each of its possible states. Nodes then update their beliefs about their own
states based upon the content of these messages, and the cycle of message pass-
ing and belief update continues until the beliefs of each node become stable. The
work in [12] shows that the SCF problem can be cast as an optimization problem
that can be eﬃciently approximated using max-sum algorithm [16] presented in
the section above. Thus, the authors oﬀer the means of converting a SCF prob-
lem into a local term graph, on which max-sum can operate. In LBP, the SCF
problem is represented by a model in which each of the participants decisions
is encoded in single variable. The states of each variable encode the individual
decisions that the participant needs to make regarding her exchange relation-
ships plus an inactive state. Moreover, the activation cost for a participant p is
encoded by means of a simple term fp, also called activation term. Each of these
activation terms has the participants variable as its scope and takes value zero
for the inactive state and the activation cost for any of the active states. In order
to ensure that decisions are consistent among participants, in LBP, there is a
compatibility term for each pair of variables representing potential partners. A
compatibility term fp1p2 encodes the compatibility between the decisions of the
two participants p1 and p2. Two participants are in incompatible states when-
ever one of them is willing to trade with the other, but the other one does not. If
two states are compatible, the value of the compatibility term is zero, otherwise
is negative inﬁnity. Hence LBP maps the SCF problem into a set of participant
variables X = {x1, ..., xN}, a set of activation terms FA = {f1, ..., fN}, one per
variable, and a set F of compatibility terms. Then, solving the SCF problem
amounts to ﬁnding a state assignment for the participant variables in X that
maximizes the following reward function:
RLBP (X) =

xiϵX
fi(xi) +

fklϵF
fkl(xk, xl)
(3)
in the equation above can be decomposed additively. Therefore, it can be mapped
into a local term graph over which max-sum can operate in order to ﬁnd a
solution to the SCF problem.

28
F. L. Covaci
2.3
Reduced Binarized Loopy Belief Propagation Algorithm
(RB-LBP)
As LBP suﬀers from scalability issues in [3] the authors introduce the Reduced
Binarized Loopy Belief Propagation algorithm (RB-LBP). RB-LBP is based
on the max-sum algorithm and simpliﬁes the calculation of max-sum messages
through careful analysis of its local terms. The variables are binary which simpli-
ﬁes the supply chain formation process and each buy and sell decision is decou-
pled, encoded in a diﬀerent variable, from the rest of buy and sell decisions. By
decoupling these decisions the algorithm is able to reduce the number of combi-
nations to take into account. Thus, the authors show in [3] that RB-LBP reduces
the computation required by LBP, to assess SCs from exponential to quadratic,
and the memory and communication requirements from exponential to linear.
The participants decisions and their costs for each participant p taking part
in the SC is encoded in two kind of variables. On the one hand, an activation
variable xp that encodes whether participant p is active (xp = 1) or inactive
(xp = 0), namely part of the SC conguration or not and the activation cost.
Moreover, in order to introduce participants activation cost, the authors make
use of activation terms. An activation term takes as parameter an activation
variable and takes as a value the activation cost of the participant when the
activation variable takes value one and zero otherwise. Formally, the equation
for an activation term fp for participant p can be expressed as:
fp(Xp) =

Cp,
if x = 1
0,
otherwise
(4)
Furthermore each possible buyer p′ of each of her input goods g, the authors in
[3] create an option variable spgp′ that encodes whether p is selling good g to
participant p′(spgp′ = 1) or not (spgp′ = 0). Similarly, for each possible seller p′
of each ps input goods g, the authors create an option variable bp′gp that encodes
whether p is buying good g from participant p′(bp′gp = 1) or not (bp′gp = 0).
In order to guarantee that only one of the providers of a given good is selected,
the authors make use of selection terms. Given a participant p oﬀering good g,
a selection term links the activation variable from the participant (namely xp)
with the diﬀerent choices for that good (namely b∗gp), and enforces that one and
only one option variable takes on value one if the activation variable is active
and that all option variables take on value zero otherwise. Formally, the equation
for a selection term fS joining the activation variable xp and option variables
o1, ..., on is expressed in [3] as:
fS(xp, o1, ..., on) =

0,
if n
i=1 oi = xp
−∞,
otherwise
(5)
Furthermore in order to guarantee coherent decisions between participants, the
equality terms are used. An equality term links buy and sell variables regarding

Optimizing Service Level Agreements in Supply Chains for Complex Projects
29
the same transaction and enforces that both variables take the same value. For-
mally, the equation for an equality term fE joining variables b and s has been
expressed in [3] as:
fE(b, s) =

0,
if b = s
−∞,
otherwise
(6)
Contrarily to LBP, there is no need to keep a table for the local terms. It is
possible to simply calculate the output of selection and equality terms using
equations given above. The additive decomposition that RB-LBP uses to tackle
the SCF problem following a P2P architecture can be expressed as:
ℜRB−LBP (X) =

xpϵXp
fp(xp) +

fSϵFS
fS(XfS) +

fEϵFE
fE(XfE)
(7)
where Xp is the set of participant variables, FS is the set of selection terms, and
FE is the set of equality terms. Having this additive decomposition, it is possible
to map the SCF into a binary local term graph by: (i) creating a simple vertex
for each variable (that summarizes all the simple terms with that variable as
their scope); (ii) creating a composite vertex for each composite term; and (iii)
connecting with an edge each simple and composite vertex that share a variable.
3
Optimizing SLA in Complex Projects Management
In complex projects environments, a component can be both a service provider
and a service consumer in diﬀerent negotiation contexts at the same time (Fig. 1).
Recursive constructs of project components as both a component supplier and
a component consumer complicates the negotiation scenarios. In particular, a
project component has to conﬁrm that its own suppliers can support the SLA
it negotiates with its consumer, before it commits to the SLA. Furthermore, it
is assumed that a consumer and its providers have conﬂicting interests on SLA
parameters; otherwise, both parties can simply reach an agreement by choosing
their common optimum in their negotiation space. In short, automated negoti-
ation within a complex project scenario using decentralized supply chain com-
munication model involves a top-down mechanism to link project requirements
to underlying components to conjointly guarantee end-to-end SLA of a project.
In the SCF process that this paper describes, the components of the system
may be provided either by internal teams or external suppliers. The project man-
agers for each component of the complex project environment are represented by
computational agents. These participant agents act on behalf of the project man-
agers during the project lifecycle and they have to drive the component devel-
opment process of the complex system with multiple system components within
dynamic constraints of scope, time and cost and meanwhile meeting the agreed
SLA requirements. Each agent (SC-Supplier of a Component) that acts on behalf
of the project manager of the component, negotiates only with a selected set of
potential components providers, which match some predeﬁned requirements of

30
F. L. Covaci
Fig. 1. Complex project example
the consumer. Hence, a project component can join a negotiation process as a
potential component provider, if it fulﬁls the given requirements of a consumer.
All the previous research work that use computational agents to solve the
SCF problem, make use of only of cost for pairwise agents and the optimization
problem is treated as a proﬁt maximization function. In comparison to all the
other SCF research papers, the present approach is to translate the SCF problem
as a negotiation game with non-transferable utilities. Our work is based on LBP
approach as the passing of messages in LBP is comparable to the placing of bids.
During the negotiation process between a service consumer and its provider,
agents send messages regarding three issues: scope, time, cost have an exact way
to estimate the utility they get, by making use of utility functions. By doing this,
they can assess the beneﬁts they would gain from a given SLA, and compare
them with their own expectations in order to make decisions.
Below I provide a formal description of the supply chain formation problem
in terms of a directed, acyclic graph (SC, E) where SC = {sc1, sc2, ..., scn}
denote set of component suppliers represented by agents and a set of edges E
connecting agents that might buy or sell from another. Let X = {x1, x2, ..., xn}
denote the set of all system components. Each component has associated three
constraints: scope, cost and time which participating agents are interested in
negotiating. Negotiation ﬁnishes with a contract (the SLA) that is composed of
the actual quality criteria of the component provided by the supplier. Notation
vsc,u
x,i
represents the expectation of user u on constraint i of component x sup-
plied by component supplier sc, which in fact, are the quality values reﬂected
in SLA. We denote by U(v) = U sc,u
x
(v) the utility that the user u gets by

Optimizing Service Level Agreements in Supply Chains for Complex Projects
31
obtaining the actual value v = (vsc,u
x,scope, vsc,u
x,cost, vsc,u
x,time) of the component that
he gets. Similarity W(v) = W sc,u
x
(v) represents that utility that the user sc
gets by delivering the actual value v. When a provider (seller) negotiates with
a consumer (buyer), both parties are interested in obtaining those contract val-
ues v = (vsc,u
x,scope, vsc,u
x,cost, vsc,u
x,time) that maximize their utility functions U(v) and
W(v) respectively. This means that at each moment during the negotiation, the
agent sends a messages to its neighbours regarding the states of his variables,
that is maximizing its expected utility function. The utility functions U(v) will
be calculated by means of weighted sum as follows:
U(v) =
n

k=1
wsc
k ∗Uk(vk), with
n

k=1
wsc
k = 1
(8)
where Uk(vk) represents the utility that the consumer obtains by receiving the
value vk for the issue ik and 0 <= wsc
k <= 1 represent the weights measuring
the importance of a given issue k for a certain agent in the chain.
In order to optimize the overall SLA in a complex project environment so that
the overall project requirements can be achieved at the best utility value, we pro-
pose an automated supply chain formation model based on the Loopy Belief Prop-
agation message passing mechanism [13]. I will express the activation function in
the activation term equation of the RB-LBP [3], as a SLA, which in our case is the
vector v = (vsc,u
x,scope, vsc,u
x,cost, vsc,u
x,time), so the equation for an activation term fSCi
for the project component SCi , i = 1, ..., n, can be expressed as:
fSCi(xSCi) =

U sci,u
x
(v), ifxsci = 1
0, otherwise
(9)
Hence the SLA optimization problem can be expressed as maximizing the
following reward function:
ℜSLA−RB−LBP (X) =

xSCiϵXSCi
fSCi(xSCi) +

fSϵFS
fS(XfS) +

fEϵFE
fE(XfE)
(10)
4
Evaluation and Simulation
As the proposed approach for message passing mechanism in the supply chain
formation process is comparable to sending bids in auction-based approaches, in
this section, I will analyze the game-theoretic properties of the proposed SLA-
RB-LBP approach and further I will provide the results of the simulation for the
proposed implemented model in PeerSim [28].
Hence regarding the game-theoretic properties of the SLA-RB-LBP app-
roach, individual rationality is guaranteed in the proposed approach. Previous
work in [9,13] use a post allocation decommitment stage to eliminate unfea-
sible allocations. Using the selections terms in the proposed approach ensures

32
F. L. Covaci
coherent decisions between participants and guarantees individual rationality.
My approach is not incentive compatible because the dominant strategy for sup-
ply chain formation participants is not to truthfully reveal their private valua-
tions, hence they do not know each others utility functions, agents being situated
under an uncertain environment. This is an issue for real-life scenario. Our app-
roach involves no payments either to or from the mechanism, and is therefore
strongly budget balanced. In a complex project environment the binary local
term graph is always a tree. In [25] the authors demonstrate that when the local
term graph is a tree, max-sum is guaranteed to converge to the optimal congu-
ration [25] and, it is known to provide neighborhood maximum congurations
[26,27]. We can conclude that the proposed approach ensures the allocative eﬃ-
ciency property and will always converge to optimal conﬁguration so that the
overall SLA can be achieved at the best utility value.
In order to validate the model proposed in Sect. 3, I have selected PeerSim
for implementing the Supply Chain SLA protocol. The reasons for making this
choice are the PeerSim performance regarding scalability and because it is based
on components that allows prototype a new protocol, combining diﬀerent plug-
gable building blocks that are in fact Java objects. PeerSim is a single-threaded
peer-to-peer simulator that is developed in a modular and scalable way [28]. The
steps of the simulation process in PeerSim are [28]: 1. set the number of nodes
in the conﬁguration ﬁle 2. select one or more existing protocols (or implement a
new one) to experiment with and initialize them 3. select one or more Control
objects to monitor the desired properties (e.g. the values of the nodes) 4. run the
simulation invoking the Simulator class with the conﬁguration ﬁle, that contains
the above information.
The simulation starts by reading the conﬁguration ﬁle, given as an input
parameter that deﬁnes the protocols to experiment. Then, both nodes and pro-
tocols are created and initialized. After the initialization phase, by default, every
instance of the protocols running on each node is executed once per simulation
cycle. The event-based simulation in PeerSim maintains the concept of simula-
tion cycle, but in each cycle are executed only the protocols instances that have
some pending events. Events (or messages) are sent by controllers or protocols
and they are managed with a handler associated to the protocol that allows to
handle a message properly. Messages exchange is simulated by a transport pro-
tocol that provides the communication primitives and allows you to add more
realism to the simulation. The nodes are scheduled according to the list of pend-
ing messages. PeerSim models the set of nodes as a collection through the class
peersim.core.Network. The overlay network is represented by a adjacency list
where every peer n of the network is connected to a set of neighbors N(n).
I have implemented in the simulator the SupplyChainSLA-LBP protocol
based on the event driven model in PeerSim. The implemented processEvent
method in Listing 1, using the message passing mechanism with two types of
messages (RequestMessage and ResponseMessage) along with order of passing
mechanism starting from the leaf nodes ensures that the selection of the partners
is according to the selection term equation in (10).

Optimizing Service Level Agreements in Supply Chains for Complex Projects
33
Listing 1. Nodes Sending Messages and Updating Values
public
void
processEvent (Node node , int
pid , Object message )
{
double
n u t i l i t y ;
i f
( message
instanceof
RequestMessage )
{
RequestMessage reqMessage=(RequestMessage ) message
;
Transport
t= ( Transport ) node . getProtocol ( transportId ) ;
ResponseMessage respMessage ;
respMessage=new ResponseMessage ( t value , c value , node ) ;
t . send ( node ,
reqMessage . src , respMessage , pid ) ;
}
e l s e
i f
( message
instanceof
ResponseMessage )
{
ResponseMessage respMessage=(ResponseMessage ) message
;
n u t i l i t y=getUtlityValue ( respMessage . t value ,
respMessage . c value ) ;
i f
( n u t i l i t y >maxutility )
{
t value=respMessage . t value ;
c value=respMessage . c value ;
maxutility=n u t i l i t y ;
}
}
The scenario that the present simulation addresses is that each node in the
supply chain has multiple options of partner agents to trade with. The number of
these options are conﬁgurable in the conﬁguration ﬁle that is send as a parameter
when the simulation starts. All of this partner agents are preselected on the basis
that they have the capability to provide the components, more speciﬁcally they
are able to provide the system components as its own scope it is deﬁned. Even
if the issue Scope it is part of the weighted utility function and has a certain
weight for each agent, the agents don’t negotiate on this issue because they
either provide the scope or the component or they are not being preselected a a
potential partner, this being accordingly with the equality term in Eq. (10).
Furthermore, each node v has two numeric values “t value” and “c value”
that are the preferred states of the two issues: Time and Cost. Also each node
has a utility function which is computed as a weighted sum from each of the

34
F. L. Covaci
issues that the agents in the supply chain are discussing on. At the beginning of
the simulation all the agents in the network are being initialized with random
preferred values for time and cost variables and also for the weights used at
computing every agent utility. Each node that is not a leaf is receiving messages
from its neighbors, composing new messages and sending them upward to its
neighbors. The message exchange between a pair of nodes is a vector of real
numbers, one for each possible state of the variable shared by both nodes. Each
node will assess messages received from the corresponding neighbor according
to his own utility function and will chose among all the possible partners the
one that maximizes his utility function. The procedure above, is repeated until
after none of the messages change from one iteration to another or a maximum
number of iterations is reached (Table 1).
Invoking the simulator with the conﬁguration ﬁle above, the maximum SLA
expresses as a utility function, that can be obtained at the root node, giving the
constraints of time and costs in the underlying network, is 33.81 and the states of
the variable shared by agents became stable at the following ﬁnal utility values:
Table 1. Utility obtained by simulation
Node Initial utility
Intermediate utility values Final utility
0
6.325188371300536
22.46334767085241
33.816967145516315
1
2.4806385045290917
3.7658475023426337
5.9102205089020075
2
13.139763118138983
19.05462493705339
19.05462493705339
3
28.712757912880438
31.89339121189109
34.07402451090173
4
2.665934636137622
6.274423250339464
6.274423250339464
5
12.56190696603451
7.737144179620706
7.737144179620706
6
5.725432091050816
5.725432091050816
5.725432091050816
7
1.3976917977590864
1.3976917977590864
1.3976917977590864
8
27.9575241095819
27.9575241095819
27.9575241095819
9
55.39176948841717
55.39176948841717
55.39176948841717
10
36.66119166541283
36.66119166541283
36.66119166541283
11
14.788420689297935
14.788420689297935
14.788420689297935
12
29.54023493801846
29.54023493801846
29.54023493801846
13
12.816045296967191
12.816045296967191
12.816045296967191
14
3.746115621145735
3.746115621145735
3.746115621145735
15
24.741574241434865
24.741574241434865
24.741574241434865
16
3.3828050529549234
3.3828050529549234
3.3828050529549234
17
5.8003154798198695
5.8003154798198695
5.8003154798198695

Optimizing Service Level Agreements in Supply Chains for Complex Projects
35
My simulation was run having the following values in the conﬁguration ﬁle:
Listing 2. Conﬁguration ﬁle
# number of
network nodes
SIZE 18
# number of
simulation
c y c l e s
CYCLES 25
network . s i z e
SIZE
# I n i t i a l i z e r s
to
use
include . i n i t
initED
n e t I n i t
# Controls
to
use
include . control
neighObs valueObsED
#Supply chain SLA negotiation
protocol
protocol . supplychainED SupplyChainSLA−LBP
# Network generation
i n i t . n e t I n i t
WireRegRootedTree
# The l i n k a b l e
protocol
to
operate on
i n i t . n e t I n i t . protocol
lnk
# Number of
outgoing
edges
to
generate
from each node
i n i t . n e t I n i t . k 3
5
Conclusions and Future Work
As todays market is in constant change, software components suppliers are faced
with ever-changing customer needs and resources costs and availability. Conse-
quently in complex projects environments it is no longer possible to maintain
SCs over extended periods of time. Thus, the ability to quickly form eﬀective,
mutually benecial trading partnerships becomes increasingly important. The old
approach for SCF, where SCs were assessed manually after extended negotiations
is no longer viable. Furthermore, human irrationality coupled with the complex-
ity of the problem often lead to ineﬃcient SCs. Therefore, there is a need for
agile and exible methods that support temporal collaboration between software
components providers and consumers.
In this paper we have proposed a decentralized and distributed approach for
optimizing SLA in complex project scenarios SupplyChainSLA-LBP protocol,
based on the RB-LBP P2P supply chain model approach for sending messages
between nodes and we have translated the activation cost into a SLA vector using
utility functions. The current approach that this paper presents uses message
passing between agents during the supply chain formation process and is closer
to real life scenarios than the previous approaches that were using only cost as
a mean for pairwise agents because uses utility functions for agents to make
decision. As a future work I will investigate the impact on the supply chain
formation when agents make coalitions. Since components in a complex project
environment can be provided either by internal teams represented by cooperative
agents that are willing to contribute to global business objectives and other

36
F. L. Covaci
components might be provided by external organizations represented by self-
interested agents which are less cooperative and hence more utility oriented we
propose to investigate how the Shapely value aﬀects the supply chain formation
when there is an internal or external coalition formation.
References
1. Barnes, M.: Time and money in contract control (1969)
2. ITIL Foundation Handbook, London, The Stationery Oﬃce (2012)
3. Penya-Alba, T., Vinyals, M., Cerquides, J., Rodriguez-Aguilar, J.A.: A scalable
message-passing algorithm for supply chain formation. In: 26th Conference on Arti-
ﬁcial Intelligence (AAAI 2012) (2012)
4. Winsper, M., Chli, M.: Decentralized supply chain formation using max-sum loopy
belief propagation. Comput. Intell. 29, 280–309 (2012)
5. Pearl, J.: Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference, vol. 1, 1st edn. Morgan Kaufmann, San Francisco (1988)
6. Collins, J., Ketter, W., Gini, M., Mobasher, B.: A multi-agent negotiation testbed
for contracting tasks with temporal and precedence constraints. Int. J. Electron.
Commer. 7, 35–58 (2002)
7. Davis, R., Smith, R.G.: Negotiation as a metaphor for distributed problem solving.
Artif. Intell. 20(1), 63–109 (1983)
8. Walsh, W.E., Wellman, M.P., Ygge, F.: Combinatorial auctions for supply chain
formation. In: Proceedings of the 2nd ACM Conference on Electronic Commerce,
pp. 260–269 (2000)
9. Walsh, W.E., Wellman, M.P.: Decentralized supply chain formation: a market pro-
tocol and competitive equilibrium analysis. J. Artif. Intell. Res. (JAIR) 19, 513–567
(2003)
10. Cerquides, J., Endriss, U., Giovannucci, A., Rodriguez-Aguilar, J.A.: Bidding lan-
guages and winner determination for mixed multi-unit combinatorial auctions. In:
IJCAI, pp. 1221–1226. Morgan Kaufmann Publishers Inc. (2003)
11. Giovannucci,
A.,
Vinyals,
M.,
Rodriguez-Aguilar,
J.A.,
Cerquides,
J.:
Computationally-eﬃcient winner determination for mixed multi-unit combi-
natorial auctions. In: Proceedings of the 7th International Joint Conference on
Autonomous Agents and Multiagent Systems, vol. 2, pp. 1071–1078 (2008)
12. Winsper, M., Chli, M.: Decentralised supply chain formation: a belief propagation-
based approach. In: Agent-Mediated Electronic Commerce (2010)
13. Winsper, M., Chli, M.: Decentralized supply chain formation using max-sum loopy
belief propagation. Comput. Intell. 29(2), 281–309 (2013)
14. Norman, T.J., Preece, A., Chalmers, S., Jennings, N.R., Luck, M., Dang, V.D.,
Nguyen, T.D., Deora, V., Shao, J., Gray, W.A., et al.: Agent-based formation of
virtual organisations. Knowl. Based Syst. 17(2), 103–111 (2004)
15. Winsper, M.: Using min-sum loopy belief propagation for decentralised supply
chain formation, Ph.D. thesis, Aston University (2012)
16. Bishop, C.M., et al.: Pattern Recognition and Machine Learning. Springer, New
York (2006)
17. Farinelli, A., Rogers, A., Petcu, A., Jennings, N.R.: Decentralised coordination
of low-power embedded devices using the max-sum algorithm. In: Proceedings of
the 7th International Joint Conference on Autonomous Agents and Multiagent
Systems, vol. 2, pp. 639–646 (2012)

Optimizing Service Level Agreements in Supply Chains for Complex Projects
37
18. Rogers, A., Farinelli, A., Stranders, R., Jennings, N.R.: Bounded approximate
decentralised coordination via the max-sum algorithm. Artif. Intell. 175(2), 730–
759 (2011)
19. Kim, Y., Krainin, M., Lesser, V.: Application of max-sum algorithm to radar coor-
dination and scheduling. In: Workshop on Distributed Constraint Reasoning (2010)
20. Pujol-Gonzalez, M., Cerquides, J., Meseguer, P., Rodr´ıguez-Aguilar, J.A., Tambe,
M.: Engineering the decentralized coordination of UAVs with limited communi-
cation range. In: Bielza, C., Salmer´on, A., Alonso-Betanzos, A., Hidalgo, J.I.,
Mart´ınez, L., Troncoso, A., Corchado, E., Corchado, J.M. (eds.) CAEPIA 2013.
LNCS (LNAI), vol. 8109, pp. 199–208. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-40643-0 21
21. Stranders, R.: Coordinating teams of mobile sensors for monitoring environmental
phenomena (2009)
22. McEliece, S.M.: The generalized distributive law. IEEE Trans. Inf. Theor. 46(2),
325–343 (2000)
23. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press, Cambridge (2009)
24. Winsper, M., Chli, M.: Using the max-sum algorithm for supply chain formation
in dynamic multi-unit environments. In: Proceedings of the 11th International
Conference on Autonomous Agents and Multiagent Systems, vol. 3, pp. 1285–1286
(2012)
25. Weiss, Y.: Correctness of local probability propagation in graphical models with
loops. Neural Comput. 12(1), 1–41 (2000)
26. Weiss, Y., Freeman, W.T.: On the optimality of solutions of the max-product
belief-propagation algorithm in arbitrary graphs. IEEE Trans. Inf. Theor. 47(2),
736–744 (2001)
27. Vinyals, M., Cerquides, J., Farinelli, A., RodrguezAguilar, J.A.: Worst-case bounds
on the quality of max-product ﬁxed-points. In: NIPS, pp. 2325–2333 (2010)
28. Montresor, A., Jelasity, M.: PeerSim: a scalable P2P simulator. In: Proceedings of
the 9th International Conference on Peer-to-Peer, pp. 99–100, Seattle, WA (2009)

A Brief Overview of Semantic Interoperability
for Enterprise Information Systems
Tarcisio Mendes de Farias, Ana Roxin(B), and Christophe Nicolle
CheckSem - LE2I UMR CNRS 6306 - Univ. Bourgogne Franche-Compt´e,
Besan¸con, France
{tarcisio.mendes-de-farias,ana-maria.roxin,cnicolle}@u-bourgogne.fr
Abstract. In the context of the globalisation, most companies changed
the way in which they do business. Not only they have to be more reactive
to changes in market forces, but they also need to be capable of adapt-
ing their business oﬀers. To do so, companies have to collaborate mainly
relying on Enterprise Information Systems (EIS) interoperability. EIS
interoperability addresses problems related to the lack of system inter-
operability in organisations. In this paper, we focus on approaches which
deliver semantic interoperability among these systems. We conclude by
identifying the main drawbacks of such approaches to be adopted by
world-wide industry and stating a future direction to solve these limits.
Keywords: Enterprise Interoperability
Model-driven interoperability (MDI) · Ontology
Semantic interoperability
1
Introduction
In the context of the data and knowledge management and the needs to give a
quick response to changes in market forces, enterprises have to collaborate. To do
so, companies use information technologies to succeed in a disparate and dynam-
ical business environment. Enterprise integration and interoperability (EII) are
some of the main ﬁeld of studies to improve this collaboration. EII addresses
problems related to the lack of the information system interoperability in organ-
isations.
We have been witnessing since 1990s, just after the advent of the World Wide
Web (WWW), a new industry and research domain: enterprise integration. In
the industrial context, the main objective is to provide tools to integrate data
from various sources without needing to previously load all data into a central
warehouse [1]. In the research context, we can separate the enterprise integration
discipline into two research ﬁelds: (i) information technologies and (ii) enterprise
modelling. Information technologies are closely related to the industry context
where developed tools refers to data integration systems. Enterprise modelling
refers to an ensemble of approaches and concepts including the conception of
c
⃝Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 38–52, 2018.
https://doi.org/10.1007/978-3-319-73459-0_3

Semantic Interoperability for EIS
39
a global system architecture, consistency between local and global objectives,
resources allocation and data consistency [2].
Beginning in the late 1990s, enterprise integration approaches need to empha-
size the Enterprise Interoperability (EI) branch of study. This is because to
interoperate partners must dynamically adapt to achieve an agreement [3]. EI
addresses this interoperation needs as it is more adaptable to be used in decen-
tralised, dynamic and interconnected systems. In this regard, EI is more cost-
eﬀective and faster implemented. Many interoperability deﬁnitions can be found
in [4–6]. For instance, [4] deﬁnes interoperability as the ability to communicate
with pier systems and access the functionality of the pier systems. From the
computer point of view, achieving interoperability means to relate two hetero-
geneous computer systems so they can work jointly and they give reciprocal
access to their resources. More speciﬁcally, EI refers to the ability to exchange
information and services between enterprise systems [7]. Moreover, the enterprise
information system (EIS) interoperability has been recognized in the last years
by industry and research communities in a similar way [8].
In general, interoperability denotes coexistence, autonomy and federated
environment, whereas integration has the meaning of coordination, uniformity
and coherency [7,9]. Concerning the degree of coupling, the tightly coupled sys-
tem refers to the fact that components cannot be detached and they are inter-
dependent. In this regard, it is the case of an integrated system. The loosely
coupled system means that components interact with each other and are con-
nected by a communication network. They can exchange data while preserving
their autonomy. In this context, integration goes beyond a “simple” interoper-
ability to involve some functional dependences [10].
This paper is divided into more four sections. Section 2 introduces a general
discussion about interoperability approaches to achieve interoperability across all
enterprise levels. Section 3 presents an overview of interoperability frameworks
in the context of EISs. Section 4 provides a literature review related to the use
of ontologies to perform semantic interoperability among EISs. Finally, Sect. 5
concludes this paper identifying, broadly speaking, the limits in the context of
semantic interoperability for EISs of the approaches presented in Sects. 3 and 4.
2
Enterprise Interoperability Levels and Approaches
There are three ways to achieve some interoperability according to ISO 14258 [11]:
1. Integrated - there is a standard format (i.e. a global view) for all constituent
systems. Diﬀerent models (i.e. local views) are described using this standard
format. This format must be able to describe constituent system models.
2. Uniﬁed - there is a common meta-level structure (i.e. a meta model) across
constituent models. This meta-model is responsible for describing and estab-
lishing semantic equivalence links among models.
3. Federated - models are dynamically linked rather than having a predeﬁned
meta-model. This assumes that concept alignment is done at the model (ontol-
ogy) level, i.e. the semantic level.

40
T. M. de Farias et al.
Panetto and Cecil [9] state that the federated approach is seen as the most
interesting one to achieve full interoperability. However, choosing one of those
three levels (presented in ISO 14258 [11]) depends on the context and require-
ments. The ﬁrst approach (i.e. integrated approach) is recommended when enter-
prises merge and their systems have to interoperate. The second approach (i.e.
uniﬁed approach) appears to be a good choice, if the needs of interoperability
is related to a long term collaboration between enterprises. This approach pro-
poses a common meta-model across diﬀerent enterprises’ models that establish
semantic equivalences among them. The last approach (i.e. federated approach)
is recommended for interoperability when required by a short term collaboration
project and/or the EISs’ autonomy must be preserved.
A lot of research has been carried out since the end of 1990 s to develop EI
frameworks. The main purpose of these frameworks is to represent in a more
structured way concepts, issues and knowledge on the enterprise interoperabil-
ity domain. The resulted structure can be illustrated using diagrams, text and
formal rules, that relate entities in the proposed framework to each other [12].
According to EN/ISO 19439 [12,13], interoperability frameworks mainly dis-
tinguish three levels of interoperability which each one is composed of various
artefacts and high-level standards. These interoperability levels are as follows:
1. Technical interoperability level refers to investigate problems and propose
solutions for the interconnection of information systems. This level is related
to physical and syntactic interoperability. To establish the physical inter-
operability, a solution is to use basic protocols, such as computer network
protocols (e.g. TCP, IP, Ethernet, etc.). The syntactic interoperability can
be developed using syntactic formats, such as XML to exchange data, how-
ever the data’s meaning is not present in this level. Moreover, we can include
in this level security and accessibility mechanisms.
2. Semantic interoperability level includes methods and tools, in the form
of ontologies or standardised data schemas (e.g. Web Ontology Language
(OWL)1, Knowledge Interchange Format - KIF2 and F-logic [14]) to be able
to exchange the data’s meaning (i.e. semantics) between interoperable sys-
tems. Thus, these systems can semantically parse a set of terms that describes
exchangeable data.
3. Organizational interoperability level is related to problems and solutions rel-
evant to business processes, functional organisation or cross-enterprise col-
laboration activities that normally involve diverse information systems and
data.
For Charalabidis et al. [8,15], a generic interoperability framework should also
consider the policy interoperability level. This level refers to the alignment of
higher enterprise functions or government policies. To do so, the policy interop-
erability can be deﬁned in the form of legal parts, business rules and crucial goals.
1 www.w3.org/TR/owl2-overview/.
2 logic.stanford.edu/kif/dpans.html.

Semantic Interoperability for EIS
41
From another point of view, Wang et al. [16] state six levels of interoperability: tech-
nical (level 1), syntactic (level 2), semantic (level 3), pragmatic (level 4), dynamic
(level 5) and conceptual (level 6). Thus, levels 1 and 2 proposed by Wang et al.
are included in the technical interoperability level (see levels in the previous para-
graph). The semantic interoperability level (mentioned in the previous paragraph)
includes level 3 and 4 proposed by Wang et al. The novelty proposed by Wang et al.
are levels 5 and 6 which intersect the semantic and organizational interoperabil-
ity. Interoperability at level 5 states that interoperable systems are able to redirect
information production and consumption based on understood modiﬁcations to
semantics, because of changing context over time. Finally, interoperable systems
at level 6 are completely aware of each others information, processes, contexts, and
modelling assumptions.
This article focuses on the semantic interoperability to interoperate enter-
prise information systems. Because of this, the next sections only mention the
most relevant works in the current state-of-the-art which somehow address the
semantic interoperability level.
3
Enterprise Interoperability Frameworks
Interoperability Frameworks (IF) establish a possible plan of actions to be done
to accomplish the desired interoperability. Thus, IFs aim to structure necessary
concepts and activities/tasks for design and build interoperable systems.
3.1
IDEAS Framework
In the context of EI, advances in the IF development were notoriously observed
during the Interoperability Developments for Enterprise Application and Soft-
ware (IDEAS) project [17]. The IDEAS project stated that in order to achieve
signiﬁcant interoperation between enterprises, interoperability must be achieved
on multiple levels: inter-enterprise coordination, business process interoperabil-
ity, semantic application interoperability, syntactical application interoperability
and physical interoperability.
The IDEAS framework goes beyond the EIS interoperability. Because, this
framework takes into account not only the EIS interoperability but also the
enterprise knowledge and business interoperability (in addition to the traditional
information and communication technology (ICT) based interoperability). The
IDEAS project states that the EISs should support enterprises’ decision makers
and allow an enterprise to operate and exchange information internally and with
others. Thus, interoperability of EISs is seen as the capability of an EIS to cooper-
ate with information systems of other external organizations. The interoperation
needs to be settled by the supply of information through inter- and intra-system
communication. The semantic dimension intersects the business, knowledge and
EIS (i.e. application, data and communication) layers. This dimension focuses on
supporting mutual concept meaning in all layers to promote a common under-
standing. The enterprises that need to interoperate with others on a speciﬁc layer

42
T. M. de Farias et al.
should agree with a mutual understanding. To achieve the semantic interoper-
ability, the IDEAS framework proposes the use of ontologies and an annotation
formalism to ensure that semantics are exchangeable and based on a common
understanding.
According to Chen et al. [7], the main drawback of the IDEAS framework is
the fact that this framework does not address the interoperability domain itself
but it is based on already existent research domains to perform interoperability
(e.g. enterprise modelling, ontology, architecture and platforms). This drawback
was addressed under the INTEROP Network of Excellence (NoE) project.
3.2
The INTEROP (NoE) Framework
INTEROP (NoE) proposes an interoperability framework that deﬁnes the
research domain of EI and contributes to structure the knowledge of this domain.
INTEROP (NoE) project considers that EISs are not interoperable because there
are barriers (i.e. incompatibilities) to interoperate at various enterprise levels.
This project identiﬁes three types of barriers as follows: conceptual, technological
and organizational barriers. To achieve full semantic interoperability, conceptual
barriers must be solved. Conceptual barriers concern the syntactic and seman-
tic diﬀerences of information to be exchanged. Thus, they are also related to
problems of data model heterogeneity. Because, even if EISs use the same data
format (e.g. XML to describe data), data models remain heterogeneous. Based
on the ATHENA3 technical architecture [18], the INTEROP (NoE) framework
states four levels of interoperability: data, service, process and business. Figure 1
illustrates the INTEROP (NoE) framework using two dimensions: ATHENA’s
interoperability levels and identiﬁed barriers of interoperability. This framework
deﬁnes the EI research domain by composing an ensemble of sub-domains. Each
sub-domain (illustrated using an empty rectangle in Fig. 1) represents a part of
knowledge relevant to interoperability. An approach to be considered relevant
has to mitigate at least one barrier at one level.
Figure 1 shows some approaches to address conceptual barrier. Triangles are
approaches which consider both syntactic and semantic barriers. In contrast to
this, approaches in the ellipse format only take into consideration the semantic bar-
rier. For example, the second version of the Uniﬁed Enterprise Modelling Language
(UEML) [19,20] aims to remove syntactic and semantic barriers at four levels (i.e.
business, process, service and data) to establish the enterprise model interoper-
ability. The second version of UEML was developed within the INTEROP NoE
project. Another example is the Process Speciﬁcation Language (PSL) that con-
tributes to remove conceptual barrier in the process level. According to [21], PSL
deﬁnes a neutral representation for manufacturing processes that supports auto-
mated reasoning. Last but not least, semantic annotation approaches [22] can be
usedfromdatatothebusinessleveltodescribedatasemantics(i.e.meta-data).The
INTEROP NoE framework also includes the interoperability approach dimension
3 http://athena.modelbased.net/wholesite.html.

Semantic Interoperability for EIS
43
Fig. 1. Enterprise interoperability sub-domains represented with the INTEROP (NoE)
framework (modiﬁed from Chen et al. [7])
(i.e. uniﬁed, integrated and federated approaches) in addition to the level and bar-
rier dimensions (see Fig. 1) to better deﬁne an interoperability sub-domain.
3.3
Other Frameworks
In the research direction of those projects/frameworks, a number of methodolo-
gies attempt to arrange and classify the diﬀerent interoperability aspects into
comprehensive interoperability frameworks. Amongst others, we can mention
the European Interoperability Framework (EIF) [23], Panetto’s framework [10],
sustainable EI framework [24] and GeoNis framework [25].
The European Interoperability Framework (EIF) [23] is deﬁned as an ensem-
ble of policies, standards and guidelines to interoperate organizations after an
agreement to do business with each other. In this case, an interoperability frame-
work is not considered a static document and may have to be adapted over time.
This is because technologies, standards and requirements also change over time.
The EIF framework provides recommendations and deﬁnes generic standards
with regard to organizational, semantic, technical, legal and political aspects of
interoperability. Consequently, this framework considers interoperation at diﬀer-
ent enterprise levels namely: data, service, process and business such as proposed
in the ATHENA and INTEROP NoE projects.
Panetto systematizes in the article [10] a classiﬁcation of enterprise models,
applications and standards that are used at diﬀerent levels to build a frame-
work of several interoperability types. These interoperability types are as fol-
lows: synchronic interoperability, model-driven interoperability (MDI), semantic-
driven interoperability (SDI), vertical interoperability, horizontal interoperability
and diachronic interoperability. In principle, MDI solutions are designed to solve
model syntactic transformation and SDI solutions aim at semantic alignment by
using automatic schema matching. Both of them address syntactic and semantic
problems. However, some researchers such as Agostinho et al. [24] and Xu et al.
[26] indirectly suggest that SDI aspects are included in MDI. For Agostinho et al.
[24], the major diﬃcult to implement the MDI approach is the mapping process

44
T. M. de Farias et al.
between diﬀerent EISs due to require a manual and fastidious process. Addition-
ally, Agostinho et al. [24] point out that in some cases, these mappings could be
semi-automatic generated. Xu et al. [26] demonstrate a close relationship between
SDI and MDI when they propose as future work to include semi-automatic sup-
port to do semantic annotation and model matching in the data repository. In this
article, we do not distinguish the SDI approach from the MDI approach.
Agostinho et al. in [24] highlight that the sustainable interoperability is a
major challenge in the EI domain, because it must handle with EIS modiﬁ-
cations and also components that enable interoperation. Thus, independently
of the technologies used, enterprises should be able to adapt to new business
requirements. Figure 2 shows the main aspects addressed by Agostinho et al.’s
framework [24]. This proposed approach contributes to a sustainable interoper-
ability. This is because it provides a robust framework to support the iterative
adaptive loop of the EIS (re)engineering and address new requirements when-
ever needed until the EIS is no more useful. The left part of Fig. 2 illustrates a
generic system engineering approach. In this approach, the authors [24] aim to
leverage all phases in the modelling process of the EIS development from the
identiﬁcation of requirements to th implementation phase. On the right part of
Fig. 2, the authors in [24] follow the ISO 11354 EI framework [27], one can verify
that the enterprise (and EISs) is composed of various levels and interoperability
barriers which are similar to levels and barriers identiﬁed with the INTEROP
(NoE) framework (see Fig. 1). Supported with domain knowledge and seman-
tics, the authors in [24] believe that pervasive information models and enterprise
architectures can contribute to automate the alignment process (e.g. among data
models at the application level). Moreover, it can contribute to the development
of dynamic interoperability enablers. These suggested enablers are sensible to
changes in the EISs in order to assure a sustainable interoperability.
Fig. 2. A sustainable interoperability framework (modiﬁed from Agostinho et al. [24])

Semantic Interoperability for EIS
45
GeoNis [25] is a framework to integrate information based on semantics and
the use of ontologies. This framework can be classiﬁed as a uniﬁed (semantic)
interoperability approach. Further details of the GeoNis framework and MDI are
discussed in the next Sect. 4.
4
Ontologies as Models for Interoperability
Ontologies have been recognized as interesting and promising in what con-
cerns data interoperability, mainly because they handle formal representations
of data and semantics [28,29]. This is also noticed by He and Xu [30] which
state that various ontology approaches for semantic interoperability trend to
become mature and more applicable in future industrial environments. Thus,
ontologies semantically support the description and the analyse of relationships
among concepts. According to authors in [31], Resource Description Framework
(RDF)4 and OWL (based on RDF) are two main modelling languages in use
for the EI ontology development. However, interoperability issues are identiﬁed
when using instances of diﬀerent ontologies. Semantic annotation, ontology har-
monisation and merging are examples of important methods for the enterprise
interoperability [32].
4.1
Model Driven Interoperability
The use of ontologies to solve semantic interoperability problems are clearly iden-
tiﬁed in the MDI architecture started under the INTEROP NoE project. The
MDI is based on enterprise interoperability concepts and the model-driven archi-
tecture (MDA) initially proposed by the Object Management Group (OMG)
[33]. Designed MDA models are structured from the user to the execution level
as follows: CIM (computational independent model), PIM (platform indepen-
dent model) and PSM (platform speciﬁc model). Firstly, the MDA proposes to
start from a business model (e.g. CIM) that does not depend on information
technologies. Secondly, it transforms the CIM model to an independent model
of the platform (e.g. CORBA, .NET, etc.). Finally, the transformation of the
PIM model into a PSM model to implement the concrete system that comput-
ers can run. The MDI allows an automatic transformation of the MDA models.
These models are designed somehow that they can interoperate with another
enterprise models using the same MDI architecture. Therefore, the MDA/MDI
approach has introduced the concept of achieving integration from the busi-
ness level down-to the implementation. So, the MDI enables to mitigate the gap
between the business requirements and the EIS implementation [24].
Figure 3 illustrates the MDI method at the conceptual level. As a reminder,
the conceptual integration focuses on concepts, meta-models, languages and
schema relationships. It aims to systematise various views of the software model
interoperability. The MDI addresses interoperability of models by using meta-
models and ontologies to support semantics. Ontologies are used to deﬁne model
4 https://www.w3.org/RDF/.

46
T. M. de Farias et al.
transformations and mappings between the various aspects of an EIS [34]. The
MDI deﬁnes a common interoperability ontology (the “Reference Ontology”
illustrated in Fig. 3). In a semantic point of view, the MDI approach proposes
veriﬁcation of the consistency of models, (semi-)automatic matching among het-
erogeneous models and the preservation of semantics during a transformation
process (i.e. CIM ⇆PIM ⇆PSM). Model correspondences across enterprises
are represented through ontology-based semantic annotations.
Fig. 3. The reference model to conceptual integration (modiﬁed from [34])
According to the authors in [24], the MDA/MDI approach still remains
an open subject, as incomplete EISs have been developed that are capable of
implementing the proposed vertical integration (using model transformations)
and interoperability by design. Except for the Model-driven Service Engineer-
ing Architecture (MDSEA) [35,36] and other initiatives [37], the solutions that
implement the MDA/MDI are scarce. Thus, new approaches, concepts and tools
are still necessary and desired. Moreover, model transformations are traditionally
static processes [24].
4.2
Towards Uniﬁed Ontology-Based Approaches
GeoNis [25] proposes an interoperability platform without a centralized control.
This platform has a middleware layer that provides the access to various data
sources. This middleware layer contains wrappers or translators for all data
sources. In addition, this layer has mediators which are responsible for storing
the list of available sources and forwarding queries and their results. The GeoNis
platform requires to implement wrappers which will allow a copy of the local data
source to be described with a speciﬁc data model into a global model (i.e. the

Semantic Interoperability for EIS
47
platform common model). Still, GeoNis has a server associated with a top-level
ontology to solve semantic heterogeneity problems. Hence, GeoNis is a framework
for semantic interoperability, that provides the semantic data interoperability
from heterogeneous data sources. Despite the GeoNis platform being initially
conceived to achieve interoperability of geographic information systems, it can
also be successfully used to interoperate other EISs. For example, the authors in
[38] applied the GeoNis approach for promoting the enterprise interoperability
of power supply companies.
The authors in [39] focus on achieving a long term stability of interopera-
ble EISs. In order to do so, they propose the integration of functionalities of
traceability in enterprise systems to support sustainability, in other words, the
maintenance of interoperability in a dynamic way. In this approach, data, seman-
tics and schema mappings are described as traceable 5-tuples and stored in a
knowledge base with reasoning capabilities. Thus, these 5-tuples deﬁne align-
ments among models. A 5-tuple includes a unique identiﬁer, mapped elements,
mapping type (e.g. semantics), match/mismatch classiﬁcation (e.g. the concep-
tual or schema mapping) and the mapping function. The mapping function is
writing using the Atlas Transformation Language (ATL) [40] that is, actually, an
executable code allowing automatic mapping. To represent and store 5-tuples,
this approach using OWL extends the Model Traceability Ontology deﬁned in
[41] to create a Communication Mediator (CM). The CM also allows for address-
ing traceability to support a sustainable interoperability. Hence, we can classify
this interoperability approach as uniﬁed, because CM is a meta-ontology (see
Fig. 4) that describes semantic equivalence links between ontologies.
Fig. 4. The Communication Mediator (CM) meta-ontology (modiﬁed from Agostinho
et al. [39])

48
T. M. de Farias et al.
4.3
Towards Federated Ontology-Based Approaches
Diﬀerent information systems designed for the same domain can have disparate
models to represent data [28,42]. To mitigate this problem, the authors in [28]
propose a multi-representation ontology (MurO). This approach is based on
description logics (DL) [43] combined with the modal logics (ML) [44]. Thus, this
hybrid approach takes the best of each logic. Rifaieh et al. [28] consider MurO
as an ensemble of ontologies which depend on context. These ontologies are
gathered without being integrated to conceive a global ontology. Hence, the con-
textual ontology is a local ontology inter-related with other ontologies through
a mapping relation between concepts. In the context of the EIS, MurO can
contribute to solve existing problems of semantic interoperability. Still, MurO
was actually applied for a research project named EISCO [28] to deal with the
multi-representation problem in the EIS domain.
Under INTEROP NoE (6th EU Framework Programme), the authors in [29]
propose a federated interoperability approach based on ontologies. They use the
OWL language to independently describe enterprise information resources as a
reference ontology in the context of each enterprise party. Due to the seman-
tic heterogeneity is aggravated with diﬀerent modelling choices, each enterprise
ontology can diﬀerently describe the same resources. To address this problem,
the authors’ approach in [29] has three semantic interoperability services for
each enterprise party: discovery, matching and acquisition services. The discov-
ery service is responsible for ﬁnding which nodes (i.e. enterprises) in a networked
organization can provide relevant resources with respect to a given concept-base
query. To do so, the discovery service provides a template to query. This tem-
plate is used by the matching service to perform the ontology matching. This
template also deﬁnes which models will be used to evaluate the semantic aﬃn-
ity (i.e. similarity) between the incoming query and concepts contained in each
enterprise ontology. Once that the ontology matching is done, the discovery ser-
vice of each enterprise returns a query answer that follows a predeﬁned template
(i.e. a list of matching concepts). This query is returned to the enterprise party
that had initially sent the query. Finally, the acquisition service uses these list
of matching concepts to query and to retrieve data (e.g. XML data) from dif-
ferent data sources in the networked organisation. This approach has various
drawbacks. Among them, we can point out:
1. Queries are restricted to concepts. It does not consider queries involving a
graph pattern and various modiﬁers (e.g. ORDER, FILTER, LIMIT, etc.)
that are found in query languages such as SPARQL5.
2. It does not allow for inferring new concept alignments in a transitive way (e.g.
A →B and B →C then A →C). It is a peer-to-peer matching approach.
3. The matching service is required at the query execution time. The list of
aligned concepts are not stored, consequently, the matching process must be
recomputed each time that a given query is re-executed.
5 www.w3.org/TR/rdf-sparql-query/.

Semantic Interoperability for EIS
49
5
Conclusion
To answer properly and quickly to changes existing in market forces and adapt to
business globalisation, enterprises should develop new technologies and method-
ologies to collaborate with each other. This paper has summarized the current
state-of-the-art in EI giving a focus on the level of semantic interoperability. The
main contribution of this position paper concerns semantic data interoperability
in order to relieve conceptual barriers. Ontology-based approaches have been
recognized by several researches as a solution to achieve semantic interoperabil-
ity. Nevertheless, there are still various drawbacks to ontology-based approaches
be used in industrial environments such as follows:
1. The uniﬁed and tightly coupled federated interoperability approaches are not
easily adaptable to assure a dynamic and autonomous EIS interoperability;
2. Loosely coupled ontology-based federated approaches mitigate considerably
the lack of adaptability. A loosely coupled federation does not deﬁne a global
integrated ontology for interoperation. However, other problems which com-
promise EIS interoperability are not taken into account:
– The approaches presented in Subsect. 4.3 do not fully handle schema het-
erogeneity because only concept (i.e. class) matching is considered (prop-
erties are ignored)
– Deﬁning mappings among ontology concepts at query execution time to
achieve dynamic and automatic interoperability signiﬁcantly deteriorates
performance
3. Considering the ontology level, none of those ontology-based approaches oﬀer
data access policy over interoperable ontologies. Deﬁning data access poli-
cies are really important to keep conﬁdential data exclusively shared among
desirable companies and/or EISs;
4. Performance issues of using an ontology-based approach for EIS interoperabil-
ity are not clearly addressed. For instance, it is crucial that a query addressed
to interoperable EISs deliver results almost in real-time. This impedes the
progress of ontology-based approaches in the context of EIS interoperability.
Therefore, based on those drawbacks, we believe that to achieve semantic inter-
operability by using ontologies in the context of EISs, we have to focus on deﬁn-
ing a loosely coupled ontology-based federated approach. Such approach must
be dynamical, adaptable to changes and eﬃcient.
Acknowledgments. This
work
has
been
ﬁnanced
by
the
French
company
ACTIVe3D (see active3d.net) and supported by the Burgundy Regional Council (see
region-bourgogne.fr).

50
T. M. de Farias et al.
References
1. Halevy, A.Y., Ashish, N., Bitton, D., Carey, M., Draper, D., Pollock, J., Rosenthal,
A., Sikka, V.: Enterprise information integration: successes, challenges and contro-
versies. In: Proceedings of the 2005 ACM SIGMOD International Conference on
Management of Data, pp. 778–787. ACM (2005)
2. Vernadat, F.B.: Enterprise modeling and integration (EMI): current status and
research perspectives. Annu. Rev. Control 26(1), 15–25 (2002)
3. Browne, J., Zhang, J.: Extended and virtual enterprises-similarities and diﬀerences.
Int. J. Agile Manag. Syst. 1(1), 30–36 (1999)
4. Vernadat, F.: Enterprise modeling and integration: principles and applications
(1996)
5. Chen, D., Vernadat, F.B.: Enterprise interoperability: a standardisation view. In:
Kosanke, K., Jochem, R., Nell, J.G., Bas, A.O. (eds.) Enterprise Inter- and Intra-
Organizational Integration. ITIFIP, vol. 108, pp. 273–282. Springer, Boston, MA
(2003). https://doi.org/10.1007/978-0-387-35621-1 28
6. Chen, D., Vernadat, F.: Standards on enterprise integration and engineering state
of the art. Int. J. Comput. Integr. Manuf. 17(3), 235–253 (2004)
7. Chen, D., Doumeingts, G., Vernadat, F.: Architectures for enterprise integration
and interoperability: past, present and future. Comput. Ind. 59(7), 647–659 (2008)
8. Charalabidis, Y., Lampathaki, F., Kavalaki, A., Askounis, D.: A review of elec-
tronic government interoperability frameworks: patterns and challenges. Int. J.
Electron. Gov. 3(2), 189–221 (2010)
9. Panetto, H., Cecil, J.: Information systems for enterprise integration, interoperabil-
ity and networking: theory and applications. Enterp. Inf. Syst. 7(1), 1–6 (2013)
10. Panetto, H.: Towards a classiﬁcation framework for interoperability of enterprise
applications. Int. J. Comput. Integr. Manuf. 20(8), 727–740 (2007)
11. ISO. 14258: Industrial automation systems–Concepts and rules for enterprise mod-
els. In: International Organization for Standardization, Geneva, Switzerland (2000)
12. ISO. ISO 19439:2006 - Enterprise integration - Framework for enterprise mod-
elling (2006). http://www.iso.org/iso/iso catalogue/catalogue tc/catalogue detail.
htm?csnumber=33833. Accessed 11 Feb 2016
13. Paviot, T., Lamouri, S., Cheutet, V.: A generic multiCAD/multiPDM interoper-
ability framework. Int. J. Serv. Oper. Inf. 6(1–2), 124–137 (2011)
14. Kifer, M.: Rules and ontologies in f-logic. In: Eisinger, N., Maluszy´nski, J. (eds.)
Reasoning Web. LNCS, vol. 3564, pp. 22–34. Springer, Heidelberg (2005). https://
doi.org/10.1007/11526988 2
15. Charalabidis, Y., Lampathaki, F., Askounis, D.: Investigating the landscape in
national interoperability frameworks (2010)
16. Wang, W., Tolk, A., Wang, W.: The levels of conceptual interoperability model:
applying systems engineering principles to M&S. In: Proceedings of the 2009 Spring
Simulation Multiconference, p. 168. Society for Computer Simulation International
(2009)
17. INTEROP. List of public deliverables from the IDEAS project I-V Lab Plat-
form (2003). http://interop-vlab.eu/ei public deliverables/ideas-deliverables/list-
of-ideas-deliverables. Accessed 11 Feb 2016
18. Ruggaber, R.: Athena-advanced technologies for interoperability of heterogeneous
enterprise networks and their applications. In: Interoperability of Enterprise Soft-
ware and Applications, pp. 459–460. Springer (2006). https://doi.org/10.1007/1-
84628-152-0 45

Semantic Interoperability for EIS
51
19. Vernadat, F.: UEML: towards a uniﬁed enterprise modelling language. Int. J. Prod.
Res. 40(17), 4309–4321 (2002)
20. Anaya, V., Berio, G., Harzallah, M., Heymans, P., Matuleviius, R., Opdahl, A.L.,
Panetto, H., Verdecho, M.J.: The uniﬁed enterprise modelling languageoverview
and further work. Comput. Ind. 61(2), 99–111 (2010)
21. NIST Process Speciﬁcation Language (PSL) (2008). http://www.mel.nist.gov/
psl/. Accessed 28 Jan 2016
22. Andrews, P., Zaihrayeu, I., Pane, J.: A classiﬁcation of semantic annotation systems
(2010)
23. ISA.
European
Interoperability
Framework
(EIF).
Towards Interoperability
for European Public Services (2011). http://ec.europa.eu/isa/documents/eif
brochure 2011.pdf. Accessed 11 Feb 2016
24. Agostinho, C., Ducq, Y., Zacharewicz, G., Sarraipa, J., Lam-pathaki, F., Poler,
R., Jardim-Goncalves, R.: Towards a sustainable inter-operability in networked
enterprise information systems: trends of knowledge and model-driven technology.
Comput. Ind. 79, 64–76 (2015)
25. Stoimenov, L.: Mediation and ontology-based framework for interoperability. In:
Ferraggine, V.E., Doorn, J.H., Rivero, L.C. (eds.) Handbook of Research on Inno-
vations in Database Technologies and Applications: Current and Future Trends,
pp. 491–507. IGI Global, Hershey, PA, USA, 2009
26. Xu, J., Bai, Z., Berre, A.J., Brovig, O.C.: Model driven interoperability through
semantic annotations using SoaML and ODM. Inf. Control Probl. Manuf. 13(1),
650–655 (2009)
27. ISO. ISO 11354–1:2011 - Advanced automation technologies and their applications
- Requirements for establishing manufacturing enterprise process interoperability -
Part 1: Framework for enterprise interoperability (2011). http://www.iso.org/iso/
iso catalogue/catalogue tc/catalogue detail.htm?csnumber=50417.
Accessed
02
Feb 2016
28. Rifaieh, R., Arara, A., Benharkat, A.N.: MurO: a multi-representation ontology
as a foundation of enterprise information systems. In: Das, G., Gulati, V.P. (eds.)
CIT 2004. LNCS, vol. 3356, pp. 292–301. Springer, Heidelberg (2004). https://doi.
org/10.1007/978-3-540-30561-3 31
29. Castano, S., Ferrara, A., Montanelli, S.: Ontology-based interoperability services
for semantic collaboration in open networked systems. In: Interoperability of Enter-
prise Software and Applications, pp. 135–146. Springer, London (2006). https://
doi.org/10.1007/1-84628-152-0 13
30. He, W., Xu, L.D.: Integration of distributed enterprise applications: a survey. IEEE
Trans. Ind. Inf. 10(1), 35–42 (2014)
31. Jardim-Goncalves, R., Grilo, A., Agostinho, C., Lampathaki, F., Charalabidis, Y.:
Systematisation of interoperability body of knowledge: the foundation for enter-
prise interoperability as a science. Enterp. Inf. Syst. 7(1), 7–32 (2013)
32. Sarraipa, J., Jardim-Goncalves, R., Steiger-Garcao, A.: MENTOR: an enabler for
interoperable intelligent systems. Int. J. Gen. Syst. 39(5), 557–573 (2010)
33. Kleppe, A.G., Warmer, J.B., Bast, W.: MDA Explained: The Model Driven Archi-
tecture: Practice and Promise. Addison-Wesley Professional, Boston (2003)
34. Elvester, B., Hahn, A., Berre, A.J., Neple, T.: Towards an interoperability frame-
work for model-driven development of software systems. In: Interoperability of
Enterprise Software and Applications, pp. 409–420. Springer, London (2006).
https://doi.org/10.1007/1-84628-152-0 36

52
T. M. de Farias et al.
35. Ducq, Y., Chen, D., Alix, T.: Principles of servitization and deﬁnition of an archi-
tecture for model driven service system engineering. In: van Sinderen, M., John-
son, P., Xu, X., Doumeingts, G. (eds.) IWEI 2012. LNBIP, vol. 122, pp. 117–128.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33068-1 12
36. Bazoun, H., Zacharewicz, G., Ducq, Y., Boye, H.: Transformation of extended
actigram star to BPMN2. 0 in the frame of model driven service engineering archi-
tecture. In: Proceedings of the Symposium on Theory of Modeling and Simulation
(2013)
37. Grangel, R., Correa, K., Antonio, F.D., Bourey, J.P., Berre, A.J.: Analysing
CIM2PIM approaches to improve interoperability. In: Enterprise Interoperabil-
ity II, pp. 115–118. Springer, London (2007). https://doi.org/10.1007/978-3-319-
04948-9
38. Stoimenov, L., Davidovic, N., Stanimirovic, A., Bogdanovic, M., Nikolic, D.: Enter-
prise integration solution for power supply company based on GeoNis interoper-
ability framework. Data Knowl. Eng. 105, 23–28 (2015)
39. Agostinho, C., Sarraipa, J., Goncalves, D., Jardim-Goncalves, R.: Tuple-based
semantic and structural mapping for a sustainable interoperability. In: Camarinha-
Matos, L.M. (ed.) DoCEIS 2011. IAICT, vol. 349, pp. 45–56. Springer, Heidelberg
(2011). https://doi.org/10.1007/978-3-642-19170-1 5
40. Tisi, M., Mart´ınez, S., Choura, H.: Parallel execution of ATL transformation rules.
In: Moreira, A., Sch¨atz, B., Gray, J., Vallecillo, A., Clarke, P. (eds.) MODELS
2013. LNCS, vol. 8107, pp. 656–672. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-41533-3 40
41. Sarraipa, J., Zouggar, N., Chen, D., Jardim-Goncalves, R.: Annotation for enter-
prise information management traceability. In: ASME 2007 International Design
Engineering Technical Conferences and Computers and Information in Engineering
Conference, pp. 893–899. American Society of Mechanical Engineers (2007)
42. Benslimane, D., Arara, A., Falquet, G., Maamar, Z., Thiran, P., Gargouri, F.:
Contextual ontologies. In: Yakhno, T., Neuhold, E.J. (eds.) ADVIS 2006. LNCS,
vol. 4243, pp. 168–176. Springer, Heidelberg (2006). https://doi.org/10.1007/
11890393 18
43. Baader, F., Calvanese, D., McGuinness, D.L., Nardi, D., Patel-Schneider, P.F.
(eds.): The Description Logic Handbook: Theory, Implementation, and Applica-
tions. Cambridge University Press, Cambridge (2003)
44. Gabbay, D.M., Kurucz, A., Wolter, F., Zakharyaschev, M. (eds.): Modal Logic
Basic. Studies in Logic and the Foundations of Mathematics, 1st edn., pp. 3–40.
Elsevier, Amsterdam (2003)

Information Systems Adoption

Accepting Information Technology Changes
in Universities - A Research Framework
Doina Danaiata(&), Ana-Maria Negovan, and Luminita Hurbean
West University of Timisoara, V. Parvan 17, 69121 Timisoara, Romania
{doina.danaiata,luminita.hurbean}@e-uvt.ro,
ana_negovan@yahoo.com
Abstract. Contemporary academic environment demands for continuous
improvement of managerial and educational performances in higher education
institutions. In this context, choosing and implementing solutions which provide
access to better services, aligning technological resources with university’s
mission and increasing data/information value is imperative. Romanian uni-
versities are faced with the need to implement Integrated Information Systems
(IIS) which can ensure an improved way of doing their day to day activities and
also provide informational support for managers. However, every implementa-
tion of integrated systems is challenging and puts to the test the skills of both
managers and implementers. The critical success factors (CSF) have been
studied extensively and we based our research on a vast body of literature.
Based on the discovered characteristics we decided to combine the CSFs with
the Technology Acceptance Model in order to deﬁne a speciﬁc model to eval-
uate and compare IIS implementation success in perspectives of managers and
end-users.
Keywords: Change management  Integrated Information System (IIS)
End user perspective  Management perspective  Critical success factors
1
Introduction
Universities must adapt to competitive requirements of the environment where they
operate. The favourable evolution of an academic institution essentially depends on the
ability to update and integrate, customize and extent their information systems in
ﬂexible and rapid way, providing instantaneous, interactive and consistent access to
information for all users.
Academic environment is undergoing rapid and remarkable changes due to the
nature of the challenges it faces: information overload, competitiveness, uncertainty
and possibility of organizational decline. According to [1, 2], the academic environ-
ment undergoes several major transformations:
1. Higher education becomes increasingly globalized.
2. Higher education system is developing the characteristics of a quasi-market.
3. Higher education is becoming a mass enterprise through the world.
4. Public funding has not kept up the expansion in student numbers.
5. Increasing state concern with quality.
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 55–69, 2018.
https://doi.org/10.1007/978-3-319-73459-0_4

6. Higher education curriculum is becoming more vocationally oriented.
7. Important changes have been done in the relations of industry and public
universities.
Information systems allow universities to gather relevant information that is used
for an improved management of educational curriculum and other related activities.
Self- knowledge at institutional level is the starting point for ensuring educational
programs of increased quality. Without an IIS that collects, processes, and delivers
important information about all activities from a university, managers won’t have an
overview about what needs their attention and the results of their innovative practices.
According to the European Association for Quality Assurance in Higher Education
(ENQA) an Integrated Information System (IIS) available in a university must provide
information, at least, about the following:
– Student lifecycle and graduation rate;
– Employment level for graduates;
– Satisfaction level of students in relation with educational programs;
– Effectiveness of professors;
– Students proﬁle;
– Available educational resources and associated costs;
– University performance indicators.
2
Integrated Information Systems in Universities -
Theoretical Framework
A success factor for organizations in a dynamic and competitive environment is an IT
strategy that supports business strategies and processes [3]. The way in which an
organization uses IT strategy to achieve business performance contributes to creation of
value and it represents a well know domain of study [3–7].
Aligning IT strategy with business strategy is crucial considering information
technology has become a fundamental part of an organization used as a competitive
asset [6]. In a 2001 article [8], Michael Porter afﬁrmed that Internet inﬂuences opera-
tional effectiveness and strategic positioning in different ways. Internet based applica-
tions provide a new way of reaching out to the customers. IIS using Internet based
interface facilitates access to the system by both university employees and stakeholders.
In developing a sustainable IT strategy, managers must consider that this will be a
part of business strategy, reﬂecting how information technology supports processes and
activities in reaching proposed objectives.
IIS provide required information for an effective organizational management [9].
IIS involve three main resources: technology, information and people. They are con-
sidered as a subset of internal control procedures in a business which covers all
practical needs for people, documents, technologies [5, 10]. IIS can be a useful tool for
collecting, analysing, evaluating, storing and disseminating information in decision
making process [11].
56
D. Danaiata et al.

A successful IIS must support an organization when an action plan has been deﬁned
and must provide required data for generating reports about critical performance
aspects. Also, feedback will be available in order to determine what action to take
depending on various aspects [12, 13].
IIS plays an important role in modern organizations, universities as well. At a
global level more and more organizations choose to implement them [7, 14].
Information systems (IS) are an important part of an organization [29]. Without IS
everyday tasks cannot be performed in an organization. For higher education institu-
tions, IIS represents a key element for smooth running activities, decreasing consid-
erably the workload and the time needed for accessing academic and economic
information. In their paper the authors [28, 29] present a systematic way in which
software applications spread information in an organization depending the right of
access to information for each user type. In Fig. 1, we have adapted theoretical ﬁndings
to higher education institutions speciﬁcs.
Information systems must respect a set of conditions in order to be considered as
Integrated Information Systems [7, 16, 30–32], as follows:
– The information system must use a unique database which can be accessed by all
users.
– The information system must gather under his umbrella all the activities performed
in an organization.
– The information system must be protected by a complex security system.
Top management (rector)
-strategic thinking and decision making using available 
IIS data- 
Middle management (dean, vicedean)
 -tactical decison making and thinking using IIS 
processed data- 
Operational management (department directors)
- operational decisions using available data, daily taks 
automation by processing and controlling data entry 
transactions- 
Operational Level (System Users)
- very important because the qulity of output (information 
used in different management levels for decision making) 
depends on the quality of the input - 
Fig. 1. Information importance depending on user’s access.
Accepting Information Technology Changes in Universities
57

2.1
Integrated Information Systems (IIS) Concept
In our literature review we have found many deﬁnitions for IIS (see Table 1). These
were very different depending on the vision of the authors these were directed in
different directions. Although, it is unanimously accepted that technology is essential in
creating an information system. When deﬁning IIS, the author must emphasize the
functional areas for which IS was created: accounting, production, sales, supply chain,
personnel and other.
Table 1. Deﬁnitions of IIS.
Source Year Concept
[33]
2014 IIS represents a set of software modules that interconnect functional areas of an
organization under a unique platform which will capture the informational
ﬂow. The dynamic of IT has allowed the development of IIS and barriers of
time and space have been broken. Integration of software platform with other
applications is possible, making the information available anywhere and
anytime
[34]
2014 IIS are implemented in order to grow the level of competitiveness of an
organization. The systems will grow with the organization itself, by covering
all functional areas and collecting all relevant information
[7]
2010 IIS are client/server based systems which are developed to process transactions
and facilitate the integration of all processes, from planning until production
but also to enhance business relations with all partners
[35]
2007 IIS includes planning and managing all the resources of an organization in an
efﬁcient, productive and proﬁtable way, by using conﬁgurable software
packages
[30]
2007 IIS is an information system that integrates all the processes of an organization
with the scope of creating value and decreasing costs by providing correct and
timely information to appropriate audience in order to support decision making
process for managing resources in a proactive and productive manner
[16]
2006 IIS are software packages that allow organizations to integrate business
processes and all relevant information. IIS supports efﬁcient resource
management and provides a complete view over an organization by providing
real time and accurate information
[36]
2005 IIS is an IT solution will provide a software package that covers all business
processes and all areas of an organization or of a group of organizations
[37]
2004 IIS is a tool used for organization management which comprises of an
integrated software package
[38]
2002 IIS is an approach to deliver a software that will support an organization by
combing different computerized systems of functional areas and running them
using a centralized database
[31]
2001 IIS comprises a set of software applications that support all key activities of an
organization.
IIS - particulars:
Ease of access to data through the organization;
Way to determine and decrease costs.
Information support for improving process and resource management in an
organization
58
D. Danaiata et al.

2.2
IIS for Higher Education Institutions
Considering information requirements found in European quality standards for higher
education and trends for higher education institutions [39–41], but also the advantages
and particularities of IIS, we can state that implementing an IIS in higher education
institutions represents a way in which these organizations can align with quality
requirements and to adapt to the dynamic environment in which these operate.
Using IIS will increase the quality of information delivered to all stakeholders.
Using a unique database [31, 38] at university level will allow to obtain consistent
and accurate data, by eliminating issues related to the use of standalone information
systems like data redundancy and inconsistency. Depending on the modules that are
implemented in higher education institutions and the functional area these cover,
managers can access information at a click. Information which is crucial in decision
making process. Moreover, IIS easily respond to reporting requirement deﬁned at
national level which have the scope of collecting unitary data about higher education
institutions and system.
University’s dynamic environment determine managers to keep up with stake-
holder’s requirements: from study programs to attracting possible students and
retaining them. The degree in which a university can attract students could be improved
by using information technology enhancements. IIS is a valuable asset for all univer-
sities and helps them adapt to ongoing changes.
IIS for universities must include general modules used for each domain of activity,
but also speciﬁc modules used for higher education, like:
– Module for
deﬁning organization structure of a university: faculties
and
departments;
– Module for managing study programs;
– Module for student data management;
– Module for managing student related activities: taxes, diplomas, accommodation
and others;
– Module/Interface which allow students to access information about their academic
and/or economic situation.
3
Research Framework
The research aims to distinct samples: top and middle management and end-users. This
allows a complete overview of IIS implementation. In general, top and middle man-
agers have a complete image of what are the advantages of implementing of IIS and
what they target.
The ﬁrst aspect that this research aims to determine or evaluate is how top managers
from Romanian universities have managed IIS implementation considering the
following:
– Strategies, speciﬁcally IT Strategies, for information systems implementation;
– Relations between university strategies and adoption of an IIS;
– Change management at university level in the context of IIS adoption.
Accepting Information Technology Changes in Universities
59

The second aspect that is targeted in our research is organizational change through
the end-user perspective. The research model aims to collect information about IIS
adoption in universities considering the perception of end-users over IT change, as
follows:
– Change management for users: information, education and training programs and
other measures undertaken to ease the transfer from the old legacy system to IIS.
– Top and middle management involvement in IIS implementation and supporting
employees in transition period.
Both models were built considering the review and analysis of critical success
factors (CSFs) for IIS implementation (see Table 2). Nevertheless, we consider that a
smooth ﬂow for IIS implementation must be supported by manager’s involvement in
all stages and at all levels, by implementation team efforts and delivered know-how,
and not the last, by effective and efﬁcient communication that targets end users in order
to reduce and avoid resistance to change.
3.1
IIS for Higher Education Institutions
The research model (Fig. 2) which aims to determine which factors impact the success
of an IIS implementation from a managerial perspective was created from scratch
considering CSFs for IIS implementation, as found in our literature review. On the other
side, we have considered the main directions which determine an organizational
behaviour in response to competitive environment and related trends, and their inﬂu-
ences on strategic directions. From our point of view one of the most important factors
are change management initiatives, because it is acknowledged in literature that people
are inherently resistant to change, and avoiding or resisting change is human nature. [22]
User resistance to change reveals that inefﬁcient communication is present between
change initiators and employees. The latter try to maintain their status quo with
undesirable behaviours toward change which are a response to management-imposed
changes in job and work methods. [23]
Table 2. List of critical success factors in IIS implementation
Critical success factors in IIS implementation
Authors
Management of implementation project
[15–17]
Implementation team- composition and competence and how team work is done [15–17]
Business Process Redesign and a minimum IIS customization
[15–19]
Effective communication and knowledge transfer
[15, 17–20]
Develop and test the new IIS, but also resolve related issues
[15, 17, 21]
End-user training
[18, 19, 21]
Change management programs
[15–17]
Support and involvement of top management
[15–20]
60
D. Danaiata et al.

The research hypotheses are:
H1: The trends of academic environment inﬂuence the strategies of a university.
H2: The strategies adopted by a university strongly inﬂuence IIS successful
implementations.
H3: The strategies adopted by a university strongly inﬂuence change management
activities.
H4: Change management is a key successful factor in IIS implementations.
H5: A successful IIS implementation is inﬂuenced by several factors.
H5.a: A correctly managed IIS implementation is crucial for obtaining a suc-
cessful outcome.
H5.b: Implementation team can have a signiﬁcant impact on IIS implementation
project.
H5.c: Business Process Redesign (Reengineering) and a minimum customiza-
tion of ISS can strongly inﬂuence IIS implementation.
H5.d: Effective communication and knowledge transfer have a signiﬁcant
impact on implementation success.
Fig. 2. Information importance depending on user’s access.
Accepting Information Technology Changes in Universities
61

H5.e: Development and test of new IIS and resolving correctly and on time
related issues have a signiﬁcant impact over successful outcome.
H5.f: End user training can signiﬁcantly increase the success rate of IIS
implementations.
H6: The success of an IIS implementation depends on demographic factors, like
age, gender, level of studies, professional and educational proﬁle, and experience.
The research form is available on Google Forms (https://goo.gl/8yXEPb) and it is
an ongoing study. Until now, answers were collected from several universities in
Romania. Answers were collected from managers in 6 universities (see Table 3).
Preliminary ﬁndings about the stage of information system implementation in
Romanian higher education institutions are presented in the Table 4. Over 95% of
respondents’ state that information systems are used in universities in order to collect
relevant information. All universities from our study use IIS (over 50%), but also other
software that responds to different speciﬁc requirements (almost 40%).
Correlating the types of information systems used in universities and the stage of
their implementation (Table 5) we have found the following:
– 40% of our respondents said that the implementation of an Integrated Information
System was successfully completed over a year ago and they are using the system.
While only 3.5% said the system is not used anymore and 14% said that they are in
the implementation of IIS is in progress.
Table 3. Manager’s Responses by Universities
University
Response rate
(% from total responses)
A.I. Cuza University Iasi
53.33%
Academy of Economic Studies Bucharest
3.33%
Babes Bolyai University Cluj
13.33%
University from Craiova
10.00%
Lucian Blaga University Sibiu
3.33%
West University Timisoara
16.67%
Table 4. Information Systems used in Universities
Type of IS used
Responses (% from total)
Stand-alone information system
39.29%
Integrated Information System
57.14%
Manual solution with IT support
3.57%
62
D. Danaiata et al.

– Almost 30% of the respondents said that they are using stand-alone application in
their universities and these were implemented over a year ago, while just 10% said
that they are in the process of implementing an information system.
– Only 3.5% said that they are using systems that require manual tasks in order to
collect, analyse and deliver correct information.
3.2
Research Model for End User Perspective on IIS Implementation
The term acceptance is used by authors with different backgrounds and approaches. In
fact, in the literature, acceptance does not have a unique deﬁnition. TAM [24] describes
acceptance as users’ decision about how and when they will use technology. [25] When
faced with such a major change in an organization you ﬁnd out that there are several
constraints related to the ability to take action in several situations, to the organization
and the business environment, to different habits.
New technologies are complex and uncertainty elements exist in decision making
for successful implementation, because people create their own attitude and intentions
toward using of new technologies. [26].
The model ﬁrst proposed by Davis [24] was successfully used in different research
domains. It was modiﬁed to better suit research needs by introducing several other
variables which could have an impact on ease of use, usefulness and behavioural
intentions.
For the research which aims to quantify managerial perspectives we will use TAM
2 which was published in the article by Venkatesh and Davis [27]. The constructors
present in the original model are presented in Table 6, as described by Venkatesh and
Davis (2000).
Table 5. Situation of IS Implementation in Romanian Universities
Stage of IS implementation
Type of
information
systems
In
progress
Was done
over 1 to 3
years ago
Was done
over 3 to 5
years ago
Was done
over 5 years
ago
Was done but the
system is not used
anymore
Stand-alone
information
system
10.71%
7.14%
3.57%
17.86%
0.00%
Integrated
Information
System
14.29%
21.43%
7.14%
10.71%
3.57%
Manual
solution with
IT support
3.57%
0.00%
0.00%
0.00%
0.00%
Accepting Information Technology Changes in Universities
63

Using TAM 2 as a starting point we have added the critical success factors for IIS
implementation (see Fig. 3).
The research hypotheses developed based on the research model are the following:
H1: The involvement of university’s management has a strong impact on an end
user perception on using an IIS.
H1.a: Top management support for end users in understanding the usefulness of
IIS implementation is crucial.
H1.b: Top management implication strongly inﬂuences behavioural intentions
of end users.
H1.c: Top management implication strongly inﬂuences behaviour in use of end
users.
H1.d: Change management activities impact the perception of end users
towards IIS usefulness.
H1.e: Change management activities impact the behavioural intentions of end
users towards IIS.
H1.f: Change management activities impact the behaviour in use of end users
towards IIS.
H2: Critical success factors for IIS implementations have a strong inﬂuence on
perceived usefulness and utility, but also over behavioural intentions and behaviour
in use.
Table 6. Constructors’ description
Constructor
Details
Experience
How the user’s experience in his domain inﬂuences the way he perceives
the new system
Availability
The degree in which the possible users feel like the decision for adopting
the new system is not mandatory
Subjective
norms
The person’s perception that most people who are important to him think
he should or should not perform the behaviour in question
Image
The degree in which the use of an innovation is perceived as a way to
increase a person’s status
Job relevance
The person’s perception on relevance level of the new system for their job
and related activities
Output quality
The degree in which the new system will support the end user in their
daily activities
Result
tangibility
The way in which ﬁnal result obtained using the system are perceived
Perceived utility
The degree to which a person believes that using a particular system
would enhance his or her job performance
Perceived ease
of use
The degree to which a person believes that using a particular system
would enhance his or her job performance
Behavioural
intention
The degree in which a person has created or not an action plan towards a
future behaviour
Behaviour in
use
Positive or negative attitude of a person related to a certain intention
64
D. Danaiata et al.

Fig. 3. Research framework for university’s’ end-user perspective- and adaptation of TAM2 including Critical Success Factors for Integrated
Information System Implementation.
Accepting Information Technology Changes in Universities
65

H3: The management of the implementation project has a strong inﬂuence over
perceived usefulness and utility, but also over behavioural intentions and behaviour
in use.
H4: The implementation teams, their composition and competences, and how they
work together as a team inﬂuence the end user’s perception on usefulness and
utility, but also his/her behavioural intentions and behaviour in use.
H5: Business Process Redesign (Reengineering) and a minimum customization
inﬂuence the end user’s perception on usefulness and utility, but also his/her
behavioural intentions and behaviour in use.
H6: Effective communication and knowledge transfer have a strong impact on end
user’s perception on usefulness and utility, but also his behavioural intentions and
behaviour in use.
H7: Development and test of new IIS and resolving correctly and on time related
issues inﬂuence the end user’s perception on usefulness and utility, but also his/her
behavioural intentions and behaviour in use.
H8: Training programs and how these are done inﬂuence the end user’s perception
on usefulness and utility, but also his/her behavioural intentions and behaviour in
use.
H9: Subjective norms have a strong impact on the end user’s perception of image,
usefulness and behavioural intentions.
H10: Image is a key factor that inﬂuences the end user’s perception on IIS
usefulness.
H11: Job relevance is a key factor that inﬂuences the end user’s perception on IIS
usefulness.
H12: Output quality is a key factor that inﬂuences the end user’s perception on IIS
usefulness.
H13: Results tangibility is a key factor that inﬂuences the end user’s perception on
IIS usefulness.
H14: Experience is a key factor that inﬂuences the end user’s perception on IIS
usefulness and behavioural intentions.
H15: Availability is a key factor that inﬂuences the end user’s behavioural
intentions.
H16: Perceived ease of use has a strong impact on end user’s perception over IIS
usefulness and behavioural intentions.
H17: Perceived usefulness has a strong impact on end user’s behavioural
intentions.
H18: Behavioural intentions have a strong impact on behaviour in use.
H19: Post implementation services (availability of IIS consultant, response time for
resolving different situations, maintenance and upgrade of IIS) have a strong
impact on behaviour in use of IIS end user’s.
H20: Does the success of an IIS implementation depend on demographic factors,
like age, gender, level of studies, professional and educational proﬁle, experience
or others?
Answers from end-users are collected using a questionnaire created in Google
Forms (https://goo.gl/Je081u). We have tested some of our hypothesis in previous
66
D. Danaiata et al.

research papers, the research framework presented here being an improved and more
comprehensive one.
Our research was done in several Romanian universities and the response rate in
presented in Table 7. The two universities with the highest response rate are from
Timisoara and Iasi, which have Integrated Information Systems already implemented.
The ﬁndings related to IS implementation stage (Table 8) showed that 40% have
implemented the system over 5 years ago, while the same situation we encounter for
implementations completed over 1 year ago up to 5 years, only 5% being completed in
the last year and 15% are in progress implementations. More important is that we have
skilled and experienced end-users. This will relate to improved quality of data entry
and of information output.
4
Conclusions
Although ERP implementation in universities is often described as difﬁcult, costly and
risky and sometimes has been considered failed or ineffective, its adoption in this sector
has continued. We witnessed a large research effort of understanding the phenomenon of
ERP (IIS) adoption and evaluation in universities. The study ﬁndings conﬁrm the unique
nature of IIS adoption in universities and summarise inﬂuences on ERP implementation,
having in mind the two major perspectives: top management and end-users.
From our previous researches we found that the most important factors in IIS
implementations are users and their perception of the change process [7]. This is why
the current research took two directions; the ﬁrst one aims to determine how top
Table 7. End-user’s Responses by University
University
Response rate
(% from total responses)
West University Timisoara
26.19%
A.I. Cuza University Iasi
57.14%
Babes Bolyai University Cluj
9.52%
Politechnic Institute Cluj-Napoca
2.38%
University from Craiova
2.38%
Academy of Economic Studies Bucharest
2.38%
Table 8. Years of end-user experience versus IS implementation stage
Years of experience and system usage
Period since IS
implementation
Up to 6
months
6 months up
to 1 year
1 to 2
years
2 to 3
years
3 to 5
years
Over 5
years
Within last year
0.00%
0.00%
0.00%
4.76%
0.00%
0.00%
1 to 3 years ago
2.38%
0.00%
2.38%
2.38%
2.38%
2.38%
3 to 5 years ago
0.00%
0.00%
0.00%
0.00%
14.29%
9.52%
Over 5 years ago
0.00%
0.00%
0.00%
0.00%
0.00%
42.86%
Accepting Information Technology Changes in Universities
67

managers from Romanian universities perceived the IIS implementation and the second
one focuses on end user’s acceptance of the new information system.
Using TAM 2 and the set of CSFs for IIS implementation, we have developed two
particular/customized research models for the two mentioned categories. This paper
presents also a set of hypothesis for both approaches that we aim to validate in the
following steps of the present research.
References
1. Knight, P.T., Trowler, P.R.: Departmental Leadership in Higher Education. The Society for
Research in Higher Education and Open University Press, Buckingham (2001)
2. Xianming, X.: Academic management and administration system reform in higher education
institutions. Front. Educ. China 1, 70–78 (2006)
3. Henderson, J., Venkatraman, N.: Strategic alignment: leveraging information technology for
transforming organizations. IBM Syst. J. 32, 472–484 (1993)
4. Luftman, J.N., Lewis, P.R., Oldach, S.H.: Transforming the enterprise: the alignment of
business and information technology strategies. IBM Syst. J. 32(1), 198–221 (1993)
5. Kerimoglu, O., Basoglu, N., Daim, T.: Organizational adoption of information technologies:
case of ERP systems. J. High Technol. Manag. Res. 19, 21–35 (2008)
6. Al Majali, D., Dahlin, Z.: Diagnosing the gap in IT - business strategic alignment: a
qualitative analysis among public shareholding ﬁrms in Jordan. Int. J. Electron. Bus. Manag.
8(4), 263–271 (2010)
7. Fotache, D., Hurbean, L., Dospinescu, O., Pavaloaia, V.D.: Procese organizaţionale şi
integrare informaţională - Enterprise Resource Planning. Editura Universităţii “Alexandru
Ioan Cuza”, Iaşi (2010)
8. Porter, M.: Strategy and the Internet. Harvard Bus. Rev. 79(3), 62–78 (2001)
9. O’Brien, J.: Management Information Systems. Irwin McGraw-Hill, Boston (1999)
10. O’Connor, N., Martinsons, M.: Management of information systems: insights from
accounting research. Inf. Manag. 43, 1014–1024 (2006)
11. Lecuit, L., Elder, J., Hurtado, C., Rantrua, F., Siblini, K., Tovo, M.: Demystifying MIS-
Guidelines for Management Information Systems in Social Funds. Technical paper no. 443.
World Bank, Washington D.C. (1999)
12. Wagner, C.: Enterprise strategy management systems: current and next generation.
J. Strateg. Inf. Syst. 13, 105–128 (2004)
13. Nasir, S.: The development, change, and transformation of management information
systems. Bus. Market. J. 25, 442–457 (2005)
14. Grabski, S.V., Leech, S.A., Schmidt, P.J.: A review of ERP research: a future agenda for
accounting information systems. J. Inf. Syst. 25(1), 37–78 (2011)
15. Nah, F., Lau, J., Kuang, J.: Critical factors for successful implementation of enterprise
systems. Bus. Process Manag. J. 7(3), 285–296 (2001)
16. Nah, F., Delgado, S.: Critical success factors for enterprise resource planning implemen-
tation and upgrade. J. Comput. Inf. Syst. 46(5), 99–113 (2006)
17. Francoise, O., Bourgault, M., Pellerin, R.: ERP implementation through critical success
factors’ management. Bus. Process Manag. J. 15(3), 371–394 (2009)
18. Muscatello, J.R., Chen, I.J.: Enterprise resource planning implementations: theory and
practice. Int. J. Enterp. Inf. Syst. 4(1), 63–77 (2008)
19. Stratman, J.K., Roth, A.V.: Enterprise resource planning competence constructs: two-stage
multi-item scale development and validation. Decis. Sci. 33(4), 601–628 (2002)
68
D. Danaiata et al.

20. Maditinos, D., Chatzoudes, D., Tsairidis, C.: Factors affecting ERP system implementation
effectiveness. J. Enterp. Inf. Manag. 25(1), 60–78 (2012)
21. Gargeya, V.B., Brady, C.: Success and failure factors of adopting SAP in ERP system
implementation. Bus. Process Manag. J. 11(5), 501–516 (2005)
22. Bovey, W., Hede, A.: Resistance to organizational change: the role of cognitive and affective
processes. Leadersh. Org. Dev. J. 22(8), 372–382 (2001)
23. Piderit, S.: Rethinking resistance and recognizing ambivalence: a multidimensional view of
attitudes toward an organizational change. Acad. Manag. Rev. 25(4), 783–794 (2000)
24. Davis, F.D.: Perceived usefulness, perceived ease of use, and user acceptance of information
technology. MIS Q. 13, 319–339 (1989)
25. Negovan, A.M., Danaiata, D., Hurbean, L.: Using technology acceptance model for
managing IT changes in universities. In: Proceedings of the 7th International Conference
Management of Technological Changes, Alexandropoulos, Greece (2011)
26. Davis, F.D., Bagozzi, R.P., Warshaw, P.R.: User acceptance of computer technology: a
comparison of two theoretical models. Manag. Sci. 35(8), 982–1003 (1989)
27. Venkatesh, V., Davis, F.D.: A theoretical extension of the technology acceptance model:
four longitudinal ﬁeld studies. Manag. Sci. 46(2), 186–204 (2000)
28. Chaffey, D., Bocij, P., Greasley, A., Hickie, S.: Business Information System, Technology,
Development & Managemnt for the E-Business, 3rd edn. Pearson Prentice Hall, Harlow
(2006)
29. Laudon, K.C., Laudon, J.P.: Management Information Systems: Managing the Digital Firm,
10th edn. Pearson Prentice Hall, Upper Saddle River (2007)
30. McGaughey, R.E., Gunasekaran, A.: Enterprise Resource Planning (ERP): Past, Present and
Future, pp. 23–35. IGI Global, Hershey (2007)
31. Aladwani, A.M.: Change management strategies for successful ERP implementation. Bus.
Process Manag. J. 7(3), 266–275 (2001)
32. Marnewick, C., Labuschagne, L.: A conceptual model for enterprise resource planning
(ERP). Inf. Manag. Comput. Secur. 13(2), 144–155 (2005)
33. Beheshti, H.M., Blaylock, B.K., Henderson, D.A., Lollar, J.G.: Selection and critical success
factors in successful ERP implementation. Compet. Rev. 24(4), 357–375 (2014)
34. Al-Ghofaili, A.A., Al-Mashari, M.A.: ERP system adoption traditional ERP systems vs.
cloud-based ERP systems. In: Fourth International Conference on Innovative Computing
Technology (INTECH), pp. 135–139 (2014)
35. Laukkanen, S., Sarpola, S., Hallikainen, P.: ERP system adoption - does the size matter? In:
Presented at International Conference on System Sciences, Hawaii (2005)
36. McAdam, R., Galloway, A.: Enterprise resource planning and organisational innovation: a
management perspective. Ind. Manag. Data Syst. 105(3), 280–290 (2005)
37. Shehab, E.M., Sharp, M.W., Supramaniam, L., Spedding, T.A.: Enterprise resource
planning: an integrative review. Bus. Process Manag. J. 10(4), 359–386 (2004)
38. Payne, W.: The time for ERP? Work Study 51(2), 91–93 (2002)
39. Projects co-ﬁnanced by European Social Fund for POSDRU 2007–2013, Quality and
Leadership for Higher Education in Romania, Vision- Higher Education in Romania in
2015, 20 June 2011
40. Projects co-ﬁnanced by European Social Fund for POSDRU 2007–2013, Quality and
Leadership for Higher Education in Romania, Green Book: To Quality and Leadership in
Romania’s Higher Education in 2015 (2011). Andreescu, L., Curaj, A., Zulean, M.
41. Projects co-ﬁnanced by European Social Fund for POSDRU 2007–2013, Quality and
Leadership for Higher Education in Romania, White Book for Quality and Leadership in
Romania’s Higher Education in 2015, ISBN 978-973-0-11850-6
Accepting Information Technology Changes in Universities
69

A Survey on Social Learning Analytics: Applications,
Challenges and Importance
Maria-Iuliana Dascalu1(✉), Constanţa-Nicoleta Bodea2,3, Radu Ioan Mogos2,
Augustin Purnus4, and Bianca Tesila1
1 Department of Engineering in Foreign Languages, Politehnica University of Bucharest,
Splaiul Independentei 313, 060042 Bucharest, Romania
maria.dascalu@upb.ro, bianca.tesila@gmail.com
2 Department of Economic Informatics and Cybernetics, The Bucharest Academy of Economic
Studies, Calea Dorobanţi 15-17, 010552 Bucharest, Romania
bodea@ase.ro, mogos.radu@gmail.com
3 Centre for Industrial and Services Economics, Romanian Academy,
Casa Academiei, Calea 13 Septembrie 13, 050711 Bucharest, Romania
4 Technical University of Civil Engineering,
Lacul Tei Bvd. 122–124, 020396 Bucharest, Romania
purnus@utcb.ro
Abstract. The current paper debates upon the importance of learning analytics,
by underlining its place in current research directions and its inﬂuence in
reshaping education with the aid of cutting-edge technologies. Deﬁnitions,
successful examples of learning analytics tools, as well as the challenges of
implementing them are revised. A special attention is awarded to learning
analytics performed in social learning environments, due to their popularity and
well-established value. In order to investigate the importance of learning analytics
to end-users (students and teachers), a survey based on two questionnaires was
performed and revealed the fact that advanced learning analytics features are
highly requested in online environments. The questionnaires were ﬁlled by 59 IT
teachers and more than 390 IT students, from various countries, fact that demon‐
strates the global signiﬁcance of the study. Also, a model of developing a feasible
social learning solution with learning analytics features is proposed, shaping thus
the feature research directions.
Keywords: Learning analytics · Social learning · Data mining
1
Introduction
Data analysis from virtual learning environments (VLE), commonly known as learning
analytics (LA), is a current concern of technology-enhanced learning (TEL) researchers,
as a follow-up of several factors: technological progress determined by the emergence
of big data and cloud technologies, sensory technologies, virtual reality; the importance
given to new forms of education (continuous, competences oriented, collaborative/
social, online) [1]; the widespread use of so-called Massive Open Online Courses
(MOOCs) or Learning Management Systems (LMS), such as Moodle, Blackboard,
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 70–83, 2018.
https://doi.org/10.1007/978-3-319-73459-0_5

Sakai; political and economic factors - the directives of the European Digital Agenda
(EDA) target increases of gross domestic product through R&D investments and
reshaping education, thus aiming the maximization of the beneﬁts given by the digital
revolution [2]. According to EDA, education should be more equitable (knowledge more
accessible to all, at lower costs), more eﬀective (the procurement of education should
be easier, the use of open resources and open online courses should be enlarged) and
should have positive impacts on economy. These goals can only be achieved through a
continuous measurement and improvement of educational activities (virtual, traditional
or blended), aspect which is taken into account by LA.
Based on the deﬁnition proposed by the Society for Learning Analytics Research
(SOLAR), LA involves “measurement, collection, analysis and reporting of data about
students and learning contexts in order to understand and optimize learning and the
environments in which it takes place” [3]. Although concerns for data analysis in educa‐
tion have existed since 1979 and have started with the publication of a research carried
out by The Open University in the UK, Fergurson emphasizes the LA evolution over
time [4]. Today, LA means “intelligent use of data produced by students and analysis
of existing patterns in the data, ﬁnding information and social connections to predict
and advise on learning paths” [5].
At the same time, the cultural values of contemporary society have changed:
according to Harvard Business Review, the time to improve one’s proﬁle and interact
on current social networks is considered very useful when searching for a job [6] and
online learning environments are seen as the great educational revolution [7]. LA aims
to explain the impact of cultural changes on the learning process and to optimize it
accordingly. Lastly, LA has a proven scientiﬁc importance acknowledged by [8], which
classiﬁes this area as being among the most emerging research ﬁelds nowadays. LA
provides results related to students’ virtual proﬁle on several levels: the descriptive level
(what happened), the diagnostic level (why it happened), the predictive level (what might
happen, e.g. it may signal the lack of performance or risk of failure of a certain student)
and prescriptive level (what should be done, e.g. recommendation of educational
resources or communities of practice to improve one’s learning performance). The main
beneﬁciaries of LA are, obviously, the students, who will become aware of their involve‐
ment or lack of involvement in educational activities, of the quality of this involvement
and its impact on their lives (e.g. for ﬁnding a job). Other important categories of
beneﬁciaries are the teachers/trainers, who will be able to improve their teaching act, to
oﬀer the students what they need and the universities, which will be able to assess and
prove the eﬃciency of their educational oﬀer.
The current paper reviews successful examples of LA applications, underlines the
challenges faced by them and reveals the importance of LA in the context of a social
learning platform, based upon the results obtained via an extensive survey undertaken
mainly in Danube Region countries during March-April 2016. Other surveys related to
LA have been made [9, 10], but not at this scale and not cross-national.
A Survey on Social Learning Analytics
71

2
Successful Examples of Learning Analytics Applications
LA answers various questions related to learners’ knowledge and pace of their progress,
helping educational institutions to deliver learner models, adjust their instruction model
and forecast the evolution of their learners with regards to a particular subject. This
section aims to give a broader look on what kind of data LA tools exploit and how they
interpret it to achieve one’s goals.
Applications Areas. The application areas of LA are: user knowledge modeling, user
behavior modeling, user experience modeling, user proﬁling, domain modeling, learning
component analysis and instructional principle analysis, trend analysis, adaptation and
personalization [11]. Each application area is concerned with using a speciﬁc type of
data, answering a particular set of questions with regards to a research topic on improving
learning and teaching [11].
Social LA. A promising direction of LA research is the one of LA performed in social
learning environments [12, 13], which are a common way of e-education [14]: the efforts
in this direction are more exploratory than explanatory. Shum & Ferguson identify several
sub-domains of social LA [15]: two of them are inherited from other areas - (1) social
network analysis to see the connection between students, educational resources and ideas
and (2) discourse analysis, to see how to build knowledge; three specialized – (1) content
analysis of actions taking place in the virtual environment, (2) analysis of the student’s
mood, meaning what motivates him to learn and (3) context analysis, supported in partic‐
ular by mobile technologies, which inform us about the learning habits, e.g. when and
where learning takes place.
Examples. There are many examples of LA applications, such as: a LA extension for
a better understanding of the educational processes within the Khan Academy platform
[16], Active Campus – an application that can connect students and resources, through
alerts about friends, colleagues or potential educational events in one’s proximity, there‐
fore promoting the “opportunistic” learning [17], MOBIlearn – which exploits the
context analysis [18] or GRAPPLE Visualisation Infrastructure Service (GVIS) plat‐
form – which aggregates data about users working in diﬀerent virtual learning environ‐
ments and create their proﬁles [19]. A notable attempt of doing LA with cutting-edge
technology is the one of Fernández-Gallego et al. [20], who performed analytics for
virtual learning worlds developed in Opensim. Below, ASSISTments and LearnTracker,
other successful LA applications validated by real-world experiments, are thoroughly
described.
ASSISTments is a web platform that successfully combines online learning assis‐
tance and assessments activities, which was developed by Worcester Polytechnic Insti‐
tute, in collaboration with the Carnegie Mellon University [21]. ASSISTments is the
proof that ﬁne-grained learner data can be structured in a meaningful way to provide
evidence of learner problem-solving sequences, knowledge status and strategic
behavior. The platform currently manages to tutor students while they practice on prob‐
lems and various theoretical concepts, whilst providing educators with a detailed view
72
M.-I. Dascalu et al.

of their students’ progress and developing skills, as well as their weak points with respect
to particular subjects. When the students log in the ASSISTments platform to submit
their assignments, they receive hints and tutoring to the appropriate extent they need.
At the same time, the platform uses all the information on how individual students
interact with its features, in terms of how they respond to the questions and how much
support they need from the system to generate correct responses and feedback, as
assessment information. Consequently, the more the students use the platform, the more
ASSISTments learns about their abilities and can provide increasingly appropriate
tutoring for each student and can provide increasingly accurate forecast on how well an
individual will perform with respect to a particular subject. This can also be stated by
various studies concerning the students’ interaction with the platform and their overall
progress. One of the most interesting ﬁndings of the platform usage was the fact that,
by taking into account the information on the quantity and quality of help the student
request by performing their assignments, the platform can forecast more accurately on
the student’s performance with regards to that subject, than by considering the number
of items the students get correct on that subject related assessments [22]. Another study
has shown that learning increases when students do their homework [23]. SkillBuilders
is another feature that has been analyzed in the scope of a study [24], which has proved
that the platform also fosters the retention of skills. This allows teachers to assign prac‐
tice of individual skills to students, based on their previous performance. In the highlight
of the plethora of the studies that have been conducted using ASSISTments, we can
conclude that this project is a successful one according to the current trend of fostering
online learning and harnessing the data it can provide in order to improve both learning
and teaching.
Another successful LA application is LearnTracker, developed by Open Universiteit
from Netherlands and used in a study to analyze the role of mobile learning analytics in
self-regulated learning [9]. LearnTracker is a cloud based solution in which teachers can
create courses and deploy them to mobile devices. From a technical point of view, the
solution is made up from a set of Java RESTful web services that implement an open
API, being easily used by mobile clients (Android, iOS, Windows and Blackberry) and
web clients (Chrome, Safari etc.). Both database and web services are deployed and
running in Google App-Engine. The LearnTracker backend operates via messages in
standard JSON. While the users interact with the platform, data is collected in mean‐
ingful chunks based on an innovative data model. The platform has been created on the
assumption that lifelong learners can study in a variety of scenarios, using a personal
device as a mediator. In this view, the platform, that performs LA using the data recorded
during the learners’ study time, can provide feedback on the time devoted to each
learning activity. This data is easily collected since, each user can manually log time
spent on a particular subject, using a particular resource. All this history makes up a user
proﬁle within the platform and facilitates the performance of meaningful analytics. The
study depicted in [9] has been conducted by repeatedly measuring the “validity and
reliability of time management” and “self-regulated learning”, in order to assess what
positive eﬀects would have in self-regulated learning a special focus on how learners
manage their time. In this view, the time-logs recorded by the students are used to analyze
and to decipher patterns describing how they learn along the day, along the week and
A Survey on Social Learning Analytics
73

during the whole course. Relevant ﬁndings suggest that using such an application to log
time and track the time devoted to study has led to the improvement for time management
skills. Other learning analytics applications are plugin-based: e.g. for Moodle learning
management system, some LA plugins are “SmartKlass” which collects data related to
user behavior [25] or “Engagement analytics” which identiﬁes activities with an impact
on student success in an online course [26], for Blackboard learning management system
there is a so-called Retention Centre which is an “early warning system” and aims at
identifying students at risk of failure, based on rules set by teachers [27].
3
Challenges in Building Meaningful Learning Analytics
Applications
Technical Challenges. LA relies on large data sets or so called Big Data. In this view,
all the educational institutions and related organizations have started to build their own
data sets around the online learning environments they provide for their enrolled
learners, for conducting their customized LA related research studies. At ﬁrst, this may
look like a good outcome – more and more institutions starting to understand and explore
how much they can harness from learners’ behaviour within a real e-learning platform.
However, this has also led to a signiﬁcant issue, which may put researchers into more
diﬃcult situations, namely the fact that there might be no interoperability among the
current multiple educational data systems. Consequently, there should be deﬁned clear
standards on how educational data should be gathered: grades, standardized test scores,
ways of representing attendance, lectures contents etc. In this view, the National Center
for Education Statistics (NCES) has already started to support eﬀorts to create intero‐
perability educational data through the medium of the Common Education Data Stand‐
ards, encouraging the usage of voluntary standards for educational data [28]. By using
such standards, it would be easier to move data across data systems and across educa‐
tional institutions and organizations, hence it would be a great achievement towards a
common research direction. Another Big Data challenge is the fact that there is so much
data that it is highly signiﬁcant to identify what kind of data needs to be collected to be
able to provide meaningful outcomes by using analytics and data mining. This also
implies focusing on what research questions should be answered and making sure that
the harnessed information is compliant with them. Moreover, researchers also need to
understand what kind of questions data mining and analytics can answer to. What we
want to ask and what these techniques can actually answer may diﬀer more than we
thought. According to one expert interviewed, if there are 100 people working for a
research topic, 99 of them should be looking for what questions to answer to and one
for the technical process of data mining [11].
Last but not least, there is a current trend for personalizing the learner experience
within various learning context, especially online ones. In this view, researchers have
to track multiple kinds of data, like online behaviour, grades, submitted assignments,
pace of progress etc. Thus, personalizing the learning experience means going through
multiple sources of data to provide an integrated view of the learner’s progress. In addi‐
tion to this, researchers seek to create learner proﬁles based on the ﬁndings, such that
74
M.-I. Dascalu et al.

the personalization can become an automate process in the next years. However, the
existing educational data sets may not be designed to oﬀer support towards creating such
predictive models – a model may need an important variable that is not covered by a
speciﬁc data set.
Institutional Challenges. Conducting research studies on LA requires having special‐
ized human resources, with high expertise on data preparation, processing and analysis.
Moreover, they should also be closer to the educational and learning process as well. This
is why the majority of such studies are conducted by educational institutions – this is the
right place for having access to real student data and real feedback from both students and
educators. However, there is still a lack of human resources within these institutions.
The eﬀort in data mining and analytics highly involves data cleaning, formatting and
alignment, leaving alone the fact that they also need to get access to data across diﬀerent
levels of the learning system. Many of the educators involved in research are over‐
whelmed by the amount of work required to go through a minimal study.
Ethical Challenges. Leveraging the work of the market research, many LA studies
have started by tracking the behavior of learners within online learning environments.
This has led to signiﬁcant privacy implications because these logs contain learners’
personal information that, even though is used by the software products themselves, no
one can guarantee that the usage of such information cannot be extended to other partic‐
ular cases as well. Many LA and educational data mining researches have exposed data
privacy weaknesses – for instance, querying social networks often leads to having access
to personal information like email address, full name, job description, education etc.
[29]. Moreover, even though users’ satisfaction seems to increase with having a person‐
alized user experience, consumer surveys also outline the fact that many people are
concerned about their identity and privacy on the Internet [29]. In this view, educational
institutions and related organizations must take into consideration the privacy of the
learners while collecting, storing, analyzing and using personal information harnessed
from learners’ education records to third parties for data mining and analytics, especially
when they do not have their own specialized human resources. Moreover, most of the
LA applications make predictions and recommend actions and activities based on the
historical behavior of the learner within a particular context. However, this can be
harmful – showing a learner that he has a low chance for passing a subject according to
his past records might have a harmful impact, from a cultural point of view. Conse‐
quently, researchers should bear an ethical responsibility to investigate the validity of
any predictive model that is used to make decisions about learners, depending on what
type of learners they actually target.
Challenges of Social LA. Ferguson [4] identiﬁes the current limitations of LA, in the
context of the explosion of social networks, blogging and tag-based systems, but also
determined by the complexity of lifelong learning, a phenomenon that occurs in a variety
of contexts (virtual, face-to-face or blended): (1) lack of suﬃcient application of science
education in the implementation of LA (which would have the following beneﬁts: opti‐
mized planning of the learning process, eﬀective implementation of the principles of
teaching and increasing students’ self-awareness); (2) poor operation of diﬀerent
A Survey on Social Learning Analytics
75

datasets (both in terms of the origin and type- mobile data, biometric, emotional, and so
on) and their combination; we should understand that the current trend is to use multiple
virtual learning environments or learning management systems such as Moodle or
informal social networks, thus a meaningful analysis of learner’s data should be done
on the data collected from all the environments in which he/she is active; (3) focusing
less on student data analysis (taking into account their proﬁle, career goals, motivational
factors, etc.) and more on grades and institutional needs, which is not a solid approach;
(4) lack of a clear set of ethical rules, taking into account the ownership and management
of data. Furthermore, Agudo-Peregrina et al. [30] highlight (5) the lack of consensus in
the LA community in terms of data relevance - what data is really important to be meas‐
ured, understood and used in subsequent modelling should be known and accepted by
all stakeholders involved in educational activities.
4
Importance of Social Learning Analytics a Questionnaires-Based
Survey
To check the importance of using social LA on daily basis, we included questions related
to this aspect within our survey which was conducted as part of a feasibility study
regarding the implementation of a pan-European social platform to support lifelong
learning and employability [31]. The target audience for our survey were IT teachers
and students.
4.1
Research Methodology
The research instruments we used in performing our survey were: literature mining,
structured interviews and two online questionnaires. The ﬁrst two instruments provided
us enough data to build the online questions, which targeted students and teachers from
technical domains. Both online questionnaires were available from 1st of March 2016
to 4th of April 2016 and were advertised on the project website, as well as within internal
networks of collaborators within Danube Region countries and social media.
The questionnaire for IT students had 4 parts, with the following titles: (A) Data
related to the interviewee proﬁle; (B) Employability as achievements gained in the
university; (C) Lifelong learning activities; (D) Opinions about the intention to use a
social learning platform to increase your employability. In a similar manner, the ques‐
tionnaire for IT teachers had 4 parts, with the following titles: (A) Data related to the
interviewee proﬁle; (B) Employability as curricular process; (C) The usage of social
networks in didactic activities; (D) Opinions about the intention to use a social learning
platform to increase your employability. For the current study, we are interested only in
the questions related to the inclusion of a learning analytics module within a learning
platform, but also whether oﬀering social features to a learning platform is a doable
objective. A qualitative and quantitative analysis was done in this respect.
76
M.-I. Dascalu et al.

4.2
Survey Results
Qualitative Analysis. The online questionnaire for IT students had 394 respondents,
but only 391 were ﬁnalized and considered valid. Most respondents were from Danube
Region countries, e.g. Romania (44.2%), Serbia (28.4%), Croatia (10%), Bosnia and
Herzegovina (7.9%), Slovakia (3.6%), Austria (2.3%), Germany, Ukraine, Bulgaria,
Montenegro, or Moldova: see the detailed distribution per country in Table 1. Most
respondents were students in their last year of bachelor studies or students currently
enrolled in a master/PhD programme, 71.6% of them having already a job experience.
Table 1. Demographics of the online questionnaires’ respondents
Country
Number of students
Number of teachers
Austria
9
11
Bosnia and Herzegovina
31
0
Bulgaria
2
0
Croatia
39
1
Czech Republic
1
2
Romania
174
28
Serbia
111
10
Slovakia
14
4
Others
10
3
From the received answers, we found out that 82.1% of them are active in social
networks, especially on Facebook, but also on LinkedIn, Google+ , Twitter or others;
73.4% are active in an online community/platform (e.g. Stackoverﬂow, online.ase.ro,
Girls Who Code, Udemy, Coursera and so on); only 24.8% of them use a Learning
Management System (LMS) within their classes; 39.4% of them are present on social
networks to interact both with their teachers and their colleagues; most students are
friends with their teachers on Facebook (87.6%) and on LinkedIn (49.6%) and they admit
that on those social networks they put questions, chat, answer questions, make announce‐
ments or just screen their teachers/colleagues - friends. Some students (37.8%) acknowl‐
edge the usefulness of professional social networks in ﬁnding a job, while only 17.1%
are really sure that they are not useful (the rest of our respondents don’t know). Most of
our respondents will use or they might use social networks in the future for keeping in
touch with their teachers and school colleagues, while 72.1% of the interrogated students
said that social network play a role in lifelong learning activities. The majority of them
(88%) consider the development of a social learning platform to be a relevant or a very
relevant initiative within Danube Region, while more than half (58.6%) see advanced
learning analytics as a mandatory feature of such a platform: see Fig. 1. All the func‐
tionalities depicted in Fig. 1 were given as choices to the respondents.
The online questionnaire for the IT teachers had 59 respondents, from various coun‐
tries, e.g. Romania (47.5%), Austria (18.6%), Serbia (16.9%), Germany, Slovakia,
Croatia, or Czech Republic: see the detailed distribution per country in Table 1. Almost
half of our respondents had 10 or more than 10 years of experience in didactic activities.
A Survey on Social Learning Analytics
77

Based on their answers, 55.9% of them admitted they use a LMS for their classes
and 57.6% of them said they would change their LMS, if suitable. The same number of
teachers admitted they use the assessment features available in their LMS. In the same
time, 44.1% acknowledged that they use a social network to keep in touch with their
students, while the rest don’t have students as friends on their social accounts.
Among the most common activities teachers do on those social networks, with regard
to interacting with their students are: making announcements (88.5% of our respond‐
ents), answer questions (57.7%) and friendly chat (30.8%). Most of them (76.9%) won’t
put grades based on students’ activities on social networks. Only 19.2% of them said
they noticed that students who are active on social networks/e-communities obtain better
results at school and a job, 3.8% said the students who are active on social networks
have poor marks, while 76.9% didn’t notice any correlation between social network use
and students’ grades/employability. Still, most of them (81.3%) consider the develop‐
ment of a social learning platform as a relevant or very relevant initiative within Danube
Region and 58 out of 59 admitted that the university would beneﬁt from using such a
platform.
Very interesting is the fact that the most desired functionality for such a social learning
platform within Danube Region is the one related to learning analytics, as seen in Fig. 2.
All the functionalities depicted in Fig. 2 were given as choices to the respondents.
Quantitative Analysis. First, we analyzed the correlation between respondent category
(teacher/student) and the option to include or not the learning analytics feature within
the platform. We applied the Fisher test and obtained the p-value to be 0.6242, fact that
indicates that there is no correlation betweem those two variables. We also tested the
correlations between the option of including a LA feature and other investigated attrib‐
utes, for students’ group and then for teachers’ group: the results are available in
Tables 2 and 3.
Fig. 1. Functionalities considered important by IT teachers within a social learning platform:
Functionality 1 – integration with university LMS; Functionality 2 – advanced learning analytics;
Functionality 3 – alumni monitoring; Functionality 4 – direct contact with companies which have
account on the platform; Functionality 5 – virtual labs; Functionality 6 – integration with other
social networks.
78
M.-I. Dascalu et al.

Table 2. Correlations between option of including LA feature and other attributes in the students’
group
Correlated feature
p-value (UE)
p-value (non-UE)
p-value (UE and
non-UE)
Being part of any e-Community 0.218
0.04294
0.04977
Being interested to develop the
platform
0.9399
0.01549
0.1268
Table 3. Correlations between option of including LA feature and other attributes in the teachers’
group
Correlated feature
p-value (UE)
p-value (non-UE)
p-value (UE and
non-UE)
Perceived level of students’
interest for learning by doing
0.07645
0.2909
0.03567
Knowing the student employ‐
ment status
0.09075
0.4909
0.0438
Perceived importance of online
courses
0.01837
0.1091
0.007816
Usage of LMS during classes
1
0.05455
0.1808
Adequacy of LMS for LLL
0.1106
0.4545
0.09608
Perceived correlation between
students’ behavior within
social networks and their
learning results
0.7254
0.0007215
0.5793
Usefulness of a social platform
for LLL
0.004837
0.6727
0.004824
University beneﬁts by using a
social platform
0.1011
0.4545
0.03993
Willingness of promotion a
social platform
0.1023
0.4545
0.04904
Fig. 2. Functionalities considered important by IT teachers within a social learning platform:
Functionality 1 – integration with university LMS; Functionality 2 – advanced learning analytics;
Functionality 3 – alumni monitoring; Functionality 4 – direct contact with companies which have
account on the platform
A Survey on Social Learning Analytics
79

For students, a correlation between the option of having LA and involvement in e-
Communities has been noticed. Even more, for students from non-UE countries there
is a correlation between having LA and being interested in developing a social learning
platform with LA (see Table 2).
For teachers from UE countries, there were correlations between LA option and:
perceived level of students’ interest for learning by doing, knowing the student employ‐
ment status, perceived importance of online courses, Usefulness of a social platform for
lifelong learning (LLL). For teachers from non-UE countries, there were correlations
between LA option and: usage of LMS during classes and perceived correlation between
students’ behavior within social networks and their learning results. For all teachers
(disrespecting their country of origin), the correlations identiﬁed at the ones from UE
countries were kept and, additionally, others were identiﬁed between LA option and:
usage of LMS during classes, usefulness of a social platform and willingness of promo‐
tion a social platform for LLL.
4.3
Discussions and Further Research Directions
Due to the high number of Romanian participants, the survey results are highly relevant
for Romania, but also for other Danube Region countries. The responses prove the
importance of combining the power of social networks with learning management
systems. Surprisingly, the percent of students who beneﬁt from LMS in class is less than
the percent of teachers: this can be explained only by the fact that students are not
motivated to activate within a LMS, but embedding more social features (including
connecting the LMS with fashionable social networks) will increase their interests
towards LMS and, consequently, learning. Advanced learning analytics are among the
most wanted features of such a social learning platform, as they can be very useful both
for teachers and students: the teachers will be able to improve the teaching act to provide
to students what they need, students can become aware of their involvement or lack of
involvement in the educational activities, but also of the eﬀect of this involvement on
their lives (e.g. on ﬁnding a job). In the end, universities themselves can analyze and
improve the eﬀectiveness of their educational oﬀer, based on LA modules. In order for
LA to be truly advanced, the learners’ activities within the social learning platform, the
context, his/her mood or the interactions with one’s peers have to be analyzed and a
multiple-level virtual proﬁle of the student has to be built: the descriptive level should
contain what happened, the diagnostic level should reveal the causes, the predictive level
should signal the lack of performance or the risks of failure and the prescriptive level
should oﬀer solutions for improvement, recommend e-communities or useful educa‐
tional resources.
The fact that there is no correlation between respondent category (teacher/student)
and the option to include or not the learning analytics feature within the platform can
be interpreted that LA is perceived similarly by students and teachers, despite diﬀerences
in their socio-professional background: this is a prerequisite for the adoption and rapid
dissemination of LA functionality on LMS targeting employability increase. Also,
several correlations between LA option and other attributes from the questionnaires were
found, fact that proves the interest towards LA, as seen in the Sect. 2 of the paper, too.
80
M.-I. Dascalu et al.

In order to assure a fast implementation of a social learning platform with LA func‐
tionalities, a model for it is proposed, as a career development management system
(CDMS), a specialized content management system (CMS) to facilitate career devel‐
opment services for graduates, with interconnected modules for social learning and
recruiting, thus providing support to increase graduates’ employability. The idea of
developing a CMS specialized for career development services is based on the success
of other types of CMS, which were created for well-deﬁned purposes: Learning Manage‐
ment Systems – e.g. Moodle, Blackboard, Website Management Systems – e.g. Word‐
Press, Drupal, Scientiﬁc Journals Management Systems – e.g. Open Journal Systems
and so on. The LA functionality can be embedded in the core part of the CDMS or as a
plugin, as other LMS have (see Sect. 2). We consider that a CMS could be a more feasible
solution than a stand-alone platform, as thus the owners can be various organizations.
5
Conclusions
According to SOLAR and Horizon initiatives, there is a strong tendency for choosing
adaptive social learning systems solutions within the educational institutions, and not
only. In this view, there are more and more possibilities to harness the power of feedback
loops at the level of individual educator and learner. Providing a visual representation
of their learning process progress and assessment, learners are now more aware of their
assets and weaknesses, adopting a “missionary spirit” towards their personal develop‐
ment [32]. Hence, they are now able to develop new skills in monitoring their own
performance and see directly how the decisions they make impact their success. As far
as the educators are concerned, they gain more visibility of their learners’ performance,
hence they can easily adapt their educational instruction or intervene when they discover
students that do not follow the collective pattern. This way, each educator can tailor their
way of tutoring and teaching with regards to each individual and community via contin‐
uous feedback, thus being able to assess the eﬀectiveness of their adaptations and inter‐
ventions.
The current paper debates upon the importance of learning analytics in nowadays
educational landscape, stressing on a particular sub-domain - social learning analytics,
which is sustained by the latest social, mobile and sensory technologies. The relevance
of the topic is demonstrated also by the responses given to our online questionnaires,
which are presented in detail in this article: LA is among the most desired functionalities
of a social learning platform, considered to be a relevant initiative for raising the
employability level of the IT graduates in the Danube Region.
Acknowledgements. The work has received funding from the grant agreement No. 16_PA07-
C2, START-SoPI, “Feasibility Study on Implementing a Pan-European Social Platform to
Support Lifelong Learning and Employability”. SoPI is part ﬁnanced by START – Danube Region
Project Fund, an initiative for the Danube Region Strategy. START is ﬁnanced by the European
Union and the City of Vienna. This work has been also funded by University Politehnica of
Bucharest, through the “Excellence Research Grants” Program, UPB – GEX. Identiﬁer: UPB–
EXCELENȚĂ–2016 “Construirea si exploatarea proﬁlului utilizatorilor in medii de invatare
virtuale: metode, implicatii psihosociale si consecinte utilizand tehnici de analiza avansata a
A Survey on Social Learning Analytics
81

datelor – LeProVE (Learning Proﬁling in Virtual Environments)”, Contract number
83/26.09.2016, code 303. (LeProVE).
References
1. Bodea, C.N., Mogos, R.I., Dascalu, M.I.: How e-learning experience enhances the social
presence in community of practice. In: El Morr, C., Maret, P. (eds.) Virtual Community
Building and the Information Society: Current and Future Directions, pp. 75–120. IGI Global,
Hershey (2012)
2. European Digital Agenda. https://ec.europa.eu/digital-agenda/
3. Society for Learning Analytics Research (SoLAR). https://solaresearch.org/
4. Ferguson, R.: Learning analytics: drivers, developments and challenges. Int. J. Technol.
Enhanced Learn. 4(5/6), 304–317 (2012)
5. Siemens, G.: What Are Learning Analytics? http://www.elearnspace.org/blog/2010/08/25/
what-are-learning-analytics/
6. Schrage, M.: Higher Education Is Overrated; Skills Aren’t. Harward Business Review. https://
hbr.org/2010/07/higher-education-is-highly-ove.html
7. Tamny, J.: Online education will be the next ‘bubble’ to pop, not traditional university
learning. In: Forbes. http://www.forbes.com/sites/johntamny/2013/06/09/online-education-
will-be-the-next-bubble-to-pop-not-traditional-university-learning/#4b2b888d694d
8. Horizon report. http://www.nmc.org/pdf/2014-nmc-horizon-report-he-EN.pdf
9. Tabuenca, B., Kalz, M., Drachsler, H., Specht, M.: Time will tell: The role of mobile learning
analytics in self-regulated learning. Comput. Educ. 89, 53–74 (2015)
10. Ali, L., Asadi, M., Gašević, D., Jovanović, J., Hatala, M.: Factors inﬂuencing beliefs for
adoption of a learning analytics tool: an empirical study. Comput. Educ. 62, 130–148 (2013)
11. Bienkowski, M., Feng, M., Means, B.: Enhancing Teaching and Learning Through
Educational Data Mining and Learning Analytics: An Issue Brief – Report (2012). https://
tech.ed.gov/wp-content/uploads/2014/03/edm-la-brief.pdf
12. Dascalu, M.I., Bodea, C.N., Moldoveanu, A., Mohora, A., Lytras, M., Ordóñez de Pablos,
P.: A recommender agent based on learning styles for better virtual collaborative learning
experiences. Comput. Hum. Behav. 45, 243–253 (2015)
13. Shah, M., Nair, C.S., Richardson, J.T.E.: Using Learning Analytics to Assess Student
Engagement and Experience, Measuring and Enhancing the Student Experience, pp. 133–
146 (2017)
14. Bodea, C.N., Dascalu, M.I., Lipai, A.: Clustering of the web search results in educational
recommender systems. In: Santos, O.C., Boticario, J.G. (eds.) Educational Recommender
Systems and Technologies: Practices and Challenges, pp. 154–181. IGI Global, Hershey
(2011)
15. Shum, S.B., Ferguson, R.: Social learning analytics. Educ. Technol. Soc. 15(3), 3–26 (2012)
16. Ruipérez-Valiente, J.A., Muñoz-Merino, P.J., Leony, D., Kloos, C.D.: ALAS-KA: a learning
analytics extension for better understanding the learning process in the Khan Academy
platform. Comput. Hum. Behav. 47, 139–148 (2015)
17. Griswold, W.G., Shanahan, P., Brown, S.W., Boyer, R., Ratto, M., Shapiro, R.B., Truong,
T.M.: ActiveCampus: experiments in community-oriented ubiquitous computing. Computer
37(10), 73–81 (2004)
18. Syvanen, A., Beale, R., Sharples, M., Ahonen, M.: Supporting pervasive learning
environments: adaptability and context awareness in mobile learning. In: Proceedings of IEEE
International Workshop on WMTE, pp. 251–253. IEEE Press, Tokushima (2005)
82
M.-I. Dascalu et al.

19. Mazzola, L., Mazza, R.: GVIS: a facility for adaptively mashing up and representing open
learner models. In: Wolpers, M., Kirschner, Paul A., Scheﬀel, M., Lindstaedt, S., Dimitrova,
V. (eds.) EC-TEL 2010. LNCS, vol. 6383, pp. 554–559. Springer, Heidelberg (2010). https://
doi.org/10.1007/978-3-642-16020-2_53
20. Fernández-Gallego, B., Lama, M., Vidal, J.C., Mucientes, M.: Learning analytics framework
for educational virtual worlds. Procedia Comput. Sci. 25, 443–447 (2013)
21. ASSISTments. https://www.assistments.org/
22. Heﬀernan, F., Koedinger, K.: User modeling and user-adapted interaction: addressing the
assessment challenge in an online system that tutors as it assesses. J. Pers. Res. (UMUAI J.)
19(3), 243–266 (2009)
23. Heﬀernan, F., Razzaq, M.: Improving learning from homework using intelligent tutoring
systems. J. Res. Technol. Educ. (JRTE) 41(3), 331–346 (2009)
24. Heﬀernan, N., Heﬀernan, C., Dietz, K., Soﬀer, D., Pellegrino, J.W., Goldman, S.R.:
Improving mathematical learning outcomes through automatic reassessment and relearning.
In: Proceedings of AERA (2012)
25. SmartKlass Plugin. https://moodle.org/plugins/local_smart_klass
26. Engagement Analytics Plugin. https://moodle.org/plugins/report_engagement
27. Eﬀective Learning Analytics. https://analytics.jiscinvolve.org/wp/2014/10/03/engagement-
reporting-tools-for-blackboard-and-moodle/
28. Common Education Data Standards. https://ceds.ed.gov/
29. Siemens, G., Baker, R.: Learning analytics and educational data mining: towards
communication and collaboration. In: Proceedings of the 2nd International Conference on
Learning Analytics & Knowledge, LAK 2012, pp. 252–254. ACM, New York (2012)
30. Agudo-Peregrina, Á.F., Iglesias-Pradas, S., Conde-González, M.Á., Hernández-García, Á.:
Can we predict success from log data in VLEs? Classiﬁcation of interactions for learning
analytics and their relation with performance in VLE-supported F2F and online learning.
Comput. Hum. Behav. 31, 542–550 (2014)
31. START-SoPI: Feasibility Study on Implementing a Pan-European Social Platform to Support
Lifelong Learning and Employability. http://startsopi.ase.ro/
32. Bowen, W.G.: Higher Education in the Digital Age. Princeton University Press, Princeton
(2014)
A Survey on Social Learning Analytics
83

Students in Social Media: Behavior,
Expectations and Views
Mircea Georgescu and Daniela Popescul(&)
“Alexandru Ioan Cuza” University, Bd. Carol I no 22, Iași, Romania
{mirceag,rdaniela}@uaic.ro
Abstract. The purpose of this study is to learn about Generation Y students’
traits, habits, needs and expectations when using Social Media (SM). The cor-
rect identiﬁcation of their behavior is, in our opinion, essential for the academic
community. Firstly, we need to understand what their real proﬁle is. Then, a
serious and consistent adaptation of our attitudes, teaching materials and ways of
providing information and a redeﬁnition of universities’ policies and procedures
are necessary. On this basis, in an empirical study, we try to determine why do
Generation Y students use SM, what are their preferences for one media or
another, how/for how long/from where do they use SM tools, and what do they
expect from universities and professors.
Keywords: Generation Y  E-learning  Web 2.0  Social Media
Social media use in universities
1
Introduction
The 21st century universities face difﬁculties in the alignment of the academic offer to
the requirements of current students, digital natives who are resident rather in Social
Media (SM) than in amphitheaters. The ﬁrst step towards solving this problem is, in our
opinion, a solid review of what we know about Generation Y students and their
behavior in these new media. The correct understanding of their traits, needs and
expectations is crucial for the academic community, as it represents the start point for a
serious and consistent adaptation of universities’ e-learning processes.
Even if there is no widespread consensus in the literature on the start and end points
for the Generation Y, we will consider, for the purposes of this paper, as members of
Generation Y the individuals born after 1980, who were early and frequent exposed to
technology [1–3], especially through electronic screens: television, movies, video games,
computer monitors. [4] Because terms as Millennials and digital natives are often used to
refer to Generation Y members, we will use them in an interchangeable way.
2
Literature Review
In a previous paper [5], we proﬁled a Generation Y student that resembles a Janus.
Such a digital native seems to be well educated, technological-savvy, and interested to
learn [1, 6], but also shallow, skeptical, cynical, critical, narcissistic, difﬁcult to “wow”
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 84–98, 2018.
https://doi.org/10.1007/978-3-319-73459-0_6

and impatient relative to his/her predecessors [7–9]. A Millennial is a multitasker,
ﬁlters and consumes relevant and interesting information with great speed, has high
expectations and is headed to fast achievements, is social-responsible, loyal and tol-
erant [10, 11], but, on the other side, does not want to grow up, stays home longer than
his/her parents or grandparents did, “wants it all” and “wants it now”, particularly in
relation to work pay and beneﬁts, career advancement, professional/personal balance
[12, 13]. In university, a Millennial wants to be permanently challenged, to be engaged
in the learning process in an interactive manner and to have the power to decide what to
choose from a rich educational offer, and also considers that his/her immediate needs
have to be satisﬁed urgently, due to the fact that usually he/she is working full or part
time while taking classes (Fig. 1).
After a systematic literature review presented selectively in Table 1, we extracted a
the main reasons for using SM as: social interaction with old or new friends, with a
special mention for making new friends; seeking out information about school, busi-
nesses, sales, deals, or products and also on events, birthdays, and parties; keeping up
to date with new information; passing time when idle or bored, by playing games,
listening to music, watching videos, browsing through proﬁles or reading statuses and
comments; self-promotion and expression of opinions; surveillance/knowledge about
others and self-education. The intensity of use is higher than in case of general pop-
ulation. SM use is part of college and university students’ daily routine, especially
because they have grown up with computers, and they spend at least 1 to 2 h daily on
social networks. Interesting controversies can be noticed: while some studies show that
even though digital natives prefer digital media in their personal lives, this is not
necessarily reﬂected in their academic lives [2, 6], other studies present students who
have the habit to access SM tools during the classes [14] or while studying, researching
and preparing classes.
This controversial type of student, with the associated controversial behavior and
changed views, is the main focus of contemporary educational approaches. E-learning
is now in its 2.0 phase, which is entirely learner centered [16]. With the help of Web
2.0 various services and technologies (wikis, forums, social network sites, RSS feeds,
Fig. 1. Portrait of a Millennial Source: authors’ adaptation, based on [1, 5–13]
Students in Social Media: Behavior, Expectations and Views
85

podcasts, blogs, video conferences and tutorials, mashups, based on ASP, AJAX,
JavaScript frameworks etc.), teaching and learning processes became less separated,
and the passive learner is transformed into an active participant in the learning process
[17]. Every individual with his personal characteristics tends more and more to con-
struct a personal learning environment which interacts with an ever growing number of
technologies and social networks. [18] From a theoretical point of view, the contri-
bution of all participants in the learning process, students and teachers, is valued in an
equal manner, and the introduction of the last information and communication tech-
nologies (ICTs) in universities must be closely tied to Generation Y students’ char-
acteristics. But, according to [19–21], universities’ usage of SM is generally in stark
contrast to the emerging Web 2.0 culture among students. Few university members had
the courage to embrace Web 2.0 technologies in their classes – some of them begun
experimenting with YouTube videos or channels, courseware provided by MIT or
Coursera.org, professional blogs and wikis, Facebook interaction with their students.
On the whole, there seems to exist a divide between the largely “digital native” college
students and their professors, more attached to old and familiar technologies as e-mail
and slide presentations.
Starting from these differences in proﬁle between teachers and students, and
knowing that SM use is an important part of college and university students’ daily
routine, the research questions formulated by us were:
Q1. Why do Generation Y students use SM?
Q2. What are their preferences for one medium or another?
Q3. How do they use SM tools, for how long and from where? Do they use them
while studying/preparing for classes?
Q4. What do they expect from university and professors?
In order to ﬁnd answers for these questions, we recruited 796 participants from
Faculty of Economy and Business Administration (FEBA), “Alexandru Ioan Cuza”
University in Iași, Romania, who received a link to an online questionnaire (http://goo.
gl/forms/ByUsK7QwPt). The participation was voluntary and anonymous.
It took approximately 10-15 min to answer the questions, which were available
from 9th to 13th November 2015.
3
Methodology
The study aimed to identify the FEBA students’ reasons for SM use, their preferences
for one media or another, their habits in using SM and their expectations from uni-
versity and professors. A number of 796 students (13,37% of all the FEBA students
enrolled in 2015/2016 academic year, bachelors and master’s degree, excluding the
distance-learning students), 205 males and 591 females, answered a questionnaire
distributed by Google Forms; all the received answers were valid. 6 students, all males,
said they don’t use SM. All respondents were born between 1980 and 1997, on average
22 years old, meaning that they belong to the younger part of Y Generation.
Our questionnaire had two main sections. Questions were constructed based on
bibliographical material presented in Sect. 2, in order to assure the face-validity of the
86
M. Georgescu and D. Popescul

Table 1. Generation Y’ SM use– a systematic literature review
Study purpose and
source
Research question(s)
Sample size and
structure
Relevant ﬁndings
To gain quantitative
insights, both about
the preferences of
digital natives and
the attitudes of
employees towards
different internal
communication
media [2]
What media
preferences do
digital natives have
regarding internal
communication
channels in the
organization they are
working for?
310 young
employees of
different
nationalities,
especially Asian
(Indian), but also
European and North
American. They
were employed in
an IT company – so,
presumably, they
are more familiar
with the digital tools
compared with an
average employee
Social relationships
with other persons
are considered to be
universal human
needs, critical to the
psychological
well-being of
individuals, which
are very well
addressed by SM.
Employees still
favor traditional
internal channels, in
spite of a strong
preference for SM
in their private lives,
and are rather
consumers than
producers of digital
content. internal
communication
To gather
preliminary
evidence of the
current adoption of
SNS’s such as
Facebook by
students [14]
How much time
students spend on
average in SN?
How and what they
think about SN?
What is students’
perspective about the
negative factors
associated with these
sites?
Do students believe
that relationships can
be formed through
SNS?
What proportion of
students consider
SNS an addiction?
561 university
students of different
nationalities,
selected based on
their voluntary
option to ﬁll in the
questionnaire
posted on two
Facebook groups of
Erasmus students
(approx.
1000 members
each), several
groups of
Portuguese
faculties, and a
group of a
university in the
Czech Republic
(more than 6700
members)
SM, especially
social networks
(SN) use provides
an important
backdrop for the
social, emotional,
and cognitive
development of
youth, accounting
for a large part of
their time. SN are
the modus operandi
of the new
generation. A strong
connection was
found between
Facebook use and
the addiction to
maintaining social
capital
(continued)
Students in Social Media: Behavior, Expectations and Views
87

Table 1. (continued)
Study purpose and
source
Research question(s)
Sample size and
structure
Relevant ﬁndings
To assess students’
attitudes toward
both existing media
channels and new
communication
media in their
private and
university lives [3]
What media
preferences do
students have with
respect to obtaining
formal information
on their university
and are these media
preferences the same
as those that they
use in their informal
social
communication?
308 undergraduate
students in 4
marketing classes at
a Croatian School of
Economics and
Business
SM tools are
perceived as
important
communications
tool for students.
They expressed an
explicit preference
for the use of SM
over traditional
media that was
nearly identical to
the preferences in
their private use of
SM for functional
communication
To identify the uses
and gratiﬁcations
that consumers
receive from using
SM [15]
What are the main
uses and
gratiﬁcations of SM
(why do the
individuals use SM,
why their friends use
SM, what do they
enjoy about SM, and
how often do they
use SM?)
25 individuals (52%
females and 48%
males) were in‐
depth interviewed
Ten SM uses and
gratiﬁcations were
found: social
interaction,
information
seeking, pass time,
entertainment,
relaxation,
communicatory
utility, expression
of opinions,
convenience utility,
information sharing,
and surveillance and
watching of others
To reveal the most
important aspects of
Facebook sharing
from a social
psychological
approach [10]
What Facebook
users update about?
What are the
underlying reasons
for updating one’s
status?
What are the
variables that
inﬂuence the amount
of sharing?
706 Facebook users
who responded to a
request to complete
an online survey
Main motivations
for using Facebook
are experience
sharing, positive
self-presentation
and popularity
seeking; reducing
loneliness and
boredom and
information
spreading
(continued)
88
M. Georgescu and D. Popescul

questionnaire. In the ﬁrst section we asked participants about their ﬁeld of study, age,
gender, level of digital competence and possession of a smartphone. In the second
section, participants were asked to communicate information about the reasons for
using SM, frequency in using SM (in general and while studying), the media they
prefer, and places of use. Likert scales and True/False items were used. At the end of
the questionnaire, two open questions were addressed, in order to ﬁnd out how students
think SM could be used by university/professors for the improvement of communi-
cation and teaching processes.
4
Results and Findings
The charts below present the distribution of the 790 students by their ﬁeld of study
(general economic studies – ﬁrst year of study, Marketing, Management, Business
Information Systems, Economics, Finance, Accounting and Information Systems,
Business Administration, Public Administration) and their level of digital competence
(based on self-evaluation). More than a half of the students are digitally literate and
extremely proﬁcient with technology, or at least that is their perception regarding
themselves. As they were subjects of a national evaluation of digital competencies at
Table 1. (continued)
Study purpose and
source
Research question(s)
Sample size and
structure
Relevant ﬁndings
To discover the
degree in which
students employ
Facebook for
different
communication
purposes; to learn
what were their
attitudes regarding
Facebook in general
and whether they
had a positive
attitude towards
formally employing
Facebook in the
teaching process; to
discover the extent
to which students
already used
Facebook in relation
with their academic
life [6]
To what degree are
the students
technically
savvy/keen on using
technology? What
are the reasons why
students use
Facebook? How do
they perceive
Facebook? Are they
interested in using
Facebook to
communicate with
their teachers and/or
peers?
224 undergraduate
and graduate
students of the
College of
Communication and
Public Relations
from the National
University of
Political Science
and Administrative
Studies, Bucharest,
Romania
Students use
Facebook because
they need to be in
contact with
long-distance
friends and
acquaintances, and
also for the
newsfeed’s
presence, which
helps them stay
informed on various
topics of interest.
The novelty of the
medium triggers
their enthusiasm
related to
technological
innovations. They
appreciate also the
entertaining aspects
of Facebook
Students in Social Media: Behavior, Expectations and Views
89

high-school graduation, we have all the reasons to consider their self-assessment as a
fair one (Fig. 2).
92% of them possess a smartphone and use it for visiting SN sites. These ﬁndings
reﬂect their preference toward a more portable learning environment and the extent in
which ICTs are part of their lives (Fig. 3).
4.1
Why Do Generation Y Students Use Social Media?
As can be seen in Fig. 4, the main reason for SM use in FEBA is the social interaction
with old or new friends. This conﬁrms the opinions expressed in theory [2] [22] that
social relationships with other persons are universal human needs, critical to the
Fig. 2. Respondents’ structure by ﬁeld of study Source: questionnaire interpretation
Fig. 3. Respondents’ structure by level of digital competencies Source:
questionnaire
interpretation
90
M. Georgescu and D. Popescul

psychological well-being of individuals, and that these needs are very well addressed
by SM. The features of social presence, as the most important factor that determines
students’ usage of SM, can also encourage students to collaborate and work together
and with their professors.
Other reasons of use mentioned by students, in order of their importance, are:
• listening to music;
• seeking out academic information;
• passing time when idle or bored;
• self-education;
• reading posts/comments;
• playing games;
• looking for new friends;
• shopping;
• expression of opinions;
• ﬁghting loneliness;
• surveillance/knowledge about others;
• self-promotion.
Even if the entertaining aspects prevail, the academic reasons are also very well
represented – more than 60% of students seek out school-related information in SM in
a frequent manner; also, 30% afﬁrm that they frequently use SM for self-education.
Fig. 4. FEBA students’ reasons for using Social Media Source: questionnaire interpretation
Students in Social Media: Behavior, Expectations and Views
91

4.2
What Are Students’ Preferences for One Medium or Another?
As literature afﬁrms, there is a difference in students’ preferences for one media or
another. [1, 8, 16]. In our study, the uncontestable students’ favorite is Facebook
(almost all students use it daily, 80% check this social network site hourly), followed
by YouTube (for listening music, watching entertainment videos, but also for educa-
tional videos). Blogs and Wikipedia are especially read, and few students use them for
posting, commenting and editing. Instagram is used by almost a half of the students. As
shown in Fig. 5, less than 10% of the participating students occasionally use pod-
casting, Twitter, RSS feeds, Flickr, LinkedIn or Myspace.
The positive correlations between the level of digital competencies and media used
that could be identiﬁed are: Forums (r = .22**), RSS feeds (r = .18**), YouTube (for
entertainment and didactic use) (r = 0.16**), reading blogs (r = .13**), Podcasting
(r = .13**),
LinkedIn
(r = .12**),
Instagram
(r = .10**),
Blogs
(comments)
(r = .10**), Twitter (r = .07**), reading Wikipedia (r = .05**), Flickr (r = .04**). The
level of digital competencies had a negative correlation with using Myspace, Facebook,
Wikipedia (editing) and Google +.
Fig. 5. Types/frequency of SM used by FEBA students Source: questionnaire interpretation
92
M. Georgescu and D. Popescul

4.3
How Do Students Use Social Media Tools, for How Long
and from Where?
As presented in Figs. 6 and 7, SM use is part of FEBA students’ daily routine. In
regards to the place of accessing SM, home is the place mostly chosen, followed by
smartphones (with Wi-Fi or mobile data connections). Half of the students never use
SM from the FEBA network. This shows that even though digital natives prefer SM in
their personal lives, this is not necessarily reﬂected in their academic lives.
Fig. 6. Place and frequency of Social Media Use Source: questionnaire interpretation
Fig. 7. Social Media and mobile phone use while studying/preparing for classes Source:
questionnaire interpretation
Students in Social Media: Behavior, Expectations and Views
93

Regarding their pattern of SM use while studying/researching, participants reported
a frequent use of their mobile phone, Facebook and YouTube.
4.4
What Do They Expect from University/Professors?
A number of 221 students answered to the open questions regarding the improvement
of communication in SM at the faculty level.
It is gratifying to notice that 31% consider the online communication between them
and FEBA as good/very good. In their view, there is enough information on the FEBA
portal (intranet) and on the already-existing Facebook groups and there’s no real need
for improvement/changes. 15% of the students that answered this questions want
permanent/daily/more frequent updates on ofﬁcial FEBA Facebook page; faster
responses to questions, and “not after some time, when the answer no longer has
value”; they request also newer and more interactive posts. They demand groups for
each subject, with extensive teaching materials (handouts, multiple choice tests,
additional/synthesis materials and case studies), exams dates and results, and latest
announcements (including notices and warnings about the difﬁculty of examinations).
Videos, applications, contests and captivating games on the topic taught should be
added, because they “facilitate the knowledge acquisition and are methods appreciated
by students, in comparison to the classical way of teaching that is outdated and boring”.
Professors should post in a constant manner and should provide guidance to those
who want to deepen certain topics, “explaining to those who have not understood the
lesson or telling them what assignments they have”. Two students even want non-stop
availability on chat or phone (in a call center!).
Other requests, formulated by fewer students, were:
• distribution of announcements/other information from the portal on Facebook –
instant retrieval or at least re-posting using hyperlinks;
• a FAQ section on the portal/Facebook;
• less advertisement and off-topic posts; even if such posts appear, students require
the admin to delete them on time and to block the access of the owners. To quote
one of them, “only few members should have the right to post on FEBA Facebook
page – professors, staff, students’ representatives – in this way, there will be no
more posts about pizza and Halloween parties”;
• video online exams, “helpful for students who have a job or study at another college
in the same time”. Also, “courses should be recorded and posted on social networks,
so each student could learn from home and understand better the topics”;
• freshmen should be informed about the need to make user accounts on multiple
platforms (portal, Facebook groups) in order to stay connected to the faculty - a
student suggested that this information should be included in the Welcome kit;
• notiﬁcations (through Facebook, e-mail, SMS, mobile app) every time when
something new is posted on the portal. “When all information and updates are sent
to students, they’ll have no such excuse that they haven’t seen/known certain
information”;
• more job offers and personal development courses on ofﬁcial Facebook page, but
also more positive posts, funny pictures, smart jokes, sweets and other prizes;
94
M. Georgescu and D. Popescul

• feedback questionnaires and other polls on various topics related to academic life
should be posted online, at the end of each semester.
• a student suggested “breaking” the FEBA page in one formal discussion group
(with opinion surveys, various announcements, extensive interaction), one aca-
demic events group, one group for volunteering, sports etc. and a news group (in
which students’ achievements should be mentioned, among others);
• the groups should be more stable (because “changes are quite disturbing”), and a set
of rules should be put in place for each group; the users who don’t respect the rules
should be blocked.
The overall image of students’ impression regarding the FEBA’s and their spe-
cializations’ Facebook pages is presented in Fig. 8.
Focusing on professors, students require them simply to open Social Networks
accounts, to be present, to post updated information (including teaching materials), to
participate in discussions at least on the ofﬁcial FEBA Facebook page and to have
“Social Media ofﬁce hours”. On the other hand, one respondent considers communi-
cation through Facebook not suitable/comfortable: “in my opinion, the professor-
student relationship should be strictly professional. An exchange of e-mail addresses is
more than enough. I do not think this Facebook communication is possible, because the
students’ view about professors does not allow it. Between discussing with the teacher
or with other students, the second option will most likely be chosen. Communication
cannot be improved by Facebook, but through ofﬂine extra-curricular activities”.
Fig. 8. Students’ opinions about FEBA’s and their specialization Facebook pages Source:
questionnaire interpretation
Students in Social Media: Behavior, Expectations and Views
95

Beyond their personality as individuals, professors are seen by their students as
valuable info-spheres [21] – multiple media documents posted by professors are per-
ceived as relevant and useful by the students, regardless of their study domain. The
Web 2.0 technologies are seen as increasing the voice and presence of the individual
user and enhancing the expression of their teaching activity. In this spirit, we may
appreciate that, if faculty members will use SM in current teaching, this innovative
practice will be well received by their students and the interaction between the two
categories will improve.
5
Conclusions
In our opinion, the results presented here are of interest both for professors as indi-
viduals, and for the universities’ and faculties’ management – especially in a world in
which the borderline between the physical and virtual life is becoming more and more
difﬁcult to draw. Also, the study offers valuable insights, as quantitative studies on SM
use are scarce, particularly in student populations. Their massive digital presence and
the really important time spent by students in SM are worth to be exploited, and the
universities can use these communication channels to become closer to their beneﬁ-
ciaries. The teaching/learning methods should be adapted to the lifestyle of a pragmatic
generation, who has high expectations and wants immediate results. The students’
involvement must be encouraged, and existing teaching methods must be changed.
Feedback should be sought and given in a constant, coherent and consistent manner.
The informal nature of SM can be the source for an increased communication
between professors and students, with positive results on both categories’ satisfaction
level and on university’ reputation. As they request open resources (learning content,
educational videos, answers to FAQ) freely available in SM, adaptation to their indi-
vidual needs and a changing of their professors’ role – from instructors to facilitators of
learning process, we should move on to learning as a shared activity, encourage their
role as active participants in learning process, helping them to develop valuable
competencies, aptitudes, skills and knowledge. In Web 2.0, the possible forms of
interaction with Generation Y students are extremely diverse: Facebook, YouTube,
Wikipedia, Twitter, WordPress can be easily used by professors to provide educational
support to students, to generate new content by adding information and permitting
comments, uploads and shares of students’ own digital resources. In this way, Web 2.0
creates a favorable background for developing and implementing a strategy of coop-
erative learning in which students work together with their professors to achieve
mutually-agreed learning objectives.
In this spirit, we think that universities should be looking at SM content as part of a
wider improved and rounded experience for students, in order to encourage horizontal
collaboration and more relaxed conversations across professors and students. It’s true
that, beyond other difﬁculties as property rights, patents, the right to intimacy, the
intellectual property of educational materials, the main problem is that strong presence
is not enough, and constant engagement is required. But, with consistent effort and
engagement, by using a variety of SM platforms to disseminate important information,
promote equality and diversity, recognize and celebrate student success and also to
96
M. Georgescu and D. Popescul

allow students to explore the academic life outside of classrooms, the universities will
be able to address properly the different interests and needs of Generation Y students.
References
1. Bolton, R., Parasuraman, A., Hoefnagels, A., Migchels, A., Kabadayi, S., Gruber, T.,
Loureiro, Y.K., Solnet, D.: Understanding generation Y and their use of social media: a
review and research agenda. J. Serv. Manag. 24(3), 245–267 (2013)
2. Friedl, J., Tkalac Verĉiĉ, A.: Media preferences of digital natives’ internal communication: a
pilot study. Publ. Relat. Rev. 37(1), 84–86 (2011)
3. Tkalac Verĉiĉ, A.T., Verĉiĉ, D.: Digital natives and social media. Publ. Relat. Rev. 39, 600–
602 (2013)
4. Weiler, A.: Information-seeking behavior in generation Y students: motivation, critical
thinking, and learning theory. J. Acad. Librarianship 31(1), 46–53 (2005)
5. Popescul, D., Georgescu, M.: Generation Y students in social media: what do we know
about Them? BRAIN – Broad Res. Artif. Intell. Neurosci. 6(3-4), 74–81 (2015)
6. Pînzaru, F., Mitan, A.: Generation Y students: using facebook for communicating with
university staff and professors. Manag. Dyn. Knowl. Econom. 6(2), 221–239 (2013)
7. Carr, N.: The Shallows: What the Internet is Doing to Our Brains. Norton & Company Inc.,
New York (2010)
8. Twenge, J.M., Konrath, S., Foster, J.D., Campbell, W.K., Bushman, B.J.: Egos inﬂating over
time: a cross-temporal meta-analysis of the narcissistic personality inventory. J. Personal. 76,
875–901 (2010)
9. Ujhelyi, A., Szabó, E.: Sharing on facebook. from loners to popularity seekers. In: Horváth,
G., Bakó, R. K., Biró-Kaszás, E. (eds.) Ten Years of Facebook. Proceedings of the Third
International
Conference
on
Argumentation
and
Rhetoric.
Partium
Press,
Oradea
(Nagyvárad), Romania (2014)
10. Van der Bergh, J., Behrer, M.: How Cool Brands Stay Hot Branding to Generation Y. Kogan
Page Limited, London (2011)
11. Corodeanu, D.T.A.: Consumer’s protection from the generation Y’s perspective. a research
based on scenarios. Procedia Econom. Finan. 20, 8–18 (2015)
12. Airinei, D., Grama, A., Fotache, D., Georgescu, M., Munteanu, A., Dospinescu, O.,
Popescul, D., Păvăloaia, V.D.: Tehnologii informaţionale aplicate în organizaţii. Editura
Universităţii “Alexandru Ioan Cuza” din Iaşi, Iași (2014)
13. Santos, D., Čuta, M.: The usage of social networks by university students (a survey of
facebook use patterns among young people). J. Gen. Anthropol. Relat. Disciplines 6(1),
15–43 (2015)
14. Whiting, A., Williams, D.: Why people use social media: a uses and gratiﬁcations approach.
Qual. Mark. Res. Int. J. 16(4), 362–369 (2013)
15. Dominic, M., Francis, S., Pilomenraj, A.: E-learning in web 3.0. Mod. Educ. Comput. Sci. 2,
8–14 (2014)
16. Hussain, F.: E-learning 3.0 = E-learning 2.0 + web 3.0? IOSR J. Res. Method Educ. 3(3),
39–47 (2013)
17. Pieri, M., Diamantini, D.: An E-learning web 2.0 experience. Procedia Soc. Behav. Sci. 116,
1217–1221 (2014)
18. Cao, Y., Hong, P.: Antecedents and consequences of social media utilization in college
teaching: a proposed model with mixed-methods investigation. Horizon 19(4), 297–306
(2011)
Students in Social Media: Behavior, Expectations and Views
97

19. Consoli, D.: A survey on the use of social networks by digital native students. In: Pătruț, B.
(ed.) Proceedings of SMART 2013. Social Media in Academia: Research and Teaching.
Medimond Monduzzi, Bologna, Italy, pp. 204–212 (2014)
20. Georgescu, M., Popescul, D.: Social media literacy in romanian universities – are we ready
yet? Procedia Econom. Finan. 15, 437–444 (2014)
21. Cheung, C.M.K., Chiu, P.Y., Lee, M.K.O.: Online social networks: why do students use
facebook? Comput. Human Behav. 27(4), 1337–1343 (2011)
98
M. Georgescu and D. Popescul

Knowledge Representation and
Processing

Designing, Implementing and Testing
the Acoustic Component of a Text to Speech
System for the Romanian Language
Razvan Alin Boldizsar1, Mihaela Ordean1,2(✉), and Corina Giurgea1,2
1 iQuest Technologies, Somesului Str. no. 14, Cluj-Napoca, Romania
{Razvan.Boldizsar,Mihaela.Ordean,Corina.Giurgea}@iquestgroup.com
2 Technical University of Cluj-Napoca, Str. Memorandumului, nr.28,
400114 Cluj-Napoca, Romania
Mihaela.Ordean@cs.utcluj.ro, Corina.Giurgea@termo.utcluj.ro
Abstract. The large majority of the available Text to Speech (TTS) synthesis
systems intended for commercial applications were developed for international
languages. However, there are also a few precedents for Romanian language. The
present paper resumes the authors’ experience in deﬁning a generic architecture
as well as in implementing and testing the Acoustic Component of the concate‐
native TTS system for the Romanian language that they have developed. The
Acoustic Component is one of the most important parts of a TTS system being
responsible for loading all the acoustic units, changing them in conformity with
a prosody model and concatenating them in order to generate a single acoustic
unit.
Keywords: Text to Speech system · Acoustic Component
Romanian language
1
Introduction
A Text to Speech (TTS) synthesizer is a system that has the capability to convert a normal
language written e-text to speech by generating a synthesised voice signal that mimics
the human voice [1]. When designing/developing a TTS system, especially one that is
intended for commercial applications, there are several goals related to the peculiar
features of the system that should be achieved.
The conversion of a written text to a voice signal is a nontrivial process. A TTS
system could be based on the concatenation of the diphones or phones extracted from a
person utterance and even of the syllables or entire words. While some applications like
reading the current time impose the synthesis of a reduced number of words, others such
as dialogue, personal assistants, SMS to speech conversion, e-learning require a much
extended vocabulary. But recording all the words of a language is I not possible in the
context of developing a TTS system.
Even though all the words of a language are stored in a dictionary, there are still
many other impediments the system should face to as for example the abbreviations,
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 101–114, 2018.
https://doi.org/10.1007/978-3-319-73459-0_7

acronyms, personal names etc. Moreover, for rendering sentences with high naturalness
the system has to convey the synthesized words with a properly intonation at the sentence
level [2]. Performing a humanlike utterance with a certain tonality supposes a good
understanding of the meaning of the words and their in-between relation as well as a
thorough knowledge of that language.
Despite the continuous rise in the computational processing power, developing a
perfect synthesis based TTS system is still a dream. For now, there is a small number
of vendors that oﬀer generic TTS products, not tailored to any application domain. The
systems are not portable. Thus, creating a customized system for a speciﬁc application
is hard and involves substantial eﬀort to redesign every component of the system [3].
Nowadays the research activity in the ﬁeld is focused mostly on the improvement of
this type of systems with respect to the naturalness and intelligibility. The research has
mostly focused on developing systems for international languages, especially for
English.
As concerning the development of TTS systems for the Romanian language, one
should be aware that the use of Romanian language related to the Natural Language
Processing (NLP) represents a challenge for any action that involves the digitization and
implementation of algorithms. That is due to the fact that the Romanian language is a
Romance language whose peculiar features confer it a distinctive status between the
other languages included under this name. Besides that, the Romanian language is
considered an under-resourced language with respect to the availability of language and
acoustic resources [4]. In the last decade, several groups from academia or industry have
been trying to answer to the above mentioned challenge by devoting their activity to the
research and the development of TTS systems for the Romanian language. Their results
were reported in the scientiﬁc literature in the ﬁeld [5–7].
For the time being, several software products able to convert a Romanian written
text to speech are available on the international market. Thus, the leading Text to Speech
company Ivona, an Amazon subsidiary has in its portfolio of synthetic voices, a
Romanian one called “Carmen” [8]. Besides this product, one can ﬁnd on the market
the Lumen Vox Text-to-Speech [9] or WebVOX with the synthetic female voice –
Ancutza, developed by Baum Engineering, a company whose aim is the development
and implementation of hardware and software products for the blind and visually
impaired [10].
Very recent, an international academic research team provided a preliminary version
of a Romanian Text-to-Speech system using the RSS database [11]. Other software
synthesizers for the Romanian language with an interactive online demo for written texts
counting up to 200 words could be accessed at the websites indicated in [12, 13]. One
can make use of them only for non-commercial purposes.
The authors of the present paper are also contributors to this active ﬁeld of research
as part of a team that designed and developed a concatenative TTS system for the
Romanian Language tailored for a commercial application actually running as a service
in several Romanian judicial institutions (like the Courts of Appeal). The context and
the particularities of the authors approach are detailed in Sect. 2. As the present paper
is focused on the Acoustic Component of the concatenative TTS system developed by
the authors it should be mentioned here that in their approach to deﬁne the architecture
102
R. A. Boldizsar et al.

of the Acoustic Component and to implement it, one of the authors’ main concern was
that of providing a relatively weak coupling of this component with the other modules.
After a short description of the architecture of this component with highlights on the
roles of diﬀerent modules it consists of and on a brieﬂy presentation of the algorithms
(Sect. 3), the authors emphasize on describing the implementation (Sect. 4) and testing
(Sect. 5) of the Acoustic Component. The directions for the future work are presented
in Sect. 6 and the Conclusions in Sect. 7.
2
Context and Particularities of the Present Approach
The design and development of the TTS system we refer to in the present paper started
in the framework of a national collaborative research project in partnership between
academia and industry. At that time, Romanian judicial institutions like the Courts of
Appeal required a software solution able to provide a real time conversion of the textual
records from the law case ﬁles to speech by means of a synthesized male voice.
Although at that time the Ivona’s TTS system oﬀering the Romanian female voice
“Carmen” [8] had already been available on the international market, the system was
not speciﬁcally designed for the particularities of the legal language. In addition, the use
of this system for commercial applications is limited by the copyright’s prohibitive costs,
terms and conditions of use. In those circumstances, a team including the authors initi‐
ated a research project with the aim of developing a Text to Speech system focused on
the legal language and taking into account its particularities in terms of vocabulary,
acronyms, abbreviations, shortcuts as well as the possibility of extension to other ﬁelds’
speciﬁc languages. The system was intended to oﬀer both male and female synthesized
voices.
During the development process, the authors faced with the lack of language and
acoustic resources for Romanian language, both of these being indispensable for any
attempt to build a TTS system. Given the fact that at that time no resources that might
be incorporated in commercial applications of the TTS for Romanian language were
available on the market, the team had to develop an acoustic database and build its own
corpus [14], starting from acoustic recordings and continuing with their segmentation.
It should be mentioned here that at the time we started the project, there were no studies
on phones and diphones for Romanian language, this subject being approached by our
team [15].
In the meanwhile, in the framework of a national research project, a partnership
between several research teams in Linguistics, Informatics etc. managed to develop the
digital form of eDTLR (the thesaurus dictionary of the Romanian Language) [16].
Details on the steps the authors have made in order to build their own TTS system
for Romanian language were reported in the two above mentioned papers as well as in
[17]. A description of the components of their own TTS system could be found in [18]
but for continuity and ease of understanding we ﬁnd necessary to resume here a brief
presentation of the main components of this system depicted in Fig. 1.
Designing, Implementing and Testing the Acoustic Component
103

Fig. 1. The structure of the Text to Speech system
Text Processing is the component responsible with the text parsing and with the
generation of the data structure that will be used by the other components of the system.
Acoustic Units Selection is the component in which, based on the text structure and
additional information the units that are for later use in the processing and concatenation
stages are selected from the Units Selection component. The Database encompasses all
the available acoustic units. In the Sound Processing component, the necessary changes
like modifying the pitch or the length are applied over the list of units from the Database
that had been already processed and concatenated. Details on the Text Processing
module as well as aspects concerning the design of this component in the particular
context of developing a concatenative TTS system for Romanian language could be
found in the authors’ previous work [18].
The analysis of symbolic prosody in the Sound Processing component adds prosodic
information to the tree structure of the processed text after its normalization. Symbolic
or abstract prosody is the link between pragmatism, semantics and syntax on one side
and the fundamental frequency, duration, energy and voice volume on the other side.
At this stage of analysis, the text is annotated with pauses, intonation, accent, tonality.
These annotations will be used by Unit Selection component and Sound Processing
component for generating a natural voice signal.
The TTS systems could rely on sound units, segments of audio signal at the level of
words, but this kind of approach would determine the substantial growth of the database
units, as already mentioned in Introduction. For the disambiguation of the homographs
(words with the same spelling and diﬀerent pronunciation) or for the phonetic analysis
of the neologisms there is necessary to build a system able to synthesize any word. The
104
R. A. Boldizsar et al.

grapheme-phoneme conversion is trivial in the systems characterized by a simple rela‐
tionship between phonology and orthography. Such a simple relation can be expressed
by means of rules and it is a feature of phonetic languages.
The TTS system developed by us for the Romanian language, which is a phonetic
language, is part of the category of rule based transcription systems. Even though
Romanian is a phonetic language, it is not exempt from a set of exceptions which cannot
be resolved through rules. The advantage is that the set of exceptions in Romanian is
small enough to be able to tackle the phonetic transcription based on rules.
The current implementation is one that followed the approach of rule based
transcription.
3
Acoustic Component
3.1
Architecture of the Acoustic Component
The Acoustic Component is responsible mostly with: loading the units coming from the
selection phase and their concatenation according to the parameters of the intonation
model as well as with saving the results in a single acoustic ﬁle.
Each phase in the acoustic processing frame is encapsulated into a module. This
approach allows testing at the module level and also increases the degree of errors
detection and processing at a higher level. All errors and warnings generated in this
component are reported into a log.
The general architecture of the Acoustic Component is sketched in Fig. 2. In this
Figure one can notice the dependencies between the components and the highlighted
data structures that were used in the process.
Fig. 2. General architecture of the Acoustic Component
The Acoustic Component needs two data structures as input, namely the tree structure
resulted from disambiguation and text processing (Structured Text) and also a list of
units resulted from the selection process (Units List). The Acoustic Component will
generate as an output a single acoustic ﬁle (Processed Sound).
Designing, Implementing and Testing the Acoustic Component
105

The naturalness of the synthetic speech generated by a Text-to-Speech system
depends on the richness of intonation contours and on the quality of rhythmic patterns.
The responsible for these two aspects is the Intonation Model where the most important
role is played by the fundamental frequency F0.
It should be reminded that beside of the intonation, the other elements of the prosody
(pauses, accent and rhythm) are into a tight relationship with the vocal synthesis, each
with diﬀerent degrees of importance. In order to obtain a speech close with the natural
one, it is compulsory to deﬁne some prosodic rules which will attach supplementary
information to the acoustic elements resulted from the selection module.
The Acoustic Component has the role of managing the operations requested by the
list of units resulted from the selection component, according with the prosodic model
already created. The modules that participate at these operations are: the module for disc
saving (Save), the module for prosody (Prosody Generation) and the one for concate‐
nation (Concatenation). The units loading module (Units Loading) is in charge with disc
accessing and with loading the current chunk into the system memory. The ﬁles used
by the current application are coded in to WAV format. The access to ﬁles is realized
by a specialized library. The module for applying the prosody (Prosody Application)
modiﬁes the sounds loaded by the module previously presented.
3.2
Brief Presentation of Algorithms Used for the Acoustic Component
The prosodic speech modiﬁcation depends generally on the following three character‐
istics of the acoustic signal: the fundamental frequency, the word utterance rate and the
vocal tonality. Many applications for sound processing depend mostly on the successful
modiﬁcation of these three characteristics. For the TTS synthesis systems, the prosodic
modiﬁcation is requested for the adaptation of existing units to the speciﬁc prosody. The
time scaling of an acoustic signal can correspond with the modiﬁcation of the funda‐
mental frequency of the same signal [19].
A method that is used for the time scaling of an acoustic signal is OLA (OverLap
and Add) [20]. As this method is devoted to the time scaling, the fundamental frequency
is not taken into consideration.
The method recommended for the modiﬁcation of the duration in time and of the
fundamental frequency for an acoustic signal can be PSOLA (Pitch Synchronous
OverLap and Add) [2]. This method, along with its variants for modelling in time and
in frequency, is widespread in common Text to Speech systems [19].
The period of an acoustic unit is implemented by using the algorithm TD-PSOLA
(Time-Domain Pitch Synchronous OverLap and Add) [20].
A method of estimating the fundamental frequency in time-domain is based on the
autocorrelation function. The fundamental frequency (F0) is the reverse of fundamental
period (T0) and is deﬁned for periodic or almost periodic signals.
The algorithm we implemented and very often used to estimate the fundamental
frequency of an acoustic signal is based on the AMDF function (Average Magnitude
Diﬀerence Function) [2].
Another part of the Acoustic Component is the Concatenation module that is respon‐
sible for concatenating the acoustic units. The concatenation operation was intensively
106
R. A. Boldizsar et al.

studied in the ﬁrst part of the project implementation when the team considered the use
of an algorithm of two or more units with as little distortion as possible.
The ﬁrst implemented concatenation algorithm was the classical one [2], the two
acoustic units just being joint close together. This algorithm was proved eﬃcient in the
incipient phase of the project which aimed to illustrate the concept of text to speech
synthesis for Romanian language. In that phase the present system was able to divide
the text into words, select the acoustic ﬁles for these words from the inventory of acoustic
units, they then being concatenated during sound processing. The acoustic unit that
resulted after concatenation was then saved on disk by the Save component.
As the project was developed, other concatenation methods were studied. The second
implemented method was the concatenation through the overlapping regions of units [2].
The annotation of acoustic inventory was submitted by including small breaks at the
beginning and at the end of each word. In this way, a simple concatenation reduces the
rhythm of playing. Using the algorithm of concatenation by overlapping, we could
determine, at the level of the sampling, the degree of intermingling of the two units.
The use of concatenation through overlapping over the frames of the letters forming
a word was proved ineﬃcient because the overlapping zone had to be selected very small
and, if for instance, a large diﬀerence in amplitude exists between two vowels, distortions
occur in areas of concatenation.
For this reason it was necessary to develop an algorithm that is able to scale the two
signals for a much smoother concatenation. The algorithm was developed by weights-
based concatenation. These weights can have values in the range (0÷1), the sum of the
two weights being equal to 1. Thus, the ﬁrst signal is multiplied by the ﬁrst weight, the
second by the second one. The result of the multiplication of the two is summarized and
placed into the output array. This sort of concatenation was derived by studying algo‐
rithms OLA and PSOLA [2] namely the Add phase of these algorithms.
The algorithms used by the Acoustic Component are organized as a library composed
of three major parts namely: the general section of the library, the section for determining
the acoustic proprieties and the section which contains the algorithms that modify the
signals.
4
Implementation of the Acoustic Component
In this section it is described the implementation of the acoustic component of the TTS
system, by means of: several details concerning the implementation, the class diagrams
for the modules of the component as well as the interfaces between these modules.
4.1
Reduced Class Diagram
As might be observed in Fig. 3, the architecture of the Acoustic Component is divided
into four sections. Sound Processing is the part in charge with the creation of processing
orders for acoustic units. Binding represents all algorithms for concatenation used in the
system. Repository encapsulates the structure of acoustic properties. Sound Access is
the section for the loading and disc saving of acoustic units.
Designing, Implementing and Testing the Acoustic Component
107

Fig. 3. Reduced class diagram of the Acoustic Component
4.2
Sound Access
The component’s classes SoundLoader and respectively SoundWritter are for loading
and respectively writing audio ﬁles, which in by the authors developed TTS system are
of WAV type. Samples writing are realized by mean of the method WriteSound() in the
SoundWritter class, while all the acoustic related information like: sample rate, number
of channels, format and sampling number are in the class RawSound. If an error occurs
during the saving process, this error is registered into a log ﬁle. In the SoundWritter class
there are implemented diﬀerent methods for linking two or more audio ﬁles. These
methods are useful during the acoustic database creation, if the words are saved in sepa‐
rate ﬁles and an only ﬁnal ﬁle is requested.
The class that encapsulates the loading operations for the samples from audio ﬁles
is SoundLoader. The method used for loading the samples is LoadSound(). All infor‐
mation red from the audio ﬁle is saved in the class RawSound received as an input
parameter. In order to facilitate the read of a part from the audio ﬁle, the method contains
two optional parameters for time speciﬁcation. If these parameters are missing, the whole
ﬁle is loaded.
The windowing process is also a part of the access of the acoustic units the
windowing process. The Window class is the abstract implementation for a windowing
class and contains all the requested methods for windowing over the samples of an
acoustic signal. The MakeWindow() method returns a copy of the windowed samples.
This method is useful for windowing directly of the samples of a signal while keeping
unmodiﬁed its integrity. In order to apply directly a windowing process over a sampling
108
R. A. Boldizsar et al.

set is called the ApplyWindow() method. In the case of this call, for each sample received
as input, is applied the windowing function, replacing the sample value with the new
computed one.
4.3
Binding
The abstract class responsible for the concatenation process is the class Binder. The class
diagram for the Binding process is represented in Fig. 4.
The abstract method BindSound() is the method used for the concatenation of two
acoustic units. The classes derived from this abstract class are: ZeroFrequency, Zero‐
Time, Simple, LinearFrequency, LinearTime.
Two of these analyzing methods are applied in the domain of signal frequency and
the others in the time domain. In this way, some classes use algorithms with applicability
in the time domain while some other classes in the frequency domain. For the current
system, the concatenation algorithms are selected up to the type of the acoustic units to
be concatenated. The classes ZeroTime, LinearTime and Simple use algorithms with
applicability in time domain and ZeroFrequency and LinearFrequency with applica‐
bility in frequency domain.
In the current system, the binding algorithms are selected up to the acoustic units to
be concatenated. In the class CreateSound there are three diﬀerent pointers to the
concatenation objects corresponding to words, syllables and phones.
Fig. 4. Class diagram for Binding process
In order to verify if the concatenation process is applicable over the two sounds to
be concatenated, the method CheckSoundBinding() from the class CreateSound is
called.
Designing, Implementing and Testing the Acoustic Component
109

4.4
Sound Processing
The class diagram for the Sound Processing is represented in Fig. 5.
The class Analyser has a key role in the synthesis process namely the selection of
the processor up to the type of the acoustic units. In this sense, three types of objects are
created and used, namely: PhonemProcessor, SyllableProcessor and WordProcessor.
The separation of the processors up to the type of the acoustic units allows the imple‐
mentation of diﬀerent approaches for processing.
The method ProcessUnits() from class Analyzer gets the string of units generated by
the selection module, commands the loading of each unit from the storage environment
and selects one of the available processors.
Fig. 5. Class diagram for the Sound Processing
4.5
Data Repository
In this section of the application is developed the supply system of the information for
each acoustic unit.
In the data repository component, the class TextProperty stores data like the name
of the acoustic unit, its position inside the word or if the unit was stressed. The class
SoundProperty contains information like the limits inside the audio ﬁle and the audio
ﬁle name. The class PitchProperty has as destination the storage of acoustic information
like the position of all F0 marks.
110
R. A. Boldizsar et al.

The class responsible with the storage and management of the objects for the prop‐
erties types is the class Unit. This class stores the type of the acoustic unit (e.g. word,
syllable, phone).
5
Performance Tests
The entire system’s performance tests were addressed towards the processing time of
all of its components. As concerning the Acoustic Component, the tests were addressed
towards the execution time at the moment when all the facilities were taken into account.
A tool of type clock was implemented in order to determine the execution time of
the component. The execution times for diﬀerent test cases were extracted by placing
marks inside the executed procedures.
The ﬁrst performance tests were conducted by using a computer with the following
conﬁguration: Intel Core Duo T2450 2.0 GHz, 1 GB RAM 533 MHz, 120 GB Western
Digital S-ATA 7200 rpm, WindowsXP Sp2. The tests were applied at the processing
level for words of diﬀerent dimensions. This conﬁguration was chosen for compatibility
reasons with the infrastructure of the customer which was implemented in the early
2000 s.
The assumption of the ﬁrst performance test targets the linear modiﬁcation of the
processing time with respect to the number of periodic regions. The results of this test
are presented in the Table 1.
As it might be noticed, the processing time of a word varies with respect to its dura‐
tion. Certain periodic regions can be detected over a synthetized word. A periodic region
refers to any association of vowels and resonant consonants like m and n. In these
regions, the fundamental frequency marks are emphasized and the success rate of the
detection algorithm for F0 marks is larger. A big number of detected marks implies a
large synthesis area.
Table 1. The results of the performance test at the word level
Word
Initial duration
Number of
vowels
Number of
periodic regions
Processing time (s)
un
0,385
1
1
0,843999
este
0,644
2
2
1,016
acesta
0,775
3
3
0,9689999
soare
0,526
3
2
0,843999
Cluj-Napoca
0.880
4
4
1,530999
persoane
0,715
4
2
0,593999
publicitate
0,970
5
4
1,7350000
conformitate
1,060
5
4
2,109
municipiului
0,990
7
3
1,0309999
In this way, the execution time of synthesis algorithms varies with respect to the total
number of periodic regions. Taking into account all of these, the next performance test
addressed the execution time of algorithms inside the words. In order to obtain valid test
Designing, Implementing and Testing the Acoustic Component
111

data, it was selected a paragraph which was split into a pre-established number of words.
First test included the ﬁrst ﬁve words of the paragraph. Next execution addressed the
ﬁrst ten words of the paragraph. In this manner, the results of the recurrent tests were
inﬂuenced only by the new words that were introduced in the test.
The ﬁrst assumption of the second test targets the linear increase of processing time
with respect to the number of the new words. The second assumption addressed in this
second test targets the determination of the re-processing time for the same input data.
The results of this second test are presented in the Table 2:
The re-processing time was obtained by running the application without stopping it
between the tests. In this way all the F0 marks were stored in the database. These marks
are discarded only when the application stops.
Table 2. The results of the performance test considering the number of words
Number of words
Processing time (s)
Re-processing time (s)
5
6,0620
0,125
10
10,17200
0,21900
15
15,06300
0,219000
25
21,484000
0,35899
35
25,60900
0,46999
6
Future Development
The interest shown to this research domain owes to the big number of development
perspectives it opens. In case of developing real time TTS systems, it is requested a
parallel architecture. With the help of sub-processes, the operations of the text
processing, the unit selection and the audio processing are going to be executed in a
parallel mode over the individual words or even over phrases. In this way, the response
time of the system is improved by interlacing the playback of a partial result and the
processing of the left portions.
The execution time of the Acoustic Component could be improved by processing the
whole information in an oﬄine mode. This information could be: LPC coeﬃcients, F0
marks, concatenation costs, etc. The loading of this data is done up to request, at run
time.
In order to improve the quality of the loaded sound, the processing phase could be
divided into two parts: initial processing and pre-processing. The initial processing could
include all the phases dictated by the intonational model, while the pre-processing phase
could contain the applied algorithms for correction and sound reconstruction. In order
to reduce the distortions introduced by concatenation algorithms, there can be used
algorithms for concatenation based on Bezier approximation curves.
For naturalness improvement of the processed sound, there could be implemented
advanced intonation models at word level, like ToBI or KIM. These models are based
on the analysis of the relations between intonation and phonetic structures which exist
in a language.
112
R. A. Boldizsar et al.

7
Conclusions
The present paper summarizes the authors’ experience in designing, implementing and
testing a generic architecture for the Sound Processing component of a TTS System for
the Romanian language.
The authors’ future work aims at analysing diﬀerent methods for prosody changes
applied to pre-recorded acoustic signals like OLA and PSOLA methods as well as at
developing and implementing their own version for the diﬀerent algorithms involved in
the prosody changes. The algorithms used in the concatenation processes will be also
analysed with respect to their eﬀects on the quality of the TTS system.
References
1. Giurgiu, M., Peev, L.: Sinteza din Text a Semnalului Vocal, vol I. Modelare Acustică si
fonologică. Editura Risoprint, Cluj- Napoca (2006). (in Romanian)
2. Huang, X., Acero, A., Hsiao-Wuen, H.: Spoken Language Processing: A Guide to Theory
Algorithm and System Development. Prentice Hall, Upper Saddle River, New Jersey (2001)
3. Simple4All Project. http://simple4all.org
4. Besacier, L., Barnard, E., Karpov, A., Schultz, T.: Introduction to the special issue on
processing under-resourced languages. Speech Commun. 56, 83–84 (2014). Editorial
5. Burileanu, D., Negrescu, C., Surmei, M.: Recent advances in Romanian language text-to-
speech synthesis. In: Proceedings of the Romanian Academy, Series A, vol. 11, no 1, pp. 92–
99 (2010)
6. Buza, O., Toderean, G., Domokos, J., Bodo, A.Z.: Building a text to speech system for
Romanian through concatenation. In: Proceedings of the 5th IEEE Conference on Speech
Technology and Human Computer Dialogue SpeD 2009, Constanta (2009). http://
www.ms.sapientia.ro/~domi/papers/SpeD2009_poster.pdf
7. SpeeD (Speech & Dialogue Research Laboratory), Current Research Projects: Enhanced Text
to Speech Synthesis in Romanian (2016). http://speed.pub.ro/research/research-projects/
8. Text to Speech Ivona. Voices. https://www.ivona.com/us/about-us/voice-portfolio/
9. LumenVoxTTSVoices. https://www.lumenvox.com/knowledgebase/index.php?/article/AA-01577/0
10. BAUM Engineering. http://www.baum.ro:40/index.php?language=ro&pagina=ttsonline
11. Romanian TTS using HTS, RSS Database. http://romaniantts.com/new/
12. Romanian language text to speech, phobos TTS demo interactive on web. http://www.phobos.ro/
demos/tts/index.html
13. The MBROLA Project. http://tcts.fpms.ac.be/synthesis/
14. Ordean, M.A., Şaupe, A., Ordean, M., Silaghi, G.C., Giurgea, C.: A Romanian language
corpus for a commercial text-to-speech application. In: Sojka, P., Horák, A., Kopeček, I.,
Pala, K. (eds.) TSD 2012. LNCS (LNAI), vol. 7499, pp. 405–414. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-32790-2_49
15. Ordean, M.A., Saupe, A., Ordean, M., Duma, M., Silaghi, G.C.: Enhanced rule based phonetic
transcription for the Romanian language. In: Wang, D., Negru, V., Ida, T., Jebelean, T., Petcu,
D., Watt, S., Zaharie, D. (eds.) SYNASC 2009 Proceedings of the 11th International
Symposium on Symbolic and Numerical Algorithms for Scientiﬁc Computing (SYNASC),
Timisoara, Romania, pp. 401–406. IEEE Computer Society (2009)
16. Cristea, D.: Romanian linguistic resources on very large scale. Comput. Sci. J. Moldova
19(2(56)), 130–145 (2011)
Designing, Implementing and Testing the Acoustic Component
113

17. Şaupe, A., Teodorescu, L.R., Ordean, M.A., Boldizsar, R., Ordean, M., Silaghi, G.C.:
Eﬃcient parsing of Romanian language for text-to-speech purposes. In: Matoušek, V.,
Mautner, P. (eds.) TSD 2009. LNCS (LNAI), vol. 5729, pp. 323–330. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-04208-9_45
18. Ordean, M., Gorgan, D.: The components of a text to speech synthesis system. In: Gorgan, D.,
Guran, A.M. (eds.) ROCHI 2009 Proceedings of the 6th National Conference on Human –
Computer Interaction, Cluj Napoca, Romania, September, pp. 35–39, MATRIX ROM, Bucuresti
(2009). (in Romanian)
19. Kleijn, W.B.: Prosodic Modiﬁcation of Speech. Elsevier Science B.V, Amsterdam (1995)
20. Jurafsky, D., Martin, J.H.: Speech and Language Processing, An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition, 2nd edn. Pearson
Prentice Hall, New Jersey (2009)
114
R. A. Boldizsar et al.

Learning Style in Ontology-Based E-Learning System
Lidia Băjenaru1(✉) and Ion Smeureanu2
1 National Institute for Research and Development in Informatics, 8-10 Mareşal Averescu,
Avenue, 011455 Bucharest, Romania
lidia.bajenaru@ici.ro
2 Bucharest University of Economic Studies, 6 Romania Square,
010374 Bucharest, Romania
smeureanu@ase.ro
Abstract. This paper presents a modality to use learning styles as the main
parameter in determining individual diﬀerences in an ontology-based personal‐
ised e-learning system. The learning style is one of the ways to identify the
student’s preferences which, along with other criteria, such as: student’s level of
knowledge, learning style, and level of education, skills, requirements and feed‐
back, lead to the adaptive system’s choice of personalised learning materials. This
work is meant to emphasize how a personalised e-learning system for the
managers of a hospital in Romania, presented in previous research, uses the
learning styles in modelling the student’s proﬁle. For this aim ontological compo‐
nents implemented with Protégé environment are used. This approach leads to
automate the e-learning environment and to develop the student’s own knowl‐
edge, the personalised learning, the educational content adapted to the learning
styles, respectively to the student’s proﬁle.
Keywords: e-Learning · Adaptive · Personalisation · Ontology · Learning style
Student model
1
Introduction
This paper presents a modality to use learning styles as the main parameter in deter‐
mining individual diﬀerences in an ontology-based personalised e-learning system.
This system is addressed to managers working in a hospital in Romania. The educa‐
tional content oﬀered to the members of the target group is adapted to their proﬁle, level
of education, learning style and needs. The need for training the Romanian medical
system is more and more highlighted by country-level studies and policies. Management
in the ﬁeld of the Health is in a continuous change, and it is the object of managers’
preoccupations in medical units as well as related institutions and also a major priority
of the political agenda in Romania and other countries.
The health human resources are the most important and most expensive resource
categories in the ﬁeld, the management of human resources (HRM) being considered a
crucial component of the health institutions success.
In our country such on-line personalised training programmes in health management
are relatively new, they can be found in post-graduate training programmes and in
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 115–129, 2018.
https://doi.org/10.1007/978-3-319-73459-0_8

ongoing professional training courses organised by software developers as well as in
operational programmes for human resource development.
Integration of information and communication technology (ICT) in educational
environments has brought important contributions to learning processes, respectively in
the development of e-learning environments and their personalization [1].
This technology widely used in schools and not only helps gaining experience in
learning and improving skills in the ﬁeld of knowledge. Essentially, it oﬀers convenient
and eﬃcient access to information and the latest knowledge, new and eﬀective methods
of instruction, learning and knowledge assessment, training and lifelong education [2].
New technologies, Semantic Web, ontologies respectively, are used in the devel‐
opment of e-learning systems to faster getting information on the web or semantic
documents, to be interpreted both by people and machine [3]. Intelligent instruction
techniques such as personalised learning and adaptive systems represent a present-day
preoccupation in the context of the variety of the users’ profiles [4]. In this context,
new pedagogical approaches are necessary to increase the quality of technologies [5].
Ontology is mainly represented as a knowledge base available for diﬀerent applica‐
tions to use and share [6]. It is also used for modelling knowledge in a particular area
of interest; it is a key issue for the integration of information from diﬀerent sources, to
meet the needs of students and their proﬁle [7].
This paper proposes the integration of semantic web technologies with knowledge
management and e-learning, to structure resources and assure the semantic interopera‐
bility in the healthcare human resources management (HHRM). In HHRM a highly
dynamic ﬁeld, the training requirements are multiple and permanent, and the large
amount of information makes it diﬃcult to the student to ﬁnd educational content,
personalised for training requirements
The development of personalization using ontologies represents one of the trends in
improving the quality and eﬃciency of e-learning [8]. Personalising learning focuses
on student-centred teaching based on the student’s proﬁle in the teaching/learning
process [8].
Personalisation of HRM learning for the members of a management team in a
medical unit, as referred to in our research, is achieved by building up of learning units,
personalised for each student, according to his proﬁle, learning style, knowledge level,
expectations and goals. Each student receives learning materials adapted to his proﬁle.
This student-focused learning is achieved by developing a speciﬁc ontology used to
model the components of the proposed system.
The learning style represents the student’s personal characteristic that deﬁnes how
he prefers to learn, to acquire and process learning materials according to his cognitive
abilities [9]. Learning style is one of the most important parameters in determining indi‐
vidual diﬀerences and it underlies the adaptive learning environments [5]. Nowadays,
learning styles represent a major theme in many scientiﬁc approaches [10].
This paper highlights how an ontology-based personalised e-learning system
presented in previous papers [11, 12] uses the learning styles in modelling the student’s
proﬁle.
116
L. Băjenaru and I. Smeureanu

2
Personal Previous Research in Ontology
Our previous paper [11] presents how ontology for modelling e-learning process in
organizing the educational information oﬀered to the management team in a speciﬁc
hospital is used. The personalisation model proposed by the prototype system implies
three aspects: student proﬁle modelling, knowledge domain modelling and learning
process modelling.
Of all the methods and methodologies that deﬁne the development of the system-
integrated ontologies, recognized and validated by the literature and used in our
approach are Methontology [13] and Method 101 [14]. Methontology oﬀers the most
detailed descriptions of ontology development.
Modelling the knowledge of the target ﬁeld (HHRM) represents a key aspect for the
integration of information that comes from diﬀerent sources, in order to support working
together within virtual communities, to improve information retrieval. Thus, the process
of structured ﬁeld modelling consists of breaking down the body of knowledge into a
set of domain knowledge elements named concepts and represents the basic meaning
pieces of knowledge or information. The knowledge is represented at diﬀerent levels of
abstraction, the lowest level being learning objects (LO).
The student model guides the entire learning process and oﬀers essential information
about each student, outlining their proﬁle and determining an instruction that is ﬂexible
and adapted to their speciﬁc characteristics. This model is built by the system incre‐
mentally, using data sources coming from the student, from the forms oﬀered by the
system as well as from the student-system interaction. All these are structured to fulﬁl
two aims: to determine the present level of student knowledge and the objectives that
must be reached by their instruction.
During the knowledge acquisition phase, all relevant information (concepts) required
for ontology conceptualization, a complete and systematic knowledge base about the
competences of the target group is collected to serve the set purpose. Based on the results
of this stage, information are organized and structured in a conceptual model through
formal representations. This model describes the requirements and the solution in terms
of the identiﬁed domain vocabulary.
The implementation of the system ontologies involves setting vocabulary, identi‐
fying classes, subclasses, properties, individuals, relations, inference rules of the type
if-then-else. Classes represent the concepts in the target domain and these are arranged
into hierarchy taxonomy. Properties describing features and attributes of the concepts
and individuals represent a speciﬁc type of the class.
The relationships between ontology components indicate interactions among domain
concepts and they deﬁne properties and attributes that characterize the domain classes.
Protégé, the most popular development ontologies environment, is chosen for the
design and implementation of our ontology.
The graphic representation of the identiﬁed concepts of the e-learning system is
presented in Fig. 1.
Learning Style in Ontology-Based E-Learning System
117

Fig. 1. Basic concepts and relationships between them in proposed e-learning system
The hierarchy of classes, subclasses may be identiﬁed by words such as has_subclass
and the classes properties are described by has_competence, has_job, has_skill, knows,
etc. Object properties represent relationships between individuals and data type prop‐
erties represent the relationship between individuals and data values, as shown in Fig. 2.
Fig. 2. Examples of relations among concepts, Domain and Range
118
L. Băjenaru and I. Smeureanu

Properties may have a domain and a range speciﬁed. Properties link individuals
from the domain to individuals from the range. For example, in the designed system
ontology the property has_skill links individuals belonging to the Person class with
individuals in the class Skill. In this case the domain is Person and the range is Skill.
Examples of object properties and Domain and Range for some concepts can been seen
in Fig. 2.
In another paper [12] the architecture of the proposed ontology-based e-learning
system and a framework for implementation in a real life platform are presented. The
system ontology design, the implementation and evaluation processes are also presented,
through the following: needs analysis, conceptual model design, ontology formalisation
with Protégé 4.3 application; ontology evaluation with the FacT++ reasoner [15].
The conceptual model gained in the conceptualisation stage of the ontology devel‐
opment, describes the demands of the ﬁeld vocabulary as well as its own solution.
Protégé is based on a logical model that makes it possible to describe identiﬁed concepts.
The complex concepts are built by deﬁning simple concepts. Added to this, the logical
model allowed the use of a reasoner, FacT++, which helped verify the consistency of
the ontology aﬃrmations and deﬁnitions and to maintain a correct hierarchy. Through
the set of logical rules it contains, the reasoner contributed to the inference of new
information about the ontology concepts or relationships, to the (in) validation of initial
axioms.
To transform the conceptual model into a formal one, more precisely to explicitly
represent identiﬁed concepts in an oﬃcial language, in our case OWL-DL (Ontology
Web Language - Description Logic) a system of description logical representations is
used.
In developing ontology an important role is played by the restrictions on property
values. A restriction describes a class of individuals based on the relationships between
members of the class they participate in. In other words, a restriction may represent a
kind of class. These restrictions decide that individuals are included or excluded from
a class.
Thus, certain rules of interpretation for General_Manager class can be seen in
Fig. 3. A member of the General_Manager class has deﬁned property value in Skill and
Competence classes.
An important role in the ontology design is played by the deﬁnition of some restric‐
tions of the considered characteristic values. To reﬁne the relationships between
concepts, logical expressions named axioms, are deﬁned. Axioms oﬀer a correct way to
add logical expressions to the designed ontology. They are used to design an explicit
expression of our ontology that allows checking the information accuracy or getting new
information.
One of the key features of ontologies that are described using OWL-DL is that they
can be processed by a reasoner. The ontology needed to be extended with integrity rules
and inference. The FaCT++ reasoner helps to correctly maintain the hierarchy by iden‐
tifying inconsistencies and redundancies, which means it automatically calculates the
classiﬁed hierarchy. FACT++ can also provide new information or validate/reject an
initial assumption.
Learning Style in Ontology-Based E-Learning System
119

The ontologies are exported in RDF Schema (Resource Description Framework) and
OWL (Web Ontology Language) formats, so as to integrate them into the open-source
platform.
3
Student Model
The topic student proﬁle modelling reached maturity in many research papers of the last
decade and it has recently become a promising technology for the personalisation and
adaptability of the e-learning systems [16].
The interest for ontology-based student modelling has been shown in various
published studies, because this technique enables students to represent models in a more
abstract way. Most student modelling projects have been carried out on the web so that
ontologies have become de facto standard for web-based knowledge representation [16].
Modelling the student proﬁle is a process dedicated to representing several aspects,
such as the cognitive analysis of the students’ performance, the identiﬁcation of previ‐
ously acquired knowledge, the storage of episodic memory and the description of
personality characteristics. In this process, relevant information is collected in order to
infer the current cognitive state of the student and to represent it so that the information
be accessible and useful for the adaptation of the tutorial system.
The student model in an adaptive educational system is built to determine the current
level of knowledge and training objectives. It also guides the teaching/learning process
and oﬀers essential information about each student, outlining their proﬁle and generating
a more ﬂexible learning experience that is tailored to their speciﬁc characteristics [16].
Fig. 3. General_Manager class
120
L. Băjenaru and I. Smeureanu

The data thus obtained are necessary for the system to provide suitably adapted educa‐
tional content [17].
In proposed e-learning system the modelling of student’s proﬁle achieved by analy‐
sing learning styles and other issues that arise in the context of e-learning personalization
in the speciﬁed domain (HHRM) is taken into account.
The implementation of learning personalisation in our e-learning system is achieved
using the student model built to determining the knowledge level and identify the
teaching objectives. The e-learning system maintains the student model updated and
collects data for this model, from diﬀerent sources. This process represents the student
modelling and implies the construction of a qualitative representation which records the
student’s proﬁle taking into consideration at least two aspects: initial student’s knowl‐
edge; knowledge acquired during the teaching process.
The techniques used for the determination of the initial student’s knowledge and of
the learning objectives lead to deﬁning the student model, a static model and a dynamic
one. The student’s proﬁle is identiﬁed by integrating a static and a dynamic model.
The static model is created at the onset of the training process and does not change
during the interaction between the student and the system. It contains information about
the following: personal identiﬁcation data, learning style, level of education and learning
interests [18].
The dynamic model is the student’s proﬁle, constantly updated with information
regarding performance and knowledge obtained during the deployment of the e-learning
process, information stored in student’s portfolio [19]. The portfolio stores information
about the student’s current performance and the results of his knowledge testing. The
system maintains updated the student model and collects data from diﬀerent sources.
The modelling of the student proﬁle is achieved with the help of ontologies, respec‐
tively with speciﬁc instruments for ontology design and implementation. The compo‐
nents of the student proﬁle, represented by classes, sub-classes, individualities, object
properties, relationships between these, deﬁne the proﬁle ontology. This ontology is
implemented with the help of Protégé.
The central concept of this ontology is Person class and includes all student’s char‐
acteristics. This class contains the subclasses PersonalInformation, Studies, Learning‐
Style, Skills, CognitiveSkills, LevelLearning.
In Fig. 4 the student’s proﬁle ontology is presented.
The PersonalInformation class contains biographical information obtained at the
registration in the system through a registration form, such as: name, age, etc.
The learning preferences refer to the student’s personal characteristics regarding the
learning style. The learning style deﬁnes how a student prefers to learn, accumulate and
process the learning materials, during the development of his cognitive capabilities. The
learning style is represented in our ontology by LearningStyle class.
The learning styles are registered in the LearningStyle class, according to Felder and
Silverman model [20]. According to this model, some students prefer graphics, others
prefer audio while others prefer text representation of the learning material, and some
students prefer to work in groups while others learn better by themselves.
Learning Style in Ontology-Based E-Learning System
121

Two of these categories of styles, active/reﬂective and visual/verbal, are approached
to in the development of the student model, the component of the prototype e-learning
system [11, 12].
Modelling the student’s proﬁle is the base for the personalization in adaptive Web-
based educational applications and in the development of the proposed e-learning
system [11, 12].
4
Learning Styles in Adaptive E-Learning Systems
The objectives of the new e-learning systems are: automation of learning and develop‐
ment of the student’s skills, personalised learning, and educational content adapted to
the learning styles, wide re-use interoperability.
Numerous studies in the latest years show the concerns in the achievement of learning
environments based on learning styles [9]. Learning based-on learning styles has positive
effects on students’ attitudes towards the course and on their performance [10].
The students are diﬀerent, depending on their level of intellectual development,
concerns in the study domain, approach to learning, and the preferred type of training.
The diﬀerences in learning approach, levels of intellectual development, attitudes about
Fig. 4. Student’s proﬁle ontology
122
L. Băjenaru and I. Smeureanu

the nature of knowledge and the ways it should be acquired and evaluated, have impli‐
cations in students’ learning styles [20].
The learning styles are the cognitive, emotional characteristics and the psychological
behaviours that are relatively stable indicators of students, which inﬂuence the way they
perceive, interact and respond to the learning environment [20]. The fact that each
student has a diﬀerent way to learn and to process information was demonstrated long
time ago, and the beneﬁts of personalization of the learning content according to the
learning styles were highlighted [20].
The term adaptive is commonly used in the literature for personalization of learning
environments [21]. The central component of the design of the personalised training
environment refers to the traits of students to be used and how they will be used.
The adaptive e-learning environment (Adaptive Learning Environment – ALE) can
provide personalised environments, diﬀerent learning strategies, support and interfaces
for solution, taking into account individual diﬀerences [22]. ALEs based on learning
styles are more productive, create a greater level of satisfaction, reduce the learning time
and increase the students’ academic achievements [23]. Adaptive learning environments
are able to: monitor the learners’ activities, interpret them on the basis of domain-speciﬁc
models, infer the students’ needs and preferences, decide what knowledge to oﬀer the
learners, thus facilitating a dynamic learning process.
The students have diverse ways of learning and of processing information, diverse
motivation levels, diverse attitudes to the teaching/learning process, as well as diverse
responses to various classroom environments and teaching practices. All these aspects
diﬀerentiate the way learning is approached to and have implications in the students’
learning styles. In this respect, numerous studies of the recent years present the concern
about the development of learning environments focused on learning styles.
The essential point in the design of the personalised learning environment proposed
in this paper is the consideration of student’s traits, of learning styles, respectively. To
increase the eﬃciency of the teaching/learning process, personalised materials for on-
line learning have been designed in order to satisfy the individual needs of students.
In this regard, numerous studies in the recent years are dedicated to the creation of
learning environments based on learning styles [5]. The learning style along with other
characteristics (level of knowledge, level of education, skills, requirements, feedback
and goals) leads to the determination of the level of adaptation and personalization of
learning materials recommended by the e-learning system. Thus, traditional media web-
based learning environments have been replaced by e-learning adaptive systems based
on learning styles.
In the literature the following models that describe the learning styles are speciﬁed:
Jung’s theory, the Kolb model, the Felder and Silverman model, the Herrmann model,
the Honey and Munford model and the Dunn and Dunn model.
Jung’s theory [24] presents the psychological types as proﬁles that would have
implications for the learning style of the students. This theory has been operationalized
by the Myers-Briggs typological indicator (MBTI) to assess personality types. People
are classiﬁed according to MBTI indicator in four scales of psychological types derived
from Jung’s theory: extrovert or introvert; sensorial or intuitive; thinkers and
researchers; judges (critics) or observers.
Learning Style in Ontology-Based E-Learning System
123

The Kolb experiential model [25] deﬁnes the learning style as representing individual
diﬀerences of learning based on the student’s preferences by using diﬀerent stages of
the learning cycle. Kolb model is based on two scales that show how people can get
information (concrete experience or abstract conceptualization) and how they process
this information (active experimentation and reﬂective observation).
The learning style developed by Felder and Silverman deﬁnes the learning dimen‐
sions: perception (sensorial/intuitive), in-take input (visual/auditory), processing
(active/reﬂective) and understanding (sequential/global). For each of the four dimen‐
sions a learning style was identiﬁed [20].
According to Felder and Silverman’s theory of learning style, the essential learning
characteristics deﬁne four types of students:
• Sensitive and intuitive students. The sensitive students are practical, preferring the
courses related to the real world; pay more attention to the facts and experiments,
while the intuitive students prefer theories, principles, avoiding routine and memo‐
rization.
• Visual and auditory (verbal) students. The visual students remember best what they
see: pictures, diagrams, charts, ﬁlms, demonstrations, while the auditory students
prefer information from words, spoken and written explanations.
• Active and reﬂective students. An active student better understands and retains infor‐
mation from an active experience, while a reﬂective student prefers to learn thinking
individually about the possibilities to solve a problem.
• Sequential and global students. The sequential students follow a way of learning and
problem solving in stage, while the global students randomly accumulate large
amounts of information, without establishing connections between them [20].
The learning styles model created by Felder and Silverman is operationalized
through the Index of Learning Styles (ILS) tool, accomplished by the authors. ILS is an
online tool used to assess students’ preferences according to the characteristics
mentioned above (active/reﬂective, sensing/intuitive, visual/verbal, and sequential/
global).
According to the study [9], between 2005-2014 most of the researches on the use of
learning styles are based on the Felder and Silverman model and the Kolb model. Many
of these studies certify the positive results regarding the eﬃciency and eﬀectiveness of
the AEH systems. They also highlight the student’s satisfaction, respectively the
usability and preferability.
The learning styles are elements which show the weaknesses and strengths of the
student. These are only preferences, not deﬁning indicators to choose a particular school
or a domain of work.
5
Learning Styles in Modelling the Student’s Proﬁle in Our
Proposed System
The prototype e-learning system for the managers of a hospital [13, 14] proposes an
ontological approach to accomplishing the personalization and adapting the educational
124
L. Băjenaru and I. Smeureanu

materials according to the student’s learning style and other personal characteristics and
goals.
Based on the Felder and Silverman model, the e-learning system proposes learning
objects adapted to the student’s learning style as the base for designing and imple‐
menting the student’s proﬁle. In the student model ontology, two types of learning styles
of Felder and Silverman model were implemented, namely visual/verbal and active/
reﬂective styles [20].
A hierarchical representation of the LearningStyle component, subclass of Person
class, respectively classes, subclasses and some individuals, oﬀered by OntoGraf appli‐
cation is shown in Fig. 5. OntoGraf is an application included in Protégé.
Fig. 5. Graphical representation of student’s learning style ontology
In Fig. 6, LearningStyle class is represented in a graphical form with the open-source
tool Graphviz that displays the structure of classes, subclasses and relations between
them such as: has subclass, has individual.
Learning Style in Ontology-Based E-Learning System
125

Fig. 6. Graph of student’s learning style ontology
The conceptual model of student’s proﬁle is exported in RDF, OWL formats and
implemented in a real platform, in our case DOKEOS, an open-source LMS is suggested.
This can be achieved by help of EasyRdf and RDF API-PHP libraries, Semantic Web
toolkit for PHP, that enable an easier processing of the RDF/OWL models.
Our target student enters his personal data in a deﬁned area, at his ﬁrst platform
accessing. The text should contain keywords related to his favourite learning style.
The analysis and interpretation of the text entered by the student are performed
according to the implemented ontology based on the pedagogical aspects of the Felder
and Silverman model, by a Semantic Web engine [26]. The Semantic Web engine repre‐
sents the software components that enable analysis, validation and information
processing available in RDF/OWL formats. The identiﬁed learning style is saved in the
student’s proﬁle. The learning material is chosen by the prototype system so that it
matches the student’s learning preferences and the type of the learning material.
In Fig. 7 the main components of the operating Framework proposed for the model‐
ling of the learning ﬂow based on learning styles are represented.
These are the main components:
• The Student Model allows the determination of the student initial knowledge level
as well as their learning objectives. It guides the student training activity and it is
incrementally built by the system by using the data sources provided by the student
and from the student-system interaction.
• The Collect/Update Module creates the student model and maintains it updated as
well as collecting data for it, from diﬀerent sources. The main static source of infor‐
mation is the student proﬁle. The student performance as well as information about
his knowledge acquisition are dynamically stored in the student’s portfolio and
contribute to the permanent updating the student model.
• The Learning Unit Allocation component together with the Semantic Web Engine
analyse the speciﬁc context of each member of the target group and, using an internal
algorithm based on the data model and on student attributes, it determines the best
personalised learning path for the student to follow.
126
L. Băjenaru and I. Smeureanu

• The Semantic Web component is developed in a a personalised way around an RDF/
OWL Semantic Web Engine and is able to process documents and information RDF/
OWL formatted. According to the proposed instruction model, the best connections
between speciﬁc resources, adapted to the student proﬁle are made Due to its exten‐
sible feature, the Semantic Web Engine is able to oﬀer ﬂexibility to the manipulation
of the RDF/OWL schemas.
• The Educational Content component is in charge of processing and generating the
best personalised educational resource based on the student model. This component
works together with Learning Unit allocation and Semantic web to provide a highly
personalised course.
• The Ontology component, the core of the intelligent e-learning system plays the role
of a comprehensive system of knowledge, in which the competencies of the target
group are concerned; it contains key concepts that allow the use of available knowl‐
edge and their relationships.
The system allows the student to access a complete set of HRM-related concepts, an
explicit formal description being associated with each of them. Once the student has
chosen the target concepts, the system triggers the instruction process by evaluating
several options that aim to design a suitable course, which would take into account the
student’s initial knowledge level, his learning style and other characteristics.
Using mechanisms that are speciﬁc to the e-learning process, presented in the paper
[26] the system can create a connection between the target concepts of the HRM chosen
by the student, the actual student knowledge level, their options, proﬁles and preferences.
The student’s expectations are fulﬁlled by providing adapted education, respectively by
Fig. 7. General conceptual architecture of e-learning system
Learning Style in Ontology-Based E-Learning System
127

oﬀering learning resources adapted to the proﬁle, following their identiﬁcation during
the student modelling process.
After completing the ontology, the results may be implemented into an e-learning
system and used by students from HRM in the public healthcare system, in order to
create and reﬁne the management competences necessary in a general hospital activity
in a dynamic manner.
6
Conclusions
The paper presents the importance of learning styles in student model develop‐
ment, the main component of an adaptive e-learning system. The student model
guides the whole learning process and provides essential information about each
student, defining his profile and providing a more flexible and adapted training. The
ontology-based e-learning system as presented in our approach uses the Felder and
Silverman model to create a personalised learning system for the managers of a
hospital in Romania. Adaptation of courses to students’ learning preferences has a
positive effect on the learning process, leading to greater efficiency, effectiveness
and/or student’s satisfaction. A personalised e-learning system is able to interpret
these data to decide on the students’ requirements and the educational content,
enabling a dynamic learning process.
We have implemented the above presented concepts using a speciﬁc ontology with
a Protégé environment in a model of e-learning system. The main beneﬁt of the proposed
e-learning method consists in personalised training addressing hospital managers in
order to improve their competences and knowledge.
Even though the current system is tailored to the needs of HHRM, its application
could be extended to other ﬁelds of knowledge and learning contexts.
References
1. Zhang, X., Ordóñez de Pablos, P., Zhu, H.: The impact of second life on team learning
outcomes from the perspective of IT capabilities. Int. J. Eng. Edu. 28(6), 1388–1392 (2012)
2. Yilmaz, R.: Knowledge sharing behaviors in e-learning community: Exploring the role of
academic self-eﬃcacy and sense of community. Comput. Hum. Behav. 63, 373–382 (2016).
https://doi.org/10.1016/j.chb.2016.05.055
3. Abanda, F.H., Tah, J.H.M., Keivani, R.: Trends in built environment semantic Web
applications: Where are we today? Expert Syst. Appl. 40, 5563–5577 (2013)
4. Jia, H., Wang, M., Ran, W., Yang, S., Liao, J., Chiu, D.: Design of a performance-oriented
workplace e-learning system using ontology. Expert Syst. Appl. 38(4), 3372–3382 (2011).
https://doi.org/10.1016/j.eswa.2010.08.122
5. Graf, S., Liu, T.C., Kinshuk Chen, N.S., Yang, S.J.H.: Learning styles and cognitive traits –
their relationship and its beneﬁts in web-based educational systems. Comput. Hum. Behav.
25(6), 1280–1289 (2009)
6. Gruber, T.R.: Toward principles for the design of ontologies used for knowledge sharing. Int.
J. Hum Comput Stud. 43(5–6), 907–928 (1995). International Workshop on Formal Ontology,
Padova, Italy
128
L. Băjenaru and I. Smeureanu

7. Cakulaa, S., Sedleniecea, M.: Development of a personalized e-learning model using methods
of ontology. Procedia Comput. Sci. 26, 113–120 (2013). Science Direct
8. Essalmi, F., Ayed, L.J.B., Jemni, M., Kinshuk, Graf, S.: A fully personalization strategy of
E-learning scenarios. Comput. Hum. Behav. 26, 581–591 (2010)
9. Özyurt, Ö., Ozyurt, H.: Learning style based individualized adaptive e-learning environments:
content analysis of the articles published from 2005 to 2014. Comput. Hum. Behav. 52, 349–358
(2015)
10. Balasubramanian, V., Anouncia, S.M.: Learning style detection based on cognitive skills to
support adaptive learning environment – a reinforcement approach. Ain Shams Eng. J. (2016).
https://doi.org/10.1016/j.asej.2016.04.012. Accessed 18 June 2016
11. Bajenaru, L., Smeureanu, I.: An ontology based approach for modelling e-learning in
healthcare human resource management. Econ. Comput. Econ. Cybern. Stud. Res. 1, 23–40
(2015)
12. Bajenaru, L., Smeureanu, I., Balog, A.: An ontology-based e-learning framework for
healthcare human resource management. Stud. Inf. Control 25(1), 99–108 (2016)
13. Fernández, M., Gómez-Pérez, A., Juristo, N.: Methontology: from ontological art towards
ontological engineering. In: AAAI 1997 Spring Symposium on Artiﬁcial Intelligence in
Knowledge Management, Stanford University, California, USA, pp. 33–40 (1997)
14. Noy, N., McGuiness, D.L.: Ontology Development 101: A Guide to Creating Your First
Ontology (2001)
15. FACT. http://owl.man.ac.uk/factplusplus
16. Paiva, R., Bittencourt, I.I., Tenorio, T., Jaques, P., Isotani, S.: What do students do on-line?
Modeling students’ interactions to improve their learning experience. Comput. Hum. Behav.
64, 769–781 (2016)
17. Clemente, J., Ramírez, J., de Antonio, A.: A proposal for student modeling based on
ontologies and diagnosis rules. Expert Syst. Appl. 38(7), 8066–8078 (2011)
18. Goguadze, G., Sosnovsky, S.A., Isotani, S., McLaren, B.M.: Evaluating a Bayesian student
model of decimal misconceptions. In: 4th International Conference on Educational Data
Mining, Eindhoven, Netherlands, pp. 301–306 (2011)
19. Bachari, E., Abelwahed, H., Adnani, M.: E-learning personalization based on dynamic
learners’ preference. Int. J. Comput. Sci. Inf. Technol. (IJCSIT) 3(3), 200–216 (2011)
20. Felder, R.M., Silverman, L.K.: Learning and teaching styles in engineering education. Eng.
Edu. 78(7), 674–681 (1988)
21. Yasir, E.A.M., Sami, M.S.: An approach to adaptive e-learning hypermedia system based on
learning styles (AEHS-LS): implementation and evaluation. Int. J. Libr. Inf. Sci. 3(1), 15–28
(2011)
22. Brusilovsky, P., Peylo, C.: Adaptive and intelligent web based educational systems. Int. J.
Artif. Intell. Edu. 13, 156–169 (2003)
23. Sangineto, E., Capuano, N., Gaeta, M., Micarelli, A.: Adaptive course generation through
learning styles representation. Univ. Access Inf. Soc. 7(1), 1–23 (2008)
24. Băjenaru, L., Balog, A., Smeureanu, I., Marinescu, I.M.: Abordare bazată pe Ontologii a unui
sistem de E-Learning în Domeniul Managementul Resurselor Umane într-un Spital
Universitar. Revista Romana de Interactiune Om-Calculator 8(2), 139–156 (2015)
25. Ontology-based approach to a system of E-Learning in the Field of Human Resource
Management in a University Hospital. Rom. J. Hum. Comput. Inter. 8(2), 139–156 (2015)
Learning Style in Ontology-Based E-Learning System
129

Getting Meaning in the Online Environment
of E-Commerce by Using Semantic Web Technologies
Sabina-Cristiana Necula
(✉)
Department of Research, Faculty of Economics and Business Administration,
Alexandru Ioan Cuza University of Iasi, Carol I Blvd, no. 22, Iasi, Romania
sabina.mihalache@gmail.com
Abstract. This paper treats the problem of information quality from the online
environment of e-commerce. We identiﬁed some determinants of the information
quality considered from the perspective of online buyers. We realized an online
questionnaire which analyzes the factors that aﬀect the information quality. We
found that, for the online buyer, the semantics of product’s characteristics, the
importance of product’s characteristics, and the semantics of product’s price are
the main factors. Therefore, we addressed the problem of semantics by using
semantic Web technologies and we present our approach in this article. Our
study’s results help the e-commerce vendors, responsible with assuring the quality
of information provided to their buyers.
Keywords: Semantic Web · e-Commerce · Information quality
The semantics of product’s characteristics · The semantics of product’s price
1
Introduction
The e-commerce environment is characterized by the presence of diverse users, prefer‐
ences, users’ knowledge and awareness of products’ characteristics. The scope of this
article is to propose some solutions in order to improve the quality of the information
oﬀered to the online buyer.
Economists have theorized that e-commerce ought to lead to intensiﬁed price compe‐
tition, as it increases consumers’ ability to gather information about products and prices.
Information quality is concerned with many aspects related to accuracy and rele‐
vance, but also with semantics.
We studied information quality by appealing to three sub-factors: (1) the semantics
of product’s characteristics, (2) the importance of product’s characteristics, and (3) the
semantics of product’s price.
The next section “The problem” treats the reason that lead to our research.
Section 3 discusses the methodology used, Sect. 4 treats Semantic Web functionalities
for e-commerce, Sect. 5 discusses the main achievements and Sect. 6 presents the main
conclusions.
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 130–137, 2018.
https://doi.org/10.1007/978-3-319-73459-0_9

2
The Problem
Even if all the existent solutions are well intended, the reality proved that the online
buyer’s satisfaction is still low due to some reasons. The literature identiﬁed causes
related to: (1) information content, product’s description, transaction’s requirements,
technological causes, institutional causes, consumer’s behavior [1], (2) the quality of
the online vendor/seller website, the Internet quality, infrastructure or contextual factors
[2], (3) the informational content of the Web site [3, 18, 19], the navigation on the Web
site [4].
As many authors noted [5–8] there is still need of empirical studies in order to clarify
the determinants of e-services quality and to measure the inﬂuence of these determinants
on online consumer’s perceptions.
A reference study from the ﬁeld belongs to Liu et al. [5] which presents a model to
evaluate online consumers’ satisfaction. They have shown that the Web site design, the
information quality, the commercial issues, the transactional issues, the security, the
payment, the delivery and the services oﬀered to online consumers inﬂuences directly
and positively the online consumers’ satisfaction.
The information quality from the online environment is considered to be a main
preoccupation both in theory as in practice [5, 9].
Concerning the information quality, the majority of the studies are centered on the
Web site design [3, 4, 10, 11].
Very often, the authors study the inﬂuence of information quality on the e-commerce
Web site quality without studying the determinants of information quality. Cheung and
Lee [9] considered to be important the accuracy, the content, the format, and the time‐
liness of information. Liu et al. [5] shown that the accuracy, the intelligibility, the
completeness, and the relevance of information are important for consumer’s satis‐
faction.
The scope of our article is to identify the causes of the information intelligibility and
accuracy, and to propose solutions for this problem.
The problem that this paper addresses resides in the question: how to enable the
discovery of relevant information within the entire amount of information and what are
the proper required ways to approach the buyer?
Very often, the buyer does not know brands’ names, but only few properties of the
product which the consumer considers to be proper for his/her needs. Also, the buyer
does not know which the most important characteristic of the product is. We approached
this problem by using Semantic Web technologies. Semantic Web has many applications
in the ﬁelds of data integration, data visualization and searching data semantically linked.
A semantic Web application consists in using Resource Description Format (RDF) [12,
17], Ontology Web Language (OWL), SPARQL Protocol and RDF Query Language
(SPARQL), and a dedicated environment but also very portable like Jena [20].
A community for Linked Open Data has emerged, developing best practices around
the publication of distributed semantic data [13]. Heath and Bizer 14] present some
valuable information about Linked Data.
For the purpose of our study, we identiﬁed some specialized vocabularies to describe
products. Schema.org and GoodRelations [15] are two vocabularies which are suitable
Getting Meaning in the Online Environment of E-Commerce
131

for describing products. Both of them oﬀer the possibility to add descriptions about
reviews, rating, manufacturer, isRelatedTo, category. GoodRelations is used either to
describe data by diﬀerent online vendors or to implement web service (GoodRelations
Amazon Checker [16]).
3
Methodology
The study is based on the results obtained from an online questionnaire addressed to the
students which follow bachelor studies of one of the major business faculty from
Romania, in the 2015–2016 university year. The sample was estimated to be represen‐
tative for the entire population. The questionnaire had a number of 185 respondents for
a conﬁdence level of 95%.
We considered the following three determinants:
• The semantics of product’s characteristics. Very often, the online consumer does
not have the necessary knowledge or information to evaluate and understand the
product’s characteristics semantics. The simple listing of product’s characteristics
does not oﬀer the necessary meaning/sense.
Therefore, we analyzed the semantics of product’s characteristics by addressing
questions regarding: the possibility to realize comparisons between text characteristics,
the existence of text explanations about the characteristics, grouping the characteristics
depending on their importance, the semantics of product’s characteristics.
• The importance of product’s characteristics. Besides the fact that, very often, the
product’s characteristics are not completely understood by online buyers, it may
happen that the respective consumers do not know the importance of product’s
characteristics. It is possible that a certain characteristic/functionality to be decisive
for the optimal use and, therefore, knowing this characteristic to be of high impor‐
tance for the final consumer. We consider that the correct description and categori‐
zation of each product’s characteristic are essential for the evaluation of product’s
characteristics.
Therefore, we asked the respondents to evaluate: the possibility to select a product
from an organized list, searching a product by using keywords related to product’s char‐
acteristics, rating/reviewing the product, the importance of product’s characteristics
(rating the characteristic). It seems that this determinant of the importance of product’s
characteristics is valuable for information quality.
• The semantics of product’s price. Although the price element is a decisive variable
in the entire acquisition process, which deserves a separate study, concerning our
study, we consider the meaning of product’s price to be important information.
Viewing the average price of the products from the same category and the descending
sorting of products depending on their price are capable to raise the signiﬁcance of
product’s price information.
132
S.-C. Necula

Our study tries to prove the direct and consistent relation between the information
quality oﬀered by the online environment of e-commerce and the overall satisfaction of
the online buyer by analyzing each of the three proposed factors.
We emailed the questionnaire to 223 respondents. We received a total of 185 usable
responses, having a response rate of 82,95%. 32,44% of the total number of respondents
did not bought online products, while 67,56% bought online products.
The resulted values of our study are presented in Table 1.
Table 1. Buyer’s overall satisfaction on the online environment of e-commerce
Indicator
Average values
Levene’s
statistic
p-value
Online
Non-online
The possibility to select a product
from an organized list
4,64
2,05
4,20
0,04*
Searching a product by using
keywords related to product’s
characteristics
4,71
2,18
4,8
0,02*
Rating/Reviewing the product
4,30
3,25
2,29
0,13
The possibility to realize
comparisons between text
characteristics
3,51
2,00
6,41
0,01*
The existence of text explanations
about the characteristics
3,37
3,02
1,72
0,19
Grouping the characteristics
depending on their importance
3,35
2,08
3,96
0,04*
The simple product’s text description 4,00
3,23
0,31
0,57
The meaning of product’s
characteristics
4,02
2,63
3,48
0,06
The importance of product’s
characteristics
4,28
2,58
6,44
0,01*
The meaning of product’s price
4,50
2,65
4,18
0,04*
*Signiﬁcant at p < 0,05
To evaluate if the diﬀerences between the two groups are signiﬁcant we used
Levene’s tests to test the null hypotheses. Levene’s test is an inferential statistic used to
assess the equality of variances for a variable calculated for two or more groups. Because
all of the Levene’s values are high and positive, we can infer that for the online
consumers the identiﬁed factors have a high level of importance.
Our results show that the Levene’s test is signiﬁcant for the semantics of product’s
price and for the importance of product’s characteristics. The results of this research
are limited by the fact that the respondents are young students, aged between 19 and 25
years. We base our solutions on the obtained results and we approach them by using
semantic Web technologies. We present these solutions in Table 2.
Getting Meaning in the Online Environment of E-Commerce
133

Table 2. The proposed solutions for raising the information quality from the online environment
of e-commerce
Factor
Solutions
The semantics of product’s characteristics
Descriptions/Reviews
The importance of product’s characteristics
Likes counting/Listing/Sorting
The semantics of product’s price
Listing/Sorting/Price comparisons
Given the promise of Semantic Web technologies to attach meanings to diﬀerent
datasets, we have chosen this technology to accomplish the scope of the article. We
chose to implement the descriptions as properties of a RDF vocabulary and improve the
semantics by using RDF descriptions. Section 4 discusses the main details of our imple‐
mentation.
4
Semantics and Improved Semantics
We want to infer new semantics about our products, so we deﬁned our vocabulary that
describes products as a hierarchy of classes and subclasses. Our vocabulary is written
in RDF(S) and entitled product. The class hierarchy is presented in Fig. 1.
Fig. 1. The class hierarchy
We chose to deﬁne our own ontology with RDF Schema because this allowed us to
deﬁne basic vocabulary items and the relations between those terms (Class, Property,
type, subClassOf, range, domain). Semantics gives “extra meaning” to particular RDF
predicates and resources, which allows us to:
1. Speciﬁes how terms should be interpreted;
2. Draw inferences.
Considering the results of our questionnaire, we implemented an object oriented
model for describing the importance of product’s characteristic and the semantics of
product’s price. This model helps us derive new information and feeds the RDF(S)
model with data. We deﬁned the attribute rating as being speciﬁc to characteristic
because we wanted to derive the characteristic that has the highest rating for every
product, so each product has a list of characteristics. Our model is presented in Fig. 2.
134
S.-C. Necula

Fig. 2. The classes to describe the semantics for the case
Our model implements the logic of getting the list of products that match the expected
characteristic and its importance and the semantics of the product’s price. Concerning
the importance of product’s characteristic, we returned the characteristic which has the
highest rating from every location. For the semantics of product’s price we returned the
list of products that have the price below the average price that comes from every loca‐
tion. For the hierarchy of products and their characteristics we used RDF(s) and RDF
description.
The use case supported on the end-user side is the one in which the user searches,
for example, a hard disk drive. The user doesn’t know any characteristic of this kind of
product, what the most important characteristic is and what the average price to make
price comparisons is. The instances come from an online e-commerce site (e.g.
www.price.ro). From this site we gathered products’ information and their prices from
diﬀerent websites. For the moment, this kind of e-commerce website doesn’t evaluate
every characteristic as a rating/review/opinion. This is another reason why we undertook
an example in which we instantiated information about characteristics’ rating.
The Web publisher describes the products in a HTML page and into an RDF docu‐
ment which uses the product ontology.
5
Discussions
We observed that buyers where not interested about the semantics of product’s charac‐
teristics, but by the importance of product’s characteristics.
Having deﬁned these central dimensions for characterizing online consumers, this
survey outlined a vision for an online store that describes its products by respecting
semantic Web principles.
We actually modeled the required extension (characteristic’s rating). The main
beneﬁts from describing data with RDF(S) and from using semantic Web technologies
consist in having data from diﬀerent locations in a structured format, which allowed us
Getting Meaning in the Online Environment of E-Commerce
135

to make inference on data and give back appropriate results. Data integration is an
important issue especially in the online environment.
The main diﬀerence between our implementation with RDF(S) and using RDFa or
common accepted ontologies as GoodRelations consists in using rating as an attribute
for characteristic and in using the average price as an indicator for the list of products
to be returned.
6
Conclusions
We identiﬁed and discussed the types of available/missing user support and we came
with potential solutions from Semantic Web area.
There are some important aspects to note:
• It is necessary to have a set of ontologies that the community accepts. In order to
reach consensus on the most useful ontology, there is a need to use these ontologies,
to evaluate them by using them and to gain proﬁt. GoodRelations is an ontology
included in schema.org, which means that Google, Yahoo! or any other search engine
uses online store’s descriptions made by respecting GoodRelations in presenting their
results.
• Each online store has to semantically describe its products according to a chosen
ontology.
Our contribution is in two areas: (1) we tried to identify the main determinants which
aﬀect the information quality from the online environment of e-commerce and (2) we
proposed solutions to improve the information quality.
The remaining question to discuss is: will the e-commerce gain in front of traditional
e-commerce by using semantic Web technologies and by having available qualitative
information in the online environment?
References
1. Kim, D., Song, Y., Braynov, S., Rao, H.: A multidimensional trust formation model in B-to-C
e-commerce: a conceptual framework and content analysis of academia/practitioner
perspectives. Decis. Support Syst. 40, 143–165 (2005)
2. Surcel, T., Dinu, V.: Auditul comerțului electronic în relație cu protecția consumatorilor.
Revista Amﬁteatru Economic 21, 115–120 (2007)
3. Egger, F.N.: From Interactions to Transactions: Designing the Trust Experience for Business-
to-Consumer Electronic Commerce (2003). [e-book PhD Thesis] Available through:
Eindhoven University of Technology (The Netherlands). ISBN 90-386-1778-X. http://
www.webusability.ch/articles/egger2003trust.pdf
4. Cheskin research and Studio Archetye/Sapient. eCommerce Trust Study (1999). http://
www.added-value.com/source/wp-content/uploads/2012/01/17__report-eComm-Trust1999.
pdf
5. Liu, X., He, M., Gao, F., Xie, P.: An empirical study of online shopping customer satisfaction
in China: a holistic perspective. Int. J. Retail Distrib. Manage. 36(11), 919–940 (2008)
136
S.-C. Necula

6. Santos, J.: E-service quality: a model of virtual service quality dimensions. Manage. Serv.
Qual. 13(3), 233–246 (2003)
7. Janda, S., Trocchia, P.J., Gwinner, K.P.: Customer perceptions of internet retail service
quality. Int. J. Serv. Ind. Manage. 13(5), 412–431 (2002)
8. Yang, Z., Jun, M.: Consumer perception of e-service quality: from internet purchaser and
non-purchaser perspectives. J. Bus. Strat. 19(1), 19–41 (2002)
9. Cheung, C.M.K., Lee, M.K.O.: Research framework for consumer satisfaction with Internet
shopping. Sprouts Working Papers on Information Systems, 5(26) 2005. [e-journal] Available
through: City University of Hong Kong, China. http://sprouts.aisnet.org/5-26
10. Pleșea, D., Pamﬁlie, R., Maiorescu, I.: Relația dintre structura comunicării și evitarea
incertitudinii reﬂectată în desgnul site-urilor web românești. Revista Amﬁteatru Economic
13(5), 628–635 (2011)
11. Homocianu, D., Necula, S.C., Airinei, D., Radu, L.D., Georgescu, M.R., Baciu, L.L., Damian,
A.C.: Multimedia for learning in economy and cybernetics. J. Econ. Comput. Econ. Cybern.
Stud. Res. 48 (2014)
12. W3C. https://www.w3.org/TR/rdf-schema/
13. Segaran, T.: Programming Collective Intelligence. O’Reilly Media, Sebastopol (2007)
14. Heath, T., Bizer, C.: Linked data: evolving the Web into a global data space (1st edn.). In:
Synthesis Lectures on the Semantic Web: Theory and Technology, vol. 1, no. 1, pp. 1–136.
Morgan & Claypool, San Rafael (2011)
15. GoodRelations. http://www.heppnetz.de/projects/goodrelations/
16. GoodRelations Amazon Checker. https://chrome.google.com/webstore/detail/goodrelations-
amazon-chec/jlfealjceojnﬂopgcelhcmghpakmklk
17. Berners-Lee, T., Hendler, J., Lassila, O.: The semantic Web. Sci. Am. 284(5), 34–43 (2001)
18. Vermeulen, I.E., Seegers, D.: Tried and tested: the impact of online hotel reviews on consumer
consideration. Tourism Manage. 30(1), 123–127 (2009)
19. Coker, B.L.S.: Seeking the opinions of others online: evidence of evaluation overshoot. J.
Econ. Psychol. 33(6), 1033–1042 (2012)
20. Yu, L.: A Developer’s Guide to the Semantic Web. Springer, Heidelberg (2014). https://
doi.org/10.1007/978-3-662-43796-4
Getting Meaning in the Online Environment of E-Commerce
137

Modeling and Simulating a Call Center Activity
Georgeta Soava
(✉) and Adina Balan
Faculty of Economics and Business Administration, University of Craiova, Craiova, Romania
georgetasoava@yahoo.ro, adinaburicea@yahoo.com
Abstract. In this paper we intend to develop an IT application for the manage‐
ment of customers and tickets in the call center of a ﬁrm that produces software.
The aim of this application is the automatic distribution of the customers’ calls
and their quick solving and to oﬀer customers support in using and repairing the
purchased software. Thus, we have begun by presenting some theoretical consid‐
erations regarding the business processes, their modelling and techniques of
modelling, which are useful in the case study. Then, we have modelled and simu‐
lated the call center activity in order to maximize productivity and reduce the
waiting time. We have decided to develop such an application as the software
developers market is in full development, and such a company must oﬀer its
customers support and consultancy and by using the customer management
system, the work of the department is more eﬃcient.
Keywords: Business process · Call center · Modeling · Simulation
1
Introduction
The dynamics of the business environment and the economic uncertainty make compa‐
nies work intelligently and so business applications are purchased, built and developed
to satisfy the demands of the business environments, which change continuously. We
approached this topic as we consider that the dynamics of today’s business environment
needs a profound perspective on business. Companies rely more and more on real time
information, which means good management of business processes, its automation. The
topic dealt with is of topical importance as modern management must be permanently
preoccupied with identifying the needs of customers and a company which develops
software products must oﬀer its customers permanent support and consultancy.
Forwards, we intend to make an application for customers and tickets management
within the call center department of a ﬁrm that produces software. The purpose of the
application is to maximize productivity of the call center department by creating an
automatic distribution system of the customer’s calls and solving their problems as fast
as possible, and also to oﬀer customers permanent support in using and troubleshooting
the purchased software. Business process modeling of the call center which is necessary
to help the staﬀ answer promptly to the customers’ demands and to keep a record of the
encountered problems, will be made using the modeling language UML (Uniﬁed
Modeling Language) and to determine the optimum number of resources which are
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 138–146, 2018.
https://doi.org/10.1007/978-3-319-73459-0_10

necessary to support the desired level of service, we will make successive simulations
with the help of the EXTEND packet.
2
Literature Review
A business process is a collection of connected structured activities or tasks, which
produce a certain service or product (intended for a certain purpose) for a certain
customer or customers [1]. The processes in companies are means by which organiza‐
tions conduct their businesses, including any activity which adds value to the internal
resources to get a product or a service destined to a certain beneﬁciary [2]. Most special‐
ists in organization management consider that business processes can be regarded as
goals in making a business strategy and as models of business automation, too by using
IT technologies [3]. With the switch to an IT society, companies pay more attention to
business processes optimization using IT technology extensively and allowing compa‐
nies a personalized modelling of business processes [4].
The business process is a collection of activities whose purpose is to obtain an output
speciﬁc to a certain customer or market, using a set of inputs [5].
Business Process Modeling (BPM) is the activity of representing a company’s
processes so that these can be analyzed and improved. BPM “is a business solution
approach which views a business as a set of processes or workﬂows” [6].
Business modeling is done with the help of a modeling language, which provides a
set of syntactic and semantic rules which allow building diagrams and related objects.
Besides the main modeling languages (UML, BPMN), in recent years a series of exten‐
sions have been developed, which include a series of characteristics of collaborative
processes and cooperation templates, as well as a series of modeling and simulating
instruments [7].
Business Process Management (BPM) at the level of a company can be deﬁned as a
structured approach of business processes, of current activities modelling and optimi‐
zation (especially the repetitive ones) and of human interactions with the interior and
exterior environment of the company [8]. The activity of any company is the dynamics
and by BPM a permanent process and modelling updating can be ensured through a
continuous feedback [9]. A language that comes to support programmers, as a means of
improving the software implementation process, in specifying, visualizing and revolu‐
tionizing software system models, is UML. UML represents the connections between
the business environment and the programming one, by architectural modelling and their
application in developing, implementing, maintenance and evolution [10]. Covering a
complete life cycle process, this platform introduces key functionalities in order to allow
a signiﬁcant optimization and improvement of all parameters that are vital for a business.
Yet, for optimum results, this approach should be used hand in hand with business
process simulation.
Through the simulation process, companies have the possibility to practice diﬀerent
activities and tasks, which will lead to processing abilities by a continuous learning,
without risks [11]. Simulations that use modern technologies allow users to test multiple
Modeling and Simulating a Call Center Activity
139

scenarios in a short period of time and to repeat the strategies that lead to success, having
the capacity to handle hundreds of variables simultaneously.
3
Modeling and Simulating a Management System of Customers
and Tickets in a Call Center
3.1
Call Center and Ticketing Activity
The Call Center department is established with the purpose to deal with a huge volume
of calls, to ensure support to customers and to oﬀer certain telemarketing services using
telephone solutions meant to answer customers’ requests and to solve them as quick as
possible and to oﬀer customers support in using and repairing the purchased software
system. The purpose of the application that we intend to make is to create an automatic
call distribution system to maximize the department productivity by surveying the
process of introducing the received calls in the corresponding queue, adding priority to
calls according to various factors (the call entrance order, its importance, the emergency
of the caller’s situation) and ﬁnally, sending the call to the available agent. The outgoing
calls will be automatically generated, monitoring the agents’ availability, productivity
and will oﬀer solutions for planning resources according to availability, using methods
of recording the performed activity and reporting in real time.
The system mechanism is the following: when receiving a call, the system calls all
the operators that are not engaged in a conversation, and the ﬁrst to answer will take the
respective call while the other operators’ telephones stop ringing. When receiving a
customer’s demand, this is recorded and in case it can be solved within the department,
the customer will get the necessary information or the solution to the problem. If the
request is more complex, whether it is about solving a programming problem or about
the necessity of developing new applications or module, the request is transmitted to the
programming department which will solve it and transmit the solution to the call center
department to be communicated to the customer and to record the problem solving.
The advantages of implementing a call center within society: maintaining a perma‐
nent connection with the customers; rapidity in dealing with requests; records of the
customers’ requests; sorting requests to maximize the solving time; recording customers
according to categories of customers (natural/legal persons); possibility to deal with
requests without having to go to the customer’s location.
The ticketing system means that the customers’ management will be achieved with
the help of some support tickets which allow highlighting, at any time, the solved and
unsolved requests of each customer. A ticket is in fact a request status at a certain
moment. The operator who takes the request by call or e-mail will ﬁll in a ticket with
details about the customer and will make a short description of the problem. At this
moment the ticket will become active and will be placed on the waiting list to be dealt
with by specialists. With the help of a ticketing system the steps in solving a problem
can be visualized as well as the date when the request was dealt with and the way in
which it was resolved.
140
G. Soava and A. Balan

3.2
Modeling the Process in the Call Center Department with the UML Modeling
The application will allow the keeping of a log of the company’s customers and their
requests, to provide support at all times and to provide high quality services. The main
characteristics and facilities provided by the application are: the secure access of appli‐
cation users (normal users and administrators); deﬁning customers classiﬁcation (phys‐
ical and legal persons); the application is designed to run in parallel with other applica‐
tions so it takes up very little space on the user’s desktop; it allows the quick ﬁnding of
customers using diﬀerent search criteria and the keeping of the tickets for each client,
so that at any moment it can determine the number of unresolved issues and the content
of each of these issues. So, we need to analyze the following information: the main
information relating to clients and tickets management, and the identiﬁcation of actors
(system administrator; the call center operator; customer; programmer; manager) and
use cases (Administration/Conﬁguration system; Customer management; Customer
ticket management; Call center operator) that are necessary for the modeling process.
Naturally, we continued the modeling and realized diﬀerent types of UML diagrams:
(1) Use Case Diagram shows a collection of use cases and actors, provides a general
description of how the system will be used and an overview of the functionalities
that are intended to be provided by the system, shows how the system interacts with
one or more players and ensures that the system will produce what is desired:
1. Call center operator use case - the actors are represented by the customer and
the call center operator. The entire activity starts with the customer’s request,
being followed by the operator’s authentication to the application and the data‐
base update. The call center operator can perform various activities: the account
administration; receiving customer’s request; customer search; customer data
update; adding customer ticket; customer ticket update; oﬀers answer to the
customer’s request.
2. Customer management use case - main actors: customer, call center operator,
programmer. The operator receives a request from the customer and then checks
the existence of customer’s personal data in the system. If the data exist, the
operator gets straight to checking the history of the tickets and their state. Then
the information about the customer’s problem is added in the database. The call
center operator will decide whether the problem will be proposed for being
solved or it can be solved within the call center department. After solving the
problem, the customer will be informed and the ticket can be closed.
3. The administrator use case has as main actors: the system administrator and
manager. The administrator can be authenticated in the application where he
can add, modify and delete users, can check each account and can obtain reports
regarding the tickets state so that the manager can make a decision regarding
the productivity of call center operators.
4. Ticket management use case - main actors: the system administrator, customer,
operator and programmer. The administrator and the operator authenticate, the
customer sends the request to the operator and the administrator manages the
account, examines and visualizes the state of the tickets sent to the operator.
The operator is informed about the problems, deciding to open a ticket, adding,
Modeling and Simulating a Call Center Activity
141

modifying or deleting a ticket, the possibility to solve the problems or not. If
the problem cannot be solved, the operator will send it to the programmer who
deals with the customer’s request, transmits the solution to the operator who
sends it to the customer and closes the ticket. Collating all the use cases
presented above, we get the detailed diagram of the use cases.
(2) The sequence diagrams will be used to illustrate the interactions between objects
and actors and objects temporally and we can better understand the moments of the
problem solving process in call centers. The main actors are: customer, supporting
actors: operator, programmer and objective: solving the customer’s problem. It
begins with the customer’s request and more stages are followed to solve this
request. It is observed that it is an iterative process, each activity being preceded
by another.
(3) The object diagrams illustrate objects and connections between them. Together with
class diagrams they represent the static structure.
(4) The class diagram describes the system structurally, highlighting its classes, attrib‐
utes, methods and the relations between classes.
(5) The activities diagram is made for modeling the dynamic aspects of a system at
diﬀerent levels, from the overall business process to the level of a class operation.
(6) The state diagram allows us to observe the various states through which the initial
process passes along various operations;
(7) Through collaboration diagrams we have illustrated by streams of messages and
links between system components. State diagrams model the eﬀect of these inter‐
actions on the internal state of each object.
(8) Collaboration diagram (also known as communication diagrams) illustrate the
message ﬂow and the connections between the system components. Messages are
added on the connections between objects. In this diagram we can notice the process
of user addition by the system administrator, as well as that of tickets and customers.
To update data collaboration between diﬀerent processes is needed. As a result of
the database checking process it is necessary that the application access the database
engine to get the information.
(9) Component diagram - the functioning of this application depends on the existence
of more components. The application can run on a computer, its performances
depending on the computer resources.
After making the modelling through the UML language and clearly determining the
logic of the model and of the connections between schemes, we know the activities in
the process, the succession in which they are performed (the process arrival and
processing for each activity) and the logical relations between activities, we considered
as useful to make a simulation of the application and to ﬁnd the best possibilities to
increase the productivity of the call center department.
3.3
Simulation of the Business Process by Using the EXTEND Packet
The objective pursued through the simulation process is to determine the optimum
number of call center operators who could ensure the shortest waiting time for customers
142
G. Soava and A. Balan

and to increase the customers’ satisfaction level. Also, the information produced after
a simulation model is very useful for evaluating the quality of the call center service.
The model considers the following aspects: telephones call at irregular intervals and
there are operators ready to answer these telephones; the operators are specialized in a
certain type of problem, but can answer more problem types; each operator will answer
according to his speciﬁc training on the phone if such a call is on hold; if telephones do
not deal with their specialization, they will answer one which is on hold. As we can
observe, the application for a call center is a waiting system (it has a signiﬁcant number
of callers, it is not known for certain when the operator will be requested, there is a
certain number of operator or performer stations that deal with the requested service,
the duration necessary to solve a problem is not known exactly and there can be uncer‐
tainties in the callers’ behavior) and so, we will use the simulation of discreet events.
The administrator is concerned with both the group productivity (the problems that
were solved compared to the total number of requests) and the waiting time which must
be reduced as well as the time needed to solve problems. The call center processes will
be assisted by a support program which means solving a problem faster. The problem
that arises is that the operator has to solve both the written problems (by email) as well
as the ones presented by phone, in the shortest possible time. Based on the information
that was obtained in the modelling process we can make the following assumptions: (1)
the requests on the support programs appear with a frequency of about 18 per hour (with
time periods between arrivals governed by an exponential distribution), two thirds of
the requests are emails and one third are phone calls; (2) emails need an average of
12 min each to be solved; it can be supposed that the actual time vary according to a
normal distribution with the 12-min average and the 2-min standard deviation; most
phone calls need only 8 min to be solved; speciﬁcally, it can be supposed that the neces‐
sary time of a phone call follows a distribution of discreet probability where 50% of the
calls need 8 min, 20% need 12 min, 20% need 17 min and 10% need 20 min; (3) starting
from these assumptions, the administrator considers that one operator, with a little eﬀort,
is able to honor the emails and phone calls.
Given this information, administrator concluded that it was correct to suppose that
with a little more eﬀort, the intervention staﬀ can honor emails as well as phone calls.
The logical argument is the following: 96 emails (12 emails/h × 8 h) every 12 min each
need 1.152 min, and 48 phone calls (6 calls/h × 8 h) at an average of 8 min each need
348 min. All these total 1.536 min or 3 working days of 8 h and 32 min each. The
administrator considers that the staﬀ is professional and will work the 32 additional
minutes that are necessary.
First, the administrator’s argumentation must be regarded carefully. The adminis‐
trator ran average of 8 min is needed. Although half (50%) of the calls need 8 min, the
average calls need 11,8 min (8 × 0,5 + 12 × 0,2 + 17 × 0,2 + 20 × 0,2). Moreover, the
probability that a phone call should need 17 min is quite high (30%) or even more, which
represents more than double of what the administrator uses in his calculations.
The analyzed process looks at modelling the work of one intervention person, so for
simulation we will set the arrival frequency of the emails and phone calls to represent
what one operator can honor. According to the mentioned arrival frequencies, the time
between arrivals is of 5 min for emails and 10 min for phone calls. The simulation process
Modeling and Simulating a Call Center Activity
143

will be initiated by modelling the work of one call center operator and we notice that
the process needs further personnel, so other operators will be added, depending on the
queue, until the shortest waiting time for customers will be obtained.
The calls arrive and are placed on the waiting line before being taken by an agent.
If the line is loaded, the call will be blocked and the one who calls will get a busy signal.
To determine whether the line is busy, the lengths of each row on hold for each type of
call are added and this sum is compared to an accepted maximum number of calls on
hold. After a series of successive simulations, we got to the customers’ shortest time of
waiting by using four operators (Fig. 1).
Fig. 1. The graphic of the call center activity performing order
The model highlights the possibility to stabilize the standard deviation for the time
that the agent gives to the one that called. The capacity of the applied scheme speciﬁes
the number of available agents.
4
Conclusions
In this paper we tried to create an overall picture of business process modeling and
simulation and the way in which they inﬂuence the activity in a company. Thus, in the
case study we developed an application that allows the customers’ management and
oﬀers support with the help of a ticketing system as the market of software developers
is fast-growing and a society that provides services of developing personalized software
must ensure permanent support and consultancy maintaining close ties to customers.
The main purpose of the application is to help the staﬀ of the customer relation depart‐
ment within the company that develops software-to-order, to answer promptly to the
customers’ requests and to keep a record of the problems they encountered during their
collaboration with the society.
In order to achieve our objectives we ﬁrst have made a modelling of the call center
activity and of the ticketing system, using the uniﬁed modelling language UML. Due to
144
G. Soava and A. Balan

the modelling process, the analyst knows the activities from the succession in which
they are performed (the process arrival and processing in the case of each activity), the
ﬁnancial resources that are necessary for doing the activities and for the logical relations
between activities. In creating the software application for this modelling we have
considered the way in which we could optimize the process of receiving the phone calls
that honor the requests. Thus, using the Extend application we have managed to deter‐
mine the optimum number of call center operators through successive simulations, that
is 4 operators, so that the waiting time will be as short as possible.
We consider that the application regarding the automatic distribution of the phone
calls to available operators can be developed by interconnecting other auxiliary tech‐
nologies to increase their eﬃciency. For example, the system of phone call automatic
distribution can integrate with desktop applications that will display the caller’s data
automatically when the call is delivered, which can lead to an increase of the agents’
productivity. Moreover, if in the future the automatic call distribution system will use
virtual queues, it will be able to oﬀer callers a telephone number which can be used to
call them back, and then it will disconnect them. Their initial position in the queue will
be maintained and when an agent gets free, the system calls the customer automatically.
If the calling systems are entirely automatized (to send notiﬁcations, with marketing
purposes or to send political messages), they place the calls that render messages for the
persons or telephone robots that answer. With such a call center system, customers can
be oﬀered real-time quality services.
The future is the virtual call center whose activity can be fast integrated in the existing
work environment, the agents can use the service anywhere (their time of work is
recorded by the software), oﬀering an eﬃcient and reliable level of services for
customers, reducing substantially the waiting time and the number of complaints.
References
1. Cognos Solutions for performance. Consultancy Services. Business process modeling (2016).
http://www.cognosconsulting.ro/servicii-cognos/sisteme-standard/business-process-
modeling/
2. The continuous optimization of the business processes. In: Market Watch Magazine, no. 96
(2007). http://www.marketwatch.ro/articol/2267/Optimizarea_continua_a_proceselor_de_
business/
3. Basu, A., Blanning, R.W.: Synthesis and decomposition of processes in organizations. Inf.
Syst. Res. 14, 337–355 (2003)
4. Dumas, M., La Rosa, M., Mendling, J., Reijers, H.A.: Fundamentals of Business Process
Management. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-33143-5
5. Tudor, C.: Integrated Information Systems for the Financial-Accounting Domain. ASE
Publishing House, Bucharest (2013)
6. Business 
Process 
Management 
(BPM). 
http://www.pnmsoft.com/resources/bpm-
tutorial/bpm/
7. Smeureanu, I., Collaborators: Establishing Demands in the Real World Context. SCIPA -
Semantic Software Services of Collaboration and Interoperability for Adaptive Business
Processes, Bucharest (2008)
Modeling and Simulating a Call Center Activity
145

8. Ward-Duttopn, N., Macehiter, N.: Business Process Management, a holistic view.
DMReview (2005)
9. ensight.ro: Business Process Management – Where do we begin from? http://www.ensight.ro/
bpm-managementul-proceselor-afaceri/
10. Daoust, N.: UML Requirements Modeling For Business Analysts. Technics Publications
LLC, New York (2012)
11. The information technology study of application integration. www.sinf.ase.ro/cursuri/
integrare/curs
146
G. Soava and A. Balan

Domain-Specific Data Analysis

Using Non-parametric Order-Alpha Hyperbolic Eﬃciency
Estimators to Assess Aspects of Melanoma
in a Romanian Hospital
Anamaria Aldea1(✉), Alexandra Limbău2, Maria Daniela Tănăsescu2, Mircea Tampa2,
and Simona Roxana Georgescu2
1 Bucharest University of Economic Studies, Bucharest, Romania
anamaria.aldea@csie.ase.ro
2 Carol Davila University of Medicine and Pharmacy, Bucharest, Romania
Abstract. Widely spread nowadays, the melanoma, a malignant tumor that
develops from melanocytes and the most aggressive skin cancer is among the
main death causes in everyday life all over the world. Our purpose is to identify
the patients with melanoma from a Romanian hospital that have the best life
prognosis given their decisions to get tested. We employ the non-parametric
robust FDH eﬃciency estimator in order to compute the eﬃciency estimates of
our sample in the input orientation. We also provide the hyperbolic measures of
eﬃciency (Wilson, 2008) that allow us to rank the patients according to their
medical information and get an insight of what is the proper time frame that a
patient must respect in order to be diagnosed. A hyperbolic α-quantile eﬃciency
estimator (Wilson, 2011) is also used to provide order-α partial frontier. Signiﬁ‐
cant conclusions for the hospital are drawn that imply the necessity of an increased
awareness of an early diagnosis among the Romanian patients, given the high
correlation found between the clinical and histopathological diagnosis.
Keywords: FDH · Order-alpha eﬃciency · Hyperbolic eﬃciency estimators
Melanoma
1
Introduction
The use of nonparametric eﬃciency techniques in the medical system is the subject of
multiple studies all over the world, from areas such as improving eﬃciency in hospital
care [13, 17] to comparisons between teaching and non-teaching hospitals [10]. The
nature of the eﬃciency analysis itself allows us to have a better understanding of the
medical databases that include either information about medical units and/or patients.
An analysis of the data provided by the personal medical information of the patients
with melanoma registered in a Romanian hospital is the subject of our paper. Not previ‐
ously perform in Romania, an analysis such as ours aims to provide a proper ranking of
the patients, ranking that expresses the best possible coordination between the time a
patient was presented in a doctor oﬃce and the life expectancy.
Our motivation is given by the low awareness regarding the perils of malignant
melanoma among the Romanian patients and we wish to ﬁnd a way to better inform
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 149–159, 2018.
https://doi.org/10.1007/978-3-319-73459-0_11

them about the chances of recovery. Depending also by the sometimes high level of
reluctance a patient has when he must present himself to a doctor oﬃce, the low aware‐
ness is a very important factor that must be addressed by doctors in Romania, as it is
seen by the doctors who co-author this paper.
Another aim of our paper is to use the data that represent the entire population of
patients diagnosed with malignant melanoma in a Romanian hospital in order to give
the doctors who made these diagnoses a tool that will help them raise their patients’
level of knowledge and understanding of these disease main triggers. Statistics provided
after the research is performed will be used by the medical doctors to hopefully educate
the potential patients and draw attention on the Romanian low level of perception
regarding the proper time to be investigated.
Malignant melanoma is one of the most aggressive skin cancers arising from the
epidermal melanocyte.
Melanoma has multifactorial etiology and its genetic and immunological back‐
ground has not yet been clearly elucidated [11]. Although most melanomas occur on
skin, they may develop at any location where melanocytes are present. The epidermis
contains two types of dendritic cells, besides keratinocytes, Langerhans’ cells and mela‐
nocytes. The most important function of these melanocytes is to produce melanin, a
pigmented natural polymer that is distributed throughout the skin to protect skin cells
from UVB radiation damage.
Melanoma is associated with a worldwide rising incidence. It is a major health
problem by sustained increases in the incidence and by a high mortality in the absence
of eﬀective treatments. Global data for melanoma is available from the World Health
Organization (WHO), which states that the incidence of melanoma has been increasing
over the past decades, with 132,000 melanomas occurring globally each year (EDF
Guidelines, 2012). According to EUCAN statistics 2012, it was estimated that in
Romania a total of 1121 new cases and 364 deaths were diagnosed with melanoma.
Romania, with a rate of 4.5 per 100,000 people, is among the countries with a low
incidence of malignant melanoma in Europe. The incidence rate in Europe is 11.1. The
European country with the highest incidence is Switzerland (25.8 cases per 100,000
people).
Deaths from malignant melanoma are also increasing, the mortality rate from malig‐
nant melanoma has risen about 2% annually. Increased incidence of melanoma is partly
due to early detection (WHO, 2012). Therefore, despite the increase in the incidence of
melanoma, the prognosis has been improving due to earlier diagnosis of melanomas in
a curable stage. Its incidence has risen sharply over past few decades, making melanoma
the sixth most common cancer [1].
Individuals with high numbers of common naevi and those with large congenital
naevi, multiple and/or atypical naevi (dysplastic naevi) are at greater risk. In individuals
with more pigmentation, melanomas are uncommon [2].
In addition to these genetic and constitutional factors, the most important exogenous
factor is exposure to UV irradiation, particularly intermittent sun exposure. There is a
complex interaction of environmental (exogenous) and endogenous factors. The role of
chronic sun exposure is controversial. Up to 65% of malignant melanomas are sun-
related (EDF Guidelines, 2012). The International Agency for Research on Cancer
150
A. Aldea et al.

(IARC) identiﬁed solar and ultraviolet radiation as a signiﬁcant environmental risk
factor for cutaneous malignant melanoma (WHO, 2012).
A number of studies indicate that the risk of malignant melanoma correlates with
personal and genetic characteristics and with the conduct of a person’s UV exposure.
Several epidemiological studies support a positive association with history of sunburn,
particularly sunburn at an early age [7].
In a prospective study with 10-year follow-up of daily application carried out, Green
et al. (2011) [9], showed that daily sunscreen use reduced signiﬁcantly the rate of inva‐
sive melanomas. Rigel et al. (2012) [15, 16] include ultraviolet radiation exposure,
family history, nevi (dysplastic, large number, or giant congenital nevi), increased age,
fair skin photo type, and occupation as the major risk factors for melanoma. A meta-
analysis of risk factors for cutaneous melanoma by Chang et al. (2009) [3] found that
the geographic distribution of malignant melanoma correlates negatively with latitude.
They stated that the exposure to solar ultraviolet rays is etiologically important.
The sex distribution of melanoma has varied by population, with high-latitude, low-
incidence. The studies of Galon [8], suggest that risk based upon sex is greater overall
in males; however, incidence is higher in women until the age of 40 and then greater in
males, with a ratio of 2:1 males: females by age 80 [6]. In Romania, malignant melanoma
is more common in males than females, with an incidence recorded among men (4.7)
than women (4.4). Nearly one-third of those who were diagnosed with melanoma die
annually (364 deaths in 2012 to 189 men and 175 women).
Malignant melanoma is a disease with an unpredictable evolution and is a potentially
lethal melanocytic neoplasm with a propensity for distant metastasis. Detected in its
early stages has a good chance of recovery, if it is treated properly. Approximately 30–
40% of the tumor metastasized to regional lymph nodes, lung, liver, intestine, bone
marrow and brain. In about 10% cases of melanoma, metastases occur in the absence of
an unknown primary tumor [12]. Excisional biopsy with safety margins according to
the thickness of the tumor should be performed. Large studies have shown that incisional
biopsies should not be performed when an excisional biopsy is technically possible.
Metastases represent the most signiﬁcant cause of death from the disease. Despite
recent advances in the treatment of metastatic melanoma the prognosis remains unfav‐
orable. The overall 5-year survival rate is less than 5%, with a median survival ranging
between 6 and 8 months [14]. Early detection and deﬁnitive therapy are necessary to
minimize the risk of metastatic disease.
While the expansion of skin screening programs may increase the detection of rela‐
tively indolent melanomas, other studies have found that the detection of more histo‐
logically aggressive melanomas may be increasing as well [16].
Given the high importance of this heath problem, we employ nonparametric tech‐
niques to better understand the data regarding patients from a Romanian hospital.
Based on a section of inputs and outputs that describe the patients’ medical data, we
use nonparametric techniques to estimate the eﬃciency of each patients based on the
idea that their medical data properly describe how well they will behave as a prognosis
compared to all the other patients in the sample. Each decision unit (patients in our paper)
will have an eﬃciency estimate that situates him on the eﬃciency frontier if the best
possible results is obtain or below the eﬃciency frontier for the rest of the population
Using Non-parametric Order-Alpha Hyperbolic Eﬃciency Estimators
151

of the sample. The distance from a point in space, given by the inputs and outputs that
describe a patient medical data to a point on the eﬃciency frontier will show the neces‐
sary increase (or decrease) in output (or input) orientation in order for the given point
in space to reach the eﬃciency frontier (and the eﬃciency estimate equal to 1).
The paper unfolds as follow: a short introduction presents the general background
of the melanoma patients, the next sections gives a short insight of the methodology we
use, the third section presents the data and the empirical analysis and the conclusions
state awareness points to be taking into account.
It is not possible to update ﬁles at a later stage. Please note that we do not need the
printed paper.
We would like to draw your attention to the fact that it is not possible to modify a
paper in any way, once it has been published. This applies to both the printed book and
the online version of the publication. Every detail, including the order of the names of
the authors, should be checked before the paper is sent to the Volume Editors.
2
Methodology
We use nonparametric techniques to report eﬃciency measures that compute the radial
distances from the sample points to the eﬃciency frontier. We also use a hyperbolic
eﬃciency mode that will no longer require an orientation so that the eﬃciency measure
will show a simultaneously adjustment in both input and output.
The technical eﬃciency methodology we use starts with a description of the produc‐
tion set of all possible combination of inputs and outputs. A probabilistic approach of
the production set is given by:
𝜓≡{(x, y), where x can produce y} ∈RN+M
+
}.
(1)
where x ∈RN
+ is the inputs vector and y ∈RM
+ is the outputs vector.
The production (eﬃciency) frontier is given by the upper boundary of P:
Ψ𝜕= {(x, y) ∈Ψ|(𝜃x, y) ∉Ψ, ∀0 < 𝜃< 1, (x, 𝜆y) ∉Ψ, ∀0 < 𝜆< 1}.
(2)
where all the points inside the frontier are considered to be technically ineﬃcient while
the decision units situated along the frontier are technically eﬃcient.
Fare (1988) [7] was the first to introduce a hyperbolic measure of the technical effi‐
ciency, which was later on developed by Wilson in 2011 [20] by proposing an alternative
estimator. The hyperbolic efficiency measure as presented in Wilson [20] is given by:
𝛾(x, y) ≡inf{𝛾> 0∕
(
𝛾x, 𝛾−1y
)
∈P}.
(3)
where this measure gives the distance from a fixed point, (x, y) to upper boundary of the
production frontier, P, along a hyperbolic path (𝛾x, 𝛾−1y), with 𝛾∈R1
++. As stated in Wilson
[20] 𝛾(x, y) ≤1, ∀(x, y) ∈P.
152
A. Aldea et al.

The distance function in (3) is deﬁned in terms of a true and unknown production
set, P, and has to be estimated using a set of observed input and outputs combination,
Sn = {xi, yi
}n
i=1.
We will further on make use of an estimator of the production set in order to get the
estimators of Shepard input-oriented distance functions as this is proposed by Deprins
et al. (1983) [5]. They proposed the free-disposal hull (FDH) of the observations in Sn
as in below:
̂PFDH
(Sn
) =
⋃
(x,,yi)∈Sn
{
(x, y) ∈Rp+q
+ ∕y ≤yi, x ≥xi
}.
(4)
The hyperbolic α-quantile distance function is deﬁned as in Wheelock and Wilson
(2008) [18],
𝛾𝛼(x, y) ≡sup{𝛾> 0∕H(𝛾x, 𝛾−1y) > 1 −𝛼}
(5)
for α ∈ (0, 1], where H(.,.) is a well-deﬁned non-standard probability function. For any
sub-unitary α and a ﬁxed point in the input/output space (x, y) ∈Rp+q
+ , the hyperbolic
α-quantile measure larger than 1 gives the simultaneous decrease in inputs and increase
in outputs necessary for a point to move to a point with a (1 − α) probability of being
dominated along a hyperbolic path given by (𝛾x, 𝛾−1y), with 𝛾> 0.
The estimator of the distance function in (4) is given by:
̂𝛾𝛼,n(x, y) ≡sup{𝛾> 0∕̂
Hn
(𝛾−1x, 𝛾y) > 1 −𝛼.
(6)
Where H(.,.) is given by Eq. (6) and I(.) is the indicator function:
̂
Hn
(
x0, y0
)
= n−1 ∑n
i=1 I
(
xi ≤x0, y ≥y0
)
.
(7)
3
Data and the Non-parametric Eﬃciency Analysis
Our database contains collected data in a Romania hospital, regarding 95 patients with
malignant melanoma, investigated over a ﬁve year period of time, during 2009 and 2014.
The data was classiﬁed by gender, age, clinical diagnosis, and histopathological diag‐
nosis, primary tumor characteristics, Clark’s level of invasion, Breslow thickness and
time of presentation. Risk factors assessed were the tendency to sunburn, increased age,
and nevi (dysplastic, large number, or giant congenital nevi). The average age at diag‐
nosis was 56. A distribution of our patients’ data is given in Fig. 1, where boxplots of
all relevant medical information is given.
As we can notice, both Breslow thickness indicator as well as the time elapsed till
the presentation to the doctor show several outliers that represents patients at risk.
We employ non parametric techniques in order to estimate the eﬃciency of our
sample by using the FDH estimator and we compute hyperbolic measures or eﬃciency.
We use both FDH input oriented measures and hyperbolic measures. Our aim is to ﬁnd
the eﬃcient units (patients) of our sample, so we can present some similarities between
Using Non-parametric Order-Alpha Hyperbolic Eﬃciency Estimators
153

the units that reacted well under the given medical condition. All the necessary programs
are performed using R 3.3.0 and the FEAR package ﬁrst implemented by Wilson in 2008
[21] and updated afterwards. In Fig. 2, we report a scatterplot of initial data representing
the inputs and outputs matrixes.
Fig. 2. Patients’ distribution according to their medical data
We performed a Factor Analysis of initial sample in order to ﬁnd the potential
outliers, but considering that relevance of our sample, we decided to include all the
outliers in the analysis so we could have a better understanding of their features
comparing to the ones of all the others patients. We use the input oriented model with
one input and one output given by the time until the patient went to the doctor as input
(named history) and an indicator of our one deﬁnition that measures the thickness of
invasion (named thickness). The thickness indicator is given by the inverse measure of
the Breslow score that is expected to have larger values for a better prognosis. Our target
is to measure the units’ eﬃciency regarding the time until medical diagnosis so the
Fig. 1. Boxplots of patients’ medical data
154
A. Aldea et al.

shorter the time the patients come to a doctor oﬃce, better the thickness indicator will
be and thus his prognostic. We name our input oriented model the time eﬃciency model
(model 1). The model that gives us our patients ranking based on their medical indicators
uses two inputs given by the time until patients went to the doctor (named history – input
1) and sun exposure (named exposure- input 2) and one output given by the above
thickness indicator (named thickness). We aim to measure the units’ eﬃciency regarding
the patient prognostic based on the time until medical diagnosis and sun exposure. The
eﬃciency units with an eﬃciency measure equal to 1 we have the best prognostic given
by the low values of both inputs. We name this the prognostic model (model 2).
3.1
Model 1 – The Time Eﬃciency Model
By applying ﬁrst model we report a total of 5 patients (5% of the sample) with a score
equal to 1 with certain common features. We notice a female predominance with early
age, clinical diagnosis represent by malignant melanoma, a very short time interval until
the presentation in our clinic and Breslow thickness less than 1 mm. We present the
patients with the eﬃciency score equal to 1 in the following Table 1.
Table 1. Patients with unitary eﬃciency estimates
ID
Sex
Age
Clinic
Placement
HP
type
Clark
Breslow
Time
(years)
Residence
Time
exposure
(years)
78 M
38
Nervus
Ant. chest
Nod
4
5.85
0.25
B
5
83 F
28
Mm
Breast left
Ext
2
0.5
0.25
CL
5
7 F
34
Nervus
Chest post
Ext
2
0.15
0.6
B
10
57 F
47
mm
Zyg. left
Ext
3
0.46
0.25
B
10
We ﬁnd only one patient with an eﬃciency score larger than 0.75 with Clark’s level
of invasion and Breslow thickness with low values and a short time interval until the
presentation in our clinic. The last 10 patients present the following characteristics:
substantially more common in females, Clark’s level of invasion is elevated, period of
time until medical evaluation and sun exposure very long and urban environment. Our
sample has a 26% eﬃciency mean and we ﬁnd out that 63 from 96 (65.6%) have an
eﬃciency measure below the sample mean. The low eﬃciency mean can be explained
by the log period of time elapsed until medical evaluation.
3.2
Model 2 – The Prognostic Model
In order to get a better look at our sample, we computed an order-alpha FDH hyperbolic
measure. Previously, a space reduction was performed using Daraio and Simar (2007)
[4] and an aggregated input was computed. A scatterplot of the aggregated input and
standardized output together with the Clark index is providing in Fig. 3 and one can note
how clusters of patients are formed based on the values of Clark index.
Using Non-parametric Order-Alpha Hyperbolic Eﬃciency Estimators
155

Fig. 3. Patients’ distribution according to aggregated input and output and Clark index
A FDH order-alpha is computed for a 95% signiﬁcance level and the eﬃciency esti‐
mates are plotted together with the initial FDH estimates in Fig. 4.
Fig. 4. Patients’ distribution according to aggregated input and output and the hyperbolic order-
alpha FDH
One may note that the sample mean increased by computing the order-alpha hyper‐
bolic eﬃciency estimates from 0.26 to 0.39. Only 3 patients with a low Clark index are
situated on the eﬃciency frontier while 4 others, 2 males and 2 females with certain
common features: young, urban environment, melanoma developed in the less exposed
areas, a Clark index of 4 and 5 are among the least eﬃcient estimates and should have
decreased their sun exposure time together with the time till the medical check-up so
they should have had a better prognosis. The eﬃciency frontiers for the both FDH and
hyperbolic order-alpha FDH are presented in Fig. 5, where the dotted lower line repre‐
sents the 95% hyperbolic FDH frontier.
156
A. Aldea et al.

Fig. 5. Full and hyperbolic order-alpha eﬃciency frontier
In our top 10 to 20 places we ﬁnd a signiﬁcant increasing eﬃciency by 22% which
show us that the hyperbolic estimator gives us a better ranking and we can empathize
their common characteristics, such as: easier clinical diagnosis (malignant melanoma),
a short time interval until doctors’ check-ups, Clark’s level of invasion and Breslow
thickness with high values, urban environment and period of sun exposure long. A closer
look to our patients with lower ranks shows us a very small increase eﬃciency which
means that the patients with low eﬃciency scores are basically the same and we just can
rank them without ﬁnding a way to signiﬁcantly improve their eﬃciency estimates. All
these patients have elevated Clark and Breslow indexes, melanomas in the most exposed
areas and the time until medical evaluation and sun exposure are too long.
4
Conclusions
Eﬃciency estimates could have increased by 45% if patients have been submitted earlier
to the doctor, considering our two inputs. Patients in this group are young, from urban
environment, with histopathological appearance, and a short period of time till the diag‐
nosis. Only 25% of the patients have eﬃciency estimates higher than the sample mean.
We report a total of 12 patients with very low eﬃciency values, most of them are females,
above 65 years old, with both Clark’s level of invasion and Breslow thickness elevated
and a prolonged sun exposure.
Since the early 90s, it has been recognized that malignant melanomas are heteroge‐
neous, and that diﬀerent clinical appearances are also associated with diﬀerent distri‐
butions by site and age and have diﬀerent results [20]. Analyzing the results of our study
we can draw several conclusions. In our population-based study, gender independently
aﬀected melanoma in all progression phases. These results suggest a biological
Using Non-parametric Order-Alpha Hyperbolic Eﬃciency Estimators
157

diﬀerence across gender in the disease–host interaction. Even if melanoma incidence
rates vary with gender and age, the age at diagnosis was not inﬂuenced by the gender.
The Breslow thickness however was inﬂuenced by the gender of the patients. In males,
Breslow thickness was statistically signiﬁcantly higher than in females. Men presented
a higher proportion of thicker Breslow (>1 mm) which was statistically signiﬁcant. The
age was also correlated to the Breslow thickness, our study showing a higher Breslow
thickness with the increase in age. Our results indeed indicate that Breslow thickness
and body site considerably inﬂuenced the gender eﬀect. These results underline that both
the tumor thickness and the level of invasion are important independent prognostic
factors.
According to the medical literature [6], melanomas are generally located in women
on the extremities, especially the lower limbs. Our study shows that the high eﬃciency
estimates may be found and the women are more likely to self-detect their melanomas
compared with men, they are young, the anatomical location of the melanoma is given
by areas less exposed to the sun (posterior and anterior chest), the age was correlated to
the Breslow thickness and the higher the Breslow thickness, the worse the prognosis.
Our patients have Breslow thickness less than 1 mm, the early detection greatly improves
the prognosis of patients with malignant melanoma which is also correlated to the urban
residence of our patients. Accurate clinical and histological diagnosis of malignant
melanoma is an element of great importance to the early detection and further treatment.
In our department (Victor Babes Clinical Hospital, Department of Dermatology) the
correlation between the clinical diagnosis and the histopathological diagnosis was
73.7%.
As a ﬁnal conclusion, one can easily see that these techniques allow a medical doctor
to get a better insight on patient medical information, and to improve the awareness of
an early date visit to a medical center for all the potential patients. A closer insight to
our patient’s data set will be further possible by applying conditional nonparametric
eﬃciency estimates. Despite potentially exciting developments in the treatment of
advanced malignant melanoma, prevention and early detection remain the primary goals
in the war against this type of cancer.
Acknowledgment. Financial support from the Romanian National Authority for Scientiﬁc
Research, CNCS-UEFISCDI, Project PN-II-ID-PCE-2011-3-0893 is gratefully acknowledged.
References
1. Bataille, V., de Vries, E.: Melanoma – Part 1: epidemiology, risk factors, and prevention.
BMJ 337, 22–49 (2008)
2. Bataille, V., et al.: Risk of cutaneous melanoma in relation to the numbers, types and sites of
Naevi: a case control study. Br. J. Cancer 73, 1605–1611 (1996)
3. Chang, Y.M., Barrett, J.H., Bishop, D.T., et al.: Sun exposure and melanoma risk at diﬀerent
latitudes: a pooled analysis of 5700 cases and 7216 controls. Int. J. Epidemiol. 38, 814–830
(2009)
158
A. Aldea et al.

4. Daraio, C., Simar, L.: Advanced and Robust Nonparametric Methods in Eﬃciency Analysis:
Methodology 
and 
Applications. 
Springer, 
New 
York 
(2007). 
https://doi.org/
10.1007/978-0-387-35231-2
5. Deprins, D., Simar, L., Tulkens, H.: Measuring labor ineﬃciency in post oﬃces. In: Pestieau,
M.M.P., Tulkens, H. (eds.) The Performance of the Public Enterprises: Concepts and
Measurements, Amsterdam, North Holland, pp. 243–267 (1984)
6. Erdei, E., Torres, S.M.: A new understanding in the epidemiology of melanoma. Expert Rev.
Anticancer Ther. 10(11), 1811–1823 (2010)
7. Fare, R.: Fundamentals of Production Theory. Springer, Heidelberg (1988). https://doi.org/
10.1007/978-3-642-51722-8
8. Galon, J., Pagès, F.: The immune score as a new possible approach for the classiﬁcation of
cancer. J. Transl. Med. 10, 1 (2012)
9. Green, A.C., Williams, G.M., Logan, V., et al.: Reduced melanoma after regular sunscreen
use: randomized trial follow-up. J. Clin. Oncol. 29(3), 257–263 (2011)
10. Grosskopf, S., Margaritis, D., Valdmanis, V.: Comparing teaching and non-teaching
hospitals: a frontier approach (teaching vs. non-teaching hospitals). Health Care Manage. Sci.
4, 83–90 (2001)
11. Koh, H.K., Sinks, T.H., et al.: Etiology of melanoma. Cancer Treat. Res. 65, 1–28 (1993)
12. Melanoma Research Foundation (MRF) (2016). http://www.melanoma.org/
13. Mutther, R.L., Rosko, M.D., Greene, W.H., Wilson, P.W.: Translating frontiers into practice:
taking the next steps towards improving hospital eﬃciency. Med. Care Res. Rev. 68, 3S–19S
(2011)
14. Neuman, H.B., Patel, A., Ishill, N., et al.: A single-institution validation of the AJCC staging
system for stage IV melanoma. Ann. Surg. Oncol. 15, 2034–2041 (2008)
15. Rigel, D.S., Carucci, J.A.: Malignant melanoma: prevention, early detection, and treatment
in the 21st century. CA Cancer J. Clin. 50, 215–236 (2013)
16. Rigel, D.S., Russak, J., Friedman, R.: The evolution of melanoma diagnosis: 25 years beyond
the ABCDs. CA Cancer J. Clin. 60, 301–316 (2012)
17. Wagstaﬀ, A.: Estimating eﬃciency in the hospital sector: a comparison of three statistical
cost frontier models. Appl. Econ. 21, 659–672 (1989)
18. Wheelock, D., Wilson, P.: Non-parametric, unconditional quantile estimation for eﬃciency
analysis with an application to federal reserve check processing operations. J. Econ. 145, 209–
225 (2008)
19. Whiteman, D.C., Pavan, W.J., Bastian, B.C.: The melanomas: a synthesis of epidemiological,
clinical, histopathological, genetic, and biological aspects, supporting distinct subtypes,
causal pathways, and cells of origin. Pigm. Cell Melanoma Res. 24(5), 879–897 (2011)
20. Wilson, P.W.: Asymptotic properties of some non-parametric hyperbolic eﬃciency
estimators. In: van Keilegom, I., Wilson, P.W. (eds.) Exploring Research Frontiers in
Contemporary Statistics and Econometrics, pp. 115–150. Springer, Heidelberg (2011).
https://doi.org/10.1007/978-3-7908-2349-3_6
21. Wilson, P.W.: FEAR: a software package for frontier eﬃciency analysis with R. Socio-Econ.
Plan. Sci. 42, 247–254 (2008)
Using Non-parametric Order-Alpha Hyperbolic Eﬃciency Estimators
159

Forecasting Solutions for Photovoltaic Power Plants
in Romania
Simona-Vasilica Oprea1(✉), Alexandru Pîrjan2, Ion Lungu1,
and Anca-Georgiana Fodor3
1 The Bucharest University of Economic Studies, 6 Piata Romana, 1st District,
010374 Bucharest, Romania
simona.oprea@csie.ase.ro, ion.lungu@ie.ase.ro
2 The Romanian-American University, 1B Expozitiei Blvd., 1st District,
012101 Bucharest, Romania
alex@pirjan.com
3 The Romanian Commercial Bank, 5 Regina Elisabeta Blvd., 3rd District,
030016 Bucharest, Romania
anca.fodor@yahoo.com
Abstract. This paper presents an extended version of the article “Forecasting
solutions for photovoltaic power plants in Romania” that has been accepted and
successfully defended at The 15th International Conference on Informatics in
Economy (IE 2016). The installing of photovoltaic power plants (PVPPs) is
increasing rapidly in Romania. The national power grid company’s estimation
regarding installed power of PVPPs shows that it will increase in the coming three
years by almost 35%. This paper develops several solutions for PVPP generation
forecast. Based on the yearly data set of two PVPPs located in Romania, ﬁrst we
have developed neural networks for predicting, analyzing and monitoring the
performance indicators for PVPP generation in Romania by applying three algo‐
rithms: Levenberg-Marquardt, the Bayesian Regularization and the Scaled
Conjugate Gradient algorithms. Secondly, for forecasting of PVPP generation
purposes, we have applied stochastic methods such as autoregressive integrated
moving average and data mining models: ensemble, neural network, decision tree,
regression models. In order to forecast the output of photovoltaic power plants,
the proposed solutions, namely stochastic and neural networks methods, are
analyzed in terms of accuracy. The generation forecast of renewable energy
sources is of high importance for the owners or administers of such power plants
who are interested in minimizing the balancing costs that are directly inﬂuenced
by forecasting errors. Our analyses on actual operating data are also focused on
ﬁnding the similarities and diﬀerences between PVPPs that operate in the same
geographical area and extending our results to other PVPPs with similar capacity/
performance and location.
Keywords: Neural networks · PPPV · Renewable energy · Stochastic methods
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 160–174, 2018.
https://doi.org/10.1007/978-3-319-73459-0_12

1
Introduction
Among the multiple existing renewable energy resources, most are related in one way
or another, indirectly or directly to the solar radiation. Thus, the unevenly warming of
the Earth’s surface determines the movement of air masses through which the wind
energy is obtained; the hydroelectric power is related to the natural water cycle that is
unquestionably determined by the sun heat. The biomass energy is also related to the
sunlight, through the photosynthesis process occurring in plants and other organisms.
On the other hand, the solar energy can be captured directly using solar collectors or
panels. There are some renewable energies that do not depend on the solar radiation,
such as the geothermal or the tidal energies [1].
In every moment, the Earth receives a tremendous amount of solar energy. Thus, in
every minute, the sun covers the energy demand of the world for an entire year, while
during a single day, it provides more energy than the global consumption requirement
for 27 years. Furthermore, the amount of solar radiation that reaches Earth over a period
of three days is equivalent to the energy stored in all known fossil energy resources today
[1]. The sun represents a free energy resource for humanity, practically inexhaustible,
that can be harvested using two methods: thermal and photovoltaic.
The thermal method of using the solar energy involves capturing this energy and
converting it into heat which is being used afterwards in a steam generator or engine
that converts it into electricity. Harvesting the solar energy through the photovoltaic
method generates electricity directly; the photovoltaic system consists of silicon cells,
which are the smallest units of the system. They are built up of diﬀerent materials and
may have slightly diﬀerent performance in terms of generation, maintenance and life‐
span. Several cells linked to each other form a module, several modules connected to
each other form a panel, a group of panels is called an array, while several arrays form
an array ﬁeld [2].
Worldwide, the renewable resources management in the power plants must be
sustained by a decision support system (DSS) that facilitates the eﬃcient monitoring
and analysis of the produced energy. While in many European countries, such systems
have already been developed to some extent, in Romania the renewable resource
management is not entirely sustained by decision support systems and the developing
costs of such systems are high [3].
Most of the photovoltaic power-plants installed in Romania are nearby located on
terrain surfaces that pose similar characteristics regarding the photovoltaic potential of
producing energy. Consequently, in this article, we aim to develop solutions that target
the forecasting of photovoltaic energy production of these types of photovoltaic solar
power plants.
Our goal is to develop solutions that enhance the forecasting accuracy provided by
the currently used software of two photovoltaic power plants located in Romania: the
PVPP Corabia, situated in the Olt County, at 5.5 km from the Danube and the PVPP
Izvoru that is located in the Giurgiu County.
At the order of the beneﬁciaries, we aim to propose a software solution that replaces
the existing ones of the beneﬁciaries, being far superior in terms of prediction accuracy,
taking into account the speciﬁc Romanian weather conditions and geographic
Forecasting Solutions for Photovoltaic Power Plants in Romania
161

environment. The potential investors in green solar energy can also beneﬁt from the
developed solution by assessing the production potential of a particular geographical
area from Romania before building a PVPP.
In order to develop a DSS for the analysis, prediction and monitoring of the tech‐
nological and business processes in the ﬁeld of renewable energy in Romania, one can
use diﬀerent approaches.
In recent years, artiﬁcial neural networks have gained an increased popularity in
forecasting diﬀerent parameters regarding the energy production. Thus, in [4, 5] are
presented examples of solar power forecasting using neural networks. In [6] is analyzed
a systematic literature review regarding the use of artiﬁcial neural networks for solar
radiation prediction, while [7] achieves the forecasting of solar radiation for local power
reserve by using artiﬁcial neural networks.
Taking into account their undeniable advantages, in this paper we develop two
approaches: a series of neural networks and a study using stochastic and data mining
models, useful in designing a decision support system [3] for predicting, analyzing and
monitoring the performance indicators in the ﬁeld of renewable energies in Romania [8].
We have used three algorithms for developing the neural networks for predicting,
analyzing and monitoring the performance indicators in the ﬁeld of renewable energies
in Romania: the Levenberg-Marquardt (LM), the Bayesian Regularization (BR) and the
Scaled Conjugate Gradient (SCG) algorithms [9, 10]. Based on these algorithms, we
have developed, trained, validated and tested several neural networks, using the Neural
Network Toolbox from the development environment Matlab R2015a.
Thus, we have ﬁrst obtained a solution that forecasts the quantity of generated energy
(in kW), when knowing humidity (in percentages), atmospheric pressure (in hPa), solar
irradiation level (in W/m2), environment temperature (in Celsius degrees) and module
temperature (in Celsius degrees).
Also ARIMA models have been applied in several scientiﬁc papers. In [11, 12],
ARIMA models were suitable for electricity consumption estimations. In [11] the
authors proposed six forecasting models developed for electricity consumption in New
Zealand. One of the six models applies ARIMA modelling technique. In [12] the authors
found a model to forecast the electricity consumption in a household and the most suit‐
able forecasting period. It resulted that ARIMA model is suitable for monthly and quar‐
terly forecasting periods, while ARMA model is suitable for daily and weekly fore‐
casting periods. In [13] the authors studied solar generation forecasting in a laboratory-
level micro-grid, ﬁnding that accuracy of ARMA model is better than the accuracy of
persistence model.
Using a second approach, several diﬀerent forecasting scenarios have been designed
using autoregressive integrated moving average (ARIMA) modelling and forecasting
tool that is one of the analyzing options applied for time series data. ARIMA models
contain three parts: autoregressive part (AR), diﬀerencing lags (I) and moving average
(MA). On one hand, AR model describes phenomena that are regular in time such as
solar radiation, electricity consumption, agricultural processes, seasonal diseases etc.
On the other hand, MA model describes irregularities from phenomena such as wind
speed, ﬂoods, ﬁnancial markets, etc.
162
S.-V. Oprea et al.

We have gathered the actual operation data corresponding to the input and output
parameters (generated power, meteorological data such as humidity, atmospheric pres‐
sure, solar irradiation level, environment temperature, module temperature) for two
PVPPs (Corabia and Izvoru). We analyzed the 7122 samples, gathered through hourly
measurements conducted over a period of one year (from January to December 2014),
at the PVPP Corabia, located in Olt County, at 5.5 km from the Danube, in Romania.
The PVPP Corabia has an installed capacity of 7 MW, resulting from a number of 28602
panels, with a panel capacity of 245 Wp, using a T-Connection to 20 kV distribution
electricity line.
We have also analyzed the operation data for PVPP Izvoru that is located in the south
part of Romania, in Giurgiu County. For this PVPP, we have gathered the actual oper‐
ation data corresponding to the input and output parameters, resulting in a total number
of 16926 samples, through hourly measurements conducted over a two-year period of
(from January 2013 to December 2014). The PVPP Izvoru has an installed capacity of
9.6 MW, resulting from a number of 40026 panels, each of them having a capacity of
240 Wp and being connected to an existing 110/20 kV substation.
In the following, we analyze the main results that we have obtained when developing,
validating and testing the neural networks for predicting, analyzing and monitoring the
performance indicators in the ﬁeld of renewable energies in Romania.
2
Neural Networks for Predicting, Analyzing and Monitoring
the Performance Indicators in the Field of Photovoltaic Power
Plants in Romania
The research that we have conducted and presented in this paper is part of the research
project “Intelligent system for predicting, analyzing and monitoring performance indi‐
cators and business processes in the ﬁeld of renewable energies (SIPAMER)”, in which
we are designing, developing and implementing a computer based information system
useful for the Romanian green energy market. The data is gathered from the renewable
power plants operators’ devices from Romania and it is stored in a cloud computing
database. Based on this cloud database storage, we are developing speciﬁc forecasting
and analytical modules, thus oﬀering strategic business decision support.
The SIPAMER intelligent system is developed using Java technology along with
business intelligence modules in order to obtain a comprehensive analysis and a detailed
report activity. Our forecasting module has as a main purpose the minimizing of the
forecast error and stores the obtained forecast into the analytical module to give assis‐
tance to the trading activity that takes place on the demanding energy market.
The developed solution was tested and validated, the forecasting solution consisting
in custom trained Matlab Neural Networks was integrated into the SIPAMER by using
the Matlab Compiler SDK in order to obtain callable functions that we were able to
compile as a Java package, thus incorporating the whole functionality into the SIPAMER
intelligent system. The system is accessible using a Java Application Programming
Interface (API) as classes and interfaces have been developed especially for the modules
responsible with business analytics and reporting. The beneﬁciaries can use intuitive
Forecasting Solutions for Photovoltaic Power Plants in Romania
163

web page interfaces and can also export their strategic information in a variety of report
formats.
In order to develop the proposed solution, we have identiﬁed the input parameters
(humidity, atmospheric pressure, solar irradiation level, environment temperature,
module temperature), along with the corresponding output (the quantity of delivered
energy).
The feedforward architecture and the specialized versions of feedforward Neural
Networks that could be developed in Matlab R2015a (ﬁtnet) were the most suitable
architectures for prediction in this particular case, taking into account two perspectives:
the improved forecasting accuracy when compared to the existing forecasting accuracy
of the beneﬁciary’s existing prediction software and most of all due to the reduced time
necessary to retrain these networks.
The reduced time for retraining is very important when, at the end of each season,
the beneﬁciary needs to adjust the prediction solution by taking into account the real
output of produced energy and the forecasted one, thus applying error correction factors
to the prediction solution with minimum downtime and low eﬀort. The feed-forward
architecture is also suitable for the already available compiling options oﬀered by the
Matlab Compiler SDK, oﬀering the possibility to implement the trained networks in the
SIPAMER prediction module without having to pay additional royalties in licensing
taxes.
After having tested a series of settings, we have decided to develop the neural
networks using the architecture that has proven to oﬀer the best prediction accuracy (for
both the Corabia and Izvoru data sets): 5 neurons for the Input data, 12 in the Hidden
layer, 1 in the Output layer and 1 for the Output data (Fig. 1).
Fig. 1. The architecture of the developed neural networks
The algorithms used for developing the prediction solutions and their mathematical
backgrounds have been studied extensively in the scientiﬁc literature: the Levenberg-
Marquardt algorithm is presented and studied extensively in [14–16]; the Bayesian
Regularization Algorithm is analyzed thoroughly in [16–18]; the Scaled Conjugate
Gradient Algorithm is analyzed comprehensively in [19–22].
We have compared and synthetized the main aspects related to the above mentioned
three algorithms in Table 1.
164
S.-V. Oprea et al.

Table 1. The main aspects related to the algorithms that were used in developing the neural
networks
The algorithm
The minimized objective
function
The implemented
methods
The performed steps
LM
The sum of squared errors The gradient descent
method and the Gauss-
Newton method
Training, validation,
testing
BR
A linear combination of
squared weights and
squared errors
The LM algorithm and
the back-propagation
Training, testing
SCG
The sum of squared errors The conjugate gradient
approach and the LM’s
approach of model-trust
region
Training, validation,
testing
In view of the undisputable advantages that these algorithms oﬀer, we have chosen
to conduct our research by incorporating them when developing the forecasting solu‐
tions. Of particular interest was to experiment to what degree a series of artiﬁcial neural
networks, developed based on these algorithms, can predict accurately important
production parameters that aﬀect the output of electricity produced from solar renewable
energy.
The data set was divided in order to train, validate and test the developed neural
networks, using the amount of percentages that has proved to oﬀer the best performance
regarding the prediction accuracy. Thus, for all the networks, regardless of the algo‐
rithms used for developing them, 70% of the data set was assigned for the training process
and 15% for the testing one. The remaining percentage of 15% of the data set was
assigned for the validation process of the neural networks developed using the LM and
the SCG algorithms, while in the case of the BR algorithm, this step does not occur.
Therefore, in this case, the remaining percentage has not been allocated, because we
have decided to have the same amount of data used in developing, training, validating
and testing the developed neural networks, in order to obtain a relevant comparison
between the ﬁnal results obtained using the developed networks. In addition, we have
decided to randomly choose the samples used in the above mentioned phases.
We have developed three neural networks for the annual data, for each of the
analyzed PVPPs and for each of the used algorithms: LM, BR and SCG. Thus, we have
developed, trained and tested 6 neural networks for prediction and we have validated 4
of them (as for the networks trained using the BR algorithm this step does not occur).
The networks have been named as to reﬂect both the location of the PVPP and the
algorithm used for developing it.
In order to analyze the performance and the prediction accuracy of the developed
neural networks, we have generated the plots representing the performance analysis
(highlighting the minimum value of the mean squared error MSE), the errors histograms
(highlighting the interval in which the most of the errors fall) and the regressions (high‐
lighting the minimum value of the correlation coeﬃcient R). We have also computed
the value of the Mean Absolute Percentage Error (MAPE) in order to provide the
Forecasting Solutions for Photovoltaic Power Plants in Romania
165

accuracy of the models. For example, when analyzing the forecasting results oﬀered by
the neural network developed in the case of the Corabia power plant, using the BR
algorithm, we have noticed that the minimum value of MSE is 0.024, the minimum value
of R is 0.934, most of the errors fall between −329 and 563.8, while the value of the
MAPE is 5.4916 (Fig. 2).
Fig. 2. The forecasting results oﬀered by the CorabiaBROutput neural network
The forecasting performance provided by all the 6 neural networks is synthesized in
Table 2.
Table 2. A comparison analysis of the results provided by the 6 developed neural networks
PVPP-neural network
MSE
R
Errors interval
MAPE
Corabia-LM
0.025
0.933
[−492.1, 433.6]
6.8181
Corabia-BR
0.024
0.934
[−329, 563.8]
5.4916
Corabia-SCG
0.033
0.927
[−672.8, 631.5]
9.7352
Izvoru-LM
0.034
0.936
[−597.2, 859.5]
5.2741
Izvoru-BR
0.035
0.935
[−493.7, 944.1]
7.4281
Izvoru-SCG
0.043
0.924
[−754.8, 702.3]
8.5498
Comparing the above-mentioned parameters registered by each of the neural
networks developed based on each of the 3 algorithms, we have noticed that the accuracy
of the forecast provided in all the cases is very good and the results are similar. However,
in the case of the Corabia power plant, the neural network based on the BR algorithm
166
S.-V. Oprea et al.

oﬀers an improved accuracy of the forecasting results than the ones developed using the
LM and SCG algorithms.
Thus, in the case of the CorabiaBROutput neural network, we have registered the
lowest value of the MSE, the highest value for the correlation coeﬃcient R, the narrowest
range of errors from all the three neural networks developed for the Corabia power plant.
When we have analyzed the values of the Mean Absolute Percentage Error (MAPE) in
order to compare the ﬁts of the three forecasting methods used for the Corabia power
plant, we have noticed that the best results are oﬀered by the neural network based on
the LM algorithm, followed by the SCG and the BR algorithms.
In the case of the Izvoru power plant, we have noticed that the neural network devel‐
oped based on the LM algorithm oﬀers the best forecasting results. Thus, in the case of
the IzvoruLMOutput neural network, we have registered the lowest MSE value, the
highest value for the correlation coeﬃcient R, the narrowest range of errors, the lowest
value of the Mean Absolute Percentage Error from all the three neural networks devel‐
oped for the Izvoru power plant, followed by the networks developed by the BR and
SCG algorithms.
The obtained results corresponding to the three types of neural networks for each of
the PVPPs conﬁrm the fact that the best prediction accuracy is obtained using a neural
network trained based on a diﬀerent type of algorithm in each of the cases. This happens
because the number of valid samples that we could use in the training process of the
networks diﬀers signiﬁcantly.
In the case of the Corabia PVPP, we could gather from the beneﬁciary 7122 valid
samples through hourly measurements conducted over a period of one year while for
the Izvoru PVPP, we have managed to gather a number of 16926 valid samples corre‐
sponding to hourly measurements conducted over a two-year period.
After testing the solution, we have integrated the Matlab neural networks into our
project by generating callable functions that incorporate the neural networks function‐
ality that were further compiled as:
• a C and C++ Shared Library
• a Java package
• a .NET assembly
• a reusable Component Object Model software component.
This facilitates the developing of high-accuracy prediction modules that can be used by
programmers who need to develop software applications for forecasting critical param‐
eters in the ﬁeld of renewable energies in Romania. A major advantage of this approach
consists in the licensing fees as neither the developer nor the beneﬁciary have to buy a
Matlab license. Another advantage of our solution consists in the ease of integration
with software that is already familiar to the beneﬁciary (for example, the Component
Object Model can be successfully implemented in Microsoft Excel). Thus, the system
can be accessed using diﬀerent Application Programming Interfaces, classes and inter‐
faces being built for the analytics and reporting modules. The whole process is depicted
in Fig. 3.
Forecasting Solutions for Photovoltaic Power Plants in Romania
167

Fig. 3. Developing a wide range of software applications
In the following, we analyze the main results that we have obtained when using
stochastic methods for predicting the generated energy of the two PVPPs.
3
Stochastic and Data Mining Methods for Predicting, Analyzing
and Monitoring the Performance Indicators in the Field
of Photovoltaic Power Plants in Romania
In this section, ﬁnding the similarities and diﬀerences between PVPPs that operate in
the same geographical area and extending our results to other PVPPs with similar
capacity/performance and location is one of the goals. The correlation coeﬃcient calcu‐
lated between energy output of PVPP Izvoru and solar irradiation is very strong (0.9154).
The similar correlation coeﬃcient (0.9215) is calculated for PVPP Corabia, its output
being strongly related to the generated power.
Since the two PVPPs are closely located, their output shows a positive association
and their operation is highly correlated (Pearson coeﬃcient = 0.9105). Therefore, to
some extent the forecasting results for one of the PVPPs could be processed in order to
obtain a good forecast for the other PVPP.
Table 3 shows a statistical analysis for the two PVPPs.
168
S.-V. Oprea et al.

Table 3. Data descriptive statistics
The statistical indicator
The PVPP
PVPP Corabia PVPP Izvoru
Mean
973.3267341
1331.192958
Standard error
19.50021758
17.22509767
Median
3
0
Mode
0
0
Standard deviation
1645.661246
2240.984545
Sample variance
2708200.936
5022011.729
Kurtosis
1.375387677
1.374346279
Skewness
1.654666835
1.651765572
Range
6591
8233
Minimum
0
0
Maximum
6591
8233
Sum
6932033
22531772
Count
7122
16926
Largest(1)
6591
8233
Smallest(1)
0
0
Conﬁdence level (95.0%)
38.22622043
33.7629846
The maximum generated power for each PVPPs is lower than the installed power
due to the inherent internal grid losses, panels’ exposure to the sun and diﬀerent mete‐
orological conditions from the ideal ones. The total output of the two PVPPs is signiﬁ‐
cantly diﬀerent and could not be compared due to the fact that the number of records is
diﬀerent as a consequence of various sets of available data.
In this section, we also analyze the main results that we have obtained using
stochastic and data mining methods. On both data sets for PVPP Corabia and Izvoru,
we applied four ARIMA models: ARIMA(1,0,0), ARIMA(1,1,0), ARIMA(1,1,1) and
ARIMA(0,0,1).
For PVPP Corabia and Izvoru, we have normalized the data set, performed data
mining analysis and obtained the following results. By plotting the partial autocorrela‐
tive functions (PACF) we determined the proper lag (equal to 1).
From the errors histogram, we notice very narrow errors intervals for ARIMA(1,0,0),
ARIMA(1,1,0) and ARIMA(1,1,1). Therefore, they are indicated as forecasting methods
from this point of view. The distribution of residuals corresponding to the four ARIMA
models (ARIMA(1,0,0), ARIMA(1,1,0), ARIMA(1,1,1) and ARIMA(0,0,1)) is repre‐
sented in Fig. 4.
Forecasting Solutions for Photovoltaic Power Plants in Romania
169

Fig. 4. Errors histograms
In Table 4, MSE and MAPE for PVPP Corabia and Izvoru are presented.
Table 4. MAPE and MSE for ARIMA models for both PVPPs
Indicator
ARIMA(1,0,0)
ARIMA(1,1,0)
ARIMA(1,1,1)
ARIMA(0,0,1)
MAPECorabia
14,078
11,554
23,761
47,629
MAPEIzvoru
9,473
9,101
20,594
52,978
MSECorabia
0.08
0.06
0.13
0.19
MSEIzvoru
0.06
0.05
0.11
0.17
The MAPE and MSE indicators show that ARIMA(1,0,0) and ARIMA(1,1,0)
models are more suitable for our data set. In case of ARIMA(1,1,1) the accuracy of the
model is lower so that it is not recommended for this data set, but it can be further applied
and checked for diﬀerent data sets. Since the stochastic methods lead to similar results
for both PVPPs, we can easily extend the forecast for one PVPP to the other PVPP
mainly due to the fact that they are in the same geographical area. The selected variable
importance is: solar radiation (85%), humidity (10%), temperature (4%), and pressure
(1%).
For the PVPP Corabia, we have also applied several data mining models in SAS
Miner. In Fig. 5 and Table 5, we have highlighted the training performance.
170
S.-V. Oprea et al.

Fig. 5. Training performance
Table 5. Model ﬁt statistics
Statistic
Train
Validation
Average squared error
0.0105
0.0103
Max absolute error
0.7408
0.7381
Root average square error
0.1023
0.1014
Figure 5 shows that the learning process is present mainly at the ﬁrst iterations.
Table 5 presents some statistics of the model such as average squared error, max absolute
error and root average square error.
In Table 6, we presented the ﬁgures for data mining models from the train and valid
averaged square error point of view.
Table 6. Model selection based on valid: average squared error
Model description
Train averaged square error
Valid average squared error
Ensemble champion
0.010473
0.010283
Neural networks
0.009925
0.010389
Decision tree
0.011498
0.010860
Regression
0.011500
0.011280
The best results have been obtained using the ensemble champion and neural
networks models.
4
Conclusions
The main contribution that we bring to the state of art is represented by the enhanced
forecasting accuracy of our solution, developed for two photovoltaic power plants
Forecasting Solutions for Photovoltaic Power Plants in Romania
171

installed in Romania, when compared with the current forecasting accuracy of the
beneﬁciary’s installed software. The development of the prediction solutions for the
Corabia and Izvoru PVPPs was accomplished taking into account at every step of the
way the beneﬁciaries’ feedback, as to obtain a speciﬁcally designed forecasting solution
that scales very well to the Romanian photovoltaic energy market.
In the case of PVPP Corabia, the neural network based on the BR algorithm oﬀers
an improved accuracy of the forecasting results than the ones developed using the LM
and SCG algorithms. Thus, in the case of the Corabia-BR neural network, we have
registered the lowest value of the MSE, the highest value for the correlation coeﬃcient
R, the narrowest range of errors from all the three neural networks developed for PVPP
Corabia. When we have analyzed the values of MAPE in order to compare the ﬁts of
the three forecasting methods used for PVPP Corabia, we have noticed that the best
results are also obtained by the neural network based on the BR algorithm, followed by
the LM and the SCG algorithms.
In the case of the PVPP Izvoru, we have noticed that the neural network developed
based on the LM algorithm oﬀers the best forecasting results. Thus, in the case of the
Izvoru-LM neural network, we have registered the lowest MSE value, the highest value
for the correlation coeﬃcient R, the narrowest range of errors, the lowest value of MAPE
from all the three neural networks developed for Izvoru PVPP, followed by the networks
developed by the BR and SCG algorithms.
As for the stochastic models, we have obtained the best results with ARIMA(1,0,0),
having an accuracy of 85–90% and with ARIMA(1,1,0), having accuracy of about 90%,
for both PVPPs. As a consequence of phenomenon nature, we recommend not to use
ARIMA(0,0,1) model due to the fact that MAPE are very high. As for the
ARIMA(1,1,1), the accuracy of the model is lower than 75% and we would not recom‐
mend for this data set, but since its performance could improve it can be further imple‐
mented and veriﬁed for other PVPPs. In case of data mining models, we obtain the best
results with the ensemble champion and neural networks models.
The forecasting solutions are a useful tool for the renewable energy producers that
are compelled by law to submit their estimations of energy production to the Romanian
National Energy Regulatory Authority and in developing decision support systems for
the eﬃcient management of electricity generation from renewable sources. The fore‐
casting solutions are of great interest for owners or administers in order to prepare their
bids on the electricity market and minimize the balancing costs. The developed solutions
are also useful for the potential investors that intend to assess the photovoltaic energy
production potential of a certain geographic area.
Acknowledgments. This paper presents a series of results obtained within the SIPAMER
research project (“Sistem Inteligent pentru Predicţia, Analiza și Monitorizarea Indicatorilor de
Performanţă a Proceselor Tehnologice şi de Afaceri în Domeniul Energiilor Regenerabile”),
PNII – “Parteneriate în domeniile prioritare”, PCCA 2013, code 0996, no. 49/2014, ﬁnanced by
the National Authority for Scientiﬁc Research (NASR).
172
S.-V. Oprea et al.

References
1. Tabriz, S.N., Behboodi, E., Aliyev, F.Q.: Towards renewability by applying solar energy
technologies for improved life cycle. IJTPE 12, 7–12 (2012)
2. Freris, L., Inﬁeld, D.: Renewable Energy in Power Systems. Wiley, UK (2008)
3. Bara, A., Andreescu, A.: Data model for SIPAMER prototype. DB J. 4, 39–48 (2014)
4. Cornaro, C., Bucci, F., Pierro, M., Frate, F., Peronaci, S., Taravat, A.: Solar radiation forecast
using neural networks for the prediction of grid connected PV plants energy production. In:
Proceedings of 28th European Photovoltaic Solar Energy Conference and Exhibition, pp.
3992–3999 (2013)
5. Abuella, M., Chowdhury, B.: Solar power forecasting using artiﬁcial neural networks. In:
Proceedings of the 47th Annual North American Power Symposium, pp. 1–5 (2015)
6. Qazi, A., Fayazb, H., Wadib, A., Raja, R.G., Rahimb, N.A., Khanc, W.A.: The artiﬁcial neural
network for solar radiation prediction and designing solar systems: a systematic literature
review. J. Clean. Prod. 104, 1–12 (2015)
7. Yan, X.: Solar radiation forecasting using artiﬁcial neural network for local power reserve.
In: Proceedings of Electrical Sciences and Technologies International Conference (CISTEM),
pp. 1–6. Curran Associates Inc., New York (2014)
8. Bara, A., Botha, I., Belciu, A., Nedelcu, B.: Exploring data in human resources big data. DB
J. 6, 3–10 (2015)
9. Beale, M.H., Martin, H.T., Demuth, H.B.: Matlab Neural Network Toolbox User’s Guide,
R2015a. www.mathworks.com
10. Lungu, I., Bâra, A., Caruțașu, G., Pîrjan, A., Oprea, S.V.: Prediction intelligent system in the
ﬁeld of renewable energies through neural networks. Econ. Comput. Econ. Cybern. Stud.
Res. 50, 85–102 (2016)
11. Mohamed, Z., Bodger, P.S.: Forecasting Electricity Consumption. A Comparison of Models
for New Zealand. http://ir.canterbury.ac.nz/bitstream/handle/10092/821/12593635_C47.
pdf?sequence=1
12. Chujai, P., Kerdprasop, N., Kerdprasop, K.: Time series analysis of household electric
consumption with ARIMA and ARMA models. In: Proceedings of the International Multi
Conference of Engineers and Computer Scientists (IMECS), pp. 295–300. Newswood
Limited I, Hong Kong (2013)
13. Huang, R., Huang, T., Gadh, R., Li, N.: Solar generation prediction using the ARMA model
in a laboratory-level micro-grid. In: Proceedings of the Third International Conference on
Smart Grid Communications (SmartGridComm), pp. 528–533. IEEE (2012)
14. Levenberg, K.: A method for the solution of certain problems in least squares. Quart. Appl.
Math. 2, 164–168 (1944)
15. Marquardt, D.: An algorithm for least-squares estimation of nonlinear parameters. SIAM J.
Appl. Math. 11, 431–441 (1963)
16. Kişi, Ö., Uncuoğlu, E.: Comparison of three back-propagation training algorithms for two
case studies. IJEMS 12, 434–442 (2005)
17. MacKay, D.J.C.: A practical Bayesian framework for backpropagation networks. Neural
Comput. 4, 448–472 (1992)
18. Foresee, D., Hagan, M.: Gauss-Newton approximation to Bayesian learning. In: Proceedings
of the 1997 International Joint Conference on Neural Networks, pp. 1930–1935. Institute of
Electrical and Electronics Engineers Inc. (1997)
19. Moller, M.: A scaled conjugate gradient algorithm for fast supervised learning. Neural Netw.
6, 525–533 (1993)
Forecasting Solutions for Photovoltaic Power Plants in Romania
173

20. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations by error
propagation. In: Rumelhart, D.E., McClelland, J.L. (eds.) Parallel Distributed Processing.
Exploration in the Microstructure of Cognition, pp. 318–362. MIT Press, Cambridge (1986)
21. Johansson, E.M., Dowla, E.U., Goodman, D.M.: Backpropagation learning for multi-layer
feed-forward neural networks using the conjugate gradient method. Int. J. Neural Syst. 2,
291–302 (1991)
22. Battiti, R.: Optimization methods for back-propagation: automatic parameter tuning and
faster convergence, In: Proceedings of the IEEE/INNS International Conference Neural
Networks, vol. 1, pp. 593–596 (1990)
174
S.-V. Oprea et al.

Reﬂecting on Romanian Universities Ranking:
An Entropy-Based Approach to Evaluate
Scientiﬁc Research
Luiza Bădin1,2, Florentin Şerban1,3, Anca-Teodora Şerban-Oprescu4,
and Silvia Dedu1,5(&)
1 Department of Applied Mathematics, Bucharest University of Economic
Studies, Piata Romana 6, Sector 1, 010374 Bucharest, Romania
{luiza.badin,ﬂorentin.serban,silvia.dedu}@csie.ase.ro
2 Gh. Mihoc - C. Iacob Institute of Mathematical Statistics and Applied
Mathematics, Calea 13 Septembrie 13, Sector 5, 050711 Bucharest, Romania
3 Doctoral School of Mathematics, University of Bucharest,
Strada Academiei 14, Sector 1, 010014 Bucharest, Romania
4 Department of Modern Languages and Business Comunication,
Bucharest University of Economic Studies,
Piata Romana 6, Sector 1, 010374 Bucharest, Romania
teodora.oprescu@rei.ase.ro
5 School of Advanced Studies of the Romanian Academy, Calea Victoriei 125,
Sector 1, 010071 Bucharest, Romania
Abstract. Quantitative evaluation of scientiﬁc research activity involves a set
of complex methodological aspects, many of which have not received so far the
deserved attention, neither in theoretical, nor in empirical studies. The concept
of entropy is widely used in decision-making problems as a useful instrument
for assessing the amount and effect of information provided by certain criteria
used to construct a composite indicator. This paper proposes the use of entropy
to evaluate scientiﬁc research performance of academic units. The ﬁeld of
observation consists of Romanian universities classiﬁed either as Advanced
Research and Education or Education and Scientiﬁc Research units, by national
ranking exercise in 2011. Our analysis considers only ISI publications - Articles
and Proceedings Papers, during 2006–2010. We argue that the evaluation of
scientiﬁc research can be better addressed and these preliminary results on
university rankings could be further validated when alternative methods of
assessment are applied.
Keywords: Entropy  Evaluation  Composite indicator
Scientiﬁc research activity  University rankings
1
Introduction
Recently, one major concern has been to ﬁnd effective methods for evaluating the
performance of the academic research sector. The ongoing discussion in academic and
policy making circles regarding efﬁciency in education acquired ever increasing
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 175–183, 2018.
https://doi.org/10.1007/978-3-319-73459-0_13

importance, directing attention toward effective management of scientiﬁc units,
resource allocation and universities as sources and creators of innovation (see, e.g.,
Bonaccorsi and Daraio [1]) and proﬁt. Mainly in terms of proﬁt, the evident shift in
education has been noticed and debated by Bonaccorsi et al. [2]: “in the last decades of
the twentieth century universities in Europe and other OECD countries have undergone
a profound transformation. They have evolved from mainly élite institutions for
teaching and research to large (public and private) organisations responsible for mass
higher education and the production and distribution of new knowledge.” Education
being an expensive commodity, even more expensive than others, see, e.g., De Witte
and Lopez-Torres [3], wise resource allocation and exploiting the vast potential of
universities have given rise to consistent literature concerning efﬁciency in education.
Furthermore, according to Bonaccorsi and Daraio [1], accurately outlined early in the
2000s, “higher education institutions are crucial to the development of the European
Research Area” [4]. As argued by Bonaccorsi and Daraio [1], unlike in the USA, at
European level (and in our case, Romanian context), there is scarce availability of
quantitative indicators for higher education institutions, at times, limited method-
ological approach to existing data and lack of alternative methods to simplify or
validate existing data analysis [5].
Following the above-mentioned, numerous attempts have been made to devise an
effective measure of performance in this area. Quantitative evaluation of scientiﬁc
research activity involves a set of complex methodological aspects, many of which
have not received so far the deserved attention, neither in theoretical, nor in empirical
studies. University rankings are generally accused of not granting the deserved
importance to what counts, being mostly based on what can be counted with minimal
costs. Entropy is used within system theory to provide a measure of the degree of
disorder within a system. It can also be employed as a quantitative measure of the
effects of information upon the system.
In this paper we propose a general method for the assessment of academic per-
formance, which can be applied when the values of several indicators are available.
This method is based on the construction of a composite indicator, deﬁned as the
weighted sum of the indicators considered in the study. One of the preliminary steps
needed consists in computing the weights of the indicators, which stand as a measure of
importance of each criterion involved in deﬁning the composite indicator. We propose
to assess the discrepancies between the values of the indicators using an approach
based on the concept of entropy, which allows us assigning relative weights to the
indicators. In case one ﬁnds a large degree of difference, the entropy will be smaller and
more accurate information will be provided. As a consequence, the weight of the
corresponding indicator will be larger. Our analysis considers only publications
indexed in Web of Science, Articles and Proceedings Papers, during 2006–2010.
We use an entropic approach [6] to obtain the weight vector based on a predeﬁned
decision matrix. Our approach relies on the methodology used in [7, 8] in order to assess
the performance in scientiﬁc research for 34 Romanian universities, classiﬁed either as
Advanced Research and Education units or Education and Scientiﬁc Research units, by
national ranking exercise in 2011. The paper unfolds as follows. Section 2 describes the
176
L. Bădin et al.

methodology for evaluating scientiﬁc research activity based on entropy. Section 3 is
concerned with data processing and presenting the numerical results obtained. The paper
ends with concluding remarks suggesting also directions for future research.
2
Research Assessment Based on Entropy
Information theory origins, as can be found in R.W. Harley, R.S. Fisher, C.E. Shannon,
N. Wiener, show that ideas in information theory are sourced in the concept of disorder.
The issue of information measurement was successfully approached by C.E. Shannon,
A.I. Hincin, D.K. Fadeev, A.N. Kolmogorov, A. Renyi, O. Onicescu, S. Guiaşu [9],
which brought important contributions regarding information measurement and tried to
ﬁnd utility to this measurement.
The problem of deﬁning an information measure contained and hence provided by
a certain experiment represents a remarkable result and constitutes the ﬁrst fundamental
step of building the mathematical information theory. The information utility has been
developed as a preference measure for certain results of an experiment.
In systems theory, entropy is employed to evaluate the degree of internal disorder.
Entropy can also be used as a quantitative measure of the effect of information provided
by the system. In case the state of a system is quantiﬁed using a certain indicator, the
existence of big differences between the values of this indicator corresponds to a small
value of entropy. Conversely, the smaller the differences, the larger the entropy. In case
an indicator is less important for characterizing the state of a system, its weight will be
small. Also, when it provides more effective information, its weight will be larger.
Entropy attains its maximum value when the values of the indicator used to evaluate
the state of the system are equal. It means that such indicators are unable to provide any
relevant information to the decision process.
We consider a state system X ¼ x1; x2; . . .; xn
ð
Þ with the state probability vector
p1 x1
ð
Þ; p2 x2
ð
Þ; . . .; pn xn
ð
Þ
ð
Þ. The Shannon entropy can be used to evaluate the available
quantity of information by
H X
ð Þ ¼ 
Xn
i¼1 p xi
ð Þ lnp xi
ð Þ:
ð1Þ
It can be normalized in order to obtain values in the interval (0,1) by using the formula
H X
ð Þ ¼  1
ln n
Xn
i¼1 p xi
ð Þ lnp xi
ð Þ:
ð2Þ
Suppose there are m evaluation objects and n evaluation indicators and in this way
the original data matrix X ¼ xij


mn is deﬁned. For an indicator j, the bigger the
difference of the index value xij, the bigger the role played by the indicator in the
comprehensive assessment. Thus, in case the values of an indicator present a low
variation degree, then the indicator plays little role in the assessment.
Reﬂecting on Romanian Universities Ranking
177

3
Research Methodology and Numerical Results
Although education is an essential part of the universities’ mission, its rather intangible
nature in terms of teaching/learning outputs is rather elusive. We chose to analyze
indicators present and veriﬁable in Web of Science, that present reliability, public
availability, with as little white noise as possible. In the future, data can be added with
maximum care. Nonetheless, when it comes to promotion and fundamental criteria for
promotion, didactic staff is being evaluated 90% of the times in terms of publications
and published research in proﬁle journals. We propose to evaluate scientiﬁc research
activity for 34 Romanian universities based on publications indexed in Web of Science
database, speciﬁcally, Articles and Proceedings Papers from 2006 to 2010. We chose to
process data in Web of Science because this database represents reliability. In a period
of scientiﬁc value uncertainty, the existence of a journal in ISI database indicates
toward a certain proven value. Education represents, certainly, a crucial component in
the process of evaluating universities. However, it is an activity difﬁcult to measure and
evaluate accurately. The goal of our paper is not to provide a ranking, but rather to
reﬂect and comment on a classiﬁcation with respect to the scientiﬁc research compo-
nent. The data set used in our study is based on reports from Romanian universities
participating in the national ranking process and is available at http://chestionar.
ueﬁscdi.ro [10]. To be accurate, data consists of total number of papers published by
the permanent teaching and research staff of the university and indexed in Web of
Science during 2006–2010 and for the following categories: articles published in
journals with impact factor calculated, journals without impact factor calculated (Sci-
ence and Social Sciences), Arts & Humanities journals, and papers published in
Conference Proceedings indexed in Web of Science. The papers are reported only for
situations in which the author/authors expressly mentioned institutional afﬁliation.
Table 1 presents the data set used in the study.
Table 1. ISI publications indexed in Web of Science during 2006–2010. Data source: http://
chestionar.ueﬁscdi.ro [10]
University
Articles/Number
of authors
Proceedings
papers/Number of
authors
The University of Bucharest
2102.60
302.52
“Babeș-Bolyai” University Cluj-Napoca
2094.00
395.00
“Alexandru Ioan Cuza” University of Iasi
1223.11
210.51
Bucharest University of Economic Studies
677.88
715.35
University of Agricultural Sciences and Veterinary
Medicine Cluj-Napoca
174.00
241.00
University of Medicine and Pharmacy “Carol
Davila” Bucharest
2672.00
836.00
“Gr.T.Popa”University of Medicine and Pharmacy
Iasi
493.21
297.65
(continued)
178
L. Bădin et al.

Table 1. (continued)
University
Articles/Number
of authors
Proceedings
papers/Number of
authors
“Iuliu Haţieganu” University of Medicine and
Pharmacy Cluj-Napoca
552.89
83.00
University Politehnica of Bucharest
2525.09
2936.42
“Gheorghe Asachi” Technical University of Iaşi
1134.05
625.16
Technical University of Cluj-Napoca
560.00
997.00
Politehnica University Timisoara
680.25
861.13
West University of Timisoara
589.77
417.33
University of Craiova
509.82
653.21
Transilvania University of Brasov
425.20
484.41
Ovidius University Constanta
545.89
389.68
“Dunărea de Jos” University of Galati
376.81
289.45
“Lucian Blaga” University of Sibiu
215.00
609.00
The University of Oradea
288.19
160.00
National University of Political Studies and Public
Administration Bucharest
40.00
55.00
“Ion Ionescu de la Brad” University of Agricultural
Sciences and Veterinary Medicine Iaşi
64.00
21.00
University of Agronomic Sciences and Veterinary
Medicine Bucharest
172.00
140.00
Banat University of Agricultural Sciences and
Veterinary Medicine Timisoara
69.35
15.24
Victor Babes University of Medicine and Pharmacy
Timisoara
346.59
101.23
University of Medicine and Pharmacy of Craiova
216.67
33.23
Technical University of Civil Engineering
Bucharest
158.00
103.00
University of Medicine and Pharmacy of Tirgu
Mures
120.36
25.85
Military Technical Academy
17.71
91.99
“Henri Coanda” Air Force Academy
9.26
25.51
“Alexandru Ioan Cuza” Police Academy
6.00
24.00
National Defense University “Carol I”
1.00
29.00
”Mircea cel Batran” Naval Academy
21.00
80.00
National Intelligence Academy “Mihai Viteazul”
Bucharest
0.00
0.00
“Nicolae Balcescu” Land Forces Academy Sibiu
5.60
101.70
Reﬂecting on Romanian Universities Ranking
179

The algorithm proposed to determine the weights consists in performing the fol-
lowing steps:
Step 1. Data standardizing.
(a) The original data matrix X ¼ xij


mn is ﬁrstly standardized by:
yij ¼
xij  min xij
max xij  min xij
;
j ¼ 1; n:
ð3Þ
(b) We apply the formula zij ¼ yijyJ
Sj
to continue the standardization, where yJ and Sj
are the mean value respectively standard deviation of jth index.
(c) As the calculation of entropy needs a natural logarithm, the index value must be
positive. So we set uij ¼ zij þ dij, where d is a number greater than min zij

. Thus,
we have the standard matrix U ¼ uij


mn. We get the value d = 0.76077.
Step 2. Computing the weights of the indices.
(a) We determine the probabilities pij for the ith unit in the jth index, by
pij ¼
uij
Pm
i¼1 uij
;
j ¼ 1; n:
ð4Þ
(b) We compute the entropy ej of the jth index, by
ej ¼  1
ln m
X
n
i¼1
pij lnpij;
j ¼ 1; n:
ð5Þ
(c) We calculate the utility of the ith index by
dj ¼ 1  ej;
j ¼ 1; n:
ð6Þ
(d) The weight of the jth index is standardized by
wj ¼
dj
Pm
i¼1 di
;
j ¼ 1; n:
ð7Þ
In Table 2 the results obtained in the ﬁrst two steps are summarized.
Table 2. The research score of academic units
Measure Articles
Proceedings papers
Entropy
0.798620 0.816550
Utility
0.201380 0.183450
Weights 0.523297 0.476703
180
L. Bădin et al.

We can remark that the diversity of input data leads to relative equal weights
corresponding to the factors considered in the evaluation, Articles and Proceedings
Papers respectively.
Step 3. Evaluating the academic units.
The value of the ith unit in the jth index is given by
fij ¼ wj  yij;
j ¼ 1; n:
ð8Þ
Thus, the total score for the ith unit is given by
fi ¼
X
n
j¼1
fij;
i ¼ 1; m:
ð9Þ
The numerical results obtained are presented in Table 3. For each university we
provide the values of the score and of its components fi1 and fi2, corresponding to the
factors used for university ranking.
Table 3. The research score of academic units
University
fi1
fi2
Score
The University of Bucharest
0.40443
0.05007
0.45450
“Babeș-Bolyai” University Cluj-Napoca
0.40277
0.06538
0.46816
“Alexandru Ioan Cuza” University of Iasi
0.23526
0.03484
0.27011
Bucharest University of Economic Studies
0.13039
0.11841
0.24880
University of Agricultural Sciences and Veterinary
Medicine Cluj-Napoca
0.03347
0.03989
0.07336
University of Medicine and Pharmacy “Carol Davila”
Bucharest
0.51395
0.13838
0.65233
“Gr.T.Popa”University of Medicine and Pharmacy Iasi
0.09487
0.04927
0.14414
“Iuliu Haţieganu” University of Medicine and Pharmacy
Cluj-Napoca
0.10635
0.01374
0.12008
University Politehnica of Bucharest
0.48569
0.48605
0.97174
“Gheorghe Asachi” Technical University of Iaşi
0.21813
0.10348
0.32161
Technical University of Cluj-Napoca
0.10771
0.16503
0.27274
Politehnica University Timisoara
0.13084
0.14254
0.27338
West University of Timisoara
0.11344
0.06908
0.18252
University of Craiova
0.09806
0.10812
0.20618
Transilvania University of Brasov
0.08179
0.08018
0.16197
Ovidius University Constanta
0.10500
0.06450
0.16950
“Dunărea de Jos” University of Galati
0.07248
0.04791
0.12039
“Lucian Blaga” University of Sibiu
0.04135
0.10080
0.14216
The University of Oradea
0.05543
0.02648
0.08192
(continued)
Reﬂecting on Romanian Universities Ranking
181

Using the proposed algorithm, the diversity of input data leads to the scores pre-
sented in Table 3. The highest score corresponds to the ﬁrst ranked university and so
on. Thus, we remark that the results of our study are similar to those obtained from the
national ranking process.
4
Conclusions and Future Research
In this paper we have proposed a simple, yet effective procedure for a rapid evaluation
of scientiﬁc research activity using the concept of entropy. Our analysis has explored
two categories of publications that are of essential importance for research activity,
without taking into account other indicators like university size, which is, indeed, very
important. Interest in this topic has arisen from increased scholar attention in university
ranking and the complexity of available data. The resulted ranking for top universities
is very similar to the ofﬁcial classiﬁcation. We consider that the ranking exercise would
be more efﬁcient if policy makers would focus more on what is important to be
observed and would avoid collecting large amount of data which is difﬁcult to check,
even randomly, or prone to be affected by statistical noise. The fact that our results are
very close to the ofﬁcial rankings that ran complex methods to obtain presented
rankings might indicate that the number of articles and proceedings represent relevant
Table 3. (continued)
University
fi1
fi2
Score
National University of Political Studies and Public
Administration Bucharest
0.00769
0.00910
0.01680
“Ion Ionescu de la Brad” University of Agricultural
Sciences and Veterinary Medicine Iaşi
0.01231
0.00348
0.01579
University of Agronomic Sciences and Veterinary
Medicine Bucharest
0.03308
0.02317
0.05626
Banat University of Agricultural Sciences and Veterinary
Medicine Timisoara
0.01334
0.00252
0.01586
Victor Babes University of Medicine and Pharmacy
Timisoara
0.06667
0.01676
0.08342
University of Medicine and Pharmacy of Craiova
0.04168
0.00550
0.04718
Technical University of Civil Engineering Bucharest
0.03039
0.01705
0.04744
University of Medicine and Pharmacy of Tirgu Mures
0.02315
0.00428
0.02743
Military Technical Academy
0.00341
0.01523
0.01863
“Henri Coanda” Air Force Academy
0.00178
0.00422
0.00600
“Alexandru Ioan Cuza” Police Academy
0.00115
0.00397
0.00513
National Defense University “Carol I”
0.00019
0.00480
0.00499
“Mircea cel Batran” Naval Academy
0.00404
0.01324
0.01728
National Intelligence Academy “Mihai Viteazul”
Bucharest
0.00000
0.00000
0.00000
“Nicolae Balcescu” Land Forces Academy Sibiu
0.00108
0.01683
0.01791
182
L. Bădin et al.

criteria in classiﬁcations regarding research or scientiﬁc production. Our approach is
able to identify relevant indicators for the purpose of evaluating scientiﬁc research.
It is important to note that the algorithm can be used with various types of entropy
measures. For future research, it would be important to decide which entropy measure
should be used in order to provide more reliable and robust results. The relevance of
our ﬁndings opens up paths to future research. The present ﬁndings could be extended
by taking into consideration other indicators which would further validate the results
obtained here. Studies that use entropy to assess scientiﬁc research are few and in
incipient stages. An inherent beneﬁt of the method is that one can realize how to better
calibrate from the initial onset in the future weights per indicator(s). The conclusions of
our work outline that the evaluation of scientiﬁc research can be easier addressed using
entropy measures. Finally, we note that alternative methods like nonparametric efﬁ-
ciency analysis can be mixed with the entropic approach in order to provide valid and
robust results.
Acknowledgments. This work was supported by a grant of the Romanian National Authority for
Scientiﬁc Research and Innovation, CNCS – UEFISCDI, project number PN-II-RU-TE-2014-
4-2905.
References
1. Bonaccorsi, A., Daraio, C. (eds.): Universities and Strategic Knowledge Creation. Edward
Elgar Publishing (2007). Number 12626
2. Bonaccorsi, A., Daraio, C., Geuna, A.: Universities in the new knowledge landscape:
Tensions, challenges, change — an introduction. Minerva 48(1), 1–4 (2010)
3. De Witte, K., Lopez-Torres, L.: Efﬁciency in education: a review of literature and a way
forward. J. Oper. Res. Soc. 68(4), 339–363 (2015)
4. Daraio, C., Bonaccorsi, A., Geuna, A., Lepori, B., Bach, L., Bogetoft, P., et al.: The
European University Landscape: a micro-characterization based on evidence from the
Aquameth project. Res. Policy 40, 148–164 (2011)
5. Bonaccorsi, A., Daraio, C., Lepori, B., Slipersaeter, S.: Indicators on individual higher
education institutions: addressing data problems and comparability issues. Res. Eval. 16(2),
66–78 (2007)
6. Shannon, C.E.: A mathematical theory of communication. Comput. Commun. Rev. 5, 3–55
(2001)
7. Ouyang, D., Xu, B., Li, Y., Huang, H.: A Comprehensive evaluation of culture force based
on entropy method: cross-sectional data in Hunan province. J. Convergence Inf. Technol. 7
(23), 416–424 (2012)
8. Liu, W., Cui, J.: Entropy coefﬁcient method to evaluate the level of sustainable development
of China’s sports. Int. J. Sports Sci. Eng. 2(2), 72–78 (2008)
9. Guiaşu, S.: Weighted entropy. Rep. Math. Phys. 2, 165–179 (1971)
10. Executive Agency for Higher Education, Research, Development and Innovation Funding,
12 April 2015. http://chestionar.ueﬁscdi.ro
Reﬂecting on Romanian Universities Ranking
183

Computational Models

Insights of Adaptive Learning Approach
to Modeling Expectations: A Numerical
Comparison with Adaptive Expectations
and Rational Expectations
Raluca-Elena Pop(&)
Economic Cybernetics and Statistics Doctoral School,
Bucharest University of Economic Studies, Bucharest, Romania
pop_ralucaelena@yahoo.com
Abstract. This study explores the macroeconomic implications of the main
theories of expectations formation, i.e. adaptive expectations, rational expecta-
tions and adaptive learning, in the context of the standard growth model that
provides the backbone of a lot of macroeconomics models that are used in
modern research. It is shown that the adaptive expectations formulation implies
a high degree of inertia even when a high correction factor is assumed. In
contrast, the rational expectations and the recursive least squares learning
algorithms exhibit a much faster return to equilibrium in case of a shock. The
paper also emphasizes the importance of the initial conditions for the behavior
of macroeconomic variables in case of the learning algorithm: if more prelim-
inary periods are allowed so that the initial values are closer to the coefﬁcients
coming from the rational expectations solution, the predicted path of the vari-
ables is much closer to the one under rational expectations.
Keywords: Adaptive expectations  Adaptive learning  Rational expectations
Stochastic growth model
1
Introduction
Expectations play a prominent role in economic theories and are a critical feature of
macroeconomic models due to their recognized substantial impact in the process of
agents’ decision making. For example, in consumption theory the permanent income
hypothesis developed by [1] (see [2] for a more recent discussion of the subject)
stresses the role of expected future incomes. The New Keynesian Phillips Curve, based
on the seminal work of [3, 4], relates the current inﬂation to real marginal cost and
future expected inﬂation. In a more recent paper [5] explores the impact of forward
looking expectations in the context of a reduced form hybrid New Keynesian Phillips
Curve for some Central and Eastern European Countries. Furthermore investment
decisions depend on present-value calculations which are conditional on expected
future prices and sales. Many other examples can be given.
In the long line of theories which have emphasized the role of expectations, we can
distinct two major expectation paradigms, i.e. adaptive expectations and rational
© Springer International Publishing AG 2018
G. C. Silaghi et al. (Eds.): IE 2016, LNBIP 273, pp. 187–199, 2018.
https://doi.org/10.1007/978-3-319-73459-0_14

expectations, where the second represents the current standard in mainstream eco-
nomics in terms of modeling expectations. If in the case of adaptive expectations
people form their views about the future based on what has happened in the past,
rational agents make decisions conditional on all of the available information. This
implies the fact that agents have a great deal of knowledge about the economy, more
exactly they use the modeler’s model to form expectations, including the true
parameter values and the distribution of the random shocks. Learning in macroeco-
nomics is a reaction to these rather strong assumptions. According to this new approach
to modeling expectations, agents are able to ﬁgure-out the reduced form equations of
the model, but they don’t know the parameterization of those equations and must learn
it. This process may or may not converge to rational expectations equilibrium in the
long run. However, learning dynamics may be of interest by themselves, helping to
provide a better ﬁt to the data.
In this context, the aim of the paper is to present the characteristics, including the
drawbacks of the main approaches to modeling expectations in economic theory, as
highlighted above, and to explore their macroeconomic implications with the aid of a
well-known model, the standard growth model that provides the backbone of a lot of
models that are used in modern macroeconomic research. For the second purpose, the
model is solved under each of the three discussed approaches and some simulations of
the evolution of capital are provided so that the different results coming from these
formulations can be appreciated.
The rest of the paper is organized as follows: the next section presents a brief
history of expectation modeling in macroeconomics, while in Sect. 3 an overview of
the stochastic growth model is provided. Section 4 explores the implications for this
model of the different approaches to modeling expectations, numerical results being
presented in Sect. 5. Finally, Sect. 6 concludes.
2
A Brief History of Expectations Modeling
In macroeconomics the importance of expectations was ﬁrst emphasized by [6], who
stressed their central role for the behavior of economic agents, although he did not
propose an explicit model of how expectations are formed.
One of the ﬁrst theory of expectations formation was the one of adaptive expec-
tations advocated, among others, by [1, 7, 8]. According to this theory, the expected
value of a variable at a certain moment of time depends on its previous expected value
adjusted for the last available forecast error. The term adaptive expectations was one of
the key concepts promoted by the monetarist theory, along with the natural unem-
ployment rate and permanent income hypothesis. The popularity of this theory reached
its peak in the 1950s and 1960s, characterized by low and relatively stable values of the
inﬂation rate, but proved to be of little use in forecasting trends or in a rapidly changing
environment as it doesn’t allow agents to make the best use of all the available
information. In other words, the main shortcoming of the theory is its rather simplistic
formulation only in terms of past data, implying the fact that agents don’t learn from
past mistakes; in reality past data is one of the many factors that inﬂuence future
behavior.
188
R.-E. Pop

The above mentioned limitation was overcome with the introduction of rational
expectations by [9], according to which agents use all available information in an
optimal way, so deviations from perfect foresight are only random. In macroeconomics
the rational expectations hypothesis was intensively promoted in the 1970s and 1980s
by [10–13] and today it represents the dominant assumption in mainstream economics
and a true building block of macroeconomic theory. A rational economic agent takes all
available information into account when forming expectations about the future and not
just past values. Though expectations may turn out to be incorrect, they will not deviate
systematically from the effective values.
However, there are some problematic features of this theory that have been high-
lighted over time. The most important one refers to the high degree of knowledge that
is requested from economic agents: they are supposed to have complete knowledge
about the economy, more exactly they know the structure of the economy, the values of
the structural parameters as well as the distribution of any exogenous shock that affects
the economy. Also, costs of forming expectations are ignored and how agents get
rational expectations is not explained. Moreover, the hypothesis is usually not sup-
ported by the empirical evidence (see [14, 15] for recent evidence).
These drawbacks of the rational expectations hypothesis have led to the develop-
ment of a number of alternative ways for modeling expectations. One of the most
promising alternatives is the adaptive learning approach: agents are supposed to have a
more limited degree of knowledge about the economy, i.e. bounded rationality, and to
learn over time about the impact of different factors on the variables of interest. This
approach has been rigorously developed over the last 25 years. [16] offer a compre-
hensive introduction to adaptive learning models, while [14, 17] conclude that this type
of models may be successfully applied for modeling inﬂation expectations in both
developed and emerging economies.
There are several reasons why the study of adaptive learning dynamics may be
important in practice. First of all, it provides a micro-foundation for the rational expec-
tations hypothesis by offering insights regarding the way in which agents may acquire
such expectations. [16] deﬁne the e-stability condition that determines the stability of the
rational expectations solution under a learning rule. They also provide the necessary
conditions for local convergence of the adaptive learning solution towards the rational
expectations one. Secondly, it can be used as a selection criteria among multiple rational
expectations equilibria by considering only those ones that are stable under learning.
Furthermore, this form of bounded rationality can be used in explaining observed
dynamics of macroeconomics and ﬁnancial variables as shown, among others, by [18].
3
The Stochastic Growth Model
This section provides an overview of the stochastic growth model.
The following problem of the representative household is deﬁned:
maxCt;KtE0
X
1
t¼0
bt C1c
t
 1
1  y
ð1Þ
Insights of Adaptive Learning Approach to Modeling Expectations
189

subject to
Ct þ Kt ¼ 1  d
ð
ÞKt1 þ Yt
ð2Þ
Yt ¼ ZtKa
t1
ð3Þ
logZt ¼ qlogZt1 þ et; et  iid 0; r2


ð4Þ
where Ct is the consumption, Kt is the aggregate capital stock, Yt is the production and
Zt is the economy-wide technological level. b, c, d, a, q and r are parameters rep-
resenting the discount factor, the inverse of the elasticity of intertemporal substitution,
the depreciation rate, the capital share of output, the shock’s persistence and the shock
innovation standard deviation respectively.
In order to solve the model, optimality conditions are derived for the maximization
problem. The competitive equilibrium is summarized by the following system of four
equations:
1 ¼ bEt
Ct
Ct þ 1

c
aZt þ 1Ka1
t
þ 1  d
ð
Þ




ð5Þ
Ct ¼ 1  d
ð
ÞKt1 þ ZtKa
t1  Kt
ð6Þ
Yt ¼ ZtKa
t1
ð7Þ
logZt ¼ q logZt1 þ et
ð8Þ
In the absence of shocks, the economy converges to a steady-state where Xt ¼ X for
any X ¼ C; K; Y
f
g and for all t. The steady-state solution to the system above is:
K ¼ h1= 1a
ð
Þ
ð9Þ
C ¼ ha= 1a
ð
Þ  dh1= 1a
ð
Þ
ð10Þ
Y ¼ ha= 1a
ð
Þ
ð11Þ
Z ¼ 1
ð12Þ
where h ¼ 1  b þ db
ð
Þ=ab.
The dynamics of the model are obtained by taking a log-linear approximation
around the steady-state values. For any variable X ¼ C; K; Y; Z
f
g, the log-linear values
(expressed as deviations from the steady-state) are deﬁned as xt ¼ ln Xt=X


and the
log-linearized system is:
0 ¼ Et c ct  ct þ 1
ð
Þ  abh 1  a
ð
Þkt þ abhzt þ 1
½

ð13Þ
190
R.-E. Pop

ct ¼
KR
C kt1 þ
Y
C zt 
K
C kt;
R ¼ 1=b
ð14Þ
yt ¼ zt þ akt1
ð15Þ
zt ¼ qzt1 þ et
ð16Þ
To solve the log-linearized version of the model the common approach in the
economic literature is to substitute all the control variables out from the Euler condi-
tion, i.e. (13), so that the entire model can be rewritten as a difference equation system
of the form:
kt ¼ a1Etkt þ 1 þ a2kt1 þ bzt
zt ¼ qzt1 þ et
ð17Þ
which shows that the agents’ decision over the level of capital they wish to hold at any
given period, kt, can be expressed as a function of the expected value of this variable
for the next period, Etkt þ 1, its past value, kt1, and the contemporaneous exogenous
variable, zt, which in turn evolves according to the second equation of the system.
Parameters a1, a2 and b are constant coefﬁcients that depend on the structural
parameters of the model and the steady-state values of the variables according to:
a1 ¼
cK
cK þ cKR þ abh 1  a
ð
ÞC
ð
Þ
a2 ¼
cKR
cK þ cKR þ abh 1  a
ð
ÞC
ð
Þ
b ¼
cY  cqY þ abhqC
cK þ cKR þ abh 1  a
ð
ÞC
ð
Þ
ð18Þ
Next, the dynamics of each of the control variables of the model, i.e. x ¼ y; c
f
g,
can be rewritten in terms of the state variables only, according to the following reduced
form:
xt ¼ /x1kt þ /x2kt1 þ /x3zt þ /x4Etkt þ 1
ð19Þ
where, as above, the coefﬁcients /xj (for j ¼ 1; 4Þ depend on the parameters of the
model and the steady-state values of the variables:
/y1 ¼ 0; /y2 ¼ a; /y3 ¼ 1; /y4 ¼ 0
/c1 ¼ K=C; /c2 ¼ KR=C; /c3 ¼ Y=C; /c4 ¼ 0
ð20Þ
A number of models, such as the real business cycle model and many of its
extensions, have a reduced form similar with the system represented in (17), as shown
by [16, 19] etc.
Insights of Adaptive Learning Approach to Modeling Expectations
191

4
Model Solution Under Different Approaches to Modeling
Expectations
From (17) it is obvious that the solution of the model depends on the assumption
regarding the formation of expectations by economic agents.
This section takes a look at the solution of the model under three approaches to
modeling expectations: adaptive expectations, rational expectations and adaptive learning.
Once the evolution of the endogenous state variable, i.e. kt, is obtained, (19) and
(20) can be used to ﬁnd the solutions for the jump variables of the model also. For this
reason this section focuses on solving the model and simulating the evolution of the
state variable kt only.
4.1
Adaptive Expectations
Assume that agents form their expectations about the future level of capital in an
adaptive manner:
Etkt þ 1 ¼ Et1kt þ t kt  Et1kt
ð
Þ;
ð21Þ
meaning that the previous period expectation, i.e. Et1kt, is adjusted by a fraction
0\t\1 of the forecast error represented by kt  Et1kt.
By substituting (21) into (17) and rearranging terms, the following solution for the
model at hand is obtained:
kt ¼ a1 1  t
ð
Þ
1  a1t
ð
Þ Et1kt þ
a2
1  a1t
ð
Þ kt1 þ
b
1  a1t
ð
Þ zt
ð22Þ
Et1kt can be determined from (21):
Et1kt ¼ kt1 þ t kt1  kt1
ð
Þ ¼ kt1
Collecting terms and using (16), (22) becomes:
kt ¼ a1 1  t
ð
Þ þ a2
1  a1t
ð
Þ
kt1 þ
bq
1  a1t
ð
Þ zt1 þ
b
1  a1t
ð
Þ et
ð23Þ
plus the initial values kt1 and zt1.
Note that the adaptive expectations solution can also be written in the form:
Etkt þ 1 ¼ t
X
1
i¼0
1  t
ð
Þikt1i
ð24Þ
which is a distributed lag with exponentially declining weights. Thus, under adaptive
expectations hypothesis, economic agents form their expectations as a sample mean of
historical values, with weights geometrically decreasing as the observation is further
into the past.
192
R.-E. Pop

4.2
Rational Expectations
According to the rational expectations hypothesis there is no systematic component in
the forecast error which agents could correct, i.e. economic agents’ expectations are
model consistent. In order to ﬁnd the rational expectation equilibria of the reduced form
model, the method of undetermined coefﬁcients is used, originally presented by [20].
Under rational expectations hypothesis, economic agents are supposed to know the
form of the solution of the model:
kt ¼ gkkkt1 þ gkzzt
ð25Þ
Substituting (16) into the expression above, it is obtained:
kt ¼ /kkt1 þ /zzt1 þ /z
q et
ð26Þ
where /k ¼ gkk and /z ¼ gkz. Solving the model using the method of undetermined
coefﬁcients implies ﬁnding /k and /z, which represent the two undetermined
coefﬁcients.
Leading (26) one period ahead yields:
Etkt þ 1 ¼ /kEtkt þ /zEtzt þ /z
q Etet þ 1
¼ /kkt þ /zzt
ð27Þ
Substituting the above expression into the reduced form model (17) it is obtained:
kt ¼ a1 /kkt þ /zzt


þ a2kt1 þ bzt
zt ¼ qzt1 þ et
ð28Þ
Rearranging terms, the previous two equations can be rewritten as:
kt ¼
a2
1  a1/k
kt1 þ a1/z þ b


q
1  a1/k
zt1 þ a1/z þ b
1  a1/k
et
ð29Þ
This last equation must equal the initial guess (26), leading to:
/k ¼
a2
1  a1/k
/z ¼ a1/z þ b


q
1  a1/k
ð30Þ
Insights of Adaptive Learning Approach to Modeling Expectations
193

which in turn gives the solutions:
/k ¼ 1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  4a1a2
p
2a1
/z ¼
bq
1  a1 q þ /k
ð
Þ
ð31Þ
So, the reduced form model has two solutions under rational expectations
hypothesis. However, only one of the two is consistent with a stable equilibrium:
/k ¼ 1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  4a1a2
p
2a1
/z ¼
bq
1  a1 q þ /k
ð
Þ
ð32Þ
the other one implying an explosive path for the state variable.
Therefore, the solution of the reduced form model is given by:
kt ¼ /kkt1 þ
bq
1  a1 q þ /k
ð
Þ zt1 þ
b
1  a1 q þ /k
ð
Þ et
ð33Þ
where /k ¼ 1 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
14a1a2
p
2a1
.
4.3
Adaptive Learning
The adaptive learning approach reduces the information requirements of rational
expectations hypothesis by assuming that agents act as statisticians or econometricians
when they form expectations about the future state of the economy. More exactly,
agents adjust their forecast rule as new data becomes available over time.
Under adaptive learning hypothesis, economic agents are supposed to know the
form of the solution of the model, but not the values of the coefﬁcients / ¼
/k; /z
	

,
which they update every period by using an econometric technique:
Etkt þ 1 ¼ /k;t1kt þ /z;t1zt
ð34Þ
where /t1 ¼
/k;t1; /z;t1
	

denotes estimates of / with information up to t  1.
Substituting the above expression into the reduced form model (17) it is obtained:
kt ¼ a1 /k;t1kt þ /z;t1zt


þ a2kt1 þ bzt
zt ¼ qzt1 þ et
ð35Þ
194
R.-E. Pop

Rearranging terms, the previous two equations can be rewritten as:
kt ¼
a2
1  a1/k;t1
kt1 þ a1/z;t1 þ b


q
1  a1/k;t1
zt1 þ a1/z;t1 þ b
1  a1/k;t1
et
ð36Þ
Next, it is further assumed that the econometric technique employed by agents in
order to update their estimates of the coefﬁcients is the ordinary least squares
(OLS) technique, which can be expressed recursively in the following way:
/t ¼ /t1 þ t1  R1
t xt1 
kt  x
0
t1  /t1


Rt ¼ Rt1 þ t1 
xt1  x
0
t1  Rt1


ð37Þ
where xt ¼ kt; zt
½

0 and Rt is the second moment matrix of the variables included in xt.
Thus, the complete model under adaptive learning, is given by:
kt ¼
a2
1  a1/k;t1
kt1 þ a1/z;t1 þ b1


q
1  a1/k;t1
zt1 þ a1/z;t1 þ b1
1  a1/k;t1
et
/t ¼ /t1 þ t1  R1
t xt1  kt  x0
t1  /t1


Rt ¼ Rt1 þ t1  xt1  x0
t1  Rt1


ð38Þ
plus the initial values kt0 and zt0, as well as the initial conditions /t0 and Rt0.
In order to assure the convergence of recursive least squares learning solution to the
rational expectations one with probability 1, the algorithm is augmented by a “pro-
jection facility”, i.e. a mechanism according to which the economic agents ignore
estimates that they know to be impossible:
/t ¼ /t1 þ t1  R1
t xt1  kt  x0
t1  /t1


Rt ¼ Rt1 þ t1  xt1  x0
t1  Rt1


/t; Rt
ð
Þ ¼
/t; Rt
ð
Þ; if /t; Rt
ð
Þ 2 D
/t1; Rt1
ð
Þ; if /t; Rt
ð
Þ 62 D

ð39Þ
More exactly, for the speciﬁc case considered in this paper, agents ignore obser-
vations that do not comply with the stationarity condition /k
j
j\1. Also it is reasonably
to assume that agents know that /k [ 0. If these conditions are not satisﬁed, i.e.
/t; Rt
ð
Þ 62 D, the values of the previous period’s estimates /t1; Rt1
ð
Þ are chosen.
Insights of Adaptive Learning Approach to Modeling Expectations
195

5
Numerical Results
This section presents some numerical results for the model solved under the three
assumptions. Following [19], the standard quarterly parameterization for the stochastic
growth model corresponding to the post-war US economy is used, i.e. a ¼ 0:36,
b ¼ 0:99, d ¼ 0:025, c ¼ 1, q ¼ 0:95 and r ¼ 0:007121.
In order to isolate the effects of the different assumptions about the agents’
expectations, following [21], it is assumed that the exogenous state variable is at its
steady state value (zt1 ¼ 0) and that the environment is deterministic (et ¼ 0). The
speed of convergence towards the steady-state under different expectations formulation
schemes is studied by considering two initial values of the endogenous state variable:
kt1 ¼ 1 or 100% below the steady-state value.
Figure 1 plots the different trajectories of kt under each case of interest. For the
model under adaptive expectations a value of 0.9 for the correction factor is assumed,
implying a relatively fast return to equilibrium under this process. Using the above
mentioned calibration, (23) becomes kt ¼ 0:9986kt1. When rational expectations is
assumed, the resulting law of motion is determined according to (33): kt ¼ 0:9653kt1.
For the adaptive learning algorithm the solution is given by the system (28). In order to
initialize the method, i.e. to ﬁnd /t0 and Rt0, the randomly generated data procedure is
used, the number of preliminary observations being ﬁxed to the minimum required for
the matrix Rt0 to be invertible (for the particular case considered here t0 ¼ 6). Given the
ﬁrst estimates /t0, the law of motion in the ﬁrst period is kt ¼ 0:9098kt1, but, as more
information become available over time, as mentioned, the law of motion is updated.
Fig. 1. Trajectory of capital under different assumptions about expectations formation
1 Note that these values imply that the annual depreciation rate, the annual growth rate and the annual
interest rate are about 10%, 2% and 4% respectively.
196
R.-E. Pop

For all the cases presented, the results are found to be symmetric with respect to the
two different initial values for kt. In case of the adaptive expectations formulation
(AE) the convergence towards the equilibrium is very slow, even though a high cor-
rection factor, i.e. 0.9, is assumed. The strong degree of inertia is difﬁcult to be
reconciled with what is seen in practice. In contrast, the rational expectations (RE) and
the adaptive learning (AL) algorithms exhibit a much faster return to equilibrium, for
this speciﬁc case the recursive learning algorithm generating the fastest convergence
towards the equilibrium.
However, as shown in Fig. 2, the initial values for learning can be very important
for the evolution of the variables of the model: if more preliminary periods are allowed
so that the initial values are closer to the coefﬁcients coming from the rational
expectations solution, the predicted path of kt is much closer to the one under rational
expectations.
6
Conclusions
In this paper an overview of the main theories of expectations formation is provided
and the empirical implications of the different approaches are explored in the context of
the standard growth model.
Expectations play a critical role in macroeconomic models. Since these models
study decisions over many periods, the expectations of economic agents (consumers,
ﬁrms, government, monetary authority) about future economic conditions are essential
Fig. 2. Trajectory of capital under different initial values for the learning algorithm
Insights of Adaptive Learning Approach to Modeling Expectations
197

for the predicted dynamics of the model. How to model these expectations has long
been controversial.
Adaptive expectations became widely-used in the 1950s and 1960s. This approach
assumes that expectations are revised in accordance with the last projection error,
implying that agents do not make an optimal use of the available information set and
thus it is possible that they do systematic errors in the forecasting process. In the 1970s
the rational expectations school challenged these assumption by acknowledging peo-
ple’s ability to change behavior when they expect economic policies to change.
Rational expectations are model-consistent expectations and they are optimal given the
available information set and the model structure. However, it seems unreasonable to
assume that agents know with certainty the parameter values when even the economists
who postulate rational expectations must themselves estimate the parameters. This
suggest that a more plausible view of rationality is that agents act as econometricians
when doing the forecasting about the future state of the economy and they estimate and
continually update the parameter values using the available information, i.e. the
adaptive learning approach.
Using the stochastic growth model, it is shown that the macroeconomic predictions
of the model differ depending on the assumptions made about expectations. The
adaptive expectations formulation implies a high degree of inertia even when a high
correction factor is assumed, evidence that is difﬁcult to be reconciled with what is seen
in practice. In contrast, the rational expectations and the recursive least squares learning
algorithms exhibit a much faster return to the steady-state when a disequilibrium is
considered, being more in line with the empirical evidence. As emphasized in the
Sect. 2 of the paper, there are several reasons why the study of adaptive learning
dynamics may be important in practice: as a micro-foundation for the rational expec-
tations assumption, as a device for selecting among multiple rational expectation
equilibria or for the adaptive learning dynamics themselves (either just the transitional
dynamics or there may be persistent learning dynamics).
The initial values for learning also matters for the dynamics of the model: if more
preliminary periods are allowed so that the initial values are closer to the coefﬁcients
coming from the rational expectations solution, the predicted path of the variables is
much closer to the one under rational expectations.
References
1. Friedman, M.: The permanent income hypothesis. In: A Theory of the Consumption
Function, pp. 20–37. Princeton University Press, Princeton (1957)
2. Țigănescu, E., Roman, M.D.: Macroeconomie. O abordare cantitativă, pp. 277–280.
Economica Publishing House București (2005)
3. Taylor, J.B.: Aggregate dynamics and staggered contracts. J. Polit. Econ. 88, 1–23 (1980)
4. Calvo, G.: Staggered prices in a utility maximising framework. J. Monetary Econ. 12(3),
383–398 (1983)
5. Murarașu, B.: An analysis of inﬂation dynamics based on an empirical improved proxy for
real marginal cost gap in the New Keynesian phillips curve. J. Econ. Bus. Manage. 3(12),
1120–1125 (2015)
198
R.-E. Pop

6. Keynes, J.M.: The General Theory of Employment, Interest and Money. Palgrave Macmillan
Publishing House, London (1936)
7. Cagan, P.: The monetary dynamics of hyperinﬂation. In: Milton, F. (ed.) Studies in the
Quantity Theory of Money. University of Chicago Press, Chicago (1956)
8. Nerlove, M.: Adaptive expectations and cobweb phenomena. Q. J. Econ. 72, 227–240
(1958)
9. Muth, J.F.: Rational expectations and the theory of price movements. Econometrica 29(3),
315–335 (1961)
10. Lucas, R.E.: Expectations and the neutrality of money. J. Econ. Theory 4, 103–124 (1972)
11. Lucas, R.E.: Econometric policy evaluation: a critique. In: Carnegie-Rochester Conference
Series, vol. 1, pp. 19–46 (1976)
12. Sargent, T.J.: Rational expectations, the real rate of interest and the natural rate of
unemployment. Brookings Pap. Econ. Act. 1973(2), 429–480 (1973)
13. Sargent, T.J., Wallace, N.: Some Unpleasant Monetarist Arithmetic. Fed. Reserve Bank
Minneap. Q. Rev. 5(3), 1–17 (1981)
14. Weber, A.: Heterogeneous expectations, learning and European inﬂation dynamics,
Economic Studies Deutsche Bundesbank Research Centre, no. 16 (2010)
15. Łyziak, T.: Inﬂation expectations in Poland, 2001–2013. Measurement and macroeconomic
testing, NBP Working Paper, no. 115 (2013)
16. Evans, G.W., Honkapohja, S.: Learning and Expectations in Macroeconomics. Princeton
University Press, Princeton (2001)
17. Jelea, A.: Modelarea anticipărilor inﬂaționiste. Aspecte empirice pentru România, ABC-ul
Lumii Financiare 1, 409–431 (2013)
18. Slobodyan, S., Wouters, R.: Learning in a medium-scale DSGE model with expectations
based on small forecasting models. Am. Econ. J. Macroecon. 4(2), 65–101 (2012)
19. Carceles-Poveda, E., Giannitsarou, C.: Adaptive learning in practice. J. Econ. Dyn. Control
31, 2659–2697 (2007)
20. McCallum, B.T.: On non-uniqueness in rational expectations models: an attempt at
perspective. J. Monetary Econ. 11, 139–168 (1983)
21. Fernandez Telleria, B.X.: Essays on real business cycle modelling under adaptive learning,
Ph.D. thesis, University of Glasgow (2013)
Insights of Adaptive Learning Approach to Modeling Expectations
199

Author Index
Aldea, Anamaria
149
Bădin, Luiza
175
Băjenaru, Lidia
115
Balan, Adina
138
Bodea, Constanţa-Nicoleta
70
Boldizsar, Razvan Alin
101
Covaci, Florina Livia
23
Danaiata, Doina
55
Dascalu, Maria-Iuliana
70
de Farias, Tarcisio Mendes
38
Dedu, Silvia
175
Fodor, Anca-Georgiana
160
Georgescu, Mircea
84
Georgescu, Simona Roxana
149
Giurgea, Corina
101
Hurbean, Luminita
55
Karagiannis, Dimitris
3
Limbău, Alexandra
149
Lungu, Ion
160
Mogos, Radu Ioan
70
Necula, Sabina-Cristiana
130
Negovan, Ana-Maria
55
Nicolle, Christophe
38
Oprea, Simona-Vasilica
160
Ordean, Mihaela
101
Pîrjan, Alexandru
160
Pop, Raluca-Elena
187
Popescul, Daniela
84
Purnus, Augustin
70
Roxin, Ana
38
Şerban, Florentin
175
Şerban-Oprescu, Anca-Teodora
175
Smeureanu, Ion
115
Soava, Georgeta
138
Tampa, Mircea
149
Tănăsescu, Maria Daniela
149
Tesila, Bianca
70


Machine Learning Specializa0on 
Clustering & 
Retrieval: 
A machine learning perspective 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Part of a specialization 
©2016 Emily Fox & Carlos Guestrin 
2 

Machine Learning Specializa0on 
3 
This course is a part of the 
Machine Learning Specialization 
©2016 Emily Fox & Carlos Guestrin 
1. Foundations 
2. Regression 
3. Classiﬁcation 
4. Clustering 
& Retrieval 
5. Recommender 
Systems 
6. Capstone 

Machine Learning Specializa0on 
What is the course about? 
©2016 Emily Fox & Carlos Guestrin 
4 

Machine Learning Specializa0on 
5 
What is retrieval? 
Search for related items 
©2016 Emily Fox & Carlos Guestrin 
Data 
ML 
Method 
Intelligence 
Nearest 
Neighbor 
Input x,{x’}:  
features for  
query point 
+ 
features of 
all other datapoints 
Output xNN:  
 “nearest” point or  
set of points to query 
Compute 
distances to 
other x’ 
 

Machine Learning Specializa0on 
6 
Retrieve “nearest neighbor” article 
©2016 Emily Fox & Carlos Guestrin  
Space of all articles, 
organized by similarity of text 
 
query article 
nearest neighbor 

Machine Learning Specializa0on 
7 
Or set of nearest neighbors 
©2016 Emily Fox & Carlos Guestrin  
Space of all articles, 
organized by similarity of text 
 
query article 
set of nearest neighbors 

Machine Learning Specializa0on 
8 
Social networks 
(people you might want 
to connect with) 
Retrieval applications 
Just about everything… 
©2016 Emily Fox & Carlos Guestrin 
Streaming content: 
-  Songs 
-  Movies 
-  TV shows 
-  … 
Images 
Products 
News articles 

Machine Learning Specializa0on 
9 
What is clustering? 
Discover groups of similar inputs 
©2016 Emily Fox & Carlos Guestrin 
Data 
ML 
Method 
Intelligence 
Clustering 
Input {x}:  
features for  
points in  
dataset 
Output {z}:  
 cluster labels per 
datapoint 
Separate  
points into  
disjoint sets 
 

Machine Learning Specializa0on 
10 
Case Study: 
Clustering documents by “topic” 
©2015 Emily Fox & Carlos Guestrin 
Data 
ML 
Method 
Intelligence 
Clustering 
SPORTS 
WORLD NEWS 
ENTERTAINMENT 
SCIENCE 

Machine Learning Specializa0on 
11 
Just like retrieval, clustering has applications 
almost everywhere 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
12 
Clustering images 
©2016 Emily Fox & Carlos Guestrin 
 For search, group as: 
- Ocean 
- Pink ﬂower 
- Dog 
- Sunset 
- Clouds 
- … 

Machine Learning Specializa0on 
13 
Or Coursera learners… 
©2016 Emily Fox & Carlos Guestrin 
Discover groups of 
learners for better 
targeting of courses 

Machine Learning Specializa0on 
14 
Impact of retrieval & clustering 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
15 
Impact of retrieval & clustering 
•  Foundational ideas 
•  Lots of information can be extracted using these tools 
(exploring user interests and interpretable structure 
relating groups of users based on observed behavior) 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Course overview 
16 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
17 
Course philosophy: Always use case studies & …  
Core 
concept 
Practical 
Visual 
Implement 
Algorithm 
Advanced 
topics 
©2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specializa0on 
18 
Overview of content 
Models 
Nearest 
neighbors 
Clustering 
Mixture of 
Gaussians 
Latent Dirichlet 
allocation 
Algorithms 
KD-trees 
Locality sensitive 
hashing 
k-means 
MapReduce 
Expectation 
Maximization 
Gibbs sampling 
Core ML 
Distance metrics 
Approximation 
algorithms 
Unsupervised 
learning 
Probabilistic 
modeling 
Data parallel 
problems 
Bayesian 
inference 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Course outline 
19 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
20 
Overview of content 
Models 
Nearest neighbors 
Module 1 
Clustering 
Module 2, 3 
Mixture of Gaussians 
Module 3 
Latent Dirichlet 
allocation 
Module 4 
Algorithms 
KD-trees 
Module 1 
Locality sensitive 
hashing 
Module 1 
k-means 
Module 2 
MapReduce 
Module 2 
Expectation 
Maximization 
Module 3 
Gibbs sampling 
Module 4 
Core ML 
Distance metrics 
Module 1 
Approximation 
algorithms 
Module 1 
Unsupervised 
learning 
Module 2 
Probabilistic 
modeling 
Module 2, 3, 4 
Data parallel 
problems 
Module 2 
Bayesian inference 
Module 4 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
21 
Module 1: Nearest neighbor search 
Reading doc 
and want to 
ﬁnd related  
doc 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
22 
Module 1: Nearest neighbor search 
©2016 Emily Fox & Carlos Guestrin  
Compute distances to all other  
documents and return closest 
 
query article 
Critical elements: 
-  Doc representation 
-  Distance measure 

Machine Learning Specializa0on 
23 
Module 1: Nearest neighbor search 
Eﬃcient and  
approximate  
NN search 
©2016 Emily Fox & Carlos Guestrin 
#awesome 
#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Line 1 
Line 2 
Line 3 
Bin index:  
[0 0 0] 
Bin index:  
[0 1 0] 
Bin index:  
[1 1 0] 
Bin index:  
[1 1 1] 
LSH 
KD-trees 

Machine Learning Specializa0on 
24 
Module 2: k-means and MapReduce 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 2 
Cluster 3 
Cluster 4 
Discover clusters of related documents 
 

Machine Learning Specializa0on 
25 
Module 2: k-means and MapReduce 
©2016 Emily Fox & Carlos Guestrin 
k-means aims to minimize sum of  
square distances to cluster centers 
 
Makes hard assignments 
of data points to clusters 
Unsupervised 
learning task 

Machine Learning Specializa0on 
26 
Module 2: k-means and MapReduce 
©2016 Emily Fox & Carlos Guestrin 
Map Phase 
(k1,v1) 
(k2,v2) 
… 
(k1’,v1’) 
(k2’,v2’) 
… 
(k1’’’,v1’’’) 
(k2’’’,v2’’’) 
… 
Split data  
across machines 
Reduce Phase 
Shuﬄe Phase 
(k1,v1) 
(k2,v2) 
… 
(k3,v3) 
(k4,v4) 
… 
(k5,v5) 
(k6,v6) 
… 
Assign tuple (ki,vi) to  
machine  h[ki] 
M1 
M2 
M1000 
M1 
M2 
M1000 
Big Data 
Parallelize 
operations 
across 
multiple 
machines 

Machine Learning Specializa0on 
Module 3: Mixture Models 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
27 
Cluster 4 
Cluster 2 
Probabilistic clustering model 
captures 
uncertainty 
in clustering 

Machine Learning Specializa0on 
Module 3: Mixture Models 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
28 
Cluster 4 
Cluster 2 
Learn user 
topic 
preferences 

Machine Learning Specializa0on 
29 
Module 3: Mixture Models 
©2016 Emily Fox & Carlos Guestrin 
Assignments of docs to clusters based on  
location and shape, not just location 
Make soft assignments 
of docs to each cluster 

Machine Learning Specializa0on 
30 
Module 3: Mixture Models 
Data 
  
 
 
 
 
 
  
©2016 Emily Fox & Carlos Guestrin 
EM algorithm à 
soft assignments 

Machine Learning Specializa0on 
31 
©2016 Emily Fox & Carlos Guestrin 
0 
1 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
Based on science 
related words, maybe 
doc in cluster 4 
Module 4: Latent Dirichlet Allocation 

Machine Learning Specializa0on 
32 
0 
1 
©2016 Emily Fox & Carlos Guestrin 
Or maybe cluster 2 
(technology) is a better ﬁt 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
Module 4: Latent Dirichlet Allocation 

Machine Learning Specializa0on 
33 
©2016 Emily Fox & Carlos Guestrin 
0 
0.2 
0.4 
0.6 
Really, it’s about science 
and technology 
 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
Module 4: Latent Dirichlet Allocation 

Machine Learning Specializa0on 
34 
Each cluster/topic deﬁned by probability of words in vocab 
©2016 Emily Fox & Carlos Guestrin 
SCIENCE 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TECH 
develop 
0.18 
computer 0.09 
processor 0.032 
user 
0.027 
internet 
0.02 
… 
… 
SPORTS 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
… 
Module 4: Latent Dirichlet Allocation 

Machine Learning Specializa0on 
35 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
TOPIC 1 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 2 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 3 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
Topic vocab 
distributions: 
… 
Document topic 
proportions: 
0 
0.2 
0.4 
0.6 
? ? ? ? 
Unsupervised 
learning task 

Machine Learning Specializa0on 
Assumed background 
36 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
37 
Courses 1, 2, & 3 in this ML Specialization 
• 
Course 1: Foundations 
-  Overview of ML case studies 
-  Black-box view of ML tasks 
-  Programming & data  
manipulation skills 
• 
Course 2: Regression 
-  Data representation (input, output, features) 
-  Basic statistical concepts: mean/variance 
-  Basic ML concepts: 
•  ML algorithm  
•  Coordinate ascent 
•  Overﬁtting 
•  Regularization 
 
• 
Course 3: Classiﬁcation 
-  Distributions and conditional distributions 
-  Maximum likelihood estimation 
-  References to: 
•  Linear classiﬁer 
•  Multiclass classiﬁcation 
•  Boosting 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
38 
Math background 
•  Basic linear algebra 
- Vectors 
- Matrices 
- Matrix multiply 
•  Basic probability 
- Fundamental laws 
- Distribution and  
conditional distribution 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
39 
Programming experience 
•  Basic Python used 
- Can pick up along the way if 
knowledge of other language 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
40 
Reliance on GraphLab Create 
•  SFrames will be used, though not required 
- open source project of Dato  
(creators of GraphLab Create) 
- can use pandas and numpy instead 
•  Assignments will: 
1.  Use GraphLab Create to 
explore high-level concepts 
2.  Ask you to implement most 
algorithms without GraphLab Create 
•  Net result:  
- learn how to code methods in Python 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
41 
Computing needs 
©2015 Emily Fox & Carlos Guestrin 
•  Using your own computer: 
-  Basic desktop or laptop 
•  64-bit required if using SFrame 
-  Access to internet 
-  Ability to: 
•  Install and run Python (and Numpy, GraphLab Create,…) 
•  Store a few GB of data 
•  Will also provide alternative, pre-conﬁgured  
machine in Cloud 

Machine Learning Specializa0on 
Let’s get started! 
42 
©2016 Emily Fox & Carlos Guestrin 



OPERATIONS
RESEARCH
METHODOLOGIES

The Operations Research Series
Series Editor: A. Ravi Ravindran
Dept. of Industrial & Manufacturing Engineering
The Pennsylvania State University, USA
Integer Programming: Theory and Practice
John K. Karlof
Operations Research Applications
A. Ravi Ravindran
Operations Research: A Practical Approach
Michael W. Carter and Camille C. Price
Operations Research Calculations Handbook
Dennis Blumenfeld
Operations Research and Management Science Handbook
A. Ravi Ravindran
Operations Research Methodologies
A. Ravi Ravindran
Forthcoming Titles
Applied Nonlinear Optimization in Modeling Environments
Janos D. Pinter
Operations Research Calculations Handbook, Second Edition
Dennis Blumenfeld
Probability Models in Operations Research
Richard C. Cassady and Joel A. Nachlas

CRC Press is an imprint of the
Taylor & Francis Group, an informa business
Boca Raton  London  New York
EDITED BY A. RAVI RAVINDRAN
OPERATIONS
RESEARCH
METHODOLOGIES

This material was previously published in Operations Research and Management Science Handbook © 2008 by Taylor 
and Francis.
CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2009 by Taylor & Francis Group, LLC 
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-4200-9182-3 (Hardcover)
This book contains information obtained from authentic and highly regarded sources. Reasonable eﬀorts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid-
ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microﬁlming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-proﬁt organization that provides licenses and registration for a variety of users. For orga-
nizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identiﬁcation and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Preface ...................................................................................... ix
Acknowledgments ......................................................................... xi
Editor ..................................................................................... xiii
Contributors .............................................................................. xv
History of Operations Research ....................................................... xvii
1
Linear Programming
Katta G. Murty ...................................................................
1-1
1.1
Brief History of Algorithms for Solving Linear Equations, Linear
Inequalities, and LPs ......................................................
1-1
1.2
Applicability of the LP Model: Classical Examples of
Direct Applications ........................................................
1-4
1.3
LP Models Involving Transformations of Variables ......................
1-12
1.4
Intelligent Modeling Essential to Get Good Results, an Example from
Container Shipping ........................................................
1-18
1.5
Planning Uses of LP Models ..............................................
1-22
1.6
Brief Introduction to Algorithms for Solving LP Models ................
1-26
1.7
Software Systems Available for Solving LP Models ......................
1-31
1.8
Multiobjective LP Models .................................................
1-31
2
Nonlinear Programming
Theodore B. Trafalis and Robin C. Gilbert ......................................
2-1
2.1
Introduction ................................................................
2-1
2.2
Unconstrained Optimization ..............................................
2-3
2.3
Constrained Optimization .................................................
2-15
2.4
Conclusion .................................................................
2-19
3
Integer Programming
Michael Weng .....................................................................
3-1
3.1
Introduction ................................................................
3-1
3.2
Formulation of IP Models .................................................
3-3
3.3
Branch and Bound Method ...............................................
3-7
3.4
Cutting Plane Method .....................................................
3-12
3.5
Other Solution Methods and Computer Solution ........................
3-15
4
Network Optimization
Mehmet Bayram Yildirim ........................................................
4-1
4.1
Introduction ................................................................
4-1
4.2
Notation ....................................................................
4-2
4.3
Minimum Cost Flow Problem .............................................
4-3
v

vi
Contents
4.4
Shortest Path Problem ....................................................
4-4
4.5
Maximum Flow Problem ..................................................
4-8
4.6
Assignment Problem .......................................................
4-13
4.7
Minimum Spanning Tree Problem ........................................
4-14
4.8
Minimum Cost Multicommodity Flow Problem ..........................
4-18
4.9
Conclusions ................................................................
4-19
5
Multiple Criteria Decision Making
Abu S. M. Masud and A. Ravi Ravindran .......................................
5-1
5.1
Some Deﬁnitions ...........................................................
5-3
5.2
The Concept of “Best Solution” ..........................................
5-4
5.3
Criteria Normalization .....................................................
5-5
5.4
Computing Criteria Weights ..............................................
5-6
5.5
Multiple Criteria Methods for Finite
Alternatives ................................................................
5-8
5.6
Multiple Criteria Mathematical Programming Problems ................
5-15
5.7
Goal Programming ........................................................
5-19
5.8
Method of Global Criterion and Compromise Programming ............
5-27
5.9
Interactive Methods .......................................................
5-29
5.10
MCDM Applications .......................................................
5-34
5.11
MCDM Software ...........................................................
5-35
5.12
Further Readings ..........................................................
5-35
6
Decision Analysis
Cerry M. Klein ....................................................................
6-1
6.1
Introduction ................................................................
6-1
6.2
Terminology for Decision Analysis ........................................
6-2
6.3
Decision Making under Risk ..............................................
6-3
6.4
Decision Making under Uncertainty ......................................
6-17
6.5
Practical Decision Analysis ................................................
6-21
6.6
Conclusions ................................................................
6-28
6.7
Resources ...................................................................
6-29
7
Dynamic Programming
Jos´e A. Ventura ...................................................................
7-1
7.1
Introduction ................................................................
7-1
7.2
Deterministic Dynamic Programming Models ............................
7-3
7.3
Stochastic Dynamic Programming Models ...............................
7-19
7.4
Conclusions ................................................................
7-24
8
Stochastic Processes
Susan H. Xu
......................................................................
8-1
8.1
Introduction ................................................................
8-1
8.2
Poisson Processes ..........................................................
8-7
8.3
Discrete-Time Markov Chains .............................................
8-14
8.4
Continuous-Time Markov Chains .........................................
8-27
8.5
Renewal Theory ............................................................
8-39

Contents
vii
8.6
Software Products Available for Solving
Stochastic Models ..........................................................
8-46
9
Queueing Theory
Natarajan Gautam ................................................................
9-1
9.1
Introduction ................................................................
9-1
9.2
Queueing Theory Basics ...................................................
9-2
9.3
Single-Station and Single-Class Queues ...................................
9-8
9.4
Single-Station and Multiclass Queues .....................................
9-21
9.5
Multistation and Single-Class Queues ....................................
9-28
9.6
Multistation and Multiclass Queues ......................................
9-34
9.7
Concluding Remarks .......................................................
9-37
10
Inventory Control
Farhad Azadivar and Atul Rangarajan ...........................................
10-1
10.1
Introduction ................................................................
10-1
10.2
Design of Inventory Systems ..............................................
10-4
10.3
Deterministic Inventory Systems ..........................................
10-9
10.4
Stochastic Inventory Systems ............................................. 10-20
10.5
Inventory Control at Multiple Locations ................................. 10-27
10.6
Inventory Management in Practice ....................................... 10-34
10.7
Conclusions ................................................................ 10-35
10.8
Current and Future Research ............................................. 10-36
11
Complexity and Large-Scale Networks
Hari P. Thadakamalla, Soundar R.T. Kumara, and R´eka Albert ...............
11-1
11.1
Introduction ................................................................
11-1
11.2
Statistical Properties of Complex Networks ..............................
11-6
11.3
Modeling of Complex Networks ........................................... 11-11
11.4
Why “Complex” Networks ................................................ 11-16
11.5
Optimization in Complex Networks ....................................... 11-18
11.6
Conclusions ................................................................ 11-26
12
Simulation
Catherine M. Harmonosky ........................................................
12-1
12.1
Introduction ................................................................
12-1
12.2
Basics of Simulation .......................................................
12-3
12.3
Simulation Languages and Software ...................................... 12-15
12.4
Simulation Projects—The Bigger Picture ................................ 12-20
12.5
Summary ................................................................... 12-22
13
Metaheuristics for Discrete Optimization Problems
Rex K. Kincaid ....................................................................
13-1
13.1
Mathematical Framework for Single Solution Metaheuristics ............
13-3
13.2
Network Location Problems ...............................................
13-3
13.3
Multistart Local Search ...................................................
13-5
13.4
Simulated Annealing .......................................................
13-6
13.5
Plain Vanilla Tabu Search .................................................
13-8
13.6
Active Structural Acoustic Control (ASAC) ............................. 13-10
13.7
Nature Reserve Site Selection ............................................. 13-13

viii
Contents
13.8
Damper Placement in Flexible Truss Structures ......................... 13-21
13.9
Reactive Tabu Search ...................................................... 13-29
13.10 Discussion .................................................................. 13-35
14
Robust Optimization
H. J. Greenberg and Tod Morrison ...............................................
14-1
14.1
Introduction ................................................................
14-1
14.2
Classical Models ...........................................................
14-2
14.3
Robust Optimization Models .............................................. 14-10
14.4
More Applications ......................................................... 14-16
14.5
Summary ................................................................... 14-30
Index ...................................................................................... I-1

Preface
Operations research (OR), which began as an interdisciplinary activity to solve complex
problems in the military during World War II, has grown in the past 50 years to a full-
ﬂedged academic discipline. Now OR is viewed as a body of established mathematical models
and methods to solve complex management problems. OR provides a quantitative analy-
sis of the problem from which the management can make an objective decision. OR has
drawn upon skills from mathematics, engineering, business, computer science, economics,
and statistics to contribute to a wide variety of applications in business, industry, govern-
ment, and military. OR methodologies and their applications continue to grow and ﬂourish
in a number of decision-making ﬁelds.
The objective of this book is to provide a comprehensive overview of OR models and
methods in a single volume. This book is not an OR textbook or a research monograph.
The intent is that the book becomes the ﬁrst resource a practitioner would reach for when
faced with an OR problem or question. The key features of this book are as follows:
• Single source guide to OR techniques
• Comprehensive resource, but concise
• Coverage of emerging OR methodologies
• Quick reference guide to students, researchers, and practitioners
• Bridges theory and practice
• References to computer software availability
• Designed and edited with nonexperts in mind
• Uniﬁed and up-to-date coverage ideal for ready reference
This book contains 14 chapters that cover not only the fundamental OR models and
methods such as linear, nonlinear, integer and dynamic programming, networks, simula-
tion, queueing, inventory, stochastic processes, and decision analysis, but also emerging OR
techniques such as multiple criteria optimization, metaheuristics, robust optimization, and
complexity and large-scale networks. Each chapter gives an overview of a particular OR
methodology, illustrates successful applications, and provides references to computer soft-
ware availability. Each chapter in this book is written by leading authorities in the ﬁeld and
is devoted to a topic listed as follows:
• Linear programming
• Nonlinear programming
• Integer programming
• Network optimization
• Multiple criteria decision making
• Decision analysis
• Dynamic programming
• Stochastic processes
• Queueing theory
• Inventory control
• Complexity and large-scale networks
ix

x
Preface
• Simulation
• Metaheuristics
• Robust optimization
This book will be an ideal reference book for OR practitioners in business, industry,
government, and academia. It can also serve as a supplemental text in undergraduate and
graduate OR courses in the universities. Readers may also be interested in the companion
book titled Operations Research Applications, which contains both functional and industry-
speciﬁc applications of the OR methodologies discussed here.
A. Ravi Ravindran
University Park, Pennsylvania

Acknowledgments
First and foremost I would like to thank the authors, who have worked diligently in writing
the various handbook chapters that are comprehensive, concise, and easy to read, bridging
the gap between theory and practice. The development and evolution of this handbook
have also beneﬁted substantially from the advice and counsel of my colleagues and friends
in academia and industry, who are too numerous to acknowledge individually. They helped
me identify the key topics to be included in the handbook, suggested chapter authors, and
served as reviewers of the manuscripts.
I express my sincere appreciation to Atul Rangarajan, an industrial engineering doc-
toral student at Penn State University, for serving as my editorial assistant and for his
careful review of the page proofs returned by the authors. Several other graduate stu-
dents also helped me with the handbook work, in particular, Ufuk Bilsel, Ajay Natarajan,
Richard Titus, Vijay Wadhwa, and Tao Yang. Special thanks go to Professor Prabha
Sharma at the Indian Institute of Technology, Kanpur, for her careful review of several
chapter manuscripts. I also acknowledge the pleasant personality and excellent typing skills
of Sharon Frazier during the entire book project.
I thank Cindy Carelli, Senior Acquisitions Editor, and Jessica Vakili, project coordinator
at CRC Press, for their help from inception to publication of the handbook. Finally, I wish
to thank my dear wife, Bhuvana, for her patience, understanding, and support when I was
focused completely on the handbook work.
A. Ravi Ravindran
xi


Editor
A. Ravi Ravindran, Ph.D., is a professor and the past department head of Industrial
and Manufacturing Engineering at the Pennsylvania State University. Formerly, he was a
faculty member at the School of Industrial Engineering at Purdue University for 13 years
and at the University of Oklahoma for 15 years. At Oklahoma, he served as the director of
the School of Industrial Engineering for 8 years and as the associate provost of the university
for 7 years, with responsibility for budget, personnel, and space for the academic area. He
holds a B.S. in electrical engineering with honors from the Birla Institute of Technology and
Science, Pilani, India. His graduate degrees are from the University of California, Berkeley,
where he received an M.S. and a Ph.D. in industrial engineering and operations research.
Dr. Ravindran’s area of specialization is operations research with research interests in
multiple criteria decision-making, ﬁnancial engineering, health planning, and supply chain
optimization. He has published two major textbooks (Operations Research: Principles and
Practice and Engineering Optimization: Methods and Applications) and more than 100 jour-
nal articles on operations research. He is a fellow of the Institute of Industrial Engineers. In
2001, he was recognized by the Institute of Industrial Engineers with the Albert G. Holzman
Distinguished Educator Award for signiﬁcant contributions to the industrial engineering
profession by an educator. He has won several Best Teacher awards from IE students. He
has been a consultant to AT&T, General Motors, General Electric, IBM, Kimberly Clark,
Cellular Telecommunication Industry Association, and the U.S. Air Force. He currently
serves as the Operations Research Series editor for Taylor & Francis/CRC Press.
xiii


Contributors
R´eka Albert
Pennsylvania State University
University Park, Pennsylvania
Farhad Azadivar
University of Massachusetts–Dartmouth
North Dartmouth, Massachusetts
Natarajan Gautam
Texas A&M University
College Station, Texas
Robin C. Gilbert
University of Oklahoma
Norman, Oklahoma
H. J. Greenberg
University of Colorado at Denver
and
Health Sciences Center
Denver, Colorado
Catherine M. Harmonosky
Pennsylvania State University
University Park, Pennsylvania
Rex K. Kincaid
College of William and Mary
Williamsburg, Virginia
Cerry M. Klein
University of Missouri–Columbia
Columbia, Missouri
Soundar R. T. Kumara
Pennsylvania State University
University Park, Pennsylvania
Abu S. M. Masud
Wichita State University
Wichita, Kansas
Tod Morrison
University of Colorado at Denver
and
Health Sciences Center
Denver, Colorado
Katta G. Murty
University of Michigan
Ann Arbor, Michigan
Atul Rangarajan
Pennsylvania State University
University Park, Pennsylvania
A. Ravi Ravindran
Pennsylvania State University
University Park, Pennsylvania
Hari P. Thadakamalla
Pennsylvania State University
University Park, Pennsylvania
Theodore B. Trafalis
University of Oklahoma
Norman, Oklahoma
Jos´e A. Ventura
Pennsylvania State University
University Park, Pennsylvania
Michael Weng
University of South Florida
Tampa, Florida
Susan H. Xu
Pennsylvania State University
University Park, Pennsylvania
Mehmet Bayram Yildirim
Wichita State University
Wichita, Kansas
xv


History of Operations
Research
A. Ravi Ravindran
Pennsylvania State University
Origin of Operations Research
To understand what operations research (OR) is today, one must know something of its
history and evolution. Although particular models and techniques of OR can be traced
back to much earlier origins, it is generally agreed that the discipline began during World
War II. Many strategic and tactical problems associated with the Allied military eﬀort were
simply too complicated to expect adequate solutions from any one individual, or even a
single discipline. In response to these complex problems, groups of scientists with diverse
educational backgrounds were assembled as special units within the armed forces. These
teams of scientists started working together, applying their interdisciplinary knowledge and
training to solve such problems as deployment of radars, anti-aircraft ﬁre control, deploy-
ment of ships to minimize losses from enemy submarines, and strategies for air defense.
Each of the three wings of Britain’s armed forces had such interdisciplinary research teams
working on military management problems. As these teams were generally assigned to the
commanders in charge of military operations, they were called operational research (OR)
teams. The nature of their research came to be known as operational research or operations
research.
The work of these OR teams was very successful and their solutions were eﬀective in
military management. This led to the use of such scientiﬁc teams in other Allied nations,
in particular the United States, France, and Canada. At the end of the war, many of the
scientists who worked in the military operational research units returned to civilian life in
universities and industries. They started applying the OR methodology to solve complex
management problems in industries. Petroleum companies were the ﬁrst to make use of
OR models for solving large-scale production and distribution problems. In the universities
advancements in OR techniques were made that led to the further development and appli-
cations of OR. Much of the postwar development of OR took place in the United States.
An important factor in the rapid growth of operations research was the introduction of
electronic computers in the early 1950s. The computer became an invaluable tool to the
operations researchers, enabling them to solve large problems in the business world.
The Operations Research Society of America (ORSA) was formed in 1952 to serve the
professional needs of these operations research scientists. Due to the application of OR in
industries, a new term called management science (MS) came into being. In 1953, a national
society called The Institute of Management Sciences (TIMS) was formed in the United
States to promote scientiﬁc knowledge in the understanding and practice of management.
The journals of these two societies, Operations Research and Management Science, as well
as the joint conferences of their members, helped to draw together the many diverse results
into some semblance of a coherent body of knowledge. In 1995, the two societies, ORSA
and TIMS, merged to form the Institute of Operations Research and Management Sciences
(INFORMS).
xvii

xviii
History of Operations Research
Another factor that accelerated the growth of operations research was the introduction
of OR/MS courses in the curricula of many universities and colleges in the United States.
Graduate programs leading to advanced degrees at the master’s and doctorate levels were
introduced in major American universities. By the mid-1960s many theoretical advances
in OR techniques had been made, which included linear programming, network analysis,
integer programming, nonlinear programming, dynamic programming, inventory theory,
queueing theory, and simulation. Simultaneously, new applications of OR emerged in service
organizations such as banks, health care, communications, libraries, and transportation. In
addition, OR came to be used in local, state, and federal governments in their planning and
policy-making activities.
It is interesting to note that the modern perception of OR as a body of established models
and techniques—that is, a discipline in itself—is quite diﬀerent from the original concept of
OR as an activity, which was preformed by interdisciplinary teams. An evolution of this kind
is to be expected in any emerging ﬁeld of scientiﬁc inquiry. In the initial formative years,
there are no experts, no traditions, no literature. As problems are successfully solved, the
body of speciﬁc knowledge grows to a point where it begins to require specialization even
to know what has been previously accomplished. The pioneering eﬀorts of one generation
become the standard practice of the next. Still, it ought to be remembered that at least a
portion of the record of success of OR can be attributed to its ecumenical nature.
Meaning of Operations Research
From the historical and philosophical summary just presented, it should be apparent that the
term “operations research” has a number of quite distinct variations of meaning. To some,
OR is that certain body of problems, techniques, and solutions that has been accumulated
under the name of OR over the past 50 years and we apply OR when we recognize a prob-
lem of that certain genre. To others, it is an activity or process, which by its very nature is
applied. It would also be counterproductive to attempt to make distinctions between “oper-
ations research” and the “systems approach.” For all practical purposes, they are the same.
How then can we deﬁne operations research? The Operational Research Society of Great
Britain has adopted the following deﬁnition:
Operational research is the application of the methods of science to com-
plex problems arising in the direction and management of large systems of men,
machines, materials and money in industry, business, government, and defense.
The distinctive approach is to develop a scientiﬁc model of the system, incor-
porating measurement of factors such as chance and risk, with which to predict
and compare the outcomes of alternative decisions, strategies or controls. The
purpose is to help management determine its policy and actions scientiﬁcally.
The Operations Research Society of America has oﬀered a shorter, but similar,
description:
Operations research is concerned with scientiﬁcally deciding how to best design
and operate man–machine systems, usually under conditions requiring the allo-
cation of scarce resources.
In general, most of the deﬁnitions of OR emphasize its methodology, namely its unique
approach to problem solving, which may be due to the use of interdisciplinary teams or
due to the application of scientiﬁc and mathematical models. In other words, each prob-
lem may be analyzed diﬀerently, though the same basic approach of operations research is
employed. As more research went into the development of OR, the researchers were able to

History of Operations Research
xix
classify to some extent many of the important management problems that arise in practice.
Examples of such problems are those relating to allocation, inventory, network, queuing,
replacement, scheduling, and so on. The theoretical research in OR concentrated on devel-
oping appropriate mathematical models and techniques for analyzing these problems under
diﬀerent conditions. Thus, whenever a management problem is identiﬁed as belonging to a
particular class, all the models and techniques available for that class can be used to study
that problem. In this context, one could view OR as a collection of mathematical mod-
els and techniques to solve complex management problems. Hence, it is very common to
ﬁnd OR courses in universities emphasizing diﬀerent mathematical techniques of operations
research such as mathematical programming, queueing theory, network analysis, dynamic
programming, inventory models, simulation, and so on.
For more on the early activities in operations research, see Refs. 1–5. Readers interested
in the timeline of major contributions in the history of OR/MS are referred to the excellent
review article by Gass [6].
References
1. Haley, K.B., War and peace: the ﬁrst 25 years of OR in Great Britain, Operations Research,
50, Jan.–Feb. 2002.
2. Miser, H.J., The easy chair: what OR/MS workers should know about the early formative
years of their profession, Interfaces, 30, March–April 2000.
3. Trefethen, F.N., A history of operations research, in Operations Research for Management,
J.F. McCloskey and F.N. Trefethen, Eds., Johns Hopkins Press, Baltimore, MD, 1954.
4. Horner, P., History in the making, ORMS Today, 29, 30–39, 2002.
5. Ravindran, A., Phillips, D.T., and Solberg, J.J., Operations Research: Principles and Prac-
tice, Second Edition, John Wiley & Sons, New York, 1987 (Chapter 1).
6. Gass, S.I., Great moments in histORy, ORMS Today, 29, 31–37, 2002.


1
Linear Programming
Katta G. Murty
University of Michigan
1.1
Brief History of Algorithms for Solving Linear
Equations, Linear Inequalities, and LPs ...........
1-1
1.2
Applicability of the LP Model: Classical
Examples of Direct Applications ....................
1-4
Product Mix Problems • Blending Problems • The Diet
Problem • The Transportation Model • Multiperiod
Production Planning, Storage, Distribution
Problems
1.3
LP Models Involving Transformations of
Variables................................................ 1-12
Min–Max, Max–Min Problems • Minimizing Positive
Linear Combinations of Absolute Values
of Aﬃne Functions
1.4
Intelligent Modeling Essential to Get Good
Results, an Example from Container Shipping.... 1-18
1.5
Planning Uses of LP Models......................... 1-22
Finding the Optimum Solutions • Infeasibility Analysis
• Values of Slack Variables at an Optimum Solution •
Marginal Values, Dual Variables, and the Dual
Problem, and Their Planning Uses • Evaluating the
Proﬁtability of New Products
1.6
Brief Introduction to Algorithms for Solving
LP Models.............................................. 1-26
The Simplex Method • Interior Point Methods for LP
1.7
Software Systems Available for Solving LP
Models .................................................. 1-31
1.8
Multiobjective LP Models ........................... 1-31
References .................................................... 1-34
1.1
Brief History of Algorithms for Solving Linear
Equations, Linear Inequalities, and LPs
The study of mathematics originated with the construction of linear equation models for real
world problems several thousand years ago. As an example we discuss an application that
leads to a model involving a system of simultaneous linear equations from Murty (2004).
1-1

1-2
Operations Research Methodologies
Example 1.1: Scrap Metal Blending Problem
A steel company has four diﬀerent types of scrap metal (SM-1 to SM-4) with the following
compositions (Table 1.1).
The company needs to blend these four scrap metals into a mixture for which the com-
position by weight is: Al—4.43%, Si—3.22%, C—3.89%, and Fe—88.46%. How should they
prepare this mixture? To answer this question, we need to determine the proportions of the
four scrap metals SM-1 to SM-4 in the blend to be prepared.
■
The most fundamental idea in mathematics that was discovered more than 5000 years ago
by the Chinese, Indians, Iranians, Babylonians, and Greeks is to represent the quantities
that we wish to determine by symbols, usually letters of the alphabet like x, y, z, and then
express the relationships between the quantities represented by these symbols in the form
of equations, and ﬁnally use these equations as tools to ﬁnd out the true values represented
by the symbols. The symbols representing the unknown quantities to be determined are
nowadays called unknowns or variables or decision variables. The process of representing
the relationships between the variables through equations or other functional relationships
is called modeling or mathematical modeling.
This process gradually evolved into algebra, one of the chief branches of mathematics.
Even though the subject originated more than 5000 years ago, the name algebra itself
came much later; it is derived from the title of an Arabic book Al-Maqala ﬁHisab al-jabr
w’almuqabalah written by Al-Khawarizmi around 825 AD. The term “al-jabr” in Arabic
means “restoring” in the sense of solving an equation. In Latin translation the title of this
book became Ludus Algebrae, the second word in this title surviving as the modern word
“algebra” for the subject, and Al-Khawarizmi is regarded as the father of algebra. The
earliest algebraic systems constructed are systems of linear equations.
In the scrap metal blending problem, the decision variables are: xj = proportion of SM-j
by weight in the mixture, for j = 1–4. Then the percentage by weight of the element Al in
the mixture will be 5x1 + 7x2 + 2x3 + x4, which is required to be 4.43. Arguing the same
way for the elements Si, C, and Fe, we ﬁnd that the decision variables x1 to x4 must satisfy
each equation in the following system of linear equations to lead to the desired mixture:
5x1 + 7x2 + 2x3 + x4 = 4.43
3x1 + 6x2 + x3 + 2x4 = 3.22
4x1 + 5x2 + 3x3 + x4 = 3.89
88x1 + 82x2 + 94x3 + 96x4 = 88.46
x1 + x2 + x3 + x4 = 1
The last equation in the system stems from the fact that the sum of the proportions of
various ingredients in a blend must always be equal to 1. This system of equations is the
mathematical model for our scrap metal blending problem; it consists of ﬁve equations
TABLE 1.1
Scrap Metal Composition Data
% of Element by Weight, in Type
Type
Al
Si
C
Fe
SM-1
5
3
4
88
SM-2
7
6
5
82
SM-3
2
1
3
94
SM-4
1
2
1
96

Linear Programming
1-3
in four variables. It is clear that a solution to this system of equations makes sense for
the blending application only if all the variables in the system have nonnegative values
in it. The nonnegativity restrictions on the variables are linear inequality constraints. They
cannot be expressed in the form of linear equations, and as nobody knew how to handle
linear inequalities at that time, they ignored them.
Linear algebra dealing with methods for solving systems of linear equations is the classical
subject that initiated the study of mathematics a long time ago. The most eﬀective method
for solving systems of linear equations, called the elimination method, was discovered by the
Chinese and the Indians over 2500 years ago and this method is still the leading method in
use today. This elimination method was unknown in Europe until the nineteenth century
when the German mathematician Gauss rediscovered it while calculating the orbit of the
asteroid Ceres based on recorded observations in tracking it. The asteroid was lost from
view when the Sicilian astronomer Piazzi tracking it fell ill. Gauss used the method of least
squares to estimate the values of the parameters in the formula for the orbit. It led to a
system of 17 linear equations in 17 unknowns that he had to solve, which is quite a large
system for mental computation. Gauss’s accurate computations helped in relocating the
asteroid in the skies in a few months’ time, and his reputation as a mathematician soared.
Another German, Wilhelm Jordan, popularized the algorithm in a late nineteenth-century
book that he wrote. From that time, the method has been popularly known as the Gauss–
Jordan elimination method. Another version of this method, called the Gaussian elimination
method, is the most popular method for solving systems of linear equations today.
Even though linear equations were resolved thousands of years ago, systems of linear
inequalities remained unsolved until the middle of the twentieth century. The following
theorem (Murty, 2006) relates systems of linear inequalities to systems of linear equations.
THEOREM 1.1
Consider the system of linear inequalities in variables x
Ai.x ≥bi,
i = 1, . . ., m
(1.1)
where Ai. is the coeﬃcient vector for the i-th constraint. If this system has a feasible solution,
then there exists a subset P = {p1, . . ., ps} ⊂{1, . . ., m} such that every solution of the system
of equations: Ai.x = bi, i ∈P is also a feasible solution of the original system of linear
inequalities (Equation 1.1).
This theorem can be used to generate a ﬁnite enumerative algorithm to ﬁnd a feasible
solution to a system of linear constraints containing inequalities, based on solving subsys-
tems in each of which a subset of the inequalities are converted into equations and the other
inequality constraints are eliminated. However, if the original system has m inequality con-
straints, in the worst case this enumerative algorithm may have to solve 2m systems of linear
equations before it either ﬁnds a feasible solution of the original system, or concludes that
it is infeasible. The eﬀort required grows exponentially with the number of inequalities in
the system in the worst case.
In the nineteenth century, Fourier generalized the classical elimination method for solving
linear equations into an elimination method for solving systems of linear inequalities. The
method called Fourier elimination, or the Fourier–Motzkin elimination method, is very
elegant theoretically. However, the elimination of each variable adds new inequalities to the
remaining system, and the number of these new inequalities grows exponentially as more
and more variables are eliminated. So this method is also not practically viable for large
problems.

1-4
Operations Research Methodologies
The simplex method for linear programming developed by Dantzig (1914–2005) in the
mid-twentieth century (Dantzig, 1963) is the ﬁrst practically and computationally viable
method for solving systems of linear inequalities. This has led to the development of linear
programming (LP), a branch of mathematics developed in the twentieth century as an
extension of linear algebra to solve systems of linear inequalities. The development of LP is
a landmark event in the history of mathematics and its applications that brought our ability
to solve general systems of linear constraints (including linear equations, inequalities) to a
state of completion.
A general system of linear constraints in decision variables x = (x1, . . ., xn)T is of the form:
Ax ≥b, Dx = d, where the coeﬃcient matrices A, D are given matrices of orders m × n,
p × n, respectively. The inequality constraints in this system may include sign restrictions
or bounds on individual variables.
A general LP is the problem of ﬁnding an optimum solution for the problem of minimizing
(or maximizing) a given linear objective function z = cx say, subject to a system of linear
constraints.
Suppose there is no objective function to optimize, and only a feasible solution of a system
of linear constraints is to be found. When there are inequality constraints in the system, the
only practical method to even ﬁnding a feasible solution is to solve a linear programming
formulation of it as a Phase I linear programming problem. Dantzig developed this Phase I
formulation as part of the simplex method for LPs that he developed in the mid-twentieth
century.
1.2
Applicability of the LP Model: Classical
Examples of Direct Applications
LP has now become a dominant subject in the development of eﬃcient computational
algorithms, the study of convex polyhedra, and in algorithms for decision making. But
for a short time in the beginning, its potential was not well recognized. Dantzig tells the
story of how when he gave his ﬁrst talk on LP and his simplex method for solving it at a
professional conference, Hotelling (a burly person who liked to swim in the sea; the popular
story about him was that when he does, the level of the ocean rises perceptibly) dismissed
it as unimportant since everything in the world is nonlinear. But Von Neumann came to
the defense of Dantzig saying that the subject will become very important (Dantzig and
Thapa, 1997, vol. 1, p. xxvii). The preface in this book contains an excellent account of
the early history of LP from the inventor of the most successful method in OR and in the
mathematical theory of polyhedra.
Von Neumann’s early assessment of the importance of LP turned out to be astonishingly
correct. Today, the applications of LP in almost all areas of science are numerous. The LP
model is suitable for modeling a real world decision-making problem if
• All the decision variables are continuous variables
• There is a single objective function that is required to be optimized
• The objective function and all the constraint functions deﬁning the constraints
in the problem are linear functions of the decision variables (i.e., they satisfy the
usual proportionality and additivity assumptions)
There are many applications in which the reasonableness of the linearity assumptions can
be veriﬁed and an LP model for the problem constructed by direct arguments. We present
some classical applications like this in this section; this material is from Murty (1995, 2005b).

Linear Programming
1-5
In all these applications you can judge intuitively that the assumptions needed to handle
them using an LP model are satisﬁed to a reasonable degree of approximation.
Of course LP can be applied to a much larger class of problems. Many important applica-
tions involve optimization models in which a nonlinear objective function that is piecewise
linear and convex is to be minimized subject to linear constraints. These problems can
be transformed into LPs by introducing additional variables. These transformations are
discussed in the next section.
1.2.1
Product Mix Problems
This is an extremely important class of problems that manufacturing companies face.
Normally the company can make a variety of products using the raw materials, machinery,
labor force, and other resources available to them. The problem is to decide how much
of each product to manufacture in a period, to maximize the total proﬁt subject to the
availability of needed resources.
To model this, we need data on the units of each resource necessary to manufacture one
unit of each product, any bounds (lower, upper, or both) on the amount of each product
manufactured per period, any bounds on the amount of each resource available per period,
the expected demand for each product, and the cost or net proﬁt per unit of each product
manufactured.
Assembling this type of reliable data is one of the most diﬃcult jobs in constructing a
product mix model for a company, but it is very worthwhile. The process of assembling
all the needed data is sometimes called the input–output analysis of the company. The
coeﬃcients, which are the resources necessary to make a unit of each product, are called
input–output (I/O) coeﬃcients, or technology coeﬃcients.
Example 1.2: The Fertilizer Product Mix Problem
As an example, consider a fertilizer company that makes two kinds of fertilizers called
Hi-phosphate (Hi-ph) and Lo-phosphate (Lo-ph). The manufacture of these fertilizers
requires three raw materials called RM 1, RM 2, and RM 3. At present their supply of
these raw materials comes from the company’s own quarry that is only able to supply max-
imum amounts of 1500, 1200, 500 tons/day, respectively, of RM 1, RM 2, and RM 3. Even
though there are other vendors who can supply these raw materials if necessary, at the
moment they are not using these outside suppliers.
They sell their output of Hi-ph and Lo-ph fertilizers to a wholesaler who is willing to buy
any amount that they can produce, so there are no upper bounds on the amounts of Hi-ph
and Lo-ph manufactured daily.
At the present rates of operation their Cost Accounting Department estimates that it is
costing the quarry $50, $40, and $60/ton respectively to produce and deliver RM 1, RM 2,
and RM 3 at the fertilizer plant. Also, at the present rates of operation, all other production
costs (for labor, power, water, maintenance, depreciation of plant and equipment, ﬂoor
space, insurance, shipping to the wholesaler, etc.) come to $7/ton to manufacture Hi-ph or
Lo-ph and deliver to the wholesaler.
The sale price of the manufactured fertilizers to the wholesaler ﬂuctuates daily, but their
averages over the last one month have been $222 and $107 per ton, respectively, for Hi-Ph
and Lo-ph fertilizers. We will use these prices to construct the mathematical model.
■
The Hi-ph manufacturing process needs as inputs 2 tons RM 1 and 1 ton each of RM 2
and RM 3 for each ton of Hi-ph manufactured. Similarly, the Lo-ph manufacturing process
needs as inputs 1 ton RM 1 and 1 ton of RM 2 for each ton of Lo-ph manufactured.

1-6
Operations Research Methodologies
So, the net proﬁt/ton of fertilizer manufactured is $(222 −2 × 50 −1 × 40 −1 × 60 −7) = 15
and (107 −1 × 50 −1 × 40 −7) = 10, respectively, for Hi-ph and Lo-ph.
There are clearly two decision variables in this problem; these are: x1 = the tons of Hi-ph
produced per day, x2 the tons of Lo-ph produced per day. Associated with each variable
in the problem is an activity that the decision maker can perform. The activities in this
example are: Activity 1: to make 1 ton of Hi-ph, Activity 2: to make 1 ton of Lo-ph. The
variables in the problem just deﬁne the levels at which these activities are carried out.
As all the data are given on a per ton basis, they provide an indication that the linearity
assumptions are quite reasonable in this problem. Also, the amount of each fertilizer man-
ufactured can vary continuously within its present range. So, LP is an appropriate model
for this problem.
Each raw material leads to a constraint in the model. The amount of RM 1 used is
2x1 + x2 tons, and it cannot exceed 1500, leading to the constraint 2x1 + x2 ≤1500. As
this inequality compares the amount of RM 1 used to the amount available, it is called a
material balance inequality. All goods that lead to constraints in the model for the problem
are called items. The material balance equations or inequalities corresponding to the various
items are the constraints in the problem. When the objective function and all the constraints
are obtained, the formulation of the problem as an LP is complete. The LP formulation of
the fertilizer product mix problem is given below.
Maximize p(x) = 15x1+ 10x2
Item
subject to
2x1+
x2 ≤1500
RM 1
x1+
x2 ≤1200
RM 2
x1
≤500
RM 3
x1≥0, x2 ≥
0
(1.2)
Real world product mix models typically involve large numbers of variables and con-
straints, but their structure is similar to that in this small example.
1.2.2
Blending Problems
This is another large class of problems in which LP is applied heavily. Blending is con-
cerned with mixing diﬀerent materials called the constituents of the mixture (these may
be chemicals, gasolines, fuels, solids, colors, foods, etc.) so that the mixture conforms to
speciﬁcations on several properties or characteristics.
To model a blending problem as an LP, the linear blending assumption must hold for each
property or characteristic. This implies that the value for a characteristic of a mixture is
the weighted average of the values of that characteristic for the constituents in the mixture,
the weights being the proportions of the constituents. As an example, consider a mixture
consisting of four barrels of fuel 1 and six barrels of fuel 2, and suppose the characteristic
of interest is the octane rating (Oc.R). If linear blending assumption holds, the Oc.R of the
mixture will be equal to (4 times the Oc.R of fuel 1 + 6 times the Oc.R of fuel 2)/(4 + 6).
The linear blending assumption holds to a reasonable degree of precision for many impor-
tant characteristics of blends of gasolines, crude oils, paints, foods, and so on. This makes it
possible for LP to be used extensively in optimizing gasoline blending, in the manufacture
of paints, cattle feed, beverages, and so on.
The decision variables in a blending problem are usually either the quantities or the
proportions of the constituents in the blend. If a speciﬁed quantity of the blend needs to
be made, then it is convenient to take the decision variables to be the quantities of the

Linear Programming
1-7
various constituents blended; in this case one must include the constraint that the sum of
the quantities of the constituents is equal to the quantity of the blend desired.
If there is no restriction on the amount of blend made, but the aim is to ﬁnd an optimum
composition for the mixture, it is convenient to take the decision variables to be the propor-
tions of the various constituents in the blend; in this case one must include the constraint
that the sum of all these proportions is 1.
We provide a gasoline blending example. There are more than 300 reﬁneries in the United
States processing a total of more than 20 million barrels of crude oil daily. Crude oil is a
complex mixture of chemical components. The reﬁning process separates crude oil into its
components that are blended into gasoline, fuel oil, asphalt, jet fuel, lubricating oil, and
many other petroleum products. Reﬁneries and blenders strive to operate at peak economic
eﬃciencies, taking into account the demand for various products. To keep the example sim-
ple, we consider only one characteristic of the mixture, the Oc.R of the blended fuels in this
example. In actual application there are many other characteristics to be considered also.
A reﬁnery takes four raw gasolines and blends them to produce three types of fuel. The
company sells raw gasoline not used in making fuels at $38.95/barrel if its Oc.R is >90,
and at $36.85/barrel if its Oc.R is ≤90. The cost of handling raw gasolines purchased and
blending them into fuels or selling them as is is estimated to be $2 per barrel by the Cost
Accounting Department. Other data are given in Table 1.2.
The problem is to determine how much raw gasoline of each type to purchase, the blend
to use for the three fuels, and the quantities of these fuels to make to maximize total daily
net proﬁt.
We will use the quantities of the various raw gasolines in the blend for each fuel as the
decision variables, and we assume that the linear blending assumption holds for the Oc.R.
Let
RGi = raw gasoline type i to purchase/day,
i = 1–4
xij =

barrels of raw gasoline type i used in making fuel
type j per day, i = 1 to 4,
j = 1, 2, 3
yi = barrels of raw gasoline type i sold as is/day
Fj = barrels of fuel type j made/day,
j = 1, 2, 3
So, the total amount of fuel type 1 made daily is F1 = x11 + x21 + x31 + x41. If this is >0, by
the linear blending assumption its Oc.R will be (68x11 + 86x21 + 91x31 + 99x41)/F1. This
is required to be ≥95. So, the Oc.R. constraint on fuel type 1 can be represented by the
linear constraint: 68x11 + 86x21 + 91x31 + 99x41 −95F1 ≥0. Proceeding in a similar manner,
we obtain the following LP formulation for this problem.
TABLE 1.2
Data for the Fuel Blending Problem
Raw Gas
Octane Rating
Available Daily
Price per
Type
(Oc.R)
(Barrels)
Barrel
1
68
4000
$31.02
2
86
5050
33.15
3
91
7100
36.35
4
99
4300
38.75
Fuel
Minimum
Selling Price
Type
Oc.R
($) (Barrel)
Demand
1
95
47.15
At most 10,000 barrels/day
2
90
44.95
No limit
3
85
42.99
At least 15,000 barrels/day

1-8
Operations Research Methodologies
Maximize
47.15F1 + 44.95F2 + 42.99F3 + y1(36.85 −31.02)
+ y2(36.85 −33.15) + y3(38.95 −36.35) + y4(38.95
−38.75) −(31.02 + 2)RG1 −(33.15 + 2)RG2
−(36.35 + 2)RG3 −(38.75 + 2)RG4
subject to
RGi = xi1 + xi2 + xi3 + yi,
i = 1, . . ., 4
0 ≤(RG1, RG2, RG3, RG4) ≤(4000, 5050, 7100, 4300)
Fj = x1j + x2j + x3j + x4j,
j = 1, 2, 3
0 ≤F1 ≤10, 000
F3 ≤15, 000
68x11 + 86x21 + 91x31 + 99x41 −95F1 ≥0
68x12 + 86x22 + 91x32 + 99x42 −90F2 ≥0
68x13 + 86x23 + 91x33 + 99x43 −85F3 ≥0
F2 ≥0,
xij, yi ≥0,
for all i, j
Blending models are economically signiﬁcant in the petroleum industry. The blending of
gasoline is a very popular application. A single grade of gasoline is normally blended from
about 3 to 10 individual components, none of which meets the quality speciﬁcations by
itself. A typical reﬁnery might have 20 diﬀerent components to be blended into four or
more grades of gasoline and other petroleum products such as aviation gasoline, jet fuel,
and middle distillates, diﬀering in Oc.R and properties such as pour point, freezing point,
cloud point, viscosity, boiling characteristics, vapor pressure, and so on, by marketing region.
1.2.3
The Diet Problem
A diet has to satisfy many constraints; the most important is that it should be palatable (i.e.,
be tasty) to the one eating it. This is a very diﬃcult constraint to model mathematically,
particularly if the diet is for a human individual. So, early publications on the diet problem
have ignored this constraint and concentrated on meeting the minimum daily requirement
(MDR) of each nutrient identiﬁed as being important for the individual’s well-being. Also,
these days most of the applications of the diet problem are in the farming sector, and farm
animals and birds are usually not very fussy about what they eat.
The diet problem is one among the earliest problems formulated as an LP. The ﬁrst paper
on it was by Stigler (1945). Those were the war years, food was expensive, and the problem
of ﬁnding a minimum cost diet was of more than academic interest. Nutrition science was
in its infancy in those days, and after extensive discussions with nutrition scientists, Stigler
identiﬁed nine essential nutrient groups for his model. His search of the grocery shelves
yielded a list of 77 diﬀerent available foods. With these, he formulated a diet problem that
was an LP involving 77 nonnegative decision variables subject to 9 inequality constraints.
Stigler did not know of any method for solving his LP model at that time, but he obtained
an approximate solution using a trial and error search procedure that led to a diet meeting
the MDR of the nine nutrients considered in the model at an annual cost of $39.93 at
1939 prices! After Dantzig developed the simplex method for solving LPs in 1947, Stigler’s
diet problem was one of the ﬁrst nontrivial LPs to be solved by the simplex method on a
computer, and it gave the true optimum diet with an annual cost of $39.67 at 1939 prices.
So, the trial and error solution of Stigler was very close to the optimum.
The Nobel prize committee awarded the 1982 Nobel prize in economics to Stigler for his
work on the diet problem and later work on the functioning of markets and the causes and
eﬀects of public regulation.

Linear Programming
1-9
TABLE 1.3
Data on the Nutrient Content of Grains
Nutrient Units/kg
of Grain Type
MDR of Nutrient
Nutrient
1
2
in Units
Starch
5
7
8
Protein
4
2
15
Vitamins
2
1
3
Cost ($/kg.) of food
0.60
0.35
The data in the diet problem consist of a list of nutrients with the MDR for each; a list
of available foods with the price and composition (i.e., information on the number of units
of each nutrient in each unit of food) of every one of them; and the data deﬁning any other
constraints the user wants to place on the diet. As an example we consider a very simple
diet problem in which the nutrients are starch, protein, and vitamins as a group; the foods
are two types of grains with data given in Table 1.3.
The activities and their levels in this model are: activity j: to include 1 kg of grain type j in
the diet, associated level = xj, for j = 1, 2. The items in this model are the various nutrients,
each of which leads to a constraint. For example, the amount of starch contained in the diet
x is 5x1 + 7x2, which must be ≥8 for feasibility. This leads to the formulation given below.
Minimize z(x) = 0.60x1 +0.35x2
Item
subject to
5x1 +
7x2 ≥8
Starch
4x1 +
2x2 ≥15
Protein
2x1 +
x2 ≥3
Vitamins
x1 ≥0,
x2≥0
Nowadays almost all the companies in the business of making feed for cattle, other farm
animals, birds, and the like use LP extensively to minimize their production costs. The
prices and supplies of various grains, hay, and so on are constantly changing, and feed
makers solve the diet model frequently with new data values, to make their buy-decisions
and to formulate the optimum mix for manufacturing the feed.
■
Once I met a farmer at a conference discussing commercial LP software systems. He
operates reasonable size cattle and chicken farms. He was carrying his laptop with him. He
told me that in the fall harvest season, he travels through agricultural areas extensively.
He always has his laptop with LP-based diet models for the various cattle and chicken feed
formulations inside it. He told me that before accepting an oﬀer from a farm on raw materials
for the feed, he always uses his computer to check whether accepting this oﬀer would reduce
his overall feed costs or not, using a sensitivity analysis feature in the LP software in his
computer. He told me that this procedure has helped him save his costs substantially.
1.2.4
The Transportation Model
An essential component of our modern life is the shipping of goods from where they are
produced to markets worldwide. Nationally, within the United States alone transportation
of goods is estimated to cost over 1 trillion/year. The aim of this problem is to ﬁnd a way
of carrying out this transfer of goods at minimum cost. Historically, it was among the ﬁrst
LPs to be modeled and studied. The Russian economist L. V. Kantorovitch studied this
problem in the 1930s and developed the dual simplex method for solving it, and published
a book on it, Mathematical Methods in the Organization and Planning of Production, in

1-10
Operations Research Methodologies
TABLE 1.4
Data for the Transportation Problem
cij (Cents/Ton)
Availability at
j = 1
2
3
Mine (Tons) Daily
Mine i = 1
11
8
2
800
2
7
5
4
300
Requirement at
plant (tons) daily
400
500
200
Russian in 1939. In the United States, (Hitchcock, 1941) developed an algorithm similar to
the primal simplex algorithm for ﬁnding an optimum solution to the transportation prob-
lem. And (Koopmans, 1949) developed an optimality criterion for a basic solution to the
transportation problem in terms of the dual basic solution (discussed later on). The early
work of L. V. Kantorovitch and T. C. Koopmans in these publications was part of their
eﬀort for which they received the 1975 Nobel prize for economics.
The classical single commodity transportation problem is concerned with a set of nodes
or places called sources that have a commodity available for shipment, and another set
of places called sinks or demand centers or markets that require this commodity. The
data consists of the availability at each source (the amount available there to be shipped
out), the requirement at each market, and the cost of transporting the commodity per unit
from each source to each market. The problem is to determine the quantity to be trans-
ported from each source to each market so as to meet the requirements at minimum total
shipping cost.
As an example, we consider a small problem where the commodity is iron ore, the sources
are mines 1 and 2 that produce the ore, and the markets are three steel plants that require
the ore. Let cij = cost (cents per ton) to ship ore from mine i to steel plant j, i = 1, 2, j = 1,
2, 3. The data are given in Table 1.4. To distinguish between diﬀerent data elements, we
show the cost data in normal size letters, and the supply and requirement data in bold face
letters.
The decision variables in this model are: xij = ore (in tons) shipped from mine i to
plant j. The items in this model are the ore at various locations. We have the following LP
formulation for this problem.
Minimize z(x) = 11x11 + 8x12 + 2x13 + 7x21 + 5x22 + 4x23
Item
Ore at
subject to
x11 + x12 + x13
= 800
mine 1
x21 + x22 + x23 = 300
mine 2
x11
+ x21
= 400
plant 1
x12
+ x22
= 500
plant 2
x13
+ x23 = 200
plant 3
xij ≥0
for all i = 1, 2, j = 1, 2, 3
Let G denote the directed network with the sources and sinks as nodes, and the various
routes from each source to each sink as the arcs. Then this problem is a single commod-
ity minimum cost ﬂow problem in G. So, the transportation problem is a special case of
single commodity minimum cost ﬂow problems in directed networks. Multicommodity ﬂow
problems are generalizations of these problems involving two or more commodities.
The model that we presented for the transportation context is of course too simple. Real
world transportation problems have numerous complicating factors, both in the constraints
to be satisﬁed and the objective functions to optimize, that need to be addressed. Starting

Linear Programming
1-11
with this simple model as a foundation, realistic models for these problems are built by
modifying it, and augmenting as necessary.
1.2.5
Multiperiod Production Planning, Storage, Distribution
Problems
The LP model ﬁnds many applications for making production allocation, planning, storage,
and distribution decisions in companies. Companies usually like to plan ahead; when they
are planning for one period, they usually like to consider also a few periods into the future.
This leads to multiperiod planning problems.
To construct a mathematical model for a multiperiod horizon, we need reliable data on
the expected production costs, input material availabilities, production capacities, demand
for the output, selling prices, and the like in each period. With economic conditions changing
rapidly and unexpectedly these days, it is very diﬃcult to assemble reliable data on such
quantities beyond a few periods from the present. That is why multiperiod models used in
practice usually cover the current period and a few periods following it, for which data can
be estimated with reasonable precision.
For example, consider the problem of planning the production, storage, and marketing
of a product whose demand and selling price vary seasonally. An important feature in this
situation is the proﬁt that can be realized by manufacturing the product in seasons during
which the production costs are low, storing it, and putting it in the market when the selling
price is high. Many products exhibit such seasonal behavior, and companies and businesses
take advantage of this feature to augment their proﬁts. A linear programming formulation
of this problem has the aim of ﬁnding the best production-storage-marketing plan over
the planning horizon, to maximize the overall proﬁt. For constructing a model for this
problem we need reasonably good estimates of the demand and the expected selling price of
the product in each period of the planning horizon; availability and cost of raw materials,
labor, machine times, etc. necessary to manufacture the product in each period; and the
availability, and cost of storage space.
As an example, we consider the simple problem of a company making a product subject
to such seasonal behavior. The company needs to make a production plan for the coming
year, divided into six periods of 2 months each, to maximize net proﬁt (= sales revenue –
production and storage costs). Relevant data are in Table 1.5. The production cost there
includes the cost of raw material, labor, machine time, and the like, all of which ﬂuctuate
from period to period. And the production capacity arises due to limits on the availability
of raw material and hourly labor.
Product manufactured during a period can be sold in the same period, or stored and sold
later on. Storage costs are $2/ton of product from one period to the next. Operations begin
in period 1 with an initial stock of 500 tons of the product in storage, and the company
would like to end up with the same amount of the product in storage at the end of period 6.
TABLE 1.5
Data for the 6-Period Production Planning Problem
Total Production
Production
Demand
Selling
Period
Cost ($/Ton)
Capacity (Tons)
(Tons)
Price ($/Ton)
1
20
1500
1100
180
2
25
2000
1500
180
3
30
2200
1800
250
4
40
3000
1600
270
5
50
2700
2300
300
6
60
2500
2500
320

1-12
Operations Research Methodologies
The decision variables in this problem are, for period j = 1–6
xj = product made (tons) during period j
yj = product left in storage (tons) at the end of period j
zj = product sold (tons) during period j
In modeling this problem the important thing to remember is that inventory equations
(or material balance equations) must hold for the product for each period. For period j this
equation expresses the following fact.
Amount of product in storage
at the beginning of period j +
the amount manufactured
during period j
⎫
⎪
⎪
⎬
⎪
⎪
⎭
=
⎧
⎨
⎩
Amount of product sold during
period j + the amount left in
storage at the end of period j
The LP model for this problem is given below:
Maximize 180(z1 + z2) + 250z3 + 270z4 + 300z5 + 320z6
−20x1 −25x2 −30x3 −40x4 −50x5 −60x6
−2(y1 + y2 + y3 + y4 + y5 + y6)
subject to xj, yj, zj ≥0
for all j = 1 to 6
x1 ≤1500, x2 ≤2000, x3 ≤2200, x4 ≤3000, x5 ≤2700, x6 < 2500
z1 ≤1100, z2 ≤1500, z3 ≤1800, z4 ≤1600, z5 ≤2300, z6 ≤2500
500 + x1 −(y1 + z1) = 0
y1 + x2 −(y2 + z2) = 0
y2 + x3 −(y3 + z3) = 0
y3 + x4 −(y4 + z4) = 0
y4 + x5 −(y5 + z5) = 0
y5 + x6 −(y6 + z6) = 0
y6 = 500
Many companies have manufacturing facilities at several locations, and usually make
and sell several diﬀerent products. Production planning at such companies involves produc-
tion allocation decisions (what products will each facility manufacture) and transportation-
distribution decisions (plan to ship the output of each facility to each market) in addition
to the material balance constraints of the type discussed in the example above for each
product and facility.
1.3
LP Models Involving Transformations of Variables
In this section, we will extend the range of application of LP to include problems that can
be modeled as those of optimizing a convex piecewise linear objective function subject to
linear constraints. These problems can be transformed easily into LPs in terms of additional
variables. This material is from Murty (under preparation).
Let θ(λ) be a real valued function of a single variable λ ∈R1. θ(λ) is said to be a piecewise
linear (PL) function if it is continuous and if there exists a partition of R1 into intervals

Linear Programming
1-13
TABLE 1.6
The PL Function θ(λ)
Interval for λ
Slope
Value of θ(λ)
λ ≤λ1
c1
c1λ
λ1 ≤λ ≤λ2
c2
θ(λ1) + c2(λ −λ1)
λ2 ≤λ ≤λ3
c3
θ(λ2) + c3(λ −λ2)
...
...
...
λ ≥λr
cr+1
θ(λr) + cr+1(λ −λr)
of the form [−∞, λ1] = {λ ≤λ1}, [λ1, λ2], . . ., [λr−1, λr], [λr, ∞] (where λ1 < λ2 < · · · < λr
are the breakpoints in this partition) such that inside each interval the slope of θ(λ) is
a constant. If these slopes in the various intervals are c1, c2, . . ., cr+1, the values of this
function at various values of λ are tabulated in Table 1.6.
This PL function is said to be convex if its slope is monotonic increasing with λ, that is, if
c1 < c2 · · · < cr+1. If this condition is not satisﬁed it is nonconvex. Here are some numerical
examples of PL functions of the single variable λ (Tables 1.7 and 1.8).
Example 1.3: PL Function θ(λ)
TABLE 1.7
PL Convex Function θ(λ)
Interval for λ
Slope
Value of θ(λ)
λ ≤10
3
3λ
10 ≤λ ≤25
5
30 + 5(λ −10)
λ ≥25
9
105 + 9(λ −25)
Example 1.4: PL Function g(λ)
TABLE 1.8
Nonconvex PL Function θ(λ)
Interval for λ
Slope
Value of θ(λ)
λ ≤100
10
10λ
100 ≤λ ≤300
5
1000 + 5(λ −100)
300 ≤λ ≤1000
11
2000 + 11(λ −300)
λ ≥1000
20
9700 + 20(λ −1000)
Both functions θ(λ), g(λ) are continuous functions and PL functions. θ(λ) is convex
because its slope is monotonic increasing, but g(λ) is not convex as its slope is not monotonic
increasing with λ.
A PL function h(λ) of the single variable λ ∈R1 is said to be a PL concave function
iﬀ−h(λ) is a PL convex function; that is, iﬀthe slope of h(λ) is monotonic decreasing as
λ increases.
PL Functions of Many Variables
Let f(x) be a real valued function of x = (x1, . . ., xn)T . f(x) is said to be a PL (piecewise lin-
ear) function of x if there exists a partition of Rn into convex polyhedral regions K1, . . ., Kr
such that f(x) is linear within each Kt, for t = 1 to r; and a PL convex function if it is
also convex. It can be proved mathematically that f(x) is a PL convex function iﬀthere
exists a ﬁnite number, r say, of linear (more precisely aﬃne) functions ct
0 + ctx, (where ct
0,

1-14
Operations Research Methodologies
and ct ∈Rn are the given coeﬃcient vectors for the t-th linear function) t = 1 to r such that
for each x ∈Rn
f(x) = Maximum{ct
0 + ctx : t = 1, . . ., r}
(1.3)
A function f(x) deﬁned by Equation 1.3 is called the pointwise maximum (or supremum)
function of the linear functions ct
0 + ctx, t = 1 to r. PL convex functions of many variables
that do not satisfy the additivity hypothesis always appear in this form (Equation 1.3) in
real world applications.
Similarly, a PL function h(x) of x = (x1, . . ., xn)T is said to be a PL concave function if
there exist a ﬁnite number s of aﬃne functions dt
0 + dtx, t = 1 to s, such that h(x) is their
pointwise inﬁmum, that is, for each x ∈Rn
h(x) = Minimum{dt
0 + dtx : t = 1, . . ., s}
Now we show how to transform various types of problems of minimizing a PL convex
function subject to linear constraints into LPs, and applications of these transformations.
Minimizing a Separable PL Convex Function Subject to Linear Constraints
A real valued function z(x) of variables x = (x1, . . ., xn)T is said to be separable if it satisﬁes
the additivity hypothesis, that is, if it can be written as the sum of n functions, each
one involving only one variable as in: z(x) = z1(x1) + z2(x2) + · · · + zn(xn). Consider the
following general problem of this type:
Minimize
z(x) = z1(x1) + · · · + zn(xn)
subject to
Ax = b
(1.4)
x ≥0
where each zj(xj) is a PL convex function with slopes in intervals as in Table 1.9, rj + 1 is
the number of diﬀerent slopes cj1 < cj2 < · · · < cj,rj+1 of zj(xj), and ℓj1, ℓj2, . . ., ℓj,rj+1 are
the lengths of the various intervals in which these slopes apply.
As the objective function to be minimized does not satisfy the proportionality assumption,
this is not an LP. However, the convexity property can be used to transform this into an LP
by introducing additional variables. This transformation expresses each variable xj as a sum
of rj + 1 variables, one associated with each interval in which its slope is constant. Denot-
ing these variables by xj1, xj2, . . ., xj,rj+1, the variable xj becomes = xj1 + · · · + xj,rj+1 and
zj(xj) becomes the linear function cj1xj1 + · · · + cj,rj+1xj,rj+1 in terms of the new variables.
The reason for this is that as the slopes are monotonic (i.e., cj1 < cj2 < · · · < cj,rj+1), for
any value of ¯xj ≥0, if (¯xj1, ¯xj2, . . ., ¯xj,rj+1) is an optimum
Minimize
cj1xj1 + · · · + cj,rj+1xj,rj+1
Subject to
xj1 + · · · + xj,rj+1 = ¯xj
0 ≤xjt < ℓjt,
t = 1, . . ., rj + 1
TABLE 1.9
The PL Function Zj(xj)
Interval
Slope in Interval
Value of zj(xj)
Length of Interval
0 ≤xj ≤kj1
cj1
cj1xj
ℓj1 = kj1
kj1 ≤xj ≤kj2
cj2
zj(kj1) + cj2(xj −kj1)
ℓj2 = kj2 −kj1
...
...
...
...
kj,rj ≤xj
cj,rj +1
zj(kj,rj ) + cj,rj +1(xj −kj,rj )
ℓj,rj +1 = ∞

Linear Programming
1-15
¯xj,t+1 will not be positive unless ¯xjk = ℓjk for k = 1 to t, for each t = 1 to rj. Hence the
optimum objective value in this problem will be equal to zj(xj). This shows that our original
problem (Equation 1.4) is equivalent to the following transformed problem which is an LP.
Minimize
j=n

j=1
t=rj+1

t=1
cjtxjt
subject to
xj =
t=rj+1

t=1
xjt,
j = 1, . . ., n
Ax=b
0 ≤xjt ≤ℓjt,
t = 1, . . ., rj + 1;
j = 1, . . . , n
x≥0.
The same type of transformation can be used to transform a problem involving the
maximization of a separable PL concave function subject to linear constraints into an LP.
Example 1.5
A company makes products P1, P2, P3 using limestone (LI), electricity (EP), water (W),
fuel (F), and labor (L) as inputs. Labor is measured in man hours, and other inputs in
suitable units.
■
Each input is available from one or more sources. The company has its own quarry for
LI, which can supply up to 250 units/day at a cost of $20/unit. Beyond that, LI can be
purchased in any amounts from an outside supplier at $50/unit. EP is only available from
the local utility. Their charges for EP are: $30/unit for the ﬁrst 1000 units/day, $45/unit for
up to an additional 500 units/day beyond the initial 1000 units/day, $75/unit for amounts
beyond 1500 units/day. Up to 800 units/day of water is available from the local utility at
$6/unit; beyond that they charge $7/unit of water/day. There is a single supplier for F who
can supply at most 3000 units/day at $40/unit; beyond that there is currently no supplier
for F. From their regular workforce they have up to 640 man hours of labor/day at $10/man
hour; beyond that they can get up to 160 man hours/day at $17/man hour from a pool of
workers.
They can sell up to 50 units of P1 at $3000/unit/day in an upscale market; beyond that
they can sell up to 50 more units/day of P1 to a wholesaler at $250/unit. They can sell
up to 100 units/day of P2 at $3500/unit. They can sell any quantity of P3 produced at a
constant rate of $4500/unit.
Data on the inputs needed to make the various products are given in Table 1.10. Formulate
the product mix problem to maximize the net proﬁt/day at this company.
Maximizing the net proﬁt is the same as minimizing its negative, which is = (the costs
of all the inputs used/day) −(sales revenue/day). We verify that each term in this sum
TABLE 1.10
I/O Data
Input Units/Unit Made
Product
LI
EP
W
F
L
P1
1
2
3
1
1
2
P2
1
2
1
4
1
1
P3
3
2
5
2
3
1

1-16
Operations Research Methodologies
is a PL convex function. So, we can model this problem as an LP in terms of variables
corresponding to each interval of constant slope of each of the input and output quantities.
Let LI, EP, W, F, L denote the quantities of the respective inputs used/day; and P1,
P2, P3 denote the quantities of the respective products made and sold/day. Let LI1 and LI2
denote the units of limestone used daily from own quarry and outside supplier. Let EP1,
EP2, and EP3 denote the units of electricity used/day at $30, 45, 75/unit, respectively. Let
W1 and W2 denote the units of water used/day at rates of $6, 7/unit, respectively. Let L1
and L2 denote the man hours of labor used/day from regular workforce, pool, respectively.
Let P11 and P12 denote the units of P1 sold at the upscale market and to the wholesaler,
respectively.
Then the LP model for the problem is:
Minimize
z = 20LI1 + 50LI2 + 30EP1 + 45EP2 + 75EP3 + 6W1 + 7W2 + 40F + 10L1
+ 17L2 −3000P11 −250P12 −3500P2 −4500P3
subject to
(1/2)P1 + P2 + (3/2)P3 = LI
3P1 + 2P2 + 5P3 = EP
P1 + (1/4)P2 + 2P3 = W
P1 + P2 + 3P3 = F
2P1 + P2 + P3 = L
LI1 + LI2 = LI,
W1 + W2 = W
EP1 + EP2 + EP3 = EP
L1 + L2 = L,
P11 + P12 = P1,
all variables ≥0
(LI1, EP1, EP2, W1) ≤(250, 1000, 500, 800)
(F, L1, L2) ≤(3000, 640, 160)
(P11, P12, P2) ≤(50, 50, 100).
1.3.1
Min–Max, Max–Min Problems
As discussed above, a PL convex function in variables x = (x1, . . ., xn)T can be expressed
as the pointwise maximum of a ﬁnite set of linear functions. Minimizing a function like
that is appropriately known as a min–max problem. Similarly, a PL concave function in x
can be expressed as the pointwise minimum of a ﬁnite set of linear functions. Maximizing
a function like that is appropriately known as a max–min problem. Both min–max and
max–min problems can be expressed as LPs in terms of just one additional variable.
If the PL convex function f(x) = min{ct
0 + ctx : t = 1, . . ., r}, then −f(x) = max{−ct
0 −
ctx : t = 1, . . ., r} is PL concave and conversely. Using this, any min–max problem can be
posed as a max–min problem and vice versa. So, it is suﬃcient to discuss max–min problems.
Consider the max–min problem
max z(x) = min{c1
0 + c1x, . . ., cr
0 + crx}
subject to
Ax = b
x ≥0
To transform this problem into an LP, introduce the new variable xn+1 to denote the
value of the objective function z(x) to be maximized. Then the equivalent LP with additional

Linear Programming
1-17
linear constraints is:
max
xn+1
subject to
xn+1 ≤c1
0 + c1x
xn+1 ≤c2
0 + c2x
...
xn+1 ≤cr
0 + crx
Ax = b
x ≥0
The fact that xn+1 is being maximized and the additional constraints together imply
that if (¯x, ¯xn+1) is an optimum solution of this LP model, then ¯xn+1 = min{c1
0 + c1¯x, . . ., cr
0 +
cr¯x} = z(¯x), and that ¯xn+1 is the maximum value of z(x) in the original max–min problem.
Example 1.6: Application in Worst Case Analysis
Consider the fertilizer maker’s product mix problem with decision variables x1 and x2 (Hi-
ph, Lo-ph fertilizers to be made daily in the next period) discussed in Example 1.2, Sec-
tion 1.2. There we discussed the case where the net proﬁt coeﬃcients c1 and c2 of these
variables are estimated to be $15 and $10, respectively. In reality, the prices of fertilizers
are random variables that ﬂuctuate daily. Because of unstable conditions, and new agri-
cultural research announcements, suppose market analysts have only been able to estimate
that the expected net proﬁt coeﬃcient vector (c1, c2) is likely to be one of {(15, 10),
(10, 15), (12, 12)} without giving a single point estimate. So, here we have three possible
scenarios. In scenario 1, (c1, c2) = (15, 10), expected net proﬁt = 15x1 + 10x2; in scenario
2, (c1, c2) = (10, 15), expected net proﬁt = 10x1 + 15x2; in scenario 3 (c1, c2) = (12, 12),
expected net proﬁt = 12x1 + 12x2. Suppose the raw material availability data in the prob-
lem is expected to remain unchanged. The important question is: which objective function
to optimize for determining the production plan for the next period.
■
Irrespective of which of the three possible scenarios materializes, at the worst the minimum
expected net proﬁt of the company will be p(x) = min {15x1 + 10x2, 10x1 + 15x2, 12x1 +
12x2} under the production plan x = (x1, x2)T . Worst case analysis is an approach that
advocates determining the production plan to optimize this worst case net proﬁt p(x) in
this situation. This leads to the max–min model: maximize p(x) = min {15x1 + 10x2, 10x1 +
15x2, 12x1 + 12x2} subject to the constraints in Equation 1.2. The equivalent LP model cor-
responding to this is:
max
p
subject to
p ≤15x1 + 10x2
p ≤10x1 + 15x2
p ≤12x1 + 12x2
2x1 + x2 ≤1500
x1 + x2 ≤1200
x1 ≤500,
x1, x2 ≥0
1.3.2
Minimizing Positive Linear Combinations of Absolute Values
of Aﬃne Functions
Consider the problem
min
z(x) = w1|c1
0 + c1x| + · · · + wr|cr
0 + crx|
subject to
Ax ≥b

1-18
Operations Research Methodologies
where the weights w1, . . ., wr are all strictly positive. In this problem the objective func-
tion to be minimized, z(x), is a PL convex function; hence this problem can be trans-
formed into an LP. To transform, deﬁne for each t = 1 to r two new nonnegative variables
u+
t = max {0, ct
0 + ctx}, u−
t = −min {0, ct
0 + ctx}. u+
t is called the positive part of ct
0 + ctx,
and u−
t its negative part. It can be veriﬁed that (u+
t )(u−
t ) is zero by deﬁnition, and because of
this we have: ct
0 + ctx = (u+
t ) −(u−
t ) and |ct
0 + ctx| = (u+
t ) + (u−
t ). Using this, we can trans-
form the above problem into the following LP:
min
w1[(u+
1 ) + (u−
1 )] + · · · + wr[(u+
r ) + (u−
r )]
subject to
c1
0 + c1x = (u+
t ) −(u−
t )
...
...
cr
0 + crx = (u+
r ) −(u−
r )
Ax ≥b
(u+
t ), (u−
t ) ≥0,
t = 1, . . ., r
Using the special structure of this problem it can be shown that the condition (u+
t )(u−
t ) = 0
for all t = 1 to r will hold at all optimum solutions of this LP. This shows that this trans-
formation is correct.
An application of this transformation is discussed in the next section. This is an important
model that ﬁnds many applications.
1.4
Intelligent Modeling Essential to Get Good
Results, an Example from Container Shipping
To get good results from a linear programming application, it is very important to develop
a good model for the problem being solved. There may be several ways of modeling the
problem, and it is very important to select the one most appropriate to model it intelligently
to get good results. Skill in modeling comes from experience; unfortunately there is no theory
to teach how to model intelligently. We will now discuss a case study of an application
carried out for routing trucks inside a container terminal to minimize congestion. Three
diﬀerent ways of modeling the problem have been tried. The ﬁrst two approaches lead to
(1) an integer programming model and (2) a large-scale multicommodity ﬂow LP model,
respectively. Both these models gave very poor results. The third and ﬁnal model developed
uses a substitute objective function technique; that is, it optimizes another simpler objective
function that is highly correlated to the original, because that other objective function is
much easier to control. This approach led to a small LP model, and gives good results.
Today most of the nonbulk cargo is packed into steel boxes called containers (typically
of size 40 × 8 × 9 in feet) and transported in oceangoing vessels. A container terminal in a
port is the place where these vessels dock at berths for unloading of inbound containers and
loading of outbound containers. The terminals have storage yards for the temporary storage
of these containers. The terminal’s internal trucks (TIT) transport containers between the
berth and the storage yard (SY). The SY is divided into rectangular areas called blocks, each
served by one or more cranes (rubber tired gantry cranes, or RTGC) to unload/load con-
tainers from/to trucks. Customers bring outbound containers into the terminal in their own
trucks (called external trucks, or XT), and pick up from the SY and take away their inbound
containers on these XT. Each truck (TIT or XT) can carry only one container at a time.
The example (from Murty et al., 2005a,b) deals with the mathematical modeling of the
problem of routing the trucks inside the terminal to minimize congestion. We represent

Linear Programming
1-19
the terminal road system by a directed network G = (N, A) where N is the set of nodes
(each block, berth unloading/loading position, road intersection, terminal gate is a node),
A is the set of arcs (each lane of a road segment joining a pair of nodes is an arc). Each
(berth unloading/loading position, block), (block, berth unloading/loading position), (gate,
block), (block, gate) is an origin–destination pair for trucks that have to go from the origin
to the destination; they constitute a separate commodity that ﬂows in G. Let T denote the
number of these commodities. Many terminals use a 4-hour planning period for their truck
routing decisions.
Let f = (f r
ij) denote the ﬂow vector of various commodities on G in the planning
period, where f r
ij = expected number of trucks of commodity r passing through arc (i, j)
in the planning period for r = 1 to T, and (i, j) ∈A. Let θ = max
T
r=1 f r
ij : (i, j) ∈A

,
μ = min
T
r=1 f r
ij : (i, j) ∈A

. Then either θ or θ −μ can be used as measures of conges-
tion on G during the planning period, to optimize.
As storage space allocation to arriving containers directly determines how many trucks
travel between each origin–destination pair, the strategy used for this allocation plays a
critical role in controlling congestion. This example deals with mathematical modeling of
the problem of storage space allocation to arriving containers to minimize congestion.
Typically, a block has space for storing 600 containers, and a terminal may have 100
(some even more) blocks. At the beginning of the planning period, some spaces in the
SY would be occupied by containers already in storage, and the set of occupied storage
positions changes every minute; it is very diﬃcult to control this change. Allocating a speciﬁc
open storage position to each container expected to arrive in the planning period has been
modeled as a huge integer program, which takes a long time to solve. In fact, even before
this integer programming model is entered into the computer, the data change. So these
traditional integer programming models are not only impractical but also inappropriate for
the problem.
So a more practical way is to break up the storage space allocation decision into two stages:
Stage 1 determines only the container quota x i, for each block i, which is the number of
newly arriving containers that will be dispatched to block i for storage during the planning
period. Stage 1 will not determine which of the speciﬁc arriving containers will be stored
in any block; that decision is left to Stage 2, which is a dispatching policy that allocates
each arriving container to a speciﬁc block for storage at the time of its arrival, based on
conditions prevailing at that time. So, Stage 2 makes sure that by the end of the planning
period the number of new containers sent for storage to each block is its quota number
determined in Stage 1, while minimizing congestion at the blocks and on the roads.
Our example deals with the Stage 1 problem. The commonly used approach is based on a
batch-processing strategy. Each batch consists of all the containers expected to arrive/leave
at each node during the planning period. At the gate, this is the number of outbound
containers expected to arrive for storage. At a block it is the number of stored containers
expected to be retrieved and sent to each berth or the gate. At each berth it is the number
of inbound containers expected to be unloaded to be sent for storage to SY. With this
data, the problem can be modeled as a multicommodity network ﬂow problem. It is a large-
scale LP with many variables and thousands of constraints. However, currently available
LP software systems are fast; this model can be solved using them in a few minutes of
computer time.
But the output from this model turned out to be poor, as the model is based solely on
the total estimated workload during the planning period. Such a model gives good results
for the real problem only if the workload in the terminal (measured in number of containers

1-20
Operations Research Methodologies
handled/unit time) is distributed more or less uniformly over time during the planning
period. In reality the workload at terminals varies a lot over time. At the terminal where
we did this work, the number of containers handled per hour varied from 50 to 400 in a
4-hour planning period.
Let fi(t) denote the ﬁll ratio in block i at time point t, which is equal to (number of
containers in storage in block i at time point t)/(number of storage spaces in block i).
We observed that the ﬁll ratio in a block is highly positively correlated with the number
of containers being moved in and out of the block/minute. So, maintaining ﬁll ratios in
all the blocks nearly equal, along with a good dispatching policy, will ensure that the
volumes of traﬃc in the neighborhoods of all the blocks are nearly equal, thus ensuring
equal distribution of traﬃc on all the terminal roads and hence minimizing congestion. This
leads to a substitute-objective-function technique for controlling congestion indirectly. For
the planning period, we deﬁne the following:
xi = The container quota for block i = number of containers arriving in this period
to be dispatched for storage to block i, a decision variable.
ai = The number of stored containers that will remain in block i at the end of
this period if no additional containers are sent there for storage during this
period, a data element.
N = The number of new containers expected to arrive at the terminal in this
period for storage, a data element.
B, A = The total number of blocks in the storage yard, the number of storage posi-
tions in each block, data elements.
The ﬁll-ratio equalization policy determines the decision variables xi to make sure that
the ﬁll ratios in all the blocks are as nearly equal as possible at one time during the period,
namely the end of the period. The ﬁll ratio in the whole yard at the end of this period will
be F = (N + 
i ai)/(A × B). If the ﬁll ratios in all the blocks at the end of this period are
all equal, they will all be equal to F. Thus, this policy determines xi to guarantee that the
ﬁll ratio in each block will be as close to F as possible by the end of this period. Using the
least sum of absolute deviations measure, this leads to the following model to determine xi.
Minimize
B

i=1
|ai + xi −AF|
subject to
B

i=1
xi = N
xi ≥0
for all i
Transforming this we get the following LP model to determine xi
Minimize
B

i=1
(u+
i + u−
i )
subject to
B

i=1
xi = N
ai + xi −AF = u+
i −u−
i
for all i
xi, u+
i , u−
i ≥0
for all i
This is a much simpler and smaller LP model with only B + 1 constraints. Using its special
structure, it can be veriﬁed that its optimum solution can be obtained by the following

Linear Programming
1-21
combinatorial scheme: Rearrange the blocks in increasing order of ai from top to bottom.
Then begin at the top and determine xi one after the other to bring ai + xi to the level AF
or as close to it as possible until all the N new containers expected to arrive are allocated.
We will illustrate with a small numerical example of an SY with B = 9 blocks, A = 600
spaces in each block, with N = 1040 new containers expected to arrive during the plan-
ning period. Data on ai already arranged in increasing order are given in the following
table. So, the ﬁll ratio in the whole yard at the end of the planning period is expected
to be F = (N + 
iai)/(AB) = 3547/5400 ≈0.67, and so the average number of containers
in storage/block will be AF ≈400. So, the LP model for determining xi for this planning
period to equalize ﬁll ratios is
Minimize
9

i=1
(u+
i + u−
i )
subject to
9

i=1
xi = 1040
ai + xi −u+
i + u−
i = 400
for all i
xi, u+
i , u−
i ≥0
for all i
The optimum solution (xi) of this model obtained by the above combinatorial scheme is
given in Table 1.11. ai + xi is the expected number of containers in storage in block i at
the end of this planning period; it can be veriﬁed that its values in the various blocks are
nearly equal.
Stage 1 determines only the container quota numbers for the blocks, not the identities
of containers that will be stored in each block. The storage block to which each arriving
container will be sent for storage is determined by the dispatching policy discussed in
Stage 2. Now we describe Stage 2 brieﬂy.
Regardless of how we determine the container quota numbers xi, if we send a consecutive
sequence of arriving container trucks to the same block in a short time interval, we will
create congestion at that block. To avoid this possibility, the dispatching policy developed
in Stage 2 ensures that the yard crane in that block has enough time to unload a truck
we send there before we send another. For this we had to develop a system to monitor
continuously over time: wi(t) = the number of trucks waiting in block i to be served by the
yard cranes there at time point t. As part of our work on this project, the terminal where we
did this work developed systems to monitor wi(t) continuously over time for each block i.
TABLE 1.11
Optimum Solution xi
Block i
ai
xi
ai + xi
1
100
300
400
2
120
280
400
3
150
250
400
4
300
100
400
5
325
75
400
6
350
35
385
7
375
0
375
8
400
0
400
9
450
0
450
Total
2570
1040

1-22
Operations Research Methodologies
They also developed a dispatching cell that has the responsibility of dispatching each truck
in the arriving stream to a block; this cell gets this wi(t) information continuously over
time.
As time passes during the planning period, the dispatching cell also keeps track of how
many containers in the arriving stream have already been sent to each block for storage.
When this number becomes equal to the container quota number for that block, they will
not send any more containers for storage to that block during the planning period. For each
block i, let xR
i (t) = xi −(number of new containers sent to block i for storage up to time
t in the planning period) = remaining container quota number for block i at time t in the
planning period.
This policy dispatches each truck arriving (at the terminal gate, and at each berth) at time
point t in the period to a block i satisfying: wi(t) = Min{wj(t) : j satisfying xR
i (t) > 0}, that
is, a block with a remaining positive quota that has the smallest number of trucks waiting
in it.
This strategy for determining the quota numbers for blocks, xi described above, along
with the dynamic dispatching policy to dispatch arriving container trucks using real time
information on how many trucks are waiting in each block, turned out to be highly eﬀective.
It reduced congestion and helped reduce truck turnaround time by over 20%.
In this work, our ﬁrst two mathematical models for the problem turned out to be ineﬀec-
tive; the third one was not only the simplest but a highly eﬀective one. This example shows
that to get good results in real world applications, it is necessary to model the problems
intelligently. Intelligent modeling + information technology + optimization techniques is a
powerful combination for solving practical problems.
1.5
Planning Uses of LP Models
When LP is the appropriate technique to model a real world problem, and the LP model is
constructed, we discuss here what useful information can be derived using the model (from
[Murty, 1995, 2005b]).
1.5.1
Finding the Optimum Solutions
Solving the model gives an optimum solution, if one exists. The algorithms can actually
identify the set of all the optimum solutions if there are alternate optimum solutions. This
may be helpful in selecting a suitable optimum solution to implement (one that satisﬁes some
conditions that may not have been included in the model, but which may be important).
Solving the fertilizer product mix problem, we ﬁnd that the unique optimum solution for
it is to manufacture 300 tons Hi-ph and 900 tons Lo-ph, leading to a maximum daily proﬁt
of $13,500.
1.5.2
Infeasibility Analysis
We may discover that the model is infeasible (i.e., it has no feasible solution). If this hap-
pens, there must be a subset of constraints that are mutually contradictory in the model
(maybe we promised to deliver goods without realizing that our resources are inadequate
to manufacture them on time). In this case the algorithms can indicate how to modify the
constraints to make the model feasible. For example, suppose the system of constraints in
the original LP is: Ax = b, x ≥0, where A is an m × n matrix and b = (bi) ∈Rm, and the
equality constraints are recorded so that b ≥0. The Phase I problem for ﬁnding an initial

Linear Programming
1-23
feasible solution for this problem is
Minimize
w(t) =
m

i=1
ti
subject to
Ax + It = b
x, t ≥0
where I is the unit matrix of order m, and t = (t1, . . ., tm)T is the vector of artiﬁcial variables.
Suppose the optimum solution of this Phase I problem is (x, t). If t = 0, x is a feasible solution
of the original LP that can now be solved using it as the initial feasible solution. If t ̸= 0,
the original LP is infeasible. Mathematically there is nothing more that can be done on the
original model. But the real world problem for which this model is constructed does not
go away; it has to be tackled somehow. So, we have to investigate what practically feasible
changes can be carried out on the model to modify it into a feasible system. Infeasibility
analysis is the study of such changes (see Murty, 1995; Murty et al., 2000; Brown and
Graves, 1977; Chinnek and Dravineks, 1991).
Sometimes it may be possible to eliminate some constraints to make the model feasi-
ble. But the most commonly used technique to make the model feasible is to modify some
data elements in the model. Making changes in the technology coeﬃcient matrix A involves
changing the technological processes used in the system; hence these changes are only con-
sidered rarely in practice. Data elements in the RHS constants vector b represent things
like resource quantities made available, delivery commitments made, and so on; these can
be modiﬁed relatively easily. That’s why most often it is the RHS constants in the model
that are changed to make the model feasible.
One simple modiﬁcation that will make the model feasible is to change the RHS constants
vector b into b = b −t. Then the constraints in the modiﬁed model are Ax = b, x ≥0; x is a
feasible solution for it. Starting with x, the modiﬁed model can be solved. As an example,
consider the system
2x1 +3x2 + x3 −x4
= 10
x1 +2x2 −x3
+x5 = 5
x1 + x2 + 2x3
= 4
xj ≥0
for all j
The Phase I problem to ﬁnd a feasible solution of this system is
Minimize
t1 + t2 + t3
subject to
2x1 +3x2 + x3 −x4
+ t1 = 10
x1 +2x2 −x3
+x5 + t2 = 5
x1 + x2 +2x3
+ t3 = 4
xj, ti ≥0
for all j, i
An optimum solution of this Phase I problem is (x, t), where x = (3, 1, 0, 0, 0)T , t =
(1, 0, 0)T , so the original system is infeasible. To make the original system feasible we can
modify the RHS constants vector in the original model b = (10, 5, 4)T to b −t = (9, 5, 4)T .
For the modiﬁed system, x is a feasible solution.
This modiﬁcation only considers reducing the entries in the RHS constants vector in the
original model; also it gives the decision maker no control on which RHS constants bi are
changed to make the system feasible. Normally there are costs associated with changing the

1-24
Operations Research Methodologies
value of bi, and these may be diﬀerent for diﬀerent i. To ﬁnd a least costly modiﬁcation of
the b-vector to make the system feasible, let
c+
i , c−
i = cost per unit increase, decrease respectively in the value of bi
pi, qi = maximum possible increase, decrease allowed in the value of bi
Then the model to minimize the total cost of all the changes to make the model feasible
is the LP
Minimize
m

i=1
(c+
i u+
i + c−
i u−
i )
subject to
Ax + Iu+ −Iu−= b
u+ ≤q, u−≤p
x, u+, u−≥0
where u+ = (u+
1 , . . ., u+
m)T , u−= (u−
1 , . . ., u−
m)T , p = (pi), q = (qi), and I is the unit matrix
of order m. If (x, u+, u−) is an optimum solution of this LP, then b′ = b −u+ + u−is the
optimum modiﬁcation of the original RHS vector b under this model; and x is a feasible
solution of the modiﬁed model (Brown and Graves, 1977; Chinnek and Dravineks, 1991).
1.5.3
Values of Slack Variables at an Optimum Solution
The values of the slack variables corresponding to inequality constraints in the model provide
useful information on which supplies and resources will be left unused and in what quantities,
if that solution is implemented.
For example, in the fertilizer product mix problem, the optimum solution is ˆx = (300, 900).
At this solution, RM 1 slack is ˆx3 = 1500 −2ˆx1 −ˆx2 = 0, RM 2 slack is ˆx4 = 1200 −ˆx1 −ˆx2 =
0, and RM 3 slack ˆx5 = 500 −ˆx1 = 200 tons.
Thus, if this optimum solution is implemented, the daily supply of RM 1 and RM 2 will
be completely used up, but 200 tons of RM 3 will be left unused. This shows that the
supplies of RM 1 and RM 2 are very critical to the company, and that there is currently an
oversupply of 200 tons of RM 3 daily that cannot be used in the optimum operation of the
Hi-ph and Lo-ph fertilizer processes.
This also suggests that it may be worthwhile to investigate if the maximum daily proﬁt
can be increased by lining up additional supplies of RM 1 and RM 2 from outside vendors or
if additional capacity exists in the Hi-ph, Lo-ph manufacturing processes. A useful planning
tool for this investigation is discussed next.
1.5.4
Marginal Values, Dual Variables, and the Dual
Problem, and Their Planning Uses
Each constraint in an LP model is the material balance constraint of some item, the RHS
constant in that constraint being the availability or the requirement of that item. The
marginal value of that item (also called the marginal value corresponding to that constraint)
is deﬁned to be the rate of change in the optimum objective value of the LP, per unit change
in the RHS constant in the constraint. This marginal value associated with a constraint is
also called the dual variable corresponding to that constraint.
Associated with every LP there is another LP called its dual problem; both share the
same data. In this context, the original problem is called the primal problem. The variables
in the dual problem are the marginal values or dual variables deﬁned above; each of these
variables is associated with a constraint in the primal. Given the primal LP, the derivation of
its dual problem through marginal economic arguments is discussed in many LP books; for

Linear Programming
1-25
example, see Dantzig (1963), Gale (1960), and Murty (1995, 2005b). For an illustration, let
the primal be
Minimize
z = cx
subject to
Ax = b
x ≥0
where x = (x1, . . ., xn)T is the vector of primal variables, and A is of order m × n. Denoting
the dual variable associated with the i-th constraint in Ax = b by πi, the vector of dual
variables associated with these constraints is the row vector π = (π1, . . ., πm). Let the dual
variable associated with the nonnegativity restriction xj ≥0 be denoted by cj for j = 1 to n,
and let c = the row vector (c1, . . ., cn). Then the dual problem is
Maximize
v = πb
subject to
πA + c = c
c ≥0
The dual variables c associated with the nonnegativity constraints x ≥0 in the primal
are called relative or reduced cost coeﬃcients of the primal variables. Given π we can get c
from c = c −πA. Hence, commonly people omit c, and refer to π itself as the dual solution.
When the optimum solution of the dual problem is unique, it is the vector of marginal
values for the primal problem. All algorithms for linear programming have the property
that when they obtain an optimum solution of the primal, they also produce an optimum
solution of the dual; this is explained in Section 1.6. Also, most software packages for LP
provide both the primal and dual optimum solutions when they solve an LP model.
For the fertilizer product mix problem (Equation 1.2) discussed in Example 1.2
(Section 1.2), the dual optimum solution π = (π1, π2, π3) = (5, 5, 0) is unique; hence it
is the vector of marginal values for the problem. As the objective function in the problem is
in units of net proﬁt dollars, this indicates that the marginal values of raw materials RM 1
and RM 2 are both $5/ton in net proﬁt dollars. As the current price of RM 1 delivered to the
company is $50/ton, this indicates that as long as the price charged by an outside vendor
per ton of RM 1 delivered is ≤$50 + 5 = 55/ton, it is worth getting additional supplies of
RM 1 from that vendor. $55/ton delivered is the breakeven price for acquiring additional
supplies of RM 1 for proﬁtability.
In the same way, as the current price of RM 2 delivered to the company is $40/ton, we
know that the breakeven price for acquiring additional supplies of RM 2 for proﬁtability is
$40 + 5 = $45/ton delivered.
Also, since the marginal value of RM 3 is zero, there is no reason to get additional supplies
of RM 3, as no beneﬁt will accrue from it.
This type of analysis is called marginal analysis. It helps companies to determine what
their most critical resources are and how the requirements or resource availabilities can be
modiﬁed to arrive at much better objective values than those possible under the existing
requirements and resource availabilities.
1.5.5
Evaluating the Proﬁtability of New Products
Another major use of marginal values is in evaluating the proﬁtability of new products.
It helps to determine whether they are worth manufacturing, and if so at what level they
should be priced so that they are proﬁtable in comparison with existing product lines.
We will illustrate this again using the fertilizer product mix problem. Suppose the com-
pany’s research chemist has come up with a new fertilizer that he calls lushlawn. Its man-
ufacture requires per ton, as inputs, 3 tons of RM 1, 2 tons of RM 2, and 2 tons of RM 3.

1-26
Operations Research Methodologies
At what rate/ton should lushlawn be priced in the market, so that it is competitive in
proﬁtability with the existing Hi-ph and Lo-ph that the company currently makes?
To answer this question, we computed the marginal values RM 1, RM 2, RM 3 to be
π1 = 5, π2 = 5, π3 = 0.
So, the input packet of (3, 2, 2)T tons of (RM 1, RM 2, RM 3)T needed to manufacture
one ton of lushlawn has value to the company of 3π1 + 2π2 + 2π3 = 3 × 5 + 2 × 5 + 2 × 0 = $25
in terms of net proﬁt dollars. On the supply side, the delivery cost of this packet of raw
materials is 3 × 50 + 2 × 40 + 2 × 60 = $350.
So, clearly, for lushlawn to be competitive with Hi-ph and Lo-ph, its selling price in the
market/ton should be ≥$25 + 350 + (its production cost/ton). The company can conduct
a market survey and determine whether the market will accept lushlawn at a price ≥this
breakeven level. Once this is known, the decision whether to produce lushlawn would be
obvious.
By providing this kind of valuable planning information, the LP model becomes a highly
useful decision making tool.
1.6
Brief Introduction to Algorithms for Solving
LP Models
1.6.1
The Simplex Method
The celebrated simplex method developed by George B. Dantzig in 1947 is the ﬁrst compu-
tationally viable method for solving LPs and systems of linear inequalities (Dantzig, 1963).
Over the years the technology for implementing the simplex method has gone through many
reﬁnements, with the result that even now it is the workhorse behind many of the successful
commercial LP software systems.
Before applying the simplex method, the LP is transformed into a standard form through
simple transformations (like introducing slack variables corresponding to inequality con-
straints, etc.). The general step in the simplex method is called a pivot step. We present
the details of it for the LP in the most commonly used standard form, which in matrix
notation is:
Minimize
z
subject to
Ax = b
cx −z = 0
x ≥0
(1.5)
where A is a matrix of order m × n of full rank m. A basic vector for this problem is a vector
of m variables among the xj, and then −z, of the form (xB, −z) where xB = (xj1, . . ., xjm),
satisfying the property that the submatrix B consisting of the column vectors of these basic
variables is a basis, that is, a nonsingular square submatrix of order m + 1. Let xD denote
the remaining vector of nonbasic variables. The primal basic solution corresponding to this
basic vector is given by
xD = 0,
xB
−z

= B−1
b
0

=
⎛
⎜
⎜
⎜
⎝
b
...
bm
−z
⎞
⎟
⎟
⎟
⎠
(1.6)
The basic vector (xB, −z) is said to be a primal feasible basic vector if the values of
the basic variables in xB in the basic solution in Equation 1.6 satisfy the nonnegativity

Linear Programming
1-27
restrictions on these variables, primal infeasible basic vector otherwise. The basic solution
in Equation 1.6 is called a basic feasible solution (BFS) for Equation 1.5 if (xB, −z) is a
primal feasible basic vector.
There is also a dual basic solution corresponding to the basic vector (xB, −z). If that
dual basic solution is π, then the last row of B−1 will be (−π, 1), so the dual basic solution
corresponding to this basic vector can be obtained from the last row of B−1.
A pivot step in the simplex method begins with a feasible basic vector (xB, −z), say
associated with the basis B, dual basic solution π, and primal BFS given in Equation 1.6.
c = c −πA is called the vector of relative cost coeﬃcients of (xj) corresponding to this basic
vector. The optimality criterion is: c ≥0; if it is satisﬁed the BFS in Equation 1.6 and π are
optimum solutions of the primal LP and its dual; and the method terminates.
If c ̸≥0, any variable xj associated with a cj < 0 will be a nonbasic variable which is
called a variable eligible to enter the present basic vector to obtain a better solution than
the present BFS. One such eligible variable, xs say, is selected as the entering variable. Its
updates column: (a1s, . . ., ams, cs)T = B−1 (column vector of xs in Equation 1.5) is called
the pivot column for this pivot step. The minimum ratio in this pivot step is deﬁned to be
θ = min {bi/ais : 1 ≤i ≤m such that ais > 0}
where (bi) are the values of the basic variables in the present BFS. If the minimum ratio
is attained by i = r, then the r-th basic variable in (xB, −z) will be the dropping variable
to be replaced by xs to yield the next basic vector. The basis inverse corresponding to the
new basic vector is obtained by performing a Gauss–Jordan pivot step on the columns of
the present B−1 with the pivot column and row r as the pivot row. The method goes to
the next step with the new basic vector, and is continued the same way until termination
occurs.
We will illustrate with the fertilizer problem (Equation 1.2) formulated in Example 1.2.
Introducing slack variables x3, x4, x5 corresponding to the three inequalities, and putting
the objective function in minimization form, the standard form for the problem in a detached
coeﬃcient tableau is as shown in Table 1.12.
It can be veriﬁed that (x1, x3, x4, −z) is a feasible basic vector for the problem. The
corresponding basis B and its inverse B−1 are
B =
⎛
⎜
⎜
⎝
2
1
0
0
1
0
1
0
1
0
0
0
−15
0
0
1
⎞
⎟
⎟
⎠,
B−1 =
⎛
⎜
⎜
⎝
0
0
1
0
1
0
−2
0
0
1
−1
0
0
0
15
1
⎞
⎟
⎟
⎠
So the corresponding primal BFS is given by: x2 = x5 = 0, (x1, x3, x4, −z)T = B−1(1500,
1200, 500, 0)T = (500, 700, 500, 7500)T .
From the last row of B−1 we see that the corresponding dual basic solution is π =
(0, 0, −15).
TABLE 1.12
Original Tableau for Fertilizer Problem
x1
x2
x3
x4
x5
−z
RHS
2
1
1
0
0
0
1500
1
1
0
1
0
0
1200
1
0
0
0
1
0
500
−15
−10
0
0
0
1
0
xj ≥0 for all j, min z

1-28
Operations Research Methodologies
TABLE 1.13
Pivot Step to Update B−1
B−1
PC
0
0
1
0
0
1
0
−2
0
1
0
1
−1
0
1
0
0
15
1
−10
0
0
1
0
0
1
−1
−1
0
0
0
1
−1
0
1
0
10
5
1
0
So, the relative cost vector is c = (0, 0, 15, 1)(the matrix consisting of columns of x1 to x5
in Table 1.12) = (0, −10, 0, 0, 0) ̸≥0. So, x2 is the only eligible variable to enter; we select it
as the entering variable. The pivot column = updated column of x2 = B−1(orginal column
of x2) = (0, 1, 1, −10)T .
The minimum ratio θ = min {700/1,500/1} = 500, attained for r = 3. So the third basic
variable in the present basic vector, x4, is the dropping variable to be replaced by x2. So,
the next basic vector will be (x1, x3, x2, −z). To update the basis inverse we perform the
pivot step with the pivot element enclosed in a box. PC = pivot column (Table 1.13).
So the bottom matrix on the left is the basis inverse associated with the new basic vector
(x1, x3, x2, −z). It can be veriﬁed that the BFS associated with this basic vector is given
by: x4 = x5 = 0, (x1, x3, x2, −z) = (500, 200, 500, 12,500).
Hence, in this pivot step the objective function to be minimized in this problem, z, has
decreased from −7500 to −12,500. The method now goes to another step with the new basic
vector, and repeats until it terminates.
The method is initiated with a known feasible basic vector. If no feasible basic vector
is available, a Phase I problem with a known feasible basic vector is set up using artiﬁcial
variables; solving the Phase I problem by the same method either gives a feasible basic
vector for the original problem, or concludes that it is infeasible.
1.6.2
Interior Point Methods for LP
Even though a few isolated papers have discussed some interior point approaches for LP as
early as the 1960s, the most explosive development of these methods was triggered by the
pioneering paper by Karmarkar (1984). In it he developed a new interior point method for
LP, proved that it is a polynomial time method, and outlined compelling reasons why it
has the potential to be also practically eﬃcient and likely to beat the simplex method for
large-scale problems. In the tidal wave that followed, many diﬀerent interior point methods
were developed. Computational experience has conﬁrmed that some of them do oﬀer an
advantage over the simplex method for solving large-scale sparse problems.
We will brieﬂy describe a popular method known as the primal-dual path following interior
point method from Wright (1996). It considers the primal LP: minimize cT x, subject to
Ax = b, x ≥0; and its dual in which the constraints are: AT y + s = c, s ≥0, where A is a
matrix of order m × n and rank m (in Section 1.5, we used the symbol c to denote s). The
system of primal and dual constraints put together is:
Ax = b
AT y + s = c
(x, s) ≥0
(1.7)
In LP literature, a feasible solution (x, y, s) to Equation 1.7 is called an interior feasible
solution if (x, s) > 0. Let F denote the set of all feasible solutions of Equation 1.7, and F0 the

Linear Programming
1-29
set of all interior feasible solutions. For any (x, y, s) ∈F0 deﬁne X = diag (x1, . . ., xn), the
square diagonal matrix of order n with diagonal entries x1, . . ., xn; and S = diag (s1, . . ., sn).
The Central Path
This path, C, is a nonlinear curve in F0 parametrized by a positive parameter τ > 0. For
each τ > 0, the point (xτ, yτ, sτ) ∈C satisﬁes: (xτ, sτ) > 0 and
AT yτ + sτ = cT
Axτ = b
xτ
j sτ
j = τ,
j = 1, . . ., n
If τ = 0, the above equations deﬁne the optimality conditions for the LP. For each τ > 0,
the solution (xτ, yτ, sτ) is unique, and as τ decreases to 0 the central path converges to
the center of the optimum face of the primal, dual pair of LPs.
Optimality Conditions
For the primal, dual pair of LPs under discussion, an (xr = (xr
j); yr = (yr
i ), sr = (sr
j)) of pri-
mal and dual feasible solutions is an optimum solution pair for the two problems iﬀxr
jsr
j = 0
for all j. These conditions are called complementary slackness optimality conditions.
We will use the symbol e to denote the column vector consisting of all 1s in Rn. From
optimality conditions, solving the LP is equivalent to ﬁnding a solution (x, y, s) satisfying
(x, s) ≥0, to the following system of 2n + m equations in 2n + m unknowns.
F(x, y, s) =
⎡
⎣
AT y + s −c
Ax −b
XSe
⎤
⎦= 0
(1.8)
This is a nonlinear system of equations because of the last equation.
The General Step in the Method
The method begins with an interior feasible solution to the problem. If no interior feasible
solution is available to initiate the method, it could be modiﬁed into an equivalent problem
with an initial interior feasible solution by introducing artiﬁcial variables.
Starting with an interior feasible solution, in each step the method computes a direction
to move at that point, and moves in that direction to the next interior feasible solution,
and continues from there the same way.
Consider the step in which the current interior feasible solution at the beginning is (x, y, s).
So, (x, s) > 0. Also, the variables in y are unrestricted in sign in the problem.
Once the direction to move from the current point (x, y, s) is computed, we may move
from it only a small step length in that direction, and since (x, s) > 0, such a move in any
direction will take us to a point that will continue satisfying (x, s) > 0. So, in computing
the direction to move at the current point, the nonnegativity constraints (x, s) ≥0 can
be ignored. The only remaining conditions to be satisﬁed for attaining optimality are the
equality conditions (Equation 1.8). So the direction ﬁnding routine concentrates only on
trying to satisfy Equation 1.8 more closely.
Equation 1.8 is a square system of nonlinear equations; (2n + m) equations in (2n + m)
unknowns. It is nonlinear because the third condition in Equation 1.8 is nonlinear. Expe-
rience in nonlinear programming indicates that the best directions to move in algorithms
for solving nonlinear equations are either the Newton direction or some modiﬁed Newton
direction. So, this method uses a modiﬁed Newton direction to move. To deﬁne that, two

1-30
Operations Research Methodologies
parameters are used: μ (an average complementary slackness property violation measure) =
xT s/n, and σ ∈[0,1] (a centering parameter). Then the direction for the move denoted by
(Δx, Δy, Δs) is the solution to the following system of linear equations
⎛
⎝
0
AT
I
A
0
0
S
0
X
⎞
⎠
⎛
⎝
Δx
Δy
Δs
⎞
⎠=
⎛
⎝
0
0
−XSe + σμe
⎞
⎠
where 0 in each place indicates the appropriate matrix or vector of zeros, I the unit matrix
of order n, and e indicates the column vector of order n consisting of all 1s. If σ = 1, the
direction obtained will be a centering direction, which is a Newton direction toward the
point (xμ, yμ, sμ) on C at which the products xjsj of all complementary pairs in this
primal, dual pair of problems are = μ. Many algorithms choose σ from the open interval
(0,1) to trade oﬀbetween twin goals of reducing μ and improving centrality.
Then take the next point to be (ˆx, ˆy, ˆs) = (x, y, s) + α(Δx, Δy, Δs), where α is a positive
step length selected so that (ˆx, ˆs) remains > 0.
With (ˆx, ˆy, ˆs) as the new current interior feasible solution, the method now goes to the
next step.
It has been shown that the sequence of interior feasible solutions obtained in this method
converges to a point in the optimum face.
A Gravitational Interior Point Method for LP
This is a new type of interior point method discussed recently in Murty (2006). We will
describe the main ideas in this method brieﬂy. It considers LP in the form: minimize
z(x) = cx subject to Ax ≥b, where A is a matrix of order m × n. Let K denote the set
of feasible solutions, and K0 its interior = {x:Ax > b}. Let Ai. denote the i-th row vector of
A; assume ||c|| = ||Ai.|| = 1 for all i.
This method also needs an interior feasible solution for initiating the method. Each iter-
ation in the method consists of two steps. We will discuss each of these steps.
Step 1: Centering step: Let x0 be the current interior feasible solution. The orthogonal
distance of the point x0 to the boundary hyperplane deﬁned by the equation Ai.x = bi is
Ai.x0 −bi for i = 1 to n. The minimum value of these orthogonal distances is the radius of
the largest sphere that can be constructed within K with x0 as center; hence its radius is
δ0 = min{Ai.x0 −bi : i = 1, . . ., m}.
This step tries to move from x0 to another interior feasible solution on the objective
plane H0 = {x : cx = cx 0} through x0, to maximize the radius of the sphere that can be
constructed within K with the center in H0 ∩K0. That leads to another LP: max δ subject
to δ ≤Ai.x −bi, i = 1, . . ., m, and cx = cx 0.
In the method, this new LP is solved approximately using a series of line searches on
H0 ∩K0 beginning with x0. The directions considered for the search are orthogonal projec-
tions of normal directions to the facetal hyperplanes of K on the hyperplane {x : cx = 0},
which form the set: P = {P.i = (I −cT c)AT
i.: i = 1, . . ., m}. There are other line search direc-
tions that can be included in this search, but this set of directions has given excellent results
in the limited computational testing done so far.
Let xr be the current center. At xr, P.i ∈P is a proﬁtable direction to move (i.e., this
move leads to a better center) only if all the dot products At.P.i for t ∈T have the same sign,
where T = {t : t ties for the minimum in {Aq.xr −bq : q = 1, . . ., m}}. If P.i is a proﬁtable

Linear Programming
1-31
direction to move at xr, the optimum step length is the optimum α in the following LP
in two variables θ, α: Max θ subject to θ −αAt.P.i ≤At.xr −bt, t = 1, . . ., m, which can be
solved very eﬃciently by a special algorithm.
The line searches are continued either until a point is reached where none of the directions
in P are proﬁtable, or the improvement in the radius of the sphere in each line search
becomes small.
Step 2: Descent Step: Let x be the ﬁnal center in H0 ∩K0 selected in the centering step.
At x two descent directions are available, −cT and x −˜x, where ˜x is the center selected
in the previous iteration. Move from ˜x in the direction among these two that gives the
greatest decrease in the objective value, to within ∈of the boundary, where ∈is a tolerance
for interiorness.
If x∗is the point obtained after the move, go to the next iteration with x∗as the new
current interior feasible solution.
It has been proved that this method takes no more than O(m) iterations if the centering
step is carried out exactly in each iteration. The method with the approximate centering
strategy is currently undergoing computational tests. The biggest advantage of this method
over the others is that it needs no matrix inversions, and hence oﬀers a fast descent method
to solve LPs whether they are sparse or not. Also, the method is not aﬀected by any
redundant constraints in the model.
1.7
Software Systems Available for Solving LP Models
There are two types of software. Solver software takes an instance of an LP model as input
and applies one or more solution methods and outputs the results. Modeling software does
not incorporate solution methods; it oﬀers a computer modeling language for expressing
LP models, features for reporting, model management, and application development, in
addition to a translator for the language. Most modeling systems oﬀer a variety of bundled
solvers.
The most commonly talked about commercial solvers for LP are CPLEX, OSL, MATLAB,
LINDO, LOQO, EXCEL, and several others. The most common modeling software systems
are AMPL, GAMS, MPL, and several others.
Detailed information about these systems and their capabilities and limitations can be
obtained from the paper by Fourer (2005), and the Web sites for the various software systems
and their vendors are also given there.
Also, users can submit jobs to the NEOS server maintained by Argonne National Labo-
ratory and retrieve job results. See the Web site http://www-neos.mcs.anl.gov/for details.
You can see a complete list of currently available solvers and detailed information on each of
them at the Optimization Software Guide Web site (http://www-fp.mcs.anl.gov/otc/Guide/
SoftwareGuide/Categories/linearprog.html).
1.8
Multiobjective LP Models
So far we have discussed LP models for problems in which there is a well-deﬁned single
objective function to optimize. But many real world applications involve several objec-
tive functions simultaneously. For example, most manufacturing companies are intensely
interested in attaining large values for several things, such as the company’s net proﬁt, its
market share, and the public’s recognition of the company as a progressive organization.

1-32
Operations Research Methodologies
In all such applications, it is extremely rare to have one feasible solution that simulta-
neously optimizes all of the objective functions. Typically, optimizing one of the objective
functions has the eﬀect of moving another objective function away from its most desirable
value. These are the usual conﬂicts among the objective functions in multiobjective models.
Under such conﬂicts, a multiobjective problem is not really a mathematically well-posed
problem unless information on how much value of one objective function can be sacriﬁced for
unit gain in the value of another is given. Such tradeoﬀinformation is usually not available,
but when it is available, it makes the problem easier to analyze.
Because of the conﬂicts among the various objective functions, there is no well-accepted
concept of optimality in multiobjective problems. Concepts like Pareto optima, or nondom-
inated solutions, or eﬃcient points, or equilibrium solutions have been deﬁned, and math-
ematical algorithms to enumerate all such solutions have been developed. Usually there
are many such solutions, and there are no well-accepted criteria to select one of them for
implementation, with the result that all this methodology remains unused in practice.
The most practical approach, and one that is actually used by practitioners is the goal
programming approach, originally proposed by Charnes and Cooper (1961, 1977), which
we will now discuss. This material is based on the section on goal programming in Murty
(2005b and manuscript under preparation).
Let c1x, . . ., ckx be the various objective functions to be optimized over the set of feasible
solutions of Ax = b, x ≥0. The goal programming approach has the added conveniences that
diﬀerent objective functions can be measured in diﬀerent units, and that it is not necessary
to have all the objective functions in the same (either maximization or minimization) form.
So, some of the objective functions among c1x, . . ., ckx may have to be minimized, others
maximized.
In this approach, instead of trying to optimize each objective function, the decision maker
is asked to specify a goal or target value that realistically is the most desirable value for that
function. For r = 1 to k, let gr be the speciﬁed goal for crx.
At any feasible solution x, for r = 1 to k, we express the deviation in the r-th objective
function from its goal, crx −gr, as a diﬀerence of two nonnegative variables
crx −gr = u+
r −u−
r
where u+
r , u−
r are the positive and negative parts of the deviation crx −gr, that is,
u+
r =

0
if crx −gr ≤0
crx −gr
if crx −gr > 0
u−
r =

0
if crx −gr ≥0
−(crx −gr)
if crx −gr < 0
The goal programming model for the original multiobjective problem will be a single
objective problem in which we try to minimize a linear penalty function of these deviation
variables of the form k
1 (αru+
r + βru−
r ), where αr, βr ≥0 for all r. We now explain how
the coeﬃcients αr, βr are to be selected.
If the objective function crx is one which is desired to be maximized, then feasible solu-
tions x which make u−
r = 0 and u+
r ≥0 are desirable, while those which make u+
r = 0 and
u−
r > 0 become more and more undesirable as the value of u−
r increases. In this case u+
r mea-
sures the (desirable) excess in this objective value over its speciﬁed target, and u−
r measures
the (undesirable) shortfall in its value from its target. To guarantee that the algorithm seeks
solutions in which u−
r is as small as possible, we associate a positive penalty coeﬃcient βr
with u−
r , and include a term of the form αru+
r + βru−
r (where αr = 0, βr > 0) in the penalty
function that the goal programming model tries to minimize. βr > 0 measures the loss or

Linear Programming
1-33
penalty per unit shortfall in the value of crx from its speciﬁed goal of gr. The value of βr
should reﬂect the importance attached by the decision maker for attaining the speciﬁed goal
on this objective function (higher values of βr represent greater importance). The coeﬃcient
αr of u+
r is chosen to be 0 in this case because our desire is to see u+
r become positive as
far as possible.
If the objective function crx is one which is desired to be minimized, then positive values
for u−
r are highly desirable, whereas positive values for u+
r are undesirable. So, for these r
we include a term of the form αru+
r + βru−
r , where αr > 0 and βr = 0 in the penalty function
that goal programming model minimizes. Higher values of αr represent greater importance
attached by the decision maker to the objective functions in this class.
There may be some objective functions cr(x) in the original multiobjective problem for
which both positive and negative deviations are considered undesirable. For objective func-
tions in this class we desire values that are as close to the speciﬁed targets as possible. For
each such objective function we include a term αru+
r + βru−
r , with both αr and βr > 0, in
the penalty function that the goal programming model minimizes.
So, the goal programming model is the following single objective problem.
Minimize
k

1
(αru+
r + βru−
r )
subject to
crx −gr = u+
r −u−
r ,
r = 1 to k
Ax = b
x, u+
r , u−
r ≥0,
r = 1 to k
As this model is a single objective function linear program it can be solved by the algo-
rithms discussed earlier. Also, as all αr and βr ≥0, and from the manner in which the
values for αr, βr are selected, (u+
r )(u−
r ) will be zero for all r in an optimum solution of this
model; that is, u+
r = maximum{crx −gr, 0}, u−
r = minimum{0, −(crx −gr)} will hold for
all r. Hence solving this model will try to meet the targets set for each objective function,
or deviate from them in the desired direction as far as possible.
The optimum solution of this goal programming model depends critically on the goals
selected and on the choice of the penalty coeﬃcients α = (α1, . . ., αk), β = (β1, . . ., βk).
Without any loss of generality we can assume that the vectors α, β are scaled so that
k
r=1 (αr + βr) = 1. Then the larger an αr or βr, the more the importance the decision
maker places on attaining the goal set for cr(x). Again, there may not be universal agree-
ment among all the decision makers involved on the penalty coeﬃcient vectors α, β to be
used; it has to be determined by negotiations among them. Once α and β are determined,
an optimum solution of the goal programming model is the solution to implement. Solving
it with a variety of penalty vectors α and β and reviewing the various optimum solutions
obtained may make the choice in selecting one of them for implementation easier. One can
also solve the goal programming model with diﬀerent sets of goal vectors for the various
objective functions. This process can be repeated until at some stage, an optimum solu-
tion obtained for it seems to be a reasonable one for the original multiobjective problem.
Exploring with the optimum solutions of this model for diﬀerent goal and penalty coeﬃ-
cient vectors in this manner, one can expect to get a practically satisfactory solution for
the multiobjective problem.
Example 1.7
As an example, consider the problem of the fertilizer manufacturer to determine the best
values for x1, x2, the tons of Hi-ph and Lo-ph fertilizer to manufacture daily, discussed in

1-34
Operations Research Methodologies
Example 1.2 (Section 1.2). There we considered only the objective of maximizing the net
daily proﬁt, c1x = $(15x1 + 10x2).
Suppose the fertilizer manufacturer is also interested in maximizing the market share of
the company, which is usually measured by the total daily fertilizer sales of the company,
c2x = (x1 + x2) tons.
In addition, suppose the fertilizer manufacturer is also interested in maximizing the pub-
lic’s perception of the company as being one on the forefront of technology, measured by
the Hi-ph tonnage sold by the company daily, c3x = x1tons. So, we now have three objective
functions c1x, c2x, c3x, all to be maximized simultaneously, subject to the constraints in
Equation 1.2.
Suppose the company has decided to set a goal of g1 = $13,000 for daily net proﬁt;
g2 = 1150 tons for total tonnage of fertilizer sold daily; and g3 = 400 tons for Hi-ph ton-
nage sold daily. Also, suppose the penalty coeﬃcients associated with shortfalls in these
goals are required to be 0.5, 0.3, 0.2, respectively. With this data, the goal programming
formulation of this problem, after transferring the deviation variables to the left-hand side, is
Minimize
0.5u−
1
+ 0.3u−
2
+ 0.2u−
3
subject to
15x1 + 10x2 + u−
1 −u+
1
= 13, 000
x1 +
x2
+
u−
2 −u+
2
=
1150
x1
+
u−
3 −u+
3 =
400
2x1 +
x2
≤
1500
x1 +
x2
≤
1200
x1
≤
500
x1,
x2,
u−
1 ,
u+
1 ,
u−
2 , u+
2
u−
3 , u+
3 ≥
0
■
An optimum solution of this problem is ˆx = (ˆx1, ˆx2)T = (350, 800)T . ˆx attains the goals
set for net daily proﬁt and total fertilizer tonnage sold daily, but falls short of the goal on
the Hi-ph tonnage sold daily by 50 tons. ˆx is the solution for this multiobjective problem
obtained by goal programming, with the goals and penalty coeﬃcients given above.
References
1. Brown, G. and Graves, G. 1977, “Elastic Programming,” Talk given at an ORSA-TIMS
Conference.
2. Charnes, A. and Cooper, W. W. 1961, Management Models and Industrial Applications of
LP, Wiley, New York.
3. Charnes, A. and Cooper, W. W. 1977, “Goal Programming and Multiple Objective Opti-
mizations, Part I,” European Journal of Operations Research, 1, 39–54.
4. Chinnek, J. W. and Dravineks, E. W. 1991, “Locating Minimal Infeasible Sets in Linear
Programming,” ORSA Journal on Computing, 3, 157–168.
5. Dantzig, G. B. 1963, Linear Programming and Extensions, Princeton University Press,
Princeton, NJ.
6. Dantzig, G. B. and Thapa, M. N. 1997, Linear Programming, 1. Introduction, 2. Theory
and Extensions, Springer-Verlag, New York.
7. Fourer, R. 2005, “Linear Programming Software Survey,” ORMS Today, 32(3), 46–55.
8. Gale, D. 1960, The Theory of Linear Economic Models, McGraw-Hill, New York.
9. Hitchcock, F. L. 1941, “The Distribution of a Product from Several Sources to Numerous
Localities,” Journal of Mathematics and Physics, 20, 224–230.

Linear Programming
1-35
10. Karmarkar, N. K. 1984, “A New Polynomial-Time Algorithm for Linear Programming,”
Combinatorica, 4, 373–395.
11. Koopmans, T. C. 1949, “Optimum Utilization of the Transportation System,” Economet-
rica, 17, Supplement.
12. Murty, K. G. 1995, Operations Research—Deterministic Optimization Models, Prentice
Hall, Englewood Cliﬀs, NJ.
13. Murty, K. G., Kabadi, S. N., and Chandrasekaran, R. 2000, “Infeasibility Analysis
for Linear Systems, A Survey,” The Arabian Journal for Science and Engineering,
25(1C), 3–18.
14. Murty, K. G. 2004, Computational and Algorithmic Linear Algebra and n-Dimensional
Geometry, Freshman-Sophomore level linear algebra book available as download at:
http://ioe.engin.umich.edu/people/fac/books/murty/algorithmic linear algebra/.
15. Murty, K. G. 2005a, “A Gravitational Interior Point Method for LP,” Opsearch, 42(1),
28–36.
16. Murty, K. G. 2005b, Optimization Models for Decision Making: Volume 1 (Junior Level),
at http://ioe.engin.umich.edu/people/fac/books/murty/opti model/.
17. Murty, K. G. Optimization Models for Decision Making: Volume 2 (Masters Level), under
preparation.
18. Murty, K. G., Wan, Y.-W., Liu, J., Tseng, M., Leung, E., Kam, K., Chiu, H.,
2005a, “Hongkong International Terminals Gains Elastic Capacity Using a Data-Intensive
Decision-Support System,” Interfaces, 35(1), 61–75.
19. Murty, K. G., Liu, J., Wan, Y.-W., and Linn, R. 2005b, “A DSS (Decision Support System)
for Operations in a Container Terminal,” Decision Support Systems, 39(3), 309–332.
20. Murty, K. G. 2006, “A New Practically Eﬃcient Interior Point Method for LP,” Algorithmic
Operations Research, Dantzig Memorial Issue, 1, 1 (January 2006)3–19; paper can be seen
at the website: http://journals.hil.unb.ca/index.php/AOR/index.
21. Stigler, G. J. 1945, “The Cost of Subsistence,” Journal of Farm Economics, 27.
22. Wright, S. J. 1996, Primal-Dual Interior-Point Methods, SIAM, Philadelphia.


2
Nonlinear Programming
Theodore B. Trafalis and
Robin C. Gilbert
University of Oklahoma
2.1
Introduction............................................
2-1
Problem Statement • Optimization Techniques
2.2
Unconstrained Optimization .........................
2-3
Line Searches • Multidimensional Optimization
2.3
Constrained Optimization............................ 2-15
Direction Finding Methods • Transformation Methods
2.4
Conclusion ............................................. 2-19
References .................................................... 2-20
2.1
Introduction
Nonlinear programming is the study of problems where a nonlinear function has to be
minimized or maximized over a set of values in Rn delimited by several nonlinear equal-
ities and inequalities. Such problems are extremely frequent in engineering, science, and
economics. Since World War II, mathematicians have been engaged to solve problems of
resource allocation, optimal design, and industrial planning involving nonlinear objective
functions as well as nonlinear constraints. These problems required the development of new
algorithms that have been beneﬁted with the invention of digital computers. This chapter
is concentrated on presenting the fundamentals of deterministic algorithms for nonlinear
programming. Our purpose is to present a summary of the basic algorithms of nonlinear
programming. A pseudocode for each algorithm is also presented. We believe that our pre-
sentation in terms of a fast cookbook for nonlinear programming algorithms can beneﬁt the
practitioners of operations research and management science.
The chapter is organized as follows. In Section 2.1.1, the deﬁnition of the general nonlinear
programming problem is discussed. In Section 2.1.2, optimization techniques are discussed.
In Section 2.2, nonlinear deterministic techniques for unconstrained optimization are dis-
cussed. Constrained optimization techniques are discussed in Section 2.3. Finally, Section 2.4
concludes the chapter.
2.1.1
Problem Statement
Let X ⊆Rn with n ∈N*. Consider the following optimization problem with equality and
inequality constraints:
minimize
x ∈X
f(x)
subject to
g(x) ≤0
h(x) = 0
(2.1)
2-1

2-2
Operations Research Methodologies
where f: X →Rm, g: X →Rp, and h: X →Rq with (m, p, q) ∈N3. The vector function f is
called the objective function, the vector function g is called the inequality constraint function,
and the vector function h is called the equality constraint function. The vector x ∈X is called
the optimization variable. If x is such that the constraints of problem (2.1) hold, then x is
called a feasible solution. Some problems generalize the above formulation and consider f,
g, and h to be random vectors. In this case, we have a stochastic optimization problem. By
opposition we may say that problem (2.1) is a deterministic optimization problem.
The categories of problems vary with the choices of X, m, p, and q. If p = q = 0 then
we say that we have an unconstrained optimization problem. Conversely, if one of q or p
is nonzero then we say that we have a constrained optimization problem. The categories of
problems when m varies are the following:
• If m = 0 we have a feasibility problem.
• If m = 1 we have a classical optimization problem. The space of feasible solutions
is called the feasible set. If a vector x is a feasible solution such that f(x) ≤f(x)
for every x ∈X, then x is called optimal solution and f(x) is called optimal value.
If X is continuous and if the objective function is convex then we have a convex
optimization problem that belongs to a very important category of optimization
problems.
• If m ≥2 then the above problem is a multiobjective optimization problem where
the space of feasible solutions is called decision space. The optimality notion in
the case m = 1 is replaced by the Pareto optimality and “optimal” solutions are
said to be Pareto optimal. A feasible solution x is Pareto optimal if for every x
in the decision space we have fi(x) ≤fi(x) for every i ∈[[1, m]].
If X ⊆Zn then we have an integer optimization problem. If only some of the components of
the optimization variables are integers then we have a mixed-integer optimization problem.
In the other cases we have a continuous optimization problem.
Most of the optimization problems found in the literature deal with the case m = 1 for
deterministic optimization problems. From now on, we will always consider this case in all of
the following discussions. In this class of optimization problems we have many subcategories
that depend on the choice of f and the choice of the constraint functions. The most common
cases are:
• The objective function f and the constraint functions g and h are linear. In this
case we have a linear optimization problem.
• The objective function f is quadratic and the constraint functions g and h are
linear. In this case we have a quadratic optimization problem.
• The objective function f is quadratic and the constraint functions g and h are
quadratic. In this case we have a quadratically constrained quadratic optimization
problem.
• The objective function f is linear and subject to second-order cone constraints
(of the type ||Ax + b||2 ≤⟨c · x⟩+ d). In this case we have a second-order cone
optimization problem.
• The objective function f is linear and subject to a matrix inequality. In this case
we have a semideﬁnite optimization problem.

Nonlinear Programming
2-3
• The objective function f and the constraint functions g and h are nonlinear. In
this case we have a nonlinear optimization problem.
We will be dealing with the latter case in this chapter.
2.1.2
Optimization Techniques
There are three major groups of optimization techniques in nonlinear optimization.
These techniques are summarized in the following list:
Deterministic techniques: These methods are commonly used in convex optimiza-
tion. Convex nonlinear optimization problems have been extensively studied in
the last 60 years and the literature is now thriving with convergence theorems
and optimality conditions for them. The backbone of most of these methods is the
so-called descent algorithm. The steepest descent method, the Newton method,
the penalty and barrier methods, and the feasible directions methods fall in this
group.
Stochastic techniques: These methods are based on probabilistic meta-algorithms.
They
seek
to
explore
the
feasibility
region
by
moving
from
feasible
solutions to feasible solutions in directions that minimize the objective value.
They have been proved to converge toward local minima by satisfying certain
conditions. Simulated annealing is an example of a stochastic technique (see
Refs. [30,36,47]).
Heuristic strategies: These methods are based on heuristics for ﬁnding good feasible
solutions to very complicated optimization problems. They may not work for
some problems and their behavior is not yet very well understood. They are
used whenever the ﬁrst two groups of techniques fail to ﬁnd solutions or if these
techniques cannot be reasonably used to solve given problems. Local searches
and swarm intelligence are some of the numerous heuristic techniques introduced
in the past few years (see Refs. [9,12,20,21,47]).
2.2
Unconstrained Optimization
In this section, several common algorithms for continuous nonlinear convex optimization are
reviewed for the cases where no constraints are present. Section 2.2.1 reviews the most usual
line searches and Section 2.2.2 presents methods for optimizing convex functions on Rn.
2.2.1
Line Searches
The purpose of line searches is to locate a minimum of real-valued functions over an interval
of R. They are extremely fundamental procedures that are frequently used as subroutines
in other optimization algorithms based on descent directions. The objective function f is
required to be strictly unimodal over a speciﬁed interval [a, b] of R; that is, there exists a
x that minimizes f over [a, b] and for every (x1, x2) ∈[a, b]2 such that x1 < x2 we have that
x ≤x1 ⇒f(x1) < f(x2) and x2 ≤x ⇒f(x2) < f(x1). Depending on the line search method,
f may need to be diﬀerentiable.

2-4
Operations Research Methodologies
There are at least three types of line searches:
• Line searches based on region elimination. These methods exploit the assumption
of strict unimodality to sequentially shrink the initial interval until it reaches a
negligible length. The Fibonacci search and the bisection search are very suc-
cessful line search methods (see the sections “Fibonacci Search” and “Bisection
Search”).
• Lines searches based on curve ﬁtting. These methods try to exploit the shape of
the objective function to accelerate the bracketing of a minimum. The quadratic
ﬁt line search and the cubic ﬁt line search are very popular curve ﬁtting line
searches (see the section “Quadratic Fit Search”).
• Approximated line searches. Contrary to the above line searches, these methods
try to ﬁnd acceptable “minimizers” with the minimum function evaluations pos-
sible. This is often necessary when it is computationally diﬃcult to evaluate the
objective function. The backtracking search is an approximated line search that
is adapted with a diﬀerent set of stopping rules (notably the popular Armijo’s
rule). See Algorithm 2.5.
Bracketing of the Minimum
Line searches often require an initial interval containing a minimum to be provided before
starting the search. The following technique provides a way to bracket the minimum of a
strictly unimodal function deﬁned over an open interval of R. It starts from an initial point
belonging to the domain of deﬁnition of the objective function and exploits the strict uni-
modality assumption to gradually expand the length of an inspection interval by following
the decreasing slope of the function. It stops if the initial point does not belong to the
domain of deﬁnition or whenever the slope of the function increases.
Algorithm 2.1 is a modiﬁed version of a method credited to Swann [52]. This vari-
ant can handle strictly unimodal functions that are deﬁned on an open interval of R
with the supplementary assumption that their values are ∞outside this interval. Such
modiﬁcation is quite useful when this bracketing technique is used with barrier func-
tions (see the section “Sequential Unconstrained Minimization Techniques”). This method
requires only functional evaluations and a positive step parameter δ. It starts from an
initial point x, determines the decreasing slope of the function (lines 1–11), and then
detects a change in the slope by jumping successively on the domain of deﬁnition by a
step length of δ times a power of 2 (lines 12–18). If the current test point is feasible,
then the step length at the next iteration will be doubled. Otherwise the step length will
be halved.
Fibonacci Search
The Fibonacci line search is a derivative-free line search based on the region elimination
scheme that ﬁnds the minimizer of a strictly unimodal function on a closed bounded interval
of R. This method is credited to Kiefer [29]. As the objective function is strictly unimodal,
it is possible to eliminate successively parts of the initial interval by knowing the objective
function value at four diﬀerent points of the interval. The ways to manage the locations
of the test points vary with the region-elimination-based line searches but the Fibonacci
search is specially designed to reduce the initial interval to a given length with the minimum

Nonlinear Programming
2-5
Algorithm 2.1: Modiﬁed Swann’s bracketing method.
Input: function f, starting point x, step parameter δ > 0.
Output: interval bracketing the minimum.
k ←1, fail ←k, δa ←δ, δb ←δ, if f(x) = ∞then return Ø;
1
repeat
2
ak ←x −δa, if f(ak) = ∞then δa ←δa/2, fail ←k + 1;
3
until f(ak) < ∞;
4
repeat
5
bk ←x + δb, if f(bk) = ∞then δb ←δb/2, fail ←k + 1;
6
until f(bk) < ∞;
7
if f(x) ≤f(ak) and f(x) ≤f(bk) then return [ak, bk];
8
ak+1 ←x;
9
if f(x) ≤f(bk) then bk+1 ←ak, δ ←δa, σ ←−1;
10
else bk+1 ←bk, δ ←δb, σ ←1;
11
while f(bk+1) < f(ak+1) do
12
k ←k + 1, ak+1 ←bk, if fail = k then δ ←δ/2 else δ ←2δ;
13
repeat
14
bk+1 ←bk + σδ, if f(bk+1) = ∞then δ ←δ/2, fail ←k + 1;
15
until f(bk+1) < ∞;
16
end
17
if σ > 0 then return [ak, bk+1] else return [bk+1, ak];
18
number of functional evaluations. The Fibonacci search is notably slightly better than the
golden section search for reducing the search interval.
As its name suggests, the method is based on the famous Fibonacci sequence (Fn)n∈N
deﬁned by F 0 = F1 = 1 and Fn+2 = Fn+1 + Fn for n ∈N. The test points are chosen such
that the information about the function obtained at the previous iteration is used in the
next. In this way the search requires only a single functional evaluation per iteration in its
main loop. Contrary to the golden section search with which the Fibonacci search is almost
identical, the reduction interval ratio at each iteration varies.
Algorithm 2.2 implements the Fibonacci search. The method requires only functional
evaluations and an initial interval [a, b]. It is assumed that a termination scalar ε > 0 that
represents the maximum length of the ﬁnal bracketing interval is given. It is suggested that
ε should be at least equal to the square root of the spacing of ﬂoating point numbers [46].
The algorithm determines ﬁrst the needed number of iterations (line 1) of its main loop
(lines 3–10). Then it returns the mid-point of the ﬁnal bracketing interval as an estimation
of the minimizer of the objective function (line 11). The region-elimination scheme is loosely
the following: if the functional values are greater on the left of the current interval (line 5),
then the leftmost part is deleted and a new test point is created on the right part (line 6).
Conversely, if the functional values are greater on the right (line 7) then the rightmost part
is deleted and a new test point on the left is created (line 8). Naturally this scheme works
only for strictly unimodal functions.
Bisection Search
Like the Fibonacci search, the bisection search is based on the region elimination scheme.
Unlike the Fibonacci search, it requires the objective function to be diﬀerentiable and
performs evaluations of the derivative of f. The bisection search, which is sometimes referred

2-6
Operations Research Methodologies
Algorithm 2.2: Fibonacci search.
Input: function f, initial interval [a, b].
Output: approximated minimum.
Data: termination parameter ε.
k ←0, determine n > 2 such that Fn > (b −a)/ε;
1
u ←a + (Fn−2/Fn)(b −a), v ←a + (Fn−1/Fn)(b −a);
2
while k < n −2 do
3
k ←k + 1;
4
if f(u) > f(v) then
5
a ←u, u ←v, v ←a + (Fn−k−1/Fn−k)(b −a);
6
else
7
b ←v, v ←u, u ←a + (Fn−k−2/Fn−k)(b −a);
8
end
9
end
10
if f(u) > f(u + ε/2) then return (u + b)/2 else return (a + u)/2;
11
to as the Bolzano search, is one of the oldest line searches. It ﬁnds the minimizer of a pseu-
doconvex function on a closed bounded interval [a, b] of R. A diﬀerentiable function is said
to be pseudoconvex on an open set X of Rn if for every (x1, x2) ∈X2 such that 0 ≤∇f(x1)t
(x2 −x1), we have f(x1) ≤f(x2). A pseudoconvex function is strictly unimodal. In our case
n = 1, therefore, a real-valued function f is pseudoconvex if for every (x1, x2) ∈(a, b)2 such
that 0 ≤df(x1)(x2 −x1) we have f(x1) ≤f(x2).
Algorithm 2.3 implements the bisection search. This search uses the information conveyed
by the derivative function to sequentially eliminate portions of the initial bracketing interval
[a, b]. The algorithm determines ﬁrst the needed number of iterations (line 1) of its main loop
(lines 2–8). Then it returns the mid-point of the ﬁnal bracketing interval as an estimation
of the minimizer of the objective function (line 9). The main loop works like the main loop
of the Fibonacci search: if the derivative value at the mid-point is strictly positive then the
rightmost part of the current interval is deleted; otherwise if the derivative value is strictly
negative then the leftmost part is deleted (line 6). In the case where the derivative value at
the mid-point is null then the algorithm stops as it has reached optimality (line 4).
Algorithm 2.3: Bisection search.
Input: derivative function df, initial interval [a, b].
Output: approximated minimum.
Data: termination parameter ε.
k ←0, determine n ≥0 such that 2n ≥(b −a)/ε;
1
while k < n do
2
k ←k + 1, u ←(a + b)/2;
3
if df (u) = 0 then return u;
4
else
5
if df (u) > 0 then b ←u else a ←u;
6
end
7
end
8
return (a + b)/2;
9

Nonlinear Programming
2-7
Quadratic Fit Search
The quadratic ﬁt search tries to interpolate the location of the minimum of a strictly
unimodal function at each iteration and reﬁnes in this way the location of the true minimizer.
In a way, this method tries to guess the shape of the function to fasten the search for the
minimizer. This method requires only functional evaluations. Nevertheless, it is sometimes
used in conjunction with the bisection search when the derivative of the objective function
is available. Another curve ﬁtting line search based on the cubic interpolation of f is also
available. However, this method requires f to be diﬀerentiable and may sometimes face
severe ill-conditioning eﬀects.
The implementation of the quadratic ﬁt search is shown in Algorithm 2.4. Using three
starting points x1 < x2 < x3 with f(x2) ≤f(x1) and f(x2) ≤f(x3), the method ﬁnds a min-
imizer of the quadratic interpolation function (lines 2–4), then corrects its position (lines
5–8) and updates the values of x1, x2, and x3 with some sort of region-elimination scheme
(lines 9–13). Once the main loop is completed (lines 1–14), the minimizer is estimated by
x2 (line 15).
Algorithm 2.4: Quadratic ﬁt search.
Input: function f, starting points x1 < x2 < x3 with f(x2) ≤f(x1) and f(x2) ≤f(x3).
Output: approximated minimum.
Data: termination parameter ε.
while x3 −x1 > ε do
1
a1 ←x2
2 −x2
3, a2 ←x2
3 −x2
1, a3 ←x2
1 −x2
2;
2
b1 ←x2 −x3, b2 ←x3 −x1, b3 ←x1 −x2;
3
x ←(a1f(x1) + a2f(x2) + a3f(x3))/(2(b1f(x1) + b2f(x2) + b3f(x3)));
4
if x = x2 then
5
if x3 −x2 ≤x2 −x1 then x ←x2 −ε/2 else x ←x2 + ε/2;
6
end
7
if x > x2 then
8
if f(x) ≥f(x2) then x3 ←x else x1 ←x2, x2 ←x;
9
else
10
if f(x) ≥f(x2) then x1 ←x else x3 ←x2, x2 ←x;
11
end
12
end
13
return x2;
14
Backtracking Search
The backtracking search is an approximated line search that aims to ﬁnd acceptable (and
positive) scalars that will give low functional values while trying to perform the least num-
ber of functional evaluations possible. It is often used when a single functional evaluation
requires non-negligible computational resources. This line search does not need to be started
with an initial interval bracketing the minimum. Instead it just needs a (positive) guess value
x1 that belongs to the domain of deﬁnition of the objective function. It is important to notice
that this approach will look for acceptable scalars that have the same sign of the initial guess.
The backtracking line search has many diﬀerent stopping criteria. The most famous one
is Armijo’s condition [1] that uses the derivative of f to check that the current test scalar

2-8
Operations Research Methodologies
x is giving “enough” decrease in f. Armijo’s condition is sometimes coupled with a supple-
mentary curvature condition on the slope of f at x. These two conditions are better known
as the Wolfe conditions (see Ref. [39]). There exist many reﬁnements for the backtracking
search, notably safeguard steps that ensure that the objective function is well deﬁned at
the current test scalar. These safeguard steps are useful whenever barrier functions are used
(see the section “Sequential Unconstrained Minimization Techniques”).
Algorithm 2.5 is a straightforward implementation of the backtracking search with
Armijo’s condition. Depending on whether Armijo’s condition is satisﬁed or not (line 2),
the initial guess is sequentially doubled until Armijo’s condition does not hold any longer
(line 3). This loop actually tries to avoid the solution to be too close to zero. Once the loop
is over, the previous solution is returned (line 4). Conversely, if Armijo’s condition does not
hold for the initial guess, the current scalar is sequentially halved until Armijo’s condition
is restored (line 6). It is important to note that the sequence generated by this algorithm
consists only of positive scalars.
Algorithm 2.5: Backtracking search with Armijo’s condition.
Input: function f, derivative df, initial guess x1 > 0.
Output: acceptable step length.
Data: parameters 0 < c < 1 (usually c = 0.2).
k ←0;
1
if f(xk+1) ≤f(0) + cxk+1 df (0) then
2
repeat k ←k + 1, xk+1 ←2xk until f(xk+1) > f(0) + cxk+1 df (0);
3
return xk;
4
else
5
repeat k ←k + 1, xk+1 ←xk/2 until f(xk+1) ≤f(0) + cxk+1 df (0);
6
return xk+1;
7
end
8
2.2.2
Multidimensional Optimization
There exist several strategies for optimizing a convex function f over Rn. Some of them
require derivatives, while others do not. However, almost all of them require a line search
inside their main loop. They can be categorized by the following:
• Methods without derivatives:
– Simplex Search method or S2 method: see the section “Simplex Search Method.”
– Pattern search methods: see the section “The Method of Hooke and Jeeves.”
– Methods with adaptive search directions: see the section “The Method of
Rosenbrock” and “The Method of Zangwill.”
• Methods with derivatives:
– Steepest descent method: see the section “Steepest Descent Method.”
– Conjugate directions methods: see the section “Method of Fletcher and Reeves”
and “Quasi-Newton Methods.”
– Newton-based methods: see the section “Levenberg–Marquardt Method.”
Concerning recent works on methods without derivatives, the reader might be interested in
the paper of Lewis et al. [33] and the article of Kolda et al. [31].

Nonlinear Programming
2-9
Simplex Search Method or S2 Method
The original simplex search method (which is diﬀerent from the simplex method used in
linear programming) is credited to Spendley et al. [51]. They suggested sequentially trans-
forming a nondegenerate simplex in Rn by reﬂecting one of its vertices through the centroid
at each iteration and by re-initializing this simplex if it remains unchanged for a certain
number of iterations. In this way it takes at most n + 1 iterations to ﬁnd a downhill direction.
Nelder and Mead [37,38] proposed a variant where operations like expansion and contraction
are allowed.
Algorithm 2.6 implements the simplex search of Nelder and Mead. First, with the help
of a scalar c > 0, a simplex S is created from a base point x1 ∈Rn (lines 1–3). Then while
the size of the simplex S is greater than some critical value (line 4), the extremal functional
values at the vertex of S are taken (line 5). The vertex with the maximum functional value
is identiﬁed as xM and the vertex with the lowest functional value is identiﬁed as xm. Then
the centroid x of the polyhedron S\{xM} is computed and the reﬂexion of xm through x,
denoted by xr, is computed (line 6). If the functional value at the reﬂexion point is smaller
than the smallest functional value previously computed (line 7), then the expansion of the
centroid x through the reﬂexion point is computed and stored in xe. Then the vertex xM
is replaced by the expansion point if the functional value at the expansion point is less
than the functional value at the reﬂexion point. Otherwise the vertex xM is replaced by the
reﬂexion point (line 8). If line 7 is incorrect then the vertex xM is replaced by the reﬂexion
point if the functional value at that point is smaller than the greatest functional value at
S\{xM} (line 10). Otherwise a contraction point xc is computed (line 12) and the simplex S
Algorithm 2.6: Nelder and Mead simplex search method.
Input: function f, starting point x1 ∈Rn, simplex size c > 0.
Output: approximated minimum.
Data: termination parameter ε, coeﬃcients α > 0 (reﬂexion), 0 < β < 1 (contraction),
γ > 1 (expansion) (α = 1, β = 0.5, γ = 2).
a ←c(√n + 1 + n −1)/(n
√
2), b ←c(√n + 1 −1)/(n
√
2);
1
for i ∈[[1, n]] do di ←b1n, (di)i ←a, xi+1 ←x1 + di;
2
S = {x1, . . ., xn+1};
3
while size(S) ≥critical value(ε) do
4
xm = argminx∈Sf(x), xM = argmaxx∈Sf(x);
5
x ←1
n
n+1
i=1,i̸=M xi, xr ←x + α(x −xM);
6
if f(xm) > f(xr) then
7
xe ←x + γ(xr −x), if f(xr) > f(xe) then xM ←xe else xM ←xr;
8
else
9
if maxi∈[[1,n+1]],i̸=Mf(xi) ≥f(xr) then xM ←xr;
10
else
11
ˆx = argminx∈{xr,xM}f(x), xc ←x + β(ˆx −x);
12
if f(xc) > f(ˆx) then for i ∈[[1, n + 1]] do xi ←xi + (xm −xi)/2;
13
else xM ←xc;
14
end
15
end
16
end
17
return
1
n + 1
n+1
i=1 xi;
18

2-10
Operations Research Methodologies
is either contracted (line 13) or the vertex xM is replaced by the contraction point (line 14).
Once the main loop is completed, the centroid of the simplex is returned as an estimation
of the minimum of f (line 18).
The Method of Hooke and Jeeves
The method of Hooke and Jeeves [27] is a globally convergent method that embeds a pattern
search that tries to guess the shape of the function f to locate an eﬃcient downhill direction.
Algorithm 2.7 implements a variant of the method of Hooke and Jeeves, found in Ref. [2],
which uses line searches that are used at diﬀerent steps. Line 3 implements the pattern
search: after calculating a downhill direction d, a line search minimizes the functional value
from the current test point xk+1 along the direction d. The result is stored at the point z1
(line 4). Then, starting from z1, a cyclic search along each coordinate direction iteratively
locates a minimizer of f and updates the elements of the sequence (zi)i∈[[1,n+1]] (line 5). This
step is an iteration of the cyclic coordinate method (see Ref. [2]). The Hooke and Jeeves
method is a combination of a pattern search (line 3) with the cyclic coordinate method
(lines 4–6).
Algorithm 2.7: Method of Hooke and Jeeves.
Input: function f, starting point x1 ∈Rn.
Output: approximated minimum.
Data: termination parameter ε, coordinate directions (e1, . . ., en).
k ←0, xk ←0;
1
repeat
2
d ←xk+1 −xk, if k ̸= 0 then λ ←argminλ∈R f(xk+1 + λd) else λ ←0;
3
k ←k + 1, z1 ←xk + λd;
4
for i ∈[[1, n]] do λ ←argminλ∈Rf(zi + λei), zi+1 ←zi + λei;
5
xk+1 ←zn+1;
6
until ||xk+1 −xk||∞< ε;
7
return xk+1;
8
The Method of Rosenbrock
The method of Rosenbrock [48] bears a similarity to the method of Hooke and Jeeves: it
uses the scheme of the cyclic coordinate method in conjunction with a step that optimizes
the search directions. At each step the set of search directions is rotated to best ﬁt the shape
of function f. Like the method of Hooke and Jeeves, the method of Rosenbrock is globally
convergent in convex cases. This method is also considered to be a good minimization
technique in many cases.
Algorithm 2.8 implements a variant of the method of Rosenbrock, found in Ref. [2], which
uses line searches. Lines 11–13 are almost identical to lines 4–6 of the method of Hooke and
Jeeves: this is the cyclic coordinate method. Lines 3–10 implement the Gram–Schmidt
orthogonalization procedure to ﬁnd a new set of linearly independent and orthogonal direc-
tions adapted to the shape of the function at the current point.
The Method of Zangwill
The method of Zangwill [53] is a modiﬁcation of the method of Powell [42], which originally
suﬀered from generating dependent search directions in some cases. The method of Zangwill

Nonlinear Programming
2-11
Algorithm 2.8: Method of Rosenbrock.
Input: function f, starting point x1 ∈Rn.
Output: approximated minimum.
Data: termination parameter ε, coordinate directions (e1, . . ., en).
k ←0;
1
repeat
2
if k ̸= 0 then
3
for i ∈[[1, n]] do
4
if λi = 0 then ai ←ei else ai ←n
ℓ=i λℓeℓ;
5
if i = 1 then bi ←ai else bi ←ai −i=1
ℓ=1 (at
idℓ)dℓ;
6
di ←bi/||bi||;
7
end
8
for i ∈[[1, n]] do ei ←di;
9
end
10
k ←k + 1, z1 ←xk;
11
for i ∈[[1, n]] do λi ←argminλ∈Rf(zi + λei), zi+1 ←zi + λiei;
12
xk+1 ←zn+1;
13
until ||xk+1 −xk||∞< ε;
14
return xk+1;
15
uses steps of the method of Hooke and Jeeves in its inner loop and, like the method of
Rosenbrock, it is sequentially modifying the set of search directions in order to best ﬁt the
shape of the function f at the current point. This method is globally convergent in the
convex cases and is considered to be a quite good minimization method.
Algorithm 2.9 is implementing the method of Zangwill. In the main loop (lines 2–15), a
set of n linearly independent search directions are generated at each iteration (lines 4–13).
To generate a new search direction, a pattern search similar to the one of the method of
Hooke and Jeeves is completed (lines 5–6). The ﬁrst search direction is discarded and the
pattern direction is inserted in the set of search directions (lines 8–9). Then to avoid the
generation of dependent search directions, the starting point of the next pattern search is
moved away by one iteration of the cyclic coordinate method (lines 9–11). After n iterations
of the inner loop (lines 4–13), a set of n independent search directions adapted to the shape
of f is generated and the last point generated by the last pattern search is the starting
point of a new iteration of the main loop (line 14).
Steepest Descent Method
The steepest descent method is a globally convergent method and an elementary minimiza-
tion technique that uses the information carried by the derivative of f to locate the steepest
downhill direction. Algorithm 2.10 implements the steepest descent method. Note that the
line search requires the step length to be positive. In this case approximate line searches
with Wolfe’s conditions can be used.
The steepest descent method is prone to severe ill-conditioning eﬀects. Its convergence
rate can be extremely slow in some badly conditioned cases. It is known for producing
zigzagging trajectories that are the source of its bad convergence rate. To get rid of the
drawback of the steepest descent method, the other methods based on the derivative of
f correct the values of the gradient of f at the current point of the iteration by a linear
operation (also called deﬂection of the gradient).

2-12
Operations Research Methodologies
Algorithm 2.9: Method of Zangwill.
Input: function f, starting point x1 ∈Rn.
Output: approximated minimum.
Data: termination parameter ε, coordinate directions (e1, . . ., en).
k ←0, for i ∈[[1, n]] do di ←ei;
1
repeat
2
k ←k + 1, y1 ←xk, z1 ←xk;
3
for i ∈[[1, n]] do
4
for j ∈[[1, n]] do λ ←argminλ∈R f(zj + λdj), zj+1 ←zj + λdj;
5
d ←zn+1 −z1, λ ←argminλ∈R f(zn+1 + λd), yi+1 ←zn+1 + λd;
6
if i < n then
7
for j ∈[[1, n −1]] do dj ←dj+1;
8
dn ←d, z1 ←yi+1;
9
for j ∈[[1, n]] do λ ←argminλ∈Rf(zj + λej), zj+1 ←zj + λej;
10
z1 ←zn+1;
11
end
12
end
13
xk+1 ←yn+1;
14
until ||xk+1 −xk||∞< ε;
15
return xk+1;
16
Algorithm 2.10: Steepest descent method.
Input: function f, gradient ∇f, starting point x.
Output: approximated minimum.
Data: termination parameter ε.
while ||∇f(x)|| ≥ε do
1
d ←−∇f(x), λ ←argmin λ∈R+ f(x + λd), x ←x + λd;
2
end
3
return x;
4
The Method of Fletcher and Reeves
The method of the conjugate gradient of Fletcher and Reeves [19] is based on the mecha-
nisms of the steepest descent method and implements a technique of gradient deﬂection. It
was derived from a method proposed by Hestenes and Stiefel [26].
This method uses the notion of conjugate directions. In other words, given an n × n
symmetric positive deﬁnite matrix H, the n linearly independent directions d1, . . ., dn of
Rn are H-conjugate if dt
iHdj = 0 for every i ̸= j. In the quadratic case, if d1, . . ., dn are
conjugate directions with the Hessian matrix H, the function f at any point of Rn can
be decomposed into the summation of n real-valued functions on R. Then n line searches
along the conjugate directions are suﬃcient to obtain a minimizer of f from any point of
Rn. The method of Fletcher and Reeves iteratively constructs conjugate directions while
at the same time it deﬂects the gradient. In the quadratic case, the method should stop
in at most n iterations. Nevertheless, while this technique may not be as eﬃcient as oth-
ers, it requires modest memory resources and therefore it is suitable for large nonlinear
problems.

Nonlinear Programming
2-13
The method of Fletcher and Reeves is implemented in Algorithm 2.11. While the increase
of f at the current point is still non-negligible (line 2), a procedure that has some similarities
with the cyclic coordinate method is repeated (lines 3–8). The method goes on by producing
a sequence of points that minimize f along each conjugate direction (line 3) and the next
conjugate direction that deﬂects the gradient is computed at line 5. As in the steepest
descent method, the line search at line 3 requires the step length to be positive. In this case
approximate line searches with Wolfe’s conditions can be used. Then whenever n consecutive
conjugate directions are generated, the method is restarted (line 7).
Algorithm 2.11: Method of the conjugate gradient of Fletcher and Reeves.
Input: function f, gradient ∇f, starting point x.
Output: approximated minimum.
Data: termination parameter ε.
i ←1, zi ←x, di ←−∇f(zi);
1
while ||∇f(zi)|| ≥ε do
2
λ ←argminλ∈R+f(zi + λdi), zi+1 ←zi + λdi;
3
if i < n then
4
α ←||∇f(zi+1)||2/||∇f(zi)||2, di+1 ←−∇f(zi+1) + αdi, i ←i + 1;
5
else
6
x ←zn+1, i ←1, zi ←x, di ←−∇f(zi);
7
end
8
end
9
return x;
10
Other conjugate gradient methods have been derived from Hestenes and Stiefel. Variants
of the method of Fletcher and Reeves have diﬀerent restart procedures (and, consequently,
a slightly diﬀerent gradient deﬂection). The reader should refer to Beale [3] and Powell [44]
for further details on restart procedures. There exist diﬀerent gradient updates, notably in
the method of Polak and Ribiere [40], in the method of Polyak [41], and in the method of
Sorenson [50]. The Polak and Ribiere update is usually deemed to be more eﬃcient than
the Fletcher and Reeves update. Using the above notations, the update is:
α ←(∇f(zi+1) −∇f(zi))t∇f(zi+1)
∇f(zi)t∇f(zi)
Quasi-Newton Methods
Quasi-Newton methods, like the method of Fletcher and Reeves, are based on the notion of
conjugate directions. However, the deﬂection of the gradient is made diﬀerently. The original
method is credited to Davidson [11]. It has been improved by Fletcher and Powell [18], and
simultaneously reﬁned by Broyden [6,7], Fletcher [15], Goldfarb [22], and Shanno [49]. In
these methods, the negative of the gradient at x, −∇f(x), is deﬂected by multiplying it by
a positive deﬁnite matrix A that is updated at each iteration (this matrix evolves toward
the inverse of the Hessian matrix in the quadratic case). These methods are like the steepest
descent method with an aﬃne scaling. Those are referred as quasi-Newton methods and, like
the nonglobally convergent Newton method, perform an aﬃne scaling to deﬂect the gradient.

2-14
Operations Research Methodologies
Note that Newton’s method uses the inverse of the Hessian at the current iteration point
(assuming that the Hessian exists and is positive deﬁnite).
Algorithm 2.12 implements a quasi-Newton method with both Davidson-Fletcher-Powell
(DFP) updates and Broyden-Fletcher-Goldfarb-Shanno (BFGS) updates. The user may
switch between these two updates by choosing the value of a parameter φ. If φ = 0 the
method is using DFP updates that produce sometimes numerical problems. If φ = 1 the
method is using BFGS updates that are deemed to exhibit a superior behavior. Other
values for φ are ﬁne too; however, some negative values may lead to degenerate cases.
As in the steepest descent method, the line search at line 3 requires the step length to
be positive. In this case approximate line searches with Wolfe’s conditions can be used.
However, inexact line searches may corrupt the positive deﬁniteness of the matrices Ai and
some quasi-Newton methods are proposing a varying φ that counters this phenomenon.
Sometimes the matrices Ai are multiplied by a strictly positive scalar updated at each
iteration to avoid some ill-conditioning eﬀects when minimizing nonquadratic functions.
For further information, the reader should refer to Fletcher [17].
Algorithm 2.12: A Quasi-Newton method.
Input: function f, gradient ∇f, starting point x.
Output: approximated minimum.
Data: termination parameter ε, parameter φ, positive deﬁnite matrix A1 (or A1 = I).
i ←1, zi ←x;
1
while ||∇f(zi)|| ≥ε do
2
d ←−Ai∇f(zi), λ ←argminλ∈R+f(zi + λd), zi+1 ←zi + λd;
3
if i < n then
4
u ←λd, v ←∇f(zi+1) −∇f(zi);
5
B1 ←uut/utv −Aivvt Ai/vtAiv;
6
B2 ←((1 + vtAiv/utv)uut −Aivut −uvtAi)/utv;
7
Ai+1 ←Ai + (1 −φ)B1 + φB2, i ←i + 1;
8
else x ←zn+1, i ←1, zi ←x;
9
end
10
return x;
11
Levenberg–Marquardt Method
The Levenberg–Marquardt method [32,35] is a globally convergent modiﬁcation of Newton’s
method. Like the quasi-Newton methods, the Levenberg–Marquardt method is an adapta-
tion of the steepest descent method with an aﬃne scaling of the gradient as a deﬂection
technique. The positive deﬁnite matrix that multiplies the gradient is the summation of the
Hessian of f at the current point (this is the approach of the Newton method) and a scaled
identity matrix cI (almost like in the steepest descent method) that enforces the positive
deﬁniteness.
Algorithm 2.13 implements the Levenberg–Marquardt method. While the increase of f at
the current point is still non-negligible (line 2), a Choleski decomposition of ckI + H(xk) is
repeated until success (lines 3–11). The decomposition gives ckI + H(xk) = LLt. Lines 12–
13 solve the linear system LLtd = −∇f(xk) and the descent direction d is obtained. Line
14 performs a line search and the next point xk+1 is computed. As in the steepest descent
method, the line search requires the step length to be positive. In this case approximate line
searches with Wolfe’s conditions can be used. Line 15 computes the ratio ρ of the actual
decrease over the predicted decrease. If the actual decrease is not enough, then the scale

Nonlinear Programming
2-15
Algorithm 2.13: Levenberg–Marquardt method.
Input: function f, gradient ∇f, Hessian H, starting point x1 ∈Rn.
Output: approximated minimum.
Data: termination parameter ε, parameter c1 (usually c1 ∈{0.25, 0.75, 2, 4}),
parameters 0 < ρ1 < ρ2 < 1 (usually ρ1 = 0.25 and ρ2 = 0.75).
k ←1;
1
while ||∇f(xk)|| ≥ε do
2
repeat
3
fail ←0, A ←ckI + H(xk);
4
for i ∈[[1, n]], j ∈[[i, n]] do
5
S ←aji −i−1
ℓ=1 ajℓaiℓ;
6
if i = j then
7
if S ≤0 then ck ←4ck, fail ←1, break else lii ←
√
S;
8
else lji ←S/lii;
9
end
10
until fail = 0;
11
for i ∈[[1, n]] do S ←−(∇f(xk))i −i−1
ℓ= 1 liℓyℓ, yi ←S/lii;
12
for i ∈[[0, n −1]] do S ←yn−i −n
ℓ=n−i+1 lℓ,n−idℓ,dn−i ←S/lii;
13
λ ←argminλ ∈R+ f(xk + λd), xk + 1 ←xk + λd;
14
ρ ←(f(xk + 1) −f(xk))/(λ∇f(xk)td + λ
2dtH(xk)d/2);
15
if ρ < ρ1 then ck+1 ←4ck;
16
else
17
if ρ2 < ρ then ck+1 ←ck/2 else ck+1 ←ck;
18
end
19
if ρ ≤0 then xk+1 ←xk;
20
k ←k + 1;
21
end
22
return xk;
23
parameter is quadrupled (line 16). Otherwise the scale parameter is halved if the actual
decrease is too big (line 18). If the ratio ρ is negative, then the method is re-initialized (20).
The operations at lines 15–20 are similar to the operations of the trust region methods.
2.3
Constrained Optimization
In this section, we review some techniques of minimization in the case where equality and
inequality constraints are present. A lot of algorithms are specialized in cases where only
one kind of constraint is present or in cases where the constraints are linear. We will review
globally convergent algorithms for the general convex case. There exist at least two types
of approaches that solve this kind of problem:
• The transformation methods. These techniques transform the objective function
to incorporate the constraints and therefore transform a constrained problem
into a sequence of unconstrained problems. Thus any unconstrained minimization
algorithm in Section 2.2 can be used to solve the constrained problem.
• The direction ﬁnding methods. These techniques converge toward a minimizer by
moving from feasible points to feasible points and determining feasible directions

2-16
Operations Research Methodologies
and feasible step lengths at each iteration. The methods described in this section
are globally convergent and use ﬁrst or second order approximations of f, g,
and h.
2.3.1
Direction Finding Methods
Penalty Successive Linear Programming Method
The penalty successive linear programming (PSLP) method is credited to Zhang et al.
[54]. It is the ﬁrst globally convergent form of the successive linear programming (SLP)
approach introduced by Griﬃth and Stewart [23]. A variant of the SLP approach that
includes quadratic approximations has been proposed by Fletcher [16]. At each iteration,
the method ﬁnds feasible directions by solving a linear programming problem based on the
ﬁrst-order approximations of the objective function and constraint functions, and on some
constraints on the direction components. In this method, the linear equality and inequality
constraints Ax ≤b are separated and treated diﬀerently than the purely nonlinear con-
straints g(x) ≤0 and h(x) = 0. This method will converge quadratically if the optimal solu-
tion is close to a vertex of the feasible region. Otherwise the convergence can be very slow.
The PSLP method is implemented in Algorithm 2.14. It needs a trust region vector d
greater than a vector λ > 0 that gives the maximum admissible values for the components of
the search directions. Line 12 uses a linear programming solver to obtain an optimal solution
Algorithm 2.14: PSLP method.
Input: function f, nonlinear constraint functions g and h, gradients ∇f, ∇gi for
i ∈[[1, p]] and ∇hi for i ∈[[1, q]], feasible starting point x1 for the linear
constraints Ax ≤b.
Output: approximated minimum.
Data: lower bound vector 0 < λ ∈Rn, trust region vector d ≥λ, scalars
0 < ρ0 < ρ1 < ρ2 < 1 (usually ρ0 = 10−6, ρ1 = 0.25, ρ2 = 0.75), multiplier α
(usually α = 2), penalty parameters 0 < μ ∈Rp, 0 < η ∈Rq; function
Dxkf : x →f(xk) + ∇f(xk)t(x −xk); function π: (x, μ, η) →p
i=1 μi max
(gi(x), 0) + q
i=1 ηi|hi(x)|; function δxkπ : (x, μ, η) →p
i=1 μi max
(Dxkgi(x), 0) + q
i=1 ηi|Dxkhi(x)| function θ:(x, μ, η) →f(x) + π (x, μ, η);
function δxkθ : (x, μ, η) →Dxkf(x) + δxkπ(x, μ, η).
k ←1;
1
repeat
2
if k ̸= 1 then
3
ρ ←(θ(xk, μ, η) −θ (x, μ, η))/(θ(xk, μ, η) −δxkθ(xk+1, μ, η));
4
if ρ < ρ0 then d ←max(d/α, λ);
5
else
6
if 0 ≤ρ < ρ1 then d ←d/α;
7
if ρ2 < ρ then d ←αd;
8
d ←max(d, λ), xk+1 ←x, k ←k + 1;
9
end
10
end
11
x ←argminx∈Rn{δxkθ(x, μ, η) : Ax ≤b, −d ≤x −xk ≤d};
12
until x = xk;
13
return x;
14

Nonlinear Programming
2-17
of the ﬁrst order approximation problem. It can be initialized by xk if xk is feasible for the
linear constraints; otherwise a feasibility step must be introduced. Line 4 calculates the ratio
of the decrease of the penalty function over the predicted decrease of the penalty function.
If the ratio ρ is negligible or negative, then the trust region is contracted and the method
is re-initialized (line 5). Otherwise the region is contracted if ρ is too small, and expanded
if ρ is large (lines 7–8). This part of the algorithm is similar to the trust region methods.
Merit Successive Quadratic Programming Method
The following method was presented in Ref. [2] and is based on the same ideas as the
PSLP method. Bazaraa et al. give credits to Han [24] and Powell [45] for this method. At
each iteration, the method ﬁnds feasible directions by solving a quadratic programming
problem based on the second-order approximations of the objective function and the ﬁrst-
order approximations of the constraint functions. This method is quite demanding in terms
of computational power; however, it does not have the drawbacks of the PSLP method.
The merit successive quadratic programming method is implemented in Algorithm 2.15.
Since the function θ at line 4 is not diﬀerentiable, only an exact line search can be used to
ﬁnd an optimal λ. Line 7 uses a quadratic programming solver to determine the next descent
direction. It can be initialized by xk if xk is feasible; otherwise a feasibility step must be
introduced. The matrix B can be updated at line 5, provided that it always remains positive
deﬁnite. As suggested by Bazaraa et al. [2], an update of B based on a quasi-Newton scheme
can be used but it is not necessary.
Algorithm 2.15: Merit successive quadratic programming method.
Input: function f, nonlinear constraint functions g and h, gradients ∇f, ∇gi for
i ∈[[1, p]] and ∇hi for i ∈[[1, q]], feasible starting point x1, positive deﬁnite
matrix B.
Output: approximated minimum.
Data: penalty parameters 0 < μ ∈Rp, 0 < η ∈Rq; function
π : (x, μ, η) →p
i=1 μi max(gi(x), 0) + q
i=1 ηi|hi(x)|; function
θ : (x, μ, η) →f(x) + π(x, μ, η); function Dxkf : x →f(xk) + ∇f(xk)t
(x −xk); function Qxk: x →Dxkf(x) + (x −xk)t B(x −xk)/2.
k ←1;
1
repeat
2
if k ̸= 1 then
3
λ ←argminλ∈R+θ(xk + λ(x −xk), μ, η);
4
xk+1 ←xk + λ(x −xk), k ←k + 1;
5
end
6
x ←argminx∈Rn{Qxk(x) : Dxkgi(x) ≤0|p
i=1Dxkhi(x) = 0|q
i=1};
7
until x = xk;
8
return x;
9
2.3.2
Transformation Methods
Sequential Unconstrained Minimization Techniques
The sequential unconstrained minimization techniques (SUMT) were developed by Fiacco
and McCormick [13,14] to solve constrained nonlinear convex optimization problems by

2-18
Operations Research Methodologies
transforming them into a sequence of unconstrained problems using penalty and barrier
functions. The introduction of penalty functions for solving constrained problems is credited
to Courant [10], while the use of barrier functions is credited to Caroll [8].
Algorithms 2.16 and 2.17 implement two diﬀerent SUMT. Algorithm 2.16 is a parametrized
SUMT in which parameter μ > 0 is progressively increased at each iteration to give more
weight to the penalty part of the unconstrained objective function. As μ increases, the solu-
tion x moves toward a feasible point. For μ suﬃciently large, x becomes close enough to an
optimal solution of the constrained problem. However, it is not recommended to start this
method with a large μ as it might slow down the convergence and trigger ill-conditioning
eﬀects. Algorithm 2.17 tries to overcome these diﬃculties by removing the parameter μ.
However, a scalar a < inf{f(x) : g(x) < 0, h(x) = 0} must be determined before the compu-
tation starts.
Algorithm 2.16: SUMT.
Input: see unconstrained optimization method (starting point x).
Output: approximated minimum.
Data: termination parameter ε, penalty parameter μ > 0, scalar α > 1, penalty
function π : x →p
i=1 πg(gi(x)) + q
i=1 πh(hi(x)) with πg and πh continuous,
πg(z) = 0 if z ≤0, πg(z) > 0 otherwise, and πh(z) = 0 if z = 0, πh(z) > 0
otherwise.
while μπ(x) ≥ε do
1
x ←argminx∈Rn{f(x) + μπ(x) : x starting point}, μ ←αμ;
2
end
3
return x;
4
Algorithm 2.17: Parameter-free SUMT.
Input: see unconstrained optimization method (starting point x).
Output: approximated minimum.
Data: termination parameter ε, scalar a < inf{f(x) : g(x) < 0, h(x) = 0},
π : (x, a) →max2(f(x) −a, 0) + p
i=1 max2(gi(x), 0) + ∥h(x)∥2
2.
while π(x, a) ≥ε do
1
x ←argminx∈Rn{π(x, a) : x starting point}, a ←f(x);
2
end
3
return x;
4
Algorithm 2.18 implements a method proposed by Fiacco and McCormick in 1968 [14]
that mixes a penalty function with a barrier function. The penalty function π is aimed for the
equality constraints and the barrier function β is aimed for the inequality constraints. Note
that the line searches that are used by the unconstrained optimization subroutines must
provide a safeguard technique to avoid the computational diﬃculties induced by the domain
of deﬁnition of barrier functions. Lines 1–8 compute a feasible point for the inequality
constraints. If such a point cannot be found, the algorithm is stopped and no solution is
returned (line 7). Using the feasible starting point x, the mixed penalty-barrier solves the
constrained problem (lines 10–13).

Nonlinear Programming
2-19
Algorithm 2.18: Mixed penalty-barrier method.
Input: see unconstrained optimization method (starting point x).
Output: approximated minimum.
Data: termination parameter ε, barrier parameter μ0 > 0, penalty parameter η > 0,
scalars 0 < c1 < 1 and c2 > 1; penalty function π : x →q
i=1 πh(hi(x)) with
πh continuous, πh(z) = 0 if z = 0, πh(z) > 0 otherwise; barrier function
βI : x →
i∈I βg(gi(x)) with I ⊆[[1, p]], βg continuous over R∗
−, βg(z) ≥0 if
z < 0 and limz→0+βg(z) = ∞. Note: β = β[[1,p]].
I ←{i ∈[[1, p]]: gi(x) < 0};
1
while I ̸= [[1, p]] do
2
Select j ∈[[1, p]]\I, μ ←μ0;
3
while μβI/(x) ≥ε do
4
x ←argminx∈Rn{gj(x) + μβI(x) : x starting point}, μ ←c1μ;
5
end
6
if gj(x) ≥0 then return ∅else I ←{i ∈[[1, p]] : gi(x) < 0};
7
end
8
μ ←μ0;
9
while μβ(x) + ηπ(x) ≥ε do
10
x ←argminx∈Rn{f(x) + μβ(x) + ηπ(x) : x starting point};
11
μ ←c1μ, η ←c2η;
12
end
13
return x;
14
Method of Multipliers
The method of multipliers (MOM) was introduced independently by Hestenes [25] and
Powell [43] in 1969. It uses Lagrange multipliers with a penalty function in a SUMT scheme.
Algorithm 2.19 implements the MOM. The algorithm iterates until the violation of the
constraints is small enough (line 2). In line 3, the penalty function is minimized with
an unconstrained optimization method and the result is stored at point xk+1. Then the
Lagrange multipliers are updated if the violation function shows some improvements at
the new iterate xk+1; k is incremented and a new iteration begins (lines 4–12). Otherwise
the penalty parameters are increased and the method is restarted (lines 13–18).
2.4
Conclusion
In this chapter, we have discussed several common algorithms on deterministic optimization
for convex nonlinear problems. However, formal explanations on the construction and the
convergence of the mentioned methods have not been discussed. Useful references were given
for each algorithm, and the most important features were brieﬂy reviewed. For further details
on nonlinear optimization, the reader might consult Bazaraa et al. [2], Bertsekas [4], Boyd
and Vandenberghe [5], Corne et al. [9], Fletcher [17], Horst et al. [28], Luenberger [34], and
Reeves [47]. Owing to limitations of space we did not discuss heuristic and evolutionary opti-
mization techniques such as tabu search [21]. Nonlinear programming techniques are used
in several areas of engineering and are powerful tools in the arsenal of operations research.
Please note that the reader of this chapter should be warned about the diﬀerent ways of
implementing each algorithm. It is important to note that within the scope of this quick
introduction we did not assess problems related to numerical instabilities that can arise

2-20
Operations Research Methodologies
Algorithm 2.19: MOM.
Input: see unconstrained optimization method (starting point x1).
Output: approximated minimum.
Data: termination parameter ε, penalty parameters 0 < μ ∈Rp, 0 < η ∈Rq, initial
multipliers 0 ≤u ∈Rp and v ∈Rq, scalars 0 < c < 1, αμ, αη > 1 (usually
c = 0.25, αμ = αη = 10), violation function ϕ : x →max(||h(x)||∞,
maxi∈[[1,p]](gi(x), 0)), penalty functions πg and πh such that πg(x, u, μ) =
p
i=1

μi max2 
gi(x) + ui
2μi , 0

−u2
i
4μi

and πh(x,v,η) =
q
i=1 (vihi(x) + ηih2
i (x)).
k ←1;
1
while ϕ(xk) ≥ε do
2
xk+1 ←argminx∈Rn{f(x) + πg(x, u, μ) + πh(x, v, η) : xk starting point};
3
if k = 1 then
4
for i ∈[[1, p]] do ui ←ui + max(2μigi(xk+1), −ui);
5
for i ∈[[1, q]] do vi ←vi + 2ηihi(xk+1);
6
k ←k + 1;
7
else
8
if ϕ(xk+1) ≤cϕ(xk) then
9
for i ∈[[1, p]] do ui ←ui + max(2μigi(xk+1), −ui);
10
for i∈[[1, q]] do vi ←vi + 2ηihi(xk+1);
11
k ←k + 1
12
else
13
for i ∈[[1, p]] do
14
if max(gi(xk+1), 0) > cϕ(xk) then μi ←αμμi;
15
end
16
for i ∈[[1, q]] do if |hi(xk+1)| > cϕ(xk) then ηi ←αηηi;
17
end
18
end
19
end
20
return xk ;
21
from round-oﬀerrors and truncation errors in every implementation. For further discussion
about the algorithmic properties of these techniques, the reader may refer to the NEOS
Guide which is an online portal to the optimization community that provides links to stable
solvers for diﬀerent platforms. The NEOS Guide can be found at the following address:
http://www-fp.mcs.anl.gov/otc/Guide/index.html.
References
1. L. Armijo. Minimization of functions having Lipschitz continuous ﬁrst-partial derivatives.
Paciﬁc Journal of Mathematics, 16(l):l–3, 1966.
2. M. Bazaraa, H. Sherali, and C. Shetty. Nonlinear Programming: Theory and Algorithms.
Third edition, Wiley, New York, 2006.
3. E. Beale. A derivation of conjugate gradients. In F. Lootsma, editor, Numerical Methods
for Nonlinear Optimization, pages 39–43, Academic Press, London, 1972.

Nonlinear Programming
2-21
4. D. Bertsekas. Nonlinear Programming. Second edition, Athena Scientiﬁc, Nashua,
NH, 1999.
5. S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
Cambridge, UK, 2004.
6. C. Broyden. Quasi-Newton methods and their application to function minimization. Math-
ematics of Computation, 21(99):368–381, 1967.
7. C. Broyden. The convergence of a class of double rank minimization algorithms 2. The new
algorithm. Journal of the Institute of Mathematics and Its Applications, 6:222–231, 1970.
8. C. Carroll. The created response surface technique for optimizing nonlinear restrained
systems. Operations Research, 9:169–184, 1961.
9. D. Corne, M. Dorigo, and F. Glover. New Ideas in Optimization. McGraw-Hill, New York,
1999.
10. R. Courant. Variational methods for the solution of problems of equilibrium and vibrations.
Bulletin of the American Mathematical Society, 49:1–23, 1943.
11. W. Davidson. Variable metric method for minimization. Technical Report ANL-5990, AEC
Research and Development, 1959.
12. M. Dorigo and T. Stiitzle. Ant Colony Optimization. MIT Press, Cambridge, MA, 2004.
13. A. Fiacco and G. McCormick. The sequential unconstrained minimization technique for
nonlinear programming, a primal-dual method. Management Science, 10:360–366, 1964.
14. A. Fiacco and G. McCormick. Nonlinear Programming: Sequential Unconstrained Mini-
mization Techniques. Wiley, New York, 1968.
15. R. Fletcher. A new approach to variable metric algorithms. Computer Journal, 13:317–322,
1970.
16. R. Fletcher. Numerical experiments with an Li exact penalty function method. In
O. Mangasarian, R. Meyer, and S. Robinson, editors, Nonlinear Programming, 4, Aca-
demic Press, New York, 1981.
17. R. Fletcher. Practical Methods of Optimization, Second edition, Wiley, New York, 1987.
18. R. Fletcher and M. Powell. A rapidly convergent descent method for minimization.
Computer Journal, 6(2):163–168, 1963.
19. R. Fletcher and C. Reeves. Function minimization by conjugate gradients. Computer
Journal, 7:149–154, 1964.
20. F. Glover and G. Kochenberger. Handbook of Metaheuristics. Kluwer Academic Publishers,
New York, 2002.
21. F. Glover and M. Laguna. Tabu Search. Kluwer Academic Publishers, New York, 1997.
22. D. Goldfarb. A family of variable metric methods derived by variational means.
Mathematics of Computation, 24:23–26, 1970.
23. R. Griﬃth and R. Stewart. A nonlinear programming technique for the optimization of
continuous process systems. Management Science, 7:379–392, 1961.
24. S. Han. A globally convergent method for nonlinear programming. Technical Report TR
75–257, Computer Science, Cornell University, Ithaca, NY, 1975.
25. M. Hestenes. Multiplier and gradient methods. Journal of Optimization Theory and
Applications, 4:303–320, 1969.
26. M. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems.
Journal of Research of the National Bureau of Standards, 49(6): 409–436, 1952.
27. R. Hooke and T. Jeeves. Direct search solution of numerical and statistical problems.
Journal of the Association of Computer Machinery, 8(2):212–229, 1961.
28. R. Horst, P. Pardalos, and N. Thoai. Introduction to Global Optimization, Second edition,
Springer, New York, 2000.
29. J. Kiefer. Sequential minimax search for a maximum. Proceedings of the American Math-
ematical Society, 4:502–506, 1953.

2-22
Operations Research Methodologies
30. S. Kirkpatrick, C. Gelatt, and M. Vecchi. Optimization by simulated annealing. Science,
220(4598):671–680, 1983.
31. T. Kolda, R. Lewis, and V. Torczon. Optimization by direct search: new perspectives on
some classical and modern methods. SIAM Review, 45(3):385–482, 2003.
32. K. Levenberg. A method for the solution of certain problems in least squares. Quarterly
Journal of Applied Mathematics, 2:164–168, 1944.
33. R. Lewis, V. Torczon, and M. Trosset. Direct search methods: then and now. In Numerical
Analysis 2000, pages 191–207, Elsevier, New York, 2001.
34. D. Luenberger. Linear and Nonlinear Programming, Second edition, Springer, New York,
2003.
35. D. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal
of the Society for Industrial and Applied Mathematics, 11(2):431–441, 1963.
36. Z.
Michalewicz.
Genetic Algorithms + Data
Structures = Evolution
Programs.
Third
edition, Springer, New York, 1996.
37. J. Nelder and R. Mead. A simplex method for function minimization. Computer Journal,
7(4):308–313, 1964.
38. J. Nelder and R. Mead. A simplex method for function minimization—errata. Computer
Journal, 8:27, 1965.
39. J. Nocedal and S. Wright. Numerical Optimization. Springer, New York, 1999.
40. E. Polak and G. Ribiere. Note sur la convergence de m´ethodes de directions conjug´ees.
Revue Fran¸caise d’Informatique et de Recherche Op´erationelle, 16:35–43, 1969.
41. B. Polyak. The conjugate gradient method in extremal problems. U.S.S.R. Computational
Mathematics and Mathematical Physics, 9(4):94–112, 1969.
42. M. Powell. An eﬃcient method for ﬁnding the minimum of a function of several variables
without calculating derivatives. Computer Journal, 7:155–162, 1964.
43. M. Powell. A method for nonlinear constraints in minimization problems. In R. Fletcher,
editor, Optimization, pages 283–298, Academic Press, London and New York, 1969.
44. M. Powell. Restart procedures for the conjugate gradient method. Mathematical Program-
ming, 12:241–254, 1977.
45. M. Powell. A fast algorithm for nonlinearly constrained optimization calculations. In
G. Watson, editor, Numerical Analysis Proceedings, Biennial Conference, Dundee, Lecture
Notes in Mathematics (630), pages 144–157, Springer, Berlin, 1978.
46. W. Press, B. Flannery, S. Teukolsky, and W. Vetterling. Numerical Recipes in C. Second
edition, Cambridge University Press, Cambridge, UK, 1992.
47. C. Reeves. Modern Heuristic Techniques for Combinatorial Problems. McGraw-Hill,
New York, 1995.
48. H. Rosenbrock. An automatic method for ﬁnding the greatest or least value of a function.
Computer Journal, 3:175–184, 1960.
49. D. Shanno. Conditioning of quasi-Newton methods for function minimizations. Mathemat-
ics of Computation, 24:641–656, 1970.
50. H. Sorenson. Comparison of some conjugate direction procedures for function minimization.
Journal of the Franklin Institute, 288:421–441, 1969.
51. W. Spendley, G. Hext, and F. Himsworth. Sequential application of simplex designs in
optimization and evolutionary operation. Technometrics, 4(4):441–461, 1962.
52. W. Swann. Report on the development of a new direct search method of optimization.
Technical Report 64/3, Imperial Chemical Industries Ltd. Central Instrument Research
Laboratory, London, 1964.
53. W. Zangwill. Minimizing a function without calculating derivatives. Computer Journal,
10(3):293–296, 1967.
54. J. Zhang, N. Kim, and L. Lasdon. An improved successive linear programming algorithm.
Management Science, 31(10):1312–1331, 1985.

3
Integer Programming
Michael Weng
University of South Florida
3.1
Introduction............................................
3-1
3.2
Formulation of IP Models ............................
3-3
Capital Budgeting Problem • The Fixed-Charge
Problem • Either-Or Constraints • If-Then Constraints
• Functions with N Possible Values
3.3
Branch and Bound Method ..........................
3-7
Branching • Computing Lower and Upper Bounds •
Fathoming • Search Strategies • A Bound-First B&B
Algorithm for Minimization (Maximization) •
An Example
3.4
Cutting Plane Method................................ 3-12
Dual Fractional Cut (The Gomory Cuts) for Pure IPs •
Dual Fractional Cutting-Plane Algorithm for ILP •
An Example • Dual Fractional Cut (The Gomory Cuts)
for MILPs • Dual Fractional Cutting-Plane Algorithm
for MILP
3.5
Other Solution Methods and Computer Solution
3-15
References .................................................... 3-16
3.1
Introduction
An integer programming (IP) problem is a mathematical (linear or nonlinear) programming
problem in which some or all of the variables are restricted to assume only integer or discrete
values. If all variables take integer values, then the problem is called a pure IP. On the other
hand, if both integer and continuous variables coexist, the problem is called a mixed integer
program (MIP). In addition, if the objective function and all constraints are linear functions
of all variables, such a problem is referred to as a pure integer linear programming (ILP) or
mixed integer linear programming (MILP) model. The term linear may be omitted unless
it is necessary to contrast these models with nonlinear programming problems.
With the use of the matrix notation, an ILP problem can be written as follows:
(ILP)
minimize
dy
subject to
By ≥b
y ≥0
and integer
where y represents the vector of integer variables, d is the coeﬃcient vector of the objective
function, and B and b are the coeﬃcient matrix and right-hand-side vector of the con-
straints, respectively. All elements of d, B, and b are known constants. Some constraints
may be equations and of the type “≤.” For an ILP with k variables and m constraints,
y = k × 1 vector, d = 1 × k vector, B = m × k matrix, and b = m × 1 vector.
3-1

3-2
Operations Research Methodologies
Similarly, an MILP is generally written as follows
(MILP)
minimize
cx + dy
subject to
Ax + By ≥b
x
≥0
y ≥0,
and integer
where the additional notation x represents the vector of continuous variables, c is the
coeﬃcient vector for x in the objective function, and A the coeﬃcient matrix for x in the
constraints.
Note that any variable in y above can assume any nonnegative integer, as long as all
constraints are satisﬁed. One special case is that all integer variables are restricted to be
binary (i.e., either 0 or 1). Such an IP problem is referred to as a binary ILP (BILP) or
binary MILP (BMILP). In this case, the constraint set (y ≥0, and integer) is replaced by
y ∈{0,1}.
Although several solution methods have been developed for ILP or MILP, none of these
methods is totally reliable in view of computational eﬃciency, particularly as the number
of integer variables increases. In fact, most methods can be classed as either enumeration
techniques, cutting-plane techniques, or a combination of these. Unlike LP, where problems
with millions of variables and thousands of constraints can be solved in a reasonable time,
computational experience with ILP or MILP remains elusive.
If all the integer restrictions on all the variables are omitted (i.e., allow an integer variable
to assume continuous values), an ILP becomes an LP, called its LP relaxation. It is obvious
that the feasible region of an ILP is a subset of the feasible region of its LP relaxation,
and therefore, the optimal objective value of an ILP is always no better than that of its
LP relaxation. In general, the optimal solution to the LP relaxation will not satisfy all the
integer restrictions. There is one exception: For an ILP, if matrix B is totally unimodular,
then the optimal solution to its LP relaxation is guaranteed to be integer-valued and there-
fore, is optimal to the ILP. Unfortunately, this is generally not the case for most ILPs. For a
minimization ILP, the optimal objective value of its LP relaxation is a lower bound for the
ILP optimal objective. Due to the computational diﬃculty associated with solving an ILP
or MILP, LP relaxation is often used in developing solution techniques for solving them.
Once the optimal solution to the LP relaxation is obtained, one may round the noninteger
values of integer variables to the closest integers. However, there is no guarantee in such
rounding that the resulting rounded solution would be feasible to the underlying ILP. In
fact, it is generally true that if the original ILP has one or more equality constraints, the
rounded solution will not satisfy all the constraints and would be infeasible to the ILP. The
infeasibility created by rounding may be tolerated in two aspects. One aspect is that, in
general, the (estimated) parameters of the problems may not be exact. But there are typical
equality constraints in integer problems where the parameters are exact. For example, the
parameters in the multiple-choice constraint y1 + y2 + · · · + yn = p (where p is the integer,
and yj = 0 or 1, for all j) are exact. The other aspect is that an integer variable can assume
large values and rounding up or rounding down will not lead to a signiﬁcant economic or
social impact. It is most likely unacceptable, however, to use rounding as an approximation
if an integer variable represents, for example, a one-time purchase of major objects such as
large ships and jumbo jets, or a decision to ﬁnance or not to ﬁnance a major project.
Before discussing solution techniques for solving an ILP or MILP in detail, we ﬁrst
describe the formulation of several typical IP problems.

Integer Programming
3-3
3.2
Formulation of IP Models
Similar to LP, there are three basic steps in formulating an IP model: (1) identifying and
deﬁning all integer and continuous decision variables; (2) identifying all restrictions and for-
mulating all corresponding constraints in terms of linear equations or inequalities; and
(3) identifying and formulating the objective as a linear function of the decision variables to
be optimized (either minimized or maximized). The remainder of this section illustrates a
variety of modeling techniques for constructing ILPs or MILPs via some sample problems.
3.2.1
Capital Budgeting Problem
Six projects are being considered for execution over the next 3 years. The expected returns
in net present value and yearly expenditures of each project as well as funds available per
year are tabulated below (all units are million dollars):
Annual expenditures
Project
Year 1
Year 2
Year 3
Returns
1
5
2
8
20
2
9
7
9
40
3
4
6
3
15
4
7
4
4
21
5
3
5
2
12
6
8
4
9
28
Available Funds
26
22
25
The problem seeks to determine which projects should be executed over the next 3 years
such that at the end of the 3-year period, the total returns of the executed projects are at
the maximum, subject to the availability of funds each year.
1. Identify the decision variables. Each project is either executed or rejected. There-
fore, the problem reduces to a “yes-no” decision for each project. Such a decision
can be represented as a binary variable, where the value 1 means “yes” and 0
means “no.” That is,
yj = 1,
if project j is selected to execute; and 0, otherwise, for j = 1, 2, . . ., 6.
2. Formulate the constraints. In this problem, the constraints are that total annual
expenditures of the selected projects do not exceed funds available for each year.
These constraints can be mathematically expressed as follows:
5y1 + 9y2 + 4y3 + 7y4 + 3y5 + 8y6 ≤26, for year 1
2y1 + 7y2 + 6y3 + 4y4 + 5y5 + 4y6 ≤22, for year 2
8y1 + 9y2 + 3y3 + 4y4 + 2y5 + 9y6 ≤25, for year 3
3. Formulate the objective function. The objective of this problem is to maximize
the total returns of the selected projects at the end of the 3-year planning period.
The total returns are given mathematically as
Z = 20y1 + 40y2 + 15y3 + 21y4 + 12y5 + 28y6

3-4
Operations Research Methodologies
The ILP model for this capital budgeting problem is
maximize
Z = 20y1 + 40y2 + 15y3 + 21y4 + 12y5 + 28y6
subject to
5y1 + 9y2 + 4y3 + 7y4 + 3y5 + 8y6 ≤26
2y1 + 7y2 + 6y3 + 4y4 + 5y5 + 4y6 ≤22
8y1 + 9y2 + 3y3 + 4y4 + 2y5 + 9y6 ≤25
y1,
y2,
y3,
y4,
y5,
y6 = 0 or 1
Using matrix notation, this ILP can be written as
(ILP)
minimize
dy
subject to
By ≥b
y ≥0,
and binary
where d = (20 40 15 21 12 28), y = (y1 y2 y3 y4 y5 y6)T ,
B =
⎡
⎣
5
9
4
7
3
8
2
7
6
4
5
4
8
9
3
4
2
9
⎤
⎦,
and
b =
⎛
⎝
26
22
25
⎞
⎠
The optimal solution to the LP relaxation, obtained by imposing the upper bounds xj ≤1,
for all j, is y1 = y2 = y5 = 1, y3 = 0.7796, y4 = 0.7627, and y6 = 0.0678 with an objective value
of $101.61 million. This solution has no meaning to the ILP, and rounding to the closest inte-
ger values leads to an infeasible solution. The optimal integer solution is y1 = y2 = y3 = y4 = 1
and y5 = y6 = 0 with Z = $96 million.
The capital budgeting problem has a special case: the set-covering problem. There are
k potential sites for new facilities and the cost associated with selecting a facility at site j
is dj. Each facility can service (or cover) a subset of m areas. For example, a facility may
represent a ﬁre station and the areas represent all sections of a city. The objective is to
select the least-cost subset of all the potential sites such that each and every area is covered
by at least one selected facility. Then the corresponding ILP can be written as above, except
that all elements in matrix B are either 0 or 1.
If the planning horizon is reduced to a one-year period, then there is a single constraint,
and the one-constraint capital budgeting problem is called the 0–1 knapsack problem. In
addition, if each integer variable can assume any nonnegative discrete values, we get the
so-called general knapsack problem.
3.2.2
The Fixed-Charge Problem
In a typical production planning problem involving n products, the production cost for
product j may consist of a variable per-unit cost cj, and a ﬁxed cost (charge) Kj(>0), which
occurs only if product j is produced. Thus, if xj = the production level of product j, then
its production cost Cj(xj) is
Cj(xj) =

Kj + cjxj,
xj > 0
0,
xj = 0
This cost function is depicted in Figure 3.1. The objective would be to minimize
Z = 
jCj(xj). This objective function is nonlinear in variables xj due to the disconti-
nuity at the origin, and can be converted into a linear function by introducing additional

Integer Programming
3-5
xj
Cj(xj)
kj
0
FIGURE 3.1
Fixed-charge cost function.
binary variables as follows. Deﬁne
yj =

1,
xj > 0
0,
xj = 0
This condition can then be expressed as a single linear constraint as
xj ≤Myj
where M(>0) is suﬃciently large to guarantee that constraint xj ≤M is redundant whenever
yj = 1. On the other hand, the minimization of the linear objective function Z = 
j(cjxj +
Kjyj) assures that yj = 0, whenever xj = 0.
3.2.3
Either-Or Constraints
There are practical instances when at least one of two constraints must be satisﬁed, and
it may be impossible to satisfy both constraints simultaneously. For example, consider the
following restriction
|xj −50| ≥5
This restriction means that xj has to deviate from 50 by at least 5. This constraint is
nonlinear and can be replaced by the following two conﬂicting linear constraints: xj −50 ≥5
(i.e., 55 −xj ≤0), and 50 −xj ≥5 (i.e., xj −45 ≤0). In this case, it is obvious that both
cannot be satisﬁed simultaneously. This conﬂict can be resolved by deﬁning a binary variable
as follows:
yj =

1,
xj ≥55
0,
xj ≤45
Then the restriction |xj −50| ≥5 can be replaced by
55 −xj ≤M(1 −y)
xj −45 ≤My
where M(>0) is a suﬃciently large constant. These two constraints guarantee that either
xj ≥55 or xj ≤45 (but not both) will hold.
Another typical either-or application concerns job sequencing on a processor: either job
A proceeds job B, or job B proceeds job A, but not both.

3-6
Operations Research Methodologies
In general, the either-or constraints can be described as follows: at least one of the two
constraints g(x) ≤b and h(x) ≤e must be satisﬁed and the other constraint may or may
not be satisﬁed. This can be modeled by using a binary variable y as
g(x) ≤b + M(1 −y)
h(x) ≤e + My
The either-or constraints can be extended to the case of satisfying at least p out of m(>p)
constraints. Suppose that p of the following m constraints must be satisﬁed.
g1(x) ≤b1
g2(x) ≤b2
:
gm(x) ≤bm
Deﬁne binary variable yi for constraint gi(x) ≤bi as follows.
yj =

1,
constraint i is satisfied
0,
otherwise
Then the following guarantees that at least p constraints will be satisﬁed.
g1(x) ≤b1 + M(1 −y1)
g2(x) ≤b2 + M(1 −y2)
:
gm(x) ≤bm + M(1 −ym)

i
yi ≥p
Replacing the last constraint 
iyi ≥p by 
iyi = p and 
iyi ≤p models, respectively,
exactly and at most p out of m constraints must be satisﬁed.
In many investment, production, or distribution problems, there might be minimum pur-
chase or production requirements that must be met. For example, an investment opportunity
might require a minimum investment of $100,000, or the introduction of a new product might
be required to produce a minimum of 2000 units. Let x represent the continuous decision
variable and C the minimum requirement. Then either x = 0 or x ≥C, and both cannot be
met simultaneously. Deﬁne a binary variable y, and this restriction can be modeled simply
by the following two constraints.
x ≤My
x ≥Cy
3.2.4
If-Then Constraints
There are situations where one constraint must be met if another constraint is to be satisﬁed.
For example, two projects, 1 and 2, are considered. If project 1 is selected, then project 2
must also be selected. This case can easily be handled as follows. Let yj = 1 if project j
is selected, and 0 if not, j = 1 and 2. Then constraint y2 ≥y1 does the trick. Note that, if
project 1 is not selected, there is no restriction on the selection of project 2 (i.e., project 2
may or may not be selected).

Integer Programming
3-7
In general, the if-then constraints can be described as follows. If constraint g1(x) ≤b is
met, then constraint g2(x) ≤e must also be satisﬁed. But if constraint g1(x) ≤b is not met,
there is no restriction on the satisfaction of constraint g2(x) ≤e. This can be modeled by
using two binary variables y1 and y2 as follows:
g1(x) ≤b + M(1 −y1)
g2(x) ≤e + M(1 −y2)
y2 ≥y1
If y1 = 1, y2 must also be 1, and the ﬁrst two constraints above reduce to their original
forms, which means that the if-then requirement is satisﬁed.
On the other hand, if y1 = 0, g1(x) ≤b + M will hold for any x, since M is so large. This
eﬀectively eliminates the original constraint g1(x) ≤b. In addition, y2 ≥y1 reduces to y2 ≥0,
and, therefore, g2(x) ≤e may or may not be satisﬁed.
3.2.5
Functions with N Possible Values
Consider the situation that a function f(x) is required to take on exactly one of N given
values. That is, f(x) = b1, b2, . . ., bN−1, or bN. This requirement can be modeled by
f(x) =

bjyj

yj = 1
yj binary,
for j = 1, 2, . . ., N
3.3
Branch and Bound Method
In this section, we discuss in detail the branch and bound (B&B) method. Without loss of
generality, it is assumed that the objective function is to be minimized. That is, the ILP is
a minimization problem.
As mentioned earlier, total enumeration is not practical to solve ILPs with a large number
of variables. However, the B&B method, which is an implicit enumeration approach, is
the most eﬀective and widely used technique for solving large ILPs. The B&B method
theoretically allows one to solve any ILP by solving a series of LP relaxation problems
(called subproblems).
The B&B method starts with solving the LP relaxation (problem). If the optimal solution
to the relaxed LP is integer-valued, the optimal solution to the LP relaxation is also optimal
to the ILP, and we are done. However, it is most likely that the optimal solution to the
LP relaxation does not satisfy all the integrality restrictions. See, for instance, the optimal
solution to the relaxed LP of the capital budgeting problem in Section 3.2.1. In this case, we
partition the ILP into a number of subproblems that are generally smaller in size or easier
to solve than the original problem. This process of partitioning any given problem into two
or more smaller or easier subproblems is commonly called branching, and each subproblem
is called a branch. The B&B method is then repeated for each subproblem.
As mentioned earlier, the optimal solution to the LP relaxation is a lower bound on the
optimal solution to the corresponding ILP. Let Z be the best known objective function value
for the original ILP. For any given subproblem, let Zrelax be the optimal objective value to its
LP relaxation. This means that the optimal integer solution to this subproblem is no better
than Zrelax. If Zrelax ≥Z, then this subproblem (branch) cannot yield a feasible integer
solution better than Z and can be eliminated from further consideration. This process

3-8
Operations Research Methodologies
P3
P9
P10
P14
P17
P4
P6
P18
P13
P5
P11
P12
P8
P16
P20
P15
P19
P7
P1
P2
P0
Zrelax  80
Zrelax  93
Zrelax  92.22
Zrelax  101.44
Zrelax  100
Zrelax  99.37
Zrelax  99.11
Zrelax  84
Zrelax  96
Zrelax  97.6
Zrelax  89.89
Zrelax  97
Zrelax  97
Zrelax  92.6
Zrelax  88
Zrelax  73
y  0
y  0
y  0
y  1
y  0
y  1
y  0
y  0
y = 0
y  0
y = 0
y = 0
y  1
y = 1
y  1
y = 1
y = 1
y  1
y = 1
y  1
Zrelax  98
Zrelax  99.74
Zrelax  99.22
Zrelax  97.5
FIGURE 3.2
The complete search tree.
of eliminating a branch from further consideration is usually called fathoming. However, if
Zrelax < Z, then a conclusion cannot be reached and further branching from this subproblem
is needed. This process continues until all branches are fathomed, and the best integer
solution is optimal to the original ILP.
The B&B method can be represented by a search tree that has many levels (Figure 3.2).
The original ILP is the root, the ﬁrst level consists of all the subproblems branched from the
original ILP, the second level consists of all the subproblems branched from all the ﬁrst
level subproblems that have not been fathomed, and so on. We now discuss in more detail
each of the three key components of the B&B method: branching, computing bounds, and
fathoming.
3.3.1
Branching
For convenience, let P0 denote a given ILP or MILP. The set of all subproblems branched
from P0 must represent all of P0 to ﬁnd an optimal solution to P0. In addition, it is an
approximation that any two subproblems are mutually exclusive. For example, let
q1, q2, . . ., qr be all the possible values that integer variable yj can assume. Let Pi rep-
resent the subproblem obtained by ﬁxing yj at qi, for i = 1, 2, . . ., r. If P0 is partitioned into
P1, P2, . . ., Pr, then
{P0} = {P1} ∪{P2} ∪. . . ∪{Pr}, and
{Pi} ∩{Ps} = Ø,
for all i ̸= s
Note that each subproblem Pi has one fewer integer variable since yj now is a ﬁxed constant.
If yj is a binary variable, there are only two obvious branches with yj = 0 and yj = 1.
Among all binary variables, yj should be selected as one that does not equal to 0 or 1
in the optimal solution to the LP relaxation. In the capital budgeting problem given in
Section 3.2.1, any of y3, y4, and y5 can be chosen as yj.
If yj is a general integer variable, it may require a lot of eﬀort to ﬁnd all its possible
feasible integer values. A more eﬀective way is to derive two branches: yj ≤t and yj ≥t + 1,
where t is a nonnegative integer. Again, one of the variables whose values are not an integer
in the optimal solution to the LP relaxation should be selected and t is the largest integer
smaller than the corresponding variable value.
As one can see, the branching process is essentially to add additional restrictions to
form subproblems. Therefore, the optimal solution to any subproblem is no better than the

Integer Programming
3-9
branching subproblem. In particular, for minimization, the optimal solution to any subprob-
lem is always greater than or equal to the optimal solution to the branching subproblem and
thus to the original ILP. As we move further from the root in the search tree, the optimal
objective values of subproblems increase or remain the same for minimization ILPs.
3.3.2
Computing Lower and Upper Bounds
Let Zbest be the objective function value of a known integer solution. Then the optimal
objective value Z∗is at least as good as Zbest. That is, Zbest is an upper bound on Z∗for a
minimization ILP. If a feasible integer solution with an objective value strictly better than
Zbest is found, then replace Zbest by the new solution. In doing so, the upper bound Zbest
remains the smallest upper bound and the corresponding feasible integer solution is called
the incumbent solution (i.e., the best known integer solution). If no integer solutions have
been identiﬁed yet, Zbest is set to be ∞.
For any subproblem Pj in the B&B search, we attempt to ﬁnd a lower bound ZL on
the optimal objective value of Pj. That is, the optimal solution of Pj cannot be better
than ZL. If ZL > Zbest (the objective value of the incumbent integer solution), then the
subproblem Pj can be discarded from further consideration, and the corresponding branch
is then fathomed. However, if ZL ≤Zbest, then a conclusion cannot be reached and the
subproblem Pj needs to be branched further.
In general, it is no easy task to ﬁnd a lower bound for an optimization problem. Recall,
however, that for any ILP, the optimal objective value associated with its LP relaxation
provides a lower bound on the optimal objective value of the ILP.
3.3.3
Fathoming
In the B&B search, the ILP (the root) is branched into two or more level-1 subproblems; each
level-1 subproblem is either fathomed or branched into two or more level-2 subproblems;
and each level-2 subproblem is either fathomed or further branched into two or more level-3
subproblems, and so on. If all subproblems (branches) are fathomed, the search stops and
the incumbent integer solution is optimal to the ILP.
A subproblem may be fathomed in one of the following three ways:
1. An optimal integer solution is found. In this case, further branching from this
subproblem is unnecessary and the incumbent solution may be updated.
2. The subproblem is found to be infeasible.
3. The optimal objective value ZL to its LP relaxation is strictly greater than Zbest.
3.3.4
Search Strategies
During the B&B searching process, there are generally many subproblems (at diﬀerent
levels) that remain to be further branched. The question is which remaining subproblem
should be selected to branch next? That is, what search strategy should be used?
Width-ﬁrst and depth-ﬁrst searches are commonly used search strategies. In width-ﬁrst
search, all subproblems at a level are examined before any subproblem at the next level
will be considered, and the search always moves forward from one level to the next. In
depth-ﬁrst search, an arbitrary subproblem at the newest level is examined ﬁrst, and the
search may move backwards. Since depth-ﬁrst search can yield a feasible solution faster,
which can be used to fathom, it is computationally more eﬃcient than width-ﬁrst search.
However, width-ﬁrst strategy generally requires less computer memory. Another commonly
used strategy is best-bound ﬁrst, where the sub-problem with the best bound is branched

3-10
Operations Research Methodologies
ﬁrst. For minimization, the subproblem with the smallest lower bound is branched. In doing
so, it is likely to generate a good integer solution early in the searching process and therefore
speed up the search process.
In the next section, we present a B&B algorithm using the bound-ﬁrst search strategy.
3.3.5
A Bound-First B&B Algorithm for Minimization
(Maximization)
Step 1: Solve the LP relaxation. If the optimal solution happens to be integer-valued,
then stop; and this is the optimal solution to the ILP. Otherwise, set Zbest = ∞(= −∞),
deﬁne the ILP as the candidate problem P, and go to Step 2.
Step 2: Let yj be a variable whose value in the optimal solution to the LP relaxation
of P is not integer, and let t be the noninteger value of yj in the solution. Branch P into
two subproblems by adding two constraints yj ≤int(t) and yj ≥int(t) + 1, where int(t) is
the integer part of t. If yj is binary, let yj = 0 and yj = 1. Solve the LP relaxation problems.
If a subproblem is infeasible, fathom it. Otherwise, proceed to Step 3.
Step 3: If an integer solution is found, fathom the corresponding subproblem, and replace
Zbest by the new feasible objective value if it is strictly smaller (greater) than Zbest.
Step 4: If a noninteger solution is found, fathom it if the optimal objective value Zrelax
is strictly greater (smaller) than the current solution, Zbest.
Step 5: If all subproblems are fathomed, then stop; an optimal integer solution has been
found with the optimal objective value Zbest.
Step 6: Replace the candidate subproblem P by the remaining subproblem with the
smallest (largest) bound Zrelax, and go to Step 2.
3.3.6
An Example
This section presents an example to illustrate the use of the B&B method. For simplicity, a
BILP problem is considered. In particular, we use the capital budgeting problem presented
in Section 3.2.1. We denote this ILP P0.
Since this is a maximization ILP, set Zbest = −∞. The optimal solution to the LP relax-
ation of P0 is y1 = y2 = y5 = 1, y3 = 0.7796, y4 = 0.7627, and y6 = 0.0678 with an objective
value Zrelax = 101.61. This is a non-integer solution. Choose, for example, y3 as yj. Fixing
y3 at 0 and 1 respectively yields the following two sub-problems that have only ﬁve variables.
P1:
maximize
Z = 20y1+ 40y2 + 21y4 + 12y5 + 28y6
subject to
5y1 + 9y2 + 7y4 + 3y5 + 8y6 ≤26
2y1 + 7y2 + 4y4 + 5y5 + 4y6 ≤22
8y1 + 9y2 + 4y4 + 2y5 + 9y6 ≤25
y1,
y2,
y4,
y5,
y6 = 0 or 1
P2:
maximize
Z = 20y1+ 40y2 + 21y4 + 12y5 + 28y6 + 15
subject to
5y1 + 9y2 + 7y4 + 3y5 + 8y6 ≤22
2y1 + 7y2 + 4y4 + 5y5 + 4y6 ≤16
8y1 + 9y2 + 4y4 + 2y5 + 9y6 ≤22
y1,
y2,
y4,
y5,
y6 = 0 or 1

Integer Programming
3-11
The optimal relaxed LP solutions are
P1: y2 = y4 = y5 = 1, y1 = 0.8947, and y6 = 0.3158 with an objective value Zrelax = 99.74.
P2: y1 = y2 = 1, y4 = 0.767, y5 = 0.7476, and y6 = 0.0485 with an objective value Zrelax =
101.44.
Since P2 has the largest lower bound Zrelax = 101.44, it is selected to branch from the
next. Let us choose to set y4 = 0 and y4 = 1. This will lead to two subproblems with only
four decision variables y1, y2, y5, and y6: P3(y4 = 0) and P4(y4 = 1). The two relaxed LP
solutions are
P3:
y2 = y6 = 1, y1 = 0.2778, and y5 = 0.8889 with Zrelax = 92.22.
P4:
y1 = y2 = 1, y6 = 0, and y5 = 0.3333 with Zrelax = 100.
Of the three remaining subproblems P1, P3, and P4, P4 is selected to branch next by
setting y5 = 0(P5) and y5 = 1(P6).
P5:
y2 = 1, y1 = 0.9474, and y6 = 0.1579 with Zrelax = 99.37.
P6:
y1 = 1, y2 = 0.6, and y6 = 0.2 with Zrelax = 97.6.
Among the four remaining subproblems P1, P3, P5, and P6, P1 is branched next by ﬁxing
y1 = 0(P7) and y1 = 1(P8).
P7:
y2 = y5 = y6 = 1, and y4 = 0.8571 with Zrelax = 98.
P8:
y2 = y4 = y5 = 1, and y6 = 0.2222 with Zrelax = 99.22.
Of P3, P5, P6, P7, and P8, the subproblem P5 is the next one to branch from. Choose y1
to be ﬁxed at 0 and 1.
P9:
y2 = 1, y6 = 0.75 with Zrelax = 97.
P10:
y2 = 1, y6 = 0.1111 with Zrelax = 99.11.
We next branch from subproblem P8 and set y6 equal to 0 and 1.
P11:
y2 = y4 = y5 = 1 with Zrelax = 93. This is an integer solution. Since Zrelax >
Zbest = −∞, reset Zbest = 93. P11 becomes the ﬁrst incumbent solution. Fathom P11.
P12:
y4 = y5 = 1, and y2 = 0.2222 with Zrelax = 89.89.
Since Zrelax < Zbest for subproblems P3 and P12, fathom both P3 and P12. Of the four
remaining subproblems P6, P7, P9, and P10, we branch from P10 next and ﬁx y6.
P13:
y2 = 1 with Zrelax = 96. Again, this is an integer solution. Since Zrelax > Zbest = 93,
reset Zbest = 96, replace the current incumbent solution P11 by P13, and fathom P13.
P14:
y2 = 0 with Zrelax = 84. This solution is all integer-values, and simply fathom P14
since Zrelax < Zbest = 96.
There are three subproblems P6, P7, and P9 that remain to be examined. We now branch
from P7 and ﬁx y4 at 0 and 1.
P15:
y2 = y5 = y6 = 1 with Zrelax = 80. Fathom P15.
P16:
y2 = y5 = 1 and y6 = 0.875 with Zrelax = 97.5. Fathom P15.

3-12
Operations Research Methodologies
There still are three subproblems P6, P9, and P16 left. Branch from P6 by ﬁxing y2 at 0
and 1.
P17:
y1 = 1 and y6 = 0.875 with Zrelax = 92.6. P17 is fathomed since Zrelax < Zbest = 96.
P18:
y1 = 0 and y6 = 0 with Zrelax = 88. P18 is fathomed since the solution is all integer-
valued with Zrelax < Zbest = 96.
There are only two subproblems remaining. Branch from P16 leads to the following:
P19:
y2 = y5 = 1 with Zrelax = 73. Fathom P19.
P20:
y2 = 1 and y5 = 2/3 with Zrelax = 97.
Both the remaining subproblems P9 and P20 have the same Zrelax = 97. Branching from
both leads to four subproblems that will be fathomed since their relaxed LP solutions are all
strictly worse than Zbest. Now all subproblems have been fathomed, and the B&B search
stops with the current incumbent solution P13(y1 = y2 = y3 = y4 = 1 and y5 = y6 = 0 with
Zbest = 96) being the optimal solution to the ILP.
This ILP has six binary variables. Since each variable can assume either 0 or 1, the
total enumeration will have to evaluate a total of 26 = 64 solutions (many of them may be
infeasible). In the above B&B search, a total of only 25 subproblems are evaluated—over
60% reduction of computational eﬀort. As the number of variables increases, computational
eﬀort reduction can become more signiﬁcant. This is especially true when the number of
variables that equal 1 in an optimal integer solution is a small portion of all the binary
integer variables.
In the B&B search, the number of possible subproblems at least doubles as the level
increases by 1. At the ﬁnal level, for an ILP with k integer variables, there are at least
2k possible sub-problems. For a small k = 20, 2k > 1 million, and k = 30, 2k > 1 billion.
Too many subproblems! One way to reduce the search eﬀort is to limit the number of
subproblems to branch further to some constant (e.g., 50). Such a search method is called
a beam search, and the constant is the beam size. Beam search may quickly ﬁnd an integer
solution that may be good, but its optimality is not guaranteed.
3.4
Cutting Plane Method
Recall that the feasible region of any LP is a convex set, and each solution found by the
simplex method corresponds to an extreme point of the feasible region. In other words,
the simplex method only searches the extreme points of the feasible region in identifying an
optimal solution. In general, the optimal extreme point associated with the optimal solution
may not be all integer-valued. If the coeﬃcient matrix B in an ILP is totally unimodular,
then each extreme point of the feasible region is guaranteed to have all integer variable
values, and therefore, the relaxed LP solution is guaranteed to be all integer-valued. But
this is generally not true for most ILPs. However, if only the optimal extreme point of
the relaxed LP feasible region is all integer-valued, then the LP relaxation solution is also
optimal to the ILP. This is the underlying motivation for a cutting-plane method.
The basic idea is to change the boundaries of the convex set of the relaxed LP feasible
region by adding “cuts” (additional linear constraints) so that the optimal extreme point
becomes all-integer when all such cuts are added. The added cuts will slice oﬀsome oﬀ
the feasible region of the LP relaxation (around the current noninteger optimal extreme
point), but will not cut oﬀany feasible integer solutions. That is, the areas sliced oﬀfrom
the feasible region of the LP relaxation do not include any of the integer solutions feasible

Integer Programming
3-13
to the original ILP. When enough such cuts are added, the new optimal extreme point of
the sliced feasible region becomes all-integer, and thus is optimal to the ILP.
We ﬁrst describe in detail the dual fractional cutting-plane method for pure ILPs and
then extend it to MILPs.
3.4.1
Dual Fractional Cut (The Gomory Cuts) for Pure IPs
Consider a maximization ILP, where all coeﬃcients are integer and all constraints are equal-
ities. Its LP relaxation is
(LP Relaxation)
maximize
dy
subject to
By = b
y ≥0
The optimal simplex solution to the LP relaxation can be expressed as follows:
yB + S−1NyN = S−1b
where yN = 0, and yB = S−1b(≥0) are respectively the vectors of non-basic and basic vari-
ables, and S and N consist of columns of coeﬃcients in the constraints corresponding to
basic and non-basic variables, respectively. The square matrix S is commonly called a basis.
Let aj denote the column of constraint coeﬃcients of variable yj. Then the above equation
can be rewritten as follows:
yB +

non-basic j
ujyj = v
where u = S−1aj and v = S−1b. If v is all-integer, then this solution is also optimal to the
ILP. Otherwise, v has at least one noninteger element. Let vr be a noninteger element in v.
A cut will be constructed based on the rth equation in the above vector equation, which
can be explicitly written as follows:
yB,r +

non-basic j
ur,jyj = vr
Since vr is noninteger and positive, it can be expressed as vr = int(vr) + fr, where int(vr)
is the integer rounded down from vr, and fr is the non-integer part of vr. Clearly, 0 < fr < 1.
Similarly, we can write ur,j = int(vr,j) + fr,j, where 0 ≤fr,j < 1, for all non-basic variable yj.
Substituting vr and ur,j into the above equation yields
yB,r +

non-basic j

int(ur,j) + fr,j

yj = int(vr) + fr
The above equation can be rewritten as

non-basic j
fr,jyj −fr = int(vr) −

yB,r +

non-basic j
int(ur,j)yj

Since the right-hand side of this equation is always integer for any feasible integer solution,
the left-hand side must also be integer-valued for all feasible integer solutions. In addition,
the ﬁrst term 
non-basic j fu,jyj is nonnegative for any nonnegative solutions. Therefore,

non-basic j
fu,jyj −fr = y

3-14
Operations Research Methodologies
where y is some nonnegative integer. Adding this as an additional constraint to the ILP will
not reduce any feasible solution. This cut is referred to as a Gomory cut (Gomory, 1960).
In addition, the ILP remains a pure IP after adding this cut.
Resolve the LP relaxation of the ILP with the added cuts. If an all-integer solution is
obtained, it is optimal to the original ILP and therefore stop. Otherwise, repeat.
The following algorithm outlines the basic steps of the dual fractional cutting-plane
approach.
3.4.2
Dual Fractional Cutting-Plane Algorithm for ILP
Step 1: Start with an all-integer simplex tableau and solve it as an LP (i.e., the LP
relaxation). If it is infeasible, so is the ILP and stop. If the optimal solution is all-integer,
it is also optimal to the ILP, and stop. Otherwise, proceed to Step 2.
Step 2: In the optimal simplex tableau, identify a row (say, row r) associated with a
non-integer basic variable. Use this row to construct a cut as follows.

non-basic j
fu,jyj −fr = y
where y is an additional nonnegative integer variable. Add this new constraint to the bottom
of the current optimal simplex tableau, and go to Step 3.
Step 3: Reoptimize the new LP relaxation, using the dual simplex method. If the new
LP relaxation is infeasible, so is the original ILP, and stop. If the optimal solution to the
new LP relaxation is all-integer, it is also optimal to the original ILP, and stop. Otherwise,
go to Step 2.
3.4.3
An Example
The following example is considered to illustrate the use of Gomory cuts.
maximize
Z = 7y1 + 9y2
subject to
−y1 + 3y2 + y3
= 6
7y1 + y2
+ y4 = 35
y1,
y2,
y3,
y4 ≥0,
and integer
The optimal solution to the LP relaxation is (y1, y2, y3, y4) = (9/2, 7/2, 0, 0) with Z = 63.
The equation associated with noninteger basic variable y2 in the ﬁnal optimal simplex
tableau is y2 + (7/22)y3 + (1/22)y4 = 7/2 which can be rewritten as
y2 +

0 + 7
22

y3 +

0 + 1
22

y4 = 3 + 1
2
Thus, the resulting Gomory cut is
7
22y3 + 1
22y4 −1
2 = y5
Solve the LP relaxation and we get the optimal solution y = (32/7, 3, 11/7, 0, 0) with
Z = 59. The equation associated with non-basic variable y1 is y1 + (1/7)y4 + (−1/7)y5 =
32/7; this yields the following Gomory cut:
71
y4
+ 6
7y5 −4
7 = y6
Again, solving the new LP relaxation yields the optimal solution y = (4, 3, 1, 4, 0, 0) with
Z = 55. This solution is all integer-valued. Therefore, it is also optimal to the original ILP,
and stop.

Integer Programming
3-15
3.4.4
Dual Fractional Cut (The Gomory Cuts) for MILPs
The cutting-plane algorithm for ILPs presented in Section 3.4.1 can be extended to deal
with MILPs. Again, the intent is to whittle the feasible region down to an optimal extreme
point that has integer values for all of the integer variables by adding cuts. Let row r
correspond to an integer variable whose value is noninteger in the optimal solution to the
LP relaxation. Then the cut takes the following form:

non-basic j
gu,jyj −fr = y
where y is a nonnegative integer variable, and gu,j =
1. ur,j, if ur,j > 0 and yj is a continuous variable,
2. [fr/(fr −1)]uv,j, if uv,j < 0 and yj is a continuous variable,
3. fr,j, if fr,j ≤fr and yj is an integer variable,
4. [fr/(1 −fr)](1 −fr,j), if fr,j ≤fr and yj is an integer variable.
3.4.5
Dual Fractional Cutting-Plane Algorithm for MILP
Step 1: Solve the LP relaxation. If it is infeasible, so is the MILP and stop. If the
optimal solution satisﬁes all integer requirements, it is also optimal to the MILP, and stop.
Otherwise, proceed to Step 2.
Step 2: In the optimal simplex tableau, identify a row (say, row r) which contains an
integer basic variable whose value is not integer. Use this row to construct a cut as follows:

non-basic j
gu,jyj −fr = y
where y is an additional nonnegative integer variable. Add this new constraint to the bottom
of the current optimal simplex tableau, and go to Step 3.
Step 3: Reoptimize the new LP relaxation, using the dual simplex method. If the new
LP relaxation is infeasible, so is the original MILP, and stop. If the optimal solution to the
new LP relaxation satisﬁes all integer restrictions, it is also optimal to the original MILP,
and stop. Otherwise, go to Step 2.
3.5
Other Solution Methods and Computer Solution
In LP, the simplex method is based on recognizing that the optimum occurs at an extreme
point of the convex feasible region deﬁned by the linear constraints. This powerful result
reduces the search for the optimum from an inﬁnite number of many solutions to a ﬁnite
number of extreme point solutions. On the other hand, pure ILPs with a bounded feasible
region are guaranteed to have just a ﬁnite number of feasible solutions. It may seem that
ILPs should be relatively easy to solve. After all, LPs can be solved extremely eﬃciently
and the only diﬀerence is that ILPs have far fewer solutions to be considered.
Unfortunately, this is not the case. While LPs are polynomially solvable via the interior
point methods (Bazarra et al., 1990), ILPs are NP-hard. The NP-hardness of ILPs can be
established since every instance of the generalized assignment problem which is NP-hard
is an ILP (for more discussion on computational complexity, refer to Garey and Johnson,
1979). Therefore, ILPs are, in general, much more diﬃcult to solve than LPs. The integer

3-16
Operations Research Methodologies
nature of the variables makes it diﬃcult to devise an eﬃcient algorithm that searches directly
among the integer points of the feasible region. In view of this diﬃculty, researchers have
developed solution procedures (e.g., B&B search and cutting-plane methods) that are based
on exploiting the tremendous success in solving LPs. A more detailed discussion of B&B
search and cutting-plane methods can be found in, for example, Salkin and Mathur (1989)
and Schrijver (1986). Other classical methods include, for example, partitioning algorithms
(Benders, 1962) and group theoretic algorithms (Gomory, 1967).
More recently, local search methods have been developed to ﬁnd heuristic solutions to
ILPs. Glover and Laguna (1997) give excellent discussion of tabu search in integer program-
ming. Heuristic search algorithms based on simulated annealing and genetic algorithms can
be found in Aarts and Korst (1990) and Rayward et al. (1996).
There are many sophisticated software packages for ILPs or MILPs that build on recent
improvements in integer programming. For example, the developers of the powerful math-
ematical programming package CPLEX have an ongoing project to further develop a fully
state-of-the-art IP module. The inclusion of the Solver in Microsoft EXCEL has certainly
revolutionized the practical use of computer software in solving ILPs. Refer to Fourer (2005)
for an excellent survey of commercial software packages.
References
1. Aarts, E. and Korst, J., Simulated Annealing and Boltzmann Machines, John Wiley &
Sons, New York, 1990.
2. Bazarra, M. S., Jarvis, J. J., and Sherali, H. D., Linear Programming and Network Flows,
second edition, John Wiley & Sons, New York, 1990.
3. Benders, J., “Partitioning Procedures for Solving Mixed-Variables Programming Prob-
lems,” Numerische Mathematik, 4, pp. 238–252, 1962.
4. Fourer, R., “Linear Programming Software Survey,” ORMS Today, June 2005.
5. Garey, M. R. and Johnson, D. S., Computers and Intractability: A Guide to the Theory of
NP-Completeness, W. H. Freeman, San Francisco, 1979.
6. Glover, F. and Laguna, M., Tabu Search, Kluwer Academic Publishers, Boston, 1997.
7. Gomory, R. E., “An Algorithm for the Mixed Integer Problem,” Research Memorandum
RM-2597, The RAND Corporation, Santa Monica, CA, 1960.
8. Gomory, R. E., “Faces of an Integer Polyhedron,” Proceedings of the National Academy of
Science, 57 (1), 16–18, 1967.
9. Rayward-Smith, V. J., Osman, I. H., Reeves, C. R., and Smith, G. D. (Eds.), Modern
Heuristic Search Methods, John Wiley & Sons, New York, 1996.
10. Salkin, H. M. and Mathur, K., Foundations of Integer Programming, North Holland,
New York, 1989.
11. Schrijver, A., Theory of Linear and Integer Programming, John Wiley & Sons, New York,
1986.

4
Network Optimization
Mehmet Bayram Yildirim
Wichita State University
4.1
Introduction............................................
4-1
4.2
Notation ................................................
4-2
4.3
Minimum Cost Flow Problem .......................
4-3
Linear Programming Formulation
4.4
Shortest Path Problem ...............................
4-4
Linear Programming Formulation • Dijkstra’s
Algorithm
4.5
Maximum Flow Problem .............................
4-8
Linear Programming Formulation • Augmenting Path
Algorithm
4.6
Assignment Problem .................................. 4-13
Linear Programming Formulation • The Hungarian
Algorithm • Application of the Algorithm Through an
Example
4.7
Minimum Spanning Tree Problem .................. 4-14
Linear Programming Formulation • Kruskal’s
Algorithm
4.8
Minimum Cost Multicommodity Flow Problem .. 4-18
Linear Programming Formulation
4.9
Conclusions ............................................ 4-19
References .................................................... 4-19
4.1
Introduction
Communication on the Internet or via phones, satellites, or landlines; distribution of goods
over a supply chain; transmission of blood through veins; movement of traﬃc on roads,
connection of towns by roads, connection of yards by railroads, ﬂight of planes between
airports, and the like, have something in common: all can be modeled as networks. Taha
(2002) reported that as much as 70% of the real-world mathematical programming prob-
lems can be represented by network-related models. These models are widely utilized in real
life for many reasons. First, networks are excellent visualization tools and easy to under-
stand as problems can be presented pictorially over networks. Second, network models have
several computationally eﬃcient and easy-to-implement algorithms that take advantage of
the special structure of networks, thus providing a key advantage for researchers to handle
large-size real-life combinatorial problems. Furthermore, network ﬂow problems appear as
subproblems when mathematical programming techniques such as decomposition, column
generation, or Lagrangian relaxation are utilized to solve large-scale complex mathemati-
cal models. This is a key advantage because using eﬃcient network algorithms results in
4-1

4-2
Operations Research Methodologies
faster convergence when the above-mentioned techniques are utilized. For example, shortest
path problems, one of the simplest network ﬂow models that can be solved quite eﬃciently,
appear in many networks: transportation, communication, logistics, supply chain manage-
ment, Internet routing, molecular biology, physics, sociology, and so on.
In this chapter, our goal is not to focus on the details of network ﬂow theory, but instead
to describe the general problem and provide a mathematical formulation for each problem.
For most of the fundamental network problems presented in this chapter, we will describe
an algorithm and illustrate how it can be applied to a sample network ﬂow problem. We
will also provide a variety of problems for some of the network models presented.
The organization of this chapter is as follows: First, the notation utilized in this chapter
is presented. Next, the minimum cost ﬂow problem is introduced. The special cases of this
problem, the shortest path problem, maximum ﬂow problem, and the assignment problem
are then presented. The multicommodity ﬂow problem and the minimum spanning tree
problem are also studied in this chapter.
4.2
Notation
A directed network (or a directed graph) is deﬁned as G = (N, A), where N is the node set
(i.e., N = {1, 2, 3, 4, 5, 6, 7}), and A is the arc set whose elements are ordered pairs of distinct
nodes (i.e., A = {(1, 2), (1, 3), (2, 3), (2, 4), (3, 6), (4, 5), (4, 7), (5, 2), (5, 3), (5, 7), (6, 7)}). An
arc (i, j) starts with a tail node i and ends with a head node j, where nodes i and j are the
endpoints of arc (i, j). In Figure 4.1, arcs (1,2) and (1,4) are directed arcs. An arc adjacency
list A(i) of a node i is the set of arcs emanating from node i, that is, A(i) = {(i, j) : (i, j) ∈A}.
For example, in the ﬁgure below, A(4) = {(4, 5), (4,7)}. The indegree of a node is the number
of incoming arcs of that node, and the outdegree of a node is the number of outgoing arcs.
The degree of a node is the sum of its indegree and outdegree. For example, node 5 has
an indegree of 1 and outdegree of 3. As a result, the degree of node 5 is 4. Consequently,

i∈N |A(i)| = |N|.
A walk in a directed graph G is a subgraph of G consisting of a sequence of nodes and
arcs. This also implies that the subgraph is connected. A directed walk is an “oriented” walk
such that for any two consecutive nodes ik and ik+1, (ik, ik+1) is an arc in A. For example,
1,2,5,7,4,2 is a walk but not a directed walk. 1,2,4,7 is a directed walk. A path is a walk
without any repetition of nodes (e.g., 1,2,5,7), and a directed path is a directed walk without
any repetition of nodes. For example, in the directed graph below, 1,2,4,5 is a directed path,
but 1,2,4,5,2 is not. A cycle is a closed path that begins and ends at the same node (e.g.,
4,5,7,4). A directed cycle is a directed closed path that begins and ends at the same node
(e.g., 2,4,5,2). A network is called acyclic if it does not contain any directed cycle. A tree is
a connected acyclic graph. A spanning tree on a given undirected graph is a subgraph of G
that is a tree and spans (touches) all nodes. A graph G = (N, A) is a bipartite graph if we
1
2
3
4
5
6
7
FIGURE 4.1
A directed network.

Network Optimization
4-3
can partition the nodes in this graph into two disjoint subsets N1 and N2 so that for every
arc (i, j) ∈A, i ∈N1 and j ∈N2, or vice versa.
An undirected network is deﬁned as G = (N, A), where N is the node set and A is the arc
set whose elements are unordered pairs of distinct nodes. An arc in an undirected network is
similar to a two-way street (i.e., ﬂow is allowed in both directions), while an arc in a directed
network allows ﬂow in one direction only. An undirected network can be transformed into
a directed network by replacing an undirected arc (i, j) with two directed arcs (i, j) and
(j, i). Consequently, node i is adjacent to node j, and node j is adjacent to node i. Thus,

i∈N |A(i)| = 2|N|. The deﬁnitions above hold for undirected networks, except that there
is no distinction between a cycle and a directed cycle, a path and a directed path, and a
walk and a directed walk.
4.3
Minimum Cost Flow Problem
The minimum cost ﬂow problem is one of the most fundamental network ﬂow problems,
where the goal is to send ﬂow from supply nodes to demand nodes using arcs with capaci-
ties and involve the minimum total cost of transportation given availability of supply and
demand in a directed network (if the network is undirected, an undirected arc between
nodes i and j is replaced with two directed arcs, (i, j) and (j, i), with the same cost and
capacity as the undirected arc to obtain a directed network).
A minimum cost ﬂow problem has several applications: distribution of a product from
manufacturing plants to warehouses, or from warehouses to retailers; ﬂow of raw materi-
als and intermediate goods through stations in a production line; routing of automobiles
through a street network; and routing of calls through a telephone system.
4.3.1
Linear Programming Formulation
Let xij denote the amount of ﬂow on arc (i, j). The minimum cost ﬂow problem can be
formulated as follows:
Minimize:

(i,j)∈A
cijxij
(4.1)
Subject to:

{j:(i,j)∈A}
xij −

{j:(j,i,)∈A}
xji = bi
i ∈N
(4.2)
lij ≤xij ≤uij
(i, j) ∈A
(4.3)
xij ≥0
(i, j) ∈A
(4.4)
where cij is the cost of arc (i, j) ∈A and bi is the supply/demand at node i. If node i is a
supply node, then bi > 0, whereas bi < 0 for a demand node, and bi = 0 for a transshipment
node. To have a feasible solution, 
i∈N bi = 0. The objective here is to minimize the cost
of transporting the commodity from supply nodes to demand nodes. Equation 4.2 is the
mass balance (ﬂow balance or conservation of ﬂow) constraints: the diﬀerence between the
total ﬂow emanating from node i (outﬂow, the ﬁrst term in Equation 4.2) and entering
node i (inﬂow, the second term in Equation 4.2) is equal to the demand/supply at that
node. Equation 4.3 states that ﬂow on any arc (i, j) should be between the allowable range,
that is, between the lower (lij) and upper bounds (uij) of ﬂow on arc (i, j), where lij ≤0
or lij > 0, and uij < ∞. When uij = ∞for all arcs (i.e., there is no upper bound on the
arc capacity), the problem becomes an uncapacitated network ﬂow problem. Note that the

4-4
Operations Research Methodologies
above formulation has |A| nonnegative variables, lower and upper bound constraints, and
|N| mass balance constraints.
The basic feasible solution of the equation system deﬁned by Equations 4.2 to 4.3 are
integer-valued if all bi’s are integer-valued. In other words, when the right-hand side for all
constraints (i.e., the supply and demand) is an integer, the network ﬂow models provide
integer solutions.
The above problem can be solved using linear programming techniques such as the sim-
plex algorithm. For network problems, calculations of the simplex tableau values become
easier. The specialized simplex algorithm to solve network problems is deﬁned as the network
simplex method. In the network simplex algorithm, a feasible spanning tree structure is suc-
cessively transformed into an improved spanning tree structure until optimality is achieved.
Minimum cost ﬂow problems have several special cases. For example, in transportation
problems, the network is bipartite. Each node is either a supply or a demand node. Supply
nodes are connected to demand nodes via arcs without capacities. The goal is to minimize
the overall transportation cost. In a transportation problem, when the supply and demand
at each node is equal to one, and the number of supply nodes is equal to the number of
demand nodes, an assignment problem is obtained. Other special cases of minimum cost
ﬂow problems include shortest path problems and maximum ﬂow problems.
4.4
Shortest Path Problem
Shortest path problems involve a general network structure in which the only relevant
parameter is cost. The goal is to ﬁnd the shortest path (the path with minimum total dis-
tance) from the origin to the destination. Computationally, ﬁnding the shortest path from an
origin to all other nodes (often also known as a shortest path tree problem) is not any more
diﬃcult than determining the shortest path from an origin to a single destination. Note that
a shortest path problem is a specialized minimum cost ﬂow problem, where the origin ships
one unit of ﬂow to every other node on the network (thus, a total of m −1 units of ﬂow).
Shortest path problems arise in a variety of practical settings, both as stand-alone prob-
lems or as subproblems of more complex settings. Applications include ﬁnding a path of
minimum time, minimum length, minimum cost, or maximum reliability. The shortest path
problem arises in a transportation network where the goal is to travel the shortest dis-
tance between two locations, or in a telecommunication problem where a message must be
sent between two nodes in the quickest way possible. Other applications include equipment
replacement, project scheduling, project management, cash ﬂow management, workforce
planning, inventory planning, production planning, DNA sequencing, solving certain types
of diﬀerential equations, and approximating functions. Shortest path problems appear as
subproblems in traﬃc assignment problems, in multicommodity ﬂow problems, and network
design problems. More detailed descriptions of some of the problems that can be modeled
as shortest path problem are provided below:
The maximum reliability path problem determines a directed path of maximum reliability
from the source node to every other node in the network where each arc is associated with
a reliability measure or probability of that arc being operational. The reliability of a path
can be calculated by the product of reliabilities of each arc on that path. To solve this
problem using a shortest path algorithm, one can take the cost of each arc as the logarithm
of its reliability and convert the maximization problem into a minimization problem by
multiplying the objective function by −1.
In an equipment replacement problem, the input is the total cash inﬂow/outﬂow for pur-
chasing the equipment using the equipment for a certain period of time, and ﬁnally selling

Network Optimization
4-5
the equipment. This data can be transformed into a network structure by assuming that the
nodes represent the timeline and that the arcs represent an equipment replacement decision.
The cost of an arc between two nodes separated by k years is the total cost of buying, keep-
ing, and then selling the equipment for k years. The goal in an equipment replacement prob-
lem is to determine the best equipment replacement policy over a T-year planning horizon,
which is equivalent to solving a shortest path problem over the network described above.
4.4.1
Linear Programming Formulation
The objective in a shortest path problem is to minimize the total cost of travel from an
origin node s (i.e., a source node) to all other nodes in a directed network where the cost of
traversing arc (i, j) is given by cij. The shortest path problem can be considered a minimum
cost ﬂow problem with the goal of sending one unit of ﬂow from the source node s to every
other node in the network. Let xij be the amount of ﬂow on arc (i, j). The shortest path
problem can be formulated as follows:
Minimize:

(i,j)∈A
cijxij
(4.5)
Subject to:

{j:(i,j)∈A}
xij −

{j:(j,i,)∈A}
xji = (n −1),
i = s
(4.6)

{j:(i,j)∈A}
xij −

{j:(j,i,)∈A}
xji = −1
i = N −{s}
(4.7)
xij ≥0
(i, j) ∈A
(4.8)
In the above formulation, the objective function, Equation 4.5, minimizes the total cost
of sending (n −1) units of ﬂow node s to all other nodes on the network. Equation 4.6 is
the mass balance constraint (node balance, conservation of ﬂow constraint) for the origin.
Equation 4.7 is the mass balance constraint for all other nodes. The above formulation has
|A| variables and |N| constraints. The shortest path problem can be solved utilizing sev-
eral specialized, very eﬃcient algorithms. We will describe one of the most famous network
optimization algorithms, Dijkstra’s algorithm, to solve a shortest path problem in the fol-
lowing. In the presence of negative cycles, the optimal solution of the shortest path problem
is unbounded.
4.4.2
Dijkstra’s Algorithm
Dijkstra’s algorithm is a widely used, simple-to-implement algorithm to solve shortest path
problems. Dijkstra’s algorithm is a label-setting algorithm: Initially, all nodes are assigned
tentative distance labels (temporary shortest path distances), and then iteratively, the short-
est path distance to a node or set of nodes at each step is determined. In the Dijkstra’s
implementation described below, d(j) is the (temporary) distance label of node j (shortest
path distance, or minimum cost directed path from the source node s to node j). Distance
labels are upper bounds on shortest path distances. Pred(j) is the immediate predecessor
of node j on the shortest path tree. LIST is a data structure in which candidate nodes that
have been assigned temporary shortest distances are stored. A(j) is the arc adjacency list.
The eﬃciency of the algorithm depends on the structure of the network (e.g., acyclic net-
works, arcs with nonnegative lengths, integer-valued arc costs, etc.) and how the LIST data

4-6
Operations Research Methodologies
structure is implemented (e.g., Dial’s implementation, and heap implementations such as
Fibonacci heap, radix heap, d-heap, etc.). Below is a description of the Dijkstra’s algorithm:
Step 1: For the source node, let d(s) = 0 and pred(s) = 0. For all other nodes (i.e.,
j ∈N −{s}), d(j) = ∞, pred(j) = |N| + 1. Set LIST := {s};
Step 2: Permanently label the node i with d(i) = min {d(j) : j ∈LIST}. Remove node
i from the LIST;
Step 3: For each (i, j) ∈A(i), do a distance update: if d(j) > d(i) + cij, then d(j) := d(i) +
cij, pred(j) := i, and if j /∈LIST, then add j to LIST;
Step 4: If LIST ̸= Ø, then go to Step 2.
In the above implementation of Dijkstra’s algorithm, we assumed that the network is
directed. In an undirected network, to satisfy this assumption, each undirected arc (i, j)
with cost cij can be replaced by two directed arcs, arc (i, j) and arc (j, i) with costs cij. It
is also assumed that the network contains a directed path from the origin (the source node)
to every other node in the network, and the cost cij for arc (i, j) ∈A is nonnegative.
We apply the Dijkstra’s algorithm to the network depicted in Figure 4.2. The iterations
of the algorithm are presented in Table 4.1. In the initialization stage, all distance labels
other than the origin (d(s) = 0) are set to inﬁnity. At each iteration, a node is permanently
labeled (i.e., the distance from the origin, node s, to that node is determined) and distance
labels of all nodes that are accessible from the permanently labeled node are updated. In
Table 4.1, shaded columns indicate nodes that have been permanently labeled. Below, we
will describe a couple of iterations of the Dijkstra’s algorithm.
At the ﬁrst iteration, initially, the LIST contains only node 1 (the origin). Thus, node 1,
the node with the shortest distance from the origin, is permanently labeled and removed
from the LIST. Nodes 2 and 3 are added to the list, as the distance for those nodes through
node 1 is less than their current temporary distance labels of inﬁnity. In the second iteration,
node 2 is the node with the shortest distance from the origin. Thus, node 2 is permanently
labeled and removed from the LIST. Distance labels of nodes 4, 5, and 6 are updated and
added to the LIST. The predecessor of these nodes is node 2. Dijkstra’s algorithm takes six
iterations to permanently label all nodes on the network. The shortest path from node 1 to
node 6 is 1-2-4-5-6, with a total distance of 60. In Table 4.1, the distance labels and prede-
cessors of each node are presented. The ﬁnal shortest path tree is presented in Figure 4.3.
For more generalized networks (including networks with negative arc lengths), shortest
path problems can be solved using label-correcting algorithms. Label-correcting algorithms
determine the shortest path distance at the time the algorithm terminates (i.e., all of the
1
2
4
3
6
20
25
35
55
35
10 
25
10
s
20 
5
FIGURE 4.2
Shortest path example: the goal is to determine the shortest path from node 1 to all
other nodes.

Network Optimization
4-7
TABLE 4.1
Initialization
LIST =
Node
1
2
3
4
5
6
d(j)
0
∞
∞
∞
∞
∞
Pred (j)
0
7
7
7
7
7
Iteration 1 Permanently labeled node = 1
LIST =
Node
1
2
3
4
5
6
d(j)
0
20
25
∞
∞
∞
Pred (j)
0
1
1
7
7
7
LIST =
Iteration 2 Permanently labeled node = 2
LIST =
Node
1
2
3
4
5
6
d(j)
0
20
25
40
55
75
Pred (j)
0
1
1
2
2
2
LIST = 3,4,5,6
Iteration 3 Permanently labeled node = 3
LIST = 4,5,6
Node
1
2
3
4
5
6
d(j)
0
20
25
40
55
75
Pred (j)
0
1
1
2
2
2
LIST = 4,5,6
Iteration 4 Permanently labeled node = 4
LIST = 5,6
Node
1
2
3
4
5
6
d(j)
20
25
40
50
65
Pred (j)
1
1
2
4
4
LIST = 5,6
Iteration 5 Permanently labeled node = 5
LIST = 6
Node
2
3
4
5
6
d(j)
20
25
40
50
60
Pred (j)
1
1
2
4
5
LIST = 6
Iteration 6 Permanently labeled node = 6
LIST =
TERMINATE
Node
2
3
4
5
6
d(j)
20
25
40
50
60
Pred (j)
1
1
2
4
5
0
0
1
0
0
1
0
0
Dijkstra’s Algorithm Implementation
1
2,3
3
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{

4-8
Operations Research Methodologies
1
2
4
3
5
6
20 
25 
35
55
35
10 
25 
10
s
20
FIGURE 4.3
Shortest path tree for the network in Figure 4.2.
distance labels are temporary until the termination). Label-setting algorithms can also be
viewed as a special case of label-correcting algorithms.
Some of the generalizations of the shortest path problems can be listed as follows: The
constrained shortest path problem ﬁnds the shortest distance from an origin to all other
nodes while keeping the traversal time of that path below a predetermined amount. The
traversal time between nodes can be diﬀerent from the distance between the given nodes.
In the k shortest path problem, one determines not only the shortest path but also the 2nd,
3rd, . . ., (k −1)st, and kth shortest paths from an origin to every other node. The multi-
criteria shortest path problem intends to determine a path that minimizes simultaneously
all the criteria (i.e., distance, time, cost, reliability) under consideration. The multicriteria
shortest path problem ﬁnds nondominated paths, that is, paths for which there is no other
path with better values for all criteria. In the all pair shortest path problem, the shortest
path between all pairs of nodes is found.
4.5
Maximum Flow Problem
In a capacitated network, often, sending a maximum amount of ﬂow from an origin (source)
node to a destination (sink) node might be advantageous. In many situations, links on the
network can be considered as having capacity that limits the quantity of a product that may
be shipped through the arc. Often in these situations, knowledge of the overall capacity of
the network might provide an advantage over competitors. This can be achieved by ﬁnding
the maximum ﬂow that can be shipped on the network. The objective in a maximum ﬂow
problem is to determine a feasible pattern of ﬂow through the network that maximizes the
total ﬂow from the supply node (source, origin) to the destination node (sink). The max-
imum ﬂow problem is a special case of a minimum cost ﬂow problem. Any maximum ﬂow
problem can be transformed into a minimum cost ﬂow problem using the following trans-
formation: add an arc with −1 cost and ∞capacity between the sink node and source node
(i.e., add arc (d, s)). All other arcs on the network have a cost of zero. As the objective in a
minimum cost ﬂow problem is to minimize total cost, the maximum possible ﬂow is delivered
to the sink node when the minimum cost ﬂow problem is solved for the new network.
Maximum ﬂow problems have several applications. For example, consider a pipeline that
transports water from lakes (sources) to a residential area (sinks) via a pipeline (arcs on
the network). Water has to be transported via pipes with diﬀerent capacities. Intermedi-
ate pumping stations (transshipment nodes) are installed at appropriate distances. Before
being available for consumption, the water has to be treated at treatment plants. The inter-
mediate pumping stations and treatment plants are transshipment nodes. The goal is to
determine the maximum capacity of this pipeline (i.e., the maximum amount that can be

Network Optimization
4-9
pumped from sources to sinks). Similar problems exist in crude oil and natural gas pipelines.
Other examples of maximum ﬂow problems are staﬀscheduling, airline scheduling, tanker
scheduling, and so on. In summary, maximal ﬂow problems play a vital role in the design
and operation of water, gas, petroleum, telecommunication, information, electricity, and
computer networks like the Internet and company intranets.
4.5.1
Linear Programming Formulation
Let f represent the amount of ﬂow in the network from source node s to sink node d (this
is equivalent to the ﬂow on arc (d, s)). Then the maximal ﬂow problem can be stated as
follows:
Maximize:
f
(4.9)
Subject to:

{j:(i,j)∈A}
xij = f
i = s
(4.10)

{j:(i,j)∈A}
xij −

{j:(j,i)∈A}
xji = 0
i = N\{s, d}
(4.11)
−

{j:(j,i)∈A}
xji = −f
i = d
(4.12)
xij ≤uij
(i, j) ∈A
(4.13)
xij ≥0
(i, j) ∈A
(4.14)
The objective is to maximize the total ﬂow sent from origin (node s) to destination
node d. Equations 4.10 through 4.12 are conservation of ﬂow (mass balance) constraints.
At the origin and destination, the net inﬂow and outﬂow are f and −f, respectively. The
mathematical program has |A| + 1 variables, |A| capacity constraints, and |N| node balance
constraints. Maximum ﬂow problems can be solved using the network simplex method.
However, algorithms such as the augmented path algorithm, the preﬂow-push algorithm,
and excess scaling algorithms can be utilized to solve large-scale maximum ﬂow problems
eﬃciently. Below we present an augmenting path algorithm.
A special case of the maximum ﬂow problem (thus, the minimum cost ﬂow problem) is
the circulation problem, in which the objective is to determine if the network ﬂow problem is
feasible, that is, if there is a solution in which the ﬂow on each arc is between the lower and
upper bounds of that arc. The circulation problem does not have any supply or demand
nodes (i.e., all nodes are transshipment nodes, thus, bi = 0 for all i ∈N). A circulation
problem can be converted to a minimum cost ﬂow problem by adding an arc with −1 cost
and ∞capacity between the sink node and source node (i.e., add arc (d, s)). All other arcs
on the network have a zero cost.
4.5.2
Augmenting Path Algorithm
In an augmenting path algorithm, the ﬂow along the paths from source node to destination
node is incrementally augmented. This algorithm maintains mass balance constraints at
every node of the network other than the origin and destination. Furthermore, instead
of the original network, a residual network is utilized to determine the paths from origin
(supply node) to destination (demand node) where there can be a positive ﬂow. The residual
network is deﬁned as follows: For each arc (i, j), an additional arc (j, i) is deﬁned. When
there is a ﬂow of eij on arc (i, j), the remaining capacity of arc (i, j) is cij −eij, whereas

4-10
Operations Research Methodologies
the remaining capacity of arc (j, i) is increased from 0 to eij. Whenever some amount of
ﬂow is added to an arc, that amount is subtracted from the residual capacity in the same
direction and added to the capacity in the opposite direction. An augmented path between
an origin–destination pair is a directed path for which every arc on the path has strictly
positive residual capacity. The minimum of these residual capacities of the augmented path
is the residual capacity (c∗) of the residual path. Then the remaining capacities on arcs
((i, j), (j, i)) are modiﬁed along the path to either (cij −eij −c∗, eij + c∗) or (cij −eij + c∗,
eij −c∗), depending on whether the ﬂow is on arc (i, j) or (j, i). The augmenting path
algorithm can be stated as follows:
Step 1: Modify the original network to obtain the residual network.
Step 2: Identify an augmenting path (i.e., a directed path from source node to sink
node) with positive residual capacity.
Step 3: Determine the maximum amount of ﬂow that can be sent over the augmenting
path (suppose this is equal to c∗). Increase the ﬂow on this path by c∗, i.e., total ﬂow = total
ﬂow + c∗.
Step 4: Decrease the residual capacity on each arc of the augmenting path by c∗,
and increase residual capacity on each arc in the opposite direction on the augmenting
path by c∗.
Step 5: If there is an augmented path from source to sink with positive residual capacity,
then go to step 3.
We apply the augmenting path algorithm to the network shown in Figure 4.4a. The
residual network is obtained after initialization. In Figure 4.4b, the residual capacity of
each arc is listed above/below the header (arrow/orientation) of each arc. For example,
initially the remaining capacity on arc (2,4) is 20 (that is the capacity of this arc) and (4,2)
is 0. The augmented paths and updates in ﬂows are given in Table 4.2. Initially, all of the
newly added reverse arcs carry zero ﬂow. At iteration 1, 10 units of ﬂow are sent on path
1-2-5-6 (the minimum residual arc capacity on this path). The shaded cells are the arcs on
this path for which there is a change in the residual arc capacity. For example, arc (5,6)
can no longer carry any ﬂow, while its reverse arc, (6,5), has 10 units of residual capacity.
In iteration 2, the bottleneck (the arc with minimum capacity) occurs at arc (5,2). As a
result, 10 units of ﬂow are transferred on path 1-3-5-2-4-6. Finally, after 10 units of ﬂow
are sent on path 1-2-4-6, there is no more augmenting path with strictly positive residual
capacity from node 1 to node 6 (see Figure 4.4c for the ﬂows on paths and the residual arc
capacities). As a result, the maximum ﬂow on this network is 30 units.
The maximum ﬂow problem can also be solved using the minimum cut algorithm. A cut
in a connected network deﬁnes the set of directed arcs, which, if removed from the network,
would make it impossible to travel from the source/origin to the sink/destination. In other
words, a cut should include at least one arc from every directed path from the source
node to the sink node to prevent ﬂow going from origin to destination. The capacity of
a cut is the sum of the capacities of the arcs in the cut. The goal in the minimum cut
problem is to determine the cut with the lowest capacity. To determine a cut, we ﬁrst
partition all nodes into two subsets where origin and destination cannot be in the same
subset, that is, N = N1 ∪N2, where origin ∈N1 and destination ∈N2. The cut is a set of all
forward arcs that connects the subset that contains the origin and the other subset with
destination. For example, suppose N1 = {1, 2} and N2 = {3, 4, 5, 6}. Then the cut includes
arcs (1,3), (2,4), (2,5) and (2,6) and the cut capacity is u13 + u24 + u25 + u26 = 135. The
minimum cut problem is the dual of the maximum ﬂow problem. Thus, any feasible solution
to the minimum cut problem provides an upper bound to the maximum ﬂow problem and

Network Optimization
4-11
1
2
4
3
5
6
20
25
35
55
35
10
25
10
s
20
1
2
4
3 
5 
6
20
25
35 
55
35
10
25 
10 
s
20
0
0
0
0
0
0
0
0
0
(a)
(b)
(c)
1
2
4 
3
5
6
0
15
35 
55
25
10
5
10
10
10
0
s
0 
20
0
0
10
20
10
20
0
10
FIGURE 4.4
Application of the augmenting path algorithm on a network. (a) Original network.
(b) Transformed network with residual arcs. (c) Paths and ﬂows on the network after the augmenting
ﬂow algorithm is run to determine the maximum ﬂows.
at optimality the minimum cut capacity is equal to the maximum ﬂow. As a result, the
maximum ﬂow problem can be solved using the following algorithm:
Step 1: Identify all the cuts of the network.
Step 2: Determine the capacities of the cuts.
Step 3: Select the cut with the minimum capacity.
Note that if all possible cuts are not identiﬁed, then this algorithm may not identify the
maximum ﬂow. For the network in Figure 4.4a, if the nodes are partitioned as N1 = {1, 3, 5}

4-12
Operations Research Methodologies
TABLE 4.2
Application of Augmenting Path Algorithm to the Network in Figure 4.4a
Residual Arc Capacity
Iteration
Path
Flow
(1,2)
(2,1)
(1,3)
(3,1)
(2,4)
(4,2)
(2,5)
(5,2)
(2,6)
(6,2)
(3,5)
(5,3)
(4,5)
(5,4)
(4,6)
(6,4)
(5,6)
(6,5)
0
20
0
25
0
20
0
35
0
55
0
35
0
10
0
25
0
10
0
1
1-2-5-6
10
10
10
25
0
20
0
25
10
55
0
35
0
10
0
25
0
0
10
2
1-3-5-2-4-6
10
10
10
15
10
10
10
35
0
55
0
25
10
10
0
15
10
0
10
3
1-2-4-6
10
0
20
15
10
0
20
35
0
55
0
25
10
10
0
5
20
0
10

Network Optimization
4-13
and N2 = {2, 4, 6}, then the cut capacity is u12 + u56 = 20 + 10 = 30, which is equal to the
maximum ﬂow in the network.
4.6
Assignment Problem
In the assignment problem, on a weighted bipartite graph G = (N, A), where for every
(i, j) ∈A, i ∈N1 and j ∈N2, or vice versa, one seeks to pair nodes in N1 and N2 in such
a way that the total cost of this pairing is minimized. In other words, given two sets of
objects, the best, most eﬃcient, least-cost pairing is sought.
There are several applications of assignment problems: For example, assigning professors
to classes; matching people to jobs, rooms, events, machines, projects, or to each other;
assigning crews to ﬂights, jobs to machines, and so on. Each assignment has a value, and
we want to make the assignments to minimize the sum of these values.
4.6.1
Linear Programming Formulation
Given a weighted bipartite network G = (N1 ∪N2, A) with |N1| = |N2| and arc weights of
cij, the assignment problem can be formulated as follows:
Minimize:

(i,j)∈A
cijxij
(4.15)
Subject to:

{j:(i,j)∈A}
xij = 1
i ∈N1
(4.16)

{i:(i,j)∈A}
xij = 1
j ∈N2
(4.17)
xij ≥0
(i, j) ∈A
(4.18)
where xij takes a value of 1 if node i and node j are paired. The objective is to achieve the
minimum cost pairing. Equation 4.16 guarantees that each node in N1 is assigned to exactly
one node in N2. Similarly, Equation 4.17 assures that any node in N2 is assigned exactly to
one node in N1. The mathematical program has |N1|2 variables and 2|N1| constraints.
Note that the assignment problem is a special case of a minimum cost ﬂow problem on
a bipartite graph, with N1 and N2 as disjoint node sets. Supply at each node in N1 is one
unit, and demand at each node in N2 is one unit. The cost on each arc is cij.
4.6.2
The Hungarian Algorithm
The assignment problem can be solved using the Hungarian algorithm. The input for the
Hungarian algorithm is an |N1| × |N1| cost matrix. The output is the optimal pairing of
each element. The Hungarian algorithm can be described as follows:
Step 1: Find the minimum element in each row, and subtract the minimums from the
cells of each row to obtain a new matrix. For the new matrix, ﬁnd the minimums in each col-
umn. Construct a new matrix (called the reduced cost matrix) by subtracting the minimums
from the cells of each column.

4-14
Operations Research Methodologies
TABLE 4.3
Setup Time in Hours
Job 1
Job 2
Job 3
Machine 1
5
1
1
Machine 2
1
3
7
Machine 3
1
5
3
Step 2: Draw the minimum number of lines (horizontal or vertical) that are needed to
cover all zeroes in the reduced cost matrix. If m lines are required, an optimal solution is
available among the covered zeroes in the matrix. Go to step 4. If fewer than m lines are
needed, then the current solution is not optimal, and proceed to step 3.
Step 3: Find the smallest uncovered element (say, k) in the reduced cost matrix. Subtract
k from each uncovered element in the reduced cost matrix, and add k to each element that
is covered by two lines. Return to step 2.
Step 4: When the optimal solution is found in step 2, the next step is determining the
optimal assignment. While making the assignments, start with the rows or columns that
contain a single zero-valued cell. Once a cell with 0 value is chosen, eliminate the row and
the column that the cell belongs to from further considerations. If no row or column is found
with only one cell with 0 value, then check if there are rows/columns with a successively
higher number of cells with 0 values, and choose arbitrarily from the available options. Note
that, if there are several zeros at multiple locations, alternative optimal solutions might
exist.
4.6.3
Application of the Algorithm Through an Example
EFGH Manufacturing has three machines and three jobs to be completed. Each machine
must complete one job. The time required to set up each machine for completing each job is
shown in Table 4.3. EFGH Manufacturing wants to minimize the total setup time required
to complete all three jobs by assigning each job to a machine.
The Hungarian algorithm is applied to the EFGH example as follows: After the row and
column minimums are subtracted from each cell in the cost matrix, two lines are required
to cover all zeros in the modiﬁed matrix in Table 4.4.c, which implies the nonoptimality of
the current solution. The minimum value of all uncovered cells is 2. As a result, subtract 2
from each uncovered cell, and add 2 to the cell that is covered by two lines (i.e., ﬁrst row,
ﬁrst column) to obtain the matrix in Figure 4.5d. For this modiﬁed matrix, the minimum
number of lines required to cover is three, and thus, the optimal solution can be obtained.
The optimal assignment is job 1 to machine 3, job 2 to machine 2, and job 3 to machine 1,
with a total setup time (cost) of 5.
4.7
Minimum Spanning Tree Problem
In a connected, undirected graph, a spanning tree, T, is a subgraph that connects all nodes
on the network. Given that each arc has a weight cij, the length (or cost) of the spanning
tree is equal to the sum of the weights of all arcs on that tree, that is, 
(i,j)∈T cij. The
minimum spanning tree problem identiﬁes the spanning tree with the minimum length. In
other words, a minimum spanning tree is a spanning tree with a length less than or equal to
the length of every other spanning tree. The minimum spanning tree problem is similar to
the shortest path problem discussed in Section 4.4, with the following diﬀerences: In shortest

Network Optimization
4-15
TABLE 4.4
Solving an Assignment Problem Using the Hungarian Method
2
0
4
6
0
0
0
2
0
5
1
1
1
1
1
5
3
1
4
0
0
0
2
6
0
4
2
Column Minimum
Row Minimum
0
0
0
(a) Subtracttherowminimum
(b) Subtract the column minimum
Uncovered minimum2
4
0
0
0
2
6
0
4
2
(c)Minimum number of lines to cover zerosis
     two, thus not optimal
(d) Minimum number of lines to cover zeros is
      three, thus optimal
6
0
0
2
0
4
0
2
0
5
1
1
1
3
7
1
5
3
(e) Assignment of jobs to machines on the
      modified cost matrix
(f) Assignment of jobs to machines on the orignal
      cost matrix, total cost  5
1
3
7
path problems, the objective is to determine the minimum total cost of a set of links (or
arc) that connect source to sink. In minimum spanning tree problems, the objective is to
select a minimum total cost set of links (or arcs) such that all the nodes are connected and
there is no source or sink node. The optimal shortest path tree from an origin to all other
nodes is a spanning tree, but not necessarily a minimum spanning tree.
Minimum spanning tree problems have several applications, speciﬁcally in network design.
For example, the following problems can be optimized by minimum spanning tree algo-
rithms: connecting diﬀerent buildings on a university campus with phone lines or high-speed
Internet lines while minimizing total installation costs; connecting diﬀerent components on
a printed circuit board to minimize the length of wires, capacitance, and delay line eﬀects;
and constructing a pipeline network connecting a number of towns to minimize the total
length of pipeline.

4-16
Operations Research Methodologies
4.7.1
Linear Programming Formulation
Let A(S) ⊆A denote the set of arcs induced by the node set S, that is, if (i, j) ∈A(S) then
i ∈S and j ∈S. The linear programming formulation can be stated as follows:
Minimize:

(i,j)∈A
cijxij
(4.19)
Subject to:

(i,j)∈A
xij = |N| −1
(4.20)

(i,j)∈A(S)
xij = |S| −1
S ⊆N
(4.21)
xij ∈{0, 1}
(i, j) ∈A
(4.22)
In this formulation, xij is the decision variable that identiﬁes if arc (i, j) should be included
in the spanning tree. The objective is to minimize the total cost of arcs included in the
spanning tree. In a network with |N| nodes, |N| −1 arcs should be included in the tree
(Equation 4.20). Using Equation 4.21, we ensure that there are no cycles on the minimum
spanning tree subgraph. Note that when A(S) = A, Equations 4.20 and 4.21 are equivalent.
4.7.2
Kruskal’s Algorithm
Kruskal’s algorithm builds an optimal spanning tree by adding one arc at a time. By deﬁning
LIST as the set of arcs that is chosen as part of a minimum spanning tree, Kruskal’s
algorithm can be stated as follows:
Step 1: Sort all arcs in non-decreasing order of their costs to obtain the set A′.
Step 2: Set LIST = Ø.
Step 3: Select the arc with the minimum cost, and remove this arc from A′.
Step 4: Add this arc to the LIST if its addition does not create a cycle.
Step 5: If the number of arcs in the list is |LIST| = |N| −1, then stop; arcs in the LIST
form a minimum spanning tree. Otherwise, go to step 3.
In the example below, a network design problem is considered. In this problem, Wichita
State University has six buildings that are planned to be connected via a gigabit network.
The cost of connecting the buildings (in thousands) for which there can be a direct con-
nection is given in Figure 4.5a. Using Kruskal’s algorithm, the minimum cost design of
the gigabit network for Wichita State University is determined. First, all arcs are sorted
in nondecreasing order of their costs: (5,6), (4,5), (1,2), (2,4), (4,6), (1,3), (2,5), (3,5), and
(2,6). In the ﬁrst four iterations, arcs (5,6), (4,5), (1,2), and (2,4) are added, that is, the
LIST = {(5,6), (4,5), (1,2), (2,4)} (see Figure 4.5b–e). In the ﬁfth iteration, arc (4,6) cannot
be added as it creates the cycle 4-5-6 (see Figure 4.5f). Finally, when arc (1,3) is added to
the LIST, all nodes on the network are connected, that is, the minimum spanning tree is
obtained. As shown in Figure 4.5g, the minimum spanning tree is LIST = {(5,6), (4,5), (1,2),
(2,4),(1,3)}, and the total cost of the minimum cost tree is 10 + 10 + 20 + 20 + 25 = 85.
Other types of minimum spanning tree problems (Garey and Johnson 1979) are as follows:
A degree-constrained spanning tree is a spanning tree where the maximum vertex degree
is limited to a certain constant k. The degree-constrained spanning tree problem is used
to determine if a particular network has such a spanning tree for a particular k. In the
maximum leaf spanning tree problem, the goal is to determine if the network has at least

Network Optimization
4-17
1
2
4
3 
5 
6
20 
25 
35 
55 
35 
10
25 
10
20 
1
2
4
3
5
6
20
25
35
55
35
10
25
10
20
1
2
4
6
20
25
35
55
35
10
25
10
20
(a)
(b)
(c)
(d)
(e)
(f)
(g)
1
2
4
3
6
20
25
35
55
35
10
25
10
20
1
2
4
6
20
25
35
55
35
10
25
10
20
1
2
4
3
5
6
20
25
35
55
35
10
25
10
20
1
2
4
3
5
6
20
25
35
55
35
10
25
10
20
5
3
5
FIGURE 4.5
Application of Kruskal’s algorithm to determine the minimum spanning tree for the net-
work shown in Figure 4.5a. (a) Original network. (b) Arc (5,6) is added. (c) Arc (4,5) is added. (d) Arc
(1,2) is added. (e) Arc (2,4) is added. (f) Arc (4,6) cannot be added as it creates a cycle. (g) Arc (1,3) is
added. Algorithm terminates.

4-18
Operations Research Methodologies
k nodes having a degree 1, where k < |N|. In the shortest total path length spanning tree
problem, the goal is to ensure that the distance between any two nodes on the minimum
spanning tree is less than a constant B.
4.8
Minimum Cost Multicommodity Flow Problem
The minimum cost multicommodity network ﬂow problem (MCMNF) is a generalization of
a minimum cost ﬂow problem that considers a single commodity. In MCMNF, the goal is to
minimize the shipping cost of several commodities from supply nodes to demand nodes while
sharing a ﬁxed arc capacity on the network. The commodities in a multicommodity ﬂow
problem can be diﬀerentiated by physical characteristics or by origin/destination pairs. The
decision of assigning a percentage of each arc’s capacity to diﬀerent commodities to minimize
the overall cost increases the complexity of this problem signiﬁcantly when compared with
other network ﬂow problems such as the shortest path and maximum ﬂow problems.
Multicommodity network ﬂow problems have several applications: in communication net-
works, packets from diﬀerent origin destination pairs share the same capacity. Further-
more, the same network might transfer several types of data, such as cable TV, phone, and
video conferencing, which might have diﬀerent service requirements simultaneously. A sim-
ilar observation can be made for transportation networks if commodities are deﬁned with
respect to origin–destination pairs or diﬀerent types of transportation such as cars, busses,
trucks, and the like. In distribution networks, commodities can be deﬁned as diﬀerent types
of products that must be transferred from plants to warehouses and then from warehouses
to customers.
In aviation networks, passengers from diﬀerent origins are transferred through hubs to
travel to their destinations. Passengers must travel on planes with diﬀerent types of capacity.
Furthermore, airports have limited capacity, which restricts the number of planes that can
park at the airport at any time. The goal is to minimize the total cost of transportation on
this network.
4.8.1
Linear Programming Formulation
In minimum cost multicommodity ﬂow problems, the objective is to minimize the overall
cost of transportation of |K| commodities from origins to destinations. Let ck
ij be the cost
of transporting a unit of commodity k on arc (i,j). The decision variable xk
ij is the amount
of commodity k transported on arc (i,j). The mathematical programming formulation for
a minimum cost multicommodity ﬂow problem can be stated as follows:
Minimize:

k∈K

(i,j)∈A
ck
ijxk
ij
(4.23)
Subject to:

{j:(i,j)∈A}
xk
ij −

{j:(j,i,)∈A}
xk
ji = bk
i
i ∈N, k ∈K
(4.24)

k∈K
xk
ij ≤uij
(i,j) ∈A
(4.25)
xk
ij ≥0
(i,j) ∈A
(4.26)
In the above formulation, the mass balance constraint for each commodity (Equation 4.24)
ensures the conservation of ﬂow at each node for that commodity. Equation 4.25 is the
capacity constraint on each arc. This mathematical model has |K||A| variables, |K||N|

Network Optimization
4-19
node balance constraints, and |A| capacity constraints. Note that all of the network models
described in this chapter, except the minimum spanning tree problem, are a special case
of the minimum cost multicommodity ﬂow problem. In the literature, this problem has
been solved using methods like Lagrangian relaxation, column generation approaches, and
Dantzig–Wolfe decomposition.
4.9
Conclusions
Network optimization is a very popular tool to solve large-scale real-life problems. Because
of the availability of very fast network optimization methods, researchers can tackle very
complex real-life problems eﬃciently. All of the network ﬂow models presented in this chap-
ter, except the minimum spanning tree problem, assume that some sort of ﬂow must be sent
through a network; thus, one has to decide how this can be achieved using the algorithms
presented in this chapter to optimize the operations in a network. On the other hand, the
minimum spanning tree problem can be used to optimize the design of a network.
This chapter presents some very basic models and solution procedures. More detailed
analysis of these network models can be found in the network ﬂow literature. The bibliogra-
phy below provides a selected list of books in operations research and network optimization.
The introduction to operations research texts by Hillier and Lieberman (2005), Jensen and
Bard (2003), and Winston (2005) contain overviews of network ﬂow models similar to those
presented here at the elementary level. The two best comprehensive references on network
models are by Ahuja, Magnanti, and Orlin (1993) and Bertsekas (1991). The notation used
in this chapter is similar to the former.
Below is a short list of websites that contain either freely available software or datasets
related to network optimization:
• NEOS Guide, a repository about optimization
http://www-fp.mcs.anl.gov/otc/GUIDE/
• Andrew Goldberg’s codes for shortest path and minimum cost ﬂow algorithms
http://www.avglab.com/andrew/#library
• Dmitri Bertsekas Network Optimization Codes
http://web.mit.edu/dimitrib/www/noc.htm
• GOBLIN-C++ Object for network optimization
http://www.math.uni-augsburg.de/opt/goblin.html
• J E Beasley OR library: data sets for OR problems
http://people.brunel.ac.uk/∼mastjjb/jeb/info.html
• C++ code for multicommodity ﬂow problems
http://www.di.unipi.it/di/groups/optimize/
Network problems can also be solved using professional LP/MIP solvers. For example,
ILOG/CPLEX, SUNSET SOFTWARE/XA, and DASH/XPRESS are powerful solvers that
take into account the network structure of large-scale problems to determine a solution in
a reasonable amount of time.
References
1. Ahuja, R. K., Magnanti, T. L., and Orlin, J. B., Network Flows: Theory, Algorithms, and
Applications, Prentice-Hall, Englewood Cliﬀs, NJ, 1993.

4-20
Operations Research Methodologies
2. Balakrishnan, V., Network Optimization, First Edition, Chapman & Hall/CRC, Boca
Raton, FL, 1995.
3. Bazaraa, M. S., Jarvis, J. I., and Sherali, H. D., Linear Programming and Network Flows,
Third Edition, Wiley, New York, 2005.
4. Bertsekas, D. P., Linear Network Optimization, MIT Press, Cambridge, MA, 1991.
5. Evans, J. R. and Minieka, E., Optimization Algorithms for Networks and Graphs, Second
Edition, Marcel Dekker, New York, 1992.
6. Garey, M. R. and Johnson, D. S., Computers and Intractability: A Guide to the Theory of
NP-Completeness, W. H. Freeman, New York, 1979.
7. Glover, F., Klingman, D., and Phillips, N. V., Network Models in Optimization and Their
Applications in Practice, Wiley, New York, 1992.
8. Hillier, F. S. and Lieberman, G. J., Introduction to Operations Research, Eighth Edition,
McGraw Hill, New York, 2005.
9. Jensen, P. A. and Bard, J. F., Operation Research Models and Methods, Wiley,
New York, 2003.
10. Jensen, P. A. and Barnes, J. W., Network Flow Programming, Wiley, New York, 1980.
11. Lawler, E. L., Combinatorial Optimization: Networks and Matroids, Holt, Rinehart and
Winston, New York, 1976.
12. Murty, K. G., Network Programming, Prentice-Hall, Englewood Cliﬀs, NJ, 1992.
13. Papadimitriou, C. H. and Steiglitz, K., Combinatorial Optimization: Algorithms and Com-
plexity, Dover, Mineola, NY, 1998.
14. Philips D. T. and Garcia-Diaz, A., Fundamentals of Network Analysis, Prentice-Hall, Engle-
wood Cliﬀs, NJ, 1981.
15. Sheﬃ, Y., Urban Transportation Networks: Equilibrium Analysis with Mathematical Pro-
gramming Methods, Prentice-Hall, Englewood Cliﬀs, NJ, 1985.
16. Taha, H. A., Operations Research: An Introduction, Seventh Edition, Prentice Hall, Upper
Saddle River, NJ, 2002.
17. Winston, W. L., Operations Research, Applications and Algorithms, Fourth Edition,
Thompson, Belmont, CA, 2005.

5
Multiple Criteria
Decision Making
Abu S. M. Masud
Wichita State University
A. Ravi Ravindran
Pennsylvania State University
5.1
Some Deﬁnitions ......................................
5-3
5.2
The Concept of “Best Solution” ....................
5-4
5.3
Criteria Normalization................................
5-5
Linear Normalization • Vector Normalization • Use of
10 Raised to Appropriate Power • Use of Range
Equalization Factor
5.4
Computing Criteria Weights .........................
5-6
Weights from Ranks • Rating Method • Ratio
Weighing Method
5.5
Multiple Criteria Methods for Finite
Alternatives ............................................
5-8
Max–Min Method • Min–Max (Regret) Method •
Compromise Programming • TOPSIS Method •
ELECTRE Method • Analytic Hierarchy Process •
PROMETHEE Method
5.6
Multiple Criteria Mathematical Programming
Problems ............................................... 5-15
Deﬁnitions • Determining an Eﬃcient Solution [12] •
Test for Eﬃciency • Classiﬁcation of MCMP Methods
5.7
Goal Programming .................................... 5-19
Goal Programming Formulation • Partitioning
Algorithm for Preemptive Goal Programs • Other Goal
Programming Models
5.8
Method of Global Criterion and Compromise
Programming .......................................... 5-27
Method of Global Criterion • Compromise
Programming
5.9
Interactive Methods................................... 5-29
Classiﬁcation of Interactive Methods • Inconsistency of
the DM • Computational Studies
5.10 MCDM Applications.................................. 5-34
5.11 MCDM Software ...................................... 5-35
5.12 Further Readings ...................................... 5-35
References .................................................... 5-36
Decision problems often exhibit these characteristics: the presence of multiple, conﬂicting
criteria for judging the alternatives and the need for making compromises or trade-oﬀs
regarding the outcomes of alternate courses of action. This chapter covers some of the
practical methods available for helping make better decisions for these types of problems.
5-1

5-2
Operations Research Methodologies
A multiple criteria decision making (MCDM) problem can be represented by the following
generalized model:
Maximize [C1(x), C2(x), . . ., Ck(x)]
(5.1)
x ∈X
where
x is any speciﬁc alternative,
X is a set representing the feasible region or available alternatives, and
C1 is the lth evaluation criterion.
MCDM problems can be broadly classiﬁed as “selection problems” or “mathematical
programming problems.” The focus of multiple criteria selection problems (MCSP) is on
selecting the best or preferred alternative(s) from a ﬁnite set of alternatives, and the alter-
natives are usually known a priori. The MCDM methods that help in identifying the “best”
alternative for such problems will be referred to as the multiple criteria methods for ﬁnite
alternatives (MCMFA). The MCSP is also referred to in the literature as multiple attribute
decision making (MADM) problem [1]. MCSP are often represented in terms of a pay-oﬀ
table. Table 5.1 shows a general format of a pay-oﬀtable, where θij is the outcome of
alternative i with respect to evaluation criterion j.
The focus of multiple criteria mathematical programming (MCMP) problems is to fashion
or create an alternative when the possible number of alternatives is high (or inﬁnite) and
all alternatives are not known a priori. MCMP problems are usually modeled using explicit
mathematical relationships, involving decision variables incorporated within constraints and
objectives. The MCDM methods for identifying the “best” alternative in a MCMP will
be referred to in this book as the multiple criteria mathematical programming methods
(MCMPM). The MCMP problem is also known in the literature as a multiple objective
decision making (MODM) problem or a vector optimization problem. MCMP problems are
usually formulated as
Max[f1(x), f2(x), . . ., fk(x)]
(5.2)
Subject to: gj(x) ≤0, j = 1, 2, . . ., m and x = {xi| i = 1, 2, . . . , n}
where
fl(x) = lth objective function,
l = 1, 2, . . ., k
gj(x) = jth constraint function,
j = 1, 2, . . ., m
TABLE 5.1
Pay-OﬀTable
Criteria
Alternatives
C1
C2
. . .
Cj
. . .
Ck
A1 = x1
θ11
θ12
θ1j
θ1k
A2 = x2
θ21
θ22
θ2j
θ2k
. . .
Ai = xi
θi1
θi2
θij
θik
. . .
Am = xm
θm1
θm2
θmj
θmk

Multiple Criteria Decision Making
5-3
5.1
Some Deﬁnitions
To provide a common understanding of the MCDM problem and its solution methods, we
provide deﬁnitions of critical terms and concepts used in this chapter. In the literature of
MCDM, these terms have special meaning and some are used interchangeably. Most of the
deﬁnitions provided here are based on those given in Refs. [1,2].
Alternatives: Alternatives are the possible courses of action in a decision problem.
Alternatives are at the heart of decision making. In many decision situations, particularly
for MCSP, alternatives can be prespeciﬁed. In such cases, it is important that every attempt
is made for the development of all alternatives. Failure to do so may result in selecting an
alternative for implementation that is inferior to other unexplored ones. In other situations,
usually in MCMP, prespeciﬁcation is not possible. In problems where prespeciﬁcation of
alternatives is not possible, alternatives are deﬁned implicitly through mathematical rela-
tionships between decision variables. The challenge here is to deﬁne appropriate decision
variables and to develop the mathematical relations involving these variables.
Attributes: These are the traits, characteristics, qualities, or performance parameters
of the alternatives.
For example, if the decision situation is one of choosing the “best” car to purchase, then the
attributes could be color, gas mileage, attractiveness, size, and the like. Attributes, from the
decision making point of view, are the descriptors of the alternatives. For MCSP, attributes
usually form the evaluation criteria.
Objectives: These are the directions of improvement or to do better, as perceived by
the decision maker (DM).
For example, considering the same example of choosing the “best” car, an objective may be
to “maximize gas mileage.” This objective indicates that the DM prefers higher gas mileage,
the higher the better. For MCMP, objectives form the evaluation criteria.
Goals: These are speciﬁc (or desired) status of attributes or objectives. Goals are
targets or thresholds of objective or attribute values that are expected to be
attained by the “best” alternative.
For example, in choosing the “best” car, a goal may be to buy a car that achieves an average
gas mileage of 20 mpg or more; another example, buy a “4-door car.”
(Evaluation) Criteria: These are the rules of acceptability or standards of judg-
ment for the alternatives. Therefore, criteria encompass attributes, goals, and
objectives.
Criteria may be true or surrogate. When a criterion is directly measurable, it is called a true
criterion. An example of a true criterion is “the cost of the car,” which is directly measured
by its dollar price. When a criterion is not directly measurable, it may be substituted by
one or more surrogate criteria. A surrogate criterion is used in place of one or more others
that are more expressive of the DM’s underlying values but are more diﬃcult to measure
directly. An example may be using “headroom for back seat passengers” as a surrogate
for “passenger comfort.” “Passenger comfort” is more expressive as a criterion, but is very
diﬃcult to measure. “Headroom for back seat passengers” is, however, easier to measure
and can be used as one of the surrogate criteria for representing “passenger comfort.”

5-4
Operations Research Methodologies
5.2
The Concept of “Best Solution”
In single criterion decision problems, the “best” solution is deﬁned in terms of an “optimum
solution” for which the criterion value is maximized (or minimized) when compared to any
other alternative in the set of all feasible alternatives. In MCDM problems, however, as the
optimum of each criterion do not usually point to the same alternative, a conﬂict exists. The
notion of an “optimum solution” does not usually exist in the context of conﬂicting, multiple
criteria. Decision making in a MCDM problem is usually equivalent to choosing the best
compromise solution. The “best solution” of an MCDM problem may be the “preferred (or
best compromise) solution” or a “satisﬁcing solution.” In the absence of an optimal solution,
the concepts of dominated and nondominated solutions become relevant. In the MCDM
literature, the terms “nondominated solution,” “Pareto optimal solution,” and “eﬃcient
solution” are used interchangeably. In addition, concepts of “ideal solution” and “anti-ideal
solution” are relevant in many MCDM methods.
Satisﬁcing Solution: It is a feasible solution that meets or exceeds the DM’s mini-
mum expected level of achievement (or outcomes) of criteria values.
Nondominated Solution: A feasible solution (alternative) x1 dominates another
feasible solution (alternative) x2 if x1 is at least as good as (i.e., as preferred
as) x2 with respect to all criteria and is better than (i.e., preferred to) x2 with
respect to at least one criterion. A nondominated solution is a feasible solution
that is not dominated by any other feasible solution. That is, for a nondominated
solution an increase in the value of any one criterion is not possible without some
decrease in the value of at least one other criterion. Mathematically, a solution
x1 ∈X is nondominated if there is no other x ∈X such that Ci(x) ≥Ci(x1),
i = 1,2, . . ., k, and Ci(x) ̸= Ci(x1).
Ideal Solution: An ideal solution H∗is an artiﬁcial solution, deﬁned in criterion
space, each of whose elements is the maximum (or optimum) of a criterion’s
value for all x ∈X. That is, the ideal solution consists of the upper bound of the
criteria set.
The ideal solution is also known as positive-ideal solution or the utopia solution. Mathe-
matically, the ideal solution for Equation 5.1 is obtained by
H∗= {H∗
i = Max Ci(x)|x ∈X,
i = 1, 2, . . ., k}
(5.3)
In all nontrivial MCDM problems, the ideal solution is an infeasible solution.
Anti-Ideal Solution: The anti-ideal solution, L∗consists of the lower bound of the
criteria set.
This solution is also known as the negative-ideal solution or the nadir solution. One math-
ematical deﬁnition of the anti-ideal solution for Equation 5.1 is
L∗= {Li∗= Min Ci(x∗
j)| j = 1, 2, . . ., m;
i = 1, 2, . . ., k}
(5.4)
That is, the minimum values in each column of the pay-oﬀtable constitute the anti-ideal
solution. This deﬁnition works well for problems where all the alternatives are prespeciﬁed.
In problem (5.2) where all alternatives are never identiﬁed, this can cause a problem. This
situation can be avoided by solving the following i = 1, 2, . . ., k single criterion minimization
problems:
Min
fi(x)
(5.5)
Subject to
x ∈X

Multiple Criteria Decision Making
5-5
For simplicity, the pay-oﬀtable is commonly used to identify the anti-ideal solution. How-
ever, with this simpliﬁcation, we run the risk of being far oﬀfrom the real anti-ideal.
5.3
Criteria Normalization
A common problem in multiple criteria decision making with the use of diﬀering units
of evaluation measures is that relative rating of alternatives may change merely because
the units of measurement have changed. This issue can be addressed by normalization.
Normalization allows intercriterion comparison. In the following discussion of normalization,
assume that a beneﬁt criterion is one in which the DM prefers more of it (i.e., more is better)
and a cost criterion is one in which the DM prefers less of it (i.e., less is better). In general,
a cost criterion can be transformed mathematically to an equivalent beneﬁt criterion by
multiplying by −1 or by taking the inverse of it.
5.3.1
Linear Normalization
Linear normalization converts a measure to a proportion of the way along the allowed range,
where the allowed range is transformed to that between 0 and 1. A measure Cj(xi), outcome
in criterion j for alternative xi, is normalized to rij as follows:
rij = Cj(xi) −Lj∗
H∗
j −Lj∗
(for beneﬁt criterion)
(5.6)
rij = Lj∗−Cj(xi)
Lj∗−H∗
j
(for cost criterion)
where
Lj∗= Max {Cj(xi),
i = 1, 2, . . ., m} for cost criterion j and Min {Cj(xi),
i = 1, 2, . . ., m} for beneﬁt criterion j
H∗
j = Min {Cj(xi),
i = 1, 2, . . ., m} for cost criterion j and Max {Cj(xi),
i = 1, 2, . . ., m} for beneﬁt criterion j
Note that, after normalization, all criteria are transformed to beneﬁt criteria (i.e., to be
maximized).
A less common form of linear normalization works as follows:
rij = H∗
j −Cj(xi)
H∗
j
(5.7)
This form of normalization considers the distance of the ideal value of a criterion from the
origin as a unit distance. Note that, after such normalization, all the criteria are transformed
to cost criteria (i.e., they are to be minimized).
5.3.2
Vector Normalization
In vector normalization, each criterion outcome Cj(xi) is divided by a norm Lpj as deﬁned
below:
rij = Cj(xi)
Lpj
(5.8)
Lpj =
i=n

i=1
Cj(xi)
p
1/p
,
1 ≤p ≤∞
(5.9)

5-6
Operations Research Methodologies
The usual values of p are 1, 2, or ∞and the corresponding Lp norms are:
L1j =
i=n

i=1
Cj(xi)

(5.10)
L2j =
i=n

i−1
Cj(xi)
2
1/2
(5.11)
L∞j = Max{
Cj(xi)
,
i = 1, 2, . . ., n}
(5.12)
5.3.3
Use of 10 Raised to Appropriate Power
This method, suggested by Steuer [3], rescales outcomes across all criteria to make them
comparable. This is achieved by multiplying each Cj(xi), i = 1, 2, . . ., n, by 10 raised to an
appropriate power that makes all outcomes of the same order of magnitude:
rij = Cj(xi) × 10qj
(5.13)
where qj is an appropriate number that will make criterion j outcomes similar in magnitude
to the other criteria outcomes.
5.3.4
Use of Range Equalization Factor
This approach, also suggested by Steuer [3], tries to make the range of variation of the
achievement values for all criteria comparable. This is done by ﬁrst computing a range
equalization factor, πj, for criterion j and then multiplying each jth criterion outcome by
this factor:
πj = 1
Δj
l=k

l=1
1
Δl
−1
(5.14)
rij = Cj(xi) × πj
(5.15)
where Δj = |H∗
j −Lj∗|.
5.4
Computing Criteria Weights
Many MCDM methods require the use of relative importance weights of criteria. Many of
these methods require ratio-scaled weights proportional to the relative value of unit changes
in criteria value functions.
5.4.1
Weights from Ranks
This is a simple and commonly used method in which only the rank order of the criteria
is used for developing the weights. First, the DM ranks the criteria in order of increasing
relative importance; the highest ranked criterion gets a rank of 1. Let ri represent the rank
of the ith criterion. Next, determine criterion weight, λi, as follows:
λi =
k −ri + 1
j=k
j=1 (k −rj + 1)
(5.16)
where k is the number of criteria. This method produces an ordinal scale but does not
guarantee the correct type of criterion importance because ranking does not capture the
strength of preference information.

Multiple Criteria Decision Making
5-7
When a large number of criteria are considered, it may be easier for the DM to provide
pairwise ranking instead of complete ranking. Assuming consistency in pairwise ranking
information, k(k −1)/2 such comparisons can be used to derive the complete rank order. The
number of times criterion i is ranked higher than all other criteria (in pairwise comparisons)
is used to generate the rank order. If cij indicates the comparison of criterion Ci with
criterion Cj, then cij = 1 if Ci is preferred over Cj and cij = 0 if Ci is not preferred over Cj.
Note: cii = 1. Next, ﬁnd criterion totals, ti = j=k
j=1 cij and criteria weights λi =
ti
 i=k
i=1 ti .
5.4.2
Rating Method
The method works as follows: ﬁrst, an appropriate rating scale is agreed to (e.g., from 0 to
10). The scale should be clearly understood to be used properly. Next, using the selected
scale, the DM provides rating for each criterion, ri, judgmentally. Finally, normalize the
ratings to determine weights:
λi =
ri
j=k
j=1 rj
(5.17)
This method fails to assure a ratio scale and may not even provide the appropriate
importance.
5.4.3
Ratio Weighing Method
In this method, originally proposed by Saaty [4], the DM compares two criteria at a time
and indicates aij, which is the “number of times criterion Ci, is more important than
criterion Cj.” At least (k −1) pairwise evaluations are needed. As inconsistency is expected
in a large number of pairwise comparisons, many such methods require more than (k −1)
comparisons. Saaty has proposed a method for determining criteria weights based on the
principal eigenvector of the pairwise comparison matrix. Let matrix A represent the pairwise
comparisons (note that aii = 1).
A =
⎡
⎢⎢⎣
1
a12
. . .
a1k
a21
1
. . .
a2k
ak1
ak2
. . .
1
⎤
⎥⎥⎦
If aij = 1/aji, aij = ail × alj, and aij > 0, then A is called a positive reciprocal matrix. With
(k −1) comparisons, the rest of the matrix can be ﬁlled by using the above relations. The
principal eigenvector of A, π, can be found by ﬁnding the largest eigenvalue, αmax, for the
following set of equations:
Aπ = αmaxπ
(5.18)
Weights are simply the normalized principal eigenvector values:
λi =
πi
j=k
j=1 πj
(5.19)
An easy way of computing the principal eigenvector and the weights is given by Harker [5].
He recursively computes the ith estimate πi as follows:
πi =
Aie
eTAie
(5.20)
Note: A1 = A, Ai = (Ai−1A) and eT = (1, 1, . . ., 1). Saaty also provides a measure for checking
the consistency of A; see the section on AHP for details. This method assures a ratio scale.

5-8
Operations Research Methodologies
5.5
Multiple Criteria Methods for Finite
Alternatives
All methods discussed in this section are appropriate for the following general MCDM
problem:
Max[C1(x), C2(x), . . ., Ck(x)]
(5.21)
Subject to
x ∈X
In the context of an MCSP,
C1(x) = the lth attribute for alternative x, θl(x)
x ∈X = the set of available alternatives
x = any alternative
θjl = θl(xj) = lth attribute value for jth alternative
5.5.1
Max–Min Method
This method is based on the assumption that the DM is very pessimistic in his/her outlook
and wants to maximize, over the decision alternatives, the achievement in the weakest crite-
rion. Alternatives are characterized by the minimum achievement among all of its criterion
values. It, thus, uses only a portion of the available information by ignoring all other crite-
rion values. To make intercriterion comparison possible, all criterion values are normalized.
Geometrically, this method maximizes the minimum normalized distance from the anti-ideal
solution along each criterion for all available alternatives.
Mathematically, this method works as follows (assuming linear normalization):
Max

Min
Cl(x) −Ll∗
H∗
l −Ll∗

, l = 1, 2, . . ., k

(5.22)
Subject to
x ∈X
where
H∗
l = ideal solution value for the lth criterion
Ll∗= anti-ideal solution value for the lth criterion
5.5.2
Min–Max (Regret) Method
In this method, it is assumed that the DM wants to minimize the maximum opportunity
loss. Opportunity loss is deﬁned as the diﬀerence between the ideal (solution) value of a
criterion and the achieved value of that criterion in an alternative. Thus, the Min–Max
method tries to identify a solution that is close to the ideal solution. Geometrically, this
method ﬁnds a solution that minimizes the maximum normalized distance from the ideal
solution along each criterion for all available alternatives.
Mathematically, this method is represented by the following problem,
Min

Max
H∗
l −Cl(x)
H∗
l −Ll∗

, l = 1, 2, . . ., k

(5.23)
Subject to
x ∈X
where
H∗
l = ideal solution value for the lth criterion
Ll∗= anti-ideal solution value for the lth criterion

Multiple Criteria Decision Making
5-9
5.5.3
Compromise Programming
Compromise programming (CP) identiﬁes the preferred solution (alternative) that is as
close to the ideal solution as possible. That is, it identiﬁes the solution whose distance from
the ideal solution is minimum. Distance is measured with one of the metrics, Mp, deﬁned
below. Distance is usually normalized to make it comparable across criteria units. Note that
CP is also known as the global criterion method. Mathematically, compromise programming
involves solving the following problem:
Min Mp(x)
(5.24)
Subject to
x ∈X
where the metric Mp is deﬁned (using linear normalization) as follows:
Mp(x) =
i=k

i=1

H∗
i −Ci(x)
H∗
i −Li∗

p1/p
,
1 ≤p ≤∞
(5.25)
As H∗
i ≥Ci(x) is always true for Equation 5.25, the CP problem formulation can be restated
as follows:
Min Mp(x) =
i=k

i=1
H∗
i −Ci(x)
H∗
i −Li∗
p1/p
,
1 ≤p ≤∞
(5.26)
Subject to
x ∈X
Using Equation 5.26 is simpler and it is the form commonly used in compromise
programming.
Note that geometrically, the distance measures in a CP problem have diﬀerent meanings
depending on the value of p chosen. For p = 1, M1(x) measures the “city-block” or “Man-
hattan block” distance (sum of distances along all axes) from H∗; for p = 2, M2(x) measures
the straight-line distance from H∗; for p = ∞, M∞(x) measures the maximum of the axial
distances from H∗.
5.5.4
TOPSIS Method
TOPSIS (technique for order preference by similarity to ideal solution) was originally pro-
posed by Hwang and Yoon [2] for the MCSP. TOPSIS operates on the principle that the
preferred solution (alternative) should simultaneously be closest to the ideal solution, H∗,
and farthest from the negative-ideal solution, L∗. TOPSIS does not require the speciﬁca-
tion of a value (utility) function but it assumes the existence of monotonically increasing
value (utility) function for each (beneﬁt) criterion. The method uses an index that combines
the closeness of an alternative to the positive-ideal solution with its remoteness from the
negative-ideal solution. The alternative that maximizes this index value is the preferred
alternative. In TOPSIS, the pay-oﬀmatrix is ﬁrst normalized as follows:
rij =
θij

i θ2
ij
1/2
i = 1, . . ., m; j = 1, . . ., k
(5.27)
Next, the weighted pay-oﬀmatrix, Q, is computed:
qij = λjrij
i = 1, 2, . . ., m; j = 1, 2, . . ., k
(5.28)
where λj is the relative importance weight of the jth attribute; λj ≥0 and  λj = 1.

5-10
Operations Research Methodologies
Using the weighted pay-oﬀmatrix, ideal and negative-ideal solutions (H∗and L∗) are
identiﬁed as follows:
H∗= {q∗
j , j = 1, 2, . . ., k} = {Max qij, for all i; j = 1, 2, . . ., k}
(5.29)
L∗= {q∗j, j = 1, 2, . . ., k} = {Min qij, for all i; j = 1, 2, . . ., k}
Based on these solutions, separation measures for each solution (alternative) are calculated:
P ∗
i =

j (qij −q∗
j )21/2
,
i = 1, 2, . . ., m
(5.30)
P∗i =

j (qij −q∗j)21/2
,
i = 1, 2, . . ., m
where P ∗
i is the distance of the ith solution (alternative) from the ideal solution and P∗i
is the distance of the same solution from the negative-ideal solution. TOPSIS identiﬁes
the preferred solution by minimizing the similarity index, D, deﬁned below. Note that all
the solutions can be ranked by their index values; a solution with a higher index value is
preferred over that with index values smaller than its value.
Di = P∗i/(P ∗
i + P∗i),
i = 1, 2, . . ., m
(5.31)
Note that 0 ≤Di ≤1; Di = 0 when the ith alternative is the negative-ideal solution and
Di = 1 when the ith alternative is the ideal solution.
5.5.5
ELECTRE Method
ELECTRE method, developed by Roy [6], falls under the category called outranking
methods. It compares two alternatives at a time (i.e., uses pairwise comparison) and attempts
to build an outranking relationship to eliminate alternatives that are dominated using the
outranking relationship. Six successive models of this method have been developed over
time. They are: ELECTRE I, II, III, IV, Tri, and IS. Excellent overviews of the history
and foundations of ELECTRE methods are given by Roy [6,7] and Rogers et al. [8]. We
will explain only ELECTRE I in this section. The outcome of ELECTRE I is a (smaller
than original) set of alternatives (called the kernel) that can be presented to the DM for
the selection of “best solution.” Complete rank ordering of the original set of alternatives
is possible with ELECTRE II.
An alternative Ai outranks another alternative Aj (i.e., Ai →Aj) when it is realistic to
accept the risk of regarding Ai as at least as good as (or not worse than) Aj, even when Ai
does not dominate Aj mathematically. This outranking relationship is not transitive. That
is, it is possible to have Ap →Aq, Aq →Ar but Ar →Ap. Each pair of alternatives (Ai,Aj) is
compared with respect to two indices: a concordance index, c(i,j), and a discordance index,
d(i,j). The concordance index c(i,j) is a weighted sum of the number of criteria in which
Ai is better than Aj. The discordance index d(i,j) is the maximum weighted diﬀerence in
criterion levels among criteria for which Ai is worse than Aj.
Let
θip = pth criterion achievement level for alternative Ai
λp = relative importance weight of criterion p; k
p=1 λp = 1 and λp > 0
rip = pth criterion achievement level (normalized) for Ai; use any appropriate
normalization scheme to derive rip from θip (see previous section)
qip = λp rip

Multiple Criteria Decision Making
5-11
g is the criterion index for which qip > qjp
l is the criterion index for which qip < qjp
e is the criterion index for which qip = qjp
s is the index of all criteria (s = g + l + e)
Then,
c(i, j) =

p∈g
λp +

p∈e
ϕpλp
(5.32)
d(i, j) = Max|qip −qjp|, ∀l
Max|qip −qjp|, ∀s
where ϕp is usually set equal to 0.5.
ELECTRE assumes that criterion levels are measurable on an interval scale for the dis-
cordance index. Ordinal measures are acceptable for the concordance index. Weights should
be ratio scaled and represent relative importance to unit changes in criterion values. Two
threshold values, α and β, are used and these are set by the DM. Sensitivity analysis with
respect to α and β is needed to test the stability of the outranking relationship.
Alternative Ai outranks alternative Aj iﬀc(i,j) ≥α and d(i,j) ≤β. Based on the outrank-
ing relation developed, the preferred set of alternatives, that is, a kernel (K), is deﬁned by
the following conditions:
1. Each alternative in K is not outranked by any other alternative in K.
2. Every alternative not in K is outranked by at least one alternative in K.
5.5.6
Analytic Hierarchy Process
The analytic hierarchy process (AHP) method was ﬁrst proposed by Saaty [4,9]. AHP is
applicable only for MCSP. With AHP, value (utility) function does not need to be evaluated,
nor does it depend on the existence of such a function. To use this method, the decision
problem is ﬁrst structured in levels of a hierarchy. At the top level is the goal or overall
purpose of the problem. The subsequent levels represent criteria, subcriteria, and so on.
The last level represents the decision alternatives.
After the problem has been structured in the form of a hierarchy, the next step is to seek
value judgments concerning the alternatives with respect to the next higher level subcriteria.
These value judgments may be obtained from available measurements or, if measurements
are not available, from pairwise comparison or preference judgments. The pairwise compar-
ison or preference judgments can be provided using any appropriate ratio scale. Saaty has
proposed the following scale for providing preference judgment.
Scale value
Explanation
1
Equally preferred (or important)
3
Slightly more preferred (or important)
5
Strongly more preferred (or important)
7
Very strongly more preferred (or important)
9
Extremely more preferred (or important)
2, 4, 6, 8
Used to reﬂect compromise between scale values

5-12
Operations Research Methodologies
After the value judgments of alternatives with respect to subcriteria and relative impor-
tances (or priorities) of the sub-criteria and criteria have been received (or computed),
composite values indicating overall relative priorities of the alternative are then determined
by ﬁnding weighted average values across all levels of the hierarchy.
AHP is based on the following set of four axioms. The description of the axioms is based
on Harker [5].
Axiom 1: Given two alternatives (or subcriteria) Ai and Aj, the DM can state θij, the
pairwise comparison (or preference judgment), with respect to a given criterion
from a set of criteria such that θji = 1/θij for all i and j. Note that θij indicates
how strongly alternative Ai is preferred to (or better than) alternative Aj.
Axiom 2: When judging alternatives Ai and Aj, the DM never judges one alternative
to be inﬁnitely better than another, that is, θij ̸= ∞with respect to any criterion.
Axiom 3: One can formulate the decision problem as a hierarchy.
Axiom 4: All criteria and alternatives that impact the decision problem are repre-
sented in the hierarchy (i.e., it is complete).
When relative evaluations of subcriteria or alternatives are obtained through pairwise
comparison, Saaty [9] has proposed a methodology (the eigenvector method) for computing
the relative values of alternatives (and relative weights of subcriteria). With this method,
the principal eigenvector is computed as follows:
θv = λmaxv
(5.33)
where v = vector of relative values (weights) and λmax = maximum eigenvalue.
According to Harker [5], the principal eigenvector can be determined by raising the matrix
θ to increasing powers k and then normalizing the resulting system:
v = lim
k→∞
(θke)
(eTθke)
(5.34)
where eT = (1, 1, . . ., 1, 1). The v vector is then normalized to the w vector, such that
i=n
i=1 wi = 1. See Equation 5.20 for an easy heuristic proposed by Harker [5] for estimating v.
Once the w vector has been determined, λmax can be determined as follows:
λmax =
j=n
j=1 θ1jwj

w1
(5.35)
As there is scope for inconsistency in judgments, the AHP method provides for a measure of
such inconsistency. If all the judgments are perfectly consistent, then λmax = n (where n is
the number of subcriteria or alternatives under consideration in the current computations);
otherwise, λmax > n. Saaty deﬁnes consistence index (CI) as follows:
CI = (λmax −n)
(n −1)
(5.36)
For diﬀerent sizes of comparison matrix, Saaty conducted experiments with randomly gen-
erated judgment values (using the 1–9 ratio scale discussed before). Against the means of

Multiple Criteria Decision Making
5-13
the CI values of these random experiments, called random index (RI), the computed CI
values are compared, by means of the consistency ratio (CR):
CR = CI
RI
(5.37)
As a rule of thumb, CR ≤0.10 indicates acceptable level of inconsistency.
The experimentally derived RI values are:
n
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
RI 0.00 0.00 0.58 0.90 1.12 1.24 1.32 1.41 1.45 1.49 1.51 1.48 1.56 1.57 1.59
After the relative value vectors, w, for diﬀerent sub-elements of the hierarchy have been
computed, the next step is to compute the overall (or composite) relative values of the
alternatives. A linear additive function is used to represent the composite relative evaluation
of an alternative. The procedure for determining the composite evaluation of alternatives is
based on maximizing the “overall goal” at the top of the hierarchy. When multiple DMs are
involved, one may take geometric mean of the individual evaluations at each level. For more
on AHP, see Saaty [9]. An excellent tutorial on AHP is available in Forman and Gass [10].
5.5.7
PROMETHEE Method
The preference ranking organization method of enrichment evaluations (PROMETHEE)
methods
have
been
developed
by
Brans
and
Mareschal
[11]
for
solving
MCSP.
PROMETHEE I generates a partial ordering on the set of possible alternatives, while
PROMETHEE II generates a complete ordering of the alternatives. The PROMETHEE
methods seek to enrich the usual dominance relation to generate better solutions for the
general selection type problem. Only PROMETHEE I will be summarized in this section.
We assume that there exists a set of n possible alternatives, A = [A1, A2, . . ., An], and
k criteria, C = [C1, C2, . . ., Ck], each of which is to be maximized. In addition, we assume
that the relative importance weights, λ = [λ1, λ2, . . ., λk], associated with the k criteria, are
known in advance. We further assume that the criteria achievement matrix θ is normalized,
using any appropriate method, to R so as to eliminate all scaling eﬀects.
Traditionally, alternative A1 is said to dominate alternative A2 iﬀθ1j ≥θ2j, ∀j and
θ1j > θ2j, for at least one j. However, this deﬁnition does not work very well in situa-
tions where A1 is better than A2 with respect to the ﬁrst criteria by a very wide margin
while A2 is better than A1 with respect to the second criteria by a very narrow margin or
A1 is better than A2 with respect to criterion 1 by a very narrow margin and A2 is better
than A1 with respect to criterion 2, again by a very narrow margin, or A1 is marginally
better than A2 with respect to both criteria.
To overcome such diﬃculties associated with the traditional deﬁnition of dominance, the
PROMETHEE methods take into consideration the amplitudes of the deviations between
the criteria. For each of the k criteria, consider all pairwise comparisons between alternatives.
Let us deﬁne the amplitude of deviation, di, between alternative a and alternative b with
respect to criterion i as
di = Ci(a) −Ci(b) = θai −θbi

5-14
Operations Research Methodologies
The following preference structure summarizes the traditional approach:
If di > 0 then a is preferred to b and we write aPb.
If di = 0 then a is indiﬀerent to b and we write aIb.
If di < 0 then b is preferred to a and we write bPa.
In PROMETHEE a preference function for criteria i, Pi(a, b), is introduced to indicate
the intensity of preference of alternative a over alternative b with respect to criterion i.
(Note: Pi(b, a) gives the intensity of preference of alternative b over alternative a.) Pi(a, b)
is deﬁned such that 0 ≤Pi(a, b) ≤1 and
Pi(a, b) = 0 if di ≤0 (equal to or less than 0), indicating “no preference” between a
and b
Pi(a, b) ≈0 if di > 0 (slightly greater than 0), indicating “weak preference” of a over b
Pi(a, b) ≈1 if di >> 0 (much greater than 0), indicating “strong preference” of a over b
Pi(a, b) = 1 if di >>> 0 (extremely greater than 0), indicating “strict preference” of a
over b
Next function Ei(a,b) is deﬁned, which can be of six forms. The most commonly used form
is the Gaussian (or normal distribution); we will use this in this section. For each criterion,
the decision maker and the analyst must cooperate to determine the parameter s where
0 < s < 1. This parameter is a threshold delineating the weak preference area from the strong
preference area. Once this parameter is established the values are calculated as follows:
Ei(a, b) =
⎧
⎨
⎩
1 −e−(di)2
2s2 ,
di ≥0
0,
di < 0
(5.38)
If Ei(a, b) > 0, then a is preferred to b with respect to criterion i. π(a, b), preference index
function, is deﬁned next as follows:
π(a, b) =
j=k

j=1
λjEj(a, b)
(5.39)
where λj is the weight of criterion j. The preference index function expresses the global
preference of alternative a over alternative b. Note:
π(a, a) = 0
0 ≤π(a, b) ≤1
π(a, b) ≈0 implies a weak global preference of a over b
π(a, b) ≈1 implies a strong global preference of a over b
π(a, b) expresses intensity of dominance of a over b
π(b, a) expresses intensity of dominance of b over a
Using π(a, b), positive-outranking ﬂow, ϕ+(a), and negative-outranking ﬂow, ϕ−(a), are
calculated as follows. Positive-outranking expresses how alternative “a” outranks all other
alternatives and, therefore, the higher the value the better the alternative is. The negative-
outranking expresses how “a” is outranked by all other alternatives and, therefore, the lower
the value the better the alternative is.
ϕ+(a) =
i=n

i=1
π(a, Ai)
(5.40)
ϕ−(a) =
i=n

i=1
π(Ai, a)
(5.41)

Multiple Criteria Decision Making
5-15
The following function is deﬁned to assist in ordering the preference of the alternatives:
⎧
⎨
⎩
a S+b,
ϕ+(a) > ϕ+(b)
a I+b,
ϕ+(a) = ϕ+(b)
(5.42)
⎧
⎨
⎩
a S−b,
ϕ−(a) < ϕ−(b)
a I−b,
ϕ−(a) = ϕ−(b)
(5.43)
The PROMETHEE I partial relation is the intersection of these two pre-orders. There are
three possible conclusions when making pairwise comparisons between alternatives.
Conclusion I : a outranks b
a P I b if
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
a S+ b and a S−b
a S+ b and a I−b
a I+ b and a S−b
(5.44)
In this case the positive ﬂow exceeds or is equal to the positive ﬂow of b and the negative ﬂow
of b exceeds or is equal to the negative ﬂow of a. The ﬂows agree and the information is sure.
Conclusion II : a indiﬀerent to b
a II b
if a I+ b and a I−b
(5.45)
In this case the positive and negative ﬂows of the two alternatives are equal. The alternatives
are concluded to be roughly equivalent.
Conclusion III : a incomparable to b
This will generally occur if alternative a performs well with respect to a subset of the criteria
for which b is weak while b performs well on the criteria for which a is weak.
5.6
Multiple Criteria Mathematical Programming
Problems
In the previous sections, our focus was on solving MCDM problems with a ﬁnite number
of alternatives, where each alternative is measured by several conﬂicting criteria. These
MCDM problems were called multiple criteria selection problems (MCSP). The methods we
discussed earlier helped in identifying the best alternative or rank order all the alternatives
from the best to the worst.
In this and the subsequent sections, we will focus on MCDM problems with an inﬁnite
number of alternatives. In other words, the feasible alternatives are not known a priori
but are represented by a set of mathematical (linear/nonlinear) constraints. These MCDM
problems are called multicriteria mathematical programming (MCMP) problems.
MCMP Problem
Max
F(x) = {f1(x), f2(x), . . ., fk(x)}
Subject to
gj(x) ≤0
for j = 1, . . ., m
(5.46)

5-16
Operations Research Methodologies
where x is an n-vector of decision variables and fi(x), i = 1, . . ., k are the k criteria/objective
functions.
Let S = {x/gj(x) ≤0,
for all “j”}
Y = {y/F(x) = y for some x ∈S}
S is called the decision space and Y is called the criteria or objective space in MCMP.
A solution to MCMP is called a superior solution if it is feasible and maximizes all the
objectives simultaneously. In most MCMP problems, superior solutions do not exist as the
objectives conﬂict with one another.
5.6.1
Deﬁnitions
Eﬃcient, Non-Dominated, or Pareto Optimal Solution: A solution xo ∈S to
MCMP is said to be eﬃcient if fk(x) > fk(xo) for some x ∈S implies that fj(x) <
fj(xo) for at least one other index j. More simply stated, an eﬃcient solution
has the property that an improvement in any one objective is possible only at
the expense of at least one other objective.
A Dominated Solution is a feasible solution that is not eﬃcient.
Eﬃcient Set: The set of all eﬃcient solutions is called the eﬃcient set or eﬃcient frontier.
Note: Even though the solution of MCMP reduces to ﬁnding the eﬃcient set, it is not
practical because there could be an inﬁnite number of eﬃcient solutions.
Example 5.1
Consider the following bi-criteria linear program:
Max Z1 = 5x1 + x2
Max Z2 = x1 + 4x2
Subject to:
x1 ≤5
x2 ≤3
x1 + x2 ≤6
x1, x2 ≥0
The decision space and the objective space are given in Figures 5.1 and 5.2, respectively.
Corner Points C and D are eﬃcient solutions whereas corner points A, B, and E are domi-
nated. The set of all eﬃcient solutions is given by the line segment CD in both ﬁgures.
An ideal solution is the vector of individual optima obtained by optimizing each
objective function separately ignoring all other objectives.
In Example 5.1, the maximum value of Z1, ignoring Z2, is 26 and occurs at point D.
Similarly, maximum Z2 of 15 is obtained at point C. Thus the ideal solution is (26,15) but
is not feasible or achievable.
Note: One of the popular approaches to solving MCMP problems is to ﬁnd an eﬃcient solu-
tion that comes “as close as possible” to the ideal solution. We will discuss these approaches
later.
■

Multiple Criteria Decision Making
5-17
B(0,3)
C(3,3)
Optimal for Z2
Optimal for Z1
x2
x1
D(5,1)
E(5,0)
Feasible
decision
space
A(0,0)
FIGURE 5.1
Decision space (Example 5.1).
B(3,12)
Z2
C(18,15)
D(26,9)
E(25,5)
(26,15)
A(0,0)
Z1
Achievable
objective
values
FIGURE 5.2
Objective space (Example 5.1).
5.6.2
Determining an Eﬃcient Solution [12]
For the MCMP problem given in Equation 5.46, consider the following single objective
optimization problem, called the Pλ problem.
Max Z =
k

i=1
λifi(x)
Subject to: x ∈S
k

i=1
λi = 1
λi ≥0
(5.47)

5-18
Operations Research Methodologies
THEOREM 5.1(Suﬃciency)
Let λi > 0 for all i be speciﬁed. If xo is an optimal solution
for the Pλ problem (Equation 5.47), then xo is an eﬃcient solution to the MCMP problem.
In Example 5.1, if we set λ1 = λ2 = 0.5 and solve the Pλ problem, the optimal solution
will be at D, which is an eﬃcient solution.
The Pλ problem (Equation 5.47) is also known as the weighted objective problem.
Warning: Theorem 5.1 is only a suﬃcient condition and is not necessary. For example, there
could be eﬃcient solutions to MCMP that could not be obtained as optimal solutions to the
Pλ problem. Such situations occur when the objective space is not a convex set. However,
for MCMP problems, where the objective functions, and constraints are linear, Theorem 5.1
is both necessary and suﬃcient.
5.6.3
Test for Eﬃciency
Given a feasible solution x ∈S for MCMP, we can test whether it is eﬃcient by solving the
following single objective problem.
Max
W =
k

i=1
di
Subject to:
fi(x) ≥fi(x) + di
for i = 1, 2, . . ., k
x ∈S
di ≥0
THEOREM 5.2
1. If Max W > 0, then x is a dominated solution.
2. If Max W = 0, then x is an eﬃcient solution.
Note: If Max W > 0, then at least one of the di’s is positive. This implies that at least one
objective can be improved without sacriﬁcing on the other objectives.
5.6.4
Classiﬁcation of MCMP Methods
In MCMP problems, often there are an inﬁnite number of eﬃcient solutions and they are not
comparable without the input from the DM. Hence, it is generally assumed that the DM has
a real-valued preference function deﬁned on the values of the objectives, but it is not known
explicitly. With this assumption, the primary objective of the MCMP solution methods is
to ﬁnd the best compromise solution, which is an eﬃcient solution that maximizes the DM’s
preference function.
In the last two decades, most MCDM research have been concerned with developing solu-
tion methods based on diﬀerent assumptions and approaches to measure or derive the DM’s
preference function. Thus, the MCMP methods can be categorized by the basic assumptions
made with respect to the DM’s preference function as follows:
1. When complete information about the preference function is available from the DM.
2. When no information is available.
3. Where partial information is obtainable progressively from the DM.

Multiple Criteria Decision Making
5-19
In the following sections we will discuss MCMP methods such as goal programming, com-
promise programming and interactive methods as examples of categories 1, 2, and 3 type
approaches.
5.7
Goal Programming
One way to treat multiple criteria is to select one criterion as primary and the other crite-
ria as secondary. The primary criterion is then used as the optimization objective function,
while the secondary criteria are assigned acceptable minimum or maximum values depending
on whether the criterion is maximum or minimum and are treated as problem constraints.
However, if careful consideration is not given while selecting the acceptable levels, a fea-
sible design that satisﬁes all the constraints may not exist. This problem is overcome by
goal programming, which has become a practical method for handling multiple criteria.
Goal programming [13] falls under the class of methods that use completely prespeciﬁed
preferences of the decision maker in solving the MCMP problem.
In goal programming, all the objectives are assigned target levels for achievement and
relative priority on achieving these levels. Goal programming treats these targets as goals to
aspire for and not as absolute constraints. It then attempts to ﬁnd an optimal solution that
comes as “close as possible” to the targets in the order of speciﬁed priorities. In this section,
we shall discuss how to formulate goal programming models and their solution methods.
Before we discuss the formulation of goal programming problems, we should discuss the
diﬀerence between the terms real constraints and goal constraints (or simply goals) as used
in goal programming models. The real constraints are absolute restrictions on the decision
variables, whereas the goals are conditions one would like to achieve but are not mandatory.
For instance, a real constraint given by
x1 + x2 = 3
requires all possible values of x1 + x2 to always equal 3. As opposed to this, a goal requiring
x1 + x2 = 3 is not mandatory, and we can choose values of x1 + x2 ≥3 as well as x1 + x2 ≤3.
In a goal constraint, positive and negative deviational variables are introduced as follows:
x1 + x2 + d−
1 −d+
1 = 3
d+
1 , d−
1 ≥0
Note that if d−
1 > 0, then x1 + x2 < 3, and if d+
1 > 0, then x1 + x2 > 3.
By assigning suitable weights w−
1 and w+
1 on d−
1 and d+
1 in the objective function, the
model will try to achieve the sum x1 + x2 as close as possible to 3. If the goal were to satisfy
x1 + x2 ≥3, then only d−
1 is assigned a positive weight in the objective, while the weight on
d+
1 is set to zero.
5.7.1
Goal Programming Formulation
Consider the general MCMP problem given in Section 5.6 (Equation 5.46). The assump-
tion that there exists an optimal solution to the MCMP problem involving multiple criteria
implies the existence of some preference ordering of the criteria by the DM. The goal pro-
gramming (GP) formulation of the MCMP problem requires the DM to specify an acceptable
level of achievement (bi) for each criterion fi and specify a weight wi (ordinal or cardinal)

5-20
Operations Research Methodologies
to be associated with the deviation between fi and bi. Thus, the GP model of an MCMP
problem becomes:
Minimize Z =
k

i=1
(w+
i d+
i + w−
i d−
i )
(5.48)
Subject to:
fi(x) + d−
i −d+
i = bi
for i = 1, . . ., k
(5.49)
gj(x) ≤0
for j = 1, . . ., m
(5.50)
xj, d−
i , d+
i ≥0
for all i and j
(5.51)
Equation 5.48 represents the objective function of the GP model, which minimizes the
weighted sum of the deviational variables. The system of equations (Equation 5.49) repre-
sents the goal constraints relating the multiple criteria to the goals/targets for those criteria.
The variables, d−
i and d+
i , in Equation 5.49 are called deviational variables, representing
the under achievement and over achievement of the ith goal. The set of weights (w+
i and
w−
i ) may take two forms:
1. Prespeciﬁed weights (cardinal)
2. Preemptive priorities (ordinal).
Under prespeciﬁed (cardinal) weights, speciﬁc values in a relative scale are assigned to
w+
i and w−
i representing the DM’s “trade-oﬀ” among the goals. Once w+
i and w−
i are spec-
iﬁed, the goal program represented by Equations. 5.48 to 5.51 reduces to a single objective
optimization problem. The cardinal weights could be obtained from the DM using any of
the methods discussed in Sections 5.1 to 5.5 including the AHP method. However, for this
method to work, the criteria values have to be scaled or normalized using the methods given
in Section 5.3.
In reality, goals are usually incompatible (i.e., incommensurable) and some goals can be
achieved only at the expense of some other goals. Hence, preemptive goal programming,
which is more common in practice, uses ordinal ranking or preemptive priorities to the
goals by assigning incommensurable goals to diﬀerent priority levels and weights to goals at
the same priority level. In this case, the objective function of the GP model (Equation 5.48)
takes the form
Minimize Z =

p
Pp

i
(w+
ipd+
i + w−
ipd−
i )
(5.52)
where Pp represents priority p with the assumption that Pp is much larger then Pp+1 and
w+
ip and w−
ip are the weights assigned to the ith deviational variables at priority p. In this
manner, lower priority goals are considered only after attaining the higher priority goals.
Thus, preemptive goal programming is essentially a sequential single objective optimization
process, in which successive optimizations are carried out on the alternate optimal solutions
of the previously optimized goals at higher priority.
The following example illustrates the formulation of a preemptive GP problem.
Example 5.2
Suppose a company has two machines for manufacturing a product. Machine 1 makes 2 units
per hour, while machine 2 makes 3 units per hour. The company has an order for 80 units.
Energy restrictions dictate that only one machine can operate at one time. The company
has 40 hours of regular machining time, but overtime is available. It costs $4.00 to run

Multiple Criteria Decision Making
5-21
machine 1 for 1 hour, while machine 2 costs $5.00/hour. The company’s goals, in order of
importance, are as follows:
1. Meet the demand of 80 units exactly.
2. Limit machine overtime to 10 hours.
3. Use the 40 hours of normal machining time.
4. Minimize costs.
■
Formulation. Letting xj represent the number of hours machine j is operating, the goal
programming model is
Minimize Z = P1(d−
1 + d+
1 ) + P2d+
3 + P3(d−
2 + d+
2 ) + P4d+
4
Subject to:
2x1 + 3x2 + d−
1 −d+
1 = 80
(5.53)
x1 + x2
+ d−
2 −d+
2 = 40
(5.54)
d+
2
+ d−
3 −d+
3 = 10
(5.55)
4x1 + 5x2 + d−
4 −d+
4 = 0
(5.56)
xi, d−
i , d+
i ≧0
for all i
where P1, P2, P3, and P4 represent the preemptive priority factors such that P1 >> P2 >>
P3 >> P4. Note that the target for cost is set at an unrealistic level of zero. As the goal is
to minimize d+
4 , this is equivalent to minimizing the total cost of production.
In this formulation, Equation 5.55 does not conform to the general model given by Equa-
tions 5.49 to 5.51, where no goal constraint involves a deviational variable deﬁned earlier.
However, if d+
2 > 0, then d−
2 = 0, and from Equation 5.54 we get
d+
2 = x1 + x2 −40
which, when substituted into Equation 5.55, yields
x1 + x2 + d−
3 −d+
3 = 50
(5.57)
Thus Equation 5.57 can replace Equation 5.55 and the problem ﬁts the general model,
where each deviational variable appears in only one goal constraint and has at most one
positive weight in the objective function.
5.7.2
Partitioning Algorithm for Preemptive Goal Programs
Linear Goal Programs
Linear goal programming problems can be solved eﬃciently by the partitioning algorithm
developed by Arthur and Ravindran [14,15]. It is based on the fact that the deﬁnition of
preemptive priorities implies that higher order goals must be optimized before lower order
goals are even considered. Their procedure consists of solving a series of linear programming
subproblems by using the solution of the higher priority problem as the starting solution
for the lower priority problem.
The partitioning algorithm begins by solving the smallest subproblem S1, which is com-
posed of those goal constraints assigned to the highest priority P1 and the corresponding

5-22
Operations Research Methodologies
 Yes 
No
Set K = 1 
Are there real 
constraints? 
K = K + 1
Read the objective
function and the
goal constraints
for Pk
Select the
initial basic feasible
solution  
Perform  
phase 1 
Print optimal solution 
Eliminate
columns with
positive
relative-cost
coefficients  
Perform
simplex
operation
Is the solution
optimal?  
Are there more priorities? 
Do alternate
solutions exist?
Calculate the
achievements
of the
remaining
goals and
priorities 
Yes
Yes
No
No
Yes
No
FIGURE 5.3
Flowchart of the partitioning algorithm.
terms in the objective function. The optimal tableau for this subproblem is then examined
for alternate optimal solutions. If none exist, then the present solution is optimal for the
original problem with respect to all the priorities. The algorithm then substitutes the values
of the decision variables into the goal constraints of the lower priorities to calculate their
attainment levels, and the problem is solved. However, if alternate optimal solutions do
exist, the next set of goal constraints (those assigned to the second highest priority) and
their objective function terms are added to the problem. This brings the algorithm to the
next largest subproblem in the series, and the optimization resumes. The algorithm contin-
ues in this manner until no alternate optimum exists for one of the subproblems or until all
priorities have been included in the optimization. The linear dependence between each pair
of deviational variables simpliﬁes the operation of adding the new goal constraints to the
optimal tableau of the previous subproblem without the need for a dual-simplex iteration.
When the optimal solution to the subproblem Sk−1 is obtained, a variable elimination
step is preformed prior to the addition of goal constraints of priority k. The elimination
step involves deleting all nonbasic columns that have a positive relative cost (Cj > 0) in the
optimal tableau of Sk−1 from further consideration. This is based on the well-known linear
programming (LP) result that a nonbasic variable with a positive relative cost in an optimal
tableau cannot enter the basis to form an alternate optimal solution. Figure 5.3 gives a
ﬂowchart of the partitioning algorithm.

Multiple Criteria Decision Making
5-23
TABLE 5.2
Solution to Subproblem S1
P1 : cj
0
0
1
1
cB
Basis
x1
x2
d−
1
d+
1
b
1
d+
1
2
3
1
−1
80
P1 : c
Row
−2
−3
0
2
Z1 = 80
0
x2
2
3
1
1
3
−1
3
80
3
P1 : c
Row
0
0
1
1
Z1 = 0
TABLE 5.3
Solution to Subproblem S2
P2 : cj
0
0
0
1
cB
Basis
x1
x2
d−
3
d+
3
b
0
x2
2
3
1
0
0
80
3
0
d−
3
1
3
0
1
−1
70
3
P2 : c
Row
0
0
0
1
Z2 = 0
We now illustrate the partitioning algorithm using Example 5.2. The subproblem S1 for
priority P1 to be solved initially is given below:
S1:
Minimize
Z1 = d−
1 + d+
1
Subject to: 2x1 + 3x2 + d−
1 −d+
1 = 80
x1, x2, d−
1 , d+
1 ≧0
The solution to subproblem S1 by the simplex method is given in Table 5.2. However,
alternate optima exist to subproblem S1 (the nonbasic variable x1 has a relative cost of
zero). As the relative costs for d+
1 and d−
1 are positive, they cannot enter the basis later;
else they destroy the optimality achieved for priority 1. Hence, they are eliminated from the
tableau from further consideration.
We now add the goal constraint assigned to the second priority (Equation 5.57):
x1 + x2 + d−
3 −d+
3 = 50
Since x2 is a basic variable in the present optimal tableau (Table 5.2), we perform a row
operation on the above equation to eliminate x2, and we get
1
3x1 + d−
3 −d+
3 = 70
3
(5.58)
Equation 5.58 is now added to the present optimal tableau after deleting the columns cor-
responding to d−
1 and d+
1 . This is shown in Table 5.3. The objective function of subproblem
S2 is given by
Minimize Z2 = d+
3
As the right-hand side of the new goal constraint (Equation 5.57) remained nonnegative
after the row reduction (Equation 5.58), d−
3 was entered as the basic variable in the new
tableau (Table 5.3). If, on the other hand, the right-hand side had become negative, the row
would be multiplied by −1 and d+
3 would become the new basic variable. Table 5.3 indicates

5-24
Operations Research Methodologies
TABLE 5.4
Solution to Subproblem S3
P3 : cj
0
0
0
1
1
cB
Basis
x1
x2
d−
3
d−
2
d+
2
b
0
x2
2
3
1
0
0
0
80
3
0
d−
3
1
3
0
1
0
0
70
3
1
d−
2
1
3
0
0
1
−1
40
3
P3 : ¯c
Row
−1
3
0
0
0
2
0
x2
0
1
0
−2
2
0
0
d−
3
0
0
1
−1
1
10
0
x1
1
0
0
3
3
40
P3 : ¯c
Row
0
0
0
1
1
Z3 = 0
that we have found an optimal solution to S2. As alternate optimal solutions exist, we add
the goal constraint and objective corresponding to priority 3. We also eliminate the column
corresponding to d+
3 . The goal constraint assigned to P3, given by
x1 + x2 + d−
2 −d+
2 = 40
is added after elimination of x2. This is shown in Table 5.4. Now x1 can enter the basis
to improve the priority 3 goal, while maintaining the levels achieved for priorities 1 and 2.
Then d−
2 is replaced by x1 and the next solution becomes optimal for subproblem S3 (see
Table 5.4). Moreover, the solution obtained is unique. Hence, it is not possible to improve
the goal corresponding to priority 4, and we terminate the partitioning algorithm. It is only
necessary to substitute the values of the decision variables (x1 = 40 and x2 = 0) into the
goal constraint for P4 (Equation 5.56) to get d+
4 = 160. Thus, the cost goal is not achieved
and the minimum cost of production is $160.
Integer Goal Programs
Arthur and Ravindran [16] show how the partitioning algorithm for linear GP problems
can be extended with a modiﬁed branch and bound strategy to solve both pure and mixed
integer GP problems. The variable elimination scheme used in the PAGP algorithm is not
applicable for integer goal programs. They demonstrate the applicability of the branch and
bound algorithm with constraint partitioning for integer goal programs with a multiple
objective nurse scheduling problem [17].
Nonlinear Goal Programs
Saber and Ravindran [18] present an eﬃcient and reliable method called the partitioning gra-
dient based (PGB) algorithm for solving nonlinear GP problems. The PGB algorithm uses
the partitioning technique developed for linear GP problems and the generalized reduced
gradient (GRG) method to solve single objective nonlinear programming problems. The
authors also present numerical results by comparing the PGB algorithm against a modiﬁed
pattern search method for solving several nonlinear GP problems. The PGB algorithm found
the optimal solution for all test problems proving its robustness and reliability, whereas the
pattern search method failed in more than half the test problems by converging to a nonop-
timal point.

Multiple Criteria Decision Making
5-25
Kuriger and Ravindran [19] have developed three intelligent search methods to solve
nonlinear GP problems by adapting and extending the simplex search, complex search, and
pattern search methods to account for multiple criteria. These modiﬁcations were largely
accomplished by using partitioning concepts of goal programming. The paper also includes
computational results with several test problems.
5.7.3
Other Goal Programming Models
In addition to the preemptive and non-preemptive goal programming models, other
approaches to solving MCMP problems using goal programming have been proposed. In
both preemptive and non-preemptive GP models, the DM has to specify the targets or
goals for each objective. In addition, in the preemptive GP models, the DM speciﬁes a
preemptive priority ranking on the goal achievements. In the non-preemptive case, the DM
has to specify relative weights for goal achievements.
To illustrate, consider the following bi-criteria linear program (BCLP):
Example 5.3: BCLP
Max f1 = x1 + x2
Max f2 = x1
Subject to: 4x1 + 3x2 ≤12
x1, x2 ≥0
Maximum f1 occurs at x = (0, 4) with (f1, f2) = (4, 0). Maximum f2 occurs at x = (3, 0)
with (f1, f2) = (3, 3). Thus the ideal values of f1 and f2 are 4 and 3, respectively, and the
bounds on (f1, f2) on the eﬃcient set will be:
3 ≤f1 ≤4
0 ≤f2 ≤3
Let the DM set the goals for f1 and f2 as 3.5 and 2, respectively. Then the GP model
becomes:
x1 + x2 + d−
1 −d+
1 = 3.5
(5.59)
x1 + d−
2 −d+
2 = 2
(5.60)
4x1 + 3x2 ≤12
(5.61)
x1, x2, d−
1 , d+
1 , d−
2 , d+
2 ≥0
(5.62)
Under the preemptive GP model, if the DM indicates that f1 is much more important than
f2, then the objective function will be
Min Z = P1d−
1 + P2d−
2
subject to the constraints (Equations 5.59 to 5.62), where P1 is assumed to be much larger
than P2.
Under the non-preemptive GP model, the DM speciﬁes relative weights on the goal
achievements, say w1 and w2. Then the objective function becomes
Min Z = w1d−
1 + w2d−
2
subject to the same constraints (Equations 5.59 to 5.62).

5-26
Operations Research Methodologies
Tchebycheﬀ(Min–Max) Goal Programming
In this GP model, the DM only speciﬁes the goals/targets for each objective. The model
minimizes the maximum deviation from the stated goal. To illustrate, the objective function
for the Tchebycheﬀgoal program becomes:
Min Max (d−
1 , d−
2 )
(5.63)
subject to the same constraints (Equations 5.59 to 5.62).
Equation 5.63 can be reformulated as a linear objective by setting
Max (d−
1 , d−
2 ) = M ≥0
Then Equation 5.63 is equivalent to
Min
Z = M
Subject to: M ≥d−
1
M ≥d−
2
and the constraints given by Equations 5.59 to 5.62.
The advantage of the Tchebycheﬀgoal program is that there is no need to get preference
information (priorities or weights) about goal achievements from the DM. Moreover, the
problem reduces to a single objective optimization problem. The disadvantages are (1) the
scaling of goals is necessary (as required in nonpreemptive GP) and (2) outliers are given
more importance and could produce poor solutions.
Fuzzy Goal Programming
Fuzzy goal programming uses the ideal values as targets and minimizes the maximum
normalized distance from the ideal solution for each objective. The lower and upper bounds
on the objectives are used for scaling the objectives. This is similar to the Min–Max (Regret)
method discussed in Section 5.5.2.
To illustrate, consider Example 5.3 again. Using the ideal values of (f1, f2) = (4, 3) and
the bounds
3 ≤f1 ≤4
0 ≤f2 ≤3
the fuzzy goal programming formulation becomes:
Min
Z = M
Subject to: M ≥4 −(x1 + x2)
4 −3
M ≥3 −x1
3 −0
4x1 + 3x2 ≤12
x1, x2 ≥0
For additional readings on the variants of fuzzy GP models, the reader is referred to Ignizio
and Cavalier [20], Tiwari et al. [21,22], Mohammed [23], and Hu et al. [24].
An excellent source of reference for goal programming methods and applications is the
textbook by Schniederjans [25].

Multiple Criteria Decision Making
5-27
5.8
Method of Global Criterion and Compromise
Programming
5.8.1
Method of Global Criterion
The method of global criterion and compromise programming [1,26,27] fall under the class
of MCMP methods that do not require any preference information from the DM.
Consider the MCMP problem given by Equation 5.46. Let
S = {x/gj(x) ≤0,
for all j}
Let the ideal values of the objectives f1, f2, . . ., fk be f ∗
1 , f ∗
2 , . . ., f ∗
k. The method of global
criterion ﬁnds an eﬃcient solution that is “closest” to the ideal solution in terms of the Lp
distant metric. It also uses the ideal values to normalize the objective functions. Thus the
MCMP reduces to:
Minimize
Z =
k

i=1
f ∗
i −fi
f ∗
i
p
Subject to:
x ∈S
The values of f ∗
i are obtained by maximizing each objective fi subject to the constraints
x ∈S, but ignoring the other objectives. The value of p can be 1, 2, 3, . . ., etc. Note that p = 1
implies equal importance to all deviations from the ideal. As p increases larger deviations
have more weight.
Example 5.4: [1]
Max f1 = 0.4x1 + 0.3x2
Max f2 = x1
Subject to:
x1 + x2 ≤400
(5.64)
2x1 + x2 ≤500
(5.65)
x1, x2 ≥0
(5.66)
Let S represent the feasible region constrained by Equations 5.64 to 5.66. Figure 5.4 illus-
trates the feasible region. The method of global criterion has two steps:
Step 1: Obtain the ideal point.
Step 2: Obtain the preferred solutions by varying the value of p = 1, 2, . . ..
Step 1: Ideal Point: Maximizing f1(x) = 0.4x1 + 0.3x2 subject to x ∈S gives the point
B as the optimal solution, with x∗(100, 300) and f ∗
1 = 130 (see Figure 5.4).
Minimizing f2(x) = x1 subject to x ∈S gives the point C as the optimal solution, with x∗
(250, 0) and f ∗
2 = 250 (see Figure 5.4). Thus, the ideal point (f ∗
1 , f ∗
2 ) = (130, 250).

5-28
Operations Research Methodologies
FIGURE 5.4
Illustration of the method of global criterion (Example 5.4).
Step 2: Obtain Preferred Solutions: A preferred solution is a nondominated or eﬃcient
solution, which is a point on the line segment BC. In Figure 5.4 all points on the line
segment BC are eﬃcient solutions.
Case 1: p = 1
Min Z
130 −(.4x1 + .3x2)
130

+
250 −x1
250

= 2 −.00708x1 −.00231x2
subject to: x ∈S
The optimal solution to the LP problem when p = 1 is given by
x1 = 250, x2 = 0, f1 = 100, f2 = 250
This is point C in Figure 5.4.
Case 2: p = 2
Min Z
130 −(.4x1 + .3x2)
130
2
+
250 −x1
250
2
subject to: x ∈S
This is a quadratic programming problem and the optimal solution is given by
x1 = 230.7, x2 = 38.6, f1 = 103.9, f2 = 230.7
This is point D in Figure 5.4.
The DM’s preferred solution should be one of the eﬃcient points on BC (Figure 5.4). It
is possible that the solutions obtained for p = 1 and p = 2 may not satisfy the DM at all!

Multiple Criteria Decision Making
5-29
5.8.2
Compromise Programming
Compromise programming [26,28] is similar in concept to the one discussed in Section 5.5.3
for MCSP and the method of global criterion. It ﬁnds an eﬃcient solution by minimizing
the Lp distance metric from the ideal point as given below.
Min Lp =
 k

i=1
λp
i (f ∗
i −fi)p
 1/p
(5.67)
Subject to x ∈S
and p = 1, 2, . . ., ∞
where λi’s have to be speciﬁed or assessed subjectively. Note that λi could be set to 1/f ∗
i .
THEOREM 5.3
Any point x∗that minimizes Lp (Equation 5.67) for λi > 0 for all i,
 λi = 1 and 1 ≤p < ∞is called a compromise solution. Zeleny [26] has proved that these
compromise solutions are non-dominated. As p →∞, Equation 5.67 becomes
Min L∞= Min Max
i
[λi(f ∗
i −fi)]
and is known as the TchebycheﬀMetric.
5.9
Interactive Methods
Interactive methods for MCMP problems rely on the progressive articulation of preferences
by the DM. These approaches can be characterized by the following procedure.
Step 1: Find a solution, preferably feasible and eﬃcient.
Step 2: Interact with the DM to obtain his/her reaction or response to the obtained
solution.
Step 3: Repeat Steps 1 and 2 until satisfaction is achieved or until some other termina-
tion criterion is met.
When interactive algorithms are applied to real-world problems, the most critical factor is
the functional restrictions placed on the objective functions, constraints, and the unknown
preference function. Another important factor is preference assessment styles (hereafter
called interaction styles). Typical interaction styles are:
a. Binary pairwise comparison—the DM must compare a pair of two dimensional
vectors at each interaction.
b. Pairwise comparison—the DM must compare a pair of p-dimensional vectors and
specify a preference.
c. Vector comparison—the DM must compare a set of p-dimensional vectors and
specify the best, the worst, or the order of preference (note that this can be done
by a series of pairwise comparisons).
d. Precise local trade-oﬀratio—the DM must specify precise values of local trade-oﬀ
ratios at a given point. It is the marginal rate of substitution between objectives
fi and fj: in other words, trade-oﬀratio is how much the DM is willing to give
up in objective j for a unit increase in objective i at a given eﬃcient solution.

5-30
Operations Research Methodologies
e. Interval trade-oﬀratio—the DM must specify an interval for each local trade-oﬀ
ratio.
f. Comparative trade-oﬀratio—the DM must specify his preference for a given
trade-oﬀratio.
g. Index speciﬁcation and value trade-oﬀ—the DM must list the indices of objectives
to be improved or sacriﬁced, and specify the amount.
h. Aspiration levels (or reference point)—the DM must specify or adjust the values
of the objectives that indicate his/her optimistic wish concerning the outcomes
of the objectives.
Shin and Ravindran [29] provide a detailed survey of MCMP interactive methods. The
survey includes
• A classiﬁcation scheme for all interactive methods.
• A review of methods in each category based on functional assumptions, inter-
action style, progression of research papers from the ﬁrst publication to all its
extensions, solution approach, and published applications.
• A rating of each category of methods in terms of the DM’s cognitive burden, ease
of use, eﬀectiveness, and handling inconsistency.
5.9.1
Classiﬁcation of Interactive Methods
Shin and Ravindran [29] classify the interactive methods as follows:
1. Feasible region reduction methods
2. Feasible direction methods
3. Criterion weight space methods
4. Trade-oﬀcutting plane methods
5. Lagrange multiplier methods
6. Visual interactive methods using aspiration levels
7. Branch-and-bound methods
We will describe each of the above approaches brieﬂy here.
Feasible Region Reduction Methods
Each iteration of this approach generally consists of three phases: a calculation phase; a
decision phase; and a feasible region reduction phase. In the calculation phase, an eﬃcient
solution that is nearest to the ideal solution, in the minimax sense for given weights, is
obtained. In the decision phase, the DM interacts with the method and his/her responses
are used to construct additional constraints in the feasible region reduction phase. The
method continues to perform the three phase iterations until the DM considers the current
solution to be the best compromise solution. Advantages of this method are that it can
terminate at a nonextreme point and extensions to integer and nonlinear cases are only
dependent on the single objective optimization method. However, the method may present
a dominated solution to the DM and it is an ad hoc procedure, as no preference function
concept is utilized. In addition, most methods in this category require a series of index
speciﬁcations and value trade-oﬀs from the DM.
The ﬁrst interactive approach in this category is the STEP method (STEM). It was
originally described as the progress orientation procedure in Benayoun et al. [30] and later

Multiple Criteria Decision Making
5-31
elaborated by Benayoun et al. [31]. STEM guarantees convergence in no more than p (num-
ber of objectives) iterations. Fichefet [32] combined the basic features of goal programming
to form the goal programming STEM method. GPSTEM guides the DM to reach a com-
promise solution in fewer iterations than STEM. It also considers bimatrix games to allow
multiple DMs in the solution process.
Feasible Direction Methods
This approach is a direct extension of the feasible direction methods developed for solving
single objective nonlinear programming problems. It starts with a feasible solution and
iteratively performs two major steps: (1) to ﬁnd a “usable direction” (along which the
preference of the DM appears to increase), called the direction ﬁnding step; and (2) to
determine the step-size from the current solution along the usable direction, called the line
search. In the direction ﬁnding step, the DM provides information about his preferences
in the neighborhood of the current solution by specifying values of local trade-oﬀs among
criteria. This, when translated into an approximation of the gradient of the preference
function at that point, guides the selection of a new solution with higher preference.
The pioneering method in this category is the GDF procedure by Geoﬀrion et al. [33].
They modiﬁed the Frank–Wolfe method for solving single objective convex programs and
applied the GDF procedure to an academic planning problem. The major drawback of the
GDF procedure is the diﬃculty in providing precise local trade-oﬀratios and the necessity
of the line search.
Sadagopan and Ravindran [34] have extended the GDF procedure to nonlinear constraints
using the generalized reduced gradient (GRG) method for solving single objective problems.
They use “reduced gradients” to overcome the nonlinearity of the constraints. They also
employ interval trade-oﬀestimates rather then precise local trade-oﬀs and eliminate the
line search.
Criterion Weight Space Methods
When the decision space is a compact convex set and the objective functions to be maxi-
mized are concave, an eﬃcient solution can be obtained by solving a single objective prob-
lem in which the objectives are combined using weights. (Recall the Pλ problem discussed
in Section 5.6.2.) The domain of the weights is deﬁned as the criterion weight space,and
this approach reaches the best compromise solution by either searching for the optimal
weight space or successively reducing the space. This approach has been popular in practice,
but most methods are applicable to only multiple objective linear programming (MOLP)
problems.
The Zionts–Wallenius (ZW) method [35] is a typical criterion weight space method. The
method optimizes an LP problem for a given arbitrary set of weights on the objectives. Then
a set of trade-oﬀs (reduced costs of the objectives), which are associated with the optimal
solution of the current LP problem, is presented to the DM. The DM’s responses include a
preference/nonpreference for a trade-oﬀor indiﬀerence. The DM’s responses are then used
in reducing the criterion weight space by additional constraints on weights and generating a
new eﬃcient solution. If the DM is satisﬁed with the new solution (i.e., he does not want any
trade-oﬀ), the procedure terminates. Malakooti and Ravindran [36] improve the ZW method
by employing the paired comparisons of alternatives and strength of preference. They claim
some advantages over the ZW method through a computational study. They also propose
an interesting approach to handle the problem of the DM’s inconsistent responses.
Steuer [37] proposes a procedure with interval criterion weights. Rather than obtain
a single eﬃcient extreme point with ﬁxed weights, a cluster of eﬃcient extreme points is

5-32
Operations Research Methodologies
generated. By widening or narrowing the intervals corresponding to each objective, diﬀerent
subsets of eﬃcient extreme points can be generated. At each iteration, after interacting with
the DM, the intervals are successively reduced until the best compromise solution is reached.
Trade-OﬀCutting Plane Methods
This approach can be viewed as another variation of the feasible direction method. The
methods of this class are unique in the way they isolate the best compromise solution. They
iteratively reduce the objective space (equivalent to the reduction of the feasible space)
by cutting planes and the line search is eliminated. The methods are applicable to general
nonlinear problems, but they require precise local trade-oﬀratios. This approach transforms
the MCMP problem through one-to-one functional mapping from decision space to objective
space.
Musselman and Talavage [38] use local trade-oﬀs to develop a cutting plane that pro-
gressively eliminates a portion of the objective space. They use the “method of centers” to
locate the new solution point near the center of the remaining objective space, thus elim-
inating the line search requirement. The method was applied to storm drainage problem.
Shin and Ravindran [39] combined the direction ﬁnding step of Sadagopan and Ravindran
[34] and the cutting plane concept. The line search is optional in the method and a compu-
tational study using GRG2 was performed. Sadagopan and Ravindran [40] present a paired
comparison method (PCM) and a comparative trade-oﬀmethod (CTM) for solving the
bicriterion problems. These methods also eliminate a certain portion of the objective space
at each iteration via interactions with the DM. The PCM was applied successfully to a
cardiovascular disease control problem in the U.S. Air Force [41].
Lagrange Multiplier Methods
The interactive methods that use Lagrange multipliers belong in this class. The interactive
surrogate worth trade-oﬀmethod (ISWTM) of Chankong and Haims [42] is a Lagrange
multiplier method. It maximizes one objective subject to a varying set of bounded objec-
tives and uses the resulting Lagrange multipliers of the constraints for interacting with the
DM. The method ﬁrst generates eﬃcient solutions that form the trade-oﬀfunctions in the
objective surface derived from the generalized Lagrangian problem, and then searches for
a preferred solution by interacting with the DM to generate shadow prices of all bounded
objectives. In the generation of a shadow price, the DM is asked interactively to assess the
indiﬀerence bounds to deﬁne a surrogate worth function. It must be pointed out that the
amount of work to determine a shadow price (surrogate worth function) might be cumber-
some, considering that it needs to be repeated at each step and for each objective.
Visual Interactive Methods
Most of the aforementioned approaches assume that the unknown DM’s preference function
remains unchanged during the interactive process. In an eﬀort to relax the assumptions con-
cerning the DM’s behavior, Korhonen and Laakso [43] presented a graphic-aided interactive
approach. Theoretically, this method can be seen as an extension of the GDF procedure in
that the line search used is analogous to that on the GDF procedure. However, it deter-
mines new search directions using reference directions suggested by Wierzbicki [44], which
reﬂect the DM’s preference. At each iteration, the method generates a picture representing
a subset of the eﬃcient frontier for interacting with the DM. By modifying the method,
“Pareto race” was developed by Korhonen and Wallenius [45] and it has been applied in

Multiple Criteria Decision Making
5-33
pricing alcoholic beverages [46]. Pareto Race enables the DM to control the entire eﬃcient
frontier through the interactive process. With the rapid advances in personal computer
technology, this type of approach is becoming popular.
Branch-and-Bound Method
The branch-and-bound method developed by Marcotte and Soland [47] divides the objective
space into subsets. Each subset is a branch from the original objective space. They further
branch into even smaller subsets if a branch is promising. At each subset they determine an
ideal solution and use this ideal solution to form an upper bound for each subset. Note that
the ideal solution of a subset dominates all the eﬃcient points of that subset. Each subset
forms a node and at each node solutions are compared against the incumbent solution by
interacting with the DM. A node is fathomed if the ideal solution pertaining to that subset
is not preferred to the incumbent solution by the DM. This method is also applicable
to the discrete case and has a number of good properties, such as vector comparisons,
nondependence on the preference function, and termination at an eﬃcient solution.
Raman [48] developed a branch-and-bound interactive method for solving bicriteria linear
integer programs. The author used the Tchebycheﬀ’s norm for generating eﬃcient solutions.
Eswaran et al. [49] implemented a weighted Tchebycheﬀ’s norm to develop an interactive
method for solving bicriteria nonlinear integer programs with applications to quality control
problems in acceptance sampling.
5.9.2
Inconsistency of the DM
Interactive methods require the DM to respond to a series of questions at each iteration.
Therefore the consistency of the DM is one of the most important factors in the success
of the methods. Because DMs are very subjective, diﬀerent starting solutions may lead
to diﬀerent best compromise solutions. Moreover, diﬀerent methods use diﬀerent types of
questions or interaction styles and could guide the DM to diﬀerent solutions. Nearly all
interactive methods require consistent responses from the DM to be successful. Thus, the
assumption of the DM’s consistency usually draws severe criticism and some researchers
underrate the abilities of the interactive methods due to this drawback.
There are generally two ways to reduce the DM’s inconsistency: (1) testing consistency
during the procedure; and (2) minimizing the DM’s cognitive burden. Testing whether the
DM’s responses are consistent with those to the previous questions has been used in trade-oﬀ
assessment schemes. Also, tests for recognizing the DM’s inconsistency have been developed
by Malakooti and Ravindran [36]. The DM’s inconsistency can be reduced by presenting
easy questions to the DM. In this way the DM has little confusion with questions and less
chance for making mistakes. Reducing the DM’s cognitive burden is one of the motivations
for new algorithmic developments in this area.
5.9.3
Computational Studies
There are only a few computational studies on interactive methods. Wallenius [50] describes
an evaluation of several MCMP methods, such as STEM and the GDF procedure, and
reports that none of the methods has been highly successful in practice. Klein et al. [51]
claim that interactive procedures are easier to use and achieve more satisfactory solutions
when compared to utility measurement methods. Their conclusions are based on simulated
study of a quality control problem with several students serving as DMs.

5-34
Operations Research Methodologies
Most methods are generally presented without computational studies, probably due to
the diﬃculty of interacting with real DMs. Even if simulated preference functions are used,
more computational study is needed to assess the usefulness of these methods. Also, the
preferences of practitioners with regard to interaction styles and methodological approaches
must be disclosed by comparative studies. This will help managers apply the interactive
methods to real problems.
5.10
MCDM Applications
One of the most successful applications of multi-criteria decision making has been in the
area of portfolio selection, an important problem faced by individual investors and ﬁnancial
analysts in investment companies. A portfolio speciﬁes the amount invested in diﬀerent
securities that may include bonds, common stocks, mutual funds, bank CDs, Treasury
notes, and others. Much of the earlier investment decisions were made by seat-of-the-pants
approaches. Markowitz [52] in the 1950s pioneered the development of the modern portfolio
theory, which uses bi-criteria mathematical programming models to analyze the portfolio
selection problem. By quantifying the trade-oﬀs between risks and returns, he showed how
an investor can diversity portfolios such that the portfolio risk can be reduced without
sacriﬁcing returns. Based on Markowitz’s work, Sharpe [53] introduced the concept of the
market risk, and developed a bi-criteria linear programming model for portfolio analysis.
For their pioneering work in modern portfolio theory, both Markowitz and Sharpe shared
the 1990 Nobel Prize in Economics. The Nobel award was the catalyst for the rapid use of
the modern portfolio theory by Wall Street ﬁrms in the 1990s.
Among the MCDM models and methods, the goal programming models have seen the
most applications in industry and government. Chapter 4 of the textbook by Schniederjan
[25] contains an extensive bibliography (666 citations) on goal programming applications
categorized by areas—accounting, agriculture, economics, engineering, ﬁnance, government,
international, management, and marketing. Zanakis and Gupta [54] also have a categorized
bibliographic survey of goal programming applications.
Given below is a partial list of MCDM applications in practice:
• Academic planning [30,55–57]
• Accounting [58]
• Environment [59–61]
• Forest management [62–64]
• Health planning [17,40,41,65–67]
• Investment planning [52,53,68–70]
• Manpower planning [71–73]
• Metal cutting [74–76]
• Production planning and scheduling [77–82]
• Quality control [49,51,83–87]
• Reliability [88]
• Supply chain management [89–97]
• Transportation [98–101]
• Waste disposal [102]
• Water resources [38,103–107]

Multiple Criteria Decision Making
5-35
5.11
MCDM Software
One of the problems in applying MCDM methods in practice is the lack of commercially
available software implementing these methods. There is some research software available.
Two good resources for these are:
http://www.terry.uga.edu/mcdm/
http://www.sal.hut.ﬁ/
The ﬁrst is the Web page of the International Society on multiple criteria decision making.
It has links to MCDM software and bibliography. A number of this software is available
free for research and teaching use. The second link is to the research group at Helsinki
University of Technology. It has links to some free downloads software, again for research
and instructional use.
Following are some links to commercially available MCDM software:
Method(s)
Software
Utility/ValueTheory
LogicalDecisions
http://www.logicaldecisions.com/
AHP
ExpertChoice
http://www.expertchoice.com/software/
AHP/SMART
CriteriumDecisionPlus
http://www.hearne.co.uk/products/decisionplus/
GoalProgramming
LindoSystems
http://www.lindo.com
PROMETHEE
Decision Lab
http://www.visualdecision.com/dlab.htm
ELECTRE
LAMSADE Web page
http://l1.lamsade.dauphine.fr/english/software.html/
5.12
Further Readings
While a need for decision making in the context of multiple conﬂicting criteria has existed
for a very long time, the roots of the discipline of multiple criteria decision making (MCDM)
go back about half a century only. Development of the discipline has taken place along two
distinct tracks. One deals in problems with a relatively small number of alternatives, often
in an environment of uncertainty. A groundbreaking book that has inﬂuenced research and
application in this area is by Keeney and Raiﬀa [108]. It is called Decision Analysis or
Multi Attribute Decision Making. Chapter 6 of this handbook discusses decision analysis
in detail. The other track deals with problems where the intent is to determine the “best”
alternative utilizing mathematical relationships among various decision variables, called
multiple criteria mathematical programming (MCMP) or commonly MCDM problems.
MCDM Text Books
Two of the earliest books describing methods for MCDM problems are Charnes and Cooper
[109] and Ijiri [58]. Other books of signiﬁcance dealing with the exposition of theories,
methods, and applications in MCDM include Lee [110], Cochran and Zeleny [111], Hwang
and Masud [1], Hwang and Yoon [2], Zeleny [25], Hwang and Lin [112], Steuer [3], Saaty [8],

5-36
Operations Research Methodologies
Kirkwood [113], Ignizio [114], Gal et al. [115], Miettinen [116], Olson [117], Vincke [118],
Figueira et al. [119], Ehrgott and Gandibleux [120], and Ehrgott [121].
MCDM Journal Articles
Recent MCDM survey articles of interest include Geldermann and Zhang [122], Kaliszewski
[123], Leskinen et al. [124], Osman et al. [125], Tamiz et al., [126], Vaidya and Kumar [127],
Alves and Climaco [128], Steuer and Na [129], and Zapounidis and Doumpos [130].
MCDM Internet Links
• Homepage of the International Society on Multiple Criteria Decision Making
http://www.terry.uga.edu/mcdm
• EURO Working Group on Multi-criteria Decision Aids
http://www.inescc.pt/∼ewgmcda/index.html
• EMOO web page by Carlos Coello
http://www.lania.mx/∼ccoello/EMOO
• Decision Lab 2000
http://www.visualdecision.com
• Kaisa Miettinen’s website
http://www.mit.jyu.ﬁ/miettine/lista.html
• Decisionarium
http://www.decisionarium.hut.ﬁ
• Vincent Mousseau’s MCDA database
http://www.lamsade.dauphine.fr/mcda/biblio/
References
1. Hwang, C.L. and Masud, A., Multiple Objective Decision Making—Methods and Applica-
tions, Springer-Verlag, New York, 1979.
2. Hwang, C.L. and Yoon, K.S., Multiple Attribute Decision Making—Methods and Applica-
tions, Springer-Verlag, New York, 1981.
3. Steuer, R.E., Multiple Criteria Optimization: Theory, Computation and Application,
John Wiley, New York, 1986.
4. Saaty, T.L., A scaling method for priorities in hierarchical structures, J Math Psychol, 15,
234–281, 1977.
5. Harker, P.T., The art and science of decision making: The analytic hierarchy process, in
The Analytic Hierarchy Process: Applications and Studies, B.L. Golden, E.W. Wasil and
P.T. Harker (Eds.), Springer-Verlag, New York, 1987.
6. Roy, B., The outranking approach and the foundations of Electre methods, in Readings in
Multiple Criteria Decision Aid, C.A. Bana e Costa (Ed.), Springer-Verlag, New York, 1990.
7. Roy, B., Multicriteria Methodology for Decision Making, Kluwer Academic Publishers,
Norwell, MA, 1996.
8. Rogers, M., Bruen, M., and Maystre, L.Y., Electre and Decision Support, Kluwer
Publishers, Norwell, MA, 2000.
9. Saaty, T.L., The Analytic Hierarchy Process, 2nd ed., RWS Publications, Pittsburgh,
PA, 1990.
10. Forman, E.H. and Gass, S., AHP—An Exposition, Oper Res, 49, 469–486, 2001.
11. Brans, J.P. and Mareschal, B., The PROMTHEE methods for MCDM, in Readings in
Multiple Criteria Decision Aid, C.A. Bana e Costa (Ed.), Springer-Verlag, New York, 1990.

Multiple Criteria Decision Making
5-37
12. Geoﬀrion, A., Proper eﬃciency and theory of vector maximum, J Math Anal Appl, 22,
618–630, 1968.
13. Ravindran, A., Ragsdell, K.M., and Reklaitis, G.V., Engineering Optimization: Methods
and Applications, 2nd ed., John Wiley, New York, 2006 (Chapter 11).
14. Arthur, J.L. and Ravindran, A., An eﬃcient goal programming algorithm using constraint
partitioning and variable elimination, Manage Sci, 24(8), 867–868, 1978.
15. Arthur, J.L. and Ravindran, A., PAGP—Partitioning algorithm for (linear) goal program-
ming problems, ACM T Math Software, 6, 378–386, 1980.
16. Arthur, J.L. and Ravindran, A., A branch and bound algorithm with constraint partitioning
for integer goal programs, Eur J Oper Res, 4, 421–425, 1980.
17. Arthur, J.L. and Ravindran, A., A multiple objective nurse scheduling model, IIE Trans,
13, 55–60, 1981.
18. Saber, H.M. and Ravindran, A., A partitioning gradient based (PGB) algorithm for solving
nonlinear goal programming problem, Comput Oper Res, 23, 141–152, 1996.
19. Kuriger, G. and Ravindran, A., Intelligent search methods for nonlinear goal programs,
Inform Syst OR, 43, 79–92, 2005.
20. Ignizio, J.M. and Cavalier, T.M., Linear Programming, Prentice Hall, Englewood Cliﬀs,
NJ, 1994 (Chapter 13).
21. Tiwari, R.N., Dharmar S., and Rao J.R., Priority structure in fuzzy goal programming,
Fuzzy Set Syst, 19, 251–259, 1986.
22. Tiwari, R.N., Dharmar, S., and Rao, J.R., Fuzzy goal programming—An additive model,
Fuzzy Set Syst, 24, 27–34, 1987.
23. Mohammed, R.H., The relationship between goal programming and fuzzy programming,
Fuzzy Set Syst, 89, 215–222, 1997.
24. Hu, C.F., Teng, C.J., and Li, S.Y., A fuzzy goal programming approach to multi objective
optimization problem with priorities, Euro J Oper Res, 171, 1–15, 2006.
25. Schniederjans, M., Goal Programming: Methodology and Applications, Kluwer Academic,
Dordrecht, 1995.
26. Zeleny, M., Multiple Criteria Decision Making, McGraw Hill, New York, 1982.
27. Tabucannon,
M.,
Multiple
Criteria
Decision
Making
in
Industry,
Elsevier,
Amsterdam, 1988.
28. Yu, P.-L., Multiple Criteria Decision Making, Plenum Press, New York, 1985.
29. Shin,
W.S.
and
Ravindran,
A.,
Interactive
multi
objective
optimization:
survey
I—continuous case, Comput Oper Res, 18, 97–114, 1991.
30. Benayoun, R., Tergny, J., and Keuneman, D., Mathematical programming with multi-
objective functions: a solution by P.O.P. (progressive orientation procedure), Metric IX,
279–299, 1965.
31. Benayoun, R., de Montgolﬁer, J., Tergny, J., and Larichev, O., Linear programming with
multiple objective functions: step method (STEM), Math Program, 1, 366–375, 1971.
32. Fichefet, J., GPSTEM: an interactive multiobjective optimization method, Prog Oper Res,
1, 317–332, 1980.
33. Geoﬀrion, A.M., Dyer, J.S., and Fienberg, A., An interactive approach for multi-criterion
optimization with an application to the operation of an academic department, Manage Sci,
19, 357–368, 1972.
34. Sadagopan, S. and Ravindran, A., Interactive algorithms for multiple criteria nonlinear
programming problems, Euro J Oper Res, 25, 247–257, 1986.
35. Zionts, S. and Wallenius, J., An interactive programming method for solving the multiple
criteria problem. Manage Sci, 22, 652–663, 1976.
36. Malakooti, B. and Ravindran, A., An interactive paired comparison simplex method for
MOLP problems, Ann Oper Res, 5, 575–597, 1985/86.

5-38
Operations Research Methodologies
37. Steuer, R.E., Multiple objective linear programming with interval criterion weights, Manage
Sci, 23, 305–316, 1976.
38. Musselman, K. and Talavage, J., A tradeoﬀcut approach to multiple objective optimiza-
tions, Oper Res, 28, 1424–1435, 1980.
39. Shin, W.S. and Ravindran, A., An interactive method for multiple objective mathematical
programming (MOMP) problems, J Optimiz Theory App, 68, 539–569, 1991.
40. Sadagopan, S. and Ravindran, A., Interactive solution of bicriteria mathematical program-
ming, Nav Res Log Qtly, 29, 443–459, 1982.
41. Ravindran, A. and Sadagopan, S., Decision making under multiple criteria—A case study,
IEEE T Eng Manage, EM-34, 127–177, 1987.
42. Chankong, V. and Haimes, Y.Y., An interactive surrogate worth tradeoﬀ(ISWT) method
for multiple objective decision making, in Multiple Criteria Problem Solving, S. Zionts
(Ed.), Springer, New York, 1978.
43. Korhonen, P. and Laakso, L., A visual interactive method for solving the multiple criteria
problem, Euro J Oper Res, 24, 277–278, 1986.
44. Wierzbicki,
A.P.,
The
use
of
reference
objectives
in
multiobjective
optimization,
in
Multiple
Criteria
Decision
Making,
G.
Fandel
and
T.
Gal
(Eds.),
Springer,
New York, 1980.
45. Korhonen, P. and Wallenius, J., A Pareto race, Nav Res Log Qtly, 35, 615–623, 1988.
46. Korhonen, P. and Soismaa, M., A multiple criteria model for pricing alcoholic beverages,
Euro J Oper Res, 37, 165–175, 1988.
47. Marcotte, O. and Soland, R.M., An interactive branch-and-bound algorithm for multiple
criteria optimization, Manage Sci, 32, 61–75, 1985.
48. Raman, T., A branch and bound method using TchebycheﬀNorm for solving bi-criteria
integer problems, M.S. Thesis, University of Oklahoma, Norman, 1997.
49. Eswaran, P.K., Ravindran, A., and Moskowitz, H., Interactive decision making involving
nonlinear integer bicriterion problems, J Optimiz Theory Appl, 63, 261–279, 1989.
50. Wallenius, J., Comparative evaluation of some interactive approaches to multicriterion
optimization. Manage Sci, 21, 1387–1396, 1976.
51. Klein, G., Moskowitz, H., and Ravindran, A., Comparative evaluation of prior versus pro-
gressive articulation of preference in bicriterion optimization, Nav Res Log Qtly, 33, 309–
323, 1986.
52. Markowitz, H., Portfolio Selection: Eﬃcient Diversiﬁcation of Investments, Wiley,
New York, 1959.
53. Sharpe, W.F., A simpliﬁed model for portfolio analysis, Manage Sci, 9, 277–293, 1963.
54. Zanakis, S.H. and Gupta, S.K., A categorized bibliographic survey of goal programming,
Omega: Int J Manage Sci, 13, 211–222, 1995.
55. Beilby, M.H. and T.H. Mott, Jr., Academic library acquisitions allocation based on multiple
collection development goals, Comput Oper Res, 10, 335–344, 1983.
56. Lee, S.M. and Clayton, E.R., A goal programming model for academic resource allocation,
Manage Sci, 17, 395–408, 1972.
57. Dinkelbach, W. and Isermann, H., Resource allocation of an academic department in the
presence of multiple criteria-some experience with a modiﬁed STEM method, Comput Oper
Res, 7, 99–106, 1980.
58. Ijiri, Y., Management Goals and Accounting for Control, Rand-McNally, Chicago, 1965.
59. Charnes, A., Cooper, W.W., Harrald, J., Karwan, K.R., and Wallace, W.A., A goal interval
programming model for resource allocation on a marine environmental protection program,
J Environ Econ Manage, 3, 347–362, 1976.
60. Sakawa, M., Interactive multiobjective decision making for large-scale systems and its appli-
cation to environmental systems, IEEE T Syst Man Cyb, 10, 796–806, 1980.

Multiple Criteria Decision Making
5-39
61. Stewart, T.J., Experience with prototype multicriteria decision support systems for pelagic
ﬁsh quota determination, Nav Res Log Qtly, 35, 719–731, 1988.
62. Schuler, A.T., Webster, H.H., and Meadows, J.C., Goal programming in forest manage-
ment, J Forestry, 75, 320–324, 1977.
63. Steuer, R.E. and Schuler, A.T., An interactive multiple objective linear programming
approach to a problem in forest management, Oper Res, 25, 254–269, 1978.
64. Harrison, T.P. and Rosenthal, R.E., An implicit/explicit approach to multiobjective opti-
mization with an application to forest management planning, Decision Sci, 19, 190–
210, 1988.
65. Musa, A.A. and Saxena, U., Scheduling nurses using goal programming techniques, IIE
Transactions, 16, 216–221, 1984.
66. Trivedi, V.M., A mixed-integer goal programming model for nursing service budgeting,
Oper Res, 29, 1019–1034, 1981.
67. Beal, K., Multicriteria optimization applied to radiation therapy planning, M.S. Thesis,
Pennsylvania State University, University Park, 2003.
68. Lee, S.M. and Lerro, A.J., Optimizing the portfolio selection for mutual funds, J Finance,
28, 1087–1101, 1973.
69. Nichols, T. and Ravindran, A., Asset allocation using goal programming, Proceedings of
the 34th International Conference on Computers and IE, San Francisco, November 2004.
70. Erlandson, Sarah, Asset allocation models for portfolio optimization, M.S. Thesis, Penn-
sylvania State University, University Park, 2002.
71. Charnes, A., Cooper, W.W., and Niehaus, R.J., Dynamic multi-attribute models for mixed
manpower systems, Nav Res Log Qtly, 22, 205–220, 1975.
72. Zanakis, S.H. and Maret, M.W., A Markovian goal programming approach to aggregate
manpower planning, J Oper Res Soc, 32, 55–63, 1981.
73. Henry, T.M. and Ravindran, A., A goal programming application for army oﬃcer accession
planning, Inform Syst OR J, 43, 111–120, 2005.
74. Philipson, R.H. and Ravindran, A., Applications of mathematical programming to metal
cutting, Math Prog Stud, 19, 116–134, 1979.
75. Philipson, R.H. and Ravindran, A., Application of goals programming to machinability
data optimization, J Mech Design: Trans ASME, 100, 286–291, 1978.
76. Malakooti, B. and Deviprasad, J., An interactive multiple criteria approach for parameter
selection in metal cutting, Oper Res, 37, 805–818, 1989.
77. Arthur, J.L. and Lawrence, K.D., Multiple goal production and logistics planning in a
chemical and pharmaceutical company, Comput Oper Res, 9, 127–237, 1982.
78. Deckro, R.J., Herbert, J.E., and Winkofsky, E.P., Multiple criteria job-shop scheduling,
Comput Oper Res, 9, 279–285, 1984.
79. Lawrence, K.D. and Burbridge, J.J., A multiple goal linear programming model for coor-
dinated and logistic planning, Int J Prod Res, 14, 215–222, 1976.
80. Lee, S.M. and Moore, J.L., A practical approach to production scheduling, Prod Inventory
Manage, 15, 79–92, 1974.
81. Lee, S.M., Clayton, E.R., and Taylor, B.W., A goal programming approach to multi-period
production line scheduling, Comput Oper Res, 5, 205–211, 1978.
82. Lashine, S.H., Foote, B.L., and Ravindran, A., A nonlinear mixed integer goal programming
model for the two-machine closed ﬂow shop, Euro J Oper Res, 55, 57–70, 1991.
83. Moskowitz, H., Ravindran, A., Klein, G., and Eswaran, P.K., A bicriteria model for accep-
tance sampling in quality control, TIMS Special Issue on Optim Stat, 19, 305–322, 1982.
84. Moskowitz, H., Plante, R., Tang, K., and Ravindran, A., Multiattribute Bayesian accep-
tance sampling plans for screening and scrapping rejected lots, IIE Transactions, 16, 185–
192, 1984.

5-40
Operations Research Methodologies
85. Sengupta, S., A goal programming approach to a type of quality control problem, J Oper
Res Soc, 32, 207–212, 1981.
86. Ravindran, A., Shin, W., Arthur, J., and Moskowitz, H., Nonlinear integer goal program-
ming models for acceptance sampling, Comput Oper Res, 13, 611–622, 1986.
87. Klein, G., Moskowitz, H., and Ravindran, A., Interactive multiobjective optimization under
uncertainty, Manage Sci, 36, 58–75, 1990.
88. Mohamed, A., Ravindran, A., and Leemis, L., An interactive availability allocation algo-
rithm, Int J Oper Quant Manage, 8, 1–19, 2002.
89. Thirumalai, R., Multi criteria multi decision maker inventory models for serial supply
chains, PhD Thesis, Pennsylvania State University, University Park, 2001.
90. Gaur, S. and Ravindran, A., A bi-criteria model for inventory aggregation under risk pool-
ing, Comput Ind Eng, 51, 482–501, 2006.
91. DiFilippo, A.M., Multi criteria supply chain inventory models with transportation costs,
M.S. Thesis, Pennsylvania State University, University Park, 2003.
92. Attai, T.D., A multi objective approach to global supply chain design, M.S. Thesis, Penn-
sylvania State University, University Park, 2003.
93. Wadhwa, V. and Ravindran, A., A multi objective model for supplier selection, Comput
Oper Res, 34, 3725–3737, 2007.
94. Mendoza, A., Santiago, E., and Ravindran, A., A three phase multicriteria method to
the supplier selection problem, Working Paper, Industrial Engineering, Pennsylvania State
University, University Park, August 2005.
95. Lo-Ping, C., Fuzzy goal programming approach to supplier selection, M.S. Thesis, Penn-
sylvania State University, University Park, 2006.
96. Natarajan, A., Multi criteria supply chain inventory models with transportation costs,
Ph.D. Thesis, Pennsylvania State University, University Park, 2006.
97. Yang, T., Multi objective optimization models for management supply risks in supply
chains, Ph.D. Thesis, Pennsylvania State University, University Park, 2006.
98. Cook, W.D., Goal programming and ﬁnancial planning models for highway rehabilitation,
J Oper Res Soc, 35, 217–224, 1984.
99. Lee, S.M. and Moore, L.J., Multi-criteria school busing models, Manage Sci, 23, 703–
715, 1977.
100. Moore, L.J, Taylor, B.W., and Lee, S.M., Analysis of a transshipment problem with mul-
tiple conﬂicting objectives, Comput Oper Res, 5, 39–46, 1978.
101. Sinha, K.C., Muthusubramanyam, M., and Ravindran, A., Optimization approach for allo-
cation of funds for maintenance and preservation of the existing highway system, Transport
Res Rec, 826, 5–8, 1981.
102. Mogharabi, S. and Ravindran, A., Liquid waste injection planning via goal programming,
Comput Ind Eng, 22, 423–433, 1992.
103. Haimes, Y.Y., Hall, W.A., and Freedman, H.T., Multiobjective Optimization in Water
Resources Systems, Elsevier, Amsterdam, 1975.
104. Johnson, L.E. and Loucks, D.P., Interactive multiobjective planning using computer graph-
ics, Comput Oper Res, 7, 89–97, 1980.
105. Haimes, Y.Y., Loparo, K.A., Olenik, S.C., and Nanda, S.K., Multiobjective statistical
method for interior drainage systems, Water Resour Res 16, 465–475, 1980.
106. Loganathan, G.V. and Sherali, H.D., A convergent interactive cutting-plane algorithm for
multiobjective optimization, Oper Res, 35, 365–377, 1987.
107. Monarchi, D.E., Kisiel, C.C., and Duckstein, L., Interactive multiobjective programming
in water resources: A case study, Water Resour Res, 9, 837–850, 1973.
108. Keeny, R.L. and Raiﬀa, H., Decision with Multiple Objectives: Preferences and Value Trade
Oﬀs, Wiley, New York, 1976.

Multiple Criteria Decision Making
5-41
109. Charnes, A. and Cooper, W.W., Management Models and Industrial Applications of Linear
Programming, Wiley, New York, 1961.
110. Lee, S.M., Goal Programming for Decision Analysis, Auerbach Publishers, Philadelphia,
PA, 1972.
111. Cochran, J.L. and Zeleny, M. (Eds.), Multiple Criteria Decision Making, University of
South Carolina Press, Columbia, SC, 1973.
112. Hwang, C.L. and Lin, M.J., Group Decision Making Under Multiple Criteria, Springer-
Verlag, New York, 1987.
113. Kirkwood, C.W., Strategic Decision Making, Wadsworth Publishing Co., Belmont,
CA, 1997.
114. Ignizio, J.P., Goal Programming and Its Extensions, Heath, Lexington, MA, 1976.
115. Gal, T., Stewart T.J., and Hanne, T. (Eds.), Multiple Criteria Decision Making: Advances
in MCDM Models, Kluwer Academic Publishers, Norwell, MA, 1999.
116. Miettinen, K.M., Nonlinear Multiobjective Optimization, Kluwer Academic Publishers,
Norwell, MA, 1999.
117. Olson, D.L., Decision Aids for Selection Problems, Springer-Verlag, New York, 1996.
118. Vincke, P., Multicriteria Decision Aid, Wiley, Chichester, England, 1992.
119. Figueira, J.S., Greco, S., and Ehrgott, M. (Eds.), Multiple Criteria Decision Analysis: State
of the Art Surveys, Springer, New York, 2005.
120. Ehrgott, M. and Gandibleux, X., Multiple Criteria Optimization: State of the Art Annotated
Bibliographic Surveys, Kluwer Academic, Dordrecht, 2002.
121. Ehrgott, M., Multicriteria Optimization, Springer, New York, 2005.
122. Geldermann, J. and Zhang, K., Software review: Decision lab 2000, J Multi-Criteria Deci-
sion Anal, 10, 317–323, 2001.
123. Kaliszewski, I., Out of the mist towards decision-maker-friendly multiple criteria decision
making, Euro J Oper Res, 158, 293–307, 2004.
124. Leskinen, P., Kangas, A.S., and Kangas, J., Rank-based modeling of preferences in multi-
criteria decision making, Euro J Oper Res, 158, 721–733, 2004.
125. Osman, M., Kansan, S., Abou-El-Enien, T., and Mohamed, S., Multiple criteria decision
making theory applications and software: a literature review, Adv Model Anal B, 48, 1–35,
2005.
126. Tamiz, M., Jones, D., and Romero, C., Goal programming for decision making: An overview
of the current state-of-the-art, Euro J Oper Res, 111, 569–581, 1998.
127. Vaidya, O.S. and Kumar, S., Analytic hierarchy process: an overview of applications, Euro
J Oper Res, 169, 1–29, 2006.
128. Alves, M.J. and Climaco, J., A review of interactive methods for multiobjective integer
and mixed-integer programming, Euro J Oper Res, 180, 99–115, 2007.
129. Steuer, R.E. and Na, P., Multiple criteria decision making combined with ﬁnance: A cate-
gorized bibliographic study, Euro J Oper Res, 150, 496–515, 2003.
130. Zopounidis, C. and Doumpos, M., Multi-criteria decision aid in ﬁnancial decision making:
methodologies and literature review, J Multi-Criteria Decision Anal, 11, 167–186, 2002.


6
Decision Analysis
Cerry M. Klein
University of Missouri–Columbia
6.1
Introduction............................................
6-1
6.2
Terminology for Decision Analysis ..................
6-2
Terminology • An Investment Example
6.3
Decision Making under Risk .........................
6-3
Maximum Likelihood • Expected Value under
Uncertainty • Expected Opportunity Loss or Expected
Regret • Expected Value of Perfect Information •
Decision Trees • Posterior Probabilities • Utility
Functions
6.4
Decision Making under Uncertainty ................ 6-17
Snack Production Example • Maximin (Minimax)
Criterion • Maximax (Minimin) Criterion • Hurwicz
Criterion • Laplace Criterion or Expected Value •
Minimax Regret (Savage Regret)
6.5
Practical Decision Analysis .......................... 6-21
Problem Deﬁnition • Objectives • Alternatives •
Consequences • Tradeoﬀs • Uncertainty • Risk
Tolerance • Linked Decisions
6.6
Conclusions ............................................ 6-28
6.7
Resources ............................................... 6-29
References .................................................... 6-30
6.1
Introduction
Everyone engages in the process of making decisions on a daily basis. Some of these decisions
are quite easy to make and almost automatic. Other decisions can be very diﬃcult to
make and almost debilitating. Likewise, the information needed to make a good decision
varies greatly. Some decisions require a great deal of information whereas others much less.
Sometimes there is not much if any information available and hence the decision becomes
intuitive, if not just a guess. Many, if not most, people make decisions without ever truly
analyzing the situation and the alternatives that exist. There is a subjective and intrinsic
aspect to all decision making, but there are also systematic ways to think about problems
to help make decisions easier. The purpose of decision analysis is to develop techniques to
aid the process of decision making, not replace the decision maker.
Decision analysis can thus be deﬁned as the process and methodology of identifying,
modeling, assessing, and determining an appropriate course of action for a given decision
problem. This process often involves a wide array of tools and the basic approach is generally
to break the problem down into manageable and understandable parts that the decision
maker can comprehend and handle. It is then necessary to take these smaller elements
and reconstitute them into a proper solution for the larger original problem. Through this
6-1

6-2
Operations Research Methodologies
process, the decision maker should also gain insight into the problem and the implementation
of the solution. This should enable the decision maker to analyze complex situations and
choose a course of action consistent with their basic values and knowledge [1].
When analyzing the decision making process, the context or environment of the decision
to be made allows for a categorization of the decisions based on the nature of the problem
or the nature of the data or both. There are two broad categories of decision problems:
decision making under certainty and decision making under uncertainty. Some break decision
making under uncertainty down further in terms of whether the problem can be modeled by
probability distributions (risk) or not (uncertainty). There is also a break down of decision
type based on whether one is choosing an alternative from a list of alternatives (attribute
based) or allocating resources (objective based), or negotiating an agreement [2]. Decision
analysis is almost always in the context of choosing one alternative from a set of alternatives
and is the approach taken here.
Decision making under certainty means that the data are known deterministically or at
least at an estimated level the decision maker is comfortable with in terms of variation.
Likewise, the decision alternatives can be well deﬁned and modeled. The techniques used
for these problem types are many and include much of what is included in this handbook
such as linear programming, nonlinear programming, integer programming, multiobjective
optimization, goal programming, analytic hierarchy process, and others.
Decision making under risk means that there is uncertainty in the data, but this uncer-
tainty can be modeled probabilistically. Note that there are some that do not use this
designation as they believe all probability is subjective; hence all decisions not known with
certainty are uncertain. However, we will use the common convention of referring to prob-
abilistic models as decision making under risk.
Decision making under uncertainty means the probability model for the data is unknown
or cannot be modeled probabilistically and hence the data are imprecise or vague.
Decisions made under risk and uncertainty are the focus of this chapter.
6.2
Terminology for Decision Analysis
There are many excellent works on decision analysis. This is a well-studied area with con-
tributions in a wide variety of ﬁelds including operations research, operations management,
psychology, public policy, leadership, and so on. With the wide variety of backgrounds
and works, it is necessary to have a common language. To that end and to help structure
the discussion the following terminology, adapted from Ravindran, Phillips, and Solberg
[1, chapter 5], is used.
6.2.1
Terminology
Decision Maker The entity responsible for making the decision. This may be a single
person, a committee, company, and the like. It is viewed here as a single entity,
not a group.
Alternatives A ﬁnite number of possible decision alternatives or courses of action
available to the decision maker. The decision maker generally has control over
the speciﬁcation and description of the alternatives. Note that one alternative to
always include is the action of doing nothing, which is maintaining the status quo.
States of Nature The scenarios or states of the environment that may occur but
are not under control of the decision maker. These are the circumstances under
which a decision is made. The states of nature are mutually exclusive events and

Decision Analysis
6-3
exhaustive. This means that one and only one state of nature is assumed to occur
and that all possible states are considered.
Outcome Outcomes are the measures of net beneﬁt, or payoﬀ, received by the decision
maker. This payoﬀis the result of the decision and the state of nature. Hence,
there is a payoﬀfor each alternative and outcome pair. The measures of payoﬀ
should be indicative of the decision maker’s values or preferences. The payoﬀs
are generally given in a payoﬀmatrix in which a positive value represents net
revenue, income, or proﬁt and a negative value represents net loss, expenses,
or costs. This matrix yields all alternative and outcome combinations and their
respective payoﬀand is used to represent the decision problem.
6.2.2
An Investment Example
As an example, consider the common decision of determining where to invest money [3,4].
The example will be limited to choosing one form of investment, but similar approaches
exist for considering a portfolio. Assume you have $10,000 to invest and are trying to
decide between a speculative stock (SS, one that has high risk, but can generate substan-
tial returns), a conservative stock (CS, one that will perform well in any environment, but
doesn’t have the potential for large returns), bonds (B), and certiﬁcates of deposit (CD).
Data from the last several years have been collected and analyzed and estimated rates of
return for each investment have been determined. These rates, however, are dependent on
the state of the economy. From the analysis it has also been determined that there are three
basic states of the economy to consider; Strong—the economy is growing at a rate greater
than 5%, Stable—the economy is growing at a rate of 3%–4%, and Weak—the economy is
growing at a rate of 0%–2%.
The information available indicates that SS has an estimated rate of return of 18% if the
economy is strong, a rate of return of 10% if the economy is stable, and a rate of return
of −5% if the economy is weak. The CS has an estimated rate of return of 13% if the
economy is strong, a rate of return of 8% if the economy is stable, and a rate of return of
1% if the economy is weak. Bonds have an estimated return of 4% if the economy is strong,
a rate of 5% if the economy is stable, and a rate of return of 6% if the economy is weak.
CD have an estimated rate of return of 7% if the economy is strong, a rate of return of 3%
if the economy is stable, and a rate of return of 2% if the economy is weak. Lastly, there
is also the alternative of DN, that is of not investing the money, and that would yield 0%
return regardless of the state of the economy.
Note that for each combination of decision alternative and state of nature there is a
corresponding payoﬀ. The payoﬀin this example is the expected rate of return. In general,
the payoﬀis some quantitative value used to measure a possible outcome. This measure is
generally given in terms of a monetary value, but any measure can be used. Likewise, the
value used is often a statistical estimate of the possible payoﬀ. As the payoﬀs result from
a combination of alternatives and states of nature, they are easily represented by a payoﬀ
matrix (Table 6.1). The payoﬀmatrix for this example is given in Table 6.1.
6.3
Decision Making under Risk
Every business situation, as well as most life situations, involves a level of uncertainty. The
modeling of the uncertainty yields diﬀerent approaches to the decision problem. One way
to deal with the uncertainty is to make the uncertain more certain. This can be done by
using probability to represent the uncertainty. That is, uncontrollable factors are modeled

6-4
Operations Research Methodologies
TABLE 6.1
PayoﬀMatrix for Investment Problem
State of Nature
Alternative
Strong (%)
Stable (%)
Weak (%)
Speculative Stock (SS)
18
10
−5
Conservative Stock (CS)
13
8
1
Bonds (B)
4
5
6
Certiﬁcates of Deposit (CD)
7
3
2
Do Nothing (DN)
0
0
0
and estimated probabilistically. When this is possible, the uncertainty is characterized by a
probability distribution.
There is a wide spectrum of uncertainty to consider when analyzing a problem. At one end
of the spectrum is complete certainty of the data. At the other end is complete uncertainty.
What lies between is varying degrees of uncertainty in which the payoﬀfor each alternative
and state of nature can be described by some probability distribution. This is what is termed
as decision making under risk in this chapter.
It should be noted that there are inherent diﬃculties with any approach related to uncer-
tainty. Uncertainty results from a lack of perfect information and a decision maker will often
need to determine whether more information is needed before a decision can be made. This
occurs at a cost, and may not yield a better decision. Additionally, the probability models
themselves may not truly reﬂect the situation or may be diﬃcult to obtain. Therefore, the
decision maker must always keep in mind that the use of the probability models is to help
the decision maker avoid adverse decisions and to help better understand the risk involved
in any decision made. The probability models are decision support tools, not exact methods
for giving a solution. Human input is always needed.
One way to estimate probabilities is to use prior probabilities for given events. The prior
probabilities come from existing information about the possible states of nature that can be
translated into a probability distribution if the states of nature are assumed to be random.
These prior probabilities are often subjective and dependent on an individual’s experience
and need to be carefully determined. For more speciﬁc information on determining prior
probabilities see Hillier and Lieberman [5, chapter 15] and Merkhofer [6].
In the given investment example, assume that a probability distribution for the possible
states of nature can be determined based on the past data related to economic growth.
These prior probabilities are given in Table 6.2.
The advantage of prior probabilities is that they can be used to determine expected values
for diﬀerent criteria. Expected values often give an acceptable estimate as to what is most
likely to happen and therefore give a good basis on which to help make a decision. Several
approaches based on expected value have been developed and are outlined below. Note,
however, that expected value is not always the best indicator and the decision maker’s
preferences and knowledge always need to be included in the process.
TABLE 6.2
Investment Problem with Prior Probabilities
State of Nature
Alternative
Strong
Stable
Weak
SS
18%
10%
−5%
CS
13%
8%
1%
B
4%
5%
6%
CD
7%
3%
2%
DN
0%
0%
0%
Prior Probability of State of Nature
0.1
0.6
0.3

Decision Analysis
6-5
TABLE 6.3
Expected Values of Each Alternative
State of Nature
Alternative
Strong
Stable
Weak
Expected Value
SS
18%
10%
−5%
6.3%
CS
13%
8%
1%
6.4%
B
4%
5%
6%
5.2%
CD
7%
3%
2%
3.1%
DN
0%
0%
0%
0%
Prior Probability
0.1
0.6
0.3
Another concept that can be included here is the concept of dominance. Dominance
implies that an alternative will never be chosen because there is another alternative that is
always better regardless of the state of nature. Hence, there is no reason to ever consider the
alternative that is dominated; it cannot give a reasonable course of action. In the investment
example the do nothing (DN) alternative is a dominated alternative. That is, there is at
least one other alternative that would always be chosen regardless of the state of nature
over do nothing. From Table 6.1 it can be seen that the alternatives, CS, B, and CD each
dominate DN. It is then reasonable to exclude the alternative DN from further consideration.
However, to illustrate this concept and to show that this alternative is never chosen, it is
left in the problem for now.
6.3.1
Maximum Likelihood
The idea behind maximum likelihood is that good things always happen. If the decision
maker is very optimistic about the future then why not choose the best possible outcome
assuming the best possible state of nature will occur.
To ﬁnd the best choice by maximum likelihood, ﬁrst ﬁnd the state of nature with the
largest probability of occurring and then choose the alternative for that state of nature with
the maximum payoﬀ.
In the investment example given by Table 6.2, the state of nature with the largest prob-
ability is stable growth. For that state of nature, the SS has the largest rate of return.
Therefore, based on this criterion the decision would be to invest in SS.
6.3.2
Expected Value under Uncertainty
For a more balanced approach, a decision maker can assume that the prior probabilities give
an accurate representation of the chance of occurrence. Therefore, instead of being overly
optimistic the decision maker can compute the expected value for each alternative over the
states of nature and then choose based on those expected values.
To implement this approach, for each alternative determine its expected value based
on the probability of the state of nature and the payoﬀfor that alternative and state of
nature. That is, for each row of the payoﬀmatrix take the sum of each payoﬀtimes the
corresponding state of nature probability.
From Table 6.2, for the alternative SS, the expected value would be (0.1 × 18%) + (0.6 ×
10%) + (0.3 × (−5%)) = 6.3%. The expected value for each alternative of the investment
example is given in Table 6.3.
Based on expected value the best decision would be to invest in CS.
Note that this type of analysis can easily be done in a spreadsheet, which aﬀords the
opportunity to do a what-if analysis. As the prior probabilities are the most questionable
of the data, in a spreadsheet, it would be possible to adjust these values to see the impact
of diﬀerent values for the prior probabilities. Likewise, it is also possible to do a sensitivity
analysis of the prior probabilities and determine ranges for the prior probabilities for which

6-6
Operations Research Methodologies
each alternative would be chosen. There exists software designed to help with this type of
analysis. One such package is called SensIt and is described in Ref. [5]. The Web address
for this software is given later.
6.3.3
Expected Opportunity Loss or Expected Regret
There are times when the actual payoﬀs and their expected values are not suﬃcient for
a decision maker. Many times when dealing with uncertainty, decision makers feel better
about a decision if they know they have not made an error that has cost them a great deal
in terms of opportunity that has been lost. This lost opportunity is generally called regret
and can be used to determine an appropriate decision.
After a decision is made and the actual state of nature becomes known, the opportunity
missed by the decision made can be determined. That is, for the investment example, if the
actual state of nature that occurred was stable growth, but it was decided that B were the
best investment, then the opportunity of going with SS was missed. Therefore, the missed
opportunity of having a 10% rate of return instead of the 5% chosen, results in a missed
opportunity (regret) of 10 −5 = 5%.
To avoid the possibility of having missed a large opportunity, expected opportunity loss
(EOL) looks to minimize the opportunity loss or regret. To do this, the possible opportunity
loss for each state of nature is determined for each alternative. This is done by taking
the largest payoﬀvalue in each column (state of nature) and then for each alternative
subtracting the payoﬀfor that alternative from the largest payoﬀin the column. For the
investment example, the opportunity loss is computed for each alternative and state of
nature in Table 6.4. Once the opportunity losses are known, the EOL is determined for
each alternative (row) by using the prior probabilities. For example, in Table 6.4 the EOL
for alternative CS is given by (0.1 × 5%) + (0.6 × 2%) + (0.3 × 5%) = 3.2%.
The criterion then is to minimize the EOL. In this example, the best decision based on
this criterion would be to select CS.
6.3.4
Expected Value of Perfect Information
All the methods presented thus far were dependent on the given prior information. The
question then arises as to whether it would be advantageous to acquire additional infor-
mation to help in the decision making process. As information is never completely reliable,
it is not known if the additional information would be beneﬁcial. Therefore, the question
becomes, what is the value of additional information? The expected value of perfect infor-
mation (EVPI) criterion gives a way to answer this question by measuring the improvement
of a decision based on new information.
The idea behind EVPI is that if the state of nature that will occur is known with certainty,
then the best alternative can be determined with certainty as well. This would give the best
value for the decision, which can then be compared to the value expected under current
TABLE 6.4
Expected Opportunity Loss (EOL) of Each Alternative
State of Nature
Alternative
Strong
Stable
Weak
EOL
SS
18 −18 = 0%
10 −10 = 0%
6 −(−5) = 11%
3.3%
CS
18 −13 = 5%
10 −8 = 2%
6 −1 = 5%
3.2%
B
18 −4 = 14%
10 −5 = 5%
6 −6 = 0%
4.4%
CD
18 −7 = 11%
10 −3 = 7%
6 −2 = 4%
6.5%
DN
18 −0 = 18%
10 −0 = 10%
6 −0 = 6%
9.6%
Prior Probability
0.1
0.6
0.3

Decision Analysis
6-7
information. The diﬀerence between the two would give the value of additional information.
Based on this idea, there are two basic approaches to determining EVPI [1].
EVPI and Expected Value under Uncertainty
This approach gives a straightforward approach for computing EVPI. The value of EPVI is
just simply the expected value under certainty minus the expected value under uncertainty.
To compute the expected value under certainty simply take the best payoﬀunder each state
of nature and multiply it by its prior probability and sum these.
For the investment example the expected value under certainty would be (0.1 × 18%) +
(0.6 × 10%) + (0.3 × 6%) = 9.6%. The expected value under uncertainty is the best alterna-
tive value, which in this example comes from alternative CS that has an expected value of
6.4% from Table 6.3. With this information EVPI is computed as EVPI = 9.6% −
6.4% = 3.2%.
Therefore, if an investment of $10,000 is being planned, the most additional information
would be worth is $10,000 × 3.2% = $320.
EVPI and EOL
If the state of nature is known with certainty then there would be no opportunity loss. That
is, under certainty the best alternative would always be chosen so there would be no regret
or opportunity loss, it would always be zero. Hence, under uncertainty EOL gives the cost of
uncertainty that could be eliminated with perfect information and therefore, EPVI = EOL.
From Table 6.4 of EOL values, the best alternative is the one with the smallest EOL that
is given by the alternative CS with an EOL of 3.2%. This is also the value for EPVI found
above. Note that for each alternative its expected value plus its EOL is 9.6%, the expected
value under certainty found above.
6.3.5
Decision Trees
To help better understand the decision process it can be represented graphically by a com-
bination of lines and nodes called a decision tree. The purpose of the tree is to pictorially
depict the sequence of possible actions and outcomes [1,3–5,7]. There are two types of nodes
used in a decision tree. A square represents a decision point or fork, which is the action
(alternative) taken by the decision maker and a circle represents an event or chance fork,
which is the state of nature. The branches (lines) in the tree represent the decision path
related to alternatives and states of nature.
Decision trees are generally most helpful when a sequence of decisions must be made, but
they can also be used to illustrate a single decision. The decision tree in Figure 6.1 is for the
investment example. Note that each square node denotes the alternative chosen and each
circular node represents the state of nature and the numbers at the end of each decision
path (lines) are the payoﬀs for that course of action.
Figure 6.1 gives a pictorial view of the information contained in Table 6.2, the payoﬀ
matrix. Notice that the same computations for expected value can be performed within
the tree. Starting at the right hand side of the tree, for each circle (level) the expected
value of that circle is computed by taking the probability on each branch times the payoﬀ
associated with each branch. For example, the top circle would have an expected value of
(0.1 × 18%) + (0.6 × 10%) + (0.3 × (−5%)) = 6.3%. The expected value can then be added
to the tree as a value at each circle. Once the values are known at each circle, the value
for the square is given by taking the maximum value of the circles. The decision tree with

6-8
Operations Research Methodologies
Invest SS
Invest CD
Invest CS
Invest B
13%
8 %
1%
4 %
5%
6%
7%
3%
2%
Stable
(0.6)
Weak
(0.3)
18%
10%
Strong
(0.1)
0%
Stable
Weak
0%
0%
Do Nothing ND
Strong 
Strong 
Stable
Stable
(0.3)
(0.1)
(0.1)
(0.6)
(0.6)
(0.3)
(0.3)
(0.6)
(0.3)
Strong
(0.1)
(0.1)
Strong 
(0.6)
Stable
Weak
Weak
Weak
−5%
FIGURE 6.1
Decision tree for investment problem.
this additional expected value information is given in Figure 6.2 and is the same as the
information given in Table 6.3.
This, however, represents a single decision. Decision trees are much more useful when
dealing with a sequence of alternatives and states of nature in which a series of decisions
must be made. These types of problems cannot be represented easily in matrix form and
thus decision trees become a powerful tool. Decision trees are useful for a wide variety of
scenarios and the interested reader is referred to Refs. [3–5,7] for additional examples.
6.3.6
Posterior Probabilities
As mentioned before, there is the possibility of gaining new knowledge to help in better
deﬁning the probability of an occurrence of a state of nature. This can be done through
additional experimentation or sampling. The improved estimates of the probabilities are
called posterior probabilities or Bayes’ probabilities due to the use of Bayes’ theorem in
their computation.
To ﬁnd posterior probabilities, additional information about the states of nature must be
acquired. This can be done by experimentation, which in this sense can refer to the use of
experts, analysts, or consultants as well as actual additional experimentation. Therefore, for
any of these forms, the additional information is viewed as being obtained by an experiment.
The experimentation will have a possible set of outcomes that will help determine which
alternative to select. Based on the outcomes of the experimentation and their probability

Decision Analysis
6-9
Invest SS
Invest CD
Invest CS
Invest B
(0.1)
(0.6)
 (0.3)
 (0.1)
 (0.1)
(0.6)
(0.6)
 (0.3)
 (0.3)
13%
8%
1%
4 %
5%
6%
7%
3%
2%
Stable
(0.6)
Weak
 (0.3)
18%
10%
−5%
Strong 
 (0.1)
0%
Stable
(0.6)
Weak
 (0.3)
0%
0%
Strong 
 (0.1)
Do Nothing ND
Strong
Strong
Strong
Stable
Stable
Stable
Weak
Weak
Weak
6.3%
5.2%
0.0%
3.1%
6.4%
6.4%
FIGURE 6.2
Decision tree for investment problem with expected values.
of occurrence, Bayes’ theorem yields the posterior probability of a state of nature. The pos-
terior probability of the state of nature given the test outcome equals the prior probability
of the state of nature times the conditional probability of the outcome given the state of
nature divided by the sum of the conditional probabilities for all the states of nature and
outcomes. To state this mathematically, let Si be the state of nature i and Oj be outcome
j of the additional experimentation. Also let P(Si) be the prior probability of the state of
nature i. Then the posterior probability of the state of nature i given test outcome j is
P(Si|Oj). This is deﬁned as:
P(Si|Oj) =
P(Si)P(Oj|Si)

k
P(Sk)P(Oj|Sk)
To help illustrate how to compute these values, assume for our investment problem that
ﬁnancial analysts who spent 20 years working on Wall Street have been hired to give their
opinion on what investment to make. They state their opinion in terms of the possible
state of nature as a probability and their willingness to invest. This opinion is viewed as
a test outcome. That is, they might state that based on the past 20 years of investing
they have done, when the economic growth is strong (greater than 5%) they have invested
70% of the time, 10% of the time they have been neutral on investing, and 20% of the
time they have not invested. This could be written as P(Invest | Strong) = 0.7, P(Neutral |
Strong) = 0.1 and P(Do Not Invest | Strong) = 0.2. The ﬁnancial analysts would also give

6-10
Operations Research Methodologies
TABLE 6.5
Conditional Probabilities for Financial
Analysts
Given State of Nature
Investment Opinion
Strong
Stable
Weak
Invest
0.7
0.6
0.3
Neutral
0.1
0.3
0.3
Don’t Invest
0.2
0.1
0.4
similar probabilities for each possible state of nature in terms of investing. Table 6.5 gives
the conditional probabilities given by the analysts for the investment problem.
With these conditional probabilities and with the prior probabilities from Table 6.2, the
posterior probabilities are determined as follows:
P(Strong | Invest) = (0.1 × 0.7)/((0.1 × 0.7) + (0.6 × 0.6) + (0.3 × 0.3)) = 0.135
P(Strong | Neutral) = (0.1 × 0.1)/((0.1 × 0.1) + (0.6 × 0.3) + (0.3 × 0.3)) = 0.036
P(Strong | Don’t Invest) = (0.1 × 0.2)/((0.1 × 0.2) + (0.6 × 0.1) + (0.3 × 0.4)) = 0.1
P(Stable | Invest) = (0.6 × 0.6)/((0.1 × 0.7) + (0.6 × 0.6) + (0.3 × 0.3)) = 0.692
P(Stable | Neutral) = (0.6 × 0.3)/((0.1 × 0.1) + (0.6 × 0.3) + (0.3 × 0.3)) = 0.643
P(Stable | Don’t Invest) = (0.6 × 0.1)/((0.1 × 0.2) + (0.6 × 0.1) + (0.3 × 0.4)) = 0.3
P(Weak | Invest) = (0.3 × 0.3)/((0.1 × 0.7) + (0.6 × 0.6) + (0.3 × 0.3)) = 0.173
P(Weak | Neutral) = (0.3 × 0.3)/((0.1 × 0.1) + (0.6 × 0.3) + (0.3 × 0.3)) = 0.321
P(Weak | Don’t Invest) = (0.3 × 0.4)/((0.1 × 0.2) + (0.6 × 0.1) + (0.3 × 0.4)) = 0.6
Additionally, the probability of a given opinion from the analyst is given by the denomi-
nator of the posterior probability related to that opinion. Hence, P(Invest) = (0.1 × 0.7) +
(0.6 × 0.6) + (0.3 × 0.3) = 0.52,
P(Neutral) = (0.1 × 0.1) + (0.6 × 0.3) + (0.3 × 0.3) = 0.28,
P(Don’t Invest) = (0.1 × 0.2) + (0.6 × 0.1) + (0.3 × 0.4) = 0.2.
Note that once the prior and conditional probabilities are known, the process of ﬁnding
the posterior probabilities can easily be done within a spreadsheet [5].
With the posterior probabilities, it is now possible to analyze the decision in terms of
this new information. This is done through the use of the decision tree by adding a chance
node at the beginning of the tree (left side) representing the opinion of the analyst and then
replacing the prior probabilities on each branch with the corresponding posterior probabili-
ties. Likewise, for each opinion the probability of that opinion can also be added to the tree
for those branches. The tree for the investment problem with posterior probabilities is given
in Figure 6.3. Note, however, that to simplify the tree the option of DN is not included as
it is a dominated alternative and would never be chosen.
For this tree, the analysis is done as before to determine expected values except now the
posterior probabilities are used and at each decision node the decision is based on the best
expected value for that node.
To do the analysis start at the right side of the tree and for each circle determine its
expected value. For example, for the neutral branch the expected value for the circle corre-
sponding to invest in SS is (0.036 × 18%) + (0.643 × 10%) + (0.321 × (−5%)) = 5.47%. The
expected value for the circle corresponding to invest in CS is (0.036 × 13%) + (0.643 × 8%) +
(0.321 × 1%) = 5.93%. The expected value for the circle corresponding to invest in bonds
is (0.036 × 4%) + (0.643 × 5%) + (0.321 × 6%) = 5.28%. Lastly, the expected value for the
circle corresponding to invest in CD is (0.036 × 7%) + (0.643 × 3%) + (0.321 × 2%) = 2.82%.
Therefore, the decision at the square on the neutral branch is given by the maximum

Decision Analysis
6-11
8.49%
Invest SS
7.46%
Strong (0.135)
Stable (0.692)
Weak (0.173)
Weak (0.173)
Weak (0.173)
Weak (0.173)
Weak (0.321)
Weak (0.321)
Weak (0.321)
Weak (0.321)
Weak (0.6)
Weak (0.6)
Weak (0.6)
Weak (0.6)
Strong (0.135)
Strong (0.135)
Strong (0.135)
Strong (0.036)
Strong (0.036)
Strong (0.036)
Strong (0.036)
Strong (0.1)
Strong (0.1)
Strong (0.1)
Strong (0.1)
Stable (0.692)
Stable (0.692)
Stable (0.692)
Stable (0.643)
Stable (0.643)
Stable (0.643)
Stable (0.643)
Stable (0.3)
Stable (0.3)
Stable (0.3)
Stable (0.3)
18%
10%
13%
1%
4%
5%
6%
7%
3%
2%
18%
10%
13%
8%
1%
4%
5%
6%
7%
3%
2%
18%
10%
13%
8%
1%
4%
5%
6%
7%
3%
2%
−5%
−5%
8%
−5%
Invest CD
2.8%
5.5%
5.5%
4.3%
1.8%
2.82%
Invest CS
Invest B
Invest CS
Invest B
Invest CS
Invest B
Invest SS
Invest CD
5.93%
5.28%
5.93%
Neutral (0.28%)
5.47%
3.37%
5.05%
Invest SS
Invest CD
8.49%
Invest (0.52)
7.18%
Don't invest (0.2)
FIGURE 6.3
Decision tree with posterior probabilities and expected values.

6-12
Operations Research Methodologies
expected value of the four circles associated with that decision square. The maximum value
is 5.93%, which corresponds to invest in CS.
From the decision tree the expected payoﬀat each decision (square) is given which yields
the decision of which stock to invest in. That is, if the analyst says to invest, then the best
investment choice would be in SS. If the analyst is neutral then the best investment would
be in CS and if the analyst says not to invest then the best investment would be in bonds.
Lastly, from the determined probabilities of each opinion the expected payoﬀfor the entire
tree can be computed as (0.52 × 8.49%) + (0.28 × 5.93%) + (0.2 × 5.5%) = 7.18%. This value
yields what one would expect the return on the investment to be if they employed the
services of the analyst. Note that this is higher than the expected return of 6.4% from
Figure 6.2, when an analyst is not used.
6.3.7
Utility Functions
In the preceding discussion and example, it was assumed that the payoﬀwas monetary even
though rates of return were used. For the investment example the return on investment could
easily be converted to monetary terms. The investment SS under strong (greater than 5%)
growth had a return of 18%. As $10,000 was being invested, the value $10,000 × 18% = $1800
could have been used instead of the rate of return. Monetary values could have been used
in place of each rate of return and then the expected payoﬀs would have been in dollars
as well. Either approach is correct and the choice is dependent on the preference of the
decision maker.
Monetary value, however, is not always the best way to model a decision as the value of
money to a decision maker can ﬂuctuate depending on the circumstances. For instance, let
us say you were going to play the lottery. The big payoﬀthis week is $42 million. It costs
you just $1 to purchase a ticket. If you have $100 in your wallet you might think spending
$1 for a chance at $42 million is a good investment. Now, if you only have $5 in your wallet
and are hungry you might think playing the lottery is not a good idea as $5 will by you a
meal but $4 will not. That $1 for the ticket is viewed diﬀerently now than it was when you
had $100. This varying value of money is called the utility of the money. Utility is given in
terms of a utility function as the utility changes based on circumstances and time.
Another way of looking at this concept is to consider the following choice or lottery [5]:
(1) Flip a coin and if it comes up heads you win $10,000 and if it comes up tails you win
nothing or (2) accept $3,000. Which would you choose? Many people would choose the $3000.
This concept is used in many popular game shows. The question though is why would one
choose the $3000 when the expected payoﬀfor the coin ﬂip, (0.5 × $10,000) + (0.5 × $0) =
$5000, is more? The answer is that choices are not always made solely on expected monetary
gain, but also on the potential for loss and other circumstances. An individual’s view of
money may also change over time and with the diﬀerent situations they face. That is the
utility of money.
When discussing utility, decision makers are usually classiﬁed into one of three classes:
risk-averse, risk-neutral, and risk-seeking. The risk-averse person has a decreasing marginal
utility for money. This means that the less money they have the more it is worth to them
(the more utility it has). That is, assume for $1 its utility is 10 and for $2 its utility is
15. The unit increase in the amount of money did not have an equivalent increase in the
utility. The utility of the additional dollar was only 5 instead of 10. As the number of dollars
increases, each additional dollar will have less utility for the decision maker. This can be
easily seen by graphing the utility of money versus the amount of money. This graph is
called the utility function. Let u(M) be the function representing the utility of $M. For a

Decision Analysis
6-13
Utility
Monetary value
FIGURE 6.4
Risk-averse utility function.
Utility
Monetary value
risk neutral
Utility
Monetary value
risk-seeker
FIGURE 6.5
Risk-neutral and risk-seeker utility functions.
risk-averse person this function will always have a concave shape, representing decreasing
marginal utility, as shown in Figure 6.4.
In contrast, the risk-neutral person views money the same regardless of the amount or
circumstances. That is, if $1 has a utility of 10 then $2 will have a utility of 20; each dollar
added will have the same utility. This would result in a linear function for utility. When
the utility of money is not considered in a problem it is the same as if the decision maker is
assumed to be risk neutral. The risk-seeking person will have an increasing marginal utility
for money. That is, the more they have the more it is worth. The utility of each additional
dollar increases. If $1 has a utility of 10 then $2 could have a utility of 25. The additional
$1 had a higher utility (15) than the previous $1 (10). These decision makers are willing to
make risky decisions for the opportunity of more money. A risk-seeker would gladly give up
the $3000 for a 50–50 chance at $10,000. The utility function for a risk-seeker would be a
convex function, representing increasing marginal utility. Figure 6.5 gives utility functions
for both the risk-neutral and risk-seeking individual.
Most people do not always ﬁt into one of the above three classiﬁcations. They tend to be
a mixture of the three depending on the circumstances and will generally change over time.
However, for a given point in time, it is assumed that a utility function for the decision
maker can be determined. Note that utility functions are unique to an individual decision
maker. Therefore, one might have two diﬀerent decision makers looking at the same problem
and each will make a diﬀerent decision based on their utility.

6-14
Operations Research Methodologies
Once a utility function for a decision maker has been determined, then the analysis
of the problem proceeds as it does when using monetary value. The only diﬀerence is that
the utility function value of the payoﬀamount is used in place of the monetary value and
the expected utility is found instead of the expected value. Decisions are then based on the
expected utility values.
How to determine or construct the utility function for a decision maker is then the fun-
damental aspect of applying utility functions. The keys to estimating a utility function for
an individual are the following two fundamental properties [5,7].
PROPERTY 6.1
Under the assumptions of utility theory, the decision maker’s utility
function for money has the property that the decision maker is indiﬀerent between two
alternative courses of action if the two alternatives have the same expected utility.
PROPERTY 6.2
The utility of the worst outcome is always zero and the utility of the
best outcome is always one.
From these two properties it can be seen that the determination of an individual’s utility
function is based on the comparison of equivalent lotteries.
A lottery is just simply a choice between outcomes with a given probability. Lotteries are
generally denoted by trees where the branches of the tree represent outcomes. For example,
a lottery in which there is a 50–50 chance of receiving $5000 and losing $1000 would be
given by:
$5000 (0.5)
$1000 (0.5)
A lottery that is certain has a probability of 1 and is represented by a single line.
$3000 (1)
To illustrate how to ﬁnd a utility function, consider the following adaptation of the
investment problem. You are to determine how to invest $5000. There are two stocks from
which to choose. Stock HR is a high risk stock and will yield a return of $1000 if there is
strong economic growth. Stock LR is a low risk stock and will yield a return of $300 if there is
strong economic growth. However, if the economic growth is weak then the HR stock will lose
$500 and the LR stock will only yield $100. The payoﬀmatrix for the two stocks HR and LR
and the states of nature, strong growth and weak growth, are given in Table 6.6. The prior
probabilities are a forecast of the possible state of the economy based on past economic data.
TABLE 6.6
PayoﬀMatrix for HR and LR Stocks
States of Nature
Alternative
Strong
Weak
HR
1,000
−500
LR
300
100
Prior Probability
0.4
0.6

Decision Analysis
6-15
For this decision problem the best possible outcome is to make $1000 and the worst pos-
sible outcome is to lose $500. Let u(x) be the utility function. Then from properties 1 and 2,
the utility values are u(1000) = 1 and u(−500) = 0.
To construct the utility function, additional values of utility must be determined. This
is done by determining a lottery with a known utility value and then ﬁnding an equivalent
lottery. The equivalent lottery then has the same utility value.
In this example, $1000 and −$500 have known utility values. Therefore, the ﬁrst step in
constructing the rest of the utility function is to ﬁnd the value x for which the decision
maker is indiﬀerent between the two lotteries below.
$1000 (0.5)
$500(0.5)
and
x (1.0)
Note that the lottery on the right has an expected utility of 1 × (0.5) + 0 × (0.5) = 0.5. As
a result, the two lotteries are indiﬀerent (equivalent) and the utility of x must be u(x) = 0.5.
Assume the decision maker states that the value x that makes these lotteries indiﬀerent is
x = −$200. Hence, u(−200) = 0.5. With this value we can now compute the expected utility
of a 50–50 lottery of u(−200) and u(−500) which is 0.5 × (0.5) + 0 × (0.5) = 0.25. Therefore,
any lottery indiﬀerent to this 50–50 lottery will have a utility of 0.25. That is, the value x
that makes the following lotteries indiﬀerent has utility 0.25.
$200 (.05) 
$500 (0.5)
x  (1.0)
and
Let x = −400 be the value that makes the lotteries indiﬀerent. Then u(−400) = 0.25. This
gives four points of the utility function. To approximate the utility function, several more
points can be determined by repeating this process. This is given below.
Find x such that the following are indiﬀerent.
$1000 (0.5)
$200(0.5)
and
x (1.0)
The lottery on the right has expected utility 1.0 × (0.5) + 0.5 × (0.5) = 0.75. Let x = 300
be the value that makes the lotteries indiﬀerent. Then u(300) = 0.75.
Next, ﬁnd x such that the following are indiﬀerent.
$1000 (0.5)
$300(0.5)
and
x (1.0)
The lottery on the right has expected utility 1.0 × (0.5) + 0.75 × (0.5) = 0.875. Let x = 600
be the value that makes the lotteries indiﬀerent. Then u(600) = 0.875.

6-16
Operations Research Methodologies
Lastly, ﬁnd x such that the following are indiﬀerent.
$300 (0.5)
$200(0.5)
and
x (1.0)
The lottery on the right has expected utility 0.75 × (0.5) + 0.5 × (0.5) = 0.625. Let x = 100
be the value that makes the lotteries indiﬀerent. Then u(100) = 0.625.
From these values an approximate utility function can be plotted based on the points
(x, u(x)). For the example, the points are (−500, 0), (−400, 0.25), (−200, 0.5), (100, 0.625),
(300, 0.75), (600, 0.875), (1000, 1). The utility function’s approximate graph is given in
Figure 6.6.
Figure 6.7 gives the decision tree for this problem based on the information in Table 6.6
and the utility values determined above. The values at each circle are the expected utilities
and the value at the decision square is the expected utility of the tree, which indicates the
best choice would be to invest in LR stock.
−500
−200
1000
600
300
−400
100
0.25
0.5
0.625
0.75
0.875
1
FIGURE 6.6
Approximate utility function.
0.625 
0
Invest HR
Invest LR
Weak growth(0.6)
1
0.4
0.675
Strong growth(0.4)
Strong growth(0.4)
Weak growth(0.6)
0.75
0.675
FIGURE 6.7
Decision tree for HR and LR stocks using expected utility.

Decision Analysis
6-17
TABLE 6.7
Expected Payoﬀand Utility for HR and LR Stock
States of Nature
Strong
Weak
Expected Value
Alternative
$
u(x)
$
u(x)
Monetary
Utility
HR
1000
1
−500
0
100
0.4
LR
300
0.75
100
0.625
180
0.675
Prior Probability
0.4
0.6
Table 6.7 has the computations for both the expected utility and the expected mone-
tary payoﬀfor comparison. Note that for this problem they give the same decision; how-
ever, this is not always the case. It is quite possible to get diﬀerent results when using
utility values as opposed to monetary values. For example, if in this investment prob-
lem the utility values remained the same but the HR stock only lost $300 instead of
$500 when the economy was weak then HR’s expected monetary value would have been
(0.4 × 1000) + (0.6 × (−300)) = 220, which is greater than the expected monetary value of
180 for the LR stock. For this scenario, stock HR would then be the decision made based
on expected monetary value which would be diﬀerent than the decision of stock LR based
on expected utility values.
Utility theory gives a way to more realistically model a problem based on the decision
maker’s preferences and attitudes toward risk. The determination of utility functions may
be diﬃcult, but there are existing utility functions that one can potentially use to help in
this process [5,7,8].
There can be drawbacks, however, to using utility functions. One is determining the
utility function and making sure the decision maker’s preferences satisfy the Von Neumann–
Morgenstern axioms of utility theory [7,9]. There is also the possibility of prospecting and
framing [7,10]. In both these cases, decision makers will make a decision that goes against
what the expected utility would suggest. Prospect theory says that a decision maker will not
treat the probabilities given for a problem as valid. They will “distort” the probabilities.
This generally occurs at the boundaries as most people are more sensitive to changes in
probabilities close to 0 or close to 1. Framing refers to how choices are presented to a decision
maker. The framing of the question and its context can result in diﬀerent outcomes. For a
more detailed discussion of the utility theory see Refs. [7–13].
In this section, the determination of the “best” decision is dependent on expected values,
which implies that a probability distribution can be determined for the states of nature.
If this is possible, then decision trees can also be employed to help illustrate the decision
process and to organize the data. There has been a variety of software developed to help
in this process. Some of the software is very sophisticated and requires a great deal of
background knowledge to use properly, and some of the software is very simple to use. For
a thorough analysis of existing software see Maxwell [14,15].
6.4
Decision Making under Uncertainty
When it is not possible to ﬁnd a probability distribution for a decision problem, or none
exists, that problem is said to be uncertain. In this situation, it is not possible to determine
expected values and one must ﬁnd other factors on which to make a decision. The payoﬀ
matrix is still employed for problems under uncertainty, but without any prior probabilities.
There are several methods to deal with this kind of uncertainty. These methods diﬀer
mainly in how the decision maker views risk and the state of nature. The diﬀerent methods
could be viewed as being similar to aspects of utility theory in that they try to take into

6-18
Operations Research Methodologies
TABLE 6.8
PayoﬀMatrix for Snack Production
State of Nature
Alternative
Decrease Slightly
Steady
Marginal Increase
Signiﬁcant Increase
A1
−1500
−400
1100
2150
A2
−450
200
500
500
A3
−850
−75
450
1100
A4
−200
300
300
300
A5
−150
−250
−450
−850
account the decision maker’s preferences and attitudes. These methods are discussed in the
context of the following problem.
6.4.1
Snack Production Example
A local food processing plant has developed a low calorie snack that has taken the market
by storm. After only being on the market for 3 months sales have outpaced forecasts by
more than 60% and the plant cannot meet demand. It is running 24/7 and is lagging behind
demand, at the moment, by 25%. To help alleviate this problem, management must decide
how to increase production. Marketing was asked to develop new demand forecasts but has
not been able to determine whether or not sales will continue to increase, hold steady, or
decrease. After several diﬀerent surveys it was found that sales could hold steady, increase
marginally (somewhere between 10% and 30%), increase signiﬁcantly (somewhere between
30% and 70%) or decrease slightly (somewhere between 1% and 10%), but marketing can-
not give a distribution for the possible outcomes. After researching possible solutions the
following alternatives have been put forth by the engineering team: (A1) Build an addi-
tional plant that would be able to increase the total production of the product by 120%;
(A2) Add an identical additional line to the current plant that would increase production
by 30%; (A3) Expand the current plant and replace current line with new technologies that
would allow total production to increase by 60%; (A4) Hire a full time operations analyst to
increase eﬃciency with an estimated increase in production of 15%; and (A5) Do Nothing
and maintain the status quo. An economic analysis of each alternative has been carried out
and the net proﬁt gained from possible increased sales minus implementation costs is used
as the payoﬀfor the alternative. For the DN alternative this would simply be lost sales.
Table 6.8 gives the payoﬀmatrix, and the values are in $1000.
6.4.2
Maximin (Minimax) Criterion
This criterion is a conservative approach to managing the unknown risk. The intent is to
determine the worst that can happen with each alternative and then pick the alternative
that gives the best worst result. When payoﬀs are proﬁt then the maximin (maximize the
minimum value possible) criterion is used and when the payoﬀis cost then the minimax
(minimize the maximum value possible) criterion is used.
Implementation of this criterion is quite simple. For maximin, identify the minimum in
each row and then select the alternative with the largest row minimum. This is given in
Table 6.9. The best choice under maximin would be alternative A4.
Under this approach, the decision maker is guaranteed they will never lose more than
$200,000 but at the same time they will never make more than $300,000, whereas a diﬀerent
alternative such as A2 would guarantee they would never lose more than $450,000 but could
make $500,000 but never any more. By choosing A4, they relinquish the opportunity to make
more money if the state of nature is anything other than a decrease in demand.

Decision Analysis
6-19
TABLE 6.9
Maximin for Snack Production
State of Nature
Decrease
Marginal
Signiﬁcant
Alternative
Slightly
Steady
Increase
Increase
Row Minimum
A1
−1500
−400
1100
2150
−1500
A2
−450
200
500
500
−450
A3
−850
−75
450
1,100
−850
A4
−200
300
300
300
−200∗
A5
−150
−250
−450
−850
−850
∗represents best alternative
TABLE 6.10
Maximax for Snack Production
State of Nature
Decrease
Marginal
Signiﬁcant
Alternative
Slightly
Steady
Increase
Increase
Row Maximum
A1
−1500
−400
1100
2150
2150∗
A2
−450
200
500
500
500
A3
−850
−75
450
1100
1100
A4
−200
300
300
300
300
A5
−150
−250
−450
−850
−150
∗represents best alternative
6.4.3
Maximax (Minimin) Criterion
This criterion is the opposite of maximin in that it is very optimistic and risk-seeking. For
this approach it is assumed that the best scenario possible will happen. Therefore, for each
row the maximum is chosen and then the alternative with the maximum row maximum is
selected. Table 6.10 illustrates this.
The alternative selected by this approach is A1, which yields the largest possible pay-
oﬀ. However, just as with the maximin approach this leaves the decision maker open to
signiﬁcant losses. For A1 two states of nature yield a loss, one of which is quite signiﬁcant.
6.4.4
Hurwicz Criterion
The two previous criteria were at the extremes, one very pessimistic and the other very
optimistic. This criterion is designed to mitigate the extremes and to allow for a range of
attitudes of the decision maker. The basis of this approach is an index of optimism given
by α, such that 0 ≤α ≤1. The more certain a decision maker is that the better states of
nature will occur, the larger the value of α, the less certain the smaller the value of α. For a
given value of α, the criterion is implemented by taking for each row (alternative), ((α × row
max) −(1 −α) ×|row min|), and then selecting the alternative with the largest value. For
the example, let α = 0.48, which indicates not a strong regard for optimism or pessimism,
but with a hint of pessimism. For the alternatives, the computations are
For A1
0.48 × (2150) −0.52 × (1500) = 44
For A2
0.48 × (500) −0.52 × (450) = 6
For A3
0.48 × (1100) −0.52 × (850) = 85
For A4
0.48 × (300) −0.52 × (200) = 40
For A5
0.48 × (−150) −0.52 × (850) = −514
The best alternative under this criterion is A3.

6-20
Operations Research Methodologies
TABLE 6.11
Laplace for Snack Production
State of Nature
Decrease
Marginal
Signiﬁcant
Alternative
Slightly
Steady
Increase
Increase
Expected Value
A1
−1500
−400
1100
2150
338∗
A2
−450
200
500
500
188
A3
−850
−75
450
1100
156
A4
−200
300
300
300
175
A5
−150
−250
−450
−850
−425
∗represents best alternative
TABLE 6.12
Minimax Regret for Snack Production
State of Nature
Decrease
Marginal
Signiﬁcant
Alternative
Slightly
Steady
Increase
Increase
Row Maximum
A1
1350
700
0
0
1350
A2
300
100
600
1650
1650
A3
700
375
650
1050
1050∗
A4
50
0
800
1850
1850
A5
0
550
1550
3000
3000
∗represents best alternative
6.4.5
Laplace Criterion or Expected Value
The Laplace criterion is another more optimistic approach to the problem. The basis of this
approach is that since the probabilities are not known for the states of nature and there is
no reason to think otherwise, each state of nature should be viewed as equally likely. This
gives a probability distribution for the states of nature, which then allows the expected
value to be determined. The alternative with the best expected value is selected.
For the snack production example, as there are four states of nature, the probability for
each state would be 0.25. The expected value for each alternative is determined by taking
the sum of the payoﬀfor each state of nature times 0.25. Table 6.11 gives the Laplace
(expectation) values and A1 is the best choice under this criterion.
6.4.6
Minimax Regret (Savage Regret)
The last approach is based on the concept of regret discussed previously. The idea is to not
look at payoﬀs, but instead at lost opportunities (regret). The regret is based on each state
of nature and is determined by looking at the best outcome of that state against the other
possible outcomes. Therefore, regret is computed for each column of the payoﬀmatrix by
taking the maximum value in that column and replacing each payoﬀvalue in the column
with the maximum value minus the payoﬀvalue. The regret matrix for the snack production
problem is given in Table 6.12. Once the regret is known, the minimax criterion is applied
that says to take the maximum value of each row and then choose the minimum of the row
maximums. For this criterion the best alternative is A3.
Making decisions under uncertainty is a diﬃcult task. The decision maker’s attitude
toward risk will aﬀect the approach taken and the possibilities for large losses or gains as
well as regrets. Generally, when the probabilities of the states of nature are truly unknown
the decision maker will make more conservative decisions.

Decision Analysis
6-21
6.5
Practical Decision Analysis
In this chapter, a number of approaches for diﬀerent types of decision making problems
have been covered. The methods and procedures presented should be viewed as decision
support tools and not as the ﬁnal solutions to the problems. To help make better decisions
in practice there are structured approaches to decision making that can be employed that
use the discussed techniques as a part of the process. The intent of this section is to provide
a practical, well thought-out approach to solving a problem in which the techniques of this
chapter are but a part. The structure being proposed and the majority of the material in
this section are based on the work of Hammond et al. [16], which should be consulted for a
more complete discussion.
There are many approaches to solving problems, but most use a basic framework that
involves deﬁning the problem and alternatives. However, oftentimes the greatest value
added to the decision process from a structured approach is the enhanced understanding and
insight gained by the decision maker from going through the process. This in itself will lead
to better decisions as most decisions made in practice are based on intuition, experience, and
gut reactions without much analysis. This is not to say that these cannot lead to good out-
comes, but this approach in the long run will produce more poor decisions than good ones.
As an example, consider the following Hospital Problem that you have been asked to
solve. As the director of a private hospital you have been tasked by the governing board
to increase proﬁts through additional services. The hospital is located in a prosperous city
that is home to a major university, several other health care facilities, good infrastructure,
and many local and nearby attractions. The hospital is well run and well respected and
especially noted for its geriatric care unit, its burn unit, its cardiac unit, its pharmacy, and
its pediatrics unit. The major problems within the facility are related to staﬃng issues, but
the board desires to increase proﬁts without any drastic cost savings accomplished through
staﬀattrition or layoﬀs. Some of the board feel that the hospital is already understaﬀed.
Any procedure that can be used to help structure the decision making process for this
problem or any problem is very helpful in practice. The practical framework given here was
developed by Hammond et al. [16], based on years of experience and research in the deci-
sion analysis ﬁeld. They present a framework containing eight essential aspects of decision
making that pertain to all decisions. These eight factors constitute the body of the process
of making eﬀective decisions. These eight factors are presented below.
6.5.1
Problem Deﬁnition
The ﬁrst and key aspect of decision making is to identify the real problem. That is, to make
sure you are solving the correct problem. “A good solution to a well-posed decision problem
is always better than an excellent solution to a poorly posed one” [16].
To deﬁne the decision problem and pose it well requires practice and eﬀort. To begin,
one should understand what caused the need for the decision in the ﬁrst place. For example,
you might be asked to determine the best conveyor system to replace your current one, but
the actual problem may not be that the current conveyor system needs replacing. The actual
problem might be a cycle time issue or it might be that you need to determine what the best
material handling system for your overall production facility is without limiting yourself to
only conveyors. Time and thought need to go into why the apparent problem has arisen
and into what the actual components are that are driving the problem. Too many mistakes
are made by solving the wrong problem.
As the problem is deﬁned, there are usually constraints that are identiﬁed that help
narrow the alternatives. These constraints may be self-imposed or imposed from outside.

6-22
Operations Research Methodologies
For example, a company might desire to maintain a zero defect quality control policy (self-
imposed) and also must meet federal regulations on hazardous waste disposal. All constraints
should be questioned to make sure they are legitimate, do not hinder you from seeing the
best decision, or that they are not just a symptom of deﬁning the wrong problem.
Next, to make sure “you are focused on the right goal” [16], it is essential that the correct
elements of the problem are identiﬁed as well as what other decisions will be aﬀected by the
decision for this problem. How this plays into the decision process should also be determined.
Use this to help reﬁne the scope of the problem deﬁnition. That is, all aspects of the problem
should be considered, but you do not want to make the problem so large or complicated
that a decision cannot be made.
Lastly, re-examine the problem deﬁnition as the process goes along. Time and informa-
tion may change your perspective or give new insights into the problem that will allow for
the problem to be restated and better deﬁned. Always look for opportunities to question
the problem statement and to redeﬁne it. This redeﬁning of the problem statement helps
focus the problem better, helps determine information needed, and tends to lead to better
decisions.
Consider the hospital problem introduced at the beginning of this section. The problem
seems at ﬁrst to be to increase proﬁt so as to please shareholders. This could be done
through cost-cutting measures and staﬃng issues but that does not address the real issue
and is against the board’s wishes. The proﬁt margins for the hospital have been increasing
slightly each year for the last 5 years and it has been determined that this facility is one of
the most proﬁtable in the state. As you ask questions of the board, you ﬁnd out that their
desire to raise proﬁts is related to wanting more positive exposure for the hospital at the cor-
porate level and to wanting to attract better doctors, as well as improving the shareholders’
positions. After much discussion and research you settle on the problem being that you need
to increase proﬁts without cutting cost while enhancing the reputation of the hospital. This
will impact several areas of concern, such as staﬃng issues and attracting quality physicians.
Therefore, you develop the following problem statement: Increase the proﬁt margin of the
hospital through the expansion of services that meet the needs of the community.
6.5.2
Objectives
With the problem well deﬁned, you now need some way to determine the best decision. This
is done by identifying what it is you really want to achieve. That is, how will you know if
your decision is a good one? How will you measure the success of the decision? These criteria
are called objectives. Hence, objectives tell you what you want to achieve and give a way
of assessing the decision you have made. One of the keys in this process is to make sure all
the objectives for the problem are identiﬁed. This helps one think through the problem and
in this process of analysis and objective identiﬁcation it might be possible to even identify
possible solutions. This also helps one develop and identify alternatives for the problem.
Objectives should help guide the entire process of decision making. They tell you what
information is needed, what decision should be made, why a decision is a good one, and the
importance of the decision in terms of time and eﬀort. Hammond et al. [16] give ﬁve steps
to help identify objectives.
1. Write down all concerns you hope to address through your decision.
2. Convert your concerns into succinct objectives.
3. Separate ends from means to establish your fundamental objectives. Means objec-
tives are milestones on the way to the end. The end or fundamental objective is
what is needed for its own sake. It is the reason for the decision. Means objectives

Decision Analysis
6-23
help generate alternatives and give insight to the problem, but fundamental objec-
tives are what are used to evaluate and compare alternatives.
4. Clarify what you mean by each objective. For example, what does improve quality
really mean?
5. Test your objectives to see if they capture your interests. That is, based on the
derived objectives test several alternatives to see what decision would be made
and whether or not that decision is one you could live with.
When identifying your fundamental objectives keep in mind that objectives are personal,
diﬀerent people can have diﬀerent objectives for the same problem, objectives should not
be limited by what is easy or hard, fundamental objectives should remain relatively stable
over time, and if a potential solution makes you uncomfortable, you may have missed an
objective [16]. The process of identifying the objectives is very important to the overall
decision process. It is imperative to take the time to get them right and to specify them
fully and completely.
Note that in many problems there are multiple objectives to be considered. In the previous
sections of this chapter, the methods presented were for a single objective. Often, when
there appear to be multiple objectives, there is still only one fundamental or end objective
and the methods developed here would be applicable to that objective. If there truly are
multiple fundamental objectives, then one would need to apply diﬀerent techniques such as
multiple objective decision making (see Chapter 5).
For the hospital problem there is one overriding fundamental objective and that is to
increase proﬁts. This is easily measured. Likewise, as part of meeting this objective it
is possible to measure its impact on the other stated objectives of enhancing reputation
and attracting high quality physicians. The enhancement of reputation is more diﬃcult
to measure, but can be determined indirectly from survey results, newspaper articles, and
increased patient demand. The attraction of high quality physicians is easily measured.
6.5.3
Alternatives
Alternatives are the potential choices one can make to solve the given problem. The dilemma,
though, is that your solution will only be as good as your set of alternatives. Therefore,
it is crucial to have a good set of alternatives that are not limited in scope. Do not think
that just because alternatives to a similar problem are known, those alternatives will suﬃce
for the new problem or that the apparent alternatives are all that is needed. Oftentimes,
bad decisions are made because the only alternatives considered are the apparent ones or
ones that are only incrementally better than the current solution. Realize that regardless of
how well you generate alternatives, there still may be a better one out there. However, do
not fall into the trap of trying to ﬁnd the “best” alternative. This may lead to spending so
much time looking for the alternative that no decision is made or that a decision is forced
on you by the delay. Find a set of alternatives with which you are satisﬁed, and stop there,
realizing you can adapt and consider new alternatives as you go through the process.
Generating a good set of alternatives takes time and eﬀort. To help in the process consider
the following techniques by Hammond et al. [16] to generate good alternatives.
1. Based on your objectives, ask how they can be achieved. Do this for means as
well as fundamental objectives.
2. Challenge the constraints. Make sure a constraint is real and is needed. Assume
there are no constraints and develop alternatives and then see how they ﬁt the
constraints or can be adapted to the constraints. Ask why the constraint is there

6-24
Operations Research Methodologies
and what can be done to meet it or overcome it. Are there ways to nullify the
constraint? Think creatively.
3. Set high aspirations. Force yourself to meet this high expectation by thinking
more out of the box.
4. Do your own thinking ﬁrst. Develop alternatives before you ask others for input.
This helps keep you from having only alternatives that might have been biased
from input from others.
5. Learn from experience. Have you made similar decisions? What was right or
wrong in that decision? Use past experience to help generate alternatives, but
do not fall into the trap of just repeating past mistakes or not considering other
alternatives since you did not before.
6. Ask others for suggestions and input. This is done only after you have carefully
thought about the problem and developed your own alternatives. This can be
viewed as a sort of a brainstorming session. Keep an open mind and consider the
suggestions and see what other alternatives they might trigger for you.
7. Never stop looking for alternatives. As you go through the decision process you
might see other alternatives or get other ideas. Never think it is too late to
consider diﬀerent alternatives.
8. Know when to quit looking for alternatives. Realize that the perfect solution
usually does not exist so spending a great deal of time looking for it is not
usually productive. If you have thought hard about your alternatives and have a
set of alternatives that give you a diversity of options and at least one you would
be satisﬁed with, then you could stop. However, remember to always consider
additional alternatives as they present themselves through the decision process.
9. Don’t forget to always include the do nothing or status quo alternative. It is
possible that the current state is the best alternative available.
For the hospital problem you have spent much time researching what other hospitals are
doing, analyzing your current strengths, market trends, and the population demographics of
your area. You have developed several alternatives based on this research. One is the devel-
opment of an outpatient clinic focused on orthopedics. You believe there is great potential
here based on the aging population, your already strong geriatrics unit, and the increasing
number of sports-related injuries occurring at the local high schools and university. Another
is the development of a complementary pediatrics unit that is designed similar to the obstet-
rics unit that allows for family visitors and birthing rooms. This unit will allow a family
member to stay in the room and the rooms will be furnished more like a bedroom than a
hospital room. These will cost more but will allow for more family input and attention. It
is believed as well that through this process you can also address some of the staﬃng issues
by having family members in the room to help monitor progress and other basic services.
Another alternative would be to upgrade the cardiac unit to allow for more state-of-the-art
surgeries and techniques. This would help attract physicians and open up the hospital to
more specialized and proﬁtable surgeries. Lastly, you could maintain the status quo and see
if proﬁts increase on their own.
After the determination of these alternatives you hold two brainstorming sessions, one
with the staﬀand one with the physicians. In the session with the staﬀsomeone mentions
the growing number of elder care facilities in the area and the lack of trained staﬀto check
pharmaceuticals and prescriptions. This leads to the alternative of using your pharmacy
and pharmacists as a pharmaceutical distribution center. You will meet the needs of the

Decision Analysis
6-25
surrounding elder care facilities by providing pharmaceutical services, quality and safety
checks on prescriptions, door-to-door delivery, and expedited services. Another idea that
came up was the development of a research unit dedicated to state-of-the-art genetic testing
and automated surgeries on the assumption this would increase reputation as well as draw
highly qualiﬁed physicians, which would result in increased proﬁts. Due to the large capital
outlay to equip such a laboratory, it was eliminated from consideration at this time. At
this point no new alternatives or alternatives that meet the constraints are presented, so
you move forward with the four alternatives: outpatient orthopedics, family pediatric unit,
upgrade cardiac unit, and the pharmaceutical distribution network.
6.5.4
Consequences
Now that the problem has been deﬁned, objectives determined, and alternatives developed
you need to be able to determine which alternative is the “best.” This is done by looking
at the consequences of the alternatives related to the objectives and determining what the
payoﬀfor each alternative will be. In the case of a single objective, the techniques described
in this chapter can be applied to ﬁnd the best alternative. The key is to make sure that as
you look at each alternative the measure of the consequences is consistent and is a measure
you are willing to use. The key to making sure you have the right consequences for the
alternatives is to build a consequence table. This is very similar to the payoﬀmatrix, and
generally the payoﬀmatrix is part of the consequences table. The diﬀerence is that the
consequences table includes more measures than just the single payoﬀmeasure and is used
to help identify the main measure that is consistent across the alternatives.
The alternatives of the hospital problem are analyzed in terms of the demographics,
potential proﬁts, costs of implementation, staﬃng issues, and other pertinent factors. Sev-
eral possible states of nature are also formulated related to the economy, demographics,
health care costs, supply and demand of nurses, and household income. Based on this
information a consequence table is developed where the main measure is potential proﬁt
increases. Additionally, within the consequence table are factors related to reputation and
physician applications and recruitment. From this consequence table it is determined that
the best alternative is to develop a pharmaceutical distribution network for local elder care
facilities.
6.5.5
Tradeoﬀs
The concept of tradeoﬀs is most commonly applied to multiobjective problems, but the
basic idea also has applicability to single objective problems.
Even with applicable techniques and appropriate consequences it may be that there is
more than one possible alternative to consider. Recall from the earlier discussion of the
diﬀerent techniques applied to the investment and snack problem that each technique gave
a diﬀerent alternative. The question then becomes: which alternative to choose.
To help in this process one can look at possible tradeoﬀs within the alternatives. First,
one should eliminate any alternative that is dominated by another alternative. As discussed
earlier, that is when there is an alternative that is better than another alternative no matter
the state of nature or the objective. The dominated alternative is eliminated from further
consideration. For the other alternatives, consider what the tradeoﬀs are of choosing one
over another. For example, in the snack production problem alternative A2 was never chosen
by any of the methods. However, if the decision maker is conservative in nature they may
be willing to choose A2 over A4 by viewing the tradeoﬀof losses in the ﬁrst two states of

6-26
Operations Research Methodologies
nature compared to the gains in the other two states of nature as acceptable for this other
conservative solution.
In the case of a single objective, when two alternatives are “equal” then other considera-
tions will need to come into play. This might include subjective considerations or additional
objectives not considered initially.
As an example, in the hospital problem there was actually one alternative that gave a
higher expected proﬁt than the distribution alternative. That alternative was the orthope-
dics unit. However, in analyzing the two alternatives it was seen that the diﬀerence in proﬁt
increases was not substantial, but the diﬀerence in reputation enhancement was quite large
in favor of the distribution network. Likewise, the distribution network integrated well with
the geriatrics unit and could possibly increase demand for this service substantially in the
future. Hence, the distribution network was chosen.
6.5.6
Uncertainty
Virtually all problems deal with uncertainty in some form or another. The methods pre-
sented in this chapter are designed for dealing with uncertainty. The key is to realize uncer-
tainty exists and that it will need to be dealt with appropriately. This involves determining
the possible states of nature and their probability of occurrence, if possible, and appropriate
courses of action based on those states of nature. The decision trees presented earlier are
an excellent tool to help in the understanding of uncertainty and risk and its impact on the
decision problem.
It is also important to understand that when uncertainty exists it is possible to make a
good decision and still have the consequences of that decision turn out poorly. Likewise, it
is possible to make a bad decision and still have the consequences turn out good. That is
the nature of risk and uncertainty. Therefore, under uncertainty and risk, the consequences
of the decision might not be the best way to assess the decision made or the process used
to arrive at that decision. What truly needs to be assessed is the decision process itself.
A poor process may occasionally yield good consequences, but over the long run it will be
detrimental. Likewise, a good process may occasionally yield poor consequences, but over
the long run this process will be beneﬁcial.
When developing the problem statement uncertainty must be included. This is done
through the use of a risk proﬁle. To determine the risk proﬁle, Hammond et al. [16] suggest
answering the following four questions:
1. What are the key uncertainties?
2. What are the possible outcomes of these uncertainties?
3. What are the chances of occurrence of each possible outcome?
4. What are the consequences of each outcome?
Answering these four questions should lead to a payoﬀmatrix with prior probabilities.
The payoﬀmatrix can then be used to generate solutions and to develop a corresponding
decision tree.
As part of the consequence table for the hospital problem uncertainties were taken
into account and probabilities determined. The key uncertainties were related to health
care costs, demographics, economic conditions, and potential competition from other area
health care providers. For example, it was determined that if the demographics indicated
a downturn in the percentage of the population over 60 and the economy was weak then

Decision Analysis
6-27
elder care facilities would lose business. The consequence of this for the distribution net-
work alternative would be an expected 2% decline in proﬁts, for the orthopedic unit it
would be an expected 3% decline in proﬁts, for the family pediatric unit it would result
in no change in proﬁt, and for the cardiac unit it would represent an expected 1% decline
in proﬁts.
6.5.7
Risk Tolerance
Once the risk proﬁle has been determined it is necessary to understand how you as the
decision maker view risk. This is the process of determining your utility function. Are you
risk-averse, risk-neutral, or risk-seeking? Taking this into account can help you make a
better decision for yourself. Here it is important to remember that the measure of success
is the decision process and not the consequences of the decision. A risk-seeker and someone
who is risk-averse will come to two completely diﬀerent decisions, but if the process is sound
then those decisions are good decisions for them.
As the decision maker for the hospital problem you have determined that you are risk-
averse, preferring more conservative approaches that give a better chance of smaller proﬁts
than riskier high-return alternatives. This is part of the reason why the family pediatric
unit and the cardiac unit did not look promising to you.
6.5.8
Linked Decisions
Very few decisions are truly made independently. Current decisions have an impact on
decisions that will need to be made later. These are called linked decisions. To make a good
decision, at this point in time, requires you to think about the impact of the current decision
on later decisions. A poor decision now may lead to limited alternatives for later decisions.
“The essence of making smart linked decisions is planning ahead” [16].
Linked decisions tend to always contain certain components. These components are:
(1) a basic decision must be made now; (2) the alternatives to choose from are aﬀected
by uncertainty; (3) desirability of an alternative is also aﬀected by the possible future deci-
sions that will need to be made based on the current decision; (4) the typical decision
making pattern is to decide, then learn, then decide, then learn, then decide, and so on.
Considering and making linked decisions is really just another decision problem. The
process outlined in this section can be used to help make the linked decisions. There are six
basic steps in dealing with linked decisions [16]:
1. Understand the basic decision problem. This includes the steps outlined here of
deﬁning the problem, specifying objectives, generating alternatives, determining
consequences, and identifying uncertainties. The uncertainties are what make
linked decisions diﬃcult, so it is important to spend time correctly identifying
and acknowledging them.
2. Identify ways to reduce critical uncertainties. This generally involves obtaining
additional information. Once obtained, then it might be possible to determine
posterior probabilities.
3. Identify future decisions linked to the basic decision. In doing this, it is important
to ﬁnd an appropriate time horizon to consider. If the horizon is too long, you
might be considering future possibilities that will not actually come into play. A
rule of thumb is often the current decision and two future linked decisions.

6-28
Operations Research Methodologies
4. Understand relationships in the linked decisions. Often a decision tree can be
used here to help illustrate relationships and to discover unseen relationships.
5. Decide what to do in the basic decision. Again, this is where a decision tree can
be most helpful in keeping the linked decisions before you. Look at what will be
happening and then work backward from there determining the consequences of
each choice. In the decision tree this is equivalent to starting at the right hand
side and working back through the branches.
6. Treat later decisions as new decision problems.
Recognition of linked decisions is the ﬁrst critical aspect. It is important to always view
decisions in terms of short-range as well as long-range impacts. A more holistic view allows
the decision maker to better gauge the impact of decisions on current objectives as well
as possible unintended consequences and later eﬀects. The consideration of the linking of
a decision to future considerations leads to better decisions. A truly good decision will be
viewed as such not only now but also in the future when its eﬀects are experienced.
For the hospital problem, part of the reason the distribution network was chosen was
its linkage with future decisions. The growing demographics of the region indicated that
elder care would become a central component of the health care system for that region.
Focusing on the distribution network would serve as a marketing device for the hospital in
this demographic as well as a way to identify potential patients with concerns other than
pharmaceuticals. It might then be possible to build the elder care reputation of the hospital
to the point where it is the ﬁrst choice among the elderly, which would then allow the
hospital to go forward with its other alternative of orthopedics targeted primarily at the
elderly. This in turn could open up other alternatives related to elder care not yet considered
and make the hospital the dominant health care delivery entity in the region. This would
then attract high quality physicians and enhance the reputation even further.
6.6
Conclusions
In this chapter, a number of approaches to diﬀerent types of decision problems under risk
and uncertainty have been presented. The intent of theses methodologies is to give tools to
the decision maker to assist in the decision making process. They should not be viewed as a
shortcut to good decisions or as a panacea for making decisions. To be a consistently good
decision maker requires a systematic and thorough approach to decision making. The goal
is to have a sound decision making process and framework that allows a decision maker to
approach any problem with conﬁdence. The worst scenario for a decision maker is to have
decisions made for them due to their slowness in responding to a situation or their inability
to decide. A decision maker’s intent should be to make the best decision possible with the
information given at the current point in time. If the process is well thought out and sound
then the decisions made should generally be good.
The purpose of this chapter has been to help decision makers develop their decision
making process and to give tools to help in that process. The following basic systems
approach to decision making summarizes the approach taken in this chapter [16]:
1. Address the right decision problem.
2. Clarify your real objectives and recognize means and ends.
3. Develop a range of creative alternatives.
4. Understand the consequences of the decisions (short and long term).

Decision Analysis
6-29
5. Make appropriate tradeoﬀs between conﬂicting objectives.
6. Deal appropriately with uncertainties.
7. If there are multiple objectives, make appropriate tradeoﬀs.
8. Take account of you risk attitude.
9. Plan ahead for decisions linked over time.
10. Make your decision, analyze possible other solutions through the use of diﬀerent
methodologies, and perform sensitivity analysis.
There is no guarantee that a decision will always be good when uncertainties are present,
but the chances of a decision being good increase signiﬁcantly when the decision process is
good. Making good decisions takes time and eﬀort but the rewards are worth the invest-
ment. This is true for decisions in everyday life as well as those one wrestles with in their
work. To help one make good decisions consistently, a decision maker needs to develop
a good process, apply the process to all decisions, be ﬂexible, adjust decisions as time
and information become available, and enjoy what they are doing; then good decisions
will occur.
6.7
Resources
Decision analysis is a well-studied ﬁeld with many resources available. In addition to the
information given here and the stated references thus far, there are many other works to
consider. Hillier and Lieberman [5, p. 717] give a good listing of actual applications of
decision analysis as presented in the practitioner-oriented journal Interfaces. Many other
articles and books exist, which cannot all be detailed here. To help one begin the process
of a deeper study of decision analysis, Refs. [17–32] are recommended as initial sources.
A quick search of the Web is also suggested in that there are many sites devoted to decision
analysis. These include academic sites as well as professional and consulting sites. Some good
places to start are Decision Analysis Society of INFORMS: http://faculty.fuqua.duke.edu/
daweb/ and International Society on Multiple Criteria Decision Making: http://www.terry.
uga.edu/mcdm/.
There is also a large selection of software available at many diﬀerent levels to help in
decision analysis. Maxwell [14,15] has done excellent surveys of decision analysis software.
The latest survey is available on the Web at http://www.lionhrtpub.com/orms/orms-10-04/
frsurvey.html.
Additionally, there is also a decision analysis software survey from OR/MS Today avail-
able at http://www.lionhrtpub.com/orms/surveys/das/das.html.
Along with these surveys, there are a number of sites dedicated to software. Some of
this software is discussed in Ref. [5], which also gives some examples and brief tutori-
als. The software mentioned in this chapter related to decision trees can be found at
http://www.treeplan.com/ and http://www.usfca.edu/∼middleton/.
Some additional software sites include, but are not limited to, http://www.palisade.com/
precisiontree/, http://www.lumina.com/, and http://www.vanguardsw.com/decisionpro/
jgeneral.htm.
Remember, though, that the software is still just a decision support tool, not a decision
making tool. It will give you the ability to analyze diﬀerent scenarios and to do sensitivity
analysis, but the decision must still be made by the decision maker.

6-30
Operations Research Methodologies
References
1. Ravindran, A., Phillips, D.T., and Solberg, J.J., Operations Research: Principles and Prac-
tice, 2nd ed., John Wiley & Sons, 1987, chap. 5.
2. Watson, S.R. and Buede, D.M., Decision Synthesis: The Principles and Practice of Decision
Analysis, Cambridge University Press, 1987.
3. Taha, H.A., Operations Research: An Introduction, 8th ed., Prentice Hall, 2007, chap. 13.
4. Arsham, H., Tools for Decision Analysis: Analysis of Risky Decisions, http://home.
ubalt.edu/ntsbarsh/opre640a/partIX.htm.
5. Hillier, F.S. and Lieberman, G.J., Introduction to Operations Research, 8th ed., McGraw
Hill, 2005, chap. 15.
6. Merkhofer, M.W., Quantifying Judgmental Uncertainty: Methodology, Experiences and
Insights, IEEE Transactions on Systems, Man, and Cybernetics, 17:5, 741–752, 1987.
7. Winston, W.L., Operations Research: Applications and Algorithms, 4th ed., Brooks/ Cole,
2004, chap. 13.
8. Pennings, J.M.E. and Smidts, A., The Shape of Utility Functions and Organizational
Behavior, Management Science, 49:9, 1251–1263, 2003.
9. Keeney, R.L. and Raiﬀa, H., Decisions with Multiple Objectives, Wiley, 1976.
10. Tversky, A. and Kahneman, D., The Framing of Decisions and the Psychology of Choice,
Science, 211:4481, 453–458, 1981.
11. Golub, A.L., Decision Analysis: An Integrated Approach, Wiley, 1997.
12. Biswas, T., Decision Making under Uncertainty, St. Martin’s Press, 1997.
13. Gass, S.I. and Harris, C.M., Eds., Encyclopedia of Operations Research and Management
Science, Kluwer Academic Publishers, 1996.
14. Maxwell, D.T., Software Survey: Decision Analysis, OR/MS Today, 29:3, 44–51, June 2002.
15. Maxwell, D.T., Decision Analysis: Aiding Insight VII, OR/MS Today, October 2004,
http://www.lionhrtpub.com/orms/orms-10-04/frsurvey.html.
16. Hammond, J.S., Keeney, R.L., and Raiﬀa, H., Smart Choices: A Practical Guide to Making
Better Decisions, Harvard Business School Press, 1999.
17. Ben-Haim, Y., Information-Gap Decision Theory: Decisions under Severe Uncertainty,
Academic Press, 2001.
18. Clemen, R.T. and Reilly, T., Making Hard Decisions with Decision Tools, Duxbury Press,
2001.
19. Connolly, T., Arkes, H.R., and Hammond, K.R., Eds., Judgment and Decision Making: An
Interdisciplinary Reader, Cambridge University Press, 2000.
20. Daellenbach, H.G., Systems and Decision Making: A Management Science Approach,
Wiley, 1994.
21. Eiser, J., Attitudes and Decisions, Routledge, 1988.
22. Flin, R., et al., (Ed.), Decision Making under Stress: Emerging Themes and Applications,
Ashgate Pub., 1997.
23. George, C., Decision Making under Uncertainty: An Applied Statistics Approach, Praeger
Pub., 1991.
24. Goodwin, P. and Wright, G., Decision Analysis for Management Judgment, 2nd ed., Wiley,
1998.
25. Gr¨unig, R., K¨uhn, R., and Matt, M., Eds., Successful Decision-Making: A Systematic
Approach to Complex Problems, Springer, 2005.
26. Kirkwood, C.W., Strategic Decision Making: Multiobjective Decision Analysis with Spread-
sheets, Duxbury Press, 1997.
27. Keeney, R.L., Foundations for Making Smart Decisions, IIE Solutions, 31:5, 24–30, May
1999.

Decision Analysis
6-31
28. Pratt, J.W., Raiﬀa, H., and Schlaifer, R., Introduction to Statistical Decision Theory, The
MIT Press, 1995.
29. Van Asselt, M., Perspectives on Uncertainty and Risk: The Prima Approach to Decision
Support, Kluwer Academic Publishers, 2000.
30. Van Gigch, J.P., Metadecisions: Rehabilitating Epistemology, Kluwer Academic Publishers,
2002.
31. Vose, D., Risk Analysis: A Quantitative Guide, 2nd ed., John Wiley & Sons, 2000.
32. Wickham, P., Strategic Entrepreneurship: A Decision-making Approach to New Venture
Creation and Management, Pitman, 1998.


7
Dynamic Programming
Jos´e A. Ventura
Pennsylvania State University
7.1
Introduction............................................
7-1
7.2
Deterministic Dynamic Programming Models ....
7-3
Capacity Expansion Problem [11,13] • Capacity
Expansion Problem with Discounting [5,11,13] •
Equipment Replacement Problem [3,8,10] • Simple
Production Problem • Dynamic Inventory Planning
Problem [11,15] • Reliability System Design [11]
7.3
Stochastic Dynamic Programming Models ........ 7-19
Stochastic Equipment Replacement Problem [3,8,10] •
Investment Planning [11]
7.4
Conclusions ............................................ 7-24
References .................................................... 7-24
7.1
Introduction
Dynamic programming (DP) is an optimization procedure that was developed by Richard
Bellman in 1952 [1,2]. DP converts a problem with multiple decisions and limited resources
into a sequence of interrelated subproblems arranged in stages, so that each subproblem is
more tractable than the original problem. A key aspect of this procedure is that the decision
in one stage cannot be made in isolation due to the resource constraints. The best decision
must optimize the objective function with respect to the current and prior stages, or the
current and future stages.
A wide variety of deterministic and stochastic optimization problems can be solved by
DP. Some of them are multi-period planning problems, such as production planning, equip-
ment replacement, and capital investment problems, in which the stages of the DP model
correspond to the various planning periods. Other problems involve the allocation of limited
resources to various activities or jobs. The latter case includes all variations of the knapsack
and cargo loading problems.
The diﬃculty of DP is in the development of the proper model to represent a particular
situation. Like an artist experience in model development is essential to be able to man-
age complex problems and establish recurrence relations between interrelated subproblems
in consecutive stages. For this reason, this chapter introduces DP by analyzing diﬀerent
applications, where decision variables may be integer or continuous, objective functions and
constraints may be either linear or nonlinear, and the data may be deterministic or random
variables with known probability distribution functions. Below, the basic components of a
7-1

7-2
Operations Research Methodologies
DP model are introduced and applied to the following nonlinear integer program with a
single constraint:
maximize z =
n

i=1
ci(xi)
subject to
n

i=1
aixi ≤b
xi ≥0, integer,
i = 1, . . ., n
where b and ai, i = 1, . . ., n, are positive real values. This optimization model represents the
allocation of b units of a resource to n diﬀerent activities. The objective is to maximize
the total proﬁt, given that function fi(xi), i = 1, . . ., n, deﬁned for xi ∈{0, 1, 2, . . ., ⌊b/ai⌋},
shows the proﬁt for activity i given that xi units of the activity are employed.
The terminology used to deﬁne a DP model includes the following main elements
[2,3,4,6,7,10,11,14,16]:
Stage (i): the original problem is divided into n stages. There is an initial stage
(stage n) and a terminating stage (stage 1). Index i represents a given stage,
i = 1, 2, . . ., n.
State (si): each stage has a number of states associated with it. The states are the
various possible conditions in which the system might be at each particular stage
of the problem.
Decision variable (x i): there is one decision variable or a subset of decision variables
for each stage of the problem.
Contribution function (ci(xi)): this function provides the value at stage i given
that the decision is xi.
Optimal value function (f i(si)): best total function value from stage i to stage n,
given that the state at stage i is si.
Optimal policy (pi(si) = x ∗
i ): optimal decision at a particular stage depends on the
state. The DP procedure is designed to ﬁnd an optimal decision at each stage for
all possible states.
Transformation function (ti(si, x i)): this function shows how the state for the
next stage changes based on the current state, stage, and decision.
Example
si+1 = ti(si, xi) = si −aixi
Recurrence relation: this is an equation that identiﬁes the optimal policy (decision)
at stage i, given that the optimal policy at stage i + 1 is available.
Example
fi(si) =
max
xi=0,1, ...,⌊si/ai⌋{ci(xi) + fi+1(si −aixi)},
si = 0, . . ., b;
i = 1, . . ., n −1
Boundary conditions: these are the initial conditions at stage n and obvious values
of the optimal value function.
Example
fn(sn) =
max
xn=0,1,...,⌊sn/an⌋{cn(xn)},
sn = 0, . . ., b
Answer: the global optimal solution of the problem is determined in the terminating
stage (stage 1).
Example
f1(b)

Dynamic Programming
7-3
...
...
s1
s2
 s3
 si
 sn
si+1
sn+1
Stage 1
Stage 2
Stage i
Stage n
x1
 x2
xi
xn
c1(x1)
c2(x2)
ci(xi)
cn(xn)
FIGURE 7.1
Graphical illustration of the stages of a DP formulation.
In this DP model, the recurrent process begins in stage n, moves backwards, and terminates
in stage 1 (backward formulation). A similar formulation could be developed in reverse
order from stage 1 to stage n (forward formulation). In many applications, both the two
formulations are equivalent and require the same computational eﬀort to solve a particular
instance of the problem. In other applications, depending on the initial conditions and the
size of the state space, one of the formulations may be more eﬃcient than the other.
Figure 7.1 provides a graphical illustration equivalent to the above DP formulation. Each
box corresponds to a stage. The state at a particular stage is predetermined by the prior
decision. Based on the state and the decision made at this stage, an outcome represented
by the contribution function and the value of the state at the following stage is obtained.
Two signiﬁcant advantages of DP can be observed in the above formulation. One is that
it transforms a problem with n decision variables into n single-variable subproblems. The
second advantage over virtually all other optimization methods is that it ﬁnds the global
maxima or minima rather than just local optima. The key limitation of DP is the dimen-
sionality of the state space. In simple terms, if the model includes several state variables,
then diﬃculties concerning the storage of information and time required to perform the
computation may appear.
The justiﬁcation of the DP procedure relies on the Principle of Optimality. Roughly, the
principle states that, if (x∗
1, x∗
2, . . ., x∗
n) is an optimal policy to a given problem and s∗
i is
the optimal state in stage i, then (x∗
i , x∗
i+1, . . ., x∗
n) is an optimal policy for the subproblem
deﬁned between stages i and n with s∗
i as the initial state. The correctness of a DP model
can be proved by showing that all possible states are considered and the recurrence relation
satisﬁes the principle of optimality.
The selection of the state variables for the DP model is critical in the sense that the
states in the state space must satisfy the Markov Property. That is, the optimal policy from
any stage i to stage n depends only on the entering state (si) and not in any other way on
previous decisions.
DP models can be classiﬁed as deterministic and stochastic, depending on the type of
data that are available to solve a problem. Obviously, if the data are known for a particular
situation, a deterministic DP model will be used to ﬁnd the best solution of the problem. If
some of the data are probabilistic, then a stochastic DP model will be developed to optimize
an expected value. Below, ﬁve applications of deterministic DP and two applications of
stochastic DP are presented in Sections 7.2 and 7.3, respectively.
7.2
Deterministic Dynamic Programming Models
7.2.1
Capacity Expansion Problem [11,13]
A supplier of electronic components for the automotive industry has decided to expand its
capacity by building nine new manufacturing facilities closer to customer locations in the
next 4 years. A builder has made an interesting oﬀer to build the facilities. The annual

7-4
Operations Research Methodologies
TABLE 7.1
Annual Facility Requirements
and Costs (in Millions of Dollars)
Year
Required
Fixed Cost
Variable Cost
(i)
Facilities (ri)
(ai)
(bi)
1
2
1.2
4.3
2
4
1
4.7
3
7
1.4
4.4
4
9
1.2
5.1
construction cost will consist of a ﬁxed cost plus a variable cost per facility to be built. The
ﬁxed cost does not have to be paid if no facilities are built in a given year. The maximum
number of facilities that can be built in a single year is four. Table 7.1 shows the desired
number of facilities that need to be ﬁnished by the end of each year along with the annual
ﬁxed and variable costs.
DP Formulation and Solution
This example can be formulated as a four-stage DP model where the stages correspond to
the four planning years. The construction cost in year i, deﬁned as ci(xi), is a function of
the number of facilities xi to be built during the year:
ci(xi) =

0,
if xi = 0,
ai + bixi,
if xi = 1, . . ., 4
At the beginning of a year (stage), the state of the system can be speciﬁed by the total
number of facilities built in the prior years or the number of remaining facilities to be
built. In the proposed model, the state of the system, si, is characterized by the number
of facilities already built. At each stage i, it is possible to determine the range of feasible
states. For example, for i = 3, s3 ∈{4, . . ., 8}, because 4 is the minimum requirement of
completed facilities by the end of year 2 and 8 is the most that can be built in 2 years.
For i = 4, s4 ∈{7, 8, 9}. The lower bound 7 is the requirement for year 3 and 9 is the overall
requirement for the entire planning horizon (4 years). The transformation function computes
the number of facilities by the beginning of year i + 1 as a function of the number of facilities
at the beginning of year i and the facilities built during the year:
si+1 = si + xi
The optimal value function, fi(si), is deﬁned as the total minimum cost for years i to 4, given
that si facilities have already been built by the beginning of year i. Then, the recurrence
relation is as follows:
fi(si) =
min
xi=λi,...,ui{ci(xi) + fi+1(si + xi)},
si = ri−1, . . ., min {4(i −1), 9};
i = 1, . . ., 3
where λi = max {0, ri −si} and ui = max {4, 9 −si} deﬁnes the range of the decision vari-
able xi, based on the minimum number of required facilities by the end of year i, and the
annual construction capacity or the facility requirements at the end of the 4-year planning
horizon. Note that the lower bound on si is the minimum number of required facilities by
the end of year i −1, and the upper bound is the minimum of the maximum production
capacity in the ﬁrst i −1 years, 4(i −1), and the total number of facilities needed by the
end of year 4.
The numerical solution for this problem is provided by the tables below. Each table
summarizes the solutions of the various subproblems solved for all feasible values of the state

Dynamic Programming
7-5
variable in one particular stage. The second column of each table shows the computations
of the values of the optimal value function for each possible decision. For example, when
s3 = 6 in stage 3, ﬁve values of the decision variable are considered. The decision x3 = 0
is infeasible because at least seven facilities must be completed by the end of the year
(s3 + x3 = 6 + 0 < 7). The case x3 = 4 is non-optimal because only nine facilities are required
by the end of year 4 (s3 + x3 = 6 + 4 > 9). The other three decisions are feasible and the
corresponding function values need to be determined to select the optimal decision that
provides minimum construction cost for years 3 and 4. In this subproblem, the optimal
decision is x∗
3 = 3 and the corresponding optimal value function is
f3(6) = c3(3) + f4(6 + 3) = (1.4 + 3 × 4.4) + 0 = 14.6.
The last three columns of the table in stage i summarize the optimal solutions for the sub-
problems in that stage, including the optimal value function, f ∗
i (si), the optimal decision,
x∗
i , and the initial state at the next stage, si+1. The optimal value function is obtained by
selecting the minimum of the function values calculated in the second column. The minimum
value corresponds to a speciﬁc decision that is optimal for the subproblem. The last column
provides the next state that is determined by using the transformation function. It is impor-
tant to point out that the actual calculations in the tables can be performed very eﬃciently.
The function value of any cell in the second column of Tables 7.1 to 7.3 is the sum of two val-
ues, ci(xi) + fi+1(si −xi). The ﬁrst value is constant for all the cells in the same subcolumn
corresponding to decision xi. The second value comes from column fi+1(xi+1) in stage i + 1.
Stage 4: (Initialization)
c(x4)
s4
x4 = 0
x4 = 1
x4 = 2
f ∗
4 (s4)
x∗
4
s5
7
Infeasible
Infeasible
1.2 + 2 × 5.1 = 11.4
11.4
2
9
8
Infeasible
1.2 + 5.1 = 6.3
Non-optimal
6.3
1
9
9
0
Non-optimal
Non-optimal
0
0
9
Stage 3:
c3(x3) + f4(s3 + x3)
s3
x3 = 0
x3 = 1
x3 = 2
x3 = 3
x3 = 4
f3(s3)
x∗
3
s4
4
Infeasible
Infeasible
Infeasible
14.6 + 11.4 = 26.0
19.0 + 6.3 = 25.3
25.3
4
8
5
Infeasible
Infeasible
10.2 + 11.4 = 21.6
14.6 + 6.3 = 20.9
19.0 + 0 = 19.0
19.0
4
9
6
Infeasible
5.8 + 11.4 = 17.2
10.2 + 6.3 = 16.5
14.6 + 0 = 14.6
Non-optimal
14.6
3
9
7
0 + 11.4 = 11.4
5.8 + 6.3 = 12.1
10.2 + 0 = 10.2
Non-optimal
Non-optimal
10.2
2
9
8
0 + 6.3 = 6.3
5.8 + 0 = 5.8
Non-optimal
Non-optimal
Non-optimal
5.8
1
9
Stage 2:
c2(x2) + f3(s2 + x2)
s2
x2 = 0
x2 = 1
x2 = 2
x2 = 3
x2 = 4
f2(s2)
x∗
2
s3
2
Infeasible
Infeasible
10.4 + 25.3 = 35.7
15.1 + 19.0 = 34.1
19.8 + 14.6 = 34.4
34.1
3
5
3
Infeasible
5.7 + 25.3 = 31.0
10.4 + 19.0 = 29.4
15.1 + 14.6 = 29.7
19.8 + 10.2 = 30.0
29.4
2
5
4
0 + 25.3 = 25.3
5.7 + 19.0 = 24.7
10.4 + 14.6 = 25.0
15.1 + 10.2 = 25.3
19.8 + 5.8 = 25.6
24.7
1
5

7-6
Operations Research Methodologies
Stage 1:
c1(x1) + f2(s1 + x1)
s1
x1 = 0
x1 = 1
x1 = 2
x1 = 3
x2 = 4
f1(s1)
x∗
1
s2
0
Infeasible
Infeasible
9.8 + 34.1 = 43.9
14.1 + 29.4 = 43.5
18.4 + 24.7 = 43.1
43.1
4
4
Optimal Solution
The optimal solution, in this case unique, can be found by backtracking, starting at the last
solved stage (stage 1) and terminating at the stage solved initially (stage 4). Using the
transformation function, it is possible to determine the state at stage i + 1, si+1, from the
state in stage i, si, and the corresponding optimal decision, x∗
i . Details of this process are
provided below.
s1 = 0,
x∗
1(0) = 4 plants to be built in year 1,
s2 = t1(0, 2) = 0 + 4 = 4,
x∗
2(4) = 1 plant to be built in year 2,
s3 = t2(3, 3) = 4 + 1 = 5,
x∗
3(5) = 4 plants to be built in year 3,
s4 = t3(6, 0) = 5 + 4 = 9,
x∗
4(9) = 0 plants to be built in year 4.
f1(0) = $43.1 millions (minimum construction cost for all 4 years).
7.2.2
Capacity Expansion Problem with Discounting [5,11,13]
The solution of the capacity expansion problem discussed in the prior section gives equal
weight to the cost of a facility paid in year 1 and in year 4. However, in reality, the money
spent in year 4 can be invested in year 1 to earn interest for the following 3 years. This
situation can be resolved by either using the present values of all cost data within the original
DP model or extending the DP model to take into account the time value of money.
If money can be safely invested at an annual interest rate r, then future expenditures can
be invested at the same rate. The eﬀect of that is that capital grows by a factor of (1 + r)
each year. Similarly, assuming that all annual costs are always eﬀective at the beginning
of the corresponding year, a dollar spent next year has the same value as β dollars today,
where β = 1/(1 + r) is called the discount rate.
The capacity expansion problem with a discount factor β = 0.9 can be solved using the
original formulation with the discounted costs shown in Table 7.2. Note that the costs in
year i have been multiplied by βi−1 to determine their present value at the beginning of
stage 1. For example, the discounted ﬁxed cost in year 3 becomes 1.4 · 0.93−1 = 1.26 and the
corresponding variable cost is 4.4 · 0.93−1 = 3.56.
Alternatively, the original DP formulation can be extended by redeﬁning the optimal
value function and including β in the recurrence relation.
TABLE 7.2
Annual Facility Requirements
and Discounted Costs (in Millions of Dollars)
Year
Required
Fixed Cost
Variable Cost
(i)
Facilities (ri)
(ai)
(bi)
1
2
1.20
4.30
2
4
0.90
4.23
3
7
1.26
3.56
4
9
0.97
3.72

Dynamic Programming
7-7
Optimal value function (f i(si)): present value of the total minimum cost at the
beginning of year i for years i to 4 given that si facilities have already been built
by the beginning of year i.
Recurrence relation:
fi(si) =
min
xi=λi,...,ui{ci(xi) + β · fi+1(si + xi)}, si = ri−1, . . ., min {4(i −1), 9}; i = 1, . . ., 3
The optimal solution of the example in Section 7.2.1 with discounting is the following:
x∗
1(0) = 4 plants to be built in year 1,
x∗
2(4) = 0 plants to be built in year 2,
x∗
3(4) = 4 plants to be built in year 3,
x∗
4(8) = 1 plant to be built in year 4.
The present value of the optimal construction cost can be determined using the revised
recurrence relation that takes into account the time value of money:
f4(8) = c4(1) = 1.2 + 5.1 · 1 = 6.3,
f3(4) = c3(4) + β · f4(4 + 4) = (1.4 + 4.4 · 4) + 0.9 · 6.3 = 24.67,
f2(4) = c2(0) + β · f3(4 + 0) = 0 + 0.9 · 24.67 = 22.20,
f1(0) = c3(4) + β · f2(0 + 4) = (1.2 + 4.3 · 4) + 0.9 · 22.20 = 38.38.
Note that the optimal policy for the discounted model diﬀers from that in the original
model. While both policies require four facilities to be built in year 1 and four additional
facilities in year 3, the ninth facility is built in year 2 in the original policy and in year 4 in
the discounted policy. The present value of the total construction cost at the beginning of
year 1 for the optimal policy is $38.38 million.
7.2.3
Equipment Replacement Problem [3,8,10]
A company needs to own a certain type of machine for the next n years. As the machine
becomes older, annual operating costs increase so much that the machine must be replaced
by a new one. The price of a new machine in year i is a(i). The annual expenses for operating
a machine of age j is r(j). Whenever the machine is replaced, the company receives some
compensation for the old one, as trade-in value. Let t(j) be the trade-in value for a machine
that has age j. At the end of year n, the machine can be salvaged for v(j) dollars, where
j is the age of the machine. Given that at the beginning of the ﬁrst year the company
owns a machine of age y, we need to determine the replacement policy for the machine that
minimizes the total cost for the next n years, given that replacement decisions can only be
made at the beginning of each year. It is assumed that annual operating costs and trade-in
and salvage values are stationary and only depend on the age of the machine.
DP Formulation and Solution
The DP formulation for this problem involves n stages corresponding to the n planning
periods (years). At the beginning of stage i, the state of the system can be completely
deﬁned by the age of the machine that has been used in the prior year. At the beginning

7-8
Operations Research Methodologies
of a stage, two possible decisions can be made: keep the current machine or replace the
machine. If we decide to keep the machine, then the only cost in the current stage will be
the cost of operating the machine. If the decision is to replace the machine, the annual cost
will include the price of a new machine, minus the trade-in value received for the current
machine, plus the operating cost of a new machine. The cost in stage i, ci(j), depends on
the age of the current machine and the decision:
ci(j) =

a(i) −t(j) + r(0),
if we buy a new machine
r(j),
if we keep the current machine
The remaining DP formulation is provided below.
Optimal value function (f i(si)): minimum cost of owning a machine from the begin-
ning of year i to the end of year n (or beginning of year n + 1), starting year i
with a machine that just turned age si.
Optimal policy (pi(si)): “buy” or “keep.”
Transformation function (ti(si, pi)): this function shows how the state for the
next stage changes based on the current state, stage, and decision.
si+1 = ti(si, pi) =
1,
if pi = “buy”
si + 1,
if pi = “keep”
Recurrence relation:
fi(si) = min
a(i) −t(si) −r(0) + fi+1(1),
if pi = “buy”
r(si) + fi+1(si + 1),
if pi = “keep” si = 1, 2, . . ., i −1, y + i −1
Boundary conditions: fn+1(si) = −v(si),
sn = 1, 2, . . ., n −1, y + n −1
Answer: f1(y).
Example 7.1
This DP formulation is illustrated by solving a numerical problem for a 4-year planning
horizon. The age of the machine at the beginning of the ﬁrst year is 2. The cost of a
new machine in year 1 is $58,000 and increases by $2,000 every year. Annual operat-
ing costs along with trade-in and salvage values are time independent and provided in
Table 7.3.
TABLE 7.3
Cost Data (in Thousands) for the Equipment
Replacement Example
Machine Age
Annual Operating
Trade-in Value
Salvage Value
(si) in Years
Cost (ri(si))
(ti(si))
(vi(si))
0
12
–
–
1
15
35
30
2
25
25
20
3
35
15
10
4
60
10
5
5
80
5
0
6
100
0
0

Dynamic Programming
7-9
Solution 7.1
f5(1) = −30, f5(2) = −20, f5(3) = −10, f5(4) = −5, f5(6) = 0
f4(1) = min
(58 + 6) −35 + 12 + (−30)
15 + (−20)

= −5, p4(1) = keep
f4(2) = min
(58 + 6) −25 + 12 + (−30)
25 + (−10)

= 15, p4(2) = keep
f4(3) = min
(58 + 6) −15 + 12 + (−30)
35 + (−5)

= 30, p4(3) = buy
f4(5) = min
(58 + 6) −5 + 12 + (−30)
80 + 0

= 41, p4(5) = buy
f3(1) = min
(58 + 4) −35 + 12 + (−5)
15 + 15

= 30, p3(1) = keep
f3(2) = min
(58 + 4) −25 + 12 + (−5)
25 + 30

= 44, p3(2) = buy
f3(4) = min
(58 + 4) −15 + 12 + (−5)
60 + 30

= 54, p3(4) = buy
f2(1) = min
(58 + 2) −35 + 12 + 30
15 + 44

= 59, p2(1) = keep
f2(3) = min
(58 + 2) −15 + 12 + 54
35 + 54

= 99, p2(3) = keep
f1(2) = min
58 −25 + 12 + 59
25 + 99

= 104, p1(2) = buy
The feasible states for stage 5 are 1 to 4, and 6. Note that, if the original machine is not
replaced at all, it will have age 6 by the beginning of year 5, and if the machine is replaced
at least one time, it will be at most 4 years old by the beginning of year 5. Thus, state 5 is
infeasible.
The total minimum cost for the 4-year planning horizon is $104,000 based on an optimal
replacement policy that includes buying a new machine at the beginning of years 1, 3, and 4.
7.2.4
Simple Production Problem
A company needs to produce at least d units of a product during the next n periods. The
production cost in period i is a quadratic function of the quantity produced. If xi units are
produced in period i, the production cost is ci(xi) = wi x2
i , where wi is a known positive
coeﬃcient, i = 1, . . ., n. The objective of this problem is to determine the optimal production
quantities that minimize the total cost for the n periods. Note that the production quantities
are not restricted to be integer. This problem can be formulated as a nonlinear program
with a quadratic objective function and a linear constraint:
Minimize z =
n

i=1
wix2
i
subject to
n

i=1
xi ≥d
xi ≥0,
i = 1, . . ., n

7-10
Operations Research Methodologies
This model illustrates a situation in which decision variables are continuous. Thus,
subproblems cannot be solved by checking all feasible solutions as the feasible region is
not ﬁnite. In general, when decision variables are continuous, an optimization procedure
must be used to ﬁnd an optimal solution. In this particular problem, the single-variable
subproblems can be solved by setting the ﬁrst derivative of the optimal value function to
zero and solving the resulting linear equation.
DP Formulation and Solution
The proposed DP model is based on a forward formulation in the sense that the boundary
conditions are given for stage 1 and the recurrent process moves from any stage i to stage
i + 1, i = 1, . . ., n −1, where stages match the production periods. The state of the system
at a given stage i, si, is deﬁned by the number of units produced in the ﬁrst i periods. The
DP formulation is presented below.
Optimal value function (f i(si)): minimum production cost for the ﬁrst i periods
given that si units are produced in these periods.
Optimal policy (pi(si) = x ∗
i ): units produced in period i.
Transformation function (ti(si, x i)): it ﬁnds the number of units produced by the
end of period i −1 as a function of the number of units produced in the ﬁrst i
periods and the number of units produced in period i.
si−1 = ti(si, xi) = si −xi
Recurrence relation:
fi(si) =
min
0≤xi≤si{wi x2
i + fi−1(si −xi)},
0 ≤si ≤d;
i = 1, . . ., n −1
Boundary conditions:
f1(s1) = w1x2
1,
where x1 = s1, 0 ≤si ≤d
Answer: fn(d).
The optimal value function in stage 1 is given in closed-form; that is, f1(s1) = w1s2
1. This
function can be used as an input to stage 2 to determine the corresponding optimal value
function in closed-form:
f2(s2) =
min
0≤x2≤s2{w2x2
2 + f1(s2 −x2)} =
min
0≤x2≤s2{w2x2
2 + w1(s2 −x2)2}
Figure 7.2 shows the single-variable quadratic function w2 x2
2 + w1(s2 −x2)2, which is
convex. The convexity of the function is proved below by showing that the second derivative
is positive.
∂{w2 x2
2 + w1(s2 −x2)2}
∂x2
= 2x2 (w1 + w2) −2 s2w1
∂2{w2 x2
2 + w1(s2 −x2)2}
∂x2
2
= 2(w1 + w2) > 0,
since w1, w2 > 0
The unbounded minimum is determined by setting the ﬁrst derivative to zero. If this mini-
mum falls between bounds, 0 ≤x2 ≤s2, it is feasible and solves the subproblem; otherwise,
the minimum in the feasible range would be one of the two boundary points.
2 x2(w1 + w2) −2 s2 w1 = 0 ⇒x2 =
w1
w1 + w2
s2 =
1
w2
1
w1
+ 1
w2
s2

Dynamic Programming
7-11
∗
x2
s2
w2 x2
2  w1 (s2  x2)2
FIGURE 7.2
Function to be minimized in stage 2.
As the above solution is feasible (between bounds), it solves the subproblem in stage 2.
This solution can be incorporated into the recurrence relation for stage 2 to compute the
following optimal value function in closed-form:
f2(s2) =
s2
2
1
w1
+ 1
w2
THEOREM 7.1
In general, the optimal policy and value function for the simple produc-
tion problem is given by the following equations:
xi =
1
wi
i
j=1
1
wj
si
and
fi(si) =
s2
i
i
j=1
1
wj
,
i = 1, . . ., n −1
PROOF 7.1
(By induction) Since the result has already been proved for stages 1 and 2,
now we only need to show that, if it is true for stage i, it must be true for stage i + 1. Note
that in stage i + 1,
fi+1(si+1) =
min
0≤xi+1≤si+1{wi+1x2
i+1 + fi(si+1 −xi+1)}
Now, the term fi(si+1 −xi+1) can be replaced by its closed-form expression, which is
assumed to be correct in stage i.
fi+1(si+1) =
min
0≤xi+1≤si+1
⎧
⎪
⎪
⎨
⎪
⎪
⎩
wi+1 x2
i+1 + (si+1 −xi+1)2
i
j=1
1
wj
⎫
⎪
⎪
⎬
⎪
⎪
⎭

7-12
Operations Research Methodologies
The minimum is obtained by setting the ﬁrst derivative to zero.
∂
∂xi+1
⎧
⎪
⎪
⎨
⎪
⎪
⎩
wi+1 x2
i+1 + (si+1 −xi+1)2
i
j=1
1
wj
⎫
⎪
⎪
⎬
⎪
⎪
⎭
=
2
i
j=1
1
wj
⎡
⎣
⎛
⎝wi+1
i

j=1
1
wj
+ 1
⎞
⎠xi+1 −si+1
⎤
⎦= 0
⇒xi+1 =
1
wi+1
i+1
j=1
1
wj
si+1
which proves the closed-form solution for the optimal policy. This value is the unique mini-
mum because the second derivative can be shown to be positive. Now, by replacing this
expression for xi+1 into fi+1(si+1), the closed-form solution for the optimal value function
can be derived:
fi+1(si+1) = wi+1
⎛
⎜
⎜
⎝
1
wi+1
i+1
j=1
1
wj
si+1
⎞
⎟
⎟
⎠
2
+
⎛
⎜
⎜
⎝si+1 −
1
wi+1
i+1
j=1
1
wj
si+1
⎞
⎟
⎟
⎠
2
i
j=1
1
wj
which can be simpliﬁed to
fi+1(si+1) =
s2
i+1
i+1
j=1
1
wj
■
In this application, DP has been used to derive a simple closed-form solution for the problem.
Below, the closed-form solution is applied to solve a numerical example.
Example 7.2
In this example, we solve a three-period production planning problem for a total demand
of 9 units. The coeﬃcients of the objective function are: w1 = 2, w2 = 3, and w3 = 6.
Solution 7.2
The closed-form solution can be applied starting at stage 3 and going backwards to stages
2 and 1.
Stage i = 3:
s3 = d = 9 ⇒x∗
3 =
1
6 · 9
1
2 + 1
3 + 1
6
= 1.5
Stage i = 2:
s2 = 9 −1.5 = 7.5 ⇒x∗
2
=
1
3 · 7.5
1
2 + 1
3
= 3

Dynamic Programming
7-13
Stage i = 1:
s1 = 7.5 −3 = 4.5 ⇒x∗
1 = 4.5
f3(9) =
92
1
2 + 1
3 + 1
6
= 81
7.2.5
Dynamic Inventory Planning Problem [11,15]
One of the important applications of DP for many decades has been in the area of inventory
planning and control over a ﬁnite number of time periods, where demand is known but may
change from period to period. Let ri be the demand in period i, i = 1, . . . , n. At the beginning
of each period the company needs to decide the number of units to be ordered to an external
supplier or produced in the manufacturing ﬂoor in that period. If an order in period i is sub-
mitted, the ordering cost consist of a ﬁxed setup cost, Ki, and a unit variable cost, ci, which
must be multiplied by the order quantity. Inventory shortages are not allowed. If inventory
is carried from period i to period i + 1, a holding cost is incurred in that period. The holding
cost in period i is determined by multiplying the unit holding cost, hi, by the number of units
of inventory by the end of period i, which are carried from period i to period i + 1. In the
following DP model the initial inventory at the beginning of period 1 and the ﬁnal inventory
at the end of period n are assumed to be zero. If this is not the case, the original problem can
be slightly modiﬁed by subtracting the initial inventory in period 1 from the demand in that
period, and adding the ﬁnal required inventory in period n to the demand of that period.
The proposed model belongs to the class of periodic review models, because the company
reviews the inventory level at the beginning of each period, which can be a week or a month,
and then the ordering decision is made. This model is an alternative to the continuous review
model in which the company keeps track of the inventory level at all times and an order
can be submitted at any time.
DP Formulation and Solution
Similar to the model in Section 7.2.3, the stages of this DP formulation correspond to the
n production periods. The cost at any stage i, i = 1, 2, . . . , n, is a function of the inventory
on hand at the beginning of the stage before ordering, xi, and the quantity ordered in that
stage, zi:
ci(xi, zi) =
hi(xi −ri),
if zi = 0
Ki + ci zi + hi(xi + zi −ri),
if zi > 0
Note that to avoid shortages, zi ≥max{0, ri −xi}, i = 1, 2, . . ., n, which means that the
initial inventory plus the order quantity must be greater than or equal to the demand in
period i.
The DP approach presented here takes advantage of the following optimality condition,
which is also known as the zero inventory ordering property (Wagner and Whitin, 1957):
“For an arbitrary demand requirement and concave costs (e.g., a ﬁxed setup cost, and linear
production and holding costs), there exists an optimal policy that orders (or produces) only
when the inventory level is zero.” This condition implies that x∗
i · z∗
i = 0, i, = 1, 2, . . ., n.
Based on this property, the only order quantities that need to be considered in each stage
(period) i are either 0, ri, ri + ri+1, . . ., ri + ri+1 + · · · + rn. By taking advantage of this
property, a simple DP model can be developed with only one state in each stage, assuming
that the initial inventory on hand, before ordering, is zero. Thus, given that an order must

7-14
Operations Research Methodologies
be submitted in stage i, the order quantity must correspond to the sum of demands of
a certain number of periods ahead, ri + ri+1 + · · · + rj, where j is the last period whose
demand will be served by this order. Therefore, j ∈{i, i + 1, . . ., n} is the decision variable.
An eﬃcient DP formulation is provided below.
Optimal value function (f i): the total cost of the best policy from the beginning of
period i to the end of period n, given that the inventory on hand, before ordering,
in period i is zero.
Optimal policy (pi = j ∗): given that the inventory on hand is zero, the optimal
policy indicates the last period to be served from the order issued in stage i.
Transformation function (ti(j ) = j + 1): it shows the next stage j + 1 in which an
order will be issued, given that a production order is submitted in stage i.
Recurrence relation: The general recurrence relation is
fi =
min
j=i,i+1,...,n{Ki + ci(ri + ri+1 + · · · + rj) + hi(ri+1 + · · · + rj)
+ hi+1(ri+2 + · · · + rj) + · · · + hj−1rj + fj+1},
i = 1, . . ., n
If the cost parameters are stationary, that is, Ki = K, ci = c, hi = h, i = 1, . . ., n,
the recurrence relation can be slightly simpliﬁed as follows:
fi =
min
j=i,i+1,...,n{K + c (ri + ri+1 + · · · + rj) + h [ri+1 + 2ri+2 + 3ri+3 + · · ·
+ (j −i)rj] + fj+1},
i = 1, . . ., n
Boundary conditions: fn+1 = 0.
Answer: f1.
Example 7.3
In this example, we consider a ﬁve-period dynamic inventory problem with stationary cost
data: K = $40, c = $10/unit, and h = $3/unit/period. The known demand for each period is
the following: r1 = 2 units, r2 = 4 units, r3 = 2 units, r4 = 2 units, and r5 = 3 units.
Solution 7.3
Stage 6: (Initialization)
f6 = 0
Stage 5: (r5 = 3)
j
K + cr5 + f6
f5
j∗
5
40 + 10 × 3 + 0 = 70
70
5
Stage 4: (r4 = 2)
j
K + c (r4 + · · · + rj) + h[r5 + · · · + (j −4)rj] + fj+1
f4
j∗
4
40 + 10 × 2 + 0 + 70 = 130
5
40 + 10 × (2 + 3) + 3 × 3 + 0 = 99
99
5

Dynamic Programming
7-15
Stage 3: (r3 = 2)
j
K + c (r3 + r4 + · · · + rj) + h[r4 + 2r5 + · · · + (j −3)rj] + fj+1
f3
j∗
3
40 + 10 × 2 + 0 + 99 = 159
4
40 + 10 × (2 + 2) + 3 × 2 + 70 = 156
134
5
5
40 + 10 × (2 + 2 + 3) + 3 × (2 + 2 × 3) + 0 = 134
Stage 2: (r2 = 4)
j
K + c (r2 + r3 + · · · + rj) + h[r3 + 2r4 + · · · + (j −2)rj] + fj+1
f2
j∗
2
40 + 10 × 4 + 0 + 134 = 214
3
40 + 10 × (4 + 2) + 3 × 2 + 99 = 205
195
5
4
40 + 10 × (4 + 2 + 2) + 3 × (2 + 2 × 2) + 70 = 208
5
40 + 10 × (4 + 2 + 2 + 3) + 3 × (2 + 2 × 2 + 3 + 3) + 0 = 195
Stage 1: (r1 = 2)
j
K + c (r1 + r2 + · · · + rj) + h[r2 + 2r3 + · · · + (j −1)rj] + fj+1
f1
j∗
1
40 + 10 × 2 + 0 + 195 = 255
2
40 + 10 × (2 + 4) + 3 × 4 + 134 = 246
3
40 + 10 × (2 + 4 + 2) + 3 × (4 + 2 × 2) + 99 = 243
243
3
4
40 + 10 × (2 + 4 + 2 + 2) + 3 × (4 + 2 × 2 + 3 + 2) + 70 = 252
5
40 + 10 × (2 + 4 + 2 + 2 + 3) + 3 × (4 + 2 × 2 + 3 + 2 + 4 × 3) + 0 = 248
Solution 7.4
Minimum Total Cost: f1 = $243.
Stage 1:
j∗= 3 ⇒Order r1 + r2 + r3 = 2 + 4 + 2 = 8 units in period 1.
Stage 4:
j∗= 5 ⇒Order r4 + r5 = 2 + 3 = 5 units in period 4.
Graphical Solution
A directed graph is constructed with n + 1 nodes. Each node i, i = 1, 2, . . ., n + 1, represents
the beginning of a production period with zero units of inventory on hand. For each node i,
an arc (i, j) is added to the graph, for j = i + 1, . . ., n + 1. Arc (i, j + 1) represents the case
where the inventory on hand at the beginning of period i is zero and an order of ri + · · · + rj
units is submitted in that period, so that at the beginning of period j + 1 the inventory level
will become zero again. The cost associated with arc (i, j + 1), denoted as ci,j+1, represents
the total ordering and holding cost for periods i to j for this case:
ci,j+1 = Ki + ci(ri + ri+1 + · · · + rj) + hi(ri+1 + · · · + rj)
+ hi+1(ri+2 + · · · + rj) + · · · + hj−1rj
If the cost parameters are stationary, that is, Ki = K, ci = c, hi = h, i = 1, . . ., n, the arc cost
ci,j+1 becomes
ci,j+1 = K + c(ri + ri+1 + · · · + rj) + h[ri+1 + 2ri+2 + 3ri+3 + · · · + (j −i)rj]

7-16
Operations Research Methodologies
f1 = 243
f2 = 195
f6 = 0
(40 + 70) + 24 = 134
6
5
4
3
2
1
(40 + 60) + 12 = 112
(40 + 40) + 6 = 86 
(40 + 50) + 9 = 99
(40 + 130) + 78 = 248
(40 + 80) + 24 = 144   
40 + 20 = 60
40 + 40 = 80
40 + 20 = 60
40 + 20 = 60
40 + 30 =70
f3 = 134
f4 = 99
f5 = 70 
(40 +100) + 42 = 182
(40 + 110) + 45 = 195
(40 + 60) + 6 = 106
(40 + 80) + 18 = 138
FIGURE 7.3
Graphical solution for the dynamic inventory example.
The optimal inventory policy is determined by ﬁnding the shortest path from node 1 to
node n + 1.
Figure 7.3 shows the 6-node graph for the example problem. The arc costs have been
computed using the formula for the stationary cost parameters. Detailed calculations of
two arc costs are shown below.
c1,6 = K + c(r1 + r2 + · · · + r5) + h[r2 + 2 r3 + · · · + 4r5]
= 40 + 10 × (2 + 4 + 2 + 2 + 3) + 3 × (4 + 2 × 2 + 3 + 2 + 4 × 3) + 0 = 248
c2,4 = K + c(r2 + r3) + hr3 = 40 + 10 × (4 + 2 + 2) + 3 × (2 + 2 × 2) + 7 = 208
Once all arc costs are calculated, the shortest path from node 1 to node 6 can be found in a
straightforward manner as the graph is acyclic. The computational eﬀort required to ﬁnd the
shortest path is exactly the same than the eﬀort required to solve all DP stages. Initially, the
optimal value function at stage 6 is set to zero, f6 = 0. At stage 5, the optimal value function
(length of the shortest path from node (stage) 5 to node 6) is f5 = c5,6 + f6 = 70 + 0 = 70.
The remaining optimal value functions can be computed recursively with the following
simpliﬁed recurrence relation:
fi =
min
j=i,i+1,...,n{ci,j + fj+1}
7.2.6
Reliability System Design [11]
Figure 7.4 shows an illustration of an electromechanical device that contains three compo-
nents in serial arrangement so that each component must work for the system to function.
The reliability of the system can be improved by installing several parallel units in one or
more of the components. Table 7.4 shows the reliability of each component as a function of
the number of parallel units.
The reliability of the device is the product of the reliabilities of the three components.
The cost of installing one, two, or three parallel units in each component (in thousands of
dollars) is given in Table 7.5.

Dynamic Programming
7-17
Component 1
Component 2
Component 3
FIGURE 7.4
Illustration of the electromechanical system.
TABLE 7.4
Reliability for Each Component
Probability of Functioning pi(xi)
Parallel
Component
Component
Component
Units (xi)
i = 1
i = 2
i = 3
1
0.6
0.7
0.8
2
0.7
0.8
0.9
3
0.9
0.9
0.95
TABLE 7.5
Cost of Each Component
(in Thousands of Dollars)
Cost of the Component ci(xi)
Parallel
Component
Component
Component
Units (xi)
i = 1
i = 2
i = 3
1
3
2
1
2
5
4
4
3
6
5
5
Because of budget limitations, a maximum of $10,000 can be expended. We need to
determine the number of units for each component so that the reliability of the system is
maximized.
DP Formulation and Solution
This problem can be formulated as a three-stage DP model. At each stage i, a decision has
to be made concerning the number of parallel units, xi, to be assigned to the ith component.
This decision depends on the remaining budget available for stages i to 3. Thus, the state
of the system, si, is the remaining budget available at the beginning of that stage (in
thousands of dollars). At each stage i, it is necessary to determine a range of feasible states.
For example, for i = 3, s3 ∈{1, . . ., 5}, because 1 is the cost of one unit of component 3
and 5 is the maximum available budget, given that at least one unit of component 1 and
one unit of component 2 have to be bought, i.e., 10 −(3 + 2) = 5. Similar arguments can be
provided to establish the range of s2: s2 ∈{3, . . ., 7}. The transformation function computes
the budget available by the beginning of state i + 1 as a function of the budget available at
the beginning of stage i and the number of units assigned to component i:
si+1 = ti(si, xi) = si −ci(xi)
where ci(xi) is the cost of xi units of component i (see Table 7.5).
The optimal value function, fi(si), gives the maximum probability of functioning of a
subsystem deﬁned by components i = 1, 2, 3 in serial arrangement given that the budget
available for these components is si. Based on this deﬁnition, the recurrent relation can be
established as follows:
fi(si) =
max
xi=1,2,3{pi(xi) · fi+1(si −ci(xi))},
si = li, . . ., ui;
i = 1, 2

7-18
Operations Research Methodologies
where li and ui are the lower and upper bounds on si, and pi(xi) is the reliability of
component i with xi units in parallel. The boundary conditions in stage 3 are deﬁned as
follows:
f3(s3) =
max
xi=1,2,3{p3(x3) : c3(x3) ≤s3}
In this problem, f1(10) gives the maximum system reliability that can be obtained with
a budget of $10,000. The numerical solution for this problem is provided in the following
tables.
Stage 3:
p3(x3)
s3
x3 = 1
x3 = 2
x3 = 3
f ∗
3 (s3)
x∗
3
s4
1
0.8
Infeasible
Infeasible
0.8
1
0
2
0.8
Infeasible
Infeasible
0.8
1
1
3
0.8
Infeasible
Infeasible
0.8
1
2
4
Non-optimal
0.9
Infeasible
0.9
2
0
5
Non-optimal
Non-optimal
0.95
0.95
3
0
Stage 2:
p2(x2) · f3(s2 −c2(x2))
s2
x2 = 1
x2 = 2
x2 = 3
f ∗
2 (s2)
x∗
2
s3
3
0.7 × 0.8 = 0.56
Infeasible
Infeasible
0.56
1
1
4
0.7 × 0.8 = 0.56
Infeasible
Infeasible
0.56
1
2
5
0.7 × 0.8 = 0.56
0.8 × 0.8 = 0.64
Infeasible
0.64
2
1
6
0.7 × 0.9 = 0.63
0.8 × 0.8 = 0.64
Infeasible
0.64
2
2
7
0.7 × 0.95 = 0.665
0.8 × 0.8 = 0.64
0.9 × 0.8 = 0.72
0.72
3
2
Stage 1:
p1(x1) · f2(s1 −c1(x1))
s1
x1 = 1
x1 = 2
x1 = 3
f ∗
1 (s1)
x∗
1
s2
10
0.6 × 0.72 = 0.432
0.7 × 0.64 = 0.448
0.9 × 0.56 = 0.504
0.504
3
4
Optimal Solution
The optimal solution is unique and can be obtained by backtracking, starting at stage 1
and moving to stages 2 and 3. Using the transformation function, it is possible to determine
the state at stage i + 1, si+1, from the state in stage i, si, and the corresponding optimal
decision, x∗
i . Details of this process are provided below.
s1 = 10,
x∗
1(10) = 3 units of component 1 are used
s2 = t1(s1, x1) = 10 −6 = 4,
x∗
2(4) = 1 unit of component 2 is used
s3 = t2(s2, x2) = 4 −2 = 2,
x∗
3(1) = 1 unit of component 3 is used
f ∗
1 (10) = 0.504 (system reliability)

Dynamic Programming
7-19
7.3
Stochastic Dynamic Programming Models
Decision makers generally face situations in which they need to make decisions with many
uncertainties concerning the consequences of these decisions. A manufacturer may produce
items for inventory without knowing how many units customers will demand for each type
of item. An investor may decide to buy a certain number of shares of a particular stock
without knowing if the stock will go up or down in the future. Although the future may be
uncertain, in general, it is still possible to estimate probability distributions for all possible
outcomes associated with a decision [4,7,9,12].
All the DP applications discussed in the previous section have considered situations in
which the consequences of all possible decisions were completely predetermined, including
the system reliability problem in Section 7.2.6. Although the objective of that problem was
to maximize the expected system reliability, it can be categorized as deterministic in the
sense that, when a decision is made at any stage, the state resulting from that decision is
precisely determined and known. The essential diﬀerence between the stochastic models to
be discussed in this section and the prior deterministic models is that the state resulting from
a decision is not predetermined, but can be described by a known probability distribution
function that depends on the initial state and the decision taken.
In stochastic DP, let the pair (i, j) denote state j of stage i and D(i, j) denote the set
of all possible decisions associated with (i, j). If the system is observed to be in (i, j), some
decision x in D(i, j) must be selected. Assuming that stage i is not the last stage n, for
example, i < n, decision x will cause a transition to some state k in stage i + 1; the particular
state k can be characterized by the following transition probability:
P i
jk(x) = probability that the state observed in stage i + 1will be k, given that the
current state in stage i is j and decision x is made.
Note that, for a particular pair (i, j) and a speciﬁc decision x ∈D(i, j), 
k P i
jk(x) = 1 and
P i
jk(x) ≥0, for any state k in the state space. Two applications of stochastic DP using this
type of probabilities are presented in this section.
Note that, in stochastic DP, a diﬀerent set of states may occur for diﬀerent replications of
the same problem even though the same policy is applied. The simple explanation is that,
for a given stage, state, and decision, the resulting state at the next stage is only known by
a probability distribution. Therefore, in stochastic DP, an optimal policy is characterized
by the best decision for each possible state at any stage.
7.3.1
Stochastic Equipment Replacement Problem [3,8,10]
First, we present an extension of the equipment replacement problem discussed in Sec-
tion 7.2.3. Now, we assume that the annual operating cost of a machine of age j is a random
variable with known probability distribution. In addition, it is assumed that the machine
may suﬀer a severe failure at the end of any year and must be replaced. The probability
of a severe failure only depends on the age of the machine at the beginning of the year.
Finally, at the beginning of a year, we allow the option of salvaging the currently owned
machine and leasing a new machine for a year. If the company has already leased a machine
during the prior year, it can lease another machine or buy a new one at the beginning of
the year. Assuming that the company owns a machine of age y by the beginning of the
ﬁrst year, and replacement or leasing decisions can only be made at the beginning of each

7-20
Operations Research Methodologies
year, we need to ﬁnd the optimal replacement/leasing policy. The input data deﬁning this
problem are:
n = number of years to be considered
y = age of the machine at the beginning of year 1
a(i) = purchasing price of a new machine at the beginning of year i
r(j) = expected annual operating cost for a machine of age j at the beginning of
the year
tw(j) = trade-in value for a machine of age j that is in working condition
vw(j) = salvage value for a machine of age j that is in working condition
tb(j) = trade-in value for a machine of age j that is broken down
vb(j) = salvage value for a machine of age j that is broken down
q(j) = probability that a machine of age j in working condition at the beginning of
a year breaks down by the end of the year
λ = annual cost for leasing a machine
DP Formulation and Solution
In this problem, it is assumed that the company does not have to pay anything if a leased
machine breaks down by the end of a year as the leasing cost includes insurance for the
machine.
The following DP formulation uses two optimal value functions. One function provides
the value of the best policy for years i to n, given that the company owns a machine at
the beginning of year i. The second function gives the value of the best policy if a machine
was leased in the prior year. In the case of the ﬁrst function, a number of states need to
be considered depending on the age of the owned machine at the beginning of year i. If the
machine was leased, only one state exists because the company does not currently own any
machine. In the two recurrence relations stated below, some states require two decisions.
The ﬁrst decision is applied at the beginning of year i and the second decision is taken only
if the company decides to own a machine during the current year and the machine incurs
a severe failure by the end of the year. If the decision taken at the beginning of the year is
to lease a machine, then the second decision is unnecessary. The following notation is used
to represent the decision making process:
B: “buy a new machine”
K: “ keep the current machine”
and
L: “lease a new machine for one year”
Stage (i): year i, i = 1, . . ., n.
State: if the company owned a machine in year i −1, then the state of the system at
the beginning of year i, denoted as si, is the age of the machine; if a machine
was leased in year i −1, the only state is that the company does not own any
machine at the beginning of year i.
Decision variable (x i = (x i1, x i2)): two decisions may have to be taken in each
state. The ﬁrst decision at the beginning of year i is xi1 ∈{“buy,” “keep,” “lease”},

Dynamic Programming
7-21
obviously the “keep” option is only available if the company owns a machine. The
second decision may be necessary at the end of stage i, if an owned machine breaks
down at the end of the year, xi2 ∈{“buy,” “lease”}.
Optimal value functions: the following two functions need to be evaluated in each
stage:
fi(si) = minimum expected cost from the beginning of year i to the beginning of
year n + 1, given that the company owns a working machine of age si at
the beginning of year i.
gi = minimum expected cost from the beginning of year i to the beginning
of year n + 1, given that the company does not own any machine at the
beginning of year i as a machine was leased in the preceding year.
Optimal policy (pf(i,si) or pg(i) = x ∗
i ): optimal
replacement/leasing
plan
for
year i.
Recurrence relation: For i = n −1, . . ., 2, 1; si = 1, 2, . . ., i, y + (i −1)
fi(si) = min
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
B:
p(i) −tw(si) + r(0) + (1 −q(0)) · fi+1(1) + q(0)
· min
⎧
⎨
⎩
B:
p(i + 1) −tb(1) + fi+1(0)
L:
λ −vb(1) + r(0) + gi+2
K:
r(si) + (1 −q(si)) · fi+1(si + 1) + q(si)
· min
⎧
⎨
⎩
B:
p(i + 1) −tb(si + 1) + fi+1(0)
L:
λ −vb(si + 1) + r(0) + gi+2
L:
λ −vw(si) + r(0) + gi+1
For i = n −1, . . ., 2, 1; si = 0,
fi(0) = K:
r(0) + (1 −q(0)) · fi+1(1) + q(0) · min
⎧
⎨
⎩
B:
p(i + 1) −tb(1) + fi+1(0)
L:
λ −vb(1) + r(0) + gi+2
For i = n −1, . . ., 2, 1,
gi = min
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
B:
p(i) + r(0) + (1 −q(0)) · fi+1(1) + q(0)
· min
⎧
⎨
⎩
B:
p(i + 1) −tb(1) + fi+1(0)
L:
λ −vb(1) + r(0) + gi+2
L:
λ + r(0) + gi+1
Boundary conditions: For si = 1, 2, . . ., n, y + (n −1),
fn(si) = min
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
B:
p(n) −tw(si) + r(0) + (1 −q(0)) · vw(1) + q(0) · vb(1)
K:
r(si) + (1 −q(si)) · vw(si + 1) + q(si) · vb(si + 1)
L:
λ −vw(si) + r(0)

7-22
Operations Research Methodologies
gn = min
⎧
⎨
⎩
B:
p(n) + r(0) + (1 −q(0)) · vw(1) + q(0) · vb(1)
L:
λ + r(0)
gn+1 = 0 (This case may be used in the computation of fn−1(si) and gn−1.)
Answer: f1(y).
7.3.2
Investment Planning [11]
We have $10,000 to invest in the following 3 years. Investments can only be made for 1 year
at the beginning of any year in multiples of $10,000. Two types of investment opportunities
are available: A and B. Investment B is more conservative than investment A. If we invest
in A in a given year, at the end of the year the money will be lost with probability 0.3 or
will be doubled with probability 0.7. If the investment is made in B, at the end of the year
the money will be returned with probability 0.9 or will be doubled with probability 0.1. The
objective of this problem is to come up with an investment policy that maximizes the total
expected return by the end of the third year. In order to simplify the solution procedure, it
is assumed that only one type of investment can be selected in a given year.
DP Formulation and Solution
In the following formulation, the set of possible decisions depends on the amount of money
available at the beginning of a year. If we have less than $10,000, the only possibility is to
keep the money, but if the money available for investment is $10,000 or more, multiples of
$10,000 can be invested in A or B.
Stage (i): year i, i = 1, 2, 3.
State (si): amount of money on hand at the beginning of year i. In year 1, s1 = $10,000.
Decision variable (x i = (x i1,x i2)): the decision in stage i consists of the amount of
money to invest, xi1, in multiples of $10,000, and the type of investment to be
made, xi2 ∈{0, A, B}, where 0 means that no investment is made.
Optimal value function (f i(si)): maximum expected amount of money from the
beginning of year i to the end of year 3 given that the money on hand is si at
the beginning of year i.
Optimal policy (pi(si) = x ∗
i ): optimal investments plan for year i, given that the
money on hand is si.
Recurrent relation: For 0 ≤si < 10, xi = (xi1, xi2) = (0, 0), fi(si) = si.
For si ≥10,
fi(si) = max
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
fi+1(si)
for (xi1, xi2) = (0, 0)
max
xi1∈A{0.3 × fi+1(si −xi1)
for xi2 = A
+ 0.7 × fi+1(si + xi1)}
max
xi1∈A{0.9 × fi+1(si) + 0.1 × fi+1(si + xi1)},
for xi2 = B
where A = {a : a = 10, 20, . . .; a ≤si} is the set of possible investment quantities
(in thousands of dollars).

Dynamic Programming
7-23
Boundary conditions: For 0 ≤s3 < 10, x3 = (x31, x32) = (0, 0), f3(s3) = s3.
For s3 ≥10,
f3(s3) = max
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
s3
for (x31, x32) = (0, 0)
max
x31∈A{0.3 × (s3 −x31) + 0.7 × (s3 + x31)}
for x32 = A
max
x31∈A{0.9 × (s3) + 0.1 × (s3 + x31)}
for x32 = B
where A = {a : a = 10, 20, . . .; a ≤s3}.
Answer: f1(10).
Numerical solution
The tables below analyze all possible investment opportunities for each year (stage) depend-
ing on the money on hand at the beginning of the year (state). The decision that gives the
maximum expected return by the end of the third year is selected.
Stage 3:
s3
0.3 × (s3 −x31) + 0.7 × (s3 + x31)
0.9 × s3 + 0.1 × (s3 + x31)
s3
x31
x32 = 0
x32 = A
x32 = B
x∗
31
x∗
32
f3(s3)
0
0
0
–
–
0
0
0
10
0
10
–
–
10
–
0.3 × (10 −10) + 0.7 × (10 + 10) = 14
0.9 × 10 + 0.1 × (10 + 10) = 11
10
A
14
20
0
20
–
–
10
–
0.3 × (20 −10) + 0.7 × (20 + 10) = 24
0.9 × 20 + 0.1 × (20 + 10) = 21
20
–
0.3 × (20 −20) + 0.7 × (20 + 20) = 28
0.9 × 20 + 0.1 × (20 + 20) = 22
20
A
28
30
0
30
–
–
10
–
0.3 × (30 −10) + 0.7 × (30 + 10) = 34
0.9 × 30 + 0.1 × (30 + 10) = 31
20
–
0.3 × (30 −20) + 0.7 × (30 + 20) = 38
0.9 × 30 + 0.1 × (30 + 20) = 32
30
–
0.3 × (30 −30) + 0.7 × (30 + 30) = 42
0.9 × 30 + 0.1 × (30 + 30) = 33
30
A
42
40
0
40
–
–
10
–
0.3 × (40 −10) + 0.7 × (40 + 10) = 44
0.9 × 40 + 0.1 × (40 + 10) = 41
20
–
0.3 × (40 −20) + 0.7 × (40 + 20) = 48
0.9 × 40 + 0.1 × (40 + 20) = 42
30
–
0.3 × (40 −30) + 0.7 × (40 + 30) = 52
0.9 × 40 + 0.1 × (40 + 30) = 43
40
–
0.3 × (40 −40) + 0.7 × (40 + 40) = 56
0.9 × 40 + 0.1 × (40 + 40) = 44
40
A
56
Stage 2:
f3(s2)
0.3 × f3(s2 −x21) + 0.7 × f3(s2 + x21)
0.9 × f3(s2) + 0.1 × f3(s2 + x21)
s2
x21
x22 = 0
x22 = A
x22 = B
x∗
21
x∗
22
f2(s2)
0
0
f3(0) = 0
–
–
0
0
0
10
0
f3(10) = 14
–
–
10
–
0.3 × f3(10 −10) + 0.7 × f3(10 + 10)
0.9 × f3(10) + 0.1 × f3(10 + 10)
10
A
19.6
= 0.3 × 0 + 0.7 × 28 = 19.6
= 0.9 × 14 + 0.1 × 28 = 15.4
20
0
f3(20) = 28
–
–
10
–
0.3 × f3(20 −10) + 0.7 × f3(20 + 10)
0.9 × f3(20) + 0.1 × f3(20 + 10)
= 0.3 × 14 + 0.7 × 42 = 33.6
= 0.9 × 28 + 0.1 × 42 = 29.4
20
–
0.3 × f3(20 −20) + 0.7 × f3(20 + 20)
0.9 × f3(20) + 0.1 × f3(20 + 20)
= 0.3 × 0 + 0.7 × 56 = 39.2
= 0.9 × 28 + 0.1 × 56 = 30.8
20
A
39.2

7-24
Operations Research Methodologies
Stage 1:
f2(s1)
0.3 × f2(s1 −x11) + 0.7 × f2(s1 + x11) 0.9 × f2(s1) + 0.1 × f2(s1 + x11)
s1 x11
x12 = 0
x12 = A
x12 = B
x∗
11 x∗
12 f1(s1)
10
0
f2(10) = 19.6
–
–
10
–
0.3 × f2(10 −10) + 0.7 × f2(10 + 10)
0.9 × f2(10) + 0.1 × f2(10 + 10)
= 0.3 × 0 + 0.7 × 39 = 27.3
= 0.9 × 19.6 + 0.1 × 39.2 = 21.56
10
A
27.3
Optimal Solution
x∗
1 = (x∗
11, x∗
12) = (10, A) ≡Invest $10,000 in A.
f1(10) = 27.3
≡The expected amount of money by the end of the
third year will be $27,300.
7.4
Conclusions
This chapter has introduced the reader to DP, which is a particular approach to solve opti-
mization problems. In an optimization problem, we try to ﬁnd the best solution from a set
of alternatives. One of the main diﬃculties of DP is in the development of the mathematical
formulation for a particular problem and the establishment of the recurrence relation that
allows us to solve instances of the problem in stages in an eﬃcient manner. For this reason,
we have taken the approach of introducing DP by discussing the formulation of some of
the important applications in the areas of industrial engineering and management science.
These applications have been categorized in deterministic models, where data is known with
certainty, and stochastic models, in which some of the information is uncertain and require
the use of probability distribution functions.
In many real-world applications, the diﬃculty of DP is the large number of states that
need to be considered to solve a problem. This diﬃculty was called the curse of dimension-
ality by Bellman (1952). The number of states can be reduced by making some additional
assumptions to the problem, which may result in a model that does not capture the real-
world setting. A DP model must be simple enough so that its behavior can be understood,
but at the same time its robustness must be veriﬁable.
References
1. Bellman, R.E. (1952), “On the Theory of Dynamic Programming,” Proceedings of the
National Academy of Sciences, 38, pp. 716–719.
2. Bellman, R.E. (1957), Dynamic Programming, Princeton University Press, Princeton, NJ.
3. Bellman, R.E. and S.E. Dreyfus (1962), Applied Dynamic Programming, Princeton Uni-
versity Press, Princeton, NJ.
4. Bertsikas, D. (1976), Dynamic Programming and Stochastic Control, Academic Press, New
York.
5. Blackwell, D. (1965), “Discounted Dynamic Programming,” Annals of Mathematical Statis-
tics, 36, pp. 226–235.
6. Cooper, L. and M.W. Cooper (1981), Introduction to Dynamic Programming, Pergamon
Press, Elmsford, NY.
7. Denardo, E.V. (1982), Dynamic Programming: Models and Applications, Prentice Hall,
Englewood Cliﬀs, NJ.

Dynamic Programming
7-25
8. Derman, C. (1963), “On Optimal Replacement Rules when Changes of State are
Markovian,” in Mathematical Optimization Techniques (R.E. Bellman, Ed.), University
of California Press, Berkeley, CA.
9. Derman, C. (1970), Finite State Markovian Decision Processes, Academic Press, New York.
10. Dreyfus, S. and A. Law (1976), The Art of Dynamic Programming, Academic Press,
New York.
11. Hillier, F.S. and G.J. Lieberman (2005), Introduction to Operations Research, 8th Ed.,
McGraw Hill, Boston, MA.
12. Ross, S.M. (1983), Introduction to Stochastic Dynamic Programming, Academic Press,
New York.
13. Manne, A.S. and A.F. Veinott, Jr. (1967), Chapter 11, in Investments for Capacity Expan-
sion: Size, Location, and Time-Phasing (A.S. Manne, Ed.), MIT Press, Cambridge, MA.
14. Taha, H.A. (2003), Operations Research: An Introduction, 7th Ed., Prentice Hall, Upper
Saddle River, NJ.
15. Wagner, H.M. and T. Whitin (1957), “Dynamic Problems in the Theory of the Firm,” in
Theory of Inventory Management, 2nd Ed. (T. Whitin, Ed.), Princeton University Press,
Princeton, NJ.
16. Winston, W.L. (2004), Operations Research: Applications and Algorithms, 4th Ed.,
Brooks/Cole-Thomson, Belmont, CA.


8
Stochastic Processes
Susan H. Xu
The Pennsylvania State University
8.1
Introduction............................................
8-1
8.2
Poisson Processes .....................................
8-7
The Exponential Distribution and Properties •
Deﬁnition of Poisson Processes • Properties of Poisson
Processes • Nonhomogeneous Poisson Processes
and Compound Poisson Processes
8.3
Discrete-Time Markov Chains ....................... 8-14
Deﬁnition of Discrete-Time Markov Chains • Transient
Analysis • Classiﬁcation of States • Limiting
Probabilities • Statistical Inference of Discrete-Time
Markov Chains
8.4
Continuous-Time Markov Chains ................... 8-27
Deﬁnition of Continuous-Time Markov Chains • Birth
and Death Processes and Applications • Transient
Analysis • Limiting Distribution • Statistical Inference
of Continuous-Time Markov Chains
8.5
Renewal Theory ....................................... 8-39
Renewal Processes • Renewal Reward Processes •
Regenerative Processes • Semi-Markov Processes •
Statistical Inference of Renewal Processes
8.6
Software Products Available for Solving
Stochastic Models ..................................... 8-46
References .................................................... 8-47
8.1
Introduction
Deterministic models and stochastic models are two broad categories of mathematical
models that aim at providing quantitative characterizations of a real system or a natural
phenomenon under study. The salient diﬀerence between the two types of models is that,
given a set of assumptions for each model, a deterministic model predicts a single outcome,
whereas a stochastic model predicts a set of possible outcomes along with the likelihood
or probability of each outcome. When a stochastic model is a more suitable choice for the
purpose of investigation, it is often the case that the underlying system can be better repre-
sented by a collection or a family of random variables, indexed by a parameter such as time or
space. Such a family of random variables is called a stochastic process. The ﬁeld of stochastic
processes represents a collection of models and methods used to depict the dynamic rela-
tionship of a family of random variables evolving in time or space.
The study of stochastic processes was started at the beginning of the twentieth cen-
tury, and it has been an actively researched area ever since, doubtlessly because of its
deep connections with practical problems. Today, stochastic processes are widely applied
8-1

8-2
Operations Research Methodologies
in diﬀerent disciplines such as engineering, business, physics, biology, health care, and the
military, to name a few. Stochastic processes can be used to understand the variability
inherent in the underlying process, to make predictions about the system behavior, to gain
insight on eﬀective design and control of the system, and to aid in managerial decision mak-
ing. The following examples illustrate the applications of stochastic processes in diﬀerent
ﬁelds.
Example 8.1: A brand switching model for consumer behavior
Suppose there are several brands of a product competing in a market. For example, those
brands might be competing brands of soft drinks. Let us assume that every week a consumer
buys one of the three brands, labeled as 1, 2, and 3. In each week, a consumer may either buy
the same brand he bought the previous week or switch to a diﬀerent brand. A consumer’s
preference can be inﬂuenced by many factors, such as brand loyalty and brand pressure
(i.e., a consumer is persuaded to purchase the same brand; see Whitaker (1978)). To gauge
consumer behavior, sample surveys are frequently conducted. Suppose that one of such
surveys identiﬁes the following consumer behavior:
Following Week
Current Week
Brand 1
Brand 2
Brand 3
Brand 1
0.51
0.35
0.14
Brand 2
0.12
0.80
0.08
Brand 3
0.03
0.05
0.92
For example, of those who currently bought brand 1, 51% buy the same brand, 35% switch
to brand 2 and 14% to brand 3, in the next week. The brand choices of a consumer over
diﬀerent weeks can be represented by a stochastic process that can enter three diﬀerent
states, namely, 1, 2, and 3. The market share of a brand during a period is deﬁned as the
average proportion of people who buy the brand during the period. The questions of interest
might be: What is the market share of a speciﬁc brand in a short run (say in 3 months)
or in a long run (i.e., the average market share of the brand when the number of weeks
observed is suﬃciently large)? How does repeat business, due to brand loyalty and brand
pressure, aﬀect a company’s market share and proﬁtability? What is the expected number
of weeks that a consumer stays with a particular brand?
■
Example 8.2: Automobile insurance
Most insurers around the world use the Bonus Malus (Latin for good-bad) system in auto-
mobile liability insurance. Such a system gives a merit rating, represented by a positive
integer-valued state, to each policyholder and determines the annual premium accordingly.
A policyholder’s state changes from year to year in response to the number of at-fault acci-
dents made by the policyholder. The system penalizes at-fault accidents of a policyholder
by increasing his state value (resulting in an annual premium surcharge) and rewards claim-
free years by decreasing his state value (resulting in a premium discount). The following

Stochastic Processes
8-3
table describes a hypothetical Bonus Malus system having four states (a real Bonus Malus
system usually has many more states):
Next State If
Current State
Annual Premium
0 Claims
1 Claim
2 Claims
≥3 Claims
1
$500
1
2
3
4
2
$600
1
3
4
4
3
$800
2
4
4
4
4
$1000
3
4
4
4
For instance, the table indicates that if a policyholder in state 2 makes no claims this
year, then the person’s rating would change to state 1 the next year. Empirical data can
be collected and analyzed so that a theoretical probability distribution on the number of
yearly claims from a policyholder can be obtained. The collection of states visited by a
policyholder, indexed by year, is a stochastic process. Based on the above table and the
probability distribution of annual claims, an insurer can compute the probability that a
policyholder changes from one state to another in successive years. From a model like
this, an insurer can compute various performance measures, such as the long-run average
premium received from a policyholder and its own insurance risk.
■
Example 8.3: Reliability
The reliability of a system, possibly consisting of several parts, is deﬁned as the probability
that the system will function during its assigned mission time. The measure is determined
mainly by the lifetime distributions of the constituting components and the structure func-
tion of the system. For example, during the mission time, a k-out-of-n system will function
if and only if at least k components out of n components will function. Special cases are the
series system, which is an n-out-of-n system, and the parallel system, which is a 1-out-of-n
system. If we associate with each time t a binary random variable that equals 1 if the system
functions at time t and 0 otherwise, then the collection of the binary random variables for
diﬀerent t is a stochastic process, representing the availability of the system over time. The
reliability of the system can be determined by the properties of the lifetime distributions of
the components and system structure.
■
Example 8.4: ALOHA protocols
ALOHA was a multiaccess communication protocol ﬁrst deployed at the University of
Hawaii in 1970. While the original version of the protocol is no longer used, its core design
concept has become the basis for the almost universal Ethernet.
It was quickly noticed that the ﬁrst version of the ALOHA protocol was not stable. Its
throughput was low and the number of backlogged packets was high, while a large portion
of available bandwidths was being wasted. Since then, several versions of the protocol have
been proposed. The following slotted and unslotted ALOHA models are based on Kulkarni
(1995) (also see Gautam, 2003).
In the slotted version, there are N users transmitting messages (in the form of packets)
via satellites. At time slots n = 1, 2, . . ., each user independently transmits a packet with
probability p. If only one user transmits a packet then the transmission is successful and
the packet departs the system. However, if two or more users simultaneously transmit, then
a collision occurs and their packets are garbled. Such packets are backlogged in the system
and have to be re-sent later. A backlogged packet will, independent of all else, retransmit

8-4
Operations Research Methodologies
with probability r in a time slot. A user with a backlogged packet will not transmit a new
packet until his backlogged packet is successfully re-sent.
In the unslotted version (again see Kulkarni, 1995, and Gautam, 2003), it is assumed
that each of the N users, when not backlogged, transmits a packet after an exponential
amount of time with rate λ. Each packet requires an exponential amount of time with rate
μ to transmit. A collision occurs when a user attempts to transmit while another user is
transmitting, which causes all transmissions to terminate instantaneously and the collided
packets to be backlogged. It is assumed that a backlogged packet retransmits after an
exponential amount of time with rate γ.
In either the slotted or unslotted version of the protocol, the number of backlogged packets
over time slots n = 1, 2, . . . or over continuous-time t ≥0 form a stochastic process. The
typical performance measures arising in the eﬃcient satellite communication of ALOHA
include the system throughput, the bandwidth utilization, the long-run behavior of the
number of backlogged packets, and the time needed to successfully transmit a packet.
■
Example 8.5: A model of social mobility
A problem of interest in the study of social structure is about the transitions between
the social status of successive generations in a family. Sociologists often assume that the
social class of a son depends only on his parents’ social class, but not on his grandparents’.
A famous U.K. study of social mobility was conducted after World War II (Glass, 1954),
which identiﬁed three social classes: upper class (executive, managerial, high administra-
tive, professional), middle class (high grade supervisor, non-manual, skilled manual), and
lower class (semi-skilled or unskilled). Each family in the society occupies one of the three
social classes, and its occupation evolves across diﬀerent generations. Glass (1954) analyzed
a random sample of 3500 male residents in England and Wales in 1949 and estimated that
the transitions between the social classes of successive generations in a family were as the
following:
Following Generation
Current Generation
Upper Class
Middle Class
Lower Class
Upper class
0.45
0.48
0.07
Middle class
0.05
0.70
0.25
Lower class
0.01
0.50
0.49
A dataset like this has enabled sociologists to answer questions such as: How many gen-
erations are necessary for a lower class family to become a higher class family? What is the
distribution of a family’s occupation in the long run?
■
With the help of the above examples, we now deﬁne a stochastic process.
DEFINITION 8.1
The collection of random variables X = {X(t), t ∈T} is called a
stochastic process, where T is called the index set.
The values assumed by process X are called the states, and the set of all possible values is
called the state space and denoted by S. Stochastic processes are further classiﬁed into four
broad classes by the nature of the index set T and state space S, where each of them can be
either discrete or continuous. The index t ∈T often corresponds to discrete units of time, and
the index set is T = {0, 1, 2, . . .}. In this case, X is called a discrete-time stochastic process,

Stochastic Processes
8-5
and it is customary to represent X by {Xn, n ≥1}. Take the brand switching problem as
an example: Xn might represent the brand preference of a consumer in week n, which can
take values 1, 2, or 3, with a discrete state space S = {1, 2, 3}. As another example, suppose
that Xn represents the total amount of claims (for convenience sake we assume it can take a
nonnegative, continuous value, although in reality it is countable) made by a policyholder in
year n, then {Xn, n ≥0} is a discrete-time stochastic process with a continuous state space.
When the index set T is an interval, X is called a continuous-time stochastic process. In most
common physical systems time is a natural index parameter, so T = (0, ∞). In this case,
we follow the convention to write X as {X(t), t ≥0}. In the unslotted ALOHA example, if
X(t) represents the number of backlogged packets at time t, t ≥0, then {X(t), t ≥0} is a
continuous-time stochastic process with a discrete state space S = {0, 1, 2, . . .}. Finally, an
example of the continuous-time stochastic process with a continuous state space might be
the cumulated amount of rainfall in an area continuously monitored during a season, or the
market price of a stock continuously observed during a trading session.
The random variables in a stochastic process often exhibit some sort of interdependence.
For instance, the current rating of a policyholder may depend on his claim history, and a
son’s occupation can be aﬀected by his ancestors’ occupations. Without knowing the depen-
dence structure of a random sequence, little can be said or done about a stochastic process.
As such, the study of stochastic processes is mainly centered around the characterizations
and solution methods of some prototypical processes that have certain types of dependence
structure. In this chapter, we shall discuss several fundamental processes that are most
frequently used in modeling real-world problems.
1. Poisson Processes: A stochastic process {N(t), t ≥0} is called a counting process
if N(t) represents the number of “events” that have occurred during the interval
[0, t]. A counting process is called a Poisson process if the interarrival times of
successive events are independently and identically distributed (iid) random vari-
ables that follow a common exponential distribution. The Poisson process inherits
the memoryless property of the exponential distribution, which translates into the
following stationary and independent increments property: at any time epoch t,
the process from time t onward is independent of its history and has the same
distribution as the original process. In essence, this property reduces the analysis
of the Poisson process into that of a sequence of independent random variables,
where those random variables represent the numbers of events occurring in non-
overlapping intervals. The Poisson process is a key building block in stochastic
modeling. For example, it has been used extensively to model the arrivals to a
service system, the traﬃc ﬂow on a highway, the number of defective items in
a manufacturing process, and the number of replacements of a component. The
subjects related to the Poisson process are covered in Section 8.2.
2. Discrete-Time Markov Chains: A discrete-time Markov chain (DTMC) is a
discrete-time stochastic process deﬁned on S = {0, 1, 2, . . .} that has the sim-
plest type of dependence structure, known as the Markov property: given the
present state of the process, the future evolution of the process is independent of
its history. The dependence structures in the brand switching, automobile insur-
ance, and social mobility examples are all of this type. As it turns out, a DTMC
can be completely speciﬁed by the transition probability matrix (e.g., the prob-
ability tables given in the aforementioned examples) and the distribution of the
initial state (e.g., the distribution of the merit rating of a policyholder at time 0),
which greatly simpliﬁes the analysis of a DTMC. Although simple, the DTMC

8-6
Operations Research Methodologies
proves to be the most useful modeling tool in analyzing practical problems. We
shall treat the topics relevant to DTMCs in Section 8.3.
3. Continuous-Time Markov Chains: In a DTMC, the process stays in a state for
a unit of time and then possibly makes a transition. If we relax this assumption
and allow the sojourn time of the process in each state to be independent and
follow a state-dependent exponential distribution, the resultant process is called
a continuous-time Markov chain (CTMC). For example, the unslotted ALOHA
protocol given in Example 8.4 can be modeled as a CTMC. As in the case of
a DTMC, a CTMC is a simple yet powerful modeling tool to treat real-world
stochastic systems. The topics related to CTMCs will be examined in Section 8.4.
4. Renewal Theory: In a Poisson process, the times between successive events are iid
exponential random variables. As an extension, a renewal process is a counting
process whose interarrival times are iid random variables following a general dis-
tribution. In renewal theory, an event is called a renewal, the time instance when
a renewal takes place is called a renewal epoch, and the time interval between two
consecutive renewals is called a renewal cycle. A renewal process has a stronger
dependence structure than a Poisson process: while the latter can probabilisti-
cally restart itself at any time, the former can only do so at a renewal epoch.
Nevertheless, renewal epochs facilitate the partition of the interval (0, ∞) into dis-
joint, independent renewal cycles, which, in turn, reduce the long-run analysis of
a renewal process to that of a typical renewal cycle. When there is a reward asso-
ciated with each renewal, the resultant process is called a renewal reward process.
A large number of practical problems can be formulated as renewal reward pro-
cesses. Renewal theory also forms a cornerstone for the development of other
stochastic processes with more complex dependence structures. For example, the
semi-Markov process can be roughly understood as a combination of a Markov
chain and a renewal process: it assumes that the process changes states as a
DTMC, but the sojourn time in each state can follow a state-dependent, but
otherwise general, distribution. We shall deal with the renewal process and its
variants in Section 8.5.
For each of the aforementioned processes, our discussion shall be focused on the following
aspects of the process:
• Process characterization: First, we shall give the formal deﬁnition of the underly-
ing process, identify its basic structure, and characterize the important properties.
• Transient analysis: Second, we shall consider how the underlying process behaves
in the transient state, i.e., to derive the distribution of X(t) or Xn for a ﬁnite t or
n. Transient analysis helps to answer questions such as “what is the distribution
of the number of backlogged packets in ALOHA at time t = 10” or “what is the
probability that a 1-out-of-n system will function in the next 24 hours?”
• Long-run analysis: Third, we shall study the long-run behavior of the process,
i.e, derive the limiting distribution of X(t) or Xn, as t or n tends to inﬁnity. The
long-run analysis seeks to answer questions such as “what is the long-run market
share of a brand” or “what proportion of families will be in the middle class in
steady-state?”
• Statistical inference: To apply results of stochastic processes to a real-life situa-
tion, data from the actual process have to be observed and analyzed to ﬁt the
characteristics of a prototypical process. This brings us to the topic of statistical

Stochastic Processes
8-7
inference of stochastic processes. We shall brieﬂy introduce parameter estimation
and hypothesis testing methods used in stochastic processes.
For each process, we shall illustrate the basic concepts and methodologies using several
practical problems extracted from diﬀerent application areas. Because of the page limitation,
the results will be stated without proof. There are numerous stochastic processes textbooks
where the reader can consult the proofs of our stated results. The References section of this
chapter lists a sample of the texts at diﬀerent levels. At the introductory level, the reader
may consult Ross (2003), Kulkarni (1999), Kao (1997), and Taylor and Karlin (1994). The
books for a more advanced level include Cinlar (1975), Ross (1996), Wolﬀ(1989), Bhat &
Miller (2002), Tijms (1994), and Kulkarni (1995). There are also many books dealing with
stochastic processes in specialized ﬁelds and a few of them are listed here: Aven & Jensen
(1999) and Barlow & Proschan (1981) for reliability, Buzacott & Shanthikumar (1993) for
manufacturing, Bolch et al. (2006) for computer networks, Kleinrock (1976) for queueing
systems, Rolski et al. (1999) and Kijima (2003) for ﬁnance and insurance, Zipkin (2000)
and Porteus (2002) for inventory systems, Helbing & Calek (1995) and Bartholomew (1982)
for social sciences, and Goel & Richter-Dyn (2004) for biology.
8.2
Poisson Processes
It appears that the Poisson process was ﬁrst investigated in detail by physicians A. Einstein
and M. Smolukhovsky in the context of Brownian processes. The ﬁrst application of the
Poisson process, published in 1910, was to describe radioactive decay occurring randomly.
The Poisson process, however, was named after the French mathematician Simeon-Denis
Poisson (1781–1840), who was credited for the introduction of the Poisson distribution, but
not of the Poisson process. Nevertheless, he certainly earned the right to be named after
this famous process connected with his distribution.
Both the Poisson process to be dealt with here and the continuous-time Markov chain
to be considered in Section 8.4 are intimately connected with the exponential distribution.
Exponential distribution plays a special role in those processes because it is mathematically
amenable and often a good approximation to the actual distribution. In the next section,
we review several salient properties of the exponential distribution.
8.2.1
The Exponential Distribution and Properties
A continuous random variable T is said to follow an exponential distribution with parameter
(or rate) λ > 0, where λ = 1/E[T], if it has the probability density function (PDF)
f(t) = λe−λt,
t ≥0
(8.1)
The property that makes the distribution attractive mathematically is that it has no
memory, or is memoryless. That is, the exponential distribution satisﬁes the condition
P(T > t + s|X > s) = P(T > t)
for any t, s > 0
The memoryless property bears the following interpretation: if T is the lifetime of a part,
then the above condition states that the probability that an s-year-old part lasts another t
years is the same as the probability that a new part lasts t years. To see that the property
is also a realistic assumption for an actual distribution, imagine that T is the time inter-
val between two successive accidents occurring on a highway, with a mean of 3 days per
accident. Suppose that there were no accidents in the past 2 days. Then, because of the

8-8
Operations Research Methodologies
total randomness of accidents, the remaining time until the next accident to occur from this
point on should be no diﬀerent, probabilistically, from the original waiting time between
two successive accidents. Thus, beyond the 2 accident-free days, we should expect to wait
another 3 days for the next accident to occur.
Besides the memoryless property, the exponential distribution also has several other nice
properties useful in stochastic modeling, as stated below.
Further Properties of the Exponential Distribution
1. The sum of a ﬁxed number of iid exponential random variables follows a Gamma
(or Erlang) distribution. Suppose that Sn = T1 + T2 + · · · + Tn, where Ti are iid
exponential random variables with rate λ. Then random variable Sn has the
probability density function
fSn(t) = λe−λt (λt)n−1
(n −1)!,
for t ≥0
For example, suppose that the occurrence times between successive accidents
on a highway are iid exponential random variables with a mean of 3 days per
accident. Then the waiting time for the 5th accident to occur is Erlang with mean
nλ = 5(3) = 15 days.
2. The minimum of independent exponential random variables is still an exponen-
tial random variable. Let Ti be the time when the ith event occurs, where Ti’s are
independent exponential variables with respective rates λi, i = 1, 2, . . ., n. Then
the time when the ﬁrst of the n events occurs, T = min(T1, T2, . . ., Tn), is an
exponential random variable with rate n
i=1 λi. For example, suppose that there
are two clerks who can serve customers at the exponential rates 3 and 5, respec-
tively. Given both clerks are currently busy, the time until one of the clerks
ﬁnishes service follows an exponential distribution with rate 3 + 5 = 8.
3. The probability that the ith event is the ﬁrst to occur among the n events is
proportional to λi. Let Ti, i = 1, 2, . . ., n, be independent exponential variables
with respective rates λi, and T = min(T1, T2, . . ., Tn). Then
P(Ti = T) =
λi
n
i=1 λi
,
i = 1, 2, . . ., n
(8.2)
Applying the above result to our two clerks example, the probability that the clerk with
the exponential rate 3 is the ﬁrst one to complete service is

3
(3+5) = 3
8

.
8.2.2
Deﬁnition of Poisson Processes
A stochastic process {N(t), t ≥0} is called a counting process if N(t) represents the number
of “events” that have occurred during the interval [0, t). Here, the “events” can be inbound
phone calls to a call center, machine breakdowns in a production system, or customer orders
for a product. Let us ﬁrst examine two properties that are desirable for a counting process.
A counting process is said to have independent increments if the numbers of events that
occur in nonoverlapping intervals are independent. This means, for example, the defective
items produced by a machine between 8:00 and 10:00 a.m. is independent of that produced
between 12:00 and 3:00 p.m. The counting process is said to have stationary increments if
the number of events that occur in an interval depends only on how long the interval is.

Stochastic Processes
8-9
In other words, N(s + t) −N(s) and N(t) are governed by the same distribution for any
s ≥0 and t > 0. For example, if the aforementioned defective counting process has stationary
increments, then the number of defective items produced during 8:00–10:00 a.m. will have
the same distribution as that produced during 1:00–3:00 p.m., since both time periods are
2 h. Note that the stationary and independent increment properties together mean that if
we partition the time interval (0, ∞) into the subintervals of an equal length, then the
number of events that occurred in those subintervals are iid random variables.
Now we are ready to formally deﬁne a Poisson process.
DEFINITION 8.2
A Poisson process with rate (or intensity) λ is a counting process
{N(t), t ≥0} for which
a. N(t) ≥0;
b. the process has independent increments;
c. the number of events in any interval of length t follows the Poisson distribution
with rate λt:
P(N(s + t) −N(s) = n) = (λt)ne−λt
n!
,
for any s ≥0
and
t > 0
(8.3)
(Property (c), in fact, implies that the Poisson process has stationary increments.)
Conditions (a)–(c) imply that the numbers of events that occur in nonoverlapping
intervals are independent Poisson random variables, and those Poisson events
occur at a constant rate λ. The Poisson process plays a key role in stochastic
modeling largely because of its mathematical tractability and practical realism.
Theoretically, the law of common events asserts that the Poisson distribution is
an approximation of the binomial distribution if the number of events is large and
the probability of actual occurrence of each event is small (Ross, 2003; Taylor &
Karlin, 1994). Empirically, it has been found that counting processes arising in
numerous applications indeed exhibit such characteristics.
From conditions (a)–(c), we can identify additional characterizations of a Poisson
process:
d. the probability that there is exactly one event occurring in a very small interval
[s, s + h) is approximately λh;
e. the probability that there are at least two events occurring in a very small interval
[s, s + h) is negligible; and
f. the interarrival times between successive events are a sequence of iid exponential
random variables with rate λ.
Conditions (d) and (e) postulate that the Poisson process is a process of “rare” events,
that is, events can only occur one at a time. In fact, conditions (d) and (e), together
with stationary and independent increments, serve as an alternative deﬁnition of a Poisson
process. Now, condition (f) means that the Poisson process is memoryless, that is, at any
time epoch, the remaining time until the next event to occur follows the same exponential
distribution, regardless of the time elapsed since the last event. Condition (f) serves as the
third alternative deﬁnition of a Poisson process. Those deﬁnitions are equivalent in the sense
that from the set of conditions for one deﬁnition one can derive the set of conditions for
another deﬁnition.

8-10
Operations Research Methodologies
Practitioners can use any of the three deﬁnitions as a yardstick to justify whether a
Poisson process is an adequate representation of the actual arrival process. For example, if
the actual arrival process shows a pattern of batch arrivals or the arrival rates at diﬀerent
time instances are diﬀerent (e.g., the arrival rate in a rush-hour is larger than that in normal
hours), then the Poisson process can be excluded outright as a candidate for modeling the
actual arrival process. Nevertheless, the law of rare events suggests that the Poisson process
is often a good approximation of the actual arrival process.
8.2.3
Properties of Poisson Processes
We often need to split a Poisson process into several subprocesses. For example, it might be
beneﬁcial to classify the customers arriving to a service system as the priority customers and
non-priority customers and route them to diﬀerent agents. Suppose that we associate with
each arrival in the Poisson process a distribution {pi : n
i = 1 pi = 1}, and let the arrival be of
type i with probability pi, i = 1, . . ., n, independent of all else. This mechanism decomposes
the original Poisson process into n subprocesses. In some applications we need to merge
several independent Poisson streams into a single arrival stream. The following theorem
states that both the decomposed processes and the superposed process are still Poisson
processes.
THEOREM 8.1
a. Let {N(t), t ≥0} be a Poisson process with rate λ. Each time an event occurs, inde-
pendent of all else, it is classiﬁed as a type i event with probability pi, n
i = 1 pi = 1.
Let {Ni(t), t ≥0} be the arrival process of type i. Then {Ni(t), t ≥0}, i = 1, . . ., n,
are n independent Poisson processes with respective rates λpi, i = 1, . . ., n.
b. Let {Ni(t), t ≥0}, i = 1, . . ., n, be independent Poisson processes with respective
rates λi, i = 1, . . ., n. Then the composite process {n
i = 1 Ni(t), t ≥0} is a Poisson
process with rate n
i = 1 λi.
Example 8.6: The case of meandering messages
This case is adapted from Nelson (1995) and is a simpliﬁed version of a real computer system.
While the numbers used here are ﬁctitious, they are consistent with actual data. A computer
at the Ohio State University (called osu.edu) receives e-mail from the outside world, and
distributes the mail to other computers on campus, including the central computer in the
College of Engineering (called eng.ohio-state.edu). Besides the mail from osu.edu, eng.ohio-
state.edu also receives e-mail directly without passing through osu.edu. Records maintained
by ohio.edu show that, over the two randomly selected days, it received 88,322 messages
on one day and 84,478 messages on the other, during the normal business hours 7:30 a.m.–
7:30 p.m. Historically, 20% of the mail goes to the College of Engineering, with the average
message size about 12K bytes. The College of Engineering does not keep detailed records,
but estimates that the direct messages to eng.ohio-state.edu is about two-and-a-half times
the traﬃc it receives from ohio.edu. The College of Engineering plans to replace eng.ohio-
state.edu with a newer computer and wants the new computer to have enough capacity to
handle even extreme bursts of traﬃc.
Provided that the rate of arrivals is reasonably steady throughout the business day, a
Poisson process is a plausible model for mailing directly to ohio.edu and directly to eng.ohio-
state.edu, because it is the result of a large number of senders acting independently. However,

Stochastic Processes
8-11
the planner should be aware that the Poisson process model is not perfect, as it does not
represent “bulk mail” that occurs when a single message is simultaneously sent to multiple
users on a list. Also, because e-mail traﬃc is signiﬁcantly lighter at night, our conclusions
have to be restricted to business hours, to amend to the stationary increments requirement
of the Poisson process.
Two business days, with 12 h per day, amounts to 86,400 s. So the estimated arrival
rate is ˆλosu = ((88,322 + 84,478)/86,400) ≈2 arrivals/s (see Section 8.2 for parameter esti-
mation in a Poisson process). The standard error is only sˆe =

ˆλosu/86, 400 ≈0.005
arrivals/s, indicating a quite precise estimation of ˆλosu. The overall arrival process to
eng.ohio-state.edu is composed of the direct arrivals and those distributed by osu.edu. If
we say that each arrival to osu.edu has probability p = 0.2 of being routed to eng.ohio-
state.edu, then the arrivals to the machine through ohio.edu form a Poisson process with
rate ˆλrouted = pˆλosu = 0.4 arrivals/s. Based on speculation, the direct arrival rate to eng.ohio-
state.edu is ˆλdirect = (2.5)ˆλrouted = 1 arrival/s. We can assign no standard error to this esti-
mate as it is not based on data. Thus, we may want to do a sensitivity analysis over a range
of values for ˆλdirect. The overall arrival process to eng.ohio-state.edu, a superposition of two
independent Poisson processes, is a Poisson process with rate ˆλeng = ˆλrouted + ˆλdirect = 1.4
arrivals/s. Based on the model, we can do some rough capacity planning by looking at the
probability of extreme bursts (a better model requires to model the system as a queueing
process). For example, if the new machine is capable of processing 3 messages/s, then
P(more than three arrivals/s) = 1 −
3

i=0
e−ˆλeng

ˆλeng
i
i!
≈0.05
If the processing time of a message also depends on its size, then the average number of
bytes received by eng.ohio-state.edu is (12Kbytes) ˆλeng = 16.8K bytes/s, assuming the mes-
sage size is independent of the arrival process. Unfortunately, we cannot make a probability
statement about the number of bytes received, as we have no knowledge of the distribution
of the message size except for its mean.
■
The next result relates a Poisson process to a uniform distribution, which provides tool for
computing the cost model of a Poisson process. Let Sn be the time of occurrence of the nth
event in a Poisson process, n = 1, 2, . . .. From property (1) of the exponential distribution
given in Section 8.2, we know that Sn has an Erlang distribution with parameters (n, λ),
n ≥1. If we are told that exactly n events have occurred during interval [0, t], that is,
N(t) = n, how does this information alter the joint distribution of S1, S2, . . ., Sn? This
can be answered intuitively as follows: as Poisson events occur completely randomly over
time, it postulates that any small interval of a ﬁxed length is equally likely to contain a
Poisson event. In other words, given N(t) = n, the occurrence times S1, . . ., Sn, considered
as unordered random variables, behave like a random sample from a uniform distribution
between [0, t]. This intuition can be formalized by the following theorem.
THEOREM 8.2
Given N(t) = n, the occurrence times S1, . . ., Sn have the same distribu-
tion as the order statistics of n independent uniform random variables in the interval [0, t].
Example 8.7: Sum quota sampling
We wish to estimate the expected interarrival time of a Poisson process. To do so, we have
observed the process for a pre-assigned quota, t, and collected a sample of the interarrival

8-12
Operations Research Methodologies
times, X1, X2, . . ., XN(t), during interval [0, t]. Note that the sample size is a random
variable. In sum quota sampling, we use the sample mean
¯XN(t) = SN(t)
N(t) = X1 + X2 + · · · + XN(t)
N(t)
,
N(t) > 0
to estimate the expected waiting time, provided that N(t) > 0. An important statistical
concern is whether E[ ¯XN(t)|N(t) > 0] is an unbiased estimator of the expected interarrival
time, say E(X1). The key is to evaluate the conditional expectation E[SN(t)|N(t) = n].
Let (U1, . . ., Un) be a sample from the uniform distribution in the interval [0, t]. Then by
Theorem 8.2,
E(SN(t) | N(t) = n) = E[max(U1, . . ., Un)] =
 t
0

1 −
x
t
n	
dx =
nt
n + 1
Then we get
E[ ¯XN(t) | N(t) > 0] =
∞

n=1
E

Sn
n | N(t) = n

P(N(t) = n | N(t) > 0)
=
∞

n=1
nt
(n + 1)n

 (λt)ne−λt
n!(1 −e−λt)

= 1
λ

1 −
λt
e−λt −1

The fraction of the bias to the true mean, E[X1] = 1/λ, is
E[ ¯XN(t) | N(t) > 0] −E[X1]
E[X1]
= −
λt
eλt −1 = −
E[N(t)]
eE[N(t)] −1
The following table computes numerically the bias due to the sum quota sampling:
E[N(t)]
Fraction Bias
E[N(t)]
Fraction Bias
5
−0.0339
20
−4.12E-8
10
−4.54E-04
25
−3.47E-10
15
−4.59E-06
30
−2.81E-12
This suggests that, although the sample mean generated by sample quota sampling is
downward biased, the bias tends to zero rapidly. Even for a moderate average sample size
of 10, the fraction of the bias is merely about 0.5%.
■
8.2.4
Nonhomogeneous Poisson Processes
and Compound Poisson Processes
In many applications, it is pertinent to allow the arrival rate of the Poisson process to be
dependent on time t, that is, λ = λ(t). The resultant process is called the nonhomogeneous
Poisson process with rate function λ(t). For such a process, the number of events in an
interval [s, s + t) follows a Poisson distribution with rate
 s+t
s
λ(u)du, and the number of
events that occur on non-overlapping intervals is independent.

Stochastic Processes
8-13
Example 8.8: Emergency 911 calls
In a study of police patrol operations in the New York City Policy Department (Green and
Kolesar, 1989), the data of 911 calls were analyzed. The study tabulated the total number
of calls to Precinct 77 in ﬁfteen-minute intervals during June–July, 1980.
The data exhibit a strong nonhomogeneous arrival pattern of the calls: the average number
of calls decreases gradually from about 37 calls at 12:00 midnight to 9 calls at 6:00 a.m.,
and then steadily increases, though with certain periods held stable, to about 45 calls when
it approaches 12:00 midnight. Based on the arrival counts, an estimated rate function λ(t)
was obtained. The hypothesis tests based on the counts and interarrival times showed that
the nonhomogeneous Poisson process was an adequate description of the arrival process of
the 911 calls.
■
Another generalization of the Poisson process is to associate each event with a random
variable (called mark): let {Yn, n ≥1} be a sequence of iid random variables that is also
independent of the Poisson process {N(t), t ≥0}. Suppose that for the nth event we associate
with it a random variable Yn, n ≥1, and deﬁne X(t) = N(t)
n=1 Yn, t ≥0. Then the process
{X(t), t ≥0} is call a compound Poisson process.
Example 8.9: Examples of compound Poisson processes
1. The Poisson process: This is a special case of the compound Poisson process since
if we let Yn ≡1, n ≥1, then X (t) = N(t).
2. Insurance risk: Suppose that insurance claims occur in accordance with a Poisson
process, and suppose that the magnitude of the nth claim is Yn. Then the cumu-
lative amount of claims up to time t can be represented by X(t).
3. Stock prices: Suppose that we observe the market price variations of a stock
according to a Poisson process. Denote Yn, n ≥1, as the change in the stock
price between the (n −1)st and nth observations. Based on the random work
hypothesis of stock prices, which is a postulation developed in ﬁnance, Yn, n ≥1,
can be modeled as iid random variables. Then X(t) represents the total price
change up to time t.
■
Statistical Inference of Poisson Processes
To apply mathematical results in a real life process, empirical data have to be collected,
analyzed, and ﬁtted to a theoretical process of choice. Parameter estimation and hypothesis
testing are two basic statistical inference tools to extract useful information from the dataset.
This section brieﬂy discusses statistical inference concerning the Poisson process.
Suppose that, for a ﬁxed time interval [0, t], we observed n Poisson events at times
0 = s0 < s1 < s2 < · · · < sn < t. Since the interarrival times si −si−1, i = 1, . . ., n, form a
random sample from an exponential distribution, the maximum likelihood function of the
sample is given by
f(λ) =
n

i=1
(λe−λ(si−si−1))e−λ(t−sn) = λne−λt
The log maximum likelihood function L(λ) = ln f(λ) is
L(λ) = lnf(λ) = n ln λ −λt

8-14
Operations Research Methodologies
Solving dL(λ)/dλ = 0 yields the maximum likelihood estimate of λ as
ˆλ = n
t
(8.4)
As the number of events n in [0, t] follows a Poisson distribution with the mean and variance
both equal to λt, ˆλ is an unbiased estimator of λ, E(ˆλ) = λ, with V (ˆλ) = λ
t . Clearly, the longer
the observation interval [0, t], the more precise the estimator ˆλ, as sˆe =

ˆλ/t decreases in t.
To test whether a process is Poisson for the data collected over a ﬁxed interval, we
can use several well-developed goodness-of-ﬁt tests based on the exponential distribution
of the interarrival times (see Gnedenko et al., 1969). We discuss here another simple test
(Bhat & Miller, 2002), using the relationship between the uniform distribution and the
Poisson process (see Theorem 8.2). Again assume that we have observed n Poisson events
at times 0 = s0 < s1 < s2 < · · · < sn <t. Recall that the si, considered as unordered random
variables, are iid uniform random variables between [0, t]. Hence, Yn = n
i=1 si is the sum
of n independent uniform random variables between [0, t], with E(Yn) = nt/2 and V (Yn) =
nt2/12. When n is suﬃciently large,
Z =
Yn −nt
2

nt2
12
(8.5)
is a standard normal random variable, by the central limit theorem.
Example 8.10: Testing for a Poisson process
This example is based on Lewis (1986), and the dataset was reprinted in Hand (1994). The
data were the number of times that 41 successive vehicles driving northbound on Route M1
in England passed a ﬁxed point near Junction 13. The 40 time points (in seconds) are given
below:
12
14
20
22
41
46
80
84
85
89
97
104
105
126
132
143
151
179
185
189
194
195
213
222
227
228
249
250
251
256
259
273
278
281
285
290
291
294
310
312
Denote si as the ith observation in the above list, where t = s40 = 312 corresponds to
the passing time of the last vehicle. We wish to test the null hypothesis that this pro-
cess is Poisson. We calculate Y40 = 40
i=1 si = 7062 and Z = Yn−nt/2
√
nt2/12 = 7062−6240
√324,480 = 1.443. The
p-value for the test is 2P(Z ≥1.443) = 0.149. Therefore, the dataset did not constitute
suﬃcient evidence to reject the null hypothesis that the underlying process is Poisson.
■
8.3
Discrete-Time Markov Chains
Most real-world systems that evolve dynamically in time exhibit some sort of temporal
dependence, that is, the outcomes of successive trials in the process are not independent
random variables. Dealing with temporal dependence in such a process is a formidable task.
As the simplest generalization of the probability model of successive trials with independent
outcomes, the discrete-time Markov chain (DTMC) is a random sequence in which the
outcome of each trial depends only on that of its immediate predecessor (known as the ﬁrst
order dependence).

Stochastic Processes
8-15
The notion of Markov chains originated from the Russian mathematician A.A. Markov
(1856–1922), who studied sequences of random variables with certain dependence structures
in his attempt to extend the weak law of large numbers and the central limit theorem. For
illustrative purposes, Markov in his 1906 manuscript applied his chain to the distribution of
vowels and consonants in Pushkin’s poem “Eugeny Onegin.” In the model, he assumed that
the outcome of each trial depends only on its immediate predecessor. The model turned out
to be a very accurate estimation of the frequency at which consonants occur in Pushkin’s
poem. Today, the Markov chain ﬁnds applications in diverse ﬁelds such as physics, biology,
sociology, meteorology, reliability, and many others, and proves to be the most useful tool
for analyzing practical problems.
8.3.1
Deﬁnition of Discrete-Time Markov Chains
Consider the discrete-time stochastic process {Xn, n = 0, 1, . . .} that assumes values in the
discrete state space S = {0, 1, 2, . . .}. We say the process is in state i at time n if Xn = i.
The process is called a discrete-time Markov chain (DTMC) if for all i0, i1, . . ., i, j, and n,
P(Xn+1 = j|Xn = i, Xn−1 = in−1, . . ., X1 = i1, X0 = i0) = P(Xn+1 = j|Xn = i)
(8.6)
The above expression describes the Markov property, which states that to predict the
future of the process it is suﬃcient to know the most recently observed outcome of the
process. The right side of Equation 8.6 is called the one-step transition probability, which
represents the conditional probability that the chain undergoes a state transition from i
to j in period n. If for all i, j, and n,
P(Xn+1 = j|Xn = i) = pij
then the DTMC is said to be stationary or time-homogeneous. We shall limit our discussion
to the stationary case. Following the convention, we arrange probabilities pij in a matrix
form, P = {pij}, and call P the one-step transition matrix. Matrix P is a stochastic matrix
in the sense that
pij ≥0 for all i, j,
and

j
pij = 1 for i = 1, 2, . . .
(8.7)
Note that a DTMC is completely speciﬁed by the transition matrix P and the initial
distribution a = {ai}, where ai = P(X0 = i) is the probability that the chain starts in state
i, i = 0, 1, . . ..
Several motivating examples given in Section 8.1, for example, the brand switching, social
mobility, automobile insurance, and slotted Aloha examples, are all DTMC models. Next,
we revisit some of those examples and also introduce other well-known DTMC models.
Example 8.1: A brand switching model (continued)
Let us continue our discussion on customers’ brand switching behavior, based on a model
extracted from Whitaker (1978). Whitaker deﬁnes brand loyalty as the proportion of con-
sumers who repurchase a brand on the next occasion without persuasion, and purchasing
pressure as the proportion of consumers who are persuaded to purchase a brand on the next
occasion. Denote wi and di, respectively, as the values of brand loyalty and brand pressure

8-16
Operations Research Methodologies
for brand i, where both di and wi are between 0 and 1 and 
i di = 1. To illustrate, consider
the following three-brand case:
Brand 1
Brand 2
Brand 3
Brand loyalty wi
0.30
0.60
0.90
Purchasing pressure di
0.30
0.50
0.20
In Whitaker’s Markov brand switching model, brand loyalty and brand pressure are
combined to give brand switching probabilities as follows:
pij =
di + (1 −di)wj
i = j
(1 −di)wj
i ̸= j
(8.8)
Here, pii, the proportion of consumers who repurchase brand i on two consecutive occasions,
includes the proportion of loyal consumers di who stay with brand i without being inﬂuenced
by purchasing pressure, and the proportion of disloyal consumers (1 −di) who remain with
brand i due to purchasing pressure wi. The proportion of switching customers, pij, j ̸= i,
consists of the proportion of disloyal consumers (1 −di) who are subjected to purchasing
pressure wj from brand j. Applying Equation 8.8 to our dataset yields
P =
⎛
⎝
0.51
0.35
0.14
0.12
0.80
0.08
0.03
0.05
0.92
⎞
⎠
This explains how we arrive at the probability table given in Example 8.1.
■
Example 8.11: A random walk model
Consider a DTMC deﬁned on the state space S = {0, 1, 2, . . ., N}, where N can be inﬁnity,
with a trigonal transition matrix:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
r0
p0
q1
r1
p1
q2
r2
p1
...
...
...
rN
pN
⎞
⎟
⎟
⎟
⎟
⎟
⎠
where qi, ri, pi ≥0 and qi + ri + pi = 1, for i = 1, . . ., N −1, and r0 + p0 = 1 and rN + N = 1.
The key feature of this DTMC is that it can only move at most one position at each step.
It has the designated name random walk, as in the original article of Karl Pearson of 1905,
it was used to describe a path of a drunk who either moves one step forward, one step
backward, or stays in the same location (a more general model allows the drunk to move to
the negative integer points, or to move on the integer points of a two-dimensional space).
Since then, the random work model has been used in physics to describe the trajectories
of particle movements, in ﬁnance to depict stock price changes, in biology to study how
epidemics spread, and in operations research to model the number of customers in discrete
queues.
When r0 = 0 (hence p0 = 1), the random walk is said to have a reﬂecting barrier at 0, as
whenever the process hits 0, it bounces back to state 1. When r0 = 1 (hence p1 = 0), the
random walk is said to have an absorbing barrier at 0, as whenever the chain hits 0, it stays

Stochastic Processes
8-17
there forever. When N < ∞, p0 = rN = 0 (hence r0 = pN = 1), pi = p, and qi = 1 −p (hence
ri = 0) for i = 1, 2, . . ., N −1, it becomes the well-known gambler’s ruin model. The model
assumes that a gambler at each play of the game either wins $1 with probability p or loses
$1 with probability q = 1 −p. The gambler will quit playing either when he goes broke (i.e.,
hits 0) or attains a fortune of N, whichever occurs ﬁrst. Then {Xn, n ≥0} represents the
gambler’s fortune over time.
■
Example 8.12: Success runs
Consider a DTMC on a state space {0, 1, . . ., N}, where N can be inﬁnity, with the transition
matrix of the form:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
p0
q0
0
0
. . .
0
0
p1
r1
q1
0
. . .
0
0
p2
0
r2
q2
. . .
0
0
...
...
...
...
...
...
pN−1
0
0
0
. . .
rN−1
qN−1
pN
0
0
0
. . .
0
qN
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
where pi, ri, qi ≥0, pi + ri + qi = 1 for i = 1, . . ., N −1, and p0 + q0 = pn + qn = 1. To see why
the name success runs is appropriate, consider a sequence of trials, each results in a success,
a failure, or a draw. If there have been i consecutive successes, then, the probabilities of a
failure, a draw, and a success in the next trial are pi, ri, and qi, respectively. Whenever there
is a failure, the process starts over with a new sequence of trials. Then {Xn, n = 1, 2, . . .}
forms a Markov chain, where Xn denotes the length of the run of successes at trial n.
The success-run chain is a source of rich examples and we consider two here. Suppose
that customer accounts receivable in a ﬁrm are classiﬁed each month into four categories:
current (state 1), 30 to 60 days past due (state 2), 60 to 90 days past due (state 3), and over
90 days past due (state 4). The company estimates the month-to-month transition matrix
of customer accounts to be
P =
⎛
⎜
⎜
⎝
0.9
0.1
0
0
0.5
0
0.5
0
0.3
0
0
0.7
0.2
0
0
0.8
⎞
⎟
⎟
⎠
Then {Xn, n ≥0} is a success-run chain, where Xn is the status of an account in month n.
Another application of the success-run chain is in reliability. Let us assume that a com-
ponent has a discrete lifetime Y with the distribution P(Y = i) = ai, i = 1, 2, . . .. Starting
with a new component, suppose that the component in service will be replaced by a new
component either when it fails or when its age surpasses a predetermined value T, whichever
occurs ﬁrst. These types of policies are called age replacement policies. The motivation of
instituting age replacement policies is to reduce unplanned system failures, as unplanned
replacements disrupt normal operations of the system and are more costly then planned
replacements. Let Xn be the age of the component in service at time n, with state space
S = {0, 1, . . ., T}. We have
P(Xn+1 = 0 | Xn = i) = P(Y = i + 1 | Y ≥i + 1) =
ai+1
∞
k=i+1 ak
≡pi,
i = 0, 1, . . ., T

8-18
Operations Research Methodologies
where pi represents that an i-period old component fails by the end of the period. Therefore,
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
p0
1 −p0
0
0
. . .
0
p1
0
1 −p1
0
. . .
0
p2
0
0
1 −p2
. . .
0
...
...
...
...
...
1
0
0
0
. . .
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
It can be easily seen that this is a success-run chain.
■
Example 8.4: A slotted ALOHA model (continued)
The slotted ALOHA described in Example 8.4 can be modeled as a DTMC {Xn, n ≥0} on
S = {0, 1, . . ., N}, where N is the total number of users and Xn is the number of backlogged
users at the beginning of the nth slot. The transition probabilities are given by (see Kulkarni,
1995):
pi,i−1 = P(None of N −Xn users transmit, exactly one of Xn users retransmits)
= (1 −p)N−iir(1 −r)i−1,
pi,i+1 = P(Exactly one of N −Xn users transmits, at least one of Xn users retransmits)
= (N −i)p(1 −p)N−i−1(1 −(1 −r)i),
pi,i+j = P(Exactly j of N −Xn users transmit)
=
(N −i)!
j!(N −i −j)!pj(1 −p)N−i−j,
2 ≤j ≤N −i,
pi,i
= P

Exactly one of N −Xn users transmits, none of Xn users retransmit; or
none of N −Xn users transmit, 0 or more than one of Xn users retransmit

= (N −i)p(1 −p)N−i−1(1 −r)i + (1 −p)N −i(1 −ir(1 −r)i−1).
■
8.3.2
Transient Analysis
Transient analysis of a DTMC is concerned with the performance measures that are functions
of the time index n. These performance measures are probability statements about the pos-
sible realization of a DTMC at time n. We present two such performance measures. The
ﬁrst one is the conditional probability that the chain goes from i to j in n transitions:
p(n)
ij
≡P(Xn = j | X0 = i),
n = 1, 2, . . .
(8.9)
They are called the n-step transition probabilities. In the matrix form, those probabilities
are denoted by P(n) = {p(n)
ij }, n = 1, 2, . . .. For example, in the brand switching model of
Example 8.1, p(5)
13 is the conditional probability that a consumer will buy brand 3 in week 5,
given he bought brand 1 in week 0. The second performance measure of interest is
a(n)
j
≡P(Xn = j), n = 1, 2, . . .
(8.10)
which is the unconditional probability that the DTMC is in state j after n steps, without
the knowledge of the initial state. For instance, in the brand switching model, a(5)
3
is the
unconditional probability that a customer will purchase brand 3 in week 5, regardless of his
choice in week 0.

Stochastic Processes
8-19
Next, we present the formulas to calculate the performance measures given in Equations
8.9 and 8.10. The basic equations to evaluate Equation 8.9 are the Chapman-Kolmogorov
(CK) equations:
p(m+n)
ij
=
∞

k=0
p(m)
ik p(n)
kj
(8.11)
The equations state that from state i, for the chain to be in state j after m + n steps, it
must be in some intermediate state k after m steps and then move from k onto j during
the remaining n steps. The CK equations allow us to construct the convenient relationship
for the transition probabilities between any two periods in which the Markov property
holds. From the theory of matrices we recognize that Equation 8.11 can be expressed as
P(m+n) = P(m) · P(n). By iterating this formula, we obtain
P(n) = P(n−1) · P = P(n−2) · P · P = · · · = P · P · · · P = Pn
(8.12)
In words, the n-step transition matrix P(n) can be computed by multiplying the one-step
transition matrix P by itself n times.
To obtain a(n)
j
deﬁned in Equation 8.10, we condition on the state of the process in period
n −1:
a(n)
j
= P(Xn = j) =

i
P(Xn = j | Xn−1 = i)P(Xn−1 = i) =

i
pija(n−1)
i
(8.13)
or, equivalently, a(n) = a(n−1)P. Iteratively using this relationship, we get
a(n) = a(n−1)P = a(n−2)P2 = · · · = a(0)Pn
(8.14)
Example 8.1: A brand switching model (continued)
Let a(0) = (0.30, 0.30, 0.40) be the brand shares in week 0. Then the brand shares in week 2,
computed by Equation 8.14, are
a(2) = a(0)P2 = (0.30, 0.30, 0.40)
⎛
⎝
0.51
0.35
0.14
0.12
0.80
0.08
0.03
0.05
0.92
⎞
⎠
2
= (0.159, 0.384, 0.457)
For example, the share of brand 3 in week 2 is a(2)
3
= 0.457. To obtain p(2)
23 , the proportion
of consumers who bought brand 2 in week 0 and buy brand 3 in week 2, we use Equation
8.12 and obtain:
P(2) = P2 =
⎛
⎝
0.51
0.35
0.14
0.12
0.80
0.08
0.03
0.05
0.92
⎞
⎠
2
=
⎛
⎝
0.306
0.466
0.228
0.160
0.686
0.154
0.049
0.096
0.855
⎞
⎠
(8.15)
Then p(2)
23 = 0.154.
■
Example 8.12: Success runs (continued)
Consider a special case of the success-run model where N = ∞and
pi0 = pi = p, pi,i + 1 = qi = q = 1 −p, i = 0, 1, . . ., N,
and
ri = 0, i = 1, . . ., N
Let us compute p(n)
0j . Although doable, it would be cumbersome to ﬁrst compute Pn and
then identify the appropriate terms p(n)
0j
from Pn. A better way is to explore the special

8-20
Operations Research Methodologies
structure of the transition matrix. Toward this end, we directly use the CK equations 8.11
and get
p(n)
00 =
∞

k=0
p(n−1)
0k
pk0 = p
∞

k=0
p(n−1)
0k
= p
Thus p(n)
00 = p for all n ≥1! Using the similar idea,
p(n)
01 =
∞

k=0
p(n−1)
0k
pk1 = p(n−1)
00
· p01 = pq =
q
if n = 1
pq if n > 1
Let us now compute p(n)
0j , for j > 1. If j > n, then p(n)
0j = 0, as it takes a minimum of j steps
to move from 0 to j. For j ≤n, we can recursively compute
p(n)
0j =
∞

k=0
p(n−1)
0k
pkj = qp(n−1)
0,j−1 = q2p(n−2)
0,j−2 = · · · = qj−1p(n−j+1)
01
=

qj
if n = j
pqj if n > j
where in the last equality, we used the expression for p(n)
01 . To verify the correctness of the
above expressions, we ﬁnd that
∞

j=0
p(n)
0j =
n−1

j=0
pqj + qn = p(1 −qn)
1 −q
+ qn = 1
8.3.3
Classiﬁcation of States
It is often necessary to obtain the limiting distribution of a DTMC as n →∞, which tells us
how the DTMC behaves in a long-run. Speciﬁcally, we consider the limiting distributions of
P(n), as n →∞. It turns out that the existence and uniqueness of such a limiting distribution
depend on the types of DTMCs. To understand possible complications that may arise in
the limiting behavior of a DTMC, we consider, in turn, the following two-state DTMCs on
the state space S = {1, 2}:
P1 =

0.7
0.3
0.4
0.6

,
P2 =

1
0
0
1

,
P3 =

0
1
1
0

,
P4 =

0.5
0.5
0
1

(8.16)
For the DTMC governed by matrix P1, simple matrix multiplications show that
P(2)
1
= P1 · P1 =

0.7
0.3
0.4
0.6
 
0.7
0.3
0.4
0.6

=

0.61
0.39
0.52
0.48

P(4)
1
= P(2)
1
· P(2)
1
=

0.61
0.39
0.52
0.48
 
0.61
0.39
0.52
0.48

=

0.575
0.425
0.567
0.433

P(8)
1
= P(4)
1
· P(4)
1
=

0.575
0.425
0.567
0.433
 
0.575
0.425
0.567
0.433

=

0.572
0.428
0.570
0.430

Observe that P(4)
1
is almost identical to P(8)
1 , and they have almost identical row values.
This suggests that there exist values πj, j = 1, 2, such that
lim
n→∞p(n)
ij
= πj,
j = 1, 2

Stochastic Processes
8-21
In this case, the limiting probability p(n)
ij
is independent of i. This DTMC is an example
of the regular Markov chain, where a Markov chain is said to be regular if for some n ≥1,
P(n) has all positive entries. It turns out that the limiting distribution exists for any regular
DTMC.
Now, the DTMC associated with P2 always returns to the same initial state. Indeed,
both states are absorbing states as once entered they are never left. Since P(n)
2
= P2 for all
n, the limit of p(n)
ij , as n tends inﬁnity, exists, but it depends on the initial state.
Next, observe that the DTMC governed by P3 alternates between states 1 and 2, for
example, p(n)
11 = 0 when n is odd, and p(n)
11 = 1 when n is even. As both states in the DTMC
are periodic, P(n)
3
does not converge to a limit as n →∞.
Finally, for the DTMC governed by transition matrix P4, we get
lim
n→∞P(n)
4
= lim
n→∞

0.5n
1 −0.5n
0
1

=

0
1
0
1

Here, state 1 is transient and the chain is eventually absorbed by state 2. Intuitively, a
state is transient if, starting from the state, the process will eventually leave the state and
never return.
Those four transition matrices illustrate four distinct types of convergence behaviors of
DTMCs. To sort out various cases, we must ﬁrst be able to classify the states of a DTMC.
It is worth noting that the calculations of the transient performance measures hold to all
DTMCs, regardless of the classiﬁcation of states.
Let us now classify states in a DTMC, using the transition matrices P1 to P4 given in
Equation 8.16 to illustrate the concepts. State j is said to be accessible from state i if,
starting from state i, there is a positive probability that j can be reached from i in a ﬁnite
number of transitions, that is, p(n)
ij > 0 for some n ≥0. States i and j are said to communicate
if they are accessible to each other. For instance, the states in P1 or P3 communicate since
each state can be accessed by another in one transition: p12 > 0 and p21 > 0. However, in P2
or P4, the two states do not communicate.
We can partition the states of a Markov chain into equivalent classes, where each equiv-
alent class contains those states that communicate with each other. For example, there is a
single class in the DTMC governed by P1 or P3, whereas there are two classes, each with a
single state, in the DTMC governed by P2 or P4. It is possible, as illustrated by P4, that
the process starts in one class and enters another class. However, once the process leaves a
class, it cannot return to that class, or else the two classes should be combined to form a
single class. The Markov chain is said to be irreducible if all the states belong to a single
class, as in the case of P1 or P3.
State i is said to be transient if limn→∞p(n)
ii = 0 and recurrent if limn→∞p(n)
ii > 0. Intu-
itively, a state is transient if, starting from the state, the process will eventually leave the
state and never return. In other words, the process will only visit the state a ﬁnite number
of times. A state is recurrent if, starting from the state, the process is guaranteed to return
to the state again and again, in fact, inﬁnitely many times. In the example of P4, state 1
is transient and state 2 is recurrent. It turns out that for a ﬁnite-state DTMC, at least one
state must be recurrent, as seen from P1 to P4.
If a state is recurrent, then it is said to be positive recurrent if, starting from the state,
the expected number of transitions until the chain return to the state is ﬁnite. It can be
shown that in a ﬁnite-state DTMC all the recurrent states are also positive recurrent. In an
inﬁnite-state DTMC, however, it is possible that a recurrent state is not positive recurrent.
Such a recurrent state is called null recurrent.

8-22
Operations Research Methodologies
State i is said be periodic if p(n)
ii > 0 only when n = d, 2d, 3d, . . ., and p(n)
ii = 0 otherwise.
A state that is not periodic is called aperiodic. In P3, both states have period d = 2, since
the chain can reenter each state only at steps 2, 4, 6, and so on. The states in the other
three matrices are all aperiodic.
It can be shown that recurrence, transientness, and periodicity are all class properties;
that is, if state i is recurrent (positive recurrent, null recurrent, transient, periodic), then all
other states in the same class of state i inherit the same property. The claim is exempliﬁed
by matrices P1 to P4.
8.3.4
Limiting Probabilities
The basic limiting theorem of a DTMC can be stated as follows.
THEOREM 8.3
For a DTMC that is irreducible and ergodic (i.e., positive recurrent and
aperiodic), limn→∞p(n)
ij , j ≥0, exists and is independent of initial state i. Let limt→∞p(n)
ij , =
πj, j ≥0. Then, π = (π0, π1, . . .) is the unique solution of
πj =
∞

i=0
πipij,
j = 0, 1, . . .
(8.17)
∞

j=0
πj = 1
(8.18)
Equation 8.17 can be intuitively understood by the CK equations: recall that
p(n)
ij
=

k
p(n−1)
ik
pkj,
for all i, j, n
If p(n)
ij
indeed converges to a value that is independent of i, then by letting n approach to
inﬁnity on both sides of the above equation, and assuming that the limit and summation
operations are interchangeable, we arrive at Equation 8.17.
Of the four matrices P1–P4, only P1 represents an irreducible ergodic DTMC. Matrix
P2 or P4 has two classes and hence is not irreducible, and matrix P3 is periodic and hence
is not ergodic.
Denote μjj as the mean recurrent time of state j, which is deﬁned as the expected number
of transitions until the process revisits state j. For example, in the brand switching example,
μjj is the expected number of weeks between successive purchases of brand j, j = 1, 2, 3. It
turns out that μjj is intimately related to πj via the equation
μjj = 1
πj
,
j ≥0
(8.19)
The relationship makes an intuitive sense: since, on average, the chain will spend one unit
of time in state j on every μjj units of time, the proportion of time it stays in state j, πj,
must be 1/μjj.
Note that in a ﬁnite-state DTMC, one of the equations in 8.17 is redundant, as we only
need Nequations to obtain Nunknowns. This is illustrated by the following two examples.

Stochastic Processes
8-23
Example 8.5: A model of social mobility (continued)
The DTMC is obviously irreducible and ergodic. Based on the estimated transition prob-
abilities shown in the table of Example 8.5, the limiting probabilities (π1, π2, π3) can be
obtained by solving
π1 = 0.45π1 + 0.05π2 + 0.01π3
π2 = 0.48π1 + 0.70π2 + 0.50π3
π1 + π2 + π3 = 1
The solution is π1 = 0.07, π2 = 0.62, π3 = 0.31. This means, regardless of the current occupa-
tion of a family, in a long-run, 7% of its descendants holds upper-class jobs, 62% middle-class
jobs, and 31% lower-class jobs. Now, given a family currently holds an upper-class job, it
takes in average μ11 = 1/π1 = 14.29 generations for the family to hold an upper-class job
again!
■
Example 8.2: Automobile insurance (continued)
In the Bonus Malus system discussed in Example 8.2, suppose that the number of yearly
claims by a policyholder follows Poisson with rate 0.5, that is, P(Y = n) = e−0.5n0.5
n!
≡θn.
Then θ0 = .6065, θ1 = .3033, θ2 = .0758, and ∞
n=3 θn = 0.0144. Based on the Bonus Males
table given in Example 8.2, we obtain the transition matrix
P =
⎛
⎜
⎜
⎝
.6065
.3033
.0758
.0144
.6065
.0000
.3033
.0902
.0000
.6065
.0000
.3935
.0000
.0000
.6065
.3935
⎞
⎟
⎟
⎠
The chain is irreducible and ergodic. The limiting distribution satisﬁes the system of
equations
π1 = .6065π1 + .6065π2
π2 = .3033π1 + .6065π3
π3 = .0758π1 + .3033π2 + .6065π4
4

i=1
πi = 1
The solution is π1 = 0.3692, π2 = 0.2395, π3 = 0.2103, and π4 = 0.1809. Based on the limiting
distribution, we can compute the average annual premium paid by a policyholder as
500π1 + 600π2 + 800π3 + 1000π4 = $677.44
■
Unfortunately, not all Markov chains can satisfy the conditions given in Theorem 8.3.
What can be said about the limiting behavior of such Markov chains? It turns out that it
depends on diﬀerent cases, as explained by the following remark.
Remark 8.1
1. The DTMC with irreducible, positive recurrent, but periodic states: In this case, π
is still a unique nonnegative solution of Equations 8.17 and 8.18. But now πj must
be understood as the long-run proportion of time that the process is in state j. An
example is the DTMC associated with P3; it has the unique solution π = (1/2, 1/2).

8-24
Operations Research Methodologies
The long-run proportions are also known as the stationary distribution of the
DTMC. The name comes from the fact that if the initial distribution of the chain
is chosen to be π, then the process is stationary in the sense that the distribution
of Xn remains the same for all n. It can be shown that when a limiting distribution
exists, it is also a stationary distribution.
2. The DTMC with several closed, positive recurrent classes: In this case, the transition
matrix of the DTMC takes the form
P =

PA
0
0
PB

where the subprocess associated with either PA or PB itself is a Markov chain.
Matrix P2 is an example of such a case. A slightly more general example is
P =
⎛
⎝
0.5
0.5
0
0.75
0.25
0
0
0
1
⎞
⎠
(8.20)
with PA =

0.5
0.5
0.75
0.25

and PB = (1). It can be easily obtained that
P(n) =

P(n)
A
0
0
P(n)
B

(8.21)
which means that each class is closed, that is, once the process is in a class, it
remains there thereafter. In eﬀect, the transition matrix is reduced to two irreducible
matrices PA and PB. From Equation 8.21, it follows that, for the matrix given in
Equation 8.20,
lim
n→∞P(n) =
⎛
⎝
πA
1
πA
2
0
πA
1
πA
2
0
0
0
πB
3
⎞
⎠
where we can obtain (πA
1 , πB
2 ) = (1/3, 2/3) in the usual way using Equations 8.17 and
8.18. We of course have πB
3 = 1. In contrast to the irreducible ergodic DTMC, where
the limiting distribution is independent of the initial state, the DTMC with several
closed, positive recurrent classes has the limiting distribution that is dependent on
the initial state.
3. The DTMC with both recurrent and transient classes: The DTMC associated with
P4 is an example of such a case. In general, there may be several transient and
several recurrent classes. In this situation, we often seek the probabilities that the
chain is eventually absorbed by diﬀerent recurrent classes. We illustrate the method
using the well-known gambler’s ruin problem described in Example 8.11.
Example 8.13: The gambler’s ruin
The DTMC associated with the gambler’s ruin problem has the transition matrix:
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
·
·
·
0
q
0
p
0
·
·
0
0
q
0
p
0
·
0
...
...
...
...
...
...
...
0
0
·
·
q
0
p
0
0
·
·
·
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(8.22)

Stochastic Processes
8-25
The DTMC has three classes, C1 = {0}, C2 = {N}, and C3 = {1, 2, . . ., N −1}, where C1 and
C2 are recurrent classes and C3 is a transient class. Starting from any state i, we want to
compute fi, the probability that the chain is absorbed by C2 (i.e., the gambler attains a
fortune of N before being ruined), for i = 0, 1, . . ., N. Conditioning on the outcome of the
ﬁrst play, we obtain a system of linear equations for the fi:
f0 = 0
fi = pf i+1 + qf i−1,
i = 1, 2, . . ., N −1
fN = 1
The solution of the above system of linear equations is given by:
fi =
⎧
⎪
⎨
⎪
⎩
1 −(q/p)i
1 −(q/p)N
p ̸= 1/2
i
N
p = 1/2
i = 0, 1, . . ., N
An application of the gambler’s ruin model in drug testing is given by Ross (2003).
■
4. The irreducible DTMC with null recurrent or transient states: This case is only
possible when the state space is inﬁnite, since any ﬁnite-state, irreducible DTMC
must be positive recurrent. In this case, neither the limiting distribution nor the
stationary distribution exists. A well-known example of this case is the random
walk model discussed in Example 8.13. It can be shown (see, for example, Ross,
2003) that when N = ∞, p0 = 1, and pi = qi = 1/2 for i ≥1, the Markov chain is null
recurrent. If N = ∞, p0 = 1, and pi = p, > 1 −p = q = qi for i ≥1, then the Markov
chain is transient.
8.3.5
Statistical Inference of Discrete-Time Markov Chains
In this section, we brieﬂy discuss parameter estimation and hypothesis testing issues for
DTMCs. Further references on the subject can be found in Bhat & Miller (2002), Basawa &
Prakasa Rao (1980), and references therein.
We ﬁrst consider how to estimate the entries in transition matrix P = {pij}, based on
data collected. Suppose we have observed a DTMC on a ﬁnite state space S = {0, 1, . . ., N}
for n transitions. Let ni be the number of periods that the process is in state i, and nij
the number of transitions from i to j, with ni = N
j=0 nij, i = 1, 2, . . ., N, and N
i=0 ni = n.
For each state i, the transition counts from this state to other states, (ni0, ni1, . . ., niN),
can be regarded as a sample of size ni from a multinomial distribution with probabilities
(pi0, pi1, . . ., piN), i = 0, 1, . . ., N. To obtain the maximum likelihood function of P, we also
have to take into account that the values (n0, n1, . . . , nN) are random variables. Whittle
(1955) shows that the maximum likelihood function has to include a correction factor A,
and takes the form
f(P) = A
N

i=0
ni!
ni0!ni1! · · · niN!pni0
i0 pni1
i1 · · · P niN
iN

8-26
Operations Research Methodologies
where A turns out to be independent of P. Therefore, the log likelihood function L(P) is
given by
L(P) = ln f(P) = ln B +
N

i=0
N

j=0
nij ln pij
= ln B +
N

i=0
⎡
⎣
N−1

j=0
nij ln pij + niN ln
⎛
⎝1 −
N−1

j=0
pij
⎞
⎠
⎤
⎦
where B contains all the terms independent of P. For each i, we take the derivatives of
L(P) with respect to pij for j = 0, 1, . . ., N −1 and set the resultant equations to zero:
∂L(P)
∂pij
= nij
pij
−
niN
1 −N−1
j=0 pij
= 0,
j = 0, 1, . . ., N −1
Solving the above equations yields the maximum likelihood estimator of pij as
ˆpij = nij
ni
,
i, j = 0, 1, . . ., N
(8.23)
Example 8.14: Voters’ attitude changes
Anderson (1954) used the data collected by the Bureau of Applied Social Research in Erie
County, Ohio, in 1940 (Lazarsfeld et al., 1948), in a study of voters’ attitude changes. The
Bureau interviewed 600 voters from May to August on their voting preferences,
D (Democrat), R (republican), and DK (Do not know or other candidates). Among the 600
people interviewed, 445 people responded to all six interviews. In that year, the Republic
Convention was held between the June and July interviews, and the Democratic Convention
was held between the July and August interviews. The transition counts for pairs of three
successive interviews are given below.
May–June
June–July
July–August
R
D
DK
Total
R
D
DK
Total
R
D
DK
Total
R
125
5
16
146
R
124
3
16
143
R
146
2
4
153
D
7
106
15
128
D
6
109
14
127
D
6
111
4
121
DK
11
18
142
171
DK
22
9
142
173
DK
40
36
96
172
445
445
445
Using Equation 8.23, we obtain the estimates of the three sets of transition probabilities as:
May–June
June–July
July–August
R
D
DK
R
D
DK
R
D
DK
R
0.856
0.034
0.110
R
0.867
0.021
0.112
R
0.961
0.013
0.026
D
0.055
0.828
0.117
D
0.047
0.845
0.108
D
0.050
0.917
0.033
DK
0.064
0.105
0.831
DK
0.127
0.052
0.821
DK
0.233
0.209
0.558
One question pertinent in the study was whether the voters’ attitudes had changed due
to the political events during the period under consideration. If no attitude changes were

Stochastic Processes
8-27
detected, the Markov chain is stationary, and hence the three sets of transition counts should
be pooled to give a single estimate of the transition matrix. We shall revisit this issue later.
■
We now discuss the likelihood ratio test for the stationarity of transition matrices. Let
pt
ij = P(Xt+1 = j|Xt = i) be the one-step transition probability from state i to state j at
time t. Our null hypothesis, H0, is pt
ij = pij, for t = 1, . . ., T. To test H0, denote nt
ij as the
transition count from i to j in period t. For example, in the voters’ attitude dataset, let t = 1,
2, 3 represent respectively May, June, and July, and states 1, 2, 3 correspond to D, R, and
DK. Then n1
12 = 5 means that 5 people changed their voting preferences from a Republican
candidate to a Democratic candidate from May to June. Under H0, the likelihood ratio test
statistic follows a χ2 distribution with (T −1)N(N −1) degrees of freedom (see Bhat &
Miller, 2002, pp. 134–135):
χ2
(T −1)N(N−1) = 2
T

t=1
N

i=0
N

j=0
nt
ij ln pt
ij
pij
(8.24)
As discussed, the maximum likelihood estimates of pt
ij and pij are given by
ˆpt
ij =
nt
ij
N
j=0 nt
ij
,
t = 1, 2, . . ., T,
ˆpij =
T
t=1 nt
ij
T
t=1
N
j=0 nt
ij
= nij
ni
Example 8.14: Voters’ attitude changes (continued)
We wish to test the stationarity of the three transition matrices, which tells us whether
the voters exhibited the same behavior across May to August. If indeed the process were
stationary, the three sets of transition counts can be pooled to form one set of transition
counts. The pooled transition counts and the estimate of the transition matrix for the pooled
data are given in the following table:
nij
R
D
DK
Total
ˆpij
R
D
DK
R
395
10
36
441
R
0.896
0.023
0.081
D
19
326
33
378
D
0.050
0.862
0.088
DK
73
63
380
516
DK
0.141
0.122
0.737
1335
Under H0, the χ2 statistic given in Equation 8.24 has (3 −1) · 3 · (3 −1) = 12 degrees
of freedom. Using the three sets of transition probabilities computed in Example 8.14
along with the above estimated transition probabilities for the pooled data, we obtain
χ2
12 = 97.644. As P(χ2
12 > 97.644) < 0.001, we conclude that there was suﬃcient evidence to
support the claim that the voters’ attitudes had been aﬀected by political events.
■
8.4
Continuous-Time Markov Chains
In a DTMC, the process stays in a state for a unit of time and then possibly makes a
transition. However, in many practical situations the process may change state at any point
of time. One type of model that is powerful to analyze a continuous-time stochastic system
is the continuous-time Markov chain (CTMC), which is similar to the DTMC but assumes

8-28
Operations Research Methodologies
that the sojourn time of the process in each state is a state-dependent exponential random
variable, independent of all else.
8.4.1
Deﬁnition of Continuous-Time Markov Chains
We call a continuous-time stochastic process {X(t), t ≥0} with state space S = {0, 1, . . .} a
continuous-time Markov chain if for all i, j, t, and s,
P(X(t + s) = j|X(s) = i, X(u) = x(u), 0 ≤u < s) = P(X(t + s) = j|X(s) = i)
(8.25)
As in the deﬁnition of the DTMC, Equation 8.25 expresses the Markov property, which
states, to predict the future state of the process, it is suﬃcient to know the most recently
observed state. Here, we only consider the stationary, or time-homogeneous, CTMC, that
is, the CTMC with its transition probability P(X(t + s) = j|X(s) = i) depending on t but
not on s, for any i, j, s, and t. We write
pij(t) ≡P(X(t + s) = j|X(s) = i)
We take the convention to arrange the probabilities pij(t) in the form of a matrix,
P(t) = {pij(t)}, which shall be called the transition matrix. Note that P(t) depends on
t, that is, a diﬀerent t speciﬁes a diﬀerent transition matrix. For each t, P(t) is a stochastic
matrix in the sense deﬁned in Equation 8.7. As in the DTMC case, the transition matrix of
the CTMC satisﬁes the Chapman–Kolmogorov (CK) equations:
P(s + t) = P(s) · P(t)
(8.26)
How do we check whether a stochastic process is a CTMC? Directly examining the Markov
property Equation 8.25 of the system is not practical and yields little insight. A more direct
construction of a CTMC is via a jump process with exponentially distributed sojourn times
between successive jumps. For this, consider a stochastic process evolving in the state space
S = {0, 1, . . .} as follows: (1) if the system enters state i, it stays there for an exponentially
distributed time with rate νi, independent of all else; (2) when the system leaves state i, it
makes a transition to state j ̸= i with probability pij, independent of how long the system
has stayed in i. By convention, the transition from a state to itself is not allowed. Deﬁne
X(t) as the state of the system at time t in the jump process described above. Then it can
be shown that {X(t), t ≥0} is a CTMC. As such, a CTMC can be described by two sets of
parameters: exponential sojourn time rates {νi} and transition probabilities {pij, j ̸= i}.
Another view of a CTMC as a jump process is as follows: when the process is in state
i, it attempts to make a jump to state j after a sojourn time Tij, j ≥0, where Tij fol-
lows an exponential distribution with rate qij, independent of all else. The process moves
from i to j if Tij happens to be the smallest among all the sojourn times Tik, k ̸= i,
that is, Tij = Ti = mink̸=i{Tik}. Then, by properties (2) and (3) of the exponential dis-
tribution (see Section 8.2), Ti follows the exponential distribution with rate 
k̸=i qik, and
P(Tij = Ti) = qij/ 
k̸=i qik.
Let us deﬁne
νi =

k̸=i
qik,
pij =
qij

k̸=i qik
,
i = 0, 1, . . ., j ̸= i
(8.27)
The preceding expressions mean that a CTMC can also be represented by a set of expo-
nential rates qij, j ̸= i, which shall be referred to as the transition rates hereafter. For
convenience, we deﬁne
qii = −νi = −

k̸=i
qik,
i = 0, 1, . . .

Stochastic Processes
8-29
Then we can arrange all the rates by a matrix, Q = {qij}. Matrix Q is known as the inﬁnites-
imal generator (or simply, the generator) of a CTMC. Supplemented by the initial distribu-
tion of the process, namely, a = {aj}, where aj = P(X(0) = j), j ≥0, Q completely speciﬁes
the CTMC.
Example 8.15: A two-component cold standby system
A system has two components, only one of which is used at any given time. The standby
component will be put in use when the online component fails. The standby component
cannot fail (thus the name cold standby). The uptime of each component, when in use,
is an exponential random variable with rate λ. There is a single repairman who repairs a
breakdown component at an exponential rate μ.
Let X(t) denote the number of failed components at time t; then X(t) can take values
0, 1, or 2. It is not diﬃcult to see that {X(t), t ≥0} is a DTMC with the parameters
ν0 = λ,
ν1 = λ + μ,
ν2 = μ,
p01 = p21 = 1,
p10 =
μ
λ + μ,
p12 =
λ
λ + μ
From Equation 8.27, we obtain the inﬁnitesimal generator Q of the CTMC as
Q =
⎛
⎝
−λ
λ
0
μ
−(λ + μ)
λ
0
−μ
μ
⎞
⎠
■
8.4.2
Birth and Death Processes and Applications
The birth and death process is a CTMC with state space S = {0, 1, 2, . . .}, where the state
represents the current number of people in the system, and the state changes when either
a birth or a death occurs. More speciﬁcally, when the population size is i, a new arrival
(birth) enters the system at an exponential rate λi, i = 0, 1, . . ., and a new departure (death)
leaves the system at an exponential rate μi, i = 1, 2, . . .. The inﬁnitesimal generator of the
birth and death process is
Q =
⎛
⎜
⎜
⎜
⎝
−λ0
λ0
0
0
0
· · ·
μ1
−(λ1 + μ1)
λ1
0
0
· · ·
0
μ2
−(λ1 + μ1)
λ2
0
· · ·
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎠
(8.28)
As seen, the key feature of the birth and death process is that, at any state, the process
can only jump to an adjacent state of that state. The Poisson process is an example of the
birth and death process, in which births occur at a constant rate λ and death rates are
all 0. The cold standby reliability system is another example. The birth and death process
may also be viewed as the continuous counterpart of the random walk model discussed in
Example 8.11.
The study of the birth and death process was started in the second decade of the twentieth
century, with its roots in biology. The investigations by the Dutch scientist A.K. Erlang on
traﬃc of telephone networks also contributed to the development of the theory of the birth
and death process and its application in queueing theory. We shall illustrate the use of a
fundamental result credited to Erlang, the Erlang C formula, in the following example.

8-30
Operations Research Methodologies
Example 8.16: A model of tele-queue: the Erlang C formula
The fundamental challenge for eﬃcient inbound call center operations is to strike a correct
balance between supply (the numbers of agents and trunks) and demand (the random and
time varying incoming calls). The call center needs to maximize the utilization of its agents,
while keeping a caller waiting time to an acceptable minimum. Gans et al. (2003) give a
comprehensive review of queueing models used in call center operations. One of the simplest
yet widely used model to evaluate a call center’s performance is the Erlang C model that
originated from A.K. Erlang. When Erlang worked for the Copenhagen Phone Company,
he used the model to calculate the fraction of callers in a small village who had to wait
because all the lines were in use.
By the queueing notation, Erlang C is an M/M/s queue, which is a birth and death
process. The model assumes that the arrival process is Poisson with rate λ, service times
are iid exponential with rate μ, and there are s agents to answer calls. Erlang C has its
parameters speciﬁed as
λi = λ,
i ≥0,
μi =
iμ
if 0 < i ≤s
sμ
if i > s
(8.29)
The death rate μi is understood as: when 0 < i ≤s, i(≤s) agents are busy, and a caller
departs the center when one of the i agents ﬁnishes service. Hence the service rate is is.
When i ≥s, all s agents are busy and they serve at an aggregated rate sμ. We will revisit
the model and discuss its solution methods in the subsequent sections.
■
Erlang C has two drawbacks. First, it assumes that the system has an inﬁnite number of
trunks so a caller who cannot be answered immediately would not be blocked. In reality, a
call center has a ﬁnite number of lines, and a caller will be blocked (i.e., get busy signals)
when all the lines are occupied. Second, it does not take customer abandonments into
account. Empirical data show that a signiﬁcant proportion of callers abandon their lines
before being served (Gans et al., 2003). The next model, Erlang A, incorporates both busy
signals and abandonment in model construction.
Example 8.17: The Erlang A model
In this model, the arrival process is assumed to be Poisson and service times are iid expo-
nential. There are s agents and k trunks, s ≤k. The maximum number of calls the system
can accommodate is k and an incoming call will get busy signals if all k lines are occupied.
Patience is deﬁned as the maximal amount of time that a caller is willing to wait for service;
if not served within this time, the caller deserts the queue. The queueing model associated
with Erlang A is the M/M/s/k + M queue, where the +M notation means that patience
is iid exponentially distributed, say with rate θ. The M/M/s/k + M model is again a birth
and death process with the transition rates
λi = λ
i = 0, . . ., k −1
μi = min{i, s}μ + max{i −s, 0}θ
i = 1, . . ., k
(8.30)
To understand the departure rate μi, note that the ﬁrst term min{i, s}μ is the service
completion rate, bearing a similar explanation as in the Erlang C model. The second term
max{i −s, 0}θ is the abandonment rate when there are i callers in the system, as the number
of callers waiting in queue is max{i −s, 0}, and each caller in queue abandons the line at
an exponential rate θ.
■

Stochastic Processes
8-31
8.4.3
Transient Analysis
Transient analysis of a CTMC is concerned with the distribution of X(t) for a ﬁnite t. Since
if we know P(t) = {pij(t)}, we can compute the distribution of X(t) by
P(X(t) = j) =
∞

i=0
pij(t)ai
where {ai} is the distribution of X0, it suﬃces to know {pij(t)}. There are several ways
to compute {pij(t)}, and we present two of them, the uniformization and Kolmogorov dif-
ferential equation methods. While the ﬁrst one is straightforward for numerical compu-
tation, the second one is mathematically more structured. Let us consider them in turn.
The uniformization method needs the condition that the νi in the CTMC is bounded. For
example, the condition holds for a ﬁnite-state CTMC. Let ν be any ﬁnite number such
that maxi{νi} ≤ν < ∞. Intuitively, what this technique does is to insert ﬁctitious transi-
tions from a state to itself so that, with both the real and ﬁctitious transitions, the CTMC
changes states at a constant rate ν, regardless of the state it is in, where ν is a uniform
upper bound on νi for any i. Let matrix ˆP = {ˆpij} be deﬁned as
ˆpij =
⎧
⎨
⎩
1 −νi
ν
if i = j
qij
ν
if i ̸= j
(8.31)
Clearly, ˆP is a stochastic matrix. The following is the key result, which gives the distri-
bution P(t):
P(t) =
∞

k=0
e−νt (νt)k
k!
ˆPk,
t ≥0
(8.32)
Example 8.18:
Suppose that the inﬁnitesimal generator of a CTMC is
Q =
⎛
⎜
⎜
⎝
−5
2
3
0
4
−6
2
0
0
2
−4
2
1
0
3
−4
⎞
⎟
⎟
⎠
Since qii = −νi, we have ν0 = 5, ν1 = 6, ν3 = 4, and ν2 = 4. Hence, we can choose ν = max{5,
6, 4, 4} = 6. The uniformized transition matrix ˆP becomes
P =
⎛
⎜
⎜
⎝
1/6
1/3
1/2
0
2/3
0
1/3
0
0
1/3
1/3
1/3
1/6
0
1/2
1/3
⎞
⎟
⎟
⎠
Let us compute P(0.5). To numerically evaluate P(t) given in Equation 8.32, we must
truncate the summation of inﬁnite terms to that of the ﬁrst K terms. We shall use K = 13
(algorithms are available to determine K that guarantees the error to be controlled within

8-32
Operations Research Methodologies
a desired level. See Nelson (1995) and Kulkarni (1995). From Equation 8.32, we get
P(0.5) =
13

k=0
e−6×0.5 (6 × 0.5)k
k!
⎛
⎜
⎜
⎜
⎝
1/6
1/3
1/2
0
2/3
0
1/3
0
0
1/3
1/3
1/3
1/6
0
1/2
1/3
⎞
⎟
⎟
⎟
⎠
k
=
⎛
⎜
⎜
⎝
0.2506
0.2170
0.3867
0.1458
0.2531
0.2381
0.3744
0.1341
0.1691
0.1936
0.4203
0.2170
0.1580
0.1574
0.3983
0.2862
⎞
⎟
⎟
⎠
Similarly, we can compute P(1), using K = 19. We get
P(1) =
⎛
⎜
⎜
⎝
0.2083
0.2053
0.3987
0.1912
0.2083
0.2053
0.3979
0.1885
0.1968
0.1984
0.4010
0.2039
0.1920
0.1940
0.4015
0.2125
⎞
⎟
⎟
⎠
We can verify that P(1) = P(0.5) · P(0.5), as we would expect from the CK equations.
■
Example 8.19: A repairable reliability system
Consider a machine that alternates between two conditions: state 0 means the machine is
in working condition, whereas state 1 means it is under repair. Suppose the working time
and repair time are both exponentially distributed with respective rates λ and μ. We are
interested in the distribution of the state of the machine at a ﬁxed time t. It is easily seen
that the process has the generator given by
Q =
−λ
λ
μ
−μ

It is convenient to let ν = λ + μ. From Equation 8.31 we obtain
ˆP =
⎛
⎜
⎝
μ
λ + μ
λ
λ + μ
μ
λ + μ
λ
λ + μ
⎞
⎟
⎠
It can be easily veriﬁed that ˆPk = ˆP, for any k ≥1. This property allows us to obtain
Equation 8.32 in the closed form (for detailed derivation, see, for example, Kulkarni, 1995):
P(t) =
⎛
⎜
⎝
μ
λ + μ
λ
λ + μ
μ
λ + μ
λ
λ + μ
⎞
⎟
⎠+
⎛
⎜
⎝
λ
λ + μ
−λ
λ + μ
−μ
λ + μ
μ
λ + μ
⎞
⎟
⎠· e−(λ+μ)t
(8.33)
From Equation 8.33, we can read that the probability that the machine, now up, will be
up at time t is
p00(t) =
μ
λ + μ +
λ
λ + μe−(λ+μ)t
■
The second approach to evaluate P(t) is based on the systems of Kolmogorov diﬀerential
equations, which assert that P(t) must satisfy
dP(t)
dt
= P(t) · Q
and
dP(t)
dt
= Q · P(t)
(8.34)

Stochastic Processes
8-33
where dP(t)/dt = {dpij(t)/dt}. The ﬁrst set of equations is called the Kolmogorov forward
equations and the second set the Kolmogorov backward equations. Under the mild con-
ditions, which are satisﬁed by most practical problems, the solutions of the forward and
backward equations are identical. We illustrate the method using the previous example.
Example 8.19: A repairable reliability system (revisited)
We shall use the forward equations to obtain P(t). Using the generator Q given in Example
8.19, we can write Equation 8.34 as
⎛
⎜
⎝
dp00(t)
dt
dp01(t)
dt
dp10(t)
dt
dp11(t)
dt
⎞
⎟
⎠=

p00(t)
p01(t)
p10(t)
p11(t)

·

−λ
λ
μ
−μ

(8.35)
Note that there are in total four diﬀerential equations in Equation 8.35. Suppose we are
only interested in p00(t). Do we have to solve all four diﬀerential equations to get p00(t)?
The answer is fortunately no. In general, if we are interested in pij(t), then we only need
to solve the set of diﬀerential equations involved in the ith row of {dpij(t)/dt}. In our case,
we end up with two diﬀerential equations:
dp00(t)
dt
= −λp00(t) + μp01(t)
dp01(t)
dt
= λp00(t) −μp01(t)
Recognizing p00(t) = 1 −p01(t), we further reduce the two equations to one:
dp00(t)
dt
= μ −(λ + μ)p00(t)
Using the standard technique to solve this linear diﬀerential equation, we obtain
p00(t) =
μ
λ + μ +
λ
λ + μe−(λ+μ)t
As expected, this result is the same as the result obtained by the uniformization method. ■
8.4.4
Limiting Distribution
We now turn our attention to the limiting behavior of the CTMC. As in the DTMC case, the
existence of the limiting distribution of P(t), when t →∞, requires the CTMC under study
to satisfy certain conditions. The suﬃcient conditions under which the limiting distribution
exists and is independent of the initial state are:
1. the CTMC is irreducible, that is, all states communicate; and
2. the CTMC is positive recurrent, that is, starting from any state, the mean time
to return to that state is ﬁnite.
Note that any ﬁnite-state, irreducible CTMC is positive recurrent, and hence, exists the
limiting distribution that is independent of the initial state.
When the above conditions are satisﬁed for the CTMC, denote π = {π0, π1, . . .} as the lim-
iting distribution of the process, where πj = limt→∞pij(t), j ≥0. To derive the set of equa-
tions that π = {π0, π1, . . .} satisfy, consider the forward equations given in Equation 8.34.

8-34
Operations Research Methodologies
If for any given j, probability pij(t) indeed converges to a constant πj, then we must have
limt→∞
dpij(t)
dt
= 0. Hence, the forward equations, in the limit, tend to
0 = πQ = (π0, π1, π2, . . .)
⎛
⎜
⎜
⎜
⎝
−v0
q01
q02
q03
· · ·
q10
−v1
q12
q13
· · ·
q20
q21
−v2
q23
· · ·
...
...
...
...
...
⎞
⎟
⎟
⎟
⎠
(8.36)
where νi = Σjqij. The above equations are equivalent to
πjvj =

i̸=j
πiqij,
for all j
(8.37)
Evidently, we also need the normalizing equation:

j
πj = 1
(8.38)
Equations 8.37 are called the balance equations and have a nice interpretation: the left
side term, πjνj, represents the long-run rate at which the process leaves state j, since, when
in j, the process leaves at rate νj, and the long-run proportion of time it is in j is πj. The
right side term, Σi̸=jπipij, is the long-run rate at which the process enters j, since, when
in state i, the process enters j at rate qij, and the proportion of time the process is in i is
πi, i ̸= j. Hence, Equation 8.37 asserts, the long-run rate into any state must be balanced
with the long-run rate out of that state.
Equations 8.37 and 8.38 together can be used to compute the limiting distribution π
of the CTMC when it exists. It turns out that the solution is also unique. The limiting
distribution π sometimes is called the stationary distribution. The name comes from the
fact that if the distribution of the initial state X0 is chosen to be π, then X(t) will have the
same distribution π for any given t. In other words, the process is stationary starting from
time 0.
Example 8.20: The limiting distribution of the birth and death process
The inﬁnitesimal generator Q of the birth and death process is given in Equation 8.28.
Using Equations 8.37 and 8.38, we get
λ0π0 = μ1π1
(λi + μi)πi = λi−1πi−1 + μi+1πi+1,
i > 0

j
πj = 1
The solution of the preceding equations is (see, for example, Ross, 2003)
π0 =
"
1 +
∞

i=1
λ0λ1, . . ., λi −1
μ1μ1, . . ., μi
#−1
(8.39)
πi = λ0λ1, . . ., λi −1
μ1μ1, . . ., μi
π0,
i > 0
(8.40)

Stochastic Processes
8-35
The necessary and suﬃcient condition for the limiting distribution to exist is
∞
i=1
λ0λ1, . . ., λi−1
μ1μ1, . . ., μi
< ∞
The above formulas will be used to derive the limiting distributions of Erlang models, which
are special cases of the birth and death process.
■
Example 8.16: The Erlang C formula (continued)
We start with computing the limiting distribution for Erlang C (i.e., the M/M/s queue).
The transition rates of Erlang C are given in Equation 8.29. Deﬁne ρ = λ/sμ as the traﬃc
intensity of the system. We assume ρ < 1, which is the necessary and suﬃcient condition for
the existence of the limiting distribution. Substituting the parameters into Equations 8.39
and 8.40, we get
π0 =
" s

i=0
(sρ)i
i!
+ ρs+1ss
s!
(1 −ρ)−1
#−1
πi =
⎧
⎪
⎨
⎪
⎩
(sρ)i
i!
π0
if i ≤s
ssρi
s! π0
if i ≥s
i ≥1
We now use π to compute key performance measures in a call center operation. One
way to use the Erlang C result is to estimate the service level (SL), which is deﬁned as the
probability that a caller’s waiting time, Wq, is no more than a predetermined number, say D:
SL = P(Wq ≤D)
(8.41)
Conditioning on whether a caller has to wait, we can write the expression of SL as
SL = P(Wq ≤D) = 1 −P(Wq > D|Wq > 0)P(Wq > 0)
(8.42)
To compute SL, we need the expressions of P(Wq > 0) and P(Wq > D|Wq > 0); each is of
interest in its own right, as the former is the proportion of callers who must wait, and the
latter is the proportion of the callers in queue who have to wait more than D units of time. As
a caller has to wait in queue if and only if there are at least s callers present when he arrives,
P(Wq > 0) = 1 −
s−1

i=0
πi = 1−
s−1

i=0
(sρ)i
i!
π0 = (sρ)sπ0
s!(1 −ρ)
(8.43)
To compute P(Wq > D|Wq > 0), we reason as follows: since πi has a geometric tail, that is,
πi + 1 = ρπi, for i ≥s, the random variable Wq|Wq > 0 has an exponential distribution with
rate (sμ −λ). Thus,
P(Wq > D|Wq > 0) = e−(sμ−λ)D
(8.44)
Substituting Equations 8.43 and 8.44 into Equation 8.42, we obtain
SL = 1 −e−(sμ−λ)D (sρ)sπ0
s!(1 −ρ)
(8.45)
The average waiting time of a caller can be computed by the formula
E(Wq) = E(Wq|Wq > 0)P(Wq > 0) =
(sρ)sπ0
(sμ −λ)(1 −ρ)s!
(8.46)

8-36
Operations Research Methodologies
Erlang C can also be used to estimate the number of agents needed to satisfy a desirable
SL. Here, the objective is to ﬁnd the minimum number of agents needed to achieve a given
SL. For example, suppose that the arrival rate is λ = 10 calls/min, the average service time
is 3 min/call (μ = 1/3), the acceptable waiting time is D = 5 min, and the desirable service
rate is 90%. If we denote Wq(λ, μ, s) as the waiting time of a caller with system parameters
λ, μ, and s, then we need to ﬁnd the minimum number of agents s∗such that
SL = P(Wq(10, 1/3, s∗) ≤5) ≥90%
To ease computation, various telecommunication service providers oﬀer free, online “cal-
culators” to evaluate performance measures such as SL given in Equation 8.45. For exam-
ple, performance analysis calculators for Erlang models are publicly available at www.
4callcenters.com and www.math.vu.nl/obp/callcenters. The results presented in Table 8.1
use the calculator available at the second listed URL, which is an Excel add-in package
called “CallCenterDSS.xla,” oﬀered by Professor G. Koole at Vrije University, Amsterdam.
In our computation, we ﬁrst compute SL for an acceptable waiting time D, for the ﬁxed
arrival rate λ, service rate μ, and number of agents s; we then compute the number of
agents s necessary to achieve an acceptable SL, for the ﬁxed arrival rate λ, service rate μ,
and acceptable waiting time D.
Example 8.17: The Erlang A model (continued)
Recall that Erlang A is an M/M/s/k + M queue, where s is the number of agents, k is the
number of trunks, s ≤k, and +M means that each caller has an independent patience time
that is exponentially distributed with rate, say θ. In principle, since Erlang A is a birth and
death process, its limiting distribution can be computed by Equations 8.37 and 8.38, using
the transition rates given in Equation 8.30. The expression of the limiting probabilities,
however, is complex, and shall not be given here. As mentioned, several Erlang calculators
are publicly available, and some of them are able to treat Erlang A. The results presented
in Table 8.2 use “CallCenterDSS.xla” from www.math.vu.nl/obp/callcenters. The table
shows that when callers become less patient, that is, when θ increases, the SL increases
(good), the percentage of abandonment increases (bad), the percentage of total calls lost
(i.e., the sum of the percentages of abandonment and calls blocked) increases (bad), and
the percentage of calls blocked decreases (good). When the number of trunks k increases,
TABLE 8.1
Calculating Performance Measures in Erlang C
Service Level
Number of Agents Needed
System Parameters: λ = 10, μ = 1/3, s = 35
System Parameters: λ = 10, μ = 1/3, D = 5
D : Acceptable Waiting
Service Level
Required SL
# of Agents
Time (minute)
%
%
Needed
2
73.08
75
34
4
74.53
80
35
6
75.91
85
36
8
77.21
90
38
10
78.44
95
40
TABLE 8.2
Calculating Performance Measures in Erlang A
System Parameters: λ = 10, μ = 1/3, s = 30, D = 5
θ: Patience Rate
k: # of Trunks
SL %
Abandonment %
Blocking %
Total Calls Lost %
3
32
87.79
5.90
5.46
11.36
6
32
89.18
8.46
3.36
11.82
3
35
80.24
10.35
0.50
10.85
6
35
85.38
11.54
0.09
11.63

Stochastic Processes
8-37
the SL decreases (bad), the percentage of calls blocked decreases (good), the percentage of
total calls lost decreases (good), and the percentage of abandonment increases (bad). Those
comparative statistics suggest that management must balance several competing criteria to
achieve eﬃcient call center operation.
■
8.4.5
Statistical Inference of Continuous-Time Markov Chains
The maximum likelihood procedure has been extensively used in the CTMC for parameter
estimation. We illustrate the idea by a special case of the birth and death process with
λ0 = λI, λi, = λB, for i ≥1 (i.e., the arrival rate when the system is idle is diﬀerent from the
arrival rate when the system is busy), and μi = μ, for i ≥1 (i.e., the service rate is a constant
regardless of the number of customers in the system). We want to estimate parameters λI,
λB, and μ.
Suppose that we have observed this special birth and death process for a total time t. Dur-
ing the period the system was idle for a total time tI and busy for a total time tB, such that
tI + tB = t. We also keep track of the following counts during the observation period [0, t]:
1. nI: the number of arrivals who observe an empty system (i.e., the number of type
0 →1 transitions). Note that nI can be considered as a random sample of the
number of Poisson events with rate λI during the observation time [0, tI];
2. nB: the number of arrivals who observe a busy system (i.e., the number of type
n →n + 1 transitions, n ≥1). Note that nB can be considered as a random sample
of the number of Poisson events with rate λB during the observation time [0, tB];
3. nd: The number of departures (i.e., the number of type n →n −1 transitions).
Note that nd can be considered as a random sample of the number of Poisson
events with rate μ during the observation time [0, tB].
Recall that we have shown in Section 8.2 that the maximum likelihood estimator of the
arrival rate in a Poisson process is ˆλ = n/t, where t is the observation period and n is the
number of events occurred during the period. Here, we again encounter the parameter esti-
mation issue of a Poisson process. However, our problem is more complicated since tI and tB
are random variables. Nevertheless, it can be shown that the maximum likelihood estimators
of λI, λB, and μ can be obtained as if tI and tB were constants (Bhat & Miller, 2002):
ˆλI = nI
tI
,
ˆλB = nB
tB
,
ˆμ = nd
tB
(8.47)
These estimators are indeed intuitively plausible. Note that the idea is to decompose the
process into three Poisson processes: the birth process when the system is empty, the birth
process when the system is busy, and the death process when the system is busy. Equation
8.47 then gives the estimate of the arrival rate in the underlying Poisson process. This idea
can be used to estimate the transition rates in the more general birth and death process.
When λI = λB = λ, the above simple birth and death process becomes an M/M/1 queue,
and the counts nI and nB can be pooled to arrive at a single estimate of λ:
ˆλ = nI + nB
t
(8.48)
Example 8.21
The manager of a supermarket needs to decide the staﬃng level of his store so that the
average waiting time of his customers is no more than 1 min. He feels that it is reasonable
to assume that customers arriving at his store follow a Poisson process, and each customer

8-38
Operations Research Methodologies
needs an exponential amount of time to be served at the checkout counter. To estimate the
traﬃc intensity of his store, he took observations for a 2-h period, during which he found 63
customers arrived at the counter and 44 customers left the counter. He also observed that
the checkout counter was completely empty for 9 min during the 2-h observation period.
The problem described above is an M/M/1 queue with arrival rate λ and service rate μ.
Using the notation given earlier, we have
t = 120 min,
tB = 111 min
nI + nB = 63,
nd = 44
From Equations 8.47 and 8.48, it can be estimated that
ˆλ = nI + nB
t
= 63
120 = 0.53,
ˆμ = nd
tB
= 44
111 = 0.40
The manager needs to determine the number of checkers to hire so that the average waiting
time of his customer is 1 min. For this, he needs to ﬁnd s so that E[Wq] = 1 min. From
Equation 8.46, s should be the smallest integer such that
E[Wq] =
1
sμ −λP(Wq > 0) ≤1 min
Substituting the estimated parameters ˆλ and ˆμ into the above expression, it is found that
the smallest integer to ensure the above condition is s = 3.
■
Next, we consider the hypothesis testing method for the DTMC in the context of the
simple birth and death process discussed earlier, based on the study of Billingsley (1961).
Suppose that we have observed the process for a duration t and collected statistics for tI, tB,
nI, nB, and nd. We are reasonably sure that the process is a birth and death process and we
wish to know whether there are convincing evidences to support the hypothesis H0 : λI = λ0
I,
λB = λ0B, μ = μ0. The log maximum likelihood function under the null hypothesis is
L(λ0
I, λ0
B, μ0) = nI ln λ0
I + nB ln λ0
B + nd ln μ0 −λ0
ItI −λ0
BtB −μ0tb + C
where C is a term involving nI, nB, and nd and independent of λ0
I, λ0
B, and μ0. The test
statistic for H0 follows χ2 distribution with 2 degrees of freedom:
2[maxL(λI, λB, μ) −L(λ0
I, λ0
B, μ0)] −2
⎡
⎢⎣
nI ln nI
λ0
ItI
+ nB ln
nB
λ0
BtB
+ nd ln
nd
μ0tB
−(nI + nB + nd) + λ0
ItI + λ0
BtB + μ0tB
⎤
⎥⎦(8.49)
Example 8.21 (continued)
Suppose the manager of the supermarket anticipates the average service time of his checkout
counter is 2 min. Further, he has assumed that the arrival rate is 0.5. To test the hypoth-
esis H0: λ = 0.5 and μ = 0.5, we use Equation 8.49 and ﬁnd that the χ2 statistic is 2.71,
with 2 degrees of freedom. Since P(χ2 ≥2.71) = 0.26 is large enough, it indicates that the
manager’s estimates of system parameters, λ = 0.5 and μ = 0.5, cannot be rejected.

Stochastic Processes
8-39
8.5
Renewal Theory
The stochastic process we have discussed so far, including the Poisson process, the DTMC
and the CTMC, all have the Markov property, that is, the future of the process from time
n (for the discrete-time case) or t (for the continuous-time case) onward depends only on
the state of the system at time n or t and is independent of the process’s history prior to
time n or t. The Markov property proves to be the key enabler to make the analysis of such
a system possible.
In this section, we discuss the stochastic processes that do not have the Markov property
at all observation points. In particular, we consider the stochastic processes that have the
Markov property only at some, possibly random, time epochs 0 = T0 ≤T1 ≤T2 ≤. . .. In other
words, only at times Tn, n = 0, 1, . . ., the future of the process can be predicted based
on the state of the system at time Tn. It turns out that the transient analysis of such
a process is much harder than the process with the Markov property. Therefore, in this
section we focus on the long-run behavior of the process. The processes we shall cover in
this section include renewal processes, renewal reward processes, regenerative processes, and
semi-Markov processes.
8.5.1
Renewal Processes
Recall that a Poisson process is a counting process in which the interarrival times between
successive events are iid random variables with an exponential distribution. As a general-
ization of the Poisson process, we deﬁne:
DEFINITION 8.3
A counting process {N(t), t ≥0} is said to be a renewal process if the
interarrival times between successive events, {X1, X2, . . .}, are a sequence of iid random
variables.
As deﬁned, X1 is the arrival epoch of the ﬁrst event, which follows some distribution F,
X2 is the time between the ﬁrst and second events, which follows the same distribution F,
and so on. Therefore, a renewal process depicts a sequence of events occurring randomly
over time, and N(t) represents the number of such events that occurred by time t. Let
S0 = 0 and
Sn =
n

j=1
Xj,
n = 1, 2, . . .
(8.50)
be the waiting time until the occurrence of the nth event. We shall call Sn the renewal epoch
of the nth event, n ≥1, and the event that occurred at a renewal epoch a renewal. Clearly,
the renewal process starts afresh, probabilistically, at renewal epochs Sn, n ≥0. In practice,
{N(t), t ≥0} and {Sn, n ≥1} are interchangeably called the renewal process.
It is often of interest to compute the expected number of renewals occuring by time t.
The function m(t) = E[N(t)] is known as the renewal function. For example, the renewal
function of a Poisson process with rate λ is m(t) = λt.
Example 8.22
The prototypical example of a renewal process is the successive replacements of light bulbs,
with the times required for replacements ignored. Suppose a new light bulb, with lifetime
X1, is put in use at time 0. At time S1 = X1, it is replaced by a new light bulb with lifetime
X2. The second light bulb fails at time S2 = X1 + X2 and is immediately replaced by the

8-40
Operations Research Methodologies
third one, and the process continues as such. It is natural to assume that the lifetimes of
light bulbs follow the same distribution and are independent of each other. Now, N(t) is
the number of light bulbs replaced up to time t, with mean m(t), and Sn is the time epoch
for the nth replacement.
■
Although in theory we can derive certain properties associated with N(t), Sn, and m(t)
(Wolﬀ, 1989), they are generally hard to evaluate for ﬁnite t or n unless the process has
the Markov property. For example, it is diﬃcult to know exactly the average number of
light bulbs replaced during a 1-year period except when the a light bulb has an exponential
lifetime. As a consequence, renewal theory is primarily concerned with the limiting behavior
of the process as t →∞. We ﬁrst investigate the limiting behavior of N(t)/t as t →∞. Let
μ be the reciprocal of the mean interarrival time:
E(Xj) = 1
μ
(8.51)
To avoid trivialities, we assume that 0 < E(Xj) < ∞. This assumption can be used to
establish the fact that N(t) is ﬁnite for ﬁnite t and goes to inﬁnity as t goes to inﬁnity.
THEOREM 8.4: The Elementary Renewal Theorem
1. limt→∞
N(t)
t
= μ with probability 1.
2. limt→∞
m(t)
t
= μ.
The elementary renewal theorem is intuitively plausible. Take the ﬁrst expression as an
example: the left side term, limt→∞N(t)/t, gives the long-run number of renewals per unit
time, and the right side term, μ, is the reciprocal of the expected time between renewals.
It becomes obvious that if each renewal occurs in average E(Xj) units of time, then the
number of renewals per unit time, in a long run, must approach to 1/E[Xj] = μ. Because
of the theorem, μ is called the renewal rate, since it can be thought of as the number of
renewals occurred per unit time in a long run.
The elementary renewal theorem, though simple and intuitive, proves to be extremely
powerful to analyze the long run behavior of non-Markov systems. We illustrate its appli-
cations with several examples.
Example 8.23: Age replacement policies
Consider an age replacement policy that calls for replacing an item when it fails or when its
age reaches T, whichever occurs ﬁrst. We shall call the policy Policy T. Suppose that the
lifetimes of successive items are iid random variables Y1, Y2, . . ., with distribution G. Under
policy T, the time the ith item spent in service is Xi = min{Yi, T}, i = 1, 2, . . ., with the
expectation
E[Xi] =
 ∞
0
P(min{Yi, T) > t)dt =
 T
0
P(Yi > t)dt
Theorem 8.4 then tells us that, in a long run, the replacements occur at the rate
μ =
1
E[Xi] =
1
 T
0 P(Yi > t)dt

Stochastic Processes
8-41
For example, suppose the lifetime is uniformly distributed between [0, 1] and T = 1/2. Then
μ =
1
E[Xi] =
1
 1/2
0
(1 −t)dt
= 3
8
■
Example 8.24: The M /G/1/1 queue
Suppose that patients arrive at a single-doctor emergency room (ER) in accordance with a
Poisson process with rate λ. An arriving patient will enter the room if the doctor is available;
otherwise the patient leaves the ER and seeks service elsewhere. Let the amount of time
the doctor treats a patient be a random variable having distribution G. We are interested
in the rate at which patients enter the ER and the service level of the ER, deﬁned as the
proportion of the arriving patients who are actually being treated.
To see how the system can be formulated as a renewal process, let us suppose that at
time 0 a patient has just entered the ER. We say that a renewal occurs whenever a patient
enters the ER. Let 1/μG be the mean time needed by the doctor to treat a patient. Then,
by the memoryless property of the exponential interarrival times, the mean time between
successive entering patients is
1
μ = 1
μG
+ 1
λ
and the rate at which patients enter the ER is
μ =
λμG
λ + μG
Since the patient arrival process is Poisson with rate λ, the service level of the ER is given by
μ
λ =
μG
λ + μG
■
8.5.2
Renewal Reward Processes
Consider a renewal process {N(t), t ≥0} with interarrival times {Xn, n ≥1}. Suppose that
associated with each interarrival time Xn is a reward Rn (or a proﬁt, i.e., reward-cost),
n ≥1. We allow Xn and Rn to be dependent (they usually are, as a reward may depend
on the length of the interval), but assume that the pairs (X1, R1), (X2, R2), . . . are iid
bivariate random variables. Let R(t) be the total reward received by the system over the
interval [0, t]:
R(t) =
N(t)

n=1
Rn
(8.52)
The reward process {R(t), t ≥0} is called the renewal reward process or cumulative pro-
cess. The expected cumulative reward up to time t is denoted by E[R(t)], t ≥0. In general,
it is very diﬃcult to derive the distribution of R(t) and to compute the expectation E[R(t)].
Fortunately, it is quite easy to compute the long-run reward rate, that is, the limit of R(t)/t
as t →∞.

8-42
Operations Research Methodologies
THEOREM 8.5: The Renewal Reward Theorem.
Let E[Xn] = E[X ] and E[Rn] = E[R]. If E[X ] < ∞and E[R] < ∞, then
1. limt→∞
R(t)
t
= E[R]
E[X], with probability 1.
2. limt→∞
E[R(t)]
t
= E[R]
E[X].
We shall call renewal intervals renewal cycles. With this terminology, the renewal reward
theorem states that the long-run average reward per unit time (or the expected reward per
unit time) is the expected reward per cycle divided by the expected cycle length. This simple
and intuitive result is very powerful to compute the long-run reward rate, or the long-run
proportion of time spent in diﬀerent states, as illustrated by the following examples.
Example 8.23: Age replacement policies (continued)
Let us assume that, under policy T, the cost for a planned replacement is cr, and that for
an unplanned replacement is cf, with cf > cr. The cost of the ith replacement, Ri, depends
on lifetime Yi as follows:
Ri =

cr
if Yi < T
cf
if Yi ≥T
Clearly, Ri depends on the in-service time of the ith item. The expected cost per
replacement is
E(R) = cfP(Yi ≤T) + crP(Yi > T)
The length of a cycle, as computed in Example 8.23, is E(X) =
 T
0 P(Yi > t)dt. We have
the long-run cost per unit time under policy T as
E(R)
E(X) = cfP(Yi ≤T) + crP(Yi > T)
 T
0 P(Yi > t)dt
Given the distribution of Yi and cost parameters cr and cf, the optimal age replacement
policy would be the value T that minimizes the preceding equation. For a numerical example,
let Yi be a uniform random variable over (0, 1) years, cr = 50, and cf = 100. Then
E(R)
E(X) = 100T + 50(1 −T)
T −T 2
2
Taking the derivative of the above equation and setting it to zero, we get T ∗=
√
3 −1 = 0.732
years. The annual cost under the optimal policy T ∗is $186.6/year.
How is policy T ∗compared with the “replace upon failure” policy? In this case the cost
rate is E(R)
E(Y ) = 100
0.5 = $200/year. Thus, even though the age replacement policy replaces the
item more often than the “replace upon failure” policy, it is actually more economical than
the latter.
■
Example 8.25: A simple continuous-review inventory model
In a continuous-review inventory system, the inventory level is continuously monitored and
information is updated each time a transaction takes place. In this setting, let us consider
a simple inventory system where customers arrive according to a renewal process with a
mean interarrival time 1/λ, and each customer requests a single item. Suppose that the

Stochastic Processes
8-43
system orders a batch of Q items each time the inventory level drops to zero. For simplicity,
the replenishment lead time is assumed to be zero. The operating costs incurred include a
ﬁxed set-up cost K each time an order is placed and a unit holding cost h for each unsold
item. The manager of the system needs to determine the optimal order quantity Q∗that
minimizes the long-run operating cost of the system.
To see how the problem can be formulated as a renewal reward process, deﬁne a cycle
as the time interval between two consecutive orders. The expected length of a cycle is the
expected time required to receive Q orders. Since the mean interarrival time of demand
is 1/λ,
E[length of a cycle] = Q
λ
Let Yn be the time between the (n −1)st and nth demands in a cycle, n = 1, . . ., Q. Since,
during the period Yn, the system holds (Q −n + 1) units of the item, the cost per cycle is
E[cost of a cycle] = K + E
" Q

n=1
h(Q −n + 1)Yn
#
= K + hQ(Q + 1)
2λ
Hence, the long-run average cost of the system is
E[cost of a cycle]
E[length of a cycle] = λK
Q + h(Q + 1)
2
Treating Q as a continuous variable and using a calculus, then the above expression is
minimized at
ˆQ =

2λK
h
If ˆQ is an integer, then it is the optimal batch size; otherwise, the optimal batch size is
either the largest integer smaller than ˆQ or the smallest integer larger than ˆQ, whichever
yields a smaller value in the cost function.
■
Example 8.26: Prorated warranty
This example is based on Example 7.11 in Kulkarni (1999). A tire company issues a 50,000
mile prorated warranty on its tires as follows: if the tire fails after its mileage life, denoted
by L, exceeds 50,000 miles, then the customer has to pay $95 for a new tire. If the tire fails
before it reaches 50,000 miles, that is, L ≤50,000, the customer pays the price of a new tire
according to the prorated formula $95 × (L/50,000). Suppose that the customer continues
to buy the same brand of tire after each failure. Let L follow the distribution
f(x) = 2 × 10−10x,
for 0 ≤x ≤100, 000 miles
The customer has the option to buy the tire without warranty for $90. Should the consumer
purchase the warranty (we assume that the customer either always gets the warranty or
never gets the warranty)? First suppose that the customer never purchases the warranty.
The cycle is deﬁned as each purchase of a new tire, with the mean
E(L) =
 100,000
0
2 × 10−10x2dx = 2 × 10−10 x3
3
&&&&
100,000
0
= 2
3 × 105
Since each new tire costs the customer $90, the average cost rate per mile is
E(R)
E(L) =
$90
2
3 × 105 = $135 × 10−5 per mile

8-44
Operations Research Methodologies
or $1.35 per 1000 miles. Now consider the option of always purchasing the tires under
the warranty. The cycle is the same as before. However, the prorated warranty implies
R = $95 × min{50,000,L}
50,000
. Hence
E(R) =
 50,000
0
2 × 10−10 ×
95x2
50, 000dx +
 100,000
50,000
2 × 10−10 × 95x dx = $87.08
The long-run cost under the warranty is
97.08
2
3 × 105 = $130.63 × 10−5 per mile
or $1.31 per 1000 miles. This implies that the customer should buy the warranty.
■
8.5.3
Regenerative Processes
Consider a stochastic process {Z(t), t ≥0} deﬁned on S = {0, 1, . . .} having the property
that the process starts afresh, probabilistically, at (possibly random) time epochs Tn, n ≥1.
This means, there exist time epochs Tn, n ≥1, such that the evolution of the process from
Tn onward follows the same probability law as the process that starts at time Tn−1. We call
such a process a regenerative process, the time epochs {Tn, n ≥1} regenerative epochs, and
Yn = Tn −Tn−1, n ≥1, regenerative cycles. One may envision that {Tn, n ≥1} constitute the
arrival times of a renewal process, and {Yn, n ≥1} the interarrival times. Indeed, a renewal
process is an example of a regenerative process. Another example is a recurrent Markov
chain, where T1 represents the time of the ﬁrst transition into the initial state.
A very useful result about the long-run behavior of a regenerative process states that
lim
t→∞P(Z(t) = i) = E[amount of time in state i in a cycle]
E[length of a cycle]
(8.53)
Example 8.27:
Suppose that an irreducible, positive recurrent CTMC {X(t), t ≥0} starts in state i. By
the Markov property, the process starts over again each time it re-enters state i. Thus Tn,
n ≥1, where Tn denotes the nth time the CTMC returns to state i, constitute regenerative
epochs. Let μii be the mean recurrent time of state i. From Equation 8.53,
lim
t→∞P(X(t) = i) = E[amount of time in state i during a recurrent time of state i]
μii
As each time the CTMC visits state i, it stays there for an exponential amount of time with
rate νi, we have
lim
t→∞P(X(t) = i) = 1/vi
μii
■
Example 8.25: A simple continuous-review inventory model (continued)
Suppose that in the continuous-review inventory system discussed in Example 8.25, we are
interested in the limiting distribution of I(t), where I(t) is the number of units on hand
at time t. Suppose that at time 0 there are Q units on hand, and each time inventory on
hand drops to 0 we order a new batch of size Q. Then the inventory process {I(t), t ≥0} is

Stochastic Processes
8-45
a regenerative process, which regenerates itself each time a new order is placed. Then, for
i = 1, 2, . . ., Q,
lim
t→∞P(I(t) = i) = E[amount of time i units on hand during a reorder cycle]
E[length of a reordering cycle]
=
1/λ
(1/λ)Q = 1
Q
That is, inventory on hand is a discrete uniform random variable between 1 and Q.
■
8.5.4
Semi-Markov Processes
Consider a process {X(t), t ≥0} that can be in any of the ﬁnite states {0, 1, 2, . . ., N}. Sup-
pose that each time the process enters state i, it remains there for a random amount of time
with rate μi and then makes a transition to state j with probability pij.The sojourn time in
a state and the next state reached do not need to be independent. Such a process is called
the semi-Markov process (SMP). Let {Tn, n ≥1}, where T0 = 0, be the sequence of epochs
at which the process makes transitions (returning to the same state is allowed). An impor-
tant property of a semi-Markov process is that the process at each of the transition epochs
Tn, n ≥1, has the Markov property. That is, for each Tn, the evolution of the process from
Tn onward depends only on X(Tn), the state of the process observed at transition time Tn.
Clearly, if the sojourn time in each state is identically 1, then the SMP is just a DTMC, and if
the sojourn time in each state is exponentially distributed, then the SMP becomes a CTMC.
Let Mj(t) be the total time that the SMP spends in state j up to time t. We deﬁne
pj = lim
t→∞
Mj(t)
t
as the long-run proportion of time that the SMP is in state j, j = 0, 1, . . ., N. Let us ﬁnd
those proportions. To do this, we denote Xn = X(Tn) as the nth state visited by the SMP,
n ≥1. Note that {Xn, n ≥1} is a DTMC governed by the transition matrix P = {pij}.
This DTMC shall be called the embedded DTMC of the SMP. For simplicity, let us assume
that the embedded DTMC is irreducible so that the limiting (or stationary) distribution
π = {π0, π1, . . ., πn} of the embedded DTMC exists. From the result of Section 8.3, π will
be the unique non-negative solution of
πj =
N

i=0
πipij
(8.54)
N

j=0
πj = 1
(8.55)
Now, since the proportion of transitions that the SMP enters state j is πj, and it remains
in state j for an average 1/μj units of time in each of such transitions, it is intuitively
plausible, and indeed can be shown formally, that pj is given by
pj =
πj/μj

j πj/μj
,
j = 0, 1, . . ., N
(8.56)
Example 8.26: Machine maintenance
We now take the repair times of the reliability system under policy T into account. Suppose
that under policy T, an item in service is repaired upon failure (emergency renewal) or at

8-46
Operations Research Methodologies
age T (preventive renewal). Suppose that the lifetime of the item is L, and emergency and
preventive renewals take random times Ze and Zp, respectively.
The aim is to determine the long-run availability of the system. The system can be in
one of the three states, 0 (up), 1 (emergency renewal), and 2 (preventive renewal). Then
the system can be modeled as a SMP with its embedded DTMC having the transition
probability matrix
P =
⎛
⎝
0
P(L < T)
P(L ≥T)
1
0
0
1
0
0
⎞
⎠
The limiting distribution of the embedded CTMC can be obtained as π0 = 1/2, π1 =
P(L < T)/2, and π2 = P(L ≥T)/2. The mean sojourn time of the system in each state is
1
μ0
= E[min(L, T)] =
 T
0
P(L > l)dl, 1
μ1
= E[Ze], 1
μ2
= E[Zp]
According to Equation 8.56, the stationary availability of the system is
A(T) =
 T
0 P(L > l)dl
 T
0 P(L > l)dl + E[Ze]P(L < T) + E[Zp]P(L ≥T)
It is practically important that the system availability does not depend on the distributions
of repair times Ze and Zp, but only on their expected values.
■
8.5.5
Statistical Inference of Renewal Processes
Let {N(t), t ≥0} be a counting process with the times between successive events denoted
by {Xn, n ≥1}. If {N(t), t ≥0} is a renewal process, we require that {Xn, n ≥1} is a
sequence of iid random variables from some distribution F. Given that we have observed
a sample of interarrival event times, (x1, x2, . . ., xn), statistical inference problems become
choosing an appropriate distribution function F and estimating its parameters, which can
be done by using standard statistical theory. We will not elaborate on the details of those
methods here. We would like, however, to provide the reader with several useful references
for further reading. Parametric inferences were studied by Barlow & Proschan (1981), and
nonparametric inferences were considered by Karr (1991). Miller & Bhat (1997) presented
several sampling methods to estimate the parameters of F when the inter-event times are
not directly observable. Basawa & Prakasa Rao (1980) and Bhat & Miller (2002) also contain
useful materials on this subject.
8.6
Software Products Available for Solving
Stochastic Models
Both special-purpose and general-purpose software products are available to solve stochastic
models, either numerically or symbolically. The special-purpose stochastic modeling soft-
ware products, listed below, focus on Markov process analyses and their applications in
diﬀerent areas such as queueing, reliability, and telecommunications. Most of them have
simulation capabilities.
1. Probabilistic Symbolic Model Checker (PRISM): PRISM is a free and
open source software developed at the University of Birmingham. PRISM is a tool
for modeling and analyzing probabilistic systems including DTMCs, CTMCs,

Stochastic Processes
8-47
and Markov decision processes (MDPs). PRISM uses a module-based system
description language for model inputs. It then translates the system descriptions
into an appropriate model and provides performance statistics. Hinton et al.
(2006) provide an overview of the main features of PRISM. The Web site of
PRISM is www.cs.bham.ac.uk/dxp/prism/index.php.
2. MCQueue: This is a free educational software for Markov Chains and queues (no
commercial use). This software package contains two modules. The ﬁrst module
is for the analysis of DTMCs and CTMCs up to 100 states. The other module cal-
culates performance measures for basic queueing models (e.g., M /G/1, M /M /s,
M /D/s, G/M /s, and M /M /s/s+M queues). The algorithms in this package are
based on the methods discussed in Tijms (2003). The package can be downloaded
from http://staﬀ.feweb.vu.nl/tijms/.
3. MARkov Chain Analyzer (MARCA): MARCA is developed at North
Carolina State University. It facilitates the generation of large Markov chain mod-
els. It also has a set of Fortran subroutines to support model construction. Order-
ing information is available at www.csc.ncsu.edu/faculty/stewart/MARCA/
marca.html.
4. Queueing theory is an important application area of stochastic processes. Not
surprisingly, quite a few software products are available for queueing analysis
and applications in telecommunications. A list of queueing software packages has
been compiled by Dr. M. Hlynka at the University of Windsor and is available
at www2.uwindsor.ca/hlynka/qsoft.html.
5. Several software packages are available for reliability applications. For exam-
ple, Relex Markov by Relex Software Corporation (www.relexsoftware.co.uk/
products/markov.ph), Markov Analysis by ITEM Software (www.itemuk.
com/markov.html), and SMART by University of California at Riverside
(www.cs.ucr.edu/ ciardo/SMART/).
6. @Risk and Crystal Balls are Excel add-ins that analyze stochastic models
by Monte Carlo simulation. Excel, with the aid of VBA, can also be used in
Monte Carlo simulation. Arena, ProModel, and Extend are popular software
packages designed for discrete-event simulations.
Several general-purpose, programming language based software products, although not
speciﬁcally developed for the stochastic modeling purpose, are popular in solving such prob-
lems. These include the proprietary computational packages such as MATLAB, Maple,
and Mathematica for general mathematical computation and open source software pack-
ages such as R (sometimes described as GNU S) for statistical computing and graphics.
Some textbooks provide computer program codes (e.g., Kao, 1997, who uses MATLAB).
Statistical packages such as SAS and Minitab are also powerful in analyzing stochastic
models.
References
1. Anderson, T. W. (1954), “Probability models for analyzing time changes in attitudes.”
In P. E. Lazarsfeld (Ed.), Mathematical Thinking in the Social Sciences, Free Press,
Glencoe, IL.
2. Aven, T. and Jensen, U. (1999), Stochastic Models in Reliability, Springer, New York.
3. Barlow, R. E. and Proschan, F. (1981), Statistical Theory of Reliability and Life Testing,
To Begin With, Silver Spring, MD.

8-48
Operations Research Methodologies
4. Bartholomew, D. J. (1982), Stochastic Models for Social Processes, Wiley Series in
Probability and Statistics, John Wiley & Sons, New York.
5. Basawa, I. V. and Prakasa Rao, B. L. S. (1980), Statistical Inference for Stochastic
Processes, Academic Press, New York.
6. Bhat, U. N. and Miller, G. K. (2002), Elements of Applied Stochastic Processes, Wiley
Series in Probability and Statistics, 3rd ed., John Wiley & Sons, New York.
7. Billingsley, P. (1961), Statistical Inference for Markov Processes, University of Chicago
Press, Chicago.
8. Bolch, G., Greiner, S., and de Meer, H. (2006), Queueing Networks and Markov Chains:
Modeling and Performance Evaluation with Computer Science Applications, 2nd ed.,
Wiley-Interscience, New York.
9. Buzacott, J. A. and Shanthikumar, J. G. (1993), Stochastic Models of Manufacturing
Systems, Prentice Hall, Englewood Cliﬀs, NJ.
10. Cinlar, E. (1975), Introduction to Stochastic Processes, Prentice Hall, Englewood Cliﬀs, NJ.
11. Gans, N., Koole, G., and Mandelbaum, A. (2003), “Telephone call centers: Tutorial, review,
and research prospects,” Manufacturing & Service Operations Management 5, 79–141.
12. Gautam, N. (2003), Stochastic Models in Telecommunications, Handbook of Statistics
21—Stochastic Processes: Modeling and Simulation (D. N. Shanbhag and C. R. Rao
(eds.)), North-Holland, Amsterdam.
13. Glass, D. F. (Ed.) (1954), Social Mobility in Britain, London: Routledge & Kegan Paul.
14. Gnedenko, B. V., Belyayev, Y. K., and Solovyev, A. D. (1969), Mathematical Models in
Reliability, Academic Press, New York.
15. Goel, N. S. and Richter-Dyn, N. (2004), Stochastic Models in Biology, The Blackburn
Press, Caldwell, NJ.
16. Green, L. V. and Kolesar, P. J. (1989), “Testing the validity of a queueing model of police
control,” Management Science 35, 127–148.
17. Hand, D. J. (1994), A Handbook of Small Data Sets, D. J. Hand, Fergus Daly, D. Lunn,
K. McConway and E. Ostrowski (Eds.), Chapman & Hall, London.
18. Helbing, D. and Calek, R. (1995), Quantitative Sociodynamics: Stochastic Methods
and Models of Social Interaction Processes (Theory and Decision Library B), Springer,
New York.
19. Hinton, A., Kwiatkowska, M., Norman, G., and Parker, D. (2006), “PRISM: A tool for
automatic veriﬁcation of probabilistic systems.” In Hermanns and Palsberg (ed.), Proceed-
ings of the 12th International Conference on Tools and Algorithms for the Construction and
Analysis of Systems (TACAS’06), Vol. 3920 of LNCS, Springer, New York, pp. 441–444.
20. Kao, E. P. C. (1997), An Introduction to Stochastic Processes, Duxbury Press, New York.
21. Karr, A. F. (1991), Point Processes and Their Statistical Inference, 2nd ed., Marcel
Dekker, New York.
22. Kijima, M. (2003), Stochastic Processes with Applications to Finance, Chapman &
Hall/CRC, Boca Raton, FL.
23. Kleinrock, L. (1976), Queueing Systems, John Wiley & Sons, New York.
24. Kulkarni, V. G. (1995), Modeling and Analysis of Stochastic Systems. Texts in Statistical
Science Series, Chapman & Hall, London.
25. Kulkarni, V. G. (1999), Modeling, Analysis, Design, and Control of Stochastic Systems.
Springer Texts in Statistics, Springer, New York.
26. Lazarsfeld, P. E., Berelson, B., and Gaudet, H. (1948), The People’s Choice, Columbia
University Press, New York.
27. Lewis, T. (1986), “M345 statistical methods, unit 2: Basic methods: Testing and
estimation,” Milton Keynes, Buckinghamshire, England: Open University Vol. 16.

Stochastic Processes
8-49
28. Miller, G. K. and Bhat, U. N. (1997), “Estimation for renewal processes with unobservable
gamma or erlang interarrival times,” Journal of Statistical Planning and Inference 61,
355–372.
29. Nelson, B. L. (1995), Stochastic Modeling: Analysis and Simulation, McGraw-Hill Series
in Industrial Engineering and Management Science, New York.
30. Porteus, E. L. (2002), Foundations of Stochastic Inventory Systems, Stanford University
Press, Stanford, CA.
31. Rolski, T., Schmidli, H., Schmidt, V., and Teugels, J. (1999), Stochastic Processes for
Insurance and Finance, Wiley Series in Probability and Statistics, John Wiley & Sons.
32. Ross, S. M. (1996), Stochastic Processes, 2nd ed., John Wiley & Sons, New York.
33. Ross, S. M. (2003), Probability Models, 8th ed., Academic Press, London.
34. Taylor, H. M. and Karlin, S. (1994), An Introduction to Stochastic Modeling, 2nd ed.,
Academic Press, London.
35. Tijms, H. C. (1994), Stochastic Models: An Algorithmic Approach, John Wiley & Sons,
New York.
36. Tijms, H. C. (2003), The First Course in Stochastic Models, John Wiley & Sons.
37. Whitaker, D. (1978), “The derivation of a measure of brand loyalty using a Markov brand
switching model,” Journal of the Operational Research Society 29, 959–970.
38. Whittle, P. (1955), “Some distribution and moment formulae for the Markov chain,”
Journal of the Royal Statistical Society B17, 235–242.
39. Wolﬀ, R. W. (1989), Stochastic Modeling and the Theory of Queues, Prentice-Hall Interna-
tional Series in Industrial and Systems Engineering, Prentice Hall, Englewood Cliﬀs, NJ.
40. Zipkin, P. (2000), Foundations of Inventory Management, McGraw-Hill/Irwin, New York.


9
Queueing Theory
Natarajan Gautam
Texas A&M University
9.1
Introduction............................................
9-1
9.2
Queueing Theory Basics..............................
9-2
Fundamental Queueing Relations • Preliminary Results
for the GI/G/s Queue
9.3
Single-Station and Single-Class Queues ............
9-8
The Classic M /M /1 Queue: Main Results • The
Multiserver System: M /M /s • Finite Capacity
M /M /s/K System • The M /M /1/K Queue • The
M /M /s/s Queue • The Inﬁnite Server M /M /∞
Queue • Finite Population Queues • Bulk Arrivals and
Service • The M /G/1 Queue • The G/M /1 Queue •
The G/G/1 Queue • The G/G/m Queue
9.4
Single-Station and Multiclass Queues .............. 9-21
Multiclass G/G/1 Queue: General Results • M /G/1
Queue with Multiple Classes
9.5
Multistation and Single-Class Queues .............. 9-28
Open Queueing Networks: Jackson Network • Closed
Queueing Networks (Exponential Service Times) •
Algorithms for Nonproduct-Form Networks
9.6
Multistation and Multiclass Queues ................ 9-34
Scenario • Notation • Algorithm
9.7
Concluding Remarks .................................. 9-37
Acknowledgments ........................................... 9-38
References .................................................... 9-38
9.1
Introduction
What do a fast food restaurant, an amusement park, a bank, an airport security check
point, and a post oﬃce all have in common? Answer: you are certainly bound to wait in
a line before getting served at all these places. Such types of queues or waiting lines are
found everywhere: computer-communication networks, production systems, transportation
services, and so on. To eﬃciently utilize manufacturing and service enterprises, it is critical
to eﬀectively manage queues. To do that, in this chapter, we present a set of analytical
techniques collectively called queueing theory. The main objective of queueing theory is to
develop formulae, expressions, or algorithms for performance metrics, such as the average
number of entities in a queue, mean time spent in the system, resource availability, probabil-
ity of rejection, and the like. The results from queueing theory can directly be used to solve
design and capacity planning problems, such as determining the number of servers, an opti-
mum queueing discipline, schedule for service, number of queues, system architecture, and
9-1

9-2
Operations Research Methodologies
TABLE 9.1
Examples to Illustrate Various Types
of Queueing Systems
Single-Class
Multiclass
Single-station
Post oﬃce
Multi-lingual call center
Multistation
Theme park
Multi-ward hospital
the like. Besides making such strategic design decisions, queueing theory can also be used
for tactical as well as operational decisions and controls.
The objective of this chapter is to introduce fundamental concepts in queues, clarify
assumptions used to derive results, motivate models using examples, and point to software
available for analysis. The presentation in this chapter is classiﬁed into four categories
depending on the types of customers (one or many) and number of stations (one or many).
Examples of the four types are summarized in Table 9.1.
The results presented in this chapter are a compilation of several excellent books and
papers on various aspects of queueing theory. In particular, the bulk of the single-station
and single-class analysis (which forms over half the chapter) is from Gross and Harris [1],
which arguably is one of the most popular texts in queueing theory. The book by Bolch
et al. [2] does a fantastic job presenting algorithms, approximations, and bounds, espe-
cially for multistage queues (i.e., queueing networks). For multiclass queues, the founda-
tions are borrowed from the well-articulated chapters of Wolﬀ[3] as well as Buzacott and
Shanthikumar [4]. The set of papers by Whitt [5] explaining the queueing network analyzer
is used for the multistage and multiclass queues. Most of the notation used in this chapter
and the fundamental results are from Kulkarni [6]. If one is interested in a single site with
information about various aspects of queues (including humor!), the place to visit is the page
maintained by Hlynka [7]. In fact, the page among other things illustrates various books on
queueing, course notes, and a list of software. A few software tools would be pointed out
in this chapter, but it would be an excellent idea to visit Hlynka’s site [8] for an up-to-date
list of queueing software. In there, software that run on various other applications (such as
MATLAB, Mathematica, Excel, etc.) are explained and the most suitable one for the reader
can be adopted.
This chapter is organized as follows. First, some basic results that are used throughout
the chapter are explained in Section 9.2. The bulk of this chapter is Section 9.3, which lays
the foundation for the other types of systems by initially considering the single-station and
single-class queue. Then in Section 9.4, the results for single-station and multiple classes are
presented. Following that, the chapter moves from analyzing a single station to a network of
queues in Section 9.5 where only one class is considered. This is extended to the most general
form (of which all the previous models are special cases) of multistation and multiclass queue
in Section 9.6. Finally, some concluding remarks are made in Section 9.7.
9.2
Queueing Theory Basics
Consider a single-station queueing system as shown in Figure 9.1. This is also called a
single-stage queue. There is a single waiting line and one or more servers. A typical example
can be found at a bank or post oﬃce. Arriving customers enter the queueing system and
wait in the waiting area if a server is not free (otherwise they go straight to a server). When
a server becomes free, one customer is selected and service begins. Upon service completion,
the customer departs the system. A few key assumptions are needed to analyze the basic
queueing system.

Queueing Theory
9-3
Waiting room
Arrivals
Server(s)
Departures
FIGURE 9.1
A single-station queueing system.
TABLE 9.2
Fields in the Kendall Notation
AP
M, G, Ek, H, PH, D, GI, etc.
ST
M, G, Ek, H, PH, D, GI, etc.
NS
denoted by s, typically 1, 2, . . ., ∞
Cap
denoted by k, typically 1, 2, . . ., ∞
default: ∞
SD
FCFS, LCFS, ROS, SPTF, etc.
default: FCFS
ASSUMPTION 9.1
The customer interarrival times, that is, the time between arrivals,
are independent and identically distributed (usually written as “iid”). Therefore, the arrival
process is what is called a renewal process. All arriving customers enter the system if there
is room to wait. Also, all customers wait till their service is completed in order to depart.
ASSUMPTION 9.2
The service times are independent and identically distributed random
variables. Also, the servers are stochastically identical; that is, the service times are sampled
from a single distribution. In addition, the servers adopt a work-conservation policy; that
is, the server is never idle when there are customers in the system.
The above assumptions can certainly be relaxed. There are a few models that do not
require the above assumptions. However, for the rest of this chapter, unless explicitly stated
otherwise, we will assume that Assumptions 9.1 and 9.2 hold.
To standardize description for queues, Kendall developed a notation with ﬁve ﬁelds:
AP/ST/NS/Cap/SD. In the Kendall notation, AP denotes arrival process characterized by
the interarrival distribution, ST denotes the service time distribution, NS is the number of
servers in the system, Cap is the maximum number of customers in the whole system (with a
default value of inﬁnite), and SD denotes service discipline which describes the service order
such as ﬁrst come ﬁrst served (FCFS), which is the default, last come ﬁrst served (LCFS),
random order of service (ROS), shortest processing time ﬁrst (SPTF), and so on. The ﬁelds
AP and ST can be speciﬁc distributions such as exponential (denoted by M which stands
for memoryless or Markovian), Erlang (denoted by Ek), phase-type (PH ), hyperexponential
(H), deterministic (D), and so on. Sometimes, instead of a speciﬁc distribution, AP and
ST ﬁelds could be G or GI, which denote general distribution (although GI explicitly says
“general independent,” G also assumes independence). Table 9.2 depicts values that can be
found in the ﬁve ﬁelds of Kendall notation.
For example, GI/H/4/6/LCFS implies that the arrivals are according to a renewal process
with general distribution, service times are according to a hyperexponential distribution,
there are four servers, a maximum of six customers are permitted in the system at a time
(including four at the server), and the service discipline is LCFS. Also, M/G/4/9 implies
that the interarrival times are exponential (whereby the arrivals are according to a Poisson

9-4
Operations Research Methodologies
process), service times are according to some general distribution, there are four servers,
the system capacity is nine customers in total, and the customers are served according to
FCFS. Finally, in an M/M/1 queue, the arrivals are according to a Poisson process, service
times exponentially distributed, there is one server, the waiting space is inﬁnite and the
customers are served according to FCFS.
9.2.1
Fundamental Queueing Relations
Consider a single-station queueing system such as the one shown in Figure 9.1. Assume
that this system can be described using Kendall notation. That means the interarrival
time distribution, service time distribution, number of servers, system capacity, and
service discipline are given. For such a system we now describe some parameters and
measures of performance. Assume that customers (or entities) that enter the queueing
system are assigned numbers with the nth arriving customer called customer-n. Most of
the results presented in this section are available in Kulkarni [6] with possibly diﬀerent
notation.
In that light, let An denote the time when the nth customer arrives, and thereby
An −An−1, an interarrival time. Let Sn be the service time for the nth customer. Let
Dn be the time when the nth customer departs. We denote X(t) as the number of cus-
tomers in the system at time t, Xn as the number of customers in the system just after the
nth customer departs, and X∗
n as the number of customers in the system just before the
nth customer arrives. Although in this chapter we will not go into details, it is worthwhile
mentioning that X(t), Xn, and X∗
n are usually modeled as stochastic processes. We also
deﬁne two other variables, which are usually not explicitly modeled but can be characterized
in steady state. These are Wn, the waiting time of the nth customer and W(t), the total
remaining workload at time t (this is the total time it would take to serve all the customers
in the system at time t). The above variables are described in Table 9.3 for easy reference,
where customer n denotes the nth arriving customer.
It is usually very diﬃcult to obtain distributions of the random variables X(t), Xn, X∗
n,
W(t), and Wn. However, the corresponding steady-state values can be obtained, that is, the
limiting distributions as n and t go to inﬁnite. In that light, let pj be the probability that
there are j customers in the system in steady state, and let πj and π∗
j be the respective
probabilities that in steady state a departing and an arriving customer would see j other
customers in the system. In addition, let G(x) and F(x) be the cumulative distribution
functions of the workload and waiting time respectively in steady state. Finally, deﬁne L as
the time-averaged number of customers in the system, and deﬁne W as the average waiting
time (averaged across all customers). One of the primary objectives of queueing models is
to obtain closed-form expressions for the performance metrics pj, πj, π∗
j , G(x), F(x), L,
TABLE 9.3
Variables and Their Mathematical and English
Meanings
Mathematical
Variable
Expression
Meaning
An
Arrival time of customer n
Sn
Service time of customer n
Dn
Departure time of customer n
X(t)
Number of customers in the system at time t
Xn
X(Dn+)
No. in system just after customer n’s departure
X∗
n
X(An−)
No. in system just before customer n’s arrival
Wn
Dn −An
Waiting time of customer n
W (t)
Total remaining workload at time t

Queueing Theory
9-5
and W. These performance metrics can be mathematically represented as follows:
pj = lim
t→∞P{X(t) = j}
πj = lim
n→∞P{Xn = j}
π∗
j = lim
n→∞P{X∗
n = j}
G(x) = lim
t→∞P{W(t) ≤x}
F(x) = lim
n→∞P{Wn ≤x}
L = lim
t→∞E[X(t)]
W = lim
n→∞E[Wn]
Let λ be the average number of customers that enter the queueing system per unit time,
referred to as the mean entering rate. Note that if the system capacity is ﬁnite, all arriving
customers do not enter and therefore λ is speciﬁcally referred to as average rate of “entering”
and not “arrival.” The relation between L and W is given by Little’s law (a result by Prof.
John D.C. Little of MIT):
L = λW
(9.1)
It is important to note two things. First, for the ﬁnite capacity case W must be inter-
preted as the mean time in the system for customers that actually “enter” the system (and
does not include customers that were turned away). Second, note that the average rate of
departure from the system (if the system is stable) is also λ. This is called conservation
of customers, whereby customers are neither created nor destroyed; therefore the average
customer entering rate equals average customer departure rate (if the system is stable).
We now focus our attention on inﬁnite capacity queues in the next section. However, while
we present results, if applicable, ﬁnite capacity queues’ extensions will be explained. In addi-
tion, in future sections too we will mainly concentrate on inﬁnite capacity queues (with some
exceptions) due to issues of practicality and ease of analysis. From a practical standpoint,
if a queue actually has ﬁnite capacity but the capacity is seldom reached, approximating
the queue as an inﬁnite capacity queue is reasonable.
9.2.2
Preliminary Results for the GI/G/s Queue
Deﬁne the following for a single-stage GI /G/s queue (interarrival times independent and
identically distributed, service time any general distribution, s servers, inﬁnite waiting room,
FCFS service discipline):
• λ: Average arrival rate into the system (inverse of the average time between two
arrivals); notice that as the capacity is ﬁnite, all customers that arrive, also enter
the system.
• μ: Average service rate of a server (inverse of the average time to serve a cus-
tomer); it is important that the units for λ and μ be the same; that is, both
should be per second or both should be per minute, and so on.
• Lq: Average number of customers in the queue, not including ones in service
(L deﬁned earlier, includes the customers at the servers).

9-6
Operations Research Methodologies
• Wq: Average time spent in the queue, not including in service (W deﬁned earlier,
includes customer service times). Note that, in units of 1/μ,
W = Wq + 1
μ
(9.2)
• ρ = λ
sμ: the traﬃc intensity, which is a dimensionless quantity.
It is important to note that while extending the results to ﬁnite capacity queues, all the
above deﬁnitions pertain only to customers that enter the system (and do not include
those that were turned away when the system was full). However, the ﬁrst result below is
applicable only for inﬁnite capacity queues as ﬁnite capacity queues are always stable.
RESULT 9.1
A necessary condition for stability of a queueing system is
ρ ≤1
For most cases, the above condition is also suﬃcient (the suﬃcient condition actually is
ρ < 1). However, in the case of queues with multiclass traﬃc that traverses through multi-
station queues, this condition may not be suﬃcient.
Little’s Law and Other Results Using Little’s Law
As described in the previous section, we once again present Little’s law, here λ = λ.
RESULT 9.2
For a GI/G/s queue,
L = λW
(9.3)
and
Lq = λWq
(9.4)
Notice that if we can compute one of L, Lq, W, or Wq, the other three can be obtained using
the above relations. Little’s law holds under very general conditions. In fact, even the service
discipline does not have to be FCFS and the servers do not need to be work conserving.
The result holds for any system with inputs and outputs. As an example, Equation 9.4 is
nothing but using Little’s law for the waiting space and not including the server. Note that
if the system is stable, the output rate on an average is also λ. Using Little’s law, some more
interesting results can be obtained.
RESULT 9.3
The probability that a particular server is busy pb is given by
pb = ρ
which can be derived from W = Wq + 1/μ and Little’s law via the relation L = Lq + λ/μ.
Also, for the special single server case of s = 1, that is, GI/G/1 queues, the probability that
the system is empty, p0 is
p0 = 1 −ρ
Based on the deﬁnition of L, it can be written as
L =
∞

j=0
jpj

Queueing Theory
9-7
In a similar manner, let L(k) be the kth factorial moment of the number of customers in
the system in steady state, that is,
L(k) =
∞

j=k
k!

j
k

pj
Also, let W (k) be the kth moment of the waiting time in steady state, that is,
W (k) = lim
n→∞E

{Wn}k
Little’s law can be extended for the M/G/s queue in the following manner.
RESULT 9.4
For an M/G/s queue,
L(k) = λkW (k)
(9.5)
Of course, the special case of k = 1 is Little’s law itself. However, the interesting result
is that all moments of the queue lengths are related to corresponding moments of waiting
times. Notice that from factorial moments it is easy to obtain actual moments.
Limiting Distributions of X (t), X n, and X ∗
n
In some situations it may not be possible to obtain pj easily. But it may be possible to get
πj or π∗
j . In that light, two results based on the limiting distributions (πj, π∗
j , and pj)
will be presented. The ﬁrst result relates the Xn and X∗
n processes in the limit (i.e., the
relation between πj and π∗
j ). The second result illustrates the relation between the limiting
distributions of X(t) and X∗
n (i.e., pj and π∗
j ). They hold under very general cases beyond
the cases presented here. However, one must be very careful while using the results in
the more general situations.
RESULT 9.5
Let πj and π∗
j be as deﬁned earlier as the limiting distributions of Xn and
X∗
n, respectively. When either one of those limits exists, so does the other and
πj = π∗
j
for all j ≥0
It can easily be shown that the limits described in the above result exists for queue length
processes of M/M/1, M/M/s, M/G/1, and G/M/s queueing systems. However, the limit
for the more general G/G/s case is harder to show but the result does hold. The result also
holds for the G/G/s/k case, whether we look at “entering” or “arriving” customers (in the
“arriving” case, departing customers denote both the ones rejected as well as the ones that
leave after service).
RESULT 9.6
If the arrival process is Poisson (i.e., an M/G/s queue), then the probability
that an arriving customer in steady state will see the queueing system in state j is the
probability that the system is in state j in the long run, that is,
pj = lim
t→∞P{X(t) = j} = π∗
j
The above result is called PASTA (Poisson Arrivals See Time Averages). PASTA is a pow-
erful result that can be used in situations beyond queueing. For example, if one is interested
in computing an average over time, instead of observing the system continuously, it can be
observed from time to time such that the interarrival times are exponentially distributed.
In fact, one common mistake made by a lot of people is to compute averages by sampling

9-8
Operations Research Methodologies
at equally spaced intervals. In fact, sampling must be done in such a manner that the time
between samples is exponentially distributed. Only then the averages obtained across such
a sample will be equal to the average across time.
Therefore, when one out of L, W, Lq, or Wq is known, the other three can be computed.
Also under certain conditions, when one out of pj, πj, or π∗
j is known, the others could be
computed. In the next few sections, we will see how to compute “one” of those terms.
9.3
Single-Station and Single-Class Queues
In this section, we consider a single queue at a single station handling a single class of cus-
tomers. We start with the simplest case of an M/M/1 queue and work our way through
more complex cases. Note that all the results can be found in standard texts such as Gross
and Harris [1] especially until Section 9.3.11.
9.3.1
The Classic M /M /1 Queue: Main Results
Consider a single-stage queueing system where the arrivals are according to a Poisson process
with average arrival rate λ per unit time (which is written as PP(λ)), that is, the time
between arrivals is according to an exponential distribution with mean 1/λ. For this system
the service times are exponentially distributed with mean 1/μ and there is a single server.
The number in the system at time t in the M/M/1 queue, that is, X(t), can be modeled
as a continuous time Markov chain (CTMC), speciﬁcally a birth and death process. The
condition for stability for the CTMC and subsequently the M/M/1 queue is that the traﬃc
intensity ρ should be less than 1, that is, ρ = λ/μ < 1. This means that the average arrival
rate should be smaller than the average service rate. This is intuitive because the server
would be able to handle all the arrivals only if the arrival rate is slower than the rate at
which the server can process on an average.
The long-run probability that the number of customers in the system is j (when ρ < 1)
is given by
pj = lim
t→∞P{X(t) = j} = (1 −ρ)ρj
for all j ≥0
Therefore, the long-run probability that there are more than n customers in the system
is ρn. In addition, the key performance measures can also be obtained. Using pj we have
L =
∞

j=0
jpj =
λ
μ −λ
and
Lq = 0p0 +
∞

j=0
jpj+1 =
λ2
μ(μ −λ)
Recall that Wn is the waiting time of the nth arriving customer and F(x) = lim
n→∞P{Wn ≤x}.
Then
F(x) = 1 −e−(μ−λ)x
for x ≥0
and therefore the waiting time for a customer arriving in steady-state is exponentially
distributed with mean 1/(μ −λ). Therefore
W =
1
μ −λ

Queueing Theory
9-9
which can also be derived using Little’s law and the expression for L. In addition, using
Little’s law for Lq,
Wq =
λ
μ(μ −λ)
M /M /1 Type Queues with Balking
When an arriving customer sees j other customers in the system, this customer joins the
queue with probability αj. In other words, this customer balks from the queueing system
with probability (1 −αj). This can be modeled as a CTMC, which is a birth and death
process with birth parameters (i.e., rate for going from state n to n + 1) λn+1 = αnλ for n ≥0
and death parameters (i.e., rate for going from state n to n −1) μn = μ for n ≥1. It is not
possible to obtain pj in closed-form (and thereby L) except for some special cases. In general,
pj =
j
k=0(λk/μk)
1 + ∞
n=1
n
i=1
λi
μi
with λ0 = μ0 = 1 and when the denominator exists. Also,
L =
∞

j=0
jpj
and using λ = ∞
n=0 λn+1pn, W can be obtained as L/λ.
M /M /1 Type Queues with Reneging
Every customer that joins a queue waits for an exp(θ) amount of time before which if the
service does not begin, the customer leaves the queueing system (which is called reneging
from the queueing system). This can be modeled as a birth and death CTMC with birth
parameters (see above for M/M/1 with balking for deﬁnition) λn+1 = λ for n ≥0 and death
parameters μn = μ + (n −1)θ for n ≥1. It is not possible to obtain pj in closed-form (and
thereby L) except for some special cases. In general,
pj =
j
k=0(λk/μk)
1 + ∞
n=1
n
i=1
λi
μi
with λ0 = μ0 = 1 and when the denominator exists. Also,
L =
∞

j=0
jpj
and using λ = λ, it is possible to obtain W as L/λ. It is crucial to note that W is the time
in the system for all customers, so it includes those customers that reneged as well as those
that were served. A separate analysis must be performed to obtain the departure rate of
customers after service. Using this departure rate as λ, if W is obtained then it would be
the average waiting time for customers that were served.
In case there is a queueing system with balking and reneging, then the analysis can be
combined. However, it must be noted that if the reneging times are not exponential, then
the analysis is a lot harder.

9-10
Operations Research Methodologies
M /M /1 Queue with State-Dependent Service
Consider an M/M/1 type queue where the mean service rate depends on the state of the
system. Many times when the number of customers waiting increases, the server starts
working faster. This is typical when the servers are humans. Therefore, if there are n cus-
tomers in the system, the mean service rate is μn. Note that in the middle of service if the
number in service increases to n + 1, the mean service rate also changes to μn+1 (further, if
it increases to n + 2 then service rate becomes μn+2, and so on). This can also be modeled as
a birth and death CTMC with birth parameters (deﬁned in M/M/1 with balking) λn+1 = λ
for n ≥0 and death parameters μn for n ≥1. It is not possible to obtain pj in closed-form
(and thereby L) except for some special cases. In general,
pj =
j
k=0(λk/μk)
1 + ∞
n=1
n
i=1
λi
μi
with λ0 = μ0 = 1 and when the denominator exists. Also,
L =
∞

j=0
jpj
and using λ = λ, W can be obtained as L/λ.
Note that if the mean service rate has to be picked and retained throughout the service
of a customer, that system cannot be modeled as a birth and death process.
M /M /1 Queue with Processor Sharing
For pj, L, and W it does not matter what the service discipline is (FCFS, LCFS, ROS, etc.)
The results are the same as long as the customers are served one at a time. Now what if the
customers are served using a processor sharing discipline? Customers arrive according to a
Poisson process with mean arrival rate λ customers per unit time. The amount of work each
customer brings is according to exp(μ); that is, if each customer were served individually it
would take exp(μ) time for service. However, the processor is shared among all customers.
So, if the system has i customers, each customer gets only an ith of the processing power.
Therefore each of the i customers get a service rate of μ/i. However, the time for the ﬁrst of
the i to complete service is according to exp(i × μ/i). Therefore, the CTMC for the number
of customers in the system is identical to that of an FCFS M/M/1 queue. And so, even the
processor sharing discipline will have identical pj, L, and W as that of the FCFS M/M/1
queue.
9.3.2
The Multiserver System: M /M /s
The description of an M/M/s queue is similar to that of the classic M/M/1 queue with the
exception that there are s servers. Note that by letting s = 1, all the results for the M/M/1
queue can be obtained. The number in the system at time t, X(t), in the M/M/s queue
can be modeled as a CTMC, which again is a birth and death process. The condition for
stability is ρ = λ/(sμ) < 1 where ρ is called the traﬃc intensity. The long run probability
that the number of customers in the system is j (when ρ < 1) is given by
pj =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
j!
λ
μ
j
p0
if 0 ≤j ≤s −1
1
s!sj−s
λ
μ
j
p0
if j ≥s

Queueing Theory
9-11
where p0 =
s−1
n=0
 1
n!(λ/μ)n
+ (λ/μ)s
s!
1
1 −λ/(sμ)
−1
. Thereby, using pj, we can derive
Lq =
p0(λ/μ)sλ
s!sμ[1 −λ/(sμ)]2
Also, Wq = Lq/λ, W = Wq + 1/μ, and L = Lq + λ/μ. The steady-state waiting time for a
customer has a cumulative distribution function (CDF) given by
F(x) = s(1 −ρ) −w0
s(1 −ρ) −1 (1 −e−μx) −
1 −w0
s(1 −ρ) −1(1 −e−(sμ−λ)x)
where w0 = 1 −
λsp0
s!μs(1−ρ).
9.3.3
Finite Capacity M /M /s/K System
In fact, this is one of the more general forms of the Poisson arrivals (with mean rate λ per
unit time) and exponential service time (with mean 1/μ) queue. Using the results presented
here, results for all the M/M/·/· type queues can be obtained. For example, letting s = 1
and K = ∞, the M/M/1 results can be obtained; K = ∞would yield the M/M/s results,
K = s would yield the M/M/s/s results, K = s = ∞would yield the M/M/∞results, and
so on. The special cases are popular because (a) the results are available in closed-form and
(b) insights can be obtained, especially while extending to the more general cases.
The number in the system at time t, X(t), in the M/M/s/K queue can be modeled as
speciﬁcally a birth and death chain CTMC. Using the pj values, one can derive
Lq = p0(λ/μ)sρ
s!(1 −ρ)2 [1 −pK−s −(K−s)ρK−s(1 −ρ)]
where ρ = λ/(sμ) and p0 =
s
n=0
 1
n!(λ/μ)n
+ (λ/μ)s
s!
K
n=s+1 ρn−s−1
.
Caution: Since this is a ﬁnite capacity queue, ρ can be greater than 1. The probability
that an arriving customer is rejected is pK as is given by pK = (λ/μ)K
s!sK−s p0. Therefore, the
average entering rate λ is given by λ = (1 −pK)λ. Hence Wq can be derived as Lq/λ. Also,
W and L can be obtained using W = Wq + 1/μ and L = Lq + λ/μ.
9.3.4
The M /M /1/K Queue
The M/M/1/K is another special case of the M/M/s/K system with s = 1. However, it
is the most fundamental ﬁnite capacity queue example with Poisson arrivals (with mean
rate λ) and exponential service times (with mean 1/μ). No more than K customers can
be in the system at any time. The traﬃc intensity ρ (where ρ = λ/μ) does not have to
be less than one. Since the capacity is ﬁnite, the system cannot be unstable. However, we
assume for now that ρ ̸= 1. For the case ρ = 1, limits results from calculus can be used (such
as L’Hospital’s rule) to obtain the corresponding value. The number of customers in the
system, not including any at the server, is
Lq =
ρ
1 −ρ −ρ(KρK + 1)
1 −ρK+1

9-12
Operations Research Methodologies
To obtain Wq, we use
Wq = L/λ
where λ is the average entering rate into the system and can be expressed as λ(1 −pK) where
pK = (λ/μ)K[1 −λ/μ]
1 −(λ/μ)K+1
Also, W and L can be obtained using W = Wq + 1/μ and L = Lq + λ/μ.
9.3.5
The M /M /s/s Queue
Although the M/M/s/s queue is a special case of the M/M/s/K system with K = s, there
are several interesting aspects and unique applications for it. Customers arrive according to
a Poisson process with mean rate λ per unit time. Essentially there are no queues. But there
are s servers that can be thought of as s resources that customers hold on to for an exponen-
tial amount of time (with mean 1/μ). In fact, queueing theory started with such a system by
a Danish Mathematician A.K. Erlang, who studied telephone switches with s lines. There
is no waiting and if all s lines are being used, the customer gets a “busy” tone on their tele-
phone. This system is also known as the Erlang loss system. However, there are many other
applications for the M/M/s/s queue such as: a rental agency with s items, gas stations
where customers do not wait if a spot is not available among s possible spots, self-service area
with maximum capacity s, and so on. In many of these systems there are no explicit servers.
The probability that there are j (for j = 0, . . ., s) customers in the system in the long
run is
pj =
(λ/μ)j
j!
s
i=0
(λ/μ)i
i!
Therefore, the “famous” Erlang loss formula is the probability that an arriving customer is
rejected (loss probability) and is given by
ps =
(λ/μ)s
s!
s
i=0
(λ/μ)i
i!
A remarkable fact is that the above formula holds good even for the M/G/s/s system with
mean service time 1/μ. We will see that in the M/G/s/s system explanation. We can derive
L = λ
μ(1 −ps)
As the eﬀective arrival rate is λ(1 −ps), W = 1/μ, which is obvious since there is no
waiting, the average time in the system W is indeed the average service time. Clearly
Lq = 0 and Wq = 0 for that same reason.
9.3.6
The Inﬁnite Server M /M /∞Queue
This is identical to the M/M/s/s system with s = ∞. In reality there are never inﬁnite
resources or servers. But when s is very large and a negligible number of customers are

Queueing Theory
9-13
rejected, the system can be assumed as s = ∞as the results are expressed in closed-form.
Systems such as the beach, grocery store (not counting the check out line), car rentals, and
the like can be modeled as M/M/∞queues.
The probability that there are j customers in the system in the long run is pj =
(λ/μ)j 1
j!e−λ/μ. Also, L = λ/μ and W = 1/μ. Of course, Lq = 0 and Wq = 0.
9.3.7
Finite Population Queues
Until this point we assumed that there are an inﬁnite number of potential customers and
the arrival rates did not depend on the number of customers in the system. Now we look
at the case where the arrivals are state-dependent. Consider a ﬁnite population of N cus-
tomers. Each customer after completion of service returns to the queue after spending exp(λ)
time outside the queueing system. There is a single server which serves customers in exp(μ)
amount of time. Clearly the arrival rate would depend on the number of customers in the sys-
tem. If X(t) denotes the number of customers in the system, then its limiting distribution is
pj = lim
t→∞P{X(t) = j} =

N
j

j!(λ/μ)j
N
i=0
N
i

i!(λ/μ)i
Clearly L = ∞
j=0 jpj; however, Lq, W, and Wq are tricky and need the eﬀective arrival rate
λ. Using the fact that λ = λ(N −L) we can get
Lq = L −λ(N −L)/μ,
W =
L
λ(N −L),
and
Wq =
Lq
λ(N −L)
Notice that the arrivals are not according to a Poisson process (which requires that
interarrival times be independent and identically distributed exponential random variables).
Therefore, PASTA cannot be applied. However, it is possible to show that the probability
that an arriving customer in steady state will see j in the system is
π∗
j = (N −j)pj
N −L
9.3.8
Bulk Arrivals and Service
So far we have only considered the case of single arrivals and single service. In fact, in prac-
tice, it is not uncommon to see bulk arrivals and bulk service. For example, arrivals into
theme parks are usually in groups, arrivals and service in restaurants are in groups, shuttle
busses perform service in batches, and so on. We only present the cases where the interar-
rival times and service times are both exponentially distributed. However, unlike the cases
seen thus far, here the CTMC models are not birth and death processes for the number in
the system.
Bulk Arrivals Case: M [X ]/M /1 Queue
Arrivals occur according to PP(λ) and each arrival brings a random number X customers
into the system. A single server processes the customers one by one, spending exp(μ) time

9-14
Operations Research Methodologies
with each customer. There is inﬁnite waiting room and customers are processed according
to FCFS. Such a system is denoted an M [X]/M/1 queue. Let ai be the probability that an
arrival batch size is i, that is, P{X = i} for i > 0 (we do not allow batch size of zero). Let
E[X] and E[X2] be the ﬁrst and second moments of X (where E[X2] = V ar[X] + {E[X]}2).
Deﬁne ρ = λE[X]/μ. The condition for stability is ρ < 1. We can derive the following results:
p0 = 1 −ρ
L = λ{E[X] + E[X2]}
2μ(1 −ρ)
Other pj values can be computed in terms of λ, μ, and ai. Note that the average entering
rate λ = λE[X]. Therefore, using Little’s law, W = L/(λE[X]). Also, Wq = W −1/μ and
thereby Lq = λE[X]Wq.
Bulk Service Case: M /M [Y ]/1 Queue
Single arrivals occur according to PP(λ). The server processes a maximum of K customers
at a time and any arrivals that take place during a service can join service (provided the
number is less than K). There is a single server, inﬁnite waiting room, and FCFS discipline.
The service time for the entire batch is exp(μ) whether the batch is of size K or not. In
fact, this is also sometimes known as the M/M [K]/1 queue (besides the M/M [Y ]/1 queue).
Notice that this system is identical to a shuttle bus type system where customers arrive
according to PP(λ) and busses arrive with exp(μ) as the inter-bus-arrival distribution. As
soon as a bus arrives, the ﬁrst K customers (if there are less than K, then all customers)
instantaneously enter the bus and the bus leaves. Then the queue denotes the number of
customers waiting for a shuttle bus.
To obtain the distribution of the number of customers waiting, let (r1, . . ., rk+1) be the
K + 1 roots of the characteristic equation (with D as the variable)
μDK+1 −(λ + μ)D + λ = 0
Let r0 be the only root among the K + 1 to be within 0 and 1. We can derive the following
results:
pn = (1 −r0)rn
0
for n ≥0
L =
r0
1 −r0
,
Lq = L −λ/μ
W =
r0
λ(1 −r0),
Wq = W −1/μ
9.3.9
The M /G/1 Queue
Consider a queueing system with PP(λ) arrivals and general service times. The service
times are iid with CDF G(·), mean 1/μ, and variance σ2. Notice that in terms of Sn, the
service time for any arbitrary customer n,
G(t) = P{Sn ≤t}
1/μ = E[Sn]
σ2 = Var[Sn]

Queueing Theory
9-15
There is a single server, inﬁnite waiting room, and customers are served according to
FCFS service discipline. It is important to note for some of the results, such as L and W,
that the CDF G(·) is not required. However, for pj and the waiting time distribution, the
Laplace Steiltjes Transform (LST) of the CDF denoted by ˜G(s) and deﬁned as
˜G(s) = E[esSn] =
∞

t=0
e−stdG(t)
is required.
Note that since the service time is not necessarily exponential, the number in the system
for the M/G/1 queue cannot be modeled as a CTMC. However, notice that if the system was
observed at the time of departure, a Markovian structure is obtained. Let Xn be the number
of customers in the system immediately after the nth departure. Then it is possible to show
that {Xn, n ≥0} is a discrete time Markov chain (DTMC) whose transition probability
matrix can be obtained. The stability condition is ρ < 1 where ρ = λ/μ. Let πj be the limiting
probability (under stability) that in the long run a departing customer sees j customers in
the system, that is,
πj = lim
n→∞P{Xn = j}
Then, using PASTA, we have pj = πj for all j. To obtain the πj, consider the generating
function φ(z) such that
φ(z) =
∞

j=0
πjzj
If the system is stable,
π0 = 1 −ρ
φ(z) = (1 −ρ)(1 −z) ˜G(λ −λz)
G(λ −λz) −z
Although πj values cannot be obtained in closed-form for the general case, they can be
derived from φ(z) by repeatedly taking derivatives with respect to z and letting z go to zero.
However, L and W can be obtained in closed-form. The average number of customers in
the system is
L = ρ + λ2
2
(σ2 + 1/μ2)
1 −ρ
The average waiting time in the system is
W = 1/μ + λ
2
(σ2 + 1/μ2)
1 −ρ
Also, Lq = L −ρ and Wq = W −1/μ.
Recall that Wn is the waiting time of the nth arriving customer and F(x) = lim
n→∞P{Wn ≤x}.
Although it is not easy to obtain F(x) in closed-form except for some special cases, it is
possible to write it in terms of the LST, ˜F(s) deﬁned as
˜F(s) = E[esWn] =
∞

x=0
e−sxdF(x)

9-16
Operations Research Methodologies
in the following manner:
˜F(s) =
(1 −ρ)s ˜G(s)
s −λ(1 −˜G(s))
It is important to realize that although inverting the LST in closed-form may not be easy,
there are several software packages that can be used to invert it numerically. In addition,
it is worthwhile to check that all the above results are true by trying exponential service
times, as after all M/M/1 is a special case of the M/G/1 queue.
The M /G/1 Queue with Processor Sharing
Similar to the M/M/1 system, for the M/G/1 system as well, the expressions for the num-
ber in system, L, and waiting time, W, it does not matter what the service discipline is
(FCFS, LCFS, ROS, etc). The results would be the same as long as the customers were
served one at a time. Now what if the customers are served using a processor sharing disci-
pline? Customers arrive according to a Poisson process with mean arrival rate λ customers
per unit time. The amount of work each customer brings is according to some general dis-
tribution with CDF G(·) as described in the M/G/1 setting earlier. Also, if each customer
were served individually it would take 1/μ time for service on an average (and a variance
of σ2 for service time). However, for this case, the processor is shared among all customers.
So if the system has i customers, each customer gets only an ith of the processing power.
Therefore, each of the i customers get a service rate of 1/i of the server speed. For this
system, it can be shown that
W =
1
μ −λ
The result indicates that the waiting time does not depend on the distribution of the service
time but on the mean alone. Also, L = λW, Wq = 0, and Lq = 0.
The M /G/∞Queue
Although this is an extension to the M/M/∞for the general service time case, the results
are identical, indicating they are independent of the distribution of service time. The prob-
ability that there are j customers in the system in the long run is
pj = e−λ/μ (λ/μ)j
j!
for j ≥0
The departure process from the queue is PP(λ). Also, L = λ/μ and W = 1/μ. Of course
Lq = 0 and Wq = 0.
The M /G/s/s Queue
This is a queueing system where the arrivals are according to a Poisson process with mean
arrival rate λ. The service times (also called holding times) are generally distributed with
mean 1/μ. There are s servers but no waiting space. The results are identical to those of the
M/M/s/s queue. In fact, the Erlang loss formula was derived for this general case initially.
For 0 ≤j ≤s, the steady-state probability that there are j customers in the system is
pj =
(λ/μ)j
j!
s
k=0
(λ/μ)k
k!

Queueing Theory
9-17
The departure process from the queue is PP((1 −ps)λ). In addition, L = λ
μ(1 −ps), W = 1/μ,
Lq = 0, and Wq = 0.
9.3.10
The G/M /1 Queue
Consider a queueing system where the interarrival times are according to some given general
distribution and service times are according to an exponential distribution with mean 1/μ.
The interarrival times are iid with CDF G(·) and mean 1/λ. This means that
G(t) = P{An+1 −An ≤t}
1/λ = E[An+1 −An] =
∞

0
tdG(t)
Assume that G(0) = 0. Also, there is a single server, inﬁnite waiting room, and customers
are served according to FCFS service discipline. It is important to note for most of the
results the LST of the interarrival time CDF denoted by ˜G(s) and deﬁned as
˜G(s) = E[es(An+1−An)] =
∞

t=0
e−stdG(t)
is required.
Similar to the M/G/1 queue, as all the random events are not necessarily exponentially
distributed, the number in the system for the G/M/1 queue cannot be modeled as a CTMC.
However, notice that if the system was observed at the time of arrivals, a Markovian struc-
ture is obtained. Let X∗
n be the number of customers in the system just before the nth
arrival. Then it is possible to model the stochastic process {X∗
n, n ≥0} as a DTMC. The
DTMC is ergodic if
ρ = λ
μ < 1
which is the stability condition. Let π∗
j be the limiting probability that in the long run an
arriving customer sees j other customers in the system, that is,
π∗
j = lim
x→∞P{X∗
n = j}
If ρ < 1, we can show that
π∗
j = (1 −α)αj
where α is a unique solution in (0,1) to
α = ˜G(μ −μα)
Using the notation Wn as the waiting time of the nth arriving customer under FCFS and
F(x) = lim
x→∞P{Wn ≤x}, we have
F(x) = 1 −e−μ(1−α)x
Therefore, under FCFS, the waiting time in the system in the long run is exponentially
distributed with parameter μ(1 −α). Using that result, the average waiting time in the
system is
W =
1
μ(1 −α)

9-18
Operations Research Methodologies
Using Little’s law, the average number of customers in the system is
L =
λ
μ(1 −α)
Note that we cannot use PASTA (as the arrivals are not Poisson, unlike the M/G/1 case).
However, it is possible to obtain pj using the following relation:
p0 = 1 −ρ
pj = ρπ∗
j−1
when j > 0
9.3.11
The G/G/1 Queue
Consider a single server queue with inﬁnite waiting room where the interarrival times and
service times are according to general distributions. The service discipline is FCFS. As the
model is so general without a Markovian structure, it is diﬃcult to model the number in
the system as an analyzable stochastic process. Therefore, it is not possible to get exact
expressions for the various performance measures. However, bounds and approximations can
be derived. There are several of them and none are considered absolutely better than others.
In this subsection, almost all the results are from Bolch et al. [2] unless otherwise noted.
Recall that Wn is the time in the system for the nth customer, Sn is the service time for the
nth customer and An is the time of the nth arrival. To derive bounds and approximations for
the G/G/1 queue, a few variables need to be deﬁned. Deﬁne In+1 = max(An+1 −An −Wn, 0)
and Tn+1 = An+1 −An. All the bounds and approximations are in terms of four parameters:
1/λ = E[Tn] average interarrival time
C2
a = Var[Tn]/{E[Tn]}2 SCOV of interarrival times
1/μ = E[Sn] average service time
C2
s = Var[Sn]/{E[Sn]}2 SCOV of service times
where SCOV is the “squared coeﬃcient of variation,” that is, the ratio of the variance to
the square of the mean (only for positive-valued random variables). Another parameter that
is often used is ρ = λ/μ, which is the traﬃc intensity.
Let random variables T, S, and I be the limiting values as n →∞of Tn, Sn, and In,
respectively. Although the mean and variance of T and S are known, E[I] can be com-
puted as E(I) = E(T) −E(S), which requires E(T) > E(S), that is, ρ < 1. It is possible to
show that
W = E(S2) −2{E(S)}2 −E(I2) + E[T 2]
2{E(T) −E(S)}
Notice that the only unknown quantity above is E(I2). Therefore, approximations and
bounds for W can be obtained through those of E[I2]. As L = λW (using Little’s law),
bounds and approximations for L can also be obtained.
Since on many occasions, the departure process from a queue is the arrival process
to another queue in a queueing network setting, it is important to study the mean and
SCOV of the departure process of a G/G/1 queue. Let Dn be the time of departure of
the nth customer. Deﬁne Δn+1 = Dn+1 −Dn as the interdeparture time with Δ being
the interdeparture time in steady state. Then it is possible to show that under stability,

Queueing Theory
9-19
E(Δ) = E(I) + E(S) = E(T) = 1/λ. Therefore, the departure rate equals arrival rate (con-
servation of ﬂow of customers). In addition, let C2
d = Var(Δ)/{E[Δ]}2; then
C2
d = C2
a + 2ρ2C2
s + 2ρ(1 −ρ) −2λW(1 −ρ)
Note that C2
d is in terms of W and hence approximations and bounds for W would yield
the same for C2
d.
Bounds for L, W , and C 2
d for G/G/1 Queue
First some bounds on W:
W ≤C2
a + ρ2C2
s
2λ(1 −ρ) + E(S)
W ≤ρ(2 −ρ)C2
a + ρ2C2
s
2λ(1 −ρ)
+ E(S)
W ≥ρ(C2
a −1 + ρ) + ρ2C2
s
2λ(1 −ρ)
+ E(S)
if T is DFR
W ≤ρ(C2
a −1 + ρ) + ρ2C2
s
2λ(1 −ρ)
+ E(S)
if T is IFR
where IFR and DFR are described subsequently. Note that ρ(2 −ρ) < 1; therefore, the ﬁrst
bound is always inferior to the second. Also, IFR and DFR respectively denote increasing
failure rate and decreasing failure rate random variables. Mathematically, the failure rate of a
positive-valued random variable X is deﬁned as h(x) = fX(x)/[1 −FX(x)], where fX(x) and
FX(x) are the probability density function and CDF of the random variable X. The reason
they are called failure rate is because if X denotes the lifetime of a particular component,
then h(x) is the rate at which that component fails when it is x time units old. IFR and
DFR imply that h(x) is respectively the increasing and decreasing functions of x. Note that
all random variables need not be IFR or DFR; they could be neither. Also, the exponential
random variable has a constant failure rate.
We can also obtain bounds via the M/G/1 (disregarding C2
a and using Poisson arrival
process with mean rate λ arrival process) and G/M/1 (disregarding C2
s and using exponen-
tially distributed service times with mean 1/μ) results. It is important to note that to use
the G/M/1 results, the distribution of the interarrival times is needed, not just the mean
and SCOV. See the table below with LB and UB referring to lower and upper bounds:
C2
a
C2
s
M/G/1
G/M/1
>1
>1
LB
LB
>1
<1
LB
UB
<1
>1
UB
LB
<1
<1
UB
UB
That means (see second result above) if C2
a > 1 and C2
s < 1 for the actual G/G/1 system,
then W using M/G/1 analysis would be a lower bound and correspondingly G/M/1 would
yield an upper bound.

9-20
Operations Research Methodologies
Next let us see what we have for L when T is DFR (although all bounds above for W
can be used by multiplying by λ)
ρ(C2
a −1 + ρ) + ρ2C2
s
2(1 −ρ)
+ ρ ≤L ≤ρ(2 −ρ)C2
a + ρ2C2
s
2(1 −ρ)
+ ρ
Finally some bounds on C2
d:
C2
d ≥(1 −ρ)2C2
a + ρ2C2
s
C2
d ≤(1 −ρ)C2
a + ρ2C2
s + ρ(1 −ρ)
if T is DFR
C2
d ≥(1 −ρ)C2
a + ρ2C2
s + ρ(1 −ρ)
if T is IFR
Approximations for L, W , and C 2
d for G/G/1 Queue
The following are some approximations for L and C2
d taken from Buzacott and Shanthikumar
[4]. Approximations for W can be obtained by dividing L by λ. There are several other
approximations available in the literature, many of which are empirical. Only a few are
presented here as follows:
Approx.
L
C 2
d
1

ρ2(1 + C2
s)
1 + ρ2C2s

C2
a + ρ2C2
s
2(1 −ρ)

+ ρ
(1 −ρ2)

C2
a + ρ2C2
s
1 + ρ2C2s

+ ρ2C2
s
2

ρ2(1 + C2
s)
2 −ρ + ρC2
s

ρ(2 −ρ)C2
a + ρ2C2
s
2(1 −ρ)

+ ρ
1 −ρ2 + ρ2C2
s + (C2
a −1)

(1 −ρ2)(2 −ρ) + ρC2
s (1 −ρ)2
2 −ρ + ρC2s

3
ρ2(C2
a + C2
s)
2(1 −ρ)
+ (1 −C2
a)C2
aρ
2
+ ρ
(1 −ρ)(1 + ρC2
a)C2
a + ρ2C2
s
9.3.12
The G/G/m Queue
Everything is similar to the G/G/1 queue explained before except that the number of servers
is m. Getting closed-form expressions was impossible for G/G/1, so naturally for G/G/m
there is no question. However, several researchers have obtained bounds and approximations
for the G/G/m queue. In fact, letting m = 1 for the G/G/m results would produce great
results for G/G/1. Notice that the traﬃc intensity ρ = λ/(mμ). The random variables S
and T, as well as parameters C2
a and C2
s used in the following bounds and approximations,
have been deﬁned in the G/G/1 system above.
• The Kingman upper bound:
Wq ≤V ar(T) + V ar(S)/m + (m −1)/(m2μ2)
2(1 −ρ)
• The Brumelle and Marchal lower bound:
Wq ≥ρ2C2
s −ρ(2 −ρ)
2λ(1 −ρ)
−m −1
m
(C2
s + 1)
2μ
• Under heavy traﬃc conditions, for the G/M/m systems,
Wq ≈V ar(T) + V ar(S)/m2
2(1 −ρ)
λ

Queueing Theory
9-21
and waiting time in the queue is distributed approximately according to an expo-
nential distribution with mean 1/Wq. Note that “heavy traﬃc” implies that ρ is
close to 1.
• And ﬁnally,
WG/G/m ≈WM/M/m
WM/M/1
WG/G/1 + E[S]
In the above approximation, the subscript for W denotes the type of queue. For
example, WM/M/m implies the mean waiting time for the M/M/m queue using
the same λ and μ as the G/G/m case.
• There are several approximations available in the literature, many of which are
empirical. The most popular one is the following. Choose αm such that
αm =
⎧
⎪
⎨
⎪
⎩
ρm + ρ
2
if ρ > 0.7
ρ
ρ+1
2
if ρ < 0.7
The waiting time in the queue is given by the approximation
Wq ≈αm
μ

1
1 −ρ
 C2
a + C2
s
2m

9.4
Single-Station and Multiclass Queues
In the models considered so far there was only a single class of customers in the system. How-
ever, there are several applications where customers can be diﬀerentiated into classes and
each class has its own characteristics. For example, consider a hospital emergency room.
The patients can be classiﬁed into emergency, urgent, and normal cases with varying arrival
rates and service time requirements. Another example is a toll booth where the vehicles can
be classiﬁed based on type (cars, buses, trucks, etc.) and each type has its own arrival rate
and service time characteristics. There are several examples in production systems (rou-
tine maintenance versus breakdowns in repair shops) and communication systems (voice
calls versus dial-up connection for Internet at a telephone switch) where entities must be
classiﬁed due to the wide variability of arrival rates and service times.
Having made a case for splitting traﬃc in queues into multiple classes, it is also important
to warn that unless absolutely necessary, due to the diﬃculty in analyzing such systems,
one should not classify. There are two situations where it does make sense to classify. First,
when the system has a natural classiﬁcation where the various classes require their own
performance measures (e.g., in a ﬂexible manufacturing system, if a machine produces three
types of parts and it is important to measure the in-process inventory of each of them
individually, then it makes sense to model them as three classes). Second, when the service
times are signiﬁcantly diﬀerent for the various classes that the distribution models would
ﬁt better, then it makes sense (e.g., if the service times have a bimodal distribution, then
classifying into two classes with unimodal distribution for each class would possibly be
better).
The next question to ask is how are the diﬀerent classes of customers organized at the
single station? There are two waiting line structures:
a. All classes of customers wait in the same waiting room. Examples: buﬀer in
ﬂexible manufacturing system, packets on a router interface, vehicles at a 1-lane

9-22
Operations Research Methodologies
road traﬃc light, and so on. Service scheduling policies are: FCFS, priority, ROS,
LCFS, and so on.
b. Each class has a waiting room of its own and all classes of customers of a par-
ticular class wait in the same waiting room. Examples: robots handling several
machine buﬀers, class-based queueing in routers, vehicles at a toll plaza (elec-
tronic payments, exact change, and full service), and the like. Service scheduling
policies (especially when there is only a single server) across the classes typically
are: priority, polling, weighted round-robin, and so on.
Within a class, the two waiting line structures use FCFS usually (LCFS and others are also
possible). If the waiting room is of inﬁnite capacity and there is no switch-over time from
one queue to another, both (a) and (b) can be treated identically. However, in the ﬁnite
waiting room case, they are diﬀerent and in fact one of the design decisions is to ﬁgure out
the buﬀer sizes, admission/rejection rules, and the like.
For the multiclass queues, several design decisions need to be made. These include:
• Assigning classes: how should the customers be classiﬁed? As alluded to before,
it is critical, especially when there is no clear-cut classiﬁcation, how customers
should be classiﬁed and how many categories to consider.
• Buﬀer sizing: what should the size of the buﬀers be or how should a big buﬀer
be partitioned for the various classes? These decisions can be made either one
time (static) or changed as the system evolves (dynamic).
• Scheduling rule: how should the customers or entities be scheduled on the servers?
For example, FCFS, shortest expected processing time ﬁrst (FCFS within a class),
round-robin across the K classes, priority-based scheduling, and so on. Sometimes
these are “given” for the system and cannot be changed; other times these could
be decisions that can be made.
• Priority allocation: if priority-based scheduling rule is used, then how should
priorities be assigned? In systems like the hospital emergency room, the priorities
are clear. However, in many instances one has to trade oﬀcost and resources to
determine priorities.
• Service capacity: how to partition resources such as servers (wholly or partially)
among classes? For example, in a call-center handling customers that speak dif-
ferent languages and some servers being multilingual, it is important to allocate
servers to appropriate queues. Sometimes these capacity allocations are made in
a static manner and other times dynamically based on system state.
There are several articles in the literature that discuss various versions of the above design
problems. In this chapter, we assume that the following are known or given: there are R
classes already determined, inﬁnite buﬀer size, scheduling rule already determined, and a
single server that serves customers one at a time. For such a system, we ﬁrst describe some
general results for the G/G/1 case next and describe speciﬁc results for the M/G/1 case
subsequently. Most of the results are adapted from Wolﬀ[3] with possibly diﬀerent notation.
9.4.1
Multiclass G/G/1 Queue: General Results
Consider a single-station queue with a single server that caters to R classes of customers.
Customers belonging to class i (i ∈{1, 2, . . ., R}) arrive into the system at a mean rate λi
and the arrival process is independent of other classes, but is also independent and identi-
cally distributed within a class. Customers belonging to class i (i ∈{1, 2, . . ., R}) require an

Queueing Theory
9-23
average service time of 1/μi. Upon completion of service, the customers depart the system.
We assume that the distribution of interarrival times and service times are known for each
class. However, notice that the scheduling policy (i.e., service discipline) has not yet been
speciﬁed. We describe some results that are invariant across scheduling policies (or at least
a subset of policies).
For these systems, except for some special cases, it is diﬃcult to obtain performance
measures such as “distribution” of waiting time and queue length like in the single class
cases. Therefore, we concentrate on obtaining average waiting time and queue length. Let Li
and Wi be the mean queue length and mean waiting time for class i customers. Irrespective
of the scheduling policy, Little’s law holds for each class, so for all i ∈[1, R],
Li = λiWi
That means that one can think of each class as a mini system in itself. Also, similar results
can be derived for Liq and Wiq which respectively denote the average number waiting in
queue (not including customers at servers) and average time spent waiting before service.
In particular, for all i ∈[1, R],
Liq = λiWiq
Wi = Wiq + 1
μi
Li = Liq + ρi
where
ρi = λi
μi
In addition, L and W are the overall mean number of customers and mean waiting time
averaged over all classes. Note that L = L1 + L2 + · · · + LR and if λ = λ1 + λ2 + · · · + λR,
the net arrival rate, then W = L/λ. For the G/G/1 case with multiple classes, more results
can be derived for a special class of scheduling policies called work-conserving disciplines
that we describe next.
Work-Conserving Disciplines under G/G/1
We now concentrate on a subset of service-scheduling policies (i.e., service disciplines) called
work-conserving disciplines where more results for the G/G/1 queue can be obtained. In fact,
many of these results have not been explained in the single class in the previous sections,
but by letting R = 1, they can easily be accomplished.
The essence of work-conserving disciplines is that the system workload at every instant of
time remains unchanged over all work-conserving service scheduling disciplines. Intuitively
this means that the server never idles and does not do any wasteful work. The server contin-
uously serves customers if there are any in the system. For example, FCFS, LCFS, and ROS
are work conserving. Certain priority policies that we will see later, such as non-preemptive
and preemptive resume policies, are also work conserving. There are policies that are non-
work-conserving, such as the preemptive repeat (unless the service times are exponential).
Usually, when the server takes a vacation from service or if there is a switch-over time (or
set-up time) during moving from classes, unless those can be explicitly accounted for in the
service times, are non-work conserving.

9-24
Operations Research Methodologies
To describe the results for the work-conserving disciplines, consider the notation used in
Section 9.4.1. Deﬁne ρ, the overall traﬃc intensity, as
ρ =
R

i=1
ρi
An R-class G/G/1 queue with a work-conserving scheduling discipline is stable if
ρ < 1
In addition, when a G/G/1 system is work conserving, the probability that the system is
empty is 1 −ρ.
Let Si be the random variable denoting the service time of a class i customer. Then we
have the second moment of the overall service time as
E[S2] = 1
λ
R

i=1
λiE[S2
i ]
We now present two results that are central to work-conserving disciplines. These results
were not presented for the single-class case (easily doable by letting R = 1).
RESULT 9.7
If the G/G/1 queue is stable, then when the system is in steady state, the
expected remaining service time at an arbitrary time in steady state is λE[S2]/2.
As the total amount of work remains a constant across all work-conserving disciplines,
and the above result represents the average work remaining for the customer at the server,
the average work remaining due to all the customers waiting would also remain a constant
across work-conserving discipline. That result is described below.
RESULT 9.8
Let Wiq be the average waiting time in the queue (not including service)
for a class i customer, then the expression
R

i=1
ρiWiq
is a constant over all work conserving disciplines.
However, quantities such as L, W, Li, and Wi (and the respective quantities with the
q subscript) will depend on the service-scheduling policies. It is possible to derive these
expressions in closed-form for M/G/1 queues that we describe next. The HOM software
[9] can be used for numerical analysis of various scheduling policies for relatively general
multiclass traﬃc.
9.4.2
M /G/1 Queue with Multiple Classes
Consider a special case of the G/G/1 queue with R classes where the arrival process is
PP(λi) for class i (i = 1, 2, . . ., R). The service times are iid with mean E[Si] = 1/μi, sec-
ond moment E[S2
i ], and CDF Gi(·) for class i (i = 1, 2, . . ., R) and ρi = λi/μi. We present
results for three work-conserving disciplines: FCFS, non-preemptive priority, and preemp-
tive resume priority.
Multiclass M /G/1 with FCFS
In this service-scheduling scheme, the customers are served according to FCFS. None of
the classes receive any preferential treatment. The analysis assumes that all the R classes

Queueing Theory
9-25
in some sense can be aggregated into one class as there is no diﬀerentiation. Hence, the net
arrival process is PP(λ) with λ = λ1 + λ2 + · · · + λR. Let S be a random variable denoting
the “eﬀective” service time for an arbitrary customer. Then
G(t) = P(S ≤t) = 1
λ
R

i=1
λiGi(t)
E[S] = 1
μ = 1
λ
R

i=1
λiE[Si]
E[S2] = σ2 + 1
μ2 = 1
λ
R

i=1
λiE[S2
i ]
ρ = λE[S]
Assume that the system is stable. Then, using standard M/G/1 results with X(t) being
the total number of customers in the system at time t, we get when ρ < 1,
L = ρ + 1
2
λ2E[S2]
1 −ρ
W = L
λ
Wq = W −1
μ
Lq = 1
2
λ2E[S2]
1 −ρ
The expected number of class i customers in the system (Li) as well as in the queue (Liq)
and the expected waiting time in the system for class i (Wi) as well as in the queue (Liq)
are given by:
Wiq = Wq = 1
2
λE[S2]
1 −ρ
Liq = λiWiq
Li = ρi + Liq
Wi = Wiq + 1
μi
M /G/1 with Non-Preemptive Priority
Here we consider priorities among the various classes. For the following analysis assume that
class 1 has highest priority and class R has the lowest. Service discipline within a class
is FCFS. The server always starts serving a customer of the highest class among those
waiting for service, and the ﬁrst customer that arrived within that class. However, the
server completes serving a customer before considering who to serve next. The meaning
of non-preemptive priority is that a customer in service does not get preempted while in

9-26
Operations Research Methodologies
service by another customer of high priority (however preemption does occur while waiting).
Assume that the system is stable.
Let αi = ρ1 + ρ2 + · · · + ρi with α0 = 0. Then we get the following results:
E[W q
i ] =
1
2
R
j=1 λjE[S2
j ]
(1 −αi)(1 −αi−1)
for 1 ≤i ≤R
E[Lq
i ] = λiE[W q
i ]
Wi = E[W q
i ] + E[Si]
Li = E[Lq
i ] + ρi
Sometimes performance measures for individual classes are required and other times aggre-
gate performance measures across all classes. The results for the individual classes can also
be used to obtain the overall or aggregate performance measures as follows:
L = L1 + L2 + · · · + LR
W = L
λ
Wq = W −1
μ
Lq = λWq
Note: In the above analysis we assume we are given which class should get the highest
priority, second highest, and so on. However, if we need to determine an optimal way
of assigning priorities, one method is now provided. If you have R classes of customers and
it costs the server Cj per unit time a customer of class j spends in the system (holding
cost for class j customer), then to minimize the total expected cost per unit time in the
long run, the optimal priority assignment is to give class i higher priority than class j if
Ciμi > Cjμj. In other words, sort the classes in the decreasing order of the product Ciμi
and assign ﬁrst priority to the largest Ciμi and the last priority to the smallest Ciμi over
all i. This is known as the Cμ rule. Also note that if all the Ci values were equal, then this
policy reduces to “serve the customer with the smallest expected processing time ﬁrst.”
M /G/1 with Preemptive Resume Priority
A slight modiﬁcation to the M/G/1 non-preemptive priority considered above is to allow
preemption during service. During the service of a customer, if another customer of higher
priority arrives, then the customer in service is preempted and service begins for this new
high priority customer. When the preempted customer returns to service, service resumes
from where it was preempted. This is a work-conserving discipline (however, if the service has
to start from the beginning which is called preemptive repeat, then it is not work conserving
because the server wasted some time serving). Here, we consider the case where upon arrival,
a customer of class i can preempt a customer of class j in service if j > i. Also, the total
service time is unaﬀected by the interruptions, if any. Assume that the system is stable.
The waiting time of customers of class i is unaﬀected by customers of class j if j > i.
Thus, class 1 customers face a standard single-class M/G/1 system with arrival rate λ1
and service time distribution G1(·). In addition, if only the ﬁrst i classes of customers are
considered, then the processing of these customers as a group is unaﬀected by the lower

Queueing Theory
9-27
priority customers. The crux of the analysis is in realizing that the work content of this
system (with only the top i classes) at all times is the same as an M/G/1 queue with FCFS
and top i classes due to the work-conserving nature. Therefore, using the results for work-
conserving systems, the performance analysis of this system is done.
Now consider an M/G/1 queue with only the ﬁrst i classes and FCFS service. The net
arrival rate is
λ(i) = λ1 + λ2 + · · · + λi
the average service times is
1
μ(i) =
i

j=1
λjE[Sj]
λ(i)
and the second moment of service times is
S2(i) =
i

j=1
λjE[S2
j ]
λ(i)
Also let ρ(i) = λ(i)/μ(i). Let W prp
jq
be the waiting time in the queue for class j customers
under preemptive resume policy. Using the principle of work conservation (see Result 9.8),
i

j=1
ρjW prp
jq
= ρ(i)
λ(i)S2(i)
2(1 −λ(i)/μ(i))
Notice that the left-hand side of the above expression is the ﬁrst i classes under preemptive
resume and the right-hand side being FCFS with only the ﬁrst i classes of customers. Now,
we can recursively compute W1q, then W2q, and so on till WRq via the above equations for
i = 1, 2, . . ., R.
Other average measures for the preemptive resume policy can be obtained as follows:
W prp
i
= W prp
iq
+ E[Si]
Lprp
i
= λiW prp
i
Lprp
iq
= Lprp
i
−ρi
Sometimes performance measures for individual classes are required and other times
aggregate performance measures across all classes. The results for the individual classes
can also be used to obtain the overall performance measures as follows:
Lprp = Lprp
1
+ Lprp
2
+ · · · + Lprp
R
W prp = Lprp
λ
W prp
q
= W prp −1
μ
Lprp
q
= λW prp
q

9-28
Operations Research Methodologies
9.5
Multistation and Single-Class Queues
So far we have only considered single-stage queues. However, in practice, there are several
systems where customers go from one station (or stage) to other stations. For example,
in a theme park the various rides are the diﬀerent stations and customers wait in lines at
each station and randomly move to other stations. Several engineering systems such as
production, computer-communication, and transportation systems can also be modeled as
queueing networks.
In this section, we only consider single-class queueing networks. The network is analyzed
by considering each of the individual stations one by one. Therefore, the main technique
would be to decompose the queueing network into individual queues or stations and develop
characteristics of arrival processes for each individual station. Similar to the single-station
case, here too we start with networks with Poisson arrivals and exponential service times,
and then eventually move to more general cases. There are two types of networks: open
queueing networks (customers enter and leave the networks) and closed queueing networks
(the number of customers in the networks stays a constant).
9.5.1
Open Queueing Networks: Jackson Network
A Jackson network is a special type of open queueing network where arrivals are Poisson and
service times are exponential. In addition, a queueing network is called a Jackson network
if it satisﬁes the following assumptions:
1. It consists of N service stations (nodes).
2. There are si servers at node i (1 ≤si ≤∞), 1 ≤i ≤N.
3. Service times of customers at node i are iid exp(μi) random variables. They are
independent of service times at other nodes.
4. There is inﬁnite waiting room at each node.
5. Externally, customers arrive at node i in a Poisson fashion with rate λi. All arrival
processes are independent of each other and the service times. At least one λi
must be nonzero.
6. When a customer completes service at node i, he or she or it departs the system
with probability ri or joins the queue at node j with probability pij. Here pii > 0
is allowed. It is required that ri + N
j=1 pij = 1 as all customers after completing
service at node i either depart the system or join another node. The routing of a
customer does not depend on the state of the network.
7. Let P = [Pij] be the routing matrix. Assume that I −P is invertible, where I is
an N × N identity matrix. The I −P matrix is invertible if there is at least one
node from where customers can leave the system.
To analyze the Jackson network, as mentioned earlier, we decompose the queueing network
into the N individual nodes (or stations). The results are adapted from Kulkarni [6]. In
steady state, the total arrival rate into node j (external and internal) is denoted by aj and
is given by
aj = λj +
N

i=1
aipij
j = 1, 2, . . ., N
Let a = (a1, a2, . . ., aN). Then a can be solved as
a = λ(I −P)−1

Queueing Theory
9-29
The following results are used to decompose the system: (a) the departure process from
an M/M/s queue is a Poisson process; (b) the superposition of Poisson process forms a
Poisson process; and (c) Bernoulli (i.e., probabilistic) splitting of Poisson processes forms
Poisson processes. Therefore, the resultant arrival into any node or station is Poisson. Then
we can model node j as an M/M/sj queue with PP(aj) arrivals, exp(μj) service, and sj
servers (if the stability condition at each node j is satisﬁed, i.e., aj < sjμj). Hence, it is
possible to obtain the steady-state probability of having n customers in node j as
φj(n) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1
n!
 aj
μj
n
φj(0)
if 0 ≤n ≤sj −1
1
sj!sn−sj
j
 aj
μj
n
φj(0)
if n ≥sj
where φj(0) =
sj−1
n=0
 1
n!(aj/μj)n

+ (aj/μj)s
j
sj!
1
1 −aj/(sjμj)
−1
Now looking back into the network as a whole, let Xi be the steady-state number of
customers in node i. Then it is possible to show that
P{X1 = x1, X2 = x2, . . ., XN = xN} = φ1(x1)φ2(x2). . . φN(xN)
The above form of the joint distribution is known as product form. In steady state, the queue
lengths at various nodes are independent random variables. Therefore, what this implies is
that each node (or station) in the network behaves as if it is an independent M/M/s queue.
Hence, each node j can be analyzed as an independent system and performance measures
can be obtained.
Speciﬁcally, it is possible to obtain performance measures at station j, such as the average
number of customers (Lj), average waiting time (Wj), time in queue not including service
(Wjq), number in queue not including service (Ljq), distribution of waiting time (Fj(x)),
and all other measures using the single-station M/M/s queue analysis in Section 9.3.2.
Besides the Jackson network, there are other product-form open queueing networks. The
state-dependent service rate and the state-dependent arrival rate problems are two cases
when product-form solution exists.
State-Dependent Service
Assume that the service rate at node i when there are n customers at that node is given
by μi(n) with μi(0) = 0. Also assume that the service rate does not depend on the states of
the remaining nodes. Then deﬁne the following: φi(0) = 1 and
φi(n) =
n

j=1
 ai
μi(j)

n ≥1
where aj is as before, the eﬀective arrival rate into node j.
The steady-state probabilities are given by
P{X1 = x1, X2 = x2, . . ., XN = xN} = c
N

i=1
φi(xi)
where the normalizing constant c is
c =
 N

i=1
 ∞

n=0
φi(n)
−1

9-30
Operations Research Methodologies
Using the above joint distribution, it is possible to obtain certain performance measures.
However, one of the diﬃculties is to obtain the normalizing constant. Once that is done, the
marginal distribution at each node (or station) can be obtained. That can be used to get
the distribution of the number of customers in the system as well as the mean (and even
higher moments). Then, using Little’s law, the mean waiting time can also be obtained.
State-Dependent Arrivals and Service
In a manner similar to the case of state-dependent service, the analysis of state-dependent
arrivals and service can be extended. Let λ(n) be the total arrival rate to the network as
a whole when there are n customers in the entire network. Assume that ui is the probabil-
ity that an incoming customer joins node i, independently of other customers. Therefore,
external arrivals to node i are at rate uiλ(n). The service rate at node i when there are ni
customers at that node is given by μi(ni) with μi(0) = 0.
Let bi be the unique solution to
bj = uj +
N

i=1
bipij
Deﬁne the following: φi(0) = 1 and
φi(n) =
n

j=1

bi
μi(j)

for n ≥1
Deﬁne ˆx = N
i=1 xi. The steady-state probabilities are given by
P{X1 = x1, X2 = x2, . . ., XN = xN} = c
N

i=1
φi(xi)
ˆx

j=1
λ(j)
where the normalizing constant c is
c =
⎧
⎨
⎩

x
N

i=1
φi(xi)
ˆx

j=1
λ(j)
⎫
⎬
⎭
−1
9.5.2
Closed Queueing Networks (Exponential Service Times)
Closed queueing networks are networks where there are no external arrivals to the system
and no departures from the system. They are popular in population studies, multipro-
grammed computer systems, window ﬂow control, Kanban, and so on. It is important to
note that the number of customers being a constant is essentially what is required. This
can happen if a new customer enters the network as soon as an existing customer leaves
(a popular scheme in just-in-time manufacturing). Most of the results are adapted from
Kulkarni [6]. We need a few assumptions to analyze these networks:
1. The network has N service stations and a total of C customers.
2. The service rate at node i, when there are n customers in that node, is μi(n)
with μi(0) = 0 and μi(n) > 0 for 1 ≤n ≤C.
3. When a customer completes service at node i, the customer joins node j with
probability pij.

Queueing Theory
9-31
Let the routing matrix P = [pij] be such that it is irreducible. That means it is pos-
sible to reach every node from every other node in one or more steps or hops. Deﬁne
π = (π1, π2, . . ., πN) such that
π = πP
and
N

i=1
πi = 1
Indeed, the P matrix is a stochastic matrix, which is a lot similar to the transition probabil-
ity matrix of DTMCs. However, it is important to note that nothing is modeled as a DTMC.
Deﬁne the following: φi(0) = 1 and
φi(n) =
n

n=1
 πi
μi(j)

n ≥1
The steady-state probabilities are given by
P{X1 = x1, X2 = x2, . . ., XN = xN} = G(C)
N

i=1
φi(xi)
where the normalizing constant G(C) is chosen such that

x1,x2,...,xN
P(X1 = x1, X2 = x2, . . ., XN = xN) = 1
Note that for this problem, similar to the two previous product-form cases, the diﬃculty
arises in computing the normalizing constant. In general it is not computationally trivial.
Some additional results can be obtained such as the Arrival Theorem explained below.
RESULT 9.9
In a closed product-form queueing network, for any x, the probability that
x jobs are seen at the time of arrival to node i when there are C jobs in the network is equal
to the probability that there are x jobs at this node with one less job in the network (i.e.,
C −1).
This gives us the relationship between the arrival time probabilities and steady-state
probabilities. Let πij(C) denote the probability that in a closed-queueing network of C
customers, an arriving customer into node i sees j customers ahead of him/her/it in steady
state. Also, let pij(C −1) denote the probability that in a “hypothetical” closed-queueing
network of C −1 customers, there are j customers in node i in steady state. Result 9.9
states that
πij(C) = pij(C −1)
Single-Server Closed Queueing Networks
Assume that for all i, there is a single server at node i with service rate μi. Then the mean
performance measures can be computed without going through the computation of the
normalizing constant. Deﬁne the following:
• Wi(k): Average waiting time in node i when there are k customers in the network;
• Li(k): Average number in node i when there are k customers in the network;
• λ(k): Overall throughput of the network when there are k customers in the
network.

9-32
Operations Research Methodologies
Initialize Li(0) = 0 for 1 ≤i ≤N. Then for k = 1 to C, iteratively compute for each i:
Wi(k) = 1
μi
[1 + Li(k −1)]
λ(k) =
k
N
i=1 aiWi(k)
Li(k) = λ(k)Wi(k)ai
The ﬁrst of the above three equations comes from the Arrival Theorem (Result 9.9).
The second and third come from Little’s law applied to the network and a single node,
respectively.
9.5.3
Algorithms for Nonproduct-Form Networks
The product form for the joint distribution enables one to analyze each node independently.
When the interarrival times or service times are not exponential, then a nonproduct form
emerges. We will now see how to develop approximations for these nonproduct-form net-
works. We present only one algorithm here, namely the diﬀusion approximation. However,
the literature is rich with several others such as the maximum entropy method, QNA for
single class, and so on. We now illustrate the diﬀusion approximation algorithm as described
in Bolch et al. [2].
Diﬀusion Approximation: Open Queueing Networks
1. Key Idea: Substitute the discrete process {Xi(t), t ≥0} that counts the number in
the node i, by a continuous diﬀusion process. Thus a product-form approximation
can be obtained that works well under heavy traﬃc (i.e., traﬃc intensity in each
node is above 0.95 at least).
2. Assumptions: Single server at each node. Service time at server i has mean 1/μi
and SCOV C2
Si. There is a single stream of arrivals into the network with inter-
arrival times having a mean of 1/λi and SCOV C2
A. There is a slight change of
notations for the routing probabilities (consider the outside world as node 0):
a. If i > 0 then pij is the probability of going from node i to node j upon service
completion in node i;
b. If i = 0 then p0j is the probability that an external arrival joins node j;
c. If j = 0 then pi0 is the probability of exiting the queueing network upon service
completion in node i.
3. The Algorithm:
a. Obtain visit ratios aj for all 1 ≤j ≤N by solving
aj =
N

i=1
pijai + p0j
with a0 = 1. Then for 1 ≤i, j ≤N, if P = [pij] an N × N matrix, then
a = [a1, a2, . . ., aN] = [p01, p02, . . ., p0N][I −P]−1.

Queueing Theory
9-33
b. For all 1 ≤i ≤N, compute the following (assume C2
S0 = C2
A):
C2
Ai = 1 +
N

j=0
(C2
Sj −1)p2
jiaj/ai
ρi = λai
μi
θi = exp
 −2(1 −ρi)
C2
Aiρi + C2
Si

φi(xi) =

1 −ρi
if xi = 0
ρi(1 −θi)θxi−1
i
if xi > 0
c. The steady-state joint probability is
p(x) =
N

i=1
φi(xi)
d. The mean number of customers in node i is
Li =
ρi
1 −θi
Diﬀusion Approximation: Closed Queueing Networks
All the parameters are identical to the open queueing network case. There are C customers
in the closed queueing network. There are two algorithms, one for large C and the other for
small C.
1. Algorithm Bottleneck (for large C)
a. Obtain visit ratios aj for all 1 ≤j ≤N by solving
aj =
N

i=1
pijai
As there will not be a unique solution, one can normalize by a1 + a2 + · · · +
aN = 1.
b. Identify the bottleneck node b as the node with the largest ai/μi value among
all i ∈[1, N].
c. Set ρb = 1. Using the relation ρb = λab/μb, obtain λ = μb/ab. Then for all i ̸= b,
obtain ρi = λai/μi.
d. Follow the open queueing network algorithm now to obtain for all i ̸= b, C2
Ai,
θi, and φi(xi).

9-34
Operations Research Methodologies
e. Then the average number of customers in node i (i ̸= b) is
Li =
ρi
1 −θi
and Lb = C −
i̸=b Li.
2. Algorithm MVA (for small C):
Consider MVA for product-form closed queueing networks (see Section 9.5.2).
Use that analysis and iteratively compute for all 1 ≤k ≤C, the quantities Wi(k),
λi(k), and Li(k). Assume overall throughput λ = λ(C). Then follow the open
queueing network algorithm (in Section 9.5.3).
9.6
Multistation and Multiclass Queues
Consider an open queueing network with multiple classes where the customers are served
according to FCFS. To obtain performance measures we use a decomposition technique.
For that, we ﬁrst describe the problem setting, develop some notation, and illustrate an
algorithm.
9.6.1
Scenario
We ﬁrst describe the setting, some of which involves underlying assumptions needed to carry
out the analysis.
1. There are N service stations (nodes) in the open queueing network. The outside
world is denoted by node 0 and the others 1, 2, . . ., N.
2. There are mi servers at node i (1 ≤mi ≤∞), for 1 ≤i ≤N.
3. The network has R classes of traﬃc and class switching is not allowed.
4. Service times of class r customers at node i are iid with mean 1/μi,r and SCOV
C2
Si,r.
5. The service discipline is FCFS.
6. There is inﬁnite waiting room at each node.
7. Externally, customers of class r arrive at node i according to a general interarrival
time with mean 1/λ0i,r and SCOV C2
Ai,r.
8. When a customer of class r completes service at node i, the customer joins the
queue at node j (j ∈[0, N]) with probability pij,r.
After verifying that the above scenario (and assumptions) is applicable, the next task is
to obtain all the input parameters for the model described above, that is, for each i ∈[1, N]
and r ∈[1, R], mi, 1/μi,r, C2
Si,r, 1/λ0i,r, C2
Ai,r, pij,r (for j ∈[0, N]).
9.6.2
Notation
Before describing the algorithm, some of the notations are in Table 9.4 for easy reference. A
few of the notations are inputs to the algorithm (as described above) and others are derived
in the algorithm. The algorithm is adapted from Bolch et al. [2], albeit with a diﬀerent set
of notations. The reader is also encouraged to refer to Bolch et al. [2] for further insights
into the algorithm.

Queueing Theory
9-35
TABLE 9.4
Notation Used in Algorithm
N
Total number of nodes
Node 0
Outside world
R
Total number of classes
λij,r
Mean arrival rate from node i to node j of class r
λi,r
Mean arrival rate to node i of class r (or mean departure rate from node i of class r)
pij,r
Fraction of traﬃc of class r that exit node i and join node j
λi
Mean arrival rate to node i
ρi,r
Utilization of node i due to customers of class r
ρi
Utilization of node i
μi
Mean service rate of node i
C2
Si
SCOV of service time of node i
C2
ij,r
SCOV of time between two customers going from node i to node j
C2
Ai,r
SCOV of class r interarrival times into node i
C2
Ai
SCOV of interarrival times into node i
C2
Di
SCOV of inter-departure times from node i
9.6.3
Algorithm
The decomposition algorithm essentially breaks down the network into individual nodes
and analyzes each node as an independent GI /G/s queue with multiple classes (note that
this is only FCFS and hence handling multiple classes is straightforward). For the GI /G/s
analysis, we require for each node and each class the mean arrival and service rates as
well as the SCOV of the interarrival times and service times. The bulk of the algorithm in
fact is to obtain them. There are three situations where this becomes hard: when multiple
streams are merged (superposition), when traﬃc ﬂows through a node (ﬂow), and when a
single stream is forked into multiple streams (splitting). For convenience, we assume that
just before entering a queue, the superposition takes place, which results in one stream.
Likewise, we assume that upon service completion, there is only one stream that gets split
into multiple streams. There are three basic steps in the algorithm (a software developed by
Kamath [10] uses the algorithm and reﬁnements; it can be downloaded for free and used for
analysis).
Step 1: Calculate the mean arrival rates, utilizations, and aggregate service rate param-
eters using the following:
λij,r = λi,rpij,r
λi,r = λ0i,r +
N

j=1
λj,rpji,r
λi =
R

r=1
λi,r
ρi,r =
λi,r
miμi,r
ρi =
R

r=1
ρi,r
(condition for stability ρ1 < 1 ∀i)

9-36
Operations Research Methodologies
μi =
1
R
r=1
λi,r
λi
1
miμi,r
= λi
ρi
C2
Si = −1 +
R

r=1
λi,r
λi

μi
miμi,r
2
(C2
Si,r + 1)
Step 2: Iteratively calculate the coeﬃcient of variation of interarrival times at each
node. Initialize all Cij,r = 1 for the iteration. Then until convergence perform i., ii., and
iii. cyclically.
i. Superposition (aggregating customers from all nodes j and all classes r the SCOV
of interarrival time into node i):
C2
Ai,r =
1
λi,r
N

j=0
C2
ji,rλj,rpji,r
C2
Ai = 1
λi
R

r=1
C2
Ai,rλi,r
ii. Flow (departing customers from node i have interdeparture time SCOV as a
function of the arrival times, service times, and traﬃc intensity into node i):
C2
Di = 1 + ρ2
i (C2
Si −1)
√mi
+ (1 −ρ2
i )(C2
Ai −1)
iii. Splitting (computing the class-based SCOV for class r customers departing from
node i and arriving at node j):
C2
ij,r = 1 + pij,r(C2
Di −1)
Note that the splitting formula is exact if the departure process is a renewal process. How-
ever, the superposition and ﬂow formulae are approximations. Several researchers have pro-
vided expressions for the ﬂow and superposition. The above is from Ward Whitt’s QNA [5].
Step 3: Obtain performance measures such as mean queue length and mean waiting
times using standard GI /G/m queues. Treat each queue as independent. Choose αmi
such that
αmi =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
ρmi
i
+ ρi
2
if ρi > 0.7
ρ
mi+1
2
i
if ρi < 0.7
Then the mean waiting time for class r customers in the queue (not including service) of
node i is approximately
Wiq ≈αmi
μi

1
1 −ρi
 C2
Ai + C2
Si
2mi


Queueing Theory
9-37
Notice that for all classes r at node i, Wiq is the waiting time in the queue. Other
performance measures at node i and across the network can be obtained using standard
relationships.
9.7
Concluding Remarks
In this chapter, we presented some of the fundamental scenarios and results for single
as well as multiclass queueing systems and networks. However, this by no means does
justice to the vast amount of literature available in the ﬁeld, as the chapter has barely
scratched the surface of queueing theory. But with this background it should be possi-
ble to read through relevant articles and books that model several other queueing sys-
tems. In a nutshell, queueing theory can be described as an analytical approach for sys-
tem performance analysis. There are other approaches for system performance analysis
such as simulations. It is critical to understand and appreciate situations when it is more
appropriate to use queueing theory as well as situations where one is better oﬀusing
simulations.
Queueing theory is more appropriate when: (a) several what-if situations need to be
analyzed expeditiously, namely, what happens if the arrival rate doubles, triples, and so
on; (b) insights into relationship between variables are required, namely, how is the ser-
vice time related to waiting time; (c) to determine the best course of action for any set of
parameters, namely, is it always better to have one queue with multiple servers than one
queue for each server; (d) formulae are needed to plug into optimization routines, namely,
to insert into a nonlinear program, the queue length must be written as a function to opti-
mize service speed. Simulations, on the other hand, are more appropriate when: (a) system
performance measures are required for a single set of numerical values; (b) performance
of a set of given policies needs to be evaluated numerically; (c) assumptions needed for
queueing models are unrealistic (which is the most popular reason for using simulations).
Having said that, in practice it is not uncommon to use a simulation model to verify ana-
lytical results from queueing models or to use analytical models for special cases to verify
simulations.
Another important aspect, especially for practitioners, is the tradeoﬀbetween using
physics versus psychology. Queueing theory in general and this chapter in particular deals
with the physics of waiting lines or queues. One should realize that the best solution is not
necessarily one that uses physics of queues but maybe some psychological considerations.
A classic example is a consultant who was approached by a hotel where customers were
complaining about how long they waited to get to their rooms using the elevators. Instead
of designing a new system with more elevators (and a huge cost thereby), the consultant
simply advised placing mirrors near the elevator and inside the elevator. By doing that,
although the actual time in the system does not improve, the perceived time surely does
as the customers sometimes do not realize they are waiting while they busily staring at the
mirrors!
From a research standpoint, there are several unsolved problems today, and a few of them
are described below. For the single-class and single-station systems, issues such as long-
range dependent arrivals and service, time-dependent arrival and service rates, nonidentical
servers, and time-varying capacity and number of servers have received limited attention.
For the multiclass and single-station queues, policies for scheduling customers, especially
when some classes have heavy-tailed service times (and there are more than one servers),
are being actively pursued from a research standpoint. For the single-class and multistation
queues, the situation where arrival and service rates at a node depend on the states of

9-38
Operations Research Methodologies
some of the other nodes has not been explored. For the multiclass and multistation case,
especially with re-entrant lines, performance analysis is being pursued for policies other
than FCFS (such as preemptive and non-preemptive priority).
Acknowledgments
The author would like to thank the anonymous reviewers for their comments and suggestions
that signiﬁcantly improved the content and presentation of this book chapter. The author
is also grateful to Prof. Ravindran for asking him to write this chapter.
References
1. D. Gross and C.M. Harris. Fundamentals of Queueing Theory, 3rd Ed. John Wiley & Sons,
New York, 1998.
2. G. Bolch, S. Greiner, H. de Meer, and K.S. Trivedi. Queueing Networks and Markov Chains,
1st Ed. John Wiley & Sons, New York, 1998.
3. R.W. Wolﬀ. Stochastic Modeling and the Theory of Queues. Prentice Hall, Englewood
Cliﬀs, NJ, 1989.
4. J.A. Buzacott and J.G. Shanthikumar. Stochastic Models of Manufacturing Systems.
Prentice-Hall, New York, 1992.
5. W. Whitt. The Queueing Network Analyzer. The Bell System Technical Journal, 62(9),
2779–2815, 1983.
6. V.G. Kulkarni. Modeling and Analysis of Stochastic Systems. Texts in Statistical Science
Series. Chapman & Hall, London, 1995.
7. M. Hlynka. Queueing Theory Page. http://www2.uwindsor.ca/˜hlynka/queue.html.
8. M.
Hlynka.
List of Queueing Theory Software.
http://www2.uwindsor.ca/˜hlynka/
qsoft.html.
9. M. Moses, S. Seshadri, and M. Yakirevich. HOM Software. http://www.stern.nyu.
edu/HOM.
10. M. Kamath. Rapid Analysis of Queueing Systems Software. http://www.okstate.edu/
cocim/raqs/.

10
Inventory Control
Farhad Azadivar
University of Massachusetts Dartmouth
Atul Rangarajan
Pennsylvania State University
10.1
Introduction.......................................... 10-1
On the Logic of Inventories • Types of Inventory •
ABC Classiﬁcation of Inventories
10.2
Design of Inventory Systems....................... 10-4
Factors in Favor of Higher Inventory Levels • Factors
in Favor of Lower Inventory Levels • A Simple
Classiﬁcation of Inventory Management Models
10.3
Deterministic Inventory Systems .................. 10-9
The Economic Order Quantity Model • Uniform
Demand with Replenishment Taking Place over
a Period of Time • EOQ Model with Back Ordering •
Time-Varying Demand: The Wagner–Whitin Model
10.4
Stochastic Inventory Systems......................10-20
Safety Stock • A Maximum–Minimum System with
Safety Stock • A Simpliﬁed Solution Method for
Stochastic Systems • Stochastic Inventory Model for
One-Period Systems • Stochastic, Multi-Period
Inventory Models: The (s, S) Policy
10.5
Inventory Control at Multiple Locations.........10-27
Multi-Echelon Inventory Models: Preliminaries •
Serial Supply Chain Inventory Models
10.6
Inventory Management in Practice ...............10-34
Inventory Management and Optimization Tools
10.7
Conclusions ..........................................10-35
10.8
Current and Future Research......................10-36
Final Remarks
References ....................................................10-37
10.1
Introduction
Each product travels a path from where it consists of one or several raw materials and ends
up in the hands of the consumer as a ﬁnished good. Along the way it goes through several
manufacturing or other transformation processes. For example, consider a petroleum-based
product, say gasoline, which starts as crude oil and is transported to a reﬁnery where it
is processed into its ﬁnal form. Assume that the crude oil is transported to the reﬁnery in
tankers that arrive at certain intervals. Obviously, the reﬁnery cannot work intermittently;
it requires a continuous input of crude oil, forcing it to build holding spaces to store crude
oil from tankers and then to use it at a continuous rate. Also, after producing gasoline
the reﬁnery cannot sell it at the exact rate at which it is produced. The reﬁnery will need
storage tanks to store its gasoline production and supply it to its users, that is, gas stations,
according to demand. A little further down the path gas stations themselves cannot ask
10-1

10-2
Operations Research Methodologies
people needing 10 gallons of gasoline to wait until they can get this exact amount from
the reﬁnery. Thus, each gas station builds a storage tank that is ﬁlled by the supplies from
the reﬁnery and provides small amounts of gas to each car that comes to the station. The
amount of raw material, semi-ﬁnished or ﬁnished product held at each node in the supply
chain (SC) is referred to as the inventory level of that particular material at that node.
Inventories are a part of our daily life; careful observation will show that we store (or
hold inventories of) many items. One needs to look no further than the kitchen at home to
see inventories of vegetables, milk, and the like. Running to the grocery store every time we
want a glass of milk would be ineﬃcient (and costly)!
10.1.1
On the Logic of Inventories
From the above petroleum example we see that one of the main functions of maintaining
inventories is to provide for a smooth ﬂow of product from its original raw form to its ﬁnal
form as a ﬁnished product in the hands of its ultimate users. However, even if all these
processes could be arranged such that the ﬂow could be kept moving smoothly without
inventories, the variability involved with some of the processes would still create problems
that holding inventories could resolve. For example, suppose that the crude oil could be
pumped out at a rate exactly equal to the input rate to the reﬁnery. Also assume that
gasoline could be sent to gas stations as it is produced through a pipeline, and further, gas
stations can supply cars at a continuous rate exactly equal to the output of the reﬁnery.
With all these unrealistic assumptions, the need for inventory still cannot be eliminated.
For instance, if the pipeline carrying the crude oil is damaged as a result of an accident,
customers of the disrupted gas stations will be aﬀected almost immediately. Or, if there is
a strike in the reﬁnery, again the eﬀect would be felt immediately. However, if inventories
of crude oil and gasoline were held at the reﬁnery and the corresponding gas stations, the
eﬀects may not be felt immediately or ever, as there would be enough supply to the end
users while the pipelines are being ﬁxed and the production gap could be ﬁlled by overtime
production. In general, there are three main reasons to hold inventory:
• Economies of scale—Usually, placing an order has a certain cost that is indepen-
dent of the quantities ordered. Thus, more frequent orders incur higher costs of
ordering paper work. This may even cause higher transportation costs because
the cost of transportation per unit is often smaller for larger orders. Economies of
scale also play an important role in those cases where buying in large quantities
results in a reduction of the unit price.
• Uncertainties—As mentioned above, as a product is converted from raw material
to ﬁnal product, variabilities in these transformational processes lead to losses.
Inventories help mitigate the negative impact of uncertainties.
• Customer service levels—While variabilities in demand and supply are inevitable,
inventories help buﬀer against these variations and ensure product availability.
Consequently, delays in satisfying customer demand are reduced (when out of
stock) and unnecessary loss of revenue is avoided while improving customer
satisfaction.
There are a few reasons why minimal inventories should be held. First, inventories repre-
sent signiﬁcant investment in capital and resources that may be employed to better purposes.
Second, many products have limited life cycles after which they become obsolete. When the
product is perishable (e.g., fresh food items), excess inventories may have to be thrown out
after expiry. Even if the product is not perishable, markdowns may be required to dispose of

Inventory Control
10-3
excess inventory, leading to lower revenues or losses. Finally, inventories may hide ineﬃcien-
cies in the system. Reduction of inventories coupled with better production processes form
the basis of just-in-time systems (see Silver et al. [1, chapter 16] for details on such systems).
Virtually all businesses hold inventory and inventory management remains one of the
biggest business challenges. Many consider it to be a necessary evil; it helps ensure prod-
uct availability for customers but consumes valuable resources to procure and maintain.
The magnitude of inventory-related operations is staggering: in 1989, the total value of
inventories held in the United States alone was close to $1 trillion [2] and had increased to
about $1.1 trillion in total inventories held by private industry alone in the United States in
1999 [3]. Furthermore, new business models have emerged with inventory management as
the strategic diﬀerentiator. Companies like Dell and Amazon.com have used supply chain
management and inventory management to achieve great success. Between 1995 and 2005,
Toyota carried 30 fewer days of inventory than General Motors, giving it immense advan-
tage [4]. It is evident that these problems merit close attention. Inventory management
problems are often extremely complex and mathematical models capture only a limited
part of this complexity. Mathematical models from the ﬁelds of management science and
operations research have emerged as strong contenders to help gain insight and provide sci-
entiﬁc methods to address inventory management problems. Recent advances in computing
and information technologies have enabled the development, solution, and implementation
of many advanced models that are used in industry today.
Fundamentally, these inventory management models address two critical issues: how often
should inventory be replenished and by how much each time? Models gain complexity
as additional factors are incorporated, more constraints are added to the problem, and
assumptions are made less rigid and more realistic. Research into mathematical inventory
models may be traced back to more than nine decades but began to receive sustained
interest from the 1950s onwards. While it would be impossible to comprehensively cover
this vast body of literature, this chapter introduces the reader to the basic issues and models
in inventory management. Consequently, the expository style used here is to bring out the
main intuition and results and not toward theoretical rigor. The interested reader is referred
to Axs¨ater [5] and Silver et al. [1] for excellent coverage of inventory management; see also
Bramel and Simchi-Levi [6], Graves et al. [2], Hadley and Whitin [7], Porteus [8], and Zipkin
[3] for more (mathematically) advanced treatments.
10.1.2
Types of Inventory
For production systems there is a need for basically three types of inventories. These are
inventories of raw material, ﬁnished products, and work-in-process (wip). Any of these
inventory systems could exist at any stage of the path of a product from its origin until it
reaches the hands of the ﬁnal users. Note that if various stages of production are performed
at diﬀerent locations, or sometimes even by diﬀerent companies, work ﬁnished in one location
could be considered the ﬁnal product for that location and raw material for the next location
downstream.
Inventory of raw materials: The need for keeping an inventory of raw materials
arises because of several factors. As mentioned in the above example, it is often
not possible or economical to procure raw materials exactly when the need for
them arises. The cost factors involved include the cost of paperwork, transporta-
tion costs, economies of scale, and unforeseeable events. Unpredictable events
such as international shortage of certain goods, a strike at the supplying plant,
a dry season, and the like, may interrupt the production process if a suﬃcient
inventory of raw materials is not on hand.

10-4
Operations Research Methodologies
Inventory of work-in-process: This category of materials refers to semi-ﬁnished
goods. These are the outputs of one stage of production that act as the input
to the following stage. The uncertainties involved in this case are less than the
raw material category but never nonexistent. The uncertainties are less because
the capacities of each stage are known and controllable. Thus, the designer can
match the capacities. However, uncertainties are still there because of problems
in scheduling, unforeseeable breakdown of machinery, and so on.
Inventory of the ﬁnished product: The inventory of the ﬁnal product involves
even more variability than is experienced with raw material. The demand for
the ﬁnal product is often uncertain, and the manufacturer has little control over
it. Another factor, which is important when considering ﬁnal product invento-
ries, is the cost involved in producing various sized lots. This sometimes favors
production of larger than demanded lot sizes because of potential savings in the
unit costs. All of these factors will be discussed in exploring inventory models.
Another important classiﬁcation is to divide inventories into cycle stocks, pipeline inven-
tories, and safety stocks. The amount of inventory on hand when ordering in batches is
referred to as cycle stock. It is used to meet normal demand patterns. On the other hand,
safety stocks refer to the average inventory on hand to buﬀer against uncertainties in sup-
ply and demand. Inventories (whether raw materials, wip, or ﬁnished products) spend quite
some time in transit between various stages of the SC (say from factory to warehouse,
warehouse to retailer, etc.) and are referred to as pipeline inventories. Such inventories are
not physically accounted for though they have been ordered and shipped. Depending on
the situation, either the buyer or the supplier must pay for these pipeline inventories and
therefore they are an important type of inventory. Pipeline inventory costs are usually an
integral part of inventory models that account for transportation costs.
10.1.3
ABC Classiﬁcation of Inventories
Most ﬁrms manage inventories of more than one product (commonly known as stock keeping
unit or SKU). Furthermore, these items may diﬀer in their costs, demands, criticality, and
so on. Management would like to monitor its most critical and high value items closely
and expend lesser eﬀort on other items. The ABC classiﬁcation is a simple methodology to
aid in this process. It has been observed in several large, multiple-item inventory systems,
that a small percentage of items represent a signiﬁcant portion of inventory investment.
ABC analysis involves computing the annual value of items to the ﬁrm as follows: the
product of the price and annual demand of each item is computed for all items. These are
then ranked in descending order and the cumulative percentages of annual dollar values and
number of SKUs are computed and plotted. An example of this is given in Figure 10.1a. The
salient characteristics of each class are given in Figure 10.1b. While the procedure involves
annual dollar value, we note that critical parts may be moved to class A or B irrespective
of monetary value to avoid disruptions. As the adage goes, “for want of a horseshoe, the
battle was lost.” See Silver et al. [1, chapter 3] for a detailed discussion on frameworks for
inventory management.
10.2
Design of Inventory Systems
The discussion so far has indicated a need for holding inventories of raw material, wip, and
ﬁnal product to take advantage of economies of scale and to counter the eﬀects of unexpected
events. The design of inventory systems refers to the process of determining the amount to

Inventory Control
10-5
1. Class A items:
• Few items (≈ 20%) deserving close attention
• Represent large investments (≈ 80%)
• May include critical components
• Significant management by exception
2. Class B items:
• Large number of items (≈ 30%) deserving 
moderate attention (less critical items)
• Represent moderate investments (≈ 10%−20%)
• Automated inventory management; occasional 
management by exception
3. Class C items:
• Large number of items (≈ 50%) deserving 
minimal attention
• Represent moderate investments (≈ 5%−10%)
• Simple inventory management system
(b)
(a)
Class C
Class B
Class A
75
0
90
Percentage value
100
20
50
Percentage of items
FIGURE 10.1
ABC analysis.
keep in inventory and the procedures and frequencies of replenishing consumed quantities,
that is, answering the two fundamental questions of how often and how much to order.
A major criterion for deciding the size of an inventory is the cost of holding the inventory.
One cannot increase the size of an inventory indeﬁnitely to eliminate all disruptions of the
production process, because the actual cost of keeping the goods in the inventory might
exceed the potential loss due to the shortage of goods and materials. On the other hand,
if lower inventory levels are maintained to reduce inventory holding costs, costs will go
up due to more frequent ordering as well as losses that may result because of potential
disruption of the production process. Thus, designing an inventory system requires ﬁnding
an arrangement that is a compromise between the costs favoring a high level of inventory
and those in favor of low levels. This, in fact, requires a system that minimizes the total
costs. Some of the relevant costs are discussed below.
10.2.1
Factors in Favor of Higher Inventory Levels
Ordering Costs and Set-up Costs
When materials are purchased, they are ordered from a supplier. Ordering requires a series
of actions with associated costs such as market research, bidding, and the like. Usually
these costs are independent of the amount ordered. Sometimes even the transportation cost
is independent of the size of the order. For example, if a company sends its own truck to
pick up the purchased materials the truck may have to make the trip whether the quantity
ordered is 1 ton or 10 tons. The ordering cost in this case would be the cost of making one
trip, independent of the amount ordered. In the analysis provided here, any cost associated
with ordering that is independent of the volume of the order is referred to as an ordering cost.
A ﬁxed ordering cost implies that larger orders require fewer orders to be placed during a
given period, thus incurring a lower total ordering cost for that period. For example, assume
that a company requires 12,000 electric motors each year and there is a ﬁxed ordering cost
of $60/order. If the company orders all the units in a single lot, it will order only once a
year at an ordering cost of $60/year. If the company orders 3000 units at a time, then it has
to order 12,000/3000 = 4 times a year at an annual ordering cost of $60 × 4 = $240. Thus,
ﬁxed ordering cost favors higher quantities per order with a lower frequency of ordering.

10-6
Operations Research Methodologies
For wip parts and ﬁnal products, where the units are produced rather than purchased,
the ordering cost is replaced by set-up costs. Set-up costs refer to the costs associated with
changing the existing setup of the machinery and production capacity to the setup required
for the next process. This involves stopping the work for a while, spending time and money
to change the arrangement of machinery and schedule, and a temporary loss of eﬃciency
due to the transition time required for the operators to get used to the new arrangements.
All these costs can be evaluated and combined into a ﬁxed set-up cost. Set-up costs are also
independent of the number of units produced because the set-up cost would be the same
for the production of either one unit or several thousand units. In inventory analysis set-up
costs are dealt with in exactly the same manner as ordering costs.
Quantity Discounts
Sometimes ordering or producing in large quantities yields a reduction in the unit cost
of purchased or produced items. Most suppliers will consider providing discounts for a
customer who buys in large quantities. Besides that, sometimes one can save by ordering
large shipments from a major supplier who normally does not sell in small quantities. The
same is true in the case of producing a product. Producing larger quantities may make
possible the use of more eﬃcient equipment, thus reducing the cost per unit. Because of
this, potential quantity discounts favor buying in larger volumes that in turn result in higher
levels of inventory. Silver et al. [1, chapter 5] cover basic quantity discount models; see also
Munson and Rosenblatt [9].
Unreliable Sources of Supply
When supply sources are unreliable, management may decide to stock up its inventories to
avoid the losses that could result from being out of stock. This also provides motivation to
increase inventory levels.
10.2.2
Factors in Favor of Lower Inventory Levels
Inventory Holding Cost
Keeping the physical items in the inventory has certain costs. Some of these costs are
• The interest on the capital invested in the units retained.
• The cost of operating the physical warehousing facility where the items are held.
These are costs such as depreciation of the building, insurance, utilities, record-
keeping, and the like.
• The cost of obsolescence and spoilage. The item that becomes obsolete as a result
of newer products in the market will lose some or all of its value. This loss in
value is considered as a cost; so are the losses in values of perishable items such as
foods or some chemicals and published items such as newspapers and magazines.
• Taxes that are based on the inventories on hand.
These costs, unlike ordering and set-up costs, are dependent upon the size of the inventory
levels. The interest cost on the investment for each item can be easily found. Inventory
holding cost per item is not as simple to estimate. This cost is usually estimated based on
the average size of the storage place, which is usually proportional to the average size of the
inventory being held. An average per unit cost of obsolescence and spoilage is even harder
to evaluate, but it can be evaluated roughly as the cost of the average number of units lost
divided by the average number of units in the inventory. The average ﬁgure is used here
because the number of items that could be lost cannot be predicted with certainty.

Inventory Control
10-7
10.2.3
A Simple Classiﬁcation of Inventory Management Models
Consider the operation of a simpliﬁed inventory system as shown in Figure 10.2. An order
of size Q units, called the order quantity, is placed with the supplier. The supplier may be
either an upstream production stage or an external stocking/production location who ships
this quantity (if available). This order is received after some (shipping) time delay. The time
between when an order is placed and when it is received is known as the lead time and we
denote it by L. The resulting inventory is used to satisfy customer demand (denoted by D).
Several variations in the supply and demand processes as well as the nature of the inven-
tory we are trying to manage result in several possible models as seen in the literature on
inventory management. Inventory models may be classiﬁed based on the following criteria
(Table 10.1).
1. Stocking location: Often suppliers and customer locations are not explicitly con-
sidered; instead we look to control inventory at a single stocking location (single
location models). When more than one stocking location is considered, such mod-
els are referred to as multi-echelon inventory models or SC inventory models in
the literature.
2. Control: In multiechelon models, the multiple locations may be under the control
of a single entity (allowing coordinated decisions) and are referred to as central-
ized SC inventory models [10]. Conversely, if multiple entities make inventory
decisions independently, we refer to these as decentralized SC inventory models
(see Refs. [11,14]). Centralized control models are usually harder to optimize
while decentralized models pose challenges in coordination of the SC locations.
Inventory
Customer
Supplier
Supply
Demand
FIGURE 10.2
Basic inventory system ﬂows.
TABLE 10.1
A Classiﬁcation of Inventory Models
Factors
Single Location
Multi-Echelon
Centralized
Control
Decentralized
Zero/Fixed LT
Deterministic
Supply process
OQ Based LT
Stochastic
Deterministic
Stationary
Demand process
Stochastic
Non-Stationary
Unknown
Unlimited
Capacity
Flexible
Fixed
Single Product
No. of items
Multiple Products
Single Source
Sourcing options
Multiple
Lateral Shipments

10-8
Operations Research Methodologies
3. Supply process lead times: Lead times are variable in reality. However, a common
simpliﬁcation is to assume that lead times are known and constant, that is,
deterministic. In fact, lead times are often assumed to be zero; when L > 0 and
deterministic, the model is usually analogous to the former case. Alternatively,
the lead time is assumed to follow a known distribution (stochastic lead time).
4. Demand process: Similar to supply processes, customer demand (simply demand
hereafter) may be deterministic and known or stochastic with known probability
distribution. The demand pattern (i.e., the underlying stochastic process) may
be time invariant or stationary with all demand parameters being constant over
time. When the parameters of the process (say the mean, standard deviation,
etc., of the distribution) change over time, the demand process is said to be
nonstationary. Nonstationary demands represent a challenging class of problems;
see Swaminathan and Tayur [15] for a review. Finally, there are many instances
when complete details about the demand distribution are unknown (e.g., new
product introductions, promotion periods, etc.). At best the estimates of the
mean and variance of demand may be known; see Gallego et al. [16], Godfrey
and Powell [17], and Moon and Gallego [18] for details.
5. Capacities:∗Inventory models often assume that capacity restrictions may be
ignored at every stocking location. This assumption may be relaxed to reﬂect
capacity limits. Sometimes, capacity may be moderately ﬂexible (e.g., outsourcing
to subcontractors, using ﬂexible manufacturing systems, etc.).
6. Number of items: Inventory models may look to manage the inventories of a single
product or multiple products simultaneously. If demand for these multiple items
is independent of the others and capacity restrictions do not exist, then the prob-
lem decomposes to several single-product problems. When capacity constraints
or demand correlations exist, the problem becomes more complex. Bramel and
Simchi-Levi [6] and Swaminathan and Tayur [15] provide references for the mul-
tiproduct case.
7. Sourcing options: Figure 10.2 shows a single upstream supplier. Single sourcing
involves the use of one supplier for each component. When more than one supplier
is used, the strategy is referred to as multiple sourcing. Lateral shipments refer
to scenarios where some locations are allowed to become suppliers (temporarily)
to other locations at the same stage in the SC. For example, a retailer may send
an emergency shipment to another retail location to meet a surge in demand. We
consider only single sourcing in this chapter; Minner [20] presents a recent review
of multiple sourcing models.
Inventory Control Models
The objective of analyzing inventory systems is to design a system that is a compromise
between factors favoring high levels of inventory and those in favor of low levels. This
compromise is intended to result in the minimum cost of maintaining an inventory system.
The cost of maintaining an inventory system is usually evaluated in terms of cost per unit
time where the unit time can be any length of time, usually a year.
∗The interplay between capacities, lead times, and inventory decisions is little researched but important.
Karmarkar [19] remains an important reference on these issues.

Inventory Control
10-9
Inventory models are mathematical models of real systems. To evaluate inventory sys-
tems mathematically, these models make several assumptions that are sometimes restrictive.
Generally, as the assumptions are relaxed, the mathematical analyses become more diﬃcult.
In this chapter some of the existing inventory models are discussed. First, it is assumed that
no uncertainty exists and deterministic models are presented. Then some limited stochastic
models will be presented.
10.3
Deterministic Inventory Systems
In deterministic inventory models it is assumed that the demand is ﬁxed and known. Also
it is assumed that when a lot is ordered, it arrives exactly at the time it is expected.
Further, when the demand pattern remains constant over time, the inventory pattern is
known exactly. Consequently, parameters like the maximum and minimum inventory levels
are known exactly. Thus, these models may also be referred to as maximum–minimum
inventory models.
10.3.1
The Economic Order Quantity Model
The economic order quantity model represents the ﬁrm inventory control model due to
Harris [21]. This model can be applied to both raw material and ﬁnished goods. In the case
of raw material inventory, the demand is represented by the amount used by the production
process over a given period. The ordered quantities in this case are the amount ordered
from the supplier. In the case of ﬁnished goods, the demand is the amount shipped to the
retailer or consumer over a given period of time, and the orders are those sent to suppliers
requesting new lots to be supplied. In this model the whole inventory process can be shown
graphically. In a graph the amount of goods or products in the inventory is plotted against
time. Let us explain this situation through an example.
Example 10.1
Consolidated Appliance Company maintains an inventory of electric motors that are used in
the production of refrigerators. Figure 10.3a shows the graphical representation of the level
of inventory as a function of time (with the y-axis being in 100 units scale). At the start
there are 3000 motors on hand. The company uses 1000 motors a day; thus at the end of
day 1, there will remain only 2000 units in the inventory. This situation is shown by point
1 with coordinates of 1 and 2000, which corresponds to units in the inventory at the end
of day 1. At the end of day 2 there are 1000 units left, which is shown by point 2 with
coordinates of 2 and 1000. At the end of day 3, the number of motors on hand should be
zero, but the arrival of a new shipment of motors, which contains 7000 units, increases the
inventory level to 7000 (point 3 in Figure 10.3a). In this way, the inventory level at the end
of each day can be represented by a point.
■
The points in Figure 10.3a show the number of units at the end of each period. However, if
one wants to demonstrate a real variation of the level of inventory by time, one can observe
the level at inﬁnitesimal time diﬀerentials. This results in a curve rather than a series
of points. As nothing is said about the pattern by which 1000 units are drawn from the
inventory each day, the curve between two consecutive ends of the day points can have any
shape. In the present model, however, it is assumed that the shape of the curve between the
points is linear. Thus the demonstration in Figure 10.3a can be changed to the one shown
in Figure 10.3b. From here on this linear graphical representation will be adopted for all

10-10
Operations Research Methodologies
0
1
2
3
4
5
6
7
8
9
10
0
10
20
30
40
50
60
70
80
Day
Quantity on hand
0 
1 
2 
3 
(a)
0
2
4
6
8
10
12
0
10
20
30
40
50
60
70
Day
Quantity on hand
(b)
FIGURE 10.3
Example 1: (a) Inventory graph. (b) Linear representation of inventory system.
the deterministic cases. There are several concepts in the design of inventory systems that
now can be explained using Figure 10.3b.
1. The system may be called a maximum–minimum system because the number of
units in the system varies between a ﬁxed maximum and a ﬁxed minimum.
2. The minimum in this model corresponds to zero. As the decision maker is sure
that what has been ordered will arrive exactly at the time expected, he will have
nothing to worry about if he is out of stock for just a moment. However, as we
will see later, this is not the case where uncertainties are involved in the time of
arrival of the new shipment or the quantities of the demand.
3. As is seen in Figure 10.3b, it is assumed that everything ordered arrives at one
point in time, thus instantly raising the level of inventory at days 3 and 10.
Furthermore, it is clear that the maximum inventory in this case is equal to the
amount ordered.
Formulating the Model
To design such an inventory system it is necessary to ﬁnd several quantitative parameters
with which the system can be uniquely deﬁned. The values of these parameters should be
such that they minimize the total cost of operating the inventory system. From Figure 10.3b
it is clear that this system can be deﬁned by three parameters: demand, the maximum level
of inventory, and the minimum level of inventory. It is assumed that the demand is ﬁxed and
the designer has no control on this parameter. Also, as mentioned above, for this model the
minimum inventory level is always zero. Thus, the only parameter to be determined in this
case is the maximum inventory level. On the other hand, it was also mentioned that the max-
imum inventory level for this model is the quantity ordered. Then the design of the system
can be deﬁned as ﬁnding the ordering quantity that minimizes the total cost of maintaining
the system. This best (optimal) order quantity is called the economic order quantity (EOQ).
To ﬁnd the optimum values of parameters of an inventory system the system is formulated
as an optimization problem according to the following procedure. First, a suitable period
of analysis is selected. This period is usually 1 year. Then the total cost of maintaining
the inventory over this period is evaluated as a function of the controllable parameters
of the system. This function is then minimized by diﬀerentiation or any other applicable
mathematical technique to obtain the optimum values for the controllable variables of the
system. To formulate the EOQ model the following notation is utilized.

Inventory Control
10-11
Q = Order quantity
Q∗= Economic order quantity
D = Demand per time-period
B = Ordering cost, $/order
E = Inventory holding costs, $/unit/time-period
U = Unit cost, $/unit
TC = Total cost per time-period
As was mentioned, the only controllable variable in this system is the order quantity.
Thus the total cost must be stated in terms of this variable. The total cost for this system
consists of two distinct types of expenses. These are inventory holding costs and ordering
or set-up costs. When the unit price of items is aﬀected by the quantity ordered (as in the
case where quantity discounts are involved), the cost of units is also added to the total cost.
In cases where the unit price is assumed constant, inclusion of this cost has no eﬀect on the
controllable variables of the system. In these situations, like the present model, this cost is
not considered.
Inventory holding costs are always present and depend on the number of units in stock and
the length of the time they are kept. In the electric motor example, suppose that keeping one
motor in inventory for 1 day costs one cent. According to our notation E = $0.01/unit/day.
This means that if 1000 units are kept in the inventory, the company bears a cost of
1000 × 0.01 = $10/day. If the amount of material kept in the inventory is constant over
time, say 1000 units, one can say that the contribution of the holding cost to the total
cost is $10/day, but as the level of inventory varies from one day to another, so does the
inventory holding cost. From Figure 10.3b, one can see that the inventory levels vary from
0 to 7000 units with the resulting holding cost varying from 0 to 7000 × 0.01 = $70/day.
To ﬁnd the average daily inventory cost, one may consider ﬁnding the inventory holding
cost for each day and calculating their average. Another method to estimate the same value
is to ﬁnd the average number of units kept in the inventory and multiply that by the holding
cost. As most of the time costs are calculated on an annual basis, it would be quite diﬃcult
to evaluate the holding cost for each of the 365 days in a year. As we will see in the following
paragraph, evaluation of the average inventory and multiplying it by the annual inventory
holding cost/unit is much easier. As we assume variations in inventory levels are linear, it
is easy to see that the average inventory is equal to the average of the maximum and the
minimum of these levels. Thus,
Average Inventory = Maximum Inventory + Minimum Inventory
2
(10.1)
However, we know that the minimum for our system is zero, and the maximum is just the
order quantity. Then
Average Inventory = Q + 0
2
= Q
2
(10.2)
The average holding cost per unit time would be
Average holding cost per unit time = E
Q
2

= EQ
2
(10.3)
Now refer to the second part of inventory costs, which is ordering or set-up cost. This cost
occurs whenever an order is placed, but this cannot be considered as the cost per day or
per month. The best way to evaluate the contribution of this cost to the inventory cost
for the evaluation period is to ﬁnd how many times the order is placed in one evaluation

10-12
Operations Research Methodologies
period. For instance, if the costs are evaluated per year and four times a year an order is
placed at a cost of $200, then the ordering cost per year would be 4 × $200 = $800. For our
electric motor problem, if we assume a cost of $125/order, the annual ordering cost can be
calculated as
No. of orders per year =
Annual Demand
Quantity ordered each time
(10.4)
= 365 × 1000
7000
= 52.14
⇒Annual ordering costs = $125 × 52.14 = $6518
Similarly, if the analysis period is 1 day, then the daily cost of ordering would be based on
ordering once a week. Thus,
No. of orders per day = 1
7
Daily ordering costs = $125 × 1
7 = $17.86
Now let us determine the frequency and the cost of ordering per period for a general model
in terms of our only controllable variable involved, the order quantity. As the items are
drawn from the inventory at a rate of D units per period, each order quantity lasts for Q/D
units of time. The number of orders placed during one period is
No. of orders per period =
1
Q/D = D
Q
(10.5)
Thus, paying $B per order, we have
Ordering costs per period = BD
Q
(10.6)
Now, returning to the total cost per period,
Total cost per period, TC = Inventory holding cost per period + Ordering cost per period
= EQ
2
+ BD
Q
(10.7)
To show that the addition of the acquisition cost of the units involved has no eﬀect in
optimizing the system, we add the cost of units to this total cost. Then,
TC = Inventory holding cost/period + Ordering cost/period + Cost of units/period
= EQ
2
+ BD
Q + UD
(10.8)
After stating the total cost as a function of the ordering quantity, we proceed to minimize
the total cost. In this case, since there is only one variable involved, simple diﬀerentiation
yields the result.
d(TC)
dQ
= E
2 −BD
Q2 = 0
⇒Economic Order Quantity, Q∗=

2BD
E
(10.9)

Inventory Control
10-13
As the above relationship shows, the cost of units involved does not play any role in this
optimum value. Thus, for this example, the economic order quantity can be calculated as
Q∗=

2 × 125 × 1000
0.01
= 5000 units
Lead Time and Reorder Point
There is another parameter that is important in designing inventory systems. Usually orders
are not received at the time they are placed. In inventory systems terminology, the length of
time between when orders are placed and the time when they are received is called the lead
time (denoted by L). In deterministic inventory models, it is assumed that the lead time
is constant and known. In stochastic inventory systems, the lead time could be a random
variable.
Going back to the design stage, we note that orders are not received immediately after
they are placed. If one waits until the inventory level reaches zero and then orders, the
inventory will be out of stock during the lead time. Thus orders have to be placed before
the inventory level has reached zero. Two methods can be used to determine when an order
should be placed. In the ﬁrst method the time at which the inventory will reach zero is
estimated and the order is placed a number of periods equal to the lead time earlier than
that estimated time. For example, if the lead time is 4 days, one could estimate the time at
which the inventory will reach the zero level and order 4 days before that time.
The second approach, which is used more often, is based on the level of inventory. In this
approach, the order is placed whenever the inventory level reaches a level called the reorder
point (ROP). This means that if the order is placed when the amount left in the inventory
is equal to the reorder point, the inventory on hand will last until the new order arrives.
Thus the reorder point is that quantity suﬃcient to supply the demand during the lead
time. Now, if we assume that both the lead time and the demand are constant, the demand
during the lead time is constant too. As the reorder point is equal to this demand, it can
be calculated from the following relation:
ROP = L × D
(10.10)
It is very important to keep in mind that the reorder point is not a point in time, but a
quantity equal to the level of the inventory at the time the order is to be placed. Figure 10.4
Time
Inventory level
Lead time
Reorder point
Economic order quantity
FIGURE 10.4
Graphical representation of economic order quantity, lead time, and reorder point.

10-14
Operations Research Methodologies
shows the values of economic order quantity, lead time, and reorder point graphically for a
simple maximum–minimum system.
10.3.2
Uniform Demand with Replenishment Taking Place over
a Period of Time
In this model, the assumption that all of the ordered units arrive at the same time is relaxed,
and is replaced by the assumption that units arrive at a uniform rate of P over a period of
time. This model is especially useful for situations where the units are produced inside the
plant and are stored to supply a uniform demand. Consequently, the model is often referred
to as the economic production quantity (EPQ) model.
The ﬁrst requirement to build up a stock is that the supply rate P be higher than the
demand D during the production or supply period. During the supply period the inventory
is replenished at a rate of P units per period and consumed at a rate of D units per period.
Thus, in this period the inventory builds up at a rate of (P −D) units per period. The
graphical representation of this system is shown in Figure 10.5. The design parameter for
this system is again the order quantity, but here the maximum inventory level and the
order quantity are not the same. The reason is that it takes some time to receive all units
of ordered quantity and during this time some of the units are consumed. Another way of
deﬁning this system is to specify the supply period instead of the order quantity. Knowing
the supply rate and the supply period, the order quantity could be determined. In this
chapter, the order quantity is speciﬁed as a design parameter.
Formulating the Model
This problem can be formulated very similarly to the basic model. The only diﬀerence here
is that the average inventory level is not half of the order quantity. As the system is still
linear and minimum inventory is zero, the average inventory is:
Average Inventory = Maximum Inventory + Minimum Inventory
2
= Maximum Inventory
2
(10.11)
Time
Inventory level
(P − D)
D
Maximum inventory
Supply 
period
FIGURE 10.5
Inventory systems with uniform supply.

Inventory Control
10-15
Then if we can state the maximum inventory in terms of order quantity, the average cost
of the inventory system can be found as a function of order quantity. The calculation for
maximum inventory is done as follows. Let ST represent the length of the supply period.
As the order quantity is Q and supply rate is P, then
ST = Q
P
During this supply period the inventory level is increased by (P −D) units per unit time.
As it is assumed that the supply starts arriving when the inventory level is zero, then the
maximum inventory level is equal to the buildup during this period. Then,
Maximum inventory, Imax = (P −D)ST = (P −D)Q
P
= Q

1 −D
P

(10.12)
The average inventory (Iavg) is then given from Equations 10.11 and 10.12 as
Iavg = Imax
2
= Q
2

1 −D
P

As a result, the inventory holding cost is EQ
2

1 −D
P

. The ordering cost is the same as
before. Thus,
TC = EQ
2

1 −D
P

+ BD
Q
(10.13)
As before, diﬀerentiating Equation 10.13 and equating to zero, we solve for the optimal Q,
which is given as
Q∗=

2BD
E

1 −D
P

(10.14)
Notes: In using Equation 10.14 it is important to note the following:
• If P is less than D, the term (1 −D/P) becomes negative. This would result in
a Q∗which is the square root of a negative value. This is due to insuﬃciency of
the supply. As mentioned earlier, to build up an inventory the production rate
must be higher than the consumption rate.
• The number of units produced and the number of units consumed over a long
period should not be confused with the production rate and consumption rate.
The production rate is higher, but production does not take place every day. To
avoid problems of this nature, it is suggested that the consumption rate always
be deﬁned in terms of the period over which the production rate is deﬁned. For
example, if the production rate is given in terms of units per day and consumption
in terms of units per month, the consumption rate should be converted into units
per day.

10-16
Operations Research Methodologies
10.3.3
EOQ Model with Back Ordering
In this model running out of stock is not forbidden, but a cost associated with back ordering
of each unit is taken into account. The characteristics of this system can be explained using
the graphical representation of this model as given in Figure 10.6. When the inventory level
reaches zero, at point O, demand for units continues. When the demands cannot be met,
they are recorded as back orders, meaning that the customer has to wait until the new
shipment arrives. Back ordered units could be considered as negative inventory. When the
new shipment arrives, the back orders are satisﬁed ﬁrst. After supplying all back ordered
units (which is assumed to take place instantly), the remainder of the units in the shipment
raises the inventory level to the maximum of Imax represented by point M in Figure 10.6.
There are two important points to be noted here. First, it is assumed that the new shipment
is ordered such that when it arrives the back orders have reached their maximum of S. This
changes the relation for the reorder point to
ROP = DL −S
(10.15)
For models where back orders are not allowed (as in the previous two models), S = 0 and
ROP = DL. The second point is that if the customer has to wait for his order, he may have
to be compensated by some means, such as a discount. These costs, which can be considered
as the penalty for late delivery, are represented by F and are stated as back ordering cost
per unit per unit time.
Formulating the Model
To design this system one controllable variable is no longer suﬃcient to deﬁne it. Three
variables can be deﬁned for the system. These are the maximum inventory (Imax), the
Time
Inventory level
t1
t2
Q
Maximum 
back orders
Maximum inventory
O
M
N
L
FIGURE 10.6
An inventory model with back ordering.

Inventory Control
10-17
maximum back orders (S), and the order quantity (Q). From Figure 10.6, see that
Q = LM + LN = Imax + S
(10.16)
This indicates that any two variables out of the above three would be suﬃcient to deﬁne
the system. The third variable can be obtained from the other two. Here, two variables, Q
and S, are selected as the design variables of the system. Then, the maximum inventory
can be obtained from Equation 10.16 as Imax = (Q −S).
As was shown earlier with a zero minimum inventory, the average inventory can be con-
sidered to be one-half of the maximum inventory. However, in this case it is important to
note that at some period of time during each inventory cycle the level of inventory is zero.
This period in Figure 10.6 is shown by t2 as the time segment between points O and L. So,
Holding cost per unit time during interval t2 = 0
Holding cost per unit time during interval t1 = E (Q −S)
2
Average holding cost/unit time =
E(Q −S)
2
(t1) + (0 × t2)
(t1 + t2)
= t1
t
E(Q −S)
2
	
(10.17)
where t = t1 + t2. The values of t1 and t2 can be found as follows.
t2 is the time during which the back orders are built up to S at a rate of D units per unit
time. Thus, t2 = S/D. Further, t = t1 + t2 is the period during which Q units are spent at a
rate of D units per unit time. Then t = Q/D and t1 = t −t2 = (Q −S)/D. Then,
Average holding cost/unit time = (Q −S)/D
Q/D
E(Q −S)
2
	
= E(Q −S)2
2Q
The ordering cost does not change from its previous value, as the same ordering pattern
continues and ordering cost/unit time = BD/Q.
For this model we have to add an additional cost that is called the back ordering cost and
is represented by F, the cost per unit back ordered for a unit time. The number of units
back ordered varies from 0 to S with an average of S/2, but again this occurs only during
the time interval t2. Then
Back ordering cost per unit time during t1 = 0
Back ordering cost per unit time during t2 = F S
2
Therefore,
Average back ordering cost/unit time =

t1 × 0 + t2
FS
2
	
t
= FS2
2Q

10-18
Operations Research Methodologies
Thus, the total inventory cost per unit time is
TC = Holding cost/unit time + Ordering cost/unit time + Back ordering cost/unit time
= E(Q −S)2
2Q
+ BD
Q + FS2
2Q
(10.18)
There are two variables of Q and S to be determined. By partial diﬀerentiation with respect
to Q and S and equating the results to zero, the values of Q and S can be found as follows.
∂(TC)
∂Q
= E
2
2Q(Q −S) −(Q −S)2
Q2
	
−BD
Q2 −FS2
2Q2 = 0
∂(TC)
∂S
= E
2
−2(Q −S)
Q
	
+ FS
Q = 0
Solving these two equations simultaneously yields the optimum values of Q and S as
Q∗=

2BD
E
×

E + F
F
(10.19)
S ∗=

2BD
E
×

E 2
F(E + F )
(10.20)
Now that we have two parameters and we know that Imax = Q −S, the optimal value for
Imax may be shown to be
I ∗
max =

2BD
E
×

F
E + F
(10.21)
10.3.4
Time-Varying Demand: The Wagner–Whitin Model
Thus far, we have assumed that demand is deterministic and constant over time (i.e., time-
invariant or stationary). In this section, we relax this assumption of stationarity; the model
is due to Wagner and Whitin [22]. The following assumptions are made.
1. Demand is deterministically known for the following N periods and given by the
vector D = {D1, D2, . . ., DN}. Note that time-varying demand implies Di ̸= Dj
for at least one pair of i, j.
2. Let the inventory at the end of period j be given by Ij and let the starting
inventory be I0 = 0.
3. Supply is assumed to be instantaneous.
4. Shortages and back orders are not permitted.
5. Holding costs are incurred on the ending inventory in every period j, with hj
being the unit holding cost for period j. Then denote Hj(x) = xhj. Similarly,
order costs for period j are given by Cj(x) = ajδ(x) + ujx where aj is the set-up
costs in period j and uj is the cost/unit in period j. Further, let δ(x) = 1 if x > 0;
0 otherwise. We assume that Hj and Cj are concave functions of their respective
arguments.
Our objective is to ﬁnd the order quantities in each period that minimized total ordering
and holding costs over the N periods. In other words, we want to ﬁnd Q = {Q1, Q2, . . ., QN}

Inventory Control
10-19
where all order quantities are non-negative. The model may be formulated as given below:
min Z =
N

i=1
[Ci(Qi) + Hi(Ii)] =
N

i=1
[aiδi(Qi) + uiQi + hiIi]
(10.22)
subject to
Ii−1 + Qi = Di + Ii
for all i
Qi, Ii ≥0
for all i
δi(Qi)
binary for all i
(10.23)
The above problem is a nonlinear, integer programming problem involving the minimization
of a concave function (since Hj and Cj are concave functions). To solve the problem, we
note the following [5,22]:
1. Inventory is held over a period if and only if the corresponding inventory costs are
lower than the ordering costs. Suppose k units are left at the end of period j −1;
then it would be optimal to hold these ⇔Hj−1(k) < Cj(k). If not, the previous
replenishment should be reduced by these k units to reduce costs.
2. Inventory is replenished only when the inventory level is zero (zero inventory
ordering property). Consequently, if an order is placed, it must cover demand for
an integer number of future periods.
Using the above observations, Wagner and Whitin [22] show that the following must be
true:
Qi−1Ii = 0
for all i
(10.24)
From the above properties and Equation 10.24, note that one of the following must be true
as a consequence:
1. Ii−1 > 0 ⇒Qi = 0; and Ii−1 > 0 ⇒Ii−1 > Di; Ii−1 ∈{Di, Di + Di+1, . . ., N
k=i Dk}.
Essentially, this means that if the last period’s ending inventory is positive, then
this inventory level is at least the current period demand (Di) and may be as
much as all remaining periods’ demand (N
k=i Dk).
2. Ii−1 = 0 ⇒Qi > 0. In fact, Qi ∈{Di, Di + Di+1, . . ., N
k=i Dk}. In this case, if
the ending inventory in period i −1 is zero, then an order must be placed in
the current period and this order quantity (Qi) must be, at a minimum, equal
to the current period’s demand with the maximum possible order quantity being
equal to the total demand of all remaining periods.
Further, note that both Ii−1 and Qi cannot equal zero (unless i = N + 1, i.e., the last period
demand has been met).
A simple method to “see” and solve the Wagner–Whitin model would be to represent it
as an acyclic graph (Figure 10.7). To do so, let each node represent the beginning of a period
1
2
3
4
i
j
N
N + 1
c1,4
c1,3
c1,2
c2,3
c3,4
ci,j
c2,4
cN,N+1
FIGURE 10.7
Graph representation of the Wagner–Whitin model.

10-20
Operations Research Methodologies
k = 1, 2, . . ., N ⇒node k + 1 represents the end of period k. The graph then has a total of
N + 1 nodes. The arc (i, j) represents ordering in period i with the next replenishment
being in period j. Consequently, all demands in periods i, i + 1, . . ., j −1 are to be met from
inventory. Further, let arc (i, j) exist only if i < j (i = 1, . . ., N; j = 2, . . ., N + 1). The total
number of arcs in the network is given by N(N + 1)
2
. Since the arc lengths (denoted by ci,j)
represent ordering and holding costs, we have
ci,j = Ci
j−1

k=i
Dk

+
j−2

k=i

Hk
 j−1

r=k+1
Dr

(10.25)
The optimal solution to the Wagner–Whitin model is given by the shortest path from node
1 to node N and may be solved eﬃciently using Dijkstra [23] and Wagelmans et al. [24].
See Lee and Nahmias [25] and Silver et al. [1] for examples and references.
Comments: Zangwill [26] solved the case where back orders are allowed. In addition to
these exact formulations, several heuristics like the Silver–Meal method, the Part–Period
Balancing method, and the like have been developed to get near-optimal solutions eﬃ-
ciently [1,27]. Finally, a requirement of the Wagner–Whitin model is that there be a ﬁnite
horizon (with known ending inventory level). Rangarajan and Ravindran [28] exploit this
requirement to solve a SC inventory model eﬃciency.
10.4
Stochastic Inventory Systems
So far, in all the models studied, it was assumed that the demand is constant and known.
Also, it was assumed that the units ordered will arrive exactly when they are expected. These
assumptions eliminated uncertainties and allowed somewhat simple solutions for designing
inventory systems. However, in the real world uncertainties almost always exist. Very few
manufacturers can claim that they know exactly what their demand will be. Also, very
seldom can they be certain that the supplier will deliver the ordered quantities exactly on
time. Thus, applying deterministic models may result in some undesirable consequences
that are discussed below.
Realistically, new orders do not necessarily arrive just when the last item in the inventory
is used up. Let us consider an example. Suppose an automobile service center stocks car
batteries. On an average, 10 cars that need new batteries are brought in every day. Let us
assume that the service shop has used a deterministic model of inventory control and orders
100 batteries every 10 days. Also, let us assume that the lead time is 3 days, which results
in a reorder point of 30 batteries. Suppose one day the shop operator notices that there
are only 30 batteries left and orders a new shipment. If the demand is deterministic and
is exactly 10 batteries a day, he would not have any problem while waiting 3 days for the
order to arrive. His 30 batteries would be suﬃcient for these 3 days. However, the demand
is usually not deterministic. During these 3 days, the total number of batteries demanded
could be 40, 30, 20, or any other number. If the demand for these 3 days happens to be 20,
there is no problem, but if it happens to be 40 or anything more than 30, the service shop
will not have suﬃcient batteries to supply the demand. In this case, the service center is
going to be out of stock for some period of time. In other words, an inventory system is out
of stock when there is no unit available to meet all or part of the demand.
As another example, consider the electric motor case. The lead time in this situation was
3 days, and the reorder point was calculated to be 3000 motors. Now, if an order is placed
at a point where there are 3000 units in stock and if a ﬁxed consumption rate of 1000 units
a day is assumed, the motors will be suﬃcient for the coming 3 days. However, if for some
reason the supplier delivers the order on the fourth day instead of the third, the inventory

Inventory Control
10-21
will be zero for 1 day, and the demand for 1000 motors will be left unsatisﬁed. Again this
inventory system will be out of stock but in this case for a diﬀerent reason.
The undesirable consequences of being out of stock vary, depending on the nature of the
operation. For example, in the case of the auto service center the customers may be willing
to wait for one more day to get replacement batteries. In this case, being out of stock does
not result in any problem. On the other hand, they may not be willing to wait and force
the service center to do something about the situation, like buying the needed batteries
from a neighboring supplier at a higher cost. It is also possible that the customers may take
their business elsewhere. This would result in a loss of proﬁt. Furthermore, those rejected
customers may never come back to this shop for future repairs, which again causes the loss of
goodwill and future revenue. In the case of the electric motors being used in manufacturing
home appliances, being out of stock for 1 day may halt the whole manufacturing process,
thus resulting in an appreciable loss.
The cost of being out of stock is called the shortage cost. The magnitude of this cost
depends on the nature of the operation. Shortage costs can somewhat be avoided by main-
taining some reserve stock called safety stock.
10.4.1
Safety Stock
In the auto service shop example, suppose the shop had ordered a new shipment at the time
when there were 40 units in stock instead of 30. It would then have been prepared to meet
a demand of up to 40 units during the lead time. On the other hand, if the demand had
followed the usual pattern of 10 a day, at the end of the lead time when the new shipment
was to arrive, 10 batteries would have still been left in stock. There also exists the possibility
of the demand being less than the expected demand. This may result in say 20 units left in
the inventory when the new shipment arrives.
To handle the uncertainties we will assume that the demand is a random variable with
an average of 30 units for the lead time period. If we decide on the reorder point of 40
units, the units left in the inventory when the new shipment arrives would also be a random
variable with an average of 10. This quantity of 10 units is called the safety stock for this
inventory situation. The signiﬁcance of safety stock is that it shows how many more units
we should keep in addition to the expected demand to cope with uncertainties. The safety
stock of 10 units for the above example may not solve all of the problems the auto service
shop faces. For example, what happens if the demand during the lead time becomes 50?
Again the shop will be 10 units short. One may suggest that the safety stock of 20 bat-
teries might be better as it could respond to even greater uncertainties. In the extreme, to
eliminate any chance of being out of stock, it may seem that a large safety stock would be
necessary.
Although a large safety stock may eliminate the risk of being out of stock, it raises another
problem. A large safety stock adds to the inventory holding costs. The capital spent on the
items in inventory is tied up and the insurance and tax costs on holdings increase. This
suggests that one should not increase the size of safety stock indeﬁnitely to eliminate the
risk of being out of stock. Rather, one has to establish a balance between the losses due to
being out of stock and the cost of holding the additional units in stock.
The optimum value of the safety stock is the value for which the sum of the cost of
being out of stock and the cost of holding the safety stock is a minimum. In stochastic
inventory systems the demand or lead time or both are random. Thus, the shortage would
be a random variable too. Then to evaluate shortage costs, some information about the
probability distribution of the demand, especially demand during the lead time, must be
given. In the derivations that follow these probability distributions will be assumed as given.

10-22
Operations Research Methodologies
Before starting to study the stochastic inventory models, two points regarding safety
stock must be made. First, when safety stock is considered, the procedure for calculating
the reorder point changes. Without safety stock the reorder point was simply the product
of lead time and the demand. With safety stock present, its value must be added to the
previous reorder point to satisfy the variations in the demand. Thus,
ROP = LD + SS
(10.26)
where ROP, L, and D are as previously deﬁned and SS is the safety stock.
It is very important at this point to examine this relationship carefully. As the factor
directly aﬀecting our decision is the reorder point rather than the safety stock, we usually
determine the best reorder point before ﬁnding the best safety stock. Safety stock can be
found from the reorder point by
SS = ROP −LD
(10.27)
Also, as safety stock is used when dealing with stochastic systems, the above relationship
could be better deﬁned by SS = ROP −Expected value of the demand during the lead time.
The second point to keep in mind is that since the basis for ordering is the reorder point,
the variation of the demand before reaching the reorder point does not create any problem.
For example, in the auto service example suppose that a new order of 100 units arrives
today. For the ﬁrst 2 days assume that the demand has been 10 batteries a day. On the
third day, when there are 80 batteries left, a demand of 20 batteries arrives and the stock is
reduced to 60. As the reorder point of 30 has not yet been reached, the shop does not have
to do anything. Suppose the next day the demand is 30 batteries, which will deplete the
inventory to 30 units. Again, the shop has nothing to worry about because no customer has
been turned away; the new reorder point has been reached, the new order is placed, and
everything is in order. Now, if during the next 3 days the demands are 15, 15, and 10,
respectively, the shop will be out of stock after 2 days, and even if it orders immediately,
the order will not arrive on time. As a result, the shop will be 10 units short.
The above example shows that having two drastic demands of 20 and 30 units before
reaching the reorder point did not create any problem, while two demands of 15 units each
after the reorder point caused shortages. Thus we could conclude that the only time we
have to worry about being out of stock is during the lead time, and the period of study for
shortages should be restricted to this period.
To deal with uncertainties, like any other stochastic system, we would need some knowl-
edge about the stochastic behavior of the system. In this case, the stochastic characteristics
of interest would be the probability distribution of the demand during the lead time. The
problem arises from the fact that given the probability distribution function of the demand
per day or any other period, it is not always easy to determine the probability distribution
of demand during the lead time, which consists of several periods. In other words, if the
probability density function of demand per day is denoted by f(x), the density function for
demand during the lead time of n days is not always a simple function of f(x). As a result of
the above discussion, the ﬁrst attempt in evaluating the safety stock must be to determine
the distribution of the demand during the lead time. In the discussion and examples of this
chapter only those cases will be presented for which this function can be easily evaluated.
10.4.2
A Maximum–Minimum System with Safety Stock
Here, a safety stock is added to the simple maximum–minimum inventory system. The
general form of this model is shown in Figure 10.8. In this ﬁgure, when the ordered units

Inventory Control
10-23
Time
Inventory level
ROP
L
EOQ = Q*
Safety stock = SS
A
B
C
FIGURE 10.8
Inventory system with safety stock.
arrive, there are still a number of units equal to the safety stock SS left in the inventory. Of
course, as was pointed out earlier, because of the random demand the number of units left
when the new order arrives is a random variable. Three of the possible amounts left in the
inventory when the new order arrives are shown in Figure 10.8 by points A, B, and C. SS is
the expected value of these and all other possible values of this quantity. When new units
are received, they are added to this safety stock. As a result the inventory level is built up
to the maximum of
Imax = SS + Q
For simplicity, it is assumed that the decline in the inventory level follows a linear path until
it reaches its minimum level of Imin = SS where another order is received. Although it may
seem unreasonable to assume a linear decline pattern when the demand is random, in the
long run this assumption does not greatly aﬀect the accuracy of the holding cost estimates.
The major eﬀect of the random demand on the cost occurs during the lead time where the
possibility of shortage exists.
Now, having identiﬁed the maximum and the minimum levels of the inventory we can
determine the total cost function as
Total Inventory Cost/Period = Inventory holding cost/period + Ordering cost/period
+ Shortage cost/period
From previous discussions we know that
Inventory holding cost/period = E (average inventory)
= E
Q + SS + SS
2

= E
Q
2 + SS


10-24
Operations Research Methodologies
Ordering cost/period = BD
Q
⇒TC = E
Q
2 + SS

+ BD
Q + SC
(10.28)
where SC is the shortage cost per period. Equation 10.28 could be used to minimize the cost
by ﬁnding the optimum values for the decision variables, Q and SS, provided that we could
state the shortage cost as a function of these two variables. This is possible if we know the
probability distribution of the random demand during the lead time. The demand during
the lead time is considered because that is the only time during which the possibility of
shortage exists. The calculation of shortage cost using that distribution is as follows.
Let the demand during the lead time be given by the random variable X with the density
function of f(x). Also let D and DL be the expected values of the demand per period and
the demand during the lead time, respectively. The shortages occur if the demand during
the lead time is more than the reorder point (denoted ROP). The reorder point in this
case is:
ROP = Expected value of the demand during the lead time + Safety stock
Expected value of the demand during lead time can be found as
DL =
 ∞
−∞
xf(x)dx
⇒ROP = DL + SS
(10.29)
Now the only time we may have shortage is when the demand during lead time exceeds the
reorder point. That is,
Shortage cost/Lead time =

0
if X ≤ROP
S × E[No. of units short]
if X > ROP
(10.30)
where S is the shortage cost per unit and E[·] denotes the expected value. Notice that
this assumption implies that the shortage cost per item is independent of the duration of
shortage. In other words, if an item is short during the lead time, the cost of its shortage
will be S whether this shortage happened 1 day before the new shipment arrived or 5 days.
However, the demand in excess of reorder point includes all values of demand greater
than ROP. Now if the demand is x and reorder point R, the shortage would be a random
variable that could be shown by (x −R). The expected value of this random variable is
E[No. of units short] =
 ∞
R
(x −R)f(x)dx
and, Shortage cost/Lead time = S
 ∞
R
(x −R)f(x)dx
As we have seen there are D/Q lead times per period
Shortage cost/Period = SC = DS
Q
 ∞
R
(x −R)f(x)dx
(10.31)
Although it is not explicitly stated, this cost is a function of Q and SS only. S is known
and the term under the integral after integration will be stated in terms of the limits of

Inventory Control
10-25
the integral that are between ∞and R = DL + SS. Then the overall inventory cost (from
Equations 10.28 and 10.31) will be
TC = E
Q
2 + SS

+ BD
Q + DS
Q
 ∞
DL+SS
(x −DL −SS)f(x)dx
(10.32)
Now, if the density function can be integrated, the total cost will be in terms of Q and SS
only. When the demand is discrete, the total cost function changes to
TC = E
Q
2 + SS

+ BD
Q + DS
Q

xi>R
(xi −R)p(xi)
(10.33)
where the summation is for all demands that are greater than ROP and p(xi) denotes
the probability mass function of the (discrete) demand distribution during the lead time.
Interestingly, Lau et al. [29] suggest that under certain conditions, order-quantity, reorder-
point models may yield impractical solutions or may be unsolvable. They suggest that cost
parameters and service-level targets be chosen carefully to yield good solutions.
10.4.3
A Simpliﬁed Solution Method for Stochastic Systems
When the demand is discrete, Equation 10.33 does not produce a continuous function for
diﬀerentiation. Also, sometimes even with continuous functions the resulting total cost
function is complicated and cannot be optimized easily. In such situations, the following
simpliﬁed method may result in a somewhat less accurate result, but the computations will
be easier. Consider Equation 10.32 or 10.33. The terms of these relations can be divided into
two categories. First, there are the terms associated with ordering and inventory holding
costs. These terms, which are similar to those in the deterministic model, are
TC1 = BD
Q + EQ
2
(10.34)
Second, there are the terms associated with holding the safety stock and shortage costs.
These terms are
TC2 = E × SS + DS
Q
 ∞
DL+SS
(x −DL −SS)f(x)dx
or TC2 = E × SS + DS
Q

xi>R
(xi −R)P(xi)
(10.35)
In the simpliﬁed model it is assumed that these two cost ﬁgures can be minimized separately
and independently from each other. This assumption is, of course, not correct because opti-
mizing a function term by term does not necessarily optimize the entire function. However,
the ease of evaluation obtained from this assumption sometimes compensates for the slight
loss of accuracy of the results.
Thus, the order quantity is found based on the ﬁrst two terms. Then this order quantity
is used in the second group of the costs to determine the optimum value of the safety stock.
The optimum order quantity from Equation 10.34 can be obtained as follows:
dTC1
dQ
= E
2 −BD
Q3 = 0
⇒Q∗=

2BD
E
=

2B E[D]
E
(10.36)

10-26
Operations Research Methodologies
Note that this is the relationship for the economic order quantity for the simple maximum–
minimum inventory model (i.e., the EOQ model). The only diﬀerence here is that demand is
represented by the expected value of the demand (as denoted explicitly in Equation 10.36).
When demand is a continuous random variable, we need to evaluate the integral in Equa-
tion 10.35. Once this is done, the resulting function becomes a single variable function in
terms of SS. Again, one has to be careful in minimizing such a function to make sure SS
falls within the acceptable range. For discrete demands the cost could be evaluated using
ROP = DL −S for the range of acceptable values of SS, and the optimum value could be
estimated as the one corresponding to the minimum cost.
10.4.4
Stochastic Inventory Model for One-Period Systems
This model applies to systems that operate for only one period and where items cannot be
carried from one period to the next. Examples of this type are perishable products, fashion
goods, or products that become obsolete or outdated after one period or selling season.
Consider keeping an inventory of fruits and vegetables to supply a stochastic demand for a
certain period. Obviously, at the end of the period, items left over are considered expired and
must be thrown away or sold at a considerable discount. Another example is the procurement
of newspapers and magazines. After a day or a week they become worthless. This model
is commonly referred to as the newsvendor model. To design an inventory system for these
situations one has to ﬁnd the amount to procure such that the expected value of the proﬁt
is maximized as given below.
The newsvendor sells a single product over a single selling season with one procurement
opportunity before the start of the season. Demand is stochastic with known distribution
function F(·) and probability density function f(·). The product is procured at a cost of
$c/unit and sold at a price of $p/unit. Any unsold inventory at the end of the selling period is
salvaged (i.e., the ﬁrm receives this amount per unsold unit) at a value of $s/unit (p > c > s).
Back orders are not permitted and supply is assumed to be instantaneous with no set-up
costs. The newsvendor is faced with the decision of how much to order at the start of the
period so as to maximize expected proﬁt (denoted by Π). As before, the order quantity is
given by Q and demand by D. To develop the proﬁt maximization model, note that
E[Π] = E[proﬁt when D ∈[0, Q]] + E[proﬁt when D ∈[Q, ∞)]
(10.37)
When demand is less than or equal to Q, then all demand is met and a loss (=(c −s)/unit)
is incurred for any excess inventory (ﬁrst term on the right). When demand exceeds the
available inventory, then all Q units are sold (indicated by the second term on the right in
Equation 10.37). Let the random demand be given by X with known distribution. Then,
we may write Equation 10.37 as
E[Π] =
Q

0
[(p −c)x −(c −s)(Q −x)]dF(x) +
∞

Q
[(p −c)Q dF(x)]
(10.38)
Using Leibniz’s rule to diﬀerentiate Equation 10.38 and setting the derivative to zero, we
can show that the optimal order quantity, Q∗, is given by
Q∗= F −1
p −c
p −s

= F −1(cf)
(10.39)
where cf = (p −c)/(p −s) is known as the critical fractile. Now, note the following.
cf = p −c
p −s =
p −c
(p −c) + (c −s) =
cu
cu + co
(10.40)

Inventory Control
10-27
where cu, the underage cost, represents the proﬁt/additional unit that is lost by being short
of demand and co, the overage cost, represents the loss incurred/additional unit that is in
excess of demand.
Example 10.2
Let demand be random and uniformly distributed between 100 and 400 over the selling sea-
son. Further, let p = 12$/unit, c = 9$/unit, and s = 0$/unit. From Equation 10.40 we have
cf = (12 −9)/(12 −0) = 0.25. The optimal order quantity, Q∗, is given by Equation 10.39.
For a uniform random variable, F(x) = x −a
b −a ; x ∈[a, b]. Therefore, Q∗= cf(b −a) + a =
175 units.
■
Comments: (i) The case when demand follows a discrete distribution is analogous to the one
above. (ii) Q∗is often called the base stock level. If the starting inventory is less than Q∗,
then the optimal action is to bring the inventory level up to Q∗(i.e., order Q = (Q∗−I0)
units, where I0 is the inventory at the start before ordering). Such a policy is called a base
stock policy in the literature and has wide applicability. Evidently, the maximum inventory
level will equal the base stock level in this model. (iii) Several modiﬁcations of this model
are possible [see 1,15,30]. The newsvendor model also forms the basis of many models
addressing decisions in pricing [31–34], supply chain inventory models [35,36], production
capacity investments [37,38], and so on.
10.4.5
Stochastic, Multi-Period Inventory Models: The (s, S) Policy
Given the simple solution to the single period problem above, it is obvious to ask if the
analysis can be extended to a multi-period setting. Multiperiod inventory models are usually
addressed using techniques from dynamic programming; see Chapter 7 of this book for
an overview. Essentially, we are interested in ﬁnding the minimum expected cost over N
periods. As such, two important cases are possible: the ﬁnite horizon case, where decisions
are to be made over a ﬁnite number of periods (N < ∞) or the inﬁnite horizon case (N = ∞).
While the derivation of such models is beyond the scope of this chapter, it has been shown
that in both cases [39,40] the optimal inventory control policy is of the form of (s, S) with
s < S. The policy essentially implies the following ordering rule: if inventory falls below the
reorder level (= s), then place an order to bring the inventory level up to S. To state this
mathematically, let inventory at the start of a period be I and the decision variable be Q,
the order quantity for that period. Then, (s, S) implies the following optimal ordering rule
Q =
(S −I)
if I < s [i.e., raise inventory to S]
0
if I ≥s [i.e., do nothing]
(10.41)
Evidently, the (s, S) policy is fairly simple in form and easy to implement. However, it
must be noted that computation of the two parameters (s and S) is not always easy though
several eﬃcient heuristics are available. The (s, S) policy has also been shown to be opti-
mal under fairly generic assumptions. For detailed development of the model, assumptions,
extensions, and solution procedures, see Lee and Nahmias [25], Porteus [8], and Zipkin [3].
10.5
Inventory Control at Multiple Locations
In the models discussed thus far, we have looked exclusively at managing inventories at a
single facility. In doing so, we have assumed that orders placed with an “external supplier”
are assured to arrive after a suitable lead time (when L > 0). The astute reader may have

10-28
Operations Research Methodologies
Material flow
FIGURE 10.9
A general multi-echelon supply chain network.
reason to pause and ask: “What if the supplier is out of stock?” Furthermore, virtually
all facilities holding inventories are part of a SC in reality. One therefore begins to see the
need for looking at inventory management at multiple locations. Such models are referred
to as SC inventory management models or as multi-echelon inventory theory in the research
literature.∗
As mentioned previously, inventory represents considerable investments and is considered
one of the drivers of SC performance; see Chopra and Meindl [43] for detailed discussions
of the role of inventories in the SC. The core challenge of managing inventories at multiple
facilities is the issue of dependencies. Consider a retailer who is supplied by a warehouse.
In the models of Sections 10.3 and 10.4, we either assume that this supplier has inﬁnite
capacities (or inventories) or that such inventories arrive after a lead time that is either ﬁxed
or described by a probability distribution. However, this ignores the explicit consideration
of the supplier inventory, the dependency of the retailer on the supplier’s inventory and
the need to perhaps control supplier inventories as well. To complicate matters further,
inventories are usually held at multiple locations in the SC and the SC may be viewed
as a network (Figure 10.9). In this section, we brieﬂy overview the basics of multi-echelon
inventory models and refer the reader to more detailed texts on this topic [2,5,6,44] for
further details and references.
10.5.1
Multi-Echelon Inventory Models: Preliminaries
Quantitative models for complex SC structures as in Figure 10.9 are diﬃcult to work
with. Based on common network topologies observed in practice and to make models
more tractable, most multi-echelon inventory models assume one of three common net-
work structures. Before delving into this, we introduce some relevant terminology. In our
∗Multi-echelon inventory theory emerged from the need to control inventories at multiple production
stages in a facility and early literature reﬂects this “production” heritage. Clark [41] and Scarf [42], pioneers
in the ﬁeld, give some perspective on the early developments and their work. In this chapter, we focus more
on SC inventory management that considers these locations to be distinct, reﬂecting contemporary practice.

Inventory Control
10-29
(a) Serial system
(b) Lateral supply
(c) Assembly network
(d) Distribution system
FIGURE 10.10
Common supply chain structures.
retailer–warehouse example, the retailer represents one stage or echelon of the SC and the
warehouse is another stage of the SC. In general, an SC may have multiple stages and
multiple facilities at each stage (as shown in Figure 10.9). In all cases, clear relationships
exist between these stages; namely, material “ﬂows” (or moves) from one stage to the next.
As we move from the raw material suppliers toward the ﬁnal stages that see end customer
demand, we are said to be moving downstream in the SC and upstream when in the reverse
direction. So, material ﬂows from upstream suppliers to downstream retailers.
We can now look at the common SC structures used in many multi-echelon inventory
models. These are represented graphically in Figure 10.10a–d.
1. Serial system, shown in Figure 10.10a, is an SC wherein every facility has at most
one upstream supplier and one downstream customer and only one facility exists
at each stage. This often represents the simplest structure.
2. Assembly network is one in which there is one downstream facility that is sup-
plied by multiple upstream suppliers, who may in turn be supplied by multiple
suppliers themselves. It is commonly assumed that each upstream stage has only
one customer at the next downstream stage. Figure 10.10c gives an example of
such a system.
3. Distribution systems are networks where upstream facilities supply multiple down-
stream stages. A special case of particular importance is when each facility has
at most one supplier but may itself supply multiple downstream customers. Such
a network is called an arborescent distribution network (Figure 10.10d).
4. Lateral supply—As mentioned in Section 10.2.3, in some cases, a facility may
receive supplies from another facility at the same stage. For example, consider
two Wal-Mart stores in a city. If one ships inventory to the other to make up for
shortages at the latter facility, this is known as a lateral shipment. This is usually
done as an emergency measure, though. Figure 10.10b represents this graphically.

10-30
Operations Research Methodologies
Note that the serial system is a special case of both the assembly and distribution network
types. In our discussions here, we will look at serial systems. See Graves et al. [2], Song and
Yao [45], and Song and Zipkin [46] for details on SC structure and assembly systems.
Inventory Coordination Policies
The eﬀect of dependencies between inventories in the SC has the potential to complicate
operations. In the single warehouse, single retailer (serial) SC example, assume that the
retailer orders every week and the warehouse every 5 days. Ideally, we would like these
two reorder intervals to be such that the overall SC inventory costs are minimized; in
other words, we would like the SC members to be coordinated. Inventory coordination
schemes help smoothen operations. Coordination takes on special signiﬁcance in a decen-
tralized control setting where members in the SC operate independently, often leading
to ineﬃciencies. Coordination is usually achieved in two ways: quantity coordination or
time coordination. In quantity coordination, SC members change their order quantities
so as to reduce system-wide costs. This is done using SC contracts that help induce this
change and redistributes some of the savings between the SC members. In time coordina-
tion schemes, the reorder intervals of the members are changed to bring some regularity
to operations (e.g., ordering weekly). Evidently, under deterministic settings, the two are
equivalent in terms of the changes in the order quantities and reorder intervals (though
contracts also look at the issue of sharing of cost savings). We discuss time coordination
policies here for two reasons: (a) they are operational in nature, fairly simple, and do
not involve issues like transfer pricing and compensation issues; and (b) optimal or near
optimal policies are available even without the use of more complex contracts and this suf-
ﬁces for the discussion at hand. Cachon [47] and Tsay et al. [48] review the contracting
literature.
The following notation is used in the discussions below. The term facility is used to denote
an inventory stocking location. In general, we denote the ﬁrst upstream facility as node 0
and the last down stream facility as node N in the serial system. The distribution system
in Figure 10.10d is commonly referred to as the single warehouse, multiple retailer system,
and we denote the warehouse as facility 0 and the retailers as facilities 1 through N. Let the
review period length at each location be Ti, i = 0, 1, . . ., N. The following are some common
coordination policies [12,49].
• Stationary Policy: If each facility orders at equally spaced points in time and in
equal quantities each time, then the policy is called a stationary policy.
• Nested Policy: If each facility orders every time any of its immediate suppliers
does (and perhaps at other times as well), then it is known as a nested policy.
• Stationary Nested: A policy is stationary and nested if Ti ≤T0 for all i = 1, 2, . . ., N.
Such a policy ensures that the order quantities at upstream stages are stationary.
Zero Inventory Ordering Property
Schwarz [50] considered a single warehouse, multi-retailer system and showed that an opti-
mal policy may be fairly complex. He then deﬁned a basic policy that has the following
properties: (a) deliveries are made to the warehouse only when the warehouse and at least
one retailer have zero inventory; (b) deliveries are made to a retailer only when the retailer
has zero inventory; and (c) all deliveries made to any given retailer between successive
deliveries to the warehouse are of equal size. This result is known as the zero-inventory

Inventory Control
10-31
ordering property. In fact, the result holds for many other systems also when demand is
deterministic.
With this background, we now proceed to consider some basic multi-echelon inventory
models facing deterministic end demand.
10.5.2
Serial Supply Chain Inventory Models
Consider a serial, two-stage SC consisting of a warehouse supplying a downstream retailer
who faces demand of D units/time period. The warehouse and retailer have order costs of
a0, a1, inventory holding costs of h0, h1, and the inventory at each facility is denoted by
I0, I1 (in appropriate units), respectively. Further, it is assumed that h0 < h1 as unit costs
and holding costs usually increase as we move downstream in the SC. We also assume that
the SC follows a nested coordination policy wherein the warehouse reorder interval (T0) is
a positive integer multiple (m) of the retailer’s reorder interval (T1); that is,
T0 = mT1;
m = 1, 2, . . .
(10.42)
Assumptions of the EOQ model are assumed to hold unless otherwise stated. Two control
policies are possible (see Section 10.2.3), namely centralized and decentralized SC control
and both are discussed below.
Decentralized Control of the Serial System
In this case, the warehouse and retailer make inventory decisions independent of the other
subject to the nested policy constraint. So the retailer solves his/her inventory problem
(using the EOQ model; Section 10.3.1) ﬁrst and communicates the optimal reorder interval
(T ∗
1 ) to the warehouse. Thus, from Equation 10.9, we have
T ∗
1 =

2a1
Dh1
;
Q∗
1 = DT ∗
1
(10.43)
The corresponding optimal cost is given by Z∗
1 = √2a1Dh1. The inventory plot for the
retailer is shown in the top panel of Figure 10.11. This is known as the retailer’s installation
stock and it follows the familiar “saw-tooth” pattern of the EOQ model.
Now, the warehouse sees a bulk demand of Q∗
1 units every T ∗
1 time units. Thus, the
warehouse inventory pattern is not a saw-tooth pattern and is represented by the step
function in the middle panel of Figure 10.11. It is fairly easy to show that in this case, the
average inventory at the warehouse (denoted ¯I0) is given by
¯I0 = Q1(m −1)
2
(10.44)
Then, the total variable costs at the warehouse is given by
Z0(m) = a0
T0
+ h0Q∗
1(m −1)
2
=
a0
mT ∗
1
+ h0DT ∗
1 (m −1)
2
(10.45)
It is easy to show that Equation 10.45 is pointwise convex in the integer variable m and
so the minimum value is attained when the ﬁrst diﬀerence (Equation 10.46) equals zero or
changes from negative to positive.
ΔZ0(m) = Z0(m + 1) −Z0(m)
(10.46)

10-32
Operations Research Methodologies
Qr
Time
Retailer installation stock
Supplier installation stock
3Qr
2Qr
Qr
Supplier echelon stock
3Qr
2Qr
Qr
Echelon Stock
Time
Time
FIGURE 10.11
The concepts of installation and echelon inventories.
Substituting Equation 10.45 in 10.46 and simplifying, we can show that the optimal solution
is given by identifying the smallest positive integer, m∗, such that the following is true.
2a0
h0D(T ∗
1 )2 ≤m∗(m∗+ 1)
(10.47)
Note that the left hand side of Equation 10.47 is a constant. The problem is a single variable
problem and eﬃcient optimization techniques may be employed to ﬁnd the optimal m∗to
get Q∗
0 and T ∗
0 . This completes the analysis of the decentralized SC.
Centralized Control of the Serial System
In this setting, we are looking to optimize the reorder intervals at both facilities simultane-
ously. It is common practice to assume that all information like costs, inventory levels, and
the like at all facilities in the centralized network are known globally. We should note that
this places greater control in the hands of the manager(s), though obtaining such detailed
information in a timely manner may not be easy in practice. The optimization problem to
minimize the system-wide total variable costs (i.e., the sum of the ordering and holding

Inventory Control
10-33
costs at both facilities) is given by
Problem C: min Zc =
a1
T1
+ a0
T0

+
h1Q1
2
+ h0Q1(m −1)
2
	
= 1
T1

a1 + a0
m

+ T1
h1D
2
+ h0D(m −1)
2
	
(10.48)
subject to
T0 = mT1;
m = 1, 2, . . .
T1 ≥0
Problem C is a nonlinear, integer programming problem in two variables, m and T1. Before
proceeding, though, let us pause to consider the average inventory expression at the ware-
house. Under more general SC structures, we may not get a nice expression for I0. Clark and
Scarf [51] introduced the concept of echelon inventories, which is deﬁned as the total inven-
tory of a product at any particular stage in the SC and all subsequent downstream stages
linked to it. From this deﬁnition, the echelon inventory at the retailer (stage N) is identical
to its installation inventory. The echelon inventory helps us simplify the computation of ¯I0
in many systems.
Consider Figure 10.11, where we assume that T0 = 3T1 ⇒Q0 = 3Q1. Now, every T1 time
units, Q1 is released from warehouse inventory to the retailer resulting in the step function
shape of warehouse inventory level over time. However, note that this inventory merely
moves down the SC to the retailer. In a centralized SC, information on this inventory is
assumed to be known. So, by superimposing the two stages’ inventory curves, we get the
warehouse’s echelon levels (bottom panel of Figure 10.11) which has the familiar saw-tooth
pattern. Thus, the mean echelon inventory at the warehouse may be approximated in a
fashion similar to that of the retailer.
To complete the analysis, we are given h0 < h1. Deﬁne h′
0 = h0 and h′
1 = h1 −h0 as the
echelon inventory holding rates at the warehouse and retailer, respectively. Further, let
gi = h′
iD
2 ; i = 0, 1. Substituting these in Equation 10.48 and simplifying gives us
Zc = 1
T1

a1 + a0
m

+ T1(g1 + mg0)
(10.49)
Now, note that the form of Equation 10.49 is similar to the EOQ model objective function,
but with two variables. To solve this for optimal values, the following method may be used:
ﬁx m to some value and compute the corresponding best value for the reorder interval
given by
T ′(m) =

(a1 + a0
m )
(g1 + mg0)
(10.50)
Again using the ﬁrst diﬀerence argument, we can show that the optimal integer multiple,
m∗, is given by the smallest integer satisfying the following relation:
m∗(m∗+ 1) ≥a0/g0
a1/g1
(10.51)
Equations 10.50 and 10.51 may be used to get the optimal reorder interval for the retailer
(T ∗
1 ) and this gives us the corresponding warehouse reorder interval (T ∗
0 = m∗T ∗
1 ). The
optimal variable costs for the serial SC is given below, completing the analysis.
Z∗
c =

a1 + a0
m

(g1 + mg0)

10-34
Operations Research Methodologies
Comments: (i) The above procedure may be generalized to a serial SC with N stages.
(ii) Schwarz [50] proved that the nested policy is indeed optimal for a serial SC. See Muck-
stadt and Roundy [52] for further details.
The models above represent the simplest multi-echelon inventory models. Several exten-
sions are possible and the reader is referred to [5,6,12,14,49,51,52] for more extensive dis-
cussions on multi-echelon inventory theory.
10.6
Inventory Management in Practice
One may wonder, given the many simpliﬁcations made in developing inventory management
models, if the models are of value in practice. The short answer is a resounding “Yes!” We
readily acknowledge that all the models are not applicable in all situations; real-world appli-
cations often use heuristics, simulations, and the like to manage inventories (in single and
multi-echelon settings). However, most of these are based on theoretical models like the ones
discussed in this chapter. Also, the focus in practice is often on getting good solutions rapidly
(and often optimal solutions are not achievable due to complex constraints, etc.). In many
situations, the models discussed previously can and have been applied in practice. A sample
of applications are listed in Table 10.2. All these cases are taken from the Interfaces journal.∗
TABLE 10.2
Examples of Inventory Management Applications in Practice
Reference
Industry
Company
Comments
Lee and Billington [53]
Computers
Hewlett-Packard (HP)
• Goal: Inventory management in
decentralized SC for HP Printers
• Inventory reduction of 10%–30%
Lin et al. [54]∗
Computers
IBM
• Goal: Decision support system (DSS) for
global SC (inventory) management
• Approx. $750 million in inventory and
markdown reductions
Koschat et al. [55]
Print media
Time Warner
• Goal: Optimize printing orders and
distribution of magazines in three stage SC
• Solutions based on the newsvendor model
• $3.5 million increase in annual proﬁts
Kapuscinski et al. [56]
Computers
Dell Inc.
• Goal: Identify inventory drivers in SC for
better inventory management at Dell DCs
• Expected savings of about $43 million;
67% increase in inventory turns; improved
customer service
Bangash et al. [57]
Electronics
Lucent Technologies
• Goal: DSS tool for inventory management
of multiple products
• Solution based on (s, S) policies
• $55 million in inventory reductions;
ﬁll rates increased by 30%
Bixby et al. [58]
Food
Swift & Co.
• Goal: Production management at
beef products facilities; DSS tool for sales
• Solution adapts production plans based on
inventories and customer orders dynamically
• $12.74 million in annual savings; better sales
force utilization
∗Franz Edelman Award winner.
∗“Interfaces, a bimonthly journal of INFORMS, is dedicated to improving the practical application of
OR/MS to decisions and policies in today’s organizations and industries. Each article provides details of
the completed application, along with the results and impact on the organization” (from the journal Web
site; http://interfaces.pubs.informs.org). INFORMS also awards the Franz Edelman Award annually, from
applications reported in Interfaces, the purpose of which is to “. . . call out, recognize and reward outstanding
examples of management science and operations research practice in the world.”

Inventory Control
10-35
TABLE 10.3
Inventory Management Software
Vendors (sample)
Vendor
Web Site
i2 Technologies
http://www.i2.com
Logility
http://www.logility.com
Manhattan Associates
http://www. manh.com
Oracle Corporation
http://www.oracle.com
SAP
http://www.sap.com
10.6.1
Inventory Management and Optimization Tools
At some point, most readers will want to implement some of the models discussed here and
elsewhere. Several tools exist for managing inventories, including analytical and optimization
applications. It is worth pointing out that several of the models described in this chapter
(e.g., EOQ, Newsvendor, etc.) may be easily implemented using a spreadsheet program.
This makes it easy to test out the models and perform what-if analysis. For large-scale
implementations, specialized software tools exist.
Most models require good data inputs and the recent trend in industry is to use enter-
prise resource planning (ERP) systems to manage large amounts of data. Often, inventory
management and SC management tools work in concert with ERP systems. Specialized
inventory management and optimization applications also exist independent of such large
(and expensive) ERP systems. In either case, many of these tools are capable of handling
large problems of varying degree of complexity. Table 10.3 lists a few inventory and SC
management software vendors that are publicly traded companies (in alphabetical order).
For a more extensive list, see Foster [59] and McCrea [60].
In concluding this section, we would like to reiterate that sophisticated tools are not
substitutes for sound analysis and good judgment. An understanding of the models being
used, their strengths, limitations, and applicability is paramount to successfully implement
a scientiﬁc inventory management program.
10.7
Conclusions
Although keeping a certain level of inventory might be necessary in many situations, one
should always consider making changes in the production or business systems so that the
optimum inventory level is held. Holding large amounts of inventory not only raises inventory
costs, it may also hide the ineﬃciencies of the system’s components. For instance, if an
unreliable supplier supplies a company with raw material, the company’s large inventory
level may hide the fact that the supplier is unreliable and may prevent managers from
looking for an alternative supplier. Conversely, if the inventory levels are too low, then the
system may become vulnerable to disruptions, leading to increased losses of money and
customer goodwill.
In this chapter, the objectives of maintaining inventories for raw material, work-in-
process, and ﬁnished products were explained. It was indicated that the purpose of studying
inventory systems is to design an optimum inventory policy for each particular case such
that the overall cost of maintaining it is minimized. The concept of ABC analysis was
presented as a simple method to classify inventories so as to ensure proper attention and
resources are given to critical and high value inventories. We discussed the various costs rel-
evant to holding inventories (inventory holding costs, ordering or set-up costs, and shortage
costs) and the inﬂuence they have on inventories.

10-36
Operations Research Methodologies
As mentioned at the outset, inventory management looks to answer the fundamental
questions of how often and how much to order. Several models of inventory analysis were
introduced to answer these questions under various settings. In each case, the relevant
variables were speciﬁed and the method to compute the optimal values was given. Models
for both deterministic and stochastic demand were presented. In reality, inventories are
held across the SC and inventories at one location usually depend on inventories at other
locations in the SC. We introduced multi-echelon inventory models that explicitly consider
multiple stocking locations. It must be pointed out that the models described in this chapter
are just a few representative cases of the existing models. Interested readers are referred to
references given in the bibliography for more extensive coverage of inventory control models.
10.8
Current and Future Research
The development and use of operations research techniques and mathematical models to
understand and control inventories continues. A very brief set of topics of current and future
research eﬀorts is given below with some references to relevant research literature.∗
• Inventory Models continue to receive signiﬁcant attention from researchers as
they are of practical value and represent signiﬁcant theoretical challenges. As
noted previously, optimal policies for multi-echelon inventory models, even in the
deterministic demand case are not known for all SC structures and identifying
such policies would be of signiﬁcant interest. In the case of stochastic multi-
echelon inventory management, there are even fewer generic results and the search
for better solutions continues.
• Interface Between Inventory Management Models and Other Business Functions:
In some sense, the models covered in this chapter (with the exception of the
newsvendor model, perhaps) represent “pure” inventory management models.
However, inventory management is not done in a vacuum and is signiﬁcantly
inﬂuenced by other business functions like marketing, pricing, competition, and
the like, and research in areas at the interface of inventory management and other
business operations has been very active in recent times. For example, we have
assumed that demand is not inﬂuenced by any action we take; in reality, promo-
tions and markdowns have signiﬁcant eﬀect on the demand patterns. Elmaghraby
and Keskinocak [61], Monahan et al. [32], Netessine [62], and Petruzzi and Dada
[33] cover pricing issues in some depth. Another emerging area of interest is to
actively incorporate the eﬀect of competitors in the analysis. Models in this cat-
egory usually use game theoretic methods in their analysis; see Cachon [63] and
Cachon and Netessine [64] for reviews. Cachon and Netessine [65] provide an
introduction to the methodologies of competitive analysis.
• Supply Chain Management and Inventories: Just as demand is inﬂuenced by
pricing, inventory decisions are often made in concert with pricing oﬀered by
suppliers. Furthermore, in an attempt to coordinate SC members, contracts are
often used, especially in the decentralized SC case. Such contracts often involve
the use of (transfer) pricing to help compensate members in an equitable fashion
and align SC members. For reviews, see Cachon [47] and Tsay et al. [48].
∗Recent issues of many of the journals mentioned in the bibliography are excellent sources to identify
current advances in inventory management.

Inventory Control
10-37
We also have assumed that the inventories are placed at existing facilities whose
locations are given. Facility locations, transportation modes, and the like can have
a signiﬁcant impact on inventory levels and ideally should be jointly optimized.
Sarmiento and Nagi [66] give a review of integrated SC models. For an overview
of the issues in transportation and inventory management, see Natarajan [67].
The placement of safety stocks in the SC is also of strategic importance. How-
ever, the problem has been shown to be N P-hard [68] and represents signiﬁcant
challenges; see Graves and Willems [69]. Billington et al. [70] present an appli-
cation of some of this research to the HP SC. Finally, the advent of global SCs
has seen inventories and facilities being spread worldwide. This has increased
the vulnerability of the SC to disruptions and has begun to receive considerable
interest, though only a limited number of models exist currently. Rangarajan and
Guide [71] discuss the unique challenges presented by disruptions and review the
relevant literature.
• Reverse Logistics and Closed-Loop SCs: We consider material ﬂow to be from
upstream stages to downstream stages in the SC. However, material may also
ﬂow in the reverse direction in the form of returns, defective parts, and the like.
This “reverse ﬂow” must be managed, sometimes due to laws to this eﬀect. They
can also represent a valuable income stream for a ﬁrm. The processes involved
in harvesting value from these products can be very diﬀerent from the manu-
facturing processes involved in forward ﬂow of material. Research into managing
inventories in the closed-loop SC is still in its infancy. The reader is referred to
Fleischmann et al. [72], Guide and Van Wassenhove [73], and Guide et al. [74].
10.8.1
Final Remarks
In conclusion, mathematical models for inventory management are abstractions of real sys-
tems. Nevertheless, they are often suﬃcient to optimally control an inventory system or,
at the very least, provide valuable insight into the behavior of the system and salient fac-
tors inﬂuencing it. As demonstrated by the examples presented earlier, these models, when
suitably applied, can lead to signiﬁcant improvements and increased proﬁtability. Despite
the long history and the many successes of inventory management methods, signiﬁcant
challenges (of both practical and theoretical interest) remain.
References
1. E.A. Silver, D.F. Pyke, and R. Peterson. Inventory Management and Production Planning
and Scheduling. John Wiley & Sons, New York, 3rd edition, 1998.
2. S.C. Graves, A.H.G. Rinnooy Kan, and P.H. Zipkin, editors. Logistics of Production and
Inventory, volume 4 of Handbooks in Operations Research and Management Science. North-
Holland, Amsterdam, 1993.
3. P.H. Zipkin. Foundations of Inventory Management. McGraw-Hill, New York, 2000.
4. G. Cachon and M. Olivares. Drivers of ﬁnished goods inventory performance in the U.S.
automobile industry. Working paper, The Wharton School, University of Pennsylvania,
October 2006.
5. S. Axs¨ater. Inventory Control. Springer, 2nd edition, 2006.
6. J. Bramel and D. Simchi-Levi. The Logic of Logistics: Theory, Algorithms and Applications
for Logistics Management. Springer-Verlag, New York, 1997.

10-38
Operations Research Methodologies
7. G. Hadley and T. Whitin. Analysis of Inventory Systems. Prentice-Hall, Englewood Cliﬀs,
NJ, 1963.
8. E.L. Porteus. Foundations of Stochastic Inventory Theory. Stanford University Press, Palo
Alto, CA, 2002.
9. C. Munson and M. Rosenblatt. Theories and realities of quantity discounts: An exploratory
study. Production and Operations Management, 7:352–359, 1998.
10. A. Federgruen. Centralized planning models for multi-echelon inventory systems under
uncertainty. In volume 4 of Handbooks in Operations Research and Management Science,
chapter 3. North-Holland, Amsterdam, 1993.
11. H.L. Lee and C. Billington. Material management in decentralized supply chains. Opera-
tions Research, 41(5):835–847, 1993.
12. A. Rangarajan. Inventory Models for Decentralized Supply Chains. Master’s thesis, The
Pennsylvania State University, University Park, December 2004.
13. C.C. Sherbrooke. METRIC: A multi-echelon technique for recoverable item control. Oper-
ations Research, 16:122–141, 1968.
14. C.C. Sherbrooke. Optimal Inventory Modeling of Systems: Multi-Echelon Techniques.
Wiley, New York, 1992.
15. J.M. Swaminathan and S.R. Tayur. Tactical planning models for supply chain management.
In A.G. de Kok and S.C. Graves, editors, Supply Chain Management: Design, Coordination
and Operation, volume 11 of Handbooks in Operations Research and Management Science,
chapter 8, pages 423–456. Elsevier, 2003.
16. G. Gallego, J.K. Ryan, and D. Simchi-Levi. Minimax analysis for ﬁnite-horizon inventory
models. IIE Transactions, 33(10):861–874, 2001.
17. G.A. Godfrey and W.B. Powell. An adaptive, distribution-free algorithm for the newsven-
dor problem with censored demands, with applications to inventory and distribution. Man-
agement Science, 47(8):1101–1112, 2001.
18. I. Moon and G. Gallego. Distribution free procedures for some inventory models. The
Journal of the Operational Research Society, 45(6):651–658, 1994.
19. U.S. Karmarkar. Manufacturing lead times, order release and capacity loading. In
S.C. Graves, A.H.G. Rinnooy Kan, and P.H. Zipkin, editors, Logistics of Production and
Inventory, volume 4 of Handbooks in Operations Research and Management Science, chap-
ter 6. North-Holland, Amsterdam, 1993.
20. S. Minner. Multiple-supplier inventory models in supply chain management: A review.
International Journal of Production Economics, 81–82:265–279, 2003.
21. F.W. Harris. How many parts to make at once. Factory, The Magazine of Management,
10(2):135–136, 1913.
22. H.M. Wagner and T.M. Whitin. Dynamic version of the economic lot size model. Manage-
ment Science, 5:89–96, 1958.
23. E.W. Dijkstra. A note on two problems in connection with graphs. Numerische Mathematik,
1:269–271, 1959.
24. A. Wagelmans, S. Van Hoesel, and A. Kolen. Economic lot sizing: An O(nlogn) algorithm
that runs in linear time in the Wagner-Whitin case. Operations Research, 40(S1):145–156,
1992.
25. H.L. Lee and S. Nahmias. Single-product, single-location models. In S.C. Graves,
A.H.G. Rinnooy Kan, and P.H. Zipkin, editors, Logistics of Production and Inventory,
volume 4 of Handbooks in Operations Research and Management Science, chapter 1, pages
3–55. North-Holland, Amsterdam, 1993.
26. W.I. Zangwill. A deterministic multi-period production scheduling model with backlogging.
Management Science, 13:105–119, 1966.

Inventory Control
10-39
27. S. Nahmias. Production and Operations Analysis. McGraw-Hill, Boston, MA, 4th edition,
2001.
28. A. Rangarajan and A.R. Ravindran. A base period inventory policy for decentralized supply
chains. Working paper No. 05–04, Center for Supply Chain Research, Pennsylvania State
University, 2005.
29. A. H-L. Lau, H-S. Lau, and D.F. Pyke. Degeneracy in inventory models. Naval Research
Logistics, 49(7):686–705, 2002.
30. M. Khouja. The single-period (news-vendor) problem: Literature review and suggestions
for future research. Omega, 27:537–553, 1999.
31. M.A. Lariviere and E.L. Porteus. Selling to the newsvendor: An analysis of price-only
contracts. Manufacturing & Services Operations Management, 3(4):293–305, 2001.
32. G.E. Monahan, N.C. Petruzzi, and W. Zhao. The dynamic pricing problem from a newsven-
dor’s perspective. Manufacturing & Services Operations Management, 6(1): 73–91, 2004.
33. N.C. Petruzzi and M. Dada. Pricing and the news vendor problem: A review with exten-
sions. Operations Research, 47(2):183–194, 1999.
34. G. Raz and E.L. Porteus. A fractiles perspective to the joint price/quantity newsvendor
model. Management Science, 52(11):1764–1777, 2006.
35. K.H. Shang and J-S. Song. Newsvendor bounds and heuristic for optimal policies in serial
supply chains. Management Science, 49(5):618–638, 2003.
36. J.A. Van Van Mieghem and N. Rudi. Newsvendor networks: Inventory management and
capacity investment with discretionary activities. Manufacturing & Services Operations
Management, 4(4):313–335, 2002.
37. A. Burnetas and S. Gilbert. Future capacity procurements under unknown demand and
increasing costs. Management Science, 47(7):979–992, 2001.
38. J.A. Van Mieghem. Capacity management, investment, and hedging: Review and
recent developments. Manufacturing & Service Operations Management, 5(4):269–302,
2003.
39. D.L. Iglehart. Optimality of (s, S) policies in the inﬁnite horizon dynamic inventory prob-
lem. Management Science, 9:259–267, 1963.
40. H.E. Scarf. The optimality of (s, S) policies in the dynamic inventory problem. In
J.A. Kenneth, K. Samuel, and S. Patrick, editors. Mathematical Methods in the Social
Sciences. Stanford University Press, Stanford, CA, 1960.
41. A.J. Clark. Multi-echelon inventory theory—a retrospective. International Journal of Pro-
duction Economics, 35:271–275, 1994.
42. H.E. Scarf. Inventory theory. Operations Research, 50(1):186–191, 2002.
43. S. Chopra and P. Meindl. Supply Chain Management: Strategy, Planning and Operation.
Prentice-Hall, Upper Saddle River, NJ, 2001.
44. S.R. Tayur, R. Ganeshan, and M.J. Magazine, editors. Quantitative Models for Supply
Chain Management. Kluwer Academic Publishers, Norwell, MA, 1999.
45. J-S. Song and D.D. Yao, editors. Supply Chain Structures: Coordination, Information and
Optimization. Kluwer Academic Publishers, Boston, MA, 2002.
46. J-S. Song and P. Zipkin. Supply chain operations: Assemble-to-order systems. In
A.G. de Kok and S.C. Graves, editors, Supply Chain Management: Design, Coordination
and Operation, volume 11 of Handbooks in Operations Research and Management Science,
chapter 6, pages 561–596. Elsevier, 2003.
47. G.P. Cachon. Supply chain coordination with contracts. In A. G. de Kok and S. C. Graves,
editors, Supply Chain Management: Design, Coordination and Operation, volume 11 of
Handbooks in Operations Research and Management Science, chapter 6, pages 229–339.
Elsevier, Boston, 2003.

10-40
Operations Research Methodologies
48. A. Tsay, S. Nahmias, and N. Agarwal. Modeling supply chain contracts: A review, chapter
10. In Quantitative Models for Supply Chain Management, pages 299–336. Kluwer Aca-
demic, Dordrecht, 1999.
49. R. Roundy. 98%-eﬀective integer-ratio lot-sizing for one-warehouse multi-retailer systems.
Management Science, 31(11):1416–1430, 1985.
50. L.B. Schwarz. A simple continuous review deterministic one-warehouse n-retailer inventory
problem. Management Science, 19:555–566, 1973.
51. A. Clark and H. Scarf. Optimal policies for a multi-echelon inventory problem. Management
Science, 6:475–490, 1960.
52. J.A. Muckstadt and R.O. Roundy. Analysis of Multistage Production Systems, In S.C.
Graves, A.H.G. Rinnooy Kan, and P.H. Zipkin, editors. Logisties of Production and Inven-
tory, volume 4 of Handbooks in Operations Research and Management Science, chapter 2,
pages 59–131. North-Holland, Amsterdam, 1993.
53. H.L. Lee and C. Billington. The evolution of supply-chain-management models and practice
at Hewlett-Packard. Interfaces, 25(5):42–63, 1995.
54. G. Lin, M. Ettl, S. Buckley, S. Bagchi, D.D. Yao, B.L. Naccarato, R. Allan, K. Kim, and
L. Koenig. Extended-enterprise supply-chain management at IBM Personal Systems Group
and other divisions. Interfaces, 30(1):7–25, 2000.
55. M.A. Koschat, G.L. Berk, J.A. Blatt, N.M. Kunz, M.H. LePore, and S. Blyakher.
Newsvendors tackle the newsvendor problem. Interfaces, 33(3):72–84, 2003.
56. R. Kapuscinski, R.Q. Zhang, P. Carbonneau, R. Moore, and B. Reeves. Inventory decisions
in Dell’s supply chain. Interfaces, 34(3):191–205, 2004.
57. A. Bangash, R. Bollapragada, R. Klein, N. Raman, H.B. Shulman, and D.R. Smith. Inven-
tory requirements planning at Lucent Technologies. Interfaces, 34(5):342–352, 2004.
58. A. Bixby, B. Downs, and M. Self. A scheduling and capable-to-promise application for
Swift & Company. Interfaces, 36(1):69–86, 2006.
59. T.A. Foster. The supply chain top 100 software vendors. Supply Chain Management
Review, pages S2–S12, September/October 2006. Available at: http://www.scmr.com/info/
59562.html.
60. B. McCrea. The state of the supply chain software industry. Supply Chain Management
Review, pages S1–S4, October 2006. Available at: http://www.scmr.com/info/59562.html.
61. W. Elmaghraby and P. Keskinocak. Dynamic pricing in the presence of inventory consider-
ations: Research overview, current practices, and future directions. Management Science,
49(10):1287–1309, 2003.
62. S. Netessine. Dynamic pricing of inventory/capacity with infrequent price changes. Euro-
pean Journal of Operational Research, 174(1):553–580, 2006.
63. G. Cachon. Competitive supply chain inventory management. In S. Tayur, R. Ganeshan,
and M. Magazine, editors, Quantitative Models for Supply Chain Management. Kluwer,
Boston, 1998.
64. G. Cachon and S. Netessine. Game theory in supply chain analysis. In D. Simchi-Levi,
S.D. Wu, and Z-J. Shen, editors, Handbook of Quantitative Supply Chain Analysis: Modeling
in the eBusiness Era, chapter 2. Kluwer, Boston, 2004.
65. G.P. Cachon and S. Netessine. Game theory in supply chain analysis. In Paul Gray, editor.
INFORMS TutORials in Operations Research, pages 200–233. INFORMS, 2006.
66. A.M. Sarmiento and R. Nagi. A review of integrated analysis of production–distribution
systems. IIE Transactions, 31(11):1061–1074, 1999.
67. A. Natarajan. Multi-Criteria Supply Chain Inventory Models with Transportation Costs.
PhD thesis, The Pennsylvania State University, University Park, PA, May 2007.
68. E. Lesnaia. Optimizing Safety Stock Placement in General Network Supply Chains. Ph.D.
thesis, Sloan School of Management, Massachusetts Institute of Technology, Cambridge,
MA, September 2004. URL http://web.mit.edu/sgraves/www/LesnaiaThesis.pdf.

Inventory Control
10-41
69. S.C. Graves and S.P. Willems. Supply chain design: Safety stock placement and supply
chain conﬁguration. In A.G. de Kok and S.C. Graves, editors, Supply Chain Management:
Design, Coordination and Operation, volume 11 of Handbooks in Operations Research and
Management Science, chapter 6, pages 95–132. Elsevier, Boston, 2003.
70. C. Billington, G. Callioni, B. Crane, J.D. Ruark, J.U. Rapp, T. White, and S.P. Willems.
Accelerating the proﬁtability of Hewlett-Packard’s supply chains. Interfaces, 34(1):59–72,
2004.
71. A. Rangarajan and V.D.R. Guide, Jr. Supply chain risk management: An overview. Work-
ing paper, January 2006.
72. M. Fleischmann, J.M. Bloemhof-Ruwaard, R. Dekker, E. van der Laan, J.A.E.E. van
Nunen, and L.N. Van Wassenhove. Quantitative models for reverse logistics: A review.
European Journal of Operational Research, 103:1–17, 1997.
73. V.D.R. Guide, Jr. and L.N. Van Wassenhove. Closed-loop supply chains: An introduction
to the feature issue. Production and Operations Management, 15(3 & 4), 2006.
74. V.D.R. Guide, Jr., T. Harrison, and L.N. Van Wassenhove. The challenge of closed-loop
supply chains. Interfaces, 33(6):3–6, 2003.


11
Complexity and
Large-Scale Networks
Hari P. Thadakamalla,
Soundar R. T. Kumara,
and R´eka Albert
Pennsylvania State University
11.1
Introduction.......................................... 11-1
11.2
Statistical Properties of Complex Networks ..... 11-6
Average Path Length and the Small-World Eﬀect •
Clustering Coeﬃcient • Degree Distribution •
Betweenness Centrality • Modularity and Community
Structures • Network Resilience
11.3
Modeling of Complex Networks ...................11-11
Random Graphs • Small-World Networks •
Scale-Free Networks
11.4
Why “Complex” Networks .........................11-16
11.5
Optimization in Complex Networks ..............11-18
Network Resilience to Node Failures • Local Search •
Other Topics
11.6
Conclusions ..........................................11-26
Acknowledgments ...........................................11-27
References ....................................................11-27
11.1
Introduction
In the past few decades, graph theory has been a powerful analytical tool for understanding
and solving various problems in operations research (OR). The study of graphs (or net-
works) traces back to the solution of the K¨onigsberg bridge problem by Euler in 1735. In
K¨onigsberg, the river Preger ﬂows through the town dividing it into four land areas as shown
in Figure 11.1a. These land areas are connected by seven diﬀerent bridges. The K¨onigsberg
bridge problem is to ﬁnd whether it is possible to traverse through the city on a route
that crosses each bridge exactly once, and return to the starting point. Euler formulated
the problem using a graph theoretical representation and proved that the traversal is not
possible. He represented each land area as a vertex (or node) and each bridge as an edge
between two nodes (land areas), as shown in Figure 11.1b. Then, he posed the question as
whether there exists a path such that it passes every edge exactly once and ends at the
start node. This path was later termed an Eulerian circuit. Euler proved that for a graph
to have an Eulerian circuit, all the nodes in the graph need to have an even degree.
Euler’s great insight lay in representing the K¨onigsberg bridge problem as a graph problem
with a set of vertices and edges. Later, in the twentieth century, graph theory developed
into a substantial area of study that is applied to solve various problems in engineering and
several other disciplines [1]. For example, consider the problem of ﬁnding the shortest route
11-1

11-2
Operations Research Methodologies
1
2
6
7
D
B
4
A
5
C
3
(a)
(b)
1
7
6
2
3
4
5
C
A
D
B
FIGURE 11.1
K¨onigsberg bridge problem: (a) shows the river ﬂowing through the town dividing it into
four land areas A, B, C, and D. The land areas are connected by seven bridges numbered from 1 to 7.
(b) Graph theoretical representation of the K¨onigsberg bridge problem. Each node represents a land area
and the edge between them represents the bridges connecting the land areas.
between two geographical points. The problem can be modeled as a shortest path problem
on a network, where diﬀerent geographical points are represented as nodes and they are
connected by an edge if there exists a direct path between the two nodes. The weights on
the edges represent the distance between the two nodes (see Figure 11.2). Let the network
be G(V , E) where V is the set of all nodes, E is the set of edges (i, j) connecting the nodes,
and w is a function such that wij is the weight of the edge (i, j). The shortest path problem
from node s to node t can be formulated as follows:
minimize

(i,j)∈ξ
wijxij
subject to

{j|(i,j)∈ξ}
xij −

{j|(j,i)∈ξ}
xij =
⎧
⎨
⎩
1
if i = s
−1
if i = t
0
otherwise
xij ≥0,
∀(i, j) ∈ξ
where xij = 1 or 0 depending on whether the edge from node i to node j belongs to the
optimal path or not, respectively. Many algorithms have been proposed to solve the shortest
path problem [1]. Using one such popular algorithm (Dijkstra’s algorithm [1]), we ﬁnd the
shortest path from node 10 to node 30 as (10 - 1 - 3 - 12 - 30) (see Figure 11.2). Note that
this problem and similarly other problems considered in traditional graph theory require
ﬁnding the exact optimal path.
In the last few years there has been an intense amount of activity in understanding and
characterizing large-scale networks, which led to the development of a new branch of science

Complexity and Large-Scale Networks
11-3
13
19
20
23
4
8
22
29
26
10
17
11
18
15
9
2
7
21
16
6
12
25
30
14
27
1
24
28
8
8
5
10
10
8
8
9
7
6
6
3
3
3
3
3
5
5
5
5
2
2
2
2
6
6
8
9
2
6
10
2
5
3
2
4
4
4
4
4
9
3
5
FIGURE 11.2
Illustration of a typical optimization problem in OR. The objective is to ﬁnd the shortest
path from node 10 to node 30. The values on the edges represent the distance between two nodes. Here, we
use the exact distances between diﬀerent nodes to calculate the shortest path 10 - 1 - 3 - 12 - 30.
called “Network Science” [2]. The scale of the size of these networks is substantially diﬀerent
from the networks considered in traditional graph theory. Further, these networks do not
have any pre-speciﬁed structure/order or any design principles. Also, the problems posed in
such networks are very diﬀerent from traditional graph theory. These large-scale networks
are referred to as complex networks, and we will discuss the reasons why they are termed
“complex” networks later in Section 11.4. The following are examples of complex networks:
• World Wide Web: It can be viewed as a network where Web pages are the nodes
and hyperlinks connecting one webpage to another are the directed edges. The
World Wide Web is currently the largest network for which topological informa-
tion is available. It had approximately one billion nodes at the end of 1999 [3]
and is continuously growing at an exponential rate. A recent study [4] estimated
the size to be 11.5 billion nodes as of January 2005.
• Internet: The Internet is a network of computers and telecommunication devices
connected by wired or wireless links. The topology of the Internet is studied at
two diﬀerent levels [5]. At the router level, each router is represented as a node and
physical connections between them as edges. At the domain level, each domain
(autonomous system, Internet provider system) is represented as a node and
inter-domain connections by edges. The number of nodes, approximately, at the
router level was 150,000 in 2000 [6] and at the domain level was 4000 in 1999 [5].
• Phone call network: The phone numbers are the nodes and every completed
phone call is an edge directed from the receiver to the caller. Abello et al. [7]
constructed a phone call network from the long distance telephone calls made
during a single day which had 53,767,087 nodes and over 170 million edges.
• Power grid network: Generators, transformers, and substations are the nodes
and high-voltage transmission lines are the edges. The power grid network of the
western United States had 4941 nodes in 1998 [8]. The North American power
grid consisted of 14,099 nodes and 19,657 edges [9] in 2005.

11-4
Operations Research Methodologies
• Airline network: Nodes are the airports and an edge between two airports repre-
sent the presence of a direct ﬂight connection [10,11]. Barthelemy et al. [10] have
analyzed the International Air Transportation Association database to form the
world-wide airport network. The resulting network consisted of 3880 nodes and
18,810 edges in 2002.
• Market graph: Recently, Boginski et al. [12,13] represented the stock market data
as a network where the stocks are nodes and two nodes are connected by an edge
if their correlation coeﬃcient calculated over a period of time exceeds a certain
threshold value. The network had 6556 nodes and 27,885 edges for the U.S. stock
data during the period 2000–2002 [13].
• Scientiﬁc collaboration networks: Scientists are represented as nodes and
two nodes are connected if the two scientists have written an article together.
Newman [14,15] studied networks constructed from four diﬀerent databases span-
ning biomedical research, high-energy physics, computer science, and physics.
One of these networks formed from the Medline database for the period from
1961 to 2001 had 1,520,251 nodes and 2,163,923 edges.
• Movie actor collaboration network: Another well-studied network is the movie
actor collaboration network, formed from the Internet Movie Database [16], which
contains all the movies and their casts since the 1890s. Here again, the actors
are represented as nodes and two nodes are connected by an edge if two actors
have performed together in a movie. This is a continuously growing network with
225,226 nodes and 13,738,786 edges in 1998 [8].
The above are only a few examples of complex networks pervasive in the real world
[17–20]. Tools and techniques developed in the ﬁeld of traditional graph theory involved
studies that looked at networks of tens or hundreds or in extreme cases thousands of nodes.
The substantial growth in the size of many such networks (see Figure 11.3) necessitates
a diﬀerent approach for analysis and design. The new methodology applied for analyzing
complex networks is similar to the statistical physics approach to complex phenomena.
The study of large-scale complex systems has always been an active research area in vari-
ous branches of science, especially in the physical sciences. Some examples are ferromagnetic
properties of materials, statistical description of gases, diﬀusion, formation of crystals, and
so on. For instance, let us consider a box containing one mole (6.022 × 1023) of gas atoms as
our system of analysis (see Figure 11.4a). If we represent the system with the microscopic
properties of the individual particles such as their position and velocity, then it would be
Change in scale
FIGURE 11.3
Pictorial description of the change in scale in the size of the networks found in many engi-
neering systems. This change in size and lack of any order necessitates a change in the analytical approach.

Complexity and Large-Scale Networks
11-5
Individual properties: Position
and velocity
Statistical properties: Pressure,
temperature
Individual properties: Degree,
neighbors, edge weights
Statistical properties: Degree,
distribution, small-worldness
PV  nRT
(a)
(b)
FIGURE 11.4
Illustration of the analogy between a box of gas atoms and complex networks. (a) A mole
of gas atoms (6.022 × 1023 atoms) in a box. (b) An example of a large-scale network. For analysis, we need
to represent both the systems using statistical properties.
next to impossible to analyze the system. Rather, physicists use statistical mechanics to
represent the system and calculate macroscopic properties such as temperature, pressure,
and the like. Similarly, in networks such as the Internet and WWW, where the number of
nodes is extremely large, we have to represent the network using macroscopic properties
(such as degree distribution, edge-weight distribution, etc.), rather than the properties of
individual entities in the network (such as the neighbors of a given node, the weights on the
edges connecting this node to its neighbors, etc.) (see Figure 11.4b). Now let us consider the
shortest path problem in such networks (for instance, WWW). We rarely require speciﬁc
shortest path solutions such as from node A to node B (from Web page A to Web page B).
Rather it is useful if we know the average distance (number of hops) taken from any node to
any other node (any Web page to any other Web page) to understand dynamical processes
(such as search in WWW). This new approach for understanding networked systems pro-
vides new techniques as well as challenges for solving conceptual and practical problems in
this ﬁeld. Furthermore, this approach has become feasible and has received a considerable
boost by the availability of computers and communication networks that have made the
gathering and analysis of large-scale datasets possible.
The objective of this chapter is to introduce this new direction of interdisciplinary research
(Network Science) and discuss the new challenges for the OR community. During the last
few years, there has been a tremendous amount of research activity dedicated to the study
of these large-scale networks. This activity was mainly triggered by signiﬁcant ﬁndings in
real-world networks that we will elaborate later in the chapter. There was a revival of
network modeling that gave rise to many path breaking results [17–20] and provoked vivid
interest across diﬀerent disciplines of the scientiﬁc community. Until now, a major part of
this research was contributed by physicists, mathematicians, sociologists, and biologists.
However, the ultimate goal of modeling these networks is to understand and optimize the
dynamical processes taking place in the network. In this chapter, we address the urgent need
and opportunity for the OR community to contribute to the fast-growing interdisciplinary
research on Network Science. The methodologies and techniques developed until now will
deﬁnitely aid the OR community in furthering this research.
The following is the outline of the chapter. In Section 11.2, we introduce diﬀerent sta-
tistical properties that are prominently used for characterizing complex networks. We also

11-6
Operations Research Methodologies
present the empirical results obtained for many real complex networks that initiated a revival
of network modeling. In Section 11.3, we summarize diﬀerent evolutionary models proposed
to explain the properties of real networks. In particular, we discuss Erd˝os–R´enyi random
graphs, small-world networks, and scale-free networks. In Section 11.4, we discuss brieﬂy
why these networks are called “complex” networks, rather than large-scale networks. We
summarize typical behaviors of complex systems and demonstrate how the real networks
have these behaviors. In Section 11.5, we discuss the optimization in complex networks
by concentrating on two speciﬁc processes, robustness and local search, which are most
relevant to engineering networks. We discuss the eﬀects of statistical properties on these
processes and demonstrate how they can be optimized. Further, we brieﬂy summarize a few
more important topics and give references for further reading. Finally, in Section 11.6, we
conclude and discuss future research directions.
11.2
Statistical Properties of Complex Networks
In this section, we explain some of the statistical properties that are prominently used in the
literature. These statistical properties help in classifying diﬀerent kinds of complex networks.
We discuss the deﬁnitions and present the empirical ﬁndings for many real networks.
11.2.1
Average Path Length and the Small-World Eﬀect
Let G(V , E) be a network where V is the collection of entities (or nodes) and E is the set
of arcs (or edges) connecting them. A path between two nodes u and v in the network G
is a sequence [u = u1, u2, . . ., un = v], where u′
is are the nodes in G and there exists an edge
from ui−1 to ui in G for all i. The path length is deﬁned as the sum of the weights on the
edges along the path. If all the edges are equivalent in the network, then the path length
is equal to the number of edges (or hops) along the path. The average path length (l) of a
connected network is the average of the shortest paths from each node to every other node
in a network. It is given by
l ≡⟨d(u, w)⟩=
1
N(N −1)

u∈V

u̸=w∈V
d(u, w)
where N is the number of nodes in the network and d(u, w) is the shortest path between u
and w. Table 11.1 shows the values of l for many diﬀerent networks. We observe that despite
the large size of the network (w.r.t. the number of nodes), the average path length is small.
This implies that any node can reach any other node in the network in a relatively small
number of steps. This characteristic phenomenon, that most pairs of nodes are connected
by a short path through the network, is called the small-world eﬀect.
The existence of the small-world eﬀect was ﬁrst demonstrated by the famous experi-
ment conducted by Stanley Milgram in the 1960s [24], which led to the popular concept of
TABLE 11.1
Average Path Length of Many Real Networks
Network
Size (number of nodes)
Average Path Length
WWW [21]
2 × 108
16
Internet, router level [6]
150,000
11
Internet, domain level [5]
4000
4
Movie actors [8]
212,250
4.54
Electronic circuits [22]
24,097
11.05
Peer-to-peer network [23]
880
4.28
Note that despite the large size of the network (w.r.t. the number of nodes), the average path
length is very small.

Complexity and Large-Scale Networks
11-7
six degrees of separation. In this experiment, Milgram randomly selected individuals from
Wichita, Kansas, and Omaha, Nebraska, to pass on a letter to one of their acquaintances by
mail. These letters had to ﬁnally reach a speciﬁc person in Boston, Massachusetts; the name
and profession of the target was given to the participants. The participants were asked to
send the letter to one of their acquaintances whom they judged to be closer (than them-
selves) to the target. Anyone who received the letter subsequently would be given the same
information and asked to do the same until it reached the target person. Over many trials,
the average length of these acquaintance chains for the letters that reached the targeted
node was found to be approximately 6. That is, there is an acquaintance path of an average
length 6 in the social network of people in the United States. Another interesting and even
more surprising observation from this experiment is discussed in Section 11.5.2. Currently,
Watts et al. [25] are involved in an Internet-based study to verify this phenomenon.
Mathematically, a network is considered to be small-world if the average path length
scales logarithmically or slower with the number of nodes N (∼logN). For example, say
the number of nodes in the network, N, increases from 103 to 106; then the average path
length will increase approximately from 3 to 6. This phenomenon has critical implications
on the dynamic processes taking place in the network. For example, if we consider the spread
of information, computer viruses, or contagious diseases across a network, the small-world
phenomenon implies that within a few steps it could spread to a large fraction of most of
the real networks.
11.2.2
Clustering Coeﬃcient
The clustering coeﬃcient characterizes the local transitivity and order in the neighborhood
of a node. It is measured in terms of the number of triangles (3-cliques) present in the net-
work. Consider a node i that is connected to ki other nodes. The number of possible edges
between these ki neighbors that form a triangle is ki(ki −1)/2. The clustering coeﬃcient of
a node i is the ratio of the number of edges Ei that actually exist between these ki nodes
and the total number ki(ki −1)/2 possible, that is,
Ci =
2Ei
ki(ki −1)
The clustering coeﬃcient of the whole network (C) is then the average of C′
is over all the
nodes in the network, that is, C = 1
n

i Ci (see Figure 11.5). The clustering coeﬃcient is
high for many real networks [17,20]. In other words, in many networks if node A is connected
to node B and node C, then there is a high probability that node B and node C are also
3
2
1
4
5
6
E1 = 3 and k1 = 5
C1 =
ΣCi
C =
C2 = 1, C3 = 2/3, C4 = 2/3, C5 = 1, C6 = 0
2×E1
k1(k1−1)
2×3
5(5 −1)
=
=
= 3/10
n
109
180
FIGURE 11.5
Calculating the clustering coeﬃcient of a node and the network. For example, node 1 has
degree 5 and the number of edges between the neighbors is 3. Hence, the clustering coeﬃcient for node 1
is 3/10. The clustering coeﬃcient of the entire network is the average of the clustering coeﬃcients at each
individual node (109/180).

11-8
Operations Research Methodologies
connected. With respect to social networks, it means that it is highly likely that two friends
of a person are also friends, a feature analyzed in detail in the so-called theory of balance [26].
11.2.3
Degree Distribution
The degree of a node is the number of edges incident on it. In a directed network, a node
has both an in-degree (number of incoming edges) and an out-degree (number of outgoing
edges). The degree distribution of the network is the function pk, where pk is the probability
that a randomly selected node has degree k. Here again, a directed graph has both in-degree
and out-degree distributions. It was found that most of the real networks including the
World Wide Web [27–29], the Internet [5], metabolic networks [30], phone call networks
[7,31], scientiﬁc collaboration networks [14,148], and movie actor collaboration networks
[33–35] follow a power-law degree distribution (p(k) ∼k−γ), indicating that the topology
of the network is very heterogeneous, with a high fraction of small-degree nodes and a few
large-degree nodes. These networks having power-law degree distributions are popularly
known as scale-free networks. These networks were called scale-free networks because of the
lack of a characteristic degree and the broad tail of the degree distribution. Figure 11.6
shows the empirical results for the Internet at the router level and co-authorship network
of high-energy physicists. The following are the expected values and variances of the node
degree in scale-free networks,
E[k] =

finite
if γ > 2
∞
otherwise
V [k] =

finite
if γ > 3
∞
otherwise
where γ is the power-law exponent. Note that the variance of the node degree is inﬁnite
when γ < 3 and the mean is inﬁnite when γ < 2. The power-law exponent (γ) of most of
the networks lies between 2.1 and 3.0, which implies that there is high heterogeneity with
respect to node degree. This phenomenon in real networks is critical because it was shown
that the heterogeneity has a huge impact on the network properties and processes such as
100
101
102
103
10
10
10
10
10
10
100
P(k)
10
0
10
1
10
2
10
3
10
4
10
10
10
10
10
10
10
100
k
k
(a)
(b)
FIGURE 11.6
The degree distribution of real networks. (a) Internet at the router level. Data courtesy
of Ramesh Govindan [6]. (b) Co-authorship network of high-energy physicists, after Newman [14].

Complexity and Large-Scale Networks
11-9
network resilience [9,36], network navigation, local search [32], and epidemiological processes
[37–41]. Later in this chapter, we will discuss the impact of this heterogeneity in detail.
11.2.4
Betweenness Centrality
The betweenness centrality (BC) of a node counts the fraction of shortest paths going
through the node. The BC of a node i is given by
BC(i) =

s̸=i̸=t
σst(i)
σst
where σst is the total number of shortest paths from node s to t and σst(i) is the number of
these shortest paths passing through node i. If the BC of a node is high, it implies that this
node is central and many shortest paths pass through this node. BC was ﬁrst introduced
in the context of social networks [42], and has been recently adopted by Goh et al. [43]
as a proxy for the load (li) at a node i with respect to transport dynamics in a network.
For example, consider the transportation of data packets in the Internet along the shortest
paths. If many shortest paths pass through a node then the load on that node would be high.
Goh et al. have shown numerically that the load (or BC) distribution follows a power-law,
PL(l) ∼l−δ with exponent δ ≈2.2, and is insensitive to the detail of the scale-free network
as long as the degree exponent (γ) lies between 2.1 and 3.0. They further showed that there
exists a scaling relation l ∼k(γ−1)/(δ−1) between the load and the degree of a node when
2 < γ ≤3. Later in this chapter, we discuss how this property can be utilized for local search
in complex networks. Many other centrality measures exist in literature and a detailed
review of these measures can be found in [44].
11.2.5
Modularity and Community Structures
Many real networks are found to exhibit a community structure (also called modular
structure). That is, groups of nodes in the network have high density of edges within the
group and lower density between the groups (see Figure 11.7). This property was ﬁrst pro-
posed in social networks [40], where people may divide into groups based on interests, age,
profession, and the like. Similar community structures are observed in many networks which
reﬂect the division of nodes into groups based on the node properties [20]. For example,
in the WWW it reﬂects the subject matter or themes of the pages, in citation networks
FIGURE 11.7
Illustration of a network with community structure. Communities are deﬁned as a group
of nodes in the network that have a higher density of edges within the group than between groups. In the
above network, the group of nodes enclosed within a dotted loop is a community.

11-10
Operations Research Methodologies
it reﬂects the area of research, in cellular and metabolic networks it may reﬂect functional
groups [45,46].
In many ways, community detection is similar to a traditional graph partitioning problem
(GPP). In GPP the objective is to divide the nodes of the network into k disjoint sets of
speciﬁed sizes such that the number of edges between these sets is the minimum. This
problem is NP-complete [47] and several heuristic methods [48–50] have been proposed to
decrease the computation time. GPP arises in many important engineering problems that
include mapping of parallel computations, laying out of circuits (VLSI design), and the
ordering of sparse matrix computations [48]. Here, the number of partitions to be made
is speciﬁed and the size of each partition is restricted. For example, in the mapping of
parallel computations, the tasks have to be divided between a speciﬁed number of processors
such that the communication between the processors is minimized and the loads on the
processors are balanced. However, in real networks, we do not have any a priori knowledge
about the number of communities into which we should divide or about the size of the
communities. The goal is to ﬁnd the naturally existing communities in the real networks
rather than dividing the network into a pre-speciﬁed number of groups. As we do not know
the exact partitions of network, it is diﬃcult to evaluate the goodness of a given partition.
Moreover, there is no unique deﬁnition of a community due to the ambiguity of how dense
a group should be to form a community. Many possible deﬁnitions exist in the literature
[42,51–54]. A simple deﬁnition given in [51,54] considers a subgraph as a community if each
node in the subgraph has more connections within the community than with the rest of
the graph. Newman and Girvan [52] have proposed another measure that calculates the
fraction of links within the community minus the expected value of the same quantity in
a randomized counterpart of the network. The higher this diﬀerence, the stronger is the
community structure. It is important to note that in spite of this ambiguity, the presence
of community structures is a common phenomenon across many real networks. Algorithms
for detecting these communities are brieﬂy discussed in Section 11.5.3.
11.2.6
Network Resilience
The ability of a network to withstand removal of nodes/edges is called network resilience or
robustness. In general, the removal of nodes and edges disrupts the paths between nodes and
can increase the distances thus making the communication between nodes harder. In more
severe cases, an initially connected network can break down into isolated components that
cannot communicate anymore. Figure 11.8 shows the eﬀect of the removal of nodes/edges on
a network. Observe that as we remove more nodes and edges, the network disintegrates into
many components. There are diﬀerent ways of removing nodes and edges to test the robust-
ness of a network. For example, one can remove nodes at random with uniform probability
or by selectively targeting certain classes of nodes, such as nodes with high degree. Usually,
the removal of nodes at random is termed as random failures and the removal of nodes with
higher degree is termed as targeted attacks; other removal strategies are discussed in detail
in Ref. [55]. Similarly, there are several ways of measuring the degradation of the network
performance after the removal. One simple way to measure it is to calculate the decrease in
size of the largest connected component in the network. A connected component is a part of
the network in which a path exists between any two nodes in that component and the largest
connected component is the largest among the connected components. The less the decrease
in the size of the largest connected component, the better the robustness of the network. In
Figure 11.8, the size of the largest connected component decreases from 13 to 9 and then
to 5. Another way to measure robustness is to calculate the increase of the average path
length in the largest connected component. Malfunctioning of nodes/edges eliminates some

Complexity and Large-Scale Networks
11-11
Edge
failure
Node
failure
FIGURE 11.8
The eﬀects of removing a node or an edge in the network. Observe that as we remove
more nodes and edges the network disintegrates into small components/clusters.
existing paths and generally increases the distance between the remaining nodes. Again, the
less the increase, the better the robustness of the network. We discuss more about network
resilience and robustness with respect to optimization in Section 11.5.1.
11.3
Modeling of Complex Networks
In this section, we give a brief summary of diﬀerent models for complex networks. Most of
the modeling eﬀorts focused on understanding the underlying process involved during the
network evolution and capture the above-mentioned properties of real networks. In speciﬁc,
we concentrate on three prominent models, namely, the Erd˝os–R´enyi random graph model,
the Watts–Strogatz small-world network model, and the Barab´asi–Albert scale-free network
model.
11.3.1
Random Graphs
One of the earliest theoretical models for complex networks was given by Erd˝os and R´enyi
[56–58] in the 1950s and 1960s. They proposed uniform random graphs for modeling complex
networks with no obvious pattern or structure. The following is the evolutionary model given
by Erd˝os and R´enyi:
• Start with a set of N isolated nodes.
• Connect each pair of nodes with a connection probability p.
Figure 11.9 illustrates two realizations for the Erd˝os–R´enyi random graph model (ER ran-
dom graphs) for diﬀerent connection probabilities. Erd˝os and R´enyi have shown that at
pc ≃1/N, the ER random graph abruptly changes its topology from a loose collection of
small clusters to one which has a giant connected component. Figure 11.10 shows the change
in size of the largest connected component in the network as the value of p increases, for
N = 1000. We observe that there exists a threshold pc = 0.001 such that when p < pc, the
network is composed of small isolated clusters and when p > pc a giant component suddenly
appears. This phenomenon is similar to the percolation transition, a topic well studied in
both mathematics and statistical mechanics [17].

11-12
Operations Research Methodologies
p  0.0
p  0.05
p  0.1
FIGURE 11.9
An Erd˝os–R´enyi random graph that starts with N = 20 isolated nodes and connects any
two nodes with a probability p. As the value of p increases the the number of edges in the network increase.
0
100
200
300
400
500
600
700
800
900
1000
0
0.002
0.004
0.006
0.008
0.01
Connection probability (p)
Size of the largest connected 
component 
FIGURE 11.10
Illustration of percolation transition for the size of the largest connected component in
the Erd˝os–R´enyi random graph model. Note that there exists pc = 0.001 such that when p < pc, the network
is composed of small isolated clusters and when p > pc a giant component suddenly appears.
In an ER random graph, the mean number of neighbors at a distance (number of hops)
d from a node is approximately <k>d, where <k> is the average degree of the network. To
cover all the nodes in the network, the distance (l) should be such that <k>l ∼N. Thus,
the average path length is given by l =
log N
log<k>, which scales logarithmically with the number
of nodes N. This is only an approximate argument for illustration; a rigorous proof can be
found in Ref. [59]. Hence, ER random graphs are small-world. The clustering coeﬃcient of
the ER random graphs is found to be low. If we consider a node and its neighbors in an
ER random graph, then the probability that two of these neighbors are connected is equal
to p (probability that two randomly chosen neighbors are connected). Hence, the clustering
coeﬃcient of an ER random graph is p = <k>
N , which is small for large sparse networks.
Now, let us calculate the degree distribution of the ER random graphs. The total number
of edges in the network is a random variable with an expected value of pN (N −1)/2, and
the number of edges incident on a node (the node degree) follows a binomial distribution
with parameters N −1 and p,
p(ki = k) = Ck
N−1pk(1 −p)N−1−k
This implies that in the limit of large N, the probability that a given node has degree k
approaches a Poisson distribution, p(k) = <k>ke−<k>
k!
. Hence, ER random graphs are

Complexity and Large-Scale Networks
11-13
Poisson
0
0.02
0.04
0.06
0.08
0.1
0.12
0
10
20
30
k
P(k)
P(k)
Power-law
1.0E-07
1.0E-06
1.0E-05
1.0E-04
1.0E-03
1.0E-02
1.0E-01
1.0E+00
1
10
100
1000
k
k
FIGURE 11.11
Comparison of networks with Poisson and power-law degree distribution of the same
size. Note that the network with Poisson distribution is homogenous in node degree. Most of the nodes
in the network have the same degree, which is close to the average degree of the network. However, the
network with power-law degree distribution is highly heterogenous in node degree. There are a few nodes
with large degree and many nodes with small degree.
statistically homogenous in node degree as the majority of the nodes have a degree close to
the average, and signiﬁcantly small and large node degrees are exponentially rare.
ER random graphs were used to model complex networks for a long time [59]. The model
was intuitive and analytically tractable; moreover, the average path length of real networks
is close to the average path length of an ER random graph of the same size [17]. However,
recent studies on the topologies of diverse large-scale networks found in nature indicated
that they have signiﬁcantly diﬀerent properties from ER random graphs [17–20]. It has
been found [8] that the average clustering coeﬃcient of real networks is signiﬁcantly larger
than the average clustering coeﬃcient of ER random graphs with the same number of
nodes and edges, indicating a far more ordered structure in real networks. Moreover, the
degree distribution of many large-scale networks is found to follow a power-law p(k) ∼k−γ.
Figure 11.11 compares two networks with Poisson and power-law degree distributions. We
observe that there is a remarkable diﬀerence between these networks. The network with
Poisson degree distribution is more homogenous in node degree, whereas the network with
power-law distribution is highly heterogenous. These discoveries along with others related
to the mixing patterns of complex networks [17–20] initiated a revival of network modeling
in the past few years.
Nonuniform random graphs are also studied [31,60–64] to mimic the properties of real-
world networks, in speciﬁc, power-law degree distribution. Typically, these models specify
either a degree sequence, which is a set of N values of the degrees ki of nodes i = 1, 2, . . ., N
or a degree distribution p(k). If a degree distribution is speciﬁed then the sequence is formed
by generating N random values from this distribution. This can be thought of as giving
each node i in the network ki “stubs” sticking out of it and then pairs of these stubs are

11-14
Operations Research Methodologies
connected randomly to form complete edges [64]. Molloy and Reed [62] have proved that
for a random graph with a degree distribution p(k), a giant connected component emerges
almost surely when 
k≥1 k(k −2)p(k) > 0, provided that the maximum degree is less than
N 1/4. Later, Aiello et al. [31,60] introduced a two-parameter random graph model P(α, γ)
for power-law graphs with exponent γ described as follows: Let nk be the number of nodes
with degree k, such that nk and k satisfy log nk = α −γ log k. The total number of nodes in
the network can be computed, noting that the maximum degree of a node in the network
is eα/γ. Using the results from Molloy and Reed [62], they showed that there is almost
surely a unique giant connected component if γ < γ0 = 3.47875 . . . , whereas, there is no
giant connected component almost surely when γ > γ0.
Newman et al. [64] have developed a general approach to random graphs by using a
generating function formalism [65]. The generating function for the degree distribution pk
is given by G0(x) = ∞
k=0 pkxk. This function captures all the information present in the
original distribution since pk = 1
k!
dkG0
dxk |x=0. The average degree of a randomly chosen node
would be <k> = 
kkp(k) = G′
0(1). Further, this formulation helps in calculating other
properties of the network [64]. For instance, we can approximately calculate the relation for
the average path length of the network. Let us consider the degree of the node reached by
following a randomly chosen edge. If the degree of this node is k, then we are k times more
likely to reach this node than a node of degree 1. Thus, the degree distribution of the node
arrived by a randomly chosen edge is given by kpk and not pk. In addition, the distribution of
the number of edges from this node (one less than the degree) qk, is (k + 1)pk+1

kkpk
= (k + 1)pk+1
<k>
.
Thus, the generating function for qk is given by G1(x) =
 ∞
k=0 (k + 1)pk+1xk
k
= G′
0(x)
G′
0(1). Note
that the distribution of the number of ﬁrst neighbors of a randomly chosen node (degree
of a node) is G0(x). Hence, the distribution of the number of second neighbors from the
same randomly chosen node would be G0(G1(x)) = 
k pk[G1(x)]k. Here, the probability
that any of the second neighbors is connected to the ﬁrst neighbors or to one another scales
as N −1 and can be neglected in the limit of a large N. This implies that the average number
of second neighbors is given by
 ∂
∂xG0(G1(x))
	
x=1 = G′
0(1)G′
1(1). Extending this method of
calculating the average number of nearest neighbors, we ﬁnd that the average number of
mth neighbors, zm, is [G′
1(1)]m−1G′
0(1) =

z2
z1
m−1
z1. Now, let us start from a node and
ﬁnd the number of ﬁrst neighbors, second, third . . . mth neighbors. Assuming that all the
nodes in the network can be reached within l steps, we have 1 + l
m=1 zm = N. As for most
graphs N ≫z1 and z2 ≫z1, we obtain the average path length of the network l = N/z1
z2/z1 + 1.
The generating function formalism can further be extended to include other features such
as directed graphs, bipartite graphs, and degree correlations [20].
Another class of random graphs that are especially popular in modeling social networks
is the exponential random graphs models (ERGMs) or p* models [66–70]. The ERGM
consists of a family of possible networks of N nodes in which each network G appears with
probability P(G) = 1
Z exp(−
i θiϵi), where the function Z is 
G exp(−
iθiϵi). This is
similar to the Boltzmann ensemble of statistical mechanics with Z as the partition function
[20]. Here, {ϵi} is the set of observables or measurable properties of the network such
as number of nodes with a certain degree, number of triangles, and so on. {θi} are an
adjustable set of parameters for the model. The ensemble average of a property ϵi is given
as ⟨ϵi⟩= 
G ϵi(G)P(G) = 1
Z ϵiexp(−
i θiϵi) =
∂f
∂θi . The major advantage of these models
is that they can represent any kind of structural tendencies such as dyad and triangle
formations. A detailed review of the parameter estimation techniques can be found in Refs.
[66,71]. Once the parameters {θi} are speciﬁed, the networks can be generated by using the
Gibbs or Metropolis–Hastings sampling methods [71].

Complexity and Large-Scale Networks
11-15
11.3.2
Small-World Networks
Watts and Strogatz [8] presented a small-world network model to explain the existence
of high clustering and small average path length simultaneously in many real networks,
especially social networks. They argued that most of the real networks are neither completely
regular nor completely random, but lie somewhere between these two extremes. The Watts–
Strogatz model starts with a regular lattice on N nodes and each edge is rewired with a
certain probability p. The following is the algorithm for the model:
• Start with a regular ring lattice on N nodes where each node is connected to its
ﬁrst k neighbors.
• Randomly rewire each edge with a probability p such that one end remains the
same and the other end is chosen uniformly at random. The other end is chosen
without allowing multiple edges (more than one edge joining a pair of nodes) and
loops (edges joining a node to itself).
The resulting network is a regular network when p = 0 and a random graph when p = 1,
since all the edges are rewired (see Figure 11.12). The above model is inspired from social
networks where people are friends with their immediate neighbors such as neighbors on
the street, colleagues at work, and so on (the connections in the regular lattice). Also,
each person has a few friends who are a long way away (long-range connections attained
by random rewiring). Later, Newman [72] proposed a similar model where instead of edge
rewiring, new edges are introduced with probability p. The clustering coeﬃcient of the
Watts–Strogatz model and the Newman model are
CWS = 3(k −1)
2(2k −1)(1 −p)3
CN =
3(k −1)
2(2k −1) + 4kp(p + 2)
respectively. This class of networks displays a high clustering coeﬃcient for small values of
p as we start with a regular lattice. However, even for small values of p, the average path
length falls rapidly due to the few long-range connections. This coexistence of high cluster-
ing coeﬃcient and small average path length is in excellent agreement with the character-
istics of many real networks [8,72]. The degree distribution of both models depends on the
parameter p, evolving from a univalued peak corresponding to the initial degree k to a
Random
Small-world
Regular
p = 0
p = 1
Increasing randomness
FIGURE 11.12
Illustration of the random rewiring process for the Watts–Strogatz model. This model
interpolates between a regular ring lattice and a random network, without changing the number of vertices
(N = 20) or edges (E = 40) in the graph. When p = 0 the graph is regular (each node has four edges); as p
increases, the graph becomes increasingly disordered until p = 1; all the edges are rewired randomly. After
Watts and Strogatz, 1998 [8].

11-16
Operations Research Methodologies
somewhat broader but still peaked distribution. Thus, small-world models are even more
homogeneous than random graphs, which is not the case with real networks.
11.3.3
Scale-Free Networks
As mentioned earlier, many real networks, including the World Wide Web [27–29], the
Internet [5], peer-to-peer networks [23], metabolic networks [30], phone call networks [7,31],
and movie actor collaboration networks [33–35], are scale-free, that is, their degree distri-
bution follows a power-law, p(k) ∼k−γ. Barab´asi and Albert [35] addressed the origin of
this power-law degree distribution in many real networks. They argued that a static ran-
dom graph or Watts–Strogatz model fails to capture two important features of large-scale
networks: their constant growth and the inherent selectivity in edge creation. Complex net-
works like the World Wide Web, collaboration networks, and even biological networks are
growing continuously by the creation of new Web pages, start of new researches, and by
gene duplication and evolution. Moreover, unlike random networks where each node has the
same probability of acquiring a new edge, new nodes entering the network do not connect
uniformly to existing nodes, but attach preferentially to nodes of a higher degree. This
reasoning led them to deﬁne the following mechanism:
• Growth: Start with a small number of connected nodes say m0 and assume that
every time a node enters the system, m edges are pointing from it, where m < m0.
• Preferential attachment: Every time a new node enters the system, each edge of
the newly entered node preferentially attaches to an already existing node i with
degree ki with the following probability:

i =
ki

j kj
It was shown that such a mechanism leads to a network with power-law degree distribution
p(k) = k−γ with exponent γ = 3. These networks were called scale-free networks because of
the lack of a characteristic degree and the broad tail of the degree distribution. The average
path length of this network scales as
log(N)
log(log(N)) and thus displays small-world property. The
clustering coeﬃcient of a scale-free network is approximately C ∼(log N)2
N
, which is a slower
decay than C = <k>N −1 decay observed in random graphs [73]. In the years following the
proposal of the ﬁrst scale-free model a large number of more reﬁned models were introduced,
leading to a well-developed theory of evolving networks [17–20].
11.4
Why “Complex” Networks
In this section, we will discuss why these large-scale networks are termed as “complex”
networks. The reason is not merely because of the large size of the network, though the
complexity does arise due to the size of the network. One must also distinguish “complex
systems” from “complicated systems” [74]. Consider an airplane as an example. Even though
it is a complicated system, we know its components and the rules governing its functioning.
However, this is not the case with complex systems. Complex systems are characterized by
diverse behaviors that emerge as a result of nonlinear spatio-temporal interactions among
a large number of components [75]. These emergent behaviors cannot be fully explained
by just understanding the properties of the individual components/constituents. Examples
of such complex systems include ecosystems, economies, various organizations/societies,

Complexity and Large-Scale Networks
11-17
FIGURE 11.13
Illustration of self-similarity in the Sierpinski triangle. When we look at a small part
of the triangle at a diﬀerent scale, then it looks similar to the original triangle. Moreover, at each scale we
look at the triangle, it is self-similar. This is a typical behavior of a scale invariant system.
the nervous system, the human brain, ant hills . . . the list goes on. Some of the behaviors
exhibited by complex systems are discussed below:
• Scale invariance or self-similarity: A system is scale invariant if the structure of
the system is similar regardless of the scale. Typical examples of scale invariant
systems are fractals. For example, consider the Sierpinski triangle in Figure 11.13.
Note that if we look at a small part of the triangle at a diﬀerent scale, then it
still looks similar to the original triangle. Similarly, at whichever scale we look
at the triangle, it is self-similar and hence scale invariant.
• Inﬁnite susceptibility/response: Most of the complex systems are highly sensitive
or susceptive to changes in certain conditions. A small change in the system
conditions or parameters may lead to a huge change in the global behavior.
This is similar to the percolation threshold where a small change in connection
probability induces the emergence of a giant connected cluster. Another good
example of such a system is a sand pile. When we are adding more sand particles
to a sand pile, they keep accumulating. But after reaching a certain point, an
addition of one more small particle may lead to an avalanche, demonstrating that
a sand pile is highly sensitive.
• Self-organization and emergence: Self-organization is the characteristic of a sys-
tem by which it can evolve itself into a particular structure based on interactions
between the constituents and without any external inﬂuence. Self-organization
typically leads to an emergent behavior. Emergent behavior is a phenomenon
in which the system global property is not evident from those of its individual
parts. A completely new property arises from the interactions between the dif-
ferent constituents of the system. For example, consider an ant colony. Although
a single ant (a constituent of an ant colony) can perform a very limited number
of tasks in its lifetime, a large number of ants interact in an ant colony, which
leads to more complex emergent behaviors.
Now let us consider the real large-scale networks such as the Internet, the WWW, and
other networks mentioned in Section 11.1. Most of these networks have power-law degree
distribution that does not have any speciﬁc scale [35]. This implies that the networks
do not have any characteristic degree and an average behavior of the system is not typical
(see Figure 11.11b). Due to these reasons, they are called scale-free networks. This heavy-
tailed degree distribution induces a high level of heterogeneity in the degrees of the vertices.
The heterogeneity makes the network highly sensitive to external disturbances. For exam-
ple, consider the network shown in Figure 11.14a. This network is highly sensitive when we
remove just two nodes in the network. It completely disintegrates into small components.
On the other hand, the network shown in Figure 11.14b, having the same number of nodes

11-18
Operations Research Methodologies
Node
failure
Node
failure
(a)
(b)
FIGURE 11.14
Illustration of high sensitivity phenomena in complex networks. (a) Observe that when
we remove the two highest degree nodes from the network, it disintegrates into small parts. The network
is highly sensitive to node removals. (b) Example of a network with the same number of nodes and edges
which is not sensitive. This network is not aﬀected much when we remove the three highest degree nodes.
The network in (a) is highly sensitive due to the presence of high heterogeneity in node degree.
and edges, is not very sensitive. Most real networks are found to have a structure similar
to the network shown in Figure 11.14a, with a huge heterogeneity in node degree. Also,
studies [37–41] have shown that the presence of heterogeneity has a huge impact on epi-
demiological processes such as disease spreading. They have shown that in networks that
do not have a heavy-tailed degree distribution, if the disease transmission rate is less than a
certain threshold, it will not cause an epidemic or a major outbreak. However, if the network
has power-law or scale-free distribution, it becomes highly sensitive to disease propagation.
They further show that no matter what the transmission rate is, there exists a ﬁnite prob-
ability that the infection will cause a major outbreak. Hence, we clearly see that these real
large-scale networks are highly sensitive or inﬁnitely susceptible. Further, all these networks
have evolved over time with new nodes joining the network (and some leaving) according to
some self-organizing or evolutionary rules. There is no external inﬂuence that controls the
evolution process or structure of the network. Nevertheless, these networks have evolved in
such a manner that they exhibit complex behaviors such as power-law degree distributions
and many others. Hence, they are called “complex” networks [76].
The above discussion on complexity is an intuitive explanation rather than one of tech-
nical details. More rigorous mathematical deﬁnitions of complexity can be found in
Refs. [77] and [78].
11.5
Optimization in Complex Networks
The models discussed in Section 11.3 are focused on explaining the evolution and growth
process of many large real networks. They mainly concentrate on the statistical properties
of real networks and network modeling. But the ultimate goal in studying and modeling the

Complexity and Large-Scale Networks
11-19
structure of complex networks is to understand and optimize the processes taking place on
these networks. For example, one would like to understand how the structure of the Inter-
net aﬀects its survivability against random failures or intentional attacks, how the structure
of the WWW helps in eﬃcient surﬁng or search on the Web, how the structure of social
networks aﬀects the spread of diseases, and so on. In other words, to design rules for opti-
mization, one has to understand the interactions between the structure of the network and
the processes taking place on the network. These principles will certainly help in redesign-
ing or restructuring the existing networks and perhaps even help in designing a network
from scratch. In the past few years, there has been a tremendous amount of eﬀort by the
research communities of diﬀerent disciplines to understand the processes taking place on
networks [17–20]. In this chapter, we concentrate on two processes, namely node failures
and local search, because of their high relevance to engineering systems, and discuss a few
other topics brieﬂy.
11.5.1
Network Resilience to Node Failures
All real networks are regularly subject to node/edge failures due to either normal malfunc-
tions (random failures) or intentional attacks (targeted attacks) [9,36]. Hence, it is extremely
important for the network to be robust against such failures for proper functioning. Albert
et al. [36] demonstrated that the topological structure of the network plays a major role
in its response to node/edge removal. They showed that most real networks are extremely
resilient to random failures. On the other hand, they are very sensitive to targeted attacks.
They attribute this to the fact that most of these networks are scale-free networks, which
are highly heterogenous in node degree. As a large fraction of nodes have small degree,
random failures do not have any eﬀect on the structure of the network. On the other hand,
the removal of a few highly connected nodes that maintain the connectivity of the network
drastically changes the topology of the network. For example, consider the Internet: despite
frequent router problems in the network, we rarely experience global eﬀects. However, if a
few critical nodes in the Internet are removed then it would lead to a devastating eﬀect.
Figure 11.15 shows the decrease in the size of the largest connected component for both
scale-free networks and ER graphs due to random failures and targeted attacks. ER graphs
are homogenous in node degree; that is, all the nodes in the network have approximately
the same degree. Hence, they behave almost similarly for both random failures and targeted
attacks (see Figure 11.15a). In contrast, for scale-free networks, the size of the largest con-
nected component decreases slowly for random failures and drastically for targeted attacks
(see Figure 11.15b).
Ideally, we would like to have a network that is as resilient as scale-free networks to
random failures and as resilient as random graphs to targeted attacks. To determine the
feasibility of modeling such a network, Valente et al. [79] and Paul et al. [80] have studied
the following optimization problem: “What is the optimal degree distribution of a network
of size N nodes that maximizes the robustness of the network to both random failures and
targeted attacks with the constraint that the number of edges remain the same?”
Note that we can always improve the robustness by increasing the number of edges in
the network (for instance, a completely connected network will be the most robust network
for both random failures and targeted attacks). Hence the problem has a constraint on
the number of edges. Valente et al. [79] showed that the optimal network conﬁguration
is very diﬀerent from both scale-free networks and random graphs. They showed that the
optimal networks that maximize robustness for both random failures and targeted attacks
have at most three distinct node degrees and hence the degree distribution is three-peaked.
Similar results were demonstrated by Paul et al. [80]. They showed that the optimal network

11-20
Operations Research Methodologies
0
35
70
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
p
p
Size of the largest connected component
0
35
70
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
(a)
(b)
Random graphs
Scale-free networks
FIGURE 11.15
The size of the largest connected component as the percentage number of nodes (p)
removed from the networks due to random failures (♦) and targeted attacks (△). (a) ER graph with number
of nodes (N) = 10,000 and mean degree <k> = 4; (b) scale-free networks generated by the Barab´asi–Albert
model with N = 10,000 and <k> = 4. The behavior with respective to random failures and targeted attacks
is similar for random graphs. Scale-free networks are highly sensitive to targeted attacks and robust to
random failures.
design is one in which all the nodes in the network except one have the same degree, k1
(which is close to the average degree), and one node has a very large degree, k2 ∼N 2/3,
where N is the number of nodes. However, these optimal networks may not be practically
feasible because of the requirement that each node has a limited repertoire of degrees.
Many diﬀerent evolutionary algorithms have also been proposed to design an optimal
network conﬁguration that is robust to both random failures and targeted attacks [81–85].
In particular, Thadakamalla et al. [84] consider two other measures, responsiveness and
ﬂexibility, along with robustness for random failures and targeted attacks, speciﬁcally for
supply-chain networks. They deﬁne responsiveness as the ability of a network to provide
timely services with eﬀective navigation and measure it in terms of the average path length
of the network. The lower the average path length, the better is the responsiveness of the net-
work. Flexibility is the ability of the network to have alternate paths for dynamic rerouting.
Good clustering properties ensure the presence of alternate paths, and the ﬂexibility of
a network is measured in terms of the clustering coeﬃcient. They designed a parameter-
ized evolutionary algorithm for supply-chain networks and analyzed the performance with
respect to these three measures. Through simulation they have shown that there exist
tradeoﬀs between these measures and proposed diﬀerent ways to improve these properties.
However, it is still unclear as to what would be the optimal conﬁguration of such survivable
networks. The research question would be “what is the optimal conﬁguration of a network
of N nodes that maximizes the robustness to random failures, targeted attacks, ﬂexibility,
and responsiveness, with the constraint that the number of edges remain the same?”

Complexity and Large-Scale Networks
11-21
Until now, we have focused on the eﬀects of node removal on the static properties of
a network. However, in many real networks, the removal of nodes will also have dynamic
eﬀects on the network as it leads to avalanches of breakdowns also called cascading failures.
For instance, in a power transmission grid, the removal of nodes (power stations) changes
the balance of ﬂows and leads to a global redistribution of loads over all the network. In
some cases, this may not be tolerated and might trigger a cascade of overload failures [86],
as happened on August 10, 1996, in eleven U.S. states and two Canadian provinces [87].
Models of cascades of irreversible [88] or reversible [89] overload failures have demonstrated
that the removal of even a small fraction of highly loaded nodes can trigger global cascades
if the load distribution of the nodes is heterogenous. Hence, cascade-based attacks can be
much more destructive than any other strategies considered in Refs. [36,55]. Later, Motter
[90] showed that a defense strategy based on a selective further removal of nodes and edges,
right after the initial attack or failure, can drastically reduce the size of the cascade. Other
studies on cascading failures include Refs. [91–95].
11.5.2
Local Search
One of the important research problems that has many applications in engineering systems
is searching in complex networks. Local search is the process in which a node tries to ﬁnd a
network path to a target node using only local information. By local information, we mean
that each node has information only about its ﬁrst or perhaps second neighbors and it is
not aware of nodes at a larger distance and how they are connected in the network. This
is an intriguing and relatively little studied problem that has many practical applications.
Let us suppose some required information such as computer ﬁles or sensor data is stored
at the nodes of a distributed network or database. Then, to quickly determine the location
of particular information, one should have eﬃcient local (decentralized) search strategies.
Note that this is diﬀerent from neighborhood search strategies used for solving combinatorial
optimization problems [96]. For example, consider the networks shown in Figure 11.16a and
11.16b. The objective is for node 1 to send a message to node 30 in the shortest possible
path. In the network shown in Figure 11.16a, each node has global connectivity information
11
16
18
15
10
17
14
2
9
9
1
4
8
7
5
27
22
24
23
28
18
15
11
26
21
25
20
19
24
8
23
28
13
12
30
16
6
10
14
17
1
3
2
27
7
5
4
29
22
30
12
29
19
13
26
21
6
25
20
3
(a)
(b)
FIGURE 11.16
Illustration of diﬀerent ways of sending message from node 1 to node 30. (a) In this
case, each node has global connectivity information about the whole network. Hence, node 1 calculates the
optimal path and send the message through this path. (b) In this case, each node has only information
about its neighbors (as shown by the dotted curve). Using this local information, node 1 tries to send the
message to node 30. The path obtained is longer than the optimal path.

11-22
Operations Research Methodologies
about the network (i.e., how each and every node is connected in the network). In such a
case, node 1 can calculate the optimal path using traditional algorithms [1] and send the
message through this path (1 - 3 - 12 - 30, depicted by the dotted line). Next, consider
the network shown in Figure 11.16b, in which each node knows only about its immediate
neighbors. Node 1, based on some search algorithm, chooses to send the message to one of
its neighbors: in this case, node 4. Similarly, node 4 also has only local information, and uses
the same search algorithm to send the message to node 13. This process continues until the
message reaches the target node. We can clearly see that the search path obtained (1 - 4 -
13 - 28 - 23 - 30) is not optimal. However, given that we have only local information available,
the problem tries to design optimal search algorithms in complex networks. The algorithms
discussed in this section may look similar to “distributed routing algorithms” that are
abundant in wireless ad hoc and sensor networks [97,98]. However, the main diﬀerence is
that the former try to exploit the statistical properties of the network topology, whereas the
latter do not. Most of the algorithms in wireless sensor networks literature ﬁnd a path to the
target node either by broadcasting or random walk and then concentrate on eﬃcient routing
of the data from the start node to the end node [97,99]. As we will see in this section, the
statistical properties of the networks have a signiﬁcant eﬀect on the search process. Hence,
the algorithms in wireless sensor networks could be integrated with these results for better
performance.
We discuss this problem for two types of networks. In the ﬁrst type of network, the global
position of the target node can be quantiﬁed and each node has this information. This
information will guide the search process in reaching the target node. For example, if we
look at the network considered in Milgram’s experiment each person has the geographical
and professional information about the target node. All the intermediary people (or nodes)
use this information as a guide for passing the messages, whereas, in the second type of
network, we cannot quantify the global position of the target node. In this case, during the
search process, we would not know whether a step in the search process is going toward
the target node or away from it. This makes the local search process even more diﬃcult.
One such kind of network is the peer-to-peer network, Gnutella [100], where the network
structure is such that one may know very little information about the location of the target
node. Here, when a user is searching for a ﬁle, the user does not know the global position
of the node that has the ﬁle. Further, when the user sends a request to one of its neighbors,
it is diﬃcult to ﬁnd out whether this step is toward the target node or away from it. For
lack of a more suitable name, we call the networks of the ﬁrst type spatial networks and the
networks of the second type nonspatial networks. In this chapter, we focus more on search
in non-spatial networks.
Search in Spatial Networks
The problem of local search goes back to the famous experiment by Stanley Milgram [24]
(discussed in Section 11.2) illustrating the short distances in social networks. Another impor-
tant observation of the experiment, which is even more surprising, is the ability of these
nodes to ﬁnd these short paths using just local information. As pointed out by Kleinberg
[101–103], this is not a trivial statement because most of the time people have only local
information in the network. That is the information about their immediate friends or per-
haps their friends’ friends. They do not have global information about the acquaintances of
all people in the network. Even in Milgram’s experiment, the people to whom he gave the
letters had only local information about the entire social network. Still, from the results of
the experiment, we can see that arbitrary pairs of strangers are able to ﬁnd short chains of
acquaintances between them by using only local information alone. Many models have been

Complexity and Large-Scale Networks
11-23
proposed to explain the existence of such short paths [8,17–20,72]. However, these mod-
els are not suﬃcient to explain the second phenomenon. The observations from Milgram’s
experiment suggest that there is something more embedded in the underlying social net-
work that guides the message implicitly from the source to the target. Such networks which
are inherently easy to search are called searchable networks. Mathematically, a network is
searchable if the length of the search path obtained scales logarithmically with the number
of nodes N (∼log N) or lesser. Kleinberg demonstrated that the emergence of such a phe-
nomenon requires special topological features [101–103]. Considering a family of network
models on an n-dimensional lattice that generalizes the Watts–Strogatz model, he showed
that only one particular model among this inﬁnite family can support eﬃcient decentralized
algorithms. Unfortunately, the model given by Kleinberg is highly constrained and repre-
sents a very small subset of complex networks. Watts et al. [104] presented another model
that is based upon plausible hierarchical social structures and contentions regarding social
networks. This model deﬁnes a class of searchable networks and oﬀers an explanation for
the searchability of social networks.
Search in Nonspatial Networks
The traditional search methods in nonspatial networks are broadcasting or random walk.
In broadcasting, each node sends the message to all its neighbors. The neighbors in turn
broadcast the message to all their neighbors, and the process continues. Eﬀectively, all
the nodes in the network would have received the message at least once or maybe even
more. This could have devastating eﬀects on the performance of the network. A hint of
the potential damages of broadcasting can be viewed by looking at the Taylorsville, NC,
elementary school project [105]. Sixth-grade students and their teacher sent out a sweet
email to all the people they knew. They requested the recipients to forward the email to
everyone they knew and notify the students by email so that they could plot their locations
on a map. A few weeks later, the project had to be canceled because they had received
about 450,000 responses from all over the world [105]. A good way to avoid such a huge
exchange of messages is by doing a walk. In a walk, each node sends the message to one
of its neighbors until it reaches the target node. The neighbor can be chosen in diﬀerent
ways depending on the algorithm. If the neighbor is chosen randomly with equal probability,
then it is called random search, while in a high-degree search the highest degree neighbor
is chosen. Adamic et al. [32] have demonstrated that a high-degree search is more eﬃcient
than random search in networks with a power-law degree distribution (scale-free networks).
A high-degree search sends the message to a more connected neighbor that has a higher
probability of reaching the target node and thus exploiting the presence of heterogeneity in
node degree to perform better. They showed that the number of steps (s) required for the
random search until the whole graph is revealed is s ∼N 3(1−2/γ) and for the high-degree
search it is s ∼N (2−4/γ). Clearly, for γ > 2.0, the number of steps taken by high-degree
search scales with a smaller exponent than the random walk search. As most real networks
have power-law degree distribution with exponent (γ) between 2.1 and 3.0, a high-degree
search would be more eﬀective in these networks.
All the algorithms discussed until now [32,101–104] have assumed that the edges in the
network are equivalent. But the assumption of equal edge weights (which may represent
the cost, bandwidth, distance, or power consumption associated with the process described
by the edge) usually does not hold in real networks. Many researchers [11,15,106–116],
have pointed out that it is incomplete to assume that all the edges are equivalent. Recently,
Thadakamalla et al. [117] have proposed a new search algorithm based on a network measure
called local betweenness centrality (LBC) that utilizes the heterogeneities in node degrees

11-24
Operations Research Methodologies
TABLE 11.2
Comparison of Diﬀerent Search Strategies in Power-Law Networks
with Exponent 2.1 and 2000 Nodes with Diﬀerent Edge-Weight Distributions
Beta
Uniform
Exp.
Power-Law
Search Strategy
σ2 = 2.3
σ2 = 8.3
σ2 = 25
σ2 = 4653.8
Random walk
1107.71
1097.72
1108.70
1011.21
(202%)
(241%)
(272%)
(344%)
Minimum edge weight
704.47
414.71
318.95
358.54
(92%)
(29%)
(7%)
(44%)
Highest degree
379.98
368.43
375.83
394.99
(4%)
(14%)
(26%)
(59%)
Minimum average node
1228.68
788.15
605.41
466.18
weight
(235%)
(145%)
(103%)
(88%)
Highest LBC
366.26
322.30
298.06
247.77
Note: The mean for all the edge-weight distributions is 5 and the variance is σ2. The
values in the table are the average distances obtained for each search strategy in these
networks. The values in the parentheses show the relative diﬀerence between the
average distance for each strategy with respect to the average distance obtained by the
LBC strategy. LBC search, which reﬂects both the heterogeneities in edge weights and
node degree, performed the best for all edge-weight distributions.
and edge weights. The LBC of a neighbor node i, L(i), is given by
L(i) =

s̸=n̸=t
s,t∈local network
σst(i)
σst
where σst is the total number of shortest paths (shortest path means the path over which
the sum of weights is minimal) from node s to t. σst(i) is the number of these shortest paths
passing through i. If the LBC of a node is high, it implies that this node is critical in the local
network. Thadakamalla et al. assume that each node in the network has information about
its ﬁrst and second neighbors and using this information the node calculates the LBC of each
neighbor and passes the message to the neighbor with the highest LBC. They demonstrated
that this search algorithm utilizes the heterogeneities in node degree and edge-weights to
perform well in power-law networks with exponent between 2.0 and 2.9 for a variety of edge-
weight distributions. Table 11.2 compares the performance of diﬀerent search algorithms for
scale-free networks with diﬀerent edge-weight distributions. The values in the parentheses
show the relative diﬀerence between the average distance for each algorithm with respect to
the average distance obtained by the LBC algorithm. In speciﬁc, they observed that as the
heterogeneity in the edge weights increase, the diﬀerence between the high-degree search
and LBC search increase. This implies that it is critical to consider the edge weights in the
local search algorithms. Moreover, given that many real networks are heterogeneous in edge
weights, it becomes important to consider an LBC-based search rather than a high-degree
search, as shown by Adamic et al. [32].
11.5.3
Other Topics
There are various other applications to real networks that include the issues related to the
structure of the networks and their dynamics. In this section, we brieﬂy summarize these
applications and give some references for further study.
Detecting Community Structures
As mentioned earlier, community structures are typically found in many real networks.
Finding these communities is extremely helpful in understanding the structure and function

Complexity and Large-Scale Networks
11-25
of the network. Sometimes the statistical properties of the community alone may be very
diﬀerent from the whole network and hence these may be critical in understanding the
dynamics in the community. The following are some of the examples:
• The World Wide Web: Identiﬁcation of communities in the Web is helpful for
implementation of search engines, content ﬁltering, automatic classiﬁcation, auto-
matic realization of ontologies, and focused crawlers [51,118].
• Social networks: Community structures are a typical feature of a social network.
The behavior of an individual is highly inﬂuenced by the community that the
individual belongs to. Communities often have their own norms, subcultures that
are an important source of a person’s identity [42,52].
• Biological networks: Community structures are found in cellular [45,119], metabolic
[46], and genetic networks [120]. Identifying them helps in ﬁnding the functional
modules that correspond to speciﬁc biological functions.
Algorithmically, the community detection problem is the same as the cluster analysis
problem studied extensively by the OR community, computer scientists, statisticians, and
mathematicians [121]. One of the major classes of algorithms for clustering is hierarchical
algorithms that fall into two broad types, agglomerative and divisive. In an agglomerative
method, an empty network (n nodes with no edges) is considered and edges are added
based on some similarity measure between nodes (e.g., similarity based on the number of
common neighbors) starting with the edge between the pairs with the highest similarity.
This procedure can be stopped at any step and the distinct components of the network are
taken to be the communities. On the other hand, in divisive methods the edges are removed
from the network based on a certain measure (e.g., the edge with the highest betweenness
centrality [52]). As this process continues the network disintegrates into diﬀerent communi-
ties. Recently, many such algorithms have been proposed and applied to complex networks
[18,122]. A comprehensive list of algorithms to identify community structures in complex
networks can be found in Ref. [122], where Danon et al. have compared them in terms of
sensitivity and computational cost.
Another interesting problem in community detection is to ﬁnd a clique of maximum car-
dinality in the network. A clique is a complete subgraph in the network. In the network
G(V, E), let G(S) denote the subgraph induced by a subset S ⊆V . A network G(V, E) is
complete if each node in the network is connected to every other node, that is, ∀i, j ∈V ,
{i, j} ∈E. A clique C is a subset of V such that the induced graph G(C) is complete. The
maximum clique problem has many practical applications such as project selection, coding
theory, computer vision, economics, and integration of genome mapping data [123–125]. For
instance, Boginski et al. [13] solve this problem for ﬁnding maximal independent set in the
market graph which can form a base for forming a diversiﬁed portfolio. The maximum clique
problem is known to be NP-hard [47], and details on the various algorithms and heuristics
can be found in Refs. [125,126]. Further, if the network size is large, then the data may
not ﬁt completely inside the computer’s internal memory. Then we need to use external
memory algorithms and data structures [127] for solving the optimization problems in such
networks. These algorithms use slower external memory (such as disks) and the resulting
communication between internal memory and external memory can be a major perfor-
mance bottleneck. Using external memory algorithms, Abello et al. [7] proposed decompo-
sition schemes that make large sparse graphs suitable for processing by graph optimization
algorithms.

11-26
Operations Research Methodologies
Spreading Processes
Diﬀusion of an infectious disease, computer virus, or information on a network consti-
tute examples of spreading processes. In particular, the spread of infectious diseases in a
population is called epidemic spreading. The study of epidemiological modeling has been
an active research area for a long time and is heavily used in planning and implementing
various prevention and control programs [128]. Recently, there has been a burst of activities
on understanding the eﬀects of the network properties on the rate and dynamics of disease
propagation [17–20]. Most of the earlier methods used the homogenous mixing hypothesis
[129], which implies that the individuals who are in contact with susceptible individuals
are uniformly distributed throughout the entire population. However, recent ﬁndings (Sec-
tion 11.2) such as heterogeneities in node degree, presence of high clustering coeﬃcients,
and community structures indicate that this assumption is far from reality. Later, many
models have been proposed [17–20,38,41,130] which consider these properties of the net-
work. In particular, many researchers have shown that incorporating these properties in
the model radically changes the results previously established for random graphs. Other
spreading processes that are of interest include spread of computer viruses [131–133], data
dissemination on the Internet [134,135], and strategies for marketing campaigns [136].
Congestion
The transport of packets or materials ranging from packet transfer in the Internet to the
mass transfer in chemical reactions in the cell is one of the fundamental processes occurring
on many real networks. Due to limitations in resources (bandwidth), the increase in the
number of packets (packet generation rate) may lead to overload at the node and unusually
long delivering times, in other words, congestion in networks. Considering a basic model,
Ohira and Sawatari [137] have shown that there exists a phase transition from a free ﬂow
to a congested phase as a function of the packet generation rate. This critical rate is com-
monly called “congestion threshold” and the higher the threshold, the better is the network
performance with respect to congestion.
Many studies have shown that an important role is played by the topology and routing
algorithms in the congestion of networks [138–146]. Toroczkai et al. [146] have shown that
on large networks on which ﬂows are inﬂuenced by gradients of a scalar distributed on
the nodes, scale-free topologies are less prone to congestion than random graphs. Routing
algorithms also inﬂuence congestion at nodes. For example, in scale-free networks, if the
packets are routed through the shortest paths then most of the packets pass through the hubs
and hence cause higher loads on the hubs [43]. Singh and Gupte [144] discuss strategies to
manipulate hub capacity and hub connections to relieve congestion in the network. Similarly
many congestion-aware routing algorithms [138,140,141,145] have been proposed to improve
performance. Sreenivasan et al. [145] introduced a novel static routing protocol that is
superior to the shortest path routing under intense packet-generation rates. They propose
a mechanism in which packets are routed through hub avoidance paths unless the hubs are
required to establish the route. Sometimes when global information is not available, routing
is done using local search algorithms. Congestion due to such local search algorithms and
optimal network conﬁgurations is studied in Ref. [147].
11.6
Conclusions
Complex networks are pervasive in today’s world and are continuously evolving. The sheer
size and complexity of these networks pose unique challenges in their design and analysis.
Such complex networks are so pervasive that there is an immediate need to develop new

Complexity and Large-Scale Networks
11-27
analytical approaches. In this chapter, we presented signiﬁcant ﬁndings and developments
in recent years that led to a new ﬁeld of interdisciplinary research, Network Science. We
discussed how network approaches and optimization problems are diﬀerent in network sci-
ence from traditional OR algorithms and addressed the need and opportunity for the OR
community to contribute to this fast-growing research ﬁeld. The fundamental diﬀerence is
that large-scale networks are characterized based on macroscopic properties such as degree
distribution and clustering coeﬃcient rather than the individual properties of the nodes and
edges. Importantly, these macroscopic or statistical properties have a huge inﬂuence on the
dynamic processes taking place on the network. Therefore, to optimize a process on a given
conﬁguration, it is important to understand the interactions between the macroscopic prop-
erties and the process. This will further help in the design of optimal network conﬁgurations
for various processes. Due to the growing scale of many engineered systems, a macroscopic
network approach is necessary for the design and analysis of such systems. Moreover, the
macroscopic properties and structure of networks across diﬀerent disciplines are found to be
similar. Hence the results of this research can easily be migrated to applications as diverse
as social networks to telecommunication networks.
Acknowledgments
The authors acknowledge the National Science Foundation (NSF, Grant # DMI 0537992)
and a Sloan Research Fellowship to one of the authors (R.A.) for making this work fea-
sible. In addition, the authors thank the anonymous reviewer for helpful comments and
suggestions. Any opinions, ﬁndings and conclusions, or recommendations expressed in this
material are those of the author(s) and do not necessarily reﬂect the views of the NSF.
References
1. R.K. Ahuja, T.L. Magnanti, and J.B. Orlin. Network Flows: Theory, Algorithms, and Appli-
cations. Prentice-Hall, Englewood Cliﬀs, NJ, 1993.
2. Committee on Network Science for Future Army Applications. Network Science. National
Academies Press, Washington, D.C., 2005.
3. S. Lawrence and C.L. Giles. Accessibility of information on the web. Nature, 400:107–109,
1999.
4. A. Gulli and A. Signorini. The indexable web is more than 11.5 billion pages. In WWW
’05: Special Interest Tracks and Posters of the 14th International Conference on World
Wide Web, pp. 902–903. ACM Press, New York, 2005.
5. M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law relationships of the internet
topology. Comput. Commun. Rev., 29:251–262, 1999.
6. R. Govindan and H. Tangmunarunkit. Heuristics for internet map discovery. IEEE
INFOCOM, 3:1371–1380, 2000.
7. J. Abello, P.M. Pardalos, and M.G.C. Resende. On maximum clique problems in very
large graphs. In: External Memory Algorithms: DIMACS Series in Discrete Mathematics
and Theoretical Computer Science, vol. 50, pp. 119–130. American Mathematical Society,
Boston, MA, 1999.
8. D.J. Watts and S.H. Strogatz. Collective dynamics of “small-world” networks. Nature,
393:440–442, 1998.
9. R. Albert, I. Albert, and G.L. Nakarado. Structural vulnerability of the North American
power grid. Phys. Rev. E, 69(2):025103, 2004.

11-28
Operations Research Methodologies
10. M. Barthelemy, A. Barrat, R. Pastor-Satorras, and A. Vespignani. Characterization and
modeling of weighted networks. Physica A, 346:34–43, 2005.
11. R. Guimera, S. Mossa, A. Turtschi, and L.A.N. Amaral. The worldwide air transportation
network: Anomalous centrality, community structure, and cities’ global roles. Proc. Nat.
Acad. Sci., 102:7794–7799, 2005.
12. V. Boginski, S. Butenko, and P. Pardalos. Statistical analysis of ﬁnancial networks.
Comput. Stat. Data Anal., 48:431–443, 2005.
13. V. Boginski, S. Butenko, and P. Pardalos. Mining market data: A network approach.
Comput. Oper. Res., 33:3171–3184, 2006.
14. M.E.J. Newman. Scientiﬁc collaboration networks: I. Network construction and funda-
mental results. Phys. Rev. E, 64(1):016131, 2001.
15. M.E.J. Newman. Scientiﬁc collaboration networks: II. Shortest paths, weighted networks,
and centrality. Phys. Rev. E, 64(1):016132, 2001.
16. The Internet Movie Database can be found on the WWW at http://www.imdb.com/.
17. R. Albert and A.L. Barab´asi. Statistical mechanics of complex networks. Rev. Mod. Phys.,
74(1):47–97, 2002.
18. S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D.U. Hwang. Complex networks:
Structure and dynamics. Phys. Reports, 424:175–308, 2006.
19. S.N. Dorogovtsev and J.F.F. Mendes. Evolution of networks. Adv. Phys., 51: 1079–1187,
2002.
20. M.E.J. Newman. The structure and function of complex networks. SIAM Review, 45:
167–256, 2003.
21. A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins,
and J. Wiener. Graph structure in the web. Comput. Netw., 33:309–320, 2000.
22. R. Ferrer i Cancho, C. Janssen, and R.V. Sol´e. Topology of technology graphs: Small
world patterns in electronic circuits. Phys. Rev. E, 64(4):046119, 2001.
23. M. Ripeanu, I. Foster, and A. Iamnitchi. Mapping the Gnutella network: Properties
of large-scale peer-to-peer systems and implications for system design. IEEE Internet
Comput. J., 6:50–57, 2002.
24. S. Milgram. The small world problem. Psychol. Today, 2:60–67, 1967.
25. D.J. Watts, P.S. Dodds, and R. Muhamad. http://smallworld.columbia.edu/index.html,
date accessed: March 22, 2006.
26. N. Contractor, S. Wasserman, and K. Faust. Testing multi-theoretical multilevel hypothe-
ses about organizational networks: An analytic framework and empirical example. Acad.
Mgmt. Rev., 31(3):681–703, 2006.
27. L.A. Adamic and B.A. Huberman. Growth dynamics of the world-wide web. Nature,
401(6749):131, 1999.
28. R. Albert, H. Jeong, and A.L. Barab´asi. Diameter of the world wide web. Nature,
401(6749):130–131, 1999.
29. R. Kumar, P. Raghavan, S. Rajalopagan, D. Sivakumar, A. Tomkins, and E. Upfal.
The web as a graph. Proceedings of the Nineteenth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems, pp. 1–10, 2000.
30. H. Jeong, B. Tombor, R. Albert, Z.N. Oltvai, and A.-L. Barab´asi. The large-scale
organization of metabolic networks. Nature, 407:651–654, 2000.
31. W. Aiello, F. Chung, and L. Lu. A random graph model for massive graphs. Proceedings of
the Thirty-Second Annual ACM Symposium on Theory of Computing, pp. 171–180, 2000.
32. L.A. Adamic, R.M. Lukose, A.R. Puniyani, and B.A. Huberman. Search in power-law
networks. Phys. Rev. E, 64(4):046135, 2001.
33. R. Albert and A.L. Barab´asi. Topology of evolving networks: Local events and universality.
Phys. Rev. Lett., 85(24):5234–5237, 2000.

Complexity and Large-Scale Networks
11-29
34. L.A.N. Amaral, A. Scala, M. Barthelemy, and H.E. Stanley. Classes of small-world
networks. Proc. Natl. Acad. Sci., 97(21):11149–11152, 2000.
35. A. L Barab´asi and R. Albert. Emergence of scaling in random networks. Science, 286
(5439):509–512, 1999.
36. R. Albert, H. Jeong, and A.L. Barab´asi. Attack and error tolerance of complex networks.
Nature, 406(6794):378–382, 2000.
37. R. Pastor-Satorras and A. Vespignani. Epidemic dynamics and endemic states in complex
networks. Phys. Rev. E, 63(6):066117, 2001.
38. R. Pastor-Satorras and A. Vespignani. Epidemic spreading in scale-free networks. Phys.
Rev. Lett., 86:3200–3203, 2001.
39. R. Pastor-Satorras and A. Vespignani. Epidemic dynamics in ﬁnite size scale-free networks.
Phys. Rev. E, 65(3):035108, 2002.
40. R. Pastor-Satorras and A. Vespignani. Immunization of complex networks. Phys. Rev. E,
65(3):036104, 2002.
41. R. Pastor-Satorras and A. Vespignani. Epidemics and immunization in scale-free networks.
In: Handbook of Graphs and Networks, Wiley-VCH, Berlin, 2003.
42. S. Wasserman and K. Faust. Social Network Analysis. Cambridge University Press,
New York, 1994.
43. K.I. Goh, B. Kahng, and D. Kim. Universal behavior of load distribution in scale-free
networks. Phys. Rev. Lett., 87(27), 278701, 2001.
44. D.
Kosch¨utzki,
K.A.
Lehmann,
L.
Peeters,
S.
Richter,
D.
Tenfelde-Podehl,
and
O. Zlotowski. Centrality indices. In: Network Analysis, pp. 16–61. Springer-Verlag,
Berlin, 2005.
45. P. Holme, M. Huss, and H. Jeong. Subnetwork hierarchies of biochemical pathways.
Bioinformatics, 19:532–538, 2003.
46. E. Ravasz, A.L. Somera, D.A. Mongru, Z.N. Oltvai, and A.L. Barab´asi. Hierarchical
organization of modularity in metabolic networks. Science, 297:1551–1555, 2002.
47. M.R. Garey and D.S. Johnson. Computers and Intractability, A Guide to the Theory of
NP-Completeness. W.H. Freeman, New York, 1979.
48. B. Hendrickson and R.W. Leland. A multilevel algorithm for partitioning graphs. In
Supercomputing ’95: Proceedings of the 1995 ACM/IEEE Conference on Supercomputing,
p. 28. ACM Press, New York, 1995.
49. B.W. Kernighan and S. Lin. An eﬃcient heuristic procedure for partitioning graphs. Bell
System Tech. J., 49:291–307, 1970.
50. A. Pothen, H. Simon, and K. Liou. Partitioning sparse matrices with eigenvectors of
graphs. SIAM J. Matrix Anal., 11(3):430–452, 1990.
51. G. Flake, S. Lawrence, and C. Lee Giles. Eﬃcient identiﬁcation of web communities. In:
Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 150–160, 2000.
52. M.E.J. Newman and M. Girvan. Finding and evaluating community structure in networks.
Phys. Rev. E, 69(2):026113, 2004.
53. G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. Uncovering the overlapping community
structure of complex networks in nature and society. Nature, 435:814–818, 2005.
54. F. Radicchi, C. Castellano, F. Cecconi, V. Loreto, and D. Parisi. Deﬁning and identifying
communities in networks. Proc. Natl. Acad. Sci., 101:2658–2663, 2004.
55. P. Holme and B.J. Kim. Attack vulnerability of complex networks. Phys. Rev. E, 65(5):
056109, 2002.
56. P. Erd˝os and A. R´enyi. On random graphs. Publicationes Mathematicae, 6:290–297, 1959.
57. P. Erd˝os and A. R´enyi. On the evolution of random graphs. Magyar Tud. Mat. Kutato
Int. Kozl., 5:17–61, 1960.

11-30
Operations Research Methodologies
58. P. Erd˝os and A. R´enyi. On the strength of connectedness of a random graph. Acta Math.
Acad. Sci. Hungar., 12:261–267, 1961.
59. B. Bollobas. Random Graphs. Academic, London, 1985.
60. W. Aiello, F. Chung, and L. Lu. A random graph model for power law graphs. Exp. Math.,
10(1):53–66, 2001.
61. F. Chung and L. Lu. Connected components in random graphs with given degree
sequences. Ann. Combinatorics, 6:125–145, 2002.
62. M. Molloy and B. Reed. A critical point for random graphs with a given degree sequence.
Random Structures Algorithms, 6:161–179, 1995.
63. M.E.J. Newman. Random graphs as models of networks. In:Handbook of Graphs and
Networks, pp. 35–68. Wiley-VCH, Berlin, 2003.
64. M.E.J. Newman, S.H. Strogatz, and D.J. Watts. Random graphs with arbitrary degree
distributions and their applications. Phys. Rev. E, 64(2):026118, 2001.
65. H.S. Wilf. Generating Functionology. Academic, Boston, 1990.
66. C. Anderson, S. Wasserman, and B. Crouch. A p* primer: Logit models for social networks.
Social Networks, 21(1):37–66, 1999.
67. O. Frank and D. Strauss. Markov graphs. J. Am. Stat. Assoc., 81:832–842, 1986.
68. P.W. Holland and S. Leinhardt. An exponential family of probability distributions for
directed graphs. J. Am. Stat. Assoc., 76:33–65, 1981.
69. D. Strauss. On a general class of models for interaction. SIAM Rev., 28:513–527, 1986.
70. S. Wasserman and P. Pattison. Logit models and logistic regressions for social
networks 1: An introduction to Markov random graphs and p*. Psychometrika,
61:401–426, 1996.
71. T.A.B. Snijders. Markov chain Monte Carlo estimation of exponential random graph
models. J. Soc. Struct., 3(2):1–40, 2002.
72. M.E.J. Newman. Models of small world. J. Stat. Phys., 101:819–841, 2000.
73. B. Bollobas and O. Riordan. Mathematical results on scale-free graphs. In:Handbook of
Graphs and Networks. Wiley-VCH, Berlin, 2003.
74. A. Vespignani. Complex networks: Ubiquity, importance, and implications. In: Frontiers of
Engineering: Reports on Leading-Edge Engineering from the 2005 Symposium, pp. 75–81.
National Academies Press, Washington, DC, 2006.
75. V. Honavar. Complex Adaptive Systems Group at Iowa State University, http://www.cs.
iastate.edu/∼honavar/cas.html, date accessed: March 22, 2006.
76. A. Vespignani. Epidemic modeling: Dealing with complexity. http://www.indiana.edu/
talks-fall04/, 2004. Date accessed: July 6, 2006.
77. R. Badii and A. Politi. Complexity: Hierarchical Structures and Scaling in Physics.
Cambridge University Press, Cambridge, UK, 1997.
78. C.H. Bennett. How to deﬁne complexity in physics, and why. In: From Complexity to Life,
pp. 34–43. Oxford University Press, New York, 2003.
79. A.X.C.N. Valente, A. Sarkar, and H.A. Stone. Two-peak and three-peak optimal complex
networks. Phys. Rev. Lett., 92(11):118702, 2004.
80. G. Paul, T. Tanizawa, S. Havlin, and H.E. Stanley. Optimization of robustness of complex
networks. Eur. Phys. J. B, 38:187–191, 2004.
81. L.F. Costa. Reinforcing the resilience of complex networks. Phys. Rev. E, 69(6):066127,
2004.
82. R. Ferrer i Cancho and R.V. Sol´e. Optimization in complex networks. In:Statistical
Mechanics of Complex Networks, pp. 114–126. Springer-Verlag, Berlin, 2003.
83. B. Shargel, H. Sayama, I.R. Epstein, and Y. Bar-Yam. Optimization of robustness and
connectivity in complex networks. Phys. Rev. Lett., 90(6):068701, 2003.

Complexity and Large-Scale Networks
11-31
84. H.P. Thadakamalla, U.N. Raghavan, S.R.T. Kumara, and R. Albert. Survivability
of multi-agent based supply networks: A topological perspective. IEEE Intell. Syst.,
19:24–31, 2004.
85. V. Venkatasubramanian, S. Katare, P.R. Patkar, and F. Mu. Spontaneous emergence
of complex optimal networks through evolutionary adaptation. Comput. & Chem. Eng.,
28(9):1789–1798, 2004.
86. R. Kinney, P. Crucitti, R. Albert, and V. Latora. Modeling cascading failures in the North
American power grid. Euro. Phys. J. B, 46:101–107, 2005.
87. M.L. Sachtjen, B.A. Carreras, and V.E. Lynch. Disturbances in a power transmission
system. Phys. Rev. E, 61(5):4877–4882, 2000.
88. A.E. Motter and Y. Lai. Cascade-based attacks on complex networks. Phys. Rev. E,
66(6):065102, 2002.
89. P. Crucitti, V. Latora, and M. Marchiori. Model for cascading failures in complex networks.
Phys. Rev. E, 69(4):045104, 2004.
90. A.E. Motter. Cascade control and defense in complex networks. Phys. Rev. Lett.,
93(9):098701, 2004.
91. B.A. Carreras, V.E. Lynch, I. Dobson, and D.E. Newman. Critical points and transitions in
an electric power transmission model for cascading failure blackouts. Chaos, 12(4):985–994,
2002.
92. Y. Moreno, J.B. Gomez, and A.F. Pacheco. Instability of scale-free networks under
node-breaking avalanches. Europhys. Lett., 58(4):630–636, 2002.
93. Y. Moreno, R. Pastor-Satorras, A. Vazquez, and A. Vespignani. Critical load and
congestion instabilities in scale-free networks. Europhys. Lett., 62(2):292–298, 2003.
94. X.F. Wang and J. Xu. Cascading failures in coupled map lattices. Phys. Rev. E, 70
(5):056113, 2004.
95. D.J. Watts. A simple model of global cascades on random networks. Proc. Natl. Acad.
Sci., 99(9):5766–5771, 2002.
96. E.
Aarts
and
J.K.
Lenstra
(eds.).
Local
Search
in
Combinatorial
Optimization.
John Wiley & Sons, Chichester, UK, 1997.
97. I.F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci. Wireless sensor networks:
A survey. Comput. Netw., 38(4):393–422, 2002.
98. J.N. Al-Karaki and A.E. Kamal. Routing techniques in wireless sensor networks: A survey.
IEEE Wireless Communications, 11(6):6–28, 2004.
99. C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed diﬀusion: A scalable and robust
communication paradigm for sensor networks. Proceedings of ACM MobiCom ’00, Boston,
MA, pp. 174–185, 2000.
100. G. Kan. Gnutella. In: Peer-to-Peer Harnessing the Power of Disruptive Technologies,
edited by A. Oram, O’Reilly, Beijing, 2001.
101. J. Kleinberg. Navigation in a small world. Nature, 406:845, 2000.
102. J. Kleinberg. The small-world phenomenon: An algorithmic perspective. Proc. 32nd ACM
Symposium on Theory of Computing, ACM Press, New York, pp. 163–170, 2000.
103. J. Kleinberg. Small-world phenomena and the dynamics of information. Adv. Neural
Inform. Process. Syst., 14:431–438, 2001.
104. D.J. Watts, P.S. Dodds, and M.E.J. Newman. Identity and search in social networks.
Science, 296:1302–1305, 2002.
105. D.J. Watts. Six Degrees: The Science of a Connected Age. W.W. Norton & Company,
New York, 2003.
106. E. Almaas, B. Kovacs, T. Viscek, Z.N. Oltval, and A.L. Barab´asi. Global organization of
metabolic ﬂuxes in the bacterium Escherichia coli. Nature, 427(6977):839–843, 2004.

11-32
Operations Research Methodologies
107. A. Barrat, M. Barthelemy, R. Pastor-Satorras, and A. Vespignani. The architecture of
complex weighted networks. Proc. Natl. Acad. Sci., 101(11):3747, 2004.
108. A. Barrat, M. Barthelemy, and A. Vespignani. Modeling the evolution of weighted
networks. Phys. Rev. E, 70(6):066149, 2004.
109. L.A. Braunstein, S. V. Buldyrev, R. Cohen, S. Havlin, and H.E. Stanley. Optimal paths
in disordered complex networks. Phys. Rev. Lett., 91(16):168701, 2003.
110. K.I. Goh, J.D. Noh, B. Kahng, and D. Kim. Load distribution in weighted complex
networks. Phys. Rev. E, 72(1):017102, 2005.
111. M. Granovetter. The strength of weak ties. Am. J. Sociol., 78(6):1360–1380, 1973.
112. A.E. Krause, K.A. Frank, D.M. Mason, R.E. Ulanowicz, and W.W. Taylor. Compartments
revealed in food-web structure. Nature, 426:282–285, 2003.
113. J.D. Noh and H. Rieger. Stability of shortest paths in complex networks with random
edge weights. Phys. Rev. E, 66(6):066127, 2002.
114. R. Pastor-Satorras and A. Vespignani. Evolution and Structure of the Internet: A
Statistical Physics Approach. Cambridge University Press, Cambridge, UK, 2004.
115. S.L. Pimm. Food Webs. University of Chicago Press, Chicago, 2nd edition, 2002.
116. S.H. Yook, H. Jeong, A.L. Barab´asi, and Y. Tu. Weighted evolving networks. Phys. Rev.
Lett., 2001:5835–5838, 86.
117. H.P. Thadakamalla, R. Albert, and S.R.T. Kumara. Search in weighted complex networks.
Phys. Rev. E, 72(6):066128, 2005.
118. R.B. Almeida and V.A.F. Almeida. A community-aware search engine. In: Proceedings of
the 13th International Conference on World Wide Web, ACM Press, New York, 2004.
119. A.W. Rives and T. Galitskidagger. Modular organization of cellular networks. Proc. Natl.
Acad. Sci., 100(3):1128–1133, 2003.
120. D.M. Wilkinson and B.A. Huberman. A method for ﬁnding communities of related genes.
Proc. Natl. Acad. Sci., 101:5241–5248, 2004.
121. P. Hansen and B. Jaumard. Cluster analysis and mathematical programming. Math.
Program., 79:191–215, 1997.
122. L. Danon, A. Diaz-Guilera, J. Duch, and A. Arenas. Comparing community structure
identiﬁcation. J. Stat. Mech., p. P09008, 2005.
123. S. Butenko and W.E. Wilhelm. Clique-detection models in computational biochemistry
and genomics. Eur. J. Oper. Res., 173:1–17, 2006.
124. J. Hasselberg, P.M. Pardalos, and G. Vairaktarakis. Test case generators and computational
results for the maximum clique problem. J. Global Optim., 3: 463–482, 1993.
125. P.M. Pardalos and J. Xue. The maximum clique problem. J. Global Optim., 4:301–
328, 1994.
126. D.J. Johnson and M.A. Trick (eds.). Cliques, Coloring, and Satisﬁability: Second DIMACS
Implementation Challenge, Workshop, October 11–13, 1993. American Mathematical
Society, Boston, 1996.
127. J. Abello and J. Vitter (eds.). External Memory Algorithms: DIMACS Series in Discrete
Mathematics and Theoretical Computer Science, vol. 50. American Mathematical Society,
Boston, MA, 1999.
128. O. Diekmann and J. Heesterbeek. Mathematical Epidemiology of Infectious Diseases:
Model Building, Analysis and Interpretation. Wiley, New York, 2000.
129. R.M. Anderson and R.M. May. Infectious Diseases in Humans. Oxford University Press,
Oxford, UK, 1992.
130. V. Colizza, A. Barrat, M. Barthlemy, and A. Vespignani. The role of the airline trans-
portation network in the prediction and predictability of global epidemics. PNAS,
103(7):2015–2020, 2006.

Complexity and Large-Scale Networks
11-33
131. J. Balthrop, S. Forrest, M.E.J. Newman, and M.M. Williamson. Technological networks
and the spread of computer viruses. Science, 304(5670):527–529, 2004.
132. A.L. Lloyd and R.M. May. How viruses spread among computers and people. Science,
292:1316–1317, 2001.
133. M.E.J. Newman, S. Forrest, and J. Balthrop. Email networks and the spread of computer
viruses. Phys. Rev. E, 66(3):035101, 2002.
134. A.-M. Kermarrec, L. Massoulie, and A. J. Ganesh. Probabilistic reliable dissemination in
large-scale systems. IEEE Trans. on Parallel and Distributed Sys., 14(3):248–258, 2003.
135. W. Vogels, R. van Renesse, and K. Birman. The power of epidemics: Robust com-
munication for large-scale distributed systems. SIGCOMM Comput. Commun. Rev.,
33(1):131–135, 2003.
136. J. Leskovec, L.A. Adamic, and B.A. Huberman. The dynamics of viral marketing, ACM
Trans. on the Web, 1(1):5, 2007.
137. T. Ohira and R. Sawatari. Phase transition in a computer network traﬃc model. Phys.
Rev. E, 58(1):193–195, 1998.
138. Z.Y. Chen and X.F. Wang. Eﬀects of network structure and routing strategy on network
capacity. Phys. Rev. E, 73(3):036107, 2006.
139. M. Argollo de Menezes and A.-L. Barab´asi. Fluctuations in network dynamics. Phys. Rev.
Lett., 92(2):028701, 2004.
140. P. Echenique, J. Gomez-Gardenes, and Y. Moreno. Improved routing strategies for internet
traﬃc delivery. Phys. Rev. E, 70(5):056105, 2004.
141. P. Echenique, J. Gomez-Gardenes, and Y. Moreno. Dynamics of jamming transitions in
complex networks. Europhys. Lett., 71(2):325–331, 2005.
142. R. Guimera, A. Arenas, A. D´ıaz-Guilera, and F. Giralt. Dynamical properties of model
communication networks. Phys. Rev. E, 66(2):026704, 2002.
143. R. Guimera, A. D´ıaz-Guilera, F. Vega-Redondo, A. Cabrales, and A. Arenas. Optimal
network topologies for local search with congestion. Phys. Rev. Lett., 89(24):248701, 2002.
144. B.K. Singh and N. Gupte. Congestion and decongestion in a communication network.
Phys. Rev. E, 71(5):055103, 2005.
145. S. Sreenivasan, R. Cohen, E. Lopez, Z. Toroczkai, and H.E. Stanley. Communication
bottlenecks in scale-free networks, Phys. Rev. E, 75(3):036105, 2007.
146. Z. Toroczkai and K.E. Bassler. Network dynamics: Jamming is limited in scale-free
systems. Nature, 428:716, 2004.
147. A. Arenas, A. Cabrales, A. Diaz-Guilera, R. Guimera, and F. Vega. Search and congestion
in complex networks. In: Statistical Mechanics of Complex Networks, pp. 175–194.
Springer-Verlag, Berlin, 2003.
148. A.L. Barab´asi, H. Jeong, Z. Neda, E. Ravasz, A. Schubert, and T. Vicsek. Evolution of
the social network of scientiﬁc collaborations. Physica A, 311:590–614, 2002.


12
Simulation
Catherine M. Harmonosky
Pennsylvania State University
12.1
Introduction........................................... 12-1
When Should Simulation Be Used? • Examples
of Successful Applications
12.2
Basics of Simulation ................................. 12-3
Basic Deﬁnitions • Need for Randomness • Output
Analysis for Discrete-Event Simulation
12.3
Simulation Languages and Software ...............12-15
A Sampling of Discrete-Event Simulation Software •
Summary
12.4
Simulation Projects—The Bigger Picture ........12-20
12.5
Summary ..............................................12-22
References ....................................................12-23
12.1
Introduction
The word “simulation” can invoke many diﬀerent images. Wind tunnels provide a physical
simulation environment for testing the aerodynamic properties of wings and cars. Water tun-
nels also provide a physical simulation environment for testing the hydraulic and acoustical
properties of submerged vehicles. Pilots train in ﬂight simulators. There are Monte Carlo
simulation methods that allow for statistical analysis of many trials of mathematical equa-
tions. Computer models allow simulation of electrical circuits during their design to ensure
completeness. In the foundry industry, there is mold solidiﬁcation simulation software, which
allows analysis of how the molten metal will solidify for a particular mold design.
This chapter focuses on discrete-event simulation. Discrete-event simulation is a computer-
based methodology that allows you to model an existing or proposed system, capturing key
characteristics and parameters of that system, such that the model emulates the behavior
and performance of the system as events take place over time. Examples of systems that
can be simulated include factories, warehouses, hospitals, amusement parks, military oper-
ations, airports, convenience stores, or supply chains. Analyzing the simulation output can
allow the user to:
• Better understand the behavior and performance of existing systems, including
determining the cause of diﬀerent system responses.
• Predict the eﬀect upon system performance due to a proposed change in the
system.
• Predict if a system design will have a performance that meets a company’s
speciﬁed goals.
12-1

12-2
Operations Research Methodologies
Simulation allows the analyst to draw inferences about new systems without building
them or about changes to existing systems without disturbing them. Consequently, it can
provide a very cost-eﬀective means of analyzing multiple “what-if” scenarios before any
investment is made in capital, resources, or materials. Some examples of the types of sys-
tem changes, either in existing systems or in a system design, that simulation can evaluate
include:
• Introducing a lean manufacturing philosophy into an existing system.
• Modifying a supply chain by deleting or adding suppliers.
• Adding, replacing, or deleting resources (equipment, workers).
• Changing process/customer ﬂow patterns.
• Changing scheduling or control rules.
• Modifying material handling methods.
12.1.1
When Should Simulation Be Used?
Of course, there are many other operations research modeling techniques that are described
in other chapters of this handbook. Techniques such as linear and integer programming are
analytical optimal methods—for the given input values, the mathematically optimal answer
will be returned. It is important to understand that discrete-event simulation is a heuristic—
although it seeks the optimal answer, it can not guarantee that it will deliver the optimal
answer. It uses numerical methods to track the changing state of the system over time.
Therefore, we say that simulation models are “run” rather than solved, generating a dynamic
series of system changing events while collecting observations of system performance that
are analyzed to estimate the true performance of the actual system being modeled [1].
Further, due to the stochastic input, each simulation run, or replication, of the same model
will have a diﬀerent speciﬁc series of events yielding slightly diﬀerent output measures.
So, why should you use simulation, when it does not guarantee the optimal? In many com-
plex manufacturing and service systems, for example, factories, hospitals, and amusement
parks, it is very diﬃcult (some may say impossible) to exactly capture the relationships
and interactions of the system parameters in strict mathematical equations, as is required
of mathematical modeling techniques. Additionally, complex systems often have signiﬁcant
sources of variability, for example, variable processing or service times, variable walking
times, probabilities of rework or re-entry into the line, nonstationary arrival rates, break-
downs or worker breaks, and the like. Variability is particularly evident in service systems
where customer behavior introduces another source of variability in addition to resource
variability. Simulation can directly incorporate most types of system variability into the
model, which is often diﬃcult to incorporate in optimal techniques. Also, unlike queueing
models, which have certain underlying assumptions regarding arrival time and processing
time distributions, in simulation you can specify the distributions that most directly match
your data. Therefore, in complex, stochastic environments we often do not have a directly
applicable optimal technique that will return an answer in a reasonable amount of time. We
must rely on heuristics, such as simulation.
12.1.2
Examples of Successful Applications
There are many reported examples of successful application of discrete-event simulation in a
variety of environments. Evaluating diﬀerent material control policies in a warehouse allowed
a company to avoid purchasing new equipment that they previously thought was necessary.
Simulation is routinely used in the automotive industry to investigate bottlenecks, evaluate

Simulation
12-3
diﬀerent possible layouts and equipment, or to model the design plans for a completely
new facility. Customer service call centers have been simulated, leading to better and faster
call routing policies to improve response time. The introduction of lean manufacturing
concepts can be investigated with simulation to determine which concepts work best for
a particular company before investing in any physical changes. In the banking industry,
simulation is used to model the processing of transactions, identify bottlenecks, and develop
new processing strategies that allow transactions to be processed faster, thus saving the bank
money or increasing revenues. The beneﬁt to the companies may be a one-time beneﬁt of
hundreds of thousands of dollars for a larger project, or some companies use simulation
extensively throughout the year, yielding $50,000–$100,000 beneﬁt per project.
Many more examples of successful applications can be found in papers in the Proceedings
of the Winter Simulation Conferences (WSC), which can be accessed via the WSC Web
site, www.wintersim.org. Some examples of tracks at the WSC 2006 conference that have
associated papers focused on speciﬁc application areas include manufacturing, aerospace
and military applications, logistics, transportation and distribution, risk analysis, homeland
security/emergency response, and biotechnology/health care. Additionally, four case study
tracks dedicated to presentations of simulation applications in a multitude of industries were
part of the conference. Although only abstracts are available for the case study presentations,
they provide another perspective of the types of simulation applications.
The Society for Modeling and Simulation International sponsors a Summer Computer
Simulation Conference, which also publishes a proceedings, as part of its annual Summer
Simulation Multiconference. Topical areas are similar to those mentioned above. More infor-
mation is available at the Society’s Web site, www.scs.org.
12.2
Basics of Simulation
Although there are many discrete-event simulation languages and a list of some languages
will follow later in this chapter, there are fundamentals about simulation and the simulation
modeling process that transcend software platforms. Fundamentally, a discrete-event simu-
lation model is stochastic (there is some type of randomness in the system), dynamic (the
system state changes over time), and system changes occur at discrete points in time [2].
12.2.1
Basic Deﬁnitions
Figure 12.1 is a graphical representation of an overview of the main ideas of a simula-
tion model. First, the model boundary must be established. For example, will your model
include the entire factory or only one department; will your model include the transporta-
tion method for patients arriving to the hospital or just start with the registration desk?
Setting the boundary deﬁnes the scope of the model. Within that boundary, entities move
through the system interacting with each other and triggering events. Entities may be parts,
people, or things and an event is an instantaneous occurrence at a discrete point in time
(hence the term “discrete-event”), typically associated with a change in the state of the
system. A discrete-event simulation tracks, over time, the state of the system, which is like
a snapshot of the system at any given time. Deﬁning the system and in some ways governing
the entity ﬂow are system parameters, for example, number of resources, queue space, and
so on. Aﬀecting the system are external factors, such as supplier schedules and customer
orders, which provide some of the input parameters for the model.
Ultimately, your goal is to analyze the output from the model, which is the performance
measures of interest, for example, average time in the system, work in process, meeting due
dates, and so on. There is output at the end of each model run or replication. A run is

12-4
Operations Research Methodologies
External factors
Entity
1
Entity
2
Entity
3
System
parameters 
Output
(performance measures)
FIGURE 12.1
Basic simulation modeling process.
typically the simulation executing over some length of simulation time or entity count, and
the output is based on all the observations that have been made as the state of the system
has changed over simulated time. When you repeat the simulation run for the same model
multiple times, you are making replications of that model, which diﬀer slightly from each
other due to the stochastic inputs.
Entities have attributes, which are descriptors of the entity characteristics, such as part
or customer type, or routing through the system. Attributes are somewhat like a simulated
version of a route sheet in a factory—they contain information speciﬁc to a particular entity
that is attached to it for the duration of its time in the system. Attributes for an entity may
change their speciﬁc values during the simulation; for example, a part’s priority may get
higher as it gets closer to its due date. The system has parameters, which are descriptors
of the system characteristic, for example, the number of machines or workers, queue sizes,
and so on. Although some system parameters may change during one simulation run (for
example the number of workers may change according to a speciﬁed schedule), they typically
remain the same value throughout one run. There are also system variables, sometimes
called global variables, which take on diﬀerent values throughout a simulation run based
on system conditions, for example, the number of entities in a queue, resource status (busy
or idle), or the number that have left the system. System variables are often used to track
time-persistent performance measures in the system.
Most popular simulation languages take a process-oriented approach to modeling entity
movement through a system, thinking of a series of events, delays, and activities for a speciﬁc
entity as a “process” [1,3]. Figure 12.2 illustrates a very simple process for one entity at a
single server or a single machine, with events, an activity, and a queue delay noted. First,
there is the arrival event for the entity. Then, as entities move through the system, they try
to access resources, for example, machine, worker, nurse, which are temporarily allocated to
a particular entity and made available again later to other entities. Queues are the waiting
lines for resources where entity movement is temporarily suspended if the resource is busy.
This type of entity delay is a status-dependent delay because the length of the delay is

Simulation
12-5
Time line: 
Arrival
event
Start of
service
event 
End of
service
event 
Activity
“being served” 
Process
Queue
delay
FIGURE 12.2
Process-oriented view of entity movement.
based on the status of factors outside of that entity’s control. For example, the length of
time an entity spends in a queue is dependent upon the processing or service times of all
the entities before it in the queue. When the resource becomes available and is allocated to
the entity, the entity begins its service activity, which begins with a “start” event and ends
with an “end” event. Thus, activities, such as this “being served” activity, are triggered by
instantaneous events and have a speciﬁc duration, making them deterministic delays.
It should be noted, however, that although most discrete-event simulation languages
adopt a process orientation, behind the scenes they are still executed one event at a time.
This requires coordination between the event calendar and the simulation clock. The event
calendar can be thought of as the “to do” list of the simulation model, and processing
events from the event calendar moves the simulation clock forward in time. Future events
are scheduled on the event calendar. Similar to our personal “to do” list, after the simulation
has processed an event, it is deleted from the list. There are three pieces of information
associated with each event: (1) the entity that is associated with that event, (2) the time
the event is to occur, and (3) the type of event. For a single-server single-queue system, a
snapshot of the event calendar is shown below:
Entity
Time
Type of Event
2
5.3
arrive to system
3
7.1
arrive to system
1
10.7
end processing
The event calendar is always sorted in chronological order. Figure 12.3 shows how the
event calendar operates. The simulation progresses by taking events one at a time from
the event calendar in chronological order and updating the simulation clock to be equal
to the event time. This means that the simulation clock time jumps forward in discrete
amounts instead of having continuous time progression. Then, the simulation processes the
model logic triggered by that event and its associated entity until the entity hits a delay.
All processing of the associated model logic, until the delay starts, occurs in zero simulated
time. Once that entity is in a delay, the simulation model must look for something else to
do. So, it returns to the event calendar, takes the next event oﬀthe calendar, and repeats
the procedure.
Our small event calendar above can illustrate the event calendar basic operation, assuming
a single-server, single-queue system. The simulation ﬁnds the next event is entity 2 arriving
at time 5.3. The simulation clock is immediately updated to time 5.3, and a new arrival
event is scheduled onto the event calendar, assuming entity 4 will arrive at time 11.2. The
simulation then attempts to move entity 2 through the logic of checking to see if the server

12-6
Operations Research Methodologies
Find next event 
Update simulation
clock to event time
Process logic
associated with
event  move entity
through logic until
it is delayed   
Go to calendar 
Happens in zero time
on simulation clock 
FIGURE 12.3
How a simulation progresses through time.
is available. Since it is busy serving entity 1, entity 2 is placed in the queue and is now
in a queue delay. The simulation clock time remains 5.3 because the logic processing took
zero time. Consequently, the simulation goes back to the event calendar to ﬁnd the most
imminent event to process. Because the event associated with entity 2 has been processed,
it is removed from the event calendar, resulting in the following updated event calendar:
Entity
Time
Type of Event
3
7.1
arrive to system
1
10.7
end processing
4
11.2
arrive to system
The next event is entity 3 arriving at time 7.1. The simulation clock is immediately
updated from time 5.3 to time 7.1, and another arrival is scheduled on the event calendar,
assuming entity 5 will arrive at time 17.5. When the simulation processes entity 3 through
the logic of checking server availability, the server is still busy with entity 1. So, entity 3 is
also placed in the queue in a delay, and the updated event calendar is below:
Entity
Time
Type of Event
1
10.7
end processing
4
11.2
arrive to system
5
17.5
arrive to system
When the simulation returns to the event calendar this time, it ﬁnds the next event is
“end processing” for entity 1 at time 10.7. The simulation clock updates from time 7.1 to
time 10.7. The logic for entity 1 is to end processing and leave the system. This triggers the
next entity in the queue, entity 2 in our example, to start its service. Entity 2 is now in a
delay for a service activity, which is a deterministic delay, and the end of service activity for

Simulation
12-7
entity 2—let’s assume it will occur at time 15.7—is put onto the event calendar in proper
chronological order as follows:
Entity
Time
Type of Event
4
11.2
arrive to system
2
15.7
end processing
5
17.5
arrive to system
Our simulation again returns to the event calendar in search of the next event to pro-
cess, and this cycle repeats continuously until the simulation ends. There are two ways
a simulation run will end: either there is a speciﬁc “end of run” event scheduled on the
event calendar at a speciﬁed time or there are no events on the event calendar, causing the
simulation to simply run out of things to do and end.
Although this is a very short example, it illustrates the frequency with which the event
calendar is accessed and must consequently be re-sorted into chronological order. This is one
reason why the execution time for a discrete-event simulation model is directly related to the
number of events and how the lists are managed. Although most simulation languages had
traditionally used linked-lists to manage the event calendar and other lists in the simulation,
binary heaps, used for example in SIMAN, or Henriksen’s Algorithm, used for example in
SLAM and GPSS/H, provide faster execution times [4].
12.2.2
Need for Randomness
Because systems that are appropriate to analyze with discrete-event simulation are stochas-
tic, the ability to include randomness in the model is essential. Reliable random number and
random variate generation are at the heart of discrete-event simulation languages. A random
number is used in a simulation model anytime a sample is generated from a distribution
or when a probability must be checked, for example, determining if a part passes or fails
inspection. One random number generator characteristic that is of particular importance
for simulation languages is the need for a long cycle length, which is the number of ran-
dom values generated before the random number string starts to repeat. When modeling
large systems, many random numbers are needed during each run. Just a few years ago,
a well-researched linear congruential generator that could provide a cycle length of about
2.1 billion values was considered adequate and the norm. Recently, however, a combined
multiple recursive generator (e.g., used in Arena) provides a cycle length of 3.1 × 1057 [3].
Having a long cycle length and the additional ability to specify diﬀerent streams for diﬀer-
ent distributions is needed for variance reduction techniques for output analysis, which is
discussed later in this chapter.
When the random number is used to generate a sample from a distribution, generating the
random number is the ﬁrst step. If the distribution has a continuous cumulative distribution
function (CDF), then the inverse transform method is used. In this method, the CDF is set
equal to the random number and then solved for the sample value. This is the preferred
method of generating a variate. If the CDF is not continuous, then other mathematical
methods are used, but they still rely on having a random number.
12.2.3
Output Analysis for Discrete-Event Simulation
The main goal of modeling is to analyze output data regarding the performance measures
of interest to your system and make decisions or recommendations based on that analysis.

12-8
Operations Research Methodologies
Random
inputs
Simulation
model logic 
Random output for a
performance measure
Calculate point
Estimate and
interval 
Estimate of the 
performance measure 
FIGURE 12.4
Randomness in inputs and outputs.
For discrete-event simulation models, which are stochastic and dynamic, the randomness
inherent in the model inputs means the model outputs also exhibit randomness (see Fig-
ure 12.4). As previously mentioned, the stochastic input causes each simulation run, or
replication, of the same model to have slightly diﬀerent output. Also, as simulation models
are run rather than solved, accurate output data analysis depends upon both an accurate
translation of the physical system into simulation model logic and setting the correct simu-
lation run control parameters so that the output will have appropriate statistical properties.
If this is done correctly, you can then use standard statistical analysis techniques, for exam-
ple, conﬁdence intervals and paired-t tests. Therefore, you must know the type of output
analysis that you want to accomplish as you develop your model.
The ﬁrst step for proper output analysis is classifying the type of system you are modeling
and the type of analysis needed as either a terminating system or a nonterminating system.
A terminating system has ﬁxed starting conditions that are usually the same for every
system start, for example, empty (no parts or customers) and resources idle, and some
event deﬁnes the end time or closing time of the system, for example, store closing time
or the time the last diner leaves. Many service systems that are not open 24 hours a day
are terminating systems, for example, banks, restaurants, retail stores. A nonterminating
system might not have the same starting conditions each day and there is not a clear ending
event. Any 24-hour operation is a nonterminating system, for example, emergency rooms,
24-hour manufacturing plants, convenience stores. Additionally, any system that carries over
work from the previous day should also be considered nonterminating because the starting
conditions each day have a diﬀerent level of work in process, and thus they are not the
same. Most manufacturing facilities exhibit this characteristic.
For systems classiﬁed as terminating systems, all the data collected during the simulation
run is included in the output analysis. These systems exhibit transient behavior when the
system starts up, which is important to include in the analysis. For nonterminating systems,
we are usually interested in the long-term, steady-state behavior of the system after the
transient or start-up conditions are gone. In these systems, only the steady-state data should
be included in the analysis. Therefore, we need slightly diﬀerent approaches to properly
analyze each case.
Regardless of the type of system, the output performance measure of interest could be
an observation-based statistic (sometimes called discrete-time or tally statistic), a time-
persistent statistic (sometimes called discrete-change or continuous-time), or a simple run-
ning count. Examples of observation-based statistics include average customer or part time
in the system or average time in the queue. Letting Xj be a single entity’s observed value
with J total entities, then the average for replication i is calculated in Equation 12.1.
xi =
J
j=1 Xj
J
(12.1)
Examples of time-persistent statistics include average work in process, average number in
the queue, and utilization. To ﬁnd the average of a time-persistent statistic, a system or
global variable tracks the value of the speciﬁc measure (e.g., the number in queue) over the
entire time of the simulation replication, as in Figure 12.5. To calculate the average, the

Simulation
12-9
0
0.5
1
1.5
2
2.5
0
1
2
3
4
5
6
7
8
9
Time in minutes
10
Number in queue
FIGURE 12.5
Tracking the number in queue.
area under the curve is divided by the total time of the simulation replication from time 0
to T, as in Equation 12.2.
yi = 1
T
T

0
Y (t)dt
(12.2)
Examples of simple running counts include the total number leaving the system, the
total number of bad parts, and total number of customers that cannot enter a queue that
is full. You simply increment a counter variable throughout the total time of the simulation
replication each time an entity matches the criteria for the count.
The next two sections discuss how to properly analyze these performance measures for
terminating and nonterminating systems.
Terminating System Analysis
As previously mentioned, stochastic input causes each simulation replication of the same
model to have a slightly diﬀerent output. Therefore, the output from one simulation repli-
cation represents one sample from the population of all possible replications, paralleling a
physical experiment that takes sample observations from an entire population. In physical
experiments, we need to have population samples exhibit independence and be normally
distributed to apply many of the common statistical analysis techniques. The same is true
for simulation, where we commonly calculate a conﬁdence interval about the mean value
and use that conﬁdence interval to reach conclusions and make recommendations.
An advantage of statistical analysis in our simulated environment is that it is very easy to
insure independence of the output data between replications. To obtain independent output
data from multiple replications, when running your simulation, you specify (1) that the
statistics be cleared (set back to 0) between replications, (2) that the model be reinitialized
to the same starting conditions (usually empty and idle) between replications, and (3) that
a diﬀerent random number seed is used for each replication to generate the model inputs.
Because these factors are easy to specify in simulation languages, it is easy to insure that
the assumption of independence will hold.
It is not quite as easy to demonstrate that the normality assumption will hold for any
simulated system. Fortunately, the performance measure of interest is typically an aver-
age over all the observations or all the time of each replication. For example, the output
“average time in the system” is the average over all the individual observations for each
entity that has exited the simulated system. Then, to ﬁnd a point estimate of the mean

12-10
Operations Research Methodologies
over multiple replications, take the average of values that are themselves averages. Simu-
lation practitioners tend to agree that based on both practical experience with empirical
testing and theoretical general versions of the central limit theorem, it is valid to use stan-
dard statistical techniques, which have an assumption of normality, with output data from
discrete-event simulation models [3].
Suppose that we have modeled a terminating system and we are interested in the average
time that a part or customer spends in our system. Our typical analysis proceeds according
to the following steps:
Step 1: Run multiple independent replications.
Step 2: Record the output value for average time spent in our system, calculated by
Equation 12.1, from each replication (1 value per replication).
Step 3: Using the average time in system values from each replication as calculated in
Step 2, calculate the mean and standard deviation of these values.
Step 4: Calculate a conﬁdence interval about the mean calculated in Step 3.
Step 5: Make recommendations based on your statistical analysis and knowledge of the
system and company goals.
For example, suppose 20 replications (n) had end of replication average time in the system
values (in minutes) of 12, 15, 10, 11, 12, 11, 16, 12, 17, 14, 13, 15, 11, 9, 10, 13, 12, 8,
12, 15. The mean (¯x) is 12.4 min with standard deviation (s) of 2.37 min. If we want a
99% conﬁdence interval (CI) (α = 0.01), then the appropriate critical value from Student’s t
distribution is tα/2,n−1 = 2.86. From the general conﬁdence interval Equation 12.3, the 99%
CI for time in the system is 12.4 ± 1.516 min or [10.884, 13.916].
x ± tα/2,n−1
S
√n
(12.3)
This conﬁdence interval means that we have an approximately 99% conﬁdence that the true
mean of the average time for parts in this system falls between 10.884 and 13.916 min [1].
The range of the conﬁdence interval also gives you a sense of the variability in the system—
the wider the range at a given conﬁdence level the more the uncertainty concerning average
time in the system.
Ultimately, it is still the responsibility of the analyst to appropriately use these data
values to reach a conclusion or make recommendations. Good, accurate statistical analysis
as described above is a key component for making appropriate recommendations. But you
must also consider the type of system, the type of company, and the goals the company has
for the speciﬁc system being modeled.
Nonterminating System Analysis
As previously mentioned, with nonterminating systems we are usually interested in the
long-term, steady-state behavior of the system after the transient or start-up conditions
are gone. Steady-state behavior does not mean that the system has no variability and that
performance measures have become a constant value. Steady-state behavior is when the
variability in the system has reached a more consistent level. This is the case in nontermi-
nating physical systems that have been in operation for a long time. When a system is brand
new, it typically starts in an empty and idle state. So, initially there is little competition for
resources, utilizations are low, and there are no queues at many resources. Consequently,
parts or customers have low time in the system values and very little of this time is nonvalue
added. As time goes on, however, queues start to build because there is more competition for
resources, time in the system values rise with more nonvalue added time (e.g., queue time)

Simulation
12-11
0.00E+00
1.00E+02
2.00E+02
3.00E+02
4.00E+02
5.00E+02
6.00E+02
1
29
57
85 113 141 169 197 225 253 281 309 337 365 393 421 449 477 505 533 561 589 617 645 673 701 729 757 785 813
Observation number
Time in system
FIGURE 12.6
Time in the system starting from empty and idle system conditions.
and so on, to a certain point where the system starts to “stabilize” at some level of variability
about some mean values for performance measures of interest, as shown in Figure 12.6.
Looking at Figure 12.6, you might estimate that the initial start-up or transient conditions
end around observation 393. The data after this point could then be considered to be
reﬂecting the steady-state behavior of the physical system. When we develop a simulation
model of a nonterminating system to analyze its steady-state behavior, our goal is to only
analyze simulation output data from the time where our model is also exhibiting steady-
state behavior. If we start our simulation model with empty and idle conditions, as we did in
terminating system analysis, a plot of the output data would look similar to Figure 12.6. If
all of data from time 0 is included in the output data calculations, our values will be biased
and will not accurately reﬂect only the steady-state system behavior. Therefore, since we
only want to analyze data after the start-up transient conditions have ended, we must have a
way of identifying the point at which the start-up transient conditions end and eliminate the
biasing eﬀect of initial transient data upon our steady-state statistical output data analysis.
There are a few methods to eliminate the eﬀect of initial transient data. One method is
to select starting conditions for your simulation model other than empty and idle. The idea
is to “pre-load” the simulation to make it look more like an ongoing system at steady state
right from time 0. Then, all the data from each replication can be used for output analysis
since all the data is from a time where the simulation is emulating the steady-state behavior
of the physical system being modeled. Pre-loading the system involves creating entities with
attribute values assigned that would be placed in diﬀerent queues throughout the system;
thus, when the simulation begins, resources throughout the system will immediately have
work to do and entities will soon be exiting the system. An advantage of this method is that
it is easy to explain to anyone, even if they are not familiar with simulation. A disadvantage
of this method is that if the system is large, you may need to pre-load many entities. This
means (1) you need to collect data from the physical system to approximate the typical

12-12
Operations Research Methodologies
number of entities in each queue and the part mix at each queue and (2) actually loading
all those entities into the simulation model can be tedious. For example, with 10 resources
and approximately 5 entities waiting at each there are 50 entities that must be entered. Also,
if the system being modeled is in the design phase, then this is not a feasible approach.
A second method is to allow the simulation to start in its typical empty and idle condition,
then delete initial transient phase data from the output analysis. Deleting early data is
easily accomplished in most simulation languages by simply specifying a warm-up period
time in the run control parameters for a model. With a warm-up period speciﬁed, only data
collected after that time is included in the output statistics provided by the simulation. This
requires estimating the point where the initial transient phase ends and steady state begins.
Typically, this is done by plotting the output performance measure of interest over time
for the output from one long replication of your model [4]. Then, by looking at the plot,
visually estimate when the data appears to stabilize and select that point as your warm-
up period length. To assist this process, plotting a moving average of the data instead of
the raw data is sometimes helpful, as shown in Figure 12.7. Figure 12.7 is the same raw
data for time in the system as Figure 12.6, but it has a moving average with a window of
width 50 superimposed. Looking at this graph, we might estimate that steady-state behavior
begins at approximately 500 observations. An advantage of this method is that it can be
applied when modeling either an existing or proposed system as it does not need data from
the physical system. A disadvantage is that determining the point at which the steady-
state behavior starts can still be diﬃcult. Also, there is some amount of wasted data and
run time during the warm-up period. In some large systems, the warm-up period can be
signiﬁcant.
0.00E+00
1.00E+02
2.00E+02
3.00E+02
4.00E+02
5.00E+02
6.00E+02
1
29 57
85 113 141 169 197 225 253 281 309 337 365 393 421 449 477 505 533 561 589 617 645 673 701 729 757 785 813
Observation number
Time in system
FIGURE 12.7
Time in the system starting from empty and idle system conditions, with a moving
average line of window width 50 imposed.

Simulation
12-13
A third method is to simply run the model for an extremely long time so that the initial
transient data is dominated by the steady-state data. With computer power and speed
continually improving, this method becomes more feasible. An advantage of this method is
that it is simple to implement. However, you still need to get a rough estimate of the point
in time that steady-state behavior begins to be certain you have a long enough run length
to truly dominate the initial transient data. This could be done using the second method
above. Another disadvantage is that some larger systems may have a long initial transient
phase, which requires a very long total run length to dominate the data.
After you have solved the problem of the initial transient data with one of the methods
above, then you need to get independent and normally distributed data so you can use the
standard statistical techniques just as we did with terminating system analysis. Perhaps
the easiest approach is to combine any of the above three methods with making multiple
independent replications. In other words, you implement a method that insures that the
eﬀect of initial transient data has been eliminated, such that the output statistics in each
replication from the simulation reﬂect the steady-state behavior of the system. Then, when
multiple independent replications are run, by making the same speciﬁcations as discussed in
the previous section on terminating systems, we will have independent samples of the steady-
state performance measures of interest. Next, apply the same standard statistical analysis
techniques for mean, standard deviation, and conﬁdence interval as shown in Equations
12.1–12.3. So, once the initial transient issue has been removed, simply follow the same
5-step analysis procedure used with terminating systems as described in Section 12.2.3.1.
One of the most common approaches is the truncated replication approach, because it is
easy to specify a warm-up period and to specify the running of independent replications.
This combines the second method above, eliminating the initial transient data by specifying
a warm-up period, with running multiple independent replications.
Another method for analyzing steady-state performance is batch means analysis, which
is quite diﬀerent from the above techniques. Batch means analysis requires that the initial
transient data be truncated from only one long replication, and then it uses the remaining
steady-state data from that one replication for analysis. However, if all the steady-state indi-
vidual observations or time-persistent values from within that one long replication are used
to calculate a sample variance, there is a serious problem with bias in the variance calcula-
tion due to the signiﬁcant correlation of data points from within one simulation replication.
Therefore, this does not allow for statistically accurate conﬁdence interval construction.
Batch means analysis acknowledges this correlation and attempts to create large enough
batches of individual data points from within the one replication, such that the calculated
means of these batches will be statistically independent from each other [3]. These batch
means may then be treated as independent observations to be used in standard statistical
calculations. Steps in batch means analysis are:
Step 1: Truncate the initial transient data.
Step 2: Group the remaining data into batches of observations that are independent.
N batches of size m.
Step 3: Let Xj(j = 1, 2, . . ., N) be the mean of batch j.
Step 4: Treat Xj like independent observations of a steady-state performance measure,
then use standard statistical analysis techniques, such as conﬁdence intervals.
Step 2 may need to be repeated with increasingly larger batch sizes (m) and a corresponding
reduction in the number of batches (N) until statistical testing indicates that the batch
means are no longer correlated.
An advantage of the batch means technique is that you only need one very long simulation
run. In a very complex system where one replication may take several hours of computing

12-14
Operations Research Methodologies
time, this can save valuable time. A disadvantage is that you must perform a statistical
analysis on the batch means testing for independence, and the proper batch size must be
determined. This technique may be more diﬃcult to explain to someone not familiar with
simulation, and it is a bit more involved with respect to statistical tests than the previ-
ous methods. However, some simulation languages have a built-in capability for analyzing
simulation output via batch means.
Comparing Two Systems
Many times the reason that a simulation model is developed is to evaluate a change to the
current system. In this case, you need to do an analysis of the output performance mea-
sures from two simulation models, one of the current system conﬁguration and one of the
changed/modiﬁed system, to estimate if there is indeed a signiﬁcant diﬀerence in perfor-
mance due to the system modiﬁcation. A common way to do this is to perform a paired-t test
using the output performance measures from the two simulation models, with H0 = there
is no diﬀerence between the mean performance and with H1 = there is a diﬀerence. This is
coupled with a simulation variance reduction technique called the common random num-
bers (CRN) method. CRN requires that within each simulation model diﬀerent random
number streams are assigned to each distribution used, but between the two models, the
same random number stream should be used for the same type of distribution. For example,
if random number stream 1 is assigned to the interarrival distribution in the model of the
current system, then random number stream 1 is assigned to the interarrival distribution in
the model of the modiﬁed system. This is similar to what we do when conducting physical
experiments when we try to control all factors in the experiment other than the one factor
that we are modifying. Then, if a change is observed in the performance measure between
the experiments, we feel conﬁdent that the diﬀerence is due to the modiﬁcation that we
made and not due to some chance variation from another system factor. The way to control
the other factors in a simulation experiment is to control the random number streams.
CRN is a variance reduction technique because it induces a positive correlation between
the outputs from the two simulations. When using paired-t tests to statistically compare
the output from the two systems, the diﬀerence between the performance measure for both
systems for each replication becomes the new performance measure of interest, as follows:
Output
Output
Diﬀerence
Current System (A)
Modiﬁed System (B)
XAi – X Bi
XA1
XB1
D1
XA2
XB2
D2
.
.
.
.
.
.
XAn
XBn
Dn
Then, use Equation 12.3 to calculate a conﬁdence interval about the mean of the dif-
ferences. If 0 is contained in the interval, then we fail to reject the null hypothesis and
assume the systems have no statistically signiﬁcant diﬀerence at the speciﬁed conﬁdence
level. If 0 is not contained in the interval, then we reject the null hypothesis and assume
there is a statistically signiﬁcant diﬀerence between the systems. The variance reduction
occurs because we are taking the diﬀerence of positively correlated output, and then the
standard deviation of those diﬀerences, which is the square root of the variance calculation

Simulation
12-15
shown in Equation 12.4, is used in the conﬁdence interval. Consequently, in Equation 12.4
the ﬁnal covariance term is now greater than 0, making the variance of the diﬀerence less
than what it would be if the random variables in the diﬀerence were independent making
the covariance equal 0.
V ar(XA −XB) = V ar(XA) + V ar(XB) −2Cov(XAXB)
(12.4)
If you reject the null hypothesis and determine there is a diﬀerence between the system
performances, then you must decide if it is a diﬀerence that would be a beneﬁt if the
modiﬁcation occurred. Finally, a cost analysis should be done to insure that it would be a
cost eﬀective change to implement.
Evaluating Multiple Potential Modiﬁcations to a System
At times there may be more than one system parameter that we are considering changing. In
this case, you should develop a design of experiments with those parameters. Once you have
identiﬁed the design points, you can modify the simulation model to emulate each design
point and run multiple replications at each point. The output from each simulation then
becomes the input to further statistical analysis, such as analysis of variance (ANOVA).
12.3
Simulation Languages and Software
There are many discrete-event simulation software products available. Although they all
have some diﬀerences to distinguish themselves in the marketplace, there are many common
features among all [1,4]:
• Graphical user interface to make model building easier.
• Sampling from several diﬀerent distributions.
• Animation to visualize the model, possibly 3-D—this allows you to observe short-
term variability in the performance of the model and can help quickly identify
bottlenecks or quickly identify errors in the simulation model logic.
• Collect a variety of performance measure statistics and provide summary output
statistics, such as means or conﬁdence intervals, in an output report.
• Provide data in tables and in graphical form.
• Easily run multiple independent replications.
Additionally, there are features that many, although not all, simulation environments
share:
• Extended statistical analysis, either as an add-on or integrated into the package—
this may include both input data analysis (e.g., ﬁtting data to a distribution) and
output data analysis (e.g., comparing systems for statistical diﬀerences, batch
means analysis).
• Import CAD drawings as a foundation for developing animation.
• Interfacing with other software, such as spreadsheets.
• Creating Visual Basic interfaces to ease changing the parameter speciﬁcation for
the model to create diﬀerent scenarios.
Some characteristics that will set simulation languages apart include the level and sophis-
tication of material handling or warehousing logic and the price of the package. Although
some simulation software companies oﬀer a bare-bones version of their packages for under

12-16
Operations Research Methodologies
$1000, prices for versions that allow you to model a reasonably sized system range from
approximately $1300 on the low end to mid-range of about $15,000 to $30,000 and up for
specialized versions.
There are two main categories of simulation languages: general-purpose and simulator.
A general-purpose language can be used to model any environment, manufacturing, ser-
vice, business processes, and so on. The terminology used in the modeling constructs is
very generic, for example, entity, resource, and process. A simulator is a language that is
customized for a particular type of industry, for example, electronic chip manufacturing,
hospital, call center. The terminology used in a simulator is speciﬁc to the environment,
for example, wave solder machine, nurse, phone line. There is a tradeoﬀbetween ease and
quickness of building a model and ﬂexibility to capture a wide variety of real-world complex
system logic. A general-purpose language oﬀers a lot of ﬂexibility regarding the structure
of the physical system logic that can be properly accommodated in the simulation model.
However, it may take longer to develop a model to make sure that the generic logic con-
structs are put together to properly emulate the logic of a speciﬁc type of environment. On
the other hand, a simulator has logic constructs that are more structured to match typical
physical system logic for a particular industry. Therefore, it has less ﬂexibility to model
anything and everything that might be encountered in all systems. But a model can usually
be developed faster using a simulator compared to a general-purpose language, assuming
the simulator is properly matched to an environment for which it was intended.
One additional comment regarding simulators: some simulators can still seem like a very
general-purpose tool within an industry. For example, a manufacturing simulator, such as
ProModel, is applicable in a wide variety of manufacturing environments. However, some
simulators for more speciﬁc environments, for example, electronics manufacturing, may not
be perfectly applicable to all types of systems within that larger category. Therefore, if you
are considering a simulator, you must make sure that the structured logic of the simulator
is truly applicable to your speciﬁc system.
12.3.1
A Sampling of Discrete-Event Simulation Software
This section contains a sampling of the many discrete-event simulation software packages
available. All the following software run on a PC, with a typical operating system require-
ment of either Windows 98, Windows 2000, or Windows XP. They also take the process-
oriented approach to modeling. Software is discussed in alphabetical order.
Arena
Arena is a general-purpose simulation software that can model manufacturing or service sys-
tems, and it is a product of Rockwell Automation [3,5]. The underlying simulation language
is SIMAN. The Basic version is focused on business process modeling; however, Standard
and Professional versions can model any environment, including continuous systems. It has
a graphical interface for model building with logic modules, whose parameters you specify,
connected with arrows designating entity ﬂow. It includes input data analysis with dis-
tribution ﬁtting through its built-in input analyzer and it allows more detailed statistical
analysis of output data through its built-in output analyzer along with graphical chart
capability. Standard output statistics, such as means and conﬁdence intervals, are provided
in the standard output report in numerical tables. Arena can model a variety of material
handling devices including fork lifts, automated guided vehicles (AGVs), and diﬀerent types
of conveyors. Version 10.0 also includes specialized logic to more easily model continuous
manufacturing that uses tanks, regulators, and pipes. It allows interfacing with various

Simulation
12-17
other software applications. Arena has built-in capability to easily run and analyze multiple
scenarios with its process analyzer, and you can add-on OptQuest for Arena to perform
optimization. OptQuest was developed by Dr. Fred Glover and is oﬀered by OptTek Sys-
tems, Inc., as an add-on to several simulation packages [6]. It is based on several methods,
including tabu search, scatter search, and neural networks.
AutoMod
AutoMod, available through Applied Materials, Inc. [7,8], can be used to model a wide
range of applications; however, it is primarily applied to manufacturing and it includes sig-
niﬁcant material handling capabilities. It has built-in templates for bridge cranes, conveyors,
power and free systems, automated storage and retrieval systems (AS/RS), and path-based
movement (automated guided vehicles (AGVs), fork lifts, humans). It has 3-D graphics and
template models of some speciﬁc environments to speed model development. However, it
also is a simulation language to be used in a variety of manufacturing environments. In
Version 11.0 you can create “composite systems,” which allow the reuse of model objects
through the encapsulation of submodels and models that are saved as library objects. It
can also link to other software through ActiveX. Output may be viewed as numerical tables
or a variety of graphical charts. They also have the product AutoStat, which can help
with determining the warm-up period, sensitivity analysis, design of experiments, and opti-
mization using evolutionary strategies. Another product, AutoView, provides postprocessed
animation capability, allowing playback and creation of presentation graphics.
Extend
Extend simulation software refers to a family of products by Imagine That, Inc., that covers
a range of applications, including continuous systems, manufacturing, and service systems
[9,10]. All products have similar structure and capability. Models are built by combining
diﬀerent iconic blocks, representing a logic step or calculation, according to the proper logic
ﬂow for entities. During a simulation run, the user may change simulation parameters on
the ﬂy to observe the near-term eﬀect of changes. Data relating to a speciﬁc block, for
example, a queue block, may be viewed via plots and tables by double clicking the block.
Output from a replication may be displayed in a table or plot, exported to another program
or copied to a diﬀerent area of the worksheet. An evolutionary optimization tool is included
in the software. Extend also has the capability of creating a hierarchical block by grouping
together blocks deﬁning the logic of a submodel representing part of the larger system.
These hierarchical blocks may be stored for future use. Extend supports the COM model
and ODBC without any additional programming by the user. The Extend blocks are open
source, so they may be customized to a speciﬁc need. Also, Extend has been chosen as
a simulation engine by third-party applications. The Extend Suite product includes Proof
Animation from Wolverine Software [11].
Flexsim
Flexsim, developed by Flexsim Software Products, Inc., is an object-oriented simulator
aimed at all types of manufacturing environments, including both discrete-event systems
and continuous ﬂow systems [12,13]. It allows a hierarchical approach to modeling, making
use of inheritance. Everything in Flexsim is open to customization and users can create
their own new objects using C++. All models are built and run in 3-D. Input data can
be imported from databases and spreadsheets using ODBC or Dynamic Data Exchange

12-18
Operations Research Methodologies
connections, and output can be exported to word processors or spreadsheets. The output
can be viewed in reports and graphs.
GPSS/H
GPSS/H is a widely used version of the GPSS simulation language originally released by
IBM in 1962, and it is available through Wolverine Software Corporation [14]. It is a general-
purpose language that can model any manufacturing or service system that can be thought
of as a queueing system. Models are built using a text-based interface, as opposed to a
graphical interface commonly used in most other simulation languages. GPSS/H has built-
in ﬁle and screen input and output, allowing data to be read into and out of standard
text ﬁles that can be interfaced with standard spreadsheets. Customized output can also
be created. GPSS/H has an interactive debugger to assist veriﬁcation, and it uses Proof
Animation, another Wolverine Software product [11], which can be viewed postprocessed
after the simulation completes or while it runs. Because of GPSS/H’s ﬂexibility and speed
of execution, it is sometimes used as the simulation engine for simulators developed for
speciﬁc environments. With GPSS/H’s structure and text-based model building interface,
it tends to be more frequently used by experienced modelers with more complex systems.
Micro Saint Sharp
Micro Saint Sharp is a general-purpose object-oriented simulation language by Micro Analy-
sis and Design, Inc. [15,16]. It was developed speciﬁcally to increase model execution speed,
and allows users to turn oﬀall interfaces for signiﬁcant speed-up. Micro Saint Sharp can
be used to model any system that can be represented by a ﬂowchart. It uses task network
modeling with activities represented as nodes and arrows connecting nodes into sequences of
activities. The user deﬁnes the structure of the task network and then deﬁnes the objects in
the network. It has a symbolic animation that animates the network diagram. This can be
helpful for quickly gaining insight into the entity ﬂow and can help with debugging. Charts
showing dynamically changing outputs can also be presented. Alternately, you can also use
their animation tool called Animator, which gives a more realistic view of the system. Micro
Saint Sharp has a plug-in interface allowing for some modularity to customize how you use
the software. Its options include OptQuest for optimization [6], the Animator tool discussed
above, and interoperability using COM services.
ProModel
ProModel Corporation has several simulation products available, including ProModel,
MedModel, and ServiceModel [17,18]. These are simulators because they are written for a
speciﬁc industry. ProModel is applicable in manufacturing and warehousing environments,
MedModel is speciﬁcally for modeling healthcare systems, and ServiceModel is designed for
service industries. However, within these broad industry categories, they are very generally
applicable to many speciﬁc environments. These products use object-oriented modeling con-
structs. Although the rest of this section focuses on ProModel, the capabilities are similar
across these three products.
ProModel is a simulator aimed at general manufacturing environments. Models can usu-
ally be built graphically with the set of modeling elements provided (e.g., resources); how-
ever, complete programming is available if necessary for very unique physical system logic.
Animation is developed when the model logic is being speciﬁed, and you can create both
2-D and 3-D perspectives. In ProModel you can also specify costing information that can
be associated with a speciﬁc resource, location, or entity, which is reported in the output

Simulation
12-19
report. The representation of output data can be in tabular form or in graphics and charts.
There is a run time interface that can be used at the start of a run to make changes to system
parameters for just that run, or it can be used to deﬁne and store multiple scenarios of the
system for experimentation. Two optimization capabilities are available. SimRunner, which
was developed by ProModel Corp. speciﬁcally for their products, uses genetic algorithms
and evolutionary strategies. It can be used to aid model analysis by determining the number
of replications needed to estimate performance measure mean values to a speciﬁc precision,
or to aid with steady-state analysis by estimating the proper warm-up period to eliminate
transient data and estimating run time. It can also be used to optimize a user-speciﬁed
performance measure by evaluating various combinations of user-deﬁned input factors. The
OptQuest for ProModel add-on is another optimization alternative.
QUEST
QUEST from Delmia Corporation is a manufacturing environment simulator that has a
graphical model building interface [1,19]. It is a 3-D simulation environment that can inte-
grate 2-D and 3-D CAD drawings to more accurately capture the layout of the factory
being modeled. It can also interface with spreadsheets and production software, such as
manufacturing execution systems (MES), material requirements planning (MRP), enter-
prise requirements planning (ERP), and scheduling packages. QUEST has an emphasis in
its capability to model a variety of material handling types, including fork lifts, conveyors,
power and free, humans, automated guided vehicles (AGVs), kinematic devices (robots),
and automated storage and retrieval systems (AS/RS). Output can be displayed numerically
in tables or graphically using various charts, or exported to a spreadsheet.
SIMUL8
SIMUL8 is a product of SIMUL8 Corporation and has a Standard and Professional version
[1,20]. It has a graphical model-building interface, allowing icons representing logic to be
placed and connected by arrows showing entity ﬂow. It also allows user-deﬁned icons that can
be saved and reused. SIMUL8 is a general-purpose language; however, it has been frequently
used to model service industries or business processes, and it also has some specialized logic
capability for modeling tanks and pipes found in continuous ﬂuid manufacturing processes.
On the input side, it can interface with Excel and in the Professional version it can also
interface with databases. Output can be viewed as either numbers in tables or graphically
via plots, pie charts, and the like. Output can also be exported directly to Excel or MiniTab,
or a ﬁle option allows you to send the date to other ﬁle types. The professional version has
additional input data ﬁtting help with STAT::FIT, a product of Geer Mountain Software
Corp. [21], and optimization capability through OptQuest for SIMUL8.
WITNESS
WITNESS 2006 is a product of Lanner Group, Inc., and has a manufacturing performance
edition (MPE), a manufacturing simulator, and a service and process performance edition
(SAPPE), a service industry simulator [22]. Its manufacturing version can model both dis-
crete manufacturing and continuous ﬂow manufacturing. Models are built using a building
block design, having a modular and hierarchical structure. It can interface with databases,
spreadsheets, and other applications. It also has a Virtual Reality module that can integrate
with the standard system to provide 3-D modeling capability. It has a simulation scenario

12-20
Operations Research Methodologies
manager to assist with experimentation, and the results can be exported to other applica-
tions. WITNESS has an optimizer integrated with its standard system, which is based on
a variant of simulated annealing.
12.3.2
Summary
There is a wide variety of discrete-event simulation software available. With the exception of
GPSS/H, all the other software mentioned here have graphical interfaces for model building.
Most have user-friendly modules that typically represent several lines of simulation language
code put together in a particularly structured way to capture a small logic element of the
physical system, for example, an entity accessing a resource for service. In software that has a
hierarchical structure, modules at higher levels have more lines of code within each module
with a speciﬁc structure, and they consequently allow less ﬂexibility to capture complex
or unusual entity ﬂows or control methods that may be present in the physical systems
being modeled. Then, it is necessary to go lower in the hierarchy to modules that represent
fewer lines of code with more elementary logic statements to buy the user more modeling
ﬂexibility. Although the graphical module approach tends to blur the lines of “modeling”
and “programming,” it is important to remember that underlying all the software mentioned
above is a simulation language and as the “picture” of the system is developed on the screen,
actual code is being created to capture that picture. So, remember that when building a
simulation model, all rules of programming apply, including garbage-in-garbage-out.
Additional information about the simulation software discussed above is available at the
following Web sites:
Arena: www.arenasimulation.com
AutoMod: www.automod.com
Extend: www.imaginethatinc.com
Flexsim: www.ﬂexsim.com
GPSS/H: www.wolverinesoftware.com
Micro Saint: www.maad.com
ProModel: www.promodel.com
QUEST: www.delmia.com
SIMUL8: www.simul8.com
WITNESS: www.witness-for-simulation.com or www.lanner.com
12.4
Simulation Projects—The Bigger Picture
Any time you are developing a model of a physical system, there are several steps in the
process that will help to make the modeling project a success, and simulation is no dif-
ferent. The steps are nothing more than the scientiﬁc process learned in grade school,
but with a few added simulation details. Some slightly diﬀerent procedures can be found
in Refs. [1–3].
Step 1: Identify the problem: A statement is needed of the problem that is initiating
the model development. For example, the manager of a facility may be concerned that the
amount of work in process (WIP) is exceeding the maximum level that the corporation
desires.
Step 2: State your objectives: Based on the problem statement, exactly what ques-
tions will be addressed by modeling this system? Specify what performance measures will

Simulation
12-21
be evaluated, what system changes will be studied, and what type of analysis of output data
is required. For example, with the problem identiﬁed in Step 1, we may need to evaluate the
utilization of resources and time spent in the system in addition to our main performance
measure of interest WIP. Possible system changes might focus on ﬁrst identifying if a single
bottleneck station exists, and specifying possible changes that could be investigated, such
as adding a resource, modifying the set-up operation, or changing the scheduling or control
policies in the system. The type of analysis for this problem may be comparing the steady-
state WIP performance of diﬀerent modiﬁcations of the system to determine which best
adheres to the corporate maximum WIP level. This may also be the best time to decide if
simulation is the correct modeling tool for this situation.
Step 3: Identify, collect, and prepare input data: Identify the input needed to
develop the model. What are the entities, the resources, and other system parameters? Are
processing times, interarrival rates, and other system parameters known or must a data
collection scheme be developed? If equipment failure is important to incorporate, does the
company have reasonable records of time between failures and downtime? If data must be
collected, it may take a few weeks or months to get enough data points for analysis, par-
ticularly for events/activities that occur less regularly, such as breakdowns. Therefore, it is
a good idea to develop the collection plan and begin that process as soon as possible, even
before you start model development.
Step 4: Formulate the model: Creating a logic ﬂow diagram of the physical system
being modeled is the ﬁrst step in formulating the model. Further, it is independent of the
particular simulation software that will be used for the project. Developing the logic ﬂow
diagram requires visiting the existing physical system or studying the plans for a proposed
physical system and talking to all parties involved with the system to ensure that the
modeler understands the way that system works or how it is proposed to work. The logic
ﬂow diagram also becomes an eﬀective communication tool between the modeler and the
system experts to help all parties have conﬁdence that the appropriate level of understanding
and detail of system operations will be properly included in the model. Once the logic ﬂow
diagram is complete, it can be translated into a simulation model in a speciﬁc simulation
language.
Step 5: Verify and validate the model: First, verify that the model is behaving as
you think it should according to your logic ﬂow diagram. This includes debugging the model
and doing some preliminary output data analysis to determine if it is reasonable given the
model inputs and parameters. Second, validate that the model is accurately representing
the physical system, that is, did we really understand the system correctly and build the
right model? Validation can be a challenging part of the simulation modeling process. For
both veriﬁcation and validation, simple animation can be helpful to identify bottlenecks or
basic logic problems in the model.
Some veriﬁcation strategies include:
1. Test a large model in stages. Break the bigger problem into logical subproblems,
for example, individual work stations or one department.
2. Send a single entity through the model at a time. If there are multiple possible
routes, force a single entity to take each route to ensure you have the proper
routing in your model.
3. Replace the random times with constant values, for example, the means of the
distributions, so that you can predict with certainty how long it should take an
entity to complete a speciﬁc route (combine with number 2).
4. Make a single replication during veriﬁcation, ﬁrst with constant times, then with
the randomness put back into the model. Look at the output data in each case,

12-22
Operations Research Methodologies
and use your engineering judgment. Are these the performance measure output
values you expect?
5. Ask an expert—show this output to a person familiar with the physical system
and get their opinion. You can also ask an outside expert familiar with simulation
modeling but not necessarily with the speciﬁc system for their opinion on your
modeling approach as well as the output data.
6. Perform some sensitivity analysis. Change an input parameter in a certain direc-
tion and see if the output data responds as expected. For example, in a moderately
loaded system, if you increase the number of arrivals per unit time, you would
expect higher utilization, longer queues, and longer time in the system.
Validation can occur simultaneously with veriﬁcation. In particular, numbers 5 and 6
above are also validation strategies. Number 5, ask the expert, can be taken one step
further if you are modeling an existing system. You can put the data from the model
beside actual performance measure data from the physical system and ask the expert if
they are reasonably close.
Step 6: Experiment and analyze the results: Once the model is veriﬁed and
validated, it can be used for its intended purpose—allow you to experiment with sys-
tem modiﬁcations. Depending upon the type and number of modiﬁcations, an appropriate
experimental design is developed. The proper statistical analysis is then done with means,
standard deviations, conﬁdence intervals, paired t-tests, and so on.
Step 7: Conclusions and recommendations: Using the data from Step 6, reach your
conclusions based on the simulation output results, and specify what you recommend for
the modiﬁcation or design of that system to address the objective identiﬁed in Step 2.
12.5
Summary
Discrete-event simulation is an important heuristic tool in the OR tool kit. It is applicable
when the performance of a large, complex system needs to be evaluated, but we cannot
properly capture the system dynamics in straightforward equations that could be used in
an optimal technique. This chapter has presented an overview of discrete-event simulation,
including discussion of basic deﬁnitions and operations of simulation, output analysis, some
examples of where simulation has been applied, simulation languages, and the steps of a
simulation project.
In addition to the software Web sites previously listed, there are some other readily acces-
sible sources for simulation information. Attending simulation conferences gives you quick
access to the latest information in both technical language developments and how people
in other companies are using simulation. Conferences also provide an excellent networking
opportunity to meet others facing similar problems. In a small- or medium-sized company,
you might be the only person engaged in simulation modeling, and you may feel isolated
with no one to bounce oﬀideas. Making contacts at a conference allows you to develop a
support network for your ideas or problems in the future. Further, most conferences have
a vendor area, which allows you to speak with many vendors in a short period of time and
make some comparisons of products. Three examples of conferences are the Winter Simu-
lation Conference (WSC) held in December at www.wintersim.org, the Summer Computer
Simulation Conference typically held in July and sponsored by the Society for Modeling and
Simulation International at www.scs.org, and the Simulation Solutions Conference usually
held in the spring at www.simsol.org.

Simulation
12-23
Another source of information is simulation software surveys that periodically appear in
professional society journals. For example, OR/MS Today [23], a publication of INFORMS
(Institute for Operations Research and the Management Sciences), reported a simulation
software survey in December of 2005, which has since been updated on their Web site [24].
For further reading about simulation modeling in general, please consult some of the
textbooks previously referenced in this chapter [1–4,25].
References
1. Banks, J. et al., Discrete-Event System Simulation, 4th ed., Pearson Prentice Hall, Upper
Saddle River, 2005, chap.1–4, 11.
2. Leemis, L.M. and Park, S.K., Discrete-Event Simulation: A First Course, Pearson Prentice
Hall, Upper Saddle River, 2006, chap. 1.
3. Kelton, W.D., Sadowski, R.P., and Sturrock, D.T., Simulation with Arena, 4th ed.,
McGraw-Hill, New York, 2007, chap. 2,3,6.
4. Fishman,
G.S.,
Discrete-Event
Simulation:
Modeling,
Programming,
and
Analysis,
Springer-Verlag, New York, 2001, chap. 5.
5. Bapat, V. and Sturrock, D., The Arena product family: Enterprise modeling solutions,
in Proc. 2003 Winter Simulation Conference, Chick, S. et al., Eds., IEEE, Piscataway,
2003, 210.
6. April, J. et al., Practical introduction to simulation optimization, in Proc. 2003 Winter
Simulation Conference, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 71.
7. Rohrer, M.W., Maximizing simulation ROI with AutoMod, in Proc. 2003 Winter Simula-
tion Conference, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 201.
8. Applied Materials, Inc., AutoMod Web site, Available at www.automod.com, accessed
2/17/06.
9. Krahl, D., Extend: an interactive simulation environment, in Proc. 2003 Winter Simulation
Conference, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 188.
10. Imagine That, Inc., Extend Web site, available at www.imaginthatinc.com, accessed
2/17/06.
11. Henriksen, J.O., General-purpose concurrent and post-processed animation with Proof, in
Proc. 1999 Winter Simulation Conference, Farrington, P.A. et al., Eds., IEEE, Piscataway,
1999, 176.
12. Norgren, W.B., Flexsim simulation environment, in Proc. 2003 Winter Simulation Confer-
ence, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 197.
13. Flexsim Software Products, Inc., Flexsim Web site, Available at www.ﬂexsim.com, accessed
2/17/06.
14. Henriksen, J.O. and Crain, R.C., GPSS/H: a 23-year retrospective view, in Proc. 2000
Winter Simulation Conference, Joines, J.A. et al., Eds., IEEE, Piscataway, 2000, 177.
15. Bloechle, W.K. and Schunk. D., Micro Saint Sharp simulation software, in Proc. 2003
Winter Simulation Conference, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 182.
16. Micro Analysis and Design, Inc., Micro Saint Web site, available at www.maad.com,
accessed 2/17/06.
17. Harrell, C.R. and Price, R.N., Simulation modeling using ProModel, in Proc. 2003 Winter
Simulation Conference, Chick, S. et al., Eds., IEEE, Piscataway, 2003, 175.
18. PROMODEL Corporation, ProModel Web site, available at www.promodel.com, accessed
2/17/06.
19. Delmia Corporation, QUEST Web site, available at www.delmia.com, accessed 2/17/06.
20. SIMUL8 Corporation, SIMUL8 Web site, available at www.simul8.com, accessed 2/17/06.

12-24
Operations Research Methodologies
21. Geer Mountain Software, Corp., STAT::FIT Web site, available at www.geerms.com,
accessed 2/17/06.
22. Lanner Group, WITNESS Web site, available at www.witness-for-simulation.com accessed
2/17/06.
23. Lionheart Publishing, OR/MS Today Web site, available at www.lionhrtpub.com/
ORMS.shtml, accessed 8/24/06.
24. OR/MS Today, Simulation software survey Web site, available at www.lionhrtpub.com/
orms/surveys/Simulation/Simulation.html, accessed 8/24/06.
25. Law, A.M. and Kelton, D.W., Simulation Modeling and Analysis, 3rd Ed., McGraw-Hill,
Boston, 2000.

13
Metaheuristics for Discrete
Optimization Problems
Rex K. Kincaid
College of William and Mary
13.1
Mathematical Framework for Single
Solution Metaheuristics ........................... 13-3
13.2
Network Location Problems ...................... 13-3
13.3
Multistart Local Search ........................... 13-5
13.4
Simulated Annealing ............................... 13-6
13.5
Plain Vanilla Tabu Search ........................ 13-8
13.6
Active Structural Acoustic Control (ASAC) ... 13-10
13.7
Nature Reserve Site Selection .................... 13-13
Problem Formulations and Data Details • MCSP
Interchange Heuristics • Tabu Search for MCSP •
Heuristics for MECP and LMECP
13.8
Damper Placement in Flexible Truss
Structures ........................................... 13-21
Optimization Models • Tabu Search for DPP •
Computational Results
13.9
Reactive Tabu Search.............................. 13-29
Basic Features of Reactive Tabu Search •
Computational Experiments
13.10
Discussion ........................................... 13-35
References .................................................... 13-36
A metaheuristic is a general algorithmic framework for ﬁnding solutions to optimization
problems. Within this framework underlying local heuristics are guided and adapted to
eﬀectively explore the solution space. Metaheuristics are designed to be robust. That is,
solutions to a wide variety of optimization problems can be found with relatively few mod-
iﬁcations. Examples of metaheuristics include simulated annealing (SA), tabu search (TS),
iterated local search (ILS), evolutionary algorithms (EA), evolutionary programs (EP),
greedy randomized adaptive search procedure (GRASP), memetic algorithms (MA), vari-
able neighborhood descent (VND), genetic algorithms (GA), scatter search (SS) and ant
colony optimization (ACO). In addition, there are many hybrid approaches combining fea-
tures of several of these techniques simultaneously.
Standard references are available for each particular metaheuristic as well as reference
and textbooks that compare and contrast a wider variety of metaheuristic approaches.
My aim here is not to do a complete survey, but to address how to attack a practical
discrete optimization problem (DOP). Metaheuristics are one popular algorithmic approach
for generating solutions to DOPs. Consequently, the primary focus of this chapter is how
to develop a particular metaheuristic for a DOP.
13-1

13-2
Operations Research Methodologies
Metaheuristics attempt to ﬁnd better solutions to a DOP by either maintaining a collection
of solutions, a population, or a single solution. Examples of population-based metaheuris-
tics include EA, EP, GA, ACO, and MA. The interested reader is referred to Goldberg [1],
Back [2], Dasgupta and Michalewicz [3], and Moscato [4] for more information on these
and other population-based techniques. Single solution metaheuristics include SA, TS, ILS,
GRASP, VND, and SS. The books by Glover and Laguna [5] and van Laarhoven and Aarts
[6] cover tabu search and simulated annealing, respectively. The books by Michalewicz and
Fogel [7] and Hoos and Stutzle [8] compare and contrast a wide variety of metaheuristics.
In particular, Ref. [8] provides a uniﬁed framework for 12 metaheuristics. Throughout this
chapter the discussion is limited to single solution metaheuristics.
The focus of this chapter is on how to ﬁnd high-quality solutions for a DOP. The intended
audience is an individual with a DOP in need of answers to the following questions. If I have
a feasible solution how do I know if it is a good one? Which solution strategy should I start
with if I don’t want to invest too much of my time? How do I improve upon these solutions to
develop better ones? Further, the DOP of interest is assumed to be nonstandard in some way
so that traditional approaches (branch and bound or linear programming relaxations) are
not applicable or do not provide tight enough bounds on the optimal value. The contents of
this chapter reﬂect my own preferences for how to go about answering the above questions.
I begin by developing a neighborhood structure for traversing the space of feasible solutions
and embed this within a multistart local search heuristic (see Sections 13.3 and 13.7) with a
random initial feasible solution. (It may not always be possible to generate random feasible
solutions, but for the problems I have faced it has been.) After implementing multistart, I
now have some idea of the diversity of local optima both with respect to solution value and
placement within the feasible space. Next, I embed this same neighborhood structure within
a simulated annealing heuristic. I use a standard set of parameter values (see Section 13.4).
I am not interested in ﬁne-tuning the search parameters. I simply want to get an idea
if the search terrain is diﬃcult. If I am not happy with the solutions I have found with
SA, I develop a tabu search heuristic. A basic tabu search (see Sections 13.5 and 13.6)
is straightforward to implement given a previously deﬁned neighborhood structure. In my
experience, tabu search is able to ﬁnd the same quality (typically with respect to objective
value but may include other desirable features as well) of solutions as simulated annealing
but in signiﬁcantly less time and with signiﬁcantly fewer function evaluations (typically an
order of magnitude decrease). If basic tabu search is unable to improve upon the solutions
found by multistart and simulated annealing, then I begin adding more advanced features
to tabu search (see Sections 13.8 and 13.9).
The chapter is organized as follows. First, a framework for single solution metaheuris-
tics is given. A description of a prototypical class of DOPs (network location problems)
is provided next. Following this section are three sections describing multistart heuristics,
simulated annealing, and tabu search, respectively. Section 13.3 on multistart heuristics uti-
lizes the notation developed in Section 13.2 to explain ﬁrst-improving and best-improving
two interchange heuristics. Section 13.4 on simulated annealing provides an outline of two
basic implementations of one approach to SA and provides recommendations on parame-
ter selections. Section 13.5 presents the basic features of tabu search. The following three
sections contain applications of metaheuristics in the location of actuators for noise control
(Section 13.6), selecting nature reserve sites (Section 13.7), and the placement of vibrational
dampers in large ﬂexible space structures (Section 13.8). Section 13.6 is a nonstandard loca-
tion problem with a complex performance measure. Traditional approaches are not applica-
ble and the utility of a basic tabu search is demonstrated. The nature reserve site selection
problem in Section 13.7 applies two known location models in an attempt to ﬁnd meaning-
ful solutions. The performance of multistart heuristics, tabu search, and branch and bound

Metaheuristics for Discrete Optimization Problems
13-3
are compared. Section 13.8 describes an unusual location problem. A traditional optimiza-
tion model is developed. LP-relaxations, branch and bound, simulated annealing, and tabu
search are highlighted as solution techniques. A recency-based diversiﬁcation scheme, an
advanced feature of tabu search, is shown to be necessary so that high quality solutions can
be uncovered. Section 13.9 presents additional features to augment the basic tabu search
described earlier in Section 13.5. We close with a short discussion in Section 13.10.
13.1
Mathematical Framework for Single
Solution Metaheuristics
Metaheuristics can be divided into those that work with a collection of solutions and those
that are guided by a single solution. For those that work with a single solution consider the
following deﬁnitions and notation. Without loss of generality, we assume that the perfor-
mance measure is to be minimized.
A state space, , is the set of all feasible states (solutions). A state S denotes an ele-
ment of . To qualitatively distinguish between the states we deﬁne a function c. The
domain of c is  and its range is typically R (the set of real numbers). c(S) is the cost
of state S. A move set Δ deﬁnes the set of moves from one state to another. The entries
in Δ are denoted by δ. The set of allowable moves from a state S is denoted Δ(S). Note
that δ ∈Δ(S) maps  →. The outcome of applying all moves δ ∈Δ(S) to S deﬁnes
the set of states reachable from S. The set of reachable states is typically referred to as the
neighborhood of S. The value of a move δ(S), c(δ(S)) −c(S), is the diﬀerence between the
cost of the new state and the cost of S. Thus, moves are improving if their value is negative
(downhill).
Each metaheuristic begins with an initial state S0 ∈, chosen at random or constructed
algorithmically. Each metaheuristic generates a sequence of moves, δ1,δ2, . . ., δn, which deter-
mines a sequence of states through which the search proceeds. The mechanism by which
a move is selected is one of the critical diﬀerences between competing metaheuristics. For
example, if the search mechanism is a greedy local search, then at state St the next move
selected is δt where
c(δt(St)) =
min
δ∈Δ(St)c(δ(St))
13.2
Network Location Problems
We will use network location problems as a platform for illustrating the development of
metaheuristics throughout this chapter. For additional information on network location
theory the interested reader is referred to Mirchandani and Francis [9] and Nickel and
Albandoz [10]. Network location theory typically addresses the question of where to locate
desirable facilities to interact with customers who must travel to the facility to obtain
a particular good or service. In these instances, a customer is assumed to prefer nearby
facilities. The prototypical problems for desirable facilities are the p-median and p-center
problems. In the p-median problem, the location of p facilities is determined so that the
average travel distance for all customers is minimized. However, there are certain types of
service for which all customers desire to be within a minimum threshold distance. Emergency
medical services is one such example. The performance measure in the p-center problem is
to minimize the maximum distance (or time) traveled by a customer to the nearest facility.

13-4
Operations Research Methodologies
Although both the p-median and p-center problem are NP-complete, in practice each of
these problems has many high quality local optima that are not diﬃcult to uncover.
Facilities that customers prefer not to have nearby are called undesirable or obnoxious.
Examples of undesirable facilities include garbage incinerators, hazardous waste dumps, and
nuclear power plants. Both the scarcity of available land for siting undesirable facilities
and an increase in public awareness of the potential dangers of living near such facilities
have made locating undesirable facilities a diﬃcult and sensitive problem. Many disparate
factors—costs, politics, and social perceptions—aﬀect the location choice. In addition, unde-
sirable facilities may be mutually obnoxious as in the case of missile silos.
Formulating an optimization problem associated with the location of desirable or undesir-
able facilities varies with respect to several criteria including the feasible region—discrete or
continuous; the number of facilities to be located—one or many; the performance measure—
single or multiobjective; the feasible solution space—a tree (acyclic) network or a network
that allows cycles; and the interactions considered—the distances between facilities or the
distances between facilities and customers. For the desirable facility location problem we
consider discrete, multiple facility, single objective models in which only the interactions
between the facilities and the customers are of importance. For the undesirable facility
location problem we consider discrete, multiple facility, single objective models in which
only the interactions between the facilities are of importance. All six models appear in the
literature and are summarized below. In each model, we are given an undirected network
and the corresponding shortest path distance matrix. For the undesirable facility problems
the performance measure seeks a set of facilities X of size p that maximizes some function
of the distance between the members of X while for the desirable facility problems we seek
a set of facilities X of size p that minimizes some function of the distance between the
members of X and the members of the (discrete) feasible region denoted, without loss of
generality, as the vertices of the network, V .
There is not complete agreement in the literature on the names for these problems. The
names given above are consistent with most of the literature. The p-dispersion problem max-
imizes the minimum distance between any two facilities. For example, missile silos should
be placed as far from one another as possible to reduce the number of facilities destroyed in
a single attack. The p-defense-sum maximizes the average distance between facilities. These
two models, perhaps because of their applicability, have received the most attention in the
literature. See Kincaid and Yellin [11] for additional information on the undesirable location
problems. Although there are no reported applications of the p-dispersion-sum problem to
date, a closely related problem formulation in Kincaid and Berger [12] has been used to
locate structural dampers on large, ﬂexible, space, truss structures (see Section 13.8 of this
chapter). As is seen in Table 13.1, three of these four problems are strongly NP-hard. Opti-
mization problems (more precisely the decision problem associated with an optimization
problem), which are still NP-hard even when all numbers in the input are bounded by some
TABLE 13.1
Discrete Network Location Problems
Name
Performance Measure
Complexity
p-median
minX⊆V

j∈V mini∈Xdij
NP-Complete
p-center
minX⊆V maxj∈V mini∈Xdij
NP-Complete
p-dispersion
maxX⊆V mini∈X minj∈Xdij
NP-Complete
p-dispersion-sum
maxX⊆V mini∈X

j∈X dij
Str. NP-Hard
p-defense
maxX⊆V

i∈X minj∈Xdij
Str. NP-Hard
p-defense-sum
maxX⊆V

i∈X

j∈X dij
Str. NP-Hard

Metaheuristics for Discrete Optimization Problems
13-5
polynomial in the length of the input, are labeled strongly NP-hard. The interested reader
is referred to the seminal reference on complexity theory [13].
13.3
Multistart Local Search
Local searches are the centerpiece of many metaheuristics for DOPs. A typical local search
is an interchange heuristic. For network location problems an interchange heuristic begins
with a feasible solution—a set of p locations—and a set of potential site locations, V .
Although we use V , the set of vertices in the underlying network for our set of potential
site locations, the set can be any predetermined ﬁnite set of points on the network. Next,
we deﬁne a set of moves Δ so that we can examine other nearby solutions. Here, we focus
on interchange heuristics that swap one site currently in the solution with one site not in
the solution, although other move sets are clearly possible. Consequently, a move δij ∈Δ
interchanges i, one of the p chosen sites, and j, one of the potential sites not yet chosen
(j ∈V \{k}; often we will abuse notation and simply write j ∈V −k). Algorithm 13.1 below
describes a rudimentary two interchange search. At Step 2, if the ﬁrst move (interchange)
that leads to an improvement in the objective value is chosen then we have a ﬁrst-improving
two interchange search. We note that in our version of a ﬁrst-improving search one complete
iteration searches all of Δ with, possibly, multiple improving moves accepted. An alternative
at Step 2 is to search among all possible interchanges and select an interchange that yields
the largest improvement in the objective value. This latter version is called a best-improving
two interchange search. Since we will consider no other move sets beyond the two inter-
change we can drop, without confusion, the two interchange identiﬁer and refer to these
heuristics simply as ﬁrst-improving and best-improving. Further comparisons of these two
approaches are made in Section 13.7. Algorithm 13.1 stops when no more improvements can
be made, that is, when X(t) is a local optima with respect to the two interchange neigh-
borhood. Ideally, the solution, X(t), at which the search stops is a global optima. However,
there is no guarantee of this for any of the location problems we have discussed nor is it
true for most DOPs of interest.
Algorithm 13.1: Rudimentary Two Interchange Search
Step 0: Choose X(0) by selecting p entries in V and set t ←0.
Step 1: If no move δi,j ∈Δt is improving, stop.
X(t) is a local optima with respect to Δt.
Step 2: Choose an improving δ∗
i,j ∈Δt.
Step 3: Apply δ∗
i,j to X(t) yielding X(t+1). Increment t if no moves remain in Δt.
Return to Step 1.
Algorithm 13.1 becomes a multistart local search by wrapping a loop around it that
generates distinct starting solutions for Step 0. These solutions can be randomly generated
or determined by another heuristic. In the latter case, the heuristics are often constructive.
Greedily adding a site that improves the performance measure the most until p sites are
selected is one such example. Multistart local searches are easy to implement and provides
a metric for testing the quality of solutions generated by any competing metaheuristic. One
way to make an eﬀective comparison is to assume a budget for the number of solutions to
be examined. If the metaheuristic cannot produce better solutions than a multistart local
search within the same budget the approach should be abandoned for that problem.

13-6
Operations Research Methodologies
13.4
Simulated Annealing
Simulated annealing (SA) was ﬁrst proposed and used in statistical mechanics by Metropolis
et al. [14]. Not until Kirkpatrick et al. [15] and Cerny [16], however, was simulated anneal-
ing implemented as a heuristic for an NP-Hard combinatorial optimization problem, the
traveling salesman problem. There are two basic approaches to implementing SA. The ﬁrst
is championed by van Laarhoven and Aarts [6], while the second is well represented by
Johnson et al. [17]. It is the latter approach that we will take. SA consists of a loop over a
block of code that generates a random move δ ∈Δ(S). If δ(S), where S is the current state
of the procedure, yields a decrease in the objective function value, c(δ(S)) < c(S), then δ(S)
is immediately accepted as the new current state. However, if δ(S) yields an increase in the
objective function value, then δ(S) still may be accepted with some positive probability as
the new current state. Typically (and in our implementation) this probability remains ﬁxed
for a predetermined number of random moves and is then decreased in a uniform manner.
Markov chain theory provides a basis for determining when simulated annealing produces
good results. Lundy and Mees [18] as well as Aarts and Van Laarhoven [19] provide detailed
descriptions as to how Markov chain theory is used to prove convergence of simulated
annealing. In particular, they show that at each temperature (each probability of acceptance
of a non-improving state) a homogeneous Markov chain is generated; the transition matrix
associated with the Markov chain is irreducible; and the state space is connected. Unfortu-
nately, theoretical convergence to a global optimum is only guaranteed when the number
of states visited is exponentially long. Consequently, practical implementations require a
knowledge of how to determine problem parameters so that the number of examined solu-
tions is not too many and the ﬁnal solution quality is high. For an extensive survey of results
obtained by simulated annealing algorithms see Collins et al. [20] and Johnson et al. [17].
The paradigm for simulated annealing is physical annealing. If a metal or a crystal is
cooled too quickly (a greedy search), then the end product is poor. The metal may be brit-
tle and break easily or the crystal structure may be irregular. Slower cooling is the preferred
method. As a result of this physical analogy the problem parameters in SA are referred to
as the initial temperature, T0, the ﬁnal temperature, Tf, and the cooling rate, Tempfac-
tor. However, as nice as this analogy is there are limitations. Johnson et al. [17] provide
an extensive set of experiments with SA’s problem parameters for the graph partitioning
problem (GPP). Selecting the values for these problem parameters can be a daunting task.
We will discuss two readily available implementations. One is given in Numerical Recipes
[21] and the other is given in Johnson et al. [17].
Although the physical annealing paradigm is helpful in explaining the process to scientists,
it does not really explain how the algorithm works in practice. I like to think of simulated
annealing as a game similar to one I played in the car on long trips. There are a set of
holes and a ball bearing in an enclosed hand-held game. The goal is to get the ball bearing
into a particular hole without getting stuck in any of the other holes. There are plastic
ridges that form a maze and, as you tilt the game, you are able to move through the maze.
In simulated annealing’s version of this game the objective function is traced out by the
plastic ridges and you try to get the ball bearing to end up in the hole associated with the
lowest valley (global minimum). The only control parameter is how vigorously you shake
the game to move the ball bearing out of undesirable holes. However, you are not allowed
to look at the game to see if you are in the correct hole (valley). The game proceeds by
successively decreasing the amount of shaking. Eventually the shaking is unable to allow an
escape from a hole. Hopefully, you are in the hole that is the global minimum. Of course
this oversimpliﬁes simulated annealing but the visual image is a good one.

Metaheuristics for Discrete Optimization Problems
13-7
The
parameter
T0
drives
the
probability
of
accepting
nonimproving
states—
exp(−Δobj/T). For a ﬁxed change in the objective function, T0 controls the amount of
shaking you are allowed to do. Moreover, if T0 is large enough you can shake your way
out of any local optima. Tempfactor denotes the rate at which T is to be decreased, while
sample size denotes the maximum number of moves sampled at each T. As there is no
formal stopping condition, a ﬁnal value for the parameter T, Tf, must be speciﬁed. In Ref.
[21], additional parameters (nsucc and nover) are included for exiting the local improvement
phase of the algorithm early if a pre-speciﬁed (nover) number of successful (nsucc) moves
has been achieved. If these parameters are not chosen appropriately, simulated annealing will
produce poor results and exceedingly long execution times. A pseudocode for this version
of simulated annealing is given below.
T = T0
nsucc = 0
while (T > Tf) do
do i = 1, sample size
[generate random move, compute Δobj]
if (Δobj < 0 or uniform(0,1) < exp(−Δobj/T))then
[update system, obj = obj + Δobj]
nsucc = nsucc +1
endif
if (nsucc ≥nover) exit loop
continue
T = T ·Tempfactor
end do
The parameters sample size, Tempfactor, nover, T0, and Tf are often problem speciﬁc.
As was shown by Lundy and Mees [18], the value of T0 must be much larger than the
diﬀerence between the worst objective function value and the best objective function value
to guarantee convergence to a globally optimal solution. Rather than specify a value for Tf
a maximum iteration bound as suggested in Ref. [18] may be used. Some experimentation
is involved in selecting the appropriate combination of Tempfactor, and sample size, the
maximum number of moves generated at each value of the parameter T (length of the
Markov chain).
Johnson et al. [17] discuss the usefulness of these parameters for the GPP. Moreover,
rather than use the somewhat unnatural parameters nsucc and nover they show that a
lower initial value of T0 achieves the same eﬀect. In particular they found that rather
than selecting a value of T0 at which nearly every move (uphill or downhill) is selected,
a value of T0 at which 30%–60% of the moves examined are accepted is more eﬀective.
They recommend a value of 40% for GPP and call this parameter initprob. In addition,
experiments with Tf led them to conclude that simulated annealing for the GPP should
be stopped when ﬁve consecutive temperatures led to less than 2% (labeled minpercent in
their parameter list) of the moves being accepted. Borrowing a term from physical annealing,
the authors refer to this as the SA process being frozen. In the pseudocode below frozen
is a counter used to determine when the frozen state occurs. Similar experiments led to
recommended values of Tempfactor = 0.95 and sample size = 16 N (here N = Δ(S) is the
neighborhood size). Lastly, the recommended choices for the above parameter values are
such that a suite of replications should be performed and the best solution found during

13-8
Operations Research Methodologies
the num reps replications recorded. Below is the pseudocode for this version of simulated
annealing.
Randomly select or construct an initial solution S.
Select T0 so that initprob = 0.40.
while (frozen < 5) do
do i = 1, sample size
[generate move, compute Δobj]
if (Δobj < 0 or uniform(0,1) < exp(−Δobj/T))then
[update system, obj = obj + Δobj]
nsucc = nsucc + 1
if current solution is best save it.
endif
continue
T = T ·Tempfactor
if nucc/sample size < minpercent
frozen = frozen + 1
else frozen = 0
end do
The robustness of the parameter choices given by Ref. [17] for the GPP is diﬃcult to prove.
However, independent of Ref. [17], Kincaid [22] found that similar values worked well for two
location problems. Moreover, in Kincaid and Yellin [11] this version of simulated annealing
was shown to work well for yet another location problem. Of course there is no guarantee
that these parameter choices are best for every DOP, but they have proven robust enough to
serve as an excellent starting choice. In addition to the selection of these parameter values,
Ref. [17] also gives critical recommendations on how to improve simulated annealing.
For example, rather than randomly select a move δ ∈Δ(S) use a random permutation
of Δ(S) and examine each move in order. Second, as we are starting at a value of T0
that does not randomly reorder the entire structure of an initial solution, starting from a
better than random solution is preferred. In particular, if the starting solution is constructed
by a process that is distinct from the mechanism used to improve solutions in simulated
annealing. Third, computing exp(−Δobj/T) is expensive. The recommended approach is to
implement a table lookup scheme, which cuts the computation time of exp(−Δobj/T) by
a third. For further details about these speedups see Ref. [17].
13.5
Plain Vanilla Tabu Search
Tabu search and, more generally, adaptive memory programming continues to have startling
success in eﬃciently generating high-quality solutions to diﬃcult practical optimization
problems. Glover and Laguna [5] provides 42 vignettes, each of which describes a diﬀer-
ent application of tabu search by researchers and practitioners. The applications include
standard problems such as the p-median problem, job shop scheduling, and the quadratic
assignment problem, as well as unusual applications including polymer chemistry, forest
management, and the control of ﬂexible space structures (see Section 13.8).
If the topology of the solution space is such that there are no steep valleys or ridges then
only the most basic features of tabu search may be needed. We describe such a basic tabu
search in this section and call it plain vanilla tabu search (PVTS) to clearly distinguish it
from the more general methodology that tabu search encompasses.

Metaheuristics for Discrete Optimization Problems
13-9
The deﬁnitions and notations that follow are taken from Glover [23,24] and Kincaid and
Berger [12]. For network location problems PVTS begins with an initial state S0, p entries
of V , chosen either at random or by some constructive heuristic. Next PVTS generates a
sequence of moves δ0, δ1, . . . which determines a sequence of states through which the search
proceeds. The mechanism by which a move is selected is crucial. PVTS, at state St, selects
the greatest available one-move improvement. That is, a next move δt is deﬁned to be
c(δt(St)) =
min
δ∈Δ−τ+αc(δ(St))
As will be described below, τ restricts the move set and α extends the move set. Without
the addition of τ and α the above equation would describe the move selection criterion in
a greedy local improvement search and the search would conclude with a local optima. In
eﬀect, PVTS selects the largest downhill move, if one is available, or the least uphill move.
Hence, there is no natural stopping criteria and the length of the sequence of generated
moves must be speciﬁed.
PVTS attempts to avoid entrapment in local optima by keeping a list of previously
selected moves and deleting them from the move set Δ for a state S in the hope of avoiding
a return to a previously observed state. A list of move attributes of length tabusize, called
a tabu list and designated by τ, is constructed and updated during each iteration of the
search. An example of a move for a location problem is to swap a pair of vertices (i, j) ∈V ,
where i is currently selected and j is not. The attributes of this move are the indices i and j,
and the designations drop for i and add for j are often applied.
An admissible move in PVTS is a move δ ∈Δ that is either not on the tabu list τ or
one that meets an aspiration level criterion. A best admissible move is one that yields the
greatest improvement or the least degradation in the objective function value (or any other
performance measure). The best admissible move is appended to the tabu list after the
examination of all moves δ ∈Δ. Once the tabu list becomes full, which occurs after tabu
size iterations, the oldest move is removed. Moreover, it is possible for a move in τ to be
selected provided that it meets one (or more) aspiration level criterion. The purpose of an
aspiration level criterion is to choose good moves by allowing the tabu status of a move to
be overridden if the move satisﬁes a particular condition. We label the set of tabu moves
that meet an aspiration criterion α. The goal is to do this in a manner that retains the
search’s ability to avoid returning to a previously generated state (cycling).
Tabu restrictions and aspiration level criteria play a dual role in constraining and guiding
the search process. Tabu restrictions allow a move to be regarded as admissible if they do
not apply, while aspiration criteria allow a move to be regarded as admissible if they do
apply. There are at least three ways to avoid cycling (see Ref. [23]), which prevent the move
from S to δ(S):
1. δ(S) has been visited previously.
2. δ has been applied to S previously. [retracing]
3. δ−1 has been applied to S previously.
The ﬁrst criterion is the strongest and is the only one of the three that fully assures cycling
will not occur. This criterion would require the search to store every state visited. Typically,
this requires too much memory and too large a computational eﬀort to perform the necessary
comparisons. If a tabu move is deemed admissible, provided that only condition (2) is met,
then it is possible to reverse the move as soon as it is made. Hence, it makes sense to
test condition (3) in addition to (2). By using conditions (2) and (3) to avoid cycling and
allowing a tabu move that leads from state x to S(x) to be made—unless the move from
S(x) to x had occurred previously—the attempt to prevent a return to an earlier location

13-10
Operations Research Methodologies
will be more strongly supported than relying solely on condition (2). Conditions (2) and (3)
together are a close approximation of (1).
As it is impractical to store a large number of states, we need to ﬁnd a way to store the
same information in a compressed form. One possibility for compressed storage is to use the
objective function value. The standard aspiration level criterion is an example of storing
information in such a compressed form. This criterion has the property that if the tabu
move (i, j) yields a state with the best objective function value encountered in any previous
iteration, then the move is allowed. If δ ∈τ ⊆Δ(S) and c(δ(S)) < cbest, then δ is admissible.
The tabu status is overridden, because the move δ meets the aspiration level criterion.
Cycling is avoided here since if δ(S) had appeared previously, then c(δ(S)) < cbest would
not be possible. This particular aspiration criterion is easy to implement but treats all states
with the same objective function value as if they were the same state. However, Kincaid and
Berger [12] found this simple aspiration level criterion to perform as well as more complicated
ones for a damper placement problem (see problem DPP1, described in Section 13.8).
13.6
Active Structural Acoustic Control (ASAC)
The optimization model in this section has a distinct network location ﬂavor (Section 13.2),
but the performance measure is nonstandard. Once a set of candidate locations is chosen,
the solution of a complex valued least squares problem must be solved to provide the metric
for comparing sets of candidate locations. In addition, the utility of plain vanilla tabu search
(Section 13.5) is demonstrated.
Most air carriers use turboprop aircraft to transport passengers between local or nearby
airports and their hubs. Turboprop aircraft are preferred for short hops due to their superior
fuel eﬃciency at low speeds. However, the beneﬁts of decreased fuel cost and increased travel
range come at the price of increased levels of cabin noise and vibration. Turboprop noise
levels are typically 10–30 dB louder than commercial jet noise levels.
Turboprop noise is fundamentally diﬀerent from jet noise. The former is caused by peri-
odic sources such as rotating engine parts, rotating props, and by vortices shed from the
propellers. Turboprop noise spectrum is dominated by a few low-frequency tones. On the
other hand, jet noise is caused by random sources such as the mixing of high speed, high
temperature gases. Jet noise spectrum is dominated by high frequency broad band noise.
Jet noise can be controlled by putting sound absorbing material in the walls of the fuselage.
This passive treatment is not eﬀective for propeller noise.
Interior noise levels are of concern to commercial airlines for three reasons. First, the
airlines must provide a comfortable working environment for their employees. Occupational
Safety and Health Administration (OSHA) mandates ear protection or restricted working
hours for employees who are exposed to high levels of noise. Second, the airlines must
attract customers. Uncomfortable noise levels may encourage the public to seek alternate
travel options. Third, the airlines must consider safety issues. High noise and vibration levels
cause fatigue in the airframe as well as fatigue in the passengers and crew.
Active control methods attempt to counteract propeller noise by introducing a secondary
(or multiple secondary) sound source that is of the same frequency and amplitude as the pro-
peller noise, but is 180◦out of phase. Thus, in theory, the result is a complete silencing of the
propeller noise source. We consider an acoustic control model in which the control inputs,
which are used to reduce interior noise, are applied directly to a vibrating structural acoustic
system (e.g., an aircraft fuselage). The feasibility of this approach has been demonstrated by
using measured data from the aft cabin of a Douglas DC-9 fuselage. A simpliﬁed model of the
ASAC problem is used to explain the process. Assume that an aircraft fuselage is represented

Metaheuristics for Discrete Optimization Problems
13-11
as a cylinder with rigid end caps and that a turboprop propeller is represented as an acoustic
point source with a frequency related to the propeller blade passage frequency. Piezoelectric
actuators bonded to the fuselage skin are represented as line force distributions in the axial
and azimuthal directions. With this simpliﬁed model, the point source produces predictable
pressure waves that are exterior to the cylinder. These periodic pressure waves cause pre-
dictable structural vibrations in the cylinder wall and predictable noise levels in the interior
space. The noise level at any interior microphone location depends on the control forces
applied at each piezoelectric actuator location. For a given set of sensor and actuator loca-
tions, the control forces that minimize the average acoustic response are easy to calculate.
However, methods for choosing good locations for the sensors and actuators are needed.
Here we will ﬁx p microphone locations (sensors) and seek to determine the best set of
k actuator locations chosen from a set of q ≫k potential locations for the active controls.
The goal is to determine the force inputs and sites for the piezoceramic actuators so that
(1) the interior noise is eﬀectively damped; (2) the level of vibration of the cylinder shell
is not increased; and (3) the power requirements needed to drive the actuators are not
excessive.
To model objective (1) a p by k complex transfer matrix H is determined. Each entry
of H provides the contribution of each of the potential k actuators sites to interior noise
reduction at each of the ﬁxed p error microphone locations whenever a unit amplitude
input is applied to actuator site k. Objective (2) requires more information which will not
be described here. Lastly, since power is directly related to the magnitude of the force inputs
required, objective (3) can be measured by considering either the Euclidean or max norm
of the actuator force input vector.
A complex transfer matrix H can be generated (either experimentally or with a math-
ematical model) for the complete set of q actuator sites. A column j of H yields the con-
tribution of a unit amplitude input at actuator site j in controlling the noise at each of
the p sensors. Hence, given a particular set of k columns (actuators) the submatrix of H
induced by these columns, Hk, is the associated transfer matrix. Once a particular set of k
actuator sites have been selected then a force (amplitude and phase) must be selected for
each actuator so that the resultant force cancels out ⃗p as nearly as possible. In the acoustic
literature the force vector ⃗f is chosen as the solution of the complex least squares problem
min
f ∥Hk ⃗f + ⃗p ∥2
(13.1)
We note that although the usual convention is to have a subtraction operator in
Equation 13.1, the acoustic literature uniformly displays the plus operator. Thus, we adhere
to acoustic literature convention. The solution to Equation 13.1 is found by solving
Hk∗Hk ⃗f = −Hk∗⃗p
(13.2)
for ⃗f (* denotes the complex conjugate transpose). This means that each time a diﬀerent
set of k columns of H is selected a new complex least squares problem must be solved.
If there exists a column of H which is collinear with ⃗p, then we would need only pick
this single actuator site, driven at the appropriate amplitude, and total silencing would
be achieved. Typically this situation does not exist. However, a likely set of columns for
a starting solution are the k columns of H most nearly collinear with ⃗p. To ﬁnd these
columns, we determine the angle between each actuator j site and ⃗p. This is done by
calculating the dot product of each column of the H matrix, ⃗hj, with the vector p and
dividing that product by the product of the norms of each. That is, the expression:
⃗hj · ⃗p
∥⃗hj∥2∥⃗p ∥2

13-12
Operations Research Methodologies
PZT Locations
2nd control cross
section at x   2.17m
1st control cross
section at x  1.41m
FIGURE 13.1
Laboratory cylinder for ASAC test.
would calculate the cosine of the angle between the two vectors. As the actuators that would
be most eﬀective, theoretically, in silencing the outside noise source would be those either
in phase or 180◦out of phase, we seek cosine values close to either 1 or −1. Thus, the
k initial actuators are chosen that have the absolute value of the cosine closest to one. This
greedy heuristic for constructing initial solutions performed better than randomly selecting
actuator sites.
The performance of this set of k columns (or any set of k columns for that matter) is
measured on a decibel scale given by the expression below.
10 log10
∥Hk ⃗f + ⃗p ∥2
∥⃗p ∥2
(13.3)
The decibel value computed in Equation 13.3 compares the interior noise norm with actuator
controls in place with the norm of the uncontrolled interior noise. Thus, a negative decibel
(dB) level signiﬁes a decrease in interior cylinder noise due to the control eﬀects of the actu-
ators. An analogous decibel expression computes the vibration level on the cylinder shell.
As we mentioned earlier, a set of experiments with a laboratory test article (see Fig-
ure 13.1) have been performed and reported on in Palumbo et al. [25]. For these experiments
the transfer matrices were generated experimentally. The predicted performance of the best
actuator sites found for the mathematical model by tabu search are shown to correlate well
with the actual measured performance.
The cylinder in Figure 13.1 is a cartoon of the laboratory test article. The cylinder in
the laboratory tests is 3.6 m long and 1.68 m in diameter. The outer shell is a nine-layer
ﬁlament wound graphite epoxy composite. Total skin thickness is 1.7 mm. In an eﬀort to
make the cylinder as similar as possible to a commercial fuselage the cylinder is stiﬀened
with composite stringers and ring frames. To complete the fuselage eﬀect, three inner trim
panel sections are attached. Each trim panel has a honeycomb core sandwiched between
two graphite epoxy laminate sheets. The interested reader is referred to Lyle and Silcox [26]

Metaheuristics for Discrete Optimization Problems
13-13
TABLE 13.2
Experimental Results with Actuator/Sensor Location
PVTS
PVTS
Modal Analysis
Freq. (Hz)
Predicted (dB)
Measured (dB)
Measured (dB)
210
−5.3
−4.4
−2.1
230
−3.8
−2.5
−1.2
275
−5.7
−3.9
−2.7
for a detailed description of the laboratory test article. The monopole noise source was a
100 W electrodynamic loudspeaker. An array of eight piezoceramic patches was installed
on one of the trim panels. A set of six microphones was swept along a boom throughout
the interior of the cylinder (near the shell) and data were taken at 462 locations. The 462
transfer functions at these locations were used to construct transfer matrices (462 by 8)
for three frequencies—210, 230, and 275 Hz—of interest. The 230 Hz case has weak acoustic
modes but strong structural modes.
The laboratory test was done with only eight potential actuator sites of which four were to
be selected. Table 13.2 catalogs the performance of PVTS versus standard acoustical tech-
niques in the selection of the best set of eight sensors. The acoustical techniques (cf. [25])
involve engineers examining visual and numerical displays of the modal decomposition of the
interior pressure ﬁeld ⃗p and the individual actuator responses. Dominant modes in the inte-
rior pressure ﬁeld are matched with dominant modes in the actuator responses. The columns
marked PVTS compare the predicted noise reduction versus the actual noise reduction mea-
sured in the laboratory. The column marked modal analysis shows noise reduction levels
achieved in laboratory tests run prior to the PVTS selection of actuator and sensor locations.
The good news in Table 13.2 is that the measured values for PVTS are nearly 1 dB better
than the modal decomposition analysis solutions. Hence, even for this relatively easy case
(only four actuators) the optimization approach proved superior. A second positive feature
is that the predicted performance, although always at least a 1 dB overestimate, is still a
reasonable predictor of the measured performance. This provides evidence that the model
assumptions made in the optimization approach are not too far oﬀ. A third positive result of
these experiments is that the acoustic engineers now believe that the optimization approach
is an important contributor in ﬁnding good solutions to the ASAC problems.
Much additional work has been done on this problem. The methodology in this section
was used to select actuator locations for an actual aircraft that was ﬂight tested. Upon
completion of the ﬂight test experiments the technology was transferred for development to
a private corporation. For additional information see Refs. [25] and [27–30].
13.7
Nature Reserve Site Selection
A variety of heuristic solution techniques are highlighted in this section for two distinct
network location models—the maximal covering species problem (MCSP) and the maximal
expected coverage problem (MECP). For MCSP, a set of computational experiments eval-
uating the performance of competing multistart searches (Section 13.3) with tabu search
(Section 13.5) is provided. For MECP, results for a tabu search are presented and are
contrasted with results for a linear programming approximation for MECP. In both MCSP
and MECP experiments with two types of tabu list structures are given.
In recent decades, human population growth and land development patterns have threat-
ened to destroy many ecosystems and, as a direct result, the species that are part of and
depend upon these ecosystems for survival. Consequently, nature reserves and parks are
being identiﬁed and established as much for species preservation and the maintenance of

13-14
Operations Research Methodologies
biological diversity as they are for human enjoyment. As selection of nature reserve sites
has become more complex, methods drawn from operations research have been introduced
to aid in the solution of this problem. One approach has been the species set cover prob-
lem (SSCP) [31], which seeks to minimize investment while protecting all target species.
A related model is the selection of a network of reserve sites so as to maximize preserva-
tion for a speciﬁed level of investment. This latter model is closely related to the maximal
covering location problem. Consequently it is referred to as the MCSP [32,33]. In both the
MCSP and the SSCP, a species is said to be covered if the species is present in at least
one site selected for the reserve system. Further, these models assume that the presence or
absence of a species in a site can be known with certainty. If, instead, we assume that the
information as to where a species resides in an ecosystem is not known with certainty or if
the foraging range of a species makes it diﬃcult to pinpoint a home site, then probabilistic
models are needed. One such model is the MECP. To be able to formulate an MECP the
probability that a species is present in a given site must be speciﬁed. These probabilities
are assumed to be independent. That is, the probability that one species is located in a
particular site is independent of any other species’ presence or absence.
We develop and test the performance of heuristics for both MCSP and MECP. Following
the literature [34,35] we use the Oregon Terrestrial Vertebrate dataset [36] as a testbed.
This dataset has location information for 426 terrestrial vertebrate species for the state
of Oregon. The data is organized by partitioning the state into 441 regular hexagons (see
Figure 13.2). The bulk of the material found in this section appears in [37].
Both MCSP and MECP are NP-Hard [34] and thus solution by exact methods (e.g., branch
and bound) can be troublesome for problem sizes of interest to the ecological commu-
nity. Hence, heuristics are popular alternatives and include greedy adding and interchange
heuristics, which require signiﬁcantly less time, but may yield suboptimal solutions. (See
Refs. [35] and [38] for a more complete summary and comparison of previous computational
4
5
7
8
9
10
11
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
401
402
403
404
405
406
407
408
409
410
411
412
417
418
419
420
421
422
423
424
425
428
429
430
431
432
433
436
437
438
439
FIGURE 13.2
Hexagon partition and numbers for the state of Oregon.

Metaheuristics for Discrete Optimization Problems
13-15
experience.) In addition to heuristics, relaxation and approximation techniques are available.
Of particular interest here is the linearized maximal expected covering problem (LMECP)
developed by Camm et al. [34] as an approximation to MECP.
An additional issue is the utility of the information provided by the optimization model to
the ecological community. Both MECP and MCSP are, at best, approximations of the true
optimization model. Hence, the provision of a single solution is problematic. As in many
public sector decision problems, tradeoﬀs are essential and the ability to provide a decision
maker with a collection of high-quality solutions from which a network of reserve sites can
be selected is a worthy goal.
13.7.1
Problem Formulations and Data Details
The MCSP can be formulated as the following binary integer program [39,40].
max

i∈I
yi
subject to
yi ≤

j∈Mi
xj,
∀i ∈I

j∈J
xj = k
yi, xj ∈{0, 1},
∀i ∈I, ∀j ∈J
where yi and xj are binary decision variables, deﬁned such that
yi =

1
if species i is covered in the reserve network
0
otherwise
xj =
1
if site j is selected for the reserve
0
otherwise
In addition, I is the set of all species, J is the set of all potential reserve sites, and Mi is
the set of sites in which species i resides.
The MCSP objective function maximizes the total number of species covered by the
reserve system. The ﬁrst constraint states that for a species to be covered, it must be present
in one or more of the sites selected for the reserve system. The next constraint restricts the
number of sites selected to k. Finally, the last constraint requires that all decision variables
are binary. Extensions of this model are possible [41] and include budget constraints and
objective function coeﬃcients to reﬂect the conservation value of a species.
The MECP can be formulated as a binary nonlinear integer program [34]
max

i∈I
⎡
⎣1 −

j∈J
(1 −pijxj)
⎤
⎦
subject to

j∈J
xj = k
xj ∈{0, 1},
∀i ∈I, ∀j ∈J
where xj are binary decision variables, deﬁned such that
xj =

1
if site j is selected for the reserve
0
otherwise

13-16
Operations Research Methodologies
In addition, I is the set of all species, J is the set of all potential reserve sites, pij is the
probability that species i exists and will survive at site j, and k denotes the number of
sites to be selected for the reserve. The MECP objective function maximizes the expected
number of species covered by the reserve system. The ﬁrst constraint restricts the number of
sites selected to k. The remaining constraints require that all decision variables are binary.
The ﬁnal model is the LMECP established in Ref. [34]. The development of LMECP is not
duplicated here as the ﬁnal model provides little insight without the complete derivation.
LMECP is a binary integer linear program. The error in the approximation of LMECP to
MECP can be controlled by the number of breakpoints. A more accurate approximation
of the MECP is obtained with more breakpoints, but at the expense of additional decision
variables. Following Ref. [34], the number of breakpoints is set at 10 for each run of the
model for all values of k tested. Similar extensions for MECP exist as for MCSP [41].
The Oregon Terrestrial Vertebrate dataset [36] has location information on 426 species
spread over a grid made up of 441 hexagons (see Figure 13.2). For each hexagon, a proba-
bility is assigned to each species existence. The probabilities are categorized as follows:
• 0.00: Not present—The habitat is unsuitable for the species.
• 0.10: Possible—The habitat is of questionable suitability for the species, but
expert local opinion is that the species might occur in the site.
• 0.80: Probable—The site contains suitable habitat for the species and there have
been veriﬁed sightings in nearby sites; local expert opinion is that it is highly
probable that the species occurs in the site.
• 0.95: Conﬁdent—A veriﬁed sighting of the species has occurred in the last two
decades.
Notice that the time span for the most conﬁdent classiﬁcation is 20 years, while survival
data on the species is not included.
For the MCSP the probability that species i is found in hexagon (site) j must be either
0 or 1. Following Ref. [35] let entries of 0.00 and 0.10 be mapped into 0 and entries of 0.80
and 0.95 be mapped into 1. Unfortunately, such certainty is not guaranteed, leading to the
designation of soft for these datasets. There are a variety of reasons for this lack of certainty.
Terrestrial vertebrates are ambulatory and the location in which they are observed may not
be their primary residence. In addition, the certainty of the observation itself may be in
question if the species is viewed from a distance or if it is based upon an interview with a
time lag. Lastly, population dynamics may aﬀect the certainty of a species location.
13.7.2
MCSP Interchange Heuristics
We test the performance of both ﬁrst-improving and best-improving heuristics for MCSP
on the terrestrial vertebrate species survey data from the Biodiversity Research Consortium
for the state of Oregon [36]. We note that a similar set of experiments was done in Ref. [35]
for the same dataset. These authors found that when compared to the 18 heuristics tested in
Ref. [38], the number of species covered by a ﬁrst-improving heuristic was always equal to or
better than the number covered by the best solution found across all 18 heuristics, for each
value of k from 1 to 24. We provide a similar set of experiments as in Ref. [35]. However, we
also compare ﬁrst-improving’s performance against best-improving as well as embed ﬁrst-
improving within a tabu search in Section 13.4. For comparative purposes, MCSP is solved
exactly. The results are recorded in Tables 13.3 through 13.7.
The ﬁrst two columns in Table 13.3 list the number of sites to be selected and the corre-
sponding optimal objective function value. Only 23 sites are needed to cover all 426 species.

Metaheuristics for Discrete Optimization Problems
13-17
TABLE 13.3
Comparative Results of Optimal
Solution versus Interchange Heuristics
Best-Imp.
Best-Imp.
First-Imp.
Sites
Opt
1 Start
Multi-Start
Multi-Start
1
254
254
254
254
2
318
318
318
318
3
356
356
356
356
4
374
374
374
374
5
384
383
384
384
6
390
388
390
390
7
395
393
395
395
8
400
397
400
400
9
403
400
403
403
10
406
401
406
405
11
408
403
408
408
12
410
406
410
410
13
412
407
412
412
14
414
410
414
414
15
416
411
416
416
16
418
412
417
417
17
419
413
419
418
18
420
414
419
420
19
422
415
421
421
20
423
416
422
422
21
424
418
423
422
22
425
419
423
423
23
426
420
424
424
24
426
421
425
425
25
426
421
426
426
TABLE 13.4
Frequency of Best
Observed Solutions for First-Improving
Sites
# Reps
Freq.
1
1
50
2
3
13
3
1
48
4
1
41
5
1
43
6
2
25
7
6
9
8
21
1
9
11
3
10
1
9
11
12
3
12
12
3
13
3
3
14
1
1
15
21
1
16
3
5
17
3
8
18
25
2
19
47
1
20
47
1
21
17
4
22
19
5
23
7
8
24
7
5
25
7
5
TABLE 13.5
Comparison of First-Improving and Tabu Search Solutions
First-Imp
TabuA
TabuB
Sites
Opt
50 Reps
2k
Asp
2k
Asp
16
418
417
418
0
418
1
17
419
418
419
0
419
1
18
420
420
420
0
420
0
19
422
421
421
0
421
0
20
423
422
422
0
422
0
21
424
422
423
0
423
0
22
425
423
424
0
424
1
23
426
424
425
0
425
1
24
426
425
426
0
426
2
TABLE 13.6
Tabu Search Results for MECP
k
MECP
Locations of Sites
1
217.4
319
2
288.5
135, 395
3
330.7
55, 135, 438
4
354.2
55, 135, 375, 438
5
368.1
55, 135, 274, 375, 438
6
376.7
55, 121, 135, 274, 375, 438
7
384.2
47, 55, 135, 289, 319, 395, 438
10
398.4
9, 46, 135, 268, 274, 345, 357, 375, 428, 440
13
407.2
9, 20, 27 121, 135, 268, 274, 314, 345, 357, 375, 428, 440
15
411.9
9, 46, 75, 121, 135, 141, 268, 273, 314, 324, 345, 357, 375, 428, 440
The third column of Table 13.3 records the objective for a single replication of the best-
improving heuristic for k = 1, . . ., 18 when the ﬁrst k sites are chosen as the initial starting
solution. The progress of the search is even more surprising as this is such a poor starting
solution. All of the ﬁrst k sites are clumped together in the northeast corner of Oregon
(see Figure 13.2). The maximum relative error between the best-improving solution and the
optimal solution is 1.67%.
Columns 4 and 5 of Table 13.3 record the best solutions found with 50 restarts of the
best-improving and ﬁrst-improving heuristics, respectively. Starting solutions for each of
the multiple starts are obtained by randomly permuting the 441 sites and then selecting
the ﬁrst k indices as the initial set of selected sites. In column 5 the optimal solution was

13-18
Operations Research Methodologies
TABLE 13.7
Tabu Search Results for MECP—with Replications
Previous
Avg
Tabu
Avg
Avg
k
Best
MECP
Tenure
# Asp
Itrs
7
384.2
384.11
20
0.6
18.7
383.95
10
0.1
18.1
384.22
4
3.1
19.5
10
398.4
398.22
20
0.7
18.2
398.36
10
1.5
21.7
398.36
5
5.7
22.0
13
407.2
407.23
20
0.9
27.6
407.20
10
0.9
27.3
407.22
7
6.8
25.6
15
411.9
411.67
20
1.8
26.6
411.67
10
1.3
26.5
411.84
8
5.2
24.3
Number of sites
Avg. number of Iterations
5
10
15
2
4
6
8
10
12
Best-improving
First-improving
FIGURE 13.3
Average number of iterations: ﬁrst-improving versus best-improving.
found at least once (see Table 13.4) out of the 50 starts for all values of k from 1 to 15. For
k > 15 the best solution diﬀers by at most two sites from the optimal solution. For k > 18
neither heuristic is able to uncover the optimal solution. Table 13.4 records the frequency
with which these best solutions are found for ﬁrst-improving search over the 50 replications.
How do the performances of ﬁrst-improving and best-improving compare? The solution
quality is nearly identical. See columns 4 and 5 of Table 13.3 for the best solutions found.
The boldface values highlight the diﬀerences between the two heuristics. Out of the 25
experiments there are 21 ties, three wins for best-improving, and one win for ﬁrst-improving.
However, both the average CPU time and the average number of iterations decreases dra-
matically for ﬁrst-improving when compared to best-improving as k increases. Instead
of increasing with the problem size parameter k, as in the case for best-improving, the
average number of iterations remains nearly constant between two and three for the ﬁrst-
improving heuristic. Figure 13.3 plots the average number of iterations required for both
best-improving and ﬁrst-improving, thus illustrating a dramatic diﬀerence between the two

Metaheuristics for Discrete Optimization Problems
13-19
approaches. Please recall that one iteration of ﬁrst-improving requires a complete perusal
of all possible two interchanges, and that multiple moves can be accepted during a single
iteration—for example, when k = 10 each iteration of ﬁrst-improving (or best-improving)
examines k(n −k) = 10(431) moves (interchanges). The diﬀerence is that best-improving
selects only one move in Δ while ﬁrst-improving may select many improving moves. The
CPU times for best-improving are growing much faster than for ﬁrst-improving, with nearly
an order of magnitude diﬀerence when k = 25. As both approaches provide solutions of sim-
ilar quality, ﬁrst-improving is the clear winner for MCSP.
13.7.3
Tabu Search for MCSP
Next we develop a simple tabu search for MCSP. We embed ﬁrst-improving within tabu
search. We note, however, that after the ﬁrst two to three iterations very few improving
interchanges exist. Consequently, after iteration three ﬁrst-improving behaves like best-
improving. The entire neighborhood is searched for every iteration to ﬁnd an improving
or at least a nonimproving solution. The question to resolve is whether the ability of tabu
search to move away from local optima and continue searching for other local optima will
outperform the ﬁrst-improving approach of the previous section [24,42].
One of the decisions that must be made to implement a tabu search is when to terminate
the search. We follow the typical practice and set an iteration limit. As the ﬁrst-improving
heuristic required between two and three iterations for each of the 50 random starts, we
let the maximum number of iterations be 100. Other decisions required to implement tabu
search include how long a move should remain tabu (also called the tabu tenure of a move)
and whether both the site dropped and the site added should be tabu or only one of the
pair. We experiment with two diﬀerent types of tabu moves. First, the complete move (drop
and add) is made tabu—TabuA. Second, only the dropped sites are tabu—TabuB. That
is, adding a dropped site back into the solution is tabu. Finally, we must decide what the
aspiration criterion will be and if it should be invoked. Again, following standard practice
[42] our aspiration criterion allows the tabu status of a move to be ignored if its objective
value is better than the current best objective value.
The ﬁrst-improving heuristic ﬁnds optimal solutions for all k ≤15 and the frequency with
which optimal solutions are uncovered is large (see Table 13.4) for k ≤6. Consequently,
we compare tabu search with ﬁrst-improving for all values of k > 15. Table 13.5 records
the objective value for both types of tabu lists, with a tabu tenure of 2k. These values
are compared to the optimal objective value (column 2) and to the best values found by the
ﬁrst-improving heuristic of the previous section (column 3). Both implementations of tabu
search ﬁnd the same or better objective values than 50 replications of ﬁrst-improving. TabuA
and TabuB yield the same objective values for all k. TabuA was relatively insensitive to the
tabu tenure parameter and values between k and 3k typically yield the same solutions. The
tabu tenure value was less robust for TabuB. We report values found with a tabu tenure of
2k for both TabuA and TabuB. The only exceptions are for k = 22, 23, and 24 for TabuB. For
k = 22 and 23 a tabu tenure of size k was best, while for k = 24 a tabu tenure of 1.5k yielded
the best result. As can be seen in column 5 and 7 of Table 13.5, no tabu moves in TabuA
met the aspiration criteria, while the aspiration criteria were met several times for TabuB.
In particular, the aspiration criterion for TabuB always led to the best observed solution
for the entire run. It is not surprising that TabuB utilizes the aspiration criteria since the
percentage of available moves held tabu by this approach is much greater than for TabuA.
How does the computational eﬀort for tabu search compare with our earlier winner—
ﬁrst-improving multistart? The average number of iterations needed by ﬁrst-improving is
nearly constant at 2.5 across all values of k. The average number of restarts, before the best

13-20
Operations Research Methodologies
observed value out of 50 restarts is found, is 15.7 for k between 11 and 25. For all k ≤15 the
best observed value over the 50 replications is optimal. But for k ≥16 it is not. Moreover, for
k ≥16 the number of replications needed to uncover the best observed solutions (see Table
13.4) varies between 1 and 47 (k = 19, 20). We argue that, a priori, it is impossible to know
how many restarts will be needed and since at least 47 must be done, 50 is a reasonable choice
for the number or restarts. How does this compare with tabu search? For TabuA the average
iteration at which the best solution is found is 19.8 with a range from 3 to 42. For TabuB
the average is 24.0 and the range is from 3 to 59. Since at least 59 iterations are needed,
60 iterations is a reasonable choice as an iteration limit for TabuB, while a 50 iteration
limit suﬃces for TabuA. (We note that if a best-improving search is embedded within our
tabu search 100 iterations are needed to ensure high quality solutions.) The CPU times
for TabuA (50 iterations) increases linearly from a time of 9.1 s when k = 16 to 13 s when
k = 24. TabuB follows a similar pattern with 15 s needed to complete 60 iterations of TabuB
for k = 24. Letting the tabu search CPU times serves as a constraint for ﬁrst-improving
then 25 restarts of ﬁrst-improving (on average) can be completed in 13 s (28 restarts in 15 s)
when k = 24. Only three restarts for best-improving can be completed in 15 s for k = 24.
Thus, TabuA and TabuB easily dominate best-improving when a CPU budget is imposed.
When restricted to 25 (or 28) restarts the results for ﬁrst-improving (column 3, Table 13.5)
remain the same with the exception of k = 19 and k = 20. The best observed solution has an
objective value of 420 for k = 19 and 421 for k = 20 when restricted to 25 (or 28) restarts.
We note as well that, like the multistart approach, tabu search uncovers a number of local
optima that may be recorded as the search progresses.
13.7.4
Heuristics for MECP and LMECP
Following Ref. [34], the resulting LMECP with 10 breakpoints for the Oregon Terrestrial
Vertebrate dataset has 2256 real variables, 441 binary variables, and 1279 constraints. The
AMPL model and data were obtained from the authors of Ref. [34]. The LMECP model
was run for various values of k. As was mentioned in Ref. [34], running the model for k = 7
sites was quite lengthy. This was true for our implementation as well with over four wall
clock hours required for k = 7 (computing environment was a network of Pentium 4s).
Next, we develop and test the performance of a tabu search for MECP. As we will see,
the solutions generated by these approaches are competitive with the approximate solutions
generated by LMECP and require far fewer computational resources. We begin with a
simple tabu search approach. The embedded heuristic is a best-improving two-interchange
heuristic described earlier for MCSP. Unlike MCSP, ﬁrst-improving found inferior solutions
(on average) when compared with best-improving for MECP. For example, 10 replications of
a ﬁrst-improving tabu search for k = 10 had an average best objective value of 397.97 while
10 replications of a best-improving tabu search for k = 10 had an average best objective
value of 398.36. While it is still true that a ﬁrst-improving tabu search requires only two to
three iterations to ﬁnd its best solution, we chose the higher solution quality for our reported
results in this section. As was the case for TabuA for MCSP, the solution quality found by
our tabu search for MECP is not particularly sensitive to the tenure of the tabu moves.
That is, tabu tenures from 6 up to 20 yield the same best solutions. We note, however, that
for shorter tabu tenures the aspiration criterion is invoked more frequently. The stopping
condition is as before—an iteration limit. For all k ≤15 the maximum iterations allowed
is 50. The same simple aspiration criterion as before is also used—the tabu status of a move
is ignored if it leads to a solution whose value is better than the current best. Initial solutions
are generated as we did for MCSP. Randomly permute the 441 sites and select the ﬁrst k
as the initial set of sites. Table 13.6 records the best solutions found by our tabu search.

Metaheuristics for Discrete Optimization Problems
13-21
The results for k = 1 to k = 7 in Table 13.6 are nearly identical to the solution reported in
Ref. [34] for LMECP. In Table 13.6 the sites that are diﬀerent than those chosen in Ref. [34]
are in boldface. The solution values in column 2 are identical to those in Ref. [34] except for
k = 6 where there is a diﬀerence of 0.2. One pair of sites that are diﬀerent (289 and 274) is
adjacent, so this is a small change. But the other pair (47 to 121) has a spatial separation
of three hexagons, which is a signiﬁcant change. Unlike LMECP computation time is not
restrictive so we report solutions on larger values of k as well. In general, heuristics for
the larger values of k run in minutes while LMECP took more than 4 wall clock hours
for k = 7.
Table 13.7 records experiments with three diﬀerent tabu restrictions, one row for each
type of tabu move in columns 3 through 6. In each case 10 replications are performed. Each
set of 10 replications takes roughly 15 min on a Pentium IV computer for k = 15. Computing
times are shorter for smaller values of k. The ﬁrst row, for each k, is the approach recorded
in Table 13.6—both the site added and the site removed are tabu. The second row, for
each k, makes it tabu to add a site that has been removed. The third row, for each k, makes
it tabu to remove a site that has been added. Column 3 gives the average best objective
value over the replications. Column 5 records the average number of tabu interchanges that
meet the aspiration criterion, while column 6 records the average iteration at which the best
solution was observed. Clearly the third method makes the greatest use of the aspiration
criterion, which can be partly explained by the restriction that the tabu tenure can be no
larger than k. If the tabu tenure is larger than k then no interchanges would be allowed. We
found that a tabu tenure of ⌈k/2⌉worked best. All implementations of tabu search were able
to uncover the best observed solution. Although all three versions of tabu search performed
well our recommendation is that the tabu restriction associated with row 3 of Table 13.7 is
the best (tabu to add a site that has been removed). Moreover, we note that for all three
tabu restrictions the solution quality is worse if the aspiration criterion is omitted.
13.8
Damper Placement in Flexible Truss Structures
The damper placement problem described in this section is similar to the p-dispersion-sum
problem (Section 13.2). A tabu search is developed for this problem that includes more
advanced features than those described in Section 13.5. In particular a recency-based diver-
siﬁcation scheme is shown to yield high-quality solutions. A comparison is made between
starting tabu search from a random set of damper locations versus using the result of a lin-
ear programming relaxation to generate an initial solution. In addition, tabu search results
are compared to an oﬀthe shelf branch and bound code as well as simulated annealing.
The demand for larger-sized space structures with lower mass has led to the development
of highly ﬂexible structures where, in eﬀect, every point can move relative to the next.
Traditionally, structural motion is viewed more simply in terms of a sum of several dozen or
more independent motions called natural motions. The problem of controlling the motion
of a ﬂexible structure is then reduced to controlling the natural motions. Associated with
each natural motion are three parameters: a mode that is a natural spatial shape, a natural
frequency that expresses the rate of oscillation, and a natural decay rate that is a measure
of the time required for the motion to decay. The contribution of each natural motion to
the overall motion depends on the degree to which it is excited by external forces.
The overall structural motion of a ﬂexible truss structure can be reduced by the use of
structural dampers that both sense and dissipate vibrations. We focus on where to locate
these dampers so that vibrations arising from the control or operation of the structure and
its payloads, or from cyclic thermal expansion and contraction of the space structure, can

13-22
Operations Research Methodologies
Manual
Optimal
FIGURE 13.4
CSI Phase I Evolutionary Model (CEM).
be damped as eﬀectively as possible. There are several mechanisms available for vibrational
damping. We consider the replacement of some of the truss members by active dampers
that sense axial displacement (strain) and induce a compensating displacement. A related
option is to replace some of the truss members with passive dampers which dissipate strain
energy due to their material properties. Each of these techniques for damping increases
the weight and cost of the truss structure, as well as changing the stiﬀness properties of
the structure. Hence, structural designers are required to locate as few dampers as possible
and still maintain an appropriate level of vibrational damping. The results in the following
sections are taken primarily from Kincaid [43].
13.8.1
Optimization Models
The CSI Phase I Evolutionary Model (CEM) in Figure 13.4 is an example of a large ﬂexible
space truss structure. (Please ignore the manual/optimal labeling in Figure 13.4 as it only
shows up in color. Moreover, the labeling is not referenced in the ensuing discussion.) Let
nmode denote the number of modes and nmemb denote the number of truss members in
the structure. A normal modes analysis of a ﬁnite element model of this structure yielded a
10 (nmode) by 1507 (nmemb) modal strain energy matrix. Let D denote this matrix with
row index set I and column index set J. The entries in the matrix have been normalized so
that each dij denotes the percentage of the total modal strain energy imparted in mode i
to truss member j.
In this section, we formulate two DOPs. The goal in each of the problems is to select p
truss members to be replaced by active (or passive) dampers so that the modal damping
ratio is maximized for all signiﬁcant modes. Maximizing the modal damping ratio is a widely
accepted goal in damper placement problems (cf. [44]). However, the modal damping ratio
is diﬃcult to determine explicitly and, consequently, the placement of active (or passive)
dampers has proved diﬃcult (cf. [45] and [46]). Both active and passive dampers dissipate
forces that are internal to the structure and are most eﬀective replacing truss members
with maximum extension or compression. The truss elements with maximum internal dis-
placement are those with the largest strain energy over all modes. Given a ﬁnite element
model and the results of a normal modes analysis, the modal strain energy in each candidate
location (truss member) for each signiﬁcant normal vibration mode can be estimated quite

Metaheuristics for Discrete Optimization Problems
13-23
accurately. The damping achieved with active dampers depends on the properties of the
damper and the control law that is implemented. We use a force-feedback control law (cf.
[46]) yielding damping ratios that are directly proportional to the fraction of modal strain
energy.
The ﬁrst damper placement problem (DPP1) assumes that the damping eﬀectiveness
of each active (or passive) damper is the same over all vibrational frequencies of interest.
Hence, the maximization of the modal damping ratio for all modes can be accomplished by
selecting the p damper locations that maximize the minimum sum of modal strain energy
over the p chosen locations. Padula and Sandridge [45] formulate DPP1 as a mixed binary
integer linear program. Their formulation is given below.
max
β
subject to
nmemb

j=1
dijxj ≥β
∀i = 1, . . ., nmode
nmemb

j=1
xj = p
β ∈R, xj ∈{0, 1}
∀j = 1, . . ., nmemb
Alternatively, DPP1 may be viewed as a combinatorial optimization problem. That is, given
D we seek to ﬁnd the nmode by p submatrix whose smallest row sum is as large as possible.
Let Z(X) = mini∈I

j∈X dij and let | · | denote the cardinality of the set X. Then DPP1
becomes
max
X⊆J
Z(X)
subject to
|X| = p
Heuristic algorithms for DPP1 were ﬁrst introduced by Kincaid and Berger [12] for a
tetrahedral truss structure developed for NASA at the MIT Space Engineering Research
Center (cf. [51]). This symmetric structure had 600 truss members and 34 vibrational modes
of interest. The heuristics tested in Ref. [12] included a variety of tabu search heuristics for
DPP1. DPP1 was studied as a maxminsum location problem on tree graphs in Ref. [47].
Characterizations of the optimal solutions for small values of p were obtained.
Unlike active dampers, each passive damper has a particular frequency, ˆω, at which its
damping eﬀectiveness is best. The damping eﬀectiveness, e(ω), deteriorates the further
the modal frequency, ω, is from ˆω. A typical graph of e(ω), where the y-axis e(ω), des-
ignates the fraction of mean strain energy dissipated by the passive damper, is roughly
ski-sloped shaped. Let ωi for all i = 1, . . ., nmode denote the frequencies of the nmode sig-
niﬁcant modes of vibration for CEM. We note that in the special case when ˆω is identical
for each of the passive dampers, and we replace each entry dij of D with e(ωi) · dij (where
e = (e(ω1), . . ., e(ωnmode))), then DPP1 is an appropriate model.
Although there are some manufacturing restrictions, passive dampers can be designed so
that ˆω occurs at nearly any frequency of interest. Table 13.8 lists the 10 modes considered
in the CEM model and their frequency ωi in Hz. In particular, we assume that passive
dampers can be designed so that ˆω is any one of the nmode frequencies (vibrational modes)
of interest. Further, we assume there is no advantage in designing the passive dampers to
have ˆω at a value other than ω1, . . ., ωnmode.

13-24
Operations Research Methodologies
TABLE 13.8
Ten Modes for CEM and Their Frequencies
Mode
1
2
3
4
5
6
7
8
9
10
ωi,Hz
1.498
2.434
2.590
4.994
5.648
5.740
6.150
6.726
6.801
7.830
To extend model DPP1, we make nmode copies of D and pre-multiply each row i of each
copy of D by ek(ωi)—the damping eﬀectiveness associated with mode i when the peak
design frequency is ωk. Each of the resulting nmode strain energy matrices corresponds
to one of the potential design variable values of ˆω. In addition, a new set of constraints
must be added to allow at most one of the xkj variables to be 1 for each j. The resulting
mixed binary integer linear program is given below. For CEM, there will be 15,070 binary
variables, 1 continuous variable, and 1518 constraints. We note that in practice this model
would require nmode prototype passive dampers to be built and tested, so that the damping
eﬀectiveness values are available. This formulation, DPP2, is new and is given below.
max
β
subject to
nmode

k=1
nmemb

j=1
ek(ωi)dijxkj ≥β
∀i = 1, . . ., nmode
nmode

k=1
xkj ≤1
∀j = 1, . . ., nmemb
nmode

k=1
nmemb

j=1
xkj = p
β ∈R, xkj ∈{0, 1}
∀j = 1, . . ., nmemb; k = 1, . . ., nmode
We illustrate DPP2 with the following small example. In this example, there are two
modes to be damped, and four truss members that may be replaced with p = 2 passive
dampers. The mean strain energy matrix D is

.25
.30
.20
.25
.10
.20
.50
.20

Each of the p passive dampers may be manufactured so that its peak damping eﬀective-
ness coincides with ω1 (mode 1) or ω2 (mode 2). Assume that e1(ω1) = 0.25, e1(ω2) = 0.20
e2(ω1) = 0.15, and e2(ω2) = 0.30. There are

4
2

=6 combinations of two truss members to be
replaced with passive dampers. Coupled with the two manufacturing design choices for each
passive damper, we get a total 24 possible solutions. Consider the solution that replaces
truss members (columns) 2 and 3 with passive dampers. The passive dampers replacing
truss members 2 and 3 are designed to damp most eﬀectively for mode 1 and mode 2,
respectively. The corresponding objective function value is
min{e1(ω1)d12 + e2(ω1)d13, e1(ω2)d22 + e2(ω2)d2}
= min{(.25)(.30) + (.15)(.20), (.20)(.20) + (.30)(.50)}
= .105

Metaheuristics for Discrete Optimization Problems
13-25
13.8.2
Tabu Search for DPP
The basic solution approach for TS consists of a construction phase that generates a starting
solution and an improvement phase that seeks to iteratively improve upon the starting solu-
tion. After max–it iterations of the improvement phase we do one of the following; intensify
the search by restarting the improvement phase at the current best solution: or diversify the
search by restarting the improvement phase in an unexplored region of the solution space;
or stop and display the best solution found.
TS begins with an initial state S0 ∈ that is either chosen at random or constructed
algorithmically. For our dataset, D, random solutions are easily generated. The p columns
are randomly chosen from the nmemb = 1507 available entries. Several possibilities exist for
constructing an initial state. One heuristic is to pick the p column indices corresponding to
the p largest entries of D. This heuristic did not perform any better than randomly selecting
p column indices of D. A second heuristic is based on solving the linear programming
(LP) relaxation of the integer programming formulation presented in Section 13.2. Solve
the LP-relaxation and pick the p largest of the nonzero decision variables in the LP optimal
solution as our initial state X0. Tables 13.9 through 13.11 catalog the performance of this
second heuristic.
The improvement phase is a greedy local improvement scheme. That is, from a state St, we
select a move that generates the greatest one move improvement in the objective function.
That is, we select a move δt from among all p · (nmemb −p) pairwise interchanges that
satisﬁes c(δt(S′
t)) = maxδ∈Δ(St) c(δ(St)). Once the interchange is made, the set of all pairwise
interchanges is generated again, and a move that improves the objective function the most is
selected. This process continues until there are no more local changes available that lead to
an improved solution. An important step in deﬁning a local optimization scheme is specifying
a method for perturbing the current solution to obtain other solutions. The neighborhood
structure we use in our implementation of TS is the p(nmemb −p) neighborhood structure.
The nmemb column indices of D are partitioned into two sets, set X of size p and set X′
of size (nmemb −p) (X′ is the complement of X in the column index set J). To generate
neighbors, we swap one entry from the set Xwith one entry from the set X′. As an example,
suppose we let J = {1, 2, 3, 4, 5} and p = 2. If X = {1, 3} and X′ = {2, 4, 5}, then one neighbor
would be X = {2, 3} and X′ = {1, 4, 5}. The size of this neighborhood is O(p(nmemb −p)).
Every member of the neighborhood is examined before the best admissible move is selected.
The primary goal of the short-term memory component of TS is to permit the heuristic
to go beyond points of local optimality, while still making high-quality moves. Choosing the
best admissible move is a critical step at each iteration of the improvement scheme. It is
TABLE 13.9
Best Objective Function Value Comparisons
P
LP
U. Bnd
IP/BB (%)
Tabu (%)
8
1.6144
1.6131
81.9
94.1
16
3.1629
3.1110
89.3
95.3
32
5.8867
5.8838
96.4
98.5
TABLE 13.10
Greedy Local Improvement Schemes with Diﬀerent Initial Solutions
P
Random %
Time (min)
IP/BB (%)
Time (min)
LP (%)
Time (min)
8
81.3
3
84.7
1
88.6
1
16
86.9
18.5
92.4
4.5
94.3
1
32
90.1
37
97.9
2
98.4
4

13-26
Operations Research Methodologies
TABLE 13.11
Performance of Tabu Search with Diﬀerent Initial Solutions
P
Random (%)
Time (min)
IP/BB (%)
Time (min)
LP (%)
Time (min)
8
88.6
14
84.7
14
94.1
14
16
95.3
210
3.3
56
95.3
56
32
98.2
270
97.9
57
98.5
57
this step that embodies the aggressive orientation of the short-term memory. Each of the
candidate moves is evaluated in turn. Initially, the evaluation of the candidate moves can
be based on the change produced in the objective function value. As the search progresses,
however, the form of evaluation employed becomes more adaptive and may incorporate
references to intensiﬁcation and diversiﬁcation.
Search methods that are based on local optimization often rely on diversiﬁcation strate-
gies to increase their eﬀectiveness in exploring the solution space. Some of the strategies
are designed with the chief purpose of preventing cycling, while others attempt to impart
additional robustness or vigor to the search. Kelly et al. [48] discuss two designs for diver-
siﬁcation strategies—recency-based and frequency-based. We use a recency-based diversiﬁ-
cation scheme in our implementation of TS.
A recency-based diversiﬁcation scheme is intended to direct the search to a solution that
is “maximally diverse” with respect to the local minimum most recently visited. There are
two important concepts in this scheme—separation and quality. First, two solutions are
considered to be increasingly diverse as their separation increases. Separation is measured
as the minimum number of moves (pairwise interchanges) necessary to get from one to
the other. Next we want an estimate of the probability that a particular solution will be
encountered in a set of solutions equally separated from a given solution. We assume that
the probability of traversing between any two solutions separated by a ﬁxed number of swaps
decreases with the diﬀerence between the two objective function values. Hence, among the
set of solutions separated from a given solution at a ﬁxed distance, we are interested in
those with the best (highest quality) objective function value. Therefore, a new solution
is considered to be maximally diverse from the current best solution if it is maximally
separated and the objective function value is close to, or better than, the objective function
value of our current solution.
For the damper placement problem associated with D, we let the separation between
two solutions A and B, d(A, B), be the minimum number of two interchanges needed to
produce solution B from solution A. Furthermore, we assume that highest quality objective
function values are more diﬃcult to obtain at a ﬁxed distance than other solutions. A state
S is described by (X, X′). Given two states SA and SB, if XA(i) ̸= XB(j) for all i, j = 1 . . . p
then d(A, B) = p. For D there are

nmemb −p
p

solutions at a distance p from any state X,
which are too many to consider. To reduce the number of solutions examined, we implement
a greedy search that interchanges each X(i) for all i = 1, . . ., p with each of the nmemb −p
indices in X′. For each i the interchange that improves the objective function value the
most or degrades it the least is selected.
In summary, our version of TS has a construction phase that generates an initial fea-
sible solution to DPP1 and DPP2. The improvement phase searches the entire O(nmemb
(nmemb −p)) move set (neighborhood) and selects the best admissible move. The tabu
size = 25 and the aspiration criterion is met if the new objective function value is larger
than the best objective function value yet generated. After repeating the improvement phase
max–it times, a recency-based diversiﬁcation scheme generates a new starting solution and
the process is repeated.

Metaheuristics for Discrete Optimization Problems
13-27
13.8.3
Computational Results
Metaheuristic search strategies such as simulated annealing and tabu search can be used
to generate solutions to the damper placement problem. However, by themselves these
metaheuristics provide little information concerning the quality of the solution found. A
traditional method for obtaining a bound on the optimal objective function value of a inte-
ger linear program is to solve the associated linear programming relaxation. That is, if xj
is restricted to be either 0 or 1, then it is relaxed by replacing the 0, 1 restriction with
0 ≤xj ≤1. The objective function value associated with the optimal solution to the linear
programming relaxation then serves as an upper bound on the optimal objective function
value for the original integer linear program if the objective was to maximize some linear
function.
The optimal objective function value of the linear programming (LP) relaxation of DPP1
and DPP2, then, provides an upper bound on the optimal objective function value for DPP1
and DPP2. In addition, if the optimal solution to the LP relaxation resulted in all xj having
values of 0 or 1, then we would have the true optimum to DPP1 or DPP2. It is unusual for
this to occur, but it does show the strength of using an LP relaxation strategy. We make
use of the upper bound obtained via the LP relaxation of DPP1 and DPP2 to evaluate the
performance of our metaheuristics. Generally, there is no guarantee that this will be a tight
upper bound but, as can be seen in Table 13.9, for DPP1 associated with the CEM, the LP
relaxation does indeed provide a tight upper bound.
A traditional solution strategy for binary integer linear programs that makes use of lin-
ear programming relaxations is branch and bound. Given a solution to an LP relaxation,
a typical branch and bound scheme ﬁxes one or more of the fractional design variables
to either 0 or 1, resolves the LP relaxation, and repeats this process until an all binary
solution is found (a feasible solution to the original binary integer linear program). At this
point the branch and bound process may continue ﬁxing fractional design variables on other
unexplored branches until the optimal binary solution is found, or stop with the current
feasible (suboptimal) solution. For more details on branch and bound the interested reader
is referred to Parker and Rardin [49].
We solved the LP relaxation of DPP1 using a commercially available branch and bound
code for linear integer programs. This code provides (slightly) improved upper bounds
(column 3 of Table 13.9) as well as a feasible solution to DPP1. The objective function
values listed in Table 13.9 under the IP/BB column were generated in this manner. All
objective function values in Tables 13.9 through 13.11 are given as a percentage of the
best-known upper bound (column 3 of Table 13.9). Table 13.9 compares the quality of
solutions generated by solving DPP1 with this branch and bound scheme (with a limit
of 10,000 iterations) to tabu search. The solutions generated in column 4, labeled Integer
Program/Branch and Bound (IP/BB) took roughly 3000 times as long to generate as those
in column 6 for tabu search. Column 1 in Table 13.9 (as well as Tables 13.10 and 13.11)
designates the number of active dampers to be placed. Column 2 of Table 13.9 lists the
optimal objective function value of the LP relaxation of DPP1 (an upper bound on the
optimal value of DPP1), while column 3 is the upper bound generated during the branch
and bound scheme (a slight improvement over the LP relaxation value). Columns 4 and
5 of Table 13.11 provide the best objective function values generated by the branch and
bound scheme and tabu search, respectively. When p = 32 Table 13.11 illustrates that the
optimal objective function value of DPP1 lies between 5.7943 (98.5%) and 5.8838, and
that the solution corresponding to 5.7943 is feasible to DPP1 (all entries are 0 or 1). As
can be seen from Table 13.11, tabu search consistently outperforms the branch and bound
approach.

13-28
Operations Research Methodologies
Tabu search can be used to further improve the branch and bound solution of DPP1 or
the LP relaxation solution of DPP1. In the latter case, fractional solutions will be present
and a mechanism for choosing a subset of the optimal decision variables must be found.
We picked the p (where p = 8, 16, or 32) decision variables with the largest nonzero value
(closest to 1). For example, when p = 8 the LP solution had 12 nonzero decision variables
in the optimal solution. Of these 12, 5 had a value of 1. When p = 32 there were even fewer
choices to be made. The optimal solution of the LP relaxation of DPP1 had only 35 nonzero
decision variables of which 29 had a value of 1.
Tables 13.10 and 13.11 summarize the performance of tabu search under three diﬀerent
initial (starting) solutions—random (columns 2 and 3), the DPP1 solution generated by
branch and bound (column 4 and 5), and the LP relaxation of DPP1 (columns 6 and 7).
Reported timings are for a 16 MHz Intel 386-class microcomputer. The solutions generated
by branch and bound for DPP1 in Table 13.9 were computed on a CONVEX computer in
about 4 min. This corresponds to approximately 200 h of computational eﬀort on the 386
microcomputer. Table 13.10 gives the best objective function values (as a percent of the
best known upper bound) obtained using a greedy local improvement search (tabu size = 0,
no aspiration, and no diversiﬁcation), as well as the time required for convergence to a local
optimum. Table 13.11 catalogs the additional improvement gained by implementing tabu
search versus a greedy local improvement search. When p = 16 and p = 32 (p = 8) and one of
the hot starts (LP relaxation or IP/BB) is used our implementation of TS sets max–it = 100
(50). In addition, when p = 8 and p = 16 we allow one restart using the recency-based diver-
siﬁcation scheme of Section 13.8.2. The quality of solutions when p = 32 is so good after 100
iterations that the recency-based diversiﬁcation and restart are not implemented. For the
randomly generated starting solutions we allowed TS to run longer—7 restarts when p = 16
and 5 restarts when p = 32. The diversiﬁcation scheme is most eﬀective with the random
starting solution version of TS. For example, when p = 8, the best solution found in the ﬁrst
set of max–it = 50 iterations was achieved at iteration 23 with a value of 1.313 (81.4%); the
diversiﬁcation scheme generated a new starting solution with a value of 1.326 (82.2 %); and
the best solution found during the ﬁnal set of max–it iterations was achieved at iteration 5
with a value of 1.429 (88.6%).
Table 13.12 records the performance of simulated annealing for DPP1. Column 4 is the
average (over the number of replications given in column 2) of the ﬁnal objective function
value as a percentage of the best-known upper bound (column 3 of Table 13.9). For simulated
annealing this typically is not the best objective function value generated, and column 3
records the best value ever generated over the given number of replications. Although Ref.
[17] reports that starting SA with a good initial solution produced better results than using
randomly generated starting solutions, this was not the case for DPP1. We used the p largest
nonzero entries from the optimal solution of the LP relaxation of DPP1 and better results
were not obtained. Simulated annealing is predicated upon a certain amount of randomness
and as observed in Ref. [17] either many replications or one extremely long run should
be performed. We have opted for 10 replications (except for p = 32 for which a single run
took 10 h) instead of one long run. In addition, we have used the set of parameters given in
Ref. [17] for our implementation of simulated annealing. Johnson et al.’s [17] experiments
are extensive and the resulting set of parameters is quite robust. Column 5 gives the average
CPU times for the simulated annealing runs. When multiplied by the number of replications
this makes simulated annealing 10 to 100 times slower than tabu search with nearly the
same objective function values.
Both TS and SA solutions select truss members to replace with dampers in the same
general area of the CEM truss. The TS solution contains four longeron truss members near
the tower, two diagonal truss members near the cross piece in front of the tail, and two

Metaheuristics for Discrete Optimization Problems
13-29
TABLE 13.12
Simulated Annealing for CSI-Phase I Design
P
Reps
Best Obj (%)
Final Obj (%)
Avg CPU (min)
TS Best (%)
8
10
89.2
88.0
169
94.1
16
10
95.4
94.1
310
95.3
32
1
98.5
97.8
626
98.5
diagonal truss members in or near the tail. The SA solution contains two longeron and one
diagonal truss members near the tower, three diagonal truss members near the cross piece
in front of the tail, and two diagonal truss members in or near the tail. Hence, the SA
solution places more dampers (ﬁve) in the tail end of the structure and uses more diagonal
truss members (six) than the TS solution.
We do not report on the computational experiments for DPP2 here. However, the LP
relaxation becomes an even better starting solution as p increases. (It takes roughly ﬁve
passive dampers to get the same performance as one active damper resulting in larger values
of p.) In fact, a greedy local improvement scheme would have produced the same results as
TS when p = 80.
As a result of the computational experiments, several conclusions may be drawn. The LP
relaxation of DPP1 and DPP2 provide good, low cost (computationally) upper bound on
the optimal value of the DPP problems for CEM. The combination of the LP relaxation
hot start and tabu search yields the highest quality solutions and the smallest run-times.
Simulated annealing produces nearly as high of quality solutions as tabu search, but, for
DPP1, is about 100 times slower.
13.9
Reactive Tabu Search
The utility of several advanced features of tabu search is highlighted in this section. A
dynamic memory structure is explained and tested as well as a frequency-based diversiﬁ-
cation scheme. The main goal of this section is to illustrate the salient features of reactive
tabu search. We do this by examining the terrain of a function f : [−10, 10] × [−10, 10] →R.
As we can plot the domain and range of f together in R3 we are able to visualize key
features of the search mechanism. The function we chose is a frequent test problem for
global (continuous) optimization routines (see Hansen [50], Jansson and Knuppel [51], and
Levy et al. [52]). The function is
f(x, y) =
 5

i=1
i(cos((i −1)x + i))
 ⎛
⎝
5

j=1
j(cos((j + 1)y + j))
⎞
⎠;
x, y ∈[−10, 10]
and is referred to as Levy No. 3 in the literature.
The Maple command plot 3d(sum(i*cos((i-1)*x+i),i=1..5)*sum(j*cos((j+1)*
y+j), j=1..5), x=-10..10, y=-10..10, grid=[50,50]); generates the plot in Fig-
ure 13.5 for f(x, y) over the interval [−10, 10] for both x and y. The resolution is a grid of
50 points along each axis (every 0.4 units) for a total of 2500 plotted points.
For our computational experiments we require a DOP. To discretize the domain of
f(x, y) for x, y ∈[−10, 10] we impose a regular grid of 800 points along each axis (every
0.025 units) for a total of 640,000 points. We were unable to plot f(x, y) at this resolution
but Figure 13.5 provides a reasonable picture of the discrete valued function at 2500 points.
It is known that Levy No. 3 has 9 global minima and 760 local minima. The nearest points

13-30
Operations Research Methodologies
0
50
100
150
y
0
5
10
x
0
2
4
6
8
10
50
100
150
10
5
FIGURE 13.5
Levy No.3 evaluated over 50 by 50 grid.
TABLE 13.13
9 Points on Grid
Closest to 9 Global Optima
x
y
f(x,y)
4.975
−1.425
−176.540
−1.300
−1.425
−176.505
−7.600
−1.425
−176.458
4.975
4.850
−176.397
−1.300
4.850
−176.363
−1.300
−7.700
−176.354
−7.600
4.850
−176.316
−7.600
−7.700
−176.307
to the 9 global optima on the 800 by 800 grid are given in Table 13.13. We don’t know how
many of the 640,000 points are local minima but we suspect that it is at least 760. Nearly
all the results in this section are taken from Kincaid and Laba [29].
13.9.1
Basic Features of Reactive Tabu Search
We begin by describing the neighborhood structure we imposed on the 800 by 800 grid
version of Levy No. 3. Given an interior point (x, y) of the grid the neighbors of this point
are all of the compass points—North, Northeast, East, Southeast, South, Southwest, West,
and Northwest—generated by adding or subtracting the grid size (0.025) from (x, y). For
example, the grid point (x, y + 0.025) is the North neighbor of (x, y). At each iteration of
tabu search the eight neighbors of the current point are evaluated and the best neighbor
among the non-tabu and tabu neighbors that satisfy the aspiration criteria is selected (where
best means lowest f(x, y) value). The aspiration criteria checks to see if the function value
at a tabu neighbor is better than the best observed function value. If so, the tabu neighbor
satisﬁes the aspiration criteria. Once a best neighbor is selected it becomes the new current
point and the (x, y) coordinates of the best neighbor are placed on the tabu list.
The tabu list is a collection of (x, y) coordinates of previously selected best neighbors. To
detect whether it is necessary to increase (or decrease) the number of points that are stored

Metaheuristics for Discrete Optimization Problems
13-31
on the tabu list we must keep track of points that are visited more than once. The idea is
that if the search continues to return to the same point several times then it is likely that
the search is cycling and one of the functions of a tabu list is to avoid cycling. Here, instead
of maintaining the frequency with which each point (640,000 of them) is uncovered by the
search, we maintain the frequency distribution of collections of points. In particular, we
place a coarser 50 by 50 grid over the ﬁner 800 by 800 grid. Each point of the coarse grid is
mapped to a 16 by 16 set of grid points (256 point in total) on the ﬁne grid. We maintain the
frequency data for each of the 50 by 50 grid points (2500 total points). We call each of these
coarse grid points a zone. Moreover, we make further use of these zones in conjunction with
the tabu list. A pass through reactive tabu search may terminate for a variety of reasons
(to be described later) and restart at a new point. Upon completion of a single pass the
frequency data on the 2500 zones are used to update a matrix describing which zones are
to remain tabu in subsequent passes. In particular if the frequency of a zone on a pass is
greater than 16 (the number of grid points on a horizontal or vertical line through a zone)
then that zone will be labeled tabu. So, in addition to the tabu list the search also checks
to see if any of the neighbors is in a tabu zone.
The name reactive tabu search (RTS) was coined by Battiti and Tecchiolli [53], who ﬁrst
described the procedure. The basic features include a tabu list whose length is dynamic and a
mechanism for detecting and escaping from a basin of local minima. We initialize the length
of the tabu list, memsize to 1000. If the frequency of a zone is larger than 256, indicating
that each of the 256 points in the zone are visited once on average, then we conclude that
we are likely to cycle and memsize is increased. That is, memsizenew = 1.2 memsizeold.
Conversely, if memsize has not increased for memsize iterations then we decrease memsize.
That is, memsizenew = 1.0/1.2 memsizeold.
Starting points from nontabu zones are generated randomly on the 800 by 800 grid. Tabu
zones are determined as described earlier, but with one additional feature. Upon completion
of a single pass of the search if the frequency of a zone is greater than 16 then that zone,
along with its eight adjacent neighboring zones, are labeled tabu with regard to ﬁnding
new starting points. The idea is to keep the random starting point away from zones that
have been previously searched. The x and y coordinates of a potential starting point are
each generated from a uniform [−10, 10] distribution. If the point (x, y) is a member of a
tabu zone (including the eight neighboring zones) then the point is discarded. This process
is repeated until a new starting point from a non-tabu zone is found. We could have been
a bit more sophisticated by sampling from only the tabu zones, but in our computational
experiments no more than ten tries were required to ﬁnd a new starting point from a
nontabu zone even after 50 passes of the search.
There are two ways for one pass of RTS to terminate—if a basin is found or if the frequency
of a zone is large. A basin is a collection of local minimums of similar magnitude from which
it is diﬃcult for a tabu search armed only with a tabu list and an aspiration criteria to
escape. For Levy No. 3, we say that the search has entered a basin if the diﬀerence between
the best value of f(x, y) and the worst value of f(x, y) is less than 30.0 for more than 1024
iterations. The number 1024 was selected with the idea that we would traverse at least
four zones and 30.0 was selected as a rough estimate. We knew that the range of f(x, y)
was roughly 350 and we wanted to avoid the ﬂat regions of the solution space. In practice,
the range of f would not be known and the allowable range for basin detection would be
a percentage of the best known maximum and minimum values. When a basin is detected
the search restarts at a point generated from two features of the basin—the coordinates of
the best point in the basin and the distance between the best and worst points in the basin.
We call this latter value the diameter of the basin. By distance we mean the maximum of
the diﬀerence between the x and y coordinates of the two points. Then we generate all eight

13-32
Operations Research Methodologies
neighbors of the best point at a distance of six times the diameter of the basin. We used a
value of six so that we would be certain to evaluate points outside the basin. Among these
eight points we select the non-tabu point with lowest function value. If all eight points are
tabu we generate a new point randomly.
The second means of terminating a single pass of RTS is to avoid repetition of a particular
point even though the search has been increasing the value of memsize, that is, detecting
when the search is likely to be stuck in a cycle from which the simple increase in the length
of the tabu list will not let the search escape. In other words, the search is stalled in a
deep valley or a collection of valleys. Here again, our search relies on zones rather than on
individual points. If the frequency of any zone is greater than 512 then we restart reactive
tabu search at a new (randomly generated) grid point. The choice of 512 follows from
deciding that if each point in a zone is visited (on average) twice then the search is most
likely cycling. Clearly this number must be larger than 256, the zone frequency at which
memsize is increased.
13.9.2
Computational Experiments
One of the goals of this section is to provide visual examples of the features of RTS. To
do this we compare RTS with a more traditional tabu search. The traditional scheme has
the same neighborhood structure, tabu zones, and aspiration criteria as RTS, but has a
ﬁxed tabu list size of 1500 and no capability to detect or escape from basins. The tabu list
size of 1500 was determined by experimentation. Indeed the performance of the traditional
tabu search is sensitive to this parameters value. In our experiment 1500 produced the best
results. Thus, the traditional tabu search does not have the ability to alter the length of
the tabu list, nor does it make any attempt to discover basins and escape from them. The
only mechanism for terminating a pass of the search is the detection of cycling associated
with stalling in a deep valley or a collection of valleys. As in RTS, if the frequency of a
zone is greater than 512 then we restart the search at a new randomly generated grid point.
To avoid extremely long passes we set a maximum of 5000 iterations per pass. Finally, the
traditional tabu search generates its random starting points just as in RTS.
Figure 13.6 summarizes the performance of the traditional tabu search on Levy No. 3.
Figure 13.6 plots two data items, the f(x, y) values on the z-axis and the (x, y) values. The
numbers from 1 to 7 denote a single pass of the search started at a randomly generated
grid point. Clearly the ﬁxed memory size of 1500 and the aspiration criteria are able to
force the search out of small local minima and in most cases avoid cycling. Five of the
seven passes terminate due to the maximum iteration limit. Only passes 1 and 2 detect
the presence of cycling associated with stalling in a deep valley before 5000 iterations are
complete. Notice that three of the nine best solutions are uncovered by the search and that
to get to these three solutions relatively deep valleys were explored and escaped. Besides
these nine valleys with minima on the order of −176 there are many valleys with minima
on the order of −145 and −116 in the landscape (see Figure 13.5). Although not illustrated
here, we observed that both the traditional search and RTS have great diﬃculty escaping
from the −145 and −116 valleys using only a tabu list and aspiration criteria. The lowest
point in pass 4 in Figure 13.6 has an objective value on the order of −116. The total number
of iterations (neighborhood searches) generated by the seven passes was 31,658.
Figures 13.7 and 13.8 summarize the performance of RTS on Levy No. 3. Figure 13.8 adds
the f(x, y) values as the z-axis to the (x, y) values recorded in Figure 13.7. Each number
(in both ﬁgures) denotes a single pass of the search. The checkerboard pattern in Figure
13.7 is most easily explained by Figure 13.8. Here we see that each checkerboard maps into
a series of adjacent valleys (local minima). The unexamined spaces of the checkerboard

Metaheuristics for Discrete Optimization Problems
13-33
40
80
0
10
5
0
5
10
10
5
0
5
0
5
160
120
0
0
5
10
1,2,3 global opt.
1,2 freq. jumps
Tabu search with diversification jumps (31,568 itrs.)
3–7 itr. limit
1
2
3
4
5
6
7
FIGURE 13.6
Static TS Levy No.3—31,568 iterations.
0
1
5
0
0
5
10
3
7
6
5
4
2,3,4,5 basins
1,6,7 global opt
1
2
Reactive tabu search with basin escape (17299 itr)
FIGURE 13.7
RTS for Levy No.3—17,299 iterations.

13-34
Operations Research Methodologies
0
0
5
10
0
5
2,3,4,5 basins
1,6,7 global opt
Reactive tabu search with basin escape (17299 itr)
1
6
7
2
5
3
4
FIGURE 13.8
RTS for Levy No.3—17,299 iterations.
most likely correspond to hills (local maximum) and were avoided by the search. Pass 1 is
nearly the same as the one generated by the traditional tabu search. The diﬀerence is that
in Figure 13.6 the search terminated due to a repeated solution (iteration 1725), but in
Figure 13.8 RTS terminates due to the detection of a basin (iteration 2943). The variable
length of the tabu list accounts for RTS not terminating at the same iteration (1725) as the
traditional tabu search scheme. Clearly the valley for pass 1 in Figure 13.8 is not a basin (a
collection of local minima). What must be happening is that the search is circling around
the valley at nearly a constant height. Hence, since the change in the objective value is less
than 30.0 (points at nearly the same depth of the valley) for more than 1024 iterations, the
basin escape mechanism of RTS is activated.
As a result the starting point of the search for pass 2 is generated from the characteris-
tics of the detected basin (best point and diameter). Starting solutions for passes 3–6 are
generated in the same manner. That is, passes 3, 4, and 5 all terminate due to the detection
of a basin. In each of these cases RTS detects what we initially had in mind for a basin—a
collection of local minima (valleys). Each of the passes (2–5) contains a pair of small valleys
that fulﬁll the basin criteria. Pass 6 terminates with a repeated zone and, therefore, pass 7
is a randomly generated starting point. Pass 7 ﬁnds a point with function value −176 at
iteration 1145 and then spends the next 3200 iterations escaping from this valley and uncov-
ering several small nearby valleys. These valleys trigger the basin escape mechanism and
pass 7 concludes. As in the traditional search results, three of the seven passes ﬁnd −176
solutions. Two of these −176 solutions (passes 1 and 7) were uncovered by the traditional
method as well.

Metaheuristics for Discrete Optimization Problems
13-35
2000
2500
3000
3500
4000
4500
5000
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
5500
Iteration number
tabu list size for RTS pass 6
Tabu list size
itr 1250-2429 valley 3 (min–141)
itr 2430-3496 valley 4 (min–176)
itr 4378 cycles to valley 2
itr 5055 cycles to valley 3
Fixed tabu list size
FIGURE 13.9
Size history of tabu list in RTS for pass 6.
Lastly we examine the length of the tabu list—memsize—in RTS. Figure 13.9 catalogs the
value of memsize over the search history (iterations) for passes 6 and 7 of RTS summarized
in Figures 13.7 and 13.8. What is the impact of allowing memsize to change? The value
of memsize increases from 833 at iteration 2363 to 2489 at iteration 2369. The growth of
memsize in iterations 2363–2369 comes just as the search is trying to escape from valley 3.
Valley 4 is a deep local minima. Its lowest point has a value on the order of −141. The
value of memsize increases again at iteration 3087 while in valley 4. Presumably this assists
the search in escaping from the −176.540 local minima at iteration 3496. After iteration
4377 the search falls back into valley 2, from which it escapes for a second time on iteration
5055. Next the search falls into valley 3 again, from which it is unable to escape prior to the
search meeting its repeated solution condition. Thus, pass 6 stops at iteration 5917 with a
memsize of 6193. These latter increases of memsize were not large enough to drive it away
from the previously discovered valleys. The search is restarted for pass 7 and memsize is
reset to 1000. Unlike pass 6 there is only one deep valley traversed (with −176.316 at its
minimum). The other six valleys are all relatively shallow, with the last two causing the
basin escape mechanism to be activated and the search restarted. The −176.316 solution is
uncovered at iteration 1145 in the third valley traversed. The value of memsize is increased
from 833 on iteration 2153 to 2489 on iteration 2159. This enables the search to escape from
the −176.316 valley on iteration 2222.
13.10
Discussion
The thrust of this chapter has been on the implementation details and performance of
local search heuristics (Sections 13.3 and 13.7), tabu search (Sections 13.5 through 13.9)
and simulated annealing (Sections 13.4 and 13.8). The focus on tabu search represents my

13-36
Operations Research Methodologies
own bias. I believe that the tabu search framework allows greater ﬂexibility and allows for
more insight about the problem structure to be built into the search than do most other
metaheuristics. Plain vanilla tabu search (Section 13.5) is straightforward to implement
with only one parameter to set—the length of time moves should remain on the tabu list.
In addition there are a variety of advanced features that can signiﬁcantly improve the
performance of tabu search if the search terrain proves diﬃcult (Sections 13.8 and 13.9).
The material presented in Sections 13.1 through 13.5 is ampliﬁed by the results presented
in Sections 13.6 through 13.9.
Returning to the questions asked at the outset: If I have a feasible solution, how do
I know if it is a good one? My recommendation here would be to begin with a plain
vanilla tabu search (Section 13.5). If the best solutions found by PVTS are not signiﬁcantly
better than your current feasible solution then you need not look further. However, having
other high-quality solutions available may provide additional insights into the optimization
model you are trying to solve and you may wish to continue the search. Which solution
strategy should I start with if I don’t want to invest too much of my time? The easiest to
implement are multistart local searches (Section 13.4). However, if you have been solving
other problems with simulated annealing and tabu search it is typically not diﬃcult to adapt
these codes to new problems. The section of the code dedicated to evaluating the quality
of a given solution must change, but often much of the remaining code requires only minor
changes.
The contents of this chapter reﬂect my own preferences for how to go about answering
the above questions. As I suggested at the outset, when faced with a nonstandard DOP, I
begin by developing a neighborhood structure for traversing the space of feasible solutions
and embed this within a multistart local search heuristic (see Sections 13.3 and 13.7) with
a random, constructed, or known initial feasible solution. Section 13.6 provides an example
of a construction scheme for generating initial solutions while in Section 13.8 rounded linear
programming relaxation solutions are used. Multistart provides a benchmark for compar-
ing the performance of any other solution approach chosen for implementation as well as
information about the search terrain. Next, I suggest embedding the neighborhood struc-
ture within a simulated annealing heuristic. I use the standard set of parameter values (see
Section 13.4) without any ﬁne-tuning. If simulated annealing is unable to improve upon the
solutions found by multistart, I often stop any further computational tests and conclude
that the search terrain is not diﬃcult and that the solutions I have found with multistart
are good enough. If simulated annealing is able to uncover better solutions than multistart,
or if I have a competing solution found by a diﬀerent approach that is better (e.g., in Sec-
tion 13.6 the modal analysis approach for actuator placement problem), then I proceed to
develop a tabu search for the problem. If basic tabu search (see Sections 13.5 and 13.6)
is unable to improve upon the solutions found by multistart and simulated annealing then
I begin adding advanced features to my tabu search (see Sections 13.8 and 13.9). In my
experience, tabu search is able to ﬁnd the same quality of solutions as simulated annealing
but in signiﬁcantly less time and with signiﬁcantly fewer function evaluations (typically an
order of magnitude decrease).
References
1. Goldberg, D.E., Genetic Algorithms in Search, Optimization and Machine Learning,
Addison-Wesley, Reading, MA, 1989, ISBN: 0201157675.
2. Back, T., Evolutionary Algorithms in Theory and Practice, Oxford University Press,
New York, 1996.

Metaheuristics for Discrete Optimization Problems
13-37
3. Dasgupta,
D.
and
Z.
Michalewicz,
Evolutionary
Algorithms—An
Overview.
In:
D. Dasgupta and Z. Michalewicz (eds.), Evolutionary Algorithms in Engineering Appli-
cations, pp. 3–28, Springer-Verlag, Heidelberg, 1997.
4. Moscato, P., Memetic Algorithms: A Short Introduction. In: D. Corne, M. Dorigo, and
F. Glover (eds.), New Ideas in Optimization, pp. 219–234, McGraw Hill, London, 1999.
5. Glover, F. and M. Laguna, Tabu Search, Kluwer Academic Publishers, Boston, 1997.
6. van Laarhoven, P.J.M. and E.H.L. Aarts, Simulated Annealing: Theory and Applications,
D. Reidel Publishing Company, Kluwer, Dordrecht, 1987.
7. Michalewicz, Z. and D.B. Fogel, How to Solve It: Modern Heuristics. Springer-Verlag,
Berlin, 2000.
8. Hoos, H.H. and T. Stutzle, Stochastic Local Search: Foundations and Applications, Morgan
Kaufmann Publishers, San Francisco, 2005.
9. Mirchandani, P.B. and R.L. Francis (eds.), Discrete Location Theory, Wiley-Interscience,
New York, 1990.
10. Nickel, S. and J.P. Albandoz, Location Theory: A Uniﬁed Approach, Springer-Verlag,
Berlin, 2005.
11. Kincaid, R.K. and L.G. Yellin, “The P-dispersion-sum problem: Results on trees and
graphs,” Location Science, 1:171–186, 1993.
12. Kincaid, R.K. and R.T. Berger, “The damper placement problem on space truss struc-
tures,” Location Science, 1:219–234, 1993.
13. Garey, M.R. and D.S. Johnson, Computers and Intractibility: A Guide to the Theory of
NP-Completeness, W.H. Freeman, New York, 1979.
14. Metropolis, N., A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller, “Equation of state
calculations by fast computing machines,” Journal of Chemical Physics, 21:1087–1092,
1953.
15. Kirkpatrick, S., C.D. Gelatt, and M.P. Vecchi, “Optimization by simulated annealing,”
Science, 220:671–680, 1983.
16. Cerny, V., “Thermodynamical approach to the traveling salesman problem: An eﬃcient
simulation algorithm,” Journal of Optimization Theory and Applications, 45:41–52, 1985.
17. Johnson, D.S., C.R. Aragon, L.A. McGeoch, and C. Schevon, “Optimization by simulated
annealing: An experimental evaluation. Part I, Graph partitioning,” Operations Research,
37:865–893, 1989.
18. Lundy, M. and A. Mees, “Convergence of an annealing algorithm,” Mathematical Program-
ming, 34:111–124, 1986.
19. Aarts, E.H.L. and P.J.M. Van Laarhoven, “Statistical cooling: A general approach to com-
binatorial optimization problems,” Phillips Journal of Research, 40:193–226, 1985.
20. Collins, N.E., R.W. Eglese, and B.L. Golden, “Simulated annealing—an annotated bibli-
ography,” American Journal of Mathematical and Management Sciences, 8:209–308, 1988.
21. Press, H., B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling, Numerical Recipes: The
Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, UK, 1986.
22. Kincaid, R.K., “Good solutions to discrete noxious location problems via metaheuristics,”
Annals of Operations Research, 40:265–281, 1992.
23. Glover, F., “Tabu search, Part I,” ORSA Journal on Computing, 1:190–206, 1989.
24. Glover, F.,“Tabu search: A tutorial,” INTERFACES, 20:74–94, 1990.
25. Palumbo, D., S.L. Padula, K.H. Lyle, J.H. Cline, and R.H. Cabell, “Performance of
Optimized Actuator and Sensor Arrays in an Active Noise Control System,” NASA
TM-110281, 1996.
26. Lyle, K.H. and R.J. Silcox, “A study of active trim panels for noise reduction in an aircraft
fuselage,” paper no. 951179 in SAE General, Corporate and Regional Aviation Meeting
and Exposition, May, 1995.

13-38
Operations Research Methodologies
27. Palumbo, D.L., “Flight test of ASAC interior noise control system,” AIAA Paper 99-1933,
1999.
28. Kincaid, R., K. Laba, and S. Padula, “Quelling cabin noise in turboprop aircraft via active
control,” Journal of Combinatorial Optimization, 1(3):1–22, 1997.
29. Kincaid, R. and K. Laba, “Reactive tabu search and sensor selection in active structural
acoustic control problems,” Journal of Heuristics, 4:199–220, 1998.
30. Kincaid, R. and S. Padula, Actuator Selection for the Control of Multi-Frequency Noise
in Aircraft Interiors. In: S. Voss, S. Martello, I. Osman, and C. Roucairol (eds.), Meta-
Heuristics: Advances and Trends in Local Search Paradigms for Optimization, Kluwer,
Dordrecht, pp. 111–124, 1999.
31. Underhill, L.G., “Optimal and suboptimal reserve selection algorithms,” Biological
Conservation, 70:85–87, 1994.
32. Church, R.L. and C.S. ReVelle, “The maximum covering location problem,” Papers in
Regional Science, 32:101–118, 1974.
33. Schilling, D.A., C.S. ReVelle, C. Cohon, and D.J. Elzinga, “Some models for ﬁre protection
location decisions,” European Journal of Operational Research, 5:1–7, 1980.
34. Camm, J.D., S.K. Norman, S. Polasky, and A.R. Solow, “Nature reserve site selection to
maximize expected species covered,” Operations Research, 50:946–955, 2002.
35. Rosing, K.E., C.S. ReVelle, and J.C. Williams, “Maximizing species representation under
limited resources: a new and eﬃcient heuristic,” Environmental Modeling and Assessment,
7:91–98, 2002.
36. Masters, L., N. Clupper, E. Gaines, C. Bogert, R. Solomon, and M. Ornes, Biodiversity
Research Consortium Species Database Manual, The Nature Conservancy, Boston, 1995.
37. Kincaid, R.K., C. Easterling, and M. Jeske, “Computational experiments with heuristics
for two nature reserve site selection problems,” Computers and Operations Research,
35(2):499–512, 2008.
38. Csuti, B., S. Polasky, P.H. Williams, R.L. Pressey, J.D. Camm, M. Kershaw, A.R. Kiester,
B. Downs, R. Hamilton, M. Huso, and K. Sahr, “A comparison of reserve selection
algorithms using data on terrestrial vertebrates in Oregon,” Biological Conservation,
80:83–97, 1997.
39. Camm, J.D., S. Polasky, A.R. Solow, and B. Csuti, “A note on optimal algorithms for
reserve site selection,” Biological Conservation, 78:353–355, 1996.
40. Church, R.L., D.M. Stoms, and F.W. Davis, “Reserve selection as a maximal covering
location problem,” Biological Conservation, 76:105–112, 1996.
41. ReVelle, C.S., J.C. Williams, and J.J. Boland, “Counterpart models in facility location
science and reserve selection science,” Environmental Modeling and Assessment, 7:71–80,
2002.
42. Ronald L. Rardin. Optimization in Operations Research, Prentice Hall, Upper Saddle
River, NJ, 1998.
43. Kincaid, R.K., “Solving the damper placement problem using local search heuristics,”
special issue of OR Spektrum on Applied Local Search, 17:149–158, 1995.
44. Anderson, E., M. Trubert, and J. Fanson, “Testing and application of a viscous pas-
sive damper for use in precision truss structures,” Proceedings of 32nd Structures,
Structural Dynamics and Materials Conference, Baltimore, MD, April, pp. 2795–2807,
1991.
45. Padula, S. and C.A. Sandridge, Passive/Active Strut Placement by Integer Programming,
In M.P. Bendsoe and C.A. Morta Soares (eds.), Topology Design of Structures, Kluwer
Academic Publishers, Dordrecht, pp. 145–156, 1993.
46. Preumont, A., “Active damping by a local force feedback with piezoelectric actuators,”
Proceedings of 32nd Structures, Structural Dynamics and Materials Conference, Baltimore,
MD, April, pp. 1879–1887, 1991.

Metaheuristics for Discrete Optimization Problems
13-39
47. Kincaid, R.K. and R.T. Berger, “The MaxMin sum problem on trees,” Location Science,
2:1–10, 1994.
48. Kelly, J.P., M. Laguna, and F. Glover, “A study of the diversiﬁcation strategies for the
quadratic assignment problem,” Computers and OR, 21(8):885–893, 1994.
49. Parker, R.G. and R.L. Rardin, Discrete Optimization, Academic Press, San Diego, CA,
1988.
50. Hansen, E. Global Optimization Using Interval Analysis, Marcel Dekker, New York, 1992.
51. Jansson, C. and O. Knuppel, “Numerical results for a self-validating global optimization
method,” working paper No. 94.1, Technische Universit¨at Hamburg-Harburg, Technische
Informatik III, D-21071 Hamburg, Germany, 1994.
52. Levy, A.V., A. Montalvo, S. Gomez, and A. Calderon, Topics in Global Optimization,
Lecture Notes in Mathematics No. 909, Springer-Verlag, New York, 1981.
53. Battiti, R. and G. Tecchiolli, “The reactive tabu search,” ORSA Journal on Computing,
6:126–140, 1994.


14
Robust Optimization
H. J. Greenberg
University of Colorado at Denver
and Health Sciences Center
Tod Morrison
University of Colorado at Denver
and Health Sciences Center
14.1
Introduction.......................................... 14-1
14.2
Classical Models ..................................... 14-2
Mean-Risk Model • Recourse Model •
Chance-Constrained Model
14.3
Robust Optimization Models ......................14-10
Worst-Case Hedge • Simple Case of Interval
Uncertainty • Minimax Regret • Uncertainty Sets
14.4
More Applications...................................14-16
14.5
Summary .............................................14-30
Acknowledgments ...........................................14-32
References ....................................................14-32
14.1
Introduction
We all want to maximize our gains and minimize our losses, but decisions have uncertain
outcomes. What if you could choose between an expected return of $1000 with no chance of
losing any amount, or an expected return of $5000 with a chance of losing $50,000. Which
would you choose? The answer depends upon how risk-averse you are. Many would happily
take the nearly certain $1000, and some would take the risk with hope of greater proﬁt.
This chapter is concerned with how to extend mathematical programming models to deal
with such uncertainty.
Sources of uncertainty could be due to at least three diﬀerent conditions [1]:
• ignorance, such as not knowing exactly how much oil is in a reserve;
• noise, such as measurement errors, or incomplete data;
• events that have not yet occurred, such as future product demand.
The eﬀects of uncertainty on decision-making include variations in actual returns or
resources consumed, but there could be a catastrophic eﬀect that one seeks to avoid com-
pletely. Losing money in an investment is not good, but losing so much that a ﬁrm goes
bankrupt is catastrophic. Losing additional lives due to a mis-estimate of where and when
some tragedy will strike is bad, but losing an ability to recover is catastrophic. Uncertainty
creates a range of concerns about the volatility of one policy versus another, and the one with
better expected value may miss essential points of concern. One hedge against uncertainty
is to diversify, keeping a greater level of ﬂexibility in what recourses are available.
In any problem that requires optimization under uncertainty we must ﬁrst answer some
basic questions: “How do we represent uncertainty?” “What are our attitudes towards risk?”
In addition, robust optimization raises the question, “What is a robust solution?” A naive
14-1

14-2
Operations Research Methodologies
approach to robust optimization is to deﬁne a robust solution as one that remains optimal, or
feasible, under any realization of the data. Unfortunately, such a deﬁnition is very restrictive
and may lead to infeasible models for reasonable problems. In the following sections we
deﬁne some alternative robust optimization models, each of which captures some aspect of
volatility or ﬂexibility.
One useful perspective is to ask how much degradation we are willing to accept in our
objective to reduce the risk that our solution is infeasible. Suppose we have two candidate
solutions, x1 and x2, and we can measure their performance (such as by computing the
expected value of each). Based on this performance, we prefer x1 to x2. Suppose, how-
ever, that there is uncertainty in the constraint Dx ≥d such that less than 1% variation
in uncertain values of D causes x1 to violate the constraints by as much as 50%—that
is, 1.01Dx 1 < 1
2d [2]. One must consider the price of robustness [3] but avoid being overly
conservative. In particular, one typically does not want to use the most pessimistic model,
where each uncertain coeﬃcient is replaced by its most pessimistic value [4]:
Dx ≥d
where Dij = min
D {Dij} and di = max
d {di}. Replacing the stochastic constraint, Dx ≥d, with
this pessimistic one rules out x1 and leaves only those x that absolutely guarantee feasibility.
It also ignores correlations among the coeﬃcients in D and between D and d, and it is not
inﬂuenced by their probability of occurrence.
The rest of this chapter is organized as follows. Section 14.2 presents a succinct description
of each model that was proposed in the early development of operations research (1950s).
Section 14.3 presents robust optimization from its beginnings to now. Section 14.4 presents
some additional applications. The last section is a summary and an indication of avenues
for further research. Most terms are deﬁned here, but see the Mathematical Programming
Glossary [5] for extended deﬁnitions.
14.2
Classical Models
The fallacy of averages [6] is the misconception resulting from the replacement of each
uncertain parameter with its expected value. This can lead to poor results in supporting
decision making for at least two reasons: (1) the parameters may be correlated—namely,
E[XY ] ̸= E[X] E[Y ], and (2) the functions may not be linear, so f(E[X]) ̸= E[f(X)].
Consider the simple two-asset investment problem using only the expected returns:
max 1000 x1 + 50,000 x2 : x ≥0, x1 + x2 = 1
where xi is the portion of the total budget, as a percentage, invested in the i-th asset. This
is a totally deterministic problem; the solution is to invest everything into asset 2, giving
an expected return of $50,000 per amount invested. Suppose there is a correlation between
investment outcomes such that the return from asset 1 could be much greater than the
return from asset 2. In particular, suppose we have three scenarios with their probabilities
of occurrence and the returns they generate, shown in Table 14.1.
Notice that the properties of asset returns do not depend upon the actual levels of invest-
ment. The total investment could be $1,000,000 to reap the net returns, or it could be $1.
This is what is implied by the constraint x1 + x2 = 1, where x i is the portion of the capital
invested in asset i.
We shall revisit this ﬁnance application as we explain modeling approaches to deal with
the uncertainty, but this chapter is concerned with the broad view, beyond this application.

Robust Optimization
14-3
TABLE 14.1
Scenarios That Are Uncertain
Scenario
Probability
Asset 1 Return
Asset 2 Return
1
0.2
1050
−2,000,000
2
0.4
975
1,000,000
3
0.4
1000
125,000
Expected Return:
1000
50,000
It is thus imperative that you examine Section 14.4 to learn how robust optimization applies
to a variety of applications and how it compares with other approaches. Seeing how diﬀerent
models apply will help to clarify the basic terms and concepts, particularly those developed
in Section 14.3.
There are three classical models that are still used today: mean-risk, recourse, and chance-
constrained. A brief discussion of each will give us a base from which to present robust
optimization.
14.2.1
Mean-Risk Model
The ﬁrst mathematical programming model of uncertainty was Markowitz’s mean-risk model
[7,8] for the portfolio selection problem. Markowitz rejected the hypothesis that an investor
should simply maximize expected returns. Instead, he suggested that investors consider
expected return a desirable objective to maximize, but only while also considering risk
undesirable, to be minimized. Recognizing that these two objectives are usually in conﬂict,
he considered them in a bi-objective model.
Consider the example in Table 14.1. One measure of risk is the expected square of how
much the actual return diﬀers from the expected return. Let R1 and R2 denote the (random)
returns from the two assets, and let p = E[R] denote their expected values. We have already
noted that E[R1] = 1000 and E[R2] = 50,000. However, the potential for a $2,000,000 loss is
daunting, so let us assess the risk of some arbitrary mixture of investments.
The total return is R1x1 + R2x2, so the square deviation from the mean is:
E

(R1x1 + R2x2 −(p1x1 + p2x2))2
= E

((R1 −p1)x1 + (R2 −p2)x2)2
= E

(R1 −p1)2
x2
1 + E

(R2 −p2)2
x2
2 + 2x1x2E [(R1 −p1)(R2 −p2)]
We write this as the quadratic form: (x1, x2)V

x1
x2

, where V is the variance-covariance
matrix:
V =

E

(R1 −p1)2
E [(R1 −p1)(R2 −p2)]
E [(R1 −p1)(R2 −p2)]
E

(R2 −p2)2

=
 σ2
1
σ12
σ12
σ2
2

To obtain the variances and covariance, we consider each scenario’s contribution.
σ2
1 = 0.2(2500)
+ 0.4(625)
+ 0.4(0)
σ2
2 = 0.2(4.2025 × 1012) + 0.4(9.025 × 1011) + 0.4(5.625 × 109)
σ12 = 0.2(−1.025 × 108) + 0.4(−2.375 × 107) + 0.4(0)
Thus,
V =

750
−30,000,000
−30,000,000
1,203,750,000,000


14-4
Operations Research Methodologies
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
12
14
× 104
Investment in asset 2
Expected return
Risk × 10−7 
FIGURE 14.1
Expected return and risk as functions of x2.
and the risk is measured by x ′Vx = 750x2
1 −2x1x2(3.00 × 106) + 1.20375 × 1012x2
2 (approx-
imating the coeﬃcient of x2
2). If we invest everything into asset 2, this means x1 = 0 and
x2 = 1. In that case, the risk is 1.20375 × 1012, reﬂecting not only a chance of much greater
return than what is “expected,” but also a risk of a great loss. At the other extreme, if we
invest everything into asset 1, x = (1, 0) and the risk is only 750, reﬂecting a much more
conservative investment. Figure 14.1 plots the expected return as a (linear) function of how
much is invested into asset 2 and the risk as a (quadratic) function. (To ﬁt on the same
plot, the risk is scaled by 10−7.)
Markowitz’s mean-risk model is
max
x∈X px −λx′V x
where X = {x ≥0 : 
ixi = 1}. We sometimes allow no investment by replacing the equation
with the inequality 
ixi ≤1. These are equivalent as we could add a risk-free asset with
zero return, also called the slack variable.
Depending upon the choice of λ, this also represents the constrained mathematical pro-
gram that maximizes expected return subject to a limit on risk:
P: max
x∈X px : x′V x ≤r
Reciprocally, one could view the mean-risk model as one of minimizing risk subject to a
required expected return:
Q: min
x∈X x′V x : px ≥μ
All three mathematical programs are related; for each λ, there is a corresponding r and μ
for which x∗(λ) is the optimal solution in P and Q, respectively.
THEOREM 14.1
Deﬁne X = {x ∈Rn
+ : n
i=1 xi ≤1}, and let x∗∈argmax{px −λx′V x:
x ∈X} for λ > 0, with μ = px∗and r = x∗′Vx∗. Then,
x∗∈argmax{px : x′V x ≤r, x ∈X} and x∗∈argmin{x′V x : px ≥μ, x ∈X}

Robust Optimization
14-5
PROOF 14.1
We have
px∗−λx∗′V x∗≥px −λx′V x
for all x ∈X
⇒px∗≥px −λ(x′V x −r) ≥px
for all x ∈X : x′V x ≤r
Similarly,
px∗−λx∗′V x∗≥px −λx′V x
for all x ∈X
⇒x∗′V x∗≤x′V x + 1
λ(μ −px) ≤x′V x
for all x ∈X : px ≥μ
■
There is also a converse, due to the Lagrange multiplier rule and the fact that the
quadratic risk function is convex (i.e., V is positive semi-deﬁnite).
THEOREM 14.2
If x∗solves P or Q, there exists λ ≥0 such that x∗solves the mean-risk
model.
PROOF 14.2
Suppose x∗solves P. Applying the Lagrange multiplier rule, there exist α,
μ, μ0 ≥0 such that
p −2αx∗′V + μ −μ01 = 0, αx∗′V x∗= αr, μx∗= 0, μ0
n
	
i=1
x∗
i = μ0
Deﬁning λ = 2α, these are the Karush–Kuhn–Tucker conditions for the optimality of x∗in
the mean-risk model. A similar proof applies if we suppose x∗solves Q.
■
Example 14.1
Suppose we have three independent assets with p = (3, 2, 1), plus a risk-free choice with zero
return. Consider the following mean-risk model:
max 3x1 + 2x2 + x3 −λ(x2
1 + x2
2 + x2
3) : x ≥0, x1 + x2 + x3 ≤1
where 1 −(x1 + x2 + x3) is the level of investment in the risk-free asset. Letting λ vary, the
parametric solutions yield the eﬃcient frontier, shown in Figure 14.2.
Setting λ = 0 ignores risk and gives the maximum expected return. This is achieved by
investing all capital into asset 1 (i.e., x1 = 1) because it has the greatest expected return.
This remains true for 0 ≤λ ≤1
2. As we increase λ past 1
2, we penalize the risk of putting
all capital into one asset and begin to invest into the next one, which is asset 2, having an
expected return of 2. Splitting the investment between assets 1 and 2 gives a greater overall
objective value:
px −λx′V x
for x = (x1, 1 −x1, 0) = 3x1 + 2(1 −x1) −λ(x2
1 + (1 −x1)2)
= x1 + 2 −λ(2x2
1 −2x1 + 1)
which is maximized at x1 = 1 + 2λ
4λ .
For λ suﬃciently great, splitting the risk is better than investing in just one asset because
the square of x1 outweighs the linear expected return. The greater the price of risk (λ), the

14-6
Operations Research Methodologies
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
Risk
Expected return
slope = λ 
μ
r
FIGURE 14.2
Eﬃcient frontier of example portfolio model.
0
0.5
1
1.5
2
2.5
3
3.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
l
Investment levels
x1(l)
x2(l)
FIGURE 14.3
Splitting the investment to reduce risk for λ > 0.
greater the split. Figure 14.3 shows how the optimal split, x1(λ), x2(λ) = 1 −x1(λ), varies
as functions of λ (x2(λ) = 0 for 0 ≤λ ≤1
2).
At λ = 1 1
2, asset 3 comes into play, and the optimality conditions are:
pi −2λxi −1
3

	
k
pk −2λ

= 0
for i = 1, 2, 3
Thus,
xi = pi −(2 + 2
3λ)
2λ

Robust Optimization
14-7
TABLE 14.2
Optimal Asset Investments as Functions of λ
Asset Investment
λ
x∗
1
x∗
2
x∗
3
px∗
x∗′Vx∗
0 −1
2
1
0
0
3
1
1
2 −1 1
2
1
2 + 1
4λ
1
2 −1
4λ
0
2 1
2 + 1
4λ
2 1
2 −1
4 λ −1
8λ
1 1
2 −3
1
3 + 1
2λ
1
3
1
3 −1
2λ
2 + 1
λ
1
3 +
1
2λ2
≥3
3
2λ
1
λ
1
2λ
3
λ
7
2λ2
which remains valid for 1 1
2 ≤λ ≤3. At λ > 3, the price of risk is so great that 
i xi < 1. The
levels are x(λ) = 1
2λ(3, 2, 1)—that is, the portion of investment in an asset is proportional
to its expected return (Table 14.2).
If you draw a vertical line in Figure 14.2, the expected return is the maximum possible if
the risk is limited to be on or to the left of that line. That is the solution to problem P. If you
draw a horizontal line, the risk is the minimum possible if the expected return is required to
be on or above that line. That is the solution to problem Q. So, we can generate the eﬃcient
frontier in Figure 14.2 by letting the price of risk, λ, vary between 0 and ∞. In practice, a
decision maker would choose a point on the eﬃcient frontier based on risk tolerance.
14.2.2
Recourse Model
The second mathematical programming model of uncertainty was Dantzig’s recourse [9].
The two-stage recourse model has two vectors of decision variables: x must be speciﬁed
before uncertain parameters become known, and y is chosen after the uncertain parame-
ters become known. A classical example is to have a linear program (LP) with uncertain
demand. The ﬁrst-stage variables (x) specify production and process operation levels. Once
the demand is known, the second-stage variables (y) take recourse in deciding what to do
about any excess or shortage.
The two-stage recourse model is thus:
min
x∈X

f(x) + E

min
y∈Y (x){Φ(y; x)}

When the underlying mathematical program is linear, we have the form:
min {cx + E[min {Φ(y; x) : y = d −Dx}] : Ax ≥b}
where Φ penalizes over- and under-satisfaction of joint constraints, Dx + y = d.
When the number of possible realizations is ﬁnite, we call each realization a scenario and
deﬁne a second-stage variable for each scenario. Let yk denote the second-stage variable for
scenario k. Then, the entire recourse model can be put into one large mathematical program:
min cx +
K
	
k=1
skCk|yk| : Ax ≥b, yk + Dkx = dk,
for k = 1, . . ., K
where sk is the probability that scenario k prevails and K is the number of scenarios. Note
that if there are additional constraints on y, such as yk ≥0 for all k, there could be no
feasible solution. Otherwise, Ck is a linear penalty on the magnitude of yk, and every x
that satisﬁes Ax ≥b is a feasible candidate since it is feasible to let yk = dk −Dkx for all k.

14-8
Operations Research Methodologies
To turn this into a linear program, we replace the absolute value with the diﬀerence of
the positive and negative part: yk = uk−vk. Then, the LP recourse model is given by:
min cx +
	
k
skCk(uk + vk) : Ax ≥b
yk + Dkx = dk, (uk, vk) ≥0, uk ≥yk, vk ≥−yk,
for k = 1, . . ., K
Every
optimal
solution
has
uk
i = max{0, yk
i }
and
vk
i = max{0, −yk
i },
with
yk =
uk −vk. Having partitioned each yk
i , we could have diﬀerent costs for slack (uk
i = yk
i > 0)
and surplus (uk
i = −yk
i > 0).
As an example, consider the following transportation problem:
min
m
	
i=1
n
	
j=1
cijxij : x ≥0
n
	
j=1
xij ≤Si
for i = 1, . . ., m
m
	
i=1
xij ≥Dj
for j = 1, . . ., n
where xij is the amount shipped from source i to destination j.
Suppose demand is uncertain and there is recourse in the face of shortage or excess. Let
fj denote the unit cost of a shortage (demand exceeds the amount shipped) for demand
market j, and let gj denote the unit cost of excess (demand is less than the amount shipped).
For example, f could be a spot market price or a cost of loss of good will, and g could be
a salvage cost (net of the production cost). Then, Φ(y) = 
jΦj(y), where
Φj(y) =

fjy
if y ≥0
−gjy
if y ≤0
We extend the transportation model to include our recourse function by introducing for
each market j in scenario k two additional non-negative variables, ˆyjk and ˇyjk, reﬂecting
over-supply and unmet demand, respectively. Letting sk be the probability of scenario k,
we have E[Φj(y)] = 
ksk(fj ˆyjk + gjˇyjk), so the recourse model is
min
m
	
i=1
n
	
j=1
cijxij +
	
k
sk(fj ˆyk
j + gjˇyk
j ) : x ≥0
n
	
j=1
xij ≤Si
for i = 1, . . ., m
m
	
i=1
xij + ˆyk
j −ˇyk
j = Dk
j
for j = 1, . . ., n, k = 1, . . ., K
This modeling approach extends to the n-stage recourse model in a natural fashion.
Besides production, the ﬁnance community found applications in setting up portfolios of
investments [10]. The stages correspond to time, and the uncertainty is with the asset prices.
Each recourse variable adjusts for the decisions of the previous stages. Deﬁcits in capital
can be made up by borrowing and surpluses can be added to reserve capital.

Robust Optimization
14-9
14.2.3
Chance-Constrained Model
The third model to appear as a certainty equivalent was the chance-constrained model, by
Charnes, Cooper, and Symonds [11]. Their motivating application was planning production
levels in the face of uncertain sales. The general model has the form:
max cx : x ∈X, P(Dx ≥d) ≥α
where α ∈[0,1].
For the portfolio problem indicated by Table 14.1, consider the chance constraint:
P(R1x1 + R2x2 ≥μ) ≥α, where μ is a speciﬁed return and α is an acceptable level of its
probability.
Let sk denote the probability of scenario k and apply Bayes’ Law to obtain:
P(R1x1 + R2x2 ≥μ) =
3
	
k=1
P

(Rk
1 −Rk
2)x1 ≥μ −Rk
2 | k

sk
where we substituted x2 = 1 −x1. We determine which scenarios (if any) satisfy the condi-
tion (Rk
1 −Rk
2)x1 ≥μ −Rk
2. Let K(x) be this set of scenarios, so
P(R1x1 + R2x2 ≥μ) =
	
k∈K(x)
sk
In particular, consider μ = 0, so the chance constraint bounds the probability that the
investment will not lose money. This chance constraint will hold for any α ∈[0, 1] if we
set x1 = 1; it will hold for α ≤0.8 if we set x1 = 0. To determine thresholds between these
extremes consider the three scenarios shown in Table 14.3.
For μ = 0, the chance constraint is the probability that there be no loss. In this case, the
condition to include scenario 1 is x1 ≥0.995; scenarios 2 and 3 always hold because x1 ≤1.
Thus, we have:
P(R1x1 + R2x2 ≥0) =
1
if x1 ≥0.995
0.8
otherwise
The chance-constrained model is
max px : x ∈X, P(R(x) ≥μ) ≥α
In our example, X ={(x1, x2) ≥0: x1 +x2 =1} and R(x)=R1x1 +R2x2 =R2 +(R1 −R2)x1.
Hence, px = 50,000 −49,000x1, and, for μ = 0, our model is
max −49,000x1 : 0 ≤x1 ≤1, x1 ≥0.995
The solution is x∗
1 = 0.995. For μ > 1050, the solution is x∗= (0, 1) if α ≤0.8, and the problem
is infeasible if α > 0.8.
TABLE 14.3
Conditions to Compute
Probability of Minimum Return
Scenario (k)
Condition for k Included in K(x)
1
x1 ≥2,000,000 + μ
2,001,050
2
x1 ≤1,000,000 −μ
999,025
3
x1 ≤125,000 −μ
124,000

14-10
Operations Research Methodologies
A class of models for which the chance-constrained paradigm is especially well suited is
when a government regulation stipulates that some limit cannot be violated more than some
speciﬁed portion of time. One example is sulfur emissions: P(Dx ≤d) ≥α requires that the
probability that the sulfur emitted (Dx) is within a prescribed limit (d) and must be at
least some speciﬁed level (α). Similar chance-constrained models for water quality control
are described in Ref. [12].
14.3
Robust Optimization Models
The modeling paradigm called “robust optimization” emerged from dissatisfaction with lim-
itations of the three classical models, which do not quite capture major eﬀects of relatively
small variations. Gupta and Rosenhead [13] ﬁrst introduced the notion of “robustness” into
an optimal solution, where they meant it to favor ﬂexibility in what recourses are subse-
quently available. Given two policies, x1 and x2, with x1 preferred on the basis of expected
return, they considered the relative ﬂexibility of how many and what recourses are available
in each case. Unlike the mean-risk model, they need not have variances available. Unlike the
recourse model, they need not have some estimated cost structure precisely deﬁned. Unlike
the chance-constrained model, they need not know any probability distributions. They can
choose based on some qualitative information. For example, if x represents levels of pro-
duction, they can prefer x2 if it admits a greater number of recourse alternatives. The key
in their deﬁnition of a robust solution is the ﬂexibility it allows after the uncertain values
become known.
A second view of robustness is the degree to which a solution is sensitive to the underlying
assumptions (beyond data values, such as functional relations). Drawing from the concept of
robust statistics, Mulvey et al. [14] introduced the concept of model and solution robustness
using a penalty function much like the recourse model. (An example in statistics is the use of
the median, rather than the mean, to estimate a parameter. The median is robust because
it does not change with small ﬂuctuations in the data on each side of the median, while
the mean does.) Unlike the recourse model, the robust model could penalize risk as in the
Markowitz model without requiring scenarios to be deﬁned. This has evolved into a collection
of robust optimization models, which we will now describe in a general way, followed by a
section of speciﬁc applications that should help clarify how these general models may arise.
14.3.1
Worst-Case Hedge
A form of robustness is to choose a policy that does as well as possible under worst possible
outcomes, regardless of the probability of occurrence. This view is that of a two-person
game, where the choice of policy x is made ﬁrst, then some “player” chooses an outcome c
that is worst for that x. This is the worst-case model:
min
x∈X max
c
cx : Dx ≥d
for all [D d]
To avoid having an inﬁnite number of constraints, and to correlate the objective coeﬃcients
with the constraint data, this is modelled by discretizing the set of uncertain values into a
ﬁnite set of scenarios:
min
z,x∈X z : z ≥ckx, Dkx ≥dk
for k = 1, . . ., K
The solution to this problem is sometimes called the absolute robust decision [15] (also see
[16]). Note that z is introduced to equal maxk ckx∗in every optimal solution, x∗.

Robust Optimization
14-11
This is pessimistic, and it may be unrealistic. The constraints require x to be feasible
in every possible realization of [D d], regardless of its probability of occurrence. The worst
objective value (over possible c-values) may be appropriate when all constraints are deter-
ministic, which we consider in the special case of interval uncertainty in the next section.
14.3.2
Simple Case of Interval Uncertainty
Consider the worst-case robust optimization model:
min
x∈X max
c∈C cx
(Only the objective coeﬃcients are uncertain.) Let ˆc denote a central value in the box [c, c],
such as its expected, or most likely, value. Deﬁne a central value solution:
ˆx ∈argmin{ˆcx : x ∈X}
In this section, we consider the restricted case where we have a box of uncertainty whose
endpoints are proportional to the central value vector: c = (1 −ε)ˆc and c = (1 + ε)ˆc for some
ε ∈[0, 1). Let P(ˆc, ε) = {c ∈[c, c] : n
j=1 cj = K}, where K = 
i ˆci. We call the equation the
constant-sum constraint.
Let Q(ε) denote the robust optimization problem:
min
x∈X
max
c∈P(ˆc,ε) cx
where X is any subset of binary vectors. The following theorem demonstrates that the cen-
tral solution also solves every robust formulation that allows percentage deviations within a
constant proportion of its central value. The proof is based on the fact that the constant-sum
constraint restricts the uncertain parameter to balance coeﬃcients at their upper bounds
with those at their lower bounds. Our method of proof is based on the duality theorem of
linear programming.
THEOREM 14.3
Let ε ∈[0, 1). Then, x∗is an optimal solution to Q(0) if, and only if,
x∗is an optimal solution to Q(ε).
PROOF 14.3
We begin with some notation and general observations. Let σ(x) =
{j:xj ̸= 0} (called the “support set” of x). Also, let 1 denote the vector of all ones:
(1, 1, . . ., 1)′. The following identities follow from the deﬁnitions of K and σ : ˆcx =

j∈σ(x) ˆcj = K −
j /∈σ(x) ˆcj, and ˆc(1 −x) = K −ˆcx = 
j /∈σ(x)ˆcj. Let L = (1 −ε) and
U = (1 + ε). The dual of the linear program,
max
c∈P(ˆc,ε) cx, is
min πK + Uμˆc −Lλˆc : λ, μ ≥0, and π + μj −λj = xj
for all j = 1, . . ., n
The dual variable π is associated with the constant-sum constraint, and λ, μ are associated
with the lower and upper bound constraints on c, respectively.
Let xε be an optimal solution to Q(ε). Our proof divides into two cases, depending on
whether ˆcx0 is greater or less than 1
2K.
Case 1: ˆcx0 ≥1
2K.
Consider the dual solution π = 1, μ = 0, and λ′ = 1 −x0. This is dual-feasible, where λ ≥0
because x0 ≤1. The dual objective value satisﬁes
πK + Uμˆc −Lλˆc = K −Lˆc(1 −x0) = K −L(K −ˆcx0) = εK + Lˆcx0

14-12
Operations Research Methodologies
Therefore,
max
c∈P(ˆc,ε) cx0 ≤εK + Lˆcx0
(14.1)
Now we deﬁne cε
j = Lˆcj for j /∈σ(xε). As we assume that ˆcx0 ≥1
2K, it follows that ˆcxε ≥
1
2K, which implies that ˆc(1 −xε) ≤1
2K. Consequently, we have
cεxε = K −
	
j /∈σ(xε)
cε
j
= K −L
	
j /∈σ(xε)
ˆcj
= K −L(K −ˆcxε) = εK + Lˆcxε
which gives us the bound:
max
c∈P(ˆc,ε) cxε ≥εK + Lˆcxε
(14.2)
Using Equations 14.1 and 14.2, we then obtain the following chain of inequalities:
max
c∈P(ˆc,ε) cxε ≥εK + Lˆcxε ≥εK + Lˆcx0 ≥
max
c∈P(ˆc,ε) cx0 ≥
max
c∈P(ˆc,ε) cxε
Thus, equality must hold throughout. This establishes the following two results:
max
c∈P(ˆc,ε) cx0 =
max
c∈P(ˆc,ε) cxε
(ﬁrst = last expression)
ˆcx0 = ˆcxε
(second = third expression and L > 0)
which completes this case.
Case 2: ˆcx0 ≤1
2K.
The dual objective value of any dual-feasible solution is an upper bound on the primal value,
cx 0. Choose π = 0, μ′ = x0, and λ = 0. This is clearly dual-feasible, and its dual objective
value is Uˆcx0. Therefore,
max
c∈P(ˆcε) cx0 ≤Uˆcx0
(14.3)
Now consider the value of ˆcxε. Suppose ˆcxε ≤1
2K. Then deﬁne cε
j = Uˆcj for j ∈σ(xε), and
note that cε ∈P(ˆc, ε). This is feasible (i.e., cε ∈P(ˆc, ε)) because cεxε ≤1
2K. It follows that
cεxε = Uˆcxε, so we have maxc∈P(ˆc,ε) cxε ≥Uˆcxε. On the other hand, suppose ˆcxε > 1
2K.
Then, deﬁne cε
j = Lˆcj for j /∈σ(xε), and note that cε ∈P(ˆc, ε). It follows from our analysis
in Case 1 that maxc∈P(ˆc,ε) cxε ≥εK + Lˆcxε. Taken together, this gives us the bound:
max
c∈P(ˆc,ε) cxε ≥min{Uˆcxε, εK + Lˆcxε}
(14.4)
Using Equations 14.3 and 14.4, we then obtain the following chain of inequalities:
max
c∈P(ˆc,ε) cxε ≥min{Uˆcxε, εK + Lˆcxε} ≥min{Uˆcx0, εK + Lˆcx0}
= Uˆcx0 ≥
max
c∈P(ˆc,ε)cx0 ≥
max
c∈P(ˆc,ε)cxε
The equality in this chain follows from our assumption that ˆcx0 ≤1
2K. We conclude that
equality
must
hold
throughout,
and
maxc∈P(ˆc,ε)cx0 = maxc∈P(ˆc,ε) cxε.
Furthermore,
this shows that Uˆcx0 = min {Uˆcxε, εK + Lˆcxε} (fourth expression = second), so either

Robust Optimization
14-13
Uˆcx0 = Uˆcxε or Uˆcx0 = εK + Lˆcxε. In the former case, we have immediately that ˆcx0 = ˆcxε.
In the latter case, we have the following chain of inequalities:
Uˆcx0 ≤εK + Lˆcx0 ≤εK + Lˆcxε = Uˆcx0
As equality must hold throughout, we conclude ˆcx0 = ˆcxε.
■
Example 14.2
Let X = {(1, 0), (0, 1), (1, 1)} and ˆc = (1, 2). The central solution is ˆx = (1, 0), and the worst-
case model is
min
x∈X max
c
c1x1 + c2x2 :
1 −ε ≤c1 ≤1 + ε
2 −2ε ≤c2 ≤2 + 2ε
c1 + c2 = 3
For each x, the maximization problem is a linear program with one equality constraint.
Therefore, an optimal c occurs at one of the four basic solutions. Of these, only two are
feasible: c = (1 ± ε, 3 −c1). The robust optimization problem is thus:
min
x∈X max{(1 + ε)x1 + (2 −ε)x2, (1 −ε)x1 + (2 + ε)x2}
We have
max {(1 + ε)x1 + (2 −ε)x2, (1 −ε)x1 + (2 + ε)x2} = ˆcx + max {ε(x1 −x2), ε(x2 −x1)}
The robust optimization model is thus equivalent to
min
x∈X{ˆcx + ε|x1 −x2|}
The central solution yields ˆc1 + ε, which dominates the value of x = (0, 1). The value of
x = (1, 1) is ˆc1 + ˆc2, which is 3. This is greater than the central solution value for ε < 1.
The relations that make this example result in the optimality of the central solution are
used in the proof. In particular, if c2 > ˆc2, we must have c1 = (1 −ε)ˆc1, in which case
c2 = K −c1 = ˆc2 −εˆc1. Thus, cx = ˆcx −2εˆc1x2 ≤ˆcx, so c = ˆc is a better value than c for
any x, whence ˆc is a maximum of the rival “player,” for any x.
Now consider the following variation of the portfolio problem. Instead of allowing capital
to be split among assets in arbitrary fashion, let xj be a binary variable that equals 1 if we
select asset j (0 otherwise). Then, let X = {x ∈{0, 1}n : ax ≤b}, where aj is the amount of
capital needed to invest in asset j and b is the total budget.
The interval uncertainty model of Theorem 14.3 assumes that the return is known to be
within the bounds, (1 −ε)ˆc to (1 + ε)ˆc for some ε ∈[0, 1), and that the sum of the returns
is the same for all possible realizations. Theorem 14.3 tells us that the assets selected are
those that solve the central value problem:
max
x∈X ˆcx
which is a knapsack problem.
Our example in Table 14.1 does not satisfy these interval assumptions. However, if we
keep the same expected returns, with ˆc = (1000, 50000), the solution for a2 ≤b < a1 + a2 is
x∗= (0, 1), assuming the actual returns satisfy
(1 −ε)(1000, 50000) ≤R ≤(1 + ε)(1000, 50000)
for some ε ∈[0, 1) and R1 + R2 = 51000.

14-14
Operations Research Methodologies
The motivating application for Theorem 14.3 was the following sensor placement problem
[17]. We are given a water distribution network that could be contaminated by some entry
at a node. Sensors can be placed on the pipes (arcs) to detect the contaminant when it ﬂows
by, and we wish to minimize the expected number of nodes that the contaminant reaches
undetected. Let xij denote a binary variable that is 1 if, and only if, node j is contaminated
without detection, given the entry is at node i. (These are actually auxiliary variables in a
model where the primary variables are where to place the sensors.) The problem is
min
x∈X
	
i
	
j∈Ri
αixij
where Ri is the set of reachable nodes from node i.
The contaminant entry probabilities (α) are estimated using data and expert judgment,
and they are subject to error. We know 
i αi = 1, and we assume (1 −ε)ˆα ≤α ≤(1 + ε)ˆα,
where ˆα is the central value (viz., the original estimate). The situation meets the assump-
tions of Theorem 14.3, so we can solve the worst-case robust optimization model by solving
the central problem:
min
x∈X
	
i
	
j∈Ri
ˆαixij
Yaman et al. [18] proved a similar theorem for the minimum spanning tree problem, where
they deﬁned the central solution to be a permanent solution, as it remains optimal under
perturbations of the data.
14.3.3
Minimax Regret
Another robust optimization model is to minimize the regret, which we now deﬁne. For each
scenario (k), let zk be the optimal objective value if the data were known with certainty—
that is,
zk = max
x∈X{ckx : Dkx ≥dk}
(The problem could be minimization, but the regret value given below is the same—the
greatest amount that a candidate solution deviates from zk.)
For the robust deviation decision, deﬁne the objective value of x ∈X : Dkx ≥dk to be the
maximum regret:
f(x) = max
k {zk −ckx}
For the relative robust decision, assume zk > 0 for all k, and deﬁne the objective value of
x ∈X : Dkx ≥dk to be the maximum regret normalized by zk:
f(x) = max
k
zk −ckx
zk
= 1 −min
k
ckx
zk
Then, the certainty equivalent models are, respectively:
min
x∈X max
k {zk −ckx} : Dkx ≥dk
for all k = 1, . . ., K
and
max
x∈X min
k
ckx
zk

: Dkx ≥dk
for all k = 1, . . ., K

Robust Optimization
14-15
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5 × 106
Investment in asset 1
Regret
 
Scenario 1
Scenario 2
Scenario 3
FIGURE 14.4
Regret functions for each scenario in example.
For the investment problem (Table 14.1), we have z1 = 1050, z2 = 1,000,000, and z3 =
125,000. Figure 14.4 shows the regret functions, zk −Rkx (with x2 = 1 −x1) for k = 1, 2, 3.
The minimum regret solution is x∗= ( 2
3, 1
3).
As long as the solution is obtained for each scenario, it is useful to present how that
solution fares in the other scenarios. This gives rise to a square array of the form:
Policy
1
2
· · ·
K
x1
z1
c2x1
· · ·
cKx1
x2
c1x2
z2
· · ·
cKx2
...
...
...
...
xK
c1xK
c2xK
· · ·
zK
The oﬀ-diagonal terms are well deﬁned when the policy is feasible (for example, x1 is
feasible in scenario 2). Otherwise, the table could show the infeasibility, or there can be
some recourse function to penalize the infeasibility consistently across scenarios.
One might add other information, such as the probability of each scenario, if available.
One can then compare these solutions with the minimax robust solution for additional
judgments about its quality. For the example, we have the array shown in Table 14.4.
The normalized regret problem is
max
0≤x1≤1min
1050x1 −2,000,000(1 −x1)
1050
, 975x1 + 1,000,000(1 −x1)
1,000,000
, 1000x1 + 125,000(1 −x1)
125,000

The solution is x∗= (0.9995, 0.0005).

14-16
Operations Research Methodologies
TABLE 14.4
Scenario Solutions for Example
Scenario
Policy
1
2
3
E[x]
x′V x
x1 = (1, 0)
1050∗
975
1000
1000
750
x2 = (0, 1)
−2,000,000
1,000,000∗
125,000
50,000
1,203,750,000,000
x3 = (0, 1)
−2,000,000
1,000,000
125,000∗
50,000
1,203,750,000,000
x∗= ( 2
3 , 1
3 )
−1,331,233
333,983
42,333
17,333
133,736,667,000
∗Optimal.
TABLE 14.5
Uncertainty Sets for Example Investment Problem
ω
{R : |R1 −1000| + |R2 −50,000| ≤ω}
<75000
∅
[75000, 950025)
{(1000, 125000)}
[950025, 2050050)
{(1000, 125000), (975, 1000000)}
≥2050050
{(1000, 125000), (975, 1000000), (1050, −2000000)}
14.3.4
Uncertainty Sets
Ben-Tal and Nemirovski [2,19] introduced the model:
max
(z,x) z : x ∈X
z ≤ckx, yk = max {0, dk −Dkx}, ∥yk∥≤rk
for k = 1, . . ., K
The key here is the representation of uncertainty by limiting the violation (y) by r. For
rk = 0, we require yk = 0, which means we require that the constraint for scenario k hold:
Dkx ≥dk. Thus, this includes the worst-case model with all scenarios required to hold for
x to be feasible. More generally, with r > 0, the feasible set is ellipsoidal, and it includes the
Markowitz mean-risk model by deﬁning the norm to be √y′V y. Furthermore, the objective
constraints can be merged with the others, so the uncertainty of c can be included in the
uncertainty of [D d].
We use the extended framework by Chen et al. [20]. Let ( ˆD, ˆd) be some central (or
reference) value, and deﬁne the uncertain set of data: Uω = {(D, d) : ∥(D, d) −( ˆD, ˆd)∥≤ω},
where ω ≥0 is called the budget of uncertainty. Then, the feasible region is deﬁned as
X(ω) = {x ∈X : Dx ≥d
for all [D d] ∈Uω}
An important observation by Chen et al. [20] is that under mild assumptions the set of x
satisfying the chance constraint, P(Dx ≥d) ≥α, contains X(ω) for some ω.
For the investment problem in Table 14.1, let the central value of the returns be their
expected values. Then, using the absolute value norm, we have the uncertainty sets shown
in Table 14.5.
This particular approach is among the most active areas of robust optimization research.
See Ref. [21] for an overview and recent results in theory, algorithm design, and applications.
14.4
More Applications
In this section, we describe more applications and apply diﬀerent robust optimization models
to them.
Job Sequencing. A batch of n jobs arrives into a system and each must be processed on a
machine. We can decide the order. Job j takes tj minutes to be completed. If we sequence

Robust Optimization
14-17
the jobs as 1, 2, . . ., n, job 1 will be in the system t1 minutes and job 2 will be in the
system t1 + t2 minutes; in general, job j will be in the system t1 + · · · + tj minutes. The
total job-time in the system is the sum:
T =
n
	
i=1
(n −i + 1)ti
If each job’s time is known with certainty, an optimal sequence, one which minimizes total
job-time T, is by descending order: t1 ≤t2 ≤· · · ≤tn. This is called the shortest processing
time (SPT) rule.
In general, a sequence is represented by a permutation, π = (π1, . . ., πn), so the total
job-time in the system is
T(π) =
n
	
i=1
(n −i + 1)tπi
Suppose the job-times are not known with certainty. One rule is to order them by their mean
values. In particular, if ti is uniformly distributed on [ai, bi], the mean value is mi = 1
2(ai + bi)
and the variance is 1
12(bi −ai)2. To illustrate, consider four jobs with intervals [5, 50], [15, 45],
[22, 40], and [31, 35]. Sequencing by their means, the expected total job-time is given by:
E[T(π)] = 4m1 + 3m2 + 2m3 + m4 = 4(27.5) + 3(30) + 2(31) + 33 = 295
A worst-case robust solution sorts the jobs by their max times. That sequence is π = (4, 3,
2, 1), and the expected total job-time is
E[T(π)] = 4b4 + 3b3 + 2b2 + b1 = 4(35) + 3(40) + 2(45) + 50 = 400
Suppose the job-times are independent. Their variances are σ2 = 1
12(452, 302, 182, 42) =
(168 3
4, 75, 27, 1 1
3). Thus, for π = (1, 2, 3, 4),
V (T(π)) = 16(168 3
4) + 9(75) + 4(27) + 1 1
3 = 3,484 1
3
For π = (4, 3, 2, 1), V (T(π)) = 733.1, which is much less.
More generally, Figure 14.5 shows the eﬃcient frontier of E[T] versus V (T); the mean-
value ordering is at one end, and the max-value ordering is at the other.
Newsboy Problem. This is a classical problem in operations research, which formed the
basis for the early inventory models. A newspaper is concerned with controlling the number
of papers to be distributed to newsstands. The cost of a paper varies, and the demand is
a random variable, d, with probability function P(d). Unsold papers are returned, where
millions of dollars are lost in the production cost. It is possible, however, for a newsstand to
order more papers the same day. There are holding and shortage costs. The proﬁt function
for ordering Q papers with demand d is given by:
f(Q, d) =
(p −c)Q −g(d −Q)
if Q ≤d
(p −c)d −(c −s)(Q −d)
if Q ≥d
where p = sales price, c = cost, g = shortage cost, and s = salvage value of unsold papers.
(s = 0 in the original newsboy problem, but we include it here, as it appears in the more
general extensions.)
The decision variable is Q, the amount ordered. An expected-value model seeks to maxi-
mize E[f(Q, d)] = 
d f(Q, d)P(d). If demand has a signiﬁcant variance, the newsboy could
miss opportunities for sales, or he could be stuck with a lot of rather worthless newspapers.

14-18
Operations Research Methodologies
500
1000
1500
2000
2500
3000
3500
294
296
298
300
302
304
306
308
310
312
314
Variance
Expected time
π  (4,3,2,1)
π  (1,2,3,4)
FIGURE 14.5
Eﬃcient frontier for mean-variance model of scheduling example.
The worst-case robust optimization model seeks to maximize mindf(Q, d). For known
demand, the optimal ordering quantity is the demand and the maximum proﬁt is z(d) =
(p −c)d. Thus, the minimax regret model is:
min
Q max
d {z(d) −f(Q, d)}
For example, suppose d has only two values: d < ¯d. Then, the worst-case robust optimization
model is:
max
d≤Q≤¯d F(Q)
where
F(Q) = min
d {f(Q, d)}
= min{f(Q, d), f(Q, ¯d)}
= min{(p −c)d −(c −s)(Q −d), (p −c)Q −g( ¯d −Q)}
= min{(p −s)d −(c −s)Q, (p −c + g)Q −g ¯d}
The solution occurs where (p −s)d −(c −s)Q = (p −c + g)Q −g ¯d, which is at Q∗=
p −s
p −s + gd +
g
p −s + g ¯d. This is independent of the cost of a newspaper (c) because both
outcomes incur the cost cQ.
The absolute minimax regret model is given by:
max{z(d) −f(Q, d), z( ¯d) −f(Q, ¯d)} = max{(c −s)(Q −d), g( ¯d −Q)}
⇒Q∗=
c −s
c −s + g d +
g
c −s + g
¯d
This says that the minimax regret ordering quantity is in the interval bounded by the two
possible demands, where the weight on the lower bound is the cost minus salvage value
and the weight on the upper bound is the shortage cost. Thus, if the salvage value or the

Robust Optimization
14-19
shortage cost is nearly zero, the minimax absolute regret solution is near the lower bound,
accepting the possible shortage (d −d). If the shortage cost dominates, the solution is near
the upper bound, accepting the possible excess ( ¯d −d). If c −s = g, the solution is at the
midpoint because the shortage and excess recourse costs are the same.
Assuming z(d) > 0 for all d, the normalized regret model is
max
Q
min
d
f(Q, d)
z(d)
For the two-demand case,
min
f(Q, d)
z(d) , f(Q, ¯d)
z( ¯d)

= min

1 −(c −s)(Q −d)
(p −c)d
, 1 −g( ¯d −Q)
(p −c) ¯d

⇒Q∗= d ¯d(c −s + g)
(c −s) ¯d + gd
This is also in the interval [d, ¯d], with coeﬃcients
Q∗=
(c −s) ¯d
(c −s) ¯d + gdd +
gd
(c −s) ¯d + gd
¯d
Unlike the absolute regret solution, these coeﬃcients depend upon the bounds. In particu-
lar, as ¯d →∞, Q∗→d(1 +
g
c−s), whereas in the absolute regret solution, Q∗→∞(assuming
gd > 0), as does the worst-case robust optimization solution.
Revenue Management [22]. A hotel can sell a room for a low price of $100, or it can hold
the room in hope of a customer who will pay the regular price of $160. The regular demand
is uncertain, but the hotel estimates that its demand will have one room available with
probability 0.25; the probability of no surplus rooms is 0.75. The decision is whether to sell
now at the lower price.
Table 14.6 shows the decision process and the outcome possibilities. The maximum
expected revenue is $120 by holding the room. However, it has a high variance, compared to
a zero variance for selling. A worst-case robust solution is to sell because we are guaranteed
$100, while holding the room could result in zero revenue.
The maximum revenue for each possible outcome is z0 = 160 (hold) and z1 = 100 (sell).
The minimax absolute regret solution is thus:
min{max{z0 −100, z1 −100},



Sell
max{z0 −160, z1 −0}



Hold
}
= min{60, 100} = 60 (sell)
The relative regret solution is:
min

max
z0 −100
z0
, z1 −100
z1

, max
z0 −160
z0
, z1 −0
z1

= min{ 3
8, 1} = 3
8 (sell)
TABLE 14.6
Outcomes for Sell or Hold Decision
Demand
Surplus
Expected
Decision
0
≥1
Revenue
Variance
Sell
100
100
100
0
Hold
160
0
120
4800
Probability
0.75
0.25

14-20
Operations Research Methodologies
In this example, the worst-case robust optimization model has the same decision as the
two minimax regret models, which is to sell the room at the discount price. These run
counter to the expected-value solution, but a risk term could be added, as in the Markowitz
modeling paradigm, in which case there is a price of risk that renders the optimal solution
to sell.
Transportation Problem. Another classical OR problem is a min-cost shipment from
sources to destinations. Let xij = amount shipped from source i to destination j. The data
are unit shipment costs (cij), supplies (si), and demands (dj). Here is the standard model:
min
m
	
i=1
n
	
j=1
cijxij : x ≥0
n
	
j=1
xij ≤si
(supply limit)
m
	
i=1
xij ≥dj
(demand requirement)
When m = n and si = dj = 1 for all i, j, this is the standard assignment problem, and xij = 1
if person i is assigned to job j (otherwise, xij = 0).
Let us ﬁrst assume that s and d are known, so the uncertainty is only in the costs. A
worst-case robust optimization model has the form:
min
x∈X max
c∈C
m
	
i=1
n
	
j=1
cijxij
where X = {x ≥0 : n
j=1 xij ≤si, m
i=1 xij ≥dj}, and C is the set of possible cost matrices.
The assignment problem satisﬁes the conditions of Theorem 14.3 if there exists a central
value (ˆc) and
C =

c : (1 −ε)ˆc ≤c ≤(1 + ε)ˆc,
	
i,j
cij =
	
i,j
ˆcij

In that case, the central solution is all we need to solve this for all ε ∈(0, 1).
If this interval uncertainty is not the case, or if this is not the assignment problem,
suppose there is a ﬁnite set of scenarios, indexed by k = 1, . . ., K. The worst-case robust
optimization model is given by:
min z : x ∈X, z ≥
m
	
i=1
n
	
j=1
ck
ijxij
for k = 1, . . ., K
Deﬁne the minimum cost for each scenario:
zk = min
x∈X
m
	
i=1
n
	
j=1
ck
ijxij
The absolute minimax regret model is the linear program:
min z : x ∈X, z ≥
m
	
i=1
n
	
j=1
ck
ijxij −zk
for k = 1, . . ., K

Robust Optimization
14-21
The relative minimax regret model is also a linear program:
min z : x ∈X, z ≥
m
	
i=1
n
	
j=1
ck
ijxij −zk
for k = 1, . . ., K
Now consider uncertain supplies or demands. The recourse model puts additional variables
associated with each scenario:
min
	
i,j
E[cij]xij + Φ(u, v) :
	
j
xij −uk
i = sk
i , ∀i
	
i
xij −vk
j = dk
j , ∀j
for k = 1, . . ., K
where Φ penalizes supply shortage (uk
i > 0) and demand shortage (vk
j > 0).
The robust optimization model introduced by Mulvey et al. [14] includes penalty functions
akin to the Markowitz mean-variance model:
min
	
i,j
E[cij]xij +
	
k
λk
 	
i
U 2
ik +
	
j
V 2
jk

: x ≥0
	
j
xij −uk
i = sk
i , Uik ≥0, Uik ≥uk
i ∀i
	
i
xij + vk
j = dk
j , Vjk ≥0, Vjk ≥vk
j ∀j
for k = 1, . . ., K
where λ > 0 is a vector of parameters that represents a cost of shortage. The penalty costs
can vary by scenario, which could include the probability of the scenario’s occurrence.
If 
i xij ≤sk
i , Uik = 0 is feasible, so there is no penalty. Similarly, if 
j xij ≥dk
j , Vjk = 0 is
feasible, so there is no penalty. If either Uik > 0 for some i or Vjk > 0 for some j, the penalty is
λk times the sum-squares of those shortages. If we follow the Markowitz modeling paradigm,
we have only one risk parameter, with λk = λ for all k.
Example 14.3
Figure 14.6 shows a 2 × 3 transportation network with supply and demand data for two
scenarios.
1
2
1
2
3
d1  (15, 20)
d2  (10, 25)
d3  (5, 15)
s1  (20, 35)
s2  (10, 25)
Demand
Supply
FIGURE 14.6
Transportation network example.

14-22
Operations Research Methodologies
TABLE 14.7
Optimal Flows for Transportation Example
Policy
Flow
Cost
Shortage
Scenario 1
5
10
5
365
0
10
0
0
Scenario 2
0
20
15
720
0
20
5
0
Recourse
0
20
15
750∗
15∗
20
5
0
∗Expected value.
The unit costs are given by:
c1 =
10
12
15
12
15
18

and
c2 =
15
12
10
13
14
15

The optimal solution to each scenario is shown in Table l4.7, along with a recourse solution
that requires demand feasibility in each scenario.
The minimax regret models are infeasible because they require a ﬂow to satisfy supply
and demand constraints for each scenario. This means x is restricted by:
	
j
xij ≤min
k
sk
i , ∀i
	
i
xij ≥max
k
dk
j , ∀j
This is infeasible whenever 
i minksk
i < 
j maxkdk
j , as in the example.
We consider a robust optimization model that penalizes shortages but does not require
supply and demand feasibility in all scenarios. Further, we use a common risk penalty
parameter (λ). For this problem, we thus have the robust optimization model:
min E[c]x + λ
	
k
 	
i
uk
i +
	
j
vk
j

:
	
j
xij −uk
i ≤sk
i , ∀i, k
	
i
xij + vk
j ≥dk
j , ∀j, k
x, u, v ≥0
With λ = 0 and the added constraints uk
i = 0 for all i, k, we obtain the recourse model
solution shown in Table 14.7. Here we allow shortages to occur at a supply node (uk
i > 0)
or at a demand node (vk
j > 0); the penalty is the same, namely λ for each unit. Figure 14.7
shows the expected cost versus shortage as λ varies.
The curves are mostly ﬂat, except that there is a sharp ramp from λ = 5.8 to λ = 6.25.
Table 14.8 shows the optimal ﬂows; note the rise in x12, then suddenly to the ﬂows that
comprise the optimal solution for scenario 1.

Robust Optimization
14-23
0
2
4
6
8
10
0
100
200
300
400
500
600
Shortage
Cost
0
100
200
300
400
500
600
0
50
100
150
200
250
300
350
400
Shortage
Cost
λ
FIGURE 14.7
Costs and shortages for transportation example.
TABLE 14.8
Optimal Flows as
Functions of λ
λ
x∗
0–5.9
0
0
0
0
0
0

6
0
4.4
0
0
0
0

6.02–6.24
0
10
0
0
0
0

≥6.29
 5
10
5
10
0
0


14-24
Operations Research Methodologies
As an alternative, consider an uncertainty-set robust optimization model, where the sup-
plies and demands are correlated to be feasible:
D =

(s, d) ≥0 :
	
i
si ≥
	
j
dj

In particular, we could have a central value and maintain constant-sum supplies and constant-
sum demands, allowing individual supplies and demands to vary independently within the
two degrees of freedom lost for the constant-sum equations. Then,
Uω = {(s, d) ∈D : ∥(s, d) −(ˆs, ˆd)∥≤ω}
The robust optimization model is
min
	
i,j
cijxij : x ≥0,
	
j
xij ≤si,
	
i
xij ≥dj

∀(s, d) ∈Uw
The robust models for cost uncertainty can be included as well. The net result is that we
can ﬁnd a shipment (or assignment) that is feasible in all admissible scenarios (i.e., in Uω),
and for which we minimize the maximum cost (or maximum regret).
A problem with this uncertainty-set model is the likelihood of infeasibility. As in the
example, we could wind up with the impossibility:
min
(s,d)∈Uω
 	
i
si −
	
j
dj

< 0
If the central value has surplus supply (i.e., 
i ˆsi > 
j ˆdj), this infeasibility can be mit-
igated by choosing ω suﬃciently small. This is appropriate when the robustness sought is
with respect to insensitivity to small perturbations in the data.
The standard transportation problem can be modiﬁed in several ways. One is to suppose
the network is sparse—not every supplier can ship to every consumer. This is realistic and
makes the deterministic problem much more tractable for very large networks. However,
the topology of the network could then become a source of uncertainty, which renders the
robust optimization models diﬃcult to solve. Another extension is to add uncertain bounds
on the ﬂows: x ≤U, where U is the capacity of arc <i, j>. If major variations are allowed,
the bounds could control the topology by having Uij = 0 mean that arc xij is absent. There
could also be gains or losses during shipment, and those values could be uncertain. Again,
if large variations are possible, a 100% loss value essentially removes the arc from the
network. In general, robust optimization models can deal eﬀectively with insensitivity to
small ﬂuctuations in the data, but allowing the topology to change is a hard problem.
Capacity Expansion. Consider the following power system problem [23,24]. Electricity
can be generated by diﬀerent types of plants, like coal-ﬁred boilers, oil-ﬁred turbines, and
nuclear power plants. During a cycle, like 24 hours, the demand for electricity varies, and
that load is sorted into the load–duration curve, as shown in Figure 14.8. The curve is
approximated by three steps, called modes: (1) base, which is ongoing demand for electricity
throughout the region being serviced; (2) peak, which is the greatest amount, but for a short
duration, usually during mid-day for temperature control in business and residence; and (3)
intermediate, which is between the base and peak loads.

Robust Optimization
14-25
TI
TP
TB  24
dP/TP
dI /TI 
dB/TB
Intermediate
Base
Load 
(demand)
Peak
Hours
FIGURE 14.8
Load–duration curve for example electric power system.
The approximation allows us to model electricity generation and demand satisfaction as
a linear program:
min
	
i
fixi +
	
j
Tj
	
i
cijyij : x, y ≥0
	
j
yij −xi ≤0
(capacity limit)
Tj
	
i
yij ≥dj
(demand)
where xi is the amount of capacity of type i that is made available (fi = annualized capital
cost), and yij is the level of operation of plant i to fulﬁll demand during mode j (cij is its
unit operating cost). The demand, dj, is by mode (base, intermediate, and peak).
The capacity decisions (x) must be made before the demands are known. Then, y must
operate within those capacity limits, which could result in a shortage (where dj is greater
than expected) or excess capacity (where dj is less than expected).
One robust optimization model is to prepare for the worst case—that is, the greatest
demand in each mode. This is likely to be overly pessimistic because demands might shift,
rather than move in parallel across modes, in which case we would inevitably incur excess
capacity.
Here is a scenario-based robust optimization model:
min fx + E[ζ] + λ1E[(ζ −E[ζ])2] + λ2E[∥κk∥2] + λ3E[∥δk∥2]
κk
i ≥xi −
	
j
yk
ij
∀i, k
(excess capacity)
δk
j ≥dk
j −T k
j
	
i
yk
ij
∀j, k
(shortage)
x, y, κ, δ ≥0,
where κk is the excess capacity vector and δk is the shortage vector for scenario k. (Note:
κk
i = max{0, xi −
j yk
ij} and δk
j = max{0, dk
j −T k
j

i yk
ji}.) The random variable, ζ, is
deﬁned over scenarios:
P

ζ =
	
j
T k
j
	
i
cijyk
ij

= sk

14-26
Operations Research Methodologies
The multipliers are
λ1 = usual price of risk, as in the Markowitz mean-risk model
λ2 = price of having excess capacity
λ3 = price of having shortage.
To illustrate, Table 14.9 gives data for four types of plants (taken from Refs. [16] and
[21]), and Table 14.10 gives the demands for each of four scenarios.
Now consider the regret models. We have
zk = min
	
i
fixi +
	
j
Tj
	
i
cijyij : x, y ≥0
	
j
yij −xi ≤0
(capacity limit)
Tj
	
i
yij ≥dk
j
(demand)
(Note: T and c are the same for all scenarios.)
Table 14.11 shows the table introduced in Section 14.3.3. The diagonal values are the
scenario optima, z1, . . ., z4. The oﬀ-diagonal entries reﬂect the cost of (xp, ykp), the best
possible response to scenario k given that the capacity decisions (xp) were ﬁxed assuming
scenario (p). We assume each scenario is equally-likely, so sk = 1
4, ∀k.
Table 14.12 shows the optimal capacities for each of the seven policies—x1, . . ., x4 are
the scenario optima; xA, xR are the absolute and relative minimax regret solutions, and xE
is the expected-cost optimum.
TABLE 14.9
Supply Options and Associated
Costs for Example
Annualized
Operating Cost
Plant
Capital Cost
(same for all modes)
A
200
30
B
500
10
C
300
20
D
0
200
TABLE 14.10
Demand Loads and Durations for Scenarios
Base
Intermediate
Peak
Scenario
Load
Duration
Load
Duration
Load
Duration
1
8
24
6
12
4
6
2
8.67
24
7
10
1.33
5
3
9
24
7.33
10
1.67
4
4
8.25
24
5
12
2.5
4
TABLE 14.11
Scenario Optima over Scenarios
Scenario
Expected
Policy
1
2
3
4
Cost
1
10,680.0∗
10,608.1
10,899.4
10,110.0
10,574.38
2
11,620.4
10,381.3∗
11,398.8
9,980.2
10,855.13
3
10,860.0
10,614.3
10,859.4∗
10,200.0
10,633.43
4
12,560.0
11,210.6
12,561.0
9,605.0∗
11,484.15
E[Cost]
10,715.0
10,898.1
10,889.4
10,055.0
10,055.00∗
Minimax
Regret Solutions
Absolute
11,048.6
10,749.9
11,228.0
9,973.6
10,750.05
Relative
11,081.1
10,771.2
11,267.2
9,965.7
10,771.30
∗Optimal.

Robust Optimization
14-27
TABLE 14.12
Optimal Capacities
Plant
Policy
A
B
C
D
x1
4.00
8.00
6.00
0
x2
8.33
8.67
0.00
1.00
x3
9.00
9.00
0.00
0
x4
2.50
8.25
5.00
2.25
xA
4.34
8.25
5.00
0.41
xR
4.30
8.25
5.00
0.45
xE
4.75
8.25
5.00
0
The absolute regret solution uses slightly more capacity of plant A, thus reducing the
amount of plant D needed to fulﬁll demand; otherwise, the two regret solutions are essen-
tially the same. The expected value solution is constrained to satisfy demand for each
scenario, and it does so without plant D. The diﬀerence with the regret solutions is the
amount of plant A capacity made available.
The total excess capacity in each case is 0, 1, 1.25, or 2.25, depending on the scenario
and policy. The expected value solution has 2.25 excess capacity of plant A in scenario 1,
while the regret solutions have only 2.05. All three solutions have no excess capacity in the
other plants.
Facility Location. A standard facility location problem is to choose the coordinates of a
point that minimizes the total (weighted) distance to each of a set of given points. The
points could represent population centers in a region and the weights are the populations.
The facility is one of service, like a hospital, and the objective is to provide an overall best
service by being centrally located. Here is a basic model:
min
x,y
m
	
i=1
widi(x, y)
where w ≥0 and di is the distance from (xi, yi) to (x, y). The distance is typically given by
some norm: di(x, y) = ∥(xi, yi) −(x, y)∥.
Suppose the problem is unweighted and we use the city-block distance (L1 norm):
min
x,y
m
	
i=1
(|xi −x| + |yi −y|)
First, we note that we can optimize each coordinate separately. Second, the minimum L1
placement is the median, given as follows. Let π be a permutation of the x values such
that xπ1 ≤xπ2 ≤· · · ≤xπm. If m is odd, the median is the mid-point, xπ m+1
2
Thus, 5 is the
median of 1,2,5,9,50. If m is even, the median can be any point in the interval such that half
the points are less and half are greater. Thus, any value in the interval (5, 9) is a median
of 1,2,5,9,50,90. We do the same thing to compute the median of {yi} to obtain the median
coordinate in the plane. We leave as an exercise that a median is the location problem
solution when using the L1 norm.
Now consider the Euclidean norm (L2). The location problem is given by:
min
x,y
m
	
i=1

(xi −x)2 + (yi −y)2
The solution to this problem is the mean: (x∗, y∗) = 1
m
m
i=1(xi, yi). Compared to the
median, this is very sensitive to the given coordinates, especially to outliers—a few points
far from the others. For that reason, the median is a more robust solution than the mean.

14-28
Operations Research Methodologies
The family of norms, Lp(v) = (
j |vj|p)
1
p , approaches L∞= maxj{|vj|}. The location
problem for the Lp norm is given by:
min
x,y
m
	
i=1
wi(|xi −x|p + |yi −y|p)
1
p
and as p →∞
min
x,y
m
	
i=1
wi max{|xi −x|, |yi −y|}
In statistical estimation, Lp is more robust than Lp+1 because it is less sensitive to the data
(both the weights and the given coordinates).
However, a robust optimization model does not sum the distances at all. It deﬁnes the
problem as:
min
x,y max
i {widi(x, y)}
We call this solution the center of the convex hull of the given points. It is insensitive to
small changes in the coordinates or weights of non-extreme points, and in that sense the
center is a robust solution. While there are two concepts of robustness, median vs. mean and
center vs. L∞, both pertain to insensitivity to the given data, but they have diﬀerent kinds
of sensitivities. The median, which is optimal with summing the L1 norms, is insensitive
to perturbations of all data values; it depends upon only their relative values. The center
depends upon the data of the outliers, which are extreme points of the convex hull. (The
converse is not true—not all extreme points are outliers.)
Figure 14.9 illustrates this for 9 points. Table 14.13 shows the objective values for each
of the three location points.
Engineering Design. The engineering design problem speciﬁes outputs (y) as a function
of inputs that consist of decision variables (x) and (non-controllable) parameters (p):
y = F(x, p)
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
90
100
Mean
Median
Center
FIGURE 14.9
Median, mean, and center are respective solutions for facility location using  L1, L2,
and max L1 objectives.

Robust Optimization
14-29
TABLE 14.13
Median, Mean, and Center Distances Using
 L1, L2, and max L1 Objectives
x
y
 L1
 L2
Max L1
Median
60
30
575∗
164.39
130
Mean
55.56
46.11
595.56
156.56∗
109.44
Center
50
50
605
157.88
100∗
∗Optimal.
TABLE 14.14
Variations in Output Current for (R, L) ∈R× L(ymin,ymax)L
R
L
ymin
ymax
max {|ymin −10|, |ymax −10|}
y
s
|y −10|
1
1
21.5
38.4
28.4
29.95
11.95
19.95
1
2
10.8
19.4
9.4
15.10
6.08
5.10
1
3
7.2
13.0
3.0
10.10
4.38
0.10
2
1
13.1
20.7
10.7
16.90
5.37
6.90
2
2
9.0
15.2
5.2
12.10
4.38
2.10
2
3
6.6
11.5
3.4
9.05
3.46
0.95
3
1
8.0
12.2
2.2
10.10
2.97
0.10
3
2
6.8
10.7
3.2
8.75
2.76
1.25
3
3
5.5
9.1
4.5
7.30
2.54
2.70
Some parameters are random variables. If the actual value of a decision variable is ran-
dom, the optimization model uses the mean of its distribution as the decision variable and
accounts for its variation in some way.
There is a target value, denoted T, and a robust design is one that chooses x (or its
mean) to minimize some combination of the distance between y and the target, E [∥y −T∥],
and variation around that, typically with a standard deviation risk metric. The model is
similar to the Markowitz mean-risk model, but not quite the same. We illustrate with some
examples.
Electric circuit example [25,26]. We want to build an AC circuit with output current
given by
y =

V
R2 + (2πfL)2
where V is the input voltage, R is the resistance, f is the frequency, and L is the self-
inductance. Resistance and self-inductance are decision variables, restricted to the sets
R = {1 ohm, 2 ohms, 3 ohms} and L = {1 henry, 2 henries, 3 henries}, respectively. Volt-
age and frequency are not decision variables, but they have some noise—that is, V and f
are random variables with known means and variances.
For each (R, L) ∈R × L, there are observed deviations in y with ymin, ymax being the
minimum and maximum noise values, shown in Table 14.14. The y is the observed mean
value of y, and s is its observed standard deviation. The last column is the distance of the
mean from the target of 10 amps.
The design parameters that minimize the distance from the target is (R, L) = (1, 3), (3, 1).
The former has a deviation of 4.38, while the latter has a deviation of 2.97. Thus, the lat-
ter would be preferred. However, the overall min-deviation solution is with (R, L) = (3, 3),
though its distance from the target is 2.70. One worst-case robust optimization model is
given by:
min
R∈R
L∈L
max{|ymin(R, L) −10|, |ymax(R, L) −10|}
The solution to this model is (R, L) = (3, 1).

14-30
Operations Research Methodologies
TABLE 14.15
Design Variables and Parameters Data
Variable/
Standard
Parameter
Mean
Deviation
Range
FPR
μFPR
0.1
1.25 ≤μFPR ≤1.6
JVR
μJVR
100
0.6 ≤μJVR ≤0.9
CET
μCET
1.5
2400 ≤μCET ≤4000
HCPR
μHCPR
0.5
10.2 ≤μHCPR ≤25
LCPR
μLCPR
0.1
1.15 ≤μLCPR ≤4.9
htce
0.891
0.0033
n/a
hte
0.933
0.0033
n/a
ltc
0.9
0.0033
n/a
Using the extreme values (ymin, ymax) is overly conservative, as they are obtained by a
large number of experiments. Instead, consider a mean-risk model, where two objectives are
to minimize the deviation from the target, |y(R, L) −10|, and the standard deviation about
the mean, s(R, L). We deﬁne a solution (R, L) to be robust if there does not exist another
solution (R′, L′) for which |y(R′, L′) −10| ≤|y(R, L) −10| and s(R′, L′) ≤s(R, L). Here we
have three robust solutions: (1, 3), (3, 2), and (3, 3).
Engine design example [27]. We have ﬁve design variables: fan pressure ratio (FPR),
exhaust jet velocity ratio (JVR), combustor exit temperature (CET), high compressor pres-
sure ratio (HCPR), and low compressor pressure ratio (LCPR). In addition, there are three
parameters: high turbine compressor eﬃciency (htce), high turbine eﬃciency (hte), and
low turbine eﬃciency (lte). The design variables and the parameters are each normally
distributed with given standard deviations, shown in Table 14.15.
The aircraft’s range is a nonlinear function of the design variables and parameters. The
robust optimization model has the form of Markowitz’s mean-risk model, except that the
design engineer uses standard deviation instead of variance:
max μRange −λσRange
where λ > 0 is chosen to reﬂect the relative importance of robustness.
Figure 14.10 gives four solutions. One is the expected-value solution (λ = 0), one is the
worst-case robust solution (λ = ∞), and two are in between. The maximum expected range
has a chance of having the aircraft’s range be less than that of the worst-case robust solu-
tion. The optimal robust solution results with μRange = 2200.52 and σRange = 54.1189, so
μRange ± 3σRange = [2038.1658, 2362.8792]. The optimal expected range results with μRange =
2671.1454 and σRange = 225.7006, so μRange ± 3σRange = [1994.0436, 3348.2472]. Which solu-
tion is best (among the four) depends on what value we place on the range. If being less than
2050 has dire consequences, the robust solution is preferred over the expected-value solution.
14.5
Summary
We have seen that robust optimization can be a worst-case model, minimax regret, or a
model of uncertainty that simply restricts the solution space in a way that avoids “too much”
deviation from some central value. Collectively, these capture much of the essence of the
classical models within their scope, except that robust optimization avoids the explicit need
for probability distribution information within the model. This diﬀerence has been key,
but current research is building a more inclusive framework, addressing the desire to use
whatever information is available.
In this view, robust optimization falls ﬁrmly in the tradition of stochastic programming
paradigms. Historically, robust optimization has emerged from the three classical paradigms,
as shown in Figure 14.11. The diagram represents fundamental concepts, like risk, ﬂexibility,

Robust Optimization
14-31
1800
2000
2200
2400
2600
2800
3000
3200
3400
0
1
2
3
4
5
6
7
8 × 10−3
Range
Distribution
 
 
Expected value
Moderate 1
Moderate 2
Worst−case
FIGURE 14.10
Solutions to the engine design example.
Recourse
1955
Chance constrained
1958
1968
Robust optimization
Worst-case
Absolute regret
Relative regret
Uncertainty sets
Mean-risk
1952
Robust − flexibility 
Robust − volatility
Robust
statistics
FIGURE 14.11
Historical perspective of stochastic programming modeling paradigms.
recourse, and the meaning of constraining for robustness. An algorithmic view would add
divisions by underlying structures, like continuous versus discrete.
The key elements of robust optimization are volatility and ﬂexibility. The former asks for
a solution that is relatively insensitive to data variations and hedges against catastrophic
outcomes. The latter is concerned with keeping options open in a sequential decision process
having recourses for the eﬀects of earlier decisions.
An area of concern, which we did not have space to present fully, is the computational
diﬃculty to solve instances of a robust optimization model. Kouvelis and Yu [15] address
this with complexity theory; the uncertainty set model by Ben-Tall and Nemirovski [2,19]
exploits the tractability of semi-deﬁnite conic programming; and, Carr et al. [17] show a pro-
gression of incremental computational diﬃculty for a particular sensor placement problem,
starting with the simple case expressed in Theorem 14.3.

14-32
Operations Research Methodologies
Acknowledgments
We thank colleagues who reviewed an earlier version of this chapter: Ignacio Grossman,
Istvan Maros, Frederic H. Murphy, Bora Tarhan, and H. Paul Williams.
References
1. J. Rosenhead. Robustness analysis: Keeping your options open. In J. Rosenhead (ed.),
Rational Analysis for a Problematic World: Problem Structuring Methods for Complexity,
Uncertainty and Conﬂict, pages 193–218, John Wiley & Sons, Chichester, UK, 1989.
2. A. Ben-Tal and A. Nemirovski. Robust optimization—methodology and applications.
Mathematical Programming, Series B, 92:453–480, 2002.
3. D. Bertsimas and M. Sim. The price of robustness. Operations Research, 52(l):35–53,2004.
4. A.L. Soyster. Convex programming with set-inclusive constraints and applications to inex-
act linear programming. Operations Research, 21(5):1154–1157, 1973.
5. A. Holder (ed.). Mathematical Programming Glossary, INFORMS Computing Society,
http://glossary.computing.society.informs.org, 2006–2007.
6. S.E. Elmaghraby. On the fallacy of averages in project risk management. European Journal
of Operational Research, 165(2):307–313, 2005.
7. H. Markowitz. Portfolio selection. Journal of Finance, 7(1):77–91, 1952.
8. H.M. Markowitz. Portfolio Selection: Eﬃcient Diversiﬁcation of Investments. John
Wiley & Sons, New York, 1959.
9. G.B. Dantzig. Linear programming under uncertainty. Management Science, l(3/4):197–
206, 1955.
10. W.T. Ziemba and J.M. Mulvey (eds.). Worldwide Asset and Liability Modeling. Cambridge
University Press, Cambridge, UK, 2003.
11. A. Charnes, W.W. Cooper, and G.H. Symonds. Cost horizons and certainty equivalents:
An approach to stochastic programming of heating oil. Management Science, 4(3):235–263,
1958.
12. H.J. Greenberg. Mathematical programming models for environmental quality control.
Operations Research, 43(4):578–622, 1995.
13. S.K. Gupta and J. Rosenhead. Robustness in sequential investment decisions. Management
Science, 15(2):B-18–29, 1968.
14. J.M. Mulvey, R.J. Vanderbei, and S.A. Zenios. Robust optimization of large-scale systems.
Operations Research, 43(2):264–281, 1995.
15. P. Kouvelis and G. Yu. Robust Discrete Optimization and Its Applications. Nonconvex-
Optimization and Its Applications. Vol. 14, Kluwer Academic Press, Dordrecht, 1997.
16. G. Cornuejols and R. Tutiincu. Optimization methods in ﬁnance. Course notes, Carnegie-
Mellon University, Pittsburgh, PA, 2005.
17. R.D. Carr, H.J. Greenberg, W.E. Hart, G. Konjevod, E. Lauer, H. Lin, T. Morrison, and
C.A. Phillips. Robust optimization of contaminant sensor placement for community water
systems. Mathematical Programming, Series B, 107(1–2):337–356, 2006.
18. H. Yaman, O.E. Karasan, and M.C. Pinar. The robust minimum spanning tree problem
with interval data. Operations Research Letters, 29(l):31–40, 2001.
19. A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Operations
Research Letters, 25(1):1–13, 1999.
20. X. Chen, M. Sim, and P. Sun. A robust optimization perspective of stochastic programming.
Optimization Online, June 2005.

Robust Optimization
14-33
21. A. Ben-Tal, L.E. Ghaoui, and A. Nemirovski. Forward: Special issue on robust optimization.
Mathematical Programming, Series B, 107(1–2):1–3, 2006.
22. S. Netessine and R. Shumsky. Introduction to the theory and practice of yield management.
INFORMS Transactions on Education, 3(l):34–44, 2002.
23. S.A. Malcolm and S.A. Zenios. Robust optimization for power systems capacity expansion
under uncertainty. Journal of the Operational Research Society, 45(9):1040–1049, 1994.
24. F.M. Murphy, S. Sen, and A.L. Soyster. Electric utility capacity expansion planning with
uncertain load forecasts. IIE Transactions, 1:452–459, 1982.
25. G. Taguchi and S. Konishi. Taguchi Methods—Research and Development. ASI Press,
Dearborn, MI, 1992.
26. T.S. Arthanari. A game theory application in robust design. Quality Engineering, 17:291–
300, 2005.
27. CD. McAllister and T.W. Simpson. Multidisciplinary robust design optimization of an
internal combustion engine. Transactions of the American Society of Mechanical Engineers,
125:124–130, 2003.


Index
A
ABC analysis, 10-4, 10-35
absolute robust decision, 14-10
absolute value, 1-17 to 1-18, 14-16
acceptance sampling, 5-33
active structural acoustic control (ASAC), 13-10
to 13-13
actuator placement, 13-13
acyclic networks, 4-2, 4-5
additivity assumptions, 1-4
admissible move, 13-9
afﬁne functions, 1-14, 1-17 to 1-18
afﬁne scaling, 2-13
age replacement policy, 8-17, 8-40 to 8-42
airline network, 11-4
Al-Khawarizmi, 1-2
Al-Maqala ﬁHisab al-jabr w’almuqabalah
(Al-Khawarizmi), 1-2
all pair shortest path problem, 4-8
ALOHA protocols, 8-3 to 8-4
alternatives
deﬁned, 6-2
multiple criteria decision making
(MCDM), 5-3
AMP, 1-31
analysis of variance (ANOA), 12-15
analytic hierarchy process (AHP) method, 5-11 to
5–13, 5-20, 5-35
animation, in simulation, 12-15, 12-21
annealing, 2-3, 3-16
ant colony optimization, 13-1 to 13-2
anti-ideal solution, 5-4
approximations, 1-5
arborescent distribution network, 10-29
arc
adjacency list, 4-2
deﬁned, 4-2
directed, 4-6
undirected, 4-3, 4-6
Arena, 12-16 to 12-17
Argonne National Laboratory, 1-31
Armijo’s conditions/rule, 2-4, 2-7 to 2-8
Arrival Theorem, 9-31 to 9-32
aspiration criteria, 5-30, 13-9, 13-21, 13-28,
13-30
assembly network, 10-29
assignment problem applications, 4-4
example of, 4-14
Hungarian algorithm, 4-13 to 4-14
linear programming formulation, 4-13
assumptions
blending problem, 1-7
linear blending, 1-6
strict unimodality, 2-4
utility theory, 6-14
attributes
multiple criteria decision making
(MCDM), 5-3
tabu search, 13-9
augmenting path algorithm, 4-9 to 4-13
automobile insurance, stochastics example, 8-2 to
8-3, 8-5, 8-23 to 8-24
AutoMod, 12-17
average path length, 11-6 to 11-7, 11-14, 11-16,
11-20
aviation networks, minimum cost,
multicommodity ﬂow problem, 4-18
axioms, analytic hierarchy process
(AHP), 5-12
B
back ordering, 10-16 to 10-18
backtracking, search guidelines, 2-7 to 2-8
balance equations, 8-34
Barab´asi-Albert complex network model,
see scale-free networks
barrier functions, 2-3 to 2-4, 2-8, 2-18
base-stock inventory, characterized, 10-27
I-1

I-2
Index
basic feasible solution (BFS), 1-27
to 1-28
basic vector, 1-26 to 1-28
basin, 13-31 to 13-32, 13-34
batch means analysis, 12-13 to 12-14
batch-processing strategy, 1-19
Bayes’ Law, 14-9
Bayes’ probabilities, 6-8
Bayes’ theorem, 6-9
beam search, 3-12
best-bound ﬁrst search, integer
programming, 3-9
best compromise solution, 5-18
“best” decision, determination factors, 6-17
best-improving search, 13-5, 13-17
to 13-20
betweenness centrality, 11-9
bi-criteria linear integer programs, 5-33
bi-criteria linear programming, 5-16, 5-25, 5-34
bi-criteria mathematical programming models,
5-34
bi-criteria nonlinear integer
programs, 5-33
bimodal distribution, 9-21
binary ILP (BILP), 3-2
binary integer variables, integer programming,
3-12
binary MILP (BMILP), 3-2
binary pairwise comparison, 5-29
binary variables, integer
programming, 3-6
biological networks, 11-25
bipartite graphs, 4-2, 11-14
bipartite networks, 4-4, 4-13
birth and death process, 8-29 to 8-30, 8-34 to
8-35, 9-10 to 9-11
bisection search, 2-5 to 2-7
blending problems, 1-6 to 1-8
Bolzano search, 2-6
Bonus Males table, 8-2
bottlenecks
closed queueing networks, 9-33 to 9-34
simulations, 12-2, 12-15
boundary conditions, dynamic programming,
7-2, 7-8, 7-10, 7-21
to 7-23
bound-ﬁrst search strategy, integer programming,
3-10
bracketing technique, 2-4 to 2-6
brainstorming, 6-24
branch and bound (B&B) ILP method
bound-ﬁrst B&B algorithm for
minimization/maximization, 3-10
branching, 3-8 to 3-9
components of, 3-7 to 3-8, 3-16
computer lower and upper bounds, 3-9
example of, 3-10 to 3-12
fathoming, 3-8 to 3-12
multiple criteria mathematical programming
(MCM), 5-33
search strategies, 3-9 to 3-10
branch and bound solution, 13-21, 13-27
to 13-28
brand loyalty, 8-15 to 8-16
brand switching, stochastics example, 8-2, 8-5,
8-22 to 8-24
breakeven price, 1-25
Brownian processes, 8-6
Broyden-Fletcher-Goldfarb-Shanno
(BFGS) method
updates, 2-14
C
C++ code, multicommodity ﬂow problems,
4-19
CAD, 12-15
capacities, inventory control, 10-8
capacity constraints, 4-19
capacity expansion
dynamic programming models, 7-3 to 7-7
problem, 14-24 to 14–27
capacity of a cut, 4-10 to 4-11
capital budgeting problems, 3-3 to 3-4, 3-7
cardinal weights, 5-20
cascading failures, 11-21
cash ﬂow management, 4-4
catastrophic effects, 14-1
centering step, 1-30 to 1-31
centrality, 11-23 to 11-24
centralized supply chains
inventory models, 10-7, 10-32 to 10-34
central value solution, 14-11 to 14-13
certainty equivalent models, 14-14 to 14-15
chance-constrained robust optimization model,
14-9 to 14-10
Chapman-Kolmogorov (CK) equations, 8-19 to
8-20, 8-22, 8-28
Choleski decomposition, 2-14
circulation problem, 4-9
class properties, 8-22
clique, 11-25
closed-loop SC, 10-37
closed-queueing networks, 9-30 to 9-32
clustering coefﬁcient, 11-7 to 11-8, 11-15, 11-20,
11-26 to 11-27
coefﬁcient matrices, 1-4
coefﬁcient vector, 1-3

Index
I-3
cold standby reliability system, 8-29
column generation approaches, 4-1, 4-19
community structure, 11-9 to 11-10, 11-24 to
11-25
comparative trade-off method (CTM), 5-32
comparative trade-off ratio, 5-30
competitive analysis, 10-36
complementary slackness optimality solutions,
1-29 to 1-30
complex networks
average path length, 11-6 to 11-7, 11-14, 11-16,
11-20
betweenness centrality, 11-9
characterized, 11-1 to 11-3, 11-26 to 11-27
clustering coefﬁcient, 11-7 to 11-8, 11-15,
11-20, 11-26 to 11-27
community structure, 11-9 to 11-10, 11-24
to 11-25
congestion, 11-26
deﬁned, 11-18
degree distribution, 11-8 to 11-9, 11-14, 11-16,
11-18 to 11-19, 11-27
emergence, 11-17
examples of, 11-4, 11-16 to 11-17
ﬂexibility, 11-20
gas atoms analogy, 11-4 to 11-5
high-sensitivity, 11-18
inﬁnite susceptibility/response, 11-17
models, 11-11 to 11-16
modularity, 11-9 to 11-10
non-uniform random graphs, 11-13
optimization, 11-18 to 11-26
resilience, 11-9 to 11-11, 11-19 to 11-21
responsiveness, 11-20
robustness, 11-19 to 11-20
scale-free networks, 11-8, 11-16 to 17
scale invariance, 11-17
self-organization, 11-17
self-similarity, 11-17
small-world effect, 11-6 to 11-7, 11-15
to 11-16
spreading processes, 11-26
statistical properties, 11-6 to 11-11
compromise programming, 5-9
computer(s), see also software programs
networks, 4-9
viruses, 11-26
concordance index, 5-10
conditional probability, 6-9 to 6-10, 8-15, 8-18
conﬁdence intervals, 11-13, 11-22, 12-8
conﬂicts, multiobjective LP models, 1-32
congestion, 11-26
conjugate gradient, 2-13
conservation of ﬂow, 4-5
conservative decisions, 6-20
consistence index (CI), 5-12 to 5-13
consistency ratio (CR), 5-13
constant-sum constraint, 14-11
constituents, blending problems, 1-6 to 1–7
constrained nonlinear convex optimization
problems, 2-17 to 2-18
constrained optimization
characterized, 2-15
direction ﬁnding methods, 2-15 to 2-17
transformation methods, 2-15, 2-17
to 2-19
constrained shortest path problem, 4-8
constraints
blending problem, 1-6
diet problem, 1-8
dual, 1-29
integer programming, 3-3
linear, 1-4 to 1-5, 1-12
linear inequality, 1-3
material balance, 1-12
mutually contradictory, 1-22
nonnegativity, 1-29 to 1-30
primal, 1-29
product mix example, 1-6
consumption rate, 10-15
container shipping, 1-18 to 1-22
continuous optimization problem, 2-2
continuous review inventory policy, 8-42
to 8-45
continuous-time Markov chain (CTMC)
birth and death processes and applications,
8-29 to 8-30
deﬁned, 8-27 to 8-29
limiting distribution, 8-33 to 8-37
queueing theory, 9-8
statistical inference, 8-37 to 8-38
transient analysis, 8-31 to 8-33
continuous-time stochastic process, 8-5
continuous variables, 1-4, 3-6
contracts, inventory management, 10-36
contribution functions, deﬁned, 7-2
convergence, 4-2
convex functions, variable transformations,
1-13
convex optimization problem, 2-2
convex piecewise linear (PL) function
deﬁned, 1-13
separable, minimizing, 1-14 to 1-16
convex polyhedra, 1-4
correlation coefﬁcient, 11-4
cost(s)
back order, 10-16 to 11-18
coefﬁcients, 1-25

I-4
Index
cost(s) (Contd.)
holding, 10-6, 10-10 to 10-12, 10-15, 10-17 to
10-18, 10-21, 10-23, 10-25, 10-31 to
10-32, 10-35
ordering, 10-5 to 10-6, 10-12, 10-20, 10-23 to
10-24, 10-32, 10-35
overage, 10-27
set-up, 10-5 to 10-6, 10-35
shortage, 10-21, 10-23 to 10-24, 10-35
underage, 10-27
variable, 10-31 to 10-32
cost per unit, 1-5
counting process, 8-39
CPLEX, 1-31, 3-16
criteria normalization, multiple criteria decision
making (MCDM)
characterized, 5-3
linear normalization, 5-5, 5-8
range equalization factor, 5-6
use of 10 raised to an appropriate
power, 5-6
vector normalization, 5-5 to 5-6
criteria weights, computation of
analytic hierarchy process (AHP)
method, 5-11 to 5-13, 5-35
compromise programming, 5-9
ELECTRE method, 5-10 to 5-11, 5-35
PROMETHEE method, 5-13 to 5-15, 5-35
from ranks, 5-6 to 5-7
rating method, 5-7
ratio weighting method, 5-7
TOPSIS method, 5-9 to 5-10
CriteriumDecisionPlus, 5-35
critical fractile, 10-26
CSI Phase I Evolutionary Model (CEM), 13-22 to
13-24
cumulative process, 8-41
curse of dimensionality, 7-24
curve ﬁtting line search, 2-7
cuts, integer programming, see cutting plane ILP
method
cutting plane
dual fractional cutting-plane algorithm, 3-14
example of, 3-14
Gomory cuts, 3-14
ILP method
cuts, deﬁned, 3-12
components of, 3-12, 3-16
dual fractional cut, 3-13 to 3-15
multiple criteria mathematical programming
(MCMP), 5-32
cyclic coordinate method, 2-13
cycling, 13-9 to 13-10, 13-26, 13-31 to 13-32
cylinder, metaheuristic applications, 13-12
D
damper placement problem c
computational results, 13-27 to 13-29
ﬁrst DPP (DPP1), 13-23
optimization problems, 13-22 to 13-24
overview of, 13-21 to 13-22
tabu search, 13-25 to 13-26, 13-29
to 13-35
Dantzig, George B., 1-4, 1-26
Dantzig-Wolfe decomposition, 4-19
DASH/XPRESS, 4-19
Davidson-Fletcher-Powell (DFP) methods energy
system optimization
updates, 2-14
decentralized networks, 11-21
decentralized supply chains
inventory models, 10-7, 10-31 to 10-32
decision analysis
alternatives, 6-2, 6-4 to 6-5, 6-21, 6-23 to 6-25,
6-28
consequences, 6-25 to 6-26, 6-28
deﬁned, 6-1
hospital problem example, 6-21 to 6-29
Hurwicz criterion, 6-19
importance of, 6-1 to 6-2, 6-28 to 6-29
information resources, 6-29
investment example, 6-3 to 6-8, 6-14
linked decisions, 6-27 to 6-29
objectives, 6-22 to 6-23, 6-28
problem deﬁnition, 6-2, 6-21 to 6-22
under risk, 6-2 to 6-17
risk tolerance, 6-17, 6-27, 6-29
software programs, 6-6, 6-17
software survey, 6-29
terminology, 6-2 to 6-3
tradeoffs, 6-25 to 6-26, 6-28
under uncertainty, 6-2 to 6-6, 6-17 to 6-20,
6-26 to 6-28
Decision Analysis (Keeney/Raiffa), 5-35
Decision Analysis Society of INFORMS,
6-29
decision makers attitudes, 6-17 to 6-18
classiﬁcation of, 6-12
deﬁned, 6-2
functions in decision analysis, 6-5
inconsistency of, 5-33
preferences, 6-17 to 6-18
Decision Lab, 5-35 to 5-36
decision space, 2-2, 5-16 to 5-17
decision trees
characterized, 6-7 to 6-8
lotteries, 6-14
stock investment example, 6-16

Index
I-5
decision variables
blending problem, 1-6 to 1-7
characterized, 1-4
continuous, 7-10
deﬁned, 1-2
discrete optimization problems,
13-15, 13-28
dynamic programming, 7-2, 7-20
to 7-22
goal programming, 5-19
integer programming, 3-3, 3-11
intelligent modeling example, 1-20
linear constraints, 1-4
partitioning algorithm, 5-24
product mix example, 1-6
robust optimization, 14-17
decomposition analysis, 13-13
complex networks, 11-25
implications of, 4-1
queueing theory, 9-34
deﬂection techniques, 2-11, 2-13 to 2-14
degree distribution, 11-8 to 11-9, 11-14, 11-16,
11-18 to 11-19, 11-27
degree, of node, 4-2
degree-constrained spanning tree problem,
4-16
degrees of freedom, 8-27 to 8-38
Dell, Inc., 10-34
demand centers/markets, 1-10
demographics, signiﬁcance of, 6-26
dependencies, inventory management, 10-28
depreciation, 10-6
depth-ﬁrst searches, integer programming,
3-9
descent algorithm, 2-3
descent method, steepest, 2-11 to 2-12, 2-14
descent step, 1-31
deterministic demand, 10-8
deterministic dynamic programming models
capacity expansion problem
components of, 7-3 to 7-6
with discounting, 7-6 to 7-7
dynamic inventory planning problem, 7-13
to 7-16
equipment replacement problem, 7-7 to
7-9
reliability system design, 7-16 to 7-18
simple production problem, 7-9 to 7-13
deterministic inventory system characterized,
10-9, 10-36
economic order quantity (EOQ) inventory
model, 10-9 to 10-14, 10-16 to
10-18, 10-31
maximum-minimum system, 10-10
time-varying demand, 10-18 to 10-20
uniform demand with replenishment, 10-14
to 10-16
deterministic techniques, 2-3
deviation variables, 1-33 to 1-34, 5-20
d-heap, 4-6
diet problems, 1-8 to 1-9
differential equations, 4-4
diffusion approximation, queueing theory,
9-32 to 9-34
Dijkstra’s algorithm, 4-5 to 4-8, 11-2
directed cycle, 4-2
directed graphs, 4-2, 11-14
directed network, 4-2 to 4-3
directed path, 4-2
directed walk, 4-2
direction ﬁnding methods, 2-15 to 2-17
direction ﬁnding step, 5-31
discount, inventory control and, 10-16
discounting, dynamic programming models,
7-6 to 7-7
discount rate, 7-6
discrete optimization problems,
metaheuristics
active structural acoustic control (ASAC),
13-10 to 13-13
damper placement in ﬂexible truss
structures, 13-21 to 13-29
implications of, 13-1
multistart local search, 13-5, 13-36
nature reserve site selection, 13-13 to
13-21
network location problems, 13-3 to 13-5
population-based, 13-2
simulated annealing, 13-6 to 13-8, 13-27,
13-36
single solution, 13-2 to 13-3
tabu search
characterized, 13-7, 13-36
damper placement project, 13-22 to
13-23
for MCSP, 13-19 to 13-20
plain vanilla, 13-8 to 13-10, 13-13, 13-36
reactive, 13-29 to 13-35
discrete-event simulation
characterized, 12-1 to 12-23
information resources, 12-30
software programs, 12-16 to 12-30
discrete-time Markov chain (DTMC)
automobile insurance, 8-22 to 8-24
brand switching model, 8-15 to 8-16, 8-19
classiﬁcation of states, 8-20 to 8-22
deﬁned, 8-14
gambler’s ruin problem, 8-24 to 8-25

I-6
Index
discrete-time Markov chain (Contd.)
limiting probabilities, 8-22 to 8-25
random walk model, 8-16 to 8-17
slotted ALOHA model, 8-18
social mobility example, 8-23
statistical inference, 8-25 to 8-27
success runs, 8-17 to 8-20
transient analysis, 8-18 to 8-20
discrete-time statistic, 12-8
discrete-time stochastic process, 8-4 to 8-5
distance labels, shortest path problem, 4-6, 4-8
distributed routing algorithms, 11-22
distribution networks, minimum cost
multicommodity ﬂow problems, 4-18
distribution problems, impact of, 1-11 to
11-12, 3-6
distribution system, 10-29
diversiﬁcation, 14-1
Dmitri Bertsekas Network Optimization
Codes, 4-19
DNA sequencing, 4-4
dominated solution, 5-18
do nothing (DN) alternative, 6-5
downstream, in supply chain, 10-29, 10-31
dual fractional cut for
MILPs, 3-15
pure ILPs, 3-13 to 3-14
dual fractional cutting plane algorithm
for MILPs, 3-15
pure ILPs, 3-14
dual problem, 1-24 to 1-25
dual simplex method, 1-9
dual variables, 1-24 to 1-25
dynamic process, complex networks, 11-27
dynamic programming (DP)
advantages of, 7-3
components of, 7-1 to 7-2
deﬁned, 7-1
deterministic models, 7-3 to 7-18
difﬁculties of, 7-1
formulation stages, 7-3 to 7-8, 7-10, 7-13
to 7-14, 7-17 to 7-18, 7-20, 7-22,
7-24
graphical solutions, 7-15 to 7-16
limitations of, 7-3
numerical solutions, 7-23 to 7-24
optimal solutions, 7-5 to 7-6, 7-18, 7-24
state variables, 7-3, 7-5
stochastic models, 7-19 to 7-24
E
echelon inventory, 10-32 to 10-33
e-commerce, future directions for, 2-21
economic order quantity (EOQ) inventory
model
with back ordering, 10-16 to 10-18
characterized, 10-9, 10-26
example of, 10-9 to 10-10
formulation of, 10-10 to 10-13
graphical presentation of, 10-13
lead time, 10-13
objective function, 10-33
reorder point (ROP), 10-13 to 10-14, 10-16,
10-31, 10-33
sawtooth pattern of, 10-31, 10-33
economic production quantity (EPQ),
inventory model, 10-14
edge-weight distributions, 11-24
efﬁcient frontier, 5-16, 14-6 to 14-17
efﬁcient point, global criterion method, 5-28
efﬁcient set, 5-16
efﬁcient solution
multiple criteria decision making
(MCDM), 5-4
multiple criteria mathematical
programming (MCMP)
deﬁned, 5-16
determination of, 5-17 to 5-18
global criterion, 5-28
theorem, 5-18
eigenvectors, 5-7, 5-12
either-or constraints, 3-5 to 3-6
ELECTRE method, 5-10 to 5-11, 5-35
Electricity, networks, 4-9
electric power system, robust optimization,
14-24 to 14-27
electronic circuits, 11-6
elementary renewal theorem, 8-40
elimination method, 1-3
embedded DTMC, 8-45
emergence, 11-17
EMOO, 5-36
endpoints, of arc, 4-2
enterprise resource planning (ERP), 10-35
enumerative algorithms, 1-3
epidemic spreading, 11-26
equality-constraint methods, 3-2
equipment replacement
decisions, 4-4 to 4-5
dynamic programming models, 7-7
to 7-9, 19–22
problem, 4-4 to 4-5
Erd¨os-R´enyi random graphs, 11-11 to 11-13
Erlang, A.K., 8-29 to 8-30
Erlang A formula, 8-30, 8-36 to 8-37
Erlang C formula, 8-29 to 8-30, 8-35 to 8-36
Erlang distribution, 8-8

Index
I-7
Erlang formula, 9-3
Erlang loss system, 9-12
Euclidean norm, 14-27
Eulerian circuit, 11-1
EURO Working Group, 5-36
evaluation criteria, multiple criteria decision
making (MCDM), 5-3
evolutionary algorithms, 13-1 to 13-2
evolutionary optimization techniques, 2-19
evolutionary programs, 13-1 to 13-2
EXCEL, 1-31
expected opportunity lost (EOL), 6-6 to 6-7
expected payoff, decision analysis, 6-17
expected proﬁt, 10-26
expected regret, 6-6
expected returns, 14-5, 14-7, 14-10
expected utilities, decision analysis,
6-14 to 6-17
expected value
decision analysis
decision trees, 6-10 to 6-12
implications of, 6-4 to 6-5
of perfect information, see expected value of
perfect information (EVPI)
under uncertainty, 6-5 to 6–6
of perfect information (EVPI)
characterized, 6-6 to 6-7
expected opportunity lost (EOL), 6-7
expected value under uncertainty, 6-7
ExpertChoices, 5-35
exponential random graphs models,
(ERGMs), 11-14
exponential random variable, 8-28
Extend, 12-17
F
facilities, location problem, 14-27 to 14-28
fallacy of averages, 14-2
fathoming, 3-8 to 3-12
feasibility
problem, 2-2
signiﬁcance of, 14-2, 14-7, 14-12, 14-15 to
14-16
feasible set, 2-2
feasible solutions, 2-2 to 2-3
fertilizer product mix problem, 1-5 to 1-6,
1-17 to 1-18, 1-22, 1-24, 1-27 to
1-28, 1-34
Fibonacci heap, 4-6
Fibonacci search, 2-4 to 2-6,
ﬁll-ratio equalization policy, 1-20 to 1-21
ﬁnancial analysts, roles in decision analysis,
6-9 to 6-10, 6-12
ﬁnished goods, inventory control, 10-9
ﬁnite population queues, 9-13
ﬁrst come ﬁrst served (FCFS), 9-3 to 9-4,
9-10, 9-17, 9-22, 9-24 to 9-25, 9-27,
9-34 to 9-35, 9-38
ﬁrst-improving search, 13-5, 13-17 to 13-20
ﬁrst-order approximations, 2-16 to 2-17
ﬁxed-charge problem, 3-4 to 3-5
ﬂexibility, 14-1, 14-10
Flexsim, 12-17 to 12-18
ﬂight simulators, 12-1
ﬂowcharts, partitioning algorithm, 5-22
ﬂow node, shortest path problem, 4-5
force-feedback control law, 13-23
Fourier-Motzkin elimination method, 1-3
framing, 6-17
Frank-Wolfe method, 5-31
frequency
implications of, 13-21
frequency-based diversiﬁcation, 13-26
fuzzy goal programming, 5-26 to 5-27
G
gambler’s ruin problem, stochastics
example, 8-17, 8-24 to 8-25
game theory, 10-36
gamma distribution, 8-8
GAMS, 1-31
gas atoms, complex networks analogy, 11-4
to 11-5
gas networks, 4-9
Gauss-Jordan
elimination method, 1-3
pivot step, 1-27
Gaussian distribution, 5-14
GDF procedure, 5-31 to 5-33
general knapsack problem, 3-4
generalized reduced gradient (GRG),
algorithm, 5-24, 5-31
general-purpose language, 12-16
genetic algorithm, 3-16, 13-1 to 13-2
G/G/1 queue, 9-23 to 9-24
G/M/1 queue, 9-17
G/M/s queue, 8-47
Gibbs sampling, 11-14
GI/G/m queue, 9-36
GI/G/s queue, 9-5 to 9-8, 9-35
goal constraints, 5-19, 9-21 to 9-23
goal programming (GP)
model, multiobjective LP models,
1-32 to 1-34
multiple criteria decision making
applications, 5-34

I-8
Index
goal programming (Contd.)
formulation, 5-19 to 5-20
partitioning algorithm, 5-21 to 5-25
preemptive, 5-20 to 5-21
signiﬁcance of, 5-19
software, 5-35
STEM (GPSTEM) method, 5-31
goals, multiple criteria decision making
(MCDM), 5-3
GOBLIN-C++ Object, 4-19
Goldberg, Andrew, codes for shortest path
and minimum cost ﬂow algorithms,
4-19
Gomory cuts, 3-14
goodness-of-ﬁt test, 8-14
GPSS/H, 12-7, 12-18, 12-20
Gram-Schmidt orthogonalization
procedure, 2-10
graphical user interface, 12-15
graphic-aided interactive approach, 5-32
graph partitioning problem (GPP), 11-10, 13-6 to
13-8
graph theory, 11-1 to 11-3
greedy heuristics, 13-25 to 13-26, 13-28
greedy local improvement schemes, 13-25
greedy randomized adaptive search procedures
(GRASP), 13-1 to 13-2
GRG2, 5-32
group theoretic algorithms, 3-16
H
head node, 4-2
hedge, robust optimization, 14-10 to 14-11
Henriksen’s Algorithm, 12-7
Hessian matrix, 2-12 to 12-4
heterogeneity, complex networks, 11-17 to
11-18, 11-23 to 11-24
heuristic optimization strategies, 2-3, 2-19
heuristic search algorithms, 3-16
Hewlett-Packard (HP)
inventory management, 10-34, 10-37
high-degree search, 11-23
highest degree, 11-24
highest LBC, 11-24
high-sensitivity networks, 11-18
holding costs, 7-13, 10-2, 10-6, 10-10 to
10-12, 10-15, 10-17 to 10-18, 10-21,
10-23, 10-25, 10-31 to 10-32, 10-35
HOM software, 9-24
homogeneous mixing hypothesis, 11-26
Hotelling, 1-4
hub avoidance paths, 11-26
Hungarian algorithm, 4-13 to 4-14
I
IBM, 10-34
ideal point, global criterion method, 5-27
ideal solution, 5-4, 5-16
if-then constraints, 3-6 to 3-7
if-then statements, tabu search, 13-10
ILOG/CPLEX, 4-19
incumbent integer solution, 3-9, 3-11
in-degree
deﬁned, 4-2
distribution, 11-8
independent increments, 8-5, 8–9
index speciﬁcation and value trade-off, 5-30
inequality constraint function, 2-2
infeasibility analysis, 1-22 to 1-23
inﬁnite horizon case, 10-27
inﬁnitesimal generator, 8-29,
8-31, 8-34
inﬂow, minimum cost ﬂow problem, 4-3
information networks, 4-9
information resources
conferences, 12-3, 12-22
databases, 11-4
literature review, 4-19, 5-35 to 5-36
professional journals, 5-36, 6-29
web sites, 1-31, 2-20, 4-19, 5-36, 6-29,
12-3, 12-22, 12-30
information technology, 1-22
initial distribution, 8-15
input-output analysis, 1-5
input-output (I/O) coefﬁcients, 1-5
inspection interval, 2-4
installation stock, 10-31 to 10-32
Institute of Management Science (TIMS),
xix
Institute of Operations Research and
Management Science (INFORMS)
development of, xx
publications, 12-23
integer goal program, 5-24
integer linear programming (ILP)
alternative solutions, 3-15 to 3-16
branch and bound (B&B) method, 3-7 to
3-10, 3-16
characterized, 3-1 to 3-3
computer solutions, 3-15 to 3-16
cutting plane method, 3-12 to 3-16
functions with N possible values, 3-7
model formulation, 3-3 to 3-7
NP-hardness, 3-15
pure, 3-1, 3-13 to 3-14
software systems, 3-16
integer optimization problem, 2-2

Index
I-9
integrated SC models, 10-37
intelligent modeling, container shipping
example, 1-18 to 1-22
interactive methods, multiple criteria
mathematical programming
(MCMP)
branch-and-bound method, 5-33
criterion weight space methods, 5-31 to
5-32
feasible direction methods, 5-31
feasible region reduction methods, 5-30 to
5-31
Lagrange multiplier methods, 5-32
trade-off cutting plane methods, 5-32
visual, 5-32 to 5-33
interactive surrogate worth trade-off method
(ISWTM), 5-32
interarrival time
queueing networks, 9-36
simulations, 12-22
interchange heuristics, 13-16 to 13-19
interior feasible solution, 1-29 to 1-31
interior point methods, linear programming
central path, 1-29
general step, 1-29 to 1-30
gravitational, 1-30 to 1-31
optimality conditions, 1-29
overview, 1-28 to 1-29
International Society on Multiple Criteria
Decision Making, 5-35 to 5-36
internet
characterized, 4-9, 4-15
as complex network, 11-3, 11-5 to 11-6,
11-8, 11-17
interval trade-off ratio, 5-30
intranets, 4-9
intuition, 6-21
inventory control
ABC classiﬁcation, 10-4, 10-35
demand process, 10-8
deterministic inventory systems, 10-9 to 10-20,
10-31, 10-35
inventory management, in practice, 10-34
to 10-36
inventory system design
graphic representations, 10-10, 10-13,
10-19
higher inventory levels, 10-5 to 10-6
implications of, 10-4 to 10-5
inventory management models, 10-7 to
10-9
lower inventory levels, 10-6
lead time, 10-7 to 10-8, 10-13 to 10-14,
10-24
logic of, 10-2 to 10-3
multiple locations, 10-27 to 10-34
optimization tools, 10-35
order quantity, 10-7
policies
base stock, 10-27
coordination, 10-30
research directions, 10-36 to 10-37
pure inventory management, 10-36
reorder point (ROP), 10-13 to 10-14, 10-22,
10-24 to 10-25, 10-31, 10-33
signiﬁcance of, 10-1 to 10-2
software programs, 10-35
stochastic inventory systems, 10-20 to
10-27, 10-36
types of inventory, 10-3 and 10-4
inventory cycle, 10-17
inventory equations, 1-12
inventory planning, 4-4, 7-13 to 7-16
investment planning, 7-22 to 7-24
investment problems, integer programming,
3-6
irreducible Markov chain, 8-21, 8-25
items, product mix example, 1-6
iterated local search, 13-1 to 13-2
J
J E Beasley OR library, data sets for OR
problems, 4-19
job sequencing, 3-5, 14-16 to 14-17
joint constraints, 14-7
Jordan, Wilhelm, 1-3
judgment values, 5-12
K
k, 4-16, 5-22
Kanban, 9-30, 22-8
Kantorovitch, L. V., 1-9 to 1-10
Karush-Kuhn-Tucker (KKT) conditions,
14-5
Kendall notation, 9-3 to 9-4
kernel, multiple criteria decision making,
5-10 to 5-11
knapsack problem, 3-4
Kolmogorov equations
backward equations, 8-33
differential equations, 8-31 to 8-32
forward equations, 8-33
K¨onigsberg bridge problem, 11-1 to 11-2
Koopmans, T. C., 1-10
k-out-of-n system, 8-3
Kruskal’s algorithm, 4-16 to 4-18
k year planning horizon, 4-5

I-10
Index
L
label-correcting algorithms, 4-6, 4-8
Lagrange multipliers, 2-19, 5-32, 14-5
Lagrangian relaxation, 4-1, 4-19
LAMSADE, 5-35
Laplace criterion, in decision analysis, 6-20
large-scale networks, see complex networks
large-scale problems, network
optimization, 4-19
last come ﬁrst served (LCFS), 9-3, 9-10, 9-22
lateral shipments, 10-8
lateral supply, 10-29
lattices, complex networks, 11-15, 11-23
lead time, inventory management, 10-7 to
10-8, 10-13
Leibniz’s rule, 10-26
Levenberg-Marquardt method, 2-14 to 2-15
likelihood ratio test, 8-27
limiting distributions, 8-21, 8-24, 8-33 to
8-37, 8-45 to 8-46
LINDO LP software, 1-31
LindoSystems, 5-35
linear algebra, 1-3 to 1-4
linear constraints, dynamic programming,
7-9
linear goal program, 5-21 to 5-24
linear inequality
constraints, 1-3
solving systems, 1-4
linearity assumptions, 1-4
linearized maximal expected covering
problem (LEMCP)
deﬁned, 13-15
development of, 13-16
heuristics for, 13-20 to 13-21
linear normalization, 5-5
linear penalty function, 1-32
linear programming (LP)
algorithms
applications, 1-1 to 1-4
solving LP models, 1-26 to 1-31
classical examples of direct applications
blending problems, 1-6 to 1-8
diet problem, 1-8 to 1-9
multiperiod production planning, storage,
distribution problems, 1-11
to 1-12
overview, 1-4 to 1-5
product mix problem, 1-5 to 1-6
transportation problem, 1-9 to 1-11
development of, 1-4
duality theorem, 14-11
intelligent modeling, container shipping
example, 1-18 to 1-22
multiobjective models, 1-32 to 1-33
Phase I problems, 1-4, 1-22 to 1-23
planning uses, 1-22 to 1-26
relaxation, 3-2, 3-4, 3-7 to 3-15, 13-25,
13-27 to 13-28
simplex method, 1-4
solving LP models
algorithm applications, 1-26 to 1-31
software systems available for, 1-9, 1-25
to 1-26, 1-31
variable transformations
absolute values of afﬁne functions, 1-17
to 1-18
min-max, max-min problems, 1-16 to
1-17
types of, 1-12 to 1-16
line searches
backtracking search, 2-4, 2-7 to 2-8
bisection search, 2-4 to 2-7
bracketing the minimum, 2-4 to 2-6
characterized, 2-3 to 2-4
Fibonacci search, 2-4 to 2-6
quadratic ﬁt search, 2-4, 2-7
types of, 2-4
linked decisions, 6-27 to 6-28
LIST data, 4-5 to 4-6, 4-16
Little’s law, 9-6 to 9-7, 9-9, 9-23, 9-30
local betweenness centrality (LBC), 11-23 to
11-24
local search
characterized, 2-3, 11-21 to 11-24
multistart, 13-5
local trade-off ratios, 5-29, 5-31
logic ﬂow diagram, 12-21
Logical Decisions, 5-35
LOQO, 1-31
lotteries, 6-14
lower bounds
dynamic programming, 7-4, 7-18
goal programming, 5-26
implications of, 3-7, 3-9, 4-4
multiple criteria decision making,
(MCDM), 5-4
robust optimization, 14-11, 14-19
Lucent Technologies, 10-34
Ludus Algebrae, 1-2
lushlawn, 1-25 to 1-26
M
management science (MS), xix
Maple, 8-47

Index
I-11
marginal analysis, 1-25, 10-27
marginal values, 1-24 to 1-25
market analysis, 1-17
market graph, as complex
network, 11-4
marketing campaigns, 11-26
market risk, 5-34
market share, 1-34
Markov, A. A., 8-15
Markov Analysis, 8-47
MARkov Chain Analyzer (MARCA),
8-47
Markov chain theory, 13-6
Markov decision processes (MDPs), 8-47
Markov property, 7-3, 8-5, 8-15
mass balance constraints, 4-4 to 4-5
material balance
equations, 1-12
inequality, 1-6
Mathematica, 8-47
Mathematical Methods in the Organization
and Planning of Production
(Kantorovich), 1-9 to 1-10
mathematical modeling deﬁned, 1-2
large-scale, 4-1
MATLAB, 1-31, 8-47
maximal covering species problem (MCSP)
characterized, 13-13 to 13-14
data details, 13-15 to 13-16
interchange heuristics, 13-16 to 13-19
problem formulation, 13-15 to 13-16
tabu search for, 13-19 to 13-20
maximal expected coverage problem (MECP),
13-20 to 13-21
maximum ﬂow problem
augmenting path algorithm, 4-9 to 4-13
characterized, 4-8 to 4-9
linear programming formulation, 4-9
maximum inventory, 10-17
maximum leap spanning tree problem, 4-16,
4-18
maximum likelihood
characterized, 6-5, 13-13 to 13-15
estimate, 8-13 to 8-14
function, 8-25 to 8-26, 8-38
heuristics for, 13-20 to 13-21
max-min
method, 5-8
problem, 1-16 to 1-17
maximum regret, 14-14
maximum reliability path problem,
4-4
maxmin (minimax) criterion, 6-18
maxmin (minimin) criterion, 6-19
MCQueue, 8-47
mean
inventory control, 10-8
simulations, 12-13, 12-22
mean arrival rate, queueing networks, 9-35
mean-risk constrained robust optimization
model, 14-3 to 14-7
mean-variance model, 5-34, 14-18
memetic algorithms, 13-1 to 13-2
memoryless properties, 8-5, 8-8 to 8-10,
8-41, 9-3
memsize, 13-31, 13-35
merit successive quadratic programming method,
2-17
metaheuristics applications, 13-2
deﬁned, 13-1 to 13-2
single solution, 13-2 to 13-3
method of centers, 5-32
method of multipliers (MOM), 2-19 to 2-20
Metropolis-Hastings sampling method, 11-14
M/G/1/1 queue, 8-41, 8-47
M/G/1 queue, 9-7, 9-14 to 9-17, 9-22, 9-24
to 9-26
M/G/s queue, 9-7
M/G/s/s queue, 9-12, 9-16 to 9-17
Micro Saint Sharp, 12-18
Microsoft Excel
CallCenterDSS.xla, 8-36
Solver, 3-16
Stochastics programs, 8-47
Min-Max method, 5-8 to 5-9, 5-26
min-max problem, 1-16 to 1-17
minimax regret, 6-20
minimax regret, robust optimization model,
14-14 to 14-16, 14-18 to 14-22,
14-26
minimization of concave function, 10-19
minimum average node weight, 11-24
minimum cost ﬂow problem, 4-3 to 4-4,
4-9, 4-13
minimum cost multicommodity network ﬂow
(MCMNF) problem, 4-18 to 4-19
minimum edge weight, 11-24
minimum spanning tree problem
applications, 4-15
characterized, 4-14 to 14-15
Kruskal’s algorithm, 4-16 to 4-18
linear programming formulation, 4-16
Minitab, 8-47
mixed integer linear programs (MILPs), 3-1
to 3-2, 3-15 to 3-16
mixed-integer optimization problem, 2-2
mixed penalty-barrier method, 2-19
mixed-integer problem (MIP), 3-1

I-12
Index
M/M/1/K queue, 9-11 to 9-12
M/M/1 queue, 8-38, 9-4, 9-7 to 9-10
M/M/s/k + M queue, 8-30, 8-36
M/M/s queue, 8-30, 8-35, 8-47, 9-7, 9-10 to
9-12, 9-29
M/M/s/s + M queue, 8-47
M/M/∞, 9-12 to 9-13, 9-16
mode
beta distribution, 15-21
implications of, 13-21
modeling, see speciﬁc types of models
deﬁned, 1-2
software, 1-31
modern portfolio theory, 5-34
modularity, complex networks, 11-9 to 11-10
mold solidiﬁcation simulation software, 12-1
monotonic slope, 1-14
Monte Carlo simulation, 12-1
move set, 13-5
movie actor collaboration network,
11-4, 11-6
MPL, 1-31
m x n, 1-4, 1-22, 1-26, 1-28
Multi Attribute Decision Making
(Keeney/Raiffa), 5-35
multiclass queues, 9-21 to 9-27, 9-34 to 9-37
multicommodity ﬂow problems, 1-10, 4-4
multicommodity network airline, ﬂow
problems, 4-18
multicriteria shortest path problem, 4-8
multidimensional optimization
characterized, 2-8
Levenberg-Marquardt method, 2-14 to
2-15
method of
Fletcher and Reeves, 2-12 to 2-13
Hooke and Jeeves, 2-10 to 2-11
Rosenbrock, 2-10 to 2-11
Zangwill, 2-10 to 2-12
quasi-Newton method, 2-13 to 2-14, 2-17
simplex method (S2), 2-9 to 2-10
steepest descent method, 2-11 to 2-12,
2-14
multi-echelon inventory
controls, 10-7, 10-31
theory, 10-28
multi-echelon supply chain network, 10-28
multiobjective LP models, 1-32 to 1-34
multiobjective optimization problem, 2-2
multiperiod production planning problems,
1-11 to 1-12
multiple attribute decision making (MADM)
problem, 5-2
multiple criteria decision making (MCDM)
applications, 5-34
“best solutions,” 5-4 to 5-5
characterized, 5-1 to 5-2
classiﬁcation scheme, 5-30
compromise programming, 5-29
criteria normalization, 5-5 to 5-6
criteria weights, computation methods,
5-6 to 5-7
deﬁnitions, 5-3
goal programming, 5-19 to 5-26, 5-34 to
5-35
information resources, literature review,
5-35 to 5-37
interactive methods, 5-29 to 5-34
method of global criterion, 5-27 to 5-28
military applications, 25-5 to 25-6
multiple attribute decision making
(MADM) problem, 5-2
multiple criteria mathematical
programming (MCMPM) methods,
5-2
multiple criteria mathematical
programming (MCMP) problems,
5-2, 5-15 to 5-19
multiple criteria methods for ﬁnite
alternatives (MCMFA), 5-2, 5-8 to
5-15
multiple criteria selection problem
(MCSP), 5-2 to 5-3, 5-11
multiple objective decision making
(MODM), 5-2
software systems, 5-35
multiple criteria mathematical programming
methods (MCMPM), 5-2
multiple criteria mathematical programming
(MCMP) problems
characterized, 5-2, 5-15
classiﬁcation of, 5-18 to 5-19
deﬁnitions, 5-16
efﬁcient solution, 5-16 to 5-18
test for efﬁciency, 5-18
multiple criteria methods for ﬁnite
alternatives (MCMFA), 5-2, 5-8 to
5-15
multiple criteria selection problem (MCSP),
5-2 to 5-3, 5-11, 5-15
multiple objective decision making
(MODM), 5-2
multiple objective linear programming
(MOLP) problems, 5-31
multiple sourcing, 10-8
multiprogrammed computer system, 9-30
multi-start local search, 13-5, 13-36

Index
I-13
multistation queues multiclass queues, 9-34
to 9-37
single-class queues, 9-28 to 9-34
mutually exclusive subproblems, 3-8
N
nadir solution, 5-4
natural decay, 13-21 to 13-22
natural frequency, 13-21
natural motions, 13-21
nature reserve site selection
MCSP
interchange heuristics, 13-16 to 13-19
tabu search for, 13-19 to 13-20
LMECP heuristics, 13-20 to 13-21
MECP heuristics, 13-20 to 13-21
overview of, 13-13 to 13-15
problem formulation and data details,
13-15 to 13-16
negative cycle, shortest path problem, 4-5
negative-ideal solution, 5-4
negative inventory, 10-16
negative-outranking ﬂow, 5-14 to 5-15
negative part, 1-18
neighborhood structure, 13-25, 13-30 to
13-32, 13-36
NEOS
Guide, web site, 2-20, 4-19
server, 1-31
nested coordination policy, 10-30
net proﬁt, 1-15 to 1-16, 1-32
network design problems, 4-4
network ﬂow models, 4-2
network location theory, 13-3
network optimization
applications, generally, 4-1 to 4-2
assignment problem, 4-4, 4-13 to 4-14
characterized, 4-1 to 4-2, 4-19
information resources, 4-19
maximum ﬂow problem, 4-8 to 4-13
minimum cost ﬂow problem, 4-3 to 4-4
minimum cost multicommodity network
ﬂow (MCMNF) problem, 4-18 to
4-19
minimum spanning tree problem, 4-14 to
4-18
notation, 4-2 to 4-3
shortest path problem, 4-4 to 4-8, 4-18
software systems, 4-19
network simplex algorithm, 4-4
Network Science, 11-2 to 11-3, 11-5
neutral branch, decision tree, 6-10
new products, proﬁtability evaluation, 1-25
to 1-26
newsboy problem, robust optimization, 14-17
to 14-19
newsvendor model, inventory management,
10-26 to 10-27
Newton direction, 1-30
Newton method, 2-3
node balance, 4-5, 4-19
node set, deﬁned, 4-2
nondominated paths, 4-8
non-dominated solution, 5-4, 5-16
nonhomogeneous Poisson processes, 8-12 to
8-14
nonlinear goal program, 5-24 to 5-25
non-linear optimization, problem, 2-3
non-preemptive priority, 9-24 to 9-26, 9-38
nonlinear programming
applications, 1-30
constrained optimization, 2-15 to 2-19
deﬁned, 2-1
dynamic programming, 7-9
problem statement, 2-1 to 2-3
techniques, 2-19
unconstrained optimization
line searches, 2-3 to 2-8
multidimensional optimization,
2-8 to 2-15
nonnegativity restriction, 1-25 to 1-27
nonproduct form-networks, diffusion
approximation
closed queueing networks, 9-33 to 9-34
open queueing networks, 9-32 to 9-33
nonquadratic functions, 2-14
nonspatial networks, search in, 11-23 to
11-24
nonstationary demands, 10-8
normal distribution, 5-14
notation, multistation queues, 9-34 to 9-35
n-out-of-n system, 8-3
n-step transition probabilities, 8-18
N × N identity matrix, 9-28, 9-32 to 9-33
null hypothesis, 8-14, 8-38
null recurrent state, 8-21 to 8-22, 8-25
Numerical Recipes, 13-6
O
objective functions
damper placement, 13-25
integer programming, 3-3, 3-7
nonlinear programming, 2-2
quadratic, 7-9
shortest path problem, 4-5

I-14
Index
objective problem, integer programming, 3-5
objective space, multiple criteria
mathematical programming
(MCMP), 5-16 to 5-17
objectives
multiple, 6-28 to 6-29
multiple criteria decision making
(MCDM), 5-3
obsolescence, 10-6
off the shelf branch and bound code, 13-21
one-step transition, 8-15
one-to-one functional mapping, 5-32
open-queueing networks, 9-28 to 9-30
operational research teams, deﬁned, xx
operations research, generally
historical perspective, xix to xxi
meaning of, xx to xxi
Operations Research, xix
operations researchers, functions of, xxi
Operations Research Society of America
(ORSA), xix to xx
optimality, multiobjective LP models, 1-32
optimal policy, dynamic programming, 7-2,
7-8, 7-10 to 7-12, 7-14, 7-20, 7-22
optimal solution, 2-2
optimal value, 2-2, 7-2, 7-8, 10-1, 10-14,
10-21 to 10-22
optimization models, 1-5, 11-18 to 11-26
optimization problem, branch and bound
method, 3-9
Optimization Software Guide Web site, 1-31
optimization techniques, 1-22
optimization vector, 2-2
optimum solutions, 1-22 to 1-24,
1-33 to 1-34
ordering costs, 10-5 to 10-6, 10-12, 10-20,
10-23 to 10-24, 10-32, 10-35
order of magnitude, 5-6, 13-36
order quantity, 10-7
ordinal weights, 5-20
Oregon Terrestrial Vertebrate dataset, 13-14
origin-destination pair, network
optimization, 4-10, 4-18
OR/MS Today, 12-23
OSL, 1-31
outcomes, decision analysis, 6-3, 6-8 to 6-9, 6-14
outdegree deﬁned, 4-2
distribution, 11-8
outﬂow, minimum cost ﬂow problem, 4-3
outliers, goal programming, 5-26
outranking relationship, 5-10 to 5-11
outsourcing, 10-8
overage cost, 10-27
overload failures, 11-21
P
paired comparison method (PCM), 5-32
paired-t test, 12-8, 12-22
pairwise comparison, 5-7, 5-10 to 5-11,
5-15, 5-29
parameter-free SUMT, 2-18
parametrized SUMT, 2-18
Pareto optimal solution
deﬁned, 5-16
multiple criteria decision making
(MCDM), 5-4
Pareto optimality, 2-2
Pare to Race, 5-32 to 5-33
Part-Period Balancing method, 10-20
partitioning
algorithm, multiple criteria decision
making, 5-21 to 5-25
branch and bound method, 3-7 to 3-8
gradient-based (PGB) algorithm, 5-24
implications of, 13-25
integer programming and, 3-16
network optimization, 4-11
passive dampers, 13-23 to 13-24
PASTA (Poisson Arrives See Time
Averages), 9-7 to 9-8
path, deﬁned, 4-2
payoff matrix, 5-9 to 5-10, 6-3 to 6-5, 6-14,
6-18, 6-26
pay-off table, 5-2, 5-4 to 5-5
p-center, 13-4
p-defense-sum, 13-4
p-dispersion, 13-4
p-dispersion-sum, 13-4
peer-to-peer network, 11-6
penalty coefﬁcients, multiobjective LP
models, 1-33 to 1-34
penalty method, 2-3
penalty successive linear programming
(PSLP) method, 2-16 to 2-17
penalty vectors, 1-33
percolation transition, 11-11 to 11-12
performance measurement, 12-14 to 12-15,
13-3 to 13-4
periodic review
inventory policy, 22-12, 22-15, 22-17
models, 7-13
permanent solution, 14-14
pessimism, 6-19
petroleum industry, blending models,
1-7 to 1-8
petroleum networks, 4-9
Phase I linear programming problems, 1-4,
1-22 to 1-23

Index
I-15
phone call network, 11-3
physical annealing, 13-6
piecewise linear (PL) function
afﬁne functions, 1-14
characterized, 1-12 to 1-13
variables and, 1-13 to 1-14
pipelines, network optimization, 4-8 to 4-9,
4-15
pivot step, 1-26 to 1-28
plain vanilla tabu search (PVTS), 13-8 to
13-10, 13-13, 13-36
planning horizon
dynamic programming, 7-4, 7-9
signiﬁcance of, 3-4, 4-5
p-median, 13-4
Poisson, Simeon-Denis, 8-6
Poisson arrival, 9-28 to 9-29
Poisson distribution, 11-13
Poisson processes compound, 8-12 to 8-14
deﬁned, 8-5, 8-8 to 8-10
exponential distribution and properties,
8-7 to 8-8
mark, deﬁned, 8-13
nonhomogenous, 8-12 to 8-14
properties from, 8-10 to 8-12
testing for, 8-14
polyhedra, mathematical theory of, 1-4, 2-9
population dynamics, 13-16
population studies, 9-30
portfolio
diversiﬁcation, 11-25
selection, 5-34
positive-outranking ﬂow, 5-14 to 5-15
positive part, 1-18
positive recurrent state, 8-21 to 8-23, 8-44
posterior probabilities, 6-8 to 6-12
potential energy, 19-2
Powell-Symmetic-Broyden (PSB) method, energy
system optimization, 19-15
power grid network, 11-3
power-law networks, 11-8, 11-13, 11-23 to
11-24
precedence diagramming method (PDM)
precise local trade-off ratio, 5-29
preemptive goal programs, linear, 5-21 to
5-24
preemptive priorities/goal programming,
5-20 to 5-21
preemptive resume priority, 9-26 to 9-27,
9-38
preference function, 5-18, 5-33
preference index, 5-14
preference judgment, 5-11
preferential attachment, 11-16
preferred solution, global criterion
method, 5-28
present value, 7-6 to 7-7
prespeciﬁed weights, 5-20
pricing strategies, 10-36
primal-dual path following interior point
method, 1-28
primal variables, 1-25 to 1-26
Principle of Optimality, 7-3
probabilistic symbolic model checker
(PRISM), 8-46 to 8-47
probability
applications, 4-4
distribution, 6-17, 8-3, 10-21, 10-28, 14-30
model, decision analysis, 6-2, 6-4
posterior, 6-8 to 6-12
probability density function (PDF), 8-7
processor sharing, 9-10, 9-16
production allocation decisions, 1-12
production planning, 4-4
production problems
dynamic programming models, 7-9 to 7-13
integer programming, 3-6
product mix, 1-5 to 1-6, 1-15
proﬁtability, 1-25 to 1-26, 10-37
proﬁt maximization, 10-26
proﬁt per unit, 1-5
project management, overview, 4
project scheduling, 4-4
PROMETHEE method, 5-13 to 5-15, 5-35
ProModel, 12-18 to 12-19
proportionality assumptions, 1-4, 1-14
prospect theory, 6-17
pseudo convex, 2-6
p × n, 1-4
Q
QNA, 9-36
quadratically constrained quadratic
optimization problem, 2-2
quadratic ﬁt search, 2-7
quadratic optimization problem, 2-2
quadratic programming problem, 2-17
quantity coordination, 10-30
quantity discounts, 10-6
quasi-Newton method, 2-13 to 2-14, 2-17
QUEST, 12-19
queueing theory
applications, 9-1 to 9-2, 9-28, 9-37 to 9-38
balking, 9-9
basics of, 8-47, 9-2 to 9-8
bulk arrivals and service, 9-13 to 9-14
closed queueing network, 9-30 to 9-34

I-16
Index
queueing theory (Contd.)
ﬁnite capacity, 9-5, 9-11
ﬁnite population, 9-13
inﬁnite capacity, 9-5
information resources, 9-2
Little’s law, 9-6 to 9-7, 9-9, 9-30
multiserver systems, 9-10 to 9-11
multistation
multiclass queues, 9-34 to 9-37
single-class queues, 9-28 to 9-34, 9-37
non-preemptive priority, 9-25 to 9-26
open-queueing networks, 9-28 to 9-30,
9-32 to 9-33
physical vs. psychology tradeoff, 9-37
preemptive resume priority, 9-26 to 9-27
queueing relations, 9-4 to 9-5
reneging, 9-9
in simulations, 12-4 to 12-5, 12-8
to 12-10
single-station queues
closed-queueing networks, 9-31 to 9-32
and multiclass queues, 9-21 to 9-27
and single-class queues, 9-8 to 9-21, 9-37
state-dependent service, 9-10, 9-29
to 9-30
queuing process, 8-11
R
R, 2-1, 2-4, 12-2
radix heap, 4-6
random graphs, 11-11 to 11-14, 11-16, 11-26
random index (RI), 5-13
random order of service (ROS), 9-3, 9-10
random search, 11-23
random walk, 8-16 to 8-17, 8-29, 11-22,
11-24
range equalization factor, 5-6
rate of return, 6-5
raw material, inventory control, 10-9
reactive tabu search (RTS)
characterized, 13-29 to 13-32
computational experiments, 13-32 to
13-35
real constraints, 5-19
recency-based diversiﬁcation, 13-3, 13-26,
13-28
recourse robust optimization model, 14-7 to 14-8,
14-21, 14-31
recurrence relation, 7-2, 7-7 to 7-8, 7-10 to
7-11, 7-14, 7-17, 7-20 to 7-22
recurrent state, 8-21 to 8-23
reduced cost matrix, 4-13 to 4-14
redundancy, 3-5
reﬁning process, blending problem
example, 1-7
regenerative cycles, 8-44
regret matrix, 6-20
relationships, in linked decisions, 6-28
relative cost
coefﬁcients, 1-25, 1-27
vector, 1-28
relative value, 5-12 to 5-13
Relex Markov, 8-47
reliability
repairable system, 8-32 to 8-33
signiﬁcance of, 4-4
stochastic processes, 8-29
stochastics example, 8-3
system
machine maintenance example,
8-45 to 8-46
renewal cycles, 8-42
renewal function, 8-39
renewal processes, statistical inference, 8-46
renewal rate, 8-40, 8-42
renewal reward process, 8-43
renewal theory
regenerative processes, 8-39, 8-44 to 8-45
renewal processes, 8-39 to 8-41
renewal reward processes, 8-39, 8-41 to
8-44
semi-Markov processes, 8-39, 8-45 to 8-46
statistical inference, 8-46
reorder point (ROP)
implications of, 10-13 to 10-14, 10-22,
10-24 to 10-25, 10-31, 10-33
reserve capital, 14-8
residual capacity, 4-10
resilience, 11-9 to 11-11, 11-19 to 11-21
resource allocation, decision making
process, 6-2
resource availability, product mix problem,
1-5, 1-17
response, in complex networks, 11-17
revenue management (RM), problem, 14-19 to
14-20
reversible failures, 11-21
RHS constants, 1-23 to 1-24
risk-averse decision makers, 6-12, 6-27
risk-averse utility function, 6-12 to 6-13
risk management, 6-3 to 6-17, 6-27
risk-neutral
decision makers, 6-12 to 6-13
utility function, 6-12 to 6-13
risk-seeker decision makers, 6-12 to 6-13,
6-27
risk-seeking utility function, 6-12 to 6-13

Index
I-17
risk tolerance, 6-17
robustness, complex networks, 11-19 to
11-20
robust optimization
capacity expansion problem, 14-24 to
14-27
classical models of
chance-constrained model, 14-9 to 14-10
characterized, 14-2 to 14-3
fallacy of averages, 14-2
mean-risk model, 14-3 to 14-7
recourse model, 14-7 to 14-8, 14-21,
14-31
deﬁned, 14-2
electric circuit example, 14-29 to 14-30
engineering design, 14-28 to 14-31
facility location problem, 14-27 to 14-28
job sequencing, 14-16 to 14-17
models of
interval uncertainty, 14-11 to 14-14,
14-20
minimax regret, 14-14 to 14-16, 14-18
to 14-22, 14-26
uncertainty sets, 14-16, 14-24 to 14-27
worst-case hedge, 14-10 to 14-11
newsboy problem, 14-17 to 14-19
revenue management, 14-19 to 14-20
sensor placement problem, 14-31
stochastic programming modeling, 14-30
to 14-31
transportation problem, 14-8, 14-20 to
14-27
rounding up/down, 3-2
round-off errors, 2-20
routers, complex networks, 11-3
S
safety stock, 10-21 to 10-25, 10-37
salvage regret, 14-18 to 14-19
SAS, 8-47
satisﬁcing solution, 5-4
satisﬁed constraints, integer programming,
3-6
savage regret, 6-20
scale-free networks, 11-8, 11-16 to 11-17
scale invariance, 11-17
scatter search, 13-1 to 13-2
scheduling, components of, 5-24,
scientiﬁc collaboration networks, 11-4
SCOV, 9-32, 9-34 to 9-36
scrap metal blending problems, 1-2 to 1-4
searchable networks, 11-23
second-order approximations, 2-16
to 2-17
secord-order cone optimization problem, 2-2
self-organization, 11-17
self-similarity, 11-17
semideﬁnite optimization problem, 2-2
SensIt software, 6-6
sensitivity analysis, 5-11, 6-5, 6-29, 12-22
sensor placement problem, 13-13, 14-31
sequential unconstrained minimization
techniques (SUMT), 2-17 to 2-18
serial supply chain inventory models
centralized control, 10-32 to 10-34
decentralized control, 10-31 to 10-32
serial system, 10-29
set-covering problem, 3-4
set-up costs, 10-5 to 10-6, 10-35
shadow price, 5-32
shortage cost, 10-21, 10-23 to 10-24
shortest path
distance matrix, metaheuristics, 13-4
dynamic programming compared
with, 7-16
problem
characterized, 4-2, 4-5, 4-14 to 4-15,
4-18
Dijkstra’s algorithm, 4-5 to 4-8
large-scale networks, 11-2
linear program formulation, 4-5
shortest processing time ﬁrst (SPTF), 9-3
shortest processing time (SPT) rule, 14-17
shortest total path length spanning tree
problem, 4-18
Sierpinski triangle, 11-17
Silver-Meal inventory method, 10-20
SIMAN, 12-7
simplex method
characterized, 2-9 to 2-10
network optimization, 4-9
simplex solution
integer programming, 3-13 to 3-15
network optimization, 4-4
simulated annealing, 2-3, 3-16, 13-1 to 13-2,
13-6 to 13-8, 13-27, 13-35 to 13-36
simulation
applications, 12-1 to 12-2, 12-15
attributes, 12-4
basics of, 12-3 to 12-15
batch means analysis, 12-13 to 12-14
characterized, 12-1 to 12-2, 12-22 to 12-23
clock, 12-5
common random numbers (CRN) method,
12-14 to 12-15
deterministic delay, 12-6

I-18
Index
simulation (Contd.)
event calendar, 12-5 to 12-7
examples of, 12-2 to 12-3
global variables, 12-4
information resources, 12-3, 12-20, 12-22
languages, 12-7, 12-15 to 12-16, 12-22
modeling process, 12-4
nonterminating system analysis, 12-8,
12-10 to 12-14
output-analysis, 12-7 to 12-15, 12-21 to 12-22
preloading, 12-11
process-oriented, 12-4 to 12-5
queues, 12-4
randomness, 12-7 to 12-8
reasons for, 12-2
replications, 12-3 to 12-4, 12-8, 12-10 to 12-11
resources, 12-4
run, 12-3, 12-10, 12-15
server availability, 12-5 to 12-6
software, 12-15 to 12-20
state of the system, 12-3 to 12-4
system parameters, 12-3 to 12-4
system variables, 12-4
terminating system analysis, 12-8 to 12-10
work in process, 12-20 to 12-22
SIMULS, 12-19
single location inventory, 10-7
single-product inventory, 10-8
single-server, single-queue system,
simulations, 12-5
single solution metaheuristics, 13-2
single sourcing, 10-8
single-station queues
closed-queueing networks, 9-31 to 9-32
and multiclass queues, 9-21 to 9-27
and single-class queues, 9-8 to 9-21
single-variable quadratic function, dynamic
programming, 7-10
single warehouse, multiple retailer inventory
system, 10-30
sink nodes, network optimization, 4-8 to 4-10,
4-15
sinks, transportation problem, 1-10
six degrees of separation, 11-7
slack variables, 1-24, 1-27, 14-1
SLAM, 12-7
slope, 1-13 to 1-14, 2-4
slotted ALOHA, 8-3, 8-18
small-world effect, 11-6 to 11-7, 11-15 to 11-16
social mobility, stochastics example, 8-4 to
8-7, 8-23
social networks, 11-25, 11-27
Society for Modeling and Simulation
International, 12-3
soft datasets, 13-16
software programs
decision analysis, 6-6, 6-17
integer linear programming, 3-16
inventory management, 10-35
linear programming, 1-9, 1-25 to 1-26, 1-31
simulations, 12-1, 12-15 to 12-20
sojourn time, 8-28
solver software, 1-31
source nodes, network optimization, 4-5, 4-8 to
4-10, 4-15
sources, transportation problem, 1-10
spanning tree, characterized, 4-2, 4-4, 4-14
sparse networks, 14-24
spatial networks, search in, 11-22 to 11-23
species set cover problem (SSCP), 13-14
splitting
activities, 14-5 to 14-6
queueing theory, 9-29, 9-35
spreading processes, 11-18 to 11-9, 11-26
spreadsheet applications, inventory control, 10-35
stage, deﬁned, 7-2
standard deviation, implications of, 10-8, 12-13
state, deﬁned, 7-2
state-dependent service, 9-10, 10-29 to 10-30
states of nature, deﬁned, 6-2 to 6-3, 6-5
state space, 8-4 to 8-5, 8-17
stationary continuous-time Markov chains, 8-28
stationary coordination policy, 10-30
stationary demands, 10-8
stationary discrete-time Markov chains, 8-15
stationary distribution, 8-24, 8-34, 8-45
stationary increments, 8-5, 8-8 to 8-9
stationary nested coordination policy, 10-30
statistical analysis, simulations,
12-1, 12-22
statistical inference
continuous-time Markov chain (CTMC), 8-31
to 8-33
discrete-time Markov chain (DTMC), 8-25 to
8-27
implications of, 8-46
renewal theory, 8-46
statistical physics, 11-4
steady-state probability, 9-29, 9-31, 9-33
step length, 2-4
STEP method (STEM), 5-30 to 5-31, 5-33
stochastic demand, 10-8
stochastic dynamic programming
models
characterized, 7-19
equipment replacement problem, 7-19
to 7-22
investment planning, 7-22 to 7-24

Index
I-19
stochastic inventory systems
characterized, 10-20
examples of, 10-20 to 10-21
maximum-minimum system, 10-22 to
10-25
multi-period inventory model (s, S) policy,
10-27
one-period system, 10-26 to 10-27
safety stock, 10-21 to 10-25, 10-37
simpliﬁed solution method, 10-25 to 10-26
stochastic matrix, 8-15, 9-31
stochastic optimization problem, 2-2
stochastic processes applications, 8-1 to 8-7
continuous-time Markov chains (CTMC),
8-27 to 8-38, 9-8
deﬁned, 8-1
discrete-time Markov chains (DTMC), 8-5 to
8-6, 8-14 to 8-27
Poisson processes, 8-7 to 8-14
renewal theory, 8-39 to 8-46
software products, 8-46 to 8-47
stochastic programming
modeling, 14-30 to 14-31
stock investment, decision analysis example,
6-3 to 6-8, 6-14
stopping rules, 2-4
storage problems, 1-11 to 1-12
subproblems
dynamic programming, 7-3, 7-5,
7-10 to 7-11
integer linear programming, B&B
method, 3-7 to 3-12
network optimization, 4-1
partitioning algorithm, 5-21 to 5-24
shortest path problem, 4-4
substitute-objective-function technique, 1-20
sum quota sampling, 8-11 to 8-12
SUNSET SOFTWARE/XA, 4-19
superposition, queueing theory, 9-35
suppliers, unreliable, 10-6
supply and demand, 4-3 to 4-4, 4-9, 8-30,
10-13
supply chain
common structures, 10-29
inventory models, 10-27
vulnerability of, 10-37
surrogate criteria, 5-3
surveys
decision analysis, 6-29
simulation software, 12-23
susceptibility, complex networks, 11-17
swarm intelligence, 2-3
Swift & Co., inventory management, 10-34
system of linear equations, 1-2
T
table lookup scheme, 13-8
tabu list, 13-9, 13-32
tabu search, 2-19, 13-1 to 13-2, 13-8 to 10, 13-25
to 13-26
tabu size, 13-9
tabu tenure, 13-18 to 13-19, 13-21
tail node, 4-2
tally statistic, 12-8
target value, 1-32
Tchebycheff (Min-Max) goal programming,
5-26
Tchebycheff Metric, 5-29
technology coefﬁcients, 1-5
telecommunication networks, 4-4, 4-9, 11-27
Tempfactor, 13-6 to 13-7
theorems
multiple criteria mathematical
programming (MCMP), 5-18, 5-29
optical policy and value function, 7-11
robust optimization, 14-4 to 14-5, 14-11
time coordination, 10-30
time-homogeneous continuous-time Markov
chain, 8-28
time-homogeneous discrete-time Markov
chains, 8-15
time value of money, 7-7
Time Warner, inventory management, 10-34
TOPSIS (technique for order preference by
similarity to ideal solution), 5-9 to
5-10
tradeoffs
in decision analysis, 6-25 to 6-26
information, 1-32
ratio, 5-29 to 5-30
trafﬁc
intensity, queueing theory, 9-36
transformation function, dynamic
programming, 7-2, 7-4, 7-8, 7-17
transformation methods, 2-15, 2-17 to 2-19
transient analysis, 8-18 to 8-20, 8-31 to 8-33
transient state, 8-21, 8-25
transition matrix, 8-20 to 8-21, 8-24
to 8-25, 8-28
transition probabilities, 8-5, 8-17,
8-26 to 8-27
transportation
networks, minimum cost multicommodity
ﬂow problem, 4-18
problems, 1-9 to 1-11, 4-3 to 4-4, 14-8,
14-20 to 14-27
transportation-distribution decisions, 1-12
transshipment nodes, 4-9

I-20
Index
tree, see speciﬁc types of trees
deﬁned, 4-2
shortest path problem, 4-6, 4-8
true criteria, 5-3
truncation errors, 2-20
turboprop, 13-10
turboprop noise, 13-14
T-year planning horizon, 4-5
U
uncapacitated network ﬂow problem,
4-3 to 4-4
uncertainty
budget of, 14-16
in decision analysis, snack product example
characterized, 6-18
expected value, 6-20
impact of, 6-4, 6-17, 6-28
Laplace criterion, 6-20
maxmin (minimax) criterion, 6-18
maxmin (minimin) criterion, 6-19
minmax regret (savage regret), 6-20
effects of, 14-1
impact on decision making process,
6-2 to 6-3, 6-17 to 6-20,
6-26 to 6-27
interval, 14-11 to 14-14
inventory management and, 10-22
robust optimization models, 14-16, 14-24 to
14-27
sources of, 14-1
underage cost, 10-27
undirected network, 4-3
uniform distribution, 8-11 to 8-12, 8-14, 13-31
uniformization, 8-31, 8-33
uniform supply, 10-14
unknowns, deﬁned, 1-2
unslotted ALOHA, 8-3 to 8-5
upper bounds
dynamic programming, 7-18
goal programming, 5-26
multiple criteria decision making
(MCDM), 5-4
robust optimization, 14-11 to 14-12
signiﬁcance of, 4-10, 13-27 to 13-28
upstream, in supply chain, 10-29
utility function approximate, 6-16
characterized, 6-12 to 6-13
classiﬁcation of, 6-13
construction of, 6-15
investment example, 6-17
lottery example, 6-14 to 6-17
utility theory, 6-14
utilization, queueing networks, 9-35
V
validation, simulations, 12-21
variable neighborhood descent, 13-1 to 13-2
variables, deﬁned, 1-2
variance-covariance matrix, 14-3
vector(s), see speciﬁc types of vectors
comparison, 5-29
optimization problem, 5-2
vibrational damping, 13-13
vibration control, 13-21 to 13-23
volatility, impact of, 14-1
Von Neumann, 1-4
voters’ attitude changes problem, 8-26 to 8-27
W
Wagner-Whitin inventory model, 10-18 to 10-20
walk, deﬁned, 4-2
warehousing, inventory control and, 10-6
warm-up period, 12-12 to 12-13
warranty, prorated, 8-43 to 8-44
water networks, 4-9
weighted average, 5-12
weighting, assignment problem, 4-13
what-if scenarios, 12-2
Whitt, Ward, 9-36
width-ﬁrst searches, integer programming, 3-9
window ﬂow control, 9-30
wireless sensor networks, 11-22
WITNESS, 12-19 to 12-20
Wolfe conditions, 2-8, 2-14
workforce planning, 4-4
work in process (WIP), 12-20 to 12-21
workload estimates, 1-19 to 1-20
World Wide Web, see also internet
as complex network, 11-3, 11-5 to 11-6,
11-9, 11-16 to 11-17, 11-19, 11-25
worst case analysis, 1-17
X
x 2 statistic, 8-27, 8-38
Z
zero inventory ordering property, 10-19,
10-30 to 10-31
zero minimum inventory, 10-17
Zionts-Wallenius (ZW) method, 5-31



Copyright © MathByte Academy

What this course is about
the Python language
the standard library
idiomatic Python
becoming an expert Python developer
obtaining a deeper understanding of the Python language
and the standard library
this is NOT an introductory course
à refer to prerequisites video or course description
à canonical CPython 3.6+ implementation
Copyright © MathByte Academy

Included Course Materials
lecture videos
Jupyter notebooks
coding videos
projects and solutions
github repository for all code
https://github.com/fbaptiste/python-deepdive
Copyright © MathByte Academy

Sequence Types
what are sequences?
slicing
à ranges
shallow vs deep copy
the sequence protocol
implementing our own sequence types
list comprehensions
à closures
sorting
à sort key functions
Copyright © MathByte Academy

Iterables and Iterators
more general than sequence types
differences between iterables and iterators
lazy vs eager iterables
the iterable protocol
the iterator protocol
writing our own custom iterables and iterators
Copyright © MathByte Academy

Generators
what are generator?
generator functions
the yield statement
how generators are related to iterators
the yield from statement
generator expressions
Copyright © MathByte Academy

Iteration Tools
Many useful tools for functional approach to iteration
à itertools module
à built-in
à functools module
Aggregators
Slicing iterables
Selection and filtering
Infinite iterators
Mapping and reducing
Grouping
Combinatorics
Copyright © MathByte Academy

Context Managers
what are context managers?
the context manager protocol
why are they so useful?
creating custom context managers using the context manager protocol
creating custom context managers using generator functions
Copyright © MathByte Academy

Projects
project after each section
should attempt these yourself first – practice makes perfect!
solution videos and notebooks provided
à my approach
à more than one approach possible
Copyright © MathByte Academy

Extras
will keep growing over time
important new features of Python 3.6 and later
best practices
random collection of interesting stuff
send me your suggestions!
additional resources
Copyright © MathByte Academy

Copyright © MathByte Academy

Python 3: Deep Dive (Part 2) - Prerequisites
This course assumes that you have in-depth knowledge of the following:
functions and function arguments
def my_func(p1, p2, *args, k1=None, **kwargs)
packing and unpacking iterables
my_func(*my_list)
f, *_, l = (1, 2, 3, 4, 5)
closures
decorators
nested scopes
free variables
@my_decorator
@my_decorator(p1, p2)
bool(obj)
Boolean truth values
named tuples
namedtuple('Data', 'field_1 field_2')
lambdas
lambda x, y: x+y
== vs is
id(obj)
Copyright © MathByte Academy

Python 3: Deep Dive (Part 2) - Prerequisites
This course assumes that you have in-depth knowledge of the following:
zip
map
reduce
filter
sorted
zip(list1, list2, list3)
map(lambda x: x**2, my_list)
reduce(lambda x, y: x * y, my_list, 10)
filter(lambda p: p. age > 18, persons)
sorted(persons, lambda p: p.name.lower())
imports
import math
from math import sqrt, sin
from math import sqrt as sq
from math import *
Copyright © MathByte Academy

Python 3: Deep Dive (Part 2) - Prerequisites
You should have a basic understanding of creating and using classes in Python
class Person:
def __init__(self, name, age):
self.name = name
self.age = age
@property
def age(self):
return self._age
@age.setter
def age(self, age):
if value <= 0:
raise ValueError('Age must be greater than 0')
else:
self._age = age
Copyright © MathByte Academy

Python 3: Deep Dive (Part 2) - Prerequisites
You should understand how special functionality is implemented in Python using special methods
class Point:
def __init__(self, x, y):
self.x = x
self.y = y
def __repr__(self):
return f'Point(x={self.x}, y={self.y})'
def __eq__(self, other):
if not isinstance(other, Point):
return False
else:
return self.x == other.x and self.y == other.y
def __gt__(self, other):
if not isinstance(other, Point):
return NotImplemented
else:
return self.x ** 2 + self.y ** 2 > other.x**2 + other.y**2
def __add__(self, other):
…
Copyright © MathByte Academy

Python 3: Deep Dive (Part 2) - Prerequisites
You should also have a basic understanding of:
for loops, while loops
break
continue
else
branching
if … elif… else…
exception handling
try:
my_func()
except ValueError as ex:
handle_value_error()
finally:
cleanup()
Copyright © MathByte Academy

Copyright © MathByte Academy

This course is all about the Python language and the standard library
I do not make use of any 3rd party library
EXCEPT
Jupyter Notebooks
à I can provide you fully annotated code
à all notebooks are downloadable
à but you should really use github
https://github.com/fbaptiste/python-deepdive
To follow along you will therefore need:
CPython 3.6 or higher
Jupyter Notebook
You favorite Python editor:
VSCode, PyCharm, command line + VIM/Nano/…
I use Anaconda's Python installation:
https://conda.io/docs/index.html
Copyright © MathByte Academy

Copyright © MathByte Academy

what are sequences?
indexing starting at 0
slices include lower bound index, but exclude upper bound index
slice objects
copying sequences – shallow and deep
implementing custom sequence types
sorting
list comprehensions
slicing
modifying mutable sequences
Copyright © MathByte Academy

Copyright © MathByte Academy

What is a sequence?
In Math:
S = x1, x2, x3, x4, …
Note the sequence of indices: 1, 2, 3, 4, …
We can refer to any item in the sequence by using it's index number 
x2
or     S[2]
(countable sequence)
In Python, we start index numbers at 0, not 1
(we'll see why later)
So we have a concept of the first element, the second element, and so on…
Python lists have a concept of positional order, but sets do not
A list is a sequence type
A set is not
S = x0, x1, x2, x3, …
à positional ordering
à
S[2] is the third element
Copyright © MathByte Academy

Built-In Sequence Types
mutable
lists
immutable
strings
tuples
in reality a tuple is more than just a sequence type
bytes
bytearrays
range
Additional Standard Types:
more limited than lists, strings and tuples
namedtuple
deque
collections package
array module
array
Copyright © MathByte Academy

Homogeneous vs Heterogeneous Sequences
Strings are homogeneous sequences
each element is of the same type (a character)
Lists are heterogeneous sequences
each element may be a different type
[1, 10.5, 'python']
Homogeneous sequence types are usually more efficient (storage wise at least)
e.g. prefer using a string of characters, rather than a list or tuple of characters
'python'
Copyright © MathByte Academy

Iterable Type vs Sequence Type
What does it mean for an object to be iterable?
it is a container type of object and we can list out the elements in that object one by one
But an iterable is not necessarily a sequence type
So any sequence type is iterable
s = {1, 2, 3}
s[0]
l = [1, 2, 3]
l[0]
for e in l
for e in s
à iterables are more general
Copyright © MathByte Academy

Standard Sequence Methods
Built-in sequence types, both mutable and immutable, support the following methods
x in s
x not in s
s1 + s2
s * n  (or  n * s)
len(s)
min(s)
max(s)
(if an ordering between elements of s is defined)
s.index(x)
s.index(x, i)
s.index(x, i, j)
index of first occurrence of x in s
index of first occurrence of x in s at or after index i
index of first occurrence of x in s at or after index i and before index j
This is not the same as the ordering (position) of elements 
inside the container, this is the ability to compare pairwise 
elements using an order comparison (e.g. <, <=, etc.)
(n an integer)
concatenation
repetition
Copyright © MathByte Academy

Standard Sequence Methods
s[i]
s[i:j]
s[i:j:k]
the element at index i
extended slice from index i, to (but not including) j, in steps of k
the slice from index i, to (but not including) j
Note that slices will return in the same container type
range objects are more restrictive:
no concatenation / repetition
min, max, in, not in not as efficient
We will come back to slicing in a lot more detail in an upcoming video
Copyright © MathByte Academy

Hashing
Immutable sequence types may support hashing
hash(s)
We'll see this in more detail when we look at Mapping Types
but not if they contain mutable types!
Copyright © MathByte Academy

Review: Beware of Concatenations
x = [1, 2]
a = x + x
a à [1, 2, 1, 2]
x = 'python'
a = x + x
a à 'pythonpython'
x = [ [0, 0] ]
a = x + x
a à [ [0, 0], [0, 0] ]
id(a[0])
id(x[0])
id(a[1])
==
==
a[0][0] = 100
a à [ [100, 0], [100, 0] ]
a[0] is x[0]
a[1] is x[0]
Copyright © MathByte Academy

Review: Beware of Repetitions
a = [1, 2] * 2
a à [1, 2, 1, 2]
a = [ [0, 0] ] * 2
a à [ [0, 0], [0, 0] ]
id(a[0])
id
id(a[1])
==
==
a[0][0] = 100
a à [ [100, 0], [100, 0] ]
a = 'python' * 2
a à 'pythonpython'
a = ['python'] * 2
a à ['python', 'python']
Same happens here, but because strings are immutable it's quite safe
Copyright © MathByte Academy

Copyright © MathByte Academy

Mutating Objects
Mutating an object means changing the object's state without creating a new object
names = ['Eric', 'John']
Eric
John
0xFF255
names
names.append('Michael')
Eric
John
Michael
names = ['Eric', 'John']
Eric
John
names
names = names + ['Michael']
0xFF255
Eric
John
Michael
0xAA2345
This is NOT mutation!
Copyright © MathByte Academy

Mutating Using []
s[i] = x
s[i:j] = s2
element at index i is replaced with x
slice is replaced by the contents of the iterable s2
del s[i]
removes element at index i
We will come back to mutating using slicing in a lot more detail in an 
upcoming video
We can even assign to extended slices:
s[i:j:k] = s2
del s[i:j]
removes entire slice
Copyright © MathByte Academy

Some methods supported by mutable sequence types such as lists
s.append(x)
appends x to the end of s
s.clear()
removes all items from s
s.insert(i, x)
inserts x at index i
s.pop(i)
removes and returns element at index i
s.remove(x)
removes the first occurrence of x in s
s.reverse()
does an in-place reversal of elements of s
s.copy()
returns a shallow copy
s.extend(iterable)
appends contents of iterable to the end of s
and more…
Copyright © MathByte Academy

Copyright © MathByte Academy

Why does sequence indexing start at 0, and not 1?
Why does a sequence slice s[i:j] include s[i], but exclude s[j]?
à there are rational and practical reasons behind doing so
this is not just an arbitrary choice
Valid Questions
We want to determine how we should handle sequences of consecutive integers
à represent positions of elements in a sequence
['a', 'b', 'c', 'd']
1    2     3    4 
0    1     2    3 
Copyright © MathByte Academy

Slice Bounds
Consider the following sequence of integers
1, 2, 3, …, 15
How can we describe this range of numbers without using an ellipsis (…)?
a)
b)
c)
d)
1 <= n <= 15
0 <  n <= 15
1 <= n < 16
0 <  n < 16
(b) and (d) can become odd at times.
Suppose we want to describe the unsigned integers 0, 1, 2, …, 10
Using (b) or (d) we would need to use a signed integer for the lower bound:
b)
-1 <  n <= 10
d)
-1 <  n < 11
Copyright © MathByte Academy

a)
c)
2 <= n <= 16
2 <= n < 17
Now consider this sequence:
2, 3, …, 16
How many elements are in this sequence?
15
Calculating number of elements from bounds in (a) and (c)
a)  15 = 16 – 2 + 1
# = upper – lower + 1
c)  15 = 17 – 2
# = upper – lower
So, (c) seems simpler for that calculation
We'll get to a second reason in a bit, but for now we'll use convention (c)
Copyright © MathByte Academy

Starting Indexing at 0 instead of 1
When we count elements we naturally start counting at 1, so why start indexing at 0?
Consider the following sequence:
2, 3, 4, …, 16
index n (1 based)
index n (0 based)
1, 2, 3, …, 15
0, 1, 2, …, 14
0 <= n < 15
1 <= n < 16
sequence length: 15
upper bound = length + 1
upper bound = length
For any sequence s, the index range is given by:
0 based: 0 <= n < len(s)
1 based: 1 <= n < len(s) + 1
So, 0 based appears simpler
Copyright © MathByte Academy

Another reason for choosing 0 based indexing
Consider this sequence:
a, b, c, d, …  z
1 based
1, 2, 3, 4, …, 26
0 based
0, 1, 2, 3, …, 25
How many elements come before d?
1 based
0 based
index(d) à 4
3 elements
index(d) à 3
4-1 elements
3 elements
So, using 0 based indexing, the number of elements that precede an element 
at some index
à is the index itself
Copyright © MathByte Academy

Summarizing so far…
choosing 0 based indexing for sequences
we have the following results
the length of a range(l, u) is given by:
the indices of any sequence s are given by:
range(0, len(s))
number of indices before index n:
l - u
n
describing ranges of indices using range(l, u) à l <= n < u
[0 <= n < len(s)]
s = [a, b, c, …, z]
indices à range(0, 26)
len(s) à 26
n elements precede s[n]
first index: 0
last index:
len(s)-1
0   1   2      25
Copyright © MathByte Academy

Slices
Because of the conventions on starting indexing at 0 and defining ranges using [lower, upper)
we can think of slicing in these terms:
Each item in a sequence is like a box, with the indices between the boxes:
a
b
c
d
e
f
0
1
2
3
4
5
s[2:4]
à [c, d]
6
6 is the length of the sequence
First 2 elements:
Everything else:
s[0:2]
s[2:6]
s[:2]
s[2:]
inclusive
exclusive
In general we can split a sequence into two 
with k elements in the first subsequence:
s[:k]
s[k:]
Copyright © MathByte Academy

Copyright © MathByte Academy

Why copy sequences?
Mutable sequences can be modified. 
Sometimes you want to make sure that whatever sequence you are working with cannot 
be modified, either inadvertently by yourself, or by 3rd party functions
Also consider this example:
We saw an example of this earlier with list concatenations and repetitions.
def reverse(s):
s.reverse()
return s
s = [10, 20, 30]
new_list = reverse(s)
new_list
à [30, 20, 10]
s
à [30, 20, 10]
We should have passed it a copy of our list if we 
did not intend for our original list to be modified
Copyright © MathByte Academy

Soapbox
def reverse(s):
s.reverse()
return s
Generally we write functions that do not modify the contents of their arguments.
à in-place methods
But sometimes we really want to do so, and that's perfectly fine
However, to clearly indicate to the caller that something is happening in-place, we should not
return the object we modified
If we don't return s in the above example, the caller will probably wonder why not?
So, in this case, the following would be a better approach:
def reverse(s):
s.reverse()
and if we do not do in-place reversal, then we return the reversed sequence
def reverse(s):
s2 = <copy of s>
s2.reverse()
return s2
Copyright © MathByte Academy

How to copy a sequence
We can copy a sequence using a variety of methods:
Simple Loop
cp = []
for e in s:
cp.append(e)
List Comprehension
cp = [e for e in s]
The copy method
cp = s.copy()
s = [10, 20, 30]
Slicing
cp = s[0:len(s)]
or, more simply
cp = s[:]
definitely non-Pythonic!
list()
list_2 = list(list_1)
Note:
tuple_2 = tuple(tuple_1) and t[:]
does not create a new tuple!
(not implemented in immutable types, such as tuples or strings)
The copy module
Copyright © MathByte Academy

Since the sequence is immutable, it is actually OK to return the same sequence
Watch out when copying entire immutable sequences
l1 = [1, 2, 3]
l2 = list(l1)
l2 à [1, 2, 3]
id(l1) ≠id(l2)
t1 = (1, 2, 3)
t2 = tuple(t1)
t2 à (1, 2, 3)
id(t1) = id(t2)
same object!
t1 = (1, 2, 3)
t2 = t1[:]
t2 à (1, 2, 3)
id(t1) = id(t2)
same object!
Same thing with strings, also an immutable sequence type
Copyright © MathByte Academy

Shallow Copies
Using any of the techniques above, we have obtained a copy of the original sequence
s = [10, 20, 30]
cp = s.copy()
cp[0] = 100
cp à [100, 20, 30]
s à [10, 20, 30]
Great, so now our sequence s will always be safe from unintended modifications?
s = [ [10, 20], [30, 40] ]
cp = s.copy()
cp[0] = 'python'
cp à ['python', [30, 40] ]
s à [ [10, 20], [30, 40] ]
cp[1][0] = 100
cp à ['python', [100, 40] ]
s à [ [10, 20], [100, 40] ]
Not quite…
Copyright © MathByte Academy

Shallow Copies
What happened?
When we use any of the copy methods we saw a few slides ago, the copy essentially copies
all the object references from one sequence to another
s = [a, b]
id(s[0]) à 2000
id(s[1]) à 3000
id(s) à 1000
cp = s.copy()
id(cp) à 5000
id(cp[0]) à 2000
id(cp[1]) à 3000
When we made a copy of s, the sequence was copied, but it's elements point to the 
same memory address as the original sequence elements
The sequence was copied, but it's elements were not
This is called a shallow copy
Copyright © MathByte Academy

Shallow Copies
If the elements of s are immutable, such as integers in this example, 
then not really important
s = [ 1, 2 ]
cp = s.copy()
s
0xF100
cp
0xF200
1
0xA100
2
0xA200
3
0xA300
cp.append(3)
cp[1] = 3
Copyright © MathByte Academy

But, if the elements of s are mutable, then it can be important
s = [ [0, 0], [0, 0] ]
cp = s.copy()
Shallow Copies
s
0xF100
cp
0xF200
[0, 0]
0xA100
[0, 0]
0xA200
cp[0]
s[0]
cp[1]
s[1]
cp[0][0] = 100
cp à [ [100, 0], [0, 0] ]
s à [ [100, 0], [0, 0] ]
Copyright © MathByte Academy

Deep Copies
So, if collections contain mutable elements, shallow copies are not sufficient to ensure the copy
can never be used to modify the original!
Instead, we have to do something called a deep copy.
For the previous example we might try this:
s = [ [0, 0], [0, 0] ]
cp = [e.copy() for e in s]
In this case:
cp is a copy of s
but also, every element of cp is a copy of the corresponding element in s
shallow copy
s
0xF100
cp
0xF200
[0, 0]
0xA100
[0, 0]
0xA200
[0, 0]
0xA300
[0, 0]
0xA400
Copyright © MathByte Academy

Deep Copies
But what happens if the mutable elements of s themselves contain mutable elements?
s = [ [ [0, 1], [2, 3] ], [ [4, 5], [6, 7] ] ]
We would need to make copies at least 3 levels deep to ensure a true deep copy
Deep copies, in general, tend to need a recursive approach
Copyright © MathByte Academy

[10, 20, b]
Deep Copies
Deep copies are not easy to do. You might even have to deal with circular references
a = [10, 20]
b = [a, 30]
a.append(b)
If you wrote your own deep copy algorithm, you would need to handle this circular reference!
[10, 20]
[a, 30]
b
a
Copyright © MathByte Academy

Deep Copies
In general, objects know how to make shallow copies of themselves
built-in objects like lists, sets, and dictionaries do - they have a copy() method
The standard library copy module has generic copy and deepcopy operations
The copy function will create a shallow copy
The deepcopy function will create a deep copy, handling nested objects, and circular 
references properly
Custom classes can implement the __copy__ and __deepcopy__ methods to allow you to 
override how shallow and deep copies are made for you custom objects
We'll revisit this advanced topic of overriding deep copies of custom 
classes in the OOP series of this course.
Copyright © MathByte Academy

Deep Copies
Suppose we have a custom class as follows:
def MyClass:
def __init__(self, a):
self.a = a
from copy import copy, deepcopy
x = [10, 20]
obj = MyClass(x)
x is obj.a à True
cp_shallow = copy(obj)
cp_shallow.a is obj.a à True
cp_deep = deepcopy(obj)
cp_deep.a is obj.a à False
[10, 20]
0xA100
obj
.a
0xF100
cp_shallow
.a
0xF200
cp_deep
.a
0xF300
[10, 20]
0xA200
x
copy of x (deep)
Copyright © MathByte Academy

Deep Copies
def MyClass:
def __init__(self, a):
self.a = a
x = MyClass(500)
y = MyClass(x)
lst = [x, y]
y.a is x à True
cp = deepcopy(lst)
cp[0] is x à False
cp[1] is y à False
cp[1].a is x à False
cp[1].a is cp[0] à True
this is not a circular reference
lst
x
y
.a
cp
cp_x
cp_y
.a
lst[0]
lst[1]
cp[1]
cp[0]
but there is a relationship
between y.a and x
relationship between cp_y.a and cp_x
is maintained!
Copyright © MathByte Academy

Copyright © MathByte Academy

We've used slicing in this course before, but now it's time to dive deeper into slicing
Slicing relies on indexing
à only works with sequence types
Immutable Sequence Types
Mutable Sequence Types
extract data
extract data
assign data
Example
l = [1, 2, 3, 4, 5]
l[0:2] à ['a', 'b', 'c', 3, 4, 5]
l[0:2] = ('a', 'b', 'c')
l[0:2] à [1, 2]
Copyright © MathByte Academy

The Slice Type
Although we usually slice sequences using the more conventional notation:
my_list[i:j]
slice definitions are actually objects
à of type slice
s = slice(0, 2)
type(s) à slice
s.start à 0
s.end à 2
l = [1, 2, 3, 4, 5]
l[s] à [1, 2]
This can be useful because we can name slices and use symbols 
instead of a literal subsequently
Similar to how you can name ranges in Excel…
Copyright © MathByte Academy

a
b
c
d
e
f
0
1
2
3
4
5
6
Slice Start and Stop Bounds
[i:j]
start at i (including i)
stop at j (excluding j)
all integers k where i <= k < j
also remember that indexing is zero-based
It can be convenient to think of slice bounds this way
[1:4]
Copyright © MathByte Academy

Effective Start and Stop Bounds
Interestingly the following works:
l = ['a', 'b', 'c', 'd', 'e', 'f']
l[3:100] à ['d', 'e', 'f']
No error!
In fact, negative indices work too:
l[-1]
à 'f'
l[-3: -1]
à ['d', 'e']
a
b
c
d
e
f
0
1
2
3
4
5
6
-6
-5
-4
-3
-2
-1
we can specify slices that are "out of bounds"
Copyright © MathByte Academy

Step Value
Slices also support a third argument – the step value
[i:j:k]
slice(i, j, k)
When not specified, the step value defaults to 1
l = ['a', 'b', 'c', 'd', 'e', 'f']
0
1
2
3
4
5
l[0:6:2]
à ['a', 'c', 'e']
l[1:6:3]
à ['b', 'e']
0, 2, 4
1, 4
l[1:15:3]
à ['b', 'e']
1, 4
l[-1:-4:-1]
-1, -2, -3
-6
-5
-4
-3
-2
-1
à ['f', 'e', 'd']
(a.k.a stride)
Copyright © MathByte Academy

Range Equivalence
Any slice essentially defines a sequence of indices that is used to select elements for another 
sequence
In fact, any indices defined by a slice can also be defined using a range
The difference is that slices are defined independently of the sequence being sliced
The equivalent range is only calculated once the length of the sequence being sliced is known
Example
[0:100]
l sequence of length 10
à range(0, 10)
l sequence of length 6
à range(0, 6)
Copyright © MathByte Academy

Transformations [i:j]
The effective indices "generated" by a slice are actually dependent on the length of the 
sequence being sliced
Python does this by reducing the slice using the following rules: 
seq[i:j]
if i > len(seq)
if j > len(seq)
à len(seq)
à len(seq)
if i < 0
à max(0, len(seq) + i)
if j < 0
à max(0, len(seq) + j)
[0:100] à range(0, 6)
i omitted or None
à 0
j omitted or None
à len(seq)
[-10:3] à range(0, 3)
[:100] à range(0, 6)
[3:] à range(3, 6)
[:] à range(0, 6)
length = 6
[-5:3] à range(1, 3)
l = ['a', 'b', 'c', 'd', 'e', 'f']
0
1
2
3
4
5
-6
-5
-4
-3
-2
-1
Copyright © MathByte Academy

Transformations [i:j:k], k > 0
With extended slicing things change depending on whether k is negative or positive
[i:j:k]
= {x = i + n * k | 0 <= n < (j-i)/k}
the indices are:
i, i+k, i+2k, i+3k, …, < j
stopping when j is reached or exceeded,
but never including j itself
k > 0
if i, j > len(seq)
à len(seq)
if i, j < 0
à max(0, len(seq) + i/j)
[0:100:2] à range(0, 6, 2)
[-10:100:2] à range(0, 6, 2)
i omitted or None
à 0
j omitted or None
à len(seq)
[:6:2] à range(0, 6, 2)
[1::2] à range(1, 6, 2)
[-5:100:2] à range(1, 6, 2)
[::2] à range(0, 6, 2)
so same rules as [i:j] – makes sense, since that would be the same as [i:j:1]
length = 6
l = ['a', 'b', 'c', 'd', 'e', 'f']
0
1
2
3
4
5
-6
-5
-4
-3
-2
-1
Copyright © MathByte Academy

Transformations [i:j:k], k < 0
[i:j:k]
= {x = i + n * k | 0 <= n < (j-i)/k}
the indices are:
i, i+k, i+2k, i+3k, …, > j
if i, j > len(seq)
à len(seq) - 1
if i, j < 0
à max(-1, len(seq) + i/j)
k < 0
[5:2:-1] à range(5, 2, -1)
[10:2:-1] à range(5, 2, -1)
[5:-2:-1] à range(5, 4, -1)
i omitted or None
à len(seq) - 1
j omitted or None
à -1
[:-2:-1] à range(5, 4, -1)
[5::-1] à range(5, -1, -1)
[::-1] à range(5, -1, -1)
l = ['a', 'b', 'c', 'd', 'e', 'f']
0
1
2
3
4
5
-6
-5
-4
-3
-2
-1
length = 6
[-2:-5:-1] à range(4, 1, -1)
[-2:-10:-1] à range(4, -1, -1)
Copyright © MathByte Academy

Summary
[i:j:k]
[i:j]
k > 0
[i:j:k] k < 0
i > len(seq)
j > len(seq)
i < 0
j < 0
i omitted / None
j omitted / None
len(seq)
len(seq)
max(0, len(seq)+i)
max(0, len(seq)+j)
len(seq)-1
len(seq)-1
max(-1, len(seq)+i)
max(-1, len(seq)+j)
0
len(seq)
len(seq)-1
-1
Copyright © MathByte Academy

Examples
l = ['a', 'b', 'c', 'd', 'e', 'f']
0
1
2
3
4
5
-6
-5
-4
-3
-2
-1
length = 6
[-10:10:1]
-10 à 0
10 à 6
à range(0, 6)
[10:-10:-1]
10 à 5
-10 à max(-1, 6-10) à max(-1, -4) à -1
à range(5, -1, -1)
We can of course easily define empty slices!
[3:-1:-1]
3 à 3
-1 à max(-1, 6-1) à 5
à range(3, 5, -1)
Copyright © MathByte Academy

Example
seq = sequence of length 6
seq[::-1]
i is omitted
à len(seq) – 1 à 5
j is omitted
à -1
à range(5, -1, -1)
à 5, 4, 3, 2, 1, 0
seq = 'python'
seq[::-1] à 'nohtyp'
Copyright © MathByte Academy

If you get confused…
The slice object has a method, indices, that returns the equivalent range start/stop/step
for any slice given the length of the sequence being sliced:
slice(start, stop, step).indices(length) à (start, stop, step)
the values in this tuple can be used to generate a list of indices using the range function
slice(10, -5, -1)
with a sequence of length 6
i=10 > 6 à 6-1 à 5
j=-5 < 0 à max(-1, 6+-5) à max(-1, 1) à 1
à range(5, 1, -1)
à 5, 4, 3, 2
slice(10, -5, -1).indices(6) à (5, 1, -1)
list(range(*slice(10,-5,-1).indices(6)))
à [5, 4, 3, 2]
Copyright © MathByte Academy

Copyright © MathByte Academy

Creating our own Sequence types
We will cover Abstract Base Classes later in this course, so we'll revisit this topic again
At it's most basic, an immutable sequence type should support two things:
returning the length of the sequence
given an index, returning the element at that index
If an object provides this functionality, then we should in theory be able to:
retrieve elements by index using square brackets []
iterate through the elements using Python's native looping mechanisms
e.g. for loops, comprehensions
(technically, we don't even really need that!)
Copyright © MathByte Academy

How Python does it
Remember that sequence types are iterables, but not all iterables are sequence types
Sequence types, at a minimum, implement the following methods:
__len__
__getitem__
At its most basic, the __getitem__ method takes in a single integer argument – the index
However, it may also choose to handle a slice type argument
So how does this help when iterating over the elements of a sequence?
Copyright © MathByte Academy

The __getitem__ method
The __getitem__ method should return an element of the sequence based on the specified index
or raise an IndexError exception if the index is out of bounds
my_list = ['a', 'b', 'c', 'd', 'e', 'f']
Python's list object implements the __getitem__ method:
my_list.__getitem__(0) à 'a'
my_list.__getitem__(1) à 'b'
my_list.__getitem__(-1) à 'f'
my_list.__getitem__(slice(None, None, -1)) 
à ['f', 'e', 'd', 'c', 'b', 'a']
(and may, but does not have 
to, support negative indices 
and slicing)
Copyright © MathByte Academy

The __getitem__ method
But if we specify an index that is out of bounds:
my_list.__getitem__(100) à IndexError
my_list.__getitem__(-100) à IndexError
All we really need from this __getitem__ method is the ability to
return an element for a valid index
raise an IndexError exception for an invalid index
Also remember, that sequence indices start at 0
i.e. we always know the index of the first element of the sequence
Copyright © MathByte Academy

Implementing a for loop
So now we know:
sequence indexing starts at 0
__getitem__(i) will return the element at index i
__getitem__(i) will raise an IndexError exception when i is out of bounds
my_list = [0, 1, 2, 3, 4, 5]
for item in my_list:
print(item ** 2)
index = 0
while True:
try:
item = my_list.__getitem__(index)
except IndexError:
break
print(item ** 2)
index += 1
The point is that if the object implements __getitem__
we can iterate through it using a for loop, or even a comprehension
Copyright © MathByte Academy

The __len__ Method
In general sequence types support the Python built-in function len()
To support this all we need to do is implement the __len__ method in our custom sequence type
my_list = [0, 1, 2, 3, 4, 5]
len(my_list) à 6
my_list.__len__() à 6
Copyright © MathByte Academy

Writing our own Custom Sequence Type
to implement our own custom sequence type we should then implement:
__len__
__getitem__
At the very least __getitem__ should:
return an element for a valid index [0, length-1]
raise an IndexError exception if index is out of bounds
Additionally we can choose to support:
negative indices
i < 0 à i = length - i
slicing
handle slice objects as argument to __getitem__
Copyright © MathByte Academy

Copyright © MathByte Academy

Concatenation
Let's use Python's list as an example
We can concatenate two lists together by using the + operator
This will create a new list combining the elements of both lists
l1 = [1, 2, 3]
l2 = [4, 5, 6]
l1 = l1 + l2
à [1, 2, 3, 4, 5, 6]
id(l1) = 0xFFF100
id(l2) = 0xFFF200
id(l1) = 0xFFF300
+
Copyright © MathByte Academy

In-Place Concatenation
Recall that for numbers I have said many times that
a = a + 10
and
a += 10
meant the same thing?
That's true for numbers…
but not in general!
it's true for numbers, strings, tuples
à in general, true for immutable types
but not lists!
l1 = [1, 2, 3]
l2 = [4, 5, 6]
l1 += l2
à [1, 2, 3, 4, 5, 6]
id(l1) = 0xFFF100
id(l2) = 0xFFF200
id(l1) = 0xFFF100
+=
the list was mutated
Copyright © MathByte Academy

In-Place Concatenation
+=
For immutable types, such as number, strings, tuples the behavior is different
t += t1
has the same effect as t = t + t1
Since t is immutable, += does NOT perform in-place concatenation
Instead it creates a new tuple that concatenates the two tuples and returns the new object
t1 = (1, 2, 3)
t2 = (4, 5, 6)
t1 += t2
à (1, 2, 3, 4, 5, 6)
id(t1) = 0xFFF100
id(t2) = 0xFFF200
id(t1) = 0xFFF300
Copyright © MathByte Academy

In-Place Repetition
Similar result hold for the * and *= operator
l1 = [1, 2, 3]
l1 = l1 * 2
à [1, 2, 3, 1, 2, 3]
id(l1) = 0xFFF100
id(l1) = 0xFFF200
But the in-place repetition operator works this way:
l1 = [1, 2, 3]
l1 *= 2
à [1, 2, 3, 1, 2, 3]
id(l1) = 0xFFF100
id(l1) = 0xFFF100
*=
the list was mutated
Copyright © MathByte Academy

Copyright © MathByte Academy

Assigning Values via Indexes, Slices and Extended Slices
We have seen how we can extract elements from a sequence by using indexing, slicing, and 
extended slicing
[i:j]
slice(i, j)
[i:j:k]
slice (i, j, k)
and they also support assignment via slices
The value being assigned via slicing and extended slicing must to be an iterable
(any iterable, not just a sequence type)
Mutable sequences support assignment via a specific index
[i]
k ≠1
(if k=1 then it's just a standard slice)
Copyright © MathByte Academy

Replacing a Slice
A slice can be replaced with another iterable
For regular slices (non-extended), the slice and the iterable need not be the same length
l = [1, 2, 3, 4, 5]
l[1:3] à [2, 3]
l[1:3] = (10, 20, 30)
l à [1, 10, 20, 30, 4, 5]
The list l was mutated
à id(l) did not change
With extended slicing, the extended slice and the iterable must have the same length
l = [1, 2, 3, 4, 5]
l[0:4:2] à [1, 3]
l[0:4:2] = [10, 30]
l à [10, 2, 30, 4, 5]
The list l was mutated
Copyright © MathByte Academy

Deleting a Slice
Deletion is really just a special case of replacement
We simply assign an empty iterable
à works for standard slicing only
(extended slicing replacement needs same length)
l = [1, 2, 3, 4, 5]
l[1:3] à [2, 3]
l[2:3] = []
l à [1, 4, 5]
The list l was mutated
Copyright © MathByte Academy

Insertions using Slices
We can also insert elements using slice assigment
The trick here is that the slice must be empty
otherwise it would just replace the elements in the slice
l = [1, 2, 3, 4, 5]
l[1:1] à []
l[1:1] = 'abc'
l[1:1] à [1, 'a', 'b', 'c', 2, 3, 4, 5]
The list l was mutated
Obviously this will also not work with extended slices
extended slice assignment requires both lengths to be the same
but for insertion we need the slice to be empty, 
and the iterable to have some values
Copyright © MathByte Academy

Copyright © MathByte Academy

Concatenation and In-Place Concatenation
When dealing with the + and += operators in the context of sequences
we usually expect them to mean concatenation
But essentially, it is just an overloaded definition of these operators
We can overload the definition of these operators in our custom classes by using the methods:
__add__
__iadd__
In general (but not necessarily), we expect:
obj1 + obj2
à obj1 and obj2 are of the same type
obj1 += obj2
à obj2 is any iterable
à result is a new object also of the same type
à result is the original obj1 memory reference
(i.e. obj1 was mutated)
Copyright © MathByte Academy

Repetition and In-Place Repetition
When dealing with the * and *= operators in the context of sequences
we usually expect them to mean repetition
But essentially, it is just an overloaded definition of these operators
We can overload the definition of these operators in our custom classes by using the methods:
__mul__
__imul__
In general (but not necessarily), we expect:
obj1 * n
à n is a non-negative integer
obj1 *= n
à n is a non-negative integer
à result is a new object of the same type as obj1
à result is the original obj1 memory reference
(i.e. obj1 was mutated)
Copyright © MathByte Academy

Assignment
We saw in an earlier lecture how we can implement accessing elements in a custom sequence type
__getitem__
à seq[n]
à seq[i:j]
à seq[i:j:k]
We can handle assignments in a very similar way, by implementing
__setitem__
There a few restrictions with assigning to slices that we have already seen (at least with lists):
For any slice we could only assign an iterable
For extended slices only, both the slice and the iterable must have the same length
Of course, since we are implementing __setitem__ ourselves, we 
could technically make it do whatever we want!
Copyright © MathByte Academy

Additional Sequence Functions and Operators
There are other operators and functions we can support:
in
__contains__
__delitem__
del
__rmul__
n * seq
The way Python works is that when it encounters an expression such as:
a * b
a + b
it first tries
a.__add__(b)
a.__mul__(b)
if a does not support the operation (TypeError), it then tries:
b.__radd__(a)
b.__rmul__(a)
Copyright © MathByte Academy

Implementing append, extend, pop
Actually there's nothing special going here.
If we want to, we can just implement methods of the same name (not special methods)
and they can just behave the same way as we have seen for lists for example
Copyright © MathByte Academy

Copyright © MathByte Academy

Sorting and Sort Keys
Sorting a sequence of numbers is something easily understood
But we do have to consider the direction of the sort:
ascending
descending
Python provides a sorted() function that will sort a given iterable
The default sort direction is ascending
The sorted() function has an optional keyword-only argument 
called reverse which defaults to False
If we set it to True, then the sort will sort in descending order
But one really important thing we need to think about: ordering
à obvious when sorting real numbers
Copyright © MathByte Academy

Sorting and Sort Keys
What about non-numerical values?
'a', 'b', 'c'
'A', 'a', 'B', 'b', 'C', 'c'
(0, 0)  (1, 1)  (2, 2)
(0, 0)  (0, 1)  (1, 0)
rectangle_1, rectangle_2, rectangle_3
'hello', 'python', 'bird', 'parrot'
strings are comparable so this is still OK
although is 'a' < 'A' or 'a' > 'A' or 'a' == 'A'
True
When items are pairwise comparable
but what happens when they are not?
à Sort Keys
(< or >)
we can use that ordering to sort
Copyright © MathByte Academy

Sorting and Sort Keys
'b', 'x', 'a'
ASCII character codes
a à 97 
b à 98
x à 120
We now associate the ASCII numerical value with each character, and sort based on that value
items
keys
'b'
98
'x'
120
'a'
97
à
'b'
98
'x'
120
'a'
97
'B'  'b'  'A'  'a'  'X'  'x'   '1'   '?'
ord('a') à 97
66   98   65   97   88   120   49    63
à
'1'  '?'  'A'  'B'  'X'  'a'   'b'   'x'
49   63   65   66   88   97    98    120
You'll note that the sort keys have a natural sort order
Copyright © MathByte Academy

Sorting and Sort Keys
Let's say we want to sort a list of Person objects based on their age
(assumes the Person
class has an age 
property)
p1.age à 30
p2.age à 15
p3.age à 5
p4.age à 32
item
p1    p2    p3    p4
keys
30    15    5     32
à
p3    p2    p1    p4
5     15    30    32
We could also generate the key value, for any given person, using a function
def key(p):
return p.age
key = lambda p: p.age
sort [p1, p2, p3, p4]
using sort keys generated by the function key = lambda p: p.age
Copyright © MathByte Academy

Sorting and Sort Keys
The sort keys need not be numerical
à they just need to have a natural sort order
item
keys
'hello'   'python'   'parrot'   'bird'
'o'         'n'        't'       'd'
ß last character of each string
à
'bird'   'python'   'hello'   'parrot''
'd'       'n'        'o'        't'
key = lambda s: s[-1]
(< or >)
Copyright © MathByte Academy

Python's sorted function
That's exactly what Python's sorted function allows us to do
Optional keyword-only argument called key
if provided, key must be a function that for any given element in the sequence being sorted
returns the sort key
The sort key does not have to be numerical
à it just needs to be values that are themselves 
pairwise comparable (such as < or >)
If key is not provided, then Python will sort based on the natural ordering of the elements
i.e. they must be pairwise comparable (<, > )
If the elements are not pairwise comparable, you will get an exception
Copyright © MathByte Academy

Python's sorted function
sorted(iterable, key=None, reverse=False) 
keyword-only
The sorted function:
•
makes a copy of the iterable
•
returns the sorted elements in a list
•
uses a sort algorithm called TimSort
à named after Tim Peters
https://en.wikipedia.org/wiki/Timsort
Python 2.3, 2002
•
a stable sort
Side note: for the "natural" sort of elements, we can always think of the keys as the elements 
themselves
sorted(iterable) ßà sorted(iterable, key=lambda x: x)
Copyright © MathByte Academy

Stable Sorts
A stable sort is one that maintains the relative order of items that have equal keys
(or values if using natural ordering)
p1.age à 30
p2.age à 15
p3.age à 5
p4.age à 32
p5.age à 15
sorted((p1, p2, p3, p4, p5), key=lambda p: p.age)
à
[ p3
p2
p5
p1
p4 ]
keys equal
p2 preceded p5 in original tuple
à p2 precedes p5 in sorted list
Copyright © MathByte Academy

In-Place Sorting
If the iterable is mutable, in-place sorting is possible
But that will depend on the particular type you are dealing with
Python's list objects support in-place sorting
The list class has a sort() instance method that does in-place sorting
l = [10, 5, 3, 2]
id(l) à 0xFF42
l.sort()
l à [2, 3, 5, 10]
id(l) à 0xFF42
•
same TimSort algorithm
•
same keyword-only arg: key
•
same keyword-only arg: reverse (default is False)
•
in-place sorting, does not copy the data
•
only works on lists (it's a method in the list class)
Compared to sorted()
Copyright © MathByte Academy

Copyright © MathByte Academy

Quick Recap
You should already know what list comprehensions are, but let's quickly recap their syntax 
and how they work:
goal à generate a list by transforming, and optionally filtering, another iterable
•
iterate over the original iterable
for item in other_list:
other_list = ['this', 'is', 'a', 'parrot']
List comprehension:
new_list = [item[::-1] for item in other_list if len(item) > 2]
•
start with some iterable
•
create empty new list
new_list = []
if len(item) > 2:
new_list.append(item[::-1])
•
skip over certain values (filter)
•
transform value and append to new list
transformation
iteration
filter
Copyright © MathByte Academy

Formatting the Comprehension Expression
If the comprehension expression gets too long, it can be split over multiple lines
For example, let's say we want to create a list of squares of all the integers 
between 1 and 100 that are not divisible by 2, 3 or 5
sq = [i**2 for i in range(1, 101) if i%2 and i%3 and i%5]
We could write this over multiple lines:
sq = [i**2 
for i in range(1, 101) 
if i%2 and i%3 and i%5]
Copyright © MathByte Academy

Comprehension Internals
Comprehensions have their own local scope – just like a function
We should think of a list comprehension as being wrapped in a function that is created by 
Python that will return the new list when executed
sq = [i**2 for i in range(10)]
def temp():
new_list = []
for i in range(10):
new_list.append(i**2)
return new_list
RHS
When the RHS is compiled:
When the line is executed:
Python creates a temporary function 
that will be used to evaluate the 
comprehension
Executes temp()
Stores the returned object (the list) in 
memory
Points sq to that object
We'll disassemble some Python code in the coding video to actually see this
Copyright © MathByte Academy

Comprehension Scopes
So comprehensions are basically functions
They have their own local scope:
[item ** 2 for item in range(100)]
local symbol
But they can access global variables:
# module1.py
num = 100
sq = [item**2 for item in range(num)]
local symbol
global symbol
As well as nonlocal variables:
def my_func(num):
sq = [item**2 for item in range(num)]
Closures!!
nonlocal symbol
Copyright © MathByte Academy

Nested Comprehensions
Comprehensions can be nested within each other
And since they are functions, a nested comprehension can access (nonlocal) variables from the 
enclosing comprehension!
[ [i * j for j in range(5)] for i in range(5)]
nested comprehension
outer comprehension
local variable: i
closure
free variable: i
local variable: j
Copyright © MathByte Academy

Nested Loops in Comprehensions
We can have nested loops (as many levels as we want) in comprehensions.
This is not the same as nested comprehensions
l = []
for i in range(5):
for j in range(5):
for k in range(5):
l.append((i, j, k))
l = [(i, j, k) for i in range(5) for j in range(5) for k in range(5)]
Note that the order in which the for loops are specified in the comprehension
correspond to the order of the nested loops
Copyright © MathByte Academy

Nested Loops in Comprehensions
Nested loops in comprehensions can also contain if statements
Again the order of the for and if statements does matter, just like a normal set of for
loops and if statements
l = []
for i in range(5):
for j in range(5):
if i==j:
l.append((i, j))
l = [(i, j) for i in range(5) for j in range(5) if i == j]
j is referenced after
it has been created
j is created here
l = [(i, j) for i in range(5) if i == j for j in range(5)]
won't work!
l = []
for i in range(5):
if i==j:
for j in range(5):
l.append((i, j))
won't work!
Copyright © MathByte Academy

Nested Loops in Comprehensions
l = []
for i in range(1, 6):
if i%2 == 0:
for j in range(1, 6):
if j%3 == 0:
l.append((i,j))
[(i, j) 
for i in range(1, 6) if i%2==0 
for j in range(1, 6) if j%3==0]
[(i, j) 
for i in range(1, 6) 
for j in range(1, 6) 
if i%2==0
if j%3==0]
[(i, j) 
for i in range(1, 6) 
for j in range(1, 6) 
if i%2==0 and j%3==0]
l = []
for i in range(1, 6):
for j in range(1, 6):
if i%2==0:
if j%3 == 0:
l.append((i,j))
l = []
for i in range(1, 6):
for j in range(1, 6):
if i%2==0 and j%3==0:
l.append((i,j))
Copyright © MathByte Academy

Copyright © MathByte Academy

Background Information
A regular strictly convex polygon is a polygon that has the following characteristics: 
•
all interior angles are less than 180°
•
all sides have equal length
edge (side)
vertex
interior angle
apothem
circumradius
Copyright © MathByte Academy

For a regular strictly convex polygon with
• n edges ( = n vertices)
• R circumradius
Background Information
interior angle = $ −2 × !"#
$
edge length s = 2 ) sin
%
$
apothem a = ) ,-.
%
$
area = !
& $ . /
perimeter = n s
edge
vertex
interior angle
apothem
circumradius
R
a
s
Copyright © MathByte Academy

Goal 1
Create a Polygon class:
Initializer
•
number of edges/vertices
•
circumradius
Properties
•
# edges
•
# vertices
•
interior angle
•
edge length
•
apothem
•
area
•
perimeter
Functionality
•
a proper representation (__repr__)
•
implements equality (==) based on # 
vertices and circumradius (__eq__)
•
implements > based on number of 
vertices only (__gt__)
Copyright © MathByte Academy

Goal 2
Implement a Polygons sequence type:
Initializer
Properties
Functionality
•
number of vertices for largest polygon in the sequence
•
common circumradius for all polygons
•
functions as a sequence type (__getitem__)
•
supports the len() function (__len__)
•
has a proper representation (__repr__)
•
max efficiency polygon: returns the Polygon with the highest area : perimeter ratio
Copyright © MathByte Academy

Copyright © MathByte Academy

Already seen: Sequences and iteration
More general concept of iteration
à get next item, no indexes needed
Iterators
Iterables
Consuming iterators manually
Relationship between sequence types and iterators
Infinite Iterables
Lazy Evaluation
Iterator Delegation
What is an iterable?
Something fit for iterating over
à we'll see a more formal definition for Python's iterable protocol
à consumables
Copyright © MathByte Academy

Copyright © MathByte Academy

Iterating Sequences
We saw that in the last section
à __getitem__
à assumed indexing started at 0
But iteration can be more general than based on sequential indexing
à iteration: __getitem__(0), __getitem__(1), etc
All we need is:
a bucket of items
à collection, container
get next item
à no concept of ordering needed
à just a way to get items out of the container one by one
a specific order in which this 
happens is not required – but 
can be
Copyright © MathByte Academy

Example: Sets
Sets are unordered collections of items
Sets are not indexable
s = {'x', 'y', 'b', 'c', 'a'}
s[0]
à TypeError – 'set' object does not support indexing
But sets are iterable
for item in s:
print(item)
à
y
c
x
b
a
Note that we have no idea of the order in 
which the elements are returned in the 
iteration
Copyright © MathByte Academy

The concept of next
For general iteration, all we really need is the concept of "get the next item" in the collection
If a collection object implements a get_next_item method
and we could iterate over the collection as follows:
for _ in range(10):
item = coll.get_next_item()
print(item)
we can get elements out of the collection, one 
after the other,  this way:
get_next_item()
get_next_item()
get_next_item()
But how do we know when to stop asking for the next item?
à StopIteration
i.e. when all the elements of the collection have been returned 
by calling get_next_item()?
built-in Exception
Copyright © MathByte Academy

Attempting to build an Iterable ourselves
Let's try building our own class, which will be a collection of squares of integers
We could make this a sequence, but we want to avoid the concept of indexing
class Squares:
def __init__(self):
self.i = 0
def next_(self):
result = self.i ** 2
self.i += 1
return result
In order to implement a next method, we need to know what we've already "handed out"
so we can hand out the "next" item without repeating ourselves
Copyright © MathByte Academy

Iterating over Squares
class Squares:
def __init__(self):
self.i = 0
def next_(self):
result = self.i ** 2
self.i += 1
return result
sq = Squares()
for _ in range(5):
item = sq.next_()
print(item)
à
0
1
4
9
16
There are a few issues:
à cannot use a for loop, comprehension, etc
à the collection is essentially infinite
à we cannot restart the iteration "from the beginning"
Copyright © MathByte Academy

Refining the Squares Class
we first tackle the idea of making the collection finite
•
we specify the size of the collection when we create the instance
•
we raise a StopIteration exception if next_ has been called too many times
class Squares:
def __init__(self):
self.i = 0
def next_(self):
result = self.i ** 2
self.i += 1
return result
class Squares:
def __init__(self, length):
self.i = 0
self.length = length
def next_(self):
if self.i >= self.length:
raise StopIteration
else:
result = self.i ** 2
self.i += 1
return result
Copyright © MathByte Academy

Iterating over Squares instances
class Squares:
def __init__(self, length):
self.i = 0
self.length = length
def next_(self):
if self.i >= self.length:
raise StopIteration
else:
result = self.i ** 2
self.i += 1
return result
sq = Squares(5)
while True:
create a collection of length 5
start an infinite loop
try:
item = sq.next_()
try getting the next item
except StopIteration:
catch the StopIteration exception à nothing left to iterate
break
break out of the infinite while loop – we're done iterating
print(item)
Output:
0
1
4
9
16
Copyright © MathByte Academy

Python's next() function
Remember Python's len() function?
We could implement that function 
for our custom type by 
implementing the special method:
__len__
Python has a built-in function: next()
We can implement that function 
for our custom type by 
implementing the special method:
__next__
class Squares:
def __init__(self, length):
self.i = 0
self.length = length
def __next__(self):
if self.i >= self.length:
raise StopIteration
else:
result = self.i ** 2
self.i += 1
return result
Copyright © MathByte Academy

Iterating over Squares instances
sq = Squares(5)
while True:
try:
item = next(sq) 
print(item)
except StopIteration:
break
Output:
0
1
4
9
16
We still have some issues:
•
cannot iterate using for loops, comprehensions, etc
•
once the iteration starts we have no way of re-starting it
•
and once all the items have been iterated (using next) the 
object becomes useless for iteration à exhausted
Copyright © MathByte Academy

Copyright © MathByte Academy

Where we're at so far…
We created a custom container type object with a __next__ method
But it had several drawbacks:
à cannot use a for loop
à once we start using next there's no going back
à once we have reached StopIteration we're basically 
done with the object
Let's tackle the loop issue first
We saw how to iterate using __next__, StopIteration, and a while loop
This is actually how Python handles for loops in general
Somehow, we need to tell Python that our class has that __next__
method and that it will behave in a way consistent with using a 
while loop to iterate
Python knows we have __next__, but how does it know we implement 
StopIteration?
Copyright © MathByte Academy

The iterator Protocol
A protocol is simply a fancy way of saying that our class is going to implement certain 
functionality that Python can count on
To let Python know our class can be iterated over using __next__ we implement the iterator protocol
The iterator protocol is quite simple – the class needs to implement two methods:
à __iter__
this method should just return the object (class instance) itself
sounds weird, but we'll understand why later
à __next__
this method is responsible for handing back the next 
element from the collection and raising the 
StopIteration exception when all elements have been 
handed out
An object that implements these two methods is called an iterator
Copyright © MathByte Academy

Iterators
An iterator is therefore an object that implements:
__iter__
à just returns the object itself
__next__
à returns the next item from the container, or raises SopIteration
If an object is an iterator, we can use it with for loops, comprehensions, etc
Python will know how to loop (iterate) over such an object 
(basically using the same while loop technique we used)
Copyright © MathByte Academy

Example
Let's go back to our Squares example, and make it into an iterator
class Squares:
def __init__(self, length):
self.i = 0
self.length = length
def __next__(self):
if self.i >= self.length:
raise StopIteration
else:
result = self.i ** 2
self.i += 1
return result
def __iter__(self):
return self
sq = Squares(5)
for item in sq:
print(item)
0
1
4
9
16
à
Still one issue though!
The iterator cannot be "restarted"
Once we have looped through all the items
the iterator has been exhausted
To loop a second time through the 
collection we have to create a new 
instance and loop through that
Copyright © MathByte Academy

Copyright © MathByte Academy

Iterators
We saw than an iterator is an object that implements
__iter__
à returns the next element
__next__
à returns the object itself
The drawback is that iterators get exhausted
à become useless for iterating again
à become throw away objects
But two distinct things going on:
maintaining the collection of items (the container)
(e.g. creating, mutating (if mutable), etc)
iterating over the collection
Why should we have to re-create the collection of items just to 
iterate over them?
Copyright © MathByte Academy

Separating the Collection from the Iterator
Instead, we would prefer to separate these two
Maintaining the data of the collection should be one object
Iterating over the data should be a separate object
à iterator
That object is throw-away
à but we don't throw away the collection
The collection is iterable
but the iterator is responsible for iterating over the collection
The iterable is created once
The iterator is created every time we need to start a fresh iteration
Copyright © MathByte Academy

Example
class Cities:
def __init__(self):
self._cities = ['Paris', 'Berlin', 'Rome', 'London']
self._index = 0
def __iter__(self):
return self
def __next__(self):
if self._index >= len(self._cities):
raise StopIteration
else:
item = self._cities[self._index]
self._index += 1
return item
Cities instances are iterators
Every time we want to run a new loop, we have to create a new 
instance of Cities
This is wasteful, because we should not have to re-create the _cities 
list every time
Copyright © MathByte Academy

Example
So, let's separate the object that maintains the cities, from the iterator itself
class Cities:
def __init__(self):
self._cities = ['New York', 'New Delhi', 'Newcastle']
def __len__(self):
return len(self._cities)
class CityIterator:
def __init__(self, cities):
self._cities = cities
self._index = 0
def __iter__(self):
return self
def __next__(self):
if self._index >= len(self._cities):
raise StopIteration
else:
etc…
Copyright © MathByte Academy

Example
To use the Cities and CityIterator together here's how we would proceed:
cities = Cities()
create an instance of the container object
city_iterator = CityIterator(cities)
create a new iterator – but see how we pass in the 
existing cities instance
for city in cities_iterator:
print(city)
can now use the iterator to iterate
At this point, the cities_iterator is exhausted
If we want to re-iterate over the collection, we need to create a new one
city_iterator = CityIterator(cities)
for city in cities_iterator:
print(city)
But this time, we did not have to re-create the collection – we just 
passed in the existing one!
Copyright © MathByte Academy

So far…
At this point we have:
a container that maintains the collection items
a separate object, the iterator, used to iterate over the collection
So we can iterate over the collection as many times as we want
we just have to remember to create a new iterator every time
It would be nice if we did not have to do that manually every time
and if we could just iterate over the Cities object instead of CityIterator
This is where the formal definition of a Python iterable comes in…
Copyright © MathByte Academy

Iterables
An iterable is a Python object that implements the iterable protocol
The iterable protocol requires that the object implement a single method
__iter__
returns a new instance of the iterator object
used to iterate over the iterable
class Cities:
def __init__(self):
self._cities = ['New York', 'New Delhi', 'Newcastle']
def __len__(self):
return len(self._cities)
def __iter__(self):
return CityIterator(self)
Copyright © MathByte Academy

Iterable vs Iterator
An iterable is an object that implements
__iter__
à returns an iterator
An iterator is an object that implements
__iter__
à returns itself (an iterator)
__next__
à returns the next element
So iterators are themselves iterables
but they are iterables that become exhausted
Iterables on the other hand never become exhausted
because they always return a new iterator that is then used to iterate
(in general, a new instance)
(not a new instance)
Copyright © MathByte Academy

Iterating over an iterable
The first thing Python does when we try to iterate over an object
it calls iter() to obtain an iterator
Python has a built-in function iter()
It calls the __iter__ method
(we'll actually come back to this for sequences!)
then it starts iterating (using next, StopIteration, etc)
using the iterator returned by iter()
Copyright © MathByte Academy

Copyright © MathByte Academy

Lazy Evaluation
This is often used in class properties
properties of classes may not always be populated when the object is created
value of a property only becomes known when the property is requested - deferred
Example
class Actor:
def __init__(self, actor_id):
self.actor_id = actor_id
self.bio = lookup_actor_in_db(actor_id)
self.movies = None
@property
def movies(self):
if self.movies is None:
self.movies = lookup_movies_in_db(self.actor_id)
return self.movies
Copyright © MathByte Academy

Application to Iterables
We can apply the same concept to certain iterables
We do not calculate the next item in an iterable until it is actually requested
Example
iterable à Factorial(n)
will return factorials of consecutive integers from 0 to n-1
do not pre-compute all the factorials
wait until next requests one, then calculate it
This is a form of lazy evaluation
Copyright © MathByte Academy

Application to Iterables
Another application of this might be retrieving a list of forum posts
Posts might be an iterable
each call to next returns a list of 5 posts (or some page size)
but uses lazy loading
à every time next is called, go back to database and get next 5 posts
Copyright © MathByte Academy

Using that lazy evaluation technique means that we can actually have infinite iterables
Since items are not computed until they are requested
we can have an infinite number of items in the collection
Don't try to use a for loop over such an iterable
unless you have some type of exit condition in your loop
à otherwise infinite loop!
Lazy evaluation of iterables is something that is used a lot in Python!
We'll examine that in detail in the next section on generators
Application to Iterables à Infinite Iterables
Copyright © MathByte Academy

iter()
Copyright © MathByte Academy

What happens when Python performs an iterationon over an iterable?
The very first thing Python does is call the iter() function on the object we want to iterate
If the object implements the __iter__ method, that method is called
and Python uses the returned iterator
What happens if the object does not implement the __iter__ method?
Is an exception raised immediately?
Copyright © MathByte Academy

Sequence Types
I just said that Python always calls iter() first
So how does iterating over a sequence type – that maybe only implemented __getitem__ work?
You'll notice I did not say Python always calls the __iter__ method
I said it calls the iter() function!!
In fact, if obj is an object that only implements __getitem__
iter(obj)
à returns an iterator type object!
Copyright © MathByte Academy

Some form of magic at work?
Not really!
Let's think about sequence types and how we can iterate over them
Suppose seq is some sequence type that implements __getitem__ (but not __iter__)
index = 0
while True:
Remember what happens when we request an index that is out of bounds from the 
__getitem__ method?
à IndexError
try:
print(seq[index])
index += 1
except IndexError:
break
Copyright © MathByte Academy

Making an Iterator to iterate over any Sequence
This is basically what we just did!
class SeqIterator:
def __init__(self, seq):
self.seq = seq
self.index = 0
def __iter__(self):
return self
def __next__:
try:
item = self.seq[self.index]
self.index += 1
return item
except IndexError:
raise StopIteration()
Copyright © MathByte Academy

Calling iter()
So when iter(obj) is called:
Python first looks for an __iter__ method
à if it's there, use it
à if it's not
look for a __getitem__ method
à if it's there create an iterator object and return that
à if it's not there, raise a TypeError exception (not iterable)
Copyright © MathByte Academy

Testing if an object is iterable
Sometimes
(very rarely!)
you may want to know if an object is iterable or not
But now you would have to check if they implement
__getitem__
__iter__
and that __iter__ returns an iterator
or
Easier approach:
try:
iter(obj)
except TypeError:
# not iterable
<code>
else:
# is iterable
<code>
Copyright © MathByte Academy

Copyright © MathByte Academy

Python provides many functions that return iterables or iterators
Additionally, the iterators perform lazy evaluation
You should always be aware of whether you are dealing with an iterable or an iterator
why?
if an object is an iterable (but not an iterator) you can iterate over it many times
if an object is an iterator you can iterate over it only once
Copyright © MathByte Academy

à iterable
range(10) 
à iterator
zip(l1, l2) 
enumerate(l1) 
à iterator
open('cars.csv') 
à iterator
dictionary .keys()
dictionary .values()
dictionary .items()
à iterable
à iterable
à iterable
and many more…
Copyright © MathByte Academy

Copyright © MathByte Academy

Iterating over the return values of a callable
Consider a callable that provides a countdown from some start value:
countdown() à 5
countdown() à 4
countdown() à 3
countdown() à 2
countdown() à 1
countdown() à 0
countdown() à -1
...
We now want to run a loop that will call countdown()
until 0 is reached
We could certainly do that using a loop and testing the 
value to break out of the loop once 0 has been reached
while True:
val = countdown()
if val == 0:
break
else:
print(val)
Copyright © MathByte Academy

An iterator approach
We could take a different approach, using iterators, and we can also make it quite generic
Make an iterator that knows two things:
the callable that needs to be called
a value (the sentinel) that will result in a StopIteration if the callable returns that value
The iterator would then be implemented as follows:
when next() is called:
call the callable and get the result
if the result is equal to the sentinel à StopIteration
and "exhaust" the iterator
otherwise return the result
We can then simply iterate over the iterator until it is exhausted
Copyright © MathByte Academy

The first form of the iter() function
We just studied the first form of the iter() function:
iter(iterable) à iterator for iterable
if the iterable did not implement the iterator protocol, but implemented the sequence protocol
iter() creates a iterator for us (leveraging the sequence protocol)
Notice that the iter() function was able to generate an iterator for us automatically
Copyright © MathByte Academy

The second form of the iter() function
iter(callable, sentinel)
This will return an iterator that will:
call the callable when next() is called
and either raise StopIteration if the result is equal to the sentinel value
or return the result otherwise
Copyright © MathByte Academy

Copyright © MathByte Academy

Iterating a sequence in reverse order
If we have a sequence type, then iterating over the sequence in reverse order is quite simple:
for i in range(len(seq)):
print(seq[len(seq) – i – 1])
for item in seq[::-1]:
print(item)
This works, but is wasteful because it makes a copy of 
the sequence
This is more efficient, but the syntax is messy
for item in reversed(seq)
print(item)
This is cleaner and just as efficient, because it 
creates an iterator that will iterate backwards 
over the sequence – it does not copy the 
data like the first example
We can override how reversed works by implementing 
the __reversed__ special method
Both __getitem__ and __len__ must be implemented
for i in range(len(seq)-1, -1, -1):
print(seq[i])
Copyright © MathByte Academy

Iterating an iterable in reverse
Unfortunately, reversed() will not work with custom iterables without a little bit of extra work
When we call reversed() on a custom iterable, Python will look for and call 
the __reversed__ function
That function should return an iterator that will be used to perform the reversed iteration
So basically we have to implement a reverse iterator ourselves
Just like the iter() method, when we call reversed() on an object:
looks for and calls __reversed__ method
if it's not there, uses __getitem__ and __len__
to create an iterator for us
exception otherwise
Copyright © MathByte Academy

Card Deck Example
In the code exercises I am going to build an iterable containing a deck of 52 sorted cards
2 Spades … Ace Spades,  2 Hearts … Ace Hearts,  2 Diamonds … Ace Diamonds,  2 Clubs … Ace Clubs
But I don't want to create a list containing all the pre-created cards
So I want my iterator to figure out the suit and card name for a given index in the sorted deck
à Lazy evaluation
SUITS = ['Spades', 'Hearts', 'Diamonds', 'Clubs']
RANKS = [2, 3, …, 10, 'J', 'Q', 'K', 'A']
We assume the deck is sorted as follows:
iterate over SUITS
for each suit iterate over RANKS
card = combination of suit and rank
Copyright © MathByte Academy

Card Deck Example
2S … AS  2H … AH  2D … AD  2C … AC
Each card in this deck has a positional index: a number from 0 to len(deck) - 1
There are len(SUITS) suits
There are len(RANKS) ranks
0 - 51
4
The deck has a length of: len(SUITS) * len(RANKS)
13
52
To find the suit index of a card at index i:
i // len(RANKS)
Examples
5th card (6S) à index 4
à 4 // 13 à 0
16th card (4H) à index 15
à 15 // 13 à 1
To find the rank index of a card at index i:
i % len(RANKS)
Examples
5th card (6S) à index 4
à 4 % 13 à 4
16th card (4H) à index 15
à 15 % 13 à 2
SUITS = ['Spades', 'Hearts', 'Diamonds', 'Clubs']
RANKS = [2, 3, …, 10, 'J', 'Q', 'K', 'A']
Copyright © MathByte Academy

Copyright © MathByte Academy

generators
à a type of iterator
generator functions
à generator factories
à they return a generator when called
generator expressions
à uses comprehension syntax
à a more concise way of creating generators
à like list comprehensions, useful for simple situations
à they are not a generator themselves
performance considerations
Copyright © MathByte Academy

Copyright © MathByte Academy

Let's recall how we would write a simple iterator for factorials
Iterators review
class FactIter:
def __init__(self, n):
self.n = n
self.i = 0
def __iter__(self):
return self
def __next__(self):
if self.i >= self.n:
raise StopIteration
else:
result = math.factorial(self.i)
self.i += 1
return result
Now that's quite a bit of work for a simple iterator!
Copyright © MathByte Academy

There has to be a better way…
What if we could do something like this instead:
def factorials(n):
for i in range(n):
emit factorial(i)
pause execution here
wait for resume
return 'done!'
and in our code we would want to do 
something like this maybe:
facts = factorials(4)
get_next(facts)
à 0!
get_next(facts)
à 1!
get_next(facts)
à 2!
get_next(facts)
à 3!
get_next(facts)
à done!
Of course, getting 0!, 1!, 2!, 3! followed by a string is odd 
And what happens if we call get_next again?
Maybe we should consider raising an exception…
And instead of calling get_next, why not just use next?
But what about that emit, pause, resume?
à yield
StopIteration?
Copyright © MathByte Academy

Yield to the rescue…
The yield keyword does exactly what we want:
it emits a value
the function is effectively suspended (but it retains its current state)
calling next on the function resumes running the function right after the yield statement
def song():
print('line 1')
yield "I'm a lumberjack and I'm OK"
print('line 2')
yield 'I sleep all night and I work all day'
lines = song()
à no output!
line = next(lines)
à 'line 1' is printed in console
line à "I'm a lumberjack and I'm OK"
line = next(lines)
à 'line 2' is printed in console
line à "I sleep all night and I work all day"
if function returns something instead of yielding (finishes running) à StopIteration exception
line = next(lines)
à StopIteration
Copyright © MathByte Academy

Generators
A function that uses the yield statement, is called a generator function
def my_func():
yield 1
yield 2
yield 3
my_func is just a regular function
calling my_func() returns a generator object
We can think of functions that contain the yield statement as generator factories
The generator is created by Python when the function is called
à gen = my_func()
The resulting generator is executed by calling next()
à next(gen)
the function body will execute until it encounters a yield statement
then it suspends itself
if it encounters a return before a yield
à StopIteration exception occurs
(Remember that if a function terminates without an explicit return, Python 
essentially returns a None value for us)
à suspended function resumes execution
until next is called again
it yields the value (as return value of next())
Copyright © MathByte Academy

Generators
def my_func():
yield 1
yield 2
yield 3
gen = my_func()
à gen is a generator
next(gen)
à 1
next(gen)
à 2
next(gen)
à 3
next(gen)
à StopIteration
Copyright © MathByte Academy

Generators
next
StopIteration
This should remind you of iterators!
In fact, generators are iterators 
à they implement the iterator protocol
__iter__
__next__
def my_func():
yield 1
yield 2
yield 3
gen = my_func()
gen.__iter__()
gen.__next__()
à iter(gen)
à returns gen itself
à next(gen)
à they are exhausted when function returns a value
à StopIteration exception
à return value is the exception message
Copyright © MathByte Academy

Example
class FactIter:
def __init__(self, n):
self.n = n
self.i = 0
def __iter__(self):
return self
def __next__(self):
if self.i >= self.n:
raise StopIteration
else:
result = math.factorial(self.i)
self.i += 1
return result
def factorials(n):
for i in range(n):
yield math.factorial(i)
fact_iter = FactIter(5)
fact_iter = factorials(5)
Copyright © MathByte Academy

Generators
Generator functions are functions which contain at least one yield statement
When a generator function is called, Python creates a generator object
Generators implement the iterator protocol
Generators are inherently lazy iterators
Generators are iterators, and can be used in the same way (for loops, comprehensions, etc)
Generators become exhausted once the function returns a value
(and can be infinite)
Copyright © MathByte Academy

Copyright © MathByte Academy

Generators become exhausted
Generators are iterators
Generator functions are functions that use yield
A generator function is a generator factory
à they return a (new) generator when called
à they can become exhausted (consumed)
This can lead to bugs if you try to iterate twice over a generator
à they cannot be "restarted"
Copyright © MathByte Academy

Example
def squares(n):
for i in range(n):
yield i ** 2
sq = squares(5)
à sq is a new generator (iterator)
l = list(sq)
l à [0, 1, 4, 9, 16]
and sq has been exhausted
l = list(sq)
l à []
Copyright © MathByte Academy

Example
This of course can lead to unexpected behavior sometimes…
def squares(n):
for i in range(n):
yield i ** 2
sq = squares(5)
enum1 = enumerate(sq)
next(sq)
à 0
next(sq)
à 1
list(enum1)
enumerate is lazy à hasn't iterated through sq yet
à [(0,4), (1, 9), (2, 16)]
notice how enumerate started at i=2
and the index value returned by enumerate is 0, not 2
Copyright © MathByte Academy

Making an Iterable
This behavior is no different than with any other iterator
As we saw before, the solution is to create an iterable that returns a new iterator every time
class Squares:
def __init__(self, n):
self.n = n
def __iter__(self):
return squares(n)
def squares(n):
for i in range(n):
yield i ** 2
sq = Squares(n)
l1 = list(sq)
l2 = list(sq)
l1 à [0, 1, 4, 9, 16]
l2 à [0, 1, 4, 9, 16]
new instance of 
the generator
Copyright © MathByte Academy

Copyright © MathByte Academy

Comprehension Syntax
We already covered comprehension syntax when we studied list comprehensions
l = [i ** 2 for i in range(5)]
As well as more complicated syntax:
•
if statements
•
multiple nested loops
•
nested comprehensions
[(i, j) 
for i in range(1, 6) if i%2==0 
for j in range(1, 6) if j%3==0]
[[i * j for j in range(5)] for i in range(5)]
Copyright © MathByte Academy

Generator Expressions
Generator expressions use the same comprehension syntax
but instead of using []
we use ()
[i ** 2 for i in range(5)]
(i ** 2 for i in range(5))
a list is returned
a generator is returned
evaluation is eager
evaluation is lazy
has local scope
has local scope
can access nonlocal 
and global scopes
can access nonlocal 
and global scopes
à including nesting, if
iterable
iterator
Copyright © MathByte Academy

Resource Utilization
List comprehensions are eager
Generators are lazy
all objects are created right away
object creation is delayed until requested by next()
à takes longer to create/return the list
à generator is created/returned  immediately
à iteration is faster (objects already created)
à iteration is slower (objects need to be created)
if you iterate through all the elements à time performance is about the same
if you do not iterate through all the elements à generator more efficient
à entire collections is loaded into memory
à only a single item is loaded at a time
in general, generators tend to have less memory overhead
Copyright © MathByte Academy

Copyright © MathByte Academy

Delegating to another iterator
Often we may need to delegate yielding elements to another iterator
file1.csv
file2.csv
file3.csv
def read_all_data():
for file in ('file1.csv', 'file2.csv', 'file3.csv'):
with open(file) as f:
for line in f:
yield line
The inner loop is basically just using the file iterator and yielding values directly
Essentially we are delegating yielding to the file iterator
Copyright © MathByte Academy

We can replace this inner loop by using a simpler syntax:
yield from
Simpler Syntax
def read_all_data():
for file in ('file1.csv', 'file2.csv', 
'file3.csv'):
with open(file) as f:
for line in f:
yield line
def read_all_data():
for file in ('file1.csv', 'file2.csv', 
'file3.csv'):
with open(file) as f:
yield from f
We'll come back to yield from, as there is a lot more to it!
Copyright © MathByte Academy

Copyright © MathByte Academy

Background Info
Along with this project is a data file:
nyc_parking_tickets_extract.csv
Here are the first few lines of data
Note that an end-of-line character is not visible, but it's there!
Summons Number,Plate ID,Registration State,Plate Type,Issue Date,Violation Code,Vehicle Body Type,Vehicle Make,Violation Description
4006478550,VAD7274,VA,PAS,10/5/2016,5,4D,BMW,BUS LANE VIOLATION
4006462396,22834JK,NY,COM,9/30/2016,5,VAN,CHEVR,BUS LANE VIOLATION
4007117810,21791MG,NY,COM,4/10/2017,5,VAN,DODGE,BUS LANE VIOLATION
àfields separated by commas
à first row contains the field names
à data rows are a mix of data types: string, date, int
Copyright © MathByte Academy

Goal 1
Your first goal is to create a lazy iterator that will produce a named tuple for each row of data
The contents of each tuple should be an appropriate data type (e.g. date, int, string)
You can use the split method for string to split on the comma
You will need to use the strip method to remove the end-of-line character (\n)
Remember, the goal is to produce a lazy iterator
à you should not be reading the entire file in memory and then processing it
à the goal is to keep the required memory overhead to a minimum
Please stick to Python's built-ins and the standard library only!
Copyright © MathByte Academy

Goal 2
Calculate the number of violations by car make.
Use the lazy iterator you created in Goal 1
Use lazy evaluation whenever possible
You can choose otherwise, but I would store the make and violation counts as a dictionary
à key = car make
à value = # violations
Copyright © MathByte Academy

Copyright © MathByte Academy

Python has many tools for working with iterables
You should already know almost all of these:
zip
map
slicing
filter
sorted
all
any
enumerate
min
max
sum
iter
len
next
reversed
reduce
built-in
functools module
Copyright © MathByte Academy

The itertools module
Slicing
dropwhile
Selecting and Filtering
islice
takewhile
filterfalse
compress
Chaining and Teeing
chain
tee
Mapping and Reducing
starmap
accumulate
Infinite Iterators
count
cycle
repeat
Zipping
zip_longest
Combinatorics
product
combinations
permutations
combinations_with_replacement
Copyright © MathByte Academy

Copyright © MathByte Academy

Aggregators
Functions that iterate through an iterable and return a single value that (usually) takes into 
account every element of the iterable
min(iterable)
à minimum value in the iterable
max(iterable)
à maximum value in the iterable
sum(iterable)
à sum of all the values in the iterable
Copyright © MathByte Academy

Associated Truth Values
You should already know this, but let's review briefly:
Every object in Python has an associated truth value
bool(obj) à True / False
Every object has a True truth value, except:
•
None
•
False
•
0 in any numeric type (e.g. 0, 0.0, 0+0j, …)
•
empty sequences (e.g. list, tuple, string, …)
•
empty mapping types (e.g. dictionary, set, …)
•
custom classes that implement a __bool__ or __len__
method that returns False or 0
which have a False truth value
Copyright © MathByte Academy

The any and all functions
any(iterable)
à returns True if any (one or more) element in iterable is truthy
all(iterable)
à returns True if all the elements in iterable are truthy
à False otherwise
à False otherwise
Copyright © MathByte Academy

Leveraging the any and all functions
Often, we are not particularly interested in the direct truth value of the elements  in our iterables
à want to know if any, or all, satisfy some condition
à if the condition is True
A function that takes a single argument and returns True or False is called a predicate
We can make any and all more useful by first applying a predicate to each element of the iterable
Copyright © MathByte Academy

First define a suitable predicate:
Example
Suppose we have some iterable
results = [pred(1), pred(2), pred(3), pred(4), pred(100)]
and we want to know if:
every element is less than 10
pred = lambda x: x < 10
Apply this predicate to every element of the iterable:
à [True,    True,     True,    True,    False]
l = [1, 2, 3, 4, 100]
Then we use all on these results
all(results)
à False
Copyright © MathByte Academy

How do we apply that predicate?
The map function
map(fn, iterable)
à applies fn to every element of iterable
A comprehension:
(fn(item) for item in iterable)
Or even:
new_list = []
for item in iterable:
new_list.append(fn(item))
Copyright © MathByte Academy

Copyright © MathByte Academy

itertools.islice
We know that we can slice sequence types
seq[i:j:k]
seq[slice(i, j, k)]
We can also slice general iterables (including iterators of course)
à islice(iterable, start, stop, step)
à islice returns a lazy iterator
l = [1, 2, 3, 4]
result = islice(l, 0, 3)
from itertools import islice
list(result)
à [1, 2, 3]
list(result)
à []
even though l was a list!
Copyright © MathByte Academy

Copyright © MathByte Academy

The filter function
You should already be familiar with the filter function
à filter(predicate, iterable)
à returns all elements of iterable where predicate(element) is True
predicate can be None – in which case it is the identity function
f(x) à x
à in other words, truthy elements only will be retained
We can achieve the same result using generator expressions:
à filter returns a lazy iterator
(item for item in iterable if pred(item))
(item for item in iterable if item)
(item for item in iterable if bool(item))
or
predicate is not None
predicate is None
Copyright © MathByte Academy

Example
filter(lambda x: x < 4, [1, 10, 2, 10, 3, 10])
à 1, 2, 3
filter(None, [0, '', 'hello', 100, False])
à 'hello', 100
à remember that filter returns a (lazy) iterator
Copyright © MathByte Academy

itertools.filterfalse
This works the same way as the filter function
but instead of retaining elements where the predicate evaluates to True
it retains elements where the predicate evaluates to False
Example
filterfalse(lambda x: x < 4, [1, 10, 2, 10, 3, 10])
à 10, 10, 10
filterfalse(None, [0, '', 'hello', 100, False])
à 0, '', False
à filterfalse returns a (lazy) iterator
Copyright © MathByte Academy

itertools.compress
No, this is not a compressor in the sense of say a zip archive!
It is basically a way of filtering one iterable, using the truthiness of items in another iterable
data = ['a',   'b', 'c', 'd', 'e']
selectors = [True, False, 1,   0]
compress(data, selectors)
à a, c
à compress returns a (lazy) iterator
None
Copyright © MathByte Academy

itertools.takewhile
The takewhile function returns an iterator that will yield items while pred(item) is Truthy 
takewhile(pred, iterable)
à at that point the iterator is exhausted
even if there are more items in the iterable whose predicate would be truthy
takewhile(lambda x: x < 5, [1, 3, 5, 2, 1])
à 1, 3
à takewhile returns a (lazy) iterator
Copyright © MathByte Academy

itertools.dropwhile
The dropwhile function returns an iterator that will start iterating (and yield all remaining elements)
once pred(item) becomes Falsy
dropwhile(pred, iterable)
dropwhile(lambda x: x < 5, [1, 3, 5, 2, 1])
à 5, 2, 1
à dropwhile returns a (lazy) iterator
Copyright © MathByte Academy

Copyright © MathByte Academy

itertools.count
The count function is an infinite iterator
similar to range
à start, step
à no stop
different from range
à infinite
à start and step can be any numeric type
Example
count(10, 2)
à 10, 12, 14, …
count(10.5, 0.1)
à 10.5, 10.6, 10.7, …
complex
Decimal
float
bool
False à 0
True à 1
à lazy iterator
takewhile(lambda x: x < 10.8, count(10.5, 0.1))
à 10.5, 10.6, 10.7
Copyright © MathByte Academy

itertools.cycle
The cycle function allows us to loop over a finite iterable indefinitely
Example
cycle(['a', 'b', 'c'])
à 'a', 'b', 'c', 'a', 'b', 'c', …
Important
If the argument of cycle is itself an iterator
à iterators becomes exhausted
cycle will still produce an infinite sequence
à does not stop after the iterator becomes exhausted
à lazy iterator
Copyright © MathByte Academy

itertools.repeat
à lazy iterator
The repeat function simply yields the same value indefinitely
repeat('spam')
à 'spam', 'spam', 'spam', 'spam', …
Optionally, you can specify a count to make the iterator finite
repeat('spam', 3)
à 'spam', 'spam', 'spam'
Caveat
The items yielded by repeat are the same object
à they each reference the same object in memory
Copyright © MathByte Academy

Copyright © MathByte Academy

Chaining Iterables
This is analogous to sequence concatenation
but not the same! 
à dealing with iterables (including iterators)
à chaining is itself a lazy iterator
We can manually chain iterables this way:
iter1
itertools.chain(*args)
iter2
iter3
for it in (iter1, iter2, iter3):
yield from it
Or, we an use chain as follows:
for item in chain(iter1, iter2, iter3):
print(item)
Variable number of positional arguments – each argument must be an iterable
à lazy iterator
Copyright © MathByte Academy

Chaining Iterables
What happens if we want to chain from iterables contained inside another, single, iterable?
l = [iter1, iter2, iter3]
chain(l)
à l
What we really want is to chain iter1, iter2 and iter3
We can try this using unpacking:
chain(*l)
à produces chained elements from iter1, iter2 and iter3
BUT
unpacking is eager – not lazy!
If l was a lazy iterator, we essentially iterated through l (not the sub 
iterators), just to unpack!
This could be a problem if we really wanted the entire chaining 
process to be lazy
Copyright © MathByte Academy

Chaining Iterables
itertools.chain.from_iterable(it)
à lazy iterator
We could try this approach:
def chain_lazy(it):
for sub_it in it:
yield from sub_it
Or we can use chain.from_iterable
chain.from_iterable(it)
This achieves the same result
à iterates lazily over it
à in turn, iterates lazily over each  iterable in it
Copyright © MathByte Academy

"Copying" Iterators
Sometimes we need to iterate through the same iterator multiple times, or even in parallel
We could create the iterator multiple times manually
iters = []
for _ in range(10):
iters.append(create_iterator())
Or we can use tee in itertools
itertools.tee(iterable, n)
à returns independent iterators in a tuple
tee(iterable, 10)
à (iter1, iter2, …, iter10)
all different objects
Copyright © MathByte Academy

Teeing Iterables
One important to thing to note
The elements of the returned tuple are lazy iterators
à always!
à even if the original argument was not
l = [1, 2, 3, 4]
tee(l, 3)
à (iter1, iter2, iter3)
all lazy iterators
not lists!
Copyright © MathByte Academy

Copyright © MathByte Academy

Mapping and Accumulation
Mapping
à applying a callable to each element of an iterable
à map(fn, iterable)
Accumulation
à reducing an iterable down to a single value
à sum(iterable)
à min(iterable)
à max(iterable)
calculates the sum of every element in an iterable
returns the minimal element of the iterable
returns the maximal element of the iterable
à reduce(fn, iterable, [initializer])
à fn is a function of two arguments
à applies fn cumulatively to elements of iterable
Copyright © MathByte Academy

You should already be familiar with map
map(fn, iterable) applies fn to every element of iterable, and returns an iterator (lazy)
map(lambda x: x**2, [1, 2, 3, 4])
à 1, 4, 9, 16
à lazy iterator
à fn must be a callable that requires a single argument
à quick review
map
Of course, we can easily do the same thing using a generator expression too
maps = (fn(item) for item in iterable)
Copyright © MathByte Academy

reduce
You should already be familiar with reduce
à quick review
Suppose we want to find the sum of all elements in an iterable:
l = [1, 2, 3, 4]
sum(l)
à 1 + 2 + 3 + 4 = 10
reduce(lambda x, y: x + y, l)
à 1
à 1 + 2 = 3
à 3 + 3 = 6
à 6 + 4 = 10
To find the product of all elements:
reduce(lambda x, y: x * y, l)
à 1
à 1 * 2 = 2
à 2 * 3 = 6
à 6 * 4 = 24
We can specify a different "start" value in the reduction
reduce(lambda x, y: x + y, l, 100)
à 110
Copyright © MathByte Academy

itertools.starmap
starmap is very similar to map
à it unpacks every sub element of the iterable argument, and passes that to the map function
à useful for mapping a multi-argument function on an iterable of iterables
l = [ [1, 2], [3, 4] ]
map(lambda item: item[0] * item[1], l)
We can use starmap:
starmap(operator.mul, l)
à 2, 12
à 2, 12
we could also just use a generator expression to do the same thing:
(operator.mul(*item) for item in l)
We can of course use iterables that contain more than just two values:
l = [ [1, 2, 3], [10, 20, 30], [100, 200, 300] ]
starmap(lambda: x, y, z: x + y + z, l)
à 6, 60, 600
Copyright © MathByte Academy

itertools.accumulate(iterable, fn)
The accumulate function is very similar to the reduce function
But it returns a (lazy) iterator producing all the intermediate results
à lazy iterator
à reduce only returns the final result
Unlike reduce, It does not accept an initializer
Note the argument order is not the same!
reduce(fn, iterable)
accumulate(iterable, fn)
à in accumulate, fn is optional
à defaults to addition
Copyright © MathByte Academy

Example
l = [1, 2, 3, 4]
functools.reduce(operator.mul, l)
Ø 1
Ø 1 * 2 = 2
Ø 2 * 3 = 6
Ø 6 * 4 = 24
à 24
itertools.accumulate(l, operator.mul)
à 1, 2, 6, 24
Copyright © MathByte Academy

Copyright © MathByte Academy

The zip Function
We have already seen the zip function
It takes a variable number of positional arguments – each of which are iterables
It returns an iterator that produces tuples containing the elements of the iterables, iterated one 
at a time
à lazy iterator
It stops immediately once one of the iterables has been completely iterated over
à zips based on the shortest iterable
zip([1, 2, 3], [10, 20], ['a', 'b', 'c', 'd'])
à (1, 10, 'a'), (2, 20, 'b')
Copyright © MathByte Academy

itertools.zip_longest(*args, [fillvalue=None])
Sometimes we want to zip, but based on the longest iterable
à need to provide a default value for the "holes"
à fillvalue
zip([1, 2, 3], [10, 20], ['a', 'b', 'c', 'd'])
à (1, 10, 'a'), (2, 20, 'b')
zip_longest([1, 2, 3], [10, 20], ['a', 'b', 'c', 'd'])
à (1, 10, 'a'), (2, 20, 'b'), (3,None, 'c'), (None, None, 'd') 
zip_longest([1, 2, 3], [10, 20], ['a', 'b', 'c', 'd'], -1)
à (1, 10, 'a'), (2, 20, 'b'), (3,-1, 'c'), (-1, -1, 'd') 
Copyright © MathByte Academy

Copyright © MathByte Academy

Sometimes we want to loop over an iterable of elements
Grouping
but we want to group those elements as we iterate through them
(1, 10, 100)
(1, 11, 101)
(1, 12, 102)
(2, 20, 200)
(2, 21, 201)
(3, 30, 300)
(3, 31, 301)
(3, 32, 302)
group 1
group 2
group 3
Suppose we have an iterable containing tuples, and we want to group based 
on the first element of each tuple
for key, group in groups:
print(key)
for item in group:
print(item)
We would like to iterate 
using this kind of 
approach:
key à 1
(1, 10, 100)
(1, 11, 101)
(1, 12, 102)
key à 2
(2, 20, 200)
(2, 21, 201)
key à 3
(3, 30, 300)
(3, 31, 301)
(3, 32, 302)
Copyright © MathByte Academy

itertools.groupby(data, [keyfunc])
à lazy iterator
The groupby function allows us to do precisely that
(1, 10, 100)
(1, 11, 101)
(1, 12, 102)
(2, 20, 200)
(2, 21, 201)
(3, 30, 300)
(3, 31, 301)
(3, 32, 302)
à normally specify keyfunc which calculates the key we want to use for grouping
Here we want to group based on the 1st element of each tuple
à grouping key
lambda x: x[0]
groupby(iterable, lambda x: x[0])
iterable
à iterator 
à of tuples (key, sub_iterator)
1, sub_iterator à (1, 10, 100), (1, 11, 101), (1, 12, 102)
2, sub_iterator à (2, 20, 200), (2, 21, 201)
3, sub_iterator à (3, 30, 300), (3, 31, 301), (3, 32, 302)
note how the sequence is sorted by the grouping key!
Copyright © MathByte Academy

Important Note
The sequence of elements produced from the "sub-iterators" are all produced
from the same underlying iterator
(1, 10, 100)
(1, 11, 101)
(1, 12, 102)
(2, 20, 200)
(2, 21, 201)
(3, 30, 300)
(3, 31, 301)
(3, 32, 302)
iterable
groups = groupby(iterable, lambda x: x[0])
1, sub_iterator à (1, 10, 100), (1, 11, 101), (1, 12, 102)
2, sub_iterator
3, sub_iterator
next(iterable)
next(iterable)
next(iterable)
next(iterable)
next(iterable)
next(iterable)
next(iterable)
next(iterable)
next(groups) actually iterates through all the elements of the current "sub-iterator"
before proceeding to the next group
(2, 20, 200)
(2, 21, 201)
à (3, 30, 300), (3, 31, 301), (3, 32, 302)
next(groups)
next(groups)
next(groups)
Copyright © MathByte Academy

Copyright © MathByte Academy

The itertool module contains a few functions for generating
permutations
combinations
It also has a function to generate the Cartesian product of multiple iterables
All these functions return lazy iterators
Copyright © MathByte Academy

Cartesian Product
{1, 2, 3} x {a, b, c}
1
2
3
a
b
c
(1, a)
(2, a)
(3, a)
(1, b)
(2, b)
(3, b)
(1, c)
(2, c)
(3, c)
2-dimensional:
! ×# =
(&, () & ∈!, ( ∈#
n-dimensional:
!& × ⋯×!' =
(&&, &(, … , &') && ∈!&, … , &' ∈!'
Copyright © MathByte Academy

Cartesian Product
Let's say we wanted to generate the Cartesian product of two lists:
l1 = [1, 2, 3]
l2 = ['a', 'b', 'c', 'd']
à notice not same length
def cartesian_product(l1, l2):
for x in l1:
for y in l2:
yield (x, y)
cartesian_product(l1, l2)
à (1, 'a'), (1, 'b'), (1, 'c'), (1, 'd'), …, (3,'d')
Copyright © MathByte Academy

itertools.product(*args)
à lazy iterator
l1 = [1, 2, 3]
l2 = ['a', 'b', 'c', 'd']
product(l1, l2)
l3 = [100, 200]
product(l1, l2, l3)
(1, 'a', 100), (1, 'a', 200), 
(1, 'b', 100), (1, 'b', 200), 
(1, 'c', 100), (1, 'c', 200),
…
(3, 'd', 100), (3, 'd', 200)
à (1, 'a'), (1, 'b'), (1, 'c'), (1, 'd'), …, (3,'d')
à
Copyright © MathByte Academy

Permutations
This function will produce all the possible permutations of a given iterable
In addition, we can specify the length of each permutation
itertools.permutations(iterable, r=None)
à r is the size of the permutation
à r = None means length of each permutation is the length of the iterable
à maxes out at the length of the iterable
Elements of the iterable are considered unique based on their position, not their value
à if iterable produces repeat values
then permutations will have repeat values too
Copyright © MathByte Academy

Combinations
Unlike permutations, the order of elements in a combination is not considered
à OK to always sort the elements of a combination
Combinations of length r, can be picked from a set
•
without replacement
à once an element has been picked from the set it 
cannot be picked again
•
with replacement
à once an element has been picked from the set it can
be picked again
Copyright © MathByte Academy

itertools.combinations(iterable, r)
itertools.combinations_with_replacement(iterable, r)
Just like for permutations:
the elements of an iterable are unique based on their position, not their value
The different combinations produced by these functions are sorted
based on the original ordering in the iterable
Copyright © MathByte Academy

Copyright © MathByte Academy

Data Files
You are given four data files
personal_info.csv
vehicles.csv
employment.csv
update_status.csv
Each file contains a common key that uniquely identifies each row – SSN
You are guaranteed that every SSN number
à appears only once in every file
à is present in all 4 files
To make the approach easier, I am going to break it down into 
multiple smaller goals
à the order of SSN in each file is the same
Copyright © MathByte Academy

Goal 1
Create (lazy) iterators for each of the four files
à returns named tuples
à data types are appropriate (string, date, int, etc)
à the 4 iterators are independent of each other (for now)
You will want to make use of the standard library module csv for this
Copyright © MathByte Academy

Reading CSV Files
CSV files are files that contain multiple lines of data à strings
The individual data fields in a row are:
delimited by some separating character
à comma, tab are common
in addition, individual fields may be wrapped in further delimiters
à quotes are common
à this allows the field value to contain what may be otherwise interpreted as a delimiter
Example
1,hello,world
à 3 values: 
1
hello
world
1,"hello,world"
à 2 values: 
1
hello, world
Copyright © MathByte Academy

Reading CSV Files
1,hello,world
1,"hello, world"
Simply splitting on the comma is not going to work in the second example!
à
1
"hello
world"
csv.reader is exactly what we need
à lazy iterator
à we can tell it what the delimiter is
à we can tell it what the quote character is
Example
Mueller-Rath,Human Resources,05-8069298,123-88-3381
"Schumm, Schumm and Reichert",Engineering,73-3839744,125-07-9434
def read_file(file_name):
with open(file_name) as f:
reader = csv.reader(f, delimiter=',', quotechar='"')
yield from reader
à yields lists of strings containing each field value
Copyright © MathByte Academy

Goal 2
Create a single iterable that combines all the data from all four files
à try to re-use the iterators you created in Goal 1
Once again, make sure returned data is a single named tuple containing all fields
When you "combine" the data, make sure the SSN's match!
Don't repeat the SSN 4 times in the named tuple – once is enough!
à by combining I mean one row per SSN containing data from all four files in a single named tuple
Remember that all the files are already sorted by SSN, and that each SSN appears once, and 
only once, in every file
à viewing files side by side, all the row SSN's will align correctly
Copyright © MathByte Academy

Goal 3
Some records are considered stale (not updated recently enough)
A record is considered stale if the last update date < 3/1/2017
The update date is located in the update_status.csv file
Modify your iterator from Goal 2 to filter out stale records
Make sure your iterator remains lazy!
Copyright © MathByte Academy

Goal 4
For non-stale records, generate lists of number of car makes by gender
If you do this correctly, the largest groups for each gender are:
Female à Ford and Chevrolet (both have 42 persons in those groups)
Male à Ford (40 persons in the group)
Copyright © MathByte Academy

Good luck!
Copyright © MathByte Academy

Copyright © MathByte Academy

What is a context?
Oxford dictionary:
The circumstances that form the setting for an event, statement, or 
idea, and in terms of which it can be fully understood.
In Python:
the state surrounding a section of code
# module.py
f = open('test.txt', 'r')
print(f.readlines())
f.close()
global scope
f à a file object
when print(f.readlines()) runs, it has a context in which it runs
à global scope
Copyright © MathByte Academy

Managing the context of a block of code
Consider the open file example:
# module.py
f = open('test.txt', 'r')
perform_work(f)
f.close()
There could be an exception before we close the file
à file remains open!
Need to better "manage" the context that perform_work(f) needs
f = open('test.txt', 'r')
try:
perform_work(f)
finally:
f.close()
this works
à writing try/finally every time can get cumbersome
à too easy to forget to close the file
Copyright © MathByte Academy

Context Managers
à create a context (a minimal amount of state needed for a block of code)
à execute some code that uses variables from the context
à automatically clean up the context when we are done with it
à enter context
à open file
à work within context
à read the file
à exit context
à close the file
Copyright © MathByte Academy

Example
with open('test.txt', 'r') as f:
print(f.readlines())
create the context
work inside the context
exit the context
à open file
à close file
Context managers manage data in our scope
à on entry
à on exit
Very useful for anything that needs to provide
Enter / Exit
Start / Stop
Set / Reset
à open / close file
à start db transaction / commit or abort transaction
à set decimal precision to 3 / reset back to original precision
Copyright © MathByte Academy

Copyright © MathByte Academy

try…finally…
The finally section of a try always executes
try:
…
except:
…
finally:
…
always executes
even if an exception occurs in except block
Very useful for writing code that should execute no matter what happens
Works even if inside a function and a return is in the try or except blocks
But this can get cumbersome!
There has to be a better way!
Copyright © MathByte Academy

Pattern
create some object
do some work with that object
clean up the object after we're done using it
We want to make this easy
à automatic cleanup after we are done using the object
Copyright © MathByte Academy

Context Managers
with
context
as obj_name:
# with block (can use obj_name)
# after the with block, context is cleaned up automatically
Example
with open(file_name) as f:
# file is now open
# file is now closed
enter the context
exit the context
PEP 343
(optional) an object is returned
object returned from context (optional)
Copyright © MathByte Academy

The context management protocol
Classes implement the context management protocol by implementing two methods:
__enter__
__exit__
with CtxManager() as obj:
# do something
# done with context
over simplified
mgr = CtxManager()
obj = mgr.__enter__()
try:
# do something
finally:
# done with context
mgr.__exit__()
setup, and optionally return some object
tear down / cleanup
exception handling
Copyright © MathByte Academy

Use Cases
Context managers can be used for much more than creating and releasing resources
Very common usage is for opening a file (creating resource) and closing the file (releasing resource)
Common Patterns
•
Open – Close
•
Lock – Release
•
Change – Reset
•
Start – Stop
•
Enter – Exit
Examples
•
file context managers
•
Decimal contexts
Copyright © MathByte Academy

How Context Protocol Works
works in conjunction with a with statement
class MyClass:
def __init__(self):
# init class
def __enter__(self):
return obj
def __exit__(self, + …):
# clean up obj
my_obj = MyClass()
works as a regular class
__enter__, __exit__ were not called
with MyClass() as obj:
à creates an instance of MyClass
à calls my_instance.__enter__()
à return value from __enter__ is assigned to obj
(not the instance of MyClass that was created)
after the with block, or if an exception occurs inside the with block: 
à my_instance.__exit__ is called
à no associated symbol, but an instance exists
à my_instance
Copyright © MathByte Academy

Scope of with block
The with block is not like a function or a comprehension
The scope of anything in the with block (including the object returned from __enter__)
is in the same scope as the with statement itself
# module.py
with open(fname) as f:
row = next(f)
f is a symbol in global scope
row is also in the global scope
f is closed, but the symbol exists
row is available and has a value
print(f)
print(row)
Copyright © MathByte Academy

The __enter__ Method
def __enter__(self):
This method should perform whatever setup it needs to
It can optionally return an object
à as returned_obj
That's all there is to this method
Copyright © MathByte Academy

The __exit__ Method
More complicated…
Remember the finally in a try statement?
à always runs even if an exception occurs
__exit__ is similar
à runs even if an exception occurs in with block
But should it handle things differently if an exception occurred?
à maybe
à so it needs to know about any exceptions that occurred
à it also needs to tell Python whether to silence the exception, or let it propagate
Copyright © MathByte Academy

The __exit__ Method
with MyContext() as obj:
raise ValueError
print ('done')
Scenario 1
__exit__ receives error, performs some clean up and silences error
print statement runs
Scenario 2
__exit__ receives error, performs some clean up and let's error propagate
print statement does not run
the ValueException is seen
no exception is seen
Copyright © MathByte Academy

The __exit__ Method
Needs three arguments:
à the exception type that occurred (if any, None otherwise)
à the traceback object if an exception occurred (if any, None otherwise)
Returns True or False:
à True = silence any raised exception
à False = do not silence a raised exception
def __exit__(self, exc_type, exc_value, exc_trace):
# do clean up work here
return True # or False
à the exception value that occurred (if any, None otherwise)
---------------------------------------------------------------------------
ValueError
Traceback (most recent call last)
<ipython-input-14-39a69b57f322> in <module>()
1 with MyContext() as obj:
----> 2     raise ValueError
Copyright © MathByte Academy

Copyright © MathByte Academy

Pattern: Open - Close
Open File
operate on open file
Close File
Open socket
operate on socket
Close socket
Copyright © MathByte Academy

Pattern: Start - Stop
Start database transaction
perform database operations
Commit or rollback transaction
Start timer
perform operations
Stop timer
Copyright © MathByte Academy

Pattern: Lock - Release
acquire thread lock
perform some operations
release thread lock
Copyright © MathByte Academy

Pattern: Change - Reset
change Decimal context precision
perform some operations using the new precision
reset Decimal context precision back to original value
redirect stdout to a file
perform some operations that write to stdout
reset stdout to original value
Copyright © MathByte Academy

Pattern: Wacky Stuff!
with tag('p'):
print('some text', end='')
<p>some text</p>
with tag('p'):
print('some', end='')
with tag('b'):
print('bold ', end='')
print('text', end='')
<p>some <b>bold<b> text</p>
Copyright © MathByte Academy

Pattern: Wacky Stuff!
with ListMaker(title='Items', prefix='- ', 
indent=3, stdout='myfile.txt') as lm:
lm.print('Item 1')
with lm :
lm.print('item 1a')
lm.print('item 1b')
lm.print(Item 2')
with lm :
lm.print('item 2a')
lm.print('item 2b')
Items
- Item 1     
- item 1a
- item 1b
- Item 2
- item 2a
- item 2b
>> myfile.txt
Copyright © MathByte Academy

Copyright © MathByte Academy

Context Manager Pattern
create context manager
enter context
(and, optionally, receive an object)
do some work
exit context
with open(file_name) as f:
data = file.readlines()
Copyright © MathByte Academy

Mimic Pattern using a Generator
def open_file(fname, mode):
f = open(fname, mode)
try:
yield f
finally:
f.close()
ctx = open_file('file.txt', 'r')
f = next(ctx)
opens file, and yields it
next(ctx)
closes file
à StopIteration exception
ctx = open_file('file.txt', 'r')
f = next(ctx)
try:
# do work with file
finally:
try:
next(ctx)
except StopIteration:
pass
Copyright © MathByte Academy

This works in general
def gen(args):
# do set up work here
try:
yield object
finally:
# clean up object here
ctx = gen(…)
obj = next(ctx)
try:
# do work with obj
finally:
try:
next(ctx)
except StopIteration:
pass
This is quite clunky still
but you should see that we can almost
create a context manager pattern using
a generator function!
Copyright © MathByte Academy

Creating a Context Manager from a Generator Function
def open_file(fname, mode):
f = open(fname, mode)
try:
yield f
finally:
f.close()
generator function
generator object à gen = open_file('test.txt', 'w')
f = next(gen)
# do work with f
next(f) à closes f
class GenContext:
def __init__(self, gen):
self.gen = gen
def __enter__(self):
obj = next(self.gen)
return obj
def __exit__(self, exc_type, exc_value, exc_tb):
next(self.gen)
return False
gen = open_file('test.txt', 'w')
with GenContext(gen) as f:
# do work
Copyright © MathByte Academy

Copyright © MathByte Academy

So far…
we saw how to create a context manager using a class and a generator function
def gen_function(args):
…
try:
yield obj
finally:
…
single yield
the return value of __enter__
cleanup phase
__exit__
class GenContextManager:
def __init__(gen_func):
self.gen = gen_func()
def __enter__(self):
return next(self.gen)
def __exit__(self, …):
next(self.gen)
returns what was yielded
runs the finally block
Copyright © MathByte Academy

Usage
with GenContextManager(gen_func):
…
We can tweak this a bit to also allow passing in 
arguments to gen_func
class GenContextManager:
def __init__(gen_obj):
self.gen = gen_obj
def __enter__(self):
return next(self.gen)
def __exit__(self, …):
next(self.gen)
And usage now becomes:
gen = gen_func(args)
with GenContextManager(gen):
…
This works, but we have to create the generator object first, 
and use the GenContextManager class
à lose clarity of what the context manager is
Copyright © MathByte Academy

Using a decorator to encapsulate these steps
gen = gen_func(args)
with GenContextManager(gen):
…
class GenContextManager:
def __init__(gen_obj):
self.gen = gen_obj
def __enter__(self):
return next(self.gen)
def __exit__(self, …):
next(self.gen)
def contextmanager_dec(gen_fn):
def helper(*args, **kwargs):
gen = gen_fn(*args, **kwargs)
return GenContextManager(gen)
return helper
Copyright © MathByte Academy

Usage Example
def contextmanager_dec(gen_fn):
def helper(*args, **kwargs):
gen = gen_fn(*args, **kwargs)
return GenContextManager(gen)
return helper
@contextmanager_dec
def open_file(f_name):
f = open(f_name)
try:
yield f
finally:
f.close()
à open_file = contextmanager_dec(open_file)
à open_file is now actually the helper closure
calling open_file(f_name)
à calls helper(f_name)
[free variable gen_fn = open_file ]
à returns GenContextManager instance
à creates the generator object
à with open_file(f_name)
Copyright © MathByte Academy

The contextlib Module
One of the goals when context managers were introduced to Python
was to ensure generator functions could be used to easily create them
Technique is basically what we came up with
PEP 343
à more complex
à exception handling
à if an exception occurs in with block, needs to be propagated 
back to generator function
à enhanced generators as coroutines
This is implemented for us in the standard library:
contextlib.contextmanager
à decorator which turns a generator function into a context manager
à later
__exit__(self, exc_type, exc_value, exc_tb)
Copyright © MathByte Academy

Copyright © MathByte Academy

In this project you are provided two CSV files
Project Setup
cars.csv
personal_info.csv
The basic goal will be to create a context manager that only requires the file name
and provides us an iterator we can use to iterate over the data in those files
The iterator should yield named tuples with field names based on the header row in the CSV file
For simplicity, we assume all fields are just strings
à first row contains the field names
Copyright © MathByte Academy

Goal 1
For this goal implement the context manager using a context manager class
i.e. a class that implements the context manager protocol
__enter__
__exit__
Make sure your iterator uses lazy evaluation
If you can, try to create a single class that implements both
the context manager protocol and the iterator protocol
Copyright © MathByte Academy

Goal 2
For this goal, re-implement what you did in Goal 1, but using a generator function instead
You'll have to use the @contextmanager from the contextlib module
Copyright © MathByte Academy

Information you may find useful
File objects implement the iterator protocol:
with open(f_name) as f:
for row in f:
print(row)
But file objects also support just reading data using the read function
we specify how much of the file to read (that can span multiple rows)
when we do this a "read head" is maintained
à we can reposition this read head
with open(f_name) as f:
print(f.read(100))
à seek()
à reads the first 100 characters
print(f.read(100))
à reads the next 100 characters
à read head is now at 100
à read head is now at 200
f.seek(0)
à moves read head back to beginning of file
Copyright © MathByte Academy

Information you may find useful
CSV files can be read using csv.reader
But CSV files can be written in different "styles"
à dialects
john,cleese,42
john;cleese;42
john|cleese|42
"john","cleese","42"
'john';'cleese';'42'
john\tcleese\t42
The csv module has a Sniffer class we can use to auto-determine the specific dialect
à need to provide it a sample of the csv file
with open(f_name) as f:
sample = f.read(2000)
dialect = csv.Sniffer().sniff(sample)
with open(f_name) as f:
reader = csv.reader(f, dialect)
Copyright © MathByte Academy

Good Luck!
Copyright © MathByte Academy

Copyright © MathByte Academy

Concurrency vs Parallelism
concurrency
Task 1
Task 2
parallelism
Task 1
Task 2
Copyright © MathByte Academy

Cooperative vs Preemptive Multitasking
cooperative
Task 1
Task 2
coroutines
Python
threading
preemptive
Task 1
Task 2
not
voluntary!
voluntary
yield
voluntary
yield
not
voluntary!
completely controlled by developer
not controlled by developer
some sort of 
scheduler involved
Copyright © MathByte Academy

Coroutines
Cooperative multitasking
Concurrent, not parallel
à Python programs execute on a "single thread"
Global Interpreter Lock
à GIL
Two ways to create coroutines in Python
à generators
à native coroutines
à uses extended form of yield
à uses async / await
à recent addition: asyncio
Copyright © MathByte Academy

This section is not about
asyncio
threading
multiprocessing
This section is about
learning the basics of generator-based coroutines
some practical applications of these coroutines
native coroutines
à parallelism
Copyright © MathByte Academy

Copyright © MathByte Academy

What is a coroutine?
cooperative routines
subroutines
subroutine is called
subroutine terminates
inner is now in control
running_averages is in control
running_averages is back in control
stack frame
created
(inner)
stack frame
destroyed
(inner)
may, or may not 
return a value
def averager():
total = 0
count = 0
def inner(value):
nonlocal total
nonlocal count
total += value
count += 1
return total / count
return inner
def running_averages(iterable):
avg = averager()
for value in iterable:
running_average = avg(value)
print(running_average)
Copyright © MathByte Academy

coroutine
def running_averages(iterable):
create instance of running_averager
start coroutine
for value in iterable:
send value to running_averager
received value back
print(received value)
def running_averager():
total = 0
count = 0
running_average = None
while True:
wait for value
receive new value
calculate new average
yield new average
stack frame
created
(running_averager)
coroutine is still active
waiting for next value to be sent
We'll come back to this example in another lecture
Copyright © MathByte Academy

Abstract Data Structures
What are queues and stacks?
A queue is a data structure that supports first-in first-out (FIFO) addition/removal of items
add elements to back of queue
remove elements from front of queue
why abstract?
many different ways of creating
concrete implementations
FIFO
A stack is a data structure that supports last-in first-out addition/removal of items
push elements
on top of stack
last pushed element is removed first (popped)
LIFO
Copyright © MathByte Academy

Using lists
stack
lst.append(item)
à appends item to end of list
lst.pop()
à removes and returns last element of list
queue
lst.insert(0, item)
lst.pop()
à inserts item to front of list
à removes and returns last element of list
So a list can be used for both a stack and a queue
But, inserting elements in a list is quite inefficient!
numbers coming up in a bit…
Copyright © MathByte Academy

The deque data structure
Python's collections module implements a data structure called deque
This is a double-ended queue
à very efficient at adding / removing items from both front and end of a collection
from collections import deque
dq = deque()
dq = deque(iterable)
dq.append(item)
dq.appendleft(item)
dq.pop()
dq.popleft()
len(dq)
dq.clear()
dq = deque(maxlen=n)
Copyright © MathByte Academy

Timings
list
deque
append (right)
pop (right)
insert (left)
pop (left)
# items = 10_000
# tests = 1_000
0.87
(times in seconds)
20.80
0.002
0.012
0.87
0.0005
0.0005
0.84
--
x4
x25
x24
Copyright © MathByte Academy

Another use case…
producer
consumer
queue
producer
consumer
performs work
grabs data from queue
adds data to queue
Copyright © MathByte Academy

Implementing a Producer/Consumer using Subroutines
à create an "unlimited" deque
à run producer to insert all elements into deque
à run consumer to remove and process all elements in deque
def produce_elements(dq):
for i in range(1, 100_000):
dq.appendleft(i)
def consume_elements(dq):
while len(dq) > 0:
item = dq.pop()
print('processing item', item)
def coordinator():
dq = deque()
producer = produce_elements(dq)
consume_elements(dq)
Copyright © MathByte Academy

Implementing a Producer/Consumer using Generators
à create a limited size deque
à producer runs until deque is filled
à yields control back to caller
à consumer runs until deque is empty
à yields control back to caller
repeat until producer is "done"
or controller decides to stop
à coordinator creates instance of producer generator
à coordinator creates instance of consumer generator
Copyright © MathByte Academy

Implementing a Producer/Consumer using Generators
def produce_elements(dq, n):
for i in range(1, n):
dq.appendleft(i)
if len(dq) == dq.maxlen:
yield
def consume_elements(dq):
while True:
while len(dq) > 0:
item = dq.pop()
# process item
yield
def coordinator():
dq = deque(maxlen=10)
producer = produce_elements(dq, 100_000)
consumer = consume_elements(dq)
while True:
try:
next(producer)
except StopIteration:
break
finally:
next(consumer)
Notice how yield is not used to yield values
but to yield control back to controller
Copyright © MathByte Academy

Copyright © MathByte Academy

Generators can be in different states
def my_gen(f_name):
f = open(f_name)
try:
for row in f:
yield row.split(',')
finally:
f.close()
rows = my_gen()
create the generator
à CREATED
run the generator
next(rows)
à RUNNING
until yield
à CLOSED
until generator return
à SUSPENDED
Copyright © MathByte Academy

Inspecting a generator's state
use inspect.getgeneratorstate to see the current state of a generator
from inspect import getgeneratorstate
g = my_gen()
getgeneratorstate(g) à GEN_CREATED
row = next(g)
getgeneratorstate(g) à GEN_SUSPENDED
lst(g)
getgeneratorstate(g) à GEN_CLOSED
(inside the generator code while it is running)
getgeneratorstate(g) à GEN_RUNNING
Copyright © MathByte Academy

Copyright © MathByte Academy

So far…
We saw how yield can produce values
à use iteration to get the produced values
After a value is yielded, the generator is suspended
How about sending data to the generator upon resumption?
Enhancement to generators introduced in Python 2.5
PEP 342
à next()
Copyright © MathByte Academy

Sending data to a generator
yield is actually an expression
it can yield a value (like we have seen before)
yield 'hello'
it can also receive values 
it is used just like an expression would
received = yield
we can combine both
received = yield 'hello'
à works, but confusing!
à use sparingly
Copyright © MathByte Academy

What's happening?
def gen_echo():
while True:
received = yield
print('You said:', received)
echo = gen_echo()
à CREATED
has not started running yet – not in a suspended state
next(echo)
à SUSPENDED
Python has just yielded (None)
generator is suspended at the yield
we can resume and send data to the generator at the same time using send()
generator resumes running exactly at the yield
the yield expression evaluates to the just received data
then the assignment to received is made
echo.send('hello')
Copyright © MathByte Academy

What's happening?
received = yield 'python'
'python' is yielded and control is returned to caller
generator is 
suspended here
caller sends data to generator: g.send('hello')
generator resumes
'hello' is the result of the yield expression
'hello' is assigned to received
generator continues running until the next yield or return
Copyright © MathByte Academy

Priming the generator
received = yield 'python'
Notice that we can only send data if the generator is suspended at a yield
So we cannot send data to a generator that is in a CREATED state – it must be in a SUSPENDED state
echo = gen_echo()
def gen_echo():
while True:
received = yield
print('You said:', received)
à SUSPENDED
echo.send('hello')
next(echo)
à CREATED
echo.send('hello')
à yes, a value has been yielded – and we can choose to just ignore it
à in this example, None has been yielded
Copyright © MathByte Academy

Priming the generator
Don't forget to prime a generator before sending values to it!
à always use next() to prime
à generator must be SUSPENDED to receive data
Later we'll see how we can "automatically" prime the generator using a decorator
Copyright © MathByte Academy

Using yield…
à used for producing data
à yield 'Python'
à used for receiving data
à a = yield
(technically this produces None)
Be careful mixing the two usages in your code
à difficult to understand
à sometimes useful
à often not needed
Copyright © MathByte Academy

Example
def running_averager():
total = 0
count = 0
running_average = None
while True:
value = yield running_average
total += value
count += 1
running_average = total / count
averager = running_averager()
next(averager)
à primed
à None has been yielded
averager.send(10)
à value received 10
à continues running until next yield
à yields running_average
à suspended and waiting
à 10
averager.send(30)
à value received 30
à eventually yields 20
Copyright © MathByte Academy

Copyright © MathByte Academy

Consider this generator function…
def read_file(f_name):
f = open(f_name)
try:
for row in f:
yield row
finally:
f.close()
Suppose the file has 100 rows
rows = read_file('test.txt')
for _ in range(10):
next(rows)
à read 10 rows
à file is still open
à how do we now close the file without iterating through the entire file?
à yield from f
Copyright © MathByte Academy

Closing a generator
We have seen the possible generator states
created, running, suspended, closed
We can close a generator by calling its close() method
def read_file(f_name):
f = open(f_name)
try:
for row in f:
yield row
finally:
f.close()
rows = read_file('test.txt')
for _ in range(10):
next(rows)
rows.close()
à finally block runs, and file is closed
why did it jump to finally? Did an exception occur?
Copyright © MathByte Academy

Behind the scenes…
When .close() is called, an exception is triggered inside the generator
The exception is a GeneratorExit exception
def gen():
try:
yield 1
yield 2
except GeneratorExit:
print('Generator close called')
finally:
print('Cleanup here…')
g = gen()
next(g)
g.close()
à Generator close called
à Cleanup here…
Copyright © MathByte Academy

Python's expectations when close() is called
•
a GeneratorExit exception bubbles up
•
the generator exits cleanly (returns)
à the exception is silenced by Python
à to the caller, everything works "normally"
•
some other exception is raised from 
inside the generator
à exception is seen by caller
if the generator "ignores" the 
GeneratorExit exception and yields
another value
à Python raises a RuntimeError: 
generator ignored GeneratorExit
in other words, don't try to catch and ignore a GeneratorExit exception
it's perfectly OK not to catch it, and simply let it bubble up
def gen():
yield 1
yield 2
g = gen()
next(g)
g.close()
Copyright © MathByte Academy

Use in coroutines
Since coroutines are generator functions, it is OK to close a coroutine also
For example, you may have a coroutine that receives data to write to a database
à coroutine opens a transaction when it is primed (next)
à coroutine receives data to write to the database
à coroutine commits the transaction when close() is called (GeneratorExit)
à coroutine aborts (rolls back) transaction if some other exception occurs
Copyright © MathByte Academy

Copyright © MathByte Academy

Sending things to coroutines
.send(data)
à sends data to coroutine
.close()
à sends (throws) a GeneratorExit exception to coroutine
we can also "send" any exception to the coroutine
à throwing an exception to the coroutine
.throw(exception)
à the exception is raised at the point where the coroutine is suspended
Copyright © MathByte Academy

How throw() is handled
à generator does not catch the exception (does nothing)
à generator catches the exception, and does something
à yields a value
à exits (returns)
à raises a different exception
à exception propagates to caller
Copyright © MathByte Academy

Catch and yield
à generator catches the exception
à handles and silences the exception
à yields a value
à yielded value is the return value of the .throw() method
def gen():
while True:
try:
received = yield
print(received)
except ValueError:
print('silencing ValueError')
None has been yielded
à generator is now SUSPENDED
Copyright © MathByte Academy

Catch and exit
à generator catches the exception
à generator exits (returns)
à caller receives a StopIteration exception
this is the same as calling next() or send() to a generator that returns instead of yielding
can think of throw() as same thing as send(), but causes an exception to be sent 
instead of plain data
def gen():
while True:
try:
received = yield
print(received)
except ValueError:
print('silencing ValueError')
return None
StopIteration
à generator is now CLOSED
is seen by caller
Copyright © MathByte Academy

Catch and raise different exception
à generator catches the exception
à generator handles exception and raises another exception
à new exception propagates to caller
à generator is now CLOSED
def gen():
while True:
try:
received = yield
print(received)
except ValueError:
print('silencing ValueError')
raise CustomException
CustomException
is seen by caller
Copyright © MathByte Academy

close() vs throw()
close()
à GeneratorExit exception is raised inside generator
can we just call?
gen.throw(GeneratorExit())
yes, but…
with close(), Python expects the GeneratorExit, or StopIteration exceptions to propagate, 
and silences it for the caller
if we use throw() instead, the GeneratorExit exception is raised inside the caller context (if 
the generator lets it)
try:
gen.throw(GeneratorExit())
except GeneratorExit:
pass
Copyright © MathByte Academy

Copyright © MathByte Academy

We always to prime a coroutine before using it
à very repetitive
à pattern is the same every time
g = gen()
à creates coroutine instance
next(g)
or g.send(None)
à primes the coroutine
This is a perfect example of using a decorator to do this work for us!
Copyright © MathByte Academy

Creating a function to auto prime coroutines
def prime(gen_fn):
g = gen_fn()
next(g)
return g
creates the generator
primes the generator
returns the primed generator
def echo():
while True:
received = yield
print(received)
echo_gen = prime(echo)
echo_gen.send('hello')
à 'hello'
Copyright © MathByte Academy

A decorator approach
We still have to remember to call the prime function for our echo coroutine before we can use it
Since echo is a coroutine, we know we always have to prime it first
So let's write a decorator that will replace our generator function with another function
that will automatically prime it when we create an instance of it
def coroutine(gen_fn):
def prime():
g = gen_fn()
next(g)
return g
return prime
def echo():
while True:
received = yield
print(received)
@coroutine
Copyright © MathByte Academy

Understanding how the decorator works
def coroutine(gen_fn):
def prime():
g = gen_fn()
next(g)
return g
return prime
def echo():
while True:
received = yield
print(received)
echo = coroutine(echo)
[same effect as using @coroutine]
à echo function is now actually the prime function
à prime is a closure
à free variable gen_fn is echo
calling echo()
à calls prime() with gen_fn = echo
g = echo()
next(g)
return g
Copyright © MathByte Academy

Expanding the decorator
def coroutine(gen_fn):
def prime():
g = gen_fn()
next(g)
return g
return prime
à cannot pass arguments to the generator function
def coroutine(gen_fn):
def prime(*args, **kwargs):
g = gen_fn(*args, **kwargs)
next(g)
return g
return prime
Copyright © MathByte Academy

Copyright © MathByte Academy

Recall…
Instead of using that loop, we saw we could just write:
def subgen():
for i in range(10):
yield i
We could consume the data from subgen in another 
generator this way:
def delegator():
for value in subgen():
yield value
def delegator():
yield from subgen()
With either definition we can call it this way:
d = delegator()
next(d)
etc…
Copyright © MathByte Academy

What is going on exactly?
caller
next(d)
delegator
yield from subgen
subgen
yield value
next
next
yield value
yield value
2-way communications
Can we send(), close() and throw() also?
Yes!
Copyright © MathByte Academy

How does the delegator behave when subgenerator returns?
it continues running normally
def delegator():
yield from subgen()
yield 'subgen closed'
def subgen():
yield 1
yield 2
d = delegator()
next(d)
à 1
next(d)
à 2
next(d)
à subgen closed
next(d)
à StopIteration
Copyright © MathByte Academy

Inspecting the subgenerator
from inspect import getgeneratorlocals, getgeneratorstate
def delegator():
a = 100
s = subgen()
yield from s
yield 'subgen closed'
def subgen():
yield 1
yield 2
d = delegator()
getgeneratorstate(d) à GEN_CREATED
getgeneratorlocals(d) à {}
next(d)
getgeneratorstate(d) à GEN_SUSPENDED
getgeneratorlocals(d) à {'a': 100, 's': <gen object>}
s = getgeneratorlocals(d)['s']
getgeneratorstate(s) à GEN_SUSPENDED
next(d)
à 1
à 2
d à SUSPENDED
s à SUSPENDED
next(d)
à 'subgen closed'
d à SUSPENDED
s à CLOSED
next(d)
à StopIteration
d à CLOSED
s à CLOSED
Copyright © MathByte Academy

Copyright © MathByte Academy

yield from and send()
yield from establishes a 2-way communication channel
between caller
and subgenerator
via a delegator
à yield from
caller
next
next
yield
delegator
subgenerator
yield
send
send
Copyright © MathByte Academy

Priming the subgenerator coroutine
We know that before we can send() to a coroutine, we have to prime it first
à next(coroutine)
How does this work with yield from?
def delegator():
yield from coro()
def coro():
while True:
received = yield
print(received)
d = delegator()
next(d)
yield from will automatically prime the coroutine when necessary
before we can send to d we have to prime it
What about coro()?
Copyright © MathByte Academy

Sending data to the subgenerator
Once the delegator has been primed
data can be sent to it using send()
def delegator():
yield from coro()
def coro():
while True:
received = yield
print(received)
d = delegator()
next(d)
d.send('python')
à python is printed by coroutine
Copyright © MathByte Academy

Control Flow
delegator
yield from coro()
print('next line of code')
caller
subgenerator
yield
next
delegator is "stuck" here until subgenerator closes
then it resumes running the rest of the code
Copyright © MathByte Academy

Multiple Delegators à pipeline
def coro():
…
yield
…
def gen1():
yield from gen2()
def gen2():
yield from coro()
d = gen1()
gen1
gen2
coro
caller
this can even be recursive
Copyright © MathByte Academy

Copyright © MathByte Academy

Closing the subgenerator
def delegator():
…
yield from subgen()
…
def subgen():
…
yield
…
next
when subgen closes
delegator resumes running exactly where it was paused 
delegator code is effectively "paused" here as long as subgen is not closed
Copyright © MathByte Academy

Closing the delegator
def delegator():
…
yield from subgen()
…
def subgen():
…
yield
…
next
d = delegator()
d.close()
à closes the subgenerator
à immediately closes the delegator as well
Copyright © MathByte Academy

Returning from a generator
A generator can return
à StopIteration
The returned value is embedded in the StopIteration exception
à we can extract that value
try:
next(g)
except StopIteration as ex:
print(ex.value)
à so can Python!
Copyright © MathByte Academy

Returning from a subgenerator
yield from
is an expression
It evaluates to the returned value of the subgenerator
result = yield from subgen()
def subgen():
…
yield
…
return result
Copyright © MathByte Academy

def delegator():
…
result =  yield from subgen()
…
def subgen():
…
yield
…
return result
next
Returning from a subgenerator
delegator receives return value
and continues running normally
yield from
à establishes conduit
subgenerator returns
à conduit is closed
à yield from evaluates to the returned value
à delegator resumes running
Copyright © MathByte Academy

Copyright © MathByte Academy

Throwing Exceptions
We can throw exceptions into a generator using the throw() method
à works with delegation as well
def delegator():
yield from subgen()
def subgen():
…
yield
…
d = delegator()
d.throw(Exc)
Subgenerator can then handle exception (or not)
Delegator does not intercept the exception
à just forwards it to subgenerator
Copyright © MathByte Academy

Exception Propogation
subgen
exception
delegator
throw
something else
caller
may handle: silence or propagate up (same or different exception)
may handle: silence or propagate up (same or different exception)
Copyright © MathByte Academy

Copyright © MathByte Academy

Data pipelines (Pulling)
data consumer
(sink)
pull
filter
pull
transform
pull
data source
(producer)
We've seen this before
à use yield and iteration to pull data through the pipeline
consumer
iterate filter_data()
write data to file
filter_data
iterate parse_data()
yield select rows only
parse_data
iterate read_data()
transform data
yield row
read_data
yield row
pull
pull
pull
Copyright © MathByte Academy

Data pipelines (Pushing)
With coroutines, we can also push (send) data through a pipeline
data source
push
push
transformer
filter
push
consumer
(producer)
(sink)
generate integers
push
square number
Example
push
filter odds only
push
log results
Copyright © MathByte Academy

Can get crazier…
broadcasting
source
transformer
broadcaster
filter
transformer
filter
…
…
…
pushes data through the pipeline
Copyright © MathByte Academy


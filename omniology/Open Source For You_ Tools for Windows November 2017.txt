Volume: 06 | Issue: 02 | Pages: 116 | November 2017
ISSN-2456-4885
` 120
a sneak peek 
Into All That Happened
What’s New In 
Java 9
Wine: A Great Tool To Run 
Windows Programs On Linux
Open Source Software That 
Partners With MS Windows
Electron: Building Cross-platform 
Desktop Apps With Web Technologies
Open Source Productivity Tools 
That Run On Windows
WindoWs
Tools for 


{ }

ISSN-2456-4885
NOVEMBER 2017
ADMIN 
22 
Using Ansible for Managing 
VMware Infrastructure
31 
DevOps Series: Provisioning  
with Ansible
41 
The Current Popularity and the 
Future of Docker
50 
Analyse Big Data With  
Apache Storm
DEVELOPERS 
54  Creating a Web Application  
Using Azure App Services
57  Electron: Building Cross-
platform Desktop Apps with Web 
Technologies
60  Yarn: A Quick, Reliable and Safe 
Way to Share Code
64 
Developing a Simple Android 
Application Using Kotlin
67 
Using PhaserJS to Speed Up 2D 
Game Development
72  Get Started with Jenkins 2.0:  
Pipeline as Code
75  What’s New in Java 9?
88  Deep Learning: Using Algorithms to 
Make Machines Think
FOR U & ME 
96  Open Source Software That Can 
Partner with MS Windows 
98  The Story Behind the All-pervasive AI
100  Key Open Source Software That 
Can Boost Business Productivity
107  Evolving Careers and Skills  
in the IT Industry
OPENGURUS 
108  Developing ARM Targets Using 
GNU MCU Eclipse
COLUMNS 
15 
CodeSport
POST SHOW REPORT 
82 
Post Show Report
Using a Virtual Machine to Run Linux on Windows
Open Source 
Productivity Tools that 
Run on Windows
08 
FossBytes
  18  
New Products
  112  
Tips & Tricks
REGULAR FEATURES
26
92
Wine: A Great Tool 
for Running Windows 
Programs on Linux
46
CONTENTS
4 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com


Indian e-commerce startup  
Fynd contributes back to the  
open source community
N
ot
e:
 A
n
y 
o
bj
e
ct
io
n
a
bl
e 
m
at
e
ri
al
, 
if
 f
o
u
n
d 
o
n 
th
e 
di
s
c,
 i
s 
u
ni
nt
e
n
d
e
d,
 a
n
d 
s
h
o
ul
d 
b
e 
at
tr
ib
ut
e
d 
to
 t
h
e 
co
m
pl
e
x 
n
at
u
r
e 
of
 I
nt
e
r
n
et
 d
at
a.
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort@
efy.i
n fo
r a f
ree 
repl
ace
men
t.
 A bootable DVD with a collection of GNU/Linux 
software, automatic hardware detection, and support 
for many graphics cards, sound cards, SCSI and USB 
devices, as well as other peripherals. One of the most 
versatile live operating system
November 2017
Knoppix 8.1
CD T
eam
 e-m
ail: 
cdtea
m@
efy.i
n
Rec
om
men
ded 
Syst
em 
Req
uire
men
ts: P
4, 1
GB 
RAM
, DV
D-R
OM 
Driv
e
Try the latest version of KNOPPIX, 
the versatile Linux distro.
• KNOPPIX 8.1
DVD OF THE MONTH
20
114
38
44
Rajarshi Bhattacharyya, 
country head, SUSE India
“The acceptance of open source among 
Indian enterprises has been remarkable”
78
Tony Wasserman, American computer scientist
Jayakumar, country 
manager, DigitalOcean
Harsh Shah, co-founder, GoFynd.io
“Software has already become 
pervasive in the developed and 
the developing worlds”
“DigitalOcean is built on a 
foundation of open source”
CONTENTS
EDITOR
RAHUL CHOPRA
EDITORIAL, SUBSCRIPTIONS & ADVERTISING
DELHI (HQ)
D-87/1, Okhla Industrial Area, Phase I, New Delhi 110020
Ph: (011) 26810602, 26810603; Fax: 26817563
E-mail: info@efy.in
MISSING ISSUES
E-mail: support@efy.in
BACK ISSUES
Kits ‘n’ Spares
New Delhi 110020 
Ph: (011) 26371661,  26371662
E-mail: info@kitsnspares.com
NEWSSTAND DISTRIBUTION
Ph: 011-40596600
E-mail: efycirc@efy.in 
ADVERTISEMENTS 
MUMBAI
Ph: (022) 24950047, 24928520 
E-mail: efymum@efy.in
BENGALURU
Ph: (080) 25260394, 25260023 
E-mail: efyblr@efy.in
PUNE
Ph: 08800295610/ 09870682995 
E-mail: efypune@efy.in
GUJARAT
Ph: (079) 61344948 
E-mail: efyahd@efy.in
JAPAN
Tandem Inc., Ph: 81-3-3541-4166 
E-mail: japan@efy.in
SINGAPORE
Publicitas Singapore Pte Ltd 
Ph: +65-6836 2272 
E-mail: singapore@efy.in
TAIWAN 
J.K. Media, Ph: 886-2-87726780 ext. 10 
E-mail: taiwan@efy.in
UNITED STATES
E & Tech Media 
Ph: +1 860 536 6677 
E-mail: usa@efy.in
Printed, published and owned by Ramesh Chopra. Printed at Tara 
Art Printers Pvt Ltd, A-46,47, Sec-5, Noida, on 28th of the previous 
month, and published from D-87/1, Okhla Industrial Area, Phase I, New 
Delhi 110020. Copyright © 2017. All articles in this issue, except for 
interviews, verbatim quotes, or unless otherwise explicitly mentioned, 
will be released under Creative Commons Attribution-NonCommercial 
3.0 Unported License a month after the date of publication. Refer to 
http://creativecommons.org/licenses/by-nc/3.0/  for a copy of the 
licence. Although every effort is made to ensure accuracy, no responsi-
bility whatsoever is taken for any loss due to publishing errors. Articles 
that cannot be used are returned to the authors if accompanied by a 
self-addressed and sufficiently stamped envelope. But no responsibility 
is taken for any loss or delay in returning the material. Disputes, if any, 
will be settled in a New Delhi court only.
SUBSCRIPTION RATES 
Year 
Newstand Price 
You Pay 
Overseas
 
(`) 
(`)
Five 
7200 
4320 
—
Three 
4320 
3030 
—
One 
1440 
1150  
US$ 120
Kindly add ` 50/- for outside Delhi cheques.
Please send payments only in favour of EFY Enterprises Pvt Ltd.
Non-receipt of copies may be reported to support@efy.in—do mention 
your subscription number.
6 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com


Apple open sources kernel of iOS 
and macOS for ARM processors
Apple has open sourced 
the kernel code of the 
latest macOS X and iOS 
updates. The company 
always shares the kernels 
of macOS and iOS on 
GitHub, but it has this time 
enabled the experience 
for ARM chips.
One of the reasons 
why Apple open sources 
a portion of macOS and iOS each year is because of its indirect dependence on 
the BSD project. There is no way to let developers use the open source codebase 
published by Apple to compile their own OS. However, the kernel guides 
coders to begin with macOS and iOS kernels.
Notably, the ARM-based source code of Apple’s kernel doesn’t mean much but 
gives Apple a way to give back to the open source community. Some open source 
experts are predicting that the iPhone maker could be working on a version of 
macOS that runs on ARM chips.
Oracle develops Fn to enter serverless computing world
Oracle is all set to step into the nascent space of serverless computing by 
developing an all-new open source, cloud-agnostic platform called Fn. 
The new open source development can be run either on-premise or in 
the cloud and works with Docker containers.
“The Fn project is a container-native Apache 2.0 licensed serverless platform 
that you can run anywhere — on any cloud or on-premise,” said Shaun Smith, 
product management director at 
Oracle, in a blog post.
Oracle’s Fn has a command 
line interface that helps developers 
build, test and deploy functions 
using orchestration tools such 
as Kubernetes, Mesosphere and 
Docker Swarm. There is a built-
in smart load balancer that routes 
traffic directly to functions to 
enhance the experience.
You can use Fn with functions 
written in Java at the initial stage. But there are also plans to expand the platform by 
adding support for Go, Ruby, Python, PHP and Node.js over time.
Along with the other features, the Fn project allows developers to take AWS 
Lambda functions and run them platform agnostically. You need to use the lambda-
node-4 runtime to begin with Lambda functions on Fn.
Kotlin 1.2 beta comes with support for Java 9
Kotlin 1.2 beta has arrived. With the new release, developers can build some 
interesting modern multi-platform applications. The experimental multi-platform 
Compiled by: 
Jagmeet Singh
FOSSBYTES
Google launches Cloud 
Firestore to help developers 
store app data easily
To enhance developer engagements 
around its Firebase application 
development platform, Google 
has launched Cloud Firestore. 
The latest innovation brings a 
fully-managed NoSQL document 
database that can store and sync app 
data at a global scale.
“We’ve aimed for the simplicity 
and ease-of-use that is always top 
priority for Firebase, while making 
sure that Cloud Firestore can scale 
to power even the largest apps,” said 
Alex Dufetel, product manager for 
Firebase at Google, in a blog post.
Available in beta, Cloud Firestore 
is designed to help developers hold 
documents and collections on the 
cloud with querying support. The 
database comes with iOS, Android and 
Web SDKs. It also supports offline 
data access and enables real-time 
data synchronisation. Additionally, 
developers can leverage the built-in 
automatic, multi-region data replication 
feature to get better cloud access. 
The client-side SDKs of 
Cloud Firestore enable serverless 
development and provide offline data 
access with local database support 
for the Web as well as Android 
and iOS platforms. Developers can 
pick the Cloud Firestore SDKs for 
Java, Go, Python and Node.js.
Google’s Cloud Firestore hasn’t 
been developed from scratch, but built 
as an upgrade to the existing Firebase 
Realtime Database, which offers the 
developer community a real-time 
database — managed and scaled by 
Google. Key limitations such as data 
structuring, querying and scaling have 
all been addressed in this upgrade.
You can get started with the 
public beta of Cloud Firestore 
by visiting the database tab in 
your Firebase console.
8 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
projects option lets developers reuse code between JVM and JavaScript, 
for which the code is first placed in a common module.
In addition to the option to reuse code, Kotlin 1.2 beta produces the code 
for both common and platform-specific parts during the compilation of the 
project. There is also a way for developers to express dependencies of common 
code on platform-specific parts via 
expected and actual declarations. If 
the declaration specifies an API, you 
need to type the alias that refers to 
an existing implementation of the 
API in an external library.
The standard library of Kotlin 1.2 
features a kotlin.math package for 
mathematical operations in cross-
platform code. The math package also 
offers better precision for math polyfills for JavaScript.
It is worth noting the compatibility with Java 9 module forest split packages 
in the new version. This version also removes the deprecated declarations in the 
kotlin.reflect package from the kotlin-reflect library.
Kotlin 1.2 also supports an array of literals in annotations and simplifies 
coding. The new version uses a more consistent syntax. 
Oracle leverages open source to develop enterprise-focused 
blockchain technology
Oracle has finally planned to enter the growing blockchain space by launching 
its enterprise-centric Oracle BlockChain Cloud Service. The new distributed 
ledger cloud platform is based on the open source Hyperledger Fabric project, 
which has been hosted by the Linux 
Foundation and receives contributions 
from a group of companies, 
including Oracle.
“Oracle Blockchain Cloud Service 
provides enterprise-grade blockchain 
capabilities and is able to accelerate 
innovation for on-premise ERP, cloud-
based SaaS (Software-as-a-Service) 
and PaaS (platform-as-a-service) customers,” said Amit Zavery, senior vice 
president, Oracle Cloud Platform.
The new blockchain platform is a part of Oracle Cloud Platform — touted to 
deliver enterprise-grade resilience, scalability and security. It also helps enterprise 
customers streamline operations and expand their market reach by using open 
source developments and maintaining open standards.
Oracle has developed an applications integration toolkit that comes with 
integrated applications, sample code, templates and design patterns to help 
enterprises rapidly deploy blockchain applications to Oracle SaaS and on-premise 
application suites. The platform also works with real-time information-sharing 
across existing Oracle ERP Cloud, Oracle SCM Cloud, Netsuite SuiteCloud 
Platform or custom blockchain application integration.
There is REST API support to let enterprises invoke blockchain 
services directly or with some pre-built integrations from Oracle Cloud. 
Oracle has also provided an API management service to enable easy app 
development in the cloud or on-premise.
Kubernetes 1.8 is all about 
enhancing security
Kubernetes 1.8 is now out with 
the prime focus on enhancing its 
security. The new version upgrades 
the existing networking support 
and includes role-based features to 
deliver a more secure experience.
First in the list of new features in 
Kubernetes 1.8 is role-based access 
control (RABC) that lets an admin 
define access permissions to various 
resources. RABC comes with 
permission access for resources like 
pods and secrets that can be granted 
to one or more users.
The orchestration framework 
already included a provision for 
networking. But with 
the new version, 
developers have 
been provided 
beta support for 
filtering outbound traffic as well. 
However, the beta package limits 
filtering in both directions, and only 
supports some destination ports and 
peers at the initial stage.
The Google team behind 
Kubernetes has upgraded the 
automatic TLS certificate rotation 
for Kubelet to beta. This helps 
to automatically generate unique 
certifications for Kubelet when 
a current one expires. The latest 
version promotes auditing features 
from the alpha to the beta stage. This 
includes formatting tools for audit 
logs and features to control users of 
each element of the cluster.
Another feature that has been 
promoted from alpha to beta is the 
set of workload APIs. This provides 
a way to orchestrate applications 
based on their overall behaviour. 
Workload APIs can also be used to 
monitor the production status.
You can find the latest 
Kubernetes release on its 
official GitHub page.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 9

FOSSBYTES
Companies like Accenture, Infosys and Intel have partnered with Oracle 
to start deploying the newly launched Oracle Blockchain Cloud Service. “Our 
collaboration with Oracle is aimed at developing a blockchain cloud service 
for enterprises that helps to advance the privacy of blockchain transactions 
and improves the interoperability of traditional databases and blockchains,” 
said Raejeanne Skillern, vice president of the data centre group and GM, 
cloud service provider group, Intel.
F# functional programming debuts on ASP.Net 
Core through Giraffe
Micro Web framework Giraffe has brought F# functional-style programming 
to ASP.Net Core. The latest development also includes interesting features like 
higher-order functions that have been much awaited on ASP.Net Core for years.
Giraffe offers a different technical philosophy that uses F# features 
like F# types, higher order functions and partial application of arguments. 
ASP.Net developers will benefit a lot by using the framework as it 
enables the development of Web API routes in a functional style. F# is 
already supported in ASP.Net Core MVC.
Developers need to plug Giraffe into the ASP.Net Core pipeline using 
middleware. Microsoft has published a detailed guide to help you use Giraffe 
to build a Web service with F# and the .NET Core 2.0 development platform. 
You might encounter some bugs since the framework is currently available 
in a beta stage. Having said that, the Giraffe team is working to improve the 
routing model for the framework.
Giraffe is known for its capabilities to build rich Web applications that are based 
on advanced F# features. Notably, the core of Giraffe is built on top of ASP.Net.
Microsoft maintains F# as the open source functional-first language. The 
language is designed to address complex computing problems using simple code.
Digitalisation and technological innovation lead to changing role 
of CIOs: Gartner
With the swift growth in the technology world, the nature of a chief information 
officer’s (CIO) job has changed. A survey by Gartner has revealed that the CIO 
role is transitioning from being a delivery executive to a business executive, 
moving from controlling cost and engineering processes to driving revenue and 
exploiting data.
The survey, dubbed the 2018 Gartner CIO Agenda Survey, conducted with 
a record number of 3,160 CIO respondents across all major industries in 98 
countries, highlights that 95 per 
cent of CIOs expect their jobs 
to change due to digitalisation. 
Respondents consider that 
world class IT delivery 
management and digitalisation 
are the two biggest 
transformations in the CIO role, 
followed by assuming increased 
and broader responsibilities and 
capabilities. “The CIO’s role must grow and develop as digital business spreads, 
and disruptive technologies, including intelligent machines and advanced 
analytics, reach the masses,” said Andy Rowsell-Jones, vice president and 
distinguished analyst at Gartner.
Mozilla pays half a million 
dollars to support open 
source projects
Mozilla has announced that it has paid 
a total of US$ 539,000 for supporting 
open source projects. The company 
runs the Mozilla Open Source Support 
(MOSS) programme to support open 
source developments around the globe.
“At Mozilla, we were born out of, 
and remain a part of, the open source 
and free software movement. Through 
the Mozilla Open Source Support 
(MOSS) programme, we recognise, 
celebrate and support open source 
projects that contribute to our work 
and to the health of the Internet,” said 
Gervase Markham, policy engineer, 
Mozilla Corporation, in a blog post.
Amongst other major open source 
projects, Mozilla awarded the biggest 
amount of 
US$ 194,000 
to Ushahidi, 
which is an 
open source 
software 
platform for 
crowdsourcing, 
monitoring, visualising and responding 
to reports from people suffering during 
times of political turmoil and subject 
to governmental or vigilante abuse. 
The Mountain View, California based 
company has also supported projects 
like WebAssembly backer Webpack, 
email security platform RiseUp, 
HTML5 game engine Phaser and 
Apache module Mod_md.
In August this year, Mozilla 
launched its ‘Global Mission Partners: 
India’ initiative, which allocated Rs 10 
million for Indian open source projects 
that promoted Mozilla’s mission. The 
company also ran several audits on 
codebases of open source developments 
such as expat (an XML parser) and 
GNU libmicrohttpd (an embedded 
HTTP server) through the Secure Open 
Source arm of MOSS.
10 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
“While delivery is still a part of the job, much greater emphasis is being placed on 
attaining a far broader set of business objectives,” Rowsell-Jones added.
The survey shows that a majority of CIO respondents believe that 
technology trends such as cyber security and artificial intelligence are set to 
significantly change their jobs in the near future. Moreover, around 35 per cent 
of the total respondents said that they have already invested and deployed some 
aspect of digital security, while 36 per cent are planning to implement some 
form of digital security.
Despite the positivity towards the change in their role, Rowsell-Jones believes 
that organisational culture could be a roadblock to success.
“CIOs need to identify the cultural behaviours that currently exist and what the 
future vision is. In doing so, they must recognise existing cultural strengths and 
position cultural change as ‘the next chapter’, rather than a massive overhaul, to 
respect employees’ contributions and invite them to come along on the journey,” 
said Rowsell-Jones.
At least 84 per cent of top CIOs surveyed have responsibility for areas of the 
business outside traditional IT. The ideal balance, as per the responses, is 56 per 
cent related to business outcomes, such as revenue growth, business margins and 
influencing business strategy, and 44 per cent related to IT delivery.
Respondents to the survey were categorised as top, typical and trailing 
performers in digitalisation. Some CIOs favour a separate digital team to boost their 
enterprise’s digital business while others consider digitalisation part of their job. 
Microsoft joins Open Source Initiative as ‘Premium’ sponsor
Microsoft has joined the Open Source Initiative (OSI) as a ‘Premium’ sponsor. 
The new development has 
happened months after the 
Redmond company entered the 
membership agreement with 
the Linux Foundation and is 
another significant effort to 
ensure a feeling of good faith 
with open source contributors.
“This is a significant 
milestone for the OSI and the 
open source software movement 
more broadly,” said Patrick Masson, general manager and board director, OSI.
The relationship between OSI and Microsoft is not a new one. In fact, the two 
organisations have been in touch for more than a decade, dating back to 2005. The 
Windows maker even opted for an open source licence when releasing .NET in 2014, 
a move that explicitly demonstrated the firm’s interest in the community-driven space.
“As Microsoft engages with open source communities more broadly and deeply, 
we are excited to support the Open Source Initiative’s efforts,” said Jeff McAffer, 
director of Microsoft’s open source programs office.
Since the release of the open source .NET Core back in November 2014, 
Microsoft has become part of a list of open source communities and organisations 
such as Open Source Eclipse Foundation, Cloud Native Computing Foundation 
and Cloud Foundry Foundation. In November 2016, the company joined the Linux 
Foundation to prove its support for Linux, something that CEO Satya Nadella has 
often mentioned at enterprise-focused events in the past.
Microsoft has joined Facebook, Google, GitHub and IBM by sponsoring 
the OSI. The corporate sponsorship gives the non-profit organisation a way to 
Microsoft expands its 
portfolio of machine 
learning tools
Microsoft has launched a new range 
of machine learning tools at its 
annual Ignite conference. With the 
new launch, the Windows maker 
wants to enable developers to utilise 
easy artificial intelligence (AI) based 
tools to build new advancements.
The new tools are the Azure 
Machine Learning Workbench, Azure 
Machine Learning Experimentation 
Service and the Azure Machine 
Learning Model Management 
Service. These are designed to help 
developers working on new AI 
models, as well as those who simply 
want to use pre-existing models.
The Azure Machine Learning 
Experimentation Service is 
designed to help developers 
deploy ML experiments. Microsoft 
has added support for open 
source frameworks like Caffe2, 
PyTorch, TensorFlow, CNTK and 
Cahiner. The experimentation 
service is designed to scale from 
local machines to hundreds of 
GPUs in the cloud.
Machine Learning Workbench, 
on the other hand, is a desktop 
client for Windows and Mac. The 
tool can act as a control panel 
for your development life cycle. 
Lastly, the Azure Machine Learning 
Model Management Service uses 
Docker containers. Developers can 
manage and deploy their models 
to any Docker container using the 
service. The company has also 
included its own Kubernetes-based 
Azure Container Service within the 
learning workbench to enhance the 
coverage of its native development.
All the new machine learning 
developments are a part of 
Microsoft’s AI platform, which the 
company claims serves over 650,000 
sessions per week.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 11

FOSSBYTES
continue its initiatives, which are dedicated to promoting and protecting open 
source software and its communities.
Intel releases Clear Containers 3.0 with 
Go language construction
Intel’s Clear Linux has launched a new generation of its containers project 
Clear Containers. Debuted as the Clear Containers 3.0, the new version brings 
support for leveraging code used for namespace-based containers and improves 
integration in the container ecosystem.
The most significant change in Clear Containers 3.0 is its Go language build. 
Intel’s Clear Linux team has finally decided to switch from C language to Go. The 
new Clear Containers version 
also comes with a new agent 
based on the libcontainer library. 
Users can use the library to 
apply policies and filters such as 
SELinux and Seccomp within 
the Clear Container guest mode.
In addition to the 
major improvements to 
Clear Containers, Intel has 
upgraded CC-runtime that 
now includes compatibility with the OCI runtime specification and Docker 
engine. Users can also run the platform using Kubernetes through CRI-O. This 
new development enables users to run both trusted and untrusted workloads in a 
Kubernetes cluster on bare metal.
Clear Containers 3.0 can be downloaded from its GitHub repository.
Huawei and SUSE to build mission-critical server 
for SAP applications
Huawei and SUSE have expanded their ongoing partnership, and will be developing 
a mission-critical server that runs SAP applications. The announcement has come 
months after the two companies agreed to make SUSE Enterprise Server compatible 
with KunLun RAS 2.0.
The new solution will be based on Huawei’s 16-/32-socket and run SUSE 
Linux Enterprise Server. Supporting 8, 16 and 32 bit Intel processors with up to 
32TB of onboard storage, sources claim that the KunLun mission-critical server 
can achieve a 3,000 per cent 
improvement in reliability. It 
will have technologies such 
as kPar physical partitioning, 
memory module hot swap and 
predictive failure analysis.
“This solution will 
ensure continuous, stable 
running of enterprise, 
business-critical applications 
and reduce unplanned downtime. Supporting memory online maintenance, it 
reduces unplanned system outages caused by memory faults to almost zero, 
thereby boosting the reliability of enterprise critical applications,” said Qiu Long, 
president of the Huawei IT server product line, in a joint statement.
SUSE has enabled its platform with enterprise-class Linux OS kernels, along 
Yahoo open sources its Big 
Data supported search tech
Months after being acquired by 
Verizon, Yahoo has decided to open 
source its Big Data processing and 
serving engine called Vespa. The 
technology was used exclusively 
for search queries on key Yahoo 
products, including Yahoo 
News and Flickr, among others.
Verizon-owned Oath, which serves 
as the parent company of Yahoo, 
claims that Vespa processes and 
serves content and ads almost 90,000 
times every second, with latencies in 
the tens of milliseconds. It is even 
supposed to handle keyword and 
image searches on a huge scale, with 
a few hundred queries per second on 
tens of billions of images.
Developer teams can leverage 
Vespa to pick content through SQL-
like queries and text search, organise 
matches and generate data-driven 
pages, as well as write data in real-
time. The technology is capable of 
distributing data and computation over 
several machines at once. 
“By releasing Vespa, we are 
making it easy for anyone to build 
applications that can compute 
responses to user requests, over large 
data sets, in real-time and at Internet 
scale — capabilities that up until 
now have been within the reach of 
only a few large companies,” Vespa’s 
distinguished architect Jon Bratseth 
wrote in a blog post. Vespa can be 
run on-premise or in the cloud and 
comes both in Docker images and 
rpm packages. Its code is available 
in a GitHub repository along with 
detailed documentation.
12 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

FOSSBYTES
with a bottom-layer firmware provided by KunLun. The Linux operating system 
includes integration with the optimised KunLun BIOS process that can automate the 
experience with user-mode tools.
“Having support and coordination from the bottom-layer hardware structural 
design and BIOS software sides is integral to SUSE’s innovations around the OS 
RAS features,” said Ralf Flaxa, president of engineering, SUSE.
In addition to the partnership with SUSE, Huawei has collaborated with others 
to solve issues related to disaster recovery, high availability and backup. This will 
make the new server a competitive model for enterprises.
SUSE has natively tied up with SAP to bring SUSE OpenStack Cloud and 
SUSE Enterprise Storage to the SAP Cloud Platform. 
Facebook relicenses React to continue to charm devs
Weeks after facing criticism from the open source community, Facebook has 
decided to relicense its projects, including React, Jest, Flow and Immutable.js. 
The company is set to switch the developments from the existing BSD + 
Patents licence, and relicense them under the MIT licence.
React faced criticism due to its availability under the BSD + Patents licence. 
Several developers even switched to some close alternatives of the React system, 
which ultimately impacted 
the company in the fast-
growing world of open 
source.
“We’re relicensing 
these projects because 
React is the foundation of 
a broad ecosystem of open 
source software for the 
Web, and we don’t want to 
hold back forward progress 
for non-technical reasons,” 
Facebook’s engineering director Adam Wolff said in a blog post.
Facebook believes that the issuance of BSD + Patents licence provides “some 
benefits” to developers deploying its projects. However, the community is not on 
the same track and wants an unrestricted licence.
Among the leading critics was the Apache Software Foundation. The non-profit 
organisation even marked the Facebook BSD + Patents licence under its Category 
X, and restricted access to React across its entire open source portfolio.
“We’re sorry for the churn,” asserted Wolff, adding that Facebook doesn’t want 
to leave the door open for developers to move to any alternatives now.
React 16 will be the first open source project by Facebook to include the licence 
update. This new version is a year-long effort of Facebook’s development team and 
is completely rewritten.
Even though Facebook has moved React and the other major projects to 
the MIT licence, it continues to keep the BSD + Patents licence for now. Wolff 
mentioned that since each project is different, alternative licensing options will 
depend on a variety of factors.
Baidu develops open source deep learning 
framework for smartphones
Chinese Internet giant Baidu aims to get bigger in the world of artificial intelligence 
(AI) by launching its open source mobile deep learning framework. Called Mobile 
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 13

FOSSBYTES
For more news, visit www.opensourceforu.com
Deep Learning (MDL), this convolution-based neural network uses the image 
sensor on Android and iOS mobile devices to recognise objects.
MDL’s neural network has low complexity and high speed. Interestingly, 
the new model is also designed to use 
the GPU on iOS devices to enable 
calculations through the network. This 
takes Apple devices a step ahead of the 
Android models.
A couple of demonstrations put up by 
Baidu on the GitHub repository show its 
different features. The first demonstration 
reveals that MDL is capable of recognising an object in less than a second. The 
second demo shows the project’s ability to identify an object from an image and 
find suitable matches from the Web.
While MDL does support developments made on UC Berkeley’s deep learning 
framework Caffe, Baidu recommends that developers use its PaddlePaddle platform 
that was launched last year. Users can convert their existing PaddlePaddle model 
to an MDL project using simple Python skills. Baidu has also given support for 
projects such as MobileNet, SqueezeNet and GoogLeNet.
MDL is available on GitHub under the MIT licence.
Infosys picks SUSE OpenStack to build private cloud solution
Infosys has launched a private cloud solution by partnering with open source 
pioneer Micro Focus SUSE. The latest development leverages SUSE OpenStack to 
help businesses move towards their digital transformation journey.
The new announcement bolsters the existing relationship between the two 
companies, and positions Infosys as a strategic Global Platinum partner for Micro 
Focus SUSE, backed by more than 500 
architects and consultants with data centre 
transformation and migration capabilities.
“We believe that this collaboration 
will impact businesses positively 
and help them propel their digital 
strategy especially in sectors such as 
financial services, life sciences, retail, 
manufacturing, energy and utilities,” said Ravi Kumar, president and deputy chief 
operating officer, Infosys, in a joint statement.
Infosys and Micro Focus SUSE have jointly invested in a centre of excellence to 
develop validated reference architectures, accelerators and factory-based migration 
solutions. The new partnership will give a boost to that centre and enhance private cloud 
adoption among enterprise customers. “System integrators like Infosys are critical in 
bridging legacy IT with emerging cloud-native technologies using SUSE OpenStack 
Cloud,” said Ronald de Jong, president, sales, Micro Focus SUSE, in a statement.
Infosys is set to enhance its presence in Europe and generate new business and 
insights through SUSE OpenStack adoption. In March this year, the Bengaluru-
headquartered company expanded its focus towards open source by joining the 
Open Invention Network (OIN). That decision will help the company persuade 
clients outside India and make the newly launched private cloud solution 
successful over time.
•   www.lulu.com
•   www.magzter.com
•   Createspace.com
•   www.readwhere.com
14 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

CODE
SPORT
Sandya Mannarswamy
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 15
A
s we have been doing over the last couple of 
months, we will continue to discuss a few 
more interview questions in this month’s 
column as well, focusing on topics in machine 
learning. Let us start off with a simple question. 
1. Let us assume that you have a neural network 
in which there is only an input layer (with a 
single input X being a number) and an output 
layer with a single output neuron. There are no 
hidden layers. You can assume any kind of output 
layer activation, be it linear, sigmoid, softmax, 
tanh or relU. The output needs to be 0 for X < 
6 and 1 for X > 6. How would you design your 
neural network? 
2. Is it possible for a classifier to have both high 
bias and high variance? In that case, would the 
training set error be greater than the test set error 
or vice versa? How can you recognise such a 
situation —whether it is possible for a classifier 
to have both high bias and high variance? 
3. You are working on developing a part of a large 
image classification system of indoor household 
images taken from different cameras placed inside 
the house. Such a system can be part of a smart 
home project or for home security considerations. 
Since many of your customers have pet cats, your 
system needs to be able to recognise cats. You 
have designed a machine learning system that 
can recognise cat images from non-cat images. 
Note that the household cameras being used in the 
setup are quite cheap, and can end up producing 
poor quality images due to variations in lighting, 
camera motion, etc. Since there are zillions of 
cat images on the Internet, you have decided 
to train your system using pet images from the 
Internet, which are typically high quality. You have 
downloaded 100,000 labelled pet images from the 
Internet to train your cat-recognition classifier. You 
have also obtained 5000 home-camera generated 
images, which may include cat images as well. 
You are planning to use 2500 of these 5000 
images as the validation set for tuning the hyper-
parameters. The remaining 2500 images would be 
used as the test set for final evaluation. After you 
have finished developing the initial model and are 
about to start the next step, which is the hyper-
parameter tuning exercise, your friends tell you 
that your methodology is incorrect because your 
training set and validation set come from different 
data distributions and, hence, it is likely that your 
chosen model would perform badly on the test 
set. Would you agree with them? If not, explain 
why. If yes, explain how you would change your 
methodology. Note that you cannot increase 
the amount of indoor household camera image 
data, which is only 5000 images, of which a few 
hundred are cat images. 
4. You are designing a classifier which will analyse 
data relating to Bengaluru traffic fines and 
build a classification system which will predict 
whether the fine for a particular violation will be 
paid or not. The input data set has information 
on the offender, such as the name, residential 
street name, house number, locality pin code, 
whether the person is a repeat offender, age, 
gender, information regarding what type of 
traffic violation occurred, the date of violation, 
where it happened, information regarding the fine 
such as the amount, any late fee, administrative 
charges, etc. Note that the different variables 
in the input data set are of different data types, 
with the type of traffic violation represented in 
string format, the residential street number being 
a numerical quantity, and the amount of the fine 
being a floating point number. The target variable 
you are predicting is a binary variable which is 
1 if the fine will be paid, or 0 if the fine will not 
be paid. You have carefully analysed the data 
In this month’s column, we discuss a few more questions on 
machine learning and deep learning.

CodeSport
Guest Column
16 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
By: Sandya Mannarswamy
The author is an expert in systems software and is currently 
working as a research scientist at Conduent Labs India 
(formerly Xerox India Research Centre). Her interests include 
compilers, programming languages, file systems and natural 
language processing. If you are preparing for systems software 
interviews, you may find it useful to visit Sandya’s  LinkedIn 
group ‘Computer Science Interview Training India’ at http://www.
linkedin.com/groups?home=HYPERLINK “http://www.linkedin.
com/groups?home=&gid=2339182”&HYPERLINK “http://www.
linkedin.com/groups?home=&gid=2339182”gid=2339182
set, and have included both the age of the offender and 
the residential street number as features in your classifier. 
Remember that both of these are numeric quantities. While 
you are confident of your system working well, your friends 
suggest that using the ‘street number’ as the numeric feature 
is incorrect. They suggest that it has to be combined with 
the ‘street name’ and the combined quantity needs to be 
encoded as a ‘categorical’ variable. Do you agree with your 
friends? If yes, explain why. If not, explain why you believe 
their reasoning is incorrect.
5. Let us consider Question (3) from above. You decided to 
add 3000 images from the indoor cameras to the training 
set of 100,000 Internet pet images. You then decided 
to use 1000 images from the indoor cameras as the 
validation/development set for hyper-parameter tuning. 
The remaining 1000 indoor camera images will be used 
as the test set. Given that you have only 3000 images in 
your training set which are actually from the application, 
for which you are building your classifier, you still have 
the issue of training and validation/test data distributions 
being different. If your trained model does not do well on 
the validation/development data set, how would you find 
out whether it is because of under-fitting or not being 
able to generalise well? Essentially, can you come up 
with a method of splitting your input data set so that you 
can find and fix the issue of the classifier under-fitting to 
the training data?
6. One way to address the issue mentioned in Question (5) 
above is to split the training set into two parts, with one 
part being used for training and the other part known as the 
training-dev set, which will be used to evaluate whether 
the classifier can fit well with the data coming from the 
same distribution as the training data. Note that this split of 
‘training-dev’ data set is needed only when the training data 
distribution and validation/test data distribution are different. 
The purpose of the ‘training-dev’ data set is to help determine 
whether the classifier can generalise well to unseen held-out 
data coming from the same distribution as the training data. 
You are told that the training error is 4 per cent and training-
dev error is 9 per cent. You have built your classifier using an 
‘N layer feed forward’ neural network. Now how would you 
improve the performance of the classifier on the training-dev 
set? Would you choose to increase the number of hidden 
layers or would you choose to increase the size of the training 
data? Explain the rationale behind your choice. 
7. In last month’s column, we had discussed full-batch gradient 
descent, mini-batch gradient descent and stochastic gradient 
descent for the optimisation of the cost function of the neural 
networks. In stochastic gradient descent and mini-batch 
gradient descent, the optimisation process can lead to local 
oscillations as it tries to move towards the minimum. This can 
result in the algorithm taking a larger number of steps towards 
convergence. One way of addressing this issue is to modify 
the gradient descent algorithm to include momentum. Gradient 
descent algorithms with momentum ‘RMSProp’ and ‘Adam’ 
are all different variants which address this issue. Basically, 
instead of updating the parameters using only the gradients 
computed in the current iteration, a weighted average of the 
gradients from previous iterations as well is used for the 
update. Do you need gradient descent with momentum if you 
are told that your cost function is convex?  
8. You are planning to consider using mini-batch gradient 
descent with momentum. Should you keep the momentum 
term larger or smaller during the initial part of the 
training? Explain the rationale behind your choice. 
9. Consider the data set presented in question (4) about the 
Bengaluru traffic fine payment prediction problem. This 
data set has a number of features which are on different 
scales. Your friends suggest that you should normalise 
the inputs first. Can you explain the process of input 
normalisation? Does it improve prediction accuracy? If yes, 
explain how. If not, what is the reason behind normalising 
the inputs to a neural network? We all know that weights W 
in a feed forward neural network should not be initialised 
to zero. Can you explain why? Given this constraint, how 
would you decide the initialisation of the weights? Can you 
use any random values?
10.  You have been asked to set up a deep learning system 
that can recognise faces of employees and send signals to 
open the entrance gate to the laboratory, as the employee 
approaches the gate. You have heard a lot about the 
advantages of end-to-end deep learning. In this case, 
if you decide to go with the end-to-end deep learning 
approach, you only provide the image of the employee 
approaching the door and the neural network provides 
an open/close signal output which can be used to drive 
the entrance gate.  The alternative is to employ a pipeline 
approach, wherein the first phase identifies the ‘face part’ 
from the image and puts a bounding box around it. The 
next phase does the face match recognition to check if 
the approaching person is a validated employee of the 
company. Which approach would you choose and explain 
the rationale behind your choice.
If you have any favourite programming questions/
software topics that you would like to discuss on this forum, 
please send them to me, along with your solutions and 
feedback, at sandyasm_AT_yahoo_DOT_com. 

Are you facing these issues
at your workplace?
Business Critical Apps 
Secured, Transparently
Know more about our next generation solutions
Visit www.instasafe.com
Email sales@instasafe.com
Call +918880220044
Enable users to work faster with a 
highly secure, yet simple to use 
secure access solution.
We’ve got you covered!
Hardware free, zero configuration, self-service
Manage remote access securely
Domain joining simplified
MPLS failover made easy
Control Cloud Applications: Google, MS O365, 
ZOHO, Salesforce, etc.

18 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Hewlett Packard, a global leader 
in the printer, laptop and PC space, 
has launched a palm-sized printer 
in India – the Sprocket. This tiny 
printer allows users to take physical 
copies of photographs stored in their 
mobile phones.
Smartphones are now the default 
and instantly available device for 
taking pictures of most important 
events, trips and memorable moments. 
The Sprocket enables users to take 
print copies of images from their 
smartphones – anywhere and at 
any time. It is compatible with both 
Android and iOS devices, and can be 
connected via Bluetooth.
Apart from printing pictures, 
the device can also be fun for kids 
as it can convert images into funky 
stickers. The device uses a special zinc 
technology photo paper, which easily 
Address: Motorola Solutions India 
Pvt Ltd, 415/2, Mehrauli Gurgaon 
Road, Sector 14, Near Maharana Pratap 
Chowk, Gurugram, Haryana 122001
NEW PRODUCTS
Palm-sized pocket 
printer from 
Hewlett Packard
Affordable and 
light Bluetooth 
headphones 
from Motorola
Indian electronics manufacturer 
Ambrane has officially announced 
the launch of a new and affordable 
smartwatch – the ASW11.
It looks like a regular watch, but 
the ASW11 turns into a smartwatch 
and fitness tracker once connected with 
a smartphone or a tablet via Bluetooth. 
The device offers a premium range 
of technologies, and has an LCD 
display with a high resolution and 
good visibility. The multi-functional 
watch allows users to accept or reject 
calls or snooze alarms. It also enables 
the user to create short reminders and 
to-do lists to manage hectic schedules. 
The feature-loaded smartwatch offers 
text messages, email alerts, fitness 
tracking as well as sleep monitoring. 
Ambrane launches  
pocket-friendly smartwatch
Motorola has introduced its latest 
Bluetooth headphones in India, called 
the Motorola Pulse Escape. The light 
and pocket-friendly headphones 
offer the latest features in this price 
segment. Designed with 40mm 
oversized drivers, the headphones 
offer an effective noise isolation 
and immersive music experience. 
The foldable ear-cup design enables 
users to carry them with comfort and 
style. On the battery front, the device 
offers 10 hours of playtime on a 
single charge, and up to two hours of 
playtime with just a 30-minute charge.
The headphones feature A2DP 
(Advanced Audio Distribution Profile), 
HFP (Hands Free Protocol) and AVRCP 
(Audio/Video Remote Control) profiles. 
They are compatible with all Android 
and Apple smartphones and tablets, 
and support the related voice assistants. 
They also come with integrated multi-
point technology that allows them to be 
connected to up to two smartphones at 
the same time. The device offers an 18.2 
metre range, Bluetooth 4.2 connectivity 
and a 3.5mm headphone jack.
The Motorola Pulse Escape 
headphones are available in black and 
white, online and at retail stores.
Address: Ambrane India, C-91/7, 
Wazirpur Industrial Area, Delhi 110052; 
Ph: 011-48089900 
fits into the printer for 5cm x 7.6cm (2 
inch x 3 inch) photos. Thus the printer 
allows users to turn their memories and 
events into brightly coloured, durable, 
smudge-proof, water-resistant and tear-
resistant photos.
The Sprocket is available in black, 
white and red, online and at HP stores.
Address: HP India Sales Pvt Ltd, 6th 
Floor, Tower D, Global Business Park, 
Mehrauli-Gurgaon Road, Gurugram, 
Haryana 122002; Ph: 088266-13522
Price:  
 ` 1,999
Apart from these features, the watch 
can also be used to manage music, and 
comes with an inbuilt pedometer and 
‘sedentary reminder’.
The Ambrane ASW11 is available in 
‘universal black’ online and at retail stores.
Price:  
 ` 8,999
Price:  
 ` 2,999

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 19
Portronics launches multi-functional 
portable projector
The prices, features and specifications are based on information provided to us, or as available 
on various websites and portals. OSFY cannot vouch for their accuracy. 
Compiled by: Aashima Sharma
Digisol’s wireless security camera
A provider of digital and portable 
solutions, Portronics has launched its 
latest projector, called Progenie. The 
device is a 100-lumen multimedia 
portable LED projector offering a 
203cm (80 inch) screen and an 854 
x 480 pixels resolution. It uses Texas 
Instruments’ DLP technology to 
ensure excellent picture quality.
Address: Portronics Digital Private 
Limited, 4E/14, Azad Bhavan, 
Jhandewalan, New Delhi 110055; 
Ph: 09555245245
Address: Digisol Systems Ltd, Smartlink 
House, Plot No. 5, Bandra-Kurla Complex 
Road, Santa Cruz (E), Mumbai 400098
A subsidiary of Smartlink Network 
Systems Ltd and a leading player 
in the networking and CCTV 
surveillance space, Digisol has 
launched a new wireless pan/tilt IP 
camera – the DG-SC3600W.
The camera comes with all 
the mounting accessories needed 
for easy installation on walls 
and ceilings.
The plug-and-play camera uses 
an application on a smartphone 
which connects with existing 
Wi-Fi/Internet connections, via 
which the surveillance can be 
done online through the cloud. 
It has a microSD card slot for 
storage of up to 64GB, on which 
the recordings are stored and can 
be retrieved using the application. 
Price:  
 ` 4,999
With an extremely portable 
size of just over 7.6cm x 7.6cm x 
3.8cm (3” x 3” x 1.5”), the device 
can project crystal clear multimedia 
content from a variety of sources.
It is useful for homes, offices 
as well as gaming due to its screen 
size. It can easily be connected to 
a DVD player, microSD card, USB 
drive and HDMI enabled device 
like laptops, streaming dongles, 
etc. The device comes with a 
multi-functional remote control 
with the usual functions like play, 
pause, stop, zoom, freeze, etc, for 
comfortable and hassle-free usage.
The projector works on 
110V-240V 50/60Hz AC power and 
comes with a built-in fan that keeps 
Price:  
 ` 25,599
the temperature under control. With 
the LED’s life span going up to 20,000 
hours, the Progenie allows the user to 
charge 5V devices like Android, iOS 
or Windows smartphones, cameras, 
etc, from a USB port.
Weighing just 200g, the user can 
easily carry the printer around in a 
small handbag or a laptop bag.
The Portronics Progenie is packed 
with an HDMI cable, AVI cable and 
a tripod. It is available at both online 
and offline stores.
Apart from these features, the camera 
comes with IR LED technology for 
clear night vision up to a 10-metre 
distance, enabling the camera to ‘see’ 
in absolute darkness.
The pan/tilt function gives 
maximum coverage of the area under 
surveillance. The highly flexible camera 
allows users to remotely monitor areas 
from a computer and smartphone. 
Additionally, the camera comes with 1 
megapixel resolution support, a free app 
for Apple iOS and Android for cloud 
viewing, a built-in microphone and 
speaker for two-way audio, email/app 
alerts and motion based recording.
The Digisol DG-SC3600W is 
available at retail stores.

Prabhakar Jayakumar, 
country manager,  
DigitalOcean
“DIGITALOCEAN 
IS BUILT ON A 
FOUNDATION OF
 OPEN SOURCE”
As open source is making things significantly better for 
developers, cloud service providers have started using 
the power of the open source community to grow their 
presence. DigitalOcean, a New York-headquartered 
company, founded in June 2011, is among the leading 
hosting companies worldwide and is one of the few 
cloud providers to have deployed open source-based 
technologies predominantly. In an exclusive conversation 
with Jagmeet Singh of OSFY, Prabhakar 
Jayakumar, country manager, DigitalOcean, 
highlights the key open source initiatives of the 
company. Edited excerpts:
Q
What does DigitalOcean 
have for developers 
around the globe?
DigitalOcean gives developers better 
software and tools to launch and scale any 
application in the cloud. By offering the 
simplest platform, the most transparent 
pricing and an elegant user interface, we 
reduce complexity so developers can build 
great software – faster.
Q
From the available portfolio, 
what are the major open 
source-focused developments by 
DigitalOcean?
DigitalOcean is built on a foundation of 
open source. Whether you look at our 
hypervisors that are running kernel-based 
virtual machines (KVM) or our Rails apps 
running on our Kubernetes clusters, they 
are all open source-powered stacks. On 
top of that, our users are deploying open 
source based solutions to our platform 
every single day.
One of our core values as a company 
is that our community is bigger than us. 
We provide hosting credits to many open 
source projects, host several meetups in 
our office and even support them across 
the world. We are also members of 
organisations such as the Cloud Native 
Foundation and Ruby Together to help 
support the technologies we use.
Additionally, Hacktoberfest is a yearly 
initiative we produce in partnership with 
GitHub to generate excitement around 
contributing to open source in the month 
of October. The aim of this initiative is 
to inspire developers to contribute back 
to the projects they use. Last year, over 
10,000 people completed the challenge 
opening up over 92,000 pull-requests to 
over 29,000 repos.
Q
Do you feel it is difficult to 
support open source being a 
cloud service provider?
No, it is not difficult. 
For U & Me
Interview
20 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Interview

Q
What was the idea behind 
bringing DigitalOcean to a 
market that already has Amazon 
Web Services, Google Cloud and 
Microsoft Azure?
When we launched back in 2012, the 
other cloud providers were neglecting the 
software developer and focused on the 
enterprise/legacy customers. Prices were 
too high, the offerings were too complex 
to navigate and the UI was unnecessarily 
complex. We wanted to provide an 
experience for developers around the 
cloud that was simple and intuitive, at the 
best price-to-performance value.
Q
How is the marketing model 
adopted by DigitalOcean 
different from the ones opted 
by market leaders like Amazon, 
Google and Microsoft?
As a company, we are focused on 
collaborating and contributing to 
developer communities around the 
world. We drive activities ranging from 
workshops and hackathons, to coding 
contests, webinars and conferences to 
engage with the developer community. 
Our meetup chapters are operational 
across more than 60 cities globally, and 
the local developer communities use 
these forums to share best practices and 
learn from subject matter experts on a 
wide range of technical topics, including 
but not limited to the cloud and DevOps. 
Our community site also provides 
extensive tutorials and guides for 
developers learning to work with Linux 
and those seeking to create complex set-
ups within their infrastructures.
Q
Why does DigitalOcean 
primarily focus on developers 
instead of targeting enterprises?
As a company that predominantly 
comprises software developers, we 
understand and empathise with the pain 
points of the developer community. When 
we started in 2012, we found a market 
gap that had to be addressed. Developers 
wanted to be relieved of the burdens 
around infrastructure management, and 
have the complexity of wrestling with 
complicated bills and unwieldy APIs 
developers as much as it does for their 
counterparts in other parts of the globe.
Q
How do you manage to fulfil the 
ever-growing cloud demand in 
the Indian market?
Not only do we plan and commission our 
infrastructure to meet the growing demand 
of our customers but we also work towards 
making our latest offerings, such as block 
storage and load balancers, available in our 
India data centre.
Q
What are the prime hindrances 
for Indian developers opting for 
cloud-first platforms?
While the cloud is the new reality and 
developers are already aware of the benefits 
that a cloud platform could bring to their 
business, there are some misconceptions 
around security and platform availability 
(up times). However, these are unfounded 
concerns and these misconceptions have 
largely been debunked. 
Q
What are the key skills a 
developer requires to pick a 
solution from DigitalOcean?
At DigitalOcean, our mission is to 
simplify infrastructure and create an 
experience that developers love. There 
are no technical prerequisites for using 
our platform. In fact, we have customers 
with little to no knowledge of technology, 
using our platform to perform a variety of 
tasks including hosting their websites or 
blogs. For those who need help, we have 
thousands of tutorials that provide detailed 
instructions on using our platform for a 
wide range of use cases. 
Q
What do you think about the 
future of the cloud in India?
India is home to the fastest growing 
ecosystem for startups and entrepreneurs. 
With the number of software developers 
throughout India expected to grow to over 
five million over the next couple of years, 
India is poised to become the largest 
developer market in the world. We’re 
focused on making it easier than ever 
before for professional developers from 
India, and around the world, to deploy and 
scale their businesses. 
removed from their development process. 
Moreover, we focus on doing the heavy-
lifting on the infrastructure side, letting 
developers channelise their energies 
towards building applications that solve 
real customer problems.
Q
What are the vital challenges 
you face while serving 
various cloud solutions in India?
There are certain nuances to setting 
up a data centre facility in India. But 
the fact that we are able to provide an 
infrastructure experience to software 
developers in the country that matches 
the quality of our services in New York 
or Singapore speaks of the developments 
that India has seen over the years.
Despite our operational costs 
being a little higher in India compared 
to other locations where we operate 
our data centres, we are maintaining 
a single pricing plan across all our 
regions worldwide. This means that our 
SSD-enabled cloud servers (known as 
Droplets) as well as all other services, 
such as load balancers, have identical 
pricing across geographies.
Q
Do you see any growth 
potential for a cloud 
solutions provider like 
DigitalOcean in the country?
Absolutely. In fact, India is the fastest 
growing international market for 
DigitalOcean. Our value proposition 
of simplicity, transparent pricing and 
robust performance resonates with Indian 
With the number of 
software developers 
throughout India 
expected to grow to 
over five million over 
the next couple of 
years, India is poised 
to become the largest 
developer market in 
the world.
For U & Me
Interview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 21

Using Ansible for Managing 
VMware Infrastructure
This article is an introduction to different Ansible modules, which can be 
used to manage various VMware objects in VMware infrastructure.
A
nsible is an IT automation tool. It can configure 
systems, deploy software and orchestrate 
advanced IT tasks such as continuous deployments 
or zero downtime rolling updates. Many of you already 
know that you can manage your VMware infrastructure 
using Ansible.
A recent development branch of Ansible, i.e., 2.4-devel 
(https://github.com/ansible/ansible/tree/devel), has 
brought in a lot of improvements in both its performance 
and its reliability.
Managing the ESXi host with different tools
The vSphere Client and the vSphere Web Client provide 
simple ways to manage your ESXi host and operate its 
virtual machines. 
Another way of managing this is the 
Virtualisation Management Object Management 
Infrastructure’s (VMOMI) network exposed API. 
VMOMI can be used by various programming 
language bindings like PyVmomi (https://github.com/
vmware/pyvmomi).
Getting started with Ansible
You can read more about Ansible official 
documentation at http://docs.ansible.com. Also, Open 
Source For You has covered various Ansible related 
topics in past editions.
Let us discuss a few Ansible VMware modules 
that are essential for managing and administrating 
various VMware objects.
Admin
How To
22 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 23
Admin
How To
Managing virtual machines
vmware_guest: This module is required in order to manage 
a virtual machine in your vCenter or standalone ESXi 
environment. The module can create, modify, rename and 
remove a virtual machine. It can also power on, power off, 
restart and suspend a virtual machine.
The following Ansible Playbook example shows how to 
create a virtual machine:
- name: Example Playbook for creating virtual machine
  hosts: localhost
  tasks:
    - name: Create a virtual machine with name testvm01
      vmware_guest:
        datacenter: datacenter01
        esxi_hostname: esxi_host_01
        hostname: 127.0.0.1
        name: testvm01
        guest_id: centos64guest
        username: user
        password: pass@123
        state: present
        disk:
          - size_gb: 1
            type: thin
            datastore: LocalDS_0
        hardware:
            memory_mb: 512
            num_cpus: 1
            osid: centos64guest
            scsi: paravirtual
You can read more about this module at http://docs.
ansible.com/ansible/latest/vmware_guest_module.html.
vmware_guest_facts: You often want to get details 
about a virtual machine like its IP address, folder path, MAC 
address, etc. This module does exactly that. vmware_guest_
facts gives you all the vital information about a virtual 
machine. You can chain the information received from this 
module to various other VMware modules.
---
- name: Example playbook to get facts about virtual machine
  hosts: localhost
  tasks:
    - name: get list of facts about virtual machines
      vmware_guest_facts:
        validate_certs: False
        hostname: esxi_hostname_01
        username: user
        password: pass@123
        datacenter: datacenter_01
        name: testvm01
        folder: /vm
      register: guest_facts_0001
    - debug: msg=”{{ guest_facts_0001 }}”
You can read more about this module at http://docs.
ansible.com/ansible/latest/vmware_guest_facts_module.html.
vmware_guest_find: This module provides information 
about the folder path in which the virtual machine is located. 
This information is required in various other VMware modules.
The following example will find the folder path related to 
a virtual machine.
---
- name: Example playbook to find the details of folder related 
to virtual machine
  hosts: localhost
  tasks:
    - name: Get folder path for virtual machine
      vmware_guest_find:
        validate_certs: false
        hostname: 127.0.0.1
        username: user
        password: pass
        name: testvm01
        datacenter: datacenter_01
      register: folder_details
- debug: msg=”{{ folder_details }}”
You can read more about this module at https://github.
com/ansible/ansible/blob/devel/lib/ansible/modules/cloud/
vmware/vmware_guest_find.py.
vmware_guest_snapshot: When we want to preserve 
the state of the virtual machine at a given point of time, we 
take a snapshot of the virtual machine. This module provides 
the functionality to manage various operations related to 
a snapshot such as create, delete, revert and restore. This 
module requires information on the folder path along with 
the virtual machine name. Here, we can use the information 
provided by vmware_guest_find on the folder path.
The following playbook example will show how to take a 
simple snapshot of a virtual machine:
---
- name: Example to take a snapshot of virtual machine
  hosts: localhost
  tasks:
    - name: Take a snapshot
      vmware_guest_snapshot:
        hostname: esxi_hostname_01
        username: root
        password: esxi@123
        name: testvm01
        folder: /vm
        snapshot_name: my_secure_stable_snapshot
      register: snap_state_001
    - debug: msg=”{{ snap_state_001 }}”

24 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
How To
You can read more about this module at http://docs.ansible.
com/ansible/latest/vmware_guest_snapshot_module.html.
Managing clusters
vmware_cluster: This module allows you to manage a cluster 
in your VMware infrastructure. It also allows you to set 
various cluster properties such as HA, vSAN and DRS.
The following example will add a sample cluster in a 
given VMware infrastructure:
---
- name: Example playbook to add a cluster
  hosts: localhost
  tasks:
    - name: Add cluster
      vmware_cluster:
        validate_certs: false
        hostname: esxi_hostname_01
        username: root
        password: esxi@123
        datacenter_name: datacenter_01
        cluster_name: esx_cluster_002
      register: new_cluster_details
    - debug: msg=new_cluster_details
You can read more about this module at http://docs.
ansible.com/ansible/latest/vmware_cluster_module.html.
Managing data centres
vmware_datacenter: This module allows you to manage a 
data centre in your VMware infrastructure. You can read more 
about this module at http://docs.ansible.com/ansible/latest/
vmware_datacenter_module.html.
[1]  https://github.com/ansible/ansible/tree/devel
[2]  http://docs.ansible.com/ansible/latest/vmware_guest_
module.html
[3]  http://docs.ansible.com/ansible/latest/vmware_guest_
facts_module.html
[4] https://github.com/ansible/ansible/blob/devel/lib/
ansible/modules/cloud/vmware/vmware_guest_find.py
[5]  http://docs.ansible.com/ansible/latest/vmware_guest_
snapshot_module.html
[6]  http://docs.ansible.com/ansible/latest/vmware_cluster_
module.html
[7]  http://docs.ansible.com/ansible/latest/vmware_
datacenter_module.html
[8]  http://docs.ansible.com/ansible/latest/list_of_cloud_
modules.html#vmware
[9]  https://github.com/ansible/ansible/blob/devel/hacking/
README.md
References
By: Abhijeet Kasurde
The author works at Red Hat and is a FOSS evangelist. He loves 
to explore new technologies and software. You can contact him 
at abhijeetkasurde21@gmail.com.
Managing VMware networks
You can manage various VMware network objects such 
as a distributed virtual switch, virtual switch, port group, 
distributed virtual switch port group, etc.
You can read more about these various modules at 
http://docs.ansible.com/ansible/latest/list_of_cloud_modules.
html#vmware.
Ansible VMware modules are undergoing heavy 
development; you can take active part in this by developing 
or testing various modules. You can read more about hacking 
Ansible modules at https://github.com/ansible/ansible/blob/
devel/hacking/README.md. 
MONTH
THEME
March 2017
Open Source Firewall, Network security and Monitoring
April 2017
Databases management and Optimisation
May 2017
Open Source Programming (Languages and tools)
June 2017
Open Source and IoT
July 2017
Mobile App Development and Optimisation
August 2017
Docker and Containers
September 2017
Web and desktop app Development
October 2017
Artificial Intelligence, Deep learning and Machine Learning
November 2017
Open Source on Windows
December 2017
BigData, Hadoop, PaaS, SaaS, Iaas and Cloud
January 2018
Data Security, Storage and Backup
February 2018
Best in the world of Open Source (Tools and Services)
OSFY Magazine Attractions During 2017-18


Admin
How To
26 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
L
inux and Windows are two of the most popular operating 
systems in the market. One can say that these two OSs 
have become essentials because of their continuous 
development and feature enhancements. Many people prefer 
one over the other. Yet, people who use Linux sometimes 
need the Windows operating system, and vice versa. For faster 
development and more security, people prefer to use Linux over 
Windows. So these people need Linux alongside Windows.
A common solution for this problem is to use a dual boot, 
which basically installs the entire operating system. However, 
due to boot loader changes (like UEFI mode), configuring 
dual boot becomes difficult. 
This article is about running Linux on Windows, without 
using the dual boot technique. There are different ways to do 
so, one of the most popular being using a virtual machine. 
Virtual machines 
Virtualisation is fairly old technology now. It was started 
around 1980 by IBM. So, currently, this technology is pretty 
stable. We can create Linux virtual machines in Windows 
using software like Virtual Box, VMware Player, and 
VMware Workstation.
If you would like to run Linux on Windows and have never used a virtual machine, this 
tutorial explains how you can do so in a few simple steps. 
A virtual machine provides the same functionality as 
a physical computer, but it is an emulation of a physical 
computer. The virtual machine can be created or destroyed 
at any moment and it will not impact the actual operating 
system, which is the best part about using it.
To install Linux on Windows, we just require two things 
— the software to create a virtual machine which is Virtual 
Box (free software) and an ISO file of the operating system.
The installation process is pretty simple, like installing 
any software in Windows. The steps for installing Ubuntu (a 
favourite Linux flavour) inside Windows are given below.
The first step is to enable virtualisation technology in the 
BIOS. If you have already enabled it, you can skip this step. 
You need to power on the system and go to the BIOS 
setup by pressing F2, Del or the Enter key, depending  upon 
your computer model.
After that, under CPU Configuration -> System 
Configuration -> Advanced or Security Tab, we need to find 
Virtualization Technology or Intel Virtualization Technology. 
If the option is disabled, then it must be enabled. If it’s 
already enabled, skip this step.
If you get any of the following error messages at any point 
Using a Virtual Machine  
to Run Linux on Windows


Admin
How To
28 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
of time, it means that virtualisation technology is not enabled 
for you and you need to enable it:
 
VT-x/AMD-V hardware acceleration is not available on 
your system
 
This host supports Intel VT-x, but Intel VT-x is disabled
 
The processor on this computer is not compatible with 
Hyper-V
If you have not installed Virtual Box, then you can go 
to the links https://www.virtualbox.org/wiki/Downloads 
and http://download.virtualbox.org/virtualbox/5.1.28/
VirtualBox-5.1.28-117968-Win.exe, and download and 
install it. The steps are very straightforward; hence, I am not 
mentioning them here.
Open the Virtual Box application once it is installed. It 
will look like what’s shown in the snapshot in Figure 1.
Click on the New button and select the OS category before 
providing the appropriate name to the virtual machine. 
Assign RAM as per your requirements. Ubuntu can run 
on 512MB, which is the minimum requirement to run Linux 
smoothly on Windows. Since I have 8GB RAM in my laptop, 
I have chosen 2048MB (2GB) RAM.
Figure 1: Virtual machine home
Figure 3: Assigning RAM
Figure 2: Creating a virtual machine
Figure 4: Select virtual hard disk
Figure 5: Select virtual hard disk type
The next option is to select the virtual hard disk. Since we 
are creating a new virtual machine, we will use the ‘Create 
virtual hard disk now’ option.
Here we need to select the virtual hard disk type. Since 


Admin
How To
30 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
By: Maulik Parekh
The author has an M. Tech degree in cloud computing 
from VIT University, Chennai. He can be reached at 
maulikparekh2@gmail.com. Website: https://www.linkedin.
com/in/maulikparekh2.
[1] https://www.virtualbox.org/wiki/Downloads
Reference
Figure 6: Disk space
Figure 9: Install Ubuntu like any normal OS
Figure 7: Selecting the ISO file
Figure 8: Installation started
we are using Virtual Box, we will use VDI which is a 
Virtual Box disk image. 
There are two options here — we can either use a fixed 
size or a dynamic size. It is better to opt for the latter so that 
the size can be increased if required.
Allocate the disk space as per your requirement.
Once it is done, go to the virtual machine that you have 
created, right-click on it and go to the settings of that machine. 
Now we must attach the ISO file so that while we run the VM, 
it should know the location of the Ubuntu ISO file. As shown in 
the snapshot (Figure 7), click on the DISK button and browse 
the .ISO file of Ubuntu OS. The ISO file is now attached.
Next, go to the virtual machine, right-click on it and start it.
You will get the Ubuntu installation screen, which is 
similar to installing any operating system that you have chosen. 
The remaining steps are similar to the installation of any OS.
If you want to close the virtual machine window, you can 
do one of three things.
1. Save the machine:  This is like hibernate. The system will 
resume from wherever you left off, the next time when you 
start — it will show you the same task at which you paused. 
2. Send the ‘Shutdown’ signal:  This is like pressing the power 
button. It will allow normal shutdown of the virtual machine.
3. Power off the machine: This is like pulling a power cable. 
It is for a hard shutdown which is required in scenarios 
where a VM gets hung or is not working properly.
At any time, if you want to destroy the virtual machine 
you created,  right-click and select the Remove option. This 
will remove all the related files.
In this article, we have seen the basic operations of 
a virtual machine. We can explore other options in a 
subsequent one. 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 31
Admin
How To
P
rovisioning is the first step in an application’s 
deployment process. In a cloud environment, software 
can be run from a Docker container, virtual machine 
or bare metal, and Ansible can be used for provisioning such 
systems. In this article, we explore how to use Ansible to 
launch Docker containers and provision virtual machines.
Setting it up 
Let’s create an Ansible playbook for the ‘Get started 
with Docker Compose’ composetest example available 
at https://docs.docker.com/compose/gettingstarted/. The 
Ansible version used on the host system (Ubuntu x86_64) 
is 2.2.0.0. You will need to install Docker CE and docker-
compose on Ubuntu. Follow the installation guide provided 
at https://docs.docker.com/engine/ installation/linux/
docker-ce/ubuntu/#install-using-the-repository to install 
Docker CE. You can then install docker-compose using the 
APT package manager: 
$ sudo apt-get install docker-compose 
The composetest/ folder consists of the following files: 
composetest/app.py
composetest/docker-compose.yml
composetest/Dockerfile
composetest/provision.yml
DevOps Series: 
Provisioning with Ansible
Ansible is the simplest way to automate apps and IT infrastructure. It meshes well with 
DevOps to deploy apps. In this ninth article in the series on DevOps, we explore the use 
of Ansible for launching Docker containers and provisioning virtual machines.
composetest/requirements.txt
The app.py file contains a basic Flask application that 
communicates with a backend Redis database server. Its file 
contents are as follows: 
from flask import Flask
from redis import Redis
app = Flask(__name__)
redis = Redis(host=’redis’, port=6379)
@app.route(‘/’)
def hello():
    count = redis.incr(‘hits’)
    return ‘Hello World! I have been seen {} times.\n’.
format(count)
if __name__ == “__main__”:
    app.run(host=”0.0.0.0”, debug=True)
An HTTP request to the Flask application returns the text 
string ‘Hello World! I have been seen N times.’ This will be 
run inside a Docker container. The requirements.txt file is 
provided to list the dependencies required for the project: 
flask

Loonycorn 
 
 
 
 
Some of our recent courses:  
 
On Udemy: 
 
 Building Voice-Enabled Apps with Amazon Alexa 
 
 Building ChatBots with Amazon Lex 
 
 GCP: Complete Google Data Engineer and Cloud Architect 
Guide  
 
On Pluralsight: 
  
 Unsupervised Machine Learning with TensorFlow 
 
 Classification Models in TensorFlow 
 
 Understanding the Foundations of TensorFlow 
 
 
 
 

Loonycorn 
Our Content: 
 
 The Ultimate Computer Science Bundle 
9 courses |  139 hours  
 
 The Complete Machine Learning Bundle 
 
10 courses |  63 hours 
 
 The Complete Computer Science Bundle 
8 courses  |  78 hours 
 
 The Big Data Bundle 
9 courses  |  64 hours 
 
 The Complete Web Programming Bundle 
 
8 courses  |  61 hours 
 
 The Complete Finance & Economics Bundle 
9 courses  |  56 hours 
 
 The Scientific Essentials Bundle 
 
7 courses  |  41 hours 
 
 ~30 courses on Pluralsight  
 
~80 on StackSocial 
~75 on Udemy 
 
About Us: 
 
 ex-Google | Stanford | INSEAD 
 80,000+ students  

34 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
How To
redis
Now let’s provision a minimalistic Docker container 
that has support for Python and is based on Alpine (a 
security-oriented, lightweight GNU/Linux distribution). The 
Dockerfile for the application is provided below for reference: 
FROM python:3.4-alpine
ADD . /code
WORKDIR /code
RUN pip install -r requirements.txt
CMD [“python”, “app.py”]
The Docker-compose.yml file is used to create the images 
and will also be used by Ansible. It defines the services that 
will be deployed in the containers: 
version: ‘2’
services:
  web:
    build: .
    ports:
     - “5000:5000”
  redis:
    image: “redis:alpine”
    ports:
      - “6379:6379”
Provisioning 
The Python Web application will be running on port 5000, 
whereas the Redis database server will be listening on 
port 6379. We will first build the application using the 
following code: 
$ docker-compose up
Creating composetest_web_1
Creating composetest_redis_1
Attaching to composetest_web_1, composetest_redis_1
redis_1  | 1:C 05 Oct 11:40:49.067 # oO0OoO0OoO0Oo Redis is 
starting oO0OoO0OoO0Oo
redis_1  | 1:C 05 Oct 11:40:49.067 # Redis version=4.0.2, 
bits=64, commit=00000000, modified=0, pid=1, just started
redis_1  | 1:C 05 Oct 11:40:49.067 # Warning: no config file 
specified, using the default config. In order to specify a 
config file use redis-server /path/to/redis.conf
redis_1  | 1:M 05 Oct 11:40:49.070 * Running mode=standalone, 
port=6379.
redis_1  | 1:M 05 Oct 11:40:49.070 # WARNING: The TCP backlog 
setting of 511 cannot be enforced because /proc/sys/net/core/
somaxconn is set to the lower value of 128.
redis_1  | 1:M 05 Oct 11:40:49.070 # Server initialized
redis_1  | 1:M 05 Oct 11:40:49.070 # WARNING overcommit_
memory is set to 0! Background save may fail under low memory 
condition. To fix this issue add ‘vm.overcommit_memory = 1’ to 
/etc/sysctl.conf and then reboot or run the command ‘sysctl 
vm.overcommit_memory=1’ for this to take effect.
redis_1  | 1:M 05 Oct 11:40:49.070 # WARNING you have 
Transparent Huge Pages (THP) support enabled in your kernel. 
This will create latency and memory usage issues with Redis. 
To fix this issue run the command ‘echo never > /sys/kernel/
mm/transparent_hugepage/enabled’ as root, and add it to your 
/etc/rc.local in order to retain the setting after a reboot. 
Redis must be restarted after THP is disabled.
redis_1  | 1:M 05 Oct 11:40:49.070 * Ready to accept 
connections
web_1    |  * Running on http://0.0.0.0:5000/ (Press CTRL+C 
to quit)
web_1    |  * Restarting with stat
web_1    |  * Debugger is active!
web_1    |  * Debugger PIN: 100-456-831
If you start a browser on the host system and open the 
URL http://0.0.0.0:5000, you will see the text from the 
Flask application. You can continue to refresh the page 
making requests to the application, and you will see the 
count increasing in the text: ‘Hello World! I have been seen 
N times.’ Pressing Ctrl+c in the above terminal will stop 
the application. Let’s now create an Ansible playbook to 
launch these containers: 
- name: Provision Flask application
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  tags: [setup]
  tasks:
  - docker_service:
      project_name: composetest
      definition:
        version: ‘2’
        services:
          web:
            build: “{{ playbook_dir }}/.”
            ports:
              - “5000:5000”
          redis:
            image: “redis:alpine”

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 35
Admin
How To
    register: output
  - debug:
      var: output
  - assert:
      that:
        - “web.composetest_web_1.state.running”
- “redis.composetest_redis_1.state.running”
The above playbook can be invoked as follows: 
$ sudo ansible-playbook provision.yml --tags setup
The docker_service module is used to compose the 
services—a Web application and a Redis database server. The 
output of launching the containers is stored in a variable and 
is used to ensure that both the backend services are up and 
running. You can verify that the containers are running using 
the docker ps command output as shown below: 
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  
CREATED             STATUS              PORTS                    
NAMES
03f6f6a3d48f        composetest_web     “python app.py”          
18 seconds ago      Up 17 seconds       0.0.0.0:5000->5000/
tcp   composetest_web_1
fa00c70da13a        redis:alpine        “docker-
entrypoint...”   18 seconds ago      Up 17 seconds       
6379/tcp                 composetest_redis_1
Scaling 
You can use the docker_service Ansible module to increase 
the number of Web services to two, as shown in the following 
Ansible playbook:
 
- name: Scale the web services to 2
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  tags: [scale]
  tasks:
    - docker_service:
        project_src: “/home/guest/composetest”
        scale:
          web: 2
      register: output
    - debug:
        var: output        
    - name: Start container two
      docker_container:
        name: composetest_web_2
        image: composetest_web
        state: started
        ports:
          - “5001:5000”
        network_mode: bridge
        networks:
          - name: composetest_default
            ipv4_address: “172.19.0.12”
The above playbook can be invoked as follows: 
$ sudo ansible-playbook provision.yml --tags scale
The execution of the playbook will create one more Web 
application server, and this will listen on Port 5001. You can 
verify the running containers as follows: 
$ docker ps
CONTAINER ID   IMAGE            COMMAND      CREATED             
STATUS         PORTS                 NAMES
66b59eb163c3   composetest_web “python app.py” 9 seconds ago       
Up 8 seconds   0.0.0.0:5001->5000/tcp   composetest_web_2
4e8a37344598   redis:alpine  “docker-entrypoint...” 11 
seconds ago    Up 10 seconds   0.0.0.0:6379->6379/tcp   
composetest_redis_1
03f6f6a3d48f composetest_web   “python app.py” 55 seconds ago      
Up 54 seconds  0.0.0.0:5000->5000/tcp   composetest_web_1
You can open another tab in the browser with the URL 
http://localhost:5001 on the host system, and the text count will 
continue to increase if you keep refreshing the page repeatedly. 
Cleaning up 
You can stop and remove all the running instances. First, stop 
the newly created Web application, as follows: 
$ docker stop 66b
You can use the following Ansible playbook to stop the 
containers that were started using Docker compose:
 
- name: Stop all!
  hosts: localhost
  connection: local
  become: true
  gather_facts: true
  tags: [stop]
  tasks:
    - docker_service:
        project_name: composetest

36 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
How To
        project_src: “{{ playbook_dir }}/.”
        state: absent
The above playbook can be invoked using the 
following command: 
$ sudo ansible-playbook provision.yml --tags stop
You can also verify that there are no containers running in 
the system, as follows: 
$ docker ps
CONTAINER ID    IMAGE         COMMAND             CREATED             
STATUS          PORTS         NAMES
Refer to the Ansible docker_service module’s 
documentation at http://docs.ansible.com/ansible/ latest/
docker_service_module.html for more examples and options. 
Vagrant and Ansible 
Vagrant is free and open source software (FOSS) that helps 
to build and manage virtual machines. It allows you to create 
machines using different backend providers such as VirtualBox, 
Docker, libvirt, etc. It is developed by HashiCorp and is written 
in the Ruby programming language. It was first released in 
2010 under an MIT licence. The Vagrantfile describes the 
virtual machine using a Ruby DSL, and an Ansible playbook 
can be executed as part of the provisioning process. 
The following dependencies need to be installed on the 
host Ubuntu system: 
$ sudo apt-get build-dep vagrant ruby-libvirt
$ sudo apt-get install qemu libvirt-bin ebtables dnsmasq 
virt-manager
$ sudo apt-get libxslt-dev libxml2-dev libvirt-dev zlib1g-dev 
ruby-dev 
Vagrant 1.8.7 is then installed on Ubuntu using a .deb 
package obtained from the https://www.vagrantup.com/ 
website. Issue the following command to install the vagrant-
libvirt provider: 
$ vagrant plugin install vagrant-libvirt
The firewalld daemon is then started on the host 
system, as follows: 
$ sudo systemctl start firewalld
A simple Vagrantfile is created inside a test directory to 
launch an Ubuntu guest system. Its file contents are given 
below for reference: 
# -*- mode: ruby -*-
# vi: set ft=ruby :
Vagrant.configure(“2”) do |config|
  config.vm.define :test_vm do |test_vm|
    test_vm.vm.box = “sergk/xenial64-minimal-libvirt”
  end
  config.vm.provision “ansible” do |ansible|
    ansible.playbook = “playbook.yml”
  end
end
When provisioning the above Vagrantfile, a minimalistic 
Xenial 64-bit Ubuntu image is downloaded, started and the 
Ansible playbook is executed after the instance is launched. 
The contents of the playbook.yml file are as follows: 
---
- hosts: all
  become: true
  gather_facts: no
  pre_tasks:
    - name: Install python2
      raw: sudo apt-get -y install python-simplejson
  tasks:
    - name: Update apt cache
      apt: update_cache=yes
    - name: Install Apache
      apt: name=apache2 state=present
The minimal Ubuntu machine has Python 3 by default, 
and the Ansible that we use requires Python 2. Hence, we 
install Python 2, update the APT repository and install the 
Apache Web server. A sample debug execution of the above 
playbook from the test directory is given below: 
$ VAGRANT_LOG=debug sudo vagrant up --provider=libvirt
Bringing machine ‘test_vm’ up with ‘libvirt’ provider...
==> test_vm: Creating image (snapshot of base box volume).
==> test_vm: Creating domain with the following settings...
==> test_vm:  -- Name:              vagrant-libvirt-test_
test_vm
==> test_vm:  -- Domain type:       kvm
==> test_vm:  -- Cpus:              1
==> test_vm:  -- Feature:           acpi
==> test_vm:  -- Feature:           apic
==> test_vm:  -- Feature:           pae
==> test_vm:  -- Memory:            512M
==> test_vm:  -- Management MAC:    
==> test_vm:  -- Loader:            

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 37
Admin
How To
==> test_vm:  -- Base box:          sergk/xenial64-minimal-
libvirt
==> test_vm:  -- Storage pool:      default
==> test_vm:  -- Image:             /var/lib/libvirt/images/
vagrant-libvirt-test_test_vm.img (100G)
==> test_vm:  -- Volume Cache:      default
==> test_vm:  -- Kernel:            
==> test_vm:  -- Initrd:            
==> test_vm:  -- Graphics Type:     vnc
==> test_vm:  -- Graphics Port:     5900
==> test_vm:  -- Graphics IP:       127.0.0.1
==> test_vm:  -- Graphics Password: Not defined
==> test_vm:  -- Video Type:        cirrus
==> test_vm:  -- Video VRAM:        9216
==> test_vm:  -- Sound Type: 
==> test_vm:  -- Keymap:            en-us
==> test_vm:  -- TPM Path:          
==> test_vm:  -- INPUT:             type=mouse, bus=ps2
==> test_vm: Creating shared folders metadata...
==> test_vm: Starting domain.
==> test_vm: Waiting for domain to get an IP address...
==> test_vm: Waiting for SSH to become available...
    test_vm: 
    test_vm: Vagrant insecure key detected. Vagrant will 
automatically replace
    test_vm: this with a newly generated keypair for better 
security.
    test_vm: 
    test_vm: Inserting generated public key within guest...
    
test_vm: Removing insecure key from the guest if it’s 
present...
    test_vm: Key inserted! Disconnecting and reconnecting 
using new SSH key...
==> test_vm: Configuring and enabling network interfaces...
==> test_vm: Running provisioner: ansible...
    test_vm: Running ansible-playbook...
PLAY ********************************************************
TASK [Install python2] **************************************
ok: [test_vm]
TASK [Update apt cache] *************************************
ok: [test_vm]
TASK [Install Apache] ***************************************
changed: [test_vm]
PLAY RECAP **************************************************
test_vm      : ok=3    changed=1    unreachable=0    failed=0   
Since you have installed virt-manager, you can now open 
up the Virtual Machine Manager to see the instance running. 
You can also log in to the instance using the following 
command from the test directory: 
$ vagrant ssh
After logging into the guest machine, you will find its 
IP address using the ifconfig command. You can then open a 
browser on the host system with this IP address to see the default 
Apache Web server home page, as shown in Figure 1.  
Figure 1: Apache Web server page
By: Shakthi Kannan
The author is a free software enthusiast and blogs at 
shakthimaan.com.

Rajarshi Bhattacharyya,  
country head, SUSE India  
(Srilanka, Bangladesh and Nepal)
“THE ACCEPTANCE 
OF OPEN SOURCE 
AMONG INDIAN 
ENTERPRISES 
HAS BEEN 
REMARKABLE”
What are the roadblocks to the success of an open 
source platform in India? Rajarshi Bhattacharyya, 
country head, SUSE India, answers this question 
in an exclusive conversation with Jagmeet Singh of 
OSFY. Edited excerpts...
Q
How has SUSE grown 
in India?
Completing 25 successful years feels 
great! Founded in 1992, SUSE has come 
a long way in the Linux market. We are 
constantly involved with the open source 
community and have many ‘firsts’ to 
our credit. We were the first enterprise 
OpenStack Cloud distribution, the first 
Linux on the mainframe, and also the 
first Linux to make KVM and Xen 
available. Besides, we are the primary 
sponsor of the community-supported 
openSUSE Project, which develops the 
openSUSE Linux distribution.
Gone are the days when SUSE was 
known merely as a distro used by a 
group of edgy techies. SUSE is now a 
common name in the IT industry, and 
that has not happened overnight. The 
combination of a clear strategy, a strong 
and buzzing community, innovation 
beyond what anyone imagined and the 
conviction to make it big, have made 
SUSE a success story.
In India, we are growing by double 
digits, and with open source being at 
the heart of the Indian government’s IT 
Policy, we are expanding at a significant 
rate. SUSE powers the ‘Make in India’ 
initiative established by the government, 
by enabling cloud services and value-
added service providers with cutting-
edge, enterprise-class open source 
technologies. Moreover, adoption of 
open solutions from SUSE allows these 
organisations to build products and 
solutions that are comparable to their 
foreign counterparts.
Q
Do you see any notable 
differences between India 
and developed regions like the 
US and UK, as an open source 
solutions provider?
India has a very distinct IT policy 
which is governed by open standards 
and actively promotes adoption of 
open source. So, being an open source 
For U & Me
Interview
38 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Interview

company, we have significant support 
from the authorities. Apart from the 
government, enterprises in India 
are going global, and in a fiercely 
competitive environment, organisations 
look for tremendous cost-effectiveness, 
as well as flexibility and reliability as 
the key deciding factors when it comes 
to technology adoption. Open source 
is the perfect choice to help enterprises 
overcome all such hurdles.
Q
What are the major challenges 
for an open source company 
like SUSE to operate profitably in 
the Indian market?
Today, the major challenge that an open 
source organisation faces in India is the 
unavailability of skilled resources. Open 
source organisations such as SUSE have 
been making huge efforts to popularise 
the adoption of open source right from 
the elementary school to a higher level of 
educations. There are a lot of universities, 
school boards and technical education 
centres nowadays that are adopting open 
source education as a part of their ICT 
curriculum.
Q
How do you overcome those 
challenges?
SUSE invests heavily in developing 
course content on cutting-edge 
new technologies. There are online 
academies as well as on-demand 
training available to master these newer 
technologies. On top of these, YouTube 
releases by experts help individuals to 
master the available technologies.
There is a unique programme 
wherein SUSE enables academic 
institutions to subscribe to all its course 
materials at zero cost. These materials 
are regularly updated. Also, students and 
individuals can get themselves certified 
on technologies like OpenStack and 
software defined storage.
Q
What are your prime 
strategies to enable 
developer engagement around 
your offerings?
We have SUSE Studio that allows 
developers to build applications on 
well-defined tiering system among our 
channels and have a wide network of both 
Tier-1 and Tier-2 to cater to the needs of 
various markets and customers across 
India. Recently, we collaborated with 
70-80 partners, and it is a constantly 
growing network in the country. Channel 
expansion is of great strategic importance 
to our business.
Q
What makes SUSE Linux 
a distinct option in the 
growing space of open source 
distributions?
SUSE Linux is reliable, highly scalable 
and includes cutting-edge technology. 
Moreover, it comes with best-in-class 
support.
Q
Do Indian enterprises view 
Linux as a part of their IT 
deployments nowadays? Is there 
any difference from the past?
The acceptance of open source among 
Indian enterprises has been remarkable. 
Looking back at Linux adoption just 
five years ago — almost 60 per cent of 
servers were on proprietary OSs, and 
only 35 per cent used to be on Linux in 
the data centre space. Now, the mix has 
changed dramatically, and Linux has 
taken over proprietary systems.
In the computing space, organisations 
are looking at an alternative option, 
because most of the applications are Web 
enabled and need a browser to access 
them. They do not see the need to pay 
for a browser. Application software like 
LibreOffice, which is a complete office 
suite, does everything that Microsoft 
Office does. This is exactly why large 
customers like the government of India 
have consciously decided to embrace 
open source. Neither government 
organisations nor enterprises want 
vendor lock-in. Also, with a proprietary 
system, you lose the freedom of choice.
Q
What are the key drivers 
enhancing Linux adoption in 
developing markets like India?
Adoption of OpenStack Cloud and 
software defined storage (SDS) are the 
most disruptive technology trends in the 
open source technologies. Recently, the 
company has also released a Container-as-
a-Service (CaaS) to cater to the developer 
community. The new model is the way 
forward for DevOps across India.
Also, SUSE encourages the 
developer community by bringing in 
a straightforward ISV (independent 
software vendor) certification process, 
which allows ISVs to get their 
products certified easily on SUSE. As a 
community programme, Open SUSE is 
a huge success.
Q
Do you believe community 
support is important to grow 
an open source product?
Indeed it is! Open source communities 
encourage innovation through 
collaboration. They provide a platform 
for developers to keep improving the 
programs or code in line with the latest 
technological advancements.
Without open source, many of the 
technologies we enjoy today would 
never have been developed or would be 
restricted from being widely used because 
of the patent law. But thanks to open 
source communities, those technologies 
have developed at a breakneck pace over 
the past few decades.
Q
How do you manage to 
market the entire SUSE 
portfolio to Indian customers?
We are a 100 per cent channel-driven 
company, and our partners are the key 
to our success. They are valued the most 
within the organisation. We have separate 
teams for managing large, medium and 
small channels. We also maintain a 
In India, we are 
growing by double 
digits, and with 
open source at the 
heart of the Indian 
government's IT Policy, 
we are expanding at a 
significant rate.
For U & Me
Interview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 39

Indian market. There is a marked shift 
in the way we do business now. Indian 
organisations are looking for a much 
more scalable platform, through which 
they can increase operational efficiency 
as well as deliver the desired result in a 
short turnaround time.
Q
Why should enterprises go in 
for a software-defined data 
storage model in today’s world?
The amount of unstructured data that 
floods an organisation’s data centre 
is just too high to ignore. And for 
businesses that continue to rely on 
traditional proprietary storage, this 
indeed is a nightmare.
Software defined data storage has 
long been hailed as the panacea for all 
such enterprise storage woes. It isn’t a 
new concept, but the ability to separate 
the storage software from the hardware 
is a game-changing innovation.
New architectures like Ceph-based 
storage solutions can scale infinitely 
without the need to refresh the entire 
platform or disrupt the existing 
functioning environment. Ceph uses 
intelligent algorithms to store data 
in a highly distributed manner. This 
means that it can optimise system 
performance without the need for teams 
of administrators to monitor and manage 
the storage constantly. Ultimately, with 
the data deluge and shrinking storage 
budgets, the future of storage is indeed 
going to be open source.
Q
What are the major obstacles 
in the present SDS space?
There aren’t any particular challenges 
when it comes to adoption. But there 
may be a lack of the economies of scale 
generally associated with the cloud. 
Many SDS providers claim that they 
provide the data centre with a cloud-
like hyper-converged architecture. 
But in reality, only a few provide 
cloud-like models.
Q
Where do microservices 
stand in the ever-growing IT 
sector in India?
Apart from DevOps and containers, 
microservices have been gaining 
popularity in the IT sector recently. 
Microservices is not an entirely new 
idea, but the concept is fresh. It can 
play a crucial role in the current, 
dynamically challenging business 
environment by helping companies 
to better align their businesses 
and IT needs.
Basically, microservices is a 
combination of several smaller 
services, or simply, a suite of small 
services, where each service has a 
well-defined boundary and all services 
are interfaced through the API. This 
new approach enforces a level of 
modularity, making the individual 
services run faster to develop, test and 
deploy. Even developers get the added 
benefit as they only need to focus on 
a single service, rather than the entire 
monolithic application, which is easy to 
understand and maintain.
Q
Is it only the increase in 
cloud adoption that has 
started pushing microservices, 
or is there an intrinsic interest 
among developers to begin with 
microservices?
I don’t think that it is only one or the 
other that has increased the adoption of 
microservices. Both the cloud and the 
intrinsic interest among developers has 
led to this. The cloud provides an easy-
to-deploy option without getting into 
capital expenditure.
Q
What do you feel about the 
Indian government’s interest 
in open source?
Undoubtedly, India is one of the biggest 
users and contributors to open source 
technology. It has come a long way and 
has evolved tremendously in the past 
few years—from viewing open source 
as just a cost-efficient alternative to 
proprietary solutions, to an imperative 
technology in the government’s flagship 
‘Digital India’ programme. Besides, 
the digital transformation has increased 
the adoption of open source both 
by enterprises and the government, 
across the country.
Today, digitisation, digitalisation 
and digital transformation are the 
three key words governing business 
environments at different levels for 
different industries. For IT-ITeS 
organisations, digital transformation is 
one of the most demanding roles that 
the CIO must play. The government 
sector, on the other hand, is witnessing 
a combination of digitisation and 
digitalisation. In the BFSI (banking, 
financial services and insurance) space, 
it is more about digitalisation and 
then embracing digital transformation 
through marketing to ensure that the 
bank reaches out to its end consumers 
through all possible ways.
Considering the volume of data 
that the government needs to deal 
with, open source is the only scalable 
and robust platform that can ensure 
a better result. Another aspect that 
goes in favour of open source is 
‘interoperability’. The government’s 
infrastructure and systems need to 
support multiple systems, standards, 
applications and processes, which is 
possible only through the adoption of 
open technologies. Open source can be 
run on mainframes or desktops, without 
compromising on performance or 
quality. The government, as a customer, 
also appreciates freedom of choice— 
the biggest advantage of open source.
Q
What are your views on the 
government’s open source 
policy? Is it beneficial for open 
source providers in any way?
The government has a well-defined 
open source policy. Yes, as the 
technology space continuously changes, 
it is the responsibility of open source 
organisations like us to contribute and 
enable the government to adapt to the 
changes in the landscape.
Q
Lastly, what’s next at SUSE 
to attract enterprises to 
open source in India and across 
the world?
Our software defined storage and 
Container-as-a-Service are the next two 
attractions for global enterprises. 
For U & Me
Interview
40 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 41
Admin
Overview
V
irtualisation refers to the creation of a ‘virtual’ 
technology—an abstraction built to support a 
diverse set of applications and systems using the 
same base set of resources. Virtual machines (VMs) are the 
implementation of virtualisation for an entire machine. They 
emulate the hardware, networking and storage, providing an 
isolated system for the end user. A VM provides the complete 
set of functionality an operating system would have provided. 
Multiple VMs can be run on a single system, thus optimising 
the usage of system resources and reducing the need for 
separate hardware for each system. Specialised software, 
known as a hypervisor, is often used on servers to manage 
the different VMs. If you have surfed the Internet, you have 
probably been exposed to a virtual machine without realising 
it. That is the beauty of this technology!
The technology has, in fact, developed over a span of 
decades, starting with IBM’s first commercial mainframe 
that supported virtualisation, which evolved to address the 
demand by Bell Labs and the Massachusetts Institute of 
Technology in the 1960s.
Containers
You’d think containers were a modern update to virtual 
machines, but the fact is that they date back to the 1970s, 
to the ‘chroot’ of UNIX machines (a command that defined 
a new root directory for the system), introducing the 
concept of isolating processes from each other without the 
need for hardware-level virtualisation. The Linux container 
was one of the first comprehensive implementations of 
the container ecosystem. A series of updates and several 
Docker containers enable developers and operators to build and ship an 
application anywhere. The author discusses the popularity of Docker container 
technology, and its future relevance and scope. 
The Current Popularity and the 
Future of Docker

42 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Admin
Overview
decades later, we have containers that are a speedier, 
more efficient construct, which use operating system-
level virtualisation instead. Containers offer a number of 
advantages over virtual machines, an important one being 
that they are lightweight and easier to deploy at scale in 
cloud-based infrastructure.
Docker
Docker is software that offers a means to develop and 
maintain containers. It primarily relies on the LXC 
interface and uses the concepts of groups and namespaces 
in Linux; however, the team has developed a programming 
interface called libcontainer, which is being used to 
develop current iterations. 
The most important reason behind the high adoption rate of 
Docker is that it is lightweight. Docker images are much lighter 
to deploy than hypervisor-based systems like virtual machines. 
Figure 1: Technological overview of a virtual machine
Figure 4: The Docker container
Figure 2: Heavyweight and non-portable
Figure 3: Lightweight and portable
Figure 5: Comparing VMs with containers 
This also increases the density of containers (compared to VMs) 
that can be deployed on a host. And this means that the size of 
a container is a few megabytes versus the gigabytes of a VM, 
and the boot time is in seconds contrasted with the minutes it 
takes for even the faster VMs to launch.
Docker Hub has gone a step further by providing a 
complete ecosystem for sharing and distributing images as 
well as maintaining existing deployments.
Enterprise usage
The application life cycle is a labour-intensive circle 
of development, integration and testing. Having a 
containerised environment greatly simplifies the 
dependency management, scalability and deployment 
of applications, since the deployment environment can 
be made completely identical across development and 
production servers. This is why Docker’s simplification 
of complex development environments makes it a leading 
product across the industry.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 43
Admin
Overview
Resource utilisation in Docker containers is maximised 
and isolation leads to better and more predictable system 
performance. Moreover, security is enhanced; even if a user is 
able to compromise the system and gain superuser privileges, 
this user is isolated from the rest of the system. Portability to 
many operating systems and the version control feature make 
the technology an attractive package for widespread adoption 
across the industry.
Rapid adoption
Docker has fast become the industrial standard for 
containerisation, with a 40 per cent increase in adoption 
across all hosts, claimed a study published by the leading 
metrics and monitoring provider, Datadog. The study further 
claimed that there is a linear increase in the containers 
deployed by users, with the numbers nearly quintupling in 
the first 10 months of being introduced to containers. The 
major trend observed has been the deployment of several 
containers simultaneously on a single host.
In a similar study, leading analytics provider Sysdig 
claims to have observed as many as 95 containers running 
on a host. A majority of the hosts used Kubernetes 
as an orchestrator to manage these containers, while 
maintaining a registry of containers to enable easy sharing 
between users and services.
The future
An analysis of current trends
The DockerCon 2016 keynote speech by Docker’s CEO 
Ben Golub provided many insights into the current level 
adoption of this technology:
 
Dockerised applications have grown about 3100 per cent 
over the past two years to about 460,000 in number.
 
Over 4 billion containers have been shared among the 
developer community. Analysts are touting 8 billion as the 
number for 2017.
 
There are nearly 125,000 Docker Meetup 
members worldwide, which is nearly half of the 
population of Iceland.
By: Swapneel Mehta
The author has worked with Microsoft Research, CERN and 
startups in AI and Cybersecurity. An open-source enthusiast, 
he enjoys spending his time organising software development 
workshops for school and college students. You can connect 
with him at https://www.linkedin.com/in/swapneelm and find 
out more at https://github.com/SwapneelM.
Figure 6: Docker adoption trends as reported by Datadog
Figure 7: Top technologies running on Docker
Docker has spurred the development of a number of 
projects including CoreOS and Red Hat’s Project Atomic, 
which are designed to be minimalist environments for 
running containers. The top 10 technologies running 
in Docker environments, as listed by Datadog, are 
shown in Figure 7.
Projections
What began with 1146 lines of code has today turned 
into a billion-dollar product. Docker has grown to a 
stage where a majority of leading tech firms have been 
motivated into releasing additional support for deploying 
containers within their products. Examples include 
Amazon integrating Docker into the Elastic Beanstalk 
system, Google introducing Docker-enabled ‘managed 
virtual machines’, and announcements from IBM and 
Microsoft with regard to Kubernetes support for multi-
container environments. Docker has already become 
a part of major Linux distributions such as Ubuntu, 
CentOS and Red Hat Enterprise Linux (RHEL), although 
the packaged versions often fail to keep up with the 
latest releases.
The Docker team has clearly laid out its goals of 
developing the core capabilities, cross-service management 
and messaging. It is likely that, in the future, there will be 
more focus on building and deploying rather than the level 
of virtualisation, which is when the fine line between VMs 
and containers becomes fuzzy.
In any case, the fact that Docker has breached the 
billion-dollar evaluation mark is an indication of the 
underlying potential of this technology. Docker is 
clearly here to stay! 

For U & Me Success Story
44 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Indian e-commerce  
startup Fynd contributes back  
to the open source community
Mumbai-based online 
fashion store, Fynd, has 
launched GoFynd.io as 
a dedicated platform 
to support the open 
source community. 
The platform includes 
solutions such as Fynd 
API, Fyx and Metricbeat, 
which were originally 
designed for internal 
operations.
Harsh Shah, co-founder, GoFynd.io
Witi, Fyx, Ansible Metricbeat and 
Gravity, which are available through the 
GoFynd.io platform.
“Indian startups are often a step 
behind when it comes to open source 
policy. We wanted to break this pattern 
and do our bit by openly sharing our 
own internal developments,” says Fynd 
co-founder Harsh Shah.
There are a total of 30 developers 
at Fynd, with 20 working primarily on 
GoFynd.io. These developers are also 
making things easier for early adopters 
by posting new blog posts on a 
regular basis.
Promotion of in-house 
developments
Fynd is using online forums to make 
people aware of its open source work. 
Additionally, the company has invited 
the developer community to not only 
use its resources, but also give its 
feedback and further build on the 
existing projects.
“By open sourcing, Fynd aims 
to make its engineering tools and 
data streams visible to the whole 
world, including companies opting 
for open source solutions as well as 
D
eploying open source is 
relatively easier for a company 
than contributing back to the 
community by releasing the source 
code for solutions developed to address 
specific internal requirements. But Fynd 
has found a way to support the growth 
of open source by launching GoFynd.io. 
The online store, which targets the youth 
with branded products across categories 
such as clothing, footwear, jewellery 
and accessories, has been primarily built 
using developer-friendly technologies, 
including Angular, Ansible, Apache 
Kafka and React. These technologies 
work with native tools and data stream 
options such as the Fynd Open API, 

For U & Me
Success Story
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 45
The GoFynd.io team at work in the company’s Mumbai office
By: Jagmeet Singh
The author is an assistant editor at EFY.
Initial open source projects on GoFynd.io
• Fynd API: This API makes it easy for developers and businesses to build a 
full-fledged omni-channel fashion e-commerce app.
• FXI: This tool provides real-time updates from delivery partners to help 
businesses monitor their packages.
• Witi: This API ensures automatic tagging of all e-commerce-specific 
catalogue images to organise, manage and search these through the 
available content.
• Metricbeat: This package monitors the host metrics across the server fleet.
• Gravity View: This tool helps developers to enable tilt-to-browse support 
on their apps, to show product images to users without the need for any 
swiping gesture
taken to deploy, go live and scale,” 
says Shah.
Apart from utilising the publicly 
developed solutions to build the online 
marketplace, Fynd has adapted open 
source to power its mobile apps as well 
as its artificial intelligence (AI) driven 
fashion-discovery and transaction bot. 
“We have used a lot of open source 
technologies in almost every aspect 
of our products,” Shah, 28, tells Open 
Source For You. There are libraries, 
models, and algorithms written by 
developers across the world that 
help Fynd build a great online 
shopping experience.
The challenges in adopting 
ready-made solutions
One of the major challenges that 
Fynd has faced while opting for open 
source is to contextually match the 
code with the use case. “Sometimes, 
adapting the available technique to fit 
our particular problem or use case can 
be tricky. You can’t always use the 
same code, algorithm or the library as 
it is,” explains Shah.
Fynd tries to resolve this challenge 
by understanding the context in which 
the code was initially written. The 
company also reaches out directly to 
the contributor to better understand 
the use case. This helps to reduce 
costs and the time spent in building 
and refining the final product.
Advice to other startups
Shah believes that open source 
has always been a key way of 
development for all startups. “With 
so much knowledge being open 
sourced, it is only going to fuel the 
use of such technologies. Everyone 
can’t build everything from scratch. 
Therefore, knowledge gained and 
shared by someone can be a good 
foundation, a ‘leap-frog’ point, 
without having to reinvent the 
wheel,” he concludes. 
developers building new projects 
using existing code,” says Shah. 
The co-founder has developed a 
concrete plan to release one new 
open source project and improve 
one existing project every month, 
in order to actively receive 
contributions from developers.
Scalable options
Just like any other open source-focused 
enterprise, Fynd doesn’t rely much 
on proprietary solutions. Instead, the 
company is planning to scale up using 
open source developments. “By using 
techniques that are already available, 
we significantly reduce the time 

Admin
Let's Try
46 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
O
ne of the main concerns of people who are switching 
to Linux is how to run the programs in it that they 
are so used to running in other operating systems, 
especially Windows. Thankfully, Wine can help in solving this 
problem. The software that Windows users usually miss the 
most on Linux is Microsoft Office. 
All .exe files can be run on Linux using Wine. Wine 
is software for UNIX-like systems, including Linux, 
OSX and the BSDs. It allows you to run native Windows 
applications. Wine stands for  ‘Wine Is Not an Emulator’. 
That’s because it isn’t. Wine isn’t a full Windows install 
or some kind of VM. It is a compatibility layer that 
essentially translates Windows binaries. This extends to 
graphics libraries like DirectX 9, which are converted to 
OpenGL. Wine allows Linux users to run many popular 
Windows applications and games as if they were running 
on Windows itself, with a similar performance. 
Wine can be tricky though. There are multiple versions 
of it with different sets of patches and two different 
current versions. Even when you get past that, different 
programs require specialised settings, in many cases, and 
can’t just be run out-of-the-box. However, once you know 
On switching over to Linux, many Windows users feel lost because they miss their favourite 
Windows programs. Wine provides a means of running those every programs on a Linux 
system. Here’s a tutorial on installing MS Office on Linux using Wine. 
your way around and have configured a program to run 
properly through Wine, it will usually run flawlessly, like 
a native application.
Installing Wine on Linux
Installing Wine on Linux is easy. All you have to do is follow 
the steps to install Wine on any Ubuntu system, as given below.
Step 1: Open the terminal Ctrl – Alt – T and run the 
following command, which will install the ppa. You will be 
asked for your root password also.
sudo add-apt-repository ppa:Ubuntu-wine/ppa
Step 2: Now install Wine by typing the following 
command. You will notice the screens shown in Figures 1 and 
2 during the installation.
sudo apt-get update && sudo apt-get install wine1.7
Step 3: To configure Wine, use the following command:
Winecfg
Wine: A Great Tool for Running 
Windows Programs on Linux

Admin
Let's Try
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 47
Select the Libraries tab, then scroll down; select the 
Riched20 library from the list and add it. When done, 
click ‘OK’.
Now you have successfully installed Wine.
Installing Microsoft Office on Ubuntu
With the help of Wine, it is extremely easy to run any 
of your favourite Windows software on Ubuntu. Here 
are some simple steps to follow after installing Wine on 
Ubuntu, in order to install Microsoft Office.
Step 1: Run the command given below to configure the 
Windows environment for 32-bit versions of Office.
export WINEPREFIX=$HOME/wine32
If you are using a 64-bit system, then just replace 
‘32’ with ‘64’ in the command.
Step 2: Run the command given below to enable the 
settings for Wine:
export WINEARCH=win32
If you are using a 64-bit system, replace ‘32’ with ‘64’.
Step 3: Open the Microsoft Office setup, right-click on 
Setup.exe and open with Wine.
Step 4: When the installer opens, click ‘Install Now’ to 
begin the installation. Follow the prompts.
Step 5: When you’re done with installation of Office, 
Figure 1: Wine downloading
Figure 2: Wine mono installation
Figure 3: Wine configuration
[1] https://linuxconfig.org/introduction-to-wine
[2] https://www.bleepingcomputer.com/
forums/t/539061/how-to-install-microsoft-office-
2010-in-ubuntu-with-wine/ 
[3] https://in.mathworks.com/help/nnet/examples/
transfer-learning-and-fine-tuning-of-convolutional-
neural-networks.html
References
By: Prof. Prakash Patel and Prof. Dulari Bhatt
Both the authors are assistant professors in the IT department 
at Gandhinagar Institute of Technology. They can be contacted 
at prakash.patel@git.org.in and dulari.bhatt@git.org.in.
restart your computer and open Ubuntu Dash. Search for 
Microsoft Office to use the suite.
Wine has a number of advantages, some of 
which are listed below.
1. Wine is better than a VM if you need limited applications. 
VMs are fine but they may be slower than Wine. VMs 
need more RAM to run a whole OS, but you can do almost 
everything in Wine faster than VMs do. 
2. You can run almost every .exe file in Ubuntu with some 
configuration.
3. Wine is a free and open source compatibility layer.
4. It provides a rich software library to developers for 
running Windows applications.
5. Wine ensures good backward compatibility with legacy 
Windows applications. 



Admin
Overview
50 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
B
ig Data analytics is one of the key areas of 
research today, and uses assorted approaches in 
data science and predictive analysis. There are 
a number of scenarios in which enormous amounts of 
data are logged every day and need deep evaluation for 
research and development. In medical science, there 
are numerous examples where processing, analysis and 
predictions from huge amounts of data are required 
regularly. As per reports from First Post, data of more 
than 50 petabytes is generated from each hospital of 500 
beds in the USA. In another research study, it was found 
that one gram of DNA is equivalent to 215 petabytes in 
digital data. In another scenario of digital communication, 
the number of smart wearable gadgets has increased from 
26 million in 2014 to more than 100 million in 2016.
The key question revolves around the evaluation 
of huge amounts of data growing at great speed. To 
preprocess, analyse, evaluate and make predictions 
on such Big Data based applications, we need high 
Open source software has an array of tools that deal with high 
speed Big Data, of which Apache Storm is very popular. This article 
discusses various aspects of Apache Storm.
performance computing (HPC) frameworks and libraries, 
so that the processing power of computers can be used with 
maximum throughput and performance.
There are many free and open source Big Data processing 
tools that can be used. A few examples of such frameworks 
are Apache Storm, Apache Hadoop, Lumify, HPCC Systems, 
Apache Samoa and ElasticSearch.
MapReduce technology
Of the above-mentioned tools, Apache Storm is one of the 
most powerful and performance-oriented real-time distributed 
computation systems under the free and open source software 
(FOSS) paradigm. Unbound and free flowing data from 
multiple channels can be effectively logged and evaluated 
using Apache Storm with real-time processing, compared 
to batch processing in Hadoop. In addition, Storm has been 
effectively adopted by numerous organisations for corporate 
applications with the integration of some programming 
language, without any issues of compatibility. The state of 
Analyse Big Data 
With Apache Storm


Admin
Overview
52 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
clusters and the distributed environment is managed via 
Apache Zookeeper within the implementation of Apache 
Storm. Research based algorithms and predictive analytics 
can be executed in parallel using Apache Storm.
MapReduce is a fault-tolerant distributed high 
performance computational framework which is used to 
process and evaluate huge amounts of data. MapReduce-like 
functions can be effectively implemented in Apache Storm 
using bolts, as the key logical operations are performed at the 
level of these bolts. In many cases, the performance of bolts 
in Apache Storm can outperform MapReduce.
Key advantages and features of Apache Storm
The key advantages and features of Apache Storm are that it 
is user friendly, free and open source. It is fit for both small 
and large scale implementations, and is highly fault tolerant 
and reliable. It is extremely fast, does real-time processing 
and is scalable. And it performs dynamic load balancing and 
optimisation using operational intelligence.
Installing Apache Storm and Zookeeper on 
a MS Windows environment
First, download and install Apache Zookeeper from 
https://zookeeper.apache.org/. 
Next, configure and run Zookeeper with the 
following commands:
MSWindowsDrive:\> cd zookeeper-Version
MSWindowsDrive:\ zookeeper-Version> copy conf\zoo_sample.cfg 
conf\zoo.cfg
MSWindowsDrive:\ zookeeper-Version> .\bin\zkServer.cmd
The following records are updated in zoo.cfg:
tickTime=2000
initLimit=10
syncLimit=5
dataDir= MSWindowsDrive:/zookeeper-3.4.8/data  
Now, download and install Apache Storm from 
http://storm.apache.org/ and set STORM_HOME to 
MSWindowsDrive:\apache-storm-Version following in 
environment variables.
Perform the modifications in storm.yaml as follows:
storm.zookeeper.servers:
– “127.0.0.1”
nimbus.host: “127.0.0.1”
storm.local.dir: “D:/storm/datadir/storm”
supervisor.slots.ports:
– 6700
– 6701
– 6702
– 6703
Figure 1: Official portal of Apache Zookeeper
Figure 2: Execution of commands to initialise Apache Zookeeper
Figure 3: Download page of Apache Storm for multiple platforms
Figure 4: Execution of commands to initialise Apache Storm

Admin
Overview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 53
In the MS Windows command prompt, go to the path of 
STORM_HOME and execute the following commands:
1. 
storm nimbus
2. 
storm supervisor
3. 
storm ui
In any Web browser, execute the URL http://
localhost:8080 to confirm the working of Apache Storm.
Apache Storm is associated with a number of key 
components and modules, which work together to do 
high performance computing. These components include 
Nimbus Node, Supervisor Node, Worker Process, Executor, 
Task and many others. Table 1 gives a brief description 
of the key components used in the implementation of 
Apache Storm.
Extraction and analytics of Twitter streams 
using Apache Storm
To extract the live data from Twitter, the APIs of Twitter4j 
are used. These provide the programming interface to 
connect with Twitter servers. In the Eclipse IDE, the 
Java code can be programmed for predictive analysis and 
evaluation of the tweets fetched from real-time streaming 
channels. As social media mining is one of the key areas 
of research in order to predict popularity, the code snippets 
are available at http://opensourceforu.com/article_source_
code/2017/nov/strom.zip can be used to extract the real-time 
streaming and evaluation of user sentiments.
Scope for research and development
The extraction of data sets from live satellite channels 
and cloud delivery points can be implemented using the 
integrated approach of Apache Storm to make accurate 
predictions on specific paradigms. As an example, live 
streaming data that gives the longitude and latitude of 
a smart gadget can be used to predict the upcoming 
position of a specific person, using the deep learning based 
approach. In bioinformatics and medical sciences, the 
probability of a particular person getting a specific disease 
can be predicted with the neural network based learning 
of historical medical records and health parameters using 
Apache Storm. Besides these, there are many domains in 
Figure 5: Apache Storm UI with the base configurations, nodes and cluster information
By: Dr Gaurav Kumar
The author is the MD of Magma Research and Consultancy 
Pvt Ltd, Ambala. He delivers expert lectures and conducts 
technical workshops on the latest technologies and tools. 
He can be contacted at kumargaurav.in@gmail.com. 
Website: www.gauravkumarindia.com    
Table 1
Components
Description
Nimbus
This is the master node in the cluster 
of Apache Storm. The other nodes 
are referred to as worker nodes. The 
master node is the key node used for 
the distribution of data to the worker 
nodes and for overall monitoring.
Supervisor
These are the nodes that accept the 
instructions sent by Nimbus. The su-
pervisor node has multiple processes 
mapped with the workers.
Worker process
This executes tasks associated with 
a particular topology. It creates the 
executors to execute the tasks.
Executor
This is the thread created to execute 
the process.
Task
This refers to the specific task to be 
performed.
Zookeeper 
framework
A service that is used by a group of 
nodes or simply clusters. Zookeeper 
assists the supervisor to communicate 
with Nimbus and maintain the states 
of communication.
Tuple
This refers to the key data structure 
in Apache Storm, which supports all 
formats and data types.
Stream
The sequence of tuples in unordered 
perspectives.
Spout
This is used to accept the input data 
from different channels like Twitter 
streaming, bioinformatics, medical 
reports, satellite data, etc.
Bolts
These are the logical processing units 
in Apache Storm.
which Big Data analytics can be done for a social cause. 
These include Aadhaar data sets, banking data sets and 
rainfall predictions. 

Developers
Let’s Try
54 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
T
he global industry is increasingly leaning towards 
Platform as a Service (PaaS), which has advanced 
considerably over time. Today, PaaS uses several 
programming languages such as .NET, Java, PHP, 
Python and Ruby. 
Microsoft Azure and its services
Microsoft Azure is one of the most dominant cloud services 
providers in the market. Microsoft Toolsets provides an easy 
way to deploy applications that are built on a platform like 
Java and .NET.
Microsoft Azure comes with some core concepts that 
are important to understand. Its services are available in 
36 regions around the world, with plans announced for 
four additional regions. Details regarding these regions are 
available at https://azure.microsoft.com/en-in/regions/. To 
view products or services available, based on the region, 
you can go to https://azure.microsoft.com/en-in/regions/
services/. ‘Resource Groups’ in Microsoft Azure is a logical 
container. It can be used to group different resources such as 
Microsoft Azure App Services, SQL databases, and storage 
accounts available in Microsoft Azure.
An App Service Plan (ASP) comprises the instance size 
and instance count on which the application is hosted, as well 
as its features. There are different capabilities and limits within 
ASPs. It is important to understand that ASPs can be shared by 
multiple applications and deployment slots are usually on the 
same ASP; however, we can change the ASP for the slots too. If 
you want to understand how pricing works in Microsoft Azure, 
you should explore Azure Calculator at https://azure.microsoft.
com/en-in/pricing/calculator/.
Microsoft Azure App Services
App Services is one of the most popular offerings from 
Microsoft Azure. Web Apps are a PaaS offering that have 
computing resources and runtime environments managed by 
Microsoft Azure. 
For Azure App Services, five pricing tiers are available – 
free, shared, basic, standard and premium. 
 
Free: Try this to implement a proof-of-concept.
 
Shared: This is for basic Web applications or static websites.
 
Basic: This is for development/test environments.
 
Standard: This is for Web and mobile applications in the 
production environment.
 
Premium: This is for enterprise scale and integration.
App Services can be easily used in DevOps practices 
too. Visual Studio Team Services can be used for continuous 
integration and delivery, or continuous deployment using 
different tasks and ways in different environments, with 
approved scenarios.
To check how these services work, go to https://azure.
microsoft.com/en-us/try/app-service/. Select Web App. Click 
on Continue and follow the rest of the steps indicated.
Creating a Web application
If you have a valid Microsoft Azure subscription, you can go 
Creating a Web Application  
Using Azure App Services
‘Azure App Services’ enables the 
development of powerful apps 
for any platform and any device. 
Let’s try and build a simple Web 
application using this PaaS offering 
from Microsoft Azure. 

Developers
Let’s Try
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 55
to https://portal.
azure.com/. To get 
some quick hands-
on experience 
on creating an 
application, follow 
the steps given below.
Log in with valid 
credentials and you 
will get the Azure 
dashboard.
First, let’s create 
an App Service Plan 
(ASP) so that we 
can host Azure App 
Services on it. Click 
on  ASP in the left 
sidebar, and then 
click on + Add. 
Give the name 
of the ASP, then the 
name of the New 
Resource Group, the 
operating system, the  location where ASP must be hosted, and 
the most important part — the pricing tier. Next, click on Create.
Once we have ASP ready, click on App Services in the left 
sidebar and then on +Add.
Select Web App. Click on Create. Give the App Name, and 
select the Resource Group that we have created while creating 
the ASP. Click on Create.
Wait for some time, and once the Azure Web App is ready, 
you can access it from the Azure dashboard.
Verify the Resource Group, the status of the Web App (its 
location and URL), the App Service Plan, FTP details and 
monitoring related details.
Visit the URL in the browser.
Go to the Azure Web App just created and click on 
Application Settings. When we create an Azure Web app, by 
Figure 1: Microsoft Azure dashboard 
Figure 2: App Service Plan in Microsoft Azure
Figure 3: Azure Web App in Microsoft Azure
Figure 4: Azure Web App overview in Microsoft Azure
Figure 5: Azure App Service in the browser
Figure 6:  Application settings of the Azure App Service 
(Azure Web apps) in the Microsoft Azure dashboard
Figure 7: Configure the Java application in Application Settings of Azure App Services
default, it is a .NET app. Let’s look at how to change it to a 
Java application.
Select the Java version as Java 8. Select the Java Minor 
version and Web Container too.
Web
Apps
App
Services
Logic
Apps
Mobile
App
API
Apps
Try this for implementing Proof 
of Concept
For basic web application or static 
websites
For the development/test 
environment
For web and mobile applications 
in the production environment
For enterprise scale and 
integration
Select app type
Select template
Work with your app
Web App
Azure Web Apps enable developers to easily deploy and scale enterprise grade web applications written in a variety 
of languages and integrated with a multitude of services without ever worrying about infrastructure management. 
Web App on
Linux
Functions App
API App
Logic App

Developers
Let’s Try
56 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
By: Mitesh S. 
The author has written the book, ‘Implementing DevOps with 
Microsoft Azure’. He occasionally contributes to https://clean-
clouds.com and https://etutorialsworld.com. Book link: https://
www.amazon.com/DevOps-Microsoft-Visual-Studio-Services-
ebook/dp/B01MSQWO4W
Figure 8: Java based Azure Web App
Figure 11: Scaling up the App Service Plan
Figure 12: Scaling out the App Service Plan
Figure 13: Log configuration of Azure App Services in Microsoft Azure
Figure 9: Properties of Azure App Services
Figure 10: Kudu editor of Azure App Services in Microsoft Azure
To keep a 64-bit platform for the application, we need to 
change the pricing plan to Standard. 
Put the Always On switch on, and click on Save.
Visit the URL of the Azure Web App and verify the change 
in the Java application.
Verify the Web Apps directory.
There are two types of scaling – vertical and horizontal. 
When scaling up, we can perform vertical scaling. Actually, 
this is a scaling up of the App Service Plan.
Go to Properties, check the status, the virtual IP address, 
the outbound IP address, and FTP related details.
For checking logs, and to know more about the structure, 
go to the Kudu editor using the URL https://osfytestapp.scm.
azurewebsites.net. Add SCM in the original URL.
In the Basic pricing tier, horizontal scaling is not 
supported, but it is supported in the Standard pricing tier.
Logging is an important aspect in case of troubleshooting 
and, by default, logging is not enabled. 
All log files can be accessed from the Azure portal as well 
as the Kudu editor. 
In this article, we have created a Web application using 
Microsoft Azure App Services, which is a PaaS offering from 
Microsoft. The road ahead can be continuous integration and 
delivery using Visual Studio Team Services to implement 
DevOps practices. 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 57
Developers
Let’s Try
T
here are three major types of software applications 
that are built — desktop, Web and mobile. The 
programming platforms used to construct each of 
these types of apps are different. Though certain languages 
such as Java play a key role in each of these application 
types, there are specialised languages for each category.  To 
name a few, PHP is for server-side scripting, CSS for adding 
style to Web pages, and XML is for data representation. 
Similarly, there are frameworks specific to each of these 
categories. These make the application-building process 
easier and more efficient for developers. 
Besides all these options, there are also a few frameworks 
that developers can use to build both desktop and mobile 
applications. This article introduces you to one such popular 
framework named Electron.  The three major characteristics 
of the Electron framework are listed below:
 
It uses popular Web technologies such as HTML, CSS and 
JavaScript for building desktop applications. 
 
 It is open source. 
 
It supports cross-platform application development. It 
is compatible with Microsoft Windows (Windows 7 and 
above), Apple Mac and Linux. The applications built 
using Electron execute on all these operating systems. 
Building apps using the Electron framework is fairly 
simple. Primarily, Electron was built for the Atom editor 
(Atom is a popular text editor/IDE — https://atom.io/). 
The efficiency of Electron is proven by the many leading 
companies such as Facebook and Microsoft that use it. A 
sample list of products built using Electron is available in the 
official home page (https://electron.atom.io/).
Installing Electron
Electron can be installed easily by using the npm install 
The programming languages used to build desktop and Web applications are obviously 
different. Electron is an effort to bridge this gap. With it, you can construct a desktop 
application with popular Web technologies such as HTML, CSS and JavaScript. This article 
introduces the basic features and the potential of the Electron framework.  
command:
npm install electron --save-dev --save-exact
Obviously, you need to have Node.js installed on your 
computer. 
Quick starting Electron
The quickest way to get started with Electron is to get the 
‘Quick start’ app into action. For that you have to carry out 
the following steps:
# Clone the Quick Start repository using Git 
$ git clone https://github.com/electron/electron-quick-start  
# Navigate to the folder electron-quick-start
$ cd electron-quick-start  
# Install the dependencies and run 
$ npm install && npm start
Another easier approach to get familiar with Electron is to 
check out the Electron-Api-Demos from GitHub.  The steps to 
build this demo app are the same as those mentioned for the 
‘Quick start’ app. 
$ git clone https://github.com/electron/electron-api-demos 
$ cd electron-api-demos 
$ npm install 
$ npm start
This app can be used to get a basic introduction to 
what can be done with Electron. You can play around with 
Electron: Building Cross-platform 
Desktop Apps with Web Technologies

58 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
the source code to get a better 
understanding of the features of 
Electron (a screenshot from the 
official GitHub page of the source is 
shown in Figure 3). 
The application structure
As stated earlier, Electron allows 
us to build desktop applications 
with Web technologies. Primarily, 
Electron facilitates the building of 
desktop applications with JavaScript 
as the core language. It provides a 
runtime with rich native operating system APIs. 
The basic structure of the simple Electron app consists of 
three files inside your application’s folder:
 
 Package.json
 
 Main.js
 
 Index.html
The structure of the package.json is the same as Node’s 
modules. The script specified in the ‘main’ field is the 
application’s startup script, which would run the main 
process. A sample package.json file is shown below:
{
 “name”    : “your-app”,   
 “version” : “0.1.0”,   
 “main”    : “main.js” 
} 
If the main field is left blank in the package.json file, then 
Electron tries to load a .js file named index.js.
There are two major types of processes in an Electron 
app, as listed below.
 
Main process: This is responsible for running the 
package.json’s main script. This main process can display 
a GUI by building Web pages. 
 
Renderer process: Electron uses Chromium for 
rendering Web pages. The multi-process architecture of 
Chromium is also used. These individual GUI (Web) 
pages also run their own process, which is called the 
renderer process.  
There is a major difference between the normal Web 
Figure 1: Electron – major characteristics
Figure 2: Electron – popular 
apps from the home page
Figure 3: Electron API demos 
pages that run inside a browser and the Electron app’s Web 
pages. The normal Web pages are executed in a sand-boxed 
environment and hence they are denied access to the native 
system resources. However, Electron Web pages can be used 
to access the lower level resources of the system with the 
help of operating system interactions. 
The main.js file (which is indicated in the package.json 
file) needs to build windows and handle events. A sample 
main.js file is shown below:
const {app, BrowserWindow} = require(‘electron’) 
const path = require(‘path’) 
const url = require(‘url’)  
// Keep a global reference of the window object, if you 
don’t, the window will be closed automatically when the 
JavaScript object is garbage collected.
 
let win  
function createWindow () 
{   
 
// Create the browser window.   
 
win = new BrowserWindow({width: 800, height: 600})    
// and load the index.html of the app.          win.
loadURL(url.format({
pathname: path.join(__dirname, ‘index.html’),         
protocol: ‘file:’,     
slashes: true   }))    
// Open the DevTools.   
win.webContents.openDevTools()    
// Emitted when the window is closed.   
win.on(‘closed’, () => {     
win = null   }) 
}  
// This method will be called when Electron has finished 
initialization and is ready to create browser windows. Some 
APIs can only be used after this event occurs. 
app.on(‘ready’, createWindow)  

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 59
Developers
Let’s Try
// Quit when all windows are closed. 
app.on(‘window-all-closed’, () => {  
 // On macOS it is common for applications and their menu bar 
to stay active until the user quits explicitly with Cmd + Q   
if (process.platform !== ‘darwin’) {
     app.quit()   
} 
})  
app.on(‘activate’, () => {
// On macOS it’s common to re-create a window in the app when 
the   // dock icon is clicked and there are no other windows 
open.   
if (win === null) {     createWindow()   
} 
})
The index.html would consist of the source code of the Web 
page. A sample is shown below:
<!DOCTYPE html> 
<html>   
  <head>    
     <meta charset=”UTF-8”>  
     <title>Hello World!</title>   
  </head>   
  <body>    
      <h1>Hello World!</h1>     
          We are using node        
<script>    document.write(process.versions.node)</script>,     
Chrome <script> document.write(process.versions.chrome)</
script>,     and Electron <script> document.write(process.
versions.electron)</script>.   
</body> 
</html> 
Now, we have built all the three required files: main.
js, package.json and index.html.  These will be executed by 
running the electron command in the source directory (if 
Electron is installed globally with npm).
electron 
Packaging Electron apps
Once the development of the application is over, it can 
be packaged for distribution. Detailed package building 
instructions for manually creating the packages are given 
at https://electron.atom.io/docs/tutorial/application-
distribution/. Apart from the manual packaging, there are 
specific tools available to assist in the package building 
process. Some of these are listed below:
 
Electron-forge
 
 Electron-builder
 
 Electron-packager
Accessibility testing of apps
Accessibility of applications has become an important 
factor these days. The Electron apps can be made accessible 
effectively and this aspect can be tested. Though the GUIs 
in the Electron apps are HTML pages, their accessibility 
cannot be audited directly using accessibility audit tools, 
as there is no direct URL available for these pages. To 
facilitate accessibility auditing of the Electron apps, new 
functionalities have been introduced to Devtron (an Electron 
DevTools extension—more information can be got at https://
electron.atom.io/devtron/) and Spectron (an Electron testing 
framework - https://electron.atom.io/spectron/). 
A sample code for accessibility auditing with Spectron is 
shown below:
app.client.auditAccessibility().then(function (audit) 
{   
   if (audit.failed) 
   {     
       console.error(audit.message)   
   } 
}) 
Similarly, the accessibility tab has been introduced in 
Devtron too. A sample screenshot is shown in Figure 4. 
This article is a basic introduction to the Electron 
framework. If you are interested in further exploring it, there 
are a large number of resources available. The community 
(https://electron.atom.io/community/) Web page of the 
Electron framework offers a lot to those interested in getting 
detailed insights. 
Figure 4: Devtron – accessibility analysis
By: Dr  K.S. Kuppusamy
The author is assistant professor at Pondicherry Central 
University, Pondicherry. He has over 12 years of teaching and 
research experience in academia and industry. He can be 
reached via mail at kskuppu@gmail.com.
[1] Electron: https://electron.atom.io/
[2] https://electron.atom.io/docs/tutorial/quick-start/
[3] https://electron.atom.io/docs/
References

60 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Let’s Try
Developers
E
arlier, developers typically looked for the package (JS, 
CSS) on the Internet, downloaded the zipped version, 
extracted it and linked it to the Web page. But the first 
four steps were very time consuming. The more dependencies 
that were required, the more time developers had to invest in 
repeating the same steps. Moreover, all these dependencies had 
to upload in a version control system, so that other developers 
could reuse the same dependencies in their projects.
To solve the problem of managing dependencies, Bower 
entered the JavaScript world. This helped developers to 
manage dependencies and share code, through the central 
registry, with other developers from around the world. But 
Bower had some shortcomings which were resolved by 
npm, which is the default package manager for Node.js. 
npm is good but it also has some shortcomings that have 
now been resolved by Yarn. 
Yarn
Yarn is a package manager for your code. It allows you 
to use and share code with other developers from around 
the world through a central registry and in other ways. 
Yarn does this quickly, securely, and reliably so you don’t 
ever have to worry. Yarn is fast, reliable and offers secure 
dependency management.
Code is shared through something called a package/
module. A package contains all the code being shared as well 
as a package.json file, which describes the package and 
dependencies.
Shown below is a sample format of package.json:
{
  “name”: “yarntest”,
  “version”: “1.0.0”,
Yarn: A Quick, Reliable and 
Safe Way to Share Code 
Yarn is a collaboration between Facebook, Exponent, Google and 
Tilde. It is a package manager which aims at more reliable and 
secure installs. It manages dependencies consistently across 
machines and also in a secure offline environment.

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 61
Developers
Let’s Try
  “description”: “”,
  “main”: “index.js”,
  “scripts”: {
    “test”: “echo \”Error: no test specified\” && exit 1”
  },
  “dependencies”: {
    “serialport”: “^4.0.0”
  },
  “author”: “”,
  “license”: “ISC”
}
Why Yarn?
npm is good, but it also has some shortcomings. Here is a list 
of some of them.
 
Nested dependencies: npm version 2 nests 
dependencies, which leads to duplicates. For users of 
Windows, file paths may get long, which causes problems 
with Windows when trying to delete them. To fix this 
problem, you should use npm version 3.
 
Queued install: It installs each dependency one after the 
other, which may take up a lot of time.
 
Single registry: If a package is not on the npm registry, 
then forget about downloading it through npm.
 
No offline installation: Every time you have to 
download dependencies from the npm registry, a working 
Internet connection is required, which takes up a lot of 
time and consumes more bandwidth.
 
Yarn overcomes most of the shortcomings of npm and 
provides additional power to manage dependencies.  Here 
are the powerful features that Yarn offers.
 
Ultra-fast: It caches every package it downloads, so 
it never needs to download it again. It also parallelises 
operations to maximise resource utilisation; so install 
times are faster than ever.
 
Offline mode: If you’ve installed a package before, you 
can install it again without any Internet connection.
 
Extremely secure: Yarn uses checksums to verify 
the integrity of every installed package before its 
code is executed.
 
Super reliable: Using a detailed, but concise, lockfile 
format, and a deterministic algorithm for installs, Yarn is 
able to guarantee that an install that worked on one system 
will work exactly the same way on any other system.
 
Same packages: You can install any package from npm 
and keep your package workflow the same.
 
Network resilience: A single request failing will not 
cause an install to fail. Requests are retried upon failure.
 
Flat mode: You can resolve mismatched versions 
of dependencies to a single version to avoid creating 
duplicates.
Yarn installation
Before using Yarn, you’ll first need to install it on your 
system. There is an increasing number of ways to install Yarn. 
But in this article, I will cover only some of the options.
1. Installation on Windows: This is done through the installer 
.msi file. Download it using the link https://yarnpkg.com/
latest.msi
Install via Chocolatey (package manager for Windows):
choco install yarn
Install via Scoop (command-line installer for Windows):
scoop install yarn
2. Installation on Centos, Fedora and RHEL Linux: Install 
it via the RPM package repository. 
Step 1: 
sudo wget https://dl.yarnpkg.com/rpm/yarn.repo -O /etc/yum.
repos.d/yarn.repo
Step 2: 
sudo yum install yarn
3. Installation on MacOS: Install Yarn through the 
Homebrew package manager. This will also install  
Node.js if it is not already installed.
brew install yarn
Once Yarn is installed, then by using the command given 
below, we can check what version it is:
yarn --version
As of now, Yarn’s current stable version is v0.27.5 and the 
latest unstable version is v.28.4 (Nightly Build – this release 
may have bugs).
Installing project dependencies
Here is a list of possible Yarn commands that help us to add and 
install project dependencies. 
1. yarn add: This command installs a package and any packages 
that it depends on. It installs the dependencies in the local 
node_modules directory and also an entry in the package.
json and yarn.lock files. So members of a team working on 
the same project can get the same modules installed on their 
machines by executing the yarn or yarn install commands. 
2. yarn install: This is used to install all dependencies for 
a project listed within package.json in the node_modules 
folder. It is most commonly used when you have 
just checked out code for a project, or when another 
developer on the project has added a new dependency that 
you need to pick up.

62 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Let’s Try
Some yarn add commands
yarn add <package …> [--exact/-E]: Using –exact or -E 
installs the exact version of the package. The default is to use 
the most recent release with the same major version.  
yarn add <package …> [--tilde/-T]: Using –tilde or 
-T installs the most recent release of the packages that have 
the same minor version. The default is to use the most recent 
release with the same major version. For example, yarn add 
debug@1.2.3 –tilde would accept 1.2.9 but not 1.3.0.
 
yarn add package-name@tag: This command is used to 
install a package of a specified tag, e.g., beta, next or latest.
 
yarn add <package…> [--dev/-D]: Using –dev or -D 
will install one or more packages in devDependencies in 
package.json.
 
By default, all these packages get installed from the npm 
registry, but we can specify a local folder path, URL, 
gzip tarball local file path, Git repository URL, etc. A few 
examples are given below.
 
yarn add package-name: Installs the package from 
the npm registry unless we specify another one in 
package.json.
 
yarn add <file:/local/local/folder>: Installs a package 
that is on your local file system. This is useful to test out 
other packages of yours that haven’t been published to the 
public/ private registry yet.
 
yarn add <file:/local/foder/tarball.tgz>: Installs a 
package from a gzipped tarball, which could be used to 
share a package before publishing it.
 
yarn add <git remote url>: Installs a package from a 
remote Git repository.
 
yarn add <git remote url>#<branch/commit/tag>: 
Installs a package from a remote Git repository at a 
specific Git branch, Git commit or Git tag.
 
yarn add <https://my-project.org/package.tgz>: Installs a 
package from a remote gzipped tarball.
 
In case you are using npm, you would use –save or 
--save-dev. In Yarn, these have been replaced by yarn 
add and yarn add –dev.
Some yarn install commands
 
yarn install –check-files: Verifies that files already 
installed in node_modules are not removed.
 
yarn install –force: Re-fetches all the packages, even 
ones that were previously installed.
 
yarn install –ignore-scripts: Does not execute any scripts 
defined in the project package.json and its dependencies. 
 
yarn install –modules-folder <path>: By default, 
packages get installed in the project node_modules 
directory. With this command, you can specify a different 
path to install all dependencies.
 
yarn install –no-lockfile: By default, for every 
installation, Yarn makes an entry in the yarn.lock file. 
This command instructs Yarn to neither read nor generate 
a yarn.lock lockfile.  
 
yarn install –production[true|false]: Yarn will not install 
any package listed in devDependencies if the NODE_ENV 
environment variable is set to production. Use this flag 
to instruct Yarn to ignore NODE_ENV, and to take its 
production-or-not status instead.
 
yarn install –offline: Runs yarn install in offline mode.
Managing dependencies
Upgrading or deleting packages will automatically update 
package.json and yarn.lock files. Other developers working 
on the project can run yarn install to sync their own node_
modules directories with the updated set of dependencies.
When we remove a package, it gets removed from prod, 
dev dependencies.
yarn remove [package-name] 
ex: yarn remove mongoose
Packages can also be upgraded to the latest or a 
lower version.
yarn upgrade [package]
yarn upgrade [package]@[version]
yarn upgrade [package]@[tag]
Other useful Yarn commands
Yarn provides rich sets of commands, but I will explain 
only some of them.
After installing Node.js and Yarn, we can start using the 
Yarn commands to manage dependencies in our projects. 
yarn init: This is the first command we should run 
to create the package.json file, which is used to manage 
information like the project’s name, version or licence 
information, as well as the author’s and contributors’ 
names – basically, the details of the most important project 
dependencies. This command walks us through an interactive 
session to create a package.json file.
yarn config commands: Here is a list of a few of these.
 
yarn config list: Displays the current configuration.
 
yarn config set <key> <value> [-g| --global]: Sets the 
config key to a certain value.
 
yarn config get <key>: Echoes the value for a 
given key to stdout.
 
yarn config delete <key>: Deletes a given key 
from the config.
yarn cache commands: These list, clean and change the 
cache directory.
 
yarn cache ls: Yarn stores every package in a global cache 
in your user directory on the file system. This command 
will print out every cached package.
 
yarn cache dir: This command will print out the path 
where Yarn’s global cache is currently stored.
 
yarn cache clean: This will clear the global cache. It will 
be populated again the next time yarn or yarn install is 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 63
Developers
Let’s Try
run. Additionally, we can specify the name of the package 
we want to clean.
 
yarn config set cache-folder <path>: Sets cache-
folder config value to configure the cache directory.
yarn clean: This command frees up space by removing 
unnecessary files and folders from package dependencies. It is 
useful in an environment where packages are checked into the 
version control directly.
On command execution, Yarn will create a .yarnclean 
file that should be added to version control. Cleaning is 
then automatically done as part of yarn install (or simply 
yarn) and yarn add.
 Note: As a best practice, it is recommended that you do not 
use this command.  This command uses a heuristic to identify 
files that may not be needed from a distributed package and 
may not be entirely safe. This command is recommended only 
if you experience issues with the number of files that are 
installed as part of node_modules. 
yarn info <package> [field]: This command will fetch 
information about a package and return it in a tree format. The 
package need not have been installed locally. 
Example: yarn info express or yarn info express 
express@1.15.0. 
Note that, by default, yarn info will not return 
the readme field (since it is often very long). To explicitly 
request that field, use yarn info react readme.
Yarn commands for managing package owners: 
Developers can write their own package and publish it either in 
a private or public registry. A package ‘owner’ in the registry is 
a user who has access to make changes to a package. A single 
package can have as many owners as you want.
Owners have permission to do the following tasks:
1. Publish new versions of the package
2. Add or remove other owners of the package
3. Change the metadata for a package
The following table lists a few yarn owner commands 
and their applications.
yarn owner ls 
<package>
Lists all of the owners of a <package>.
yarn owner 
add <user> 
<package>
Adds the <user> as an owner of 
the <package>. You must already be 
an owner of the <package> in order to 
run this command
yarn owner 
rm <user> 
<package>
Removes the <user> as an owner of 
the <package>. You must already be 
an owner of the <package> in order to 
run this command.
Commands for publishing a package to the npm 
registry: Once a package is published, you can never modify 
that specific version, so take care before publishing it. 
The following table lists a few yarn publish commands 
and their applications.
yarn publish
Publishes the package defined by 
the package.json in the current 
directory.
yarn publish 
[folder]
Publishes the package contained 
in the specified folder. Project 
package.json should specify the 
package details.
yarn pub-
lish –access 
<public|restricted>
The –access flag controls 
whether the npm registry publishes 
this package as a public package, 
or is restricted.
Yarn publish –new-
version <version>
Skips the prompt for the new ver-
sion by using the value of version 
instead.
Command for running a defined package script in 
package.json: Define a scripts objects in your package.json 
file like the one I have defined in the code given below:
{
          “name”: “my-package-name”,
          “scripts”: {
                   “build”: “babelsrc-dlib”,
                   “test”: “test-code”
          }
}
Here, executing the command yarn run test on console 
will execute the script named ‘test-code’ defined in your 
package.json.
Yarn is highly compatible with npm. Projects built 
using Yarn can still be installed via npm, and vice versa. 
I have been using it for a long time and till now have not 
found any problems with it. The Yarn project is backed 
by companies like Google and Facebook; so I believe it 
will be developed actively.
Yarn is not supposed to replace npm; rather, it provides 
an improved set of features. It uses the same package.json 
file and saves dependencies to node_modules.
In conclusion, both npm and Yarn are great dependency 
management tools, but I prefer to use the latter. 
By: Manish Sharma
The author has a master’s degree in computer applications, 
and is currently working as a technology architect at Infosys, 
Chandigarh. He can be reached at cloudtechgig@gmail.com.
https://yarnpkg.com
Reference

64 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
How To
A
ndroid, which was declared open source from the 
year 2007, works on Java and C++ as its official 
languages. In May 2017, at the Google I/O, it was 
announced that Kotlin, a statically typed programming 
language that is 100 per cent interoperable with Java, was also 
an official language of Android. 
For current Android developers, Kotlin is a chance to use 
an Android-centric language to solve the usual problems, 
such as runtime exceptions and Java’s lengthy coding. Google 
assures continued support for Java, which means that your 
existing coding patterns are preserved. Kotlin, on the other 
hand, is an easy solution for a beginner to get started with 
Android’s own programming language.
Configuring Kotlin
Android Studio 3.0 comes with pre-installed Kotlin. For all 
other versions below 3.0, the user needs to install Kotlin 
manually as follows:
 
Open Settings (Preferences in Mac).
 
 Select Plugins and search for Kotlin (Figure 1).
 
 Hit Install to add Kotlin to your Android Studio.
Now, if the Android Studio version is not 3.0, we need 
to add the following code into the build.gradle (Project: 
Kotlin is a statically typed programming language specially designed for modern 
multi-platform applications. Experiment and use it to become the proud creator of 
your own Android app by following the simple instructions in this article.
MyApplication) file (located at the bottom of the left side 
of the Project window, inside Gradle Scripts), under the 
dependencies section.
classpath “org.jetbrains.kotlin:kotlin-gradle-
plugin:$kotlin_version”
classpath “org.jetbrains.kotlin:kotlin-android-
extensions:$kotlin_version”
Also add the Kotlin version into the buildscript 
section, as follows:
ext.kotlin_version = ‘1.1.x’
Now, add the following line to the build.gradle (Module: 
app) file, as the second line, followed by the com.android.
application plugin:
apply plugin: ‘kotlin-android’
Creating a new project with Kotlin
Android Studio 3.0 users can directly select Kotlin as their 
default language while creating a new project; as usual, 
Developing a Simple Android 
Application Using Kotlin

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 65
Developers
How To
in Kotlin. Unlike 
Java, Kotlin requires 
explicit annotations for 
overridable members 
for overrides.
We can see a 
considerable difference 
in the Java and Kotlin 
syntax. Kotlin is a 
simple and easy-to-code 
language, allowing users 
to skip the headache 
of verbosity.
Coding with Kotlin: The layout design
Let’s create our simple application, which will print 
‘Welcome to OSFY’ on the screen. Here we need a 
TextView, which we will refer to and set a text to it. Let’s 
now open the layout file, which is located at app > res > 
layout > activity_main.xml (the default name). Figure 4 
shows the location.
Here, we can see that a TextView has already been 
added to the layout file. Switch to the ‘Design’ section from 
the TextView (this can be changed at the bottom part of 
the displayed window, containing code). Next, select the 
TextView from the Component Tree window (Figure 5).
In Figure 5, on the left, we have the Palette window, 
where we can select and add more widgets to the application 
screen. To the right, we have the Properties window, where 
we can make property changes to the selected TextView.
The most important property is the ID, so provide a 
suitable name, or rather, set the ID value as ‘textview’ itself 
for easy identification. This ID is used to uniquely identify 
the particular element in our Java/Kotlin code. We can 
select File > New > New Project. The slide asking for the 
project’s name will have an option underneath, ‘Include 
Kotlin Support’. Checking the box will enable Kotlin, and 
this is the starting activity — the MainActivity (default 
name) named MainActivity.kt. For other versions, the 
user needs to convert the existing Java class to Kotlin, by 
opening the MainActivity.java file, and then selecting Code 
> Convert Java file to Kotlin file (Figure 2).
Figure 3 shows the differences between Java class and 
Kotlin class. The following differences can be noted:
1. The extension has changed from .Java to .kt.
2. extends AppCompatActivity is changed to: 
AppCompatActivity(). In Kotlin, the declaration of 
a variable name is followed by a ‘:’, after which the 
datatype is specified.
3. ‘override’ becomes part of the onCreate function 
and the function syntax is changed to ‘fun onCreate’ 
Figure 1: Adding Kotlin to Android Studio
Figure 2: Converting Java to Kotlin
Figure 3: Differences between the Java and the Kotlin class
Figure 4: Location of the layout file
Figure 5: Selecting the TextView element from the Component Tree window

66 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
How To
also view many other properties, such as setting size, giving 
colour, etc, by selecting the ‘View all properties’ link at the 
bottom of the Properties window.
Coding with Kotlin: The Java/Kotlin part
In normal Java coding, we need the following code to refer to a 
TextView, and for setting a text to the above added TextView:
TextView t;
t=(TextView) findViewById(R.id.textview);
t.setText(“Welcome to OSFY”);
The above code is used to set a text to the TextView 
element, with the ID ‘textview’ added to the layout file, in 
Java coding.
In Kotlin, the following code is used:
val t: TextView = findViewById(R.id.textview) as TextView;
t.setText(“Welcome to OSFY”);
Here we can see the changes in the coding, while using 
Java and Kotlin. Also note that if there is any object we 
declare in the code, which has a null value as default, it will 
not compile in Kotlin. That is:
//Java
String t;
t=null;
//Kotlin
val t: String = null
The above code will execute in Java, but will not compile 
in Kotlin. This is because Kotlin provides ‘null safety’ or, in 
other words, Kotlin marks an end to the long-time problem of 
NullPointerException.
Running your first project using Kotlin
In order to run the application, we need to click the Run 
button located on the top panel of Android Studio. A 
window to choose the device will appear (Figure 6). We can 
either use an Android virtual device, or an Android device 
connected to the computer. 
If you are using an Android device, 
then you must enable developer 
options in it, before enabling Android 
Debugging in the menu options. 
Kotlin and Oreo
Android recently announced its next 
Android version, Android Oreo, along 
with sparkling new development 
features. These include downloadable 
fonts, font support in XML, emoji 
compatibility, auto sizing TextView, and 
other style and theme options.
What we have just learnt about the 
features of Kotlin, which is an Android-centric programming 
language, will help us to work with the new Android features.  
For example, let’s look at auto-sizing TextView. When 
we code for older versions, prior to Oreo, we need to design 
separate design files for varying screen sizes, which fit 
the screen features, like height, width, etc. But in Oreo, 
we have an auto-sizing option, which will auto-resize the 
text according to the target device screen. For this, the 
following code is used:
<?xml version="1.0" encoding="utf-8"?>
<TextView
    android:layout_width="match_parent"
    android:autoSizeTextType="uniform" />
A new property called autoSizeTextType has been 
introduced in Oreo, which will control the auto sizing of 
the TextView. The value can either be uniform or none, 
based on the requirement. We have already learned that 
connecting elements is easier using Kotlin. This is evident 
when using the auto-sized TextView programmatically 
with Kotlin, as follows:
val t: TextView = findViewById(R.id.textview) as TextView;
t.setText(“Welcome to OSFY”);
TextViewCompat.setAutoSizeTextTypeWithDefaults(t,TextViewComp
at.AUTO_SIZE_TEXT_TYPE_UNIFORM);
TextViewCompat.setAutoSizeTextTypeWithDefaults is used 
for the purpose, where the arguments are TextView instant 
and auto size text type, which can be either TextViewCompat.
AUTO_SIZE_TEXT_TYPE_NONE or TextViewCompat.
AUTO_SIZE_TEXT_TYPE_UNIFORM. 
By: Jis Joe Mathew
The author is assistant professor of computer science and 
engineering at Amal Jyoti College, Kanirapally, Kerala. He can be 
contacted at jisjoemathew@gmail.com.
Figure 6: Selecting the device to run the application
Figure 7: The output 
of the program on the 
device screen

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 67
P
haserJS is a popular, free and open source HTML5 
framework for game development on mobile and 
desktop browser platforms. It uses HTML5 Canvas 
for rendering and also WebGL if the device or browser 
supports it. It is one of the most popular and easy-to-learn 
game development frameworks and is based on pure Web 
technologies such as HTML, CSS and JavaScript. With over 
15,000 stars and almost 5000 forks at the time of writing 
this article, it is also one of the most popular and actively 
maintained game frameworks on GitHub.
Thousands of developers all around the world use Phaser, 
as it has some great features which are listed below.
Supports JavaScript and TypeScript: PhaserJS supports 
both JavaScript and TypeScript; so either of these languages 
can be used to develop games.
Uses WebGL and Canvas: PhaserJS uses both WebGL 
and Canvas internally, and has the ability to switch between 
them based on browser support. This allows fast and robust 
rendering across various mobile and desktop platforms.
Optimised for mobile browsers: PhaserJS was built 
keeping the mobile Web browser as the target platform, so it is 
highly optimised for mobile as well as for desktop browsers.
Built-in physics engines: PhaserJS has three built-in 
physics engines — Arcade Physics, a lightweight engine 
useful for developing arcade style games; Ninja Physics for 
advanced tile support; and p2.js, which supports character 
rigging, constraints and advanced polygon support.
Assets preloader: Assets, images, sprites, sheets, sounds, 
tilemaps, JSON data, XML, etc, are loaded with ease, 
parsed and handled automatically—to be used in games and 
stored in the global cache.
This tutorial will help you build a 2D platform arcade style game, using the 
PhaserJS framework, within a few minutes.
Particles, animation and sprites: PhaserJS has a built-in 
particle system, which allows developers to easily create cool 
particle effects  such as rain, fire, snow and water. It supports 
Flash CS6/CC JSON files to create animation for sprites.
Input support: It supports multi-touch, mouse and 
keyboard. It also allows the user to code custom gesture control.
Plugin system: PhaserJS allows developers to 
create their own plugins for their games and share them 
with the community.
Games created using PhaserJS depend on and require 
a modern browser that supports the Canvas tag, which 
includes Chrome, Internet Explorer 9+, Firefox, Safari and 
Opera. It also works on mobile Web browsers such as stock 
Android 2.x browsers and later, and Mobile Safari for iOS5 
and later versions.
Tip: The PhaserJS framework is open source and the 
source code of the framework is available on GitHub. 
Developers/programmers are encouraged to participate and 
contribute to the project.
Installing the prerequisites
Before we start developing our game using PhaserJS, we 
need to install Node.js, as the PhaserJS framework is also 
distributed as an npm package and this makes it easy to set up 
the project structures to develop games.
 Note:  It is assumed that you have some basic 
knowledge of Web technologies like HTML, CSS 
and JavaScript. If you don’t, W3Schools (http://www.
w3schools.com/) is a good place to start. The site has some 
great tutorials for Web technologies that are easy to follow.
Using PhaserJS to Speed Up 
2D Game Development

Developers
How To
68 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Installing Node.js
Download and install Node.js (https://nodejs.org/). PhaserJS 
is also distributed as an npm package, so we will use Node 
and npm to install the PhaserJS framework.
Installing Phaser JS
We can install PhaserJS as a node package. Just run 
the following command in the terminal or at the 
command prompt:
npm install phaser
This command will install the PhaserJS framework.
Installing an HTTP server
To run the PhaserJS project, we need to install an HTTP 
server. Run the following command in a terminal or at the 
command prompt:
 npm install -g http-server
This will install an HTTP server as a global module.
Testing if everything is installed properly
Create a folder for your project, and change directory to it. 
Run the following commands at the command prompt or in 
the terminal:
mkdir my-game
cd my-game
Initialise the folder using the following npm command:
npm init –y
This will create a package.json manifest file with vital 
default information about the project, and will add the 
necessary node modules.
Now create an index.html file and add the following 
code to it. 
<!doctype html>
<head>
    <title>My Game</title>
    <script type=”text/javascript” src=”node_modules/phaser/
build/phaser.min.js”></script>
</head>
<body>
    <div id=”helloWorld”></div>
</body>
<script>
var game = new Phaser.Game(640, 480, Phaser.AUTO, 
‘helloWorld’, {
    create: create
});
function create() {
    var text = “OPEN SOURCE FOR YOU”;
    var style = {
        font: “45px “,
        fill: “#ff0000”,
        align: “center”
    };
    var t = game.add.text(game.world.centerX, 240, text, 
style);
    t.anchor.set(0.5);
}
</script>
</html>
Finally, start the server by running the following 
command at the command prompt:
hs
The hs command will start the HTTP server. Now, load 
http://localhost:8080 in your browser to run the project. If 
you see the output screen shown in Figure 1, then you have 
successfully installed all the dependencies required.
Creating a simple 2D platform game
Now we are going to create a simple 2D platform game, that 
has a character who collects rings and scores points. The game 
consists of walls, on which the character can jump in order 
to collect the rings and score points. The player will use the 
arrows on the keyboard to control the character’s movements.
I have created some assets, like the background image, 
game sprites, etc. You can download these from http://www.
aniket.co/labs/phaser/assets.zip. We are going to use these to 
develop this game. This zip file consists of four asset files — 
Figure 1: The my-game project running in the browser

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 69
the sky, the player, the platform and the ring that will be used 
for our game.
Create a new folder that we can call ring-collector. Move 
into that directory.
mkdir ring-collector
cd ring-collector
Initialise the folder using npm, and install PhaserJS again 
for this folder as well. It will pull all the necessary node_
modules. Run the following commands in a terminal or at the 
command prompt:
npm init -y
npm install phaser
Create a folder called assets and extract the downloaded 
assets.zip into it. Create an index.html file, and add the 
following code into it:
<!doctype html> 
<head> 
    <title>Ring Collector</title>
 
<script type=”text/javascript” src=”node_modules/phaser/
build/phaser.min.js”></script>
</head>
<body>
<script type=”text/javascript” src=”game.js”>
</script>
</body>
</html>
In the index.html file, load the PhaserJS framework and 
our game’s logic.
Now create a file called game.js in the same folder, and add 
the following code. I have also added comments for each line of 
code to explain what it actually does and why we are using it.
let game = new Phaser.Game(800, 600, Phaser.AUTO, ‘’, { 
preload: preload, create: create, update: update });
function preload() {
 
//Loading assets such as background image, object image, 
game sprites.
    game.load.image(‘sky’, ‘assets/sky.png’);
    game.load.image(‘ground’, ‘assets/platform.png’);
    game.load.image(‘ring’, ‘assets/ring.png’);
    game.load.image(‘player’, ‘assets/player.png’);
}
// Variables
let player;
let platforms;
let cursors;
let rings;
let points = 0;
let pointsText;
function create() {
 
// Since we are building an arcade style game, we enable 
arcade physics system using following function
    game.physics.startSystem(Phaser.Physics.ARCADE);
    // This function will add ‘sky’ asset as background to 
our canvas.  
    game.add.sprite(0, 0, ‘sky’);
    // The platforms group contains the ground and the 3 
ledges we can jump on
    platforms = game.add.group();
    // Enable physics for any object that is created in this 
group
    platforms.enableBody = true;
    // Here we create the ground.
    let ground = platforms.create(0, game.world.height - 64, 
‘ground’);
    // Scale it to fit the width of the game (the original 
sprite is 400x32 in size)
    ground.scale.setTo(2, 2);
    // This stops it from falling away when you jump on it
    ground.body.immovable = true;
    // Now let’s create three ledges, at 3 different 
locations
    let ledge = platforms.create(400, 400, ‘ground’);
    ledge.body.immovable = true;
    ledge = platforms.create(-150, 250, ‘ground’);
    ledge.body.immovable = true;
 
    ledge = platforms.create(400, 150, ‘ground’);
    ledge.body.immovable = true;
    // The player and its settings, such as height and width
    player = game.add.sprite(32, game.world.height - 150, 
‘player’);
    // Enable physics on the player
    game.physics.arcade.enable(player);
    // Player physics properties, such as bounce and bounding 
it to canvas only so it does not go off screen
    player.body.bounce.y = 0.2;
    player.body.gravity.y = 300;
    player.body.collideWorldBounds = true;

Developers
How To
70 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
    // Adding some rings to collect
    rings = game.add.group();
    // We will also enable physics for any ring that is 
created in this group
    rings.enableBody = true;
    // Here we’ll create 10 of them evenly spaced apart
    for (let i = 0; i < 10; i++)
    {
        // Create a ring inside of the ‘rings’ group
        let ring = rings.create(i * 70, 0, ‘ring’);
        // Add some gravity
        ring.body.gravity.y = 300;
   }
    // The pointsText displays current score in top right 
corner of canvas
    pointsText = game.add.text(624, 16, ‘Points: 0’, { 
fontSize: ‘32px’, fill: ‘#000’ });
    // Using keyboard controls
    cursors = game.input.keyboard.createCursorKeys();    
}
function update() {
    // Collide the player and the rings with the platforms
    game.physics.arcade.collide(player, platforms);
    game.physics.arcade.collide(rings, platforms);
    // Checks if the player overlaps with any of the rings, 
if he does call the collectRing function
    game.physics.arcade.overlap(player, rings, collectRing, 
null, this);
    // Reset the players velocity (movement), if no key is 
pressed
    player.body.velocity.x = 0;
 
 
// If left or right key is pressed
    if (cursors.left.isDown) 
    {
        // Move left
        player.body.velocity.x = -150;
    }
    else if (cursors.right.isDown) 
    {
        // Move right
        player.body.velocity.x = 150;
    }
    else
    {
        // Dont move
        player.animations.stop();
        player.frame = 4;
    }
    
    //  Allows the players to jump only if they are touching 
the ground.
    if (cursors.up.isDown && player.body.touching.down)
    {
        player.body.velocity.y = -350;
    }
}
function collectRing (player, ring) {
    
    // Removes the ring from the screen, when player touches 
the ring
    ring.kill();
    //  Add and update the score points
    points = points + 10;
    pointsText.text = ‘Points: ‘ + points;
}
Here, on the first line, we have created an instance of a 
Phaser.Game object and assigned it to the game variable. 
The first two parameters in the Phaser.Game function are the 
width and height of the canvas that Phaser creates. The third 
parameter can be Phaser.CANVAS, Phaser.WEBGL or Phaser.
AUTO, which are for rendering the setting you want to use for 
your game. I recommend that you use Phaser.AUTO, which 
will automatically try to use either WebGL or Canvas, based 
on whichever the browser supports. The fourth parameter is 
an empty string. Here, you can give the ID of a DOM element 
where you would like to add the canvas element on the page. 
We have left it blank, so by default it will be appended to the 
body element. The last parameter is an object that consists of 
three references to Phaser’s essential functions.
The essential functions are preload(), create() and update().
preload(): This is used to load the assets needed for our 
game. PhaserJS will automatically look for this function when 
it starts, and load any asset defined within it.
create(): This function is used to create the game scene. 
Here, we add the assets loaded such as the background image, 
platform, player/characters and other game sprites.
update(): This function is called by the core game loop 
in every frame. It helps to check what is happening in every 
frame, like if there is a collision between the player and 
object, score updates, etc.
Finally, you should end up with the following 
directory structure:
assets/
• 
platform.png

Developers
How To
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 71
• 
player.png
• 
ring.png
• 
sky.png
node_modules/
• 
phaser/
game.js
index.html
package.json
Now that we have added the code, let us start a server 
and see the final outcome. Run the following command in the 
same directory:
hs
The hs command will start the HTTP server. Now, load 
http://localhost:8080 in your browser to run the project. 
If you see the output screen shown in Figure 2, then you 
have successfully created a simple 2D platform arcade style 
game. You can use the arrows on the keyboard to control the 
character and collect rings.
I have uploaded this sample project to my GitHub 
repository at https://github.com/aniketkudale/ring-collector. 
You can download it for reference. 
You will also find various game physics examples and 
techniques at https://phaser.io/examples, which you can use in 
your game to make it more fun-filled. 
Figure 2: The game we developed is launched in the browser
[1]  https://phaser.io/
[2]  https://photonstorm.github.io/phaser-ce/ 
References
By: Aniket Eknath Kudale 
The author is presently employed at TIBCO Software Inc., Pune, 
and has more than three years’ of work experience. His interests 
include Web technologies, computer vision and security. You can 
reach him at kudale@aniket.co.

72 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
How To
With the Pipeline suite of plugins, the mantra of Jenkins 2.0—of ‘Pipeline as Code’ —is 
gaining momentum. This is evident from the growing number of organisations embracing 
this new way of codifying build flow. This article’s objective is to lower the entry banner 
for the DevOps and engineering teams that are at the threshold of switching over to 
Jenkins 2.0 and Pipeline as Code, and help them get started.
Get Started with 
Jenkins 2.0: Pipeline as Code
F
or many years, 
Jenkins has been 
the most preferred 
choice when it comes to 
practising automated continuous 
integration. The traditional (also 
known as freestyle) way of setting 
up new projects in Jenkins has been 
(and still is) through the graphical user 
interface. With the help of the GUI, pre- and 
post-build tasks, including handling parameters 
and injecting custom environment variables, can be 
glued together in a single project.
With the release of Jenkins 2.0, there has been a shift in 
the strategy to set up new projects in Jenkins. Almost all the 
build tasks are treated as Pipeline as Code, using the suite 
of Pipeline plugins, which means that the complete build 
flow can be codified. Therefore, it can be tracked in version 
control, thus enabling collaboration across teams. For a quick 
overview and highlights of the features in Jenkins 2.0, visit 
https://jenkins.io/2.0/.
Jenkinsfile is where Pipeline as Code is maintained, and it 
should be placed in the root directory of the source code. 
Partitioning CI/CD (Continuous Integration/Continuous 
Deployment) into logically unique but interconnected parts, 
and the visual representation of this entire chain of events, is 
termed as build flow and this is what makes Pipeline attractive.
Pipeline is gradually evolving into the new way to 
programmatically construct a logical flow of tasks — for 
example, compile source code, generate artifacts and deploy 
them – all by using a variety of plugins and third-party tools. 
However, it should be emphasised that the build system 
(make, ant, or a mix of scripts in whatever language you 
choose) does not need to be revamped 
and can be reused as is.
Prerequisites or assumptions
Before we proceed further, make sure 
you have access to Jenkins (v2.3x and 
above) with standard plugins, and are 
able to set up a test project. You should 
also have the basic familiarity to set up 
new projects in Jenkins.
This article will be based on the syntax of 
scripted Pipeline, a general-purpose DSL that’s 
based on the Groovy programming language. However, 
the declarative syntax can be used too to achieve the 
same results with minor modifications. 
The definitive place to start reading about Jenkins 
Pipeline is https://jenkins.io/doc/book/pipeline/. 
The link https://jenkins.io/doc/pipeline/steps/ is the reference 
for various steps supported in the Pipeline environment. 
A comparison of the two syntaxes (imperative and 
declarative) is documented at https://jenkins.io/doc/book/
pipeline/syntax/#compare.
Anatomy 
Let’s begin with analysing Pipeline as Code by focusing on 
the individual stages, one at a time. Groovy code runs in a 
JVM environment in a build agent (not called slaves any 
more) and in a tightly controlled environment, i.e., a Groovy 
sandbox. This sandbox environment ensures that Pipeline as 
Code cannot be used to invoke system calls. These calls are 
made available by Jenkins for administrative purposes – so 
use them with caution!
An easy and quick reference on how to auto-generate 
code snippets for most of the Pipeline steps is available with 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 73
Developers
How To
the option Pipeline Syntax (as shown in 
Figure 1), which can be accessed from 
the Jenkins project’s landing page, if the 
project is set up using Pipeline-style. This 
will launch a new tab/window, as per 
the browser’s configuration, providing 
the dropdown list of the various steps 
available, along with the options and a 
small text box to display the generated 
code snippet (as seen in Figure 2) for the 
selected step.
Here is a snippet of Pipeline as Code:
#!groovy
node(‘label’) { 
  stage(‘Prep’) {
    git url:’https://github.com/your-name/your-repo-path.
git’, branch: ‘master’
  }
}
It’s a good practice to start the script with the shebang 
header #!groovy, which helps in code highlighting. Some 
of the best practices in writing Jenkins Pipeline are listed at 
https://dzone.com/articles/top-10-best-practices-for-jenkins-
pipeline?fromrel=true. The ‘node’ block helps to group the 
various stages, and execute the steps defined in the stages in the 
build agents that have the ‘label’. stage block groups different 
individual steps, which are executed on the build agent. A few 
examples of the commonly used steps are git, sh and echo.
Single-line comments are preceded by two forward 
slashes ( // ), and comments spanning multiple lines start with 
/* and end with */.
/*
This is a multi-line comment to highlight its usage and it 
ends here.
*/
Build flow
Let’s construct an example of a scripted pipeline to: i) 
prepare the workspace and download the source code, ii) 
compile the source code (a small program written in C 
language), and iii) archive the artifacts. These tasks will be 
codified using scripted Pipeline syntax. For the purposes 
of this discussion, the Jenkins project has been created and 
named ‘OSFY-pipeline demo’.
Jenkinsfile in the link, https://github.com/mramanathan/
osfy-pipeline-demo/blob/master/pipeline_bin/Jenkinsfile., is 
the complete source (written in Groovy) for this demo project.
Though the Jenkins documentation mandates that this file 
be named Jenkinsfile and placed at the root of the repository, 
in our example, it is named Jenkinsfile.groovy and kept 
under the folder pipeline_bin. This convention of placing the 
Jenkinsfile at the root of the repo should be followed while 
setting up the project configuration in Jenkins. 
A clean start
To ensure that every trigger of the OSFY project starts a fresh 
build job with a clean workspace, we use the step deleteDir(). 
While node and stage blocks are covered, a new step named dir 
is introduced here, and deleteDir is wrapped inside this step.
node(‘linux’) {
   stage(‘Clean Start’) {
     dir(‘demo-sources’) {
       deleteDir()
     }
   }
}
The contents of the folder, demo-sources, if present, will 
be removed. In the subsequent stages, the same folder will 
be reused to maintain continued access to the downloaded 
source code.
Download the source code
The git step references the master branch to download source 
code from the specified GitHub repo that’s referenced using 
the URL. This step can include other options, like using the 
credentials which may or may not be different from that 
set-up in the Jenkins master. The contents of the repo are 
downloaded and stored in demo-sources.
stage(‘Prepare Workspace’) {
   dir(‘demo-sources’) {
     git url: ‘https://github.com/mramanathan/osfy-pipeline-
demo.git’, branch: ‘master’
   }   
}
Figure 1: Pipeline syntax
Figure 2: Snippet generator
Figure 3: Script path
pipeline_bin/Jenkinsfile.groovy
Script Path
Steps
archiveArtifacts: Archive the artifacts
Sample Step
Files to archive
Advanced...
Generate Pipeline Script

74 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
How To
While the git step is enough 
to handle most of the scenarios 
to download the source code, 
situations might arise where the 
download (which is essentially a 
Git clone) could fail if the source 
code repository is quite large, i.e., 
not just in terms of source code 
but other assets, like, binary files, 
system images, build packages, and extensive meta data, like 
branches and tags. The remedy for this is available as another 
step— checkout.
Compiling the source code
From the dir, demo-sources, we will call the make utility 
program to compile the code and generate the object artifact. 
The make utility uses a makefile that helps to build a tree 
of dependencies, define targets and variables, and perform 
actions. This makefile is also available in the root of the 
GitHub repo, mentioned in the section titled ‘Build Flow’.
The source code is inside the src directory at the root 
of the GitHub repository. It is a tiny little program written 
in C language:
stage(‘Generate Binary’) {
   dir(‘demo-sources’) {
      sh “make pipimage”
   }
}
Archive
To enable the build artifact to be downloaded and used for 
further testing, we use the archive step to save the output 
from the previous compile stage. Once archived, the artifact 
can be downloaded from the project page in Jenkins for 
simple build projects. Jenkins supports integration with 
third-party artifactory tools, like, for example, JFrog and 
Maven, which help to version and promote the artifacts.
stage(‘Archive’) {
   dir(‘demo-sources’) {
      archive excludes: ‘*.txt’, includes: ‘pipimage’
   }
}
The build artifact, thus generated, can be downloaded 
by clicking on the down arrow button that’s available upon 
completing the build. Its details can also be checked on the 
project page.
Stage view
For every job run of the project that’s triggered either 
manually or automatically, the Pipeline plugin presents a 
visual representation of all the completed and failed stages. 
By: Ramanathan Muthaiah
The author works for a cyber security firm in Bengaluru. He enjoys 
the outdoors when he’s not churning out Jenkins Pipeline code.
Figure 5: Build artifact
Figure 6: Artifact download
Figure 7: Pipeline stages
Figure 8: Commit info
Figure 4: makefile
If the build is successful, then the stages are shown in green, 
else, the failed stage is shown in red and the build stops 
there without executing any further stages in red. 
If the Jenkins project is configured to trigger automatic 
builds on new commits in the configured GitHub repo, which 
can be enabled via WebHooks, then the commit information 
will be displayed along with the stages.
With this strategy to codify the entire build flow by 
breaking it up into its logical, constituent blocks and grouping 
them together, Jenkins Pipeline is in perfect alignment with 
the current trend to instrument the infrastructure using code. 
Of course, with the support of 1000+ plugins available in 
the ecosystem, there is a multitude of other possibilities to 
extend Jenkins capabilities, such as, to detect and set the 
build statuses, as well as send out notifications to Slack and 
HipChat. With BlueOcean, the revamped user interface (that 
offers the capability to write Jenkinsfile using declarative 
syntax), Jenkins is certainly evolving with the times.
For bigger projects, global Pipeline shared libraries reduce 
the maintenance burden by abstracting away the common 
tasks from the main Jenkinsfile. This approach brings down 
the entry barrier for engineering teams adapting or switching 
to Jenkins Pipeline. Refer to the Jenkins documentation at 
https://jenkins.io/doc/book/pipeline/shared-libraries/. 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 75
Developers
Insight
T
he module system in Java 9 is one of the most talked 
about and awaited feature. So, what’s it all about?
Java provides an encapsulation 
mechanism with the help of access modifiers, 
and any adequately sized Java project will 
have classes across multiple packages. If 
the project is large enough, you will have 
dependencies spanning multiple JAR files. 
Two problems with this approach are:
 
Not all the classes declared ‘public’ are part 
of the APIs that you might want to expose.
 
 What if two JARs have the same class 
declared under the same package (split 
packages)? This can lead to class loading 
issues that you will get to know about only at the runtime.
To solve these issues, Java 9 has introduced the concept 
of modules. A module is a higher-level abstraction over 
packages. To create a module, you need to add a module-
info.java file to your project, and explicitly define the 
dependencies on other modules using the requires keyword 
as well as the packages from which you want to expose 
the public classes using the exports keyword. Do note 
that other public classes inside any package(s) will not be 
accessible outside this module, other than the ones inside 
the exported packages. A sample package definition will 
look like what follows:
module my-module {
  exports com.example.java9.classes_to_be_exposed_as_apis;
  requires java.base;
}
JLink: For creating smaller Java runtimes
Java runtime has evolved considerably since its inception, 
and thousands of classes have been added to it to improve 
functionality, which has turned it into a huge monolith. Not 
all the features are used by most of the programs at any given 
time. For example, until Java 8, you could create a small 
‘Hello world’ program that only used classes from the java.
lang package, but the runtime would still include all the 
features (xml parsers, logging APIs, SQL functionality, etc) 
available in Java.
The JDK itself was rewritten using the module system, so 
there is no single rt.jar file that has all the features of Java; 
and it has been refactored into small individual modules. This 
means that you can create custom runtimes that only include 
the parts of JDK that are used in your module. Once you have 
After a couple of delays, Java 9 is finally here. Three years after Java 8, it comes 
with some of the biggest changes to the Java platform, adding more than 150 new 
features. In this article, we will cover some of the major changes. 
a Java module JAR file, you can use the JLink tool in the JDK 
to generate the runtime:
jlink --module-path <modulepath> --add-modules 
<modules> --limit-modules <modules> --output <path>
You can find the dependent modules required 
by your module using the jdeps tool. Just to give 
you some perspective, the standard JDK 9 runtime 
is around 430MB and for a simple ‘Hello World’ 
application, the runtime generated using the 
above command is around 24MB, which is 
great for devices with memory restrictions, for 
IoT applications and for custom runtimes to 
run inside containers like Docker. 
Private methods inside interfaces
Java 8 introduced features with which you could have default 
function implementations inside an interface but all the default 
implementations behaved like public methods. Well, now you 
can have private methods inside the interfaces that can be used 
by the default methods for better code organisation. 
public interface Car {
    void setCarColor(String color);// normal interface method
    default void startCar () {  
 
// default method 
 
System.out.println(“Car is starting...”);
 
startEngine();
 
System.out.println(“Car started! “);
    }
    private void startEngine() { 
 
// Private method not available outside the interface as 
an API
     System.out.println(“Engine Started “);
    }
}
JShell: The interactive Java REPL
Like Python, PHP and several other languages, now Java 
too features an interactive Read-Eval-Print-Loop. To get 
started, just type ‘jshell’ in the console and start typing 
your Java code:
C:\Program Files\Java\jdk-9\bin>jshell
What’s New in Java 9?

76 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Insight
|  Welcome to JShell -- Version 9-ea
|  For an introduction type: /help intro
jshell> int x= 10
x ==> 10
jshell> System.out.println(“value of x is: “+ x);
value of x is: 10
Native HTTP/2 and Web sockets 
Finally, the age old HttpURLConnection has been replaced 
with the new HttpClient in Java 9, which provides out-of-
the-box support for the HTTP/2 protocol, Web sockets and 
async HTTP requests. 
HttpClient client = HttpClient.newHttpClient();
HttpRequest req =
   HttpRequest.newBuilder(URI.create(“http://metalop.
com”))
              .header(“User-Agent”,”Java”)
              .GET()
              .build();
HttpResponse<String> response = client.send(req, 
HttpResponse.BodyHandler.asString());
System.out.println(response.statusCode());
System.out.println(response.body());
Beautiful, isn’t it? No InputStream or Reader is 
involved — instead, the API offers a BodyHandler which 
allows us to read the string directly from the response.
Enhancements to @Deprecated annotation 
Deprecated annotation is one of the three original built-
in annotations in Java. Over the years, users realised 
that it is quite ambiguous in nature. It didn’t provide a 
lot of information previously, like why something was 
deprecated; could it be used or would it be removed in 
future versions of the library; if it was to be removed, 
then at which version would this occur, etc. So, now 
the deprecated annotation provides additional fields 
like forRemoval and since for more lucidity. There was 
another small enhancement whereby, if you were using a 
deprecated class and suppressed the warning, there would 
still be a warning left in the place it was used due to the 
import statement, and there was no way to annotate the 
import statement. This issue has also been fixed in the 
current release of Java 9.
Multi-release JAR files
Multiple, Java-release-specific versions of class/resource 
files can now coexist in the same JAR file. This is good 
news for library maintainers, who will not be limited to 
using the minimum subset of the API provided by the least 
version of Java they want to support. 
A multi-release JAR file is one whose MANIFEST.MF 
file includes the entry Multi-Release: true in its main section. 
Also, META-INF contains a versions subdirectory whose 
integer-named subdirectories — starting with 9 (for Java 
9) — store version-specific class and resource files. JEP 238 
offers the following (enhanced) example:
JAR content root
  A.class
  B.class
  C.class
  D.class
  META-INF
     MANIFEST.MF
     versions
        9
           A.class
           B.class  
If this JAR is read by JDK 9, it will use classes A and B 
for Java 9 inside the 9 directory, but if it is read by a pre-Java 
9 JDK, it will read classes from the root.
Other updates in Java 9
Java 9 packs several other updates. For instance, JavaDocs 
now supports HTML 5 and the search feature, and new 
methods have been added to the Stream interface: dropWhile, 
takeWhile, ofNullable. The iterate method now also allows 
you to provide a predicate on when to stop iterating.
IntStream.iterate(1, i -> i <=10, i -> i + 1).forEach(System.
out::println);
The collections framework has been updated to 
now allow a population of collections at the time of 
creation, as shown below: 
List<Integer> integers = List.of(1, 2, 3);
There are several other new features in Java 9. To get a 
comprehensive list of all the new additions to Java 9, you can 
visit the release notes at https://docs.oracle.com/javase/9/
whatsnew/toc.htm. 
By: Shiva Saxena
The author is a FOSS enthusiast. He currently works as a 
consultant, and is involved in developing enterprise application 
and Software-as-a-Service (SaaS) products. He has hands-
on development experience with Android, Apache Camel, C#, 
.NET, Hadoop, HTML5, Java, OData, PHP, React, etc, and loves 
to explore new and bleeding-edge technologies. He can be 
reached at shivasaxena@outlook.com.


Tony Wasserman
“SOFTWARE HAS ALREADY 
BECOME PERVASIVE IN 
THE DEVELOPED AND THE 
DEVELOPING WORLDS”
While hardware is acquiring new forms, software is 
adapting to changes in the landscape. But how do 
developers benefit from this vicious cycle of hardware 
and software changes? Likewise, what should decision 
makers keep in mind while building new technology 
solutions? American computer scientist Tony 
Wasserman answers these questions in a conversation 
with Jagmeet Singh of OSFY. Wasserman also shares 
his views of the growth prospects of open source, 
based on his experience as a board member of the 
Open Source Initiative, professor of the software 
management practice at Carnegie Mellon, Silicon 
Valley and as the executive director of the CMU 
(Carnegie Mellon University) Center for Open Source 
Investigation. Edited excerpts...
Q
How do you see software 
evolving in the computing 
world?
Software has already become pervasive 
in the developed and in the developing 
worlds. Over the past decade, we have 
seen important advances in embedded 
software (now called the Internet of 
Things) and in data management and 
analytics, which are being applied 
to machine learning and artificial 
intelligence. These advances are 
leading to increasingly powerful 
autonomous systems, such as self-
driving cars, industrial and personal 
robots and surveillance systems. Even 
digital cameras, which are highly 
computerised, are now able to install 
additional apps.
While we will still need professional 
developers to create system level 
functions and development tools, there 
will be a growing role for ‘low code’ 
and ‘no code’ tools. The former will be 
used by software developers and may 
involve connecting and/or customising 
proven software components. The latter 
will be accessible to people without 
formal programmer training.
Professional developers will 
continue to use sophisticated 
programming languages and 
development environments, much as 
they do today, though the preferred 
languages may evolve over time.
Q
What are the major trends 
shaping up the IT industry?
The most important trends involve 
the continuing decrease in the cost of 
computer hardware, particularly for 
storage, along with the growth of cloud-
based computing services. Consumers 
have long become accustomed to using 
hosted services and they make extensive 
use of mobile devices, Chromebooks and 
tablets that connect to remote computing 
services. Enterprises and governments 
are becoming more aware of the costs of 
maintaining their own IT systems, and are 
For U & Me
Interview
78 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Interview

now much more comfortable relying on 
third parties for their primary computing 
resources. The use of cloud computing 
services reduces their overall computing 
costs, including the costs of maintaining 
and upgrading their own systems and of 
preventing security breaches.
Q
What is the ultimate impact 
of open source in the present 
software landscape?
The software landscape has seen a mix 
of proprietary and open source software 
throughout this century. In some areas, 
notably data management, open source 
projects have offered innovative and 
powerful solutions that are more 
advanced than those from proprietary 
vendors. Customers are choosing these 
technologies as much for their features 
as for the availability of the source code, 
which they are not likely to modify. But 
many customers are not comfortable 
with the community-based support 
found in many open source projects 
and prefer to pay for commercial 
support from a company that can offer 
guaranteed levels of service quality 
as well as have the ability to make 
any needed code changes to address 
critical software bugs. Open source 
projects are not likely to drive out 
proprietary software, if only because 
so many organisations have huge 
existing investments in those products. 
But free and open source software is 
likely to continue to be the driving 
force for collaborative projects and 
new technologies from R&D groups in 
academia and industry.
Q
What are the vital concerns 
raised by the software 
industry that are being addressed 
by the Center for Open Source 
Investigation (COSI) at Carnegie 
Mellon University, Silicon Valley?
The Center for Open Source 
Investigation (COSI) has a longstanding 
primary focus on evaluation, adoption, 
and the use of open source software 
by companies, governments and other 
organisations. A second important area 
is education about open source software. 
welcoming of new members. Initial 
contributions to such projects are rejected 
with harsh criticism that discourages 
people from continuing participation. 
Some projects have developed a ‘toxic 
culture’ that discriminates against various 
types of people. Also, projects tend to 
be monolingual, for instance, only in 
English or Hindi, so a person who is 
not fluent in that language may find 
it difficult to join the project. Even in 
more welcoming projects, programmers 
must adjust to the possibility that their 
contribution may not be accepted by the 
project committers.
Q
Is it difficult for big 
organisations to completely 
rely on FOSS?
Yes. There are some business areas such 
as enterprise resource planning and tax 
reporting in which proprietary solutions 
are more advanced than FOSS projects. 
Also, certain proprietary applications are 
the de facto standard for some common 
tasks. These applications include Adobe 
Photoshop, which has a dominant 
position for image manipulation and 
a vast community of customers who 
have been trained in its use. While there 
are indeed some FOSS alternatives to 
Photoshop available easily, they are 
widely regarded as not having the power 
offered by the proprietary solution.
In short, there is no reason for an 
organisation to rely completely on 
one type of solution. Organisations 
should instead be looking for the 
best solution for a specific need. It 
may be proprietary, open source, or a 
combination of the two. Similarly, the 
solution may run on the organisation's 
own systems, in the cloud, or as a 
hybrid of the two.
Q
Do you think there are still 
security concerns related to 
adopting open source?
Yes, this is the case even though various 
studies show that open source is at least 
as secure as proprietary software, and 
major security breaches have often 
occurred in proprietary systems. The fear 
is not rational, but most organisations are 
The OSSPAL project (originally 
the Business Readiness Rating) is 
intended to help people find high-
quality open source projects, and the 
newer FLOSSBOK project is aimed 
at identifying the wide range of topics 
related to open source software, along 
with sources for more information on 
them. Both projects rely on financial 
donations to support their work, and on 
volunteers to move them forward.
Q
How do open source 
developments help academia 
enhance skills in young talent?
Well-written open source code provides 
students with good examples of how to 
write code in a particular programming 
language and style. Also, as students 
try to join existing projects, they can 
learn about the various approaches to 
collaboration and about the tools that 
are used to track issues, manage the 
parts of a project, build new versions, 
and release them to the community.
Overall, open source development 
helps students to gain insights into the 
technology and process of software 
development, as well as on open source 
licences. They get excited the first time 
they make a contribution that is accepted 
into the project code base, and that 
serves as a motivation for continuing to 
participate in open source communities.
Q
What are the major obstacles 
programmers face when 
adapting to the open source 
culture?
Many open source projects are not very 
For U & Me
Interview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 79

naturally resistant to change. Thus, it is 
a slow process to get organisations to 
adopt open source for a business-critical 
system, and security issues are only one 
factor causing the delay.
Q
How can open source 
adoption help companies 
improve their profitability?
Many studies have shown that the total 
cost of ownership (TCO) for open 
source software is much lower than 
that of proprietary software. But as 
companies move more and more of 
their computing to the cloud, the TCO 
difference is reduced, so the financial 
impact of adopting open source is less 
than it once was. Increased profitability 
is a minor, but valuable, reason for 
choosing open source software.
Q
Do you see federal-level 
adoption having a positive 
effect on overall open source 
deployments?
Yes. Success stories in the use of any 
open source product (not just software) 
encourage people to see if they might 
benefit from following the same idea. 
In some countries, notably Brazil, the 
government not only uses open source 
software but has also developed a body 
of ‘public software’ that can be used 
by others, and requires its suppliers to 
deliver their solutions as open source 
software. The US government has 
increased its open source efforts across 
most federal departments and makes 
some of its open source code available 
through code.gov. Efforts such as these 
show that open source software projects 
can be successful and deliver value 
to the community.
Q
Lastly, what is the role of 
organisations like Open Source 
Initiative and Linux Foundation in 
taking open source to new heights? 
The Linux Foundation has greatly 
improved its outreach and now serves as the 
umbrella organisation for a large number 
of open source projects. Beyond that, it has 
organised the Open Source Summit in the 
US and Europe, both of which draw large 
audiences that include many people with 
very little open source experience. The 
Open Source Initiative (OSI) continues 
to focus on licence approvals, but has 
also established individual and affiliate 
membership programmes aimed at building 
its own community. However, the OSI is 
relatively small and works primarily within 
the existing open source community, so 
it does not have the overall impact of 
some other organisations. The OSI and 
the Linux Foundation have the strongest 
influence in North America and must be 
complemented by similar organisations 
elsewhere around the world. Such groups, 
taken together, can help to promote the 
value of open source software. 
For U & Me
Interview
80 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com


Open Source India 2017
Here’s a pictorial overview of all that took place at this mega event for open source enthusiasts.
O
ctober 2017 witnessed 
a mega open source 
event – the 14th 
edition of the annual, much-
awaited Open Source India 
(OSI). The iconic convention 
was held on October 13 and 
14, 2017, at NIMHANS 
Convention Center, 
Bengaluru. It featured packed 
sessions by over 65 speakers, 
and 14 workshops running in 
parallel, over two days. Here 
is a pictorial round-up for 
those who missed the event.
The 14th edition of Open 
Source India was attended by a large number of open source supporters from India and around the globe. Apart from the talks by 
experts, the event also became a place for exhibitors to showcase their various community-centric offerings to hundreds of attendees.
Divyanshu Verma 
senior engineering 
manager, Intel R&D
Asheem Bakhtawar
regional director,
India, Middle East and
Africa, 2ndQuadrant 
India Pvt Ltd
Balaji Kesavaraj
head marketing,
India and SAARC, 
Autodesk
Dibya Prakash 
founder,
ECDZone
Janardan Revuru
Open Source
Evangelist
Dhiraj Khare
national alliance 
manager, Liferay India
Connectivity
Partner
Our Advisory Panel
OSI 2017:  
WHERE THE INDUSTRY  
MET THE COMMUNITY
82 | November  2017 | oPeN SoUrCe For YoU | www.openSourceForU.com

Open Source India 2017
01:  Tony Wasserman, professor of the software management 
practice at Carnegie Mellon University, Silicon Valley
02: A panel discussion on ‘Open Source vs Enterprise Open 
Source: Is this a key reason for the success of  
Open Source?’
03:  Dhiraj Khare, national alliance manager, Liferay India
04: Andrew Aitken, global open source head, Wipro Limited
05: Debabrata Nayak, project director, NeGD (MeitY)
06:   An attentive audience
1
2
3
4
5
6
7
8
9
13
10
11
12
07:  Sandeep Alur, director, technical engagements (partners), 
Microsoft Corporation (India) Pvt Ltd
08: Joerg Simon, ISECOM and Fedora Project
09: Vivek Sridhar, developer advocate, DigitalOcean
10: Salman Khan, technology architect, Liferay India
11: Gagan Mehra, director, information strategy, MongoDB
12:  Pavan Deolasee, PostgreSQL consultant,  
2ndQuadrant India Pvt Ltd
13:  Participants at the ‘app development’ track
www.openSourceForU.com | oPeN SoUrCe For YoU | November 2017 | 83

16
14
15
21
22
18
19
24
14:  Nishant Sahay, senior architect, Wipro Limited
15: Visitors trying their luck at the 
  
Salesforce competition
16:  K. Rajasekhar, deputy director general, NIC,  
MeitY, Government of India
17: Dr Michael Meskes, CEO, credativ GmbH
18: People interacting at the NetApp booth
19: Rajdeep Dua, director, developers relations, Salesforce India 
20: Bindukaladeepan Chinasamy, senior technical evangelist, 
Microsoft Corporation (India) Pvt Ltd
21: Visitors at the InstaSafe booth
22: Manpreet Singh Ghotra, Salesforce India
23:  Nikhil Sontakke, director, 2ndQuadrant India Pvt Ltd
24: Interactions at the Prodevans booth
25: Rajesh Jeyapaul, architect, mentor and advocate, IBM
26: Bhavani Ananth, architect, Wipro Limited
23
20
17
25
26
Open Source India 2017
84 | November  2017 | oPeN SoUrCe For YoU | www.openSourceForU.com

27
28
30
29
31
32
34
37
35
38
33
36
27:  Ram Iyengar, manager, developer relations, Zoho
28: Abhijit Ghosh, NoSQL and open source practice head, Infosys
29: Karthik P.R., CEO and DB architect, MyDBOPS
30: Sujatha Sivakumar, principal member, 
  
technical staff, Oracle
31: A queue at Microsoft’s booth
32: Adnyesh Dalpati, director - solutions architect  
and presales, Alef Mobitech
33: Sudhir Rawat, senior technical evangelist,  
Microsoft Corporation (India) Pvt Ltd
34: Soh Hiong, senior consultant, NetApp,  
delivering a keynote address
35: Krishna M. Kumar, chief architect in cloud R&D, Huawei India
36: A workshop conducted by members from Huawei India
37: Rajkumar Natarajan, CIO, Prodevans Technologies LLP
38: Rajesh Sola, education specialist, KPIT Technologies Ltd
Open Source India 2017
www.openSourceForU.com | oPeN SoUrCe For YoU | November 2017 | 85

86 | deCember 2013 | oPeN SoUrCe For YoU | www.LinuxForU.com
39:   Sanil Kumar D., chief architect  
in cloud R&D, Huawei India
40: Raghavendra Deshpande, developer advocate –  
IBM Cloud Bluemix
41: Aahit Gaba, counsel, open source, HP Enterprise
42: Uma Mukkara, co-founder and COO at OpenEBS/CloudByte
43: Vishal Singh, VP – IT infra and solutions,  
Eli Research India Pvt Ltd
44: Sanjay Manwani, MySQL India director, Oracle
45: Suman Debnath, project leader, Toshiba
46: Neependra Khare, founder, CloudYuga Technologies
47: Ramakrishna Rama, director - software, Dell India R&D
48: Srinivasan Ravindran, programme head - FOSS technologies, 
ICFOSS, Government of Kerala
49: Visitors at the DigitalOcean booth
50: A workshop conducted by Prodevans
51: People trying out new technologies
52: Anupam Ghosh, Siemens Technology and Services Pvt Ltd
40
44
48
41
42
45
49
39
43
47
46
52
51
50
Open Source India 2017
86 | November  2017 | oPeN SoUrCe For YoU | www.openSourceForU.com

87 | deCember 2013 | oPeN SoUrCe For YoU | www.LinuxForU.com
53:  Biju George, co-founder, InstaSafe
54: (L to R) Gopakumar Thekkedath, Divyanshu Verma, senior 
engineering manager, Intel R&D; Ashok Sharma, founder, QOS 
Technology;  Sandeep Athiyarath, founder, FCOOS
55: Siji Sunny, director, Mobile Embedded Labs Private Limited
56: Rahul Arvind Jadhav, chief architect, Huawei India
57: Dibya Prakash, founder, ECDZone
58: Kumar Priyansh, developer, BackSlash Linux
59: Biju K. Nair, executive director, SFLC.in
60: Monojit Basu, founder and director of TechYugadi  
IT Solutions & Consulting
61: Ananda Murthy, data centre solutions architect,  
Micro Focus - SUSE
62: Sudharshan Govindan, IBM
63: Ravi Trivedi, founder and CEO, PushEngage
64: Chandan Kumar, OpenStack community member
65: Mahati C. speaking during the OpenStack mini conference
66: Janki Chhatbar  OpenStack community member 
53
56
59
63
54
57
60
64
55
58
61
62
66
65
Open Source India 2017
www.openSourceForU.com | oPeN SoUrCe For YoU | November 2017 | 87

88 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Insight
T
he human brain has evolved over many, many years 
and is one of our most important organs. The brain 
perceives every smell, taste, touch, sound and sight. 
Many decisions are taken by the brain every nano second, 
without our knowledge.
Having evolved over several thousands of years, the 
human brain has become a very sophisticated, complex and 
intelligent machine. What was not possible even as a dream 
during the 18th century and the beginning of the 19th century 
has become child’s play now in terms of technology. Many 
adult brains can recognise multiple complex situations and 
take decisions very, very fast because of this evolution. The 
brain learns new things very fast now and takes decisions 
quickly, compared to those taken a few decades ago.
 A human now has access to vast amounts of information 
and processes a huge amount of data, day after day, and is 
able to digest all of it very quickly. 
Our brain is made up of approximately 100 billion nerve 
Deep learning is part of the broader family of machine learning methods. It 
was introduced with the objective of moving machine learning closer to its 
main goal—that of artificial intelligence. 
cells, called neurons, which have the amazing ability to 
gather and transmit electrochemical signals. We can think 
of them as the gates and wires in a computer. Each of our 
experiences, senses and various normal functions trigger 
a lot of neuron based reactions/communications. Figure 1 
shows the parts of a basic neuron. 
The human brain and its neural network have been the 
subject of extensive research for the last several years, leading 
to the development of AI and machine learning technologies. 
The decade-long dream of building intelligent machines 
with brains like ours has finally materialised. Many complex 
problems can be now solved using deep learning techniques 
and algorithms. The simulation of human brain-like activities 
is becoming more plausible every moment.
How different is deep learning compared to 
machine learning
Machine learning was defined by Arthur Samuel as, “The 
Deep Learning: Using Algorithms 
to Make Machines Think

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 89
Developers
Insight
field of study that gives computers the ability to learn without 
being explicitly programmed.” This was back in 1959.
In machine learning, computers are taught to solve 
certain problems with massive lists of rules, and are 
provided with models. In deep learning, the model 
provided can be evaluated with examples and a small 
set of instructions to modify it when it makes a mistake. 
Over time, a suitable model is able to solve the problem 
extremely accurately. That is why deep learning has become 
very popular and is catching every one’s eye. In the book 
‘Fundamentals of Deep Learning’, the authors Nikhil 
Buduma and Nicholas Locascio state: “Deep learning is 
a subset of a more general field of artificial intelligence 
called machine learning, which is predicated on this idea of 
learning from example.”
Deep learning explained in detail
According to the free tutorial website computer4everyone.
com, “Deep learning is a sub-field of machine learning 
concerned with algorithms inspired by the structure and 
function of the brain called artificial neural networks.” 
Andrew Ng, who formally founded Google Brain, 
which eventually resulted in the commercialisation of 
deep learning technologies across a large number of 
Google services, has spoken and written a lot about deep 
learning. In his early talks on deep learning, Ng described 
deep learning in the context of traditional artificial 
neural networks. 
At this point, it would be unfair if I did not mention other 
experts who have contributed to the field of deep learning. 
They include:
 
Geoffrey Hinton for restricted Boltzmann machines 
stacked as deep-belief networks (sometimes he is referred 
to as the father of machine learning)
 
Yann LeCun for convolutional networks (he was a student 
of Hinton)
 
Yoshua Bengio, whose team has developed Theano (an 
open source solution for deep learning)
 
Juergen Schmidhuber, who developed recurrent nets and 
LSTMs (long short-term memory, which is a recurrent 
neural network (RNN) architecture)
According to Andrew Ng, because of the huge volumes of 
data now available for computation and the recent advances in 
algorithms, deep learning technology has been adopted across 
the globe pretty quickly.
The potential for applications of deep learning in the 
modern world is humongous. Application fields include 
speech synthesis, learning based on past history, facial 
recognition, self-driving cars, medical sciences, stock 
predictions, and real estate rate predictions, to name a few.
Prerequisites to understanding deep 
learning technologies
There are a number of discussion forums and blogs on 
whether one has to know deep mathematics to understand 
deep learning. In my view, this should be evaluated on a 
case-to-case basis. Having said that, it is better to know the 
following if you really want to understand deep learning and 
are serious about it:
 
The basic functions of neural networks
 
An understanding of the basics of calculus
 
 An understanding of matrices, vectors and linear algebra
 
 Algorithms (supervised, unsupervised, online, batch, etc)
 
 Python programming
 
 Case-to-case basis mathematical equations
Figure 1: Basic neuron design
Figure 3: Why deep learning?
Figure 2: Layered learning mechanisms

90 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
Developers
Insight
 Note 1: In this article, I have tried my best to make 
you understand the basic concepts of deep learning, the 
differences between machine learning and AI, as well as some 
basic algorithms. The scope of this article does not allow a 
description of all the algorithms with mathematical functions.
Basics about neural networks
At its core, the neuron is optimised such that it receives 
information from other neurons and, in parallel, processes 
this information in a unique way, before sending the 
outcome (results) to other cells. This process works as 
shown in Figure 4.
Because of the advances mentioned earlier, humans 
are now able to simulate artificial neural networks using 
algorithms and computers. Each of the incoming connections 
is dynamically strengthened or weakened based on how many 
times it is used (this is also how humans learn new concepts). 
After being weighted by the strength of the respective 
connections, the inputs (Figure 5) are summed together in 
the cell body. This sum is then transferred to a new signal for 
other neurons to catch and analyse (this is called propagation 
along the cell’s axon and sent off to other neurons).
Using mathematical vector forms, this can be 
represented as follows: 
Inputs as a vector X = [X1,X2 ….Xn]
And weights of the neuron as W = [W1, W2 …. Wn]
Therefore the output Y can be represented as Y = f(X.W + b)
Where “b” is the bias term.
As such, many of these bunches of individual artificial 
neurons are connected together to form an ANN (artificial 
neural network). This ANN is a combo of various input 
and output layers along with hidden layers (as depicted 
in Figure 6).
 Note 2: A hidden layer neuron is one whose output is 
connected to the inputs of other neurons and is therefore not 
visible as a network output (hence the term ‘hidden layer’).
 Note 3: Refer to the article ‘Do deep nets really need to be 
deep?’ by Lei Jimmy Ba and Rich Caruana for more details.
Some time back, in 2014, it was observed that a training 
set with a huge data set in a shallow net, with one fully 
connected feed-forward hidden layer on the available 
data, yielded 86 per cent of the test data. But if this same 
data set was trained in a deeper neural net consisting of a 
convolutional layer, pooling layer, and three fully-connected 
feed-forward layers on the same data, 91 per cent accuracy 
on the same test set was obtained. This 5 per cent increase in 
accuracy of the deep net over the shallow net occurs because 
of the following reasons: 
a)  The deep net has more parameters. 
b) The deep net can learn more complex functions, given the 
same number of parameters. 
c) The deep net has better bias and learns more interesting/
useful functions leading to invention and improvement of 
many algorithms. 
A few algorithms
The scope of this article allows me to describe just a 
few important algorithms, based on these learnings and 
improvements. The following are the most used and popular 
algorithms. They are categorised as training feed-forward 
neural networks.
Gradient descent: Imagine there is a ball inside a bucket 
and the goal is to ‘get as low as possible’. This requires 
optimisation. In this case, the ball is optimising its position 
(left to right or right to left, based on the condition) to find the 
lowest point in the bucket.
The only information available is the slope of the side of 
the bucket at its current position, pictured with the blue line in 
Figure 7. Notice that when the slope is negative (downward 
from left to right), the ball should move to the right. However, 
when the slope is positive, the ball should move to the left. As 
Figure 4: Functional description of a neuron
Figure 6: Artificial neural networks
Figure 5: Schematic for a neuron in an artificial neural network 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 91
Developers
Insight
you can see, this is more than enough information to find the 
bottom of the bucket in a few iterations. This is a sub-field of 
optimisation called gradient optimisation (gradient is just a 
fancy word for slope or steepness).
 Note 4: To know more, you can refer to the article 
‘Improving our neural network by optimising Gradient 
Descent’, posted by Iamtrask, better known as ‘Trask’.
The complexity of calculating the slope will become 
more and more challenging if, instead of a bucket, we have 
an uneven surface (like the surface of the moon or Mars). 
Getting the ball to the lowest position in this situation needs a 
lot of mathematical calculations and also more data sets. 
Gradient descent isn’t perfect. There are some solutions 
around the corner that can help us to overcome these 
challenges, like:
 a)  Using multiple random starting states
 b)  Increasing the number of possible slopes and considering 
more neural networks 
 c)  Using optimisations like ‘Native Gradient Descent’ 
 d)  Adding and tuning the alpha parameter 
 Note 5: What is alpha? As described above, the alpha 
parameter reduces the size of each iteration’s update in the 
simplest way possible.
Finally, gradient descent that uses non-linearities to solve 
the above problem is popular. Here, Sigmoidal neurons are 
used as training neurons.
The back-propagation algorithm: The back-propagation 
approach was pioneered by David E., Geoffrey Hinton and 
Ronald J. Sometimes we do not know what the hidden units 
are doing, but what we can do is compute how fast the error 
function changes as we change the hidden activity. With this 
we can find out how fast the error changes when we change 
the weight of an individual connection. Using this, we try to 
find the steepest descent.  
Each hidden unit can affect many output units, and to 
compute the error derivatives for the activities of the layers 
below, we have to back-propagate to find the nearest values; 
hence, the name.
Stochastic and mini-batch gradient descent: There are 
three variants of gradient descent, which differ in how much 
data is used to compute the gradient of the objective function. 
Depending on the amount of data, one has to make a trade-off 
between the accuracy of the parameter update and the time 
it takes to perform an update. Instead of a single static error 
surface, by using a dynamic error surface and by descending 
on this stochastic surface, the ability to navigate flat regions 
improves significantly.
 Note 6: Do refer to ‘An overview of gradient descent 
optimisation algorithms’ by Sebastian Ruder.
a) Batch gradient descent: Batch gradient descent (also 
known as Vanilla gradient descent) computes the gradient 
of the cost function with respect to the parameter θ for the 
entire training data set:
b)  Stochastic gradient descent: Stochastic gradient descent 
(SGD), in contrast, performs a parameter update for each 
training example — x(i) and label y(i):
 
Batch gradient descent performs redundant computations 
for large data sets, as it recomputes gradients for similar 
examples before each parameter update. SGD does away 
with this redundancy by performing one update at a time. 
It is therefore usually much faster and can also be used to 
learn online. 
c) SGD fluctuation: Batch gradient descent converges to the 
minimum point of the basin the parameters are placed in. 
On the one hand, SGD’s fluctuation enables it to jump to 
new and potentially better local minima, but on the other 
hand, this complicates convergence to the exact minimum, 
as SGD will keep overshooting. However, it has been shown 
that when we slowly decrease the learning rate, SGD shows 
the same convergence behaviour as batch gradient descent, 
almost certainly converging to a local or the global minimum 
for non-convex and convex optimisation, respectively.
 
 Note 7: Machine learning systems are mainly classified as 
supervised/unsupervised learning, semi-supervised learning, 
reinforcement learning, batch and online learning, etc. All 
these systems mainly work on training the data set. These 
broad categories are sub-divided into respective algorithms 
based on the training data set. A few popular examples are: 
k-nearest neighbours, linear and logistic regression, SVMs, 
decision trees and random forests, neural networks (RNN, 
CNN, ANN), clustering (k-means, HCA,), etc.
Figure 7: Gradient descent
Figure 8: Gradient descent on an uneven surface
Continued to page 95...

92 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Overview
T
he personal computer has become a part of most 
professionals’ lives, irrespective of the arena in which 
they work. Even a small shopkeeper uses a PC to 
maintain the records of his daily sales and purchases. Today, 
we are all pretty dependent on computers, and their efficiency 
and productivity directly impacts our personal productivity. If 
we spend around three hours a day to do a set of tasks, a quick 
and productive computer system may help us finish the same 
tasks in, maybe, around two-and-a-half hours. More than 86 
per cent of the computers used in our homes and offices run 
on some form of the Windows OS. Hence, the productivity of 
Windows’ computer systems is really important. 
There has been an enormous increase in the amount of 
data that our computer needs to process on a daily basis. For 
instance, a computer at a shopping centre needs to add and 
maintain the records of numerous customers who make daily 
purchases along with the list of items purchased by them, 
their prices and offers, etc. All this adds up to gigabytes of 
data, which impacts the performance of the computer when it 
computes large data sets. The productivity of Windows systems 
also depends upon factors like the frequency with which the 
different system files are cleaned, the way the large log files or 
a file system are handled, drive space utilisation, etc. 
Productivity tools are software programs that allow a computer user to do certain 
specific tasks quickly and efficiently. The productivity tools listed here are open 
source and specifically meant for the Windows operating system.
Thankfully, there are many open source tools running 
on Windows that can increase a system’s productivity. 
These are available free of cost and can be easily tried out 
by anyone. Also, there is enough documentation and many 
write-ups available for these open source tools, which makes 
it much easier to use them. It is also easier to modify them 
as per our requirements since, being open source, their 
source code is easily accessible.
The need for open source productivity tools
I feel productivity tools should be named facilitation tools.  
Also, these tools offer a lot more than what we discussed. 
Let’s check out why they are in such great demand. 
1. Productivity tools are in high demand as they help 
users to perform any specific task with much ease and 
flexibility, in less time. They are quite user-friendly.
2. Open source productivity tools are available free of cost 
and do not carry any pricing overheads, which is one of 
the main reasons why these tools are preferred.
3. Also, the developers of open source productivity tools 
make some or all of the source code freely available to 
the public. This allows others to take the code and alter 
it to create their own software. 
Open Source Productivity Tools 
that Run on Windows

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 93
For U & Me
Overview
4. Since all open source productivity tools are handled by 
the open source community, developers can actually 
monitor the user response and tweak their applications to 
better meet user needs. This really benefits users.
5. Productivity software organises different features in 
an intuitive way without actually copying another 
developer’s work. Some of the users are quite picky 
about specific features of the system. If they need to 
switch from one product to the other, they might get 
frustrated in case the second product does not organise 
its different features in the same way as the one 
they’ve become used to. 
6. Different productivity tools also take care of the tricky 
tasks such as converting spreadsheets full of data into 
graphs and charts, which makes it really easy to perform 
different analysis tasks on large data sets. This also 
saves a lot of the manual effort required to analyse 
huge data sets.
7. One of the emerging trends in productivity software 
is collaborative software. This means that multiple 
people can work together on the same file and that too, 
at the same time. For different desktop applications, all 
files must be saved on a networked disk drive that is 
accessible to all the collaborators. In the case of Web 
services, different users save files to a database present 
on the Web. All the collaborators can work on the specific 
file from any of the computers connected to the Internet.
A few open source productivity tools 
and their features
Scribus: This is an open source desktop publishing program 
which is available free of cost. It is developed keeping in 
mind the user interface as displayed in case of Paint.NET 
and Inkscape. 
Its features are listed below.
 
It sports a different layout, and the design tools are at par 
with its commercial competitors. 
 
It uses a macro language called GIMP, which is 
available with a number of pre-packaged macros (e.g., a 
calendar generator).
 
It can produce professional-quality CMYK PDFs. It also 
has a ‘preflight check’ function in order to make sure that 
what we see is really what we eventually get. 
We can get the best results out of Scribus when we use it 
in conjunction with a text processing system; it is not really a 
word processor in its own right and is not suited yet to laying 
out long-form documents. Also, as in the case of Inkscape, 
it's internal support for colour matching systems is missing. 
We can partly work around this limitation.
SeaMonkey 2.0.6: SeaMonkey bundles different 
applications like Web browsing, IRC chat, e-mail and an 
HTML editor, into a single application. It is quite similar 
to Firefox and Thunderbird but, unfortunately, it hasn’t 
attracted much attention yet. SeaMonkey 2.0 is really worth 
a look. Apart from the plugin-friendly browser and the email 
client, the most attractive part is the HTML editor, which is 
slightly more polished than KompoZer but is not available 
separately. It is not a substitute for any full-blown page design 
application like DreamWeaver, but it really works well for the 
basic HTML assembly and the clean-up. It can also examine 
the tag structure of the existing pages. 
AbiWord 2.8.6: This is actually a simple word processing 
application. It has been developed to emulate the look and 
feel of Microsoft Word circa Office 97 through Microsoft 
Office 2003. AbiWord also emulates the vast majority 
of Word’s functions. This is especially useful for a live-
collaboration function, which lets us connect to some other 
AbiWord user present across a network and to work on the 
same set of documents in real-time. 
However, users do need to be cautious about a few things. 
 
AbiWord can be tripped up by the documents created in 
other programs. 
 
Some of the formats are not properly preserved, and 
some features supported in Word don’t always function 
as expected. 
 
When working with the files created outside AbiWord, we 
should use copies instead of originals.
Task Coach 1.1.3: Task Coach is an application that 
uses a simpler and checklist-oriented approach to help us 
stay on top of our workload. Different tasks that need to be 
done can be organised into different categories such as the 
assigned dates and progress percentages. They can also be 
time-tracked, so that we get an idea of how much time we are 
really spending on any given project. 
Some of the advanced features of this tool are:
 
Can create sub-tasks and add ‘effort’ annotations to a 
given task, but we don’t need to know about how to use 
them to make use of Task Coach.
 
We can also visualise our tasks as more than just a list. 
We can see them using a timeline, a calendar view and a 
hierarchical view (for the sub-tasks). 
 
These task lists can also be synced using iCalendar, or 
through a Funambol server if we use one of them.
Dia 0.97: Dia is actually an open source tool that helps in 
drawing flowcharts and diagrams. It is much like AbiWord for 
illustrations. However, AbiWord just gives us what we need to 
create a certain class of the design and does not really burden 
Figure 1: Plugin loading screen for Scribus (Image source: googleimages.com)

94 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Overview
Tips to make a Windows system more productive
1. Install the latest network adapter firmware, storage area network drivers, and network adapter drivers.
2. Disable all hyper-threading present on BizTalk Server and SQL Server computers.
3. Configure the antivirus software to avoid real-time scanning of BizTalk Server executables and file drops.
4. Review the disk controller stripe’s size and volume allocation units.
5. Optimise Windows Server’s performance for background services.
6. Disable non-essential services like Messenger, Clipbook, Telephony, etc.
7. Disable intrusion detection network scanning between the computers in the BizTalk Server environment.
8. Defragment all disks present in the BizTalk Server environment on a regular basis.
9. Disable real-time scanning of the data and transaction files if antivirus software is installed on the SQL Server computer.
10.  Configure firewall(s) for BizTalk Server.
11.  Use the Interrupt Filter Configuration tool to bind network adapter interrupts to specific processors present on multi-
processor computers.
12.  Use the NTFS file system on all volumes. Do not use NTFS file compression.
13.  Assign the MSDTC log file directory to a separate dedicated drive.
14.  Monitor drive space utilisation.
15.  Implement a strategy to avoid disk fragmentation.
Figure 2: Drawing flowcharts using Dia (Image source: googleimages.com)
Figure 3: Advantages of open source productivity tools (Image source: googleimages.com)
us beyond that. Its controls can be a little difficult to work 
with. For instance, it assumes that different people will draw 
the new shapes by starting from the top-left corner and then 
moving down, before moving to the right. (We get some odd 
results if we try drawing objects going from right to left.)
Dia is easy to use, and comes with a slew of common 
object categories such as hydraulic, electrical, programming, 
and so on. With Dia, it is really easy to jump-start a 
drawing of any sort.
Advantages of open source productivity tools
1. Available free of cost: First of all, as soon as we add 
an open source tag to any tool, it certifies that the 
tool is available free of cost.  
2. Versatile and flexible: People choose open source 
productivity tools because they have more freedom 
over such tools. They can analyse the code in order to 
be sure that it does not do anything they don’t want it 
to, and can also change it if they want to do so. Even 
a non-programmer can use these tools by modifying 
them to specific needs.
3. Documentation and help available: All open source 
productivity tools have documentation available 
online, which really helps to use them easily. Since 
the documentation is publicly available, users can 
study it to improve and make finer software. They 
can also share their code with the others, hence 
evolving their skills.
4. Highly stable: People usually choose open source 
productivity tools rather than the licensed versions for 
some essential and long-term projects. As we know, 
programmers publicly distribute the code for different 

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 95
For U & Me
Overview
By: Vivek Ratan 
The author is a B.Tech in electronics and instrumentation 
engineering. He is currently working as an automation test 
engineer at Infosys, Pune and as a freelance educator at 
LearnerKul, Pune. He can be reached at ratanvivek14@gmail.
com for any suggestions or queries.
[1] http://www.wikipedia.org/
[2]  http://www.makeuseof.com/
[3]  https://msdn.microsoft.com/
[4]  http://www.techrepublic.com/
References
open source tools. Individuals who have faith in that tool 
for difficult computing tasks can really be certain that 
such open source tools won’t disappear if their original 
creators stop working on them. They will always be 
supported by the open source community.
5. Security: Open source productivity tools are 
comparatively safer and steadier than the licensed ones. 
Just as anyone can view and change the code for different 
open source tools, they can also point out and correct 
the faults that a program’s original developer might have 
missed out. Also, since many developers can work on 
different parts of open source tool code without actually 
asking for the approval of the original developer, they can 
Open source support for deep learning
TensorFlow is an open source software library released in 
2015 by Google to make it easier for developers to design, 
build and train deep learning models. Actually, TensorFlow 
originated as an internal library that Google developers used 
to build models in-house. Multiple functionalities were added 
later, as it became a powerful tool.
At a higher level, TensorFlow is a Python library 
that allows users to express arbitrary computation as 
a graph of data flows. Nodes in this graph represent 
mathematical operations, whereas edges represent data that is 
communicated from one node to another. Data in TensorFlow 
is represented as tensors, which are nothing but multi-
dimensional arrays (from 1D tensor, 2D tensor, 3D tensor…
etc.). The TensorFlow library can be downloaded straight 
away from https://www.tensorflow.org/.
There are many other popular open source libraries that 
have popped up over the last few years for building neural 
networks. These are:
1) Theano: Built by the LISA lab at the University of 
Montreal (https://pypi.python.org/pypi/Theano)
2) Keras (https://pypi.python.org/pypi/Keras)
3) Torch: Largely maintained by the Facebook AI research 
team (http://torch.ch/)
All these options boast of a sizeable developer 
community, but of all these TensorFlow stands out because 
of its features such as the shorter data training time, ease of 
By: Shashidhar Soppin 
The author is a senior architect and has 16+ years of experience 
in the IT industry having specialised in virtualisation, cloud 
computing, Docker, open source, etc. He can be contacted at 
shashi.soppin@gmail.com.
writing and debugging code, availability of many classes of 
models, and multiple GPU support on a single machine. Also, 
it is well suited for production-ready use cases.
Deep learning has become an extremely active area 
of research and is finding many applications in real life 
challenges. Companies such as Google, Microsoft and 
Facebook are actively forming deep learning teams in-house. 
The coming days will witness more advances in this field— 
agents capable of mastering Atari games, driving cars, trading 
stocks profitably and controlling robots. 
[1] Neuron: http://science.howstuffworks.com/life/inside-the-
mind/human-brain/brain1.htm
[2] https://www.tensorflow.org/
[3] Gradient Descent: https://iamtrask.github.io/2015/07/27/
python-network-part2/
[4] http://ruder.io/optimizing-gradient-descent/
[5] https://en.wikipedia.org/wiki/Geoffrey_Hinton
[6] https://en.wikipedia.org/wiki/Yann_LeCun
References
Continued from page 91...
A few myths
1. Malware is the reason for a slow computer.
2. Clearing the personal data from our hard drive will 
boost its performance.
3. Replacing and upgrading different components 
ensures guaranteed speed.
4. Cleaning the registry speeds up the system.
5. A fresh install of the operating system is the ultimate 
way to gain speed.
modify, correct and improve the code more quickly than 
in the case of licensed ones.
6. Scope for continuous change and improvement: There 
is always scope for change and improvement with 
open source productivity tools, as everyone can access 
the code and apply their innovation to it. There is a 
large community working for this, so the probability 
and rate of improvement is really high, compared to 
closed source software tools. 

For U & Me
Overview
96 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
T
he expression ‘open source software’ refers to 
software that is available free, and its source code has 
been made publicly available for study, modification 
and use on any system. Often, the goal in releasing this 
source code is to enable community collaboration in 
developing this software in a more open, efficient and rapid 
manner. Depending on the licence, open source software can 
have different terms for unrestricted usage. 
Microsoft Windows is representative of a family of 
operating systems released over the years, each catering to 
an industry, or customised to fit a subset of business and 
development purposes. 
Microsoft has long faced flak from the Linux community 
for keeping its operating system closed source, even as it 
manages to outsell all Linux distributions, combined. Yet, 
Microsoft has recently become a member of the Linux 
Foundation and maintains a presence on GitHub. It has 
upped its commitment to software developers and the open 
source community with the recent and rapid release of 
a lot of features, especially the Windows Subsystem for 
Linux, which allows developers to run the Linux command 
The open source world is filled with software that can be used along with the 
proprietary Windows operating system, without causing any inconvenience. Here’s a 
selection of software that is free, open source and compatible with MS Windows.
line directly in Windows without the overhead of a virtual 
machine. Some popular features of the new Windows 
release include increased control over the task manager, an 
inbuilt package manager, multi-desktop support, and so on. 
Let us take a closer look at the various categories of 
open source software available for use on Windows.
Web browsers
Firefox: A product created by the Mozilla Foundation, 
Firefox is an open source Web browser first released in 
2002. It is the default browser on most Linux distributions 
but has a huge following on Windows machines as well.
Chromium: The father of the modern-day Chrome Web 
browser, Chromium is based on the original source code 
that Google then modified to create Chrome. It has set itself 
up as a serious contender to Firefox, being the default Web 
browser on Linux systems.
Communication software
Pidgin:  A free and intuitive multi-protocol chat client 
used by hundreds of thousands of people, Pidgin can 
Open Source Software That Can 
Partner with MS Windows

For U & Me
Overview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 97
connect to AIM, Google Talk, ICQ, IRC, XMPP and other 
networks concurrently.
Email client
Thunderbird: Developed by Mozilla in 2004, Thunderbird is 
still one of the most popular open source software challenging 
Microsoft Outlook today. It caters to the growing demand for 
an email client for modern users.
Productivity and utilities software
LibreOffice: A powerful office suite, LibreOffice provides 
an uncluttered and intuitive user interface that serves to boost 
productivity. The software kit includes several applications 
that make it one of the friendliest and fastest-growing office 
suites in the free and open source community.
PuTTY: This is a free implementation of SSH and Telnet 
for Windows and UNIX platforms, along with an xterm 
terminal emulator. It is written and maintained primarily by 
Simon Tatham.
7zip: This is a free and open source file-archiver—a 
software used to put groups of files into compressed folders 
termed as ‘archives’. Written by Igor Pavlov in 1999, it has a 
high compression ratio, making it a very effective software.
File sharing software
FileZilla: Born as a class project of three students in 2001, 
this tame software has since evolved into a gigantic full-
featured file manager and file server. FileZilla offers protected 
file sharing, which requires a user name and password to 
access the shared data. 
DC++: Technological progress has given us the direct 
connect (DC) protocol used for sharing files over the Internet. 
The open source DC++ is the most popular peer-to-peer file 
sharing client based on the direct connect protocol.
Media players
VLC media player: VLC is widely used as a media player 
and streaming software. It is capable of playing files, discs, 
Web cams, devices and streams. It can even be used as a 
video downloader.
Audacity: A free and open source audio editor and 
recorder, Audacity allows users to record live audio, convert 
Figure 1: Microsoft Windows, an oft-perceived thorn in the side of open source proponents
Figure 2: Open source Web 
browsers and other tools
Figure 3: Communication and  
file sharing utilities for developers
Figure 4: Hypervisor and media-based software for Windows
By: Swapneel Mehta
The author has worked at Microsoft Research, CERN, and 
at startups in AI and cyber security. He is an open source 
enthusiast who enjoys spending time organising software 
development workshops for school and college students. You 
can contact him at https://www.linkedin.com/in/swapneelm, 
https://github.com/SwapneelM or http://www.ccdev.in.
records and tapes to digital formats, or mix pre-built 
digital audio tracks.
GIMP: The brainchild of Spencer Kimball and Peter 
Mattis, the GNU Image Manipulation Program or GIMP 
serves as an open source graphics editor, an alternative 
to Adobe Photoshop. It can be used to create and update 
photos or clip-art in various image formats such as JPEG, 
PNG, TIFF, and so on.
Hypervisors and emulators
VirtualBox: VirtualBox is professional grade, open 
source virtualisation software. It is a general-purpose full 
virtualiser for x86 hardware, targeted at server, desktop 
and embedded use.
Cygwin: This is a large collection of GNU and 
open source tools that provide functionality similar to a 
Linux distribution on Windows. Cygwin provides native 
integration of Windows-based applications, data, and 
other system resources with applications, software tools 
and data of the UNIX-like environment. 

98 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Overview
I
n 1950, a man called Alan Turing posed the question, ‘Can 
machines think?’ in his paper titled, ‘Computing Machinery 
and Intelligence’, and the world has never been the same 
ever since. The general consensus is that this was the first 
step into the world of artificial intelligence (AI). It was in this 
paper that Turing posed his now popular Turing Test, also 
known as the Imitation Game (there’s now a popular movie 
by that title). The term ‘artificial intelligence’ however, was 
yet to be coined and widely used.
Time rolled by and the language called C was invented 
between 1969 and 1973 in Bell Labs. This led to a new kind 
of revolution. We could now give machines a step-by-step list 
of instructions, which would faithfully be carried out by them.  
This was the period during which the Internet was born and 
nourished. These events led to the programming profession 
evolving to the state it is in today.
The task of a programmer is to understand a real-world 
situation, define inputs to a program (along with the program 
The invention of the digital computer has been one of the defining moments of the 
modern era. It all started off by building a machine that could follow commands to 
the letter. We have come a long way since then. This article talks about a few other 
tech developments that can be termed defining moments in human history – artificial 
intelligence, machine learning and deep learning.
itself) and then write out that program in some programming 
language. As long as you can write down a list of instructions 
in the sequence in which tasks need to be performed, a 
computer can follow those instructions. Sometime later, John 
McCarthy came onto the scene with the Lisp language and 
coined the term ‘artificial intelligence’. Lisp was a different 
kind of programming language altogether. Readers who have 
the time could read up more about this language. 
Soon, people began investigating the kind of problems 
that programming could solve. Problems that needed 
intelligent decisions to arrive at a solution came to be known 
as AI. This field grew, incorporating functions like search, 
planning, pattern recognition, classification, causality 
inference and so on. It was thus that AI came to be a field to 
be studied, whose implementation on digital computers would 
accomplish tasks that were considered ‘intelligent’. The 
evolution of this branch of technology was amazing, as it was 
programmability and reliability that put humans into space 
The Story Behind the 
All-pervasive AI

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 99
For U & Me
Overview
(apart from other tech developments that played major roles).
The problem arose when the task to be accomplished by 
a machine was something that humans did, but which did 
not seem to follow a step-by-step procedure. Take sight, for 
example; a normal human being is able to detect objects, 
identify them and also locate them, all without ever being 
‘taught’ how to ‘see’. This challenge of getting machines 
to see has evolved into the field of computer vision. Tasks 
for which the steps to reach completion were not obvious, 
were difficult for machines to perform. This was because of 
the nature of programming. Programmers needed to break 
down a task into a series of sequential steps before being 
able to write down instructions for the computer to follow 
in some languages like C. Machine learning (ML) was a 
big break from conventional programming. You no longer 
needed to know the steps to solve a problem. All you needed 
were examples of the task being done, and the system would 
develop its own steps. This was amazing! As long as you 
could find the right inputs to feed to the system, it would 
discover a way to get the target outputs. 
This model was applied to a lot of places over time. Rule 
based systems came to be popular in medical systems. Bayes 
Theorem dominated the Internet sales business. Support 
Vector Machines were beautiful machines, which worked like 
magic. Hidden Markov Models were very useful for stock 
markets and other ‘time series-like’ things. All in all, the 
world benefited from these developments.
There was still a problem though. The advances were few 
and far between. Take for example the task of recognising 
objects in images. Before ML, all efforts were being put into 
developing a better sequence of steps for recognising objects in 
images. When AlexNet burst onto the scene, the best performing 
algorithms had errors of approximately 26 per cent. AlexNet, 
however, had an error of almost 16 per cent, which was a major 
leap forward. Object recognition from images has now reached 
super human performance levels. What changed was that instead 
of asking a computer to follow a certain list of steps (a program), 
the computer was asked to find its own steps (a neural network, 
an SVM or a random forest are programs in some senses) after 
being given a lot of examples of what it was supposed to do. The 
problem was with finding the right set of inputs. 
Continuing our discussion of the image recognition task, 
people were feeding a bunch of features to classifiers like 
SVMs and logistic regression models. But these features were 
generated by humans and were not good enough for the task 
at hand. Programs like SIFT, HOG, Canny edges, etc, were 
developed to work around this, but even these were not good 
enough. What AlexNet introduced was the ability to learn the 
correct representations based on some task from the most basic 
input available, namely, the pixels. This was deep learning 
— the ability to build representations and use them to build 
other representations. Deep learning is not limited to neural 
networks, as a lot of people believe. Deep SVMs (arccosine 
kernel) have been developed along with Deep Random Forests 
(gcForest). In any task, if you need to employ deep learning, 
first ask yourself if there is a low level input that you can 
provide? In the case of language based tasks, it’s words; for 
images, it is pixels; for audio, it is a raw signal and so on.
There are still many misconceptions about these fields in 
the public mind, especially because of misinterpretations by 
the popular media. One of the main reasons is that, typically, 
reporters either don’t bother to read the tech papers that are 
published before reporting on them, or fail to understand them 
completely. This leads to unfortunate rumours like Facebook’s 
AI scare (the Deal or no Deal paper). Again, we only hear 
of the major changes in the field from the news channels 
and not the slow build-up towards a particular breakthrough. 
Sometimes, the people claiming to be ‘experts’ on the subject 
and brought in to discuss issues on news channels may not have 
kept up with developments in the field. They cover up their 
outdated knowledge by useless romanticisation of the things 
being discussed. This further hampers the public mind’s ability 
to grasp the true developments in the field of AI.
Most professionals in the AI field have started out by 
working with the various tools even before they have had a 
chance to understand and learn the algorithms behind AI or 
ML. This has led to the learning of misleading concepts and, 
in many cases, outright wrong practices. The Indian industry 
suffers from the classic case of resume-building in this field. 
People who have used the tools once or twice claim to be 
proficient in the field, whereas it takes much more than that to 
master the basics here. There is no doubt about the advantages 
AI, ML and DL bring to the table. What is in doubt is the 
ability to train people who can use these technologies.
As of writing this article, the best place to start is 
Coursera’s AI/ML courses. Most, if not all, content there 
is world class. If that does not slake your thirst, MIT 
OpenCourseWare on YouTube is also a wonderful learning 
place. Then there is our very own NPTEL available on 
YouTube which offers courses on AI. All things considered, 
if one gets the opportunity to learn about AI from the people 
who invented it, one must grab it with both hands. 
By: Arjoonn Sharma
The author is a research engineer at CLAP Research. You can 
connect with him at github.com/theSage21and arjoonn.com.
[1] Computing machinery and Intelligence: http://www.loebner.
net/Prizef/TuringArticle.html
[2] The Summer Vision Project: https://dspace.mit.edu/
handle/1721.1/6125
[3] Alex Net: https://papers.nips.cc/paper/4824-imagenet-
classification-with-deep-convolutional-neural-networks.pdf
[4] Arccosine kernel: https://cseweb.ucsd.edu/~saul/papers/
nips09_kernel.pdf
[5] gcForest: https://github.com/sig-ml/bleedml
[6] Deal or no Deal: https://arxiv.org/abs/1706.05125v1
References

100 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Let's Try
B
usinesses that restrict themselves to proprietary 
software like Microsoft Office get a raw deal. Not only 
do they have to pay for the software but they have to 
factor in the cost incurred every time the software becomes 
corrupt. This includes the fee to be paid to the computer 
technician to re-install the software. All this creates a vicious 
cycle, where costs and delays keep mounting. It should be 
the primary aim of every business to develop a system that 
automates maintenance to the maximum possible extent.
This is where open source software like LibreOffice, 
Apache OpenOffice, Scribus, GIMP, Inkscape, Firefox, 
Thunderbird, WordPress, VLC media player, etc, come 
in. My company, MultiSpectra Consultants, uses open 
source software to the maximum possible extent, thereby 
streamlining business processes. It makes updating the 
software and its maintenance very easy. The required software 
can be freely downloaded from the Internet and updates can 
also be applied by simply downloading the latest version of 
the relevant software.
It is worthwhile for every business to investigate how best it can leverage the power of open 
source software to reduce expenses and increase revenues. This article takes a quick look at 
some of the key open source software that can help an organisation to perform well.
With free and open source software (FOSS) anyone is 
freely licensed to use, copy, study and change the software in 
any way, and the source code is openly shared so that people 
are encouraged to voluntarily improve the design of the 
software. This is in contrast to proprietary software, for which 
the software is under restrictive copyright and the source code 
is usually hidden from the users. The benefits of using FOSS 
include lower software costs, higher security and stability 
(especially with regard to malware), better privacy protection 
and more control over the hardware. 
So let’s take a brief look at some of the key open 
source software.
LibreOffice and Apache OpenOffice
These are two actively developed forks of OpenOffice.org, 
which is no longer being developed. LibreOffice is being 
developed by The Document Foundation. The centralised 
launch centre of LibreOffice for all its modules is a very 
useful feature. LibreOffice was forked from OpenOffice in 
Key Open Source Software That Can 
Boost Business Productivity

www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 101
For U & Me
Let's Try
2010. Its suite comprises programs for word processing, the 
creation and editing of spreadsheets, slideshows, diagrams 
and drawings, working with databases, and composing 
mathematical formulae. It is available in 110 languages. 
LibreOffice uses the OpenDocument file format (odf) as its 
native format to save documents for all of its applications. 
It also supports the file formats of most other major office 
suites, including Microsoft Office, through a variety of 
import/export filters. LibreOffice is available for Microsoft 
Windows, Linux and Mac OS. A LibreOffice Viewer for 
Android devices is also available.
LibreOffice and Apache OpenOffice have similar modules 
called by the same names — Writer, Calc, Impress, Draw, 
Math and Base. A key advantage that LibreOffice has over 
Apache OpenOffice is the ability to save documents in the 
docx format. Although Apache OpenOffice can read docx 
documents, it cannot save documents in this format. A brief 
overview of the modules of LibreOffice follows.
LibreOffice Writer: This is a word processor with similar 
functionality as, and file support for, Microsoft Word or 
WordPerfect. It has extensive WYSIWYG word processing 
capabilities but can also be used as a basic text editor.
LibreOffice Calc: This is a spreadsheet program, similar 
to Microsoft Excel or Lotus 1-2-3. It has a number of unique 
features including a system that automatically defines a series 
of graphs based on information available to the user.
LibreOffice Impress: This is a presentation program 
resembling Microsoft PowerPoint. Presentations can be 
exported as swf files, allowing them to be viewed on any 
computer with Adobe Flash Player installed.
LibreOffice Draw: This is a vector graphics editor and 
diagramming tool similar to Microsoft Visio and comparable 
in features to early versions of CorelDraw. It provides 
connectors between shapes which are available in a range of 
line styles, and facilitates drawings such as flowcharts. It also 
includes features similar to desktop publishing software such 
as Scribus and Microsoft Publisher. It can also act as a PDF 
file editor.
LibreOffice Math: This is an application designed 
for creating and editing mathematical formulae. It uses 
a variant of XML for creating formulae, as defined in 
the OpenDocument specification. These formulae can be 
incorporated into other documents in the LibreOffice suite, 
such as those created by Writer or Calc, by embedding them 
into the document.
LibreOffice Base: This is a database management program 
similar to Microsoft Access. LibreOffice Base allows the 
creation and management of databases, and preparation of 
forms and reports that provide end users easy access to data.
Wikipedia reports that a detailed 60-page report in June 
2015 compared the progress of the LibreOffice project with 
its cousin Apache OpenOffice. It showed that, “Apache 
OpenOffice received about 10 per cent of the improvements 
LibreOffice did in the period of time studied.”
Scribus
Scribus is a desktop publishing (DTP) application and is free 
software. It is available for Microsoft Windows, Linux and 
Mac OS. Scribus is designed for layouts and typesetting, 
and prepares files for professional-quality image-setting 
equipment. It can also create animated and interactive 
PDF presentations and forms. It is used in the design of 
newspapers, brochures, newsletters, posters and books.
GIMP
The GNU Image Manipulation Program (GIMP) is a free 
and open source raster graphics editor used for image 
retouching and editing, free-form drawing, resizing, 
cropping, photo-montages, conversion to different image 
formats and more specialised tasks. It is the open source 
equivalent of Adobe Photoshop. GIMP is available for 
Microsoft Windows, Linux and Mac OS.
GIMP has been primarily developed by volunteers as a 
free software project, and is associated with both the GNU 
and GNOME projects. The version number used in GIMP is 
expressed in a major-minor-micro format, with each number 
carrying a specific meaning. The first (major) number is 
incremented only for major developments (and is currently 
2). The second (minor) number is incremented with each 
release of new features, with odd numbers reserved for in-
progress development versions and even numbers assigned 
to stable releases. The third (micro) number is incremented 
before and after each release (resulting in even numbers 
for releases and odd numbers for development snapshots), 
with any bug fixes subsequently applied and released for a 
stable version. The user interface of GIMP is designed by a 
dedicated design and usability team.
GIMP is presented in two forms — single window 
mode and multiple window mode. GIMP 2.8 defaults to the 
multiple window mode, in which a set of windows contains 
all GIMP’s functionality. By default, tools and tool settings 
are on the left and other dialogue boxes are on the right. A 
layers tab is often to the right of the tools tab and allows a 
user to work on separate image layers, individually. Layers 
can be edited by right-clicking on a particular one to bring 
up edit options for that layer. The tools tab and layers 
tab are the most common dockable tabs.
The current version of GIMP works with various 
operating systems including Microsoft Windows, Linux 
and Mac OS. Many Linux distributions have GIMP as a 
part of their desktop operating systems, including Fedora 
and Debian.
Inkscape
Inkscape is a free and open source vector graphics editor. 
It can be used to create or edit vector graphics such as 
illustrations, diagrams, line arts, charts, logos and complex 
paintings. Inkscape’s primary vector graphics format is 
Scalable Vector Graphics (SVG); however, many other 

DRIVING 
TECHNOLOGY,
INNOVATION &
INVESTMENTS
www.IndiaElectronicsWeek.com

Profit from IoT
India’s Electronics 
Manufacturing Show
Is there a show in India that showcases the 
latest in electronics manufacturing such as 
rapid prototyping, rapid production and table 
top manufacturing?
Yes, there is now - EFY Expo 2018. With this 
show’s focus on the areas mentioned and it 
being co-located at India Electronics Week, it 
has emerged as India's leading expo on the 
latest manufacturing technologies and 
electronic components. 
Who should attend?
• Manufacturers: CEOs, MDs, and those 
involved in firms that manufacture 
electronics and technology products
• Purchase decision makers: CEOs, 
purchase managers, production managers 
and those involved in electronics 
manufacturing
• Technology decision makers: Design 
engineers, R&D heads and those involved 
in electronics manufacturing
• Channel partners: Importers, distributors, 
resellers of electronic components, tools 
and equipment
• Investors: Startups, entrepreneurs, 
investment consultants and others 
interested in electronics manufacturing
Why you should attend
• Get updates on the latest technology 
trends in rapid prototyping and production, 
and in table top manufacturing
• Get connected with new suppliers from 
across India to improve your supply chain
• Connect with OEMs, principals and brands 
seeking channel partners and distributors
• Connect with foreign suppliers and 
principals to represent them in India
• Explore new business ideas and 
investment opportunities in this sector
Our belief is that the LED bulb is the culmination 
of various advances in technology. And such a 
product category and its associated industry 
cannot grow without focusing on the latest 
technologies. But, while there are some good 
B2B shows for LED lighting in India, none has a 
focus on ‘the technology that powers lights’. 
Thus, the need for LEDAsia.in.
Who should attend?
• Tech decision makers: CEOs, CTOs, R&D 
and design engineers and those developing 
the latest LED-based products
• Purchase decision makers: CEOs, 
purchase managers and production 
managers from manufacturing firms that use 
LEDs
• Channel partners: Importers, distributors, 
resellers of LEDs and LED lighting products
• Investors: Startups, entrepreneurs, 
investment consultants interested in this 
sector
• Enablers: System integrators, lighting 
consultants and those interested in smarter 
lighting solutions (thanks to the co-located 
IoTshow.in)
Why you should attend
• Get updates on the latest technology trends 
defining the LED and LED lighting sector
• Get a glimpse of the latest components, 
equipment and tools that help manufacture 
better lighting products
• Get connected with new suppliers from 
across India to improve your supply chain
• Connect with OEMs, principals, lighting 
brands seeking channel partners and 
systems integrators
• Connect with foreign suppliers and principals 
to represent them in India
• Explore new business ideas and investment 
opportunities in the LED and lighting sector
• Get an insider’s view of ‘IoT + Lighting’ 
solutions that make lighting smarter
Showcasing the Technology 
that Powers Light
www.IndiaElectronicsWeek.com
India’s #1 IoT show. At Electronics For You, we 
strongly believe that India has the potential to 
become a superpower in the IoT space, in the 
upcoming years. All that's needed are platforms 
for different stakeholders of the ecosystem to 
come together.
We’ve been building one such platform: 
IoTshow.in--an event for the creators, the 
enablers and customers of IoT. In February 
2018, the third edition of IoTshow.in will bring 
together a B2B expo, technical and business 
conferences, the Start-up Zone, demo sessions 
of innovative products, and more.
Who should attend?
•  Creators of IoT solutions: OEMs, design 
houses, CEOs, CTOs, design engineers, 
software developers, IT managers, etc
•  Enablers of IoT solutions: Systems 
integrators, solutions providers, distributors, 
resellers, etc
•  Business customers: Enterprises, SMEs, 
the government, defence establishments, 
academia, etc
Why you should attend
•  Get updates on the latest technology trends 
that define the IoT landscape
•  Get a glimpse of products and solutions that 
enable the development of better IoT 
solutions
•  Connect with leading IoT brands seeking 
channel partners and systems integrators
•  Connect with leading suppliers/service 
providers in the electronics, IT and telecom 
domain who can help you develop better IoT 
solutions, faster
•  Network with the who’s who of the IoT world 
and build connections with industry peers
•  Find out about IoT solutions that can help 
you reduce costs or increase revenues
•  Get updates on the latest business trends 
shaping the demand and supply of IoT 
solutions
Colocated
shows

India’s Mega Tech Conference
The EFY Conference (EFYCON) started out as a tiny 
900-footfall community conference in 2012, going by the 
name of Electronics Rocks. Within four years, it grew 
into ‘India’s largest, most exciting engineering 
conference,’ and was ranked ‘the most important IoT 
global event in 2016’ by Postscapes. 
In 2017, 11 independent conferences covering IoT, 
artificial intelligence, cyber security, data analytics, cloud 
technologies, LED lighting, SMT manufacturing, PCB 
manufacturing, etc, were held together over three days, 
as part of EFY Conferences.
Key themes of the conferences and 
workshops in 2018
• Profit from IoT: How suppliers can make money and 
customers save it by using IoT
• IT and telecom tech trends that enable IoT 
development
•  Electronics tech trends that enable IoT development
• Artificial intelligence and IoT
• Cyber security and IoT
• The latest trends in test and measurement 
equipment
• What's new in desktop manufacturing
• The latest in rapid prototyping and production 
equipment
Who should attend
• Investors and entrepreneurs in tech
• Technical decision makers and influencers
• R&D professionals
• Design engineers
• IoT solutions developers
• Systems integrators
• IT managers
www.IndiaElectronicsWeek.com
We spoke to a few members of the 
tech community to understand why 
they had not attended earlier editions of 
India Electronics Week (IEW). Our aim 
was to identify the most common 
reasons and share them with you, so 
that if you too had similar reasons, you 
may choose not to attend IEW 2018. 
This is what they shared… 
#1. Technologies like IoT, AI and 
embedded systems have no future
Frankly, I have NO interest in new 
technologies like Internet of Things 
(IoT), artificial intelligence, etc. I don't 
think these will ever take off, or become 
critical enough to affect my organisation 
or my career.
#2. I see no point in attending 
tech events
What's the point in investing energy 
and resources to attend such events? I 
would rather wait and watch—let others 
take the lead. Why take the initiative to 
understand new technologies, their 
impact and business models?
#3. My boss does not like me
My boss is not fond of me and doesn't 
really want me to grow professionally. 
And when she came to know that IEW 
2018 is an event that can help me 
advance my career, she cancelled my 
application to attend it. Thankfully, she 
is attending the event! Look forward to 
a holiday at work. 
 #4. I hate innovators! 
Oh my! Indian startups are planning to 
give LIVE demonstrations at IEW 
2018? I find that hard to believe. Worse, 
if my boss sees these, he will expect 
me to create innovative stuff too. I better 
find a way to keep him from attending.
 #5. I am way too BUSY
I am just too busy with my ongoing 
projects. They just don't seem to be 
getting over. Once I catch up, I'll invest 
some time in enhancing my knowledge 
and skills, and figure out how to meet 
my deadlines.
#6. I only like attending vendor 
events
Can you imagine an event where most 
of the speakers are not vendors? 
Where most talks will not be by people 
trying to sell their products? How 
boring! I can't imagine why anyone 
would want to attend such an event. I 
love sales talks, and I am sure 
everybody else does too. So IEW is a 
big 'no-no' for me.
#7. I don't think I need hands-on 
knowledge
I don't see any value in the tech 
workshops being organised at IEW. 
Why would anyone want hands-on 
knowledge? Isn't browsing the Net and 
watching YouTube videos a better 
alternative?
#8. I love my office!
Why do people leave the comfort of 
their offices and weave through that 
terrible traffic to attend a technical 
event? They must be crazy. What’s the 
big deal in listening to experts or 
networking with peers? I'd rather enjoy 
the coffee and the cool comfort of my 
office, and learn everything by browsing 
the Net!
 
#9. I prefer foreign events
While IEW's IoTshow.in was voted the 
‘World's No.1 IoT event’ on 
Postscapes.com, I don't see much 
value in attending such an event in 
India—and that, too, one that’s being 
put together by an Indian organiser. 
Naah! I would rather attend such an 
event in Europe.
Hope we've managed to convince 
you NOT to attend IEW 2018! 
Frankly, we too have NO clue why 
10,000-plus techies attended IEW in 
March 2017.  Perhaps there's 
something about the event that we've 
not figured out yet. But, if we haven't 
been able to dissuade you from 
attending IEW 2018, then you may 
register at http://register.efy.in.
Reasons Why You Should NOT Attend IEW 2018
Colocated
shows
Conference 
Pass Pricing
One day pass
INR 1999
PRO pass
INR 7999
Special privileges 
and packages for...
Defence and defence 
electronics personnel
Academicians
Group and bulk 
bookings
SPECIAL PACKAGES FOR
• Academicians  • Defence personnel
• Bulk/Group bookings 

• Profit from IoT   
 
• Rapid prototyping and production
• Table top manufacturing 
• LEDs and LED lighting
1. The best locations sell out first
2. The earlier you book—the better the rates; and the more the deliverables
3. We might just run out of space this year!
EFY Enterprises Pvt Ltd | D-87/1, Okhla Industrial Area, Phase -1, New Delhi– 110020
Colocated
shows
To get more details on how exhibiting at IEW 2018 can help you achieve your sales and marketing goals,
Write to us at growmybiz@efy.in
Contact us at +91-9811155335 Or
Why exhibit at IEW 2018?
The themes
The co-located shows
Why you should risk being an early bird
More technology 
decision makers and 
influencers attend IEW 
than any other event
It’s a technology- 
centric show and not 
just a B2B event
Over 3,000 visitors 
are conference 
delegates
Besides purchase 
orders, you can bag 
‘Design Ins’ and 
‘Design-Wins’ too
Co-located events 
offer cross-pollination 
of business and 
networking 
opportunities
India’s only test 
and measurement 
show is also a 
part of IEW
360-degree promotions 
via the event, publications 
and online!
 The only show in 
Bengaluru in the FY 
2017-18
Your brand and solutions 
will reach an audience of 
over 500,000 relevant and 
interested people 
IEW connects you with 
customers before the 
event, at the event, and 
even after the event
Bag year-end orders; 
meet prospects in early 
February and get orders 
before the FY ends
The world’s No.1 IoT 
show is a part of IEW and 
IoT is driving growth
It’s an Electronics 
For You Group 
property
IEW is being held at a 
venue (KTPO) that’s 
closer to where all the 
tech firms are 
Special packages for 
‘Make in India’, ‘Design in 
India’, ‘Start-up India’ and 
‘LED Lighting’ exhibitors

106 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
For U & Me
Let's Try
formats can be imported and exported. Inkscape can render 
primitive vector shapes (e.g., rectangles, ellipses, polygons, 
arcs, spirals, stars and 3D boxes) and text. These objects 
may be filled with solid colours, patterns, radial or linear 
colour gradients and their borders may be stroked, both with 
adjustable transparency. Embedding and optional tracing of 
raster graphics is also supported, enabling the editor to create 
vector graphics from photos and other raster sources. Created 
shapes can be further manipulated with transformations, such 
as moving, rotating, scaling and skewing.
Firefox
Mozilla Firefox (or simply Firefox) is a free and open source 
Web browser developed by the Mozilla Foundation and its 
subsidiary, the Mozilla Corporation. Firefox is available 
for Microsoft Windows, Linux and Mac operating systems. 
Firefox for Android is available for Android (formerly 
Firefox for Mobile, it also ran on the discontinued Firefox 
OS). Features include tabbed browsing, spell checking, 
incremental find and live bookmarking. Smart bookmarks, 
a download manager, private browsing and location-aware 
browsing are also available.
Firefox can have themes added to it so that users can 
experience it in a more personal way. There are websites 
where users can create and download personalised themes 
with their choice of colours and images. However, Mozilla 
has announced its intention to discontinue Firefox themes. 
The Firefox add-on website also allows users to add games, 
ad-blockers, screenshot apps, and many other useful 
applications.
Firefox was the browser that challenged Internet 
Explorer’s monopoly in the first decade of this century, but 
has since lost momentum and the most popular browser 
in the world now is Google Chrome.
Thunderbird
Mozilla Thunderbird or Thunderbird is a free, open source, 
cross-platform email, news, RSS and chat client developed by 
the Mozilla Foundation. The project strategy was modelled 
after that of the Mozilla Firefox Web browser.
WordPress
WordPress is a free and open source content management 
system (CMS) based on PHP and MySQL. WordPress is 
installed on a Web server which either is part of an Internet 
hosting service or is a network host itself; the first case may 
be on a service like WordPress.com, for example, and the 
second case is a computer running the software package 
WordPress.org. An example of the second case is a local 
computer configured to act as its own Web server hosting 
WordPress for single-user testing or learning purposes. 
Features include a plugin architecture and a template 
system. WordPress was used by more than 26.4 per cent of 
the top 10 million websites as of April 2016. It is reportedly 
the easiest and most popular website management or 
blogging system in use on the Web, and supports more than 
60 million websites. It has a Web template system that uses 
a template processor.
WordPress users may install and switch between different 
themes, allowing users to change the look and functionality of 
a WordPress website; these can be installed without altering 
the content or health of the site. Every WordPress website 
requires at least one theme, and each one should be designed 
using WordPress standards with structured PHP, valid HTML 
and Cascading Style Sheets (CSS).
WordPress is the blogging platform of choice for 
businesses along with Google’s Blogger. Blogging is also 
possible on Medium, Weebly and LinkedIn. A business 
aiming to gain visibility by blogging must have a presence on 
each of these platforms.
VLC media player
VLC media player is free and open source. It is a multimedia 
framework written by the VideoLAN project. VLC is a 
portable multimedia player, encoder and streamer supporting 
many audio and video codecs and file formats as well as 
DVDs, VCDs and various streaming protocols. It is able 
to stream over networks and to transcode multimedia files, 
and save them in various formats. VLC used to stand for 
VideoLAN Client but, since VLC is no longer simply a 
client, that no longer applies. VLC media player is available 
for Microsoft Windows, Linux and Mac OS. VLC is also 
available for Android devices. The default distribution of 
VLC includes a large number of free decoding and encoding 
libraries avoiding the need for finding/calibrating proprietary 
plugins. A key point in favour of VLC is its ability to play 
virtually every video and audio codec (note the inability of 
Windows Media Player to play the amr audio codec).
While open source software can be obtained free, there 
are also some issues involved when using these. One is 
the frequency of updates, which depends solely on the 
developers. Frequent updates are preferred so that the 
software remains useful. Another issue is the stability of 
the software. Business critical software must be stable and 
bug-free. Compatibility with proprietary software used by 
business partners is another issue. A company must be able 
to open a document sent by a business partner who uses 
proprietary software.
My company has developed what it calls the MultiSpectra 
OS. This basically consists of Ubuntu Linux with LibreOffice, 
Scribus, GIMP, Inkscape, Firefox, Thunderbird and VLC 
media player. 
By: Dr Amartya Kumar Bhattacharya
The author is the chairman and MD of MultiSpectra 
Consultants, Kolkata. He is eminently qualified and can be 
contacted at dramartyakumar@gmail.com.

For U & Me
Overview
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 107
T
he Indian IT industry is going through testing times. 
Amidst hype around the huge layoffs, which is fuelling 
a lot of speculation, the bitter truth is that automation 
is killing some jobs, there is slow 
growth in the traditional services 
business coupled with an increase in 
protectionism, and newer business 
models that decouple revenue and 
headcount have emerged. The need 
to take charge of one’s career has 
never been felt as strongly as it is 
being felt now. It is a fact that skills 
that could fetch jobs at a premium a 
few years back are no longer fancied 
today. It is tough for people who 
have not invested in themselves or grown as professionals to 
survive and stay relevant in today’s fast-changing times.
The good news, however, is that jobs are aplenty but only 
for those who have stayed relevant by investing in themselves, 
having taken risks to step out of their comfort zone to learn 
what is ‘new’. Automation may have made  some jobs 
redundant but it has also created a new set of jobs. With the 
decline in traditional services, and the sharp increase in digital 
and engineering R&D services, skills have become much 
more important than scale. The types of skills in demand have 
changed but the number of new jobs continues to be high.
A recent NASSCOM report indicates that 40 per cent of 
the tech professionals in India need to reskill over the next 
five years, if they want to survive the onslaught of automation.  
Another startling statistic is that India will lose up to 640,000 
low-skilled jobs to automation by 2021, and the IT industry 
will shrink by 14 per cent within the same time period. With 
open source technology platforms becoming mainstream, the 
potential for developers to experiment and evolve is vast. Jobs 
and skills are not limited by how much you know, but by what 
you can do with the knowhow. Open source platforms have 
always been a source of innovation for developers. 
Different open source tools and open standards are important, 
and will continue to be so in the future, in order to ensure that all 
the devices being used today are able to interconnect properly. 
These open source tools will also be responsible for the back-end 
tasks of processing the large volumes of Big Data that all of these 
devices will generate in the future.
It is imperative for every individual to understand what 
skills they have an aptitude for, what interests them and 
what the industry trends are. This will help them pick skills 
that make them much more competitive in the job market. 
IT professionals must keep learning and reinvent continuously to stay in 
demand in the fast-changing job market.
Also, individuals need to introspect every few years in 
order to stay relevant in the industry and prolong their IT 
career. Organisations and individuals who have consistently 
reinvented themselves every few 
years have managed to forge ahead.
Gone are the days of 
generalists—the IT industry 
needs specialists today. Learning 
and implementing any new age 
technology, which includes 
certifications in mobile and Web 
development, data science and Big 
Data analytics, machine learning, 
UI/UX design, robotics, cyber 
security and so on, will be in 
demand. One could even choose to be an SME in the field of 
security, finance and payments.
An important point to note here is that soft skills have 
emerged as a critical skill without which survival in the IT 
industry is a challenge today — this was not the case a decade 
ago. With the industry becoming boundary-less and global, 
job seekers must have good communication and presentation 
skills along with good stakeholder management, as well as 
negotiation and interpersonal skills. Also, working on open 
source projects not only enables one to enjoy the work and 
contribute back to community-developed projects, but also 
helps to hone one’s skills and stay relevant in the job market.
Honing skills need not be restricted to junior level 
employees or developers but applies to mid-level managers 
and senior leaders too. Mid-level managers who know how to 
manage traditional engagements should learn how to manage 
engagements with newer development methodologies. They 
can get new certifications under their belt, making them 
more attractive to prospective employers. Senior leaders, on 
the other hand, who have a proven track record of creating 
leadership pipelines can consider becoming certified 
leadership coaches or certified executive coaches, as they 
have the ability to bring out the best in people; that’s a skill 
that will always be in demand.
The industry will always be in a state of evolution, so it is 
up to individuals whether they view this as a challenge or an 
opportunity. The time is now or never, and if you do not take 
control of your own career, no one else will. 
Evolving Careers and Skills  
in the IT Industry
By: Ashwani Nandini
The author is India head - delivery assurance, GlobalLogic India. 
She can be reached at a.nandini@globallogic.com

OpenGurus
Let’s Try
108 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
T
he GNU MCU Eclipse project, formerly known as 
GNU ARM Eclipse, offers a variety of plugins and 
components for the development of ARM targets for 
the Cortex-M series architecture. In this article, we’ll discuss 
various components offered under this project, the supported 
features, and the installation and usage of each. We’ll follow 
that up by building a simple project and emulating the Blinky 
code under the Qemu environment.
This project offers the following components 
for ARM development:
 
MCU plugins to generate template code, as well as to 
build, flash and debug.
 
Qemu, to emulate the generated outcome without the need 
of actual hardware, with good support for popular STM32 
series targets.
 
OpenOCD, to flash and debug code through JTAG probes 
and debugging hardware.
 
Windows Build Tools, for the Linux-like make utility and 
basic commands like rm and echo used in makefiles.
 
Packs manager, to install Keil/ARM CMSIS packs (in an 
experimental state, at present).
But the toolchain is not hosted under this project, and 
these plugins work well with standard toolchains designed 
for supported architecture. Let’s now take a tour of these 
components, their installation and usage.
Toolchain
You need a toolchain to build the code for the target 
architecture. For this, download the prebuilt binaries of 
GNU ARM Embedded Toolchain from developer.arm.
com/open-source/gnu-toolchain/gnu-rm according to the 
host environment, i.e., Windows, Linux or Mac OS. This 
toolchain has a typical cycle of quarterly releases, and the 
latest release is 6-2017-q2-update which ships with GCC 6. 
From 6-2016-q4-major onwards, prebuilt binaries for 32-bit 
Linux have been dropped and only support for 64-bit Linux 
is available. However, one can build the toolchain from 
available sources for any host architecture. Releases up to 
v5-2016-q3 are also available at launchpad.net/gcc-arm-
embedded, with binary support for 32-bit Linux only, and 
through PPA for Ubuntu distributions.
This toolchain provides various tools for development 
and the anatomy of generated files with the prefix ‘arm-none-
eabi-‘. A few significant tools are:
 
arm-none-eabi-gcc, which is a wrapper for various phases 
of C development 
Developing ARM Targets  
Using GNU MCU Eclipse
If you are thinking of kickstarting the development of microcontroller targets, but 
don’t have suitable hardware, or if you are worrying about hardware issues, here 
is the perfect solution for you. The GNU MCU Eclipse project comes with great 
features to overcome all these worries.

OpenGurus
Let’s Try
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 109
 
arm-none-eabi-g++, a wrapper for various 
phases of C++ development
 
arm-none-eabi-ld, which links several object files and 
libraries, generating an executable with the help of linker 
maps and startup code
 
arm-none-eabi-as, for assembling code and 
to generate object files
 
arm-none-eabi-cpp, for preprocessing source code
 
arm-none-eabi-objcopy, to convert an executable file from 
elf format into bin or hex format
 
arm-none-eabi-nm, to check the symbol table
 
arm-none-eabi-objdump, which is to disassemble 
generated object files and section analysis
 
arm-none-eabi-readelf, for reading the headers based on 
the magic number present
 
arm-none-eabi-size, to check the section wise footprint of 
the generated executable file
 
arm-none-eabi-gdb, for debugging and tracing with the 
help of debugging hardware
OpenOCD
This on-chip debugger utility comes with flashing and remote 
debugging support for various target boards. It works with 
different JTAG probes connected to the host with onboard or 
external hardware debuggers. GNU MCU Eclipse OpenOCD 
is a fork of the original OpenOCD and it is designed 
to work well with MCU plugins. Prebuilt binaries are 
available at gnu-mcu-eclipse.github.io/openocd/releases for 
various host environments.
Qemu
Qemu, informally known as ‘Quick emulator’, can execute 
applications built for various target architectures and can 
emulate full system images also. Mainline Qemu has limited 
support for bare metal targets (a few of the TI Stellaris family 
only) based on the Cortex-M architecture.
GNU MCU Eclipse Qemu is a fork of the original Qemu 
and comes with support for various popular targets of the 
STM32 Discovery and Nucleo series. Support for more 
targets like NXP/Freescale Freedom Kinetis series, NXP 
LPC series, TI Tiva series, Atmel SAM series and Infineon 
CPU Card/Board series is in the pipeline for future releases. 
Please stay tuned to gnu-mcu-eclipse.github.io/qemu/ for 
more updates.
Download the latest stable version of Qemu binaries 
from github.com/gnu-mcu-eclipse/qemu/releases, extract to a 
suitable location and update the path to the bin directory.
IDE with plugins
Download the latest stable version of GNU MCU Eclipse IDE 
for C/C++ developers from github.com/gnu-mcu-eclipse/org.
eclipse.epp.packages/releases. These archives are already 
bundled with CDT and MCU plugins. The first major release 
(which is the latest one, as of now) is based on the Neon 3 
release of Eclipse IDE and v4.1.1 of MCU plugins.
Alternatively, you can download the regular Eclipse 
IDE for C/C++ developers using the Eclipse installer or 
through offline archive options, and update the MCU 
plugins via Eclipse Market Place. For this, navigate to 
Help --> Eclipse Market Place, search for GNU MCU 
Eclipse and install the plugins.
These plugins are available at https://gnu-mcu-eclipse.
netlify.com/v4-neon-updates, which can be specified for 
manual installation by navigating to Help --> Install New 
Software and adding a new repository with the above URL. 
This repository supersedes the earlier plugins of v3.x hosted 
at http://gnuarmeclipse.sourceforge.net/updates.
The plugins come with support for various targets from 
the STM32 series, the NXP/Freescale Kinetis series of boards 
and are capable of the following:
 
Generating template projects
 
Building the code with the help of a cross toolchain
 
Flashing and debugging code using OpenOCD, J-Link or 
CodeRed modes 
 
Emulating using Qemu in the absence of hardware
To map the necessary components with the installed 
plugins, go to Windows --> Preferences --> MCU and fill 
the suitable locations of Toolchain, OpenOCD, Qemu, 
etc, to enable project wizards to make use of the tools 
available in these directories.
 Note: Actual components like Toolchain, OpenOCD 
and Qemu need to be installed individually, as mentioned 
in earlier sections. This plugin set just supports the above 
components in a friendly mode.
Project templates
The installed plugins (prebundled or manually installed via 
Market Place) enable project wizards for the STM32 Fxxx 
series, Freescale Kinetis KLxx series, Analog Devices’ 
ADuCM36X family of boards and a template for the generic 
Cortex-M series target. These wizards help us to create empty 
projects or Blinky templates with vendor-specific abstraction 
APIs and target-specific options under Project settings.
Blinky demo
Let’s try a Blinky demo for the STM32 F4 Discovery 
target. This board is equipped with four user LEDs, 
one user pushbutton, an onboard accelerometer, USB 
OTG connector, an audio interface and pin out for other 
interfaces. OpenOCD works well with the onboard ST-Link 
hardware for debugging purposes. Create a new C project 
using File --> New --> C Project. Next, choose STM32F4xx 
C/C++ project as Type and give it a suitable name, 
say f4-blinky, as shown in Figure 1.
In the next wizard, choose the desired MCU chip 
according to the target board and configure other 

OpenGurus
Let’s Try
110 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com
parameters like Flash memory size, clock speed, etc. 
Choose the Blinky template under Content type, and leave 
other options with default values for the initial work 
as shown in Figure 2. Follow default values for Folder 
settings and select configurations in the next two wizards. 
In the final wizard, provide the path of the bin directory 
under the extracted toolchain and hit Finish to complete 
the set-up process.
Right-click on the generated project, i.e., f4-blinky 
under the Project Explorer pane, and then click on Build 
Project. On successfully completing the build, you can see 
the following console output: 
Finished building target: f4-blinky.elf
Invoking: GNU ARM Cross Create Flash Image
arm-none-eabi-objcopy -O ihex “f4-blinky.elf” “f4-blinky.
hex” Finished building: f4-blinky.hex
Invoking: GNU ARM Cross Print Size
arm-none-eabi-size --format=berkeley “f4-blinky.elf”
   text 
   data 
    bss 
    dec 
    hex 
filename
   8599 
    160 
    420 
   9179 
   23db 
f4-blinky.elf
Finished building: f4-blinky.siz
Project settings
Once the project is generated, you can tune various options 
for the preprocessor, C/C++ compiler, linker, etc. For this, 
right-click on the project, and go to Properties --> C/C++ 
Build --> Settings.
Under the Tool Settings tab, you can choose various 
options like Target, Assembler, Compiler, Linker, Flash, 
Figure 1: Project template selection
Figure 2: Target configuration
Figure 3: Project settings
etc. Plugins will generate suitable options of the toolchain 
for each selection. For example, if you want to generate a 
Flash image in Raw Binary format instead of Intel HEX, 
choose the output file format under GNU ARM Cross 
Create Flash Image --> Output File Format. Likewise, 
you can play around with various options under this tab 
view and check the selected values under ‘All Options’ 
expected for various tools in CLI mode. This way you 
can also learn about the various options to be used with a 
toolchain in simple interactive steps.
Flashing the code – Run/Debug
Once project settings are customised, rebuild the project. 
Right click the project and navigate to Debug --> Debug 
As --> Debug Configurations. Double-click on GDB 

OpenGurus
Let’s Try
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 111
By: Rajesh Sola 
The author is a faculty member of C-DAC’s Advanced Computing 
Training School, and an evangelist in the embedded systems and 
IoT domains. You can reach him at rajeshsola@gmail.com.
Figure 4: Blinky demo of Qemu debugging settings
https://gnu-mcu-eclipse.github.io/en.wikipedia.
org/wiki/RISC-V
Reference
QEMU Debugging to generate a new configuration (for 
the first time only). Under the Debugger tab, fill in the 
Board name as STM32F4-Discovery and Device name as 
STM32F407VG, as shown in Figure 4. You can try this with 
other board names as listed by qemu-system-gnuarmeclipse 
-M ? and other device (MCU cores) names as listed by 
qemu-system-gnuarmeclipse -mcu ?
Now, launch the GDB session by hitting the Debug 
option. The Eclipse perspective will be changed to debug 
mode and you can try various debug options like suspend, 
resume, terminate, step into, step over, step return, etc. 
Also, you can watch the state of variables, breakpoints and 
registers. You can also watch peripheral registers if the packs 
plugin and target-specific packs are installed.
Similarly, if you are flashing the code on a real target, 
create an OpenOCD configuration and fill in the config 
options as -f board/stm32f4discovery.cfg under the Debugger 
tab, and launch an OpenOCD session. You may rename 
the generated configuration in a convenient way. You can 
specify a suitable OpenOCD script name for other targets 
under the config options.
Once the debug session is launched, you will observe 
that the selected LED is blinking for a given time interval 
on the real target or on the graphical window enabled by 
Qemu emulation, and semi-hosting output from the target 
under the Console tab. At present, the Qemu based simulator 
supports LED blinking and button-pressing only; to test other 
peripherals, you need to go for a real target.
If you are not planning to trace the code, step by step, or 
watch the intermediate snapshots, right click on the project 
and select Run Configurations under the Run As sub-menu. 
This executes code directly on a real target or under Qemu. 
Run and debug sessions can also be launched using the 
available menu options or toolbar options.
RISC-V architecture support
RISC-V, pronounced as RISC five, is an open instruction 
set architecture that is based on RISC principles. It enables 
open hardware design in a better way as the ISA is patent-
free, unlike other architectures, and many CPU designs are 
available under the BSD licence. Several projects based on 
the RISC-V architecture are hosted at github.com/riscv like 
the toolchain, debugger, emulator and Linux kernel port, 
file system, standard libraries, etc. The GNU MCU Eclipse 
project comes with added support for this open architecture 
through the plugins of v4.x onwards.
Since the project code, necessary libraries, toolchain and 
plugins are open under this environment, you can choose 
the desired versions of the toolchain and debugger from 
any repository of your choice. This helps in getting a better 
understanding of the whole development cycle and the 
anatomy of each phase, as well as the generated outcome. 
You can try these steps and explore further! 
The latest from the Open Source world is here.
OpenSourceForU.com
Join the community at facebook.com/opensourceforu
Follow us on Twitter @OpenSourceForU
THE COMPLETE MAGAZINE ON OPEN SOURCE

TIPS
TRICKS
&
To download an example, use the following 
command:
git clone https://github.com/hemanth22/CSVToCustomDSV.git 
—Hemanth Bitra,  
hemanthbitra@bitroid.in
Resetting a forgotten root user account 
password in CentOS 7/RHEL
We can easily reset the root user account by booting into 
the single mode using the following steps.
Step 1: While booting the system, you see a screen 
with two options as shown below:
CentOS Linux, with Linux 3.10.0-123.el7.x86_64
CentOS Linux, with Linux 0-rescue-27ca624e1d5b47ad97bba2bbc
648d347
Use the arrow keys as soon as you see the screen 
with the above two options so that a time-out does not 
take place. Now select the first option and press the ‘e’ 
key to edit.
Step 2:  You will be presented with the kernel 
parameters’ screen, and you may have to use the arrow 
key to see the line where the ‘vmlinuz’ mount point is 
set. Here, search for the text ‘rhgb quiet’ and replace 
this text with ‘init=/bin/bash’ without quotes, and 
press Control+x to boot the system with this edited 
configuration.
Step 3: After booting, you will see what’s shown 
below:
               
 bash-4.2# 
Converting a CSV file to a DSV file 
using the column and cat commands
To understand how to convert a CSV file to a DSV file, 
let’s look at a few examples of ESS.csv.
ESS.csv is a comma-separated file; its left and right 
enclosure is ‘“’ and the field separator is ‘,’. 
Example 1: To convert the ESS.csv file to a 
delimited file (i.e., dsv) using the column command, try 
the following command:
cat ESS.csv | column -t -s ‘”’ -o ‘’ | column -t -s ‘,’ -o 
‘;’ > ESS.dsv
The above command changes the left and right 
enclosures from ‘“’ to none and the field separator is ‘;’.
You can customise the left enclosure as per your 
requirements.
Example 2: Change the left and right enclosures 
from ‘“’ to ‘‘’ and the field separator is ‘;’.
cat ESS.csv | column -t -s ‘”’ -o ‘’’ | column -t -s ‘,’ -o 
‘;’ > ESS.dsv
Example 3: Change the left and right enclosures 
from ‘“’ to ‘:’ and the field separator is ‘;’.
cat ES.csv | column -t -s \’\’\’ -o ‘:’ | column -t -s ‘,’ 
-o ‘;’ > ES.dsv
Example files are available on GitHub at 
https://github.com/hemanth22/CSVToCustomDSV 
and at the GitHub Wiki guide https://github.com/
hemanth22/CSVToCustomDSV/wiki/Guide-to-use-
CSVToCustomDSV.
112 | NOVEMBER 2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com

               
You will directly get the bash shell with root 
privileges where you can reset the root password using 
the ‘passwd’ command as shown below:
bash-4.2# passwd
 You can successfully reset the root account 
password in this way. 
—Sharmin Shaikh,  
imsharmeen@gmail.com
Never miss the Ubuntu notification 
‘system restart required’
We must reboot the server when we install a new kernel 
or libraries. Ubuntu Linux can tell you if the system 
needs a reboot when you login as root user.
 
*** System restart required ***
 
The above warning will appear on the login screen, 
but we may miss this warning. So here is the simple 
script for administrators to find out exactly when the 
system requires a reboot.
You can manually find the file /var/run/reboot-
required, which indicates that a system restart is 
required. 
Instead of a manual search, you can use the 
following script to identify the Ubuntu notification of 
‘system restart required’ at every login and never miss 
the warning. 
Add the following lines in the file /root/status.sh:
 
#!/bin/bash
if [ -f /var/run/reboot-required ]
then
    echo -e “33[33;31m[*** Hello $USER, you must reboot 
your machine ***]33[0m”
else
    echo -e “33[33;32m\n*** System reboot is NOT required 
now ***\n`cat /var/lib/update-notifier/updates-available`\
n33[0m”
fi
 
Now we can add the file path /root/status.sh in /
root/.bashrc file to check for the ‘Reboot required’ task 
at every login made by the root user.
 
# echo “/root/status.sh” >> /root/.bashrc && tail -n3 /
root/.bashrc
—Ranjithkumar T,  
ranjith.stc@gmail.com
Multiple tips for daily use
1. Copy from or paste into the terminal in Ubuntu
Many people try to paste into the terminal by 
using Ctrl+v. But this will not work. We need to use 
Ctrl+Shift+v to paste into the terminal. Similarly, to copy 
from the terminal, select the text to be copied and use 
Ctrl+Shift+c.
This is the simplest and easiest way for new users to 
overcome the problems of copying and pasting into the 
terminal. 
2. Using ‘tac’ to output the contents of a file 
instead of ‘cat’
Many people use the ‘cat’ command to output the contents 
on the terminal. If the contents are huge, then users have to 
scroll up till they reach the first line of the content. Instead 
of scrolling all the way up till the beginning and scanning, 
users can replace ‘cat’ with ‘tac’. The latter works just like 
the ‘cat’ command — the only difference is that the results 
are printed in reverse order; i.e., the first line comes last and 
the last line appears first. So users can easily see the first 
line without scrolling all the way to the beginning. 
3. Getting to know the permissions of files using the ‘ls-l’ 
command on the terminal in Ubuntu
One can get to know the permission of any file in the 
system by executing the ‘ls-l’ command on the terminal. 
The permissions are shown in the first column. r-read, 
w-write and x-execute are permissions for different 
users, i.e., the first three characters are to show 
permissions of the owner on that file, the next three are 
for group permissions and the last three are for others’ 
permissions on that file. There will be an initial ‘-’ 
before listing permissions of that file. So users can get to 
know who all can access the file that they themselves are 
using or going to.
As an example, -rwxrw-r-- tells us that there is read, 
write and execute permissions for the owner;  read and 
write permissions for the group; and only read permission 
for others, on the file. 
—Anirudh Kalwa, 
anirudh.3194@gmail.com
Share Your Open Source Recipes!
The joy of using open source software is in finding ways to get 
around problems—take them head on, defeat them! We invite 
you to share your tips and tricks with us for publication in 
OSFY so that they can reach a wider audience. Your tips could 
be related to administration, programming, troubleshooting or 
general tweaking. Submit them at www.opensourceforu.com. 
The sender of each published tip will get a T-shirt.
www.OpenSourceForU.com | OPEN SOURCE FOR YOU | NOVEMBER 2017 | 113

OSFY DVD
DVD OF THE MONTH
Try the latest version of KNOPPIX, the versatile Linux distro.
N
ot
e:
 A
n
y 
o
bj
e
ct
io
n
a
bl
e 
m
at
e
ri
al
, 
if
 f
o
u
n
d 
o
n 
th
e 
di
s
c,
 i
s 
u
ni
nt
e
n
d
e
d,
 a
n
d 
s
h
o
ul
d 
b
e 
at
tr
ib
ut
e
d 
to
 t
h
e 
co
m
pl
e
x 
n
at
u
r
e 
of
 I
nt
e
r
n
et
 d
at
a.
In c
ase 
this
 DV
D do
es n
ot w
ork 
prop
erly,
 wri
te t
o us
 at s
upp
ort@
efy.i
n fo
r a f
ree 
repl
ace
men
t.
 A bootable DVD with a collection of GNU/Linux 
software, automatic hardware detection, and support 
for many graphics cards, sound cards, SCSI and USB 
devices, as well as other peripherals. One of the most 
versatile live operating system
November 2017
Knoppix 8.1
CD T
eam
 e-m
ail: 
cdtea
m@
efy.i
n
Rec
om
men
ded 
Syst
em 
Req
uire
men
ts: P
4, 1
GB 
RAM
, DV
D-R
OM 
Driv
e
What is a live DVD?
A live CD/DVD or live disk contains a bootable 
operating system, the core program of any computer, 
which is designed to run all your programs and manage 
all your hardware and software.
Live CDs/DVDs have the ability to run a complete, 
modern OS on a computer even without secondary 
storage, such as a hard disk drive. The CD/DVD directly 
runs the OS and other applications from the DVD drive 
itself. Thus, a live disk allows you to try the OS before 
you install it, without erasing or installing anything on 
your current system. Such disks are used to demonstrate 
features or try out a release. They are also used for 
testing hardware functionality, before actual installation. 
To run a live DVD, you need to boot your computer 
using the disk in the ROM drive. To know how to set 
a boot device in BIOS, please refer to the hardware 
documentation for your computer/laptop.
KNOPPIX 8.1
KNOPPIX is a bootable DVD with a collection of 
GNU/Linux software, automatic hardware detection, 
and support for many graphics cards, sound cards, 
SCSI and USB devices, as well as other peripherals. 
KNOPPIX can be used as a Linux demo, an 
educational DVD, a rescue system, or adapted as a 
platform for commercial software product demos. 
Although KNOPPIX is primarily designed to be used 
as a live DVD, it can even be installed on a hard disk 
like a typical operating systems. It also works as a 
resource toolkit for system administrators. 
The latest release comes with UEFI secure boot 
support. The BFQ (Budget Fair Queue Scheduler) 
has been now included in the standard kernel within 
the multi-path scheduler, automatically activated 
for slow disks. Other highlights are LibreOffice 
5.4.1, GIMP 2.8.20, Chromium 60.0.3112.78 and 
Firefox 55 Web browser with ublock Origin and the 
NoScript Security-Plugin. For more details on the 
list of updates, you can visit http://www.knopper.net/
knoppix/knoppix810-en.html
114 | NOVEMBER  2017 | OPEN SOURCE FOR YOU | www.OpenSourceForU.com


Loonycorn 
 
 
 
 
Some of our recent courses:  
 
On Udemy: 
 
 Building Voice-Enabled Apps with Amazon Alexa 
 
 Building ChatBots with Amazon Lex 
 
 GCP: Complete Google Data Engineer and Cloud Architect 
Guide  
 
On Pluralsight: 
  
 Unsupervised Machine Learning with TensorFlow 
 
 Classification Models in TensorFlow 
 
 Understanding the Foundations of TensorFlow 
 
 
 
 
October 2017 


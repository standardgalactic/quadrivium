Solutions Manual 
 
 
 
 
 
 
A First Course in 
PROBABILITY 
Seventh Edition 
 
 
 
 
 
Sheldon Ross 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Prentice Hall, Upper Saddle River NJ 07458 


Table of Contents 
 
 
 
 
Chapter 1 ..............................................................................1 
 
Chapter 2 ..............................................................................10 
 
Chapter 3 ..............................................................................20 
 
Chapter 4 ..............................................................................46 
 
Chapter 5 ..............................................................................64 
 
Chapter 6 ..............................................................................77 
 
Chapter 7 ..............................................................................98 
 
Chapter 8 ..............................................................................133 
 
Chapter 9 ..............................................................................139 
 
Chapter 10 ............................................................................141 

Chapter 1 
 
1 
Chapter 1  
 
Problems 
 
1. 
(a) By the generalized basic principle of counting there are 
 
 
 
 
26 ⋅ 26 ⋅ 10 ⋅ 10 ⋅ 10 ⋅ 10 ⋅ 10 = 67,600,000 
 
 
(b) 26 ⋅ 25 ⋅ 10 ⋅ 9 ⋅ 8 ⋅ 7 ⋅ 6 = 19,656,000 
 
2. 
64 = 1296 
 
3. 
An assignment is a sequence i1, …, i20 where ij is the job to which person j is assigned.  Since 
only one person can be assigned to a job, it follows that the sequence is a permutation of the 
numbers 1, …, 20 and so there are 20! different possible assignments. 
 
4. 
There are 4! possible arrangements.  By assigning instruments to Jay, Jack, John and Jim, in 
that order, we see by the generalized basic principle that there are 2 ⋅ 1 ⋅ 2 ⋅ 1 = 4 possibilities. 
 
5. 
There were 8 ⋅ 2 ⋅ 9 = 144 possible codes.  There were 1 ⋅ 2 ⋅ 9 = 18 that started with a 4. 
 
6. 
Each kitten can be identified by a code number i, j, k, l where each of i, j, k, l is any of the 
numbers from 1 to 7.  The number i represents which wife is carrying the kitten, j then 
represents which of that wife’s  7 sacks contain the kitten; k represents which of the 7 cats in 
sack j of wife i is the mother of the kitten; and l represents the number of the kitten of cat k in 
sack j of wife i.  By the generalized principle there are thus 7 ⋅ 7 ⋅ 7 ⋅ 7 = 2401 kittens 
 
7. 
(a) 6! = 720  
 
 
(b) 2 ⋅ 3! ⋅ 3! = 72 
 
 
 
(c) 4!3! = 144  
 
(d) 6 ⋅ 3 ⋅ 2 ⋅ 2 ⋅ 1 ⋅ 1 = 72 
 
8. 
(a) 5! = 120 
 
(b) 
!
2
!
2
!
7
 = 1260 
 
(c) 
!
2
!
4
!
4
!
11
 = 34,650 
 
(d) 
!
2
!
2
!
7
 = 1260 
 
9. 
!
4
!
6
!)
12
(
 = 27,720 
 
10. 
(a) 8! = 40,320 
 
(b) 2 ⋅ 7! = 10,080 
 
(c) 5!4! = 2,880 
 
(d) 4!24 = 384 

2 
 
Chapter 1 
11. 
(a) 6! 
 
(b) 3!2!3!  
 
(c) 3!4! 
 
12. 
(a) 305 
 
(b) 30 ⋅ 29 ⋅ 28 ⋅ 27 ⋅ 26 
 
13. 






2
20  
 
14. 






5
52  
 
15. 
There are 












5
12
5
10
 possible choices of the 5 men and 5 women.  They can then be paired up 
in 5! ways, since if we arbitrarily order the men then the first man can be paired with any of 
the 5 women, the next with any of the remaining 4, and so on.  Hence, there are 












5
12
5
10
!
5
 
possible results. 
 
16. 
(a) 






+






+






2
4
2
7
2
6
 = 42 possibilities. 
 
(b) There are 6 ⋅ 7 choices of a math and a science book, 6 ⋅ 4 choices of a math and an 
economics book, and 7 ⋅ 4 choices of a science and an economics book.  Hence, there are 
94 possible choices. 
 
17. 
The first gift can go to any of the 10 children, the second to any of the remaining 9 children, 
and so on.  Hence, there are 10 ⋅ 9 ⋅ 8 ⋅ ⋅ ⋅ 5 ⋅ 4 = 604,800 possibilities. 
 
18. 


















3
4
2
6
2
5
 = 600 
 
19. 
(a) There are 


















+












2
4
1
2
3
8
3
4
3
8
 = 896 possible committees. 
 
 
There are 












3
4
3
8
 that do not contain either of the 2 men, and there are 


















2
4
1
2
3
8
that 
contain exactly 1 of them. 
 
 
(b) There are 


















+












3
6
2
6
1
2
3
6
3
6
= 1000 possible committees. 

Chapter 1 
 
3 
 
(c) There are 












+












+












2
5
3
7
3
5
2
7
3
5
3
7
 = 910 possible committees.  There are 












3
5
3
7
 in 
which neither feuding party serves; 












3
5
2
7
 in which the feuding women serves; and 












2
5
3
7
 in which the feuding man serves. 
 
20. 






+


















+






3
6
5
6
,
4
6
1
2
5
6
 
 
21. 
!
4
!
3
!
7
 = 35.  Each path is a linear arrangement of 4 r’s and 3 u’s (r for right and u for up).  For 
instance the arrangement r, r, u, u, r, r, u specifies the path whose first 2 steps are to the right, 
next 2 steps are up, next 2 are to the right, and final step is up. 
 
22. 
There are 
!
2
!
2
!
4
 paths from A to the circled point; and 
!1!
2
!
3
 paths from the circled point to B.  
Thus, by the basic principle, there are 18 different paths from A to B that go through the 
circled piont. 
 
23. 
3!23 
 
25. 






13
,
13
,
13
,
13
52
 
 
27. 
!
5
!
4
!
3
!
12
5
,4
,3
12
=






 
 
28. 
Assuming teachers are distinct. 
 
(a) 48 
 
(b) 
4)
2
(
!
8
2
,2
,2
,2
8
=






 = 2520. 
 
29. 
(a) (10)!/3!4!2! 
 
 
(b) 
!
2
!
4
!
7
2
3
3






 
 
30. 
2 ⋅ 9! − 228! since 2 ⋅ 9! is the number in which the French and English are next to each other 
and 228! the number in which the French and English are next to each other and the U.S. and 
Russian are next to each other. 
 

4 
 
Chapter 1 
31. 
(a) number of nonnegative integer solutions of x1 + x2 + x3 + x4 = 8. 
 
Hence, answer is 






3
11  = 165 
 
 
 
(b) here it is the number of positive solutions—hence answer is 






3
7  = 35 
 
32. 
(a) number of nonnegative solutions of x1 + … + x6 = 8  
 
 
answer = 






5
13  
 
 
(b) (number of solutions of x1 + … + x6 = 5) × (number of solutions of x1 + … + x6  = 3) = 












5
8
5
10
 
 
33. 
(a) x1 + x2 + x3 + x4 = 20, x1 ≥ 2, x2 ≥ 2, x3 ≥ 3, x4 ≥ 4  
 
 
Let y1 = x1 − 1, y2 = x2 − 1, y3 = x3 − 2, y4 = x4 − 3 
 
 
 
 
y1 + y2 +  y3 + y4 = 13, yi > 0 
 
 
 
Hence, there are 






3
12  = 220 possible strategies. 
 
 
(b) there are 






2
15  investments only in 1, 2, 3 
 
 
there are 






2
14  investments only in 1, 2, 4 
 
 
there are 






2
13  investments only in 1, 3, 4 
 
 
there are 






2
13  investments only in 2, 3, 4 
 
 
 






2
15  + 






2
14  + 






+






3
12
2
13
2
 = 552 possibilities 
 
 
 

Chapter 1 
 
5 
Theoretical Exercises 
 
2. 
∑=
i
m
i n
1
 
 
3. 
n(n − 1) ⋅ ⋅ ⋅ (n − r + 1) = n!/(n − r)! 
 
4. 
Each arrangement is determined by the choice of the r positions where the black balls are 
situated. 
 
5. 
There are 






j
n  different 0 − 1 vectors whose sum is j, since any such vector can be 
characterized by a selection of j of the n indices whose values are then set equal to 1.  Hence 
there are 






∑
=
j
n
n
k
j
 vectors that meet the criterion. 
 
6. 






k
n  
 
7. 






−
−
+






−
1
1
1
r
n
r
n
 = 
!)1
(!)
(
!)1
(
!)
1
(!
!)1
(
−
−
−
+
−
−
−
r
r
n
n
r
n
r
n
 
 
 
 
 
  = 






=




+
−
−
r
n
n
r
n
r
n
r
n
r
n
!)
(!
!
 
 
8. 
There are 






+
r
m
n
 gropus of size r.  As there are 






−






i
r
m
i
n
 groups of size r that consist of i 
men and r − i women, we see that  
 
 
 
∑
=






−






=






+
r
i
i
r
m
i
n
r
m
n
0
. 
 
9. 






−






=






∑
=
i
n
n
i
n
n
n
n
i 0
2
 = 
2
0∑
=






n
i
i
n
 
 
10. 
Parts (a), (b), (c), and (d) are immediate.  For part (e), we have the following: 
 
 
  
            






k
n
k
  = 
!)1
(!)
(
!
!
!)
(
!
!
−
−
=
−
k
k
n
n
k
k
n
n
k
 
 
 






−
+
−
1
)1
(
k
n
k
n
 = 
!)1
(!)
(
!
!)1
(!)1
(
!
)1
(
−
−
=
−
+
−
+
−
k
k
n
n
k
k
n
n
k
n
 
 
 
       






−
−
1
1
k
n
n
 = 
!)1
(!)
(
!
!)1
(!)
(
!)1
(
−
−
=
−
−
−
k
k
n
n
k
k
n
n
n
 
 
 

6 
 
Chapter 1 
11. 
The number of subsets of size k that have i as their highest numbered member is equal to 






−
−
1
1
k
i
, the number of ways of choosing k − 1 of the numbers 1, …, i − 1.  Summing over i 
yields the number of subsets of size k. 
 
12. 
Number of possible selections of a committee of size k and a chairperson is 






k
n
k
 and so 
∑
=






n
k
k
n
k
1
 represents the desired number.  On the other hand, the chairperson can be anyone of 
the n persons and then each of the other n − 1 can either be on or off the committee.  Hence, 
n2n − 1 also represents the desired quantity. 
 
 
(i) 
2
k
k
n





 
 
(ii) n2n − 1 since there are n possible choices for the combined chairperson and secretary and 
then each of the other n − 1 can either be on or off the committee. 
 
(iii) n(n − 1)2n − 2 
 
 
(c) From a set of n we want to choose a committee, its chairperson its secretary and its 
treasurer (possibly the same).  The result follows since 
 
 
 
(a) there are n2n − 1 selections in which the chair, secretary and treasurer are the same 
person. 
 
 
(b) there are 3n(n − 1)2n − 2 selection in which the chair, secretary and treasurer jobs are 
held by 2 people. 
 
 
(c) there are n(n − 1)(n − 2)2n − 3 selections in which the chair, secretary and treasurer are 
all different. 
 
 
(d) there are 
3
k
k
n





selections in which the committee is of size k. 
 
13. 
(1 − 1)n = ∑
=
−
−






n
i
n
i
n
0
1
)1
(
 
 
14. 
(a) 






−
−






=











i
j
i
n
i
n
i
j
j
n
 
 
 
(b) From (a), 











∑
=
i
j
j
n
n
i
j
 = 
i
n
n
i
j
i
n
j
i
n
i
n
−
=






=






−
−





∑
2
1
 
 
 
(c) 
j
n
n
i
j
i
j
j
n
−
=
−











∑
)1
(
= 
j
n
n
i
j
j
i
n
i
n
−
=
−






−
−





∑
)1
(
1
 
 
 
 
 
          = 
k
i
n
i
n
k
k
i
n
i
n
−
−
−
=
−






−





∑
)1
(
0
 = 0 
 

Chapter 1 
 
7 
15. 
(a) The number of vectors that have xk = j is equal to the number of vectors x1 ≤ x2 ≤ … ≤ xk−1 
satisfying 1 ≤ xi ≤ j.  That is, the number of vectors is equal to Hk−1(j), and the result follows. 
 
 
(b)  
 
 
 
H2(1) = H1(1) = 1 
 
 
 
H2(2) = H1(1) + H1(2) = 3 
 
 
 
H2(3) = H1(1) + H1(2) + H1(3) = 6 
 
 
 
H2(4) = H1(1) + H1(2) + H1(3) + H1(4) = 10 
 
 
 
H2(5) = H1(1) + H1(2) + H1(3) + H1(4) + H1(5) = 15 
 
 
 
H3(5) = H2(1) + H2(2) + H2(3) + H2(4) + H2(5) = 35 
 
16. 
(a) 1 < 2 < 3, 1 < 3 < 2, 2 < 1 < 3, 2 < 3 < 1, 3 < 1 < 2, 3 < 2 < 1,  
 
 
1 = 2 < 3, 1 = 3  < 2, 2 = 3 < 1, 1 < 2 = 3, 2 < 1 = 3, 3 < 1 = 2, 1 = 2 = 3 
 
(b) The number of outcomes in which i players tie for last place is equal to 






i
n , the number 
of ways to choose these i players, multiplied by the number of outcomes of the remaining 
n − i players, which is clearly equal to N(n − i). 
 
 
(c) ∑
=
−






n
i
n
N
i
n
1
)1
(
 = ∑
=
−






−
n
i
i
n
N
i
n
n
1
)
(
 
 
 
 
 
            = ∑
−
=






1
0
)
(
n
j
j
N
j
n
 
 
 
 
where the final equality followed by letting j = n − i. 
 
 
(d) N(3) = 1 + 3N(1) + 3N(2) = 1 + 3 + 9 = 13 
 
 
N(4) = 1 + 4N(1) + 6N(2) + 4N(3) = 75 
 
17. 
A choice of r elements from a set of n elements is equivalent to breaking these elements into 
two subsets, one of size r (equal to the elements selected) and the other of size n − r (equal to 
the elements not selected). 
 
18. 
Suppose that r labelled subsets of respective sizes n1, n2, …, nr are to be made up from 
elements 1, 2, …, n where n = ∑
=
r
i
in
1
.  As 






−
−
r
i
n
n
n
n
,...
1
1
,...,
1
 represents the number of 
possibilities when person n is put in subset i, the result follows. 
 

8 
 
Chapter 1 
19. 
By induction: 
 
 
 
(x1 + x2 + … + xr)n 
 
 
 
= 
1
1
1
)
...
(
2
1
0
1
i
n
r
i
n
i
x
x
x
i
n
−
=
+
+






∑
 by the Binomial theorem 
 
 
= 
1
1
1
0
1
i
n
i
x
i
n
∑
=






 
ri
i ,...,
2
...∑
∑
2
2...
,...,
1
2
1
i
r
i
r
x
x
i
i
i
n






−
 
 
 
 
             
1
2 ...
i
n
i
i
r
−
=
+
+
 
 
 
 
= 
ri
i ,...,
1
...∑
∑∑
  
ri
r
i
r
x
x
i
i
n
...
,...,
1
1
1






 
 
            
n
i
i
i
r =
+
+
+
...
2
1
 
 
 
where the second equality follows from the induction hypothesis and the last from the 
identity 






=






−






r
n
i
i
n
i
i
i
n
i
n
,...,
,...,
1
2
1
1
. 
 
20. 
The number of integer solutions of  
 
 
 
x1 + … + xr = n, xi ≥ mi 
 
 
is the same as the number of nonnegative solutions of 
 
 
 
y1 + … + yr = n − ∑
r
i
m
1
, yi ≥ 0. 
 
 
Proposition 6.2 gives the result 










−
−
+
−∑
1
1
1
r
r
m
n
r
i
. 
 
21. 
There are 






k
r  choices of the k of the x’s to equal 0.  Given this choice the other r − k of the 
x’s must be positive and sum to n.   
 
By Proposition 6.1, there are 






+
−
−
=






−
−
−
k
r
n
n
k
r
n
1
1
1
 such solutions. 
 
Hence the result follows. 
 
22. 






−
−
+
1
1
n
r
n
 by Proposition 6.2. 
 

Chapter 1 
 
9 
23. 
There are 






−
+
j
n
j
1  nonnegative integer solutions of 
 
 
 
∑
=
=
n
i
i
j
x
1
 
 
 
Hence, there are 






−
+
∑
=
j
n
j
k
j
1
0
 such vectors. 
 
 
 

10 
 
Chapter 2 
Chapter 2  
 
Problems 
 
1. 
(a) S = {(r, r), (r, g), (r, b), (g, r), (g, g), (g, b), (b, r), b, g), (b, b)} 
 
(b) S = {(r, g), (r, b), (g, r), (g, b), (b, r), (b, g)} 
 
2. 
S = {(n, x1, …, xn−1), n ≥ 1, xi ≠ 6, i = 1, …, n − 1}, with the interpretation that the outcome is 
(n, x1, …, xn−1) if the first 6 appears on roll n, and xi appears on roll, i, i = 1, …, n − 1.  The 
event 
c
n
n E )
(
1
∞
=
∪
 is the event that 6 never appears. 
 
3. 
EF = {(1, 2), (1, 4), (1, 6), (2, 1), (4, 1), (6, 1)}. 
 
E ∪ F occurs if the sum is odd or if at least one of the dice lands on 1.  FG = {(1, 4), (4, 1)}.  
EFc is the event that neither of the dice lands on 1 and the sum is odd.  EFG = FG. 
 
4. 
A = {1,0001,0000001, …}  B = {01, 00001, 00000001, …} 
 
(A ∪ B)c = {00000 …, 001, 000001, …} 
 
5. 
(a) 25 = 32 
 
 
(b) 
 
W = {(1, 1, 1, 1, 1), (1, 1, 1, 1, 0), (1, 1, 1, 0, 1), (1, 1, 0, 1, 1), (1, 1, 1, 0, 0), (1, 1, 0, 1, 0) 
 
 
(1, 1, 0, 0, 1), (1, 1, 0, 0, 0), (1, 0, 1, 1, 1), (0, 1, 1, 1, 1), (1, 0, 1, 1, 0), (0, 1, 1, 1, 0), (0, 0, 1, 1, 1)  
 
 
(0, 0, 1, 1, 0), (1, 0, 1, 0, 1)} 
 
 
(c) 8 
 
(d) AW = {(1, 1, 1, 0, 0), (1, 1, 0, 0, 0)} 
 
6. 
(a) S = {(1, g), (0, g), (1, f), (0, f), (1, s), (0, s)} 
 
(b) A = {(1, s), (0, s)} 
 
(c) B = {(0, g), (0, f), (0, s)} 
 
(d) {(1, s), (0, s), (1, g), (1, f)} 
 
7. 
(a) 615  
 
 
(b) 615 − 315 
 
 
 
(c) 415 
 
 
8. 
(a) .8 
 
(b) .3 
 
(c) 0 
 
9. 
Choose a customer at random.  Let A denote the event that this customer carries an American 
Express card and V the event that he or she carries a VISA card.  
 
 
 
P(A ∪ V) = P(A) + P(V) − P(AV) = .24 + .61 − .11 = .74. 
 
 
Therefore, 74 percent of the establishment’s customers carry at least one of the two types of 
credit cards that it accepts. 
 

Chapter 2 
 
11 
10. 
Let R and N denote the events, respectively, that the student wears a ring and wears a 
necklace. 
 
 
 
(a) P(R ∪ N) = 1 − .6 = .4 
 
 
(b) .4 = P(R ∪ N) = P(R) + P(N) − P(RN) = .2 + .3 − P(RN)  
 
 
Thus, P(RN) = .1 
 
11. 
Let A be the event that a randomly chosen person is a cigarette smoker and let B be the event 
that she or he is a cigar smoker. 
 
 
(a) 1 − P(A ∪ B) = 1 − (.07 + .28 − .05) = .7.  Hence, 70 percent smoke neither.   
 
 
(b) P(AcB) = P(B) − P(AB) = .07 − .05 = .02.  Hence, 2 percent smoke cigars but not 
cigarettes. 
 
12. 
(a) P(S ∪ F ∪ G) = (28 + 26 + 16 − 12 − 4 − 6 + 2)/100 = 1/2 
 
 
The desired probability is 1 − 1/2 = 1/2. 
 
 
(b) Use the Venn diagram below to obtain the answer 32/100. 
 
 
 
14       10          10
S              F
8
G
2            4
2
 
 
 
(c) since 50 students are not taking any of the courses, the probability that neither one is 
taking a course is 












2
100
2
50
 = 49/198 and so the probability that at least one is taking a 
course is 149/198. 
 
13. 
 
 
 
 
 
 
 
(a) 
20,000 
 
 
 
 
 
 
 
 
(b) 
12,000 
 
 
 
 
 
 
 
 
(c) 
11,000  
 
 
 
 
 
 
 
 
(d) 
68,000 
 
 
 
 
 
 
 
 
(e) 
10,000 
 
 
 
1000      7000     19000
I              II
0
III
1000        3000
1000

12 
 
Chapter 2 
14. 
P(M) + P(W) + P(G) − P(MW) − P(MG) − P(WG) + P(MWG) = .312 + .470 + .525 − .086 − 
.042 − .147 + .025 = 1.057 
 
15. 
(a) 












5
52
5
13
4
 
 
(b) 




































5
52
1
4
1
4
1
4
3
12
2
4
13
 
 
(c) 






























5
52
1
44
2
4
2
4
2
13
 
 
(d) 






























5
52
1
4
1
4
2
12
3
4
13
 
 
(e) 


















5
52
1
48
4
4
13
 
 
16. 
(a) 
5
6
2
3
4
5
6
⋅
⋅
⋅
⋅
 
 
(b) 
5
6
3
4
5
2
5
6
⋅
⋅






 
 
(c) 
5
6
2
3
2
5
4
2
6


















 
 
 
(d) 
21
3
5
4
5
6






⋅
⋅
  
 
(e) 
5
6
3
5
5
6






⋅
 
 
(f) 
5
6
4
5
5
6






⋅
 
 
 
(g) 
5
6
6  
 
17. 
58
63
64
8
1
2
⋅⋅⋅
⋅
∏=
i i
 
 
18. 
51
52
16
4
2
⋅
⋅
⋅
 
 
19. 
4/36 + 4/36 +1/36 + 1/36 = 5/18 
 
20. 
Let A be the event that you are dealt blackjack and let B be the event that the dealer is dealt 
blackjack.  Then, 
 
 
 
P(A ∪ B) = P(A) + P(B) − P(AB) 
 
 
 
          = 
49
50
51
52
15
3
16
4
4
51
52
16
4
4
⋅
⋅
⋅
⋅
⋅
⋅
⋅
+
⋅
⋅
⋅
 
 
 
 
          = .0983 
 
 
where the preceding used that P(A) = P(B) = 2 × 
51
52
16
4
⋅
⋅
.  Hence, the probability that neither 
is dealt blackjack is .9017. 
 

Chapter 2 
 
13 
21. 
(a)  
p1 = 4/20, p2 = 8/20, p3 = 5/20, p4 = 2/20, p5 = 1/20 
 
 
 
 
(b) There are a total of 4 ⋅ 1 + 8 ⋅ 2 + 5 ⋅ 3 + 2 ⋅ 4 + 1 ⋅ 5 = 48 children. Hence,  
 
 
 
 
q1 = 4/48, q2 = 16/48, q3 = 15/48, q4 = 8/48, q5 = 5/48 
 
22. 
The ordering will be unchanged if for some k, 0 ≤ k ≤ n, the first k coin tosses land heads and 
the last n − k land tails.  Hence, the desired probability is (n + 1/2n 
 
23. 
The answer is 5/12, which can be seen as follows: 
 
 
 
1 = P{first higher} + P{second higher} + p{same} 
 
 
   = 2P{second higher} + p{same} 
 
 
   = 2P{second higher} + 1/6 
 
 
Another way of solving is to list all the outcomes for which the second is higher.  There is 1 
outcome when the second die lands on two, 2 when it lands on three, 3 when it lands on four, 
4 when it lands on five, and 5 when it lands on six.  Hence, the probability is  
 
(1 + 2 + 3 + 4 + 5)/36 = 5/12. 
 
25. 
P(En) = 
5
2
)
(
,
36
6
36
26
1
1
=






∑
∞
=
−
n
n
n
E
P
 
 
27. 
Imagine that all 10 balls are withdrawn 
 
 
P(A) = 
!
10
!
3
3
2
3
4
5
6
7
!
5
3
4
5
6
7
!
7
3
6
7
!
9
3
⋅
⋅
⋅
⋅
⋅
⋅
⋅
+
⋅
⋅
⋅
⋅
⋅
+
⋅
⋅
⋅
+
⋅
 
 
28. 
P{same} = 












+






+






3
19
3
8
3
6
3
5
 
 
P{different} = 






















3
19
1
8
1
6
1
5
 
 
 
If sampling is with replacement 
 
 
P{same} = 
3
3
3
3
)
19
(
8
6
5
+
+
 
 
 
P{different} = P(RBG) + P{BRG) + P(RGB) + … + P(GBR) 
 
 
 
         = 
3)
19
(
8
6
5
6
⋅
⋅
⋅
 
 
 
 

14 
 
Chapter 2 
29. 
(a) 
)1
)(
(
)1
(
)1
(
−
+
+
−
+
−
m
n
m
n
m
m
n
n
  
 
 
 
 
(b) Putting all terms over the common denominator (n + m)2(n + m − 1) shows that we must 
prove that 
 
 
 
n2(n + m − 1) + m2(n + m − 1) ≥ n(n − 1)(n + m) + m(m − 1)(n + m) 
 
 
 
which is immediate upon multiplying through and simplifying. 
 
30. 
(a) 
!
4
4
9
4
8
!
3
3
8
3
7






















 = 1/18 
 
 
(b) 






















4
9
4
8
3
8
3
7
 − 1/18 = 1/6 
 
 
(c) 






















+











4
9
4
8
3
8
4
7
4
8
3
7
 = 1/2 
 
31. 
P({complete} = 
9
2
3
3
3
1
2
3
=
⋅
⋅
⋅
⋅
 
 
P{same} = 
9
1
27
3 =
 
 
32. 
g
b
g
g
b
g
b
g
+
=
+
−
+
!)
(
!)1
(
 
 
33. 
323
70
4
20
2
15
2
5
=

















 
 
34. 












13
52
13
32
  
 
35. 
1 − 












3
54
3
30
 ≈ .8363 
 

Chapter 2 
 
15 
36. 
(a) 












2
52
2
4
 ≈ .0045, 
 
 
(b) 












2
52
2
4
13
 = 1/17 ≈ .0588 
 
37. 
(a) 












5
10
5
7
 = 1/12 ≈ .0833 
 
 
(b) 

















5
10
1
3
4
7
 + 1/12 = 1/2 
 
38. 
1/2 = 












2
2
3
n  or n(n − 1) = 12 or n = 4. 
 
39. 
25
12
5
5
5
3
4
5
=
⋅
⋅
⋅
⋅
 
 
40. 
P{1} = 
64
1
44
4 =
 
 
 
P{2} = 
256
84
4
4
2
4
4
2
4
4 =




+






+






 
 
P{3} = 
64
36
4
!
2
!
4
1
3
3
4
4 =











 
 
P{4} = 
64
6
4
!
4
4 =
 
 
41. 
1 − 
4
4
6
5  
 
42. 
1 − 
n






36
35
 
 
43. 
n
n
n
n
2
!
)
2
)(
1
(
2
=
−
−
 in a line 
 
1
2
!
!)
2
(
2
−
=
−
n
n
n
n
 if in a circle, n ≥ 2 
 
44. 
(a) If A is first, then A can be in any one of 3 places and B’s place is determined, and the 
others can be arranged in any of 3! ways.  As a similar result is true, when B is first, we 
see that the probability in this case is 2 ⋅ 3 ⋅ 3!/5! = 3/10 
 
 
(b) 2 ⋅ 2 ⋅ 3!/5! = 1/5 
 
 
(c) 2 ⋅ 3!/5! = 1/10 

16 
 
Chapter 2 
45. 
1/n if discard, 
k
k
n
n
1
)1
(
−
−
 if do not discard 
 
46. 
If n in the room, 
 
 
 
P{all different} = 
12
12
12
)
13
(
11
12
⋅
⋅
⋅
−
⋅
⋅
⋅
n  
 
 
When n = 5 this falls below 1/2.  (Its value when n = 5 is .3819) 
 
47. 
12!/(12)12 
 
48. 
20
4
4
)
12
(
)!
2
(
)!
3
(
!)
20
(
4
8
4
12











 
 
49. 

















6
12
3
6
3
6
 
 
50. 
































13
39
13
52
5
31
8
8
8
39
5
13
 
 
51. 
n
m
n
N
n
m
n
/
)1
(
−
−






 
 
52. 
(a) 
13
14
15
16
17
18
19
20
6
8
10
12
14
16
18
20
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
 
 
 
(b) 
13
14
15
16
17
18
19
20
2
!
2
!
8
6
9
1
10
6
⋅
⋅
⋅
⋅
⋅
⋅
⋅











 
 
53. 
Let Ai be the event that couple i sit next to each other.  Then 
 
 
 
!
8
!
4
2
!
8
!
5
2
4
!
8
!
6
2
6
!
8
!
7
2
4
)
(
4
3
2
4
1
⋅
−
⋅
+
⋅
−
⋅
=
∪=
i
i
A
P
 
 
 
and the desired probability is 1 minus the preceding. 
 

Chapter 2 
 
17 
54. 
P(S ∪ H ∪ D ∪ C) = P(S) + P(H) + P(D) + P(C) − P(SH) − … − P(SHDC) 
 
 
 
 
= 












+












−












13
52
13
13
4
13
52
13
26
6
13
52
13
39
4
 
 
 
 
 
= 






+






−






13
52
4
13
26
6
13
39
4
 
 
55. 
(a) P(S ∪ H ∪ D ∪ C) = P(S) + … − P(SHDC) 
 
 
 
= 


















−


















+






















−












13
52
5
44
2
2
13
52
7
46
2
2
4
13
52
9
48
2
2
2
2
6
13
52
2
2
4
4
3
 
 
 
 
 
= 












−






+






−






13
52
5
44
7
46
4
9
48
6
11
50
4
 
 
 
(b) P(1 ∪ 2 ∪ … ∪ 13) = 

















+

















−












13
52
1
40
3
13
13
52
5
44
2
13
13
52
9
48
13
 
 
56. 
Player B.  If Player A chooses spinner (a) then B can choose spinner (c).  If  A chooses (b) 
then B chooses (a).  If A chooses (c) then B chooses (b).  In each case B wins probability 5/9. 
 

18 
 
Chapter 2 
Theoretical Exercises 
 
5. 
Fi = 
c
j
i
j
i
E
E
1
1
=
=∩
 
 
6. 
(a) EFcGc 
 
 
(b) EFcG 
 
 
(c) E ∪ F ∪ G 
 
 
(d) EF ∪ EG ∪ FG 
 
 
(e) EFG 
 
 
(f) EcFcGc 
 
 
(g) EcFcGc ∪ EFcGc ∪ EcFGc ∪ EcFcG 
 
 
(h) (EFG)c 
 
 
(i) EFGc ∪ EFcG ∪ EcFG 
 
 
(j) S 
 
7. 
(a) E 
 
 
(b) EF 
 
 
 
(c) EG ∪ F 
 
8. 
The number of partitions that has n + 1 and a fixed set of i of the elements 1, 2, …, n as a 
subset is Tn−i.  Hence, (where T0 = 1).  Hence, as there are 






i
n  such subsets. 
 
 
Tn+1 = 
∑
∑
∑
=
−
=
−
=
−






+
=






+
=






n
k
k
n
i
i
n
n
i
i
n
T
k
n
T
i
n
T
i
n
1
1
0
0
1
1
. 
 
11. 
1 ≥ P(E ∪ F) = P(E) + P(F) − P(EF) 
 
 
12. 
P(EFc ∪ EcF) = P(EFc) + P(EcF) 
 
 
 
          = P(E) − P(EF) + P(F) − P(EF) 
 
13. 
E = EF ∪ EFc 
 

Chapter 2 
 
19 
15. 






+






−






r
N
M
k
r
N
k
M
 
 
16. 
P(E1 … En) ≥ P(E1 … En−1) + P(En) − 1  by Bonferonni’s Ineq. 
 
 
 
 
       ≥ ∑
−1
1
)
(
n
iE
P
 − (n − 2) + P(En) − 1 by induction hypothesis 
 
19. 
)1
(
1
)1
(
1
+
−
+






−
+
+
−






−






−
k
m
n
k
m
n
r
n
r
k
m
r
n
 
 
21. 
Let y1, y2, …, yk denote the successive runs of losses and x1, …, xk the successive runs of wins.  
There will be 2k runs if the outcome is either of the form y1, x1, …, yk xk or x1y1, … xk, yk where 
all xi, yi are positive, with x1 + … + xk = n, y1 + … + yk = m.  By Proposition 6.1 there are 






−
−






−
−
1
1
1
1
2
k
m
k
n
 number of outcomes and so  
 
 
 
 
P{2k runs} = 






+






−
−






−
−
n
n
m
k
m
k
n
1
1
1
1
2
. 
 
 
There will be 2k + 1 runs if the outcome is either of the form x1, y1, …, xk, yk, xk+1 or y1, x1, …, 
yk, xk yk + 1 where all are positive and ∑
ix  = n, ∑
iy  = m.  By Proposition 6.1 there are 






−
−






−
1
1
1
k
m
k
n
 outcomes of the first type and 






−






−
−
k
m
k
n
1
1
1
 of the second. 
 
 

20 
 
Chapter 3 
Chapter 3  
 
Problems 
 
1. 
P{6  different} = P{6, different}/P{different} 
 
 
 
 
  = 
6
/
5
}
6
nd
2,6
s1
{
}
6
nd
2,6
st
1
{
=
≠
+
≠
=
t
P
P
 
 
 
 
 
  = 
6.5
6
/
5
6
/
1
2
 = 1/3 
 
 
could also have been solved by using reduced sample space—for given that outcomes differ it 
is the same as asking for the probability that 6 is chosen when 2 of the numbers 1, 2, 3, 4, 5, 6 
are randomly chosen. 
 
2. 
P{6  sum of 7} = 
6
/
1
)}
1
,6
{(
P
 = 1/6 
 
 
P{6  sum of 8} = 
36
/
5
)}
2
,6
{(
P
 = 1/5 
 
 
P{6  sum of 9} = 
36
/
4
)}
3
,6
{(
P
 = 1/4 
 
 
P{6  sum of 10} = 
36
/
3
)}
4
,6
{(
P
 = 1/3 
 
 
P{6  sum of 11} = 
36
/
2
)}
5
,6
{(
P
 = 1/2 
 
 
P{6  sum of 12} = 1. 
 
3. 
P{E has 3  N − S has 8} = 
8}
 
has
 
{
8}
 
has
 
 
3,
 
has
 
{
S
N
P
S
N
E
P
−
−
 
 
 
 
 
 
 
  = 

















































26
52
18
39
8
13
13
26
26
52
10
21
3
5
18
39
8
13
 = .339 
 
4. 
P{at least one 6  sum of 12} = 1.  Otherwise twice the probability given in Problem 2. 
 
5. 
12
8
13
9
14
5
15
6
 
 
6. 
In both cases the one black ball is equally likely to be in either of the 4 positions.  Hence the 
answer is 1/2. 
 
7. 
P1 g and 1 b  at least one b} = 
4
/
3
2
/
1
 = 2/3 
 

Chapter 3 
 
21 
8. 
1/2 
 
9. 
P{A = w  2w}  = 
}
2
{
}
2
,
{
w
P
w
w
A
P
=
 
 
 
 
 
= 
}
2
{
}
,
,
{
}
,
,
{
w
P
w
C
w
B
w
A
P
w
C
w
B
w
A
P
=
≠
=
+
≠
=
=
 
 
 
 
 
 
= 
11
7
4
1
3
2
3
2
4
1
3
1
3
1
4
3
3
2
2
1
4
1
3
1
3
1
4
3
3
2
3
1
=
+
+
+
 
 
10. 
11/50 
 
11. 
(a) P(BAs) = 
1 3
3 1
(
)
1
52 21
52 51
2
(
)
17
52
s
s
P BA
P A
+
=
=
 
 
 
Which could have been seen by noting that, given the ace of spades is chosen, the other 
card is equally likely to be any of the remaining 51 cards, of which 3 are aces. 
 
 
(b) P(BA) = 
4 3
( )
1
52 51
48 47
( )
33
1
52 51
P B
P A =
=
−
 
 
12. 
(a) (.9)(.8)(.7) = .504 
 
 
(b) Let Fi denote the event that she failed the ith exam. 
 
 
 
 
496
.
)
2
)(.
9
(.
504
.
1
)
(
)
)
(
2
1
3
2
1
2
=
−
=
F
F
P
F
F
F
F
P
c
c
c
c
c
 = .3629 
 
13. 
P(E1) = 

















13
52
12
48
1
4
,  
P(E2E1) = 

















13
39
12
36
1
3
 
 
P(E3E1E2) = 

















13
26
12
24
1
2
,  
P(E4E1E2E3) = 1. 
 
 
Hence,  
 
 
 
p = 
















⋅















⋅
















13
26
12
24
1
2
13
39
12
36
1
3
13
52
12
48
1
4
 
 
14. 
768
35
18
9
16
7
14
7
12
5
−
. 
 
 
 

22 
 
Chapter 3 
15. 
Let E be the event that a randomly chosen pregnant women has an ectopic pregnancy and S 
the event that the chosen person is a smoker.  Then the problem states that 
 
 
 
 
P(ES) = 2P(ESc), P(S) = .32 
 
 
Hence, 
 
 
 
 
P(SE) = P(SE)/P(E) 
 
 
 
 
= 
)
(
)
(
)
(
)
(
)
(
)
(
C
c
S
P
S
E
P
S
P
S
E
P
S
P
S
E
P
+
 
 
 
 
 
= 
)
(
)
(
2
)
(
2
c
S
P
S
P
S
P
+
 
 
 
 
 
= 32/66 ≈ .4548 
 
16. 
With S being survival and C being C section of a randomly chosen delivery, we have that 
 
 
 
 
.98 = P(S) = P(SC).15 + P(SC2) .85 
 
 
 
 
           = .96(.15) + P(SC2) .85 
 
 
Hence  
 
 
 
 
P(SCc) ≈ .9835. 
 
17. 
P(D) = .36, P(C) = .30, P(CD) = .22 
 
 
(a) P(DC) = P(D) P(CD) = .0792 
 
(b) P(DC) = P(DC)/P(C) = .0792/.3 = .264 
 
18. 
(a) P(Indvoted) = ∑
type)
(
)
ype
voted
(
)
Ind
(
)
Ind
voted
(
P
t
P
P
P
 
 
 
 
 
    = 
)
24
(.
58
.
)
3
(.
62
.
)
46
(.
35
.
)
46
(.
35
.
+
+
 ≈ 331 
 
 
(b) P{Libvoted} = 
)
24
(.
58
.
)
3
(.
62
.
)
46
(.
35
.
)
30
(.
62
.
+
+
 ≈ .383 
 
 
(c) P{Convoted} = 
)
24
(.
58
.
)
3
(.
62
.
)
46
(.
35
.
)
24
(.
58
.
+
+
 ≈ .286 
 
 
(d) P{voted} = .35(.46) + .62(.3) + .58(.24) = .4862 
 
 
That is, 48.62 percent of the voters voted. 
 

Chapter 3 
 
23 
19. 
Choose a random member of the class.  Let A be the event that this person attends the party 
and let W be the event that this person is a woman. 
 
 
(a) P(WA) = 
)
(
)
(
)
(
)
(
)
(
)
(
M
P
M
A
P
W
P
W
A
P
W
P
W
A
P
+
 where M = Wc 
 
 
 
 
        = 
)
62
(.
37
.
)
38
(.
48
.
)
38
(.
48
.
+
 ≈ .443 
 
 
 
Therefore, 44.3 percent of the attendees were women. 
 
 
(b) P(A) = .48(.38) + .37(.62) = .4118 
 
 
 
Therefore, 41.18 percent of the class attended. 
 
20. 
(a) P(FC) = 
)
(
)
(
C
P
FC
P
 = .02/.05 = .40 
 
 
(b) P(CF) = P(FC)/P(F) = .02/.52 = 1/26 ≈ .038 
 
21. 
(a) P{husband under 25} = (212 + 36)/500 = .496 
 
 
(b) P{wife overhusband over} = P{both over}/P{husband over} 
 
 
 
 
 
 
 
 = 
)
500
/
252
(
)
500
/
54
(
 
 
 
 
 
 
 
 = 3/14 ≈ .214 
 
 
(c) P{wife overhusband under} = 36/248 ≈ .145 
 
22. 
a. 
6 5 4
5
6 6 6
9
⋅⋅
=
⋅⋅
 
 
b. 
1
1
3!
6
=
 
 
c. 
5 1
5
9 6
54
=
 
 
23. 
P(ww transferred}P{w tr.} + P(wR  tr.}P{R  tr.} = 
9
4
3
2
3
1
3
1
3
2
=
+
. 
 
 
P{w transferred w} = 
9
4
3
1
3
2
}
{
tr.}
{
tr.}
{
=
w
P
w
P
w
w
P
 = 1/2. 
 
 

24 
 
Chapter 3 
24. 
(a) P{g − gat least one g } = 
4
/
3
4
/
1
  = 1/3. 
 
 
(b) Since we have no information about the ball in the urn, the answer is 1/2. 
 
26. 
Let M be the event that the person is male, and let C be the event that he or she is color blind.  
Also, let p denote the proportion of the population that is male. 
 
 
 
P(MC) = 
)
1
)(
0025
(.
)
05
(.
)
05
(.
)
(
)
(
)
(
)
(
)
(
)
(
p
p
p
M
P
M
C
P
M
P
M
C
P
M
P
M
C
P
c
c
−
+
=
+
 
 
27. 
Method (b) is correct as it will enable one to estimate the average number of workers per car.  
Method (a) gives too much weight to cars carrying a lot of workers.  For instance, suppose 
there are 10 cars, 9 transporting a single worker and the other carrying 9 workers.  Then 9 of 
the 18 workers were in a car carrying 9 workers and so if you randomly choose a worker then 
with probability 1/2 the worker would have been in a car carrying 9 workers and with 
probability 1/2 the worker would have been in a car carrying 1 worker. 
 
28. 
Let A denote the event that the next card is the ace of spades and let B be the event that it is 
the two of clubs. 
 
 
(a) P{A} = P{next card is an ace}P{Anext card is an ace} 
 
 
 
    = 
128
3
4
1
32
3
=
 
 
 
(b) Let C be the event that the two of clubs appeared among the first 20 cards. 
 
 
 
 
P(B) = P(BC)P(C) + P(BCc)P(Cc) 
 
 
 
         = 
1536
29
48
29
32
1
48
19
0
=
+
 
 
29. 
Let A be the event that none of the final 3 balls were ever used and let Bi denote the event that 
i of the first 3 balls chosen had previously been used.  Then, 
 
 
 
P(A) = P(AB0)P(B0) + P(AB1)P(B1) + P(AB2)P(B2) + P(AB3)P(B3) 
 
 
 
  = 












−

















+
∑
=
3
15
3
9
6
3
15
3
6
3
0
i
i
i
i
 
 
 
 
  = .083 
 
30. 
Let B and W be the events that the marble is black and white, respectively, and let B be the 
event that box i is chosen.  Then, 
 
 
 
P(B) = P(BB1)P(B1) + P(BB2)P(B2) = (1/2)(1/2) = (2/3)(1/2) = 7/12 
 
 
 
P(B1W) = 
12
/
5
)
2
/
1
)(
2
/
1(
)
(
)
(
)
(
1
1
=
W
P
B
P
B
W
P
 = 3/5 

Chapter 3 
 
25 
31. 
Let C be the event that the tumor is cancerous, and let N be the event that the doctor does not 
call. Then 
 
 
 
β = P(CN) = 
(
)
(
)
P NC
P N
 
 
 
 
 
 = 
(
) ( )
(
) ( )
(
) (
)
c
c
P N C P C
P N C P C
P N C
P C
+
 
 
 
 
 
 = 
1 (1
)
2
α
α
α
+
−
 
 
 
 
 
 = 2
1
α
α
α ≥
+
 
 
 
with strict inequality unless α = 1. 
 
32. 
Let E be the event the child selected is the eldest, and let Fj be the event that the family has j 
children. Then,  
 
 
 
P(FjE) = 
(
)
( )
j
P EF
P E
 
 
 
 
        = 
(
) (
)
(
) (
)
j
j
j
j
j
P F P E F
P F P E F
∑
 
 
 
 
        = 
(1/ )
.1 .25(1/ 2)
.35(1/3)
.3(1/ 4)
jp
j
+
+
+
 = .24 
 
 
Thus, P(F1E) = .24, P(F4E) = .18. 
 
33. 
Let V be the event that the letter is a vowel.  Then 
 
 
 
P(EV) = 
)
5
/
3
)(
5
/
2
(
)
5
/
2
)(
2
/
1(
)
5
/
2
)(
2
/
1(
)
(
)
(
)
(
)
(
)
(
)
(
+
=
+
A
P
A
V
P
E
P
E
V
P
E
P
E
V
P
 = 5/11 
 
34. 
P(GC) = 
)
(
)
(
)
(
)
(
)
(
)
(
c
c
G
P
G
C
P
G
P
G
C
P
G
P
G
C
P
+
 = 54/62 
 
35. 
P{A = superior  A fair, B poor} 
 
= 
poor}
 
 
fair,
 
{
superior}
 
superior
 
poor
 
 
fair,
 
{
B
A
P
A
A
B
A
P
 
 
= 
4
3
2
1
30
5
30
10
2
1
30
15
30
10
2
1
30
15
30
10
=
+
. 
 

26 
 
Chapter 3 
36. 
P{Cwoman} = 
}
{
}
women
{
}
{
}
women
{
}
{
}
women
{
}
{
}
women
{
C
P
C
P
B
P
B
P
A
P
A
P
C
P
C
P
+
+
 
 
 
 
 
           = 
2
1
225
100
7.
225
75
6.
225
50
5.
225
100
7.
=
+
+
 
 
37. 
(a) P{fairh} = 
3
1
2
1
2
1
2
1
2
1
2
1
=
+
. 
 
 
(b) P{fairhh} = 
5
1
2
1
2
1
4
1
2
1
4
1
=
+
. 
 
 
(c) 1 
 
 
 
38. 
P{tailsw} = 
111
36
75
36
36
2
1
12
5
2
1
15
3
2
1
15
3
=
+
=
+
. 
 
39. 
P{acc.no acc.} = 
acc.}
 
{no
acc.
  
acc.,
 
no
{
P
P
 
 
 
 
 
 = 
185
46
)
8
(.
10
7
)
6
(.
10
3
)
8
)(.
2
(.
10
7
)
6
)(.
4
(.
10
3
=
+
+
. 
 
40. 
(a) 
14
9
13
8
12
7
 
 
 
(b) 
14
13
12
5
8
7
3
⋅
⋅
⋅
⋅
 
 
 
(c) 
14
13
12
7
6
5
⋅
⋅
⋅
⋅
 
 
 
(d) 
14
13
12
7
6
5
3
⋅
⋅
⋅
⋅
 
 

Chapter 3 
 
27 
41. 
P{ace} = P{aceinterchanged selected} 27
1  
 
 
 
 
+P{aceinterchanged not selected} 27
26  
 
 
 
= 
27
51
129
27
26
51
3
27
1
1
⋅
=
+
. 
 
42. 
P{Afailure} = 
29
10
)
2
)(.
05
(.
)3
)(.
03
(.
)
5
)(.
02
(.
)
5
)(.
02
(.
=
+
+
 
 
43. 
P{2 headedheads} = 
9
4
3
2
4
4
4
3
3
1
2
1
3
1
)1(
3
1
)1(
3
1
=
+
+
=
+
+
. 
 
45. 
P{5thheads} = ∑
i
th
i
P
i
h
P
P
P
}
{
}
{
}
{5
}
5
heads
{
th
th
th
 
 
 
 
           = 
11
1
10
1
10
10
1
10
5
10
1
=
∑
=
i
i
. 
 
46. 
Let M and F denote, respectively, the events that the policyholder is male and that the 
policyholder is female.  Conditioning on which is the case gives the following. 
 
 
 
P(A2A1) = 
)
(
)
(
1
2
1
A
P
A
A
P
 
 
 
 
          = 
)
1
)(
(
)
(
)
1
)(
(
)
(
1
1
2
1
2
1
α
α
α
α
−
+
−
+
F
A
P
M
A
P
F
A
A
P
M
A
A
P
 
 
 
 
          = 
)
1(
)
1(
2
2
α
α
α
α
−
+
−
+
f
m
f
m
p
p
p
p
 
 
 
Hence, we need to show that 
 
 
 
)
1[
2
2
α
α
−
+
f
m
p
p
 > (pmα + pf(1 − α))2 
 
 
or equivalently, that 
 
 
 
]
)
1(
1[
)
(
2
2
2
2
a
p
p
f
m
−
−
−
+
−
α
α
α
 > 2α(1 − α)pfpm 
 
 
 

28 
 
Chapter 3 
 
Factoring out α(1 − α) gives the equivalent condition 
 
 
 
 
m
f
m
pf
p
p
2
2
2
>
+
 
 
 
or 
 
 
 
 
 
 
(pm − pf)2 > 0 
 
 
which follows because pm ≠ pf.  Intuitively, the inequality follows because given the 
information that the policyholder had a claim in year 1 makes it more likely that it was a type 
policyholder having a larger claim probability.  That is, the policyholder is more likely to me 
male if pm > pf (or more likely to be female if the inequality is reversed) than without this 
information, thus raising the probability of a claim in the following year. 
 
47. 
P{all white} = 




+
+
+
+
11
1
12
2
13
3
14
4
15
5
12
2
13
3
14
4
15
5
13
3
14
4
15
5
14
4
15
5
15
5
6
1
 
 
 
P{3all white} = 
 white}
all
{
13
3
14
4
15
5
6
1
P
 
 
48. 
(a) P{silver in othersilver found} 
 
 
 
= 
found}
 
{
found}
 
 
other,
in 
 
{
S
P
S
S
P
. 
 
 
To compute these probabilities, condition on the cabinet selected. 
 
 
 
= 
}1/2
found
 
{
}1/2
found
 
{
2
/
1
B
S
P
A
S
P
+
 
 
       = 
3
2
2
/
1
1
1
=
+
. 
 
49. 
Let C be the event that the patient has cancer, and let E be the event that the test indicates an 
elevated PSA level. Then, with p = P(C), 
 
 
 
P(CE) = 
(
) ( )
(
) ( )
(
) (
)
c
c
P E C P C
P E C P C
P E C
P C
+
 
 
 
Similarly, 
 
 
 
P(CEc) = 
(
) ( )
(
) ( )
(
) (
)
c
c
c
c
c
P E C P C
P E C P C
P E C
P C
+
 
 
 
 
         = 
.732
.732
.865(1
)
p
p
p
+
−
 

Chapter 3 
 
29 
 
50. 
Choose a person at random 
 
 
P{they have accident} = P{acc. good}P{g} + P{acc.ave.}P{ave.} 
 
 
 
 
 
+ P{acc.bad P(b)} 
 
 
 
 
           = (.05)(.2) + (.15)(.5) + (.30)(.3) = .175 
 
 
P{A is good no accident} = 825
.
)
2
(
95
.
 
 
P{A is averageno accident} = 
825
.
)
5
)(.
85
(.
 
 
51. 
Let R be the event that she receives a job offer. 
 
 
(a) P(R) = P(Rstrong)P(strong) + P(Rmoderate)P(moderate) + P(Rweak)P(weak) 
 
 
 
  = (.8)(.7) + (.4)(.2) + (.1)(.1) = .65 
 
 
(b)  
P(strongR) = 
)
(
strong)
(
strong)
(
R
P
P
R
P
 
 
 
 
 
         = 
65
56
65
.
)
7
)(.
8
(.
=
 
 
 
 
Similarly, 
 
 
 
 
 
 
P(moderateR) = 65
8 , P(weakR) = 65
1  
 
 
(c)  
P(strongRc) = 
)
P(R
P
R
P
c
c
(strong)
strong)
(
 
 
 
 
 
          = 
35
14
35
.
)
7
)(.
2
(.
=
 
 
 
 
Similarly, 
 
 
 
 
P(moderateRc) = 35
12 , P(weakRc) = 35
9  
 
52. 
Let M, T, W, Th, F be the events that the mail is received on that day.  Also, let A be the event 
that she is accepted and R that she is rejected. 
 
 
(a) P(M) = P(MA)P(A) + P(MR)P(R) = (.15)(.6) + (.05)(.4) = .11 
 

30 
 
Chapter 3 
 
(b) P(TMc) = 
)
(
)
(
c
M
P
T
P
 
 
 
         = 
)
(
1
)
(
)
(
)
(
)
(
M
P
R
P
R
T
P
A
P
A
T
P
−
+
 
 
 
         = 
89
16
89
.
)
4
)(.
1
(.
)
6
)(.
2
(.
+
 
 
(c) P(AMcTcWc) = 
)
(
)
(
)
(
c
c
c
c
c
c
W
T
M
P
A
P
A
W
T
M
P
 
 
 
 
                 = 
27
12
)
4
)(.
75
(.
)
6
)(.
4
(.
)
6
)(.
25
.
20
.
15
.
1(
=
+
−
−
−
 
 
 
(d) P(ATh) = 
)
(
)
(
)
(
Th
P
A
P
A
Th
P
 
 
 
 
          = 
5
3
)
4
)(.
15
(.
)
6
)(.
15
(.
)
6
)(.
15
(.
=
+
 
 
 
(e) P(Ano mail) = 
mail)
 
no
(
)
(
)
mail
 
no
(
P
A
P
A
P
 
 
 
 
 
    = 
25
9
)
4
)(.
4
(.
)
6
)(.
15
(.
)
6
)(.
15
(.
=
+
 
 
53. 
Let W and F be the events that component 1 works and that the system functions. 
 
 
 
P(WF) = 
1
)
2
/
1(
1
2
/
1
)
(
1
)
(
)
(
)
(
−
−
=
−
=
n
c
F
P
W
P
F
P
WF
P
 
 
55. 
P{Boy, F} = 
x
+
16
4
 
P{Boy) = 
x
+
16
10
 
P{F} = 
x
+
16
10
 
 
 
so independence ⇒ 4 = 
x
+
⋅
16
10
10
 ⇒ 4x = 36 or x= 9. 
 
 
A direct check now shows that 9 sophomore girls (which the above shows is necessary) is 
also sufficient for independence of sex and class. 
 
56. 
P{new} = 
∑
∑
−
−
=
i
i
n
i
i
i
p
p
p
i
P
1
)
1(
}
  
 type
 
new
{
 
 

Chapter 3 
 
31 
57. 
(a) 2p(1 − p) 
 
 
 
(b) 
)
1(
2
3
2
p
p
−






 
 
 
(c) P{up on firstup 1 after 3} 
 
 
 
= P{up first, up 1 after 3}/[3p2(1 − p)] 
 
 
 
= p2p(1 − p)/[3p2(1 − p)] = 2/3. 
 
58. 
(a) All we know when the procedure ends is that the two most flips were either H, T, or T, H.  
Thus,  
 
 
 
 
P(heads) = P(H, TH, T or T, H) 
 
 
 
 
  = 
p
p
p
p
p
p
H
T
P
T
H
P
T
H
P
)
1(
)
1(
)
1(
)
,
(
)
,
(
)
,
(
−
+
−
−
=
+
 = 2
1  
 
 
(b) No, with this new procedure the result will be heads (tails) whenever the first flip is tails 
(heads).  Hence, it will be heads with probability 1 − p. 
 
59. 
(a) 1/16 
 
 
(b) 1/16 
 
 
(c) The only way in which the pattern H, H, H, H can occur first is for the first 4 flips to all 
be heads, for once a tail appears it follows that a tail will precede the first run of 4 heads 
(and so T, H, H, H will appear first).  Hence, the probability that T, H, H, H occurs first is 
15/16. 
 
60. 
From the information of the problem we can conclude that both of Smith’s parents have one 
blue and one brown eyed gene.  Note that at birth, Smith was equally likely to receive either a 
blue gene or a brown gene from each parent.  Let X denote the number of blue genes that 
Smith received. 
 
 
(a) P{Smith blue gene} = P{X = 1X ≤ 1} = 
4
/
1
1
2
/
1
−
 = 2/3 
 
 
(b) Condition on whether Smith has a blue-eyed gene. 
 
 
P{child blue} = P{blueblue gene}(2/3) + P{blueno blue}(1/3) 
 
 
 
 
    = (1/2)(2/3) = 1/3 
 
 
(c) First compute 
 
 
P{Smith bluechild brown} = 
3
/
2
blue}2/3
Smith 
brown
 
child
{
P
 
 
 
 
 
 
 
 = 1/2 
 
 
 
Now condition on whether Smith has a blue gene given that first child has brown eyes. 
 
 
 
P{second child brown} = P{brownSmith blue}1/2 + P{brownSmith no blue}1/2 
 
 
 
 
 
 
= 1/4 + 1/2 = 3/4 

32 
 
Chapter 3 
61. 
Because the non-albino child has an albino sibling we know that both its parents are carriers.  
Hence, the probability that the non-albino child is not a carrier is 
 
 
 
P(A, AA, a or a, A or A, A) = 3
1  
 
 
Where the first gene member in each gene pair is from the mother and the second from the 
father.  Hence, with probability 2/3 the non-albino child is a carrier. 
 
 
(a) Condition on whether the non-albino child is a carrier.  With C denoting this event, and 
Oi the event that the ith offspring is albino, we have: 
 
 
 
P(O1) = P(O1C)P(C) + P(O1Cc)P(Cc) 
 
 
 
    = (1/4)(2/3) + 0(1/3) = 1/6 
 
 
(b) 
)
(
1
2
c
O
O
P
 = 
)
(
)
(
1
2
1
c
c
O
P
O
O
P
 
 
 
 
           = 
6
/
5
)
(
)
(
)
(
)
(
2
1
2
1
c
c
c
c
C
P
C
O
O
P
C
P
C
O
O
P
+
 
 
 
 
           = 
20
3
6
/
5
)
3
/
1(
0
)
3
/
2
)(
4
/
1
)(
4
/
3
(
=
+
 
 
62. 
(a) P{both hitat least one hit}  = 
hit}
 
one
least 
{at 
hit}
both 
{
P
P
 
 
 
 
 
 
 
= p1p2/(1 − q1q2) 
 
 
(b) P{Barb hitat least one hit} = p1/(1 − q1q2) 
 
 
Qi = 1 − pi, and we have assumed that the outcomes of the shots are independent. 
 
63. 
Consider the final round of the duel.  Let qx = 1 − px 
 
 
(a) P{A not hit} = P{A not hitat least one is hit} 
 
 
 
 
  = P{A not hit, B hit}/P{at least one is hit} 
 
 
 
 
  = qBpA/(1 − qAqB) 
 
 
 
(b) P{both hit} = P{both hitat least one is hit} 
 
 
 
 
= P{both hit}/P{at least one hit} 
 
 
 
 
= pApB/(1 − qAqB) 
 
 
(c) (qAqB)n − 1(1 − qAqB) 
 
 
(d) P{n roundsA unhit} = P{n rounds, A unhit}/P{A unhit} 
 
 
 
 
 
   = 
)
1
/(
)
(
1
B
A
A
B
B
A
n
B
A
q
q
p
q
q
p
q
q
−
−
 
 
 
 
 
 
   = (qAqB)n − 1(1 − qAqB) 
 

Chapter 3 
 
33 
 
(e) P(n roundsboth hit} = P{n rounds both hit}/P{both hit} 
 
 
 
 
 
   = 
)
1
/(
)
(
1
B
A
A
B
B
A
n
B
A
q
q
p
p
p
p
q
q
−
−
 
 
 
 
 
 
   = (qAqB)n−1(1 − qAqB) 
 
 
 
Note that (c), (d), and (e) all have the same answer. 
 
64. 
If use (a) will win with probability p.  If use strategy (b) then 
 
 
 
P{win} = P{winboth correct}p2  + P{winexactly 1 correct}2p(1 − p) 
 
 
 
 
+ P{winneither correct}(1 − p)2 
 
 
 
       = p2 + p(1 − p) + 0 = p 
 
 
 
Thus, both strategies give the same probability of winning. 
 
65. 
(a) P{correctagree} = P{correct, agree}/P{agree} 
 
 
 
 
          = p2/[p2 + (1 − p)2] 
 
 
 
 
          = 36/52 = 9/13 
when p = .6 
 
 
(b) 1/2 
 
66. 
(a) [I − (1 − P1P2)(1 − P3P4)]P5 = (P1P2 + P3P4 − P1P2P3P4)P5 
 
 
(b) Let E1 = {1 and 4 close}, E2 = {1, 3, 5 all close} 
 
 
 
E3 = {2, 5 close}, E4 = {2, 3, 4 close}.  The desired probability is 
 
67. 
P(E1 ∪ E2 ∪ E3 ∪ E4) = P(E1) + P(E2) + P(E3) + P(E4) − P(E1E2) − P(E1E3) − P(E1E4)  
 
 
 
 
 
− P(E2E3) − P(E2E4) + P(E3E4) + P(E1E2E3) + P(E1E2E4) 
 
 
 
 
 
+ P(E1E3E4) + P(E2E3E4) − P(E1E2E3E4) 
 
 
 
 
 
          = P1P4 + P1P3P5 + P2P5 + P2P3P4 − P1P3P4P5 − P1P2P4P5 − P1P2P3P4  
 
 
 
 
 
− P1P2P3P5 − P2P3P4P5 − 2P1P2P3P4P5 + 3P1P2P3P4P5. 
 
 
(a) P1P2(1 − P3)(1 − P4) + P1(1 − P2)P3(1 − P4) + P1(1 − P2(1 − P3)P4  
 
 
+ P2P3(1 − P1)(1 − P4) + (1 − P1)P2(1 − P3)P4 + (1 − P1)(1 − P2)P3P4 
 
 
+ P1P2P3(1 − P4) + P1P2(1 − P3)P4 + P1(1 − P2)P3P4 + (1 − P1)P2P3P4  + P1P2P3P4. 
 
 
(c) ∑
=
−
−






n
k
i
i
n
i
p
p
i
n
)
1(
 
 

34 
 
Chapter 3 
68. 
Let Ci denote the event that relay i is closed, and let F be the event that current flows from  
 
A to B. 
 
 
 
P(C1C2F) = 
)
(
)
(
2
1
F
P
F
C
C
P
 
 
 
 
 
= 
)
(
)
(
)
(
4
3
2
1
4
3
2
1
5
2
1
2
1
p
p
p
p
p
p
p
p
p
C
C
P
C
C
F
P
−
+
 
 
 
 
 
= 
)
(
4
3
2
1
4
3
2
1
5
2
1
5
p
p
p
p
p
p
p
p
p
p
p
p
−
+
 
 
69. 
1. (a) 
128
9
2
1
4
3
2
1
4
3
2
1
=
 
2. (a) 
32
1
2
1
2
1
2
1
2
1
2
1
=
 
 
 
 
(b) 
128
9
2
1
4
3
2
1
4
3
2
1
=
 
 
(b) 
32
1
2
1
2
1
2
1
2
1
2
1
=
 
 
 
 
(c) 128
18  
 
(c) 16
1  
 
 
 
(d) 128
110  
 
(d) 16
15  
 
70. 
(a) P{carrier3 without} 
 
 
= 
2
/
1
1
2
/
1
8
/
1
2
/
1
8
/
1
+
 = 1/9. 
 
 
(b) 1/18 
 
71. 
P{Braves win} = P{BB wins 3 of 3} 1/8 + P{BB wins 2 of 3} 3/8 
 
 
 
 
+ P{BB wins 1 of 3} 3/8 + P{BB wins 0 of 3} 1/8 
 
 
 
 
=
64
38
2
1
4
3
8
3
4
3
2
1
4
1
8
3
8
1
=
+




+
+
 
 
 
where P{BB wins i of 3} is obtained by conditioning on the outcome of the other series.  
For instance 
 
 
P{BB win 2 of 3} = P{BD or G win 3 of 3, B win 2 of 3} 1/4 
 
 
 
 
      = P{BD or G win 2 of 3, B win 2 of 3} 3/4 
 
 
 
 
      = 
4
3
4
1
2
1
+
. 
 
 
By symmetry P{D win} = P{G win} and as the probabilities must sum to 1 we have. 
 
 
 
P{D win} = P{G win} = 64
13 . 

Chapter 3 
 
35 
72. 
Let f denote for and a against a certain place of legislature.  The situations in which a given 
steering committees vote is decisive are as follows: 
 
given member 
other members of S.C. 
other council members 
for 
both for 
3 or 4 against 
for 
one for, one against 
at least 2 for 
against 
one for, one against 
at least 2 for 
against 
both for 
3 of 4 against 
 
 
P{decisive} = p34p(1 − p)3 + p2p(1 − p)(6p2(1 − p)2 + 4p3(1 − p) + p4) 
 
 
 
 
+ (1 − p)2p(1 − p)(6p2(1 − p)2 + 4p3(1 − p) + p4) 
 
 
 
 
+ (1 − p)p24p(1 − p)3. 
 
73. 
(a) 1/16, 
(b) 1/32, 
(c) 10/32, 
(d) 1/4, 
(e) 31/32. 
 
74. 
Let PA be the probability that A wins when A rolls first, and let PB be the probability that B 
wins when B rolls first.  Using that the sum of the dice is 9 with probability 1/9, we obtain 
upon conditioning on whether A rolls a 9 that 
 
 
 
 
PA = 
)
1(
9
8
9
1
B
P
−
+
 
 
 
Similarly, 
 
 
 
 
PB = 
)
1(
36
31
36
5
A
P
−
+
 
 
 
Solving these equations gives that PA = 9/19 (and that PB = 45/76.) 
 
75. 
(a) The probability that a family has 2 sons is 1/4; the probability that a family has exactly 1 
son is 1/2.  Therefore, on average, every four families will have one family with 2 sons 
and two families with 1 son.  Therefore, three out of every four sons will be eldest sons. 
 
 
 
Another argument is to choose a child at random.  Letting E be the event that the 
child is an eldest son, letting S be the event that it is a son, and letting A be the event that 
the child’s family has at least one son, 
 
 
 
 
P(ES) = 
)
(
)
(
S
P
ES
P
 
 
 
 
 
= 2P(E) 
 
 
 
 
= 




+
4
1
)
(
4
3
)
(
2
c
A
E
P
A
E
P
 
 
 
 
 
= 




+
4
1
0
4
3
2
1
2
 = 3/4 
 

36 
 
Chapter 3 
 
(b) Using the preceding notation 
 
 
 
 
P(ES) = 
)
(
)
(
S
P
ES
P
 
 
 
 
 
= 2P(E) 
 
 
 
 
= 




+
8
1
)
(
8
7
)
(
2
c
A
E
P
A
E
P
 
 
 
 
 
= 




8
7
3
1
2
 = 7/12 
 
76. 
Condition on outcome of initial trial 
 
 
 
P(E before F) = P(E b FE)P(E) + P(E b FF)P(F) 
 
 
 
 
 
+ P(E b Fneither E or F)[1 − P(E) − P(F)] 
 
 
 
 
    = P(E) + P(E b F)(1 − P(E) − P(F)]. 
 
 
Hence, 
 
 
 
P(E b F) = 
)
(
)
(
)
(
F
P
E
P
E
P
+
. 
 
77. 
(a) This is equal to the conditional probability that the first trial results in outcome 1 (F1) 
given that it results in either 1 or 2, giving the result 1/2. More formally, with L3 being 
the event that outcome 3 is the last to occur 
 
 
 
 
 
P(F1L3) = 
3
1
1
3
(
) (
)
(1/ 2)(1/3)
1/ 2
(
)
1/3
P L F P F
P L
=
=
 
 
 
(b) With S1 being the event that the second trial results in outcome 1, we have 
 
 
 
 
P(F1S1L3) = 
3
1
1
1
1
3
(
) (
)
(1/ 2)(1/9)
1/6
(
)
1/3
P L F S
P F S
P L
=
=
 
 
78. 
(a) Because there will be 4 games if each player wins one of the first two games and then one 
of them wins the next two, P(4 games) = 2p(1 − p)[p2 + (1 − p)2]. 
 
 
(b) Let A be the event that A wins.  Conditioning on the outcome of the first two games gives 
 
 
 
 
P(A = P(Aa, a)p2 + P(Aa, b)p(1 − p) + P(Ab, a)(1 − p)p + P(Ab, b)(1 − p)2 
 
 
 
        = p2 + P(A)2p(1 − p) 
 
 
 
where the notation a, b means, for instance, that A wins the first and B wins the second 
game.  The final equation used that P(Aa, b) = P(Ab, a) = P(A).  Solving, gives 
 
 
 
 
P(A) = 
)
1(
2
1
2
p
p
p
−
−
 

Chapter 3 
 
37 
79. 
Each roll that is either a 7 or an even number will be a 7 with probability 
 
 
 
p = 
2
/
1
6
/
1
6
/
1
even)
(
)
7
(
)
7
(
+
=
+ P
P
P
 = 1/4 
 
 
Hence, from Example 4i we see that the desired probability is 
 
 
 
 
i
i
i
i
−
=∑






7
7
2
)
4
/
3
(
)
4
/
1(
7
 = 1 − (3/4)7 − 7(3/4)6(1/4) 
 
80. 
(a)  
P(Ai) = (1/2)i, if i < n 
 
 
 
          = (1/2)n−1, if i = n 
 
 
(b) 
1
1
1
2
1
1
2
)
2
/
1(
)
2
/
1(
−
−
=
=
−
+
∑
n
n
n
i
n
i
n
i
 
 
 
(c) Condition on whether they initially play each other.  This gives 
 
 
 
Pn = 
1
2
2
1
1
2
2
2
1
2
1
−






−
−
+
−
n
n
n
n
P
 
 
 
 
where 
2
2
1 





 is the probability they both win given they do not play each other. 
 
 
(d) There will be 2n − 1 losers, and thus that number of games. 
 
 
(e) Since the 2 players in game i are equally likely to be any of the 






2
2n
 pairs it follows that 
P(Bi) = 






2
2
1
n
. 
 
 
(f) Since the events Bi are mutually exclusive 
 
 
 
 
P(∪ Bi) = 
1
)
2
/
1(
2
2
)1
2
(
)
(
−
=






−
=
∑
n
n
n
iB
P
 
 
81. 
30
15
)
11
/
9
(
1
)
11
/
9
(
1
−
−
 
 
82. 
(a) P(A) = 
(
)(
)
[
])
(
1
1
2
2
2
1
2
1
A
P
P
P
P
−
−
+
 or P(A ) = 
2
2
2
1
2
2
2
1
2
1
P
P
P
P
P
−
+
 
 
 
(c) similar to (a) with 
3
iP  replacing 
2
iP . 
 

38 
 
Chapter 3 
 
(b) and (d) Let 
)
( ij
ij P
P
 denote the probability that A wins when A needs i more and B needs j 
more and A(B) is to flip.  Then 
 
 
 
 
Pij = P1Pi−1,j + 
ijP
P)
1(
1
−
 
 
 
 
ijP  = 
ij
j
i
P
P
P
P
)
1(
2
1
,
2
−
+
−
. 
 
 
 
These equations can be recursively solved starting with 
 
 
 
 
P01 = 1, P1,0 = 0. 
 
83. 
(a) Condition on the coin flip 
 
 
 
P{throw n is red} = 
2
1
6
2
2
1
6
4
2
1
=
+
 
 
 
(b) P{rrr} = 
5
3
3
1
2
1
3
2
2
1
3
1
2
1
3
2
2
1
}
{
}
{
2
2
3
3
=






+












+






=
rr
P
rrr
P
 
 
 
(c) P{Arr} = 
2
1
3
1
2
1
3
2
2
1
3
2
}
{
)
(
}
{
2
2
2






+


















=
rr
P
A
P
A
rr
P
 = 4/5 
 
84. 
(b) P(A wins) = 
7
3
8
4
9
5
10
6
11
7
12
8
6
4
7
3
8
4
9
5
10
6
11
7
12
8
9
4
10
6
11
7
12
8
12
4
+
+
+
 
 
 
P(B wins) = 
5
4
6
2
7
3
8
4
9
5
10
6
11
7
12
8
8
4
9
5
10
6
11
7
12
8
11
4
12
8
+
+
 
 
 
P(C wins) = 
5
1
6
2
7
3
8
4
9
5
10
6
11
7
12
8
7
4
8
4
9
5
10
6
11
7
12
8
10
4
11
7
12
8
+
+
 
 
85. 
Part (a) remains the same.  The possibilities for part (b) become more numerous. 
 
86. 
Using the hint 
 
 
 
P{A ⊂ B} = 
n
n
i
i
n
i
n
n
i
i
n
i
n
4
/
2
2
)
2
/
2
(
0
0
∑
∑
=
=






=






 = (3/4)n 
 
 
 
where the final equality uses 
 
 
 
 
∑
=
−






n
i
i
n
i
i
n
0
1
2
 = (2 + 1)n 
 

Chapter 3 
 
39 
 
(b) P(AB = φ) = P(A ⊂ Bc) = (3/4)n, by part (a), since Bc is also equally likely to be any of the 
subsets. 
 
87. 
P{ithall heads} = 
∑
=
k
j
n
n
k
j
k
i
0
)
/
(
)
/
(
. 
 
88. 
No—they are conditionally independent given the coin selected. 
 
 
89. 
(a) P(J3 votes guiltyJ1 and J2 vote guilty} 
 
 
 
= P{J1, J2, J3 all vote guilty}/P{J1 and J2 vote guilty} 
 
 
 
= 
142
97
)
2
(.
10
3
)
7
(.
10
7
)
2
(.
10
3
)
7
(.
10
7
2
2
3
3
=
+
+
. 
 
 
(b) P(J3 guiltyone of J1, J2 votes guilty} 
 
 
 
= 
26
15
)
8
)(.
2
(.
2
10
3
)
3
)(.
7
(.
2
10
7
)
8
)(.
2
(.
2
.)
2
(
10
3
)
3
)(.
7
(.
2
)
7
(.
10
7
=
+
+
. 
 
 
(c) P{J3 guilty J1, J2 vote innocent} 
 
 
= 
102
33
)
8
(.
10
3
)
3
(.
10
7
)
8
)(.
2
(.
10
3
)
3
)(.
7
(.
10
7
2
2
2
2
=
+
+
. 
 
 
 
Ei are conditionally independent given the guilt or innocence of the defendant. 
 
90. 
Let Ni denote the event that none of the trials result in outcome i, i = 1, 2.  Then  
 
 
 
P(N1 ∪ N2) = P(N1) + P(N2) − P(N1N2) 
 
 
 
 
= (1 − p1)n + (1 − p2)n − (1 − p1 − p2)n 
 
 
 
Hence, the probability that both outcomes occur at least once is  
 
 
1 − (1 − p1)n − (1 − p2)n + (p0)n. 
 

40 
 
Chapter 3 
Theoretical Exercises 
 
1. 
P(ABA) = 
)
(
)
(
)
(
)
(
B
A
P
AB
P
A
P
AB
P
∪
≥
 = P(ABA ∪ B) 
 
2. 
If A ⊂ B 
 
 
 
P(AB) = 
)
(
)
(
B
P
A
P
, P(ABc) = 0,  
P(BA) = 1,  
P(BAc) = 
)
(
)
(
c
c
A
P
BA
P
 
 
3. 
Let F be the event that a first born is chosen.  Also, let Si be the event that the family chosen 
in method a is of size i. 
 
 
 
Pa(F) = 
∑
∑
=
i
i
i
i
i
m
n
i
S
P
S
F
P
1
)
(
)
(
 
 
 
Pb(F) = ∑
i
iin
m
 
 
 
Thus, we must show that  
 
 
 
2
/
m
i
n
in
i
i
i
i
≥
∑
∑
 
 
 
or, equivalently,  
 
 
 
 
∑
∑
∑
∑
≥
j
j
i
i
j
j
i
i
n
n
j
n
in
/
 
 
 
or,  
 
 
 
∑∑
∑∑
≠
≠
≥
j
i
j
i
j
i
j
i
n
n
n
n
j
i
 
 
 
Considering the coefficients of the term ninj, shows that it is sufficient to establish that 
 
 
 
2
≥
+ i
j
j
i
 
 
 
or equivalently 
 
 
 
 
 
i2 + j2 ≥ 2ij 
 
 
which follows since (i − j)2 ≥ 0. 
 

Chapter 3 
 
41 
4. 
Let Ni denote the event that the ball is not found in a search of box i, and let Bj denote the 
event that it is in box j. 
 
 
 
P(BjNi) = 
)
(
)
(
)
(
)
(
)
(
)
(
c
i
c
i
i
i
i
i
j
j
i
B
P
B
N
P
B
P
B
N
P
B
P
B
N
P
+
 
 
 
 
         = 
i
i
i
j
P
P
P
−
+
−
1
)
1(
α
 if j ≠ i 
 
 
 
         = 
i
i
i
i
i
P
P
P
−
+
−
−
1
)
1(
)
1(
α
α
 if j = i 
 
5. 
None are true. 
 
6. 
∏
−
−
=





∩
−
=





∪
n
i
c
i
n
i
n
E
P
E
P
E
P
1
1
1
)]
(
1[
1
1
 
 
7. 
(a) They will all  be white if the last ball withdrawn from the urn (when all balls are 
withdrawn) is white.  As it is equally likely to by any of the n + m balls the result follows. 
 
 
(b) P(RBG) = 
b
r
b
g
b
r
g
G
RBG
P
g
b
r
g
+
+
+
=
+
+
)
last
 
(
.   
 
 
Hence, the answer is 
g
r
g
g
b
r
b
g
b
r
b
r
bg
+
+
+
+
+
+
+
)
)(
(
. 
 
8. 
(a) P(A) = P(AC)P(C) + P(AC c)P(C c) > P(BC)P(C) + P(BC c )P(C c) = P(B) 
 
 
(b) For the events given in the hint 
 
 
 
P(AC) = 
(
) ( )
(1/6)(1/6)
1/3
3/36
3/36
P C A P A =
=
 
 
 
 
Because 1/6 = P(A is a weighted average of P(AC) and P(ACc), it follows from the 
result P(AC) > P(A) that P(AC c) < P(A). Similarly,  
 
 
 
 
 
 
 
1/3 = P(BC) > P(B) > P(BC c) 
 
 
 
However, P(ABC) = 0 < P(ABC c). 
 
9. 
P(A) = P(B) = P(C) = 1/2, P(AB) = P(AC) = P(BC) = 1/4.  But, P(ABC) = 1/4. 
 
 
 
 
 
10. 
P(Ai,j) = 1/365.  For i ≠ j ≠ k, P(Ai,jAj,k) = 365/(365)3 = 1/(365)2.  Also, for i ≠ j ≠ k ≠ r, 
P(Ai,jAk,r) = 1/(365)2. 
 
11. 
1 − (1 − p)n ≥ 1/2, or, n ≥ 
)
1
log(
)
2
log(
p
−
−
 
 

42 
 
Chapter 3 
12. 
∏
−
=
−
1
1
)
1(
i
j
j
i
a
a
 is the probability that the first head appears on the ith flip and ∏
∞
=
−
1
)
1(
i
ia  is the 
probability that all flips land on tails. 
 
13. 
Condition on the initial flip.  If it lands on heads then A will win with probability Pn−1,m 
whereas if it lands tails then B will win with probability Pm,n (and so A will win with 
probability 1 − Pm,n). 
 
14. 
Let N go to infinity in Example 4j. 
 
15. 
P{r successes before m failures} 
 
= P{rth success occurs before trial m + r} 
 
= ∑
−
+
=
−
−






−
−
1
)
1(
1
1
r
m
r
n
r
n
r
p
p
r
n
. 
 
16. 
If the first trial is a success, then the remaining n − 1 must result in an odd number of 
successes, whereas if it is a failure, then the remaining n − 1 must result in an even number of 
successes. 
 
17. 
P1 = 1/3 
 
P2 = (1/3)(4/5) + (2/3)(1/5) = 2/5 
 
P3 = (1/3)(4/5)(6/7) + (2/3)(4/5)(1/7) + (1/3)(1/5)(1/7) = 3/7 
 
P4 = 4/9 
 
 
 
(b) Pn = 2
1
n
n +
 
 
 
(c) Condition on the result of trial n to obtain 
 
 
 
 
Pn = (1 − Pn−1)
1
1
2
2
1
2
1
n
n
P
n
n
−
+
+
+  
 
 
(d) Must show that 
 
 
 
 
 
 
1
1
1
2
1
2
1
2
1 2
1
2
1 2
1
n
n
n
n
n
n
n
n
n
−
−


=
−
+


+
−
+
−
+


 
 
 
 
or equivalently, that 
 
 
 
 
1
1
2
2
1
2
1 2
1
2
1 2
1
n
n
n
n
n
n
n
n
n
−
=
+
+
−
+
−
+  
 
 
 
But the right hand side is equal to  
 
 
 
 
2 (
1)
(2
1)(2
1)
2
1
n
n n
n
n
n
n
+
−
=
−
+
+  
 

Chapter 3 
 
43 
18. 
Condition on when the first tail occurs. 
 
19. 
Pn,i = 
1
,1
1
,1
)
1(
−
−
+
−
−
+
i
n
P
i
n
P
p
p
 
 
20. 
αn+1 = αnp + (1 − αn)(1 − p1) 
 
Pn = αnp + (1 − αn)p1 
 
21. 
(b) Pn,1 = P{A receives first 2 votes} = 
1
1
)1
(
)1
(
+
−
=
+
−
n
n
n
n
n
n
 
 
 
Pn, 2 = P{A receives first 2 and at least 1 of the next 2} 
 
 
 
 = 
2
2
)1
(
1
2
1
1
1
2
+
−
=






−
⋅
−
+
−
+
n
n
n
n
n
n
n
n
 
 
 
(c) Pn,m = 
m
n
m
n
+
−
, n ≥ m. 
 
 
(d) Pn,m = P{A always ahead} 
 
 
 
 = P{A alwaysA receives last vote} 
m
n
n
+
 
 
 
 
 
+ P{A alwaysB receives last vote} 
m
n
m
+
 
 
 
 
 = 
m
n
n
+
Pn−1,m + 
1
,
−
+
m
nP
m
n
m
 
 
 
(e) The conjecture of (c) is true when n + m = 1 (n = 1, m = 0). 
 
 
Assume it when n + m = k.  Now suppose that n + m = k + 1.  By (d) and the induction 
hypothesis we have that 
 
 
 
 
Pn,m = 
m
n
m
n
m
n
m
n
m
n
m
m
n
m
n
m
n
n
+
−
=
−
+
+
−
+
+
+
−
−
−
+
1
1
1
1
 
 
 
 
which completes the proof. 
 
22. 
Pn = Pn−1p + (1 − Pn−1)(1 − p) 
 
     = (2p − 1)Pn−1 + (1 − p) 
 
     = 




−
+
−
−1
)1
2
(
2
1
2
1
)1
2
(
n
p
p
 + 1 − p  by the induction hypothesis 
 
     = 
p
p
p
n
−
+
−
+
−
1
)1
2
(
2
1
2
1
2
 
 
     = 
n
p
)1
2
(
2
1
2
1
−
+
. 
 

44 
 
Chapter 3 
23. 
P1,1 = 1/2.  Assume that Pa,b = 1/2 when k ≥ a + b and now suppose a+ b = k + 1.  Now 
 
 
 
Pa,b = P{last is whitefirst a are white}






+
a
b
a
1
 
 
 
 
+ P{last is whitefirst b are black}





+
b
a
b
1
 
 
 
 
+ P{last is whiteneither first a are white nor first b are black} 
 
 
 

















+
−






+
−
b
a
b
a
b
a
1
1
1
 = 




+
−
+
−
+
+
!)
(
!
!
!)
(
!
!
1
2
1
!)
(
!
!
b
a
b
a
b
a
b
a
b
a
b
a
 = 2
1  
 
 
where the induction hypothesis was used to obtain the final conditional probability above. 
 
24. 
The probability that a given contestant does not beat all the members of some given subset of 
k other contestants is, by independence, 1 − (1/2)k.  Therefore P(Bi), the probability that none 
of the other n − k contestants beats all the members of a given subset of k contestants, is  
 
[1 − (1/2)k]n−k.  Hence, Boole’s inequality we have that 
 
 
 
 
P(∪ Bi) ≤ 
k
n
k
k
n
−
−






]
)
2
/
1(
1[
 
 
 
Hence, if 
k
n
k
k
n
−
−






]
)
2
/
1(
1[
 < 1 then there is a positive probability that none of the 






k
n  
events Bi occur, which means that there is a positive probability that for every set of k 
contestants there is a contestant who beats each member of this set. 
 
25. 
P(EF) = P(EF)/P(F) 
 
 
P(EFG)P(GF) = 
)
(
)
(
)
(
)
(
)
(
)
(
F
P
EFG
P
F
P
FG
P
FG
P
EFG
P
=
 
 
 
P(EFGc)P(GcF) = 
)
(
)
(
F
P
EFG
P
c
. 
 
 
The result now follows since 
 
 
 
 
P(EF) = P(EFG) + P(EFGc) 
 
27. 
E1, E2, …, En are conditionally independent given F if for all subsets i1, …, ir of 1, 2, …, n 
 
 
 
 
(
)
(
)
∏
=
=
r
j
F
i
i
i
j
r
E
P
F
E
E
P
1
...
1
. 

Chapter 3 
 
45 
 
28. 
Not true.  Let F = E1. 
 
29. 
P{next m headsfirst n heads} 
 
= P{first n + m are heads}/P(first n heads} 
 
= 
1
1
1
0
1
0
+
+
+
=
∫
∫
+
m
n
n
dp
p
dp
p
n
m
n
. 
 
 

46 
 
Chapter 4 
Chapter 4 
 
Problems 
 
1. 
P{X = 4} = 
91
6
2
14
2
4
=












  
 
 
P{X = 0} = 
91
1
2
14
2
2
=












 
 
 
P{X = 2} = 
91
8
2
14
1
2
2
4
=

















 
 
 
P{X = −1} = 
91
16
2
14
1
2
1
8
=

















 
 
 
P{X = 1} = 
91
32
2
14
1
8
1
4
=

















 
 
 
P{X = −2} = 
91
28
2
14
2
8
=












 
 
2. 
p(1) = 1/36 
p(5) = 2/36 
p(9) = 1/36 
p(15) = 2/36 
p(24) = 2/36 
 
p(2) = 2/36 
p(6) = 4/36 
p(10) = 2/36 
p(16) = 1/36 
p(25) = 1/36 
 
p(3) = 2/36 
p(7) = 0  
p(11) = 0 
p(18) = 2/36 
p(30) = 2/36 
 
p(4) = 3/36 
p(8) = 2/36 
p(12) = 4/36 
p(20) = 2/36 
p(36) = 1/36 
 
4. 
P{X = 1} = 1/2,  P{X = 2} = 
18
5
9
5
10
5
=
, P{X = 3} = 
36
5
8
5
9
4
10
5
=
, 
 
P{X = 4} = 
168
10
7
5
8
3
9
4
10
5
=
,  P{X = 5} = 
252
5
6
5
7
8
9
10
2
3
4
5
=
⋅
⋅
⋅
⋅
⋅
⋅
, 
 
P{X = 6} = 
252
1
6
7
8
9
10
1
2
3
4
5
=
⋅
⋅
⋅
⋅
⋅
⋅
⋅
⋅
 
 
5. 
n − 2i, i = 0, 1, …, n 
 
6. 
P(X = 3} = 1/8, P{X = 1} = 3/8, P{X = −1} = 3/8, P{X = −3} = 1/8 
 
8. 
(a) p(6) = 1 − (5/6)2 = 11/36, p(5) = 2 1/6  4/6 + (1/6)2 = 9/36 
 
 
p(4) = 2 1/6  3/6 + (1/6)2 = 7/36, p(3) = 2 1/6  2/6 + (1/6)2 = 5/36 
 
 
p(2) = 2 1/6  1/6 + (1/6)2 = 3/36, p(1) = 1/36 
 
 
(d) p(5) = 1/36, p(4) = 2/36, p(3) = 3/36, p(2) = 4/36, p(1) = 5/36 
 
 
p(0) = 6/36, p(−j) = p(j), j > 0 
 

Chapter 4 
 
47 
11. 
(a) P{divisible by 3} = 1000
333   P{divisible by 105} = 1000
9
 
 
 
P{divisible by 7} = 1000
142  
 
 
P{divisible by 15} = 1000
66  
 
 
 
In limiting cases, probabilities converge to 1/3, 1/7, 1/15, 1/10 
 
 
(b) P{µ(N) ≠ 0} = P{N is not divisible by 
2
ip , i ≥ 1} 
 
 
= ∏
i
N
P{
 is not divisible by 
2
ip } 
 
 
= ∏
−
i
ip2
/
1
1(
) = 6/π2 
 
13. 
p(0) = P{no sale on first and no sale on second} 
 
 
 = (.7)(.4) = .28 
 
p(500) = P{1 sale and it is for standard} 
 
 
     = P{1 sale}/2 
 
 
     =[P{sale, no sale} + P{no sale, sale}]/2 
 
 
     = [(.3)(.4) + (.7)(.6)]/2 = .27 
 
 
p(1000) = P{2 standard sales} + P{1 sale for deluxe} 
 
 
 
= (.3)(.6)(1/4) + P{1 sale}/2 
 
 
 
= .045 + .27 = .315 
 
 
p(1500) = P{2 sales, one deluxe and one standard} 
 
 
 
= (.3)(.6)(1/2) = .09 
 
 
p(2000) = P{2 sales, both deluxe} = (.3)(.6)(1/4) = .045 
 
14. 
P{X = 0} = P{1 loses to 2} = 1/2 
 
 
P{X = 1} = P{of 1, 2, 3:  3 has largest, then 1, then 2} 
 
 
 
   = (1/3)(1/2) = 1/6 
 
 
P{X = 2} = P{of 1, 2, 3, 4:  4 has largest and 1 has next largest} 
 
 
 
   = (1/4)(1/3) = 1/12 
 
 
P{X = 3} = P{of 1, 2, 3, 4, 5:  5 has largest then 1} 
 
 
 
   = (1/5)(1/4) = 1/20 
 
 
P{X = 4} = P{1 has largest} = 1/5 
 

48 
 
Chapter 4 
15. 
P{X = 1} = 11/66 
 
 
P{X = 2} = 






+






−
∑
=
j
j
j
54
11
66
12
11
2
 
 
 
P{X = 3} = 






+
+






+
−






−
∑∑
≠
≠
=
k
j
j
k
j
j
k
k
j
42
11
54
12
66
12
1
2
 
 
 
P{X = 4} = 1 − ∑
=
=
3
1
}
1
{
i
X
P
 
 
16. 
P{Y1 = i} = 66
12
i
− 
 
P{Y2 = i} = ∑
≠






+
−






−
i
j
j
i
j
54
12
66
12
 
 
P{Y3 = i} = 






+
+






+
−






−
∑∑
≠
≠
≠
j
k
j
k
j
i
k
j
k
i
j
42
11
54
12
66
12
 
 
All sums go from 1 to 11, except for prohibited values. 
 
20. 
(a) P{x > 0} = P{win first bet} + P{lose, win, win} 
 
 
 
         = 18/38 + (20/38)(18/38)2 ≈ .5918 
 
 
(b) No, because if the gambler wins then he or she wins $1. 
 
 
However, a loss would either be $1 or $3. 
 
 
(c) E[X] = 1[18/38 + (20/38)(18/38)2] − [(20/38)2(20/38)(18/38)] − 3(20/38)3 ≈ −.108 
 
21. 
(a) E[X] since whereas the bus driver selected is equally likely to be from any of the 4 buses, 
the student selected is more likely to have come from a bus carrying a large number of 
students. 
 
 
(b) P{X = i} = i/148, i = 40, 33, 25, 50 
 
 
 
E[X] = [(40)2 + (33)2 + (25)2 + (50)2]/148 ≈ 39.28 
 
 
E[Y] = (40 + 33 + 25 + 50)/4 = 37 
 
22. 
Let N denote the number of games played. 
 
 
(a) E(N) = 2[p2 + (1 − p)2] + 3[2p(1 − p)] = 2 + 2p(1 − p) 
 
 
 
The final equality could also have been obtained by using that N = 2 + ] where I is 0 if 
two games are played and 1 if three are played.  Differentiation yields that 
 
 
 
 
p
N
E
dp
d
4
2
]
[
−
=
 
 
 
and so the minimum occurs when 2 − 4p = 0 or p = 1/2. 

Chapter 4 
 
49 
 
(b) E[N] = 3[p3 + (1 − p)3 + 4[3p2(1 − p)p + 3p(1 − p)2(1 − p)] 
 
 
 
 
+ 5[6p2(1 − p)2 = 6p4 − 12p3 + 3p2 + 3p + 3 
 
 
 
Differentiation yields 
 
 
 
 
]
[N
E
dp
d
 = 24p3 − 36p2 + 6p + 3 
 
 
 
Its value at p = 1/2 is easily seen to be 0. 
 
23. 
(a) Use all your money to buy 500 ounces of the commodity and then sell after one week.  
The expected amount of money you will get is 
 
 
 
 
E[money] = 
2000
2
1
500
2
1
+
 = 1250 
 
 
(b) Do not immediately buy but use your money to buy after one week.  Then 
 
 
 
 
E[ounces of commodity] = 
250
2
1
1000
2
1
+
 = 625 
 
24. 
(a) 
4
/
3
4
7
4
3
)
1(
−
=
−
−
p
p
p
,  
(b)    
2
4
11
2
)
1(
4
3
+
−
=
−
+
−
p
p
p
 
 
 
 
18
/
11
2
4
11
4
/
3
4
7
=
⇒
+
−
=
−
p
p
p
, maximum value = 23.72 
 
 
(c) 
)
1(
4
3
q
q
−
−
 
(d)   
)
1(
2
4
3
q
q
−
+
−
, minimax value = 23/72 
 
 
attained when q = 11/18 
 
25. 
(a) 
2
11
)
10
...
2
1(
10
1
=
+
+
+
 
 
 
(b) after 2 questions, there are 3 remaining possibilities with probability 3/5 and 2 with 
probability 2/5.  Hence. 
 
 
 
 
E[Number] = 
5
17
3
2
2
3
1
2
5
3
)
3
(
5
2
=




+
+
+
. 
 
 
 
The above assumes that when 3 remain, you choose 1 of the 3 and ask if that is the one. 
 
27. 
C − Ap = 






+
=
⇒
10
1
10
p
A
C
A
 
 
28. 
3 ⋅ 20
4  = 3/5 
 

50 
 
Chapter 4 
29. 
If check 1, then (if desired) 2:  Expected Cost = C1 + (1 − p)C2 + pR1 + (1 − p)R2; 
 
if check 2, then 1:  Expected Cost = C2 + pC1 + pR1 + (1 − p)R2 so 1, 2, best if  
 
C1 + (1 − p)C2 ≤ C2 + pC1, or C1 ≤ 
2
1
C
p
p
−
 
 
30. 
E[X] = ∑
∞
=1
)
2
/
1(
2
n
n
n
 = ∞ 
 
 
(a) probably not 
 
 
(b) yes, if you could play an arbitrarily large number of games 
 
31. 
E[score] = p*[1 − (1 − P)2 + (1 − p*)(1 − p2) 
 
 
dp
d  = 2(1 − p)p* − 2p(1 − p*) 
 
 
= 0 ⇒ p = p* 
 
32. 
If T is the number of tests needed for a group of 10 people, then 
 
 
 
E[T] = (.9)10 + 11[1 − (.9)10] = 11 − 10(.9)10 
 
35. 
If X is the amount that you win, then 
 
 
 
P{X = 1.10} = 4/9 = 1 − P{X = −1} 
 
 
E[X] = (1.1)4/9 − 5/9 = −.6/9 ≈ =−.067 
 
 
Var(X) = (1.1)2(4/9) + 5/9 − (.6/9)2 ≈ 1.089 
 
36. 
Using the representation 
 
 
 
 
N = 2 + I 
 
 
where I is 0 if the first two games are won by the same team and 1 otherwise, we have that 
 
 
 
 
Var(N) = Var(I) = E[I]2 − E2[I] 
 
 
Now, 
E[I]2 = E[I} = P{I = 1} = 2p{1 − p} and so 
 
 
 
Var(N) = 2p(1 − p)[1 − 2p(1 − p)] = 8p3 − 4p4 − 6p2 + 2p 
 
 
Differentiation yields 
 
 
 
 
)
(
Var N
dp
d
 = 24p2 − 16p3 − 12p + 2 
 
 
and it is easy to verify that this is equal to 0 when p = 1/2. 
 

Chapter 4 
 
51 
37. 
E[X2] = [(40)3 + (33)3 + (25)3 + (50)3]/148 ≈ 1625.4 
 
 
 
 
Var(X = E[X2] − (E[X])2 ≈ 82.2 
 
E[Y2] = = [(40)2 + (33)2 + (25)2 + (50)2]/4 = 1453.5, 
Varr(Y) = 84.5 
 
38. 
(a) E[(2 + X)2] = Var(2 + X) + (E[2 + X])2 = Var(X) + 9 = 14 
 
 
(b) Var(4 + 3X) = 9 Var(X) = 45 
 
39. 
4)
2
/
1(
2
4





 = 3/8  
 
40. 
1
4
)
3
/
2
(
)
3
/
1(
4
5





 + (1/3)5 = 11/243 
 
 
41. 
∑
=






10
7
10
)
2
/
1(
10
i
i
 
 
42. 
3
2
5
4
2
3
)
1(
2
3
)
1(
4
5
)
1(
3
5
p
p
p
p
p
p
p
p
+
−






≥
+
−






+
−






 
 
⇔ 6p3 − 15p2 + 12p − 3 ≥ 0 
 
⇔ 6(p − 1/2)(p − 1)2 ≥ 0 
 
⇔ p ≥ 1/2 
 
43. 
5
4
2
3
)
2
(.
)
8
(.
)
2
(.
4
5
)
8
(.
)
2
(.
3
5
+






+






 
 
44. 
i
n
i
n
k
i
i
n
i
n
k
i
p
p
i
n
p
p
i
n
−
=
−
=
−






−
+
−






∑
∑
)
1(
)
1(
)
1(
2
2
1
1
α
α
 
 
45. 
with 3:  P{pass} = 




+






+




+






3
2
3
2
)
4
(.
)
6
(.
)
4
(.
2
3
3
2
)
8
(.
)
2
(.
)
8
(.
2
3
3
1
 
 
 
 
 
  = .533 
 
 
with 5:  P{pass} = 
∑
∑
=
−
−
=






+






5
3
5
5
5
3
)
6
(.
)
4
(.
5
3
2
)
2
(.
)
8
(.
5
3
1
i
i
i
i
i
i
i
i
 
 
 
 
 
  = .3038 
 
47. 
(a) and (b):  
 (i)    
i
i
i
p
p
i
−
=
−






∑
9
9
5
)
1(
9
,  
(ii)    
i
i
i
p
p
i
−
=
−






∑
8
8
5
)
1(
8
, 
 
(iii) 
i
i
i
p
p
i
−
=
−






∑
7
7
4
)
1(
7
 where p = .7 in (a) and p = .3 in (b). 

52 
 
Chapter 4 
48. 
The probability that a package will be returned is p = 1 − (.99)10 − 10(.99)9(.01).  Hence, if 
someone buys 3 packages then the probability they will return exactly 1 is 3p(1 − p)2. 
 
49. 
(a) 
7
3
7
3
10
10
1
1
.4 .6
.7 .3
7
7
2
2




+








 
 
(b) 
7
3
7
3
9
1
1
.4 .6
.7 .3
2 6
2
.55

+


 
 
50. 
(a) P{H, T, T6 heads}  
= P(H, T, T and 6 heads}/P{6 heads} 
 
 
 
 
 
 
= P{H, T, T}P{6 headsH, T, T}/P{6 heads} 
 
 
 
 
 
 
= 
4
6
2
5
2
6
10
5
7
q
p
q
p
pq












 
 
 
 
 
 
 
=1/10 
 
 
(b) P{T, H, T6 heads}  
= P(T, H, T and 6 heads}/P{6 heads} 
 
 
 
 
 
 
= P{T, H, T}P{6 headsT, H, T}/P{6 heads} 
 
 
 
 
 
 
= 
4
6
2
5
2
6
10
5
7
q
p
q
p
p
q












 
 
 
 
 
 
 
=1/10 
 
51. 
(a) e−.2  
(b)   1 − e−.2 − .2e−.2 = 1 − 1.2e−.2 
 
Since each letter has a small probability of being a typo, the number of errors should 
approximately have a Poisson distribution. 
 
52. 
(a)  1 − e−3.5 − 3.5e−3.5 = 1 − 4.5e−3.5 
 
 
(b) 4.5e−3.5 
 
 
 
Since each flight has a small probability of crashing it seems reasonable to suppose that 
the number of crashes is approximately Poisson distributed. 
 
53. 
(a) The probability that an arbitrary couple were both born on April 30 is, assuming 
independence and an equal chance of having being born on any given date, (1/365)2.  
Hence, the number of such couples is approximately Poisson with mean 80,000/(365)2 ≈ 
.6.  Therefore, the probability that at least one pair were both born on this date is 
approximately 1 − e−.6. 
 
 
(b) The probability that an arbitrary couple were born on the same day of the year is 1/365.  
Hence, the number of such couples is approximately Poisson with mean 80,000/365 ≈ 
219.18.  Hence, the probability of at least one such pair is 1 − e−219.18 ≈ 1. 
 
54. 
(a) e−2.2  
(b)  1 − e−2.2 − 2.2e−2.2 = 1 − 3.2e−2.2 
 

Chapter 4 
 
53 
55. 
2.4
3
2
1
2
1
−
−+
e
e
 
 
56. 
The number of people in a random collection of size n that have the same birthday as yourself 
is approximately Poisson distributed with mean n/365.  Hence, the probability that at least 
one person has the same birthday as you is approximately 1 − e−n/365.  Now, e−x = 1/2 when  
 
x = log(2).  Thus, 1 − e−n/365 ≥ 1/2 when n/365 ≥ log(2).  That is, there must be at least 365 
log(2) people. 
 
57. 
(a) 1 − e−3 − 3e−3 − e−3
3
2
2
17
1
2
3
−
−
=
e
 
 
 
(b) P{X ≥ 3X ≥ 1} = 
3
3
1
2
17
1
}
1
{
}
3
{
−
−
−
−
=
≥
≥
e
e
X
P
X
P
 
 
59. 
(a) 1 − e−1/2 
 
 
(b) 
2
/
1
2
1
−
e
 
 
 
(c) 1 − e−1/2 = 
2
/
1
2
1
−
e
 
 
60. 
P{beneficial2} = 
4
/
1
}
beneficial
not 
 
2
{
4
/
3
}
beneficial
 
2
{
}3/4
beneficial
2
{
P
P
P
+
 
 
 
 
 
 = 
4
1
2
5
4
3
2
3
4
3
2
3
2
5
2
3
2
3
−
−
−
+ e
e
e
 
 
61. 
1 − e−1.4 − 1.4e−1.4 
 
62. 
If Ai is the event that couple number i are seated next to each other, then these events are, 
when n is large, roughly independent.  As P(Ai = 2/(2n − 1) it follows that, for n large, the 
number of wives that sit next to their husbands is approximately Poisson with mean  
 
2n/(2n − 1) ≈ 1.  Hence, the desired probability is e−1 = .368 which is not particularly close to 
the exact solution of .2656 provided in Example 5n of Chapter 2, thus indicating that n = 10 
is not large enough for the approximation to be a good one. 
 
63. 
(a) e−2.5 
 
 
(b) 1 − e−2.5 − 2.5e−2.5 − 
5.2
3
5.2
2
!
3
)
5.2
(
2
)
5.2
(
−
−
−
e
e
 

54 
 
Chapter 4 
64. 
(a) 1 − ∑
=
−
7
0
4
!
/
4
i
i i
e
 ≡ p 
 
 
(b) 1 − (1 − p)12 − 12p(1 − p)11 
 
 
(c) (1 − p)i−1p 
 
65. 
(a) 1 − e−1/2 
 
 
(b) P{X ≥ 2X ≥ 1} = 
2
/
1
2
/
1
2
/
1
1
2
1
1
−
−
−
−
−
−
e
e
e
 
 
 
(c) 1 − e−1/2 
 
 
(d) 1 − exp {−500 − i)/1000} 
 
66. 
Assume n > 1. 
 
(a) 
2
2
1
n − 
 
(b) 
2
2
2
n −
 
 
(c) exp{−2n/(2n − 1)} ≈ e−1 
 
67. 
Assume n > 1. 
 
(a) 2
n  
 
(b) Conditioning on whether the man of couple j sits next to the woman of couple i gives the 
result:  
2
1
1
2
2
2
3
1
1
1
1
(
1)
n
n
n
n
n
n
n
−
−
+
=
−
−
−
−
−
 
 
(c) e−2 
 
68. 
exp(−10e−5} 
 
69. 
With Pj equal to the probability that 4 consecutive heads occur within j flips of a fair coin, P1 
= P2 = P + 3 = 0, and  
 
 
 
P4 = 1/16 
 
 
P5 = (1/2)P4 + 1/16 = 3/32 
 
 
P6 = (1/2)P5 + (1/4)P4 + 1/16 = 1/8 
 
 
P7 = (1/2)P6 + (1/4)P5 + (1/8)P4 + 1/16 = 5/32 
 
 
P8 = (1/2)P7 + (1/4)P6 + (1/8)P5 + (1/16)P4 + 1/16 = 6/32 
 
 
P9 = (1/2)P8 + (1/4)P7 + (1/8)P6 + (1/16)P5 + 1/16 = 111/512 
 
 
P10 = (1/2)P9 + (1/4)P8 + (1/8)P7 + (1/16)P6 + 1/16 = 251/1024 = .2451 
 
 
The Poisson approximation gives 
 
 
 
 
P10 ≈ 1 − exp{−6/32 − 1/16} = 1 − e−.25 = .2212 

Chapter 4 
 
55 
 
70. 
e−λt + (1 − e−λt)p 
 
71. 
(a) 
5
38
26 





 
 
 
(b) 
38
12
38
26
3






 
 
72. 
P{wins in i games} = 
4
4
)
4
(.
)
6
(.
3
1
−





−
i
i
 
 
73. 
Let N be the number of games played.  Then 
 
 
P{N = 4} = 2(1/2)4 = 1/8,  
 
P{N = 5} = 
4)
2
/
1
)(
2
/
1(
1
4
2






 = 1/4 
 
P{N = 6} = 
4
2
)
2
/
1(
)
2
/
1(
2
5
2






 = 5/16, 
 
P{N = 7} = 5/16 
 
E[N] = 4/8 + 5/4 + 30/16 + 35.16 = 93/16 = 5.8125 
 
74. 
(a) 
5
3
2 





 
 
 
(b) 
8
7
2
6
3
5
3
2
3
1
3
2
7
8
3
1
3
2
6
8
3
1
3
2
5
8






+

















+

















+

















 
 
 
(c) 
3
1
3
2
4
5
5











 
 
 
(d) 
2
5
3
1
3
2
4
6

















 
 
76. 
)
2
/
1(
)
2
/
1(
)
2
/
1(
)
2
/
1(
2
1
2
1
2
2
1
1
2
1
k
N
N
k
N
N
N
k
N
N
N
k
N
N
−
+
−
+






−
+
+






−
+
 
 
77. 
k
N
N
k
N
−






−
2)
2
/
1(
2
2
 
 
 
)
2
/
1(
)
2
/
1(
1
1
2
2
1
2
−
−






−
−
−
k
N
N
k
N
 
 

56 
 
Chapter 4 
79. 
(a) P{X = 0} = 












10
100
10
94
 
 
 
(b) P{X > 2} = 1 − 

















+











+






10
100
2
6
8
94
1
6
9
94
10
94
 
 
80. 
P{rejected1 defective} = 3/10 
 
P{rejected4 defective} = 1 − 












3
10
3
6
 = 5/6 
 
P{4 defectiverejected} = 
10
7
10
3
10
3
6
5
10
3
6
5
+
 = 75/138 
 
81. 
P{rejected} = 1 − (.9)4 
 

Chapter 4 
 
57 
Theoretical Exercises 
 
1. 
Let Ei = {no type i in first n selections} 
 
 
P{T > n} = 





∪
=
i
N
i
E
P
1
 
 
 
 
   = 
k
j
i
n
k
J
I
n
j
i
i
n
i
p
p
P
P
P
J
<
<
<
∑∑∑
∑∑
∑
−
−
+
−
−
−
−
)
1(
)
1(
)
1(
 
 
 
 
 
… + (−1)N∑
i
n
iP  
 
 
P{T = n} = P{T > n − 1} − P{T > n} 
 
3. 
1 − 
)
(
lim
0
h
a
F
h
−
→
 
 
4. 
Not true.  Suppose P{X = b} = ε > 0 and bn = b + 1/n.  Then 
}
(
lim
n
b
b
b
X
P
n
<
→
 = P{X ≤ b} ≠ 
P{X < b}. 
 
5. 
When α > 0 
 
 
 
P{αX + β ≤ x} = 






−
=


−
≤
α
β
α
β
x
F
x
x
P
 
 
 
When α < 0 
 
 
 
P{αX + β ≤ x} = 






−
−
−
=


−
≥
+
→
1
lim
1
0
α
β
α
β
x
F
x
X
P
h
. 
 
6. 
∑
∞
=
≥
1
}
{
i
i
N
P
 = ∑∑
∞
=
∞
=
=
1
1
}
{
i
k
k
N
P
 
 
 
 
         = ∑∑
∞
=
∞
=
=
1
1
}
{
k
i
K
N
P
 
 
 
 
         = 
]
[
}
{
1
N
E
k
N
kP
k
=
=
∑
∞
=
. 
 
7. 
∑
∞
=
>
0
}
{
i
i
N
P
i
 = ∑∑
∞
=
∞
+
=
=
0
1
}
{
i
i
k
k
N
P
i
 
 
 
 
           = 
∑
∑
−
=
∞
=
=
1
0
1
}
{
k
i
k
i
k
N
P
 
 
 
 
           =
2
/
)1
(}
{
1
k
k
k
N
P
k
−
=
∑
∞
=
 
 
 
 
           = 
2
}
{
}
{
1
1
2






=
−
=
∑
∑
∞
=
∞
=
k
k
k
N
kP
k
N
P
k
 
 

58 
 
Chapter 4 
8. 
 
E[cX] = cp + c−1(1 − p) 
 
 
Hence, 1 = E[cX] if 
 
 
cp + c−1(1 − p) = 1 
 
or, equivalently 
 
 
pc2 − c + 1 − p = 0 
 
or 
 
 
(pc − 1 + p)(c− 1) = 0 
 
 
Thus, c = (1 − p)/p. 
 
9. 
E[Y] = E[X/σ − µ/σ] = 
]
[
1
X
E
σ
 − µ/σ = µ/σ − µ/σ = 0 
 
 
 
Var(Y) = (1/σ)2 Var(X) = σ2/σ2 = 1. 
 
10. 
E[1/(X + 1)] = 
i
n
i
n
i
p
p
i
i
n
n
i
−
=
−
−
+
∑
)
1(
!
!)
(
!
1
1
0
 
 
 
 
        = 
i
n
i
n
i
p
p
i
i
n
n
−
=
−
+
−
∑
)
1(
!)1
(!)
(
!
0
 
 
 
 
        = 
i
n
i
n
i
p
p
i
n
p
n
−
+
=
−






+
+
+
∑
)
1(
1
1
)1
(
1
1
0
 
 
 
 
        = 
j
n
j
n
j
p
p
j
n
p
n
−
+
+
=
−






+
+
∑
1
1
1
)
1(
1
)1
(
1
 
 
 
 
        = 




−






+
−
+
−
+
0
1
0
)
1(
0
1
1
)1
(
1
n
p
p
n
p
n
 
 
 
 
        = 
]
)
1(
1[
)1
(
1
1
+
−
−
+
n
p
p
n
 
 
11. 
For any given arrangement of k successes and n − k failures: 
 
 
 
 
P{arrangementtotal of k successes} 
 
 
 
= 






=
−






−
=
−
−
k
n
p
p
k
n
p
p
k
P
P
k
n
k
k
n
k
1
)
1(
)
1(
successes}
 
{
t}
arrangemen
{
 
 
12. 
Condition on the number of functioning components and then use the results of Example 4c 
of Chapter 1: 
 
 
 
 
Prob = 
















−
+
−






∑
=
−
i
n
i
n
i
p
p
i
n
n
i
i
n
i
1
)
1(
0
 
 
where 






−
+
i
n
i
1  = 0 if n − i > i + 1.  We are using the results of Exercise 11. 
 

Chapter 4 
 
59 
13. 
Easiest to first take log and then determine the p that maximizes log P{X = k}. 
 
 
 
 
log P{X = k} = log






k
n  + k log p + (n − k) log (1 − p) 
 
 
 
 
p
k
n
p
k
k
x
P
p
−
−
−
=
=
∂
∂
1
}
{
log
 
 
 
 
 
 
= 0 ⇒ p = k/n maximizes 
 
14. 
(a) 1 − 
p
p
p
n
n
−
−
=
∑
∞
=
1
1
1
α
α
 
 
 
(b) Condition on the number of children:  For k > 0 
 
 
 
 
P{k boys} = 
n
n
p
n
k
P
α
children}
 
{
1∑
∞
=
 
 
 
 
 
     = 
n
n
k
n
p
k
n
α
)
2
/
1(
∑
∞
=






 
 
 
 
 
P{0 boys} = 1 − 
∑
∞
=
+
−
1
)
2
/
1(
1
n
n
n
p
p
p
α
α
 
 
17. 
(a) If X is binomial (n, p) then, from exercise 15, 
 
 
 
 
P{X is even} = [1 + (1 − 2p)n]/2 
 
 
 
 
          = [1 + (1 − 2λ/n)n]/2  when λ = np 
 
 
 
 
          → (1 + e−2λ)/2  as n approaches infinity 
 
 
(b) P{X is even} = e−λ
)!
2
/(
2
n
n
n
∑λ
 = e−λ(eλ  + e−λ)/2 
 
18. 
log P{X = k} = −λ + k log λ − log (k!) 
 
 
λ
λ
k
k
X
P
+
−
=
=
∂
∂
1
}
{
log
 
 
 
 
 
  = 0 ⇒ λ = k 
 

60 
 
Chapter 4 
19. 
E[X n] = 
!
/
0
i
e
i
i
i
n
λ
λ
−
∞
=∑
 
 
 
   = 
!
/
1
i
e
i
i
i
n
λ
λ
−
∞
=∑
 
 
 
   = 
!)1
/(
1
1
−
−
∞
=
−
∑
i
e
i
i
i
n
λ
λ
 
 
 
   = 
!
/
)1
(
1
0
1
j
e
j
j
j
n
+
−
∞
=
−
∑
+
λ
λ
 
 
 
   = 
!
/
)1
(
0
1
j
e
j
j
j
n
λ
λ
λ
−
∞
=
−
∑
+
 
 
 
   = 
]
)1
[(
1
−
+
n
X
E
λ
 
 
 
Hence [X 3] = λE(X + 1)2] 
 
 
 
       = 
!
/
)1
(
0
2
i
e
i
i
i
λ
λ
λ
−
∞
=∑
+
 
 
 
 
       = 






+
+
∑
∑
∑
∞
=
−
∞
=
−
−
∞
=
0
0
0
2
!
/
!
/
2
!
/
i
i
i
i
i
i
i
e
i
ie
i
e
i
λ
λ
λ
λ
λ
λ
λ
 
 
 
 
       = 
)1
]
[
2
]
[
[
2
+
+
X
E
X
E
λ
 
 
 
 
       = λ(Var(X) = E2[X] + 2E[X] + 1) 
 
 
            = λ(λ + λ2 + 2λ + 1) = λ(λ2 + 3λ + 1) 
 
20. 
Let S denote the number of heads that occur when all n coins are tossed, and note that S has a 
distribution that is approximately that of a Poisson random variable with mean λ.  Then, 
because X is distributed as the conditional distribution of S given that S > 0, 
 
 
 
 
P{X = 1} = P{S = 1S > 0} = 
λ
λ
λ
−
−
−
≈
>
=
e
e
S
P
S
P
1
}
0
{
}
1
{
 
 
21. 
(i) 1/365 
 
 
(ii)  1/365 
 
 
(iii) 1  The events, though independent in pairs, are not independent. 
 
22. 
(i) Say that trial i is a success if the ith pair selected have the same number.  When n is large 
trials 1, …, k are roughly independent. 
 
(ii) Since, P{trial i is a success} = 1/(2n − 1) it follows that, when n is large, Mk is 
approximately Poisson distributed with mean k/(2n − 1).  Hence, 
 
 
 
 
P{Mk = 0} ≈ exp[−k/(2n − 1)] 
 
 
(iii) and (iv) P{T > αn} = P{Mαn = 0} ≈ exp[−αn/(2n − 1)] → e−α/2 
 

Chapter 4 
 
61 
23. 
(a) P(Ei) = 1 − 
2
365
0
365 (1/365) (364/365)
j
j
j
j
−
=






∑
 
 
 
(b) exp(−365P(E1)} 
 
24. 
(a) There will be a string of k consecutive heads within the first n trials either if there is one 
within the first n − 1 trials, or if the first such string occurs at trial n; the latter case is 
equivalent to the conditions of 2. 
 
 
(b) Because cases 1 and 2 are mutually exclusive 
 
 
 
 
Pn = Pn−1 + (1 − Pn−k−1)(1 − P)pk 
 
25. 
P(m counted) = 
!
/
events)
 
(
n
e
n
m
P
n
n
λ
λ
−
∑
 
 
 
 
          = 
!
/
)
1(
n
e
p
p
m
n
n
m
n
m
m
n
λ
λ
−
−
∞
=
−






∑
 
 
 
 
          = 
)
1
(
)!
(
)]
1(
[
!
)
(
p
m
n
m
n
m
p
e
m
n
p
m
p
e
−
−
∞
=
−
−
∑
−
−
λ
λ
λ
λ
 
 
 
 
          = 
!
)
(
m
p
e
m
p λ
λ
−
 
 
 
Intuitively, the Poisson λ random variable arises as the approximate number of successes in n 
(large) independent trials each having a small success probability α (and λ  nα).  Now if each 
successful trial is counted with probability p, than the number counted is Binomial with 
parameters n (large) and αp (small) which is approximately Poisson with parameter αpn = λp. 
 
27. 
P{X = n + kX > n} = 
}
{
}
{
n
X
P
k
n
X
P
>
+
=
 
 
 
 
 
        = 
n
k
n
p
p
p
)
1(
)
1(
1
−
−
−
+
 
 
 
 
 
        = p(1 − p)k−1 
 
 
If the first n trials are fall failures, then it is as if we are beginning anew at that time. 
 
28. 
The events {X > n} and {Y < r} are both equivalent to the event that there are fewer than r 
successes in the first n trials; hence, they are the same event. 
 
29. 
}
{
}
1
{
k
X
P
k
X
P
=
+
=
  = 






−
−












−
−
−






+
k
n
Np
N
k
Np
k
n
np
N
k
Np
1
1
 
 
 
 
 
= 
)1
)(
1
(
)
)(
(
+
+
−
−
+
−
−
k
n
Np
N
k
k
n
k
Np
 
 

62 
 
Chapter 4 
30. 
P{Y = j} = 












−
−
n
N
n
j
1
1
, n ≤ j ≤ N 
 
E[Y] = ∑
=












−
−
N
n
j
n
N
n
j
1
1
 
 
 
 = 
∑
=












N
n
j
n
j
n
N
n
 
 
 
 = 
∑
+
+
=






−
+
−






1
1
1
1
1
N
n
i
n
i
n
N
n
 
 
 
= 






+
+






1
1
n
N
n
N
n
 
 
 
= 
1
)1
(
+
+
n
N
n
 
 
31. 
Let Y denote the largest of the remaining m chips.  By exercise 28 
 
 
 
P{Y = j} = 






+






−
−
m
n
m
m
j
1
1
, m ≤ j ≤ n + m 
 
 
Now, X = n + m − Y and so 
 
 
 
P{X = i} = P{Y = m + n − i} = 






+






−
−
−
+
m
n
m
m
i
n
m
1
1
, i ≤ n 
 
32. 
P{X = k} = 
∏
−
−
−
=
n
i
n
n
k
k
i
2
0
1
, k > 1 
 
34. 
E[X] = 
1
2
2
1
2
1
0
−
=
−






−
=∑
n
n
n
k
n
n
k
n
k
 
 
E[X 2] = 
1
2
)1
(
2
1
2
2
0
2
−
+
=
−






−
=∑
n
n
n
k
n
n
n
k
n
k
 
 
 
Var(X) = E[X 2 ] − {E[X])2 = 
2
2
2
2
)1
2
(
2
)1
(
2
−
+
−
−
−
n
n
n
n
n
n
 
 
 
 
 
 
  ~ 
4
2
2
2
2
2
n
n
n
n
=
−
 
 

Chapter 4 
 
63 
 
E[Y] = 
2
1
+
n
, E[Y 2] = 
∫
∑
+
=
1
1
2
2
1
2
3
~
x
~
/
n
n
i
n
n
dx
n
i
 
 
Var(Y) ~ 
12
~
2
1
3
2
2
2
n
n
n






+
−
 
 
35. 
(a) P{X > i} = 
1
1
1
...
3
2
2
1
+
=
+
i
i
i
 
 
(b) P(X < ∞} = 
}
{
lim
i
X
P
i
≤
∞
→
 
 
 
 
          = 
1
))
1
/(
1
1(
lim
=
+
−
i
i
 
 
(c) E[X] = ∑
=
i
i
X
iP
}
{
 
 
 
 
  = ∑
>
−
−
>
i
i
X
P
i
X
P
i
}
{
}
1
{
(
 
 
 
   
  = ∑






+
−
i
i
i
i
1
1
1
 
 
 
 
  = ∑+
i i
1
1
 
 
 
 
  = ∞ 
 
 

64 
 
Chapter 5 
Chapter 5 
 
Problems 
 
1. 
(a) 
4
/
3
1
)
1(
1
1
2
=
⇒
=
−
∫
−
c
dx
x
c
 
 
 
(b) F(x) = 






+
−
=
−
∫
−
3
2
3
4
3
)
1(
4
3
3
1
2
x
x
dx
x
x
, −1 < x < 1  
 
2. 
∫
−
−
−
−
−
=
2
/
2
/
2
/
4
2
x
x
x
e
xe
dx
xe
.  Hence, 
 
 
 
 
4
/
1
1
0
2
/
=
⇒
=
∫
∞
−
c
dx
xe
c
x
 
 
 
P{X > 5} = 
]
4
10
[
4
1
4
1
2
/
5
2
/
5
5
2
/
−
−
∞
−
+
=
∫
e
e
dx
xe x
 
 
 
 
 
 
       
2
/
5
4
14
−
=
e
 
 
3. 
No.  f(5/2) < 0 
 
4. 
(a) 
∫
∫
∞
∞
=
−
=
20
20
2
2
/
1
10
10
x
dx
x
. 
 
 
(b) F(y) = 
y
dx
x
y
10
1
10
10
2
−
=
∫
, y > 10.  F(y) = 0  for y < 10. 
 
 
(c) ∑
=
−

















6
3
6
3
1
3
2
6
i
i
i
i
 since 
15
10
)
15
(
=
F
.  Assuming independence of the events that the 
devices exceed 15 hours. 
 
5. 
Must choose c so that 
 
 
 
 
.01 = 
5
1
4
)
1(
)
1(
5
c
dx
x
c
−
=
−
∫
 
 
so c = 1 − (.01)1/.5. 
 

Chapter 5 
 
65 
6. 
(a) E[X] = 
∫
∫
∞
−
∞
−
=
0
2
0
2
/
2
2
4
1
dx
e
y
dx
e
x
y
x
 = 2Γ(3) = 4 
 
 
(b) By symmetry of f(x) about x= 0, E[X] = 0 
 
 
 
(c) E[X] = 
∞
=
∫
∞
5
5 dx
x
 
 
7. 
∫
+
1
0
2)
(
dx
bx
a
 = 1 or 
1
3 =
+ b
a
 
 
5
3
)
(
1
0
2
=
+
∫
dx
bx
a
x
 or 
5
/
3
4
2
=
+ b
a
.  Hence, 
 
 
a = 5
3 ,  b = 5
6  
 
8. 
E[X] = 
2
)
3
(
0
2
=
Γ
=
∫
∞
−dx
e
x
x
 
 
9. 
If s units are stocked and the demand is X, then the profit, P(s), is given by 
 
 
 
P(s) = bX − (s − X)Ρ  
if X ≤ s 
 
 
 
 = sb 
 
 
if X > s 
 
 
Hence 
 
 
 
E[P(s)]  = 
∫
∫
∞
+
−
−
s
s
dx
x
sbf
dx
x
f
x
s
bx
)
(
)
(
)
)
(
(
0
A
 
 
 
 
       = 



−
+
−
+
∫
∫
∫
s
s
s
dx
x
f
sb
dx
x
f
s
dx
x
xf
b
0
0
0
)
(
1
)
(
)
(
)
(
A
A
 
 
 
 
       = sb + 
∫
−
+
s
dx
x
f
s
x
b
0
)
(
)
(
)
(
A
 
 
 
Differentiation yields 
 
 
 
)]
(
[
s
P
E
ds
d
 = 




−
+
+
∫
∫
s
s
dx
x
f
s
dx
x
xf
ds
d
b
b
0
0
)
(
)
(
)
(
A
 
 
 
 
 
= b + 




−
−
+
∫
s
dx
s
f
s
sf
s
sf
b
0
)
(
)
(
)
(
)
(
A
 
 
 
 
 
= b − 
∫
+
s
dx
x
f
b
0
)
(
)
(
A
 
 
 
 

66 
 
Chapter 5 
 
Equating to zero shows that the maximal expected profit is obtained when s is chosen so that 
 
 
 
 
F(s) = 
A
+
b
b
 
 
 
where F(s) = ∫
s
dx
x
f
0
)
(
 is the cumulative distribution of demand. 
 
10. 
(a) P{goes to A} = P{5 < X < 15 or 20 < X < 30 or 35 < X < 45 or 50 < X < 60}. 
 
 
 
 
   = 2/3  since X is uniform (0, 60). 
 
 
(b) same answer as in (a). 
 
11. 
X is uniform on (0, L). 
 
 
 






<






−
−
4
/
1
,
min
X
X
L
X
L
X
P
 
 
 
= 1 − 






>






−
−
4
/
1
,
min
X
X
L
X
L
X
P
 
 
 
= 1 − 


>
−
>
−
4
/
1
,4
/
1
X
X
L
X
L
X
P
 
 
 
= 1 − P{X > L/5, X < 4L/5} 
 
 
= 


<
<
−
5
/
4
5
1
L
X
L
P
 
 
 
= 1 − 
5
2
5
3 =
. 
 
13. 
P{X > 10} = 3
2 , P{X > 25 X > 15} = 
30
/
15
30
/
5
}
15
{
25
{
=
>
>
X
P
X
P
 = 1/3 
 
where X is uniform (0, 30). 
 
14. 
E[Xn] = 
1
1
1
0
+
=
∫
n
dx
xn
 
 
P{Xn ≤ x} = P{X ≤ x1/n} = x1/n 
 
E[Xn] = 
1
1
1
1
1
0
/
1
1
1
1
0
+
=
= ∫
∫






−
n
dx
x
n
dx
x
n
x
n
n
 
 
15. 
(a) Φ(.8333) = .7977 
 
(b) 2Φ(1) − 1 = .6827 
 
(c) 1 − Φ(.3333) = .3695 
 
(d) Φ(1.6667) = .9522 
 
(e) 1 − Φ(1) = .1587 
 

Chapter 5 
 
67 
16. 
P{X > 50} = 


>
−
4
10
4
40
X
P
 = 1 − Φ(2.5) = 1 − .9938 
 
Hence, (P{X < 50})10 = (.9938)10 
 
17. 
E[Points] = 10(1/10) + 5(2/10) + 3(2/10) = 2.6 
 
18. 
.2 = 


−
>
−
σ
σ
5
9
5
X
P
 = P{Z > 4/σ}  where Z is a standard normal.  But from the normal 
table P{Z < .84) ≈ .80 and so  
 
 
 
.84 ≈ 4/σ or σ ≈ 4.76 
 
 
That is, the variance is approximately (4.76)2 = 22.66. 
 
19. 
Letting Z = (X − 12)/2 then Z is a standard normal.  Now, .10 = P{Z > (c − 12)/2}.  But from 
Table 5.1, P{Z < 1.28} = .90 and so 
 
 
 
(c − 12)/2 = 1.28  or  c = 14.56 
 
20. 
Let X denote the number in favor.  Then X is binomial with mean 65 and standard deviation 
)
35
(.
65
 ≈ 4.77.  Also let Z be a standard normal random variable. 
 
 
(a) P{X ≥ 50} = P{X ≥ 49.5} = P{X − 65}/4.77 ≥ −15.5/4.77 
 
 
 
 
 
         ≈ P{Z ≥ −3.25} ≈ .9994 
 
 
(b) P{59.5 ≤ X ≤ 70.5} ≈ P{−5.5/4.77 ≤ Z ≤ 5.5/4.77} 
 
 
 
 
 
= 2P{Z ≤ 1.15} − 1 ≈ .75 
 
 
(c) P{X ≤ 74.5} ≈ P{Z ≤ 9.5/4.77} ≈ .977 
 
22. 
(a) P{.9000 − .005 < X < .9000 + .005} 
 
 
 
= 


<
<
−
003
.
005
.
003
.
005
.
Z
P
 
 
 
 
= P{−1.67 < Z < 1.67} 
 
 
 
= 2Φ(1.67) − 1 = .9050. 
 
 
 
Hence 9.5 percent will be defective (that is each will be defective with probability  
 
 
1 − .9050 = .0950). 
 
 
(b) 






Φ
=


<
<
−
σ
σ
σ
005
.
2
005
.
005
.
Z
P
 − 1 = .99 when 
 
 
 
 
0019
.
575
.2
005
.
995
.
005
.
=
⇒
=
⇒
=






Φ
σ
σ
σ
. 
 

68 
 
Chapter 5 
23. 
(a) P{149.5 < X < 200.5} = 












−
<
<
−
6
5
6
1
1000
6
1000
5.
200
6
5
6
1
1000
6
1000
5.
149
Z
P
 
 
 
 
 
 
    = 






−
Φ
−






−
Φ
36
/
5000
7.
166
5.
149
36
/
5000
7.
166
5.
200
 
 
 
 
 
 
    ≈ Φ(2.87) + Φ(1.46) − 1 = .9258. 
 
 
(b) P{X < 149.5} = 












−
<
5
4
5
1
800
)
5
/
1(
800
5.
149
Z
P
 
 
 
 
 
   = P{Z < −.93} 
 
 
 
 
   = 1 − Φ(.93) = .1762. 
 
24. 
With C denoting the life of a chip, and φ the standard normal distribution function we have 
 
 
 
P{C < 1.8 × 106} = 






×
×
−
×
5
6
6
10
3
10
4.1
10
8.1
φ
 
 
 
 
 
          = φ(1.33) 
 
 
 
 
          = .9082 
 
 
Thus, if N is the number of the chips whose life is less than 1.8 × 106 then N is a binomial 
random variable with parameters (100, .9082).  Hence, 
 
 
 
P{N > 19.5} ≈ 1 − 






−
)
0918
(.
82
.
90
82
.
90
5.
19
φ
 = 1 −φ(−24.7) ≈ 1 
 
25. 
Let X denote the number of unacceptable items among the next 150 produced.  Since X is a 
binomial random variable with mean 150(.05) = 7.5 and variance 150(.05)(.95) = 7.125, we 
obtain that, for a standard normal random variable Z. 
 
 
 
P{X ≤ 10}  = P{X ≤ 10.5} 
 
 
 
 
= 






−
≤
−
125
.7
5.7
5.
10
125
.7
5.7
X
P
 
 
 
 
 
≈ P{Z ≤ 1.1239} 
 
 
 
 
= .8695 
 
 
The exact result can be obtained by using the text diskette, and (to four decimal places) is 
equal to .8678. 
 
27. 
P{X > 5,799.5} = 






>
500
,2
5.
799
Z
P
 
 
 
 
 
= P{Z > 15.99} = negligible. 

Chapter 5 
 
69 
28. 
Let X equal the number of lefthanders.  Assuming that X is approximately distributed as a 
binomial random variable with parameters n = 200, p = .12, then, with Z being a standard 
normal random variable, 
 
 
 
P{X > 19.5} = 






−
>
−
)
88
)(.
12
(.
200
)
12
(.
200
5.
19
)
88
)(.
12
(.
200
)
12
(.
200
X
P
 
 
 
 
 
 ≈ P{Z > −.9792} 
 
 
 
 
 ≈ .8363 
 
29. 
Let s be the initial price of the stock.  Then, if X is the number of the 1000 time periods in 
which the stock increases, then its price at the end is  
 
 
 
 
suXd1000-X = sd1000
X
d
u 





 
 
 
Hence, in order for the price to be at least 1.3s, we would need that 
 
 
 
 
d1000
3.1
>






X
d
u
 
 
or 
 
 
 
 
X > 
)
/
log(
)
log(
1000
)
3.1
log(
d
u
d
−
 = 469.2 
 
 
That is, the stock would have to rise in at least 470 time periods.  Because X is binomial with 
parameters 1000, .52, we have 
 
 
 
 
P{X > 469.5} = 






−
>
−
)
48
)(.
52
(.
1000
)
52
(.
1000
5.
469
)
48
)(.
52
(.
1000
)
52
(.
1000
X
P
 
 
 
 
 
          ≈ P{Z > −3.196} 
 
 
 
 
          ≈ .9993 
 
30. 
P{in black} = 
)
1
}(
white
5
{
}
black
5
{
}
black
5
{
α
α
α
−
+ P
P
P
 
 
 
 
       = 
18
/
)
6
5
(
8
/
)
4
5
(
8
/
)
4
5
(
2
2
2
2
3
1
)
1(
2
2
1
2
2
1
−
−
−
−
−
−
−
+
e
e
e
π
α
α
π
α
π
 
 
 
 
      = 
8
/
1
8
/
1
8
/
1
3
)
1(
2
2
−
−
−
−
+
e
e
e
α
α
α
 
 
 
 
α is the value that makes preceding equal 1/2 

70 
 
Chapter 5 
31. 
(a) 
[
]
a
X
E
−
 = 






−
−
=
−
+
−
∫
∫
A
a
a
A
A
dx
x
a
A
dx
a
x
a
A
a
2
0
2
)
(
)
(
 
 
 
 
   
2
/
0
1
2
)
(
A
a
A
a
da
d
=
⇒
=
−
=
 
 
 
(b) 
[
]
a
X
E
−
 = 
∫
∫
∞
−
−
−
+
−
a
x
a
x
dx
e
a
x
dx
e
x
a
λ
λ
λ
λ
)
(
)
(
0
 
 
 
 
           = 
a
a
a
a
a
a
ae
e
ae
e
ae
e
a
λ
λ
λ
λ
λ
λ
λ
λ
λ
−
−
−
−
−
−
−
+
+
−
+
+
−
1
)
1(
 
 
 
 
Differentiation yields that the minimum is attained at a  where 
 
 
 
 
2
/
1
=
−a
e λ
 or a  = log 2/λ 
 
 
(c) Minimizing a = median of F 
 
32. 
(a) e−1 
 
(b) e−1/2 
 
33. 
e−1 
 
34. 
(a) P{X > 20} = e−1 
 
 
(b) P{X > 30X > 10 = 
4
/
3
4
/
1
}
10
{
}
30
{
=
>
>
X
P
X
P
 = 1/3 
 
35. 
(a) 








−∫
50
40
)
(
exp
dt
t
λ
 = e−.35 
 
 
(b) e−1.21 
 
36. 
(a) 1 − F(2) = 








−∫
2
0
3
exp
dt
t
 = e−4 
 
 
(b) exp[−(.4)4/4] − exp[−(1.4)4/4] 
 
 
(c) 








−∫
2
1
3
exp
dt
t
 = e−15/4 
 
37. 
(a) P{X > 1/2} = P{X  > 1/2} + P{X < −1/2} = 1/2 
 
 
(b) P{X ≤ a} = P{−a ≤ X ≤ a} = a, 0 < a < 1.  Therefore, 
 
 
 
1
)
(
=
a
f X
, 0 < a < 1 
 
That is, X is uniform on (0, 1). 

Chapter 5 
 
71 
38. 
For both roots to be real the discriminant (4Y)2 − 44(Y + 2) must be ≥ 0.  That is, we need that 
Y2 ≥ Y + 2.  Now in the interval 0 < Y < 5. 
 
 
 
 
Y2 ≥ Y + 2 ⇔ Y ≥ 2  and so 
 
 
 
P{Y2 ≥ Y + 2} = P{Y ≥ 2} = 3/5. 
 
39. 
FY(y) = P{log X ≤ y} 
 
 
  = P{X ≤ ey} = FX(ey) 
 
 
fY(y) = fX(ey)ey = 
y
e
ye
e
−
 
 
40. 
FY(y) = P{eX ≤ y} 
 
 
 
= FX(log y) 
 
 
fY(y) = 
y
y
y
f X
1
1
)
(log
=
, 1 < y < e 
 

72 
 
Chapter 5 
Theoretical Exercises 
 
1. 
The integration by parts formula ∫
∫
−
=
vdu
uv
udv
 with dv = 
2
2
bx
bxe−
−
, u = −x/2b yields 
that 
 
 
 
 
∫
∫
∫
∞
∞
−
−
∞
−
+
−
=
0
0
0
2
2
2
2
2
1
2
dx
e
b
b
xe
dx
e
x
bx
bx
bx
 
 
 
 
 
      = 
∫
∞
−
0
2
/
2
/
3
2
)
2
(
1
dy
e
b
y
 by y = 
b
x 2  
 
 
 
 
      = 
2
/
3
2
/
3
4
)
2
(
1
2
2
b
b
π
π
=
 
 
 
where the above uses that 
∫
∞
−
0
2
/
2
2
1
dy
e y
π
 = 1/2.  Hence, a = 
π
2
/
3
4b
 
 
2. 
∫
∞
−
<
0
}
{
dy
y
Y
P
  = 
dy
dx
x
f
y
Y
∫∫
∞−
∞
−
0
)
(
 
 
 
 
 
= 
∫
∫∫
∞
−
∞
−
−
−
=
0
0
0
)
(
)
(
x
xf
dx
dy
x
f
Y
x
Y
 dx 
 
 
Similarly, 
 
 
 
∫
∫
∞
∞
=
>
0
0
)
(
}
{
dx
x
xf
dy
y
Y
P
Y
 
 
Subtracting these equalities gives the result. 
 
4. 
E[aX + b] = ∫
+
dx
x
f
b
ax
)
(
)
(
 = ∫
∫
+
dx
x
f
b
dx
x
xf
a
)
(
)
(
 
 
 
 
 
 
           = aE[X] + b 
 
5. 
E[Xn] = ∫
∞
>
0
}
{
dt
t
X
P
n
 
 
 
   = 
dx
nx
x
X
P
n
n
n
∫
∞
−
>
0
1
}
{
 by t = xn, dt = nxn−1dx 
 
 
   = 
dx
nx
x
X
P
n 1
0
}
{
−
∞
∫
>
 
 
6. 
Let X be uniform on (0, 1) and define Ea to be the event that X is unequal to a.  Since 
a
a E
∩
 is 
the empty set, it must have probability 0. 
 

Chapter 5 
 
73 
7. 
SD(aX + b) = 
σ
σ
a
a
b
aX
=
=
+
2
2
)
(
Var
 
 
 
8. 
Since 0 ≤ X ≤ c, it follows that X2 ≤ cX.  Hence, 
 
 
 
Var(X) = E[X2] −(E[X])2 
 
 
 
     ≤ E[cX − (E[X])2 
 
 
 
     = cE[X] − (E[X])2 
 
 
 
     = E[X](c − E[X]) 
 
 
 
     = c2[α(1 − α)]   where α = E[X]/c 
 
 
 
     ≤ c2/4 
 
 
where the last inequality first uses the hypothesis that P{0 ≤ X ≤ c} = 1 to calculate that 0 ≤ α 
≤ 1 and then uses calculus to show that 
1
0
maximum
≤
≤α
α(1 − α) = 1/4. 
 
9. 
The final step of parts (a) and (b) use that −Z is also a standard normal random variable. 
 
 
(a) P{Z > x} = P{−Z < −x} = P{Z < −x} 
 
 
 
(b) P{Z > x} = P{Z > x} + P{Z < −x} = P{Z > x} + P{−Z > x} 
 
 
 
 
 
 
 
  = 2P{Z > x} 
 
 
(c) P{Z< x} = 1 − P{Z > x} = 1 − 2P{Z > x} by (b) 
 
 
 
 
 
 
    = 1 − 2(1 − P{Z < x}) 
 
10. 
With c = (
)
σ
π
2
/
1
 we have 
 
 
f(x) = 
2
2
2
/
)
(
σ
µ
−
−x
ce
 
 
 
f ′(x) = 
2
2
/
)
(
/)
(
2
2
σ
µ
σ
µ
−
−
−
−
x
ce
x
 
 
 
f ′′(x) = 
2
2
2
2
2
/
)
(
2
2
2
/
)
(
4
)
(
σ
µ
σ
µ
σ
µ
σ
−
−
−
−
−
−
−
−
x
x
e
c
x
e
c
 
 
Therefore, 
 
 
f ′′(µ + σ) = f′′(µ − σ) = cσ−2e−1/2 − cσ−2e−1/2 = 0 
 
11. 
E[X2] = 
2
0
0
1
2
/
2
]
[
2
2
2
}
{
λ
λ
λ
=
=
=
>
∫
∫
∞
−
∞
−
X
E
dx
xe
dx
x
x
X
P
x
 
 
12. 
(a) 
2
a
b +
 
 
 
(b) µ 
 
 
(c) 1 − e−λm = 1/2 or m = λ
1  log 2 
 
13. 
(a) all values in (a, b) 
 
 
(b) µ 
 
 
(c) 0 

74 
 
Chapter 5 
14. 
P{cX < x} = P{X < x/c} = 1 − e−λx/c 
 
15. 
λ(t) = 
( )
t
a
a
t
a
a
t
F
t
f
−
=
−
=
1
/)
(
/
1
)
(
, 0 < t < a 
 
16. 
If X has distribution function F and density f, then for a > 0 
 
 
 
 
FaX(t) = P{aX ≤ t} = F(t/a) 
 
 
and 
 
 
 
 
fax = 
)
/
(
1
a
t
f
a
 
 
 
Thus,  
 
 
 
)
/
(
1
)
/
(
1
)
/
(
1
)
(
a
t
a
a
t
F
a
t
f
a
t
X
aX
λ
λ
=
−
=
. 
 
18. 
E[Xk] = ∫
∞
−
0
dx
e
x
x
k
λ
λ
 = 
∫
∞
−
−
0
)
(
dx
x
e
k
x
k
λ
λ
λ
λ
 
 
 
 
 
        = λ−k Γ(k+1) = k!/λk 
 
19. 
E[Xk] = 
∫
∞
−
−
Γ
0
1
)
(
)
(
1
dx
x
e
x
t
t
x
k
λ
λ
λ
 
 
 
   = 
∫
∞
−
+
−
−
Γ
0
1
)
(
)
(
dx
x
e
t
k
t
x
k
λ
λ
λ
λ
 
 
 
   = 
)
(
)
(
k
t
t
k
+
Γ
Γ
−
λ
 
 
 
Therefore,  
 
 
 
 
E[X] = t/λ, 
 
 
 
E[X2] = = λ−2Γ(t + 2)/Γ(t) = (t + 1)t/λ2 
 
 
and thus 
 
 
 
 
Var(X) = (t + 1)t/λ2 − t2/λ2 = t/λ2 
 

Chapter 5 
 
75 
20. 
Γ(1/2) = ∫
∞
−
−
0
2
/
1 dx
x
e x
 
 
 
    = 
∫
∞
−
0
2
/
2
2
dy
e
y
 by  x = y2/2, dx = ydy = 
x
2
 dy 
 
 
    = 
∫
∞
−
−
0
2
/
2
/
1
2
)
2
(
2
dy
e y
π
π
 
 
 
    = 
}
0
{
2
>
Z
P
π
 where Z is a standard normal 
 
 
    = π  
 
21. 
1/λ(s) = ∫
≥
−
−
−
−
s
x
t
s
t
x
s
e
dx
x
e
1
1
)
(
/
)
(
λ
λ
λ
λ
λ
λ
 
 
 
    = ∫
≥
−
−
−
s
x
t
s
x
dx
s
x
e
1
)
(
)
/
(
λ
 
 
 
    = ∫
≥
−
−
+
0
1
)
/
1(
y
t
y
dy
s
y
e λ
 by letting y = x − s 
 
 
As the above, equal to the inverse of the hazard rate function, is clearly decreasing in s when  
 
t ≥ 1 and increasing when t ≤ 1 the result follows. 
 
22. 
λ(s) = c(s − v)β−1, s > v which is clearly increasing when β ≥ 1 and decreasing otherwise. 
 
23. 
F(α) = 1 − e−1 
 
24. 
Suppose X is Weibull with parameters v, α, β.  Then 
 
 
 


≤
−
=






≤






−
β
β
α
α
/
1x
v
X
P
x
v
X
P
 
 
 
 
= P{X ≤ v + αx1/β} 
 
 
 
= 1 − exp{−x}. 
 
25. 
We use Equation (6.3). 
 
 
E[X] = B(a + 1, b)/B(A, b) = 
b
a
a
a
b
a
b
a
a
+
=
Γ
+
Γ
+
+
Γ
+
Γ
)
(
)
(
)1
(
)1
(
 
 
 
E[X2] = B(a + 2, b)/B(a, b) = 
)
)(
1
(
)1
(
)
(
)
(
)
2
(
)
2
(
b
a
b
a
a
a
a
b
a
b
a
a
+
+
+
+
=
Γ
+
Γ
+
+
Γ
+
Γ
 
 
Thus, 
 
 
 
 
Var(X) = 
2
2
2
)
)(
1
(
)
(
)
)(
1
(
)1
(
b
a
b
a
ab
b
a
a
b
a
b
a
a
a
+
+
+
=
+
−
+
+
+
+
 
 
26. 
(X − a)/(b − a) 
 
 

76 
 
Chapter 5 
28. 
P{F(X ≤ x} = P{X ≤ F−1(x)} 
 
 
 
       = F(F−1(x)) 
 
 
 
       = x 
 
29. 
FY(x) = P{aX + b ≤ x} 
 
 
   = 


−
≤
a
b
x
X
P
  when a > 0 
 
 
   = FX((x − b)/a)  when a > 0. 
 
 
fY(x) = a
1 fX((x − b)/a)   if a > 0. 
 
 
When a< 0, FY(x) =






−
−
=


−
≥
a
b
x
F
a
b
x
X
P
X
1
 and so 
 
fY(x) = 






−
−
a
b
x
f
a
X
1
. 
 
30. 
FY(x) = P{eX ≤ x} 
 
 
  = P{X ≤ log x} 
 
 
  FX(log x) 
 
 
fY(x) = fX(log x)/x 
 
 
 
  = 
2
2
2
/
)
(log
2
1
σ
µ
σ
π
−
−
x
e
x
 
 
 

Chapter 6 
 
77 
Chapter 6 
 
Problems 
 
2. 
(a) p(0, 0) = 
12
13
7
8
⋅
⋅
 = 14/39, 
 
 
p(0, 1) = p(1, 0) = 
12
13
5
8
⋅
⋅
 = 10/39 
 
 
p(1, 1) = 
12
13
4
5
⋅
⋅
 = 5/39 
 
 
(b) p(0, 0, 0) = 
11
12
13
6
7
8
⋅
⋅
⋅
⋅
 = 28/143 
 
 
p(0, 0, 1) = p(0, 1, 0) = p(1, 0, 0) = 
11
12
13
5
7
8
⋅
⋅
⋅
⋅
 = 70/429 
 
 
p(0, 1, 1) = p(1, 0, 1) = p(1, 1, 0) = 
11
12
13
4
5
8
⋅
⋅
⋅
⋅
 = 40/429 
 
 
p(1, 1, 1) = 
11
12
13
3
4
5
⋅
⋅
⋅
⋅
 = 5/143 
 
3. 
(a) p(0, 0) = (10/13)(9/12) = 15/26 
 
 
p(0, 1) = p(1, 0) = (10/13)(3/12) = 5/26 
 
 
p(1, 1) = (3/13)(2/12) = 1/26 
 
 
(b) p(0, 0, 0) = (10/13)(9/12)(8/11) = 60/143 
 
 
p(0, 0, 1) = p(0, 1, 0) = p(1, 0, 0) = (10/13)(9/12)(3/11) = 45/286 
 
 
p(i, j, k) = (3/13)(2/12)(10/11) = 5/143 
if i + j + k = 2 
 
 
p(1, 1, 1) = (3/13)(2/12)(1/11) = 1/286 
 
4. 
(a) p(0, 0) = (8/13)2, p(0, 1) = p(1, 0) = (5/13)(8/13),  p(1, 1) = (5/13)2 
 
 
(b) p(0, 0, 0) = (8/13)3 
 
 
p(i, j, k) = (8/13)2(5/13) if i + j + k = 1 
 
 
p(i, j, k) = (8/13)(5/13)2 if i + j + k = 2 
 
5. 
p(0, 0) = (12/13)3(11/12)3 
 
p(0, 1) = p(1, 0) = (12/13)3[1 − (11/12)3] 
 
p(1, 1) = (2/13)[(1/13) + (12.13)(1/13)] + (11/13)(2/13)(1/13) 
 

78 
 
Chapter 6 
8. 
fY(y) = ∫
−
−
−
y
y
ydx
e
x
y
c
)
(
2
2
 
 
 
= 
y
e
cy
−
3
3
4
, −0 < y < ∞ 
 
 
 
∫
∞
0
)
(
dy
y
fY
= 1 ⇒ c= 1/8 and so fY(y) = 
6
3
y
e
y
−
, 0 < y < ∞ 
 
 
fX(x) = ∫
∞
−
−
x
ydy
e
x
y
)
(
8
1
2
2
 
 
 
 = 
)
1(
4
1
x
e
x
+
−
 upon using ∫
−
−
−
−
+
+
=
−
y
y
y
y
e
ye
e
y
e
y
2
2
2
2
 
 
9. 
(b) fX(x) = 
)
2
(
7
6
2
7
6
2
2
0
2
x
x
dy
xy
x
+
=






+
∫
 
 
 
(c) P{X > Y} = 
56
15
2
7
6
1
0 0
2
=






+
∫∫
x
dydx
xy
x
 
 
(d) P{Y > 1/2X < 1/2} = P{Y > 1/2, X < 1/2}/P{X < 1/2} 
 
 
 
 
 
 
= 
∫
∫∫
+






+
2
/
1
0
2
2
2
/
1
2
/
1
0
2
)
2
(
2
dx
x
x
dxdy
xy
x
 
 
10. 
(a) fX(x) = e−x , fY(y) = e−y, 0 < x < ∞, 0 < y < ∞ 
 
 
 
P{X < Y} = 1/2 
 
 
(b) P{X < a} = 1 − e−a 
 
11. 
!
2
!1
!
2
!
5
(.45)2(.15)(.40)2 
 
12. 
e−5 + 5e−5 + 
5
3
5
2
!
3
5
!
2
5
−
−+
e
e
 
 

Chapter 6 
 
79 
14. 
Let X and Y denoted respectively the locations of the ambulance and the accident of the 
moment the accident occurs. 
 
 
 
P{Y − X < a} = P{Y < X < Y + a} + P{X < Y < X + a} 
 
 
 
 
 
        = 
∫
∫
+
L
L
a
y
y
dxdy
L
0
)
,
min(
2
2
 
 
 
 
 
        = 








+
∫∫
∫∫
−
+
−
a
L
a
y
y
L
a
L
L
y
dxdy
dxdy
L
0
2
2
 
 
 
 
 
        = 1 − 






−
=
−
+
−
L
a
L
a
a
L
L
a
L
a
L
2
)
(
2
,  0 < a < L 
 
15. 
(a) 1 = ∫∫
∫∫
∈
=
)
,
(
)
,
(
y
x
R
dydx
c
dydx
y
x
f
 = cA(R) 
 
 
where A(R) is the area of the region R. 
 
 
(b) f(x, y) = 1/4, −1 ≤ x, y ≤ 1 
 
 
           = f(x)f(y) 
 
 
where f(v) = 1/2, −1 ≤ v ≤ 1. 
 
 
(c) P{X 2 + Y 2 ≤ 1} = ∫∫
c
dydx
4
1
 = (area of circle)/4 = π/4. 
 
16. 
(a) A = ∪Ai, 
 
 
(b) yes 
 
(c) P(A) = ∑
)
(
iA
P
 = n(1/2)n−1 
 
17. 
3
1  since each of the 3 points is equally likely to be the middle one. 
 
18. 
P{Y − X > L/3} = ∫∫
>
−x
y
L
dydx
L
3
/
2
4
 
 
 
 
 
   
L
y
L
<
<
2
 
 
 
 
 
   0 < x < 2
L  
 
 
 
 
= 








+ ∫∫
∫∫
+
2
/
6
/
3
/
6
/
0
2
/
2
4
L
L
L
L
x
L
L
L
dydx
dydx
L
 
 
 
 
 
= 






−
+
72
7
24
5
12
4
2
2
2
2
L
L
L
L
 = 7/9 
 

80 
 
Chapter 6 
19. 
1
1
0
0
0
1 
x
dydx
dx
x
=
∫∫
∫
 = 1 
 
 
(a) 
1 1
ln( )
y
dx
y
x
= −
∫
, 0 < y < 1 
 
 
(b) 
0
1
x
dy
x
∫
 = 1,   0 < y < 1 
 
 
 
 
 
(c) 1
2  
 
 
(d) Integrating by parts gives that  
 
 
 
1
1
0
0
 ln( )
1
( ln( )
)
y
y dy
y
y
y dy
= −−
−
∫
∫
 
 
 
 
yielding the result 
 
 
 
E[Y] = 
1
0
ln( )
y
y dy
−∫
 = 1/4 
 
20. 
(a) yes:  fX(x) = xe−x, fY(y) = e−y, 0 < x< ∞, 0 < y < ∞ 
 
 
(b) no:  fX(x) = 
)
1(
2
)
,
(
1
x
dy
y
x
f
x
−
=
∫
, 0 < x < 1 
 
 
 
       fY(y) = 
y
dx
y
x
f
y
2
)
,
(
0
=
∫
, 0 < y < 1 
 
21. 
(a) We must show that ∫∫
∞
∞
−
∞
∞
−
dxdy
y
x
f
)
,
(
 = 1.  Now, 
 
 
 
 
∫∫
∞
∞
−
∞
∞
−
dxdy
y
x
f
)
,
(
  
= ∫∫
−
1
0
1
0
24
y
dxdy
xy
 
 
 
 
 
 
 
= ∫
−
1
0
2)
1(
12
dy
y
y
 
 
 
 
 
 
 
= ∫
+
−
1
0
3
2
)
2
(
12
dy
y
y
y
 
 
 
 
 
 
 
= 12(1/2 − 2/3 + 1/4) = 1 
 
 
(b)  
E[X] = ∫
1
0
)
(
dx
x
xfX
 
 
 
 
 
  = 
∫
∫
−x
dydx
xy
x
1
0
1
0
24
 
 
 
 
 
  = ∫
−
1
0
2
2
)
1(
12
dx
x
x
 = 2/5 
 
 
(c) 2/5 

Chapter 6 
 
81 
22. 
(a) No, since the joint density does not factor. 
 
 
(b) fX(x) = ∫
+
1
0
)
(
dy
y
x
 = x + 1/2,  0 < x < 1. 
 
 
(c) P{X + Y < 1} = ∫∫
−
+
1
0
1
0
)
(
x
dydx
y
x
 
 
 
 
 
   = ∫
−
+
−
1
0
2
]
2
/
)
1(
)
1(
[
dx
x
x
x
 = 1/3 
 
23. 
(a) yes 
 
 
 
fX(x) = 12x(1 − x)
),
1(
6
1
0
x
x
ydy
−
=
∫
 0 < x < 1 
 
 
 
fY(y) = 12y∫
−
1
0
)
1(
dx
x
x
 = 2y, 0 < y < 1 
 
 
(b) E[X] = 
dx
x
x
)
1(
6
1
0
2
−
∫
 = 1/2 
 
(c) E[Y] = ∫
1
0
2
2
dy
y
 = 2/3 
 
(d) Var(X) = ∫
−
−
1
0
3
4
/
1
)
1(
6
dx
x
x
 = 1/20 
 
(e) Var(Y) = 
9
/
4
2
1
0
3
−
∫
dy
y
 = 1/18 
 
24. 
P{N = n} = 
)
1(
0
1
0
p
pn
−
−
 
 
(b) P{X = j} = pj/(1 − p0) 
 
(c) P{N = n, X = j} = 
j
n
p
p
1
0
−
 
 
25. 
!
1
i
e−
 by the Poisson approximation to the binomial. 
 
26. 
(a) FA,B,C(a, b, c) = abc   0 < a, b, c < 1 
 
 
(b) The roots will be real if B2 ≥ 4AC.  Now 
 
 
P{AC ≤ x} = 
∫∫
∫∫
∫∫
+
=
≤
≤
≤
≤
≤
1
/
0
0
1
0
0
0
1
1
/
x
a
x
x
c
c
a
a
x
dcda
dcda
dadc
 
 
 
 
     = x − x log x. 
 
 
 
Hence, FAC(x) = x − x log x and so 
 
 
 
fAC(x) = − log x , 0 < x < 1 
 
 

82 
 
Chapter 6 
 
 
P{B2/4 ≥ AC} = ∫∫
−
1
0
4
/
0
2
log
b
xdxdb  
 
 
 
 
    = 
db
b
b
b
∫






−
1
0
2
2
2
)
4
/
log(
4
4
 
 
 
 
 
    = 
36
5
6
2
log
+
 
 
 
 
where the above uses the identity 
 
 
 
 
 
∫
−
=
9
3
log
log
3
3
2
x
x
x
xdx
x
. 
27. 
(a) P{X + Y ≤ a} = ∫∫
−
−
a
x
a
ydydx
e
0
0
 = a − 1 + e−a, a < 1 
 
 
 
 
   = ∫∫
−
−
1
0
0
x
a
ydydx
e
 = 1 − e−a(e − 1), a > 1 
 
 
(b) P{Y > X/a} = ∫∫
∞
−
1
0
/ a
x
ydydx
e
 = a(1 − e−1/a) 
 
28. 
P{X1/X2 < a} = ∫∫
∞
−
−
0 0
2
1
2
1
ay
y
x
dxdy
e
e
λ
λ
λ
λ
 
 
 
 
  = (
)
dy
e
e
y
ay
2
1
2
0
1
λ
λ
λ
−
∞
−
∫
−
 
 
 
 
  = 1 − 
2
1
1
1
2
2
λ
λ
λ
λ
λ
λ
+
=
+
a
a
a
 
 
 
P{X1/X2 < 1} = 
2
1
1
λ
λ
λ
+
 
 
29. 
P{I2R ≤ w} = ∫∫
≤
≤
≤
−
0
0
1
1
2
2
)
1(
6
x
y
x
w
y
ydydx
x
x
 
 
 
 
 
= 
∫∫
∫∫
−
+
−
1
/
0
0
1
0
2
)
1(
12
)
1(
12
w
x
w
w
ydydx
x
x
ydydx
x
x
 
 
 
 
= 3w − 2w3/2 = 6w(1 + (log w)/2 − 
w ) 
 
 
 
= 4w3/2 − 3w(1 + log w), 0 < w < 1 
 

Chapter 6 
 
83 
30. 
(a) e−2 
 
 
(b) 1 − e−2 − 2e−2 = 1 − 3e−2 
 
 
The number of typographical errors on each page should approximately be Poisson 
distributed and the sum of independent Poisson random variables is also a Poisson 
random variable. 
 
31. 
(a) 1 − e−2.2 − 2.2e−2.2 − e−2.2(2.2)2/2! 
 
 
 
(b) 1 − ∑
=
−
4
0
4.4
!
/
)
4.4
(
i
i i
e
,  
(c) 
1 − ∑
=
−
5
0
6.6
!
/
)
6.6
(
i
i i
e
 
 
 
 
The reasoning is the same as in Problem 26. 
 
32. 
(a) If W = X1 + X2 is the sales over the next two weeks, then W is normal with mean 4,400 
and standard deviation 
2)
230
(
2
 = 325.27.  Hence, with Z being a standard normal, we 
have 
 
 
 
 
P{W > 5000} = 


−
>
27
.
325
4400
5000
Z
P
 
 
 
 
 
 
   = P{Z > 1.8446} = .0326 
 
 
(b) P{X > 2000} = P{Z > (2000 − 2200)/230} 
 
 
 
 
  = P{Z > −.87} = P{Z < .87} = .8078 
 
 
 
Hence, the probability that weekly sales exceeds 2000 in at least 2 of the next 3 weeks  
 
 
p3 + 3p2(1 − p) where p = .8078. 
 
 
 
We have assumed that the weekly sales are independent. 
 
33. 
Let X denote Jill’s score and let Y be Jack’s score.  Also, let Z denote a standard normal 
random variable. 
 
 
(a) P{Y > X} = P{Y − X > 0} 
 
 
 
   ≈ P{Y − X > .5} 
 
 
 
   = 






+
−
−
>
+
−
−
−
2
2
2
2
)
15
(
)
20
(
)
170
160
(
5.
)
15
(
)
20
(
)
170
160
(
X
Y
P
 
 
 
 
   ≈ P{Z > .42} ≈ .3372 
 
 
(b) P{X + Y > 350} = P{X + Y > 350.5} 
 
 
 
 
       = 






+
>
+
−
+
2
2
2
2
)
15
(
)
20
(
5.
20
)
15
(
)
20
(
330
Y
X
P
 
 
 
 
 
       ≈ P{Z > .82} ≈ .2061 
 

84 
 
Chapter 6 
34. 
Let X and Y denote, respectively, the number of males and females in the sample that never 
eat breakfast.  Since 
 
 
 
 
E[X] = 50.4, Var(X) = 37.6992,  E[Y] = 47.2, Var(Y) = 36.0608 
 
 
it follows from the normal approximation to the binomial that  is approximately distributed as 
a normal random variable with mean 50.4 and variance 37.6992, and that Y is approximately 
distributed as a normal random variable with mean 47.2 and variance 36.0608.  Let Z be a 
standard normal random variable. 
 
 
(a) P{X + Y ≥ 110} = P{X + Y ≥ 109.5} 
 
 
 
 
       = 






−
≥
−
+
76
.
73
6.
97
5.
109
76
.
73
6.
97
Y
X
P
 
 
 
 
 
       ≈ P{Z > 1.3856} ≈ .0829 
 
 
(b) P{Y ≥ X} = P{Y − X ≥ −.5} 
 
 
 
   = 






−
−
−
≥
−
−
−
76
.
73
)
2.3
(
5.
76
.
73
)
2.3
(
X
Y
P
 
 
 
 
   ≈ P{Z ≥ .3144} ≈ .3766 
 
35. 
(a) P{X1 = 1X2 = 1} = 4/12 = 1 − P{X1 = 0X2 = 1} 
 
 
(b) P{X1 = 1X2 = 0} = 5/12 = 1 − P{X1 = 0X2 = 0} 
 
36. 
(a) P{X1 = 1X2 = 1} = 5/13 = 1 − P{X1 = 0X2 = 1} 
 
 
(b) same as in (a) 
 
37. 
(a) P{Y1 = 1Y2 = 1} = 2/12 = 1 − P{Y1 = 0Y2 = 1} 
 
 
(b) P{Y1 = 1Y2 = 0} = 3/12 = 1 − P{Y1 = 0Y2 = 0} 
 
38. 
(a) P{Y1 = 1Y2 = 1} = p(1, 1)/[1 − (12/13)3] = 1 − P{Y1 = 0Y2 = 1} 
 
 
(b) P{Y1 = 1Y2 = 0} = p(1, 0)/(12/13)3 = 1 − P{Y1 = 0Y2 = 0} 
 
 
where p(1, 1) and p(1, 0) are given in the solution to Problem 5. 
 
39. 
(a) P{X = j, Y = i} = 
j
1
5
1
, j = 1, …, j, i = 1, …, j 
 
 
(b) P{X = jY = i} = 
,
/
1
1
5
/
1
5
1
5
5
∑
∑
=
=
=
i
k
i
k
k
j
k
j
 5 ≥ j ≥ i. 
 
 
(c) No. 
 

Chapter 6 
 
85 
40. 
For j = i:  P{Y = iX = i} = 
}
{
36
1
}
{
}
,
{
i
X
P
i
X
P
i
X
i
Y
P
=
=
=
=
=
 
 
For j < i:  P{Y = jX = i} = 
}
{
36
2
i
X
P
=
 
 
Hence 
 
 
 
1 = 
}
{
36
1
}
{
36
)1
(
2
}
{
1
i
X
P
i
X
P
i
i
X
j
Y
P
i
j
=
+
=
−
=
=
=
∑
=
 
 
and so, P{X = i} = 36
1
2 −
i
 and 
 
 
 
P{Y = jX = i} = 





−
−
1
2
2
2
1
i
i
i
    
i
j
i
j
<
=
 
 
42. 
(a) fXY(xy) = 
∫
+
−
+
−
dx
xe
xe
y
x
y
x
)
1
(
)
1
(
 = (y + 1)2xe−x(y+1), 0 < x 
 
 
(b) fYX(yx) = 
∫
+
−
+
−
dy
xe
xe
y
x
y
x
)1
(
)1
(
 = xe−xy, 0 < y 
 
 
 
P{XY < a}  = ∫∫
∞
+
−
0
/
0
)
1
(
x
a
y
x
dydx
xe
 
 
 
 
 
= ∫
∞
−
−
−
0
)
1(
dx
e
e
x
a
 = 1 − e−a 
 
 
 
fXY(a) = e−a , 0 < a 
 
43. 
fYX(yx) = 
∫−
−
−
−
−
x
x
x
x
dx
e
y
x
e
y
x
)
(
)
(
2
2
2
2
 
 
 
         = 
)
(
4
3
2
2
3
y
x
x
−
,  −x < y < x 
 
 
FYX(yx)  = 
∫−
−
y
x
dy
y
x
x
)
(
4
3
2
2
3
 
 
 
           = 
),
3
/
2
3
/
(
4
3
3
3
2
3
x
y
y
x
x
+
−
 −x < y < x 
 
 

86 
 
Chapter 6 
44. 
f(λn) = 
}
{
)
(
}
{
n
N
P
g
n
N
P
=
=
λ
λ
 
 
 
     = C1e−λλnαe−αλ(αλ)s−1 
 
 
     = C2e−(α+1)λλn+s−1 
 
 
where C1 and C2 do not depend on λ.  But from the preceding we can conclude that the 
conditional density is the gamma density with parameters α + 1 and n + s.  The conditional 
expected number of accidents that the insured will have next year is just the expectation of 
this distribution, and is thus equal to (n + s)/(α + 1). 
 
45. 
P{X1 > X2 + X3} + P{X2 > X1 + X3} + P{X3 > X1 + X2} 
 
 
 
= 3P{X1 > X2 + X3} 
 
 
      
3,2
,1
1
0
3
2
1
3
2
1
3
=
≤
≤
>
>
∫∫∫
=
i
ix
x
x
x
dx
dx
dx
(take a = 0, b = 1) 
 
 
 
= 
∫∫
∫∫∫
−
−
+
−
−
=
1
0
1
0
3
2
3
2
1
0
1
0
1
3
2
1
3
3
3
2
)
1(
3
3
x
x
x
x
dx
dx
x
x
dx
dx
dx
 
 
 
= 
2
/
1
2
)
1(
3
3
1
0
2
3
=
−
∫
dx
x
. 
 
46. 
2
2
0
!
2
!
2
!
5
)
(
)
3
(
















=
∫
∫
∞
−
−
−
x
x
x
x
x
X
dx
xe
xe
dx
xe
x
f
 
 
 
       = 30(x + 1)2e−2xxe−x[1 − e−x(x + 1)]2 
 
47. 
3
2 





−
L
d
L
 
 
48. 
∫
∫
−
=
4
/
3
4
/
1
2
2
4
/
3
4
/
1
)
1(
!
2
!
2
!
5
)
(
)
3
(
dx
x
x
dx
x
f X
 
 
49. 
(a) P{min Xi ≤ a} = 1 − P{min Xi > a} = 1 − ∏
−
−
=
>
a
i
e
a
X
P
λ
5
1
}
{
 
 
 
(b) P{max Xi ≤ a} = ∏
−
−
=
≤
5)
1(
}
{
a
i
e
a
X
P
λ
 
 
50. 
)
,
(
)
4
(
)
1
( ,
y
x
f
X
X
 = 
y
zdz
x
Y
X
2
2
2
!
2
!
4
2







∫
,  x < y 
 
 
 
    = 48xy(y2 − x2). 
 

Chapter 6 
 
87 
 
P(X(4) − X(1) ≤ a} = ∫∫
−
+
−
a
x
a
dydx
x
y
xy
1
0
0
2
2
)
(
48
 
 
 
 
 
  + ∫∫
−
−
1
1
1
0
2
2
)
(
48
a
dydx
x
y
xy
 
 
51. 
π
π
θ
2
1
2
)
,
(
1
r
r
r
fR
=
=
, 0 ≤ r ≤ 1, 0 ≤ θ < 2π. 
 
 
Hence, R  and θ are independent with θ being uniformly distributed on (0, 2π) and R having 
density fR(r) = 2r, 0 < r < 1. 
 
52. 
fR,θ(r,θ) = r,   0 < r sin θ < 1,   0 < r cos θ < 1,   0 < θ < π/2,   0 < r < 
2  
 
53. 
J = 
u
z
u
z
u
z
u
x
cos
2
sin
2
2
sin
2
1
2
cos
2
1
2
/
1
2
/
1
−
−
−
= cos2 u + sin2 u = 1 
 
 
fu,z(u, z) - 
z
e−
π
2
1
.  But x2 + y2 = 2z so 
 
 
fX,Y(x, y) = 
2
/
)
(
2
2
2
1
y
x
e
+
−
π
 
 
54. 
(a) If u = xy, v = xy, then J = 
2
1
y
x
y
x
y
−
 = 
y
x
2
−
 and  
 
 
y = 
v
u /
, x = 
vu .  Hence, 
 
 
(b) fu,v(u, v) = 
(
)
2
,
2
1
/
,
2
1
vu
v
u
vy
f
v
Y
X
=
, u ≥ 1, u
1  < v < u 
 
 
 
fu(u) = 
u
u
dv
vu
u
u
log
1
2
1
2
/
1
2
=
∫
, u ≥ 1. 
 
 
 
For v > 1 
 
 
 
 
fV(v) = ∫
∞
=
v
v
du
vu
2
2
2
1
2
1
, v > 1 
 
 
For v < 1 
 
 
 
 
fV(v) = 
2
1
2
1
2
/
1
2
=
∫
∞
du
vu
, 0 < v < 1. 
 

88 
 
Chapter 6 
55. 
(a) u = x + y, v = x/y ⇒ y = 
1
+
v
u
, x = 
1
+
v
uv  
 
 
 
J = 
u
v
y
x
y
y
y
x
y
x
y
2
2
2
2
)1
(
)
(
1
1
/
/
1
1
1
+
−
=
+
−
=






+
−
=
−
 
 
 
 
fu,v (u, v) = 
2)1
( +
v
u
, 0 < uv < 1 + v, 0 < u < 1 + v 
 
57. 
y1 = x1 + x2, y2 = 
1xe .  J  = 
0
1
1
1xe
 = 
1xe
−
 = −y2 
 
 
x1 = log y2, x2 = y1 − log y2 
 
 
 
)
log
(
log
2
2
1
,
2
1
2
2
1
1
)
,
(
y
y
y
Y
Y
e
e
y
y
y
f
−
−
−
=
λ
λ
λ
λ
 
 
 
 
 
1
2
2
1
y
e
y
λ
λ
−
=
, 1 ≤ y2, y1 ≥ log y2 
 
58. 
u = x + y,  v = x + z,  w = y + z ⇒ z = 
2
,
2
,
2
u
v
w
y
u
w
v
x
u
w
v
+
−
=
+
−
=
−
+
 
 
 
 
 
J = 
1
1
0
1
0
1
0
1
1
 = −2 
 
 
f(u, v, w) = 


+
+
−
)
(
2
1
exp
2
1
w
v
u
, u + v > w,  u + w > v,  v + w + u   
 
59. 
P(Yj = ij, j = 1, …, k + 1} = P{Yj = ij, j = 1, …, k} P(Yk+1 = ik+1Yj = ij, j = 1, …, k} 
 
 
 
 
 
  = 
}
,...,
1
,
1
{
!
!)
(!
1
1
k
j
i
Y
i
Y
n
P
n
k
n
k
j
j
k
k
i
i
=
=
=
−
+
−
+
=∑
 
 
 
 
 
 
      k!(n − k)!/n!,  if ∑
+
=
+
=
1
1
1
k
j
j
n
i
 
 
 
 
 
 
  = 
 
 
 
 
 
      0, otherwise 
 
 
Thus, the joint mass function is symmetric, which proves the result. 
 
60. 
The joint mass function is  
 
 
 
P{Xi = xi, i = 1, …, n} = 1/




k
n , xi ∈ {0, 1}, i = 1, …, n, ∑
=
=
n
i
i
k
x
1
 
 
 
As this is symmetric in x1, …, xn the result follows. 
 

Chapter 6 
 
89 
Theoretical Exercises 
 
1. 
P{X ≤ a2,  Y ≤ b2} = P{a1 < X ≤ a2,  b1 < Y ≤ b2} 
 
 
 
 
      + P{X ≤ a1,  b1 < Y ≤ b2} 
 
 
 
 
      + P{a1 < X ≤ a2,  Y ≤ b1} 
 
 
 
 
      + P{X ≤ a1,  Y ≤ b1}. 
 
 
The above following as the left hand event is the union of the 4 mutually exclusive right hand 
events.  Also, 
 
 
 
P{X ≤ a1, Y ≤ b2} = P{X ≤ a1,  b1 < Y ≤ b2 } 
 
 
 
 
 
+ P{X ≤ a1,  Y ≤ b1} 
 
 
and similarly, 
 
 
 
P{X ≤ a2, Y ≤ b1} = P{a1 ≤ X  ≤ a2, < Y ≤ b1 } 
 
 
 
 
 
+ P{X ≤ a1,  Y ≤ b1}. 
 
 
Hence, from the above 
 
 
 
F(a2, b2) = P{a1 < X ≤ a2, b1 < Y ≤ b2} + F(a1, b2) − F(a1, b1)  
 
 
 
 
+ F(a2, b1) − F(a1, b1) + F(a1, b1). 
 
2. 
Let Xi denote the number of type i events, i= 1, …, n. 
 
 
P{X1 = r1, …, Xn = rn} = 






=
=
∑
events
 
,...,
1`
1
1
n
i
n
n
r
r
X
r
X
P
 
 
 
 
 
 






×
∑
∑
−
n
i
r
r
e
n
i
1
1
λ
λ
! 
 
 
 
 
          = 
!
...
!
!...
!
1
1
1
1
1
1












∑
∑
∑
−
n
i
r
r
n
r
n
n
i
r
e
p
P
r
r
r
n
i
n
λ
λ
 
 
 
 
 
          = ∏
=
−
n
i
i
r
p
P
r
e
i
i
i
1
!
)
(λ
λ
 
 
3. 
Throw a needle on a table, ruled with equidistant parallel lines a distance D apart, a large 
number of times.  Let L, L < D, denote the length of the needle.  Now estimate π by fD
L
2
 
where f is the fraction of times the needle intersects one of the lines. 
 

90 
 
Chapter 6 
5. 
(a) For a > 0 
 
 
 
 
FZ(a) = P{X ≤ aY} 
 
 
 
 
   = ∫∫
∞
0
/
0
)
(
)
(
y
a
Y
X
dxdy
y
f
x
f
 
 
 
 
 
 
   = ∫
∞
0
)
(
)
(
dy
y
f
ay
F
Y
X
 
 
 
 
fZ(a) = ∫
∞
0
)
(
)
(
dy
y
yf
ay
f
Y
X
 
 
 
(b)  
FZ(a) = P{XY < a} 
 
 
 
 
   = ∫∫
∞
0
/
0
)
(
)
(
y
a
Y
X
dxdy
y
f
x
f
 
 
 
 
 
   = ∫
∞
0
)
(
)
/
(
dy
y
f
y
a
F
Y
X
 
 
 
 
fZ(a) = ∫
∞
0
)
(
1
)
/
(
dy
y
f
y
y
a
f
Y
X
 
 
 
 
If X is exponential with rate λ and Y is exponential with rate µ then (a) and (b) reduce to 
 
 
(a) FZ(a) = ∫
−
−
λ
µ
λ
µ
λ
0
dy
e
y
e
y
ay
 
 
 
 
(b) FZ(a) = ∫
∞
−
−
0
/
1
dy
e
y
e
y
y
a
µ
λ
µ
λ
 
 
6. 
Interpret Xi as the number of trials needed after the (i − 1)st  success until the ith success 
occurs, i = 1, …, n, when each trial is independent and results in a success with probability  p.  
Then each Xi is an identically distributed geometric random variable and ∑
=
n
i
i
X
1
, representing 
the number of trials needed to amass n successes, is a negative binomial random variable. 
 
 7. 
(a) P{cX ≤ a} = P{X ≤ a/c} and differentiation yields 
 
 
 
 
fcX(a) = 
)
(
)
/
(
)
/
(
1
1
/
t
c
a
e
c
c
a
f
c
t
c
a
X
Γ
=
−
−
λ
λ
λ
. 
 
 
Hence, cX is gamma with parameters  (t, λ/c). 
 
 
(b) A chi-squared random variable with 2n degrees of freedom can be regarded as being the 
sum of n independent chi-square random variables each with 2 degrees of freedom 
(which by Example is equivalent to an exponential random variable with parameter λ).  
Hence by Proposition 
2
2n
X
 is a gamma random variable with parameters (n, 1/2) and the 
result now follows from part (a). 

Chapter 6 
 
91 
8. 
(a) P{W ≤ t} = 1 − P{W > t} = 1 − P{X > t, Y > t} = 1 − [1 − FX(t)] [1 − FY(t)] 
 
 
(b) fW(t) = fX(t)[1 − FY(t)] + fY(t) [1 − FX(t)] 
 
 
 
Dividing by [1 − FX(t)][1 − FY(t)] now yields 
 
 
 
 
λW(t) = fX(t)/[1 − FX(t)] + fY(t)/[1 − FY(t)] = λX(t) + λY(t) 
 
9. 
P{min(X1, …, Xn) > t} = P{X1 > t, …, Xn > t} 
 
 
 
 
           = e−λt…e−λt  = e−nλt 
 
 
thus showing that the minimum is exponential with rate nλ. 
 
10. 
If we let Xi denote the time between the ith and (i + 1)st failure, i = 0, …, n − 2, then it follows 
from Exercise 9 that the Xi are independent exponentials with rate 2λ.  Hence, ∑
−
=
2
0
n
i
i
X  the 
amount of time the light can operate is gamma distributed with parameters (n − 1, 2λ). 
 
11. 
I = 
5
4
3
2
1
x
x
x
x
x
>
<
>
<
∫∫∫∫∫
 f(x1) … f(x5)dx1…dx5 
 
  = 
5
4
3
2
1
u
u
u
u
u
>
<
>
<
∫∫∫∫∫
 du1 … du5 
by ui = F(xi),  i = 1, …, 5 
 
 
         0 < ui < 1 
 
  = 
5
2
2
...du
du
u
∫∫∫∫
 
 
  = 
2
/)
1(
2
3
u
−
∫∫∫
  du3… 
 
  = 
5
4
3
4
4
2
/]
3
/
[
du
du
u
u −
∫∫
 
 
  = ∫
−
1
0
4
2
2
/]
3
/
[
du
u
u
 = 2/15 
 
12. 
Assume that the joint density factors as shown, and let 
 
 
 
 
Ci = ∫
∞
∞
−
,
)
(
dx
x
gi
 i = 1, …, n 
 
 
Since the n-fold integral of the joint density function is equal to 1, we obtain that 
 
 
 
 
1 = ∏
=
n
i
i
C
1
 
 
 
Integrating the joint density over all xi except xj gives that 
 
 
 
 
j
j
j
j
i
i
j
j
j
X
C
x
g
C
x
g
x
f
j
/)
(
)
(
)
(
=
=
∏
≠
 
 

92 
 
Chapter 6 
 
If follows from the preceding that 
 
 
 
 
f(x1, …, xn) = ∏
=
n
j
j
X
x
f
j
1
)
(
 
 
 
which shows that the random variables are independent. 
 
13. 
No.  Let Xi = 
−
−
0
success
 a 
is
  
 trial
if
1
i
.  Then 
 
 
)
(
}
,...,
{
}
,...,
{
)
,...,
(
,...,
1
1
1
1
x
f
x
x
P
x
X
x
x
P
x
x
x
X
f
X
m
n
m
n
m
n
m
n
X
X
+
+
+
+
=
=
 
 
 
 
 
 
       = 
∑
∑
−
+
−
i
i
x
m
n
x
x
cx
)
1(
 
 
and so given ∑
+m
n
i
X
1
 = n the conditional density is still beta with parameters n + 1, m + 1. 
 
14. 
P{X = iX + Y = n} = P{X = i, Y = n − i}/P{X + Y = n} 
 
 
 
 
 
       = 
2
2
1
1
)
1(
1
1
)
1(
)
1(
−
−
−
−
−




−
−
−
n
i
n
i
p
p
n
p
p
p
p
 = 
1
1
−
n
 
 
15. 
P{X = kX + Y = m} = 
}
{
}
,
{
m
Y
X
P
m
Y
X
k
X
P
=
+
=
+
=
 
 
 
 
 
        = 
}
{
}
,
{
m
Y
X
P
k
m
Y
k
X
P
=
+
−
=
=
 
 
 
 
 
        = 
m
n
m
k
m
n
k
m
k
n
k
p
p
m
n
p
p
k
m
n
p
p
k
n
−
+
−
−
−
−




−




−
−




2)
1(
2
)
1(
)
1(
 
 
 
 
 
        = 








−




m
n
k
m
n
k
n
2
 
 
16. 
P(X = n, Y = m) = 
2
2
(
,
) (
)
i
P X
n Y
m X
i P X
i
=
=
=
=
∑
 
 
 
 
 
= 
1
2
3
min( ,
)
(
)
3
1
2
0
(
1)! (
)! !
m i
n i
i
n m
i
e
n
m
i
i
λ
λ
λ
λ
λ
λ
−
−
−
+
+
=
−
−
∑
 
 

Chapter 6 
 
93 
17. 
(a) P{X1 > X2X1 > X3} = 
}
{
)}
,
,
max(
{
3
1
3
2
1
1
X
X
P
X
X
X
X
P
>
=
 = 
3
/
2
2
/
1
3
/
1
=
 
 
 
(b) P{X1 > X2X1 < X3} = 
2
/
1
!
3
/
1
}
{
}
{
3
1
2
1
3
=
<
>
>
X
X
P
X
X
X
P
 = 1/3 
 
 
(c) P{X1 > X2X2 > X3} = 
3
/
1
2
/
1
!
3
/
1
}
{
}
{
3
2
3
2
1
=
=
>
>
>
X
X
P
X
X
X
P
 
 
 
(d) P{X1 > X2X2 < X3} = 
}
{
)}
,
,
min(
{
3
2
3
2
1
2
X
X
P
X
X
X
X
P
<
=
 = 
3
/
2
2
/
1
3
/
1
=
 
 
18. 
P{U > sU > a} = P{U > s}/P{U > a} 
 
 
 
 
  = 
a
s
−
−
1
1
, a < s < 1 
 
 
P{U < sU < a} = P{U < s}/P{U < a} 
 
 
 
 
  = s/a, 0 < s < a 
 
 
Hence, UU > a is uniform on (a, 1), whereas UU < a is uniform over (0, a). 
 
19. 
fWN(wn) = 
}
{
)
(
}
{
n
N
P
w
f
w
W
n
N
P
W
=
=
=
 
 
 
           = Ce−w
!
n
wn
βe−βw(βw)t−1 
 
 
           =  C1e−(β+1)wwn+t−1 
 
 
where C and C1 do not depend on w.  Hence, given N = n, W is gamma with parameters  
 
(n + t, β + 1). 
 
20. 
)
,...,
(
1
,...,
1
,
n
n
i
X
W
x
x
w
f
i
=
  = 
)
,...,
(
)
(
)
,...,
(
1
1
n
w
n
x
x
f
w
f
w
x
x
f
 
 
 
 
 
 
= ∏
=
−
−
−
n
1
i
1
)
(
t
w
wx
w
e
we
C
i
β
β
 
 
 
 
 
 
= 
1
1
−
+






+
−
∑
t
n
x
w
w
Ke
n
i
β
 
 

94 
 
Chapter 6 
21. 
Let Xij denote the element in row i, column j. 
 
 
 
P{Xij is s saddle point} 
 
 
 
= 


=
>
≠
=
ik
k
ij
kj
i
k
ik
m
k
X
X
X
X
P
min
,
max
min
,...,
1
 
 
 
 
= {
} {
}
ik
k
ij
kj
i
k
ik
k
X
X
P
X
X
P
min
max
min
=
>
≠
 
 
 
where the last equality follows as the events that every element in the ith row is greater than 
all elements in the jth column excluding Xij is clearly independent of the event that Xij is the 
smallest element in row i.  Now each size ordering of the n + m − 1 elements under 
consideration is equally likely and so the probability that the m smallest are the ones in row i 
is 




−
+
m
m
n
1
1
.  Hence 
 
 
 
P{Xij is a saddlepoint} = 
!)1
(
!)1
(!)1
(
1
1
1
−
+
−
−
=




−
+
m
n
n
m
m
m
m
n
 
 
 
and so 
 
 
 
 
 
P{there is a saddlepoint} = 



∪
t}
saddlepoin
 a 
is
 
{
,
ij
j
i
X
P
 
 
 
 
 
 
         = ∑
j
i,
t}
saddlepoin
 a 
is
 
{
ij
X
P
 
 
 
 
 
 
         = 
!)1
(
!
!
−
+ m
n
n
m
 
 
22. 
For 0 < x < 1 
 
 
P([X] = n, X − [X] < x) = P(n < X < n + x) = e−nλ − e−(n + x)λ = e−nλ(1 − e−xλ) 
 
 
Because the joint distribution factors, they are independent. [X] + 1 has a geometric 
distribution with parameter p = 1 − e −λ and x − [X] is distributed as an exponential with rate λ 
conditioned to be less than 1. 
 
23. 
Let Y = max (X1, …, Xn) , Z = min(X1, …, Xn) 
 
 
P{Y ≤ x} = P{Xi ≤ x, i= 1, …, n} = 
)
(
}
{
1
x
F
x
X
P
n
n
i
=
≤
∏
 
 
 
P{Z > x} = P{Xi > x, i = 1, …, n} = 
n
n
i
x
F
x
X
P
)]
(
1[
}
{
1
−
=
>
∏
. 
 
 

Chapter 6 
 
95 
24. 
(a) Let d = D/L.  Then the desired probability is 
 
 
 
n!
∫
∫
∫
∫
∫
−
+
−
+
+
−
−
−
−
−
+
−
−
−
d
d
x
d
d
x
d
x
n
n
d
n
d
n
d
x
n
n
n
dx
dx
dx
dx
2
1
1
1
1
2
1
)
1
(
1
0
)
2
(
1
3
2
1
1
...
...
 
 
 
 
 
= [1 − (n − 1)d]n. 
 
 
(b) 0 
 
25. 
∑
=
−
−




=
n
j
i
i
n
i
x
x
F
x
F
i
n
x
F
j
)]
(
1
)[
(
)
(
)
(
 
 
i
n
n
j
i
i
X
x
F
x
f
x
iF
i
n
x
f
j
−
=
−
−






= ∑
)]
(
1
)[
(
)
(
)
(
1
)
(
 
 
 
 
 
− ∑
=
−
−
−
−




n
j
i
i
n
i
x
f
x
F
i
n
x
F
i
n
)
(
)]
(
1
)[
)(
(
1
 
 
 
 
        = 
i
n
i
n
j
i
x
F
x
f
x
F
i
i
n
n
−
−
=
−
−
−
∑
)]
(
1
)[
(
)
(
!)1
(!)
(
!
1
 
 
 
 
 
− 
k
n
k
n
j
k
x
F
x
f
x
F
k
k
n
n
−
−
+
=
−
−
−
∑
)]
(
1
)[
(
)
(
!)1
(!)
(
!
1
1
 by k = i + 1 
 
 
        = 
j
n
j
x
F
x
f
x
F
j
j
n
n
−
−
−
−
−
)]
(
1
)[
(
)
(
!)1
(!)
(
!
1
 
 
26. 
n
n
n
X
x
x
n
n
n
x
f
)
1(
!
!
!)1
2
(
)
(
)
1
(
−
+
=
+
 
 
27. 
In order for X(i) = xi , X(j) = xj , i < j , we must have 
 
 
(i) i − 1 of the X’s less than xi 
 
 
(ii) 1 of the X’s equal to xi 
 
 
(iii) j − i − 1 of the X’s between xi and xj 
 
 
(iv) 1 of the X’s equal to xj 
 
 
(v) n − j of the X’s greater than xj 
 
 
Hence, 
 
 
)
,
(
)
(
)
( ,
j
i
X
x
x
x
f
j
i
 
 
= 
)
(
)]
(
)
(
)[
(
)
(
!)
(!1!)1
(!1!)1
(
!
1
1
j
i
j
i
j
i
i
i
x
f
x
F
x
F
x
f
x
F
j
n
i
j
i
n
−
−
−
−
−
−
−
−
× [1 − F(xj)n−j 
 

96 
 
Chapter 6 
29. 
Let X1, …, Xn be n independent uniform random variables over (0, a).  We will show by 
induction on n that 
 
 
 
 
P{X(k) − X(k−1) > t} = 









−
0
n
a
t
a
    
a
t
i
 a
t 
>
<
 f
 
if
   
 
It is immediate when n = 1 so assume for n − 1.  In the n case, consider 
 
 
 
 
P{X(k) − X(k−1) > tX(n) = s}. 
 
 
Now given X(n) = s, X(1) , …, X(n−1) are distributed as the order statistics of a set of n − 1 
uniform (0, s) random variables.  Hence, by the induction hypothesis 
 
 
 
 
P{X(k) − X(k−1) > tX(n) = s} = 









−
−
0
1
n
s
t
s
    
s
t
i
 s
t
>
<
 f
 
if
 
 
and thus, for t < a,  
 
 
 
P{X(k) − X(k−1) > t = 
n
n
n
n
a
t
a
t
a
ds
a
ns
s
t
s






−
=






−
−
−
∫
1
1
 
 
which completes the induction.  (The above used that 
n
n
n
X
a
ns
a
a
s
n
s
f
n
1
1 1
)
(
)
(
−
−
=






=
). 
 
30. 
(a) P{X > X(n)} = P{X is largest of n + 1} = 1/(n + 1) 
 
 
(b) P{X > X(1)} = P{X is not smallest of n + 1} = 1 − 1/(n + 1) = n/(n + 1) 
 
 
(c) This is the probability that X is either the (i + 1)st or (i + 2)nd or … jth smallest of the n + 1 
random variables, which is clearly equal to (j − 1)/(n + 1). 
 
33. 
The Jacobian of the transformation is 
 
 
 
 
J = 
2
2
/
/
0
/
1
1
y
x
y
x
y
−
=
−
 
 
 
Hence, 
x
y
J
/
2
1 =
−
.  Therefore, as the solution of the equations u = x, v = x/y is x= u, y = 
u/v, we see that 
 
 
 
 
fu,v(u, v) = 
2
/
)
/
(
2
,
2
2
2
2
2
1
)
/
,
(
v
u
u
Y
X
e
v
u
v
u
u
f
v
u
+
−
=
π
 
 

Chapter 6 
 
97 
 
Hence, 
 
 
 
 
fV(u) = 
∫
∞
∞
−
+
−
du
e
u
v
v
u
2
/
)
/
1
1(
2
2
2
2
1
π
 
 
 
 
 
= 
∫
∞
∞
−
−
du
e
u
v
u
2
2
2
/
2
2
1
σ
π
, where σ2 = v2/(1 + v2) 
 
 
 
 
= 
du
ue
v
u
∫
∞
−
0
2
/
2
2
2
1
σ
π
 
 
 
 
 
= 
dy
e
v
y
∫
∞
−
0
2
2
1 σ
π
 
 
 
 
 
= 
)
1(
1
2
v
+
π
 
 
 
 
 
 

98 
 
Chapter 7 
Chapter 7 
 
Problems 
 
1. 
Let X = 1 if the coin toss lands heads, and let it equal 0 otherwise.  Also, let Y denote the 
value that shows up on the die.  Then, with p(i, j) = P{X = i, Y = j} 
 
 
 
 
E[return] = 
∑
∑
=
=
+
6
1
6
1
)
,0
(
2
)
,1(
2
j
j
j
p
j
j
jp
 
 
 
 
 
   = 
)
5.
10
42
(
12
1
+
 = 52.5/12 
 
2. 
(a) 6 ⋅ 6 ⋅ 9 = 324 
 
 
(b) X = (6 − S)(6 − W)(9 − R) 
 
 
(c) E[X] = 6(6)(6)P{S = 0, W = 0, R = 3} + 6(3)(9)P{S = 0, W = 3, R = 0} 
 
 
 
 
+ 3(6)(9)P{S = 3, W = 0, R = 0} + 6(5)(7)P{S = 0, W = 1, R = 2} 
 
 
 
 
+ 5(6)(7)P{S = 1, W = 0, R = 2} + 6(4)(8)P{S = 0, W = 2, R  = 1} 
 
 
 
 
+ 4(6)(8)P{S = 2, W = 0, R = 1} + 5(4)(9)P{S = 1, W = 2, R = 0} 
 
 
 
 
+ 4(5)(9)P{S = 2, W = 1, R = 0} + 5(5)(8)P{S = 1, W = 1, R = 1} 
 
 
 
 
= 




+






+






+






⋅
+






+












)
9
)(
6
)(
6
(
200
6
2
6
360
9
2
6
384
2
9
6
420
3
6
324
3
9
216
3
21
1
 
 
 
      ≈ 198.8 
 
3. 
]
[
a
Y
X
E
−
 = 
dydx
y
x
a
∫∫
−
1
0
1
0
.  Now 
 
 
∫
∫
∫
−
+
−
=
−
1
0
1
0
)
(
)
(
x
a
x
a
a
dy
x
y
dy
y
x
dy
y
x
 
 
 
  
     ∫
∫
−
+
=
x
x
a
a
du
u
du
u
0
1
0
 
 
 
 
     
)1
/(
]
)
1(
[
1
1
+
−
+
=
+
+
a
x
x
a
a
 
 
 
Hence, 
 
 
 
]
[
a
Y
X
E
−
 = 
∫
+
+
−
+
+
1
0
1
1
]
)
1(
[
1
1
dx
x
x
a
a
a
 
 
 
 
 
= 
)
2
)(
1
(
2
+
+
a
a
 
 

Chapter 7 
 
99 
4. 
]
[
Y
X
E
−
 = 
∑∑
=
=
−
m
i
m
j
j
i
m
1
1
2
1
.  Now,  
 
 
 
∑
∑
∑
+
=
=
=
−
+
−
=
−
m
i
j
i
j
m
j
i
j
j
i
j
i
1
1
1
)
(
)
(
 
 
 
 
 
          = [i(i − 1) + (m − i)(m − i + 1)]/2 
 
 
Hence, using the identity ∑
=
m
j
j
1
2  = m(m + 1)(2m + 1)/6, we obtain that  
 
 
 
]
[
Y
X
E
−
 = 
m
m
m
m
m
m
m
m
m
3
)1
)(
1
(
2
)1
(
6
)1
2
)(
1
(
1
2
−
+
=




+
−
+
+
 
 
5. 
The joint density of the point (X, Y) at which the accident occurs is 
 
 
 
 
f(x, y) = 9
1 , −3/2 < x, y < 3/2 
 
 
 
 
    = f(x) f(y) 
 
 
where 
 
 
 
 
f(a) = 1/3, −3/2 < a < 3/2. 
 
 
Hence we may conclude that X and Y are independent and uniformly distributed on  
 
(−3/2, 3/2) Therefore, 
 
 
 
 
E[X + Y] = 
2
/
3
3
4
3
1
2
2
/
3
0
2
/
3
2
/
3
=
=
∫
∫
−
xdx
dx
x
. 
 
6. 
∑
∑
=
=
=






10
1
10
1
]
[
i
i
i
i
X
E
X
E
 = 10(7/2) = 35. 
 
8. 
E[number of occupied tables] = 
∑
∑
=
=
=






N
i
i
N
i
i
X
E
X
E
1
1
]
[
 
 
Now,  
 
 
E[Xi] = P{ith arrival is not friends with any of first i − 1} 
 
 
   = (1 − p)i−1 
 
 
and so 
 
 
 
 
E[number of occupied tables] = ∑
=
−
−
N
i
i
p
1
1
)
1(
 
 

100 
 
Chapter 7 
7. 
Let Xi equal 1 if both choose item i and let it be 0 otherwise; let Yi equal 1 if neither A nor B 
chooses item i and let it be 0 otherwise.  Also, let Wi equal 1 if exactly one of A and B choose 
item i and let it be 0 otherwise.  Let 
 
 
 
 
X = ∑
=
10
1
i
i
X ,   Y = ∑
=
10
1
i
iY ,    W = ∑
=
10
1
i
i
W  
 
 
(a)  
E[X] = ∑
=
10
1
]
[
i
i
X
E
 = 10(3/10)2 = .9 
 
 
(b)  
E[Y] = ∑
=
10
1
]
[
i
iY
E
 = 10(7/10)2 = 4.9 
 
 
(c) Since X + Y + W = 10, we obtain from parts (a) and (b) that  
 
 
 
 
E[W] = 10 − .9 − 4.9 = 4.2 
 
 
 
Of course, we could have obtained E[W] from  
 
 
 
 
E[W] = ∑
=
10
1
]
[
i
i
W
E
 = 10(2)(3/10)(7/10) = 4.2 
 
9. 
Let Xj equal 1 if urn j is empty and 0 otherwise.  Then  
 
E[Xj] = P{ball i is not in urn j, i ≥ j} = ∏
=
−
n
j
i
i)
/
1
1(
 
 
Hence, 
 
(a) E[number of empty urns] = ∑∑
=
=
−
n
j
n
j
i
i
1
)
/
1
1(
 
 
(b) P{none are empty} = P{ball j is in urn j, for all j} 
 
 
 
 
 
= ∏
=
n
j
j
1
/
1
 
 
10. 
Let Xi equal 1 if trial i is a success and 0 otherwise.   
 
 
(a) .6.  This occurs when P{X1 = X2 = X3} = 1.  It is the largest possible since  
 
 
1.8  = ∑
=
=
=
}
1
{
3
}
1
{
i
i
X
P
X
P
.  Hence, P{Xi = 1} = .6 and so 
 
 
 
 
P{X = 3} = P{X1 = X2 = X3 = 1} ≤ P{Xi = 1} = .6. 
 
 
(b) 0.  Letting 
 
 
X1 = 
otherwise
 0
.6
 
if
 1
≤
U
  ,  
X2 = 
otherwise
 0
.4
 
if
 1
≤
U
  , 
X3 = 
otherwise
 0
.3
 
if
 1
≤
U
 
 
 
Hence, it is not possible for all Xi to equal 1. 

Chapter 7 
 
101 
11. 
Let Xi equal 1 if a changeover occurs on the ith flip and 0 otherwise.  Then 
 
 
 
 
E[Xi] = P{i − 1 is H, i is T} + P{i − 1 is T, i is H} 
 
 
 
 
   = 2(1 − p)p,  i ≥ 2. 
 
 
E[number of changeovers] = [
] ∑
∑
=
=
n
i
i
i
X
E
X
E
1
]
[
 = 2(n − 1)(1 − p) 
 
12. 
(a) Let Xi equal 1 if the person in position i is a man who has a woman next to him, and let it 
equal 0 otherwise.  Then 
 
 
 
 
E[Xi] = 










−
−
−
−
−
=
−
otherwise
,
)
2
2
)(
1
2
(
)
2
)(
1
(
1
2
1
2n
 
1,
 
  
if
 
,
1
2
2
1
n
n
n
n
i
n
n
 
 
 
Therefore, 
 
 
 
 





∑
=
n
i
i
X
E
1
  = ∑
=
n
i
i
X
E
2
1
]
[
 
 
 
 
 
 
= 






−
−
+
−
2
4
3
)
2
2
(
1
2
2
2
1
n
n
n
n
n
 
 
 
 
 
 
= 
2
4
3
2
−
−
n
n
n
 
 
 
(b) In the case of a round table there are no end positions and so the same argument as in part 
(a) gives the result 
 
 
 
 
2
4
3
)
2
2
)(
1
2
(
)
2
)(
1
(
1
2
−
=




−
−
−
−
−
n
n
n
n
n
n
n
 
 
 
 
where the right side equality assumes that n > 1. 
 
13. 
Let Xi be the indicator for the event that person i is given a card whose number matches his 
age.  Because only one of the cards matches the age of the person i 
 
 
 
 
∑
∑
=
=
=






1000
1
1000
1
]
[
i
i
i
i
X
E
X
E
 = 1 
 
14. 
The number of stages is a negative binomial random variable with parameters m and 1 − p.  
Hence, its expected value is m/(1 − p). 
 

102 
 
Chapter 7 
15. 
Let Xi,j , i ≠ j equal 1 if i and j form a matched pair, and let it be 0 otherwise. 
 
 
Then 
 
 
 
E[Xi,j] = P{i, j is a matched pair} = 
)1
(
1
−
n
n
 
 
Hence, the expected number of matched pairs is 
 
 
 
 
2
1
)1
(
1
2
]
[
,
,
=
−






=
=








∑
∑
<
<
n
n
n
X
E
X
E
j
i
j
i
j
i
j
i
 
 
16. 
E[X] = ∫
>
−
−
=
x
y
x
y
e
dy
e
y
π
π
2
2
1
2
/
2
/
2
2
 
 
17. 
Let Ii equal 1 if guess i is correct and 0 otherwise. 
 
 
(a) Since any guess will be correct with probability 1/n it follows that  
 
 
 
E[N] = 
1
/
]
[
1
=
=
∑
=
n
n
I
E
n
i
i
 
 
 
(b) The best strategy in this case is to always guess a card which has not yet appeared.  For 
this  strategy, the ith guess will be correct with probability 1/(n − i + 1) and so 
 
 
 
 
E[N] = ∑
=
+
−
n
i
i
n
1
)1
/(
1
 
 
 
(c) Suppose you will guess in the order 1, 2, …, n.  That is, you will continually guess card 1 
until it appears, and then card 2 until it appears, and so on.  Let Ji denote the indicator 
variable for the event that you will eventually be correct when guessing card i; and note 
that this event will occur if among cards 1 thru i, card 1 is first , card 2 is second, …, and 
card i is the last among these i cards.  Since all i! orderings among these cards are equally 
likely it follows that 
 
 
 
 
E[Ji] = 1/i!  and thus  E[N] = 
∑
∑
=
=
=






n
i
n
i
i
i
J
E
1
1
!
/
1
 
 
18. 
E[number of matches]  = 



=





∑
-
-
-
0
card
on 
match 
1
,
52
1
 i
I
I
E
i
i
 
 
 
 
 
 
= 
4
13
1
52
=
  since E[Ii] = 1/13 
 

Chapter 7 
 
103 
19. 
(a) E[time of first type 1 catch] − 1 =
1
1
1
−
p
 using the formula for the mean of a geometric 
random variable. 
 
 
(b) Let 
 
 
 
 
Xj = 



otherwise.
0
1 
 type
a 
before
caught 
 
is
 
 
 type
a 
1
j
 
 
 
 
Then 
 
 
 
 
∑
∑
≠
≠
=








1
1
]
[
j
j
j
j
X
E
X
E
 
 
 
 
 
           ∑
≠
=
1
1}
 
 type
before
 
 
type
{
j
j
P
 
 
 
 
 
           ∑
≠
+
=
1
1)
/(
j
j
j
P
P
P
, 
 
 
 
where the last equality follows upon conditioning on the first time either a type 1 or type j 
is caught to give. 
 
 
 
 
P{type j before type 1} = P{jj or 1} = 
1P
P
P
j
j
+
 
 
20. 
Similar to (b) of 19.  Let 
 
 
 
Xj = 

-
-
-
0
1 
ball
 
before
 
removed
 
 
ball
1
j
 
 
 
 
1}
 
ball
 
before
 
 
ball
{
]
[
1
1
1
j
P
X
E
X
E
j
j
j
j
j
∑
∑
∑
≠
≠
≠
=
=








 
 
 
 
 
 
    ∑
≠
=
1
1}
or 
 
{
j
j
j
P
 
 
 
 
 
 
    ∑
≠
+
=
1
)
(
)1(
/)
(
j
j
W
W
j
W
 
 
21. 
(a) 
97
3
365
364
365
1
3
100
365

















 
 

104 
 
Chapter 7 
 
(b) Let Xj = 

-
-
-
0
birthday
 
someones
 
is
 
day 
 
if
 
1
j
 
 
 
 
 














−
=
=






∑
∑
100
365
1
365
1
365
364
1
365
]
[
j
j
X
E
X
E
 
 
22. 
From Example 3g, 1 + 
6
2
6
3
6
4
6
5
6
+
+
+
+
 
 
23. 
∑
∑
∑
∑
+
=






+
8
1
5
1
8
1
5
1
)
(
]
[
i
i
i
i
Y
E
X
E
Y
X
E
 
 
 
 
 
   
110
147
120
3
8
20
3
11
2
5
=
+
=
 
 
24. 
Number the small pills, and let Xi equal 1 if small pill i is still in the bottle after the last large 
pill has been chosen and let it be 0 otherwise, i = 1, …, n.  Also, let Yi, i = 1, …, m equal 1 if 
the ith small pill created is still in the bottle after the last large pill has been chosen and its 
smaller half returned. 
 
 
Note that X = 
∑
∑
=
=
+
m
i
i
n
i
i
Y
X
1
1
.  Now, 
 
 
 
E[Xi] = P{small pill i is chosen after all m large pills} 
 
 
          = 1/(m + 1) 
 
 
 
 
 
E[Yi] = P{ith created small pill is chosen after m − i existing large pills} 
 
 
          = 1/(m − i + 1) 
 
 
Thus,  
 
 
 
(a) E[X] = n/(m + 1) + ∑
=
+
−
m
i
i
m
1
)1
/(
1
 
 
 
 
 
(b) Y = n + 2m − X and thus 
 
 
 
 
E[Y] = n + 2m − E[X] 
 
25. 
P{N ≥ n} P{X1 ≥ X2 ≥ … ≥ Xn} = !
1
n  
 
 
E[N] = 
e
n
n
N
P
n
n
=
=
≥
∑
∑
∞
=
∞
=
1
1
!
1
}
{
 
 

Chapter 7 
 
105 
26. 
(a) E[max]  = ∫
>
1
0
}
{max
dt
t
P
 
 
 
 
       = ∫
≤
−
1
0
)}
{max
1(
dt
t
P
 
 
 
 
       = ∫
+
=
−
1
0
1
/
1(
n
n
dt
tn
 
 
 
(b) E[min] = ∫
>
1
0
4
}
{min
t
t
p
 
 
 
 
      = 
1
1
)
1(
1
0
+
=
−
∫
n
dt
t n
 
 
27. 
Let X denote the number of items in a randomly chosen box.  Then, with Xi equal to 1 if item 
i is in the randomly chosen box 
 
 
 
 
E[X] = 
10
10
101
]
[
101
1
101
1
>
=
=






∑
∑
=
=
i
i
i
i
X
E
X
E
 
 
 
Hence, X can exceed 10, showing that at least one of the boxes must contain more than 10 
items. 
 
28. 
We must show that for any ordering of the 47 components there is a block of 12 consecutive 
components that contain at least 3 failures.  So consider any ordering, and randomly choose a 
component in such a manner that each of the 47 components is equally likely to be chosen.  
Now, consider that component along with the next 11 when moving in a clockwise manner 
and let X denote the number of failures in that group of 12.  To determine E[X], arbitrarily 
number the 8 failed components and let, for i = 1, …, 8,  
 
 
 
 
Xi = 

otherwise
0,
components
 
12
 
of
 
group
 
 the
among
 
is
 
component 
 
failed
 
if
,1
i
 
 
 
Then, 
 
 
 
 
X = ∑
=
8
1
i
i
X  
 
 
and so 
 
 
 
 
E[X] = ∑
=
8
1
]
[
i
i
X
E
 
 

106 
 
Chapter 7 
 
Because Xi will equal 1 if the randomly selected component is either failed component 
number i or any of its 11 neighboring components in the counterclockwise direction, it 
follows that E[Xi] = 12/47.  Hence, 
 
 
 
 
E[X] = 8(12/47) = 96/47 
 
 
Because E[X] > 2 it follows that there is at least one possible set of 12 consecutive 
components that contain at least 3 failures. 
 
29. 
Let Xii be the number of coupons one needs to collect to obtain a type i.  Then 
 
 
 
 
1
]
,
,
,
[min(
2,1
,7
/
8
)]
,
[min(
4,3
,5
/
8
)]
,
,
[min(
3
/
4
)]
,
[min(
4,3
,2,1
,2
)]
,
[min(
4
)]
,
[min(
4,3
,3
/
8
]
{
2,1
,8
]
[
4
3
2
1
4
,3
2
1
4
3
2
1
)
=
=
=
=
=
=
=
=
=
=
=
=
=
=
X
X
X
X
E
i
X
X
X
E
j
X
X
X
E
X
X
E
j
i
X
X
E
X
X
E
i
X
E
i
X
E
i
j
j
i
i
i
 
 
 
(a) E[max Xi] = 2 ⋅ 8 + 2 ⋅ 8/3 − (4 + 4 ⋅ 2 + 4/3) + (2 ⋅ 8/5 + 2 ⋅ 8/7) − 1 = 35
437  
 
 
(b) E[max(X1, X2)] = 8 + 8 − 4 = 12 
 
 
(c) E[max(X3, X4)] = 8/3 + 8/3 − 4/3 = 4 
 
 
(d) Let Y1 = max(X1, X2), Y2 = max(X3, X4).  Then 
 
 
 
 
E[max(Y1, Y2)] = E[Y1] + E[Y2] − E[min(Y1, Y2)] 
 
 
 
giving that  
 
 
 
 
E[min(Y1, Y2)] = 12 + 4 − 
35
123
35
437 =
 
 
30. 
E[(X − Y)]2 = Var(X − Y) = Var(X) + Var(−Y) = 2σ2 
 
31. 
)
(
Var
10
Var
1
10
1
X
X
i
i
=





∑
=
.  Now 
 
 
Var(X1) = 
2
2
1
)
2
/
7
(
]
[
−
X
E
 
 
 
 
       = [1 + 4 + 9 + 16 + 25 + 36]/6 − 49/4 
 
 
 
       = 35/12 
 
 
and so 





∑
=
10
1
Var
i
i
X
 = 350/12. 

Chapter 7 
 
107 
32. 
Use the notation in Problem 9, 
 
 
 
 
X = ∑
=
n
j
j
X
1
 
 
 
where Xj is 1 if box j is empty and 0 otherwise.  Now, with 
 
 
E[Xj] = P{Xj = 1} = ∏
=
−
n
j
i
i)
/
1
1(
, we have that 
 
 
 
Var(Xj) = E[Xj](1 − E[Xj]). 
 
 
Also, for j < k 
 
 
 
E[XjXk] = 
∏
∏
=
−
=
−
−
n
k
i
k
j
i
i
i
)
/
2
1(
)
/
1
1(
1
 
 
Hence, for j < k,  
 
 
Cov(Xj, Xk) = 
∏
∏
∏
∏
=
=
=
−
=
−
−
−
−
−
n
k
i
n
j
i
n
k
i
k
j
i
i
i
i
i
)
/
1
1(
)
/
1
1(
)
/
2
1(
)
/
1
1(
1
 
 
 
Var(X ) = 
)
,
(
Cov
2
])
[
1(]
[
1
k
j
j
n
j
j
X
X
X
E
X
E
+
−
∑
=
 
 
33. 
(a) E[X2 + 4X + 4] = E[X2] + 4E[X] + 4 = Var(X) + E2[X] + 4E[X] + 4 = 14 
 
 
(b) Var(4 + 3X) = Var(3X) = 9Var(X) = 45 
 
34. 
Let Xj = 

otherwise
0
other
each 
 
next to
 
seated
 
are
 
 
couple
 
if
1
j
 
 
 
(a) 
19
20
19
2
10
10
1
=
=





∑
j
X
E
;  P{Xj = 1} = 19
2   since there are 2 people seated next to wife j 
and so the probability that one of them is her husband is 19
2 . 
 
 
(b) For i ≠ j, E[XiXj] = P{Xi = 1, Xj = 1} 
 
 
 
 
        = P{Xi = 1}P{Xj = 1Xi = 1} 
 
 
 
 
        = 
18
2
19
2
  since given Xi = 1 we can regard couple i as a single entity. 
 
 
 
 
Var














−
⋅
+





−
=







∑
=
2
10
1
19
2
18
2
19
2
9
10
19
2
1
19
2
10
j
j
X
 
 

108 
 
Chapter 7 
35. 
(a) Let X1 denote the number of nonspades preceding the first ace and X2 the number of 
nonspades between the first 2 aces.  It is easy to see that  
 
 
 
 
P{X1 = i,  X2 = j} = P{X1 = j,  X2 = i} 
 
 
 
and so X1 and X2 have the same distribution.  Now E[X1] = 5
48  by the results of Example 
3j and so E[2 + X1 + X2] = 5
106 . 
 
 
(b) Same method as used in (a) yields the answer 
14
265
1
14
39
5
=






+
. 
 
 
(c) Starting from the end of the deck the expected position of the first (from the end) heart is, 
from Example 3j, 14
53 .  Hence, to obtain all 13 hearts we would expect to turn over  
 
 
52 − 14
53  + 1 = 14
13 (53). 
 
36. 
Let Xi = 

otherwise
0
1
on 
 
lands
  
roll
1
i
, 
Yi = 

otherwise
0
2
on 
 
lands
  
roll
1
i
 
 
 
Cov(Xi, Yj) = E[Xi Yj] − E[Xi]E[Yj] 
 
 
 
      = 
 
0
36
1
36
1
when 
0
X
 
(since
 
36
1
i





≠
=
−
=
=
=
−
j
i
j
i
Y
j
i
j
 
 
 
 
 
Cov
)
,
(
Cov
,
j
i
i
j
j
j
i
i
Y
X
Y
X
∑∑
∑
∑
=
 
 
 
 
 
 
 
36
n
−
=
 
 
37. 
Let Wi,  i = 1, 2, denote the ith outcome. 
 
 
 
 
Cov(X, Y) = Cov(W1 + W2 , W1 − W2) 
 
 
 
 
    = Cov(W1, W1) − Cov(W2, W2) 
 
 
 
 
    = Var(W1) − Var(W2) = 0 
 
38. 
E[XY] = ∫∫
∞
−
0 0
2
2
x
xdydx
e
y
 
 
 
    = 
4
1
8
)3
(
8
1
0
2
0
2
2
=
Γ
=
= ∫
∫
∞
−
∞
−
dy
e
y
dx
e
x
y
x
 

Chapter 7 
 
109 
 
E[X] = 
∫
∫
−
∞
=
x
x
x
x
dy
x
e
x
f
dx
x
xf
0
2
0
2
)
(
,
)
(
 = 2e−2x 
 
 
  = 2
1  
 
 
E[Y] = 
∫
∫
∞
−
∞
=
0
2
0
2
)
(
,
)
(
dx
x
e
y
f
dy
y
yf
x
Y
Y
  
 
 
 = ∫∫
∞∞
−
0
2
2
y
x
dxdy
x
e
y
 
 
 
 = ∫∫
∞
−
0 0
2
2
x
x
dydx
x
e
y
 
 
 
 = 
4
1
4
)
2
(
4
1
2
0
2
=
Γ
=
= ∫
∫
−
∞
−
dy
ye
dx
xe
x
 
 
 
Cov(X, Y) = 
8
1
4
1
2
1
4
1
=
−
 
 
39. 
Cov(Yn, Yn) = Var(Yn) = 3σ2 
 
Cov(Yn, Yn+1) = Cov(Xn + Xn+1 + Xn+2,  Xn+1 + Xn+2 + Xn+3) 
 
 
 
          = Cov(Xn+1 + Xn+2,  Xn+1 + Xn+2) = Var(Xn+1 + Xn+2) = 2σ2 
 
Cov(Yn, Yn+2) = Cov(Xn+2, Xn+2) = σ2 
 
Cov(Yn, Yn+j) = 0 when j ≥ 3 
 
40. 
fY(y) = e−y
y
y
x
e
dx
e
y
−
−
=
∫
/
1
.  In addition, the conditional distribution of X given that Y = y is 
exponential with mean y.  Hence, 
 
 
 
E[Y] = 1,  E[X] = E[E[XY]] = E[Y] = 1 
 
 
Since, E[XY] = E[E[XYY]] = E[YE[XY]] = E[Y2] = 2  (since Y is exponential with mean 1, it 
follows that E[Y2] = 2).  Hence, Cov(X, Y) = 2 − 1 = 1. 
 
41. 
The number of carp is a hypergeometric random variable. 
 
 
E[X] = 10
60  = 6 
 
 
Var(X) = 
99
336
10
7
10
3
99
)
80
(
20
=
 from Example 5c. 
 

110 
 
Chapter 7 
42. 
(a) Let Xi = 

otherwise
0
 woman
a 
and
man 
 a 
of
 
consists
 
pair 
 
1
i
 
 
 
 
E[Xi] = P{Xi = 1} = 19
10  
 
 
E[XiXj] = P{Xi = 1, Xj = 1} = P{Xi = 1}P{Xj = 1X2 = 1} 
 
 
 
 
 
= 
17
9
19
10
, i ≠ j 
 
 
 
19
100
10
1
=





∑
i
X
E
 
 
 
 
Var 
17
18
)
19
(
900
19
10
17
9
19
10
9
10
19
10
1
19
10
10
2
2
10
1
=














−
⋅
+





−
=





∑
i
X
 
 
 
(b) Xi = 

otherwise
0
couple
 
married
 a 
of
 
consists
 
pair 
 
1
i
 
 
 
 
E[Xi] = 19
1 , E[XiXj] = P{Xi = 1}P{Xj = 1Xi = 1} = 
17
1
19
1
,  i ≠ j 
 
 
 
19
10
10
1
=





∑
i
X
E
 
 
 
Var
17
18
)
19
(
180
19
1
17
1
19
1
9
10
19
15
19
1
10
2
2
10
1
=














−
⋅
+
=





∑
i
X
 
 
43. 
E[R] = n(n + m + 1)/2 
 
 
Var(R) = 


















+
+
−
+
−
+
∑
+
=
2
1
2
2
1
1
m
n
m
n
i
m
n
nm
m
n
i
 
 
The above follows from Example 3d since when F = G, all orderings are equally likely and 
the problem reduces to randomly sampling n of the n + m values 1, 2, …, n + m. 
 

Chapter 7 
 
111 
44. 
From Example 8l 
m
n
nm
m
n
n
+
+
+
.  Using the representation of Example 2l the variance can be 
computed by using 
 
 
 
E[I1Il+j] = 



−
+
−
−
+
+
2
1
1
0
m
n
n
m
n
m
m
n
n
       
1
1
,
1
,
<
≤
−
=
j
n
j
 
 
 
 
E[IiIi+j] = 



−
+
−
+
−
+
+
−
−
)
3
)(
2
)(
1
)(
(
)1
)(
1
(
0
m
n
m
n
m
n
m
n
n
m
mn
   
1
1
,
1
,
<
≤
−
=
j
n
j
 
 
45. 
(a) 
2
1
)
(
Var
)
(
Var
)
,
(
Cov
3
2
2
1
3
2
2
1
=
+
+
+
+
X
X
X
X
X
X
X
X
 
 
(b) 0 
 
46. 
E[I1I2] = ∑
=
12
2
2
1
}
 
rolls
]P{bank 
 
rolls
bank 
 
[
i
i
i
I
I
E
 
 
 
     = ∑
i
i
P
i
P
}
 
rolls
bank 
{
})
an 
greater th
 
is
 
roll
{
(
2
 
 
 
     = 
]
[
2
1I
E
 
 
 
     ≥ (E[I1])2 
 
          = E[I1] E[I2] 
 
47. 
(a) It is binomial with parameters n − 1 and p. 
 
 
(b) Let xi,j equal 1 if there is an edge between vertices i and j, and let it be 0 otherwise.  Then, 
Di = 
k
i
i
k
X ,
∑
≠
, and so, for i ≠ j 
 
 
 
 
Cov(Di, Dj) = Cov








∑
∑
≠
≠
j
r
j
r
i
k
k
i
X
X
,
, ,
 
 
 
 
 
      = ∑∑
≠
≠
i
k
j
r
j
r
k
i
X
X
Cov
)
,
(
,
,
 
 
 
 
 
      = Cov(Xi, j , Xi, j) 
 
 
 
 
      = Var(Xi, j) 
 
 
 
 
      = p(1 − p) 
 
 
 
where the third equality uses the fact that except when k = j and r = i, Xi ,k and Xr, j are 
independent and thus have covariance equal to 0.  Hence, from part (a) and the preceding 
we obtain that for i ≠ j, 
 
 
 
 
 
ρ(Di, Dj) = 
1
1
)
1(
)1
(
)
1(
−
=
−
−
−
n
p
p
n
p
p
 
 
 

112 
 
Chapter 7 
48. 
(a) E[X] = 6 
 
 
(b) E[XY = 1] = 1 + 6 = 7 
 
 
(c) 
)
6
5
(
5
4
5
1
5
4
4
5
1
5
4
3
5
1
5
4
2
5
1
1
4
3
2
+






+












+






+
+
 
 
49. 
Let Ci be the event that coin i is being flipped (where coin 1 is the one having head 
probability .4), and let T be the event that 2 of the first 3 flips land on heads. Then 
 
 
 
P(C1T) = 
1
1
1
1
2
2
(
) (
)
(
) (
)
(
) (
)
P T C P C
P T C P C
P T C
P C
+
 
 
 
 
        = 
2
2
2
3(.4) (.6)
3(.4) (.6)
3(.7) (.3)
+
 = .395 
 
 
 
 
 
Now, with Nj equal to the number of heads in the final j flips, we have 
 
 
 
 
 
 
 
E[N10T] = 2 + E[N7T] 
 
 
 
Conditioning on which coin is being used, gives 
 
 
 
 
E[N7T] = E[N7TC1]P(C1T) + E[N7TC2]P(C2T) = 2.8(.395) + 4.9(.605) = 4.0705 
 
 
 
 
Thus, E[N10T] = 6.0705. 
 
50. 
fXY(xy) = 
y
x
y
y
x
y
y
x
e
y
dx
y
e
e
y
e
e
/
0
/
/
1
/
/
−
∞
−
−
−
−
=
∫
,    0 < x < ∞ 
 
 
Hence, given Y = y, X is exponential with mean y, and so 
 
 
 
E[X 2Y = y] = 2y2 
 
51. 
fXY(xy) = 
,
1
/
/
0
y
dx
y
e
y
e
y
y
y
=
∫
−
−
   0 < x < y 
 
 
E[X 3Y = y] = 
4
/
1
3
0
3
y
dx
y
x
y
=
∫
 
 
52. 
The average weight, call it E[W], of a randomly chosen person is equal to average weight of 
all the members of the population.  Conditioning on the subgroup of that person gives 
 
 
 
E[W] = 
∑
∑
=
=
=
r
i
i
i
i
r
i
p
w
p
i
W
E
1
1
]
 
subgroup
 
of
member 
 
{
 

Chapter 7 
 
113 
53. 
Let X denote the number of days until the prisoner is free, and let I denote the initial door 
chosen.  Then 
 
 
 
E[X] = E[XI = 1](.5) + E[XI = 2](.3) + E[XI = 3](.2) 
 
 
 
 = 
(2 + E[X])(.5) + (4 + E[X])(.3) + .2 
 
 
Therefore, 
 
 
 
E[X] = 12 
 
54. 
Let Ri denote the return from the policy that stops the first time a value at least as large as i 
appears.  Also, let X be the first sum, and let pi = P{X = i}.  Conditioning on X yields 
 
 
 
E[R5] = ∑
=
=
12
2
5
}
[
i
ip
i
X
R
E
 
 
 
 
   = E[R5)(p2 + p3 + p4) + ∑
=
12
5
i
i
ip  − 7p7 
 
 
 
   = 
]
[
36
6
5
R
E
 + 5(4/36) + 6(5/36) + 8(5/36) + 9(4/36) + 10(3/36) + 11(2/36) + 12(1/36) 
 
 
 
   = 
]
[
36
6
5
R
E
 + 190/36 
 
Hence, E[R5] = 19/3 ≈ 6.33.  In the same fashion, we obtain that 
 
 
 
 
E[R6] = 
36
1
]
[
36
10
6 +
R
E
[30 + 40 + 36 + 30 + 22 + 12] 
 
 
implying that   
 
 
 
 
E[R6] = 170/26 ≈ 6.54 
 
 
Also,  
 
 
 
 
E[R8] = 
)
140
(
36
1
]
[
36
15
8 +
R
E
 
 
 
or,  
 
 
 
 
E[R8] = 140/21 ≈ 6.67 
 
 
In addition, 
 
 
 
 
 
 
E[R9] = 
)
100
(
36
1
]
[
26
20
9 +
R
E
 
 
 
or 
 
 
 
 
E[R9] = 100/16 = 6.25 
 
 

114 
 
Chapter 7 
 
And 
 
 
 
 
E[R10] = 
)
64
(
36
1
]
[
36
24
10 +
R
E
 
 
or 
 
 
 
 
E[R10] = 64/12 ≈ 5.33 
 
 
The maximum expected return is E[R8]. 
 
55. 
Let N denote the number of ducks.  Given N = n, let I1, …, In be such that  
 
Ii =  

otherwise
0
hit
 
is
 
duck
 
if
1
 i
 
 
 
E[Number hitN = n] = 





∑
=
n
i
iI
E
1
 
 
 
 
 
         = 
,
6.
1
1
]
[
10
1













−
−
=
∑
=
n
n
I
E
n
i
i
 since given  
 
 
N = n, each hunter will independently hit duck i with probability .6/n. 
 
 
 
 
E[Number hit] = 
!
/
6
6.
1
6
10
0
n
e
n
n
n
n
−
∞
=∑





−
 
 
56. 
Let Ii = 

otherwise
0
floor 
at 
 
stops
elevator 
1
i .  Let X be the number that enter on the ground floor. 
 
 
 
∑
∑
=
=














−
−
=
=
=






=
N
i
k
i
N
i
i
N
N
N
k
X
I
E
k
X
I
E
1
1
1
1
]
[
 
 
 
          
!
)
10
(
1
10
0
1
k
e
N
N
N
N
I
E
k
k
k
N
i
i
−
∞
=
=
∑
∑






−
−
=






 
 
 
 
                    = N − Ne−10/N = N(1 − e−10/N) 
 
57. 
]
[
]
[
1
X
E
N
E
X
E
N
i
i
=





∑
=
 = 12.5 
 
58. 
Let X denote the number of flips required.  Condition on the outcome of the first flip to 
obtain. 
 
 
 
E[X] = E[Xheads]p + E[xtails](1 − p) 
 
 
 
  = [1 + 1/(1 − p)]p + [1 + 1/p](1 − p) 
 
 
 
  = 1 + p/(1 − p) + (1 − p)/p 
 

Chapter 7 
 
115 
59. 
(a) E[total prize shared] = P{someone wins} = 1 − (1 − p)n+1 
 
 
 
(b) Let Xi be the prize to player i.  By part (a) 
 
 
 
 
1
1
1
)
1(
1
+
+
=
−
−
=





∑
n
n
i
i
p
X
E
 
 
 
 
But, by symmetry all E[Xi] are equal and so 
 
 
 
 
E[X] = [1 − (1 − p)n+1]/(n + 1) 
 
 
(c) E[X] = p E[1/(1 + B)]  where B, which is binomial with parameters n and p, represents the 
number of other winners. 
 
60. 
(a) Since the sum of their number of correct predictions is n (one for each coin) it follows 
that one of them will have more than n/2 correct predictions.  Now if N is the number of 
correct predictions of a specified member of the syndicate, then the probability mass 
function of the number of correct predictions of the member of the syndicate having more 
than n/2 correct predictions is 
 
 
 
 
P{i correct} = P{N = i} + P(N = n − i}  i > n/2 
 
 
 
 
       = 2P{N = i} 
 
 
 
 
       = P{N = iN > n/2} 
 
 
(b) X is binomial with parameters m, 1/2. 
 
 
(c) Since all of the X + 1 players (including one from the syndicate) that have more than n/2 
correct predictions have the same expected return we see that 
 
 
 
 
(X + 1) ⋅ Payoff to syndicate = m + 2 
 
 
 
implying that 
 
 
 
 
E[Payoff to syndicate] = (m + 2) E[(X + 1)−1] 
 
 
(d) This follows from part (b) above and (c) of Problem 56. 
 
61. 
(a) P(M ≤ x) = 
1
1
1
( )
(
) (
)
( ) (1
)
1
(1
) ( )
n
n
n
n
pF x
P M
x N
n P N
n
F
x p
p
p F x
∞
∞
−
=
=
≤
=
=
=
−
= −
−
∑
∑
 
 
 
 
(b) P(M ≤ xN = 1) = F(x) 
 
 
 
(c) P(M ≤ xN > 1) = F(x)P(M ≤ x) 
 
 
(d) P(M ≤ x) = P(M ≤ xN = 1)P(N = 1) + P(M ≤ xN > 1)P(N > 1) 
 
 
 
         = F(x)p + F(x)P(M ≤ x)(1 − p) 
 
 
 
again giving the result 
 
 
 
 
P(M ≤ x) = 
( )
1
(1
) ( )
pF x
p F x
−
−
 

116 
 
Chapter 7 
62. 
The result is true when n = 0, so assume that  
 
 
 
P{N(x) ≥ n} = xn/(n − 1)! 
 
 
Now,  
 
 
 
P{N(x) ≥ n + 1} = ∫
=
+
≥
1
0
1
}
1
)
(
{
dy
y
U
n
x
N
P
 
 
 
 
 
       = ∫
≥
−
x
dy
n
y
x
N
P
0
}
)
(
{
 
 
 
 
 
       = ∫
≥
x
du
n
u
N
P
0
}
)
(
{
 
 
 
 
 
       = ∫
−
−
x
n
n
u
0
1
!)1
/(
 du by the induction hypothesis 
 
 
 
 
       = xn/n! 
 
 
 
which completes the proof. 
 
 
(b) E[N(x)] = 
x
n
n
n
n
e
n
x
n
x
N
P
n
x
N
P
=
=
+
≥
=
>
∑
∑
∑
∞
=
∞
=
∞
=
0
0
0
!
/
}
1
)
(
{
)
(
{
 
 
63. 
(a) Number the red balls and the blue balls and let Xi equal 1 if the ith red ball is selected and 
let it by 0 otherwise.  Similarly, let Yj equal 1 if the jth blue ball is selected and let it be 0 
otherwise. 
 
 
 
 
Cov
∑∑
∑
∑
=








i
j
j
i
j
j
i
i
Y
X
Y
X
)
,
(
Cov
,
 
 
 
Now, 
 
 
 
E[Xi] = E[Yj] = 12/30 
 
 
 
E[XiYj] = P{red ball i and blue ball j are selected} = 












12
30
10
28
 
 
 
Thus, 
 
 
 
Cov(X, Y) = 




−












2)
30
/
12
(
12
30
10
28
80
 = −96/145 
 

Chapter 7 
 
117 
 
(b) E[XYX] = XE[YX] = X(12 − X)8/20 
 
 
 
where the above follows since given X, there are 12-X additional balls to be selected from 
among 8 blue and 12 non-blue balls.  Now, since X is a hypergeometric random variable 
it follows that 
 
 
 
E[X] = 12(10/30) = 4 and E[X 2] = 12(18)(1/3)(2/3)/29 + 42 = 512/29 
 
 
 
As E[Y] = 8(12/30) = 16/5, we obtain 
 
 
 
 
 
E[XY] = 
)
29
/
512
48
(
5
2
−
 = 352/29, 
 
 
and 
 
 
 
 
Cov(X, Y) = 352/29 − 4(16/5) = −96/145 
 
64. 
(a) E[X] = E[Xtype 1]p + E[Xtype 2](1 − p) = pµ1 + (1 − p)µ2 
 
 
(b) Let I be the type. 
 
 
 
E[XI] = µI ,  Var(XI) = 
2
I
σ  
 
 
Var(X) = 
)
(
Var
]
[
2
I
I
E
µ
σ
+
 
 
 
 
      = 
2
2
1
2
2
2
1
2
2
2
1
]
)
1(
[
)
1(
)
1(
µ
µ
µ
µ
σ
σ
p
p
p
p
p
p
−
+
−
−
+
+
−
+
 
 
65. 
Let X be the number of storms, and let G(B) be the events that it is a good (bad) year.  Then 
 
 
 
 
E[X] = E[XG]P(G) + E[XB]P(B) = 3(.4) + 5(.6) = 4.2 
 
 
If Y is Poisson with mean λ, then E[Y 2] = λ + λ2.  Therefore, 
 
 
 
 
E[X 2] = E[X 2G]P(G) + E[X 2B]P(B) = 12(.4) + 30(.6) = 22.8 
 
 
Consequently, 
 
 
 
 
Var(X) = 22.8 − (4.2)2 = 5.16 
 
66. 
E[X 2] = 
]}
3
[
]
2
[
]1
[
{
3
1
2
2
2
=
+
=
+
=
Y
X
E
Y
X
E
Y
X
E
 
 
 
   = 
]}
)
7
[(
]
)
5
[(
9
{
3
1
2
2
X
E
X
E
+
+
+
+
 
 
 
   = 
]}
[
2
]
[
24
83
{
3
1
2
X
E
X
E
+
+
 
 
 
   = 
]}
[
2
443
{
3
1
2
X
E
+
  since E[X] = 15 
 
 
Hence, 
 
 
 
Var(X) = 443 − (15)2 = 218. 
 

118 
 
Chapter 7 
67. 
Let Fn denote the fortune after n gambles. 
 
 
E[Fn] = E[E[FnFn−1]] = E[2(2p − 1)Fn−1p + Fn−1 − (2p − 1)Fn−1] 
 
 
 
 
           = (1 + (2p − 1)2)E[Fn− 1] 
 
 
 
 
           = [1 + (2p − 1)2]2E[Fn−2] 
 
 
 
 
          # 
 
 
 
 
           = [1 + (2p − 1)2]nE[F0] 
 
68. 
(a) .6e−2 + .4e−3 
 
 
(b) .6e−2
!
3
3
4.
!
3
2
3
3
3
−
+
e
 
 
 
(c) P{30} = 
3
2
3
3
3
3
2
2
4.
6.
!
3
3
4.
!
3
2
6.
}
0
{
}
0,3
{
−
−
−
−
−
−
+
+
=
e
e
e
e
e
e
P
P
 
 
69. 
(a) 
2
1
0
=
∫
∞
−
−
dx
e
e
x
x
 
 
(b) 
16
1
96
)
4
(
96
1
!
3
0
3
0
3
=
Γ
=
=
∫
∫
∞
−
−
∞
−
dy
y
e
dx
e
x
e
y
x
x
 
 
 
(c) 
81
2
3
2
!
3
4
0
0
3
=
=
∫
∫
∞
−
−
∞
−
−
−
dx
e
e
dx
e
x
e
e
x
x
x
x
x
 
 
70. 
(a) ∫
=
1
0
2
/
1
pdp
 
 
 
(b) 
3
/
1
1
0
2
=
∫
dp
p
 
 
71. 
P{X = i} = ∫
=
1
0
}
{
dp
p
i
X
P
 = 
dp
p
p
i
n
i
n
i
−
−





∫
)
1(
1
0
 
 
 
 
 
 
       = 
)1
/(
1
!)1
(
!)
(!
+
=
+
−






n
n
i
n
i
i
n
 
 
 

Chapter 7 
 
119 
72. 
(a) P{N ≥ i} = 
i
dp
p
dp
p
i
N
P
i
/
1
)
1(
}
{
1
0
1
1
0
=
−
=
≥
∫
∫
−
 
 
(b) P{N = i} = P{N ≥ i} − P{N ≥ i + 1} = 
)1
(
1
+
i
i
 
 
 
(c) E[N] = 
∞
=
=
≥
∑
∑
∞
=
∞
=
1
1
/
1
}
{
i
i
i
i
N
P
. 
 
73. 
(a) E[R] = E[E[RS]] = E[S] = µ 
 
 
(b) Var(RS) = 1, E[RS] = S 
 
 
Var(R) = 1 + Var(S) = 1 + σ2 
 
 
(c) fR(r) = 
ds
s
r
F
s
f
S
R
S
)
(
)
(
∫
 
 
 
 
 = ∫
−
−
−
−
ds
e
e
C
s
r
s
2
/
)
(
2
/
)
(
2
2
2
σ
µ
 
 
 
 
 = ∫












+






+
+
−
−
2
2
2
2
1
2
1
exp
σ
σ
σ
σ
µ
r
S
K
 ds exp {−(ar2 + br)} 
 
 
 
Hence, R is normal. 
 
 
(d) E[RS] = E[E[RSS]] = E[SE[RS]] = E[S 2] = µ2 + σ2 
 
 
 
Cov (R, S) = µ2 + σ2 − µ2 = σ2 
 
75. 
X is Poisson with mean λ = 2 and Y is Binomial with parameters 10, 3/4.  Hence 
 
 
(a) P{X + Y = 2} = P{X = 0)P{Y = 2} + P{X = 1}P{Y = 1} + P{X = 2}P{Y = 0} 
 
 
 
 
  = 
10
2
9
2
8
2
2
)
4
/
1(
2
)
4
/
1
)(
4
/
3
(
1
10
2
)
4
/
1(
)
4
/
3
(
2
10
−
−
−
+






+






e
e
e
 
 
 
(b) P{XY = 0} = P{X = 0} + P{Y = 0} − P{X = Y = 0} 
 
 
 
           = e−2 + (1/4)10 − e−2(1/4)10 
 
 
(c) E[XY] = E[X]E[Y] = 2 ⋅ 10 ⋅ 4
3  = 15 
 

120 
 
Chapter 7 
77. 
The joint moment generating function, E[etX+sY] can be obtained either by using  
 
 
 
 
E[etX+sY] = ∫∫
+
dx
dy
y
x
f
e
sY
tX
)
,
(
 
 
 
or by noting that Y is exponential with rate 1 and, given Y, X is normal with mean Y and 
variance 1.  Hence, using this we obtain  
 
 
 
 
E[etX+sYY] = esYE[EtXY] = 
2
/
2t
Yt
sYe
e
+
 
 
 
and so 
 
 
 
 
E[etX+sY] = 
]
[
)
(
2
/
2
Y
t
s
t
e
E
e
+
 
 
 
 
 
 = 
1
2
/
)
1(
2
−
−
−
t
s
et
, s + t < 1 
 
 
Setting first s and then t equal to 0 gives 
 
 
 
 
E[etX] = 
,
)
1(
1
2
/
2
−
−t
et
 t < 1 
 
 
 
E[esY] = (1 − s)−1, s < 1 
 
78. 
Conditioning on the amount of the initial check gives  
 
 
 
E[Return] = E[ReturnA]/2 + E[ReturnB]/2 
 
 
 
           = {AF(A) + B[1 − F(A)]}/2 + {BF(B) + A[1 − F(B)]}/2 
 
 
 
           = {A + B + [B − A][F(B) − F(A)]}/2 
 
 
 
           > (A + B)/2 
 
 
where the inequality follows since [B − A] and [F(B) − F(A) both have the same sign. 
 
 
(b) If x < A then the strategy will accept the first value seen:  if x > B then it will reject the 
first one seen; and if x lies between A and B then it will always yield return B.  Hence, 
 
 
 
E[Return of x-strategy] = 
otherwise
2
/)
(
 
 
 
 
 
if
B
A
B
x
A
B
+
<
<
 
 
 
(c) This follows from (b) since there is a positive probability that X will lie between A and B. 
 
79. 
Let Xi denote sales in week i. Then 
 
 
 
   E[X1 + X2] = 80 
 
 
Var(X1 + X2) = Var(X1) + Var(X2) + 2 Cov(X1, X2) 
 
 
 
 
= 72 + 2[.6(6)(6)] = 93.6 
 
 
(a) With Z being a standard normal 
 
 
 
 
P(X1 + X2 > 90) = 
90
80
93.6
P Z
−


>




 
 
 
 
 
 
= P(Z > 1.034) ≈ .150 

Chapter 7 
 
121 
 
(b) Because the mean of the normal X1 + X2 is less than 90 the probability that it exceeds 90 
is increased as the variance of X1 + X2 increases. Thus, this probability is smaller when 
the correlation is .2. 
 
 
(c) In this case,  
 
 
 
P(X1 + X2 > 90) = 
90
80
72
2[.2(6)(6)]
P Z


−
>




+


 
 
 
 
 
 
= P(Z > 1.076) ≈ .141 
 

122 
 
Chapter 7 
Theoretical Exercises 
 
1. 
Let µ = E[X].  Then for any a 
 
 
 
E[(X − a)2 = E[(X − µ + µ  − a)2] 
 
 
 
           = E[(X − µ)2] + (µ − a)2 + 2E[(x − µ)(µ − a)] 
 
 
 
           = E[(X − µ)2] + (µ − a)2 + 2(µ − a)E[(X − µ)] 
 
 
 
           = E[(X − µ)2 + (µ − a)2 
 
2. 
E[X − a = 
∫
∫
>
<
−
+
−
a
x
a
x
dx
x
f
a
x
dx
x
f
x
a
)
(
)
(
)
(
)
(
 
 
 
 
     = aF(a) − 
])
(
1[
)
(
)
(
∫
∫
>
<
−
−
+
a
x
a
x
a
F
a
dx
x
xf
dx
x
xf
 
 
 
Differentiating the above yields 
 
derivative = 2af(a) + 2F(a) − af(a) − af(a) − 1 
 
Setting equal to 0 yields that 2F(a) = 1 which establishes the result. 
 
3. 
E[g(X, Y)] = 
da
a
Y
X
g
P
}
)
,
(
{
0
>
∫
∞
 
 
 
 
    = 
∫
∫∫
∫∫
∫
=
>
∞
)
,
(
0
)
,
(
:
,
0
)
,
(
)
,
(
y
x
g
a
y
x
g
y
x
dydx
y
x
daf
dydxda
y
x
f
 
 
 
 
 
 
 
        = ∫∫
dydx
y
x
g
)
,
(
 
 
4. 
g(X) = g(µ) + g′(µ)(X − µ) + g′′(µ) 
2
)
(
2
µ
−
X
 + … 
 
 
  ≈ g(µ) + g′(µ)(X − µ) + g′′(µ) 
2
)
(
2
µ
−
X
 
 
 
Now take expectations of both sides. 
 
5. 
If we let Xk equal 1 if Ak occurs and 0 otherwise then 
 
 
 
 
X = ∑
=
n
k
k
X
1
 
 
Hence, 
 
 
 
E[X] = 
∑
∑
=
=
=
n
k
k
n
k
k
A
P
X
E
1
1
)
(
]
[
 
 
But 
 
 
 
E[X] = 
∑
∑
=
=
=
≥
n
k
k
n
k
C
P
k
X
P
1
1
)
(
}
{
. 
 

Chapter 7 
 
123 
6. 
 X = 
dt
t
X∫
∞
0
)
(
 and taking expectations gives 
 
 
E[X] = 
∫
∫
∞
∞
>
=
0
0
}
{
)]
(
[
dt
t
X
P
dt
t
X
E
 
 
7. 
(a) Use Exercise 6 to obtain that 
 
 
 
E[X] = 
∫
∫
∞
∞
>
≥
>
0
0
}
{
}
{
dt
t
Y
P
dt
t
X
P
 = E[Y] 
 
 
(b) It is easy to verify that 
 
 
 
 
X+ ≥st Y+  and  Y− ≥ st X− 
 
 
 
Now use part (a). 
 
8. 
Suppose X ≥st Y and f is increasing.  Then  
 
 
  P{f(X) > a} = P{X > f −1(a)} 
 
 
 
 
  ≥ P{Y > f −1(a)}  since x ≥st Y 
 
 
 
 
  = P{f(Y) > a} 
 
 
Therefore, f(X) ≥st f(Y) and so, from Exercise 7, 
 
E[f(X)] ≥ E[f(Y)]. 
 
 
 
On the other hand, if E[f(X)] ≥ E[f(Y)] for all increasing functions f, then by letting f be the 
increasing function 
 
 
 
 
f(x) = 
otherwise
0
if
1
t
x >
 
 
 
then 
 
 
 
P{X > t} = E[f(X)] ≥ E[f(Y)] = P{Y > t} 
 
 
and so X >st Y. 
 
9. 
Let 
 
 
 
Ij = 



otherwise
0
flip
 
 
at the
 
begins
 
 
size
 
of
run 
 a 
if
1
th
j
k
 
 
 
 
Then 
 
 
 
 
Number of runs of size k = ∑
+
−
=
1
1
k
n
j
jI  
 

124 
 
Chapter 7 
 
 
 
E[Number of runs of size k = 







∑
+
−
=
1
1
k
n
j
jI
E
 
 
 
 
 
 
 
      = P(I1 = 1) + 
)1
(
)1
(
1
2
=
+
=
+
−
−
=∑
k
n
k
n
j
j
I
P
I
P
 
 
 
 
 
 
 
      = pk(1 − p) + (n − k − 1)pk(1 − p)2 + pk(1 − p) 
 
10. 
1 = 






=






=






∑
∑
∑
∑
∑
n
i
n
i
i
n
n
i
n
i
X
X
nE
X
X
E
X
X
E
1
1
1
1
1
1
 
 
 
Hence, 
 
 
 
 
n
k
X
X
E
n
i
k
i
/
1
1
=






∑
∑
 
 
11. 
Let 
 
 
 
Ij = 

otherwise
0
occurs
never  
 
  
outcome
1
j
 
 
 
Then X = ∑
r
jI
1
 and E[X] = ∫
=
−
r
j
n
jp
1
)
1(
 
 
12. 
Let 
 
 
 
Ij = 

otherwise
0
 
on trial
 
success
1
j  
 
 
 
 
∑
∑
=






n
j
n
j
P
I
E
1
1
  independence not needed 
 
 
 
 
Var
∑
∑
−
=






n
j
j
n
j
p
p
I
1
1
)
1(
  independence needed 
 
13. 
Let 
 
 
 
Ij = 

otherwise
0
at 
 
record
1
j  
 
 
 
∑
∑
∑
∑
=
=
=






n
j
n
j
n
j
n
j
j
X
X
X
P
I
E
I
E
1
1
1
1
1
/
1
}
,...,
 
of
largest 
 
is
 
{
]
[
 
 
 
 
Var
∑
∑
∑





−
=
=






n
n
j
n
j
j
j
I
I
1
1
1
1
1
1
)
(
Var
 
 

Chapter 7 
 
125 
15. 
µ = ∑
=
n
i
ip
1
 by letting Number = ∑
=
n
i
i
X
1
 where 

=
-
-
-
0
success
 
is
 
1
i
Xi
 
 
 
Var(Number) = ∑
=
−
n
i
i
i
p
p
1
)
1(
 
 
 
maximization of variance occur when pi ≡ µ/n 
 
 
minimization of variance when pi  = 1, i = 1, …, [µ], p[µ]+1 = µ − [µ] 
 
 
To prove the maximization result, suppose that 2 of the pi are unequal—say pi ≠ pj.  Consider 
a new p-vector with all other pk, k ≠ i, j, as before and with 
2
j
i
j
i
p
p
p
p
+
=
=
.  Then in the 
variance formula, we must show 
 
 
 






+
−






+
2
1
2
2
j
i
j
i
p
p
p
p
 ≥ pi(1 − pi) + pj(1 − pj) 
 
 
or equivalently, 
 
 
 
2
2
2
)
(
2
j
i
j
i
j
i
p
p
p
p
p
p
−
=
−
+
 ≥ 0. 
 
 
The maximization is similar. 
 
16. 
Suppose that each element is, independently, equally likely to be colored red or blue.  If we 
let Xi equal 1 if all the elements of Ai are similarly colored, and let it be 0 otherwise, then 
∑=
r
i
i
X
1
 is the number of subsets whose elements all have the same color.  Because 
 
 
 
 
[
] ∑
∑
∑
=
=
=
=
=






r
i
A
r
i
i
r
i
i
i
X
E
X
E
1
1
1
)
2
/
1(
2
 
 
 
it follows that for at least one coloring the number of monocolored subsets is less than or 
equal to ∑=
−
r
i
Ai
1
1
)
2
/
1(
 
 
17. 
(
)
2
2
2
2
1
2
2
1
)
1(
)
1(
Var
σ
λ
σ
λ
λ
λ
−
+
=
−
+
X
X
 
 
2
2
2
1
2
2
2
2
2
1
0
)
1(
2
2
)
(
σ
σ
σ
λ
σ
λ
λσ
λ
+
=
⇒
=
−
−
=
d
d
 
 
As Var(λX1 + (1 − λ)X2) = [
]
2
2
1
)
)
1(
(
µ
λ
λ
−
−
+
X
X
E
 we want this value to be small. 
 

126 
 
Chapter 7 
18. 
(a. Binomial with parameters m and Pi + Pj. 
 
 
(b) Using (a) we have that Var(Ni + Nj) = m(Pi + Pj)(1 − Pi − Pj) and thus 
 
 
 
m(Pi + Pj)(1 − Pi − Pj) = mPi(1 − Pi) + mPj(1 − Pj) + 2 Cov(Ni, Nj) 
 
 
 
Simplifying the above shows that  
 
 
 
 
Cov(Ni, Nj) = −mPiPj. 
 
19. 
Cov(X + Y, X − Y) = Cov(X, X) + Cov(X, −Y) + Cov(Y, X) + Cov(Y, −Y) 
 
 
 
 
    = Var(X) − Cov(X, Y) + Cov(Y, X) − Var(Y) 
 
 
 
 
    = Var(X) − Var(Y) = 0. 
 
20. 
(a) Cov(X, YZ) 
 
 
 
= E[XY − E[XZ]Y − XE[YZ] + E[XZ]E[YZ] [Z] 
 
 
 
= E[XYZ] − E[XZ] E[YZ] − E[XZ]E[YZ] + E[XZ]E[YZ] 
 
 
 
= E[XYZ] − E[XZ]E[YZ] 
 
 
 
where the next to last equality uses the fact that given Z, E[XZ] and E[YZ] can be 
treated as constants. 
 
 
(b) From (a)  
 
 
 
 
E[Cov(X, YZ)] = E[XY] − E[E[XZ]E[YZ]] 
 
 
 
On the other hand, 
 
 
 
 
Cov(E[XZ], E[YZ] = E[E[XZ]E[YZ]] − E[X]E[Y] 
 
 
 
and so 
 
 
 
 
E[Cov(X, YZ)] + Cov(E[XZ], E[YZ]) 
= E[XY] − E[X]E[Y] 
 
 
 
 
 
 
 
 
= Cov(X, Y) 
 
 
(c) Noting that Cov(X, XZ) = Var(XZ) we obtain upon setting Y = Z that 
 
 
 
 
Var(X) = E[Var(XZ)] + Var(E[XZ]) 
 
21. 
(a) Using the fact that f integrates to 1 we see that 
 
 
 
c(n, i) ≡ 
dx
x
x
i
n
i
−
−
−
∫
)
1(
1
0
1
 = (i − 1)!(n − i)!/n!.  From this we see that  
 
 
 
E[X(i)] = c(n + 1, i + 1)/c(n, i) = i/(n + 1) 
 
 
]
[
2
)
(i
X
E
 = c(n + 2, i + 2)/c(n, i) =  
)1
)(
2
(
)1
(
+
+
+
n
n
i
i
 
 

Chapter 7 
 
127 
 
and thus 
 
 
 
Var(X(i)) = 
)
2
(
)1
(
)
1
(
2
+
+
−
+
n
n
i
n
i
 
 
 
(b) The maximum of i(n + 1 − i) is obtained when i = (n + 1)/2  and the minimum when i is 
either 1 or n. 
 
22. 
Cov(X, Y) = b Var(X), Var(Y) = b2 Var(X) 
 
 
 
b
b
X
b
X
b
Y
X
=
=
)
(
Var
)
(
Var
)
,
(
2
ρ
 
 
26. 
Follows since, given X, g(X) is a constant and so 
 
 
 
E[g(X)YX] = g(X)E[YX] 
 
27. 
E[XY] = E[E[XYX]] 
 
 
     = E[XE[YX]] 
 
 
Hence, if E[YX] = E[Y], then E[XY] = E[X]E[Y].  The example in Section 3 of random 
variables uncorrelated but not independent provides a counterexample to the converse. 
 
28. 
The result follows from the identity 
 
 
 
E[XY] = E[E[XYX]] = E[XE[YX]]  which is obtained by noting that, given X, X may be  
 
 
treated as a constant. 
 
29. 
x = E[X1 + … + XnX1 + … + Xn = x] = [
]
[
]
∑
∑
=
+
+
=
x
X
X
E
x
X
X
E
i
n
i
...
1
 
 
 
 
 
 
 
        = 
[
]
∑
= x
X
X
nE
i
1
 
 
 
Hence, E[X1X1 + … + Xn = x] = x/n 
 
30. 
E[NiNjNi] = NiE[NjNi] = Ni(n − Ni)
i
j
p
p
−
1
 since each of the n − Ni trials no resulting in 
outcome i will independently result in j with probability pj/(1 − pi).  Hence, 
 
 
E[NiNj] = 
[
]
(
)
[
])
1(
1
]
[
1
2
2
2
2
i
i
i
i
i
j
i
i
i
j
p
np
p
n
p
n
p
p
N
E
N
nE
p
p
−
−
−
−
=
−
−
 
 
 
 
= n(n − 1)pi pj 
 
 
and 
 
 
 
Cov(Ni, Nj) = n(n − 1)pi pj − n2pi pj = −npi pj 
 

128 
 
Chapter 7 
31. 
By induction:  true when t = 0, so assume for t − 1.  Let N(t) denote the number after stage t. 
 
 
 
E[N(t)N(t − 1)] = N(t − 1) − E[number selected] 
 
 
 
 
        = N(t − 1) − N(t − 1)
r
w
b
r
+
+
 
 
 
E[N(t)N(t − 1)] = N(t − 1)
r
w
b
w
b
+
+
+
 
 
 
E[N(t)] = 
w
r
w
b
w
b
t






+
+
+
 
 
32. 
E[X1X2Y = y] = E[X1Y = y]E[X2Y = y] = y2 
 
 
Therefore, E[X1X2Y] = Y2.  As E[XiY] = Y, this gives that 
 
 
 
E[X1X2] = E[E[X1X2Y]] = Ei[Y2],  E[Xi] = E[E[XiY]] = E[Y] 
 
 
Consequently, 
 
 
 
Cov(X1, X2) = E[X1X2] − E[X1]E[X2] = Var(Y) 
 
34. 
(a) E[TrTr−1] = Tr−1 + 1 + (1 − p)E[Tr] 
 
 
(b) Taking expectations of both sides of (a) gives 
 
 
 
 
E[Tr] = E[Tr−1] + 1 + (1 − p)E[Tr] 
 
 
 
or 
 
 
 
 
 
E[Tr] = 
]
[
1
1
1
−
+
rT
E
p
p
 
 
 
(c) Using the result of part (b) gives 
 
 
 
E[Tr] = 
]
[
1
1
1
−
+
rT
E
p
p
 
 
 
 
   = 






+
+
−]
[
1
1
1
1
2
rT
E
p
p
p
p
 
 
 
 
   =  1/p + (1/p)2 + (1/p)2E[Tr−2] 
 
 
 
   = 1/p + (1/p)2 + (1/p)3 + (1/p)3E[Tr−3] 
 
 
 
   = 
]
[
)
/
1(
)
/
1(
0
1
T
E
p
p
r
r
i
i +
∑
=
 
 
 
 
   = ∑
=
r
i
i
p
1
)
/
1(
  since E[T0] = 0. 
 

Chapter 7 
 
129 
35. 
P(Y > X) = 
(
)
j
j
P Y
X X
j p
>
=
∑
 
 
 
 
  = 
(
)
j
j
P Y
j X
j p
>
=
∑
 
 
 
 
  = 
(
)
j
j
P Y
j p
>
∑
 
 
 
 
  =
(1
) j
j
j
p
p
−
∑
 
 
36. 
Condition on the first ball selected to obtain 
 
 
 
Ma,b = 
1
,
,1
−
−
+
+
+
b
a
b
a
M
b
a
b
M
b
a
a
, a, b > 0 
 
 
 
Ma,0 = a,  
  M0,b = b,  
Ma,b = Mb,a 
 
 
 
M2,1 = 3
4 ,     M3,1 = 4
7 ,  
M3,2 = 3/2 
 
37. 
Let Xn denote the number of white balls after the nth drawing 
 
 
 
E[Xn+1Xn] = 
1
1
1
1
)1
(
+






+
−
=






+
−
+
+
+
n
n
n
n
n
X
b
a
b
a
X
X
b
a
X
X
 
 
 
Taking expectations now yields (a). 
 
 
To prove (b), use (a) and the boundary condition M0 = a 
 
 
(c) P{(n + 1)st is white} = E[P{(n + 1)st is whiteXn}] 
 
 
 
 
= 
b
a
M
b
a
X
E
n
n
+
=




+
 
 
40. 
For (a) and (c), see theoretical Exercise 18 of Chapter 6.  For (c) 
 
 
 
E[XY] = E[E[XYX]] = E[XE[YX]] 
 
 
 
 
    = 














−
+
x
x
y
y
X
X
E
µ
σ
σ
ρ
µ
(
 
 
 
 
    = 
(
)
2
2
2
x
x
x
y
x
x
y
y
x
σ
µ
σ
σ
ρ
µ
σ
σ
ρ
µ
µ
+
+
−
 
 
 
and so 
 
 
 
Corr(X, Y) = 
ρ
σ
σ
σ
ρσ
=
x
y
x
y
 

130 
 
Chapter 7 
41. 
(a) No 
 
 
(b) Yes, since fY(xI = 1) = fX(x) = fX(−x) = fY(xI = 0) 
 
 
(c) fY(x) = 
)
(
)
(
2
1
)
(
2
1
x
f
x
f
x
f
X
X
X
=
−
+
 
 
 
(d) E[XY] = E[E[XYX]] = E[XE[YX]] = 0 
 
 
(e) No, since X and Y are not jointly normal. 
 
42. 
If E[YX] is linear in X, then it is the best linear predictor of Y with respect to X. 
 
43. 
Must show that E[Y 2] = E[XY].  Now 
 
 
 
E[XY] = E[XE[XZ]] 
 
 
 
    = E[E[XE[XZ] Z]] 
 
 
 
    = E[E 2[XZ]] = E[Y 2] 
 
44. 
Write Xn = ∑
−
=
1
1
n
X
i
iZ  where Zi is the number of offspring of the ith individual of the (n − 1)st 
generation.  Hence, 
 
 
 
 
E[Xn] = E[E[XnXn−1]] = E[µXn−1] = µE[Xn−1] 
 
 
so, 
 
 
 
 
E[Xn] = µE[Xn−1] = µ2E[Xn−2] … = µnE[X0] = µn 
 
 
(c) Use the above representation to obtain 
 
 
 
 
E[XnXn−1] = µXn−1, Var(XnXn−1) = σ2Xn−1 
 
 
 
Hence, using the conditional Variance Formula, 
 
 
 
 
Var(Xn) = µ2 Var(Xn−1) + σ2µn−1 
 
 
(d) π = P{dies out} 
 
 
 
   = ∑
=
j
j
i
p
j
X
P
}
out
 
dies
{
 
 
 
   = ∑
j
j
j p
π
, since each of the j members of the first generation can be thought of as  
 
 
 
starting their own (independent) branching process. 
 

Chapter 7 
 
131 
46. 
It is easy to see that the nth derivative of ∑
∞
=0
2
!
/
)
2
/
(
j
j
j
t
 will, when evaluated at t = 0, equal 0 
whenever n is odd (because all of its terms will be constants multiplied by some power of t).  
When n = 2j the nth derivative will equal 
)
2!
/(
}
{
j
n
n
n
j
t
dt
d
 plus constants multiplied by powers 
of t.  When evaluated at 0, this gives that 
 
 
 
 
E[Z2j] - (2j)!/(j!2j) 
 
47. 
Write X = σZ + µ where Z is a standard normal random variable.  Then, using the binomial 
theorem, 
 
 
 
 
E[X n] = 
i
n
i
i
n
i
Z
E
i
n
−
=∑






µ
σ
]
[
0
 
 
 
Now make use of theoretical exercise 46. 
 
48. 
φY(t) = E[etY] = E[et(aX+b)] = etbE[etaX] = etbφX(ta) 
 
49. 
Let Y = log(X).  Since Y is normal with mean µ and variance σ2 it follows  that its moment 
generating function is 
 
 
 
 
M(t) = E[etY] = 
2
/
2
2t
t
e
σ
µ +
 
 
 
Hence, since X = eY, we have that 
 
 
 
 
E[X] = M(1) = 
2
/
2
σ
µ+
e
 
 
 
and 
 
 
 
 
E[X 2] = M(2) = 
2
2
2
σ
µ+
e
 
 
 
Therefore,  
 
 
 
 
Var(X) = 
)1
(
2
2
2
2
2
2
2
2
−
=
−
+
+
+
σ
σ
µ
σ
µ
σ
µ
e
e
e
e
 
 
50. 
ψ(t) = log φ(t) 
 
 
ψ ′(t) = φ′(t)/φ(t) 
 
 
ψ ′′(t) = 
)
(
))
(
(
)
(
)
(
2
2
t
t
t
t
φ
φ
φ
φ
′
−
′′
 
 
 
ψ ′′(t)
2
2
0
])
[
(
]
[
X
E
X
E
t
−
=
=
 = Var(X). 
 

132 
 
Chapter 7 
51. 
Gamma (n, λ) 
 
52. 
Let φ(s, t) = E[esX+tY] 
 
 
 
 
]
[
]
[
)
,
(
0
0
0
0
2
XY
E
XYe
E
t
s
t
s
t
s
tY
sX
t
s
=
=
∂
∂
∂
=
=
+
=
=
φ
 
 
 
 
]
[
)
,
(
],
[
)
,
(
0
0
0
0
Y
E
t
s
t
X
E
t
s
s
t
s
t
s
=
∂
∂
=
∂
∂
=
=
=
=
φ
φ
 
 
53. 
Follows from the formula for the joint moment generating function. 
 
54. 
By symmetry, E[Z 3 ] = E[Z] = 0 and so Cov(Z,Z 3) = 0. 
 
55. 
(a) This follows because the conditional distribution of Y + Z given that Y = y is normal with 
mean y and variance 1, which is the same as the conditional distribution of X given that  
 
 
Y = y. 
 
 
(b) Because Y + Z and Y  are both linear combinations of the independent normal random 
variables Y and Z, it follows that Y + Z, Y has a bivariate normal distribution. 
 
 
(c)        µx = E[X] = E[Y + Z] = µ 
 
 
  
2
x
σ  = Var(X) = Var(Y + Z) =Var(Y) + Var(Z) = σ2 + 1 
 
 
ρ = Corr(X, Y) = 
2
2
Cov(
, )
1
1
Y
Z Y
σ
σ σ
σ
+
=
+
+
 
 
 
(d) and (e)  The conditional distribution of Y given X = x is normal with mean 
 
 
 
 
E[YX = x] = µ + 
2
2
(
)
(
)
1
x
x
x
x
σ
σ
ρ
µ
µ
µ
σ
σ
−
=
+
−
+
 
 
 
 
 
and variance 
 
 
 
 
 
Var(YX = x) = 
2
2
2
2
2
1
1
1
σ
σ
σ
σ
σ


−
=


+
+


  
 

Chapter 8 
 
133 
Chapter 8 
 
Problems 
 
1. 
P{0 ≤ X ≤ 40} = 1 − P{X − 20 > 20} ≥ 1 − 20/400 = 19/20 
 
2. 
(a) P{X ≥ 85} ≤ E[X]/85 = 15/17 
 
 
(b) P{65 ≤ X ≤ 85) = 1 − P{X − 75 > 10} ≥ 1 − 25/100 
 
 
(c) 
n
n
X
P
n
i
i
25
25
5
75
/
1
≤






>
−
∑
=
 so need n = 10 
 
3. 
Let Z be a standard normal random variable.  Then, 
 
 
 






>
−
∑
=
5
75
/
1
n
i
i n
X
P
 ≈ P{Z > 
n } ≤ .1 when n = 3 
 
4. 
(a) 
15
/
20
15
20
1
≤






>
∑
=
i
i
X
P
 
 
 
(b) 






>
=






>
∑
∑
=
=
20
1
20
1
5.
15
15
i
i
i
i
X
P
X
P
 
 
 
 
 
      ≈ 






−
>
20
20
5.
15
Z
P
 
 
 
 
 
      = P{Z > −1.006} 
 
 
 
 
      ≈ .8428 
 
5. 
Letting Xi denote the ith roundoff error it follows that 





∑
=
50
1
i
i
X
E
 = 0,  
 
Var





∑
=
50
1
i
i
X
 = 50 Var(X1) = 50/12, where the last equality uses that .5 + X is uniform (0, 1) 
and so Var(X) = Var(.5 + X) = 1/12.  Hence, 
 
 
 
{
}
3
>
∑
i
X
P
 ≈ P{N(0, 1) > 3(12/50)1/2} by the central limit theorem  
 
 
 
 
    = 2P{N(0, 1) > 1.47 = .1416 
 
6. 
If Xi is the outcome of the ith roll then E[Xi] = 7/2   Var(Xi) = 35/12 and so 
 






≤
=






≤
∑
∑
=
=
5.
300
300
79
1
79
1
i
i
i
i
X
P
X
P
 
 
 
 
 
  ≈ 
}
58
.1
)1,0
(
{
)
12
/
35
79
(
)
2
/
7
(
79
5.
300
)1,0
(
2
/
1
≤
=






×
−
≤
N
P
N
P
 = .9429 

134 
 
Chapter 8 
7. 
}
5.
)1,0
(
{
)
25
100
(
500
525
)1,0
(
525
100
1
>
=






×
−
>
≈






>
∑
=
N
P
N
P
X
P
i
i
 = .3085 
 
 
where the above uses that an exponential with mean 5 has variance 25. 
 
8. 
If we let Xi denote the life of bulb i and let Ri be the time to replace bulb i then the desired 
probability is 






≤
+∑
∑
=
=
99
1
100
1
550
i
i
i
i
R
X
P
.  Since ∑
∑
+
i
i
R
X
 has mean 100 × 5 + 99 × .25 = 
524.75 and variance 2500 + 99/48 = 2502 it follows that the desired probability is 
approximately equal to P{N(0, 1) ≤ [550 − 524.75]/(2502)1/2} = P{N(0, 1) ≤ .505} = .693 
 
It should be noted that the above used that 
 
 
 
Var(Ri) = Var






]1,0
[
Unif
2
1
 = 1/48 
 
9. 
Use the fact that a gamma (n, 1) random variable is the sum of n independent exponentials 
with rate 1 and thus has mean and variance equal to n, to obtain: 
 
 
 






>
−
01
.
n
n
X
P
 = {
}
n
n
n
X
P
01
.
/
>
−
 
 
 
 
 
         ≈ {
}
n
N
P
01
.
)1,0
(
>
 
 
 
 
 
         = 
{
}
n
N
P
01
.
)1,0
(
2
>
 
 
 
Now P{N(0, 1) > 2.58} = .005 and so n = (258)2. 
 
10. 
If Wn is the total weight of n cars and A is the amount of weight that the bridge can withstand 
then Wn − A is normal with mean 3n − 400 and variance .09n + 1600.  Hence, the probability 
of structural damage is 
 
 
 
 
P{Wn − A ≥ 0} ≈ {
}
1600
09
.
/)
3
400
(
+
−
≥
n
n
Z
P
 
 
 
Since P{Z ≥ 1.28} = .1 the probability of damage will exceed .1 when n is such that 
 
 
 
 
400 − 3n ≤ 1.28
1600
09
.
+
n
 
 
 
The above will be satisfied whenever n ≥ 117. 
 
12. 
Let Li denote the life of component i. 
 
 
 
 
)
101
(
50
10
1
1000
100
1
+
=





∑
=
i
iL
E
 = 1505 
 
 
Var
∑
∑
∑
=
=
=
+
+
=






+
=






100
1
2
2
100
1
2
100
1
100
1
)
101
)(
100
(
)
100
(
10
10
i
i
i
i
i
i
L
 
 
 
Now apply the central limit theorem to approximate. 

Chapter 8 
 
135 
13. 
(a) 






>
−
=
>
7
/
15
5
/
14
74
}
80
{
X
P
X
P
 ≈ PPZ > 2.14} ≈ .0162 
 
 
(b) 






>
−
=
>
7
/
24
8
/
14
74
}
80
{
Y
P
Y
P
 ≈ P{Z > 3.43} ≈ .0003 
 
 
(c) Using that 
25
/
196
64
/
196
)
(
+
=
−X
Y
SD
 ≈ 3.30 we have 
 
 
 
}
2.2
{
>
−X
Y
P
 = 
}
30
.3
/
2.2
30
.3
/
}
{
>
−X
Y
P
 
 
 
 
 
       ≈ P{Z > .67} ≈ .2514 
 
 
(d)  same as in (c) 
 
14. 
Suppose n components are in stock.  The probability they will last for at least 2000 hours is 
 
 
 
p = 






−
≥
≈






≥
∑
=
n
n
Z
P
X
P
n
i
i
30
100
2000
2000
1
 
 
 
where Z is a standard normal random variable.  Since 
 
 
.95 = P{Z ≥ −1.64} it follows that p ≥ .95 if 
 
 
 
 
n
n
30
100
2000 −
 ≤ −1.64 
 
 
or, equivalently, 
 
 
 
 
(2000 − 100n)/
n  ≤ −49.2 
 
 
and this will be the case if n ≥ 23. 
 
15. 






>
∑
=
000
,
10
1
000
,
700
,2
i
i
X
P
 ≈ P{Z ≥ (2,700,000 − 2,400,000)/(800 ⋅ 100)} = P{Z ≥ 3.75} ≈ 0 
 
18. 
Let Yi denote the additional number of fish that need to be caught to obtain a new type when 
there are at present i distinct types.  Then Yi is geometric with parameter 
4
4
i
−. 
 
 
 
E[Y] = 
3
25
4
2
4
3
4
1
3
0
=
+
+
+
=





∑
=
i
iY
E
 
 
 
 
Var[Y] = Var
9
130
12
2
9
4
3
0
=
+
+
=





∑
=
i
iY
 
 

136 
 
Chapter 8 
 
Hence, 
 
 
 
 
10
1
9
1300
3
25
3
25
≤






>
−
Y
P
 
 
 
and so we can take a = 
3
1300
25 −
,  b = 
3
1300
25 +
. 
 
 
Also, 
 
 
 
10
1
9
130
130
3
25
2 =
+
≤


>
−
a
a
Y
P
 when a = 
3
1170 . 
 
 
Hence 






+
>
3
1170
25
Y
P
 ≤ .1. 
 
20. 
g(x) = xn(n−1) is convex.  Hence, by Jensen’s Inequality 
 
 
E[Y n/(n−1)] ≥ E[Y])n/(n−1)  Now set Y = X n−1 and so  
 
E[X n] ≥ (E[X n−1])n/(n−1) or (E[X n])1/n ≥ (E[X n−1])1/(n−1) 
 
21. 
No 
 
22. 
(a) 20/26 ≈ .769 
 
 
(b) 20/(20 + 36) = 5/14 ≈ .357 
 
 
(d) p ≈ P{Z ≥ (25.5 − 20)/
20 } ≈ P{Z ≥ 1.23} ≈ .1093 
 
 
(e) p = .112184 
 

Chapter 8 
 
137 
Theoretical Exercises 
 
1. 
This follows immediately from Chebyshev’s inequality. 
 
2. 
P{D >α} = P{X − µ > αµ} ≤ 
2
2
2
2
2
1
r
α
µ
α
ς
=
 
 
3. 
(a) 
λ
λ
λ
=
 
 
 
(b) 
)
1
/(
)
1(
p
np
p
np
np
−
=
−
 
 
 
(c) answer = 1 
 
 
(d) 
3
12
/
1
2
/
1
=
 
 
 
(e) answer = 1 
 
 
(d) answer = 
σ
µ /
 
 
4. 
For ε > 0, let δ  > 0 be such that g(x) − g(c)  < ε whenever x − c ≤ δ.  Also, let B be such 
that g(x)  < B.  Then, 
 
 
 
 
E[g(Zn)] = 
∫
∫
>
−
≤
−
+
δ
δ
c
x
n
c
x
n
x
dF
x
g
x
dF
x
g
)
(
)
(
)
(
)
(
 
 
 
 
 
≤ (ε + g(c))P{Zn − c ≤ δ} + BP{Zn − c > δ} 
 
 
In addition, the same equality yields that 
 
 
 
 
E[g(Zn)] ≥ (g(c) − ε)P{Zn − c ≤ δ} − BP{Zn − c > δ} 
 
 
Upon letting n → ∞ , we obtain that  
 
 
 
 
lim sup E[g(Zn)] ≤ g(c) + ε 
 
 
 
lim inf  E[g(Zn)] ≥ g(c) − ε 
 
 
The result now follows since ε is arbitrary. 
 
5. 
Use the notation of the hint.  The weak law of large numbers yields that 
 
 
 
 
0
}
/)
...
(
{
lim
1
=
>
−
+
+
∞
→
ε
c
n
X
X
P
n
n
 
 

138 
 
Chapter 8 
 
Since X1 + … + Xn is binomial with parameters n, x, we have 
 
 
 
 
k
n
k
n
k
n
x
x
k
n
n
k
f
n
X
X
f
E
−
=
−






=










+
+
∑
)
1(
)
/
(
...
1
1
 
 
 
The result now follows from Exercise 4. 
 
6. 
E[X] = ∑
∑
=
∞
+
=
=
+
=
k
i
k
i
i
X
P
i
i
X
P
i
1
1
}
{
}
{
 
 
 
  ≥ 
}
{
1
k
X
P
i
k
i
=
∑
=
 
 
 
  = P{X = k}k(k + 1)/2 
 
 
  ≥ 
}
{
2
2
k
X
P
k
=
 
 
7. 
Take logs and apply the central limit theorem 
 
8. 
It is the distribution of the sum of t independent exponentials each having rate λ. 
 
9. 
1/2 
 
10. 
Use the Chernoff bound:  e−tiM(t) = 
ti
et
e
−
−)1
(
λ
 will obtain its minimal value when t is chosen 
to satisfy 
 
 
 
 
λet = i, and this value of t is negative provided i < λ. 
 
 
Hence, the Chernoff bound gives 
 
 
 
 
P{X ≤ i} ≤ ei−λ(λ/i)i 
 
11. 
e−tiM(t) = (pet + q)ne−ti and differentiation shows that the value of t that minimizes it is such 
that 
 
 
 
npet = i(pet + q) or et = 
p
i
n
iq
)
( −
 
 
 
Using this value of t, the Chernoff bound gives that 
 
 
 
 
P{X ≥ i} ≤ 
i
i
i
n
iq
p
i
n
q
i
n
iq
)
/(
)
( −






+
−
 
 
 
 
= 
n
i
i
i
i
n
i
n
q
i
p
i
n
nq
)
(
)
(
)
(
−
−
 
 
12. 
1 = E[eθX] ≥ eθE[X] by Jensen’s inequality.   
 
 
Hence, θE[X] ≤ 0 and thus θ > 0. 
 

Chapter 9 
 
139 
Chapter 9 
 
Problems and Theoretical Exercises 
 
1. 
(a) P(2 arrivals in (0, s) 2 arrivals in (0, 1)} 
 
 
 
=P{2 in (0, s), 0 in (s, 1)}/e−λλ2/2) 
 
 
= [e−λs(λs)2/2][e−(1−s)λ]/(e−λλ2/2) = s2 = 1/9 when s = 1/3 
 
 
(b) 1 − P{both in last 40 minutes) = 1 − (2/3)2 = 5/9 
 
2. 
e−3s/60 
 
3. 
e−3s/60 + (s/20)e−3s/60 
 
8. 
The equations for the limiting probabilities are: 
 
 
 
 
∏c  = .7∏c + .4∏s + .2∏g 
 
 
 
∏s = .2∏x + .3∏s + .4∏g 
 
 
 
∏g = .1∏c + .3∏s + .4∏g 
 
 
 
∏c + ∏s + ∏g = 1 
 
 
and the solution is: ∏c = 30/59, ∏s = 16/59, ∏g = 13/59.  Hence, Buffy is cheerful 3000/59 
percent of the time. 
 
9. 
The Markov chain requires 4 states: 
 
 
 
0 = RR = Rain today and rain yesterday 
 
 
 
 
 
1 = RD = Dry today, rain yesterday 
 
 
 
2 = DR = Rain today, dry yesterday 
 
 
 
3 = DD = Dry today and dry yesterday 
 
 
with transition probability matrix 
 
 
 
 
P = 
8.
2.
0
0
0
0
6.
4.
7.
3.
0
0
0
0
2.
8.
 
 

140 
 
Chapter 9 
 
The equations for the limiting probabilities are: 
 
 
 
∏0 = .8∏0 + .4∏2 
 
 
∏1 = .2∏0 + .6∏2 
 
 
∏2 = .3∏1 + .2∏3 
 
 
∏3 = .7∏1 + .8∏3 
 
 
∏0 + ∏1 + ∏2 + ∏3 = 1 
 
 
which gives 
 
 
 
∏0 = 4/15, ∏1 = ∏2 = 2/15, ∏3 = 7/15. 
 
 
Since it rains today when the state is either 0 or 2 the probability is 2/5. 
 
10. 
Let the state be the number of pairs of shoes at the door he leaves from in the morning.  
Suppose the present state is i, where i > 0.  Now after his return it is equally likely that one 
door will have i and the other 5 − i pairs as it is that one will have i − 1 ant the other 6 − i.  
Hence, since he is equally likely to choose either door when he leaves tomorrow it follows 
that 
 
 
 
Pi,i = Pi,5−i = Pi,i−1 = Pi,6−i = 1/4 
 
 
provided all the states i, 5 − i, i − 1, 6 − i are distinct.  If they are not then  the probabilities are 
added.  From this it is easy to see that the transition matrix Pij, i, j = 0, 1, …, 5 is as follows: 
 
 
 
 
P = 
4
/
1
4
/
1
0
0
4
/
1
4
/
1
0
4
/
1
4
/
1
4
/
1
4
/
1
0
0
0
2
/
1
2
/
1
0
0
0
4
/
1
4
/
1
4
/
1
4
/
1
0
4
/
1
4
/
1
0
0
4
/
1
4
/
1
2
/
1
0
0
0
0
2
/
1
 
 
 
Since this chain is doubly stochastic (the column sums as well as the row sums all equal to 
one) it follows that ∏i = 1/6, i = 0, …, 5, and thus he runs barefooted one-sixth of the time. 
 
11. 
(b) 1/2 
 
 
(c) Intuitively, they should be independent. 
 
 
(d) From (b) and (c) the (limiting) number of molecules in urn 1 should have  a binomial 
distribution with parameters (M, 1/2). 
 
 

Chapter 10 
 
141 
Chapter 10 
 
1. 
(a) After stage k the algorithm has generated a random permutation of 1, 2, …, k.  It then 
puts element k + 1 in position k + 1; randomly chooses one of the positions 1, …, k + 1 
and interchanges the element in that position with element k + 1. 
 
 
(b) The first equality in the hint follows since the permutation given will be the permutation 
after insertion of element k if the previous permutation is i1, …, ij−1, i, ij, …, ik−2 and the 
random choice of one of the k positions of this permutation results in the choice of 
position j. 
 
2. 
Integrating the density function yields that that distribution function is 
 
 
 
 
F(x) = 
0
,2
/
1
0
,
2
/
2
2
>
−
>
−
x
e
x
e
s
x
 
 
which yields that the inverse function is given by 
 
 
 
 
F−1(u) = 
2
/
1
 
if
2
/
])
1[2
log(
12
 
if
2
/)
2
log(
>
−
−
<
u
u
u
u
 
 
 
Hence, we can simulate X from F by simulating a random number U and setting X = F −1(U). 
 
3. 
The distribution function is given by 
 
 
 
 
F(x) = 
6
3
,2
12
/
,3
2
,1
4
/
2
2
≤
≤
−
−
≤
≤
+
−
x
x
x
x
x
x
 
 
 
Hence, for u ≤ 1/4, F−1(u) is the solution of 
 
 
 
 
x2/4 − x + 1 = u 
 
 
that falls in the region 2 ≤ x ≤ 3.  Similarly, for u ≥ 1/4, F−1(u) is the solution of 
 
 
 
 
x − x2/12 − 2 = u 
 
 
that falls in the region 3 ≤ x ≤ 6.  We can now generate X from F by generating a random 
number U and setting X = F−1(U). 
 
4. 
Generate a random number U and then set X = F−1(U).  If U ≤ 1/2 then X = 6U − 3, whereas if 
U ≥ 1/2 then X is obtained by solving the quadratic 1/2 + X 2/32 = U in the region 0 ≤ X ≤ 4. 
 

142 
 
Chapter 10 
5. 
The inverse equation F−1(U) = X is equivalent to 
 
 
or  
 
 
 
1 − 
β
αX
e−
 = U 
 
 
 
X = {−log(1 − U)/α}1/β 
 
 
Since 1 − U has the same distribution as U we can generate from F by generating a random 
number U and setting X = {−log(U)/α}1/β. 
 
6. 
If λ(t) = ctn then the distribution function is given by 
 
 
 
 
1 − F(t) = exp{−ktn+1}, t ≥ 0  where k = c/(n + 1) 
 
 
Hence, using the inverse transform method we can generate a random number U and then set 
X such that 
 
 
 
 
exp{−kXn+1} = 1 − U 
 
 
or 
 
 
 
 
X = {−log(1 − U)/k}1/(n+1) 
 
 
Again U can be used for 1 − U. 
 
7. 
(a) The inverse transform method shows that U1/n works. 
 
 
(b) P{MaxUi ≤ v} = P{U1 ≤ x, …, Un ≤ x} 
 
 
 
 
    = ∏P{Ui ≤ x}  by independence 
 
 
 
 
    = xn 
 
 
(c) Simulate n random numbers and use the maximum value obtained. 
 
8. 
(a) If Xi has distribution Fi , i = 1, …, n, then, assuming independence, F is the distribution of 
MaxXi.  Hence, we can simulate from F by simulating Xi, i = 1, …, n and setting  
 
 
X = MaxXi. 
 
 
(b) Use the method of (a) replacing Max by Min throughout. 
 
9. 
(a) Simulate Xi from Fi, i = 1, 2.  Now generate a random number U and set X equal to X1 if 
U < p and equal to X2 if U > p. 
 
 
(b) Note that 
 
 
 
 
F(x) = 
)
(
3
2
)
(
3
1
2
1
x
F
x
F
+
 
 
 
 
where 
 
 
 
 
F1(x) = 1 − e−3x,    x > 0,  F2(x) = x,   0 < x < 1 

Chapter 10 
 
143 
 
 
 
Hence, using (a) let U1, U2, U3 be random numbers and set 
 
 
 
 
 
X = 
3
/
1
 
if
3
/
1
if
3
/)
log(
3
2
3
1
>
<
−
U
U
U
U
 
 
 
 
where the above uses that −log(U1)/3 is exponential with rate 3. 
 
10. 
With g(x) = λe−λx 
 
 
 
 
}
2
/
)
(
exp{
)
2
(
2
}
2
/]
)
[(
exp{
)
2
(
2
)
2
(
2
)
(
)
(
2
2
/
1
2
/
2
2
2
/
1
2
/
1
2
/
2
2
λ
π
λ
λ
λ
π
λ
π
λ
λ
λ
−
−
−
−
−
=
=
−
−
x
e
x
e
e
x
g
x
f
x
x
 
 
 
Hence, c = 
]
)
2
(
/[
2
2
/
1
2
/
2
π
λ
λ
e
 and simple calculus shows that this is minimized when λ = 1. 
 
11. 
Calculus yields that the maximum value of f(x)/g(x) = 60x3(1 − x)2 is attained when x = 3/5 
and is thus equal to 1296/625. Hence, generate random numbers U1 and U2 and set X = U1 if 
U2 ≤
108
/
)
1(
3125
2
1
3
1
U
U
−
.  If not, repeat. 
 
12. 
Generate random numbers U1, …, Un, and approximate the integral by [k(U1) + … + k(Un)]/n.  
This works by the law of large numbers since E[k(U)] = ∫
1
0
)
(
dx
x
k
. 
 
16. 
E[g(X)/f(X)] = ∫
∫
=
dx
x
g
dx
x
f
x
f
x
g
)
(
)
(
)]
(
/)
(
[
 
 
 
 


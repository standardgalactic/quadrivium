1
Chapter 1
1–1.
(a) P(A ∪B ∪C) = 1 −P(A ∪B ∪C) = 1 −0.25 = 0.75
(b) P(A ∪B) = P(A) + P(B) −P(A ∩B) = 0.18
1–2.
(a)
A
B
C
A∩(B∩C) = (A∩B)∩C
A
B
C
A∪(B∪C) = (A∪B)∪C
U
U
(b)
A
B
C
A∪C
A
B
C
A∪(B∩C) = (A∪B)∩C(A∪C)
U
U
A∪B
=
A
A  C
B
C
A
B
C
A∩(B∪C) = (A∩B)∪(A∩C)
U
U
=
(c)
A
B
A∩B = A
U
(d)
A
B
A∪B = B
U
(e)
A
B
U
A∩B = ∅ ⇒  A⊂B
(f)
A B C
U
A⊂B and B⊂C ⇒ A⊂C
1–3.
(a) A ∩B = {5},
(b) A ∪B = {1, 3, 4, 5, 6, 7, 8, 9, 10},
(c) A ∩B = {2, 3, 4, 5},
(d) A ∩(B ∩C) = U = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10},
(e) A ∩(B ∪C) = {1, 2, 5, 6, 7, 8, 9, 10}

2
1–4. P(A) = 0.02, P(B) = 0.01, P(C) = 0.015
P(A ∩B) = 0.005, P(A ∩C) = 0.006
P(B ∩C) = 0.004, P(A ∩B ∩C) = 0.002
P(A ∪B ∪C) = 0.02 + 0.01 + 0.015 −0.005 −0.006 −0.004 + 0.002 = 0.032
1–5. S = {(t1, t2): t1 ∈R, t2 ∈R, t1 ≥0, t2 ≥0}
A
t1
t2
t1 + t2 = 0.3
B
t1
t2
0.15
0.15
C
t1
t2
A = {(t1, t2): t1 ∈R, t2 ∈R, 0 ≤t1 ≤0.3, 0 ≤t2 ≤0.3 −t1}
B = {(t1, t2): t1 ∈R, t2 ∈R, 0 ≤t1 ≤0.15, 0 ≤t2 ≤0.15}
C = {(t1, t2): t1 ∈R, t2 ∈R, t1 ≥0, t2 ≥0, t1 −0.06 ≤t2 ≤t1 + 0.06}
1–6.
(a) S = {(x, y): x ∈R, y ∈R, 0 ≤x ≤y ≤24}
(b)
i)
t1
t2
24
23
24
1
y – x = 1
ii)
x
y
24
t2
t1
iii)
x
y
24
19.2
24
4.8

3
1–7. S = {NNNNN, NNNND, NNNDN, NNNDD, NNDNN,
NNDND, NNDD, NDNNN, NDNND, NDND,
NDD, DNNNN, DNNND, DNND, DND, DD}
1–8. {0, 1}A = {∅, {a}, {b}, {c}, {d}, {a, b}, {a, c}, {a, d}, {b, c}, {b, d},
{c, d}, {a, b, c}, {a, b, d}, {a, c, d}, {b, c, d}, {a, b, c, d}}
1–9. N = Not Defective, D = Defective
(a) S = {NNN, NND, NDN, NDD, DNN, DND, DDN, DDD}
(b) S = {NNNN, NNND, NNDN, NDNN, DNNN}
1–10. p′ = Lot Fraction Defective
50 · p′ = Lot No. of Defectives
P(Scrap Lot|n = 10, N = 50, p′) = 1 −
µ50p′
0
¶ µ50(1 −p′)
10
¶
µ50
10
¶
If p′ = 0.1, P(scrap lot) ∼= 0.689.
She might wish to increase sample size.
1–11. 6 · 5 = 30 routes
1–12. 263 · 103 = 17,576,000 possible plates ⇒scheme feasible
1–13.
µ15
6
¶ µ8
2
¶ µ4
1
¶
= 560,560 ways
1–14. P(X ≤2) =
2
X
k=0
µ20
k
¶ µ 80
4 −k
¶
µ100
0.4
¶
∼= 0.97
1–15. P(Accept|p′) =
1
X
k=0
µ300p′
k
¶ µ300(1 −p′)
10 −k
¶
µ300
10
¶
1–16. There are 512 possibilities, so the probability of randomly selecting one is 5−12.
1–17.
µ8
2
¶
= 28 comparisons

4
1–18.
µ40
2
¶
= 780 tests
1–19. P 40
2
= 40!
38! = 1560 tests
1–20.
µ10
5
¶
= 252
1–21.
µ5
1
¶ µ5
1
¶
= 25
µ5
2
¶ µ5
2
¶
= 100
1–22. [1 −(0.2)(0.1)(0.1)][1 −(0.2)(0.1)](0.99) = 0.968
1–23. [1 −(0.2)(0.1)(0.1)][1 −(0.2)(0.1)](0.9) = 0.880
1–24. RS = R1{1 −[1 −(1 −(1 −R2)(1 −R4))(R5)][1 −R3]}
1–25. S = Siberia
U = Ural
P(S) = 0.6, P(U) = 0.4, P(F|S) = P(F|S) = 0.5
P(F|U) = 0.7, P(F|U) = 0.3
P(S|F) =
(0.6) · (0.5)
(0.6)(0.5) + (0.4)(0.3)
.= 0.714
1–26. RS = (0.995)(0.993)(0.994) = 0.9821
1–27. A: 1st ball numbered 1
B: 2nd ball numbered 2
P(B) = P(A) · P(B|A) + P(A) · P(B|A)
= 1
m ·
1
m −1 + m −1
m
· 1
m
= m2 −m + 1
m2(m −1)
1–28. 9 × 9 −9 = 72
possible numbers
D1 + D2 even:
32 possibilities
P(D1 odd and D2 odd|D1 + D2 even) = 20
32.

5
1–29. A: over 6′
M: male
F: female
P(M) = 0.6, P(F) = 0.4, P(A|M) = 0.2, P(A|F) = 0.01
P(F|A) =
P(F) · P(A|F)
P(F) · P(A|F) + P(M) · P(A|M)
=
(0.04)(0.01)
(0.4)(0.01) + (0.6)(0.2) = 0.004
0.124
∼= 0.0323
1–30. A: defective
Bi: production on machine i
(a) P(A) = P(B1) · P(A|B1) + P(B2) · P(A|B2) + P(B3) · P(A|B3)
+ P(B4) · P(A|B4)
= (0.15)(0.04) + (0.30)(0.03) + (0.20)(0.05) + (0.35)(0.02)
= 0.032
(b) P(B3|A) = (0.2)(0.05)
0.032
= 0.3125
1–31. r = radius
P(closer to center) =
π
³r
2
´2
πr2
= 1
4
1–32. P(A ∪B ∪C) = P((A ∪B) ∪C)
(associative law)
= P(A ∪B) + P(C) −P((A ∪B) ∩C)
= P(A) + P(B) −P(A ∩B) + P(C) −P((A ∩C) ∪(B ∩C))
= P(A) + P(B) + P(C) −P(A ∩B) −P(A ∩C)
−P(B ∩C) + P(A ∩B ∩C)
1–33. For k = 2, P(A1 ∪A2) = P(A1) + P(A2) −P(A1 ∩A2); Thm. 1–3.
Using induction we show that if true for k −1, then true for k, i.e.,

6
If
P(A2 ∪A3 ∪· · · ∪Ak) =
k
X
i=2
P(Ai) −
X
2≤i<j≤k
P(Ai ∩Aj) +
X
2≤i<j<r≤k
P(Ai ∩Aj ∩Ar)
−
X
2≤i<j<r<ℓ≤k
P(Ai ∩Aj ∩Ar ∩Aℓ) + · · ·
(Eq. 1)
Then
P(A1 ∪A2 ∪· · · ∪Ak) =
k
X
i=1
P(Ai) −
X
1≤i<j≤k
P(Ai ∩Aj) +
X
1≤i<j<r≤k
P(Ai ∩Aj ∩Ar)
−
X
1≤i<j<r<ℓ≤k
P(Ai ∩Aj ∩Ar ∩Aℓ) + · · ·
(Eq. 2)
By hypothesis, and letting A1 ∩Ai replace Ai in Eq. 1,
P((A1∩A2)∪(A1∩A3)∪· · ·∪(A1∩Ak)) =
k
X
i=2
P(A1∩Ai)−
X
2≤i<j≤k
P(A1∩Ai∩Aj)
+
X
2≤i<j<r≤k
P(A1∩Ai∩Aj∩Ar)−
X
2≤i<j<r≤k
P(A1∩Ai∩Aj∩Ar∩Aℓ)+· · ·
(Eq. 3)
By Thm. 1–3,
P(A1 ∪(A2 ∪A3 ∪· · · ∪Ak)) = P(A1) + P(A2 ∪A3 ∪· · · ∪Ak)
−P((A1 ∩A2) ∪· · · ∪(A1 ∩Ak))
So from using Eq. 1 through 3,
P(A1 ∪A2 ∪· · · ∪Ak)
= P(A1) +
" k
X
i=2
P(Ai) −
X
2≤i<j≤k
P(Ai ∩Aj) +
X
2≤i<j<r≤k
P(Ai ∩Aj ∩Ar) −· · ·
#
−
" k
X
i=2
P(A1 ∩Ai) −
X
2≤i<j≤k
P(A1 ∩Ai ∩Aj) +
X
2≤i<j<r≤k
P(A1 ∩Ai ∩Aj ∩Ar) −· · ·
#
=
k
X
i=1
P(Ai) −
X
1≤i<j≤k
P(Ai ∩Aj) +
X
1≤i<j<r≤k
P(Ai ∩Aj ∩Ar)
+ · · · + (−1)k−1 · P(A1 ∩A2 ∩· · · ∩Aj)

7
1–35. P(B) = (365)(364) · · · (365 −n + 1)
365n
n
10
20
21
22
23
24
25
30
40
50
60
P(B)
0.117
0.411
0.444
0.476
0.507
0.538
0.569
0.706
0.891
0.970
0.994
1–36. P(win on 1st throw) = 6
36 + 2
36 = 8
36
P(win after 1st throw) = P(win on 4) + P(win on 5) + P(win on 6)
+ P(win on 8) + P(win on 9) + P(win on 10)
P(win on 4) = 3
36 ·
"
3
36 +
µ27
36
¶
· 3
36 +
µ27
36
¶2
· 3
36 + · · ·
#
= 1
36
P(win on 5) = 4
36 ·
"
4
36 +
µ26
36
¶ µ 4
36
¶
+
µ26
36
¶2
·
µ 4
36
¶
+ · · ·
#
= 2
45
P(win on 6) = 5
36
"
5
36 +
µ25
36
¶ µ 5
36
¶
+
µ25
36
¶2
·
µ 5
36
¶
+ · · ·
#
= 25
396
P(win on 8) = P(win on 6) = 25
396
P(win on 9) = P(win on 5) = 2
45
P(win on 10) = P(win on 4) = 1
36
P(win) = 8
36 +
·
2 · 1
36 + 2 · 2
45 + 2 · 25
396
¸
= 0.4929
1–37. P 8
8 = 8! = 40,320
1–38. Let B, C, D, E, X represent the events of arriving at points so labeled.
P(B) = P(C) = P(D) = P(E) = 1
4
P(X|B) = 1
3,
P(X|C) = 1,
P(X|D) = 1,
P(X|E) = 2
5
P(X) = P(B) · P(X|B) + P(C) · P(X|C) + P(D) · P(X|D) + P(E) · P(X|E)
=
µ1
4 · 1
3
¶
+
µ1
4 · 1
¶
+
µ1
4 · 1
¶
+
µ1
4 · 2
5
¶
= 41
60

8
1–39. P(B3|A) =
P(B3) · P(A|B3)
3
X
i=1
P(Bi) · P(A|Bi)
=
·
(0.5)(0.3)
(0.2)(0.2) + (0.3)(0.5) + (0.5)(0.3)
¸
= 0.441
1–40. F: Structural Failure
DS: Diagnosis as Structural Failure
P(F|DS) =
P(F) · P(DS|F)
P(F) · P(DS|F) + P(F) · P(DS|F)
=
(0.25)(0.9)
(0.25)(0.9) + (0.75)(0.2) =
0.225
0.225 + 0.150 = 0.6

1
Chapter 2
2–1. RX = {0, 1, 2, 3, 4}, P(X = 0) =

4
0



48
5



52
5


P(X = 1) =
µ4
1
¶ µ48
4
¶
µ52
5
¶
,
P(X = 2) =
µ4
2
¶ µ48
3
¶
µ52
5
¶
,
P(X = 3) =
µ4
3
¶ µ48
2
¶
µ52
5
¶
,
P(X = 4) =
µ4
4
¶ µ48
1
¶
µ52
5
¶
2–2.
µ = 0 · 1
6 + 1 · 1
6 + 2 · 1
3 + 3 · 1
12 + 4 · 1
6 + 5 · 1
12 = 26
12
σ2 =
·
02 · 1
6 + 12 · 1
6 + 22 · 1
3 + 32 · 1
12 + 42 · 1
6 + 52 · 1
12
¸
−
µ26
12
¶2
= 83
36
2–3.
Z ∞
0
ce−x dx = 1 ⇒c = 1, so
f(x) =
½ e−x
if x ≥0
0
otherwise
µ =
Z ∞
0
xe−x dx = −xe−x|∞
0 +
Z ∞
0
e−x dx = 1
σ2 =
Z ∞
0
x2e−x dx −12 =
·
−x2e−x|∞
0 +
Z ∞
0
2xe−x dx
¸
−1
=
·
−2xe−x|∞
0 +
Z ∞
0
2e−x dx
¸
−1
= 2 −1 = 1
2–4. FT(t) = PT(T ≤t) = 1 −e−ct; t ≥0
∴fT(t) = F ′
T(t) =
½ ce−ct
if t ≥0
0
otherwise

2
2–5.
(a) Yes
(b) No, since GX(∞) ̸= 1 and GX(b) ̸≥GX(a) if b ≥a
(c) Yes
2–6.
(a) fX(x) = F ′
X(x) = e−x; 0 < x < ∞
= 0; x ≤0
(c) hX(x) = H′
X(x) = ex; −∞< x ≤0
= 0; x > 0
2–7. Both are since pX(x) ≥0, all x; and Σall x pX(x) = 1
2–8. The probability mass function is
x
pX(x)
−1
1
5
0
1
10
+1
2
5
+2
3
10
ow
0
E(X) =
µ
−1 · 1
5
¶
+
µ
0 · 1
10
¶
+
µ
1 · 2
5
¶
+
µ
2 · 3
10
¶
= 4
5
V (X) =
µ
(−1)2 · 1
5
¶
+
µ
02 · 1
10
¶
+
µ
12 · 2
5
¶
+
µ
22 · 3
10
¶
−
µ4
5
¶2
= 29
25

3
FX(x) =



































0,
x < 0
1
5,
−1 ≤x < 0
3
10,
0 ≤x < 1
7
10,
1 ≤x < 2
1,
x ≥2
2–9. P(X < 30) = P(X ≤29) =
29
X
x=0
e−20(20)x
x!
= 0.978
2–10.
(a) PX(x) = FX(x) −FX(x −1)
=
"
1 −
µ1
2
¶x+1#
−
·
1 −
µ1
2
¶x¸
=
µ1
2
¶x
−
µ1
2
¶x+1
=
¡ 1
2
¢ ¡ 1
2
¢x ; x = 0, 1, 2, . . .
= 0; ow
(b) PX(0 < X ≤8) = FX(8) −FX(0) = 0.498
(c) PX(X even) = 1
2
∞
X
k=0
µ1
4
¶k
= 2
3
2–11.
(a)
Z 2
0
kx dx +
Z 4
2
k(4 −x) dx = 1 ⇒k = 1
4
and fX(x) ≥0 for k = 1
4

4
(b)
µ =
Z 2
0
1
4x2 dx +
Z 4
2
1
4(4x −x2) dx = 2
σ2 =
Z 2
0
1
4x3 dx +
Z 4
2
1
4(4x2 −x3) dx −22 = 2
3
(c) FX(x) = 0;
x < 0
=
Z x
0
1
4t dt = x2
8 ;
0 ≤x < 2
=
Z 2
0
1
4x dx +
Z x
2
1
4(4 −t) dt = −1 + x −x2
8 ;
2 ≤x < 4
= 1;
x > 4
2–12.
(a) k
·Z a
0
x dx +
Z 2a
a
(2a −x) dx
¸
= 1 ⇒k = 1
a2
(b) FX(x) = 0; x < 0
=
Z x
0
kt dt = k
µx2
2
¶
; 0 ≤x < a
=
Z a
0
kx dx +
Z x
a
k(2a −t) dt = k
µa2
2
¶
+ k
·
2a(x −a) + a2 −x2
2
¸
for a ≤x ≤2a
= 1; x > 2a
(c)
µ = k
·Z a
0
x2 dx +
Z 2a
a
(2ax −x2) dx
¸
= a
σ2 = a2
6
2–13. From Chebyshev’s inequality 1 −1
k2 = 0.75 ⇒k = 2 with µ = 2, σ =
√
2, and the
interval is [14 −2
√
2, 14 + 2
√
2].
2–14.
(a)
Z 0
−1
kt2 dt = 1 ⇒k = 3

5
(b)
µ =
Z 0
−1
3t3 dt = 3
"
t4
4
¯¯¯¯
0
−1
#
= −3
4
σ2 =
Z 0
−1
3t4 dt −
µ
−3
4
¶2
= 3
Ã
t5
5
¯¯¯¯
0
−1
!
−9
16 = 3
80
(c) FT(t) = 0;
t < −1
=
Z t
−1
3u2 du = t3 + 1;
−1 ≤t ≤0
= 1;
t > 0
2–15.
(a) k
µ1
2 + 1
4 + 1
8
¶
= 1 ⇒k = 8
7
(b)
µ = 8
7
"
1 ·
µ1
2
¶
+ 2 ·
µ1
2
¶2
+ 3
µ1
2
¶3#
= 11
7
σ2 = 8
7
"
12 ·
µ1
2
¶
+ 22 ·
µ1
2
¶2
+ 32 ·
µ1
2
¶3#
−
µ11
7
¶2
= 26
49
(c) FX(x) = 0;
x < 1
= 8
14;
1 ≤x < 2
= 12
14;
2 ≤x < 3
= 1;
x ≥3
2–16.
∞
X
n=0
krn = 1 => k = 1 −r
2–17. Using Chebychev’s inequality, 1 −1
k2 = 0.99 ⇒k = 10,
µ = 2, σ =
√
0.4, so the interval is [2 −10(0.6324)]; 2 + 10(0.6324)]. The letters
should be mailed 8.3 days before delivery date required.
2–18.
(a) µA = 1000(0.2) + 1050(0.3) + 1100(0.1) + 1150(0.3) + 1200(0.05) + 1350(0.05)
= 1097.5
µB = 1135
(b) Assume independence so µA|B=1130 = µA = 1097.5

6
(c) With independence, P(A = k and B = k) = P(A = k) · P(B = k). So
X
k
P(A = k)P(B = k) = (0.1)(0.2) + (0.3)(0.1) + (0.1)(0.3) + (0.3)(0.3)
+ (0.05)(0.1) + (0.05)(0.1) = 0.18
2–19. p(xi) = xi −1
36
;
xi = 2, 3, . . . , 6
= 13 −xi
36
;
xi = 7, 8, . . . , 12
so
xi
2
3
4
5
6
7
8
9
10
11
12
p(xi)
1
36
2
36
3
36
4
36
5
36
6
36
5
36
4
36
3
36
2
36
1
36
2–20. µ = 7, σ2 = 105
18
2–21.
(a) FX(x) = 0;
x < 0
= x2/9;
0 ≤x < 3
= 1;
x ≥3
(b) µ = 2, σ2 = 1
2
(c) µ′
3 = 54
5
(d) m = 3
√
2
2–22. µ = 0, σ2 = 25, σ = 5
P[|X −µ| ≥kσ] = P[|X| ≥5k] = 0 if k > 1 and = 1, 0 < k ≤1.
From Chebychev’s inequality, the upper bound is
1
k2.
2–23. F(x) = 0; x < 0
F(x) =
Z x
0
³ u
t2
´
e−(u2/2t2) du =
Z x2/2t2
0
e−v dv
= 1 −e−x2/2t2; x ≥0

7
2–24. F(x) =
Z x
0
1
σπ
du
{1 + ((u −µ)2/σ2)}; −∞< x ≤∞
Let t = u −µ
σ
, dt = 1
σdu and
F(x) =
Z
x−u
σ
0
1
π ·
dt
1 + t2 = 1
π tan−1
µx −u
σ
¶
; −∞< x < ∞
2–25.
Z π/2
0
k sin y dy = 1 ⇒k[−cos y|π/2
0
] = 1 ⇒k = 1
µ =
Z π/2
0
y sin y dy = sin
³π
2
´
= 1
2–26. Assume X continuous
µk =
Z ∞
−∞
(x −µ)kfX(x) dx =
Z ∞
−∞
" k
X
j=0
µk
j
¶
(−µj)xk−j
#
fX(x) dx
=
k
X
j=0
(−1)j
µk
j
¶
µj
Z ∞
−∞
xk−jfX(x) dx
=
k
X
j=0
(−1)j
µk
j
¶
µjµ′
k−j

1
Chapter 3
3–1.
(a)
y
pY (y)
0
0.6
20
0.3
80
0.1
ow
0
(b) E(Y ) =
X
y
y · pY (y) = 14
V (Y ) =
X
y
y2 · pY (y) −(14)2 = 564
3–2. Let Proﬁt = P = 10 + 2X
P(P ≤p) = P(10 + 2X ≤p) = P
µ
X ≤p −10
2
¶
=
Z
p−10
2
0
x
18dx
FP(p) = x2
36
¯¯¯¯
p−10
2
0
=
1
144(p2 −20p + 100)
fP(p) = 1
72(p −10); 10 ≤p ≤22
= 0; ow
3–3.
(a) P(T < 1) = 1 −e−1/4 = 0.221
(b) E[P] = 200 −200P(T < 1) = 155.80
3–4.
(a)
x
pX(x)
y = 2000(12 −x)
pY (y)
10
0.1
4000
0.1
11
0.3
2000
0.3
12
0.4
0
0.4
13
0.1
−2000
0.1
14
0.1
−4000
0.1
ow
0
ow
0
(b) E(X) = 10(0.1) + 11(0.3) + 12(0.4) + 13(0.1) + 14(0.1) = 11.8 days
V (X) = 102(0.1) + 112(0.3) + 122(0.4) + 132(0.1) + 142(0.1) −(11.8)2
= 1.16 days2

2
E(Y ) = (4000)(0.1) + (2000)(0.3) + 0(0.4) + (−2000)(0.1)
+ (−4000)(0.1) = $400
V (Y ) = (4000)2(0.1) + (2000)2(0.3) + 02(0.4) + (−2000)2(0.1)
+ (−4000)2(0.1) −4002 = 4,640,000($2)
3–5. FZ(z) = P(Z ≤z) = P(X2 ≤z) = P(|X| ≤√z)
= P(0 ≤X ≤√z) =
R √z
0
2xe−x2 dx
Let u = x2, du = 2x dx, so
FZ(z) =
Z z
0
e−u du = 1 −ez
fZ(z) = e−z; z ≥0
= 0; ow
3–6.
(a) E(Di) =
9
X
d=0
d · pDi(d) = 1
10(1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9) = 4.5
(b) V (Di) =
9
X
d=0
d2 · pDi(d) −(4.5)2
= 1
10[12 + 22 + 32 + 42 + 52 + 62 + 72 + 82 + 92] −(20.25) = 8.25
(c) d
y
y
pY (y)
0
4
0
0.2
1
3
1
0.2
2
2
2
0.2
3
1
3
0.2
4
0
4
0.2
5
0
ow
0
6
1
7
2
8
3
9
4
E(Y ) =
2
10(1 + 2 + 3 + 4) = 2
V (Y ) =
2
10(12 + 22 + 32 + 42) −4 = 2

3
3–7.
R = Revenue/Gal
R = 0.92; A < 0.7
= 0.98; A ≥0.7
E(R) = 0.92 P(A < 0.7) + 0.98 P(A ≥0.7)
= 0.92(0.7) + 0.98(0.3) = 93.8c|/gal.
3–8. MX(t) =
Z ∞
β
etx1
θe−1
θ (x−β) dx = 1
θeβ/θ
Z ∞
β
e−x( 1
θ −t) dx
= 1
θ
µ1
θ −t
¶−1
eβt, for 1
θ −t > 0
M ′
X(t) = 1
θ
µ1 −θt
θ
¶−2
eβt
·1 −θt
θ
β + 1
¸
M ′′
X(t) = 1
θ
µ1 −θt
θ
¶−3
eβt
"µ1 −θt
θ
¶2
β2 +
µ1 −θt
θ
¶
β + 2 +
µ1 −θt
θ
¶
β
#
E(X) = M ′
X(0) = β + θ
V (X) = M ′′
X(0) −(β + θ)2 = θ2
3–9.
(a) FY (y) = P(2X2 ≤y) = P
·
−
ry
2 ≤X ≤+
ry
2
¸
= P
µ
0 ≤X ≤
ry
2
¶
=
Z √y
2
0
e−x dx = −e−x|
√y
2
0
= 1 −e−√y
2
fY (y) = F ′
Y (y) = 1
4
³y
2
´−1/2
· e−(y/2)1/2; y > 0
= 0; otherwise

4
(b) FV (v) = P(X1/2 ≤v) = P(X ≤v2)
=
Z v2
0
e−x dx = −e−x|v2
0 = 1 −e−v2
fV (v) = F ′
V (v) = 2ve−v2; v > 0
= 0; otherwise
(c) FU(u) = P(ℓn(X) ≤u) = P(X ≤eu) =
Z eu
0
e−x dx
= 1 −e−eu
fU(u) = F ′
U(u) = e−(eu−u); u > 0
= 0; otherwise
3–10. Note that as stated Y > y0 ⇒signal read (not Y > |y0|), so
PY (Y > y) =
Z π/2
tan−1y
1
2πdx +
Z 3π/2
tan−1y+π
1
2πdx
FY (y) = 1 −PY (Y > y)
= 1
2 + 1
π tan−1y; y ≥0
fY (y) = 1
π
µ
1
1 + y2
¶
; −∞< y < +∞
Note the symmetry in (
1
1+y2).
3–11. Let
S = Stock Level
L(X, S) = 0.5(X −S), X > S
= 0.25(S −X), X ≤S
E(L(X, S)) =
Z S
106 0.25(S −X) · 106 dx +
Z 2·106
S
0.5(X −S) · 10−6 dx
dE[L(X, S)]
dS
= 6
810−6S −5
4 = 0 ⇒S = 5
3106

5
3–12.
µG = E(G) =
Z 4
0
gfG(g) dg =
Z 4
0
g2
8 dg = 8/3.
E(G2) =
Z 4
0
g3
8 dg = 8.
σ2
G = V (G) = E(G2) −(E(G))2 = 8/9.
H(G) = (3 + 0.05G)2
H(µG) = (3 + 0.05µG)2
H′(µG) = (0.1)(3 + 0.05µG)
H′′(µG) = 0.005.
Thus,
µA ≈H(µG) + 1
2H′′(µG)σ2
G = H(8/3) + 1
2(0.005)8
9 = 9.82
and
σ2
A ≈(H′(µG))2σ2
G = 0.082.
3–13.
(a) FY (y) = P(4 −X2 ≤y) = P(X2 ≥4 −y)
= P(X ≤−√4 −y) + P(X ≥√4 −y)
= 0 +
Z 2
√4−y
dx = 2 −
p
4 −y
fY (y) = 1
2(4 −y)−1/2; 0 ≤y ≤3
= 0; otherwise
(b) FY (y) = P(ex ≤y) = P(X ≤ℓn(y))
=
Z ℓn(y)
1
dx = ℓn(y) −1
fY (y) = 1
y; e1 ≤y ≤e2
= 0; otherwise

6
3–14.
y =
3
(1 + x)2
x =
√
3y−1/2 −1
dx
dy = −
√
3
2 y−3/2
¯¯¯¯
dx
dy
¯¯¯¯ =
√
3
2 y−3/2
fY (y) = fX(x)
¯¯¯¯
dx
dy
¯¯¯¯
= fX(
√
3y−1/2 −1)
√
3
2 y−3/2
= exp[−
√
3y−1/2 + 1]
√
3
2 y−3/2;
0 ≤y ≤3
3–15. With equal probability pX(1) = · · · = pX(6) = 1
6,
MX(t) =
6
X
x=1
µ1
6
¶
etx
E(X) = M ′
X(0) = 7
2
V (X) = M ′′
X(0) −[M ′
X(0)]2 = 35
12
3–16.
(a) Using the substitution y = bx2, we obtain
1
=
Z ∞
0
ax2e−bx2 dx
=
Z ∞
0
a y
b e−y dy
2√by
=
a
2b3/2
Z ∞
0
y(3/2)−1e−y dy
=
a
2b3/2 Γ(3
2) =
a
2b3/2
1
2 Γ(1
2) = a√π
4b3/2
Thus,
a = 4b3/2
√π .

7
(b) Similarly,
µX = E(X) =
Z ∞
0
ax3e−bx2 dx =
a
2b2 Γ(2) =
2
√
πb
and
E(X2) =
a
2b5/2 Γ
µ5
2
¶
=
3
2b.
These facts imply that
σ2
X = E(X2) −[E(X)]2 =
3
2b −4
πb = 3π −8
2πb .
Now we have
H(x)
=
18x2
H(µX)
=
18µ2
X = 18 · 4
πb = 72
πb
H′(µX)
=
36µX = 36 ·
2
√
πb
=
72
√
πb
H′′(µX)
=
36
Then
µY
.=
H(µX) + 1
2H′′(µX) · σ2
X
=
72
πb + 1
2 · 36 · 3π −8
2πb
= 27
b
and
σ2
Y
.=
(H′(µX))2σ2
X
=
µ 72
√
πb
¶2µ3π −8
2πb
¶
= 2592(3π −8)
π2b2
3–17. E(Y ) = 1, V (Y ) = 1
H(Y ) =
√
Y 2 + 36
E(X) ∼= H(µY ) + 1
2H′′(µY )σ2
Y
V (X) ∼= [H′(µY )]2σ2
Y
H′(y) = y(y2 + 36)−1/2,
H′′(y) = 36(y2 + 36)−3/2
E(X) ∼= (37)1/2 + 1
2(36)(37)−3/2
.= 6.16
V (X) ∼= (37)−1 = 0.027027

8
3–18. E(P) =
Z 1
0
(1 + 3r)6r(1 −r) dr =
Z 1
0
(6r + 12r2 −18r3) dr = 5
2
Since H(r) = 1 + 3r is increasing,
fP(p) = 6
µp −1
3
¶ µ
1 −p −1
3
¶ µ1
3
¶
= 2
9(5p −p2 −4); 1 < p < 4
= 0; otherwise
3–19. MX(t) =
Z ∞
0
etx · 4xe−2x dx
=
Z ∞
0
4xex(t−2) dx =
µ
1 −t
2
¶−2
E(X) = M ′
X(0) = 1
V (X) = M ′′
X(0) −[M ′
X(0)]2 = 1
2
3–20.
V = π · X2
4
· 1
E(V ) = π
4 E(X2) = π
4 [V (X) + (E(X))2]
= π
4 · [25 · 10−6 + 22] = 3.14162
3–21. MY (t) = E(etY ) = E(et(aX+b))
= etbE(e(at)X)
= etbMX(at)
3–22.
(a)
R 1
0 k(1 −x)a−1xb−1 dx = 1.
Let Γ(p) =
R ∞
0 up−1e−u du deﬁne the gamma function. Integrating by parts
and applying L’Hˆospital’s Rule, it can be shown that Γ(p + 1) = p · Γ(p).
Make a variable change u = v2 so that Γ(1
2) = √π.

9
Working with Γ(p), let u = v2 so du = 2v dv and
Γ(p) = 2
Z ∞
0
(v2)p−1e−v2v dv = 2
Z ∞
0
v2p−1e−v2 dv.
Then Γ(a) · Γ(b) = 4
Z ∞
0
s2a−1e−s2 ds
Z ∞
0
t2b−1e−t2 dt
= 4
Z ∞
0
Z ∞
0
s2a−1t2b−1e−(s2+t2) ds dt
Let s = ρ cos θ, t = ρ sin θ, so the Jacobian = ρ
Γ(a) · Γ(b) = 4
Z ∞
0
Z π/2
0
(ρ cos θ)2a−1(ρ sin θ)2b−1e−ρ2ρ dθ dρ
= 4
Z ∞
0
ρ2a+2b−1e−ρ2 dρ
Z π/2
0
(cos θ)2a−1(sin θ)2b−1 dθ
Substitute ρ2 = y in the ﬁrst integral and sin2 θ = x in the second.
Γ(a)Γ(b) =
Z ∞
0
ya+b−1e−y dy
Z 1
0
xb−1(1 −x)a−1 dx
= Γ(a + b)
Z 1
0
xb−1(1 −x)a−1 dx
so
Z 1
0
xb−1(1 −x)a−1 dx = Γ(a) · Γ(b)
Γ(a + b)
⇒k =
Γ(a + b)
Γ(a) · Γ(b)

10
(b) E(Xk) =
Γ(a + b)
Γ(a) · Γ(b)
Z 1
0
xb+k−1(1 −x)a−1
=
Γ(a + b)
Γ(a) · Γ(b)
Z 1
0
x(b+k)−1(1 −x)a−1 dx
=
Γ(a + b)
Γ(a) · Γ(b) · Γ(a)Γ(b + k)
Γ(a + b + k) .
Since Γ(1) = 1 we obtain Γ(p + 1) = p! for p a positive integer.
Then E(X) =
(a + b −1)!
(a −1)!(b −1)! =
b
a + b
E(X2) =
[(a + b) −1]!
(a −1)!(b −1)! · (a −1)!(b + 1)!
(a + b + 1)!
=
b(b + 1)
(a + b + 1)(a + b)
so V (X) =
ab
(a + b)2(a + b + 1)
3–23. MX(t) = 1
2 + 1
4et + 1
8e2t + 1
8e3t
M ′
X(t) = 1
4et + 1
4e2t + 3
8e3t
M ′′
X(t) = 1
4et + 1
2e2t + 9
8e3t
(a) E(X) = M ′
X(0) = 7
8
V (Y ) = M ′′
X(0) −[M ′
X(0)]2 = 15
8 −
µ7
8
¶2
= 71
64
(b)
x
y
y
pY (y)
0
4
0
1
8
1
1
1
3
8
2
0
4
1
2
3
1
ow
0
ow
0

11
FY (y) = 0; y < 0
= 1
8; 0 ≤y < 1
= 1
2; 1 ≤y < 4
= 1; y ≥4
3–24. µ3 = E(X −µ′
1)3
= E(X3) −3µ′
1 · E(X2) + 3µ′
1 · E(X) −(µ′
1)3
= µ′
3 −3µ′
1µ′
2 + 2(µ′
1)3.
A r.v. is symmetric about a if f(x + a) = f(−x + a) and we assume X contin-
uous. Since X −a and a −X have the same p.d.f., E(X −a) = E(a −X) or
E(X) −a = a −E(X), so E(X) = a = µ′
1. Now,
E(X −µ′
1)r = E(X −a)r =
Z ∞
−∞
(x −a)rf(x) dx
=
Z a
−∞
(x −a)rf(x) dx +
Z ∞
0
(x −a)rf(x) dx
=
Z 0
−∞
yrf(y + a) dy +
Z ∞
0
yrf(y + a) dy
= −
Z 0
−∞
(1 −y)rf(−y + a) dy +
Z ∞
0
yrf(y + a) dy
=
Z ∞
0
(−y)rf(y + a) dy +
Z ∞
0
yrf(y + a) dy
= (−1)r
Z ∞
0
yrf(y + a) dy +
Z ∞
0
yrf(y + a) dy
= 0 for odd r.
Thus E(X −µ′
1)3 = 0.

12
3–25. If
R ∞
−∞|x|rf(x) dx = k < ∞, then
R ∞
−∞|x|nf(x) dx < ∞, where 0 ≤n < r.
Proof: If |x| ≤1, then |x|n ≤1 and if |x| > 1, then |x|n ≤|x|r.
Then
Z ∞
−∞
|x|nf(x) dx =
Z
|x|≤1
|x|n · f(x) dx +
Z
|x|>1
|x|n · f(x) dx
≤
Z
|x|≤1
1f(x) dx +
Z
|x|>1
|x|rf(x) dx ≤1 + k < ∞
3–26.
MX(t) = exp
µ
µt + σ2t2
2
¶
⇒ψX(t) = µt + σ2t2
2
dψX(t)/dt|t=0 = [µ + σ2t]t=0 = µ
d2ψX(t) dt2|t=0 = σ2|t=0 = σ2
drψX(t)/dtr|t=0 = 0; r ≥3
3–27. From Table XV, using the ﬁrst column with scaling: u1 = 0.10480, u2 = 0.22368,
u3 = 0.24130, u4 = 0.42167, u5 = 0.37570, u6 = 0.77921, . . . , u20 = 0.07056.
FX(x) = 0;
x < 0
= 0.5;
0 ≤x < 1
= 0.75;
1 ≤x < 2
= 0.875;
2 ≤x < 3
= 1;
x ≥3
So, x1 = 0, x2 = 0, x3 = 0, x4 = 0, x5 = 0, x6 = 2, . . . , x20 = 0
3–28. The c.d.f. is
FT(t) = 1 −e−t/4
Thus, we set
t = −4ℓn(1 −FT(t)) = −4ℓn(1 −u)
From Table XV, using the second column with scaling: u1 = 0.15011, u2 = 0.46573,
u3 = 0.48360, . . . , u10 = 0.36257

13
So,
t1
=
−4ℓn(0.84989) = 0.6506
t2
=
−4ℓn(0.53427) = 2.5074
...
t10
=
−4ℓn(0.63143) = 1.8391

1
Chapter 4
4–1.
(a)
x
0
1
2
3
4
5
pX(x)
27/50
11/50
6/50
3/50
2/50
1/50
y
0
1
2
3
4
pY (y)
20/50
15/50
10/50
4/50
1/50
(b)
y
0
1
2
3
4
pY |0(y)
11/27
8/27
4/27
3/27
1/27
(c)
x
0
1
2
3
4
5
pX|0(x)
11/20
4/20
2/20
1/20
1/20
1/20
4–2.
(a)
y
1
2
3
4
5
6
7
8
9
pY (y)
44/100
26/100
12/100
8/100
4/100
2/100
2/100
1/100
1/100
x
1
2
3
4
5
6
pX(x)
26/100
21/100
17/100
15/100
11/100
10/100
(b)
y
1
2
3
4
5
6
7
8
9
x = 1
pY |1(y)
10/26
6/26
3/26
2/26
1/26
1/26
1/26
1/26
1/26
x = 2
pY |2(y)
8/21
5/21
3/21
2/21
1/21
1/21
1/21
0
0
x = 3
pY |3(y)
8/17
5/17
2/17
1/17
1/17
0
0
0
0
x = 4
pY |4(y)
7/15
4/15
2/15
1/15
1/15
0
0
0
0
x = 5
pY |5(y)
6/11
3/11
1/11
1/11
0
0
0
0
0
x = 6
pY |6(y)
5/10
3/10
1/10
1/10
0
0
0
0
0
4–3.
(a)
Z 10
0
Z 100
0
k
1000 dx1 dx2 = 1 ⇒k = 1
(b) fX1(x1) =
Z 10
0
1
1000 dx2 =
1
100;
0 ≤x1 ≤100
= 0;
otherwise
fX2(x2) =
Z 100
0
1
1000 dx1 =
1
1000x1
¯¯¯¯
100
0
= 1
10;
0 ≤x2 ≤10
= 0;
otherwise

2
(c) FX1,X2(x1, x2) = 0; x1 ≤0, x2 ≤0
= 0; x1 ≤0, x2 > 0
= 0; x1 > 0, x2 ≤0
=
Z x1
0
Z x2
0
1
1000 dt2 dt1 = x1x2
1000; 0 < x1 < 100, 0 < x2 < 10
=
Z x1
0
Z 10
0
dt2 dt1
1000 = x1
100; 0 < x1 < 100, x2 ≥10
=
Z x2
0
Z 100
0
dt1 dt2
1000 = x2
10; x1 ≥100, 0 < x2 < 10
= 1; x1 ≥100, x2 ≥10.
4–4.
(a) k
Z 4
2
Z 2
0
(6 −x1 −x2) dx1 dx2 = 1 ⇒k = 1
8
(b) P(X1 < 1, X2 < 3) =
Z 3
2
Z 1
0
1
8(6 −x1 −x2) dx1 dx2 = 3
8
(c) P(X1 + X2 ≤4) = 1
8
Z 4
2
Z 4−x2
0
(6 −x1 −x2) dx1 dx2 = 2
3
(d) fX1(x1) = 1
8
Z 4
2
(6 −x1 −x2) dx2 = 1
8(6 −2x1); 0 ≤x1 ≤2
P(X1 < 1.5) =
Z 3/2
0
1
8(6 −2x1) dx1 .= 0.844
(e) fX1(x1) see (d)
fX2(x2) =
Z 2
0
1
8(6 −x1 −x2) dx1 = 1
8(10 −2x2); 2 ≤x2 ≤4
4–5.
(a) P
µ
W ≤2
3, Y ≤1
2
¶
=
Z 2/3
0
Z 1
0
Z 1/2
0
Z 1
0
16wxyz dz dy dx dw = 1
9
(b) P
µ
X ≤1
2, Z ≤1
4
¶
=
Z 1
0
Z 1/2
0
Z 1
0
Z 1/4
0
16wxyz dz dy dx dw = 1
64
(c)
Z 1
0
Z 1
0
Z 1
0
16wxyz dz dy dx = 2w; 0 ≤w ≤1
= 0; otherwise
Note: x, y, z and w are independent

3
4–6. From Problem 4–4
fX(x) = 1
8(6 −2x); 0 ≤x ≤2
fY (y) = 1
8(10 −2y); 2 ≤y ≤4
fX|y(y) = fX,Y (x, y)
fY (y)
= 6 −x −y
10 −2y ; 0 ≤x ≤2, 2 ≤y ≤4
fY |x(x) = fX,Y (x, y)
fX(x)
= 6 −x −y
6 −2x ; 2 ≤y ≤4, 0 ≤x ≤2
4–7. E(Y |X = 3) =
9
X
y=1
y · pY |3(y) = 33
17
4–8.
(a)
x1
51
52
53
54
55
pX1(x1)
0.28
0.28
0.22
0.09
0.13
x2
51
52
53
54
55
pX2(x2)
0.18
0.15
0.35
0.12
0.20
(b)
x2
51
52
53
54
55
otherwise
PX2|51(x2)
6
28
7
28
5
28
5
28
5
28
0
PX2|52(x2)
5
28
5
28
10
28
2
28
6
28
0
PX2|53(x2)
5
22
1
22
10
22
1
22
5
22
0
PX2|54(x2)
1
9
1
9
5
9
1
9
1
9
0
PX2|55(x2)
1
13
1
13
5
13
3
13
3
13
0
E(X2|x1 = 51) =
55
X
x2=51
x2 · pX2|51(x2) = 52.857
E(X2|x1 = 52) =
55
X
x2=51
x2 · pX2|52(x2) = 52.964
E(X2|x1 = 53) =
55
X
x2=51
x2 · pX2|53(x2) = 53.0

4
E(X2|x1 = 54) =
55
X
x2=51
x2 · pX2|54(x2) = 53.0
E(X2|x1 = 55) =
55
X
x2=51
x2 · pX2|55(x2) = 53.461
4–9.
fX1(x1) =
Z 1
0
6x2
1x2 dx2 = 3x2
1x2
2|1
x2=0 = 3x2
1; 0 ≤x1 ≤1
fX2(x2) =
Z 1
0
6x2
1x2 dx1 = 2x3
1x2|1
x1=0 = 2x2; 0 ≤x2 ≤1
fX1|x2(x1) = fX1,X2(x1, x2)
fX2(x2)
= 6x2
1x2
2x2
= 3x2
1; 0 ≤x1 ≤1
fX2|x1(x2) = fX1,X2(x1, x2)
fX1(x1)
= 2x2; 0 ≤x2 ≤1
E(X1|x2) =
Z 1
0
x1fX1|x2(x1) dx1 = 3
4
E(X2|x1) =
Z 1
0
x2fX2|x1(x2) dx2 = 2
3
4–10.
(a) fX1(x1) = 2x1e−x2
1;
x1 ≥0
fX2(x2) = 2x2e−x2
2;
x2 ≥0
(b) fX1|x2(x1) = fX1,X2(x1, x2)
fX2(x2)
= 2x1e−x2
1; x1 ≥0
fX2|x1(x2) = 2x2e−x2
2; x2 ≥0
Since fX1|x2(x1) = fX1(x1) and fX2|x1(x2) = fX2(x2), X1 and X2 are indepen-
dent
(c) E(X1|x2) = E(X1) =
Z ∞
0
x1(2x1e−x2
1) dx1 =
Z ∞
0
2x2
1e−x2
1 dx1
Let u = x1, du = dx1, dv = 2x1e−x2
1, v = −e−x2
1, and integrate by parts, so
E(X1) = −x1e−x2
1|∞
0 +
Z ∞
0
e−x2
1 dx1 = 0 + π
4
Similarly, E(X2) = π/4.

5
4–11. z = xy, t = x ⇒x = t, y = z
t
¯¯¯¯¯¯
∂x/∂t
∂x/∂z
∂y/∂t
∂y/∂z
¯¯¯¯¯¯
=
¯¯¯¯¯¯
1
0
−z
t2
1
t
¯¯¯¯¯¯
= 1
t
so p(z, t) = g(t)h(z/t)|1/t|
Thus ℓ(z) =
R ∞
−∞g(t)h(z/t)|1/t| dt
4–12. a = s1 · s2, t = s1
so
s2 = a
t , s1 = t
Jacobian is 1
t
And p(a, t) = (2t)(1
8)(a
t ) · 1
t = a
4t
∴ℓ(a) =
Z 1
a/4
a
4tdt = a
4 ℓn(t)
¯¯¯
1
a/4
= −a
4 ℓn a
4; 0 < a < 4
4–13. f(x, y) = g(x) · h(y)
Z = X
Y
U = Y
y = u
x = uz
¯¯¯¯
∂x/∂u
∂x/∂z
∂y/∂u
∂y/∂z
¯¯¯¯ =
¯¯¯¯
z
u
1
0
¯¯¯¯ = u
p(z, u) = g(uz) · h(u)|u|
ℓ(z) =
Z ∞
−∞
g(uz)h(u)|u| du
4–14. f(v, i) = 3e−3i · e−v
r = v
i
u = i
v = ru
i = u
The Jacobian is u, thus
p(r, u) = (e−ru)(3e−3u)(u) = 3ue−u(3+r); u ≥0
∴ℓ(r) = 3
Z ∞
0
ue−u(3+r) du =
3
(3 + r)2; r ≥0

6
4–15.
Y = X1 + X2 + X3 + X4
E(Y ) = 80, V (Y ) = 4(9) = 36
4–16. If X1 and X2 are independent, then
pX1,X2(x1, x2) = pX1(x1)pX2(x2)
for all x1, x2.
Thus,
pX2|x1(x2) = pX1,X2(x1, x2)
pX1(x1)
= pX1(x1)pX2(x2)
pX1(x1)
= pX2(x2)
for all x1, x2.
Similarly,
pX1|x2(x1) = pX1(x1).
Now prove the converse. Assume that
pX2|x1(x2) = pX2(x2)
and
pX1|x2(x1) = pX1(x1)
for all x1, x2.
Then
pX1,X2(x1, x2) = pX2|x1(x2)pX1(x1) = pX2(x2)pX1(x1)
for all x1, x2.
4–17.
X2 = A + BX1 ⇒E(X2) = A + B · E(X1)
V (X2) = B2 · V (X1), E(X1X2) = E(X1(A + BX1))
= AE(X1) + BE(X2
1)
So
ρ2 = [E(X1X2) −E(X1)E(X2)]2
V (X1) · V (X2)
= [AE(X1) + BE(X2
1) −E(X1)(A + B · E(X1))]2
V (X1) · B2V (X1)
= [BE(X2
1) −B(E(X1))2]2
B2V (X1)2
= B2V (X1)2
B2V (X1)2 = 1
ρ = −1 if B < 0 and ρ = 1 if B > 0.

7
4–18. MX2(t) = E(etX2) = E(et(A+BX1)) = E(eAt · eBtX1)
= eAt · E(eBtX1) = eAt · MX1(Bt)
4–19.
fX1(x1) =
Z 1
x1
2 dx2 = 2(1 −x1); 0 ≤x1 ≤1
fX2(x2) =
Z x2
0
2 dx1 = 2x2; 0 ≤x2 ≤1
E(X1) = 2
Z 1
0
x1(1 −x1) dx1 = 1
3
E(X2) = 2
Z 1
0
x2
2 dx2 = 2
3
E(X2
1) = 2
Z 1
0
x2
1(1 −x1) dx1 = 2
·x3
1
3 −x4
1
4
¸1
0
= 1
6
E(X2
2) = 2
Z 1
0
x3
2 dx2 = 1
2
V (X1) = E(X2
1) −(E(X1))2 = 1
18
V (X2) = E(X2
2) −(E(X2))2 = 1
18
E(X1X2) =
Z 1
0
Z 1/2
0
2x1x2 dx1 dx2 = 2
Z 1
0
x2
·x2
1
2
¸x2
0
dx2
=
Z 1
0
x3
2 dx2 = 1
4
ρ = E(X1X2) −E(X1)E(X2)
p
V (X1)V (X2)
=
1
4 −
µ1
3
¶ µ2
3
¶
sµ 1
18
¶ µ 1
18
¶ = 1
2

8
4–20.
E(U) = A + BE(X1), E(V ) = C + DE(X2)
V (U) = B2 · V (X1), V (V ) = D2V (X2)
E(UV ) = E[(A + BX1)(C + DX2)]
= AC + AD · E(X2) + BC · E(X1) + BD · E(X1X2)
ρU,V = E(UV ) −E(U) · E(V )
p
V (U) · V (V )
= AC + AD · E(X2) + BC · E(X1) + BD · E(X1X2) −[(A + BE(X1))(C + DE(X2))]
p
B2V (X1)D2V (X2)
= BD[E(X1X2) −E(X1)E(X2)]
|BD|
p
V (X1)V (X2)
= BD
|BD|ρX1,X2
4–21. X and Y are not independent. Note
(a) PX,Y (5, 4) = 0 ̸= pX(5) · pY (4) =
1
502.
(b)
E(X) = 0.9
E(Y ) = 1.02
E(X2) = 2.38
E(Y 2) = 2.14
V (X) = 1.57
V (Y ) = 1.0996
E(XY ) = 0.74
ρ = E(XY ) −E(X)E(Y )
p
V (X)V (Y )
= 0.74 −(0.9)(1.02)
p
(1.57)(1.0996)
= −0.135
4–22.
(a) A sale will take place if Y ≥X
(b)
ZZ
x≤y
f(x, y) dx dy
(c) E(Y |Y ≥x) =
ZZ
x≤y
yf(x, y) dx dy
, ZZ
x≤y
f(x, y) dx dy
4–23.
(a) fX(x) =
Z √
1−x2
0
2
πdy = 2
πy
¯¯¯¯
√
1−x2
0
= 2
π
√
1 −x2; −1 < x < +1
fY (y) =
Z √
1−y2
−√
1−y2
2
πdx = 4
π
p
1 −y2; 0 < y < 1

9
(b) fX|y(x) = fX,Y (x, y)
fY (y)
=
2
π
µ 4
π
¶ p
1 −y2
=
1
2
p
1 −y2; −
p
1 −y2 < x <
p
1 −y2
fY |x(y) = fX,Y (x, y)
fX(x)
=
2
π
µ 2
π
¶ √
1 −x2
=
1
√
1 −x2; 0 < y <
√
1 −x2
(c)
E(X|y) =
Z √
1−y2
−√
1−y2
x
2
p
1 −y2dx =
1
2
p
1 −y2

x2
2
¯¯¯¯
√
1−y2
−√
1−y2

= 0
E(Y |x) =
Z √
1−x2
0
y
√
1 −x2dy =
1
√
1 −x2
Ã
y2
2
¯¯¯¯
√
1−x2
0
!
=
√
1 −x2
2
4–24. X and Y continuous
E(X|Y = y) =
Z ∞
−∞
xfX|y(x) dx =
Z ∞
−∞
xfX,Y (x, y)
fY (y)
dx
=
Z ∞
−∞
fX(x)fY (y)
fY (y)
dx =
Z ∞
−∞
xfX(x) dx = E(X)
Change X and Y , reversing roles to show E(Y |X) = E(Y ) if X and Y are inde-
pendent.
4–25.
E(X|y) =
Z ∞
−∞
xfX|y(x) dx =
Z ∞
−∞
xfX,Y (x, y)
fY (y)
dx
E[E(X|Y )] =
Z ∞
−∞
E(X|y) · fY (y) dy =
Z ∞
−∞
Z ∞
−∞
·
xfX,Y (x, y)
fY (y)
dx
¸
fY (y) dy
=
Z ∞
−∞
x
·Z ∞
−∞
fX,Y (x, y) dy
¸
dx =
Z ∞
−∞
xfX(x) dx = E(X).
Reverse roles of X and Y to show E(E(Y |X)) = E(Y ).

10
4–26. w = s + d, let y = s −d
s = w + y
2
,
d = w −y
2
s = 10 →w + y = 20
d = 10 →w −y = 20
s = 40 →w + y = 80
d = 30 →w −y = 60
As illustrated
s
d
10
20
30
40
10
20
30
w
y
10
20
60
70
10
–10
–20
20
30
30
40
50
w – y = 20
w – y = 60
w + y = 80
w + y = 20
The Jacobian J =
¯¯¯¯¯
∂s
∂w
∂s
∂y
∂d
∂w
∂d
∂y
¯¯¯¯¯ =
¯¯¯¯
1/2
1/2
1/2
−1/2
¯¯¯¯ = −1
2
∴fW,Y (w, y) = 1
30 · 1
20 · 1
2; 10 < w + y
2
< 40, 10 < w −y
2
< 30
fW(w) =
Z w−20
20−w
1
1200dy = w −20
600
; 20 < w < 40
=
Z w−20
w−60
1
1200dy = 20
600; 40 < w ≤50
=
Z 80−w
w−60
1
1200dy = 70 −w
600
; 50 < w < 70
= 0; ow
4–27.
(a)
fY (y) =
Z 1
0
(x + y) dx = y + 1
2; 0 < y < 1
fX|y(x) = fX,Y (x, y)
fY (y)
= x + y
y + 1
2
; 0 < x < 1, 0 < y < 1

11
E(X|Y = y) =
Z 1
0
x · (x + y)
µ
y + 1
2
¶dx =
1
y + 1
2
·x3
3 + x2y
2
¸1
0
=
2 + 3y
3(1 + 2y); 0 < y < 1.
(b) fX(x) =
Z 1
0
(x + y) dy =
·
xy + y2
2
¸1
y=0
= x + 1
2; 0 < x < 1
E(X) =
Z 1
0
[x2 + (x/2)] dx = [(x3/3) + (x2/4)]1
0 = 7
12
(c) E(Y ) =
Z 1
0
[y2 + (y/2)] dy = [(y3/3) + (y2/4)]1
0 = 7
12
4–28.
(a)
Z ∞
0
Z ∞
0
k(1 + x + y)
(1 + x)4(1 + y)4dx dy = 1 ⇒k = 9
2
(b)
fX(x) =
Z ∞
0
(9/2)(1 + x + y) dy
(1 + x)4(1 + y)4
=
(9/2)
(1 + x)4
Z ∞
0
dy
(1 + y)4
+ (9/2)x
(1 + x)4
Z ∞
0
dy
(1 + y)4 +
(9/2)
(1 + x)4
Z ∞
0
y dy
(1 + y)4
= 3(3 + 2x)
4(1 + x)4 ; x > 0
4–29.
(a) k
Z ∞
0
Z ∞
0
(1 + x + y)−n dx dy = 1,
k
Z ∞
0
·(1 + x + y)−n+1
(−n + 1)
¸∞
0
dy = 1
so
k
n −1
Z ∞
0
dy
(1 + y)n−1 = 1 ⇒
k
(n −1)(n −2) = 1 ⇒k = (n −1)(n −2)
(b) FX,Y (x, y) = P(X ≤x, Y ≤y) = k
Z x
0
Z y
0
(1 + s + t)−n dt ds
= k
· (1 + s + y)−n+2
(−n + 2)(−n + 1) −
(1 + s)−n+2
(−n + 2)(−n + 1)
¸x
s=0
= 1 −(1 + x)2−n −(1 + y)2−n + (1 + x + y)2−n; x > 0, y > 0

12
4–30. From Eq. 4–58
n ≥p(1 −p)
ϵ2 · α
=
(0.5)(0.5)
(0.05)2(0.05) = 2000
4–31.
(a) g(x, y) = (2xe−x2)(2ye−y2) so X and Y are independent
(b) X and Y are not independent. For given X, Y may only assume values greater
than that value of X.
(c) See Prob. 4–29 with k = (4 −1)(4 −2) = 6 and n = 4.
fX(x) = 2(1 + x)−3; x > 0
fY (y) = 2(1 + y)−3; y > 0
fX,Y (x, y) ̸= fX(x) · fY (y) so the variables are not independent.
4–32. The probability is 2/3 since P(X > Y > Z) = P(X < Y < Z) = 1
6
4–33.
(a) P
µ
0 ≤X ≤1
2, 0 ≤Y ≤1
2
¶
=
Z 1/2
0
Z 1/2
0
1 dx dy = 1
4
(b) P(X > Y ) =
Z 1
0
Z x
0
1 dy dx =
Z 1
0
x dx = 1
2.
4–34. P
µ
0 ≤Y ≤1, Y ≤X ≤Y + 1
4 ≤1
¶
= 7
32
y (receipt)
x (request)
1
1
x = y
x = y + (1/4)
4–35.
(a) FZ(z) = P(Z ≤z) = P(a + bX ≤z) = P
µ
X ≤z −a
b
¶
= FX
µz −a
b
¶
fZ(z) = fX
µz −a
b
¶ ¯¯¯¯
1
b
¯¯¯¯ .

13
(b) FZ(z) = P
µ 1
X ≤z
¶
= P
µ
X ≥1
z
¶
= 1 −FX
µ1
z
¶
fZ(z) = fX
µ1
z
¶ 1
z2
(c) FZ(z) = P(ℓn(X) ≤Z) = P(eℓn(x) ≤ez) = P(X ≤ez) = FX(ez)
fZ(z) = fX(ez)ez
(d) FZ(z) = P(eX ≤z) = P(X ≤ℓn z) = FX(ℓn z)
fZ(z) = fX(ℓn(z))
¯¯¯¯
1
z
¯¯¯¯
The range space for Z is determined from the range space of X and the
deﬁnition of the transformation.

1
Chapter 5
5–1. The probability mass function is
x
p(x)
0
(1 −p)4
1
4p(1 −p)3
2
6p2(1 −p)2
3
4p3(1 −p)
4
p4
otherwise
0
5–2.
P(X ≥5)
=
6
X
x=5
µ6
x
¶
(0.95)x(0.05)6−x
=
6(0.95)5(0.05) + (0.95)6
=
0.9672
5–3. Assume independence and let W represent the number of orders received.
P(W ≥4)
=
12
X
w=4
µ12
w
¶
(0.5)w(0.5)12−w
=
(0.5)12
12
X
w=4
µ12
w
¶
=
1 −(0.5)12
3
X
w=0
µ12
w
¶
= 0.9270
5–4. Assume customer decisions are independent.
P(X ≥10) = 1 −P(X ≤9) = 1 −
9
X
x=0
µ20
x
¶ µ1
3
¶x µ2
3
¶20−x
= 0.0918
5–5.
P(X > 2) = 1 −P(X ≤2) = 1 −
2
X
x=0
µ50
x
¶
(0.02)x(0.98)50−x = 0.0784.

2
5–6.
MX(t)
=
E[etX] =
n
X
x=0
etx
µn
x
¶
px(1 −p)n−x
=
(pet + q)n,
where q = 1 −p
E[X]
=
M
′
X(0) = [n(pet + q)n−1pet]t=0 = np
E[X2]
=
M
′′
X(0) = np[et(n −1)(pet + q)n−2(pet) + (pet + q)n−1et]t=0
=
(np)2 −np2 + np
V (X)
=
E[X2] −(E[X])2 = n2p2 −np2 + np −n2p2 = np(1 −p) = npq
5–7.
P(ˆp ≤0.03)
=
P
µ X
100 ≤0.03
¶
= P(X ≤3)
=
3
X
x=0
µ100
3
¶
(0.01)x(0.99)100−x = 0.9816
5–8.
P(ˆp > p +
p
pq/n)
=
P
µ
ˆp > 0.07 +
p
(0.07)(0.93)/200
¶
=
P(X > 200(0.088))
=
P(X > 17.6)
=
1 −P(X ≤17.6)
=
1 −
17
X
x=0
µ200
x
¶
(0.07)x(0.93)200−x
=
0.1649

3
By two standard deviations,
P(ˆp > p + 2
p
pq/n)
=
P(ˆp > 0.106)
=
P(X > 21.2)
=
1 −P(X ≤21)
=
1 −
21
X
x=0
µ200
x
¶
(0.07)x(0.93)200−x
=
0.0242
By three standard deviations,
P(ˆp > p + 3
p
pq/n)
=
P(X > 24.8)
=
1 −P(X ≤24)
=
1 −
24
X
x=0
µ200
x
¶
(0.07)x(0.93)200−x
=
0.0036
5–9. P(X = 5) = (0.95)4(0.05) = 0.0407
5–10. A: Successful on ﬁrst three calls,
B: Unsuccessful on fourth call
P(B|A) = P(B) = 0.90
if A and B are independent
5–11.
P(X = 5)
=
p4(1 −p) = f(p)
df(p)
dp
=
5p4 −4p3 = 0 ⇒p = 4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
f(p)
p

4
5–12.
(a) X = trials required up to and including the ﬁrst sale
C(X)
=
1000X + 3000(X −1)
=
4000X −3000
E[C(X)]
=
4000E[X] −3000 = 4000
µ 1
0.1
¶
−3000 = 37000
(b) Since $37000 > $15000, the trips should not be undertaken.
(c)
P(C(X) > 100000)
=
P(4000X −3000 > 100000)
=
P
µ
X > 103000
4000
¶
.= P(X > 25.75)
=
1 −P(X ≤25)
=
1 −
25
X
x=1
(0.90)x−1(0.10)
5–13.
MX(t) =
pet
1 −qet,
where q = 1 −p
E[X]
=
M
′
X(0) =
·(1 −qet)(pet) + (pet)(qet)
(1 −qet)2
¸
t=0
=
(1 −q)p + pq
(1 −q)2
= p
p2 = 1
p
E[X2]
=
M
′′
X(0) =
·(1 −qet)2(pet) + 2pet(1 −qet)(qet)
(1 −qet)4
¸
t=0
=
(1 + q)p
(1 −q)3 = 1 + q
p2
V (X) =
q
p2

5
5–14.
P(X ≤2)
=
P(X = 1) + P(X = 2) = 0.8 + (0.2)(0.8) = 0.96
P(X ≤3)
=
3
X
x=1
(0.2)x−1(0.8) = 0.992
5–15.
P(X = 36) = (0.95)35(0.05) = 0.0083
5–16.
(a)
P(X = 8) =
µ7
2
¶
(0.1)2(0.9)5 = 0.0124
(b)
P(X > 8) =
∞
X
x=9
µx −1
2
¶
(0.1)2(0.9)x−3
5–17.
P(X = 4) =
µ3
1
¶
(0.8)2(0.2)2 = 0.0768
P(X < 4) = P(X = 2) + P(X = 3) = (0.8)2 +
µ2
1
¶
(0.8)2(0.2) = 0.896
5–18. Suppose X1, X2, . . . , Xr are independent geometric random variables, each with
parameter p. X1 is the number of trials to ﬁrst success, X2 the number of trials
from ﬁrst to the second, etc. Let
X = X1 + X2 + . . . + Xr
The moment generating function for the geometric is
pet
1−qet, so
MX(f) =
rY
i=1
MXi(t) =
·
pet
1 −qet
¸r
E[X] = M
′
X(t)|t=0 = r
p

6
We could also have obtained this result as follows.
E[X] =
r
X
i=1
E[Xi] = r
µ1
p
¶
= r
p
Continuing,
V (X) =
h
M
′′
X(t)|t=0
i
−
µr
p
¶2
= rq
p2
We could also have obtained this result as follows.
V (X) =
r
X
i=1
V (Xi) = rq
p2
5–19.
E[X] = r
p = 5
0.8 = 6.25,
V (X) = rq
p2 = (5)(0.2)
(0.8)2
= 1.5625
5–20. X = Mission number on which 4th hit occurs.
p(x) =



µx −1
3
¶
(0.8)4(0.2)x−4
x = 4, 5, 6, . . .
0
otherwise
P(X ≤7) =
7
X
x=4
µx −1
3
¶
(0.8)4(0.2)x−4
5–21. (X, Y, Z) ∼multinomial (n = 3, p1 = 0.4, p2 = 0.3, p3 = 0.3). The probability that
one company receives all orders is
P(3, 0, 0) + P(0, 3, 0) + P(0, 0, 3)
=
3!
3!0!0!(0.4)3(0.3)0(0.3)0 +
3!
0!3!0!(0.4)0(0.3)3(0.3)0 +
3!
0!0!3!(0.4)0(0.3)0(0.3)3
=
0.43 + 0.33 + 0.33
.= 0.118

7
5–22.
(a) (X1, X2, X3, X4) ∼multinomial (n = 5, p1 = p2 = p3 = p4 = 1
4). Therefore,
P(5, 0, 0, 0) + P(0, 5, 0, 0) + P(0, 0, 5, 0) + P(0, 0, 0, 5) is the probability that
one company gets all ﬁve. That is,
4
·
5!
5!0!0!0!
¸ µ1
4
¶5 µ1
4
¶0 µ1
4
¶0 µ1
4
¶0
=
1
256
(b)
1 −
"
4(60)
µ1
4
¶5#
.= 0.7656
5–23.
P(Y1 = 4, Y2 = 1, Y3 = 3, Y4 = 2) =
10!
4!1!3!2!(0.2)4(0.2)1(0.2)3(0.4)2 .= 0.005
5–24.
P(Y1 = 0, Y2 = 0, Y3 = 0, Y4 = 10) =
10!
0!0!0!10!(0.2)0(0.2)0(0.2)0(0.4)10
P(Y1 = 5, Y2 = 0, Y3 = 0, Y4 = 5) =
10!
5!0!0!5!(0.2)5(0.2)0(0.2)0(0.4)5
5–25.
(a)
P(X ≤2) =
2
X
x=0
µ4
x
¶ µ 21
5 −x
¶
µ24
5
¶
.= 0.98
(b)
P(X ≤2) =
2
X
x=0
µ5
x
¶ µ 4
25
¶x µ21
25
¶5−x .= 0.97
5–26. The approximation improves as n
N decreases. n = 5, N = 100 is a better condition
than n = 5, N = 25.

8
5–27. we want the smallest n such that
P(X ≥1) = 1 −
µ7
0
¶ µ18
n
¶
µ25
n
¶
≥0.95
⇔
µ18
n
¶
µ25
n
¶ ≤0.05.
By trial and error, we ﬁnd that n = 8 does the job.
We could instead use the binomial approximation; now we want n such that
0.05
≥
µn
0
¶ µ 7
25
¶0 µ18
25
¶n
=
µ18
25
¶n
.
We ﬁnd that n .= 9.
5–28.
MX(t) = E[etX] =
∞
X
x=0
etxe−ccx
x!
= e−c
∞
X
x=0
(cet)x
x!
= e−cecet = ec(et−1)
5–29.
P(X < 10) = P(X ≤9) =
9
X
x=0
e−25(25)x
x!
.= 0.0002
5–30.
P(X > 20) = P(X ≥21)
=
∞
X
x=21
e−10(10)x
x!
=
1 −P(X ≤20) = 1 −
20
X
x=0
e−10(10)x
x!
=
0.002
5–31.
P(X > 5)
=
P(X ≥6)
=
1 −P(X ≤5) = 1 −
5
X
x=0
e−44x
x!
.=
0.2149

9
5–32. Mean count rate = (1 −p)c. Therefore,
P(Yt = y) = e−[(1−p)c]t[(1 −p)ct]y
y!
y = 0, 1, 2, . . .
5–33. Using a Poisson model,
P(X ≤3)
=
3
X
x=0
e−λλx
x!
λ = 15000(0.002) = 30
P(X ≥5)
=
∞
X
x=5
e−30(30)x
x!
= 1 −
4
X
x=0
e−30(30)x
x!
5–34. Y = Number of requests.
(a)
P(Y > 3) = 1 −P(Y ≤3) = 1 −
3
X
y=0
e−22y
y!
(b)
E[Y ] = c = 2
(c)
P(Y ≤y) ≥0.9
so y = 4 and P(Y ≤4) = 0.9473
(d) X = Number serviced.
y
x
p(x)
xp(x)
0
0
e−2
0
1
1
2e−2
2e−2
2
2
2e−2
4e−2
3 or more
3
1 −5e−2
3 −15e−2
E[X] = 1.78
(e) Let M = number of crews going to central stores. Then M = Y −X
E[M] = E[Y ] −E[X] = 2 −1.78 = 0.22
5–35. Using a Poisson model,
P(X < 3) = P(X ≤2) =
2
X
x=0
e−2.5(2.5)x
x!
.= 0.544

10
5–36. Let Y = No. Boarding
Let X = No. Recorded
Y
0
1
2
3
4
5
6
7
8
9
≥10
X
0
1
2
3
4
5
6
7
8
9
10
pX(x)
=
e−ccx
x! ,
x = 0, 1, 2, . . . , 9
=
∞
X
i=10
e−cci
i!
= 1 −
9
X
i=0
e−cci
i!
,
x = 10
5–37.
(a) Let X denote the number of errors on 50 pages. Then
X ∼Binomial(5, 50
200) = Binomial(5, 1/4).
This implies that
P(X ≥1) = 1 −P(X = 0) = 1 −
µ5
0
¶
(1/4)0(3/4)5 = 0.763.
(b) Now X ∼Binomial(5,
n
200), where n is the number of pages sampled.
We want the smallest n such that
5
X
i=3
µ5
i
¶ µ n
200
¶iµ200 −n
200
¶5−i
≥0.90
By trial and error, we ﬁnd that n = 151 does the trick.
We could also have done this problem using a Poisson approximation. For (a),
we would use λ = 0.025 errors / page with 50 pages. Then c = 50(0.025) =
1.25, and we would eventually obtain P(X ≥1) = 1 −e−1.25(1.25)0
0!
.= 0.7135,
which is a bit oﬀof our exact answer. For (b), we would take c = n(0.025),
eventually yielding n = 160 after trial and error.
5–38.
P(X = 0) = e−cc0
0!
with c = 10000(0.0001) = 1,
P(X = 0) = e−1 = 0.3679
and
P(X ≥2) = 1 −P(X ≤1) = 0.265

11
5–39. X ∼Poisson with α = 10(0.1) = 0.10
P(X ≥2) = 1 −P(X ≤1) = 0.0047
5–40. Kendall and Stuart state: “the liability of individuals to accident varies.” That
is, the individuals who compose a population have diﬀerent degrees of accident
proneness.
5–41. Use Table XV and scaling by 10−5.
(a) From Col. 3 of Table XV,
Realization 1
Realization 2
u1 = 0.01536 < 0.5 ⇒x1 = 1
u1 = 0.63661 > 0.5 ⇒x1 = 0
u2 = 0.25595 < 0.5 ⇒x2 = 1
u2 = 0.53342 > 0.5 ⇒x2 = 0
u3 = 0.22527 < 0.5 ⇒x3 = 1
u3 = 0.88231 > 0.5 ⇒x3 = 0
u4 = 0.06243 < 0.5 ⇒x4 = 1
u4 = 0.48235 < 0.5 ⇒x4 = 1
u5 = 0.81837 > 0.5 ⇒x5 = 0
u5 = 0.52636 > 0.5 ⇒x5 = 0
u6 = 0.11008 < 0.5 ⇒x6 = 1
u6 = 0.87529 > 0.5 ⇒x6 = 0
u7 = 0.56420 > 0.5 ⇒x7 = 0
u7 = 0.71048 > 0.5 ⇒x7 = 0
u8 = 0.05463 < 0.5 ⇒x8 = 1
u8 = 0.51821 > 0.5 ⇒x8 = 0
x = 6
x = 1
Continue to get three more realizations.
(b) Use Col. 4 of Table XV (p = 0.4).
Realization 1
u1 = 0.02011 ≤0.4 ⇒x = 1
Realization 2
u1
=
0.85393 > 0.4
u2
=
0.97265 > 0.4
u3
=
0.61680 > 0.4
u4
=
0.16656 < 0.4 ⇒x = 4
Realization 3
u1
=
0.42751 > 0.4
u2
=
0.69994 > 0.4
u3
=
0.07972 < 0.4 ⇒x = 3
Continue to get seven more realizations of X.

12
(c) λt = c = 0.15, e−0.15 = 0.8607. Using Col. 6 of Table XV,
Realization
ui
product
< e−0.15?
x
#1
u1 = 0.91646
0.91646
No
u2 = 0.89198
0.81746
Yes
x = 1
#2
u1 = 0.64809
0.64809
Yes
x = 0
#3
u1 = 0.16376
0.16376
Yes
x = 0
#4
u1 = 0.91782
0.91782
No
u2 = 0.53498
0.49102
Yes
x = 1
#5
u1 = 0.31016
0.31016
Yes
x = 0
5–42. X ∼Geometric with p = 1/6.
y = x1/3
Using Col. 5 of Table XV, we obtain the following realizations.
# 1
u1
=
0.81647 > 1/6
u2
=
0.30995 > 1/6
u3
=
0.76393 > 1/6
u4
=
0.07856 < 1/6 ⇒x = 4, y = 1.587
# 2
u1 = 0.06121 < 1/6 ⇒x = 4, y = 1
Continue to get additional realizations.

1
Chapter 6
6–1.
fX(x) = 1
4;
0 < x < 4
P
µ1
2 < X < 7
4
¶
=
Z 7/4
1/2
dx
4
=
5
16,
P
µ9
4 < X < 27
8
¶
=
Z 27/8
9/4
dx
4
=
9
32
6–2.
fX(x) =
4
34;
143
4
< x < 177
4
P(X < 40) =
Z 40
143/4
4
34 dx = 1
2
P(40 < X < 42) =
Z 42
40
4
34 dx =
4
17
6–3.
fX(x) = 1
2,
0 ≤x ≤2
FY (y)
=
P(Y ≤y) = P
µ
X ≤y −5
2
¶
=
Z
y−5
2
0
dx
2
= y −5
4
So
fY (y) = 1
4,
5 < y < 9
6–4. The p.d.f. of proﬁt, X, is
fX(x) =
1
2000;
0 < x < 2000
Y = Brokers Fees = 50 + 0.06X
FY (y)
=
P(50 + 0.06X ≤y)
=
P
µ
X ≤y −50
0.06
¶
=
Z
y−50
0.06
0
dx
2000 = y −50
120
fY (y) =
1
120;
50 < y < 170

2
6–5.
MX(t)
=
E(etX) =
Z β
α
etx
dx
β −α
=
etx
(β −α)t
¯¯¯¯
β
α
=
etβ −etα
(β −α)t
Using L’Hˆospital’s rule when necessary, we obtain
E(X)
=
M ′
X(0) =
1
β −α
£
t−1βetβ −t−2etβ −t−1αetα + t−2etα¤
t=0
=
1
β −α
£
β2etβ −β2etβ/2 −α2etα + α2etα/2
¤
t=0
=
β + α
2
and
E(X2)
=
M
′′
X(0)
=
1
β −α
£
t−1β2etβ −βetβt−2 + 2etβt−3 −t−2βetβ
−t−1α2etα + αetαt−2 + t−2αetα −2etαt−3¤
t=0
=
1
β −α
·β3 −α3
3
¸
V (X) = E(X2) −[E(X)]2 = (β −α)2
12
6–6.
E(X) = β + α
2
= 0 ⇒β + α = 0
V (X) = (β −α)2
12
= 1 ⇒β2 −2αβ + α2 = 12
⇒α = −
√
3,
β = +
√
3

3
6–7. The CDF for Y is
y
FY (y)
y < 1
0
1 ≤y < 2
0.3
2 ≤y < 3
0.5
3 ≤y < 4
0.9
y > 4
1
Generate realizations of ui ∼Uniform[0,1] random numbers as described in Section
6–6; use these in the inverse as yi = F −1
Y (ui), i = 1, 2, . . .. For example, if u1 =
0.623, then y1 = F −1
Y (0.623) = 3.
6–8.
fX(x)
=
1
4;
0 < x < 4
=
0;
otherwise
The roots of y2 + 4xy + (x + 1) = 0 are real if 16x2 −4(x + 1) ≥0, or if
(x −1
8)2 −17
64 ≥0
or where x ≤1
8(1 −
√
17) or x ≥1
8(1 +
√
17)
P
µ
X ≤1
8(1 −
√
17)
¶
= 0
P
µ
X ≥1
8(1 +
√
17)
¶
=
Z 4
1
8 (1+
√
17)
dx
4
=
1
32(31 −
√
17)
6–9.
MX(t)
=
Z ∞
0
etxλe−λx dx = λ
Z ∞
0
ex(t−λ) dx,
which converges if t < λ. Thus for t < λ,
MX(t)
=
λ
t −λ [ex(t −λ)]∞
x=0 =
λ
λ −t =
1
1 −λ/t, t < λ

4
E(X)
=
M
′
X(0) =
£
λ(λ −t)−2¤
t=0 = 1
λ
E(X2)
=
M
′′
X(0) =
£
2λ(λ −t)−3¤
t=0 =
2
λ2
V (X)
=
E(X2) −[E(X)]2 =
2
λ2 −1
λ2 =
1
λ2
6–10. X denotes life length,
E(X) = 1
λ = 3 ⇒λ = 1
3
P ∗denotes proﬁt
P ∗
=
1000,
X > 1
=
750,
X ≤1
E(P ∗)
=
1000P(X > 1) + 750P(X ≤1)
=
1000e−1/3 + 750(1 −e−1/3) = $929.13
6–11.
P(X < 1
2) =
Z 1/2
0
1
3e−x/3dx = 1 −e(−1/3)(1/2) = 0.154
or 15.4% experience failure in the ﬁrst six months.
6–12. P ∗= Proﬁt, T = Life Length
P ∗
=
rY −dY,
T > Y
=
rT −dY,
T ≤Y
E(P ∗)
=
(rY −dY )P(T ≥Y ) −dY P(T ≤Y ) + r
Z Y
0
tθe−θt dt
=
r(θ−1 −θ−1e−θY ) −dY
dE(P k)
dY
= re−θY −d = 0 ⇒Y = −θ−1ℓn(d
r)
For Y to be positive, 0 < d
r < 1.

5
6–13. X = Life Length,
E(X) = 1
λ = 3 ⇒λ = 1
3
P(X < 1) = 1 −e−1/3 = 0.283
28.3% of policies result in a claim.
6–14. No.
P(X ≤2) = 1 −e−2λ
P(X ≤3) = 1 −e−3λ
1 −e−2λ = 2
3(1 −e−3λ) ⇒1 = 3e−2λ −2e−3λ
Only λ = 0 satisﬁes this condition; but we must have λ > 0 , so there is no value
of λ for which
P(X ≤2) = 2
3P(X ≤3)
6–15.
CI
=
C,
X > 15;
=
C + Z,
X ≤15
CII
=
3C,
X > 15
=
3C + Z,
X ≤15
E(CI)
=
CP(X > 15) + (C + Z)P(X ≤15)
=
Ce−15/25 + (C + Z)(1 −e−15/25)
.= C + 0.4512Z
E(CII)
=
3CP(X > 15) + (3C + Z)P(X ≤15)
=
3Ce−15/25 + (3C + Z)(1 −e−15/35)
.= 3C + 0.3426Z
E(CII) −E(CI) = 2C −0.1026Z
which favors process I if C > 0.0513Z.
6–16.
P(X > x+s|X > x) = P(X > s) = P(X > 10000) = e−10000/20000 = 0.6064
and P(X < 30000|X > 20000) = 0.3936.

6
6–17.
Γ(p) =
Z ∞
0
xp−1e−xdx
Let x = y2 ⇒dx
dy = 2y. So Γ(p) =
R ∞
0 y2p−1e−y2dy and Γ(1
2) = 2
R ∞
0 e−y2dy.
µ
Γ(1
2)
¶2
=
4
Z ∞
0
e−y2 dy ·
Z ∞
0
e−x2 dx
=
4
Z ∞
0
Z ∞
0
e−(x2+y2) dx dy
Let x = ρ cos(θ) and y = ρ sin(θ). So
µ
Γ(1
2)
¶2
=
4
Z π/2
0
Z ∞
0
ρe−ρ2 dρ dθ
=
2
Z π/2
0
dθ = π
So Γ( 1
2) = √π.
6–18. Integrate by parts with u = xn−1, dv = e−xdx
Γ(n)
=
Z ∞
0
xn−1e−xdx =
£
−e−xxn−1¤∞
0 + (n −1)
Z ∞
0
xn−2e−xdx
=
0 + (n −1)
Z ∞
0
xn−2e−xdx = (n −1) · Γ(n −1)
Repeatedly using the approach above, we get
Γ(n)
=
(n −1) · Γ(n −1) = (n −1)(n −2) · Γ(n −2)
=
· · · = (n −1)(n −2) · · · Γ(1).
Since Γ(1) =
R ∞
0 e−xdx = 1, we have Γ(n) = (n −1)!
6–19.
Y = X1 + · · · + X10

7
g(xi) =
½ 7e−7xi
if xi > 0
0
otherwise
P(Y > 1)
=
Z ∞
1
7
Γ(10)(7y)9e−7ydy
=
9
X
k=0
e−7·17k
k! = 0.8305
6–20.
λ = 6; t = 4 ⇒λt = 24
P(X ≥24) = 1 −P(X ≤23) = 1 −
23
X
x=0
e−2424x
x!
6–21.
MX(t)
=
E(etX) =
Z ∞
0
etx
λ
Γ(r)(λx)r−1e−λxdx
=
λr
Γ(r)
Z ∞
0
xr−1e−x(λ−t)dx
The integral converges if (λ −t) > 0 or λ > t. Let u = x(λ −t), dx
du =
1
(λ−t). So
MX(t)
=
λr
Γ(r)
Z ∞
0
µ
u
λ −t
¶r−1
e−u(λ −t)−1 du
=
µ
λ
λ −t
¶r
·
1
Γ(r)
Z ∞
0
ur−1e−u du
=
µ
λ
λ −t
¶r
·
1
Γ(r) · Γ(r) =
µ
λ
λ −t
¶r
=
(1 −(t/λ))−r, where λ > t
6–22.
P(Y > 24)
=
Z ∞
24
0.25
Γ(4) (0.25y)3e−0.25y dy
=
3
X
k=0
e−0.25·24(0.25 · 24)k
k!
= 0.1512

8
6–23. E(X) = r/λ = 40, V (X) = r/λ2 = 400
λ = 0.1, r = 4
P(X < 20)
=
Z 20
0
0.1
Γ(4)(0.1x)3e−0.1x dx
=
1 −
3
X
k=0
e−0.1·20(0.1 · 20)k
k!
= 0.1429
P(X < 60) = 1 −
3
X
k=0
e−0.1·60(0.1 · 60)k
k!
= 0.8488
6–24.
E(X) =
λr
Γ(r)
Z ∞
0
x(x −u)r−1e−λ(x−u)dx
Let y = λ(x −u) ⇒dx
dy = 1
λ
E(X)
=
λr−2
Γ(r)
Z ∞
0
(y + λu)(y/λ)r−1e−y dy
=
1
λΓ(r)
Z ∞
0
yre−ydy +
λu
λΓ(r)
Z ∞
0
yr−1e−y dy
=
Γ(r + 1)
λΓ(r)
+ uΓ(r)
Γ(r)
=
r
λ + u
6–26.
fX(x) = Γ(λ + r)
Γ(λ)Γ(r)xλ−1(1 −x)r−1
λ = r = 1 gives
fX(x)
=
Γ(2)
Γ(1)Γ(1)x0(1 −x)0
=
½ 1
if 0 < x < 1
0
otherwise

9
6–27. λ = 2, r = 1
fX(x)
=
Γ(3)
Γ(2)Γ(1)x(1 −x)0
=
½ 2x
if 0 < x < 1
0
otherwise
λ = 1, r = 2
fX(x)
=
Γ(3)
Γ(1)Γ(2)x0(1 −x)
=
½ 2(1 −x)
if 0 < x < 1
0
otherwise
6–29. See solution to 3-22.
6–30.
E(X) =
Z ∞
0
x β
δ
µx −γ
δ
¶β−1
e−( x−γ
δ
)β dx
First let y = x−γ
δ
⇒dx = δdy
E(X)
=
Z ∞
0
(δy + γ) β
δ yβ−1e−yβδ dy
=
β
Z ∞
0
(δy + γ)yβ−1e−yβ dy
Let u = yβ ⇒dy = β−1y−β+1du
E(X)
=
β
Z ∞
0
(δu1/β + γ)u(1−1/β)e−uβ−1u(1/β−1) du
=
γ
Z ∞
0
e−u du +
Z ∞
0
δu1/βe−u du
=
γ + δΓ
µ
1 + 1
β
¶
Using the same approach
E(X2) =
Z ∞
0
x2 β
δ
µx −γ
δ
¶β−1
e−( x−γ
δ
) dx

10
Let y = x−γ
δ
⇒dx = δdy
E(X2) =
Z ∞
0
(δy + γ)2 β
δ yβ−1 e−yβ δ dy
u = yβ ⇒dy = β−1y1−β
E(X2)
=
Z ∞
0
(δu1/β + γ)2 β
δ u1−1/β e−uδ β−1 u1/β−1 du
=
δ2 Γ
µ
1 + 2
β
¶
+ 2γδ Γ
µ
1 + 1
β
¶
+ γ2
So
V (X) = E(X2) −[E(X)]2 = δ2
·
Γ
µ
1 + 2
β
¶
−Γ2
µ
1 + 1
β
¶¸
6–31.
F(x) = 1 −e−( x−γ
δ
)β
F(1.5) = 1 −e−( 1.5−1
0.5 )2 .= 0.63
6–32.
F(x) = 1 −e−( x−0
400 )1/3
1 −F(600) = e−( 600
400)1/3 .= 0.32
6–33.
F(x) = 1 −e−( x−0
400 )1/2
1 −F(800) = 1 −e−21/2 .= 0.24
6–34. The graphs are identical to those shown in Figure 6–8.
6–35.
F(x) = 1 −e−( x−0
200 )1/4
(a) 1 −F(1000) = 1 −e−51/4 .= 0.22
(b) 0 + 200Γ(5) = 200 · 24 = 4800 = E(X)

11
6–36. P ∗= Proﬁt
P ∗
=
$100;
x ≥8760
=
−$50;
x < 8760
E(P ∗)
=
−50
Z 8760
0
20000−1e−20000−1x dx + 100
Z ∞
8760
20000−1e−20000−1x dx
=
−50(1 −e−876/2000) + 100e−876/2000
=
$46.80/set
6–37. r/λ = 20
(r/λ2)1/2 = 10
r = 4, λ = 0.2
P(X ≤15) = F(15) = 1 −
3
X
k=0
e−33k/k! = 0.3528
6–38. (a) Use Table XV, Col. 1 with scaling and Equation 6–35.
u1 = 0.10480
x1 = 10 + u1(10) = 11.0480
u2 = 0.22368
x2 = 10 + u2(10) = 12.2368
u3 = 0.24130
x3 = 10 + u3(10) = 12.4130
...
...
u10 = 0.85475
x10 = 10 + u10(10) = 18.5475
(b) Use Table XV, Col. 2; xi = −50000 ℓn(1 −ui)
u1 = 0.15011
x1 = −50000 ℓn(0.84989) = 8132.42
u2 = 0.46573
x2 = −50000 ℓn(0.53427) = 31345.51
u3 = 0.48360
x3 = −50000 ℓn(0.51640) = 33043.68
u4 = 0.93093
x4 = −50000 ℓn(0.06907) = 133631.74
u5 = 0.39975
x5 = −50000 ℓn(0.60025) = 25520.45
(c) a =
√
3 = 1.732, b = 4 −ℓn(4) + 3−1/2 = 3.191. Now use Table XV, Col. 3.

12
Realization 1: u1 = 0.01563, u2 = 0.25595
y = 2
µ0.01563
0.98437
¶1.732
= 0.00153
3.191 −ℓn[(0.01563)20.25595] = 12.871
y ≤12.87
x1 = y/4 = 0.0003383
Realization 2: u1 = 0.22527, u2 = 0.06243
y = 2
µ0.22527
0.77473
¶1.732
= 0.2354
3.191 −ℓn[(0.22527)20.06243] = 8.9456
y ≤8.9456
x2 = y/4 = 0.05885
Continue for additional realizations.
Note: Since r is an integer, an alternate scheme which may be more eﬃcient here
is to let xi = xi1 + xi2, where xij is exponential with parameter λ = 4.
xij = −0.25 ℓn(1 −uij), i = 1, 2, . . . , 5, j = 1, 2
Realization 1: u1 = 0.01563, u2 = 0.25595
x11 = −0.25 ℓn(0.98437) = 0.003938
x12 = −0.25 ℓn(0.74405) = 0.073712
This yields x1 = 0.07785. Continue for more realizations.
(d) Use Table XV, Col. 4 with scaling.
u1 = 0.02011
x1 = 0 + 100[−ℓn(0.02011)]2 = 390.654
u2 = 0.85393
x2 = 0 + 100[−ℓn(0.85393)]2 = 15.791
...
...
u10 = 0.53988
x10 = 0 + 100[−ℓn(0.53988)]2 = 61.641

13
6–39. (a) Using Table XV, Col. 5, and y = x0.3, we get
u1 = 0.81647
x1 = −10 ℓn(0.18353) = 16.954
y1 = 2.3376
u2 = 0.30995
x2 = −10 ℓn(0.69005) = 3.7099
y2 = 1.4818
...
...
...
u10 = 0.53060
x10 = −10 ℓn(0.46940) = 7.5630
y10 = 1.8348
(b) Using the gamma variates in Problem 6–38(c) and Table XV, Col. 3 entry #25,
y1 = (0.000383)1/2
(0.28834)1/2
= 0.03645
y2 = (0.05885)1/2
(0.04839)1/2 = 1.102797
...
etc.

1
Chapter 7
7–1.
(a) P(0 ≤Z ≤2) = Φ(2) −Φ(0) = 0.97725 −0.5 = 0.47725
(b) P(−1 ≤Z ≤1) = Φ(1) −Φ(−1) = 2Φ(1) −1 = 0.68268
(c) P(Z ≤1.65) = Φ(1.65) = 0.95053
(d) P(Z ≥−1.96) = Φ(1.96) = 0.9750
(e) P(|Z| ≥1.5) = 2[1 −Φ(1.5)] = 0.1336
(f) P(−1.9 ≤Z ≤2) = Φ(2) −Φ(−1.9) = Φ(2) −[1 −Φ(1.9)] = 0.94853
(g) P(Z ≤1.37) = 0.91465
(h) P(|Z| ≤2.57) = 2Φ(2.57) −1 = 0.98984
7–2. X ∼N(10, 9).
(a) P(X ≤8) = Φ
µ8 −10
3
¶
= Φ
µ
−2
3
¶
= 0.2525
(b) P(X ≥12) = 1 −Φ
µ12 −10
3
¶
= 1 −Φ
µ2
3
¶
= 0.2525
(c) P(2 ≤X ≤10) = Φ
µ10 −10
3
¶
−Φ
µ2 −10
3
¶
= 0.5 −Φ(−2.67) = 0.496
7–3. From Table II of the Appendix
(a) c = 1.56
(b) c = 1.96
(c) c = 2.57
(d) c = −1.645
7–4. P(Z ≥Zα) = α ⇒Φ(Zα) = 1 −α.
(a) Z0.025 = 1.96
(b) Z0.005 = 2.57
(c) Z0.05 = 1.645
(d) Z0.0014 = 2.99

2
7–5. X ∼N(80, 100).
(a) P(X ≤100) = Φ
µ100 −80
10
¶
= Φ(2) = 0.97725
(b) P(X ≤80) = Φ
µ80 −80
10
¶
= 0.5
(c) P(75 ≤X ≤100) = Φ
µ100 −80
10
¶
−Φ
µ75 −80
10
¶
= Φ(2) −Φ(−0.5) =
0.97725 −0.30854 = 0.66869
(d) P(X ≥75) = 1 −Φ
µ75 −80
10
¶
= 1 −Φ(−0.5) = Φ(0.5) = 0.69146
(e) P(|X −80| ≤19.6) = Φ(1.96) −Φ(−1.96) = 0.95
7–6.
(a) P(X > 680) = 1 −Φ
µ680 −600
60
¶
= 1 −Φ(1.33) = 0.09176
(b) P(X ≤550) = Φ
µ550 −600
60
¶
= Φ(−5/6) = 1 −Φ(5/6) = 0.20327
7–7. P(X > 500) = 1 −Φ
µ500 −485
30
¶
= 1 −Φ(0.5) = 0.30854, i.e., 30.854%
7–8.
(a) P(X ≥28.5) = 1 −Φ
µ28.5 −30
1.1
¶
= 1 −Φ(−1.36) = Φ(1.36) = 0.91308
(b) P(X ≤31) = Φ
µ31 −30
1.1
¶
= 0.819
(c) P(|X −30| > 2) = 1 −
·
Φ
µ 2
1.1
¶
−Φ
µ
−2
1.1
¶¸
= 1 −[0.96485 −0.03515] =
0.0703
7–9. X ∼N(2500, 5625). Then P(X < ℓ) = 0.05 implies that
P
µ
Z < ℓ−2500
75
¶
= 0.05
or
ℓ−2500
75
= −1.645.
Thus, ℓ= 2376.63 is the lower speciﬁcation limit.

3
7–10.
MX(t)
=
E(etX) =
1
σ
√
2π
Z ∞
−∞
etx e−(x−µ)2
2σ2
dx
=
σ
σ
√
2π
Z ∞
−∞
et(yσ+µ) e−y2/2 dy
(letting y = (x −µ)/σ)
=
eµt
√
2π
Z ∞
−∞
e−(y2−2σty)/2 dy
=
eµt
√
2π
Z ∞
−∞
e−(y2−2σty+σ2t2−σ2t2)/2 dy
=
eµt
√
2π
Z ∞
−∞
e−(y−σt)2/2eσ2t2/2 dy
=
eµt+(1/2)σ2t2
1
√
2π
Z ∞
−∞
e−w2/2 dw
(letting w = y −σt)
=
eµt+(1/2)σ2t2,
since the integral term equals 1.
7–11.
FY (y)
=
P(aX + b ≤y) = P
µ
X ≤y −b
a
¶
=
Φ
µ y−b
a −µ
σ
¶
=
Φ
µy −b −aµ
aσ
¶
= Φ
µy −(aµ + b)
aσ
¶
This implies that Y ∼N(aµ + b, a2σ2).
7–12. X ∼N(12, (0.02)2).
(a) P(X > 12.05) = 1 −Φ
µ12.05 −12
0.02
¶
= 1 −Φ(2.5) = 0.00621

4
(b)
P(X > c) = 0.9
⇒
1 −Φ
µc −12
0.02
¶
= 0.9
⇒
Φ
µc −12
0.02
¶
= 0.1
⇒
c −12
0.02
= −1.28
⇒
c = 12 −0.0256 = 11.97
(c)
P(11.95 ≤X ≤12.05)
=
Φ
µ12.05 −12
0.02
¶
−Φ
µ11.95 −12
0.02
¶
=
Φ(2.5) −Φ(−2.5) = 0.9876
7–13. X ∼N(µ, (0.1)2).
(a) Take µ = 7.0. Then
P(X > 7.2) + P(X < 6.8)
=
1 −Φ
µ7.2 −7
0.1
¶
+ Φ
µ6.8 −7
0.1
¶
=
1 −Φ(2) + Φ(−2)
=
1 −0.97725 + 0.02275 = 0.0455
(b)
1 −Φ
µ7.2 −7.05
0.1
¶
+ Φ
µ6.8 −7.05
0.1
¶
=
1 −Φ(1.5) + Φ(−2.5)
=
1 −0.93319 + 0.00621 = 0.07302
(c)
Φ
µ7.2 −7.25
0.1
¶
−Φ
µ6.8 −7.25
0.1
¶
=
Φ(−0.5) −Φ(−4.5)
.=
1 −Φ(0.5) = 0.3085

5
(d)
Φ
µ7.2 −6.75
0.1
¶
−Φ
µ6.8 −6.75
0.1
¶
.= 1 −Φ(0.5) = 0.3085
7–14. X ∼N(50, 25), Y ∼N(45, 6.25).
If Y ≥X, i.e., if Y −X ≥0, a transaction will occur.
Let W = Y −X ∼N(−5, 31.25).
P(W > 0) = P
µ
Z ≥0 + 5
5.59
¶
= 1 −Φ(0.89) = 0.1867.
7–15. $9.00 = revenue / capacitor, k = manufacturing cost for process A, 2k = manufac-
turing cost for process B. The proﬁts are
P ∗
A =
½
9 −k
if 1000 ≤X ≤5000
9 −k −3
otherwise
P ∗
B =
½
9 −2k
if 1000 ≤X ≤5000
9 −2k −3
otherwise
Therefore,
E(P ∗
A)
=
(9 −k)P(1000 ≤X ≤5000) + (6 −k)[1 −P(1000 ≤X ≤5000)]
=
(9 −k)0.9544 + (6 −k)0.0456 = 8.8632 −k
E(P ∗
B)
=
(9 −2k)P(1000 ≤X ≤5000) + (6 −2k)[1 −P(1000 ≤X ≤5000)]
=
(9 −2k)(1) + (6 −k)(0) = 9 −2k
Since E(P ∗
A) < E(P ∗
B) when k < 0.1368, use process B; When k ≥0.1368, use
process A.
7–16. The proﬁt P is
P =



C
if 6 ≤X ≤8
−R1
if X < 6
−R2
if X > 8

6
E(P)
=
CP(6 ≤X ≤8) −R1P(X < 6) −R2P(X > 8)
=
C[Φ(8 −µ) −Φ(6 −µ)] −R1Φ(6 −µ) −R2[1 −Φ(8 −µ)]
=
(C + R2)Φ(8 −µ) −(C + R1)Φ(6 −µ) −R2
Then
dE(P)
dµ
= −(C + R2)Φ(8 −µ) + (C + R1)Φ(6 −µ) = 0,
or
C + R2
C + R1
= Φ(6 −µ)
Φ(8 −µ) = e14−2µ
Thus,
µ = 7 −1
2 ℓn
µC + R2
C + R1
¶
.
7–17. If R1 = R2 = R, then µ = 7 −0.5 ℓn(1) = 7, which is the midpoint of the interval
[6,8].
7–18. µ = 7 −1
2 ℓn
µ12
10
¶
= 6.909.
7–19. X ∼N(70, 16).
(a) We have
P(62 ≤X ≤72)
=
Φ
µ72 −70
4
¶
−Φ
µ62 −70
4
¶
=
Φ(0.5) −Φ(−2)
=
0.69146 −0.02275 = 0.66871.
(b) c = 1.96σ = 7.84.
(c) (9)(0.66871) = 6.018.

7
7-20. E(Y ) = E
µ n
X
i=1
Xi
¶
=
n
X
i=1
E(Xi) = nµ, so
E(Zn) = E(Y ) −nµ
√
σ2n
= 0
V (Y ) = V
µ n
X
i=1
Xi
¶
=
n
X
i=1
V (Xi) = nσ2
V (Zn) = V (Y )
nσ2
= 1
7–21. E( ¯X) = E
µ1
n
n
X
i=1
Xi
¶
= 1
n
n
X
i=1
E(Xi) = nµ
n
= µ
V ( ¯X) = V
µ1
n
n
X
i=1
Xi
¶
=
1
n2
n
X
i=1
V (Xi) = nσ2
n2
= σ2
n
7–22. X1 ∼N(1.25, 0.0009) and X2 ∼N(1.2, 0.0016).
Y = X1 −X2, E(Y ) = 0.05, V (Y ) = 0.0025.
Y ∼N(0.05, 0.0025)
P(Y < 0) = Φ
µ0 −0.05
0.05
¶
= Φ(−1) = 1 −Φ(1) = 0.15866.
7–23. Xi ∼N(2, 0.04), i = 1, 2, 3, and Y = X1 + X2 + X3 ∼N(6, 0.12).
Then
P(5.7 < Y < 6.3)
=
Φ
µ6.3 −6.0
0.3464
¶
−Φ
µ5.7 −6.0
0.3464
¶
=
Φ(0.866) −Φ(−0.866) = 0.6156.
7–24. E(Y ) = E(X1) + 2E(X2) + E(X3) + E(X4) = 4 + 2(4) + 2 + 3 = 17.

8
With independence,
V (Y ) = V (X1) + 22V (X2) + V (X3) + V (X4) = 3 + 4(4) + 4 + 2 = 25.
P(15 ≤Y ≤20)
=
Φ
µ20 −17
5
¶
−Φ
µ15 −17
5
¶
=
Φ(0.6) −Φ(−0.4)
=
0.72575 −0.34458 = 0.38117.
7–25. E(Xi) = 0, V (Xi) = 1/12.
Y =
50
X
i=1
Xi,
E(Y ) = 0,
V (Y ) = 50/12
P(Y > 5) = 1 −Φ
µ 5 −0
p
50/12
¶
= 1 −Φ(2.45) = 0.00714.
7–26. E(Xi) = 1, V (Xi) = 0.0001, i = 1, 2, . . . , 100.
Y =
100
X
i=1
Xi.
Assuming that the Xi’s are independent, we use the central limit theorem to ap-
proximate the distribution of Y ∼N(1000, 0.01). Then
P(Y > 102) = P
µ
Z > 102 −100
0.1
¶
= 1 −Φ(20) .= 0.
7–27. ¯X ∼N(11.9, 0.0025) and n = 9.
Thus, µ = 11.9, σ2/n = σ2/9 = 0.0025, so
σ2 = 0.0225. All of this implies that X ∼N(11.9, 0.0225). Then
P(11.8 < X < 12.2) = Φ(2) −Φ(−0.67) = 0.7258,
so that there are 27.4% defective.
If µ = 12, then
P(11.8 < X < 12.2) = Φ(1.33) −Φ(−1.33) = 0.8164,
or 18.4% defective. This is the optimal value of the mean.

9
7–28. Y =
n
X
i=1
E(Xi), where Xi is the travel time between pair i.
E(Y )
=
n
X
i=1
E(Xi) = 30
V (Y )
=
n
X
i=1
V (Xi)
=
(0.4)2 + (0.6)2 + (0.3)2 + (1.2)2 + (0.9)2 + (0.4)2 + (0.4)2 = 3.18.
Thus,
P(Y ≤32) = Φ
µ32 −30
√
3.18
¶
= Φ(1.12) = 0.86864.
7–29. p = 0.08, n = 200, np = 16, √npq = 3.84.
(a) P(X ≤16) = Φ
µ16.5 −16
3.84
¶
= Φ(0.13) = 0.55172.
(b) Φ
µ15.5 −16
3.84
¶
−Φ
µ14.5 −16
3.84
¶
= Φ(−0.13) −Φ(−0.391) = 0.1.
(c) Φ
µ20.5 −16
3.84
¶
−Φ
µ11.5 −16
3.84
¶
= Φ(1.17) −Φ(−1.17) = 0.758.
(d) Φ
µ14.5 −16
3.84
¶
−Φ
µ13.5 −16
3.84
¶
= Φ(−0.391) −Φ(−0.651) = 0.09.
7–30. P(0.05 ≤ˆp ≤0.15) = 0.95 implies that
P
µ0.05 −0.10
p
0.09/n
≤Z ≤0.15 −0.10
p
0.09/n
¶
= Φ
µ
0.05
0.3/√n
¶
−Φ
µ −0.05
0.3/√n
¶
= 0.95
⇒2 Φ
µ
0.05
0.3/√n
¶
= 1.95
⇒Φ(0.167√n) = 0.9750
⇒0.167√n = 1.96
⇒n = 139

10
7–31. Z1 =
p
−2 ℓn(u1) · cos(2πu2), Z2 =
p
−2 ℓn(u1) · sin(2πu2). Note that the sine and
cosine calculations are carried out in radians.
Obtain uniforms from Col. 2.
u1
u2
z1
z2
0.15011
0.46573
−1.902
0.416
0.48360
0.93093
1.093
−0.507
0.39975
0.06907
1.229
0.569
These results give
x1
=
100 + 2(−1.902) = 96.196
x2
=
100 + 2(0.416) = 100.832
x3
=
100 + 2(1.093) = 102.186
x4
=
100 + 2(−0.507) = 98.986
x5
=
100 + 2(1.229) = 102.458
x6
=
100 + 2(0.569) = 101.138
7–32. Calculate Z1 and Z2 as in Problem 7–31, obtaining uniforms from Col. 4.
u1
u2
z1
z2
0.02011
0.08539
2.402
1.429
0.97265
0.61680
−0.175
−0.158
0.16656
0.42751
−1.700
0.833
These results give realizations of X1.
x1
3x1
10 + 1.732(2.402) = 14.161
42.483
10 + 1.732(1.429) = 12.475
37.425
10 + 1.732(−0.175) = 9.697
29.091
10 + 1.732(−0.158) = 9.726
29.179
10 + 1.732(−1.700) = 7.056
21.167
10 + 1.732(0.833) = 11.443
34.328

11
Meanwhile, we use Col. 5 to calculate realizations of X2.
x2
−2x2
20(0.81647) = 16.329
−32.658
20(0.30995) = 6.199
−12.398
20(0.76393) = 15.279
−30.558
20(0.07856) = 1.571
−3.142
20(0.06121) = 1.224
−2.448
20(0.27756) = 5.551
−11.102
Finally, we get the six realizations of Y ,
y1
=
9.825
y2
=
25.027
y3
=
−1.467
y4
=
26.039
y5
=
18.719
y6
=
23.226
7–33. Using the zi realizations from Problem 7–31,
(−1.092)2
=
3.618
(0.416)2
=
0.173
(1.093)2
=
1.195
(−0.507)2
=
0.257
(1.229)2
=
1.510
7–34. Let z1, z2, . . . , zn be realizations of N(0, 1) r.v.’s.
yi = µY + σzi, i = 1, 2, . . . , n.
xi = eyi, i = 1, 2, . . . , n.

12
7–35. Generate a pair z1, z2 of N(0, 1) r.v.’s.
Let x1 = µ1 + σ1z1 and x2 = µ2 + σ2z2. Thus, Xi ∼N(µi, σ2
i ), i = 1, 2.
Let y1 = √x1/x2
2.
Repeat this procedure for as many realizations as desired.
7–36. This is a normal distribution truncated on the right, with p.d.f.
f(x)
=
1
Φ(r−µ
σ ) σ
√
2π exp
·
−(x −µ)2
2σ2
¸
if −∞< x ≤r
=
0
if x > r
For our problem, r = 2600, µ = 2500, and σ = 50. Now, after a bit of calculus,
E(X)
=
Z ∞
−∞
xf(x) dx
=
µ −
σ
Φ( r−µ
σ )
√
2π exp
·
−(r −µ)2
2σ2
¸
=
2500 −
50
0.9772
√
2π exp
·
−(2600 −2500)2
2(50)2
¸
=
2497.24
7–37. E(X) = e62.5, V (X) = e125(e25 −1), median(X) = e50, mode(X) = e25
7–38. W is lognormal with parameters 17.06 and 7.0692, or ℓn(W) ∼N(17.06, 7.0692).
Thus, P(L ≤W ≤R) = 0.90 implies P(ℓn(L) ≤ℓn(W) ≤ℓn(R)) = 0.90, or
Φ
µℓn(R) −17.06
√
7.0692
¶
−Φ
µℓn(L) −17.06
√
7.0692
¶
= 0.90.
Assuming that the interval [ℓn(L), ℓn(R)] is symmetric about 17.06, we obtain
ℓn(L) = 17.06 −c and ℓn(R) = 17.06 + c, so that
Φ
µ
c
2.6588
¶
−Φ
µ
−c
2.6588
¶
= 0.90.

13
This means that
c
2.6588 = 1.645, or c = 4.374.
ℓn(L) = 12.69, or L = 324486.8 and
ℓn(R) = 21.43, or R = 2027359410
7–39. Y ∼N(µ, σ2), Y = ℓn(X), or X = eY . The function ey is strictly increasing in y;
thus, from Theorem 3–1,
f(x) =
1
xσ
√
2π exp
·
−(ℓn(x) −µ)2
2σ2
¸
;
x ≥0.
7–41. X1 ∼N(2000, 2500), X2 ∼N(0.10, 0.01), ρ = 0.87.
E(X1|x2)
=
µ1 + ρ(σ1/σ2)(x2 −µ2)
=
2000 + (0.87)(50/0.1)(0.098 −0.10)
=
1999.13
V (X1|x2)
=
σ2
1(1 −ρ2) = 2500(1 −0.7569) = 607.75
P(X1 > 1950|x2 = 0.098)
=
P
µ
Z > 1950 −1999.13
√
607.75
¶
=
1 −Φ(−1.993) = 0.9769
7–42. X1 ∼N(75, 25), X2 ∼N(83, 16), ρ = 0.8.
E(X2|x1)
=
µ2 + ρ(σ2/σ1)(x1 −µ1)
=
83 + (0.8)(4/5)(80 −75)
=
86.2
V (X2|x1)
=
σ2
2(1 −ρ2) = 16(1 −0.64) = 5.76
P(X2 > 80|x1 = 80)
=
P
µ
Z > 80 −86.2
√
5.76
¶
= 0.9951

14
7–43.
(a) f(x1, x2) = k implies that
k =
1
2πσ1σ2
p
1 −ρ2 exp
½
−1
2(1 −ρ2)
·µx1 −µ1
σ1
¶2
−2ρ(x1 −µ1)(x2 −µ2)
σ1σ2
+
µx2 −µ2
σ2
¶2¸¾
For a selected value of k, the quantity in brackets assumes a value, say c; thus,
µx1 −µ1
σ1
¶2
−2ρ(x1 −µ1)(x2 −µ2)
σ1σ2
+
µx2 −µ2
σ2
¶2
−c = 0,
which is a quadratic in x1 −µ1 and x2 −µ2. If we write the general second-
degree equation as
Ay2
1 + By1y2 + Cy2
2 + Dy1 + Ey2 + F = 0,
we can determine the nature of the curve from the second-order terms. In
particular, if B2 −4AC < 0, the curve is an ellipse. In any case,
B2 −4AC =
µ 2ρ
σ1σ2
¶2
−
4
σ2
1σ2
2
= 4(ρ2 −1)
σ2
1σ2
2
< 0,
the last inequality a result of the fact that ρ2 < 1 (for ρ ̸= 0). Thus, we have
an ellipse.
(b) Let σ2
1 = σ2
2 = σ2 and ρ = 0. Then the equation of the curve becomes
µx1 −µ1
σ1
¶2
+
µx2 −µ2
σ2
¶2
−c = 0,
which is a circle with center (µ1, µ2) and radius σ√c.
7–44.
F(r)
=
P(R ≤r)
=
P
µq
X2
1 + X2
2 ≤r
¶
=
P(X2
1 + X2
2 ≤r2)
=
Z Z
A
1
2πσ2 exp
·
−(t2
1 + t2
2)
2σ2
¸
dt1 dt2,
where A = {(x1, x2) : x2
1 + x2
2 ≤r2}.

15
Let x1 = ρ cos(θ) and x2 = ρ sin(θ). Then
P(R ≤r)
=
Z r
0
Z 2π
0
ρ
2πσ2 exp(−ρ2/2σ2) dθ dρ
=
1 −exp(−r2/2σ2)
Thus,
f(r) = (r/σ2) exp(−r2/2σ2);
r > 0.
7–45. Using the fact that Pn
i=1 X2
i has a χ2
n distribution, we obtain
f(r) =
rn−1e−r2/2
2(n−2)/2Γ(n/2);
r ≥0
7–46. Let Y1 = X1/X2 and Y2 = X2, with X2 ̸= 0. Then the Jacobian is
|J| =
¯¯¯¯
y2
y1
0
1
¯¯¯¯ = |y2|
So we have
f(y1, y2) =
1
2πσ1σ2
p
1 −ρ2|y2| exp
½
−1
2(1 −ρ2)
·µy1y2
σ1
¶2
−2ρy1y2
2
σ1σ2
+
µy2
σ2
¶2¸¾
So the marginal is
fY1(y1)
=
Z ∞
−∞
f(y1, y2) dy2
=
p
1 −ρ2
πσ1σ2
·µy1
σ1
−ρ
σ2
¶2
+ 1 −ρ2
σ2
2
¸−1
;
−∞< y1 < ∞
When ρ = 0 and σ2
1 = σ2
2 = σ2, the distribution becomes
fY1(y1) =
1
π(1 + y2
1);
−∞< y1 < ∞,
also known as the Cauchy distribution.

16
7–47. The CDF is
FY (y)
=
P(Y ≤y) = P(Z2 ≤y)
=
P(−√y ≤Z ≤√y)
=
2
Z √y
0
1
2πe−z2/2 dz
Take z = √u so that dz = (2√u)−1 du. Then
FY (y) =
Z y
0
1
2πu(1/2)−1e−u/2 du
and so, by Leibniz’ rule,
f(y) =
1
2π y−1/2e−y/2,
y > 0.
7–48. The CDF is
FY (y)
=
P(Y ≤y) = P
µ n
X
i=1
X2
i ≤y
¶
=
Z Z
· · ·
Z
A
(2π)−n/2 exp
·
−1
2
n
X
i=1
x2
i
¸
dx1 dx2 · · · dxn,
where
A =
½
(x1, x2, . . . , xn) :
n
X
i=1
x2
i ≤y
¾
.
Transform to polar coordinates:
x1
=
y1/2 cos(θ1)
x2
=
y1/2 sin(θ1) cos(θ2)
x3
=
y1/2 sin(θ1) sin(θ2) cos(θ3)
...
xn−1
=
y1/2 sin(θ1) sin(θ2) · · · sin(θn−2) cos(θn−1)
xn
=
y1/2 sin(θ1) sin(θ2) · · · sin(θn−2) sin(θn−1)

17
The Jacobian of this transformation is
¯¯¯¯¯¯¯¯¯
∂x1
∂y
∂x1
∂θ1
· · ·
∂x1
∂θn−1
∂x2
∂y
∂x2
∂θ1
· · ·
∂x2
∂θn−1
...
...
...
...
∂xn
∂y
∂xn
∂θ1
· · ·
∂xn
∂θn−1
¯¯¯¯¯¯¯¯¯
or, after a little algebra,
¯¯¯¯¯¯¯¯¯¯¯
cos(θ1)
2√y
−√y sin(θ1)
0
· · ·
0
sin(θ1) cos(θ2)
2√y
√y cos(θ1) cos(θ2)
−√y cos(θ1) sin(θ2)
· · ·
0
...
...
...
...
...
sin(θ1)··· sin(θn−1)
2√y
√y cos(θ1) sin(θ2) · · · sin(θn−1)
· · ·
· · ·
√y sin(θ1) sin(θ2) · · · cos(θn−1)
¯¯¯¯¯¯¯¯¯¯¯
In other words, J = (1/2)y(n/2)−1|∆n|, where ∆n is an n × n matrix obtained by
taking out (2√y)−1 from the ﬁrst column and √y from the last n −1 columns.
Expanding this determinant with respect to the last column, we have
|∆n|
=
sin(θ1) sin(θ2) · · · sin(θn−2)|∆n−1|
=
sinn−2(θ1) sinn−3(θ2) · · · sin(θn−2)
This transformation gives variables whose limits are much easier. In the region
covered by A, we have 0 ≤θi ≤π for i = 1, 2, . . . , n −2, and 0 < θn−1 < 2π. Thus,
P
µ n
X
i=1
X2
i ≤y∗
¶
=
Z Z
· · ·
Z
A
1
(2π)n/2
1
2 y(n/2)−1e−y/2|∆n| dy dθn−1 · · · dθ1
=
1
2(2π)n/2
Z y∗
0
y(n/2)−1e−y/2 dy
Z 2π
0
dθn−1
Z π
0
sin(θn−2) dθn−2 · · ·
Z π
0
sinn−2(θ1) dθ1
=
K
Z y∗
0
y(n/2)−1e−y/2 dy ≡F(y∗)
Thus,
f(y) = F ′(y) = Ky(n/2)−1e−y/2,
y ≥0.

18
To evaluate K, use K
R ∞
0 f(y) dy = 1. This ﬁnally gives
f(y) =
1
2n/2Γ(n/2)y(n/2)−1e−y/2,
y ≥0.
7–49. For x ≥0,
F(x)
=
P(|X| ≤x) = P(−x ≤X ≤x)
=
Z x
−x
1
√
2π e−t2/2 dt
=
2
Z x
0
1
√
2π e−t2/2 dt,
so that
f(x)
=
2
√
2π e−x2/2,
x > 0
=
0,
otherwise

1
Chapter 8
8–1. ¯x = 131.30, s2 = 113.85, s = 10.67.
8–2. Descriptive Statistics for Y
Mean
=
34.767
Variance
=
1.828
Standard Dev
=
1.352
Skewness
=
0.420
Kurtosis
=
2.765
Minimum
=
32.100
Maximum
=
37.900
n = 64
Lower Limit
Cell Count
32.1000
1
X
32.4625
4
XXXX
32.8250
3
XXX
33.1875
2
XX
33.5500
7
XXXXXXX
33.9125
6
XXXXXX
34.2750
9
XXXXXXXXX
34.6375
7
XXXXXXX
35.0000
7
XXXXXXX
35.3625
5
XXXXX
35.7250
2
XX
36.0875
2
XX
36.4500
4
XXXX
36.8125
1
X
37.1750
1
X
37.5375
3
XXX

2
8–3. Descriptive Statistics for X
Mean
=
89.476
Variance
=
17.287
Standard Dev
=
4.158
Skewness
=
0.251
Kurtosis
=
1.988
Minimum
=
82.600
Maximum
=
98.000
n = 90
Lower Limit
Cell Count
82.60
4
XXXX
83.37
6
XXXXXX
84.14
4
XXXX
84.91
6
XXXXXX
85.68
7
XXXXXXX
86.45
3
XXX
87.22
8
XXXXXXXX
87.99
4
XXXX
88.76
4
XXXX
89.53
7
XXXXXXX
90.30
6
XXXXXX
91.07
4
XXXX
91.84
3
XXX
92.61
4
XXXX
93.38
4
XXXX
94.15
5
XXXXX
94.92
4
XXXX
95.69
3
XXX
96.46
1
X
97.23
3
XXX

3
8–4.
Number of Defects
Frequency
Relative Freq
1
1
0.0067
2
14
0.0933
3
11
0.0733
4
21
0.1400
5
10
0.0667
6
18
0.1200
7
15
0.1000
8
14
0.0933
9
9
0.0600
10
15
0.1000
11
4
0.0267
12
4
0.0267
13
6
0.0400
14
5
0.0333
15
1
0.0067
16
1
0.0067
17
1
0.0067
150
1.0000
¯x = 6.9334, s2 = 12.5056, R = 16, ˜x = 6.5, MO = 4. The data appear to follow a
Poisson distribution, though s2 seems to be somewhat greater than ¯x.
8–5. ¯x = 131.30, s2 = 113.85, s = 10.67.
8–6.
Class Interval
Frequency
Relative Freq
32 ≤X < 33
6
0.094
33 ≤X < 34
11
0.172
34 ≤X < 35
22
0.344
35 ≤X < 36
14
0.219
36 ≤X < 37
6
0.094
37 ≤X < 38
5
0.077
64
1.000
¯x = 34.7672, s2 = 1.828, ˜x = (34.6 + 34.7)/2 = 34.65. The data appear to follow a
normal distribution.

4
8–7.
Class Interval
Frequency
Relative Freq
82 ≤X < 84
6
0.067
84 ≤X < 86
14
0.156
86 ≤X < 88
18
0.200
88 ≤X < 90
11
0.122
90 ≤X < 92
14
0.156
92 ≤X < 94
8
0.088
94 ≤X < 96
12
0.133
96 ≤X < 98
6
0.067
98 ≤X < 100
1
0.011
¯x = 89.4755, s2 = 17.2870. The data appear to follow a either a gamma or a
Weibull distribution.
8–8.
(a) Descriptive Statistics for Time
Mean
=
14.355
Variance
=
356.577
Standard Dev
=
18.883
Skewness
=
1.809
Kurtosis
=
5.785
Minimum
=
0.190
Maximum
=
72.890
n = 19
Lower Limit
Cell Count
0.1900
13
XXXXXXXXXXXXX
10.5757
1
X
20.9614
0
31.3471
4
XXXX
41.7329
0
52.1186
0
62.5043
1
X
(b) ¯x = 14.355, s2 = 356.577, s = 18.88, ˜x = 6.5.
8–9. ¯x = 126.875, s2 = 660.12, s = 25.693

5
8–10.(a,b)
83
4
84
3
85
3
86
7 7
87
7 5 8 6 9 4
88
5 6 3 2 3 5 3 6 7 4 9
89
8 2 0 9 8 6 3 8 3 7
90
8 3 1 9 4 1 4 6 4 3 5 0 7
91
5 1 0 0 8 2 8 6 1 1 6 2 0
92
7 3 7 6 7 2 2 2
93
3 2 4 3 0 7
94
7 2 2 4
95
6
96
1
97
98
8
99
100
3
(c) ¯x = 90.6425, s2 = 7.837, s = 2.799
(d) ˜x = median = 90.45. There are several modes, e.g., 91.0, 919.1, 92.7.

6
8–12.
(a)
Frequency
32
5 6 9 8 1 7
6
33
1 6 6 8 4 6 8 1 6 5 6
11
34
2 5 3 7 7 2 7 6 9 7 1 6 0 1 6 7 6 5 6 1 7 3
22
35
6 1 0 4 1 3 2 0 1 4 9 8 5 7
14
36
2 8 8 4 6 8
6
37
9 8 1 6 3
5
(b) ¯x = 34.7672, s2 = 1.828
(c)
Frequency
32
1 5 6 7 8 9
6
33
1 1 4 5 6 6 6 6 6 8 8
11
34
0 1 1 1 2 2 3 3 5 5 6 6 6 6 6 7 7 7 7 7 7 9
22
35
0 0 1 1 1 2 3 4 4 5 6 7 8 9
14
36
2 4 6 8 8 8
6
37
1 3 6 8 9
5
(d) ˜x = 34.65
8–13.
(a)
Frequency
82
6 9
2
83
0 1 6 7
4
84
0 1 1 1 2 5 6 9
8
85
0 1 1 1 4 4
6
86
1 1 1 4 4 4 4 6 7 7
10
87
3 3 3 3 5 6 6 7
8
88
2 2 3 6 8
5
89
1 1 4 6 6 7
6
90
0 0 1 1 3 4 5 6 6 6
10
91
1 2 4 7
4
92
1 4 4
3
93
1 1 2 2 7
5
94
1 1 1 3 3 4 6 7
8
95
1 2 3 6
4
96
1 3 4 8
4
97
3 8
2
98
0
1

7
(b) ¯x = 89.25, Q1 = 86.1, Q3 = 93.1.
(c) IQR = Q3 −Q1 = 7.0
8–14. min = 82.6, Q1 = 86.1, ¯x = 89.25, Q3 = 93.1, max = 98.1
8–15. min = 32.1, Q1 = 33.8, ¯x = 34.65, Q3 = 35.45, max = 37.9
8–16. min = 1, Q1 = 4, ¯x = 7, Q3 = 10, max = 17
8–18. The descriptive measures developed in this chapter are for numerical data only.
The mode, however, does have some meaning. For these data, the mode is the
letter e.
8–19.
(a)
n
X
i=1
(Xi −¯X)
=
n
X
i=1
Xi −
n
X
i=1
¯X =
n
X
i=1
Xi −n ¯X
=
n
X
i=1
Xi −
n
X
i=1
Xi = 0
(b)
n
X
i=1
(Xi −¯X)2
=
n
X
i=1
(X2
i + ¯X2 −2Xi ¯X)
=
n
X
i=1
X2
i + n ¯X2 −2 ¯X
n
X
i=1
Xi
=
n
X
i=1
X2
i + n ¯X2 −2n ¯X2
=
n
X
i=1
X2
i −n ¯X2
8–20. ¯x = 1.1933, s2 = 0.000266, s = 0.016329, ˜x = median = (1.19 + 1.20)/2 = 1.195,
mode = 1.21.
8–21. ¯x = 74.0020, s = 0.0026, s2 = 6.875 × 10−6

8
8–22. ¯x = 62.75, s = 2.12, s2 = 4.5
8–23.
(a) Sample average will be reduced by 63.
(b) Sample mean and standard deviation will be 100 units larger; the sample
variance will be 10000 units larger.
8–24. ¯y = a + b¯x, sy = bsx
8–25. a = ¯x
8–26.
(a) 89.336
(b) 89.237
8–27. There is no guarantee that LN is an integer. For example, if we want a 10% trimmed
mean with 23 observations, then we would have to trim 2.3 observations from each
end. Since we cannot do this, some other procedure must be used. A reasonable
alternative is to calculate the trimmed mean with two observations trimmed from
each end, then to repeat this procedure with three observations trimmed from each
end, and ﬁnally to interpolate between the two diﬀerent values of the trimmed
mean.
8–29.
(a) ¯x = 120.22481, s2 = 5.66001
(b) median = 120, mode = 121
8–30.
(a) ¯x = −0.20472, s2 = 3.96119
(b) median = mode = 0
8–31. For 8–29, cv = 2.379/120.225 = 0.01979.
For 8–30, cv = 1.990/(−0.205) = −9.722.
8–32. ¯x ≈51.124, s2 ≈586.603, ˜x ≈48.208, mode ≈36.334
8–33. ¯x ≈22.407, s2 ≈208.246, ˜x ≈22.813, mode ≈23.64
8–34. ¯x ≈847.885, s2 ≈15987.81, s ≈126.44, ˜x ≈858.98, mode ≈1050

1
Chapter 9
9–1. Since
f(xi) =
1
σ
√
2π exp
·
−(xi −µ)2
2σ2
¸
,
we have
f(x1, x2, . . . , x5)
=
5
Y
i=1
f(xi)
=
5
Y
i=1
1
σ
√
2π exp
·
−(xi −µ)2
2σ2
¸
=
µ
1
σ22π
¶5/2
exp
·
−1
2σ2
5
X
i=1
(xi −µ)2
¸
9–2. Since
f(xi) = λe−λxi,
we have
f(x1, x2, . . . , xn)
=
n
Y
i=1
f(xi)
=
n
Y
i=1
λe−λxi
=
λn exp
·
−λ
n
X
i=1
xi
¸
9–3. Since f(xi) = 1, we have
f(x1, x2, x3, x4) =
4
Y
i=1
f(xi) = 1

2
9–4. The joint probability function for X1 and X2 is
pX1,X2(0, 0)
=
µN −M
0
¶ µM
2
¶
µN
2
¶
pX1,X2(0, 1)
=
µN −M
1
¶ µM
1
¶
2
µN
2
¶
pX1,X2(1, 0)
=
µN −M
1
¶ µM
1
¶
2
µN
2
¶
pX1,X2(1, 1)
=
µN −M
2
¶ µM
0
¶
µN
2
¶
Of course,
pX1(x1)
=
1
X
x2=0
pX1,X2(x1, x2)
and
pX2(x2)
=
1
X
x1=0
pX1,X2(x1, x2)
So pX1(0) = M/N, pX1(1) = 1 −(M/N), pX2(0) = M/N, pX2(1) = 1 −(M/N).
Thus, X1 and X2 are not independent since
pX1,X2(0, 0) ̸= pX1(0)pX2(0)
9–5. N(µ, σ2/n) = N(5, 0.00125)
9–6. σ/√n = 0.1/
√
8 = 0.0353

3
9–7. Use estimated standard error S/√n.
9–8. N(−5, 0.22)
9–9. The standard error of ¯X1 −¯X2 is
s
σ2
1
n1
+ σ2
2
n2
=
r
(1.5)2
25
+ (2.0)2
30
= 0.473
9–10. Y = ¯X1 −¯X2 is a linear combination of the 55 variables Xij, i = 1, j = 1, 2, . . . , 25,
i = 2, j = 1, 2, . . . , 30. As such, we would expect Y to be very nearly normal with
mean µY = −0.5 and variance (0.473)2 = 0.223.
9–11. N(0, 1)
9–12. N(ˆp, ˆp(1 −ˆp)/n)
9–13. se(ˆp) =
p
p(1 −p)/n, bse(ˆp) =
p
ˆp(1 −ˆp)/n,
9–14.
MX(t)
=
E(etX) =
Z ∞
0
etx
1
2n/2Γ(n/2) x(n/2)−1 e−x/2 dx
=
1
2n/2Γ(n/2)
Z ∞
0
x(n/2)−1 e−x[(1/2)−t] dx
This integral converges if 1/2 > t.
Let u = x[(1/2) −t]. Then dx = [(1/2) −t]−1du. Thus,
MX(t)
=
1
2n/2Γ(n/2)
Z ∞
0
u(n/2)−1
[(1/2) −t](n/2)−1 e−u
1
[(1/2) −t] du
=
1
2n/2Γ(n/2)[(1 −2t)/2]n/2
Z ∞
0
u(n/2)−1 e−u du
=
1
(1 −2t)n/2,
t < 1/2,
since Γ(n/2) =
Z ∞
0
u(n/2)−1 e−u du.

4
9–15. First of all,
M ′
X(t)
=
n(1 −2t)−(n/2)−1
M ′′
X(t)
=
n(n + 2)(1 −2t)−(n/2)−2
Then
E(X)
=
M ′
X(0) = n
E(X2)
=
M ′′
X(0) = n(n + 2)
V (X)
=
E(X2) −[E(X)]2 = 2n
9–16. Let T = Z/
p
χ2
n/n = Z
p
n/χ2
n. Now
E(T) = E(Z)E(
p
n/χ2
n ) = 0, because E(Z) = 0.
V (T) = E(T 2), because E(T) = 0. Thus,
V (T) = E[Z2(n/χ2
n)] = E(Z2)E(n/χ2
n).
Note that E(Z2) = V (Z) = 1, so that
V (T)
=
E(n/χ2
n)
=
Z ∞
0
(n/s)
2n/2Γ(n/2)s(n/2)−1 e−s/2 ds
=
n
2n/2Γ(n/2)
Z ∞
0
s(n/2)−2 e−s/2 ds
=
n
2(n/2)−1Γ(n/2)
Z ∞
0
(2u)(n/2)−2 e−u du
=
nΓ(n
2 −1)
2Γ(n
2)
,
if n > 2
=
nΓ(n
2 −1)
2(n
2 −1)Γ(n
2 −1),
if n > 2
=
n
n −2,
if n > 2

5
9–17. E(Fm,n) = E[(χ2
m/m)/(χ2
n/n)] = E(χ2
m/m)E(n/χ2
n).
E(χ2
m/m) = (1/m)E(χ2
m) = 1.
From Problem 9–16, we have E(n/χ2
n) = n/(n −2).
Therefore, E(Fm,n) = n/(n −2), if n > 2.
To ﬁnd V (Fm,n), let X ∼χ2
m and Y ∼χ2
n. Then
E(F 2
m,n) = (n/m)2E(X2)E(1/Y 2).
Since E(X2) = V (X) + [E(X)]2 and X ∼χ2
m, we have E(X2) = 2m + m2. Now
E(1/Y 2)
=
Z ∞
0
(1/y2)
2n/2Γ(n/2)y(n/2)−1 e−y/2 dy
=
1
2n/2Γ(n/2)
Z ∞
0
y(n/2)−3 e−y/2 dy
=
1
2n/2Γ(n/2)
Z ∞
0
2(2u)(n/2)−3 e−u du
=
1
(n −2)(n −4),
if n > 4
Thus,
V (Fm,n)
=
(n/m)2E(X2)E(1/Y 2) −
µ
n
n −2
¶2
=
n2(2m + m2)
m2(n −2)(n −4) −
n2
(n −2)2 =
2n2(m + n −2)
m(n −2)2(n −4)
9–18. X(1) is greater than t if and only if every observation is greater than t. Then
P(X(1) > t)
=
P(X1 > t, X2 > t, . . . , Xn > t)
=
P(X1 > t)P(X2 > t) · · · P(Xn > t)
=
P(X > t)P(X > t) · · · P(X > t)
=
[1 −F(t)]n

6
So FX(1)(t) = 1 −P(X(1) > t) = 1 −[1 −F(t)]n.
If X is continuous, then so is X(1); so
fX(1)(t) = F ′
X(1)(t) = n[1 −F(t)]n−1f(t)
Similarly,
FX(n)(t)
=
P(X(n) ≤t)
=
P(X1 ≤t, X2 ≤t, . . . , Xn ≤t)
=
P(X1 ≤t)P(X2 ≤t) · · · P(Xn ≤t)
=
P(X ≤t)P(X ≤t) · · · P(X ≤t)
=
[F(t)]n
Since X(n) is continuous,
fX(n)(t) = F ′
X(n)(t) = n[F(t)]n−1f(t)
9–19.
F(t) =



0
t < 0
1 −p
0 ≤t < 1
1
t ≥1
Then
P(X(n) = 1)
=
FX(n)(1) −FX(n)(0) = [F(1)]n −[F(0)]n = 1 −(1 −p)n
P(X(1) = 0)
=
1 −[1 −F(0)]n = 1 −[1 −(1 −p)]n = 1 −pn
9–20.
fX(1)(t) = n
·
1 −Φ
µt −µ
σ
¶¸n−1
1
σ
√
2π exp
·
−(x −µ)2
2σ2
¸
fX(n)(t) = n
·
Φ
µt −µ
σ
¶¸n−1
1
σ
√
2π exp
·
−(x −µ)2
2σ2
¸

7
9–21. f(t) = λe−λt,
t > 0
F(t) = 1 −e−λt
FX(1)(t) = 1 −[1 −F(t)]n = 1 −[1 −(1 −e−λt)]n = 1 −e−nλt
fX(1)(t) = nλe−nλt,
t > 0
FX(n)(t) = [F(t)]n = (1 −e−λt)n
fX(n)(t) = n(1 −e−λt)n−1λe−λt,
t > 0
9–22. fX(n)(X(n)) = n[F(X(n))]n−1f(X(n))
Treat F(X(n)) as a random variable giving the fraction of objects in the population
having values of X ≤X(n).
Let Y = F(X(n)). Then dy = f(X(n))dx(n), and thus f(y) = nyn−1, 0 ≤y ≤1.
This gives
E(Y ) =
Z 1
0
nyn dy =
n
n + 1.
Similarly, fX(1)(X(1)) = n[1 −F(X(1))]n−1f(X(1))
Treat F(X(1)) as a random variable giving the fraction of objects in the population
having values of X ≤X(1).
Let Y = F(X(1)). Then dy = f(X(1))dx(1), and thus f(y) = n(1−y)n−1, 0 ≤y ≤1.
This gives
E(Y ) =
Z 1
0
ny(1 −y)n−1 dy

8
The family of Beta distributions is deﬁned by p.d.f.’s of the form
g(x) =
½ [β(r, s)]−1xr−1(1 −x)s−1
0 < x < 1
0
otherwise
where β(r, s) = Γ(r)Γ(s)/Γ(r + s).
Thus,
E(Y )
=
n
Z 1
0
y(1 −y)n−1 dy = nβ(2, n)
=
nΓ(2)Γ(n)
Γ(n + 2)
=
n!1!
(n + 1)! =
1
n + 1
9–23.
(a) 2.73
(b) 11.34
(c) 34.17
(d) 20.48
9–24.
(a) 2.228
(b) 0.687
(c) 1.813
9–25.
(a) 1.63
(b) 2.85
(c) 0.241
(d) 0.588

1
Chapter 10
10–1. Both estimators are unbiased. Now, V (X1) = σ2/2n while V (X2) = σ2/n. Since
V (X1) < V (X2), X1 is a more eﬃcient estimator than X2.
10–2. E(ˆθ1) = µ, E(ˆθ2) = (1/2)E(2X1 −X6 + X4) = (1/2)(2µ −µ + µ) = µ. Both
estimators are unbiased.
V (ˆθ1) = σ2/7,
V (ˆθ2) =
µ1
2
¶2
V (2X1 −X6 + X4)
=
µ1
4
¶
[4V (X1) + V (X6) + V (X4)] =
µ1
4
¶
6σ2 = 3σ2/2
ˆθ1 has a smaller variance than ˆθ2.
10–3. Since ˆθ1 is unbiased, MSE(ˆθ1) = V (ˆθ1) = 10.
MSE(ˆθ2) = V (ˆθ2) + (Bias)2 = 4 + (θ −θ/2)2 = 4 + θ2/4.
If θ <
√
24 = 4.8990, ˆθ2 is a better estimator of θ than ˆθ1, because it would have
smaller MSE.
10–4. MSE(ˆθ1) = V (ˆθ1) = 12, MSE(ˆθ2) = V (ˆθ2) = 10,
MSE(ˆθ3) = E(ˆθ3 −θ)2 = 6. ˆθ3 is a better estimator because it has smaller MSE.
10–5. E(S2) = (1/24)E(10S2
1 + 8S2
2 + 6S2
3) = (1/24)(10σ2 + 8σ2 + 6σ2)
= (1/24)24σ2 = σ2
10–6. Any linear estimator of µ is of the form ˆθ = Pn
i=1 aiXi where ai are constants. ˆθ is
an unbiased estimator of µ only if E(ˆθ) = µ, which implies that Pn
i=1 ai = 1. Now
V (ˆθ) = Pn
i=1 a2
i σ2. Thus we must choose the ai to minimize V (ˆθ) subject to the
constraint P ai = 1. Let λ be a Lagrange multiplier. Then
F(ai, λ) =
n
X
i=1
a2
i σ2 −λ
Ã n
X
i=1
ai −1
!
and ∂F/∂ai = ∂F/∂λ = 0 gives
2aiσ2 −λ = 0; i = 1, 2, . . . , a
n
X
i=1
ai = 1
The solution is ai = 1/n. Thus ˆθ = X is the best linear unbiased estimator of µ.

2
10–7.
L(α) =
n
Y
i=1
αXie−α/Xi! = αΣXie−nα
, n
Y
i=1
Xi!
ℓn L(α) =
n
X
i=1
Xi ℓn α −nα −ℓn
Ã n
Y
i=1
Xi!
!
d ℓn L(α)
dα
=
n
X
i=1
Xi/α −n = 0
ˆα =
n
X
i=1
Xi/n = X
10–8. For the Poisson distribution, E(X) = α = µ′
1. Also, M ′
1 = X. Thus ˆα = X is the
moment estimator of α.
10–9.
L(λ) =
n
Y
i=1
λe−λti = λne−λ Pn
i=1 ti
ℓn L(λ) = n ℓn λ −λ
n
X
i=1
ti
d ℓn L(λ)
dλ
= (n/λ) −
n
X
i=1
ti = 0
ˆλ = n
, n
X
i=1
ti = (t)−1
10–10. E(t) = 1/λ = µ′
1, and M ′
1 = t. Thus 1/λ = t or ˆλ = (t)−1.
10–11. If X is a gamma random variable, then E(X) = r/λ and V (X) = r/λ2. Thus
E(X2) = (r + r2)λ2. Now M ′
1 = X and M ′
2 = (1/n) Pn
i=1 X2
i . Equating moments,
we obtain
r/λ = X,
(r + r2)λ2 = (1/n)
n
X
i=1
X2
i
or,
ˆλ = X
,"
(1/n)
n
X
i=1
X2
i −X
2
#
ˆr = X
2
,"
(1/n)
n
X
i=1
X2
i −X
2
#

3
10–12. E(X) = 1/p, M ′
1 = X. Thus 1/p = X or ˆp = 1/X.
10–13. L(p) =
n
Y
i=1
(1 −p)Xi−1p = pn(1 −p)ΣXi−n
ℓn L(p) = n ℓn p + (Pn
i=1 Xi −n)ℓn (1 −p). From d ℓn L(p)/dρ = 0, we obtain
(n/ˆp) −
Ã n
X
i=1
Xi −n
!,
(1 −ˆρ) = 0
ˆp = n
, n
X
i=1
Xi = 1/X
10–14. E(X) = p, M ′
1 = X. Thus ˆp = X.
10–15. E(X) = np (n is known), M ′
1 = XN (X is based on a sample of N observations.)
Thus np = XN or ˆp = XN/n.
10–16. E(X) = np, V (X) = np(1 −p), E(X2) = np −np2 + n2p2
M ′
1 = X, M ′
2 = (1/N) PN
i=1 X2
i . Equating moments,
np = X, np −np2 + n2p2 = (1/N)
N
X
i=1
X2
i
ˆn = X
2
,"
X −(1/N)
N
X
i=1
(Xi −X)2
#
, ˆp = X/ ˆN
10–17.
L(p) =
N
Y
i=1
µ n
Xi
¶
pXi(1 −p)n−Xi =
" N
Y
i=1
µ n
Xi
¶#
p
Pn
i=1 Xi(1 −p)nN−PN
i=1 Xi
ℓn L(p) =
N
X
i=1
ℓn
µ n
Xi
¶
+
Ã N
X
i=1
Xi
!
ℓn p +
Ã
nN −
N
X
i=1
Xi
!
ℓn(1 −p)
d ℓn L(p)
dp
=
N
X
i=1
Xi
,
p −
Ã
nN −
N
X
i=1
Xi
!,
(1 −p) = 0
ˆp = X/n

4
10–18. L =
n
Y
i=1
µβ
δ
¶ µXi −γ
δ
¶βτ
exp
"
−
µXi −γ
δ
¶β#
The system of partial derivatives ∂L/∂δ = ∂L/∂β = ∂L/∂γ = 0 yield simultane-
ous nonlinear equations that must be solved to produce the maximum likelihood
estimators. In general, iterative methods must be used to ﬁnd the maximum like-
lihood estimates. A number of special cases are of practical interest; for example,
if γ = 0, the two-parameter Weibull distribution results. Both iterative and linear
estimation techniques can be used for the two-parameter case.
10–19. Let X be a random variable and c be a constant. Then Chebychev’s inequality is
P(|X −c| ≥ϵ) ≤(1/ϵ2)E(X −c)2
Thus,
P(|ˆθ −θ| ≥ϵ) ≤(1/ϵ2)E(ˆθ −θ)2
Now E(ˆθ −θ)2 = V (ˆθ) + [E(ˆθ) −θ]2.
Then
P(|ˆθ −θ| ≥ϵ) ≤(1/ϵ2){V (ˆθ) + [E(ˆθ) −θ]2}
If ˆθ is unbiased then E(ˆθ) −θ = 0 and if limn→∞V (ˆθ) = 0 we see that limn→∞
P(|ˆθ −θ| ≥ϵ) ≤0, or limn→∞P(|ˆθ −θ| ≥ϵ) = 0.
10–20.
E(X) = E[aX1 + (1 −a)X2] = aE(X1) + (1 −a)E(X2)
= aµ + (1 −a)µ = µ
V (X) = a2V (X1) + (1 −a)2V (X2) = a2(σ2/n1) + (1 −a)2(σ2/n2)
dV (X)
da
= 2a
µσ2
n1
¶
−2(1 −a)
µσ2
n2
¶
= 0
a∗=
σ2/n2
σ2/n1 + σ2/n2
=
n1
n1 + n2
10–21.
L(γ) =
n
Y
i=1
(γ + 1)Xγ
i = (γ + 1)n
n
Y
i=1
Xγ
i
ℓn L(γ) = n ℓn(γ + 1) +
n
X
i=1
γ ℓn Xi

5
d ℓn L(γ)
dγ
= n
,
(γ + 1) +
n
X
i=1
ℓn Xi = 0
ˆγ = −1 −
Ã
n
, n
X
i=1
ℓn Xi
!
10–22.
L(γ) =
n
Y
i=1
λe−λ(Xi−Xℓ) = λne−λ(Pn
i=1 Xi−nXℓ)
ℓn L(λ) = n ℓn λ −λ
Ã n
X
i=1
Xi −nXℓ
!
d ℓn L(λ)
dλ
= n
,
λ −
Ã n
X
i=1
Xi −nXℓ
!
= 0
10–23. Assume Xℓunknown, and we want to maximize n ℓn λ −λ Pn
i=1(Xi −Xℓ) with
respect to Xℓ, subject to Xi ≥Xℓ. Thus we want Pn
i=1(Xi −Xℓ) to be a minimum,
subject to Xi ≥Xℓ. Thus ˆXℓ= min(X1, X2, . . . , Xn) = X(1).
10–24. E(G) = E
"
K
n−1
X
i=1
(Xi+1 −Xi)2
#
= K
"n−1
X
i=1
E(Xi+1 −Xi)2
#
= K
"n−1
X
i=1
E
¡
X2
i+1 −2XiXi+1 + X2
i
¢
#
= K
n−1
X
i=1
[E(X2
i+1) −2E(XiXi+1) + E(X2
i )]
= K[(n −1)(σ2 + µ2) −2(n −1)µ2 + (n −1)(µ2 + σ2)]
= K[2(n −1)σ2]
For K[2(n −1)σ2] to equal σ2, K = [2(n −1)]−1. Thus
G =
1
2(n −1)
n−1
X
i=1
(Xi+1 −Xi)2
is an unbiased estimator of σ2.

6
10–25. f(x1, x2, . . . , xn|µ) = (2πσ2)−n/2 exp
µ
−1
2Σ(xi −µ)2/σ2
¶
,
f(µ) = (2πσ2
0)−1/2 exp
µ
−1
2(x −µ0)2/σ2
0
¶
f(µ|x1, x2, . . . , xn) = c−1/2(2π)−1/2 exp
(
−c
2
·
µ −1
c
µnx
σ2 + µ0
σ2
0
¶¸2)
where c = n
σ2 + 1
σ2
0
10–26. f(x1, x2, . . . , xn|1/σ2) = (2πσ2)−n/2 exp
µ
−1
2Σ(xi −µ)2/σ2
¶
f(1/σ2) =
1
Γ(m + 1)(mσ2
0)m+1(1/σ2)me−mσ2
0/σ2
The posterior density for 1/σ2 is gamma with parameters m+(n/2)+1 and mσ2
0 +
Σ(xi −µ)2.
10–27. f(x1, x2, . . . , xn|p0) = pn(1 −p)Σxi−n,
f(p) = Γ(a + b)
Γ(a)Γ(b)pa−1(1 −p)b−1
The posterior density for p is a beta distribution with parameters a + n and b +
Σxi −n.
10–28. f(x1, x2, . . . , xn|p) = pΣxi(1 −p)n−Σxi,
f(p) = Γ(a + b)
Γ(a)Γ(b)pa−1(1 −p)b−1,
The posterior density for p is a beta distribution with parameters a + Σxi and
b + n −Σxi
10–29. f(x1, x2, . . . , xn|λ) = λΣxie−nλ/Πxi!
f(λ) =
1
Γ(m + 1)
µm + 1
λ0
¶m+1
λme−(m+1)λ/λ0
The posterior density for λ is gamma with parameters r = m + Σxi + 1 and
δ = n + (m + 1)/λ0.

7
10–30. From Exercise 10–25 and using the relationship that the Bayes’ estimator for µ
using a squared-error loss function is given by ˆµ = 1
c[ nx
σ2 + µ0
σ2
0 ], we have
ˆµ −
·25
40 + 1
8
¸−1 ·25(4.85)
40
+ 4
8
¸
= 4.708
10–31. ˆµ = 1.05 −
0.1
2
Φ
µ1.20 −1.05
0.1
2
¶
−Φ
µ0.98 −1.05
0.1
2
¶(2π)−1/2
×
h
e−1/2( 1.20−1.05
0.1/2
) −e−1/2( 0.98−1.05
0.1/2
)i
= 1.05 −(0.0545)(0.399)(0.223 −4.055) = 0.967
10–32. Σχi = 6270, ˆλ = 0.000323
10–33. ˆµ =
· 25
0.1 +
1
0.04
¸−1 ·25(10.05)
0.1
+ 10
0.04
¸
= 10.045
weight ∼N(10.045, 0.1)
P(weight < 9.95) = P
µ
Z < 9.95 −10.045
0.316
¶
= Φ(−0.301) ≈0.3783
10–34. From a previous exercise, the posterior is gamma with parameters r = Σxi + 1 and
δ = n + (1/λ0). Since n = 10 and Σxi = 45,
f(λ|x1, . . . , x10) =
1
Γ(46)(14)46λ45e−46λ
The Bayes interval requires us to ﬁnd L and U so that
(14)46
Γ(46)
Z U
L
λ45e−46λ dλ = 0.95
Since r is integer, tables of the Poisson distribution could be used to ﬁnd L and U.
10–35.
(a) f(x1|θ) = 2x
θ2 ,
f(θ) = 1, 0 < θ < 1,
f(x1, θ) = 2x
θ2 , and
f(x1) =
Z 1
x
2x
θ2 dθ = −2x1
θ
Z 1
x
= 2 −2x; 0 < x < 1

8
The posterior density is
f(θ|x1) = f(x1, θ)
f(x1)
=
2x
θ2(2 −2x)
(b) The estimator must minimize
Z =
Z
ℓ(ˆθ; θ)f(θ|x1) dθ =
Z 1
0
θ2(ˆθ −θ)2
2x
θ2(2 −2x)dθ
=
2x
2 −2x
·
ˆθ2 −ˆθ + 1
3
¸
From dZ
dˆθ = 0 we get ˆθ = 1
2
10–36. f(x1|p) = px(1 −p)1−x, f(x1, p) = 6px+1(1 −p)2−x, and
f(x1) =
Z 1
0
6px+1(1 −p)2−x dp = Γ(x + 2)Γ(3 −x)
4
The posterior density for p is
f(p|x1) = 24px+1(1 −p)2−x
Γ(x + 2)Γ(3 −x)
For a squared-error loss, the Bayes estimator is
ˆp = E(p|x1) =
Z 1
0
24px+2(1 −p)2−x
Γ(x + 2)Γ(3 −x) = Γ(x + 3)
5Γ(x + 2) = x + 3
5
If ℓ(ˆp; p) = 2(ˆp −p)2, the Bayes estimator must minimize
Z =
24
Γ(x + 2)Γ(3 −x)
Z 1
0
2(ˆp −p)2px+1(1 −p)2−x dp
=
2
Γ(x + 2)
·
ˆp2Γ(x + 2) −2ˆpΓ(x + 3)
5
+ Γ(x + 4)
30
¸
From dz/dˆp = 0, ˆp = x+2
5
10–37. For α1 = α2 = α/2, α = 0.05; x ± 1.96( σ
√n)
For α1 = 0.01, α2 = 0.04; x −1.751( σ
√n) ≤µ ≤x + 2.323( σ
√n)
α1 = α2 = α/2 is shorter.

9
10–38.
(a) N(0, 1)
(b) X −Zα/2


s
X
n

≤λ ≤X + Zα/2


s
X
n


10–39.
(a) x −Zα/2
σ
√n ≤µ ≤x + Zα/2
σ
√n
74.03533 ≤µ ≤74.03666
(b) x −Zα
σ
√n ≤µ
74.0356 ≤µ
10–40.
(a) 1003.04 ≤µ ≤1024.96
(b) 1004.80 ≤µ
10–41.
(a) 3232.11 ≤µ ≤3267.89
(b) 3226.49 ≤µ ≤3273.51
The width of the conﬁdence interval in (a) is 35.78, and the width of the interval
in (b) is 47.01. The wider conﬁdence interval in (b) reﬂects the higher conﬁdence
coeﬃcient.
10–42. n = (Zα/2σ/E)2 = [(1.96)25/5]2 = 96.04 ≃97
10–43. For the total width to be 8, the half-width must be 4, therefore n = (Zα/2σ/E)2 =
[(1.96)25/4]2 = 150.06 ≃150 or 151.
10–44. n = (Zα/2σ/E)2 = 1000(1.96/15)2 = 17.07 ≃18.
10–45.
(a) 0.0723 ≤µ1 −µ2 ≤0.3076
(b) 0.0499 ≤µ1 −µ2 ≤0.33
(c) µ1 −µ2 ≤0.3076
10–46. 3.553 ≤µ1 −µ2 ≤8.447
10–47. −3.68 ≤µ1 −µ2 ≤−2.12
10–48.
(a) 2238.6 ≤µ ≤2275.4
(b) 2242.63 ≤µ
(c) 2240.11 ≤µ ≤2275.39

10
10–49. 183.0 ≤µ ≤256.6
10–50. 4.05 −t0.10,24(0.08/
√
25) ≤µ ⇒4.029 ≤µ
10–51. 13
10–52.
(a) 546.12 ≤µ ≤553.88
(b) 546.82 ≤m
(c) µ ≤553.18
10–53. 94.282 ≤µ ≤111.518
10–54.
(a) 7.65 ≤µ1 −µ2 ≤12.346
(b) 8.03 ≤µ1 −µ2
(c) µ1 −µ2 ≤11.97
10–55. −0.839 ≤µ1 −µ2 ≤−0.679
10–56.
(a) −0.561 ≤µ1 −µ2 ≤1.561
(b) µ1 −µ2 ≤1.384
(c) −0.384 ≤µ1 −µ2
10–57. 0.355 ≤µ1 −µ2 ≤0.455
10–58. −30.24 ≤µ1 −µ2 ≤−19.76
10–59. From 10–48, s = 34.51, n = 16, and (n −1)s2 = 17864.1
(a) 649.60 ≤σ2 ≤2853.69
(b) 714.56 ≤σ2
(c) σ2 ≤2460.62
10–60.
(a) 1606.18 ≤σ2 ≤26322.15
(b) 1755.68 ≤σ2
(c) σ2 ≤21376.78
10–61. 0.0039 ≤σ2 ≤0.0157
10–62. σ2 ≤193.09
10–63. 0.574 ≤σ2
1/σ2
2 ≤3.614

11
10–64. s2
1 = 0.29, s2
2 = 0.34, s2
1/s2
2 = 1.208, n1 = 12, n2 = 18
(a) 0.502 ≤σ2
1/σ2
2 ≤2.924
(b) 0.423 ≤σ2
1/σ2
2 ≤3.468
(c) 0.613 ≤σ2
1/σ2
2
(d) σ2
1/σ2
2 ≤2.598
10–65. 0.11 ≤σ2
1/σ2
2 ≤0.86
10–66. 0.089818 ≤p ≤0.155939
10–67. n = 4057
10–68. p ≤0.00348
10–69. n = (Zα/2/E)2p(1 −p) = (2.575/0.01)2p(1 −p) = 66306.25p(1 −p). The most
conservative choice of p is p = 0.5, giving n = 16576.56 or n = 16577 homeowners.
10–70. 0.0282410 ≤p1 −p2 ≤0.0677590
10–71. −0.0244 ≤p1 −p2 ≤0.0024
10–72. −8.50 ≤µ1 −µ2 ≤1.94
10–73. −2038 ≤µ1 −µ2 ≤3774.8
10–74. Since X and S2 are independent, we can construct conﬁdence intervals for µ and σ2
such that we are 90 percent conﬁdent that both intervals provide correct conclusions
by constructing a 100(0.90)1/2 percent conﬁdence interval for each parameter. That
is, we need a 95 percent conﬁdence interval on µ and σ2. Thus, 3.938 ≤µ ≤4.057
and 0.0049 ≤σ2 ≤0.0157 provides the desired simultaneous conﬁdence intervals.
10–75. Assume that all three variances are equal. A 95 percent simultaneous conﬁdence
interval on µ1 −µ2, µ1 −µ3, and µ2 −µ3 will require that the individual intervals
use α/3 = 0.05/3 = 0.0167.
For µ1 −µ2, s2
p = 1.97, t0.0167/2,18 ≃2.64; −3.1529 ≤µ1 −µ2 ≤0.1529
For µ1 −µ3, s2
p = 1.76, t0.0167/2,23 ≃2.59; −1.9015 ≤µ1 −µ3 ≤0.9015
For µ2 −µ3, s2
p = 1.24, t0.0167/2,23 ≃2.59; −0.1775 ≤µ2 −µ3 ≤2.1775

12
10–76. The posterior density for µ is truncated normal:
f(µ|x1, . . . , x16) =
r
16
2π10
"
Φ
Ã
12 −8
p
10/16
!
−Φ
Ã
6 −8
p
10/16
!#−1
e(−1/2)( µ−8
10/16)2
for 6 < µ ≤12. From the normal tables, the 90% interval estimate for µ is centered
at 8 and is from 8 −(1.795)(1.054) = 6.108 to 9.892. Since 6.108 < 9 < 9.892, we
have no evidence to reject H0.
10–77. The posterior density for 1/σ2 is gamma w/parameters r+(n/2) and λ+Σ(xi−µ)2.
For r = 3, λ = 1, n = 10, µ = 5, Σ(xi −5)2 = 4.92, the Bayes estimate of 1/σ2 is
(1/σ2) =
3+5
1+4.92 = 1.35. The integral:
0.90 =
Z U
L
1
8(5.92)8(1/σ2)7e−5.92/σ2
10–78. Z =
Z ∞
−∞
(ˆθ −θ)2f(θ|x1, x2, . . . , xn) dθ
= ˆθ2
Z ∞
−∞
f(θ|x1, x2, . . . , xn) dθ −2ˆθ
Z ∞
−∞
θf(θ|x1, x2, . . . , xn) dθ
+
Z ∞
−∞
θ2f(θ|x1, x2, . . . , xn) dθ
Let
E(ˆθ) =
Z ∞
−∞
θf(θ|x1, x2, . . . , xn) dθ = µθ
E(ˆθ2) =
Z ∞
−∞
θ2f(θ|x1, x2, . . . , xn) dθ = τθ
Then
Z = ˆθ2 −2ˆθµθ + τθ
dZ
dˆθ
= 2ˆθ −2µθ = 0
so
ˆθ = µθ.

1
Chapter 11
11–1.
(a) H0: µ ≤160
H1: µ > 160
Z0 = x −µ0
σ√n = 158 −160
3/2
= −1.333
The ﬁber is acceptable if Z0 > Z0.05 = 1.645. Since Z0 = −1.33 < 1.645, the
ﬁber is not acceptable.
(b) d = µ−µ0
σ
= 165−160
3
= 1.67, if n = 4 then using the OC curves, we get β ≃0.05.
11–2.
(a) H0: µ = 90
H1: µ < 90
Z0 = x −µ0
σ√n = 90.48 −90
p
5/5
= 0.48
Since Z0 is not less than −Z0.05 = −1.645, do not reject H0. There is no
evidence that mean yield is less than 90 percent.
(b) n = (Zα + Zβ)2σ2/δ2 = (1.645 + 1.645)25/(5)2 = 2.16 ≃3. Could also use the
OC curves, with d = (µ0 −µ)/σ = (90 −85)/
√
5 = 2.24 and β = 0.05, also
giving n = 3.
11–3.
(a) H0: µ = 0.255
H1: µ ̸= 0.255
Z0 = x −µ0
σ/√n = 0.2546 −0.255
0.0001/
√
10
= −12.65
Since |Z0| = 12.65 > Z0.025 = 1.96, reject H0.
(b) d = |µ−µ0|
σ
= |0.2552−0.225|
0.0001
= 2, and using the OC curves with α = 0.05 and
β = 0.10 gives n ≃3.
Could also use n ≃(Zα/2 + Zβ)2σ2/δ2 = (1.96 +
1.28)2(0.0001)2/(0.0002)2 = 2.62 = 3.
11–4.
(a) H0: µ = 74.035
H1: µ ̸= 74.035
Z0 = x −µ0
σ/√n = 74.036 −74.035
0.001/
√
15
= 3.87
Since Z0 > Zα/2 = 2.575, reject H0.
(b) n ∼= (Zα/2 + Zβ)2σ2
δ2
= 0.712, n = 1
11–5.
(a) H0: µ = 1.000
Z0 = x −µ0
σ√n = 1014 −1000
25/
√
20
= 2.50
H1: µ ̸= 1.000
|Z0| = 2.50 > Z0.005 = 1.96, reject H0.
11–6.
(a) H0: µ = 3500
Z0 = x −µ0
σ√n = 3250 −3500
p
1000/12
= −27.39
H1: µ ̸= 3500
|Z0| = 27.39 > Z0.005 = 2.575, reject H0.

2
11–7.
(a) H0: µ1 = µ2
H1: µ1 ̸= µ2
Z0 =
x1 −x2
q
σ2
1
n1 + σ2
2
n2
= 1.349
Since Z0 < Zα/2 = 1.96, do not reject H0.
(d) d = 3.2, n = 10, α = 0.05, OC curves gives β ≈0, therefore power ≈1.
11–8. µ1 = New machine, µ2 = Current machine
H0: µ1 −µ2 ≤2, H1: µ1 −µ2 > 2
Use the t-distribution assuming equal variances: t0 = −5.45, do not reject H0.
11–9. H0: µ1 −µ2 = 0,
H1: µ1 −µ2 ̸= 0
Z0 =
x1 −x2
q
σ2
1
n1 + σ2
2
n2
= 2.656
Since Z0 > Zα/2 = 1.645, reject H0.
11–10. H0: µ1 −µ2 = 0,
H1: µ1 −µ2 > 0
Z0 =
x1 −x2
q
σ2
1
n1 + σ2
2
n2
= −6.325
Since Z0 < Zα = 1.645, do not reject H0.
11–11. H0: µ1 −µ2 = 0,
H1: µ1 −µ2 < 0
Z0 =
x1 −x2
q
σ2
2
n1 + σ2
2
n2
= −7.25
Since Z0 < −Zα = −1.645, reject H0.
11–12. H0: µ = 0
t0 = x −µ0
s/√n = −0.168 −0
8.5638/
√
10 = −0.062
H1: µ ̸= 0
|t0| = 0.062 < t0.025,9 = 2.2622, do not reject H0.
11–13.
(a) t0 = 1.842, do not reject H0.
(b) n = 8 is not suﬃcient; n = 10.
11–14. H0: µ = 9.5
t0 = 10.28 −9.5
2.55/
√
6
= 0.7492
H1: µ > 9.5
t0.05,5 = 2.015, do not reject H0.
11–15. t0 = 1.47, do not reject at α = 0.05 level of signiﬁcance. It can be rejected at the
α = 0.10 level of signiﬁcance.

3
11–16.
(a) H0: µ = 7.5
t0 = 6.997 −7.5
1.279/
√
18 = −1.112
H1: µ < 7.5
t0.05,7 = 1.895, do not reject H0,
the true scrap rate is not < 7.5%.
(b) n = 5
(c) 0.95
11–17. n = 3
11–18. d = |δ|
σ = 20
10 = 2, n = 3
11–19.
(a) H0: µ1 = µ2
t0 =
x1 −x2
sp
p
1/n1 + 1/n2
=
25.617 −21.7
0.799
p
1/6 + 1/6
= 8.49
H1: µ1 > µ2
t0 > t0.01,10 = 2.7638
(b) H0: µ1 −µ2 = 5
t0 =
x1 −x2 −5
sp
p
1/n1 + 1/n2
= 2.35, do not reject H0.
H1: µ1 −µ2 > 5
(c) Using sp = 0.799 as an estimate of σ,
d = (µ1 −µ2)/(2σ) = 5/2(0.799) = 3.13, n1 = n2 = 6, α = 0.01, OC curves
give β ≈0, so power ≃1.
(d) OC curves give n = 5.
11–20. t0 = −0.02, do not reject H0.
11–21. H0: σ2
1 = σ2
2
s1 = 9.4186
s2
1 = 88.71
H1: σ2
1 ̸= σ2
2
s2 = 10.0222
s2
2 = 100.44
α = 0.05
Reject H0 if F0 > F0.025,9,9 = 3.18.
F0 = 88.71
100.44 = 0.8832
∴do not reject H0: σ2
1 = σ2
2.
11–22.
(a) H0: σ2
1 = σ2
2
F0 = s2
1/s2
2 = 101.17/94.73 = 1.07
H1: σ2
1 ̸= σ2
2
do not reject H0.

4
(b) H0: µ1 = µ2
t0 =
x1 −x2
sp
p
1/n1 + 1/n2
=
12.5 −10.2
9.886
p
1/8 + 1/9
H1: µ1 > µ2
= −0.48, do not reject.
11–23.
(a) H0: µ1 = µ2
sp =
r
1480 + 1425
18
= 12.704
H1: µ1 ̸= µ2
t0 =
20.0 −15.8
12.704
p
1/10 + 1/10
= 0.74, do not reject H0.
Reject H0 if |t0| < t0.005,9 = 3.250.
(b) d = 10/[2(12.7)] = 0.39, Power = 0.13, n∗= 19
(c) n1 = n2 = 75
11–24.
(a) H0: µ1 = µ2
t0 =
x1 −x2
sp
p
1/n1 + 1/n2
=
20.0 −21.5
1.40
p
1/10 + 1/10
= −2.40
H1: µ1 ̸= µ2
reject H0.
(b) Use sp = 1.40 as an estimate of σ. Then d = |µ1 −µ2|/2σ = 2/2(1.40) = 0.7.
If α = 0.05 and n1 = n2 = 10, OC curves gives β ≃0.5. For β ≃0.15, we
must have n1 = n2 ≃30.
(c) F0 = s2
1/s2
2 = 2.25/1.69 = 1.33, do not reject H0.
(d) λ = σ1/σ2 = 2, α = 0.05, n1 = n2 = 10, OC curves give β ≃0.50.
11–25. H0: µ1 = µ2
t0 =
x1 −x2
sp
p
1/n1 + 1/n2
=
8.75 −8.63
0.57
p
1/12 + 1/18
= 0.56
H1: µ1 ̸= µ2
do not reject H0.
11–26. H0: σ2 = 16
If α = 0.05, λ = σ1/σ0 = 3/4, and β = 0.10, then
H1: σ2 < 16
n ≃55. Thus n = 10 is not good.
For the sample of n = 10 given, χ2
0 = (n−1)s2
σ2
0
= 9(14.69)
16
= 8.26. Since χ2
0.05,9 = 3.325,
do not reject H0.
11–27.
(a) H0: σ = 0.00002
χ2
0 = (n −1)s2
σ2
0
= 7(0.00005)2
(0.00002)2 = 43.75
H1: σ > 0.00002
Since χ2
0 > χ2
0.01,7 = 18.475, reject H0.
The claim is unjustiﬁed.

5
(b) A 99% one-sided lower conﬁdence interval is 0.3078 × 10−4 ≤σ2.
(c) λ = σ1/σ2 = 2, α = 0.01, n = 8, OC curves give β ≃0.30.
(d) λ = 2, β ≃0.05, α = 0.01, OC curves give n = 17.
11–28. H0: σ = 0.005
H1: σ > 0.005
If α = 0.01, β = 0.10, λ = 0.010/0.005 = 2, then the OC curves give n = 14.
Assuming n = 14, then χ2
0 = (n−1)s2
σ2
0
= 13(0.007)2
(0.005)2 = 25.48 < χ2
0.01,13 = 27.688, and we
do not reject. The 95% one-sided upper conﬁdence interval is σ2 ≤0.155 × 10−3.
11–29.
(a) H0: σ2 = 0.5
χ2
0 = (n −1)s2
σ2
0
= 11(0.10388)
0.5
= 2.28, reject H0.
H1: σ2 ̸= 0.5
(b) λ = σ/σ0 = 1/0.707 = 1.414, β ≈0.58
11–30. H0: σ2
1 = σ2
2
F0 = s2
1/s2
2 = 2.25 × 10−4/3.24 × 10−4
H1: σ2
1 ̸= σ2
2
= 0.69, do not reject H0.
Since σ2
1 = σ2
2, the test on means in Exercise 11–7 is appropriate. If λ = σ1/σ2 =
√
2.5 = 1.58, then using α = 0.01, β = 0.10, the OC curves give n ≃75.
11–31. H0: σ2
1 = σ2
2
F0 = s2
1/s2
2 = 0.9027/0.0294 = 30.69
H1: σ2
1 > σ2
2
F0 > F0.01,8,10 = 5.06, so reject H0.
If λ =
√
4 = 2, α = 0.01, and taking n1 ≃n2 = 10 (say), we get β ≃0.65.
11–32.
(b) H0: µ1 −µ2 = 0,
t0 = (0.984 −0.907)
q
11.37( 1
25 + 1
30)
= 0.0843, do not reject H0.
H1: µ1 −µ2 ̸= 0
11–33. H0: µd = 0
t0 = d −0
sd/√n =
5.0 −0
15.846/
√
10 = 0.998,
H1: µd ̸= 0
do not reject H0.
11–34. Using µD = µA −µB, t0 = −1.91, do not reject H0.
11–35. H0: µd = 0
H1: µd ̸= 0
Reject H0 if |t0| > t0.025,5 = 2.571.
t0 =
3 −0
1.41/
√
6 = 5.21
∴reject H0
11–36. t0 = 2.39, reject H0.

6
11–37. H0: p = 0.70, H1: p ̸= 0.70; Z0 =
(699−700)
√
1000(0.7)(0.3) = 0.586, do not reject H0.
11–38. H0: p = 0.025, H1: p ̸= 0.025; Z0 =
(18−200)
√
8000(0.975)(0.025) = −13.03, reject H0.
11–39. The “best” test will maximize the probability that H0 is rejected, so we want to
max Z0 =
X1 −X2
p
σ2
1/n1 + σ2
2/n2
subject to n1 + n2 = N.
Since for a given sample, X1 −X2 is ﬁxed, this is equivalent to
min L = σ2
1
n1
+ σ2
2
n2
subject to n1 + n2 = N.
Since n2 = n1 −N, we have
min L = σ2
1
n1
+
σ2
2
N −n1
and from dL/dn1 = 0 we ﬁnd
σ1
σ2
= n1
n2
,
which says that the observations should be assigned to the populations in the same
ratio as the standard deviations.
11–40. z0 = 6.26, reject H0.
11–41. H0: p1 = p2,
H1: p1 < p2; Z0 =
(0.01−0.021)
√
0.016(0.984)(
1
1000+
1
1200) = −2.023, do not reject H0.
11–42. H0: p1 = p2,
H1: p1 ̸= p2; Z0 =
(0.042−0.064)
√
0.053(0.947)(
2
500) = −1.55, do not reject H0.
11–43. Let 2σ2 = σ2
1/n1+σ2
2/n2 be the speciﬁed sample variance. If we minimize c1n1+c2n2
subject to the constraint σ2
1/n1 + σ2
2/n2 = 2σ2, we obtain the solution
n1
n2
=
s
σ2
1c2
σ2
2c1
.
11–44. H0: µ1 = 2µ2
H1: µ1 > 2µ2
Z0 = X1 −2X2
r
σ2
1
n1
+ 4σ2
2
n2

7
11–45. H0: σ2 = σ2
0
H1: σ2 ̸= σ2
0
β = P
µ
χ2
1−α/2,n−1 ≤(n −1)S2
σ2
0
≤χ2
α/2,n−1
¯¯¯¯σ2 = σ2
1 ̸= σ2
0
¶
= P
µσ2
0
σ2
1
χ2
1−α/2,n−1 ≤(n −1)S2
σ2
1
≤σ2
0
σ2
1
χ2
α/2,n−1
¶
which can be evaluated using tables of χ2.
11–46. H0: σ2
1 = σ2
2
H1: σ2
1 ̸= σ2
2
β = P
µ
F1−α/2,u,v ≤S2
1
S2
2
≤Fα/2,u,v
¯¯¯¯
σ2
1
σ2
2
¶
= P
µσ2
2
σ2
1
F1−α/2,u,v ≤S2
1
S2
2
σ2
2
σ2
1
≤σ2
2
σ2
1
Fα/2,u,v
¶
Since (σ2
2/σ2
1)(S2
1/S2
2) follows an F-distribution, β may be evaluated by using tables
of F.
11–47.
(a) Assume the class intervals are deﬁned as follows:
Class Interval
Oi
Ei
(Oi −Ei)2/Ei
−∞< X < 11
6
6.15
0.004
11 ≤X < 16
11
10.50
0.024
16 ≤X < 21
16
19.04
0.485
21 ≤X < 26
28
24.68
0.447
26 ≤X < 31
22
24.54
0.263
31 ≤X < 36
19
14.63
1.305
36 ≤X < 41
11
12.36
0.150
41 ≤X
4
5.10
0.237
χ2
0 = 2.915
The expected frequencies are obtained by evaluating n[Φ( ci−x
s ) −Φ( ci−1−x
s
)]
where ci is the upper boundary of cell i. For our problem,
Ei = 117
·
Φ
µci −25.61
9.02
¶
−Φ
µci−1 −25.61
9.02
¶¸
.

8
Since χ2
0 = 2.915 < χ2
0.05,5 = 11.070, do not reject H0.
(b) To use normal probability paper for data expressed in a histogram, ﬁnd the
cumulative probability associated with each interval, and plot this against
the upper boundary of each cell.
Cell Upper Bound
Observed Frequency
Pj
11
6
0.051
16
11
0.145
21
16
0.282
26
28
0.521
31
22
0.709
36
19
0.872
41
11
0.966
45
4
1.006
Normal probability plot.
11–48. Estimate ˆλ = x = 4.9775. The expected frequencies are
Defects
0
1
2
3
4
5
6
7
8
9
10
11
12
Oi
4
13
34
56
70
70
58
42
25
15
9
3
1
Ei
2.76
13.72
34.15
56.66
70.50
70.50
70.18
58.22
41.40
25.76
14.25
3.21
1.33

9
Three cells have expected values less than 5, so they are combined with other cells
to get:
Defects
0–1
2
3
4
5
6
7
8
9
10–12
Oi
17
34
56
70
70
58
42
25
15
13
Ei
16.48
34.15
56.66
70.50
70.50
70.18
58.22
41.40
25.76
18.79
χ2
0 = 1.8846, χ2
0.05,8 = 15.51, do not reject H0, the data could follow a Poisson
distribution.
11–49. x
Oi
Ei
(Oi −Ei)2/Ei
0
967
1000
1.089
1
1008
1000
0.064
2
975
1000
0.625
3
1022
1000
0.484
4
1003
1000
0.009
5
989
1000
0.121
6
1001
1000
0.001
7
981
1000
0.361
8
1043
1000
1.849
9
1011
1000
0.121
χ2
0 = 4.724 < χ2
0.05,9 = 16.919. Therefore, do not reject H0.
11–50.
(a) Assume that data given are the midpoints of the class intervals.
Class Interval
Oi
Ei
(Oi −Ei)2/Ei
X < 2.095
0
1.79∗
2.095 ≤X < 2.105
16
6.65
6.77
2.105 ≤X < 2.115
28
22.18
1.53
2.115 ≤X < 2.125
41
56.39
4.20
2.125 ≤X < 2.135
74
108.92
11.19
2.135 ≤X < 2.145
149
159.60
0.70
2.145 ≤X < 2.155
256
178.02
34.16
2.155 ≤X < 2.165
137
150.81
1.26
2.165 ≤X < 2.175
82
96.56
2.20
2.175 ≤X < 2.185
40
47.59
1.21
2.185 ≤X < 2.195
19
18.09
0.04
2.195 ≤X < 2.205
11
5.12
3.31
2.205 ≤X
0
1.28∗
∗Group into next cell
χ2
0 = 66.57 > χ2
0.05,8 = 15.507, reject H0.

10
(b) Upper Cell Bound
Observed Frequency
Pj
2.105
16
0.019
2.115
28
0.052
2.125
41
0.100
2.135
74
0.186
2.145
149
0.361
2.155
256
0.661
2.165
137
0.872
2.175
82
0.918
2.185
40
0.965
2.195
19
0.987
2.205
11
1.000
11–51.
X(j)
P(j)
X(j)
P(j)
188.12
0.0313
203.62
0.5313
193.71
0.0938
204.55
0.5938
193.73
0.1563
208.15
0.6563
195.45
0.2188
211.14
0.7188
200.81
0.2813
219.54
0.7813
201.63
0.3438
221.31
0.8438
202.20
0.4063
224.39
0.9063
202.21
0.4688
226.16
0.9688

11
11–52. χ2
0 = 11.649 < χ2
0.05,6 = 12.592. Do not reject.
11–53. χ2
0 = 0.0331 < χ2
0.05,1 = 3.841. Do not reject.
11–54. χ2
0 = 25.554 < χ2
0.05,9 = 16.919. Reject H0.
11–55. χ2
0 = 2.465 < χ2
0.05,4 = 9.488. Do not reject.
11–56. χ2
0 = 10.706 > χ2
0.05,3 = 7.81. Reject H0.
11–57. The observed and expected frequencies are
IA
A
Total
L
216
245
461
170.08
290.92
M
226
409
635
234.28
400.72
H
114
297
411
151.64
259.36
Total
556
951
1507

12
χ2
0 = 34.909, reject H0. Based on this data, physical activity is not independent of
socioeconomic status.
11–58. χ2
0 = 13.6289 < χ2
0.05.8 = 15.507. Do not reject.
11–59. Expected Frequencies:
17
62
55
22
81
71
17
62
54
ˆµ1 = 0.304
ˆµ1 = 0.127
ˆµ2 = 0.394
ˆµ2 = 0.465
ˆµ3 = 0.302
ˆµ3 = 0.408
χ2
0 =
3
X
i=1
3
X
j=1
(Oij −Eij)2
Eij
= 2.88 + 1.61 + 0.16 + 2.23 + 0.79 + 13.17
+ 0 + 5.23 + 6 = 22.06
χ2
0.05,4 = 9.488
∴reject H0, pricing strategy and facility conditions are not independent
11–60.
(a)
Non Defective
Defective
Machine 1
Machine 2
468
479
32
21
500
500 ⇒473.5
26.5
473.5
26.5
947
53
1000
χ2
0 = 0.064 + 1.141 + 0.013 + 1.41 = 2.897
χ2
0.05,1 = 3.841, do not reject H0, the populations do not diﬀer
(b) homogeneity
(c) yes

1
Chapter 12
12–1.
(a) Analysis of Variance
Source
DF
SS
MS
F
P
Factor
3
80.17
26.72
3.17
0.047
Error
20
168.33
8.42
Total
23
248.50
(b)

2
(c) Tukey’s pairwise comparisons
Family error rate = 0.0500
Individual error rate = 0.0111
Critical value = 3.96
Intervals for (column level mean) - (row level mean)
1
2
3
2
-5.357
4.024
3
-0.857
-0.190
8.524
9.190
4
-2.190
-1.524
-6.024
7.190
7.857
3.357
(d)

3
12–2.
(a) Analysis of Variance for Obs
Source
DF
SS
MS
F
P
Flowrate
2
3.648
1.824
3.59
0.053
Error
15
7.630
0.509
Total
17
11.278

4
(b)

5
12–3.
(a) Analysis of Variance for Strength
Source
DF
SS
MS
F
P
Technique
3
489740
163247
12.73
0.000
Error
12
153908
12826
Total
15
643648
(b) Tukey’s pairwise comparisons
Family error rate = 0.0500
Individual error rate = 0.0117
Critical value = 4.20
Intervals for (column level mean) - (row level mean)
1
2
3
2
-423
53
3
-201
-15
275
460
4
67
252
30
543
728
505

6
12–4.
(a) Random eﬀects
Analysis of Variance for Output
Source
DF
SS
MS
F
P
Loom
4
0.34160
0.08540
5.77
0.003
Error
20
0.29600
0.01480
Total
24
0.63760
(b) σ2
τ = 0.01412
(c) σ2 = 0.0148
(d) 0.035
(e)

7
12–5.
(a) Analysis of Variance for Density
Source
DF
SS
SS
MS
F
P
Temp
3
0.13911
0.13911
0.04637
2.62
0.083
Error
18
0.31907
0.31907
0.01773
Total
21
0.45818
(b) ˆµ = 21.70, ˆτ1 = 0.023
ˆτ2 = −0.166
ˆτ3 = 0.029
ˆτ4 = 0.059
(c)

8
12–6.
(a) Analysis of Variance for Conductivity
Source
DF
SS
MS
F
P
Coating
4
1060.50
265.13
16.35
0.000
Error
15
243.25
16.22
Total
19
1303.75
(b) ˆµ = 139.25, ˆτ1 = 5.75
ˆτ2 = 6.00
ˆτ3 = −7.75
ˆτ4 = −10.00
ˆτ5 = 6.00
(c) 142.87 ≤µ1 ≤147.13, 7.363 ≤µ1 −µ2 ≤24.137

9
(d) Tukey’s pairwise comparisons
Family error rate = 0.0500
Individual error rate = 0.00747
Critical value = 4.37
Intervals for (column level mean) - (row level mean)
1
2
3
4
2
-9.049
8.549
3
4.701
4.951
22.299
22.549
4
6.951
7.201
-6.549
24.549
24.799
11.049
5
-9.049
-8.799
-22.549
-24.799
8.549
8.799
-4.951
-7.201
(e)

10
12–7.
(a) Analysis of Variance for Response Time
Source
DF
SS
MS
F
P
Circuit
2
260.9
130.5
4.01
0.046
Error
12
390.8
32.6
Total
14
651.7
(b) Tukey’s pairwise comparisons
Family error rate = 0.0500
Individual error rate = 0.0206
Critical value = 3.77
Intervals for (column level mean) - (row level mean)
1
2
2
-17.022
2.222
3
-7.222
0.178
12.022
19.422
(c) c1 = y1· −2y2· + y3·,
SSc1 = 14.4
c2 = y1· −y3·,
SSc2 = 246.53
Only c2 is signiﬁcant at α = 0.05
(d) 0.88

11
12–8.
(a) Analysis of Variance for Shape
Source
DF
SS
MS
F
P
Nozzle
4
0.10218
0.02554
8.92
0.000
Efflux
5
0.06287
0.01257
4.39
0.007
Error
20
0.05730
0.00286
Total
29
0.22235
(c)

12
12–9.
(a) Analysis of Variance for Strength
Source
DF
SS
MS
F
P
Chemical
3
12.95
4.32
2.38
0.121
Bolt
4
157.00
39.25
21.61
0.000
Error
12
21.80
1.82
Total
19
191.75
(c)

13
12–10. µ = 1
4Σµ1 = 220
4 = 55, τ1 = 50 −55 = −5, τ2 = 60 −55 = 5, τ3 = 50 −55 = −5,
τ4 = 60 −55 = 5. Then Φ2 = 2
a
nΣτ 2
i
2σ2 = 2
4
n(100)
2(25) = n, Φ = √n. β < 0.1, α = 0.05,
OC curves give n = 7.
12–11.
µ = 1
5Σµi = 940
5
= 188, τi = µi −188, i = 1, 2, . . . , 5
τ1 = −13, τ2 = 2, τ3 = −28, τ4 = 12, τ5 = 27
Φ2 = n
a
Στ 2
i
σ2 = n
5
µ1830
100
¶
= 3.66n, Φ = 1.91√n
If β ≤0.05, α = 0.01, OC curves give n = 3. If n = 3, then β ∼= 0.03.
12–12. The test statistic for the two-sample t-test (with n1 = n2 = n) is
t0 = y1· −y2·
Sp
p
2/n
−t2n−2
t2
0 = (y1· −y2·)2(n/2)
S2
p
=
³y1·
n + y2·
n
´2
·
³n
2
´
S2
p
=
y2
1·
2n + y2
2·
2n −y1·y2·
n
S2
p

14
But since y1·y2· = y2
1·
2 + y2
2·
2 −(y1·+y2·)2
2
, the last equation becomes
t2
0 =
y2
1·
n + y2
2·
n −(y1· + y2·)2
2n
S2
p
= SSTreatments
S2
p
Note that S2
p =
P2
i=1
Pn
j=1(yij−yi·)2
2n−2
= MSE. Therefore, t2
0 = SSTreatments
MSE
, and since the
square of tu is F1,u (in general), we see that the two tests are equivalent.
12–13. V
µ
a
X
i=1
ciyi·
¶
=
a
X
i=1
V (ciyi·) =
a
X
i=1
c2
i V (yi·)
=
a
X
i=1
c2
i V
Ã ni
X
j=1
yij
!
=
a
X
i=1
c2
i
ni
X
j=1
V (y)
= σ2
a
X
i=1
nic2
i
12–14. For 4 treatments, a set of orthogonal contrasts is
3y1· −y2· −y3· −y4·
2y2· −y3· −y4·
y3· −y4·
Assuming equal n, the contrast sums of squares are
Q2
1 = (3y1· −y2· −y3· −y4·)2
12n
,
Q2
2 = (2y2· −y3· −y4·)2
6n
Q2
3 = (y3· −y4·)2
2n
Now
Q2
1 + Q2
2 + Q2
3 =
9
4
X
i=1
y2
i· −6
X X
i<j
yi·yj·
12n
and since
X X
i<j
yi·yj· = 1
2
"
y2
·· −
4
X
i=1
y2
i·
#
,
Q2
1 + Q2
2 + Q2
3 =
12
4
X
i=1
y2
i· −3y2
··
12n
=
4
X
i=1
y2
i·
n
−y2
··
4n
= SSTreatments

15
12–15.
(a) 15ˆµ + 5ˆτ1 + 5ˆτ2 + 5ˆτ3 = 307
5ˆµ + 5ˆτ1
= 104
5ˆµ
+ 5ˆτ2
= 111
5ˆµ
+ 5ˆτ3 = 92
If P3
i=1 ˆτi = 0, then ˆµ = 20.47, ˆτ1 = 0.33, ˆτ2 = 1.73, and ˆτ3 = 2.07.
d
τ1 −τ2 = ˆτ1 −ˆτ2 = −1.40
(b) If ˆτ3 = 0, then ˆµ = 18.40, ˆτ1 = 2.40, ˆτ2 = 3.80, and ˆτ3 = 0. These estimators
diﬀer from those found in part (a). However, note that
d
τ1 −τ2 = ˆτ1 −ˆτ2 = 2.40 −3.80 = −1.40
which agrees with part (a), because contrasts in the τi are uniquely estimated.
(c)
Estimate Using
Function
Part (a) Solution
Part (b) Solution
µ + τ1
20.80
20.80
2τ1 −τ2 −τ3
1.00
1.00
µ + τ1 + τ2
22.53
24.60

1
Chapter 13
13–1. Analysis of Variance for Wear
Source
DF
SS
MS
F
P
CS
2
0.0317805
0.0158903
15.94
0.000
DC
2
0.0271854
0.0135927
13.64
0.000
CS*DC
4
0.0006873
0.0001718
0.17
0.950
Error
18
0.0179413
0.0009967
Total
26
0.0775945
13–2. Analysis of Variance for Finish
Source
DF
SS
MS
F
P
Drying
2
27.4
13.7
0.01
0.986
Paint
1
355.6
355.6
0.38
0.601
Drying*Paint
2
1878.8
939.4
5.03
0.026
Error
12
2242.7
186.9
Total
17
4504.4
13–3. −23.93 ≤µ1 −µ2 ≤5.15
13–4. Analysis of Variance for Strength
Source
DF
SS
MS
F
P
operator
2
2.250
1.125
0.29
0.759
machine
3
28.833
9.611
2.46
0.160
operator*machine
6
23.417
3.903
0.84
0.565
Error
12
56.000
4.667
Total
23
110.500
13–5. The results would be a mixed model. The test statistics would be:
Eﬀect
F0
Operator
0.241
Machine
2.46
Operator∗Machine
0.84
There is no change in conclusions.

2
13–6. Analysis of Variance for time
Source
DF
SS
MS
F
P
operator
2
0.01005
0.00503
0.07
0.937
engineer
1
0.04688
0.04688
0.62
0.512
operator*engineer
2
0.15005
0.07503
1.26
0.350
Error
6
0.35785
0.05964
Total
11
0.56483
13–7. Analysis of Variance for current
Source
DF
SS
MS
F
P
glass
1
14450.0
14450.0
273.79
0.000
phos
2
933.3
466.7
8.84
0.004
glass*phos
2
133.3
66.7
1.26
0.318
Error
12
633.3
52.8
Total
17
16150.0
13–8.

3
There does not appear to be a problem with constant variance across levels of either
factor.
13–9. Analysis of Variance for strength
Source
DF
SS
MS
F
P
Conc
2
7.7639
3.8819
10.62
0.001
Freeness
2
19.3739
9.6869
26.50
0.000
Time
1
20.2500
20.2500
55.40
0.000
Conc*Freeness
4
6.0911
1.5228
4.17
0.015
Conc*Time
2
2.0817
1.0408
2.85
0.084
Freeness*Time
2
2.1950
1.0975
3.00
0.075
Conc*Freeness*Time
4
1.9733
0.4933
1.35
0.290
Error
18
6.5800
0.3656
Total
35
66.3089

4
13–10. Estimated Effects and Coefficients for warpage (coded units)
Term
Effect
Coef
SE Coef
T
P
Constant
1.3250
0.01398
94.81
0.000
A
0.2250
0.1125
0.01398
8.05
0.000
B
-0.1625
-0.0813
0.01398
-5.81
0.000
C
-0.4500
-0.2250
0.01398
-16.10
0.000
A*B
-0.5125
-0.2563
0.01398
-18.34
0.000
A*C
0.0000
0.0000
0.01398
0.00
1.000
B*C
0.2875
0.1438
0.01398
10.29
0.000
A*B*C
0.0625
0.0313
0.01398
2.24
0.056
Based on the p-values, factors A, B, C, and the interactions AB and BC are
signiﬁcant at the 5% level of signiﬁcance.
13–11. Using only the signiﬁcant factors and interactions, the resulting residuals are as
follows.

5
13–12. Estimated Effects and Coefficients for score (coded units)
Term
Effect
Coef
SE Coef
T
P
Constant
182.781
0.4891
373.68
0.000
A
-9.063
-4.531
0.4891
-9.26
0.000
B
-1.312
-0.656
0.4891
-1.34
0.198
C
-2.687
-1.344
0.4891
-2.75
0.014
D
3.937
1.969
0.4891
4.02
0.001
A*B
4.062
2.031
0.4891
4.15
0.001
A*C
0.688
0.344
0.4891
0.70
0.492
A*D
-2.187
-1.094
0.4891
-2.24
0.040
B*C
-0.563
-0.281
0.4891
-0.57
0.573
B*D
-0.188
-0.094
0.4891
-0.19
0.850
C*D
1.688
0.844
0.4891
1.72
0.104
A*B*C
-5.187
-2.594
0.4891
-5.30
0.000
A*B*D
4.687
2.344
0.4891
4.79
0.000
A*C*D
-0.938
-0.469
0.4891
-0.96
0.352
B*C*D
-0.938
-0.469
0.4891
-0.96
0.352
A*B*C*D
2.437
1.219
0.4891
2.49
0.024

6
13–13.

7

8
13–14. See solution for 13–12 for the standard errors.
13–15. Estimated Effects and Coefficients for strength
Term
Effect
Coef
SE Coef
T
P
Constant
2872.06
40.47
70.97
0.000
A
1430.88
715.44
40.47
17.68
0.000
B
3506.62
1753.31
40.47
43.33
0.000
C
-168.37
-84.19
40.47
-2.08
0.054
D
443.37
221.69
40.47
5.48
0.000
E
394.13
197.06
40.47
4.87
0.000
A*B
1168.37
584.19
40.47
14.44
0.000
A*C
93.37
46.69
40.47
1.15
0.266
A*D
31.62
15.81
40.47
0.39
0.701
A*E
30.88
15.44
40.47
0.38
0.708
B*C
-130.87
-65.44
40.47
-1.62
0.125
B*D
-44.12
-22.06
40.47
-0.55
0.593
B*E
-43.37
-21.69
40.47
-0.54
0.599
C*D
80.88
40.44
40.47
1.00
0.333
C*E
-93.38
-46.69
40.47
-1.15
0.266
D*E
193.38
96.69
40.47
2.39
0.030
Main eﬀects A, B, D, E and interactions AB and DE are signiﬁcant.

9
13–16.
(a)

10
(b)
(c) Estimated Effects and Coefficients for inches
Term
Effect
Coef
SE Coef
T
P
Constant
35.938
0.6355
56.55
0.000
A
-16.125
-8.062
0.6355
-12.69
0.000
B
3.125
1.562
0.6355
2.46
0.057
C
-1.125
-0.562
0.6355
-0.89
0.417
D
-1.125
-0.562
0.6355
-0.89
0.417
A*B
-4.375
-2.188
0.6355
-3.44
0.018
A*C
-0.625
-0.313
0.6355
-0.49
0.644
A*D
-3.125
-1.563
0.6355
-2.46
0.057
B*C
1.625
0.812
0.6355
1.28
0.257
B*D
0.125
0.063
0.6355
0.10
0.925
C*D
-0.625
-0.312
0.6355
-0.49
0.644
13–17. Block 1
Block 2
(1)
a
ab
b
ac
c
bc
abc

11
13–18. Block 1
Block 2
(1) ad
a abc
ab bd
b bcd
ac cd
c acd
bc abcd
d abd
13–19. Block 1
Block 2
Block 3
Block 4
(1)
a
c
d
ab
b
abc
abd
bcd
cd
bd
bc
acd
abcd
ad
ac
13–20. AC and BDE confounded: ABCDE generalized interaction
Block 1
Block 2
Block 3
Block 4
(1)
a
b
ab
ac
c
abc
bc
bd
abd
d
ad
abcd
bcd
acd
cd
be
abe
e
ae
abce
bce
ace
ce
de
ade
bde
abde
acde
cde
abcde
bcde
13–21.
(a) Estimated Effects and Coefficients for strength
Term
Effect
Coef
SE Coef
T
P
Constant
50.50
7.377
6.85
0.000
Block
-3.50
7.377
-0.47
0.648
A
-57.00
-28.50
7.377
-3.86
0.005
B
-13.25
-6.62
7.377
-0.90
0.395
C
26.25
13.12
7.377
1.78
0.113
A*B
7.25
3.62
7.377
0.49
0.636
A*C
-2.75
-1.38
7.377
-0.19
0.857
B*C
2.50
1.25
7.377
0.17
0.870

12
(b)
(c) & (d) This design is not as eﬃcient as possible. If we were to confound a diﬀer-
ent interaction in each replicate this would provide some information on all
interactions.

13
13–22. Please refer to the original reference for an analysis of the data from this experiment.
13–23.
(a) I = ABCD. Alias Structure:
ℓA = A + BCD
ℓAB = AB + CD
ℓCE = CE + ABD
ℓB = B + ACD
ℓAC = AC + BD
ℓAD = AD + CB
ℓC = C + ABD
ℓAE = AE + BCD
ℓBD = BD + AC
ℓD = D + ABC
ℓBC = BC + AD
ℓCD = CD + AB
ℓE = E + ABCD
ℓBE = BE + ACD
ℓDE = DE + ABC
(b) design: 25−1D = ABC
ℓA = 0.238
ℓAB = −0.024
ℓBD = 0.042
ℓB = −0.16
ℓAC = 0.0042
ℓCD = −0.024
ℓC = −0.043
ℓBC = −0.026
ℓBE = 0.1575
ℓD = 0.0867
ℓAD = −0.026
ℓCE = −0.029
ℓE = −0.242
ℓAE = 0.059
ℓDE = 0.036
Conclusion: assuming 3 and 4 factor interactions insigniﬁcant, factors A & E
are important; possible also B and BE.
13–24.
(a) The generators used were I = ACE and I = BDE.
(b) I = ACE = BDE = ABCD
13–25.
(a) D = ABC
(b) Term
Effect
Coef
SE Coef
T
P
Constant
2.04538
0.007423
275.55
0.000
A
0.06675
0.03338
0.007423
4.50
0.021
B
0.02625
0.01313
0.007423
1.77
0.175
C
0.02025
0.01012
0.007423
1.36
0.266
D
-0.00375
-0.00187
0.007423
-0.25
0.817
Only factor A appears to be signiﬁcant.

14
(c)
13–26. 23−1 with 2 replicates.
13–27.
24−1
I = ABCD
Aliases:
ℓA = A + BCD
ℓAB = AB + CD
ℓB = B + ACD
ℓAC = AC + BD
ℓC = C + ABD
ℓAD = AD + BC
ℓD = D = ABC

15
A
B
C
D = ABC
(1)
−
−
−
−
190
a
+
−
−
+
174
b
−
+
−
+
181
ab
+
+
−
−
183
c
−
−
+
+
177
ac
+
−
+
−
181
bc
−
+
+
−
188
abc
+
+
+
+
173
Sweetener (A) & Temperature (D)
A = −6.25
ℓAB = −0.25
inﬂuence taste.
B = 0.75
ℓAC = 0.75
C = −2.25
ℓAD = 0.75
D = −9.25
13–28. 25−1
I = ABCDE
Treatment
A
B
C
D
E = ABCD
Combination
Strength
−
−
−
−
+
e
800
+
−
−
−
−
a
900
−
+
−
−
−
b
3400
+
+
−
−
+
abe
6200
−
−
+
−
−
c
600
+
−
+
−
+
ace
1200
−
+
+
−
+
bce
3006
+
+
+
−
−
abc
5300
−
−
−
+
−
d
1000
+
−
−
+
+
ade
1500
−
+
−
+
+
bde
4000
+
+
−
+
−
abd
6100
−
−
+
+
+
cde
1500
+
−
+
+
−
acd
1100
−
+
+
+
−
bcd
3300
+
+
+
+
+
abcde
6300
ℓA = 10,994
ℓAB = 9394
The estimates for A, B, and AB are large
ℓB = 29,006
ℓC = −1594
ℓD = 3394
ℓE = 2806

16
13–29. 25−2
I = ABCD, I = ACE
Treatment
A
B
C
D = ABC
E = AC
Combination
Strength
−
−
−
−
+
e
800
+
−
−
+
−
ade
1500
−
+
−
+
+
bde
4000
+
+
−
−
−
abe
6200
−
−
+
+
−
cde
1500
+
−
+
−
+
ace
1200
−
+
+
−
−
bce
3006
+
+
+
+
+
abcde
6300
ℓA = 5894
ℓC = −494
ℓAB = 8094
ℓB = 14506
ℓD = 2094
ℓE = 94
13–30. 26−3
III
I = ABD = ACE = BCF = BCDE = ABEF = ACDF = DEF
A = BD = CE = ABCF = ABCDE = BEF = CDF = ADEF
B = AD = CE = ABCF = CDE = AEF = ABCEF = BDEF
C = ABCD = AE = BD = BDE = ABCEF = ADF = CDEF
D = AB = ACDE = BCDF = BCE = ABDEF = ACF = EF
E = ABDE = AC = BCEF = BCD = ABF = ACDEF = DF
F = ABDF = ACEF = BC = BCDEF = ABE = ACD = DE

1
Chapter 14
14–1.
(a) ˆy = 10.5 −0.00156 yards
(b) Predictor
Coef
SE Coef
T
P
Constant
10.451
2.514
4.16
0.000
yards
-0.001565
0.001090
-1.43
0.163
S = 3.414
R-Sq = 7.3%
R-Sq(adj) = 3.8%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
23.99
23.99
2.06
0.163
Residual Error
26
302.97
11.65
Total
27
326.96
(c) −0.00380 ≤β1 ≤0.00068
(d) R2 = 7.3%
(e)

2
Based on the residual plots there appears to be a severe outlier. This point
should be investigated and if necessary, the point removed and the analysis
rerun.
14–2. ˆy = 10.5 −0.00156(1800) = 7.63 or approximately 8.
6 ≤E(y|x0 = 1800) ≤9.27
14–3.
(a) ˆy = 31.6 −0.0410 displacement
(b) Predictor
Coef
SE Coef
T
P
Constant
31.648
1.812
17.46
0.000
displace
-0.040975
0.005406
-7.58
0.000
S = 1.976
R-Sq = 81.5%
R-Sq(adj) = 80.1%$
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
224.43
224.43
57.45
0.000
Residual Error
13
50.79
3.91
Total
14
275.21
(c) R2 = 81.5%
(d) ˆy = 20.381, 19.374 ≤E(y|x0 = 275) ≤21.388

3
14–4. ˆy = 31.6 −0.0410(275) = 20.381, 10.37 ≤E(y|x0 = 275) ≤21.39
14–5.

4
14–6.
(a) ˆy = 13.3 + 3.32 taxes
(b) Predictor
Coef
SE Coef
T
P
Constant
13.320
2.572
5.18
0.000
taxes
3.3244
0.3903
8.52
0.000
S = 2.961
R-Sq = 76.7%
R-Sq(adj) = 75.7%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
636.16
636.16
72.56
0.000
Residual Error
22
192.89
8.77
Total
23
829.05
(c) 76.7%
(d)

5

6
14–7.
(a) ˆy = 93.3 + 15.6x
(b) Predictor
Coef
SE Coef
T
P
Constant
93.34
10.51
8.88
0.000
x
15.649
4.345
3.60
0.003
S = 11.63
R-Sq = 48.1%
R-Sq(adj) = 44.4%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
1755.8
1755.8
12.97
0.003
Residual Error
14
1895.0
135.4
Lack of Fit
8
1378.6
172.3
2.00
0.207
Pure Error
6
516.4
86.1
Total
15
3650.8
(c) 7.997 ≤β1 ≤23.299
(d) 74.828 ≤β0 ≤111.852
(e) (126.012, 138.910)
14–8.

7
14–9.
(a) ˆy = −6.34 + 9.21 temp
(b) Predictor
Coef
SE Coef
T
P
Constant
-6.336
1.668
-3.80
0.003
temp
9.20836
0.03377
272.64
0.000
S = 1.943
R-Sq = 100.0%
R-Sq(adj) = 100.0%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
280583
280583
74334.36
0.000
Residual Error
10
38
4
Total
11
280621
(c) t0 =
ˆβ1 −β1,0
rMSE
SXX
= 9.20836 −10
r
4
3309
= −23.41; t0.025,10 = 2.228; Reject β1 = 0
(d) 525.58 ≤E(y|X = 50) ≤529.91
(e) 521.22 ≤yx=58 ≤534.28

8
14–10.

9
14–11.
(a) ˆy = 77.7895 + 11.8634x
(b) Predictor
Coef
SE Coef
T
P
Constant
77.863
4.199
18.54
0.000
hydrocar
11.801
3.485
3.39
0.003
S = 3.597
R-Sq = 38.9%
R-Sq(adj) = 35.5%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
148.31
148.31
11.47
0.003
Residual Error
18
232.83
12.94
(c) 38.9%
(d) 4.5661 ≤β ≤19.1607
14–12.
(a)

10
(b)

11
14–13.
(a) AvgSize = −1922.7 + 564.54 Level
(b) Predictor
Coef
SE Coef
T
P
Constant
-1922.7
530.9
-3.62
0.003
Level
564.54
32.74
17.24
0.000
S = 459.0
R-Sq = 95.5%
R-Sq(adj) = 95.2%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
1
62660784
62660784
297.38
0.000
Residual Error
14
2949914
210708
Total
15
65610698
(c) (0.0015, 0.0019)
(d) 95.5%
(e)

12
14–14. x = OR Grade, y = Statistics Grade
(a) ˆy = −0.0280 + 0.9910x
(b) r = 0.9033
(c) t0 = r√n −2
r
√
1 −r2 = 0.9033
√
18
√1 −0.8160 = 8.93, reject H0.
(d) Z0 = (arctanh(0.9033) −arctanh(0.5))
√
17 = 3.88, reject H0.
(e) 0.7676 ≤ρ ≤0.9615
14–15. x = weight, y = BP
(a) ˆy = 69.1044 + 0.4194x
(b) r = 0.7735
(c) t0 = r√n −3
√
1 −r2 = 0.7735
√
23
√1 −0.5983 = 5.85, reject H0.
(d) Z0 = (arctanh(0.7735) −arctanh(0.6))
√
23 = 1.61, do not reject
(e) 0.5513 ≤ρ ≤0.8932

13
14–16. Note that SSR = ˆβ1Sxy = ˆβ2
1Sxx, and
V (ˆβ1) = E(ˆβ2
1) −[E(ˆβ1)]2 = σ2
Sxx
E(ˆβ2
1) = β2
1 + σ2
Sxx
Therefore
E(SSR) = E(ˆβ2
1)Sxx = σ2 + β2
1Sxx
E(MSR) = E
µSSR
1
¶
= σ2 + β2
1Sxx
14–17. E(ˆβ1) = E
µSxy
Sxx
¶
=
1
Sxx
E(Sxy)
=
1
Sxx
E
n
X
i=1
(xi1 −x1)yi
=
1
Sxx
n
X
i=1
(xi1 −x1)E(yi)
=
1
Sxx
n
X
i=1
(xi1 −x1)(β0 + β1xi1 + β2xi2)
= β1 + β2
n
X
i=1
xi2(xi1 −x1)
Sxx
In general, ˆβ1 is a biased estimator of β1.
14–18. V (ˆβ1) = σ2/Sxx, which is minimized if we can make Sxx is as large as possible.
Since Pn
i=1(xi −x)2 = Sxx, place the x’s as far from x as possible. If n is even, put
n/2 trials at each end of the region of interest. If n is odd, put 1 trial at the center
and (n −1)/2 trials at each end. These designs should be used only when you are
positive that the relationship between y and x is linear.

14
14–19.
L =
n
X
i=1
wi(yi −β0 −β1xi)2
∂L
∂β0
= −2
n
X
i=1
wi(yi −ˆβ0 −ˆβ1xi) = 0.
∂L
∂β1
= −2
n
X
i=1
wixi(yi −ˆβ0 −ˆβ1xi) = 0.
Simpliﬁcation of these two equations gives the normal equations for weighted least
squares.
14–20. If y = (β0 + β1x + ϵ)−1, then 1/y = β0 + β1x + ϵ is a straight-line regression model.
The scatter diagram of y∗= 1/y versus x is linear.
x
10
15
18
12
9
8
11
6
y∗
5.88
7.69
11.11
6.67
5.00
4.76
5.56
4.17
14–21. The no-intercept model would be the form y = β1x + ϵ. This model is likely not a
good one for this problem, because there is no data near the point x = 0, y = 0,
and it is probably unwise to extrapolate the linear relationship back to the origin.
The intercept is often just a parameter that improves the ﬁt of the model to the
data in the region where the data were collected.
14–22. b = 14
Σxi = 65.262
Σx2
i = 385.194
x = 4.662
Σyi = 208
Σy2
i = 3490
y = 14.857
Σxiyi = 1148.08
Sxx = 80.989
Sxy = 178.473
Syy = 599.714
ˆβ1 = 2.204
ˆβ0 = 4.582
ˆy = 4.582 + 2.204x
r =
Sxy
(SxxSyy)1/2 = 0.9919,
R2 = 0.9839
H0: ρ = 0
t0 = 27.08 > t0.05,12 = 1.782.
∴reject H0
H1: ρ ̸= 0
A strong correlation does not imply a cause and eﬀect relationship.

1
Chapter 15
15–1.
(a) ˆy = 7.30 + 0.0183x1 −0.399x4
(b) Predictor
Coef
SE Coef
T
P
Constant
7.304
5.179
1.41
0.176
x1
0.018299
0.004972
3.68
0.002
x4
-0.3986
0.1912
-2.08
0.053
S = 1.922
R-Sq = 64.1%
R-Sq(adj) = 59.9%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
2
112.263
56.131
15.19
0.000
Residual Error
17
62.824
3.696
Total
19
175.086
(c)

2
(d) The MSE has improved with the x1x4 model, but the R2 and adjusted R2
have decreased.
15–2.
(a) ˆy = −27.9 + 0.0136x1 + 30.7x2 −0.0670x3
(b) Predictor
Coef
SE Coef
T
P
Constant
-27.892
6.035
-4.62
0.000
x1
0.013597
0.003247
4.19
0.001
x2
30.685
6.513
4.71
0.000
x3
-0.06701
0.01916
-3.50
0.003
S = 1.167
R-Sq = 87.6%
R-Sq(adj) = 85.2%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
3
153.305
51.102
37.54
0.000
Residual Error
16
21.782
1.361
Total
19
175.086

3
(c)
15–3. (−0.8024, 0.0044)
15–4. (−0.1076, −0.0264)

4
15–5.
(a) ˆy = −1.81 + 0.00360x2 + 0.194x7 −0.00482x8
(b)

5
(c) Predictor
Coef
SE Coef
T
P
Constant
-1.808
7.901
-0.23
0.821
x2
0.0035981
0.0006950
5.18
0.000
x7
0.19396
0.08823
2.20
0.038
x8
-0.004815
0.001277
-3.77
0.001
S = 1.706
R-Sq = 78.6%
R-Sq(adj) = 76.0%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
3
257.094
85.698
29.44
0.000
Residual Error
24
69.870
2.911
Total
27
326.964
15–6.
(a) ˆy = 33.45 −0.05435x1 + 1.0782x6
(b) Predictor
Coef
SE Coef
T
P
Constant
33.449
1.576
21.22
0.000
x1
-0.054349
0.006329
-8.59
0.000
x6
1.0782
0.6997
1.54
0.138
S = 2.834
R-Sq = 82.9%
R-Sq(adj) = 81.3%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
2
856.24
428.12
53.32
0.000
Residual Error
22
176.66
8.03
Total
24
1032.90

6
(c)
(d) x6 is not signiﬁcant with x1 included in the model.

7
15–7.
(a) ˆy = −103 + 0.605 x1 + 8.92 x2 + 1.44 x3 + 0.014 x4
(b) Predictor
Coef
SE Coef
T
P
Constant
-102.7
207.9
-0.49
0.636
x1
0.6054
0.3689
1.64
0.145
x2
8.924
5.301
1.68
0.136
x3
1.437
2.392
0.60
0.567
x4
0.0136
0.7338
0.02
0.986
S = 15.58
R-Sq = 74.5%
R-Sq(adj) = 59.9%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
4
4957.2
1239.3
5.11
0.030
Residual Error
7
1699.0
242.7
Total
11
6656.3
(c) H0: β3 = 0, F0 = 0.361 (not signiﬁcant)
H0: β4 = 0, F0 = 0.0004 (not signiﬁcant)
(d)

8
15–8.
(a) y = 62.4 + 1.55 x1 + 0.510 x2 + 0.102 x3 −0.144 x4
(b) Predictor
Coef
SE Coef
T
P
Constant
62.41
70.07
0.89
0.399
x1
1.5511
0.7448
2.08
0.071
x2
0.5102
0.7238
0.70
0.501
x3
0.1019
0.7547
0.14
0.896
x4
-0.1441
0.7091
-0.20
0.844
S = 2.446
R-Sq = 98.2%
R-Sq(adj) = 97.4%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
4
2667.90
666.97
111.48
0.000
Residual Error
8
47.86
5.98
Total
12
2715.76
(c) H0: β4 = 0, F0 = 0.04 (not signiﬁcant)
(d) The t statistics are given in part (b)
(e) H0: β2 = β3 = β4 = 0,
SSR(β2, β3, β4|β1, β0) = SSR(β1, β2, β3, β4|β0) −SSR(β1|β0)
= 2667.90 −1450.08
= 1217.82
F0 = (1217.82/3)/5.98 = 67.88; at least one of the variables is signiﬁcant.
(f) (−1.1588, 2.1792)

9
15–9.
(a) y = −26219 + 189x −0.331x2
(b) Predictor
Coef
SE Coef
T
P
Constant
-26219
11911
-2.20
0.079
x
189.20
80.24
2.36
0.065
x2
-0.3312
0.1350
-2.45
0.058
S = 45.20
R-Sq = 87.3%
R-Sq(adj) = 82.2%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
2
70284
35142
17.20
0.006
Residual Error
5
10213
2043
Total
7
80497
(c) See the t-test results in part (b)
(d)

10
15–10.
(a) y = −4.33 + 4.89x −2.59x2
(b) Predictor
Coef
SE Coef
T
P
Constant
-4.3330
0.8253
-5.25
0.001
x
4.887
1.379
3.54
0.009
x2
-2.5855
0.4886
-5.29
0.001
S = 0.7017
R-Sq = 91.9%
R-Sq(adj) = 89.6%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
2
39.274
19.637
39.89
0.000
Residual Error
7
3.446
0.492
Total
9
42.720
(c) see part (b)

11
(d)

12
15–11. ˆy = 759.39 −7.607x′ −0.331(x′)2
15–13.
(a) y = −4.5 + 1.38x + 1.47x2
(b) Predictor
Coef
SE Coef
T
P
Constant
-4.46
14.63
-0.30
0.768
x
1.384
5.497
0.25
0.807
x2
1.4670
0.4936
2.97
0.016
S = 1.657
R-Sq = 99.6%
R-Sq(adj) = 99.5%
Analysis of Variance
Source
DF
SS
MS
F
P
Regression
2
5740.6
2870.3
1044.99
0.000
Residual Error
9
24.7
2.7
Total
11
5765.3
(c) see part (b)
15–15. The ﬁtted model is ˆy = 11.503 + 0.153x1 −0.094x2 −0.0306x1x2
The t-statistic for H0: β3 = 0 is t0 = 1.79. We conclude that the slopes are the
same.
15–16. y = β0 + β1x1 + β2(x1 −x∗)x2 + ε, where x2 is an indicator variable with x2 = 0 if
x1 ≤x∗and x2 = 1 if x1 > x∗.
15–17. y = β0 +β1x1 +β2(x1 −x∗)x2 +β3 +ε, where x2 and x3 are indicator variables with
x2 = x3 = 0 if x1 ≤x∗and x2 = x3 = 1 if x1 > x∗. β3 estimates the eﬀect of the
discontinuity.
15–18. The model is as in Exercise 15–16, except now X∗is unknown and must be
estimated. This is a nonlinear regression problem. It could be solved by using
one-dimensional or line search methods, which could be used to obtain the trial
values of x∗.
15–19. ˆb1 = 0.594
ˆb4 = −0.336
15–20. ˆb1 = 0.441
ˆb2 = 0.505
ˆb3 = −0.315
15–21. V IF1 = 1.2, V IF4 = 1.2
15–22.
(a) All possible regressions from Minitab displaying the best two models for each
combination of variables.

13
x x x x x x x x x
Vars
R-Sq
R-Sq(adj)
C-p
S
1 2 3 4 5 6 7 8 9
1
54.5
52.7
20.4
2.3929
X
1
35.2
32.7
39.3
2.8548
X
2
74.3
72.3
3.1
1.8324
X
X
2
66.0
63.2
11.2
2.1097
X
X
3
78.6
76.0
0.9
1.7062
X
X X
3
77.8
75.0
1.7
1.7410
X X
X
4
80.1
76.7
1.4
1.6812
X
X X X
4
79.5
75.9
2.0
1.7073
X X
X X
5
80.7
76.3
2.8
1.6941
X X
X X X
5
80.7
76.3
2.9
1.6957
X
X
X X X
6
81.2
75.8
4.4
1.7118
X X X
X X X
6
81.1
75.6
4.5
1.7174
X X
X
X X X
7
81.4
74.9
6.2
1.7442
X X X
X X X X
7
81.3
74.8
6.2
1.7470
X X
X
X X X X
8
81.6
73.8
8.0
1.7814
X X X X
X X X X
8
81.4
73.6
8.2
1.7895
X X X X X X X X
9
81.6
72.3
10.0
1.8302
X X X X X X X X X
(b) Stepwise regression from Minitab:
Alpha-to-Enter: 0.15
Alpha-to-Remove: 0.15
Response is
y
on
9 predictors, with N =
28
Step
1
2
3
Constant
21.788
14.713
-1.808
x8
-0.00703 -0.00681 -0.00482
T-Value
-5.58
-7.05
-3.77
P-Value
0.000
0.000
0.001
x2
0.00311
0.00360
T-Value
4.40
5.18
P-Value
0.000
0.000
x7
0.194
T-Value
2.20
P-Value
0.038

14
S
2.39
1.83
1.71
R-Sq
54.47
74.33
78.63
R-Sq(adj)
52.72
72.27
75.96
C-p
20.4
3.1
0.9
The stepwise procedure found variables x2, x7, and x8 signiﬁcant.
(c) Forward selection
Forward selection.
Alpha-to-Enter: 0.25
Response is
y
on
9 predictors, with N =
28
Step
1
2
3
4
Constant
21.788
14.713
-1.808
-1.822
x8
-0.00703 -0.00681 -0.00482 -0.00401
T-Value
-5.58
-7.05
-3.77
-2.87
P-Value
0.000
0.000
0.001
0.009
x2
0.00311
0.00360
0.00382
T-Value
4.40
5.18
5.42
P-Value
0.000
0.000
0.000
x7
0.194
0.217
T-Value
2.20
2.45
P-Value
0.038
0.023
x9
-0.0016
T-Value
-1.31
P-Value
0.202
S
2.39
1.83
1.71
1.68
R-Sq
54.47
74.33
78.63
80.12
R-Sq(adj)
52.72
72.27
75.96
76.66
C-p
20.4
3.1
0.9
1.4
Forward selection found variables x2, x7, x8, and x9 signiﬁcant.

15
(d) Backward elimination
Backward elimination.
Alpha-to-Remove: 0.1
Response is
y
on
9 predictors, with N =
28
Step
1
2
3
4
5
6
7
Constant
-7.292
-7.294
-9.130
-7.695
-4.627
-1.822
-1.808
x1
0.0008
0.0008
T-Value
0.40
0.42
P-Value
0.690
0.681
x2
0.00363
0.00363
0.00363
0.00358
0.00371
0.00382
0.00360
T-Value
4.32
4.59
4.69
4.76
5.13
5.42
5.18
P-Value
0.000
0.000
0.000
0.000
0.000
0.000
0.000
x3
0.12
0.12
0.17
0.17
T-Value
0.47
0.49
0.75
0.77
P-Value
0.643
0.632
0.461
0.451
x4
0.032
0.032
0.037
0.035
0.026
T-Value
0.77
0.80
1.00
0.97
0.78
P-Value
0.453
0.431
0.329
0.342
0.445
x5
0.000
T-Value
0.00
P-Value
1.000
x6
0.0016
0.0016
0.0015
T-Value
0.49
0.51
0.48
P-Value
0.630
0.618
0.639
x7
0.154
0.154
0.189
0.193
0.235
0.217
0.194
T-Value
1.02
1.10
1.72
1.79
2.54
2.45
2.20
P-Value
0.324
0.284
0.102
0.088
0.019
0.023
0.038
x8
-0.0039
-0.0039
-0.0042
-0.0044
-0.0037
-0.0040
-0.0048
T-Value
-1.90
-1.95
-2.34
-2.50
-2.48
-2.87
-3.77
P-Value
0.074
0.066
0.030
0.021
0.021
0.009
0.001
x9
-0.0018
-0.0018
-0.0017
-0.0017
-0.0018
-0.0016
T-Value
-1.26
-1.30
-1.26
-1.28
-1.40
-1.31
P-Value
0.222
0.210
0.221
0.213
0.176
0.202
S
1.83
1.78
1.74
1.71
1.70
1.68
1.71
R-Sq
81.56
81.56
81.39
81.18
80.65
80.12
78.63
R-Sq(adj)
72.34
73.80
74.88
75.80
76.25
76.66
75.96
C-p
10.0
8.0
6.2
4.4
2.9
1.4
0.9
Backward elimination found variables x2, x7, and x8 signiﬁcant.

16
15–23.
(a) All possible regressions from Minitab displaying the best two models for each
combination of variables.
24 cases used; 1 case contains missing values.
x x
x x x x x x x x x 1 1
Vars
R-Sq
R-Sq(adj)
C-p
S
1 2 3 4 5 6 7 8 9 0 1
1
80.9
80.1
5.6
2.9679
X
1
77.1
76.1
10.7
3.2514
X
2
82.6
81.0
5.2
2.8974
X
X
2
82.5
80.9
5.4
2.9082
X
X
3
84.3
82.0
5.0
2.8231
X
X
X
3
84.0
81.5
5.5
2.8552
X
X
X
4
85.0
81.9
6.0
2.8283
X
X
X
X
4
84.9
81.7
6.2
2.8411
X X X
X
5
86.7
83.0
5.8
2.7400
X
X
X
X
X
5
85.8
81.8
7.1
2.8347
X
X X X
X
6
88.6
84.5
5.3
2.6141
X
X
X
X X
X
6
87.6
83.2
6.6
2.7244
X
X
X
X
X X
7
89.6
85.1
5.9
2.5649
X
X X X
X X
X
7
89.0
84.1
6.8
2.6465
X
X
X
X X
X X
8
90.2
84.9
7.2
2.5786
X
X X X
X X
X X
8
90.0
84.6
7.4
2.6068
X X X
X X X X
X
9
90.5
84.4
8.8
2.6285
X X X
X X X X
X X
(b) Stepwise regression
Alpha-to-Enter: 0.15
Alpha-to-Remove: 0.15
Response is
y
on 11 predictors, with N =
24
N(cases with missing observations) =
1 N(all cases) =
25
Step
1
Constant
34.43
x1
-0.0482
T-Value
-9.66
P-Value
0.000
S
2.97
R-Sq
80.93
R-Sq(adj)
80.06
C-p
5.6
The stepwise procedure found x1 signiﬁcant.

17
(c) Forward selection
Forward selection.
Alpha-to-Enter: 0.25
Response is
y
on 11 predictors, with N =
24
N(cases with missing observations) =
1 N(all cases) =
25
Step
1
2
3
Constant
34.43
33.50
32.36
x1
-0.0482
-0.0544
-0.1034
T-Value
-9.66
-8.40
-2.65
P-Value
0.000
0.000
0.015
x6
1.05
1.02
T-Value
1.44
1.43
P-Value
0.164
0.169
x3
0.070
T-Value
1.28
P-Value
0.217
S
2.97
2.90
2.86
R-Sq
80.93
82.65
83.95
R-Sq(adj)
80.06
80.99
81.55
C-p
5.6
5.2
5.5
The forward selection procedure found x1, x3, and x6 signiﬁcant.

18
(d) Backward elimination
Backward elimination.
Alpha-to-Remove: 0.1
Response is
y
on 11 predictors, with N =
24
N(cases with missing observations) =
1 N(all cases) =
25
Step
1
2
3
4
5
6
7
8
9
Constant
-17.6442 -18.5202
-3.7497
-0.9652
-0.6957
-4.4555
-1.9112
-8.9148
0.3409
x1
-0.142
-0.142
-0.139
-0.127
-0.102
-0.089
-0.061
T-Value
-2.57
-2.68
-2.66
-2.54
-2.18
-2.11
-1.50
P-Value
0.024
0.019
0.019
0.023
0.044
0.050
0.152
x2
-0.076
-0.075
-0.096
-0.092
T-Value
-0.93
-0.97
-1.30
-1.26
P-Value
0.369
0.348
0.215
0.227
x3
0.231
0.230
0.240
0.224
0.140
0.146
0.099
0.035
T-Value
2.34
2.44
2.58
2.48
2.26
2.44
1.79
0.96
P-Value
0.037
0.030
0.022
0.026
0.038
0.026
0.090
0.348
x4
2.4
2.4
T-Value
0.87
0.90
P-Value
0.402
0.383
x5
6.8
6.7
6.8
6.4
6.1
6.6
3.0
3.6
2.9
T-Value
2.06
2.34
2.40
2.31
2.15
2.47
1.83
2.23
2.02
P-Value
0.062
0.036
0.031
0.036
0.047
0.024
0.083
0.038
0.057
x6
1.11
1.14
1.42
1.37
0.66
T-Value
0.88
0.99
1.30
1.26
0.70
P-Value
0.398
0.338
0.215
0.226
0.494
x7
-4.2
-4.1
-3.4
-3.8
-3.9
-3.6
T-Value
-1.47
-1.67
-1.46
-1.72
-1.74
-1.67
P-Value
0.167
0.118
0.165
0.106
0.100
0.114
x8
0.28
0.28
0.30
0.29
0.28
0.31
0.26
0.324
0.246
T-Value
2.11
2.20
2.40
2.31
2.26
2.54
2.12
2.72
2.81
P-Value
0.056
0.046
0.031
0.035
0.038
0.021
0.048
0.014
0.011
x9
-0.02
T-Value
-0.05
P-Value
0.959
x10
-0.0119
-0.0121
-0.0126
-0.0121
-0.0115
-0.0133
0.0113
-0.0142
-0.0099
T-Value
-1.82
-2.09
-2.21
-2.15
-2.01
-2.65
-2.21
-2.90
-5.00
P-Value
0.093
0.057
0.044
0.049
0.061
0.017
0.040
0.009
0.000
x11
-2.4
-2.5
-2.3
T-Value
-0.89
-0.93
-0.87
P-Value
0.393
0.370
0.400
S
2.75
2.65
2.63
2.61
2.65
2.61
2.74
2.83
2.82
R-Sq
91.04
91.04
90.48
89.97
88.90
88.57
86.70
85.04
84.31
R-Sq(adj)
82.83
84.15
84.36
84.62
84.05
84.53
83.00
81.89
81.96
C-p
12.0
10.0
8.8
7.4
6.9
5.3
5.8
6.0
5.0
Backward elimination found x5, x8, and x10 signiﬁcant.

19
15–24.
(a) All possible regressions from Minitab displaying the best two models for each
combination of variables.
x x x x
Vars
R-Sq
R-Sq(adj)
C-p
S
1 2 3 4
1
67.5
64.5
138.7
8.9639
X
1
66.6
63.6
142.5
9.0771
X
2
97.9
97.4
2.7
2.4063
X X
2
97.2
96.7
5.5
2.7343
X
X
3
98.2
97.6
3.0
2.3087
X X
X
3
98.2
97.6
3.0
2.3121
X X X
4
98.2
97.4
5.0
2.4460
X X X X
(b) Stepwise regression
Alpha-to-Enter: 0.15
Alpha-to-Remove: 0.15
Response is
y
on
4 predictors, with N =
13
Step
1
2
3
4
Constant
117.57
103.10
71.65
52.58
x4
-0.738
-0.614
-0.237
T-Value
-4.77
-12.62
-1.37
P-Value
0.001
0.000
0.205
x1
1.44
1.45
1.47
T-Value
10.40
12.41
12.10
P-Value
0.000
0.000
0.000
x2
0.416
0.662
T-Value
2.24
14.44
P-Value
0.052
0.000
S
8.96
2.73
2.31
2.41
R-Sq
67.45
97.25
98.23
97.87
R-Sq(adj)
64.50
96.70
97.64
97.44
C-p
138.7
5.5
3.0
2.7
Stepwise procedure found x1 and x2 signiﬁcant.

20
(c) Forward selection
Forward selection.
Alpha-to-Enter: 0.25
Response is
y
on
4 predictors, with N =
13
Step
1
2
3
Constant
117.57
103.10
71.65
x4
-0.738
-0.614
-0.237
T-Value
-4.77
-12.62
-1.37
P-Value
0.001
0.000
0.205
x1
1.44
1.45
T-Value
10.40
12.41
P-Value
0.000
0.000
x2
0.42
T-Value
2.24
P-Value
0.052
S
8.96
2.73
2.31
R-Sq
67.45
97.25
98.23
R-Sq(adj)
64.50
96.70
97.64
C-p
138.7
5.5
3.0
Forward selection found x1, x2, and x4 signiﬁcant.

21
(d) Backward elimination
Backward elimination.
Alpha-to-Remove: 0.1
Response is
y
on
4 predictors, with N =
13
Step
1
2
3
Constant
62.41
71.65
52.58
x1
1.55
1.45
1.47
T-Value
2.08
12.41
12.10
P-Value
0.071
0.000
0.000
x2
0.510
0.416
0.662
T-Value
0.70
2.24
14.44
P-Value
0.501
0.052
0.000
x3
0.10
T-Value
0.14
P-Value
0.896
x4
-0.14
-0.24
T-Value
-0.20
-1.37
P-Value
0.844
0.205
S
2.45
2.31
2.41
R-Sq
98.24
98.23
97.87
R-Sq(adj)
97.36
97.64
97.44
C-p
5.0
3.0
2.7
Backward elimination found x1 and x2 signiﬁcant.
15–25. V IF1 = 38.5, V IF2 = 254.4, V IF3 = 46.9, V IF4 = 282.5. The variance inﬂation
factors indicate a problem with multicollinearity.

1
Chapter 16
16–1. H0: ˜µ = 7.0
n = 10
H1: ˜µ = 7.0
α = 0.05
CR: R ≤R∗
α (Table X)
R+ = 8, R−= 2 ⇒R = 2
Since R > R∗
α, do not reject H0
16–2. H0: ˜µ = 8.5
H1: ˜µ ̸= 8.5
α = 0.05
R+ = 8, R−= 11, R = min(8, 11) = 8, R∗
0.05 = 5
Since R > R∗
0.05, do not reject H0
16–3.
(a) H0: ˜µ = 3.5
Critical Region: R−< R∗
d
H1: ˜µ > 3.5
(b) n = 10
α = 0.05
R∗
0.05 = 1
R = 3
Since R−> R∗
0.05, do not reject H0
(c) Probability of not rejecting H0 (˜µ = 3.5) when ˜µ = 4.5
p = P(X > 1) =
Z ∞
1
1
β e−(1/β)x dx = e−1/β
16–4. n = 10
σ = 1
H0: µ = 0
H1: µ > 0
(a) α = 0.025
(b) p = P(X > 0) = P(Z > −1) = 1 −Φ(−1) = 0.8413
R∗
0.05 = 1
β = 1 −
1
X
x=0
µ10
x
¶
(0.1587)x(0.8413)10−x = 0.487

2
16–5. H0: ˜µd = 0
H1: ˜µd ̸= 0
α = 0.05
CR: R < R∗
0.05 = 1
R−= 2, R+ = 6 ⇒R = 2
Since R is not less than R∗, do not reject H0
16–8. H0: µ = 7
R+ = 50.5
H1: µ ̸= 7
R−= 4.5
R = min(50.5, 4.5) = 4.5
R0.05 = 8
reject H0
16–9. H0: µ = 8.5
n = 20
H1: µ ̸= 8.5
α = .05
CR: R ≤R∗
0.05 = 52
R+ = 19 + 10.5 + · · · + 1 = 88.5
R−= 12 + 20 + · · · + 3.5 = 121.5
R = 88.5 > R∗
0.05
∴do not reject H0
conclude titanium content is 8.5%
16–10.
H0: µ = 8.5
µR = 20(21)
4
= 10.5
H1: µ ̸= 8.5
σ2
R = 10(21)(41)
24
= 717.5
Z0 = 85 −105
√
717.5 = −0.747
16–12. R+ = 24.5
R = min(24.5, 11.5) = 11.5
R−= 11.5
R∗
0.05 = 3
Do not reject H0, µd = 0.

3
16–13. H0: µ1 = µ2
n1 = 8
n2 = 9
H1: µ1 > µ2
CR: R1 < R∗
α = 51
Table IX
α = 0.05
R2 = 75, R1 = 78
Since R2 is not less than or equal to 51, do not reject H0.
Conclude the two
circuits are equivalent.
16–14. n1 = n2
R1 = 40
R2 = 6(13) −40 = 38
R∗
0.05 = 26
Do not reject H0
Airline
Minutes
Rank
D
0
1
D
1
2.5
A
1
2.5
A
2
4
A
3
5
A
4
6.5
D
4
6.5
A
8
8
D
9
9
D
10
10
D
13
11
A
15
12

4
16–15. n1 = n2 = 10,
n > 8 large-sample approximation
H0: µ1 = µ2
H1: µ1 ̸= µ2
α = 0.05
CR: |Z0| > Z0.025 = 1.96
µR1 = (20)(21)
4
= 105,
σ2
R1 = 10(10)(21)
12
= 175
R1 = 1 + 2 + 3 + · · · + 19.5 = 77
Z0 = 77 −105
√
175
= −2.117; |Z0| = 2.117 > 1.96
Reject H0; conclude unit 2 is superior.
16–16. H0: techniques do not diﬀer
H1: techniques diﬀer
Ranks
Ri
1
14
11.5
6
7
38.5
2
16
11.5
9
15
51.5
3
5
8
10
13
36
4
15
3
1.5
4
10
S2 = 1
15
·
1495 −16(17)2
4
¸
= 22.6
K =
1
22.6
·38.52
4
+ 51.52
4
+ 362
4 + 102
4 −16(17)2
4
¸
= 10.03
χ2
0.05,3 = 7.81, reject H0; conclude the techniques diﬀer
16–17. H0: methods do not diﬀer
H1: methods diﬀer
α = 0.10
CR: K > χ2
0.10,2 = 4.61
Method
Ranks
ri
1
10.5
9
12
7
5
43.5
2
10.5
15
14
8
6
53.5
3
1
4
3
2
13
83
K =
12
15(16)
µ43.52
5
+ 53.52
5
+ 53.52
5
+ 232
5
¶
−3(16) = 4.835
at 0.1 signiﬁcance, reject H0.

5
16–18. H0: manufacturers do not diﬀer
H1: they diﬀer
Ranks
Ri
A
12
1
3
9
13
38
B
4
17
14
7
20
62
C
19
15
16
11
18
79
D
5
10
2
8
6
31
K =
12
20(21)
·382 + 622 + 792 + 312
5
¸
−3(21) = 8.37
χ2
0.05,3 = 7.81. Reject H0; conclude the manufacturers diﬀer.

1
Chapter 17
17–1.
(a) X-bar Chart: UCL = 34.32 + 0.577(5.65) = 37.58
LCL = 34.32 −0.577(5.65) = 31.06
R chart:
UCL = 2.115(5.65) = 12
LCL = 0
There is one observation beyond the upper control limit. Removal of this point
results in the following control charts:

2
The process now appears to be in control.
(b) ˆσ = R
d2
= 5.74
2.326 = 2.468, PCR = USL −LSL
6σ
=
20
6(2.468) = 1.35
PCRK = min
·45 −34.09
3(2.468) , 34.09 −25
3(2.468)
¸
= min[1.474, 1.2277] = 1.2277
(c) 0.205%

3
17–2. P(X < µ + 3σ/√n|µX = µ + 1.5σ)
= P(Z < µ + 3σ/√n −(µ + 1.5σ)
σ/√n
= P(Z < 3 −1.5√n)
= probability of failing to detect shift on 1st sample following the shift.
[P(Z < 3 −1.5√n)]3 = prob of failing to detect shift for 3 consecutive samples
following the shift.
For n = 4, [P(Z < 0)]3 = (0.5)3 = 0.125
For n = 4 with 2-sigma limits,
[P(Z < 2 −3)]3 = [P(Z < −1)]3 = (0.1587)3 = 0.003997
17–3.
(a) ARL = 1/α
(b) ARL = 1/(1 −β)
(c) If k changes from 3 to 2, the in-control ARL will get much shorter (from about
370 to 20). This is not desirable.
(d) For a 1-sigma shift, β ≃0.8, so the ARL is approximately ARL =
1/(1 −0.8) = 5.
17–4. X = 362.75
25
= 14.51, R = 8.60
25 = 0.34
(a) X Chart: UCL = 14.706, CL = 14.31, LCL = 14.314
R Chart: UCL = 0.719, CL = 0.34, LCL = 0
(b) ˆσ = R/d2 = 0.34/2.326 = 0.14617, ˆµ = 14.51
6σ natural tolerance limits = 14.51 ± 3(0.14617) = 14.51 ± 0.4385
P(X > 14.90) = 0.00379, P(X < 14.10) = 0.00252
Fraction defective = 0.00631
(c) PCR = 15 −14
6(0.146) = 1.141
PCRK = min[1.119, 1.164] = 1.119

4
17–5. D/2
17–6.
(a) X = 214.25
20
= 10.7125
R = 133
20 = 6.65
X Chart: UCL = 10.7125 + 0.729(6.65) = 15.56
LCL = 5.86
R Chart: UCL = 2.282(6.65) = 15.175, LCL = 0; process in control
(b) ˆσ = Rd2 = 6.65/2.059 = 3.23
PCR = 15 −5
6(3.23) = 0.516
17–7.
(a) Normal probability plot
The normality assumption appears to be satisﬁed.

5
(b)
The process appears to be in control.
17–8.
(a) Normal probability plot
The normality assumption appears to be satisﬁed.

6
(b)
The process appears to be in control.
17–9.
The process is out of control.

7
17–10. p = 0.05
UCL = 0.05 + 3(0.0218) = 0.115
P(X < 0.115|µ = 0.08) = 1 −P
µ
Z < 0.115 −0.08
0.0654
¶
= 1 −P(Z < 0.535)
= 1 −0.7036 = 0.2964
Probability of detecting shift
on ﬁrst sample following shift
P(detecting before 3rd sample) = 1 −(0.7036)2 = 0.5049
17–11. For the detection probability to equal 0.5, the magnitude of the shift must
bring the fraction nonconforming exactly to the upper control limit.
That is,
δ = k
p
p(1 −p)/n, where δ is the magnitude of the shift.
Solving for n gives
n = (k/δ)2p(1 −p).
For example, if k = 3, p = 0.01 (the in-control fraction
nonconforming), and δ = 0.04, then n = (3/0.04)2(0.01)(0.99) = 56.
17–12.
(a) PCR = 1.5
(b) About 7 defective parts per million.
(c) PCRk = 1 PCR unchanged.
(d) About 0.135 percent defective.
17–13. Center the process at µ = 100. The probability that a shift to µ = 105 will be
detected on the ﬁrst sample following the shift is about 0.15. A p-chart with n = 7
would perform about as well.

8
17–14.
The process appears to be in control.
17–15.
The process is out of control. Removing two out-of-control points and revising the
limits results in:

9
The process is now in control.
17–16.
The process appears to be out of control.
17–17. UCL = 16.485; detection probability = 0.434
17–18. UCL = 19.487, CL = 10, LCL = 0.513

10
17–19.
Since the sample sizes are equal, the c and u charts are equivalent.
17–20.

11
17–21.
17–22.

12
17–23.
17–24.
(a)
R(t) =
Z ∞
t
f(x) dx =



1
if t < α
β−t
β−α
if α ≤t ≤β
0
if t > β
(b)
Z ∞
0
R(t) dt = α + β
2
(c) h(t) = f(t)
R(t) =
1
β −t,
α ≤t ≤β.
(d) H(t) =
Z t
0
h(t) dt = −ℓn
µ β −t
β −α
¶
e−H(t) = β −t
β −α = R(t).
17–25. RS(t) = e−λst, λs = λ1 + λ2 + λ3 = 7.6 × 10−2
(a) RS(60) = e−7.6×10−2×60 = 0.0105
(b) MTTF = 1/λs =
1
7.6×10−2 = 13.16 hours

13
17–26. λ1 = λ2 = λ3 = λ4 = λ5 = 0.002
(a) R(1000) =
5
X
k=2
µ5
k
¶
(0.367)k(0.633)5−k = 0.6056
(b) R(1000) = 1 −(0.633)5 = 0.8984
17–27. R(1000) =
3
X
k=0
e−1(1)k
k!
= 0.98104
17–28. λ = 1/160 = 6.25 × 10−3
17–29. 0.84, 0.85
17–30. If ˆθ is the maximum likelihood estimator of θ and φ = g(θ) is a single-valued
function of θ, then ˆφ = g(ˆθ) is the MLE of φ.
To prove this, note that L(θ),
the likelihood function, has a maximum at θ = ˆθ. Furthermore, θ = g−1(φ), so
the likelihood function is L[g−1(φ)], which has a maximum at ˆθ = g−1(φ) or at
φ = g(ˆθ). In the problem stated, R is of the form g(θ) = e−t/θ, so the problem is
solved.
17–31.
(a) 3842
(b) [913.63, ∞)
17–32.
(a) ˆR(300) = e−300/ˆθ = e−300/3842 = 0.9249
ˆRL(300) = e−300/ˆθL = e−300/913.63 = 0.72
(b) ˆL0.9 = 3842 ℓn(1/0.9) = 404.795

1
Chapter 18
18–1.
(a) λ = 2 arrivals/hr, µ = 3 services/hr, ρ = 2/3.
P(n > 5) = 1 −P(n ≤5) = 1 −
5
X
j=0
µ1
3
¶µ2
3
¶j
= 0.088
(b) L =
ρ
1 −ρ = 2, Lq =
λ2
µ(µ −λ) = 4/3
(c) W =
1
µ −λ = 1 hour
18–2.
(a) State 0 = rain; State 1 = clear.
P =
· 0.7
0.3
0.1
0.9
¸
(b)
p0
=
0.7p0 + 0.1p1
1
=
p0 + p1
This implies p0 = 1/4, p1 = 3/4.
(c) To ﬁnd p(3)
11 , note that
P (3) = P 3 =
· 0.412
0.588
0.196
0.804
¸
Thus, p(3)
11 = 0.804.
(d) f (2)
10 = p(2)
10 −f (1)
10 p10 = 0.16 −(0.1)(0.1) = 0.15
(e) µ0 = 1/p0 = 4 days
18–3.
P =
·
p
1 −p
1 −p
p
¸
P ∞=
· 1/2
1/2
1/2
1/2
¸

2
18–4.
(a)
P =


1 −2λ∆t
2λ∆t
0
µ∆t
1 −(µ + 3
2λ)∆t
3
2λ∆t
0
µ∆t
1 −µ∆t


(b)
−2λp0 + µp1
=
0
2λp0 −(µ + 3
2λ)p1 + µp2
=
0
3
2λp1 −µp2
=
0
p0 + p1 + p2
=
1
Solving yields
p0
=
µ2
µ2 + 2λµ + 3λ2
p1
=
2λµ
µ2 + 2λµ + 3λ2
p2
=
3λ2
µ2 + 2λµ + 3λ2
18–5.
(a) pii = 1 implies that State 1 is an absorbing state, so States 0 and 3 are
absorbing.
(b) p0 = 1. The system never leaves State 0.
(c) b10 = 2
3 · 1 + 1
6b10 + 1
6b20
b20 = 2
3b10 + 1
6b20
b10 = 20
21, b20 = 16
21
b13 = 1
6b13 + 1
6b23
b23 = 2
3b13 + 1
6b23 + 1
6 · 1
b13 = 1
21, b23 = 5
21

3
p0 = 1
2 · 20
21 + 1
2 · 16
21 = 18
21
(d) p0 = 1
4 · 1 + 1
4 · 20
21 + 1
4 · 16
21 + 1
4 · 1 = 39
42
18–6.
(a)
P =


1
0
0
0
· · ·
0
0
0
q
0
p
0
· · ·
0
0
0
0
q
0
p
· · ·
0
0
0
...
0
0
0
0
· · ·
q
0
p
0
0
0
0
· · ·
0
0
1


(b)
P =


1
0
0
0
0
0.7
0
0.3
0
0
0
0.7
0
0.3
0
0
0
0.7
0
0.3
0
0
0
0
1


b10
=
0.7(1) + 0.3b20
b20
=
0.7b10 + 0.3b20
b30
=
0.7b20
b10
=
0.953
b20
=
0.845
b30
=
0.591
b34
=
0.7b24 + 0.3(1)
b24
=
0.7b14 + 0.3b34
b14
=
0.3b24
b14
=
0.0465
b24
=
0.155
b34
=
0.408

4
18–7.
(a)
P
=


0
p
0
1 −p
1 −p
0
p
0
0
1 −p
0
p
p
0
1 −p
0


A
=
[1 0 0 0]
(b,c) The transition matrix P is said to be doubly stochastic since each row and each
column add to one. For such matrices, it is easy to show that the steady-state
probabilities are all equal. Thus, for p = q = 1/2 or p = 4/5, q = 1/5, we
have p1 = p2 = p3 = p4 = 1/4.
18–9. λ = 1/10, µ = 1/3, ρ = 3/10.
(a) P(wait) = 1 −p0 = ρ = 3/10.
(b) Lq =
λ2
µ(µ −λ) = 9/70.
(c) Wq = 3 =
λ
µ(µ −λ) ⇒λ = 1/6 = criteria for adding service.
(d) P(Wq > 0) =
Z ∞
10
λ(1 −ρ) e−(µ−λ)wq dwq = 3
10 e−7/3 .= 0.03.
(e) P(W > 10) = e−1
3( 7
10)(0.10) .= 0.10.
(f) ρ = 1 −p0 = 3/10.
18–10. λ = 15, µ = 27, N = 3, ρ = 15
27 = 5
9.
(a) L = 5
9
·1 −4(5
9)3 + 3(5
9)4
4
5[1 −( 5
9)]4
¸
.= 0.83.
(b)
µ5
9
¶3·
4
9
1 −( 5
9)4
¸
.= 0.084 .
(c) Since both the number in the system and the number in the queue are the
same when the system is empty.

5
18–11. The general steady-state equations for the M/M/s queueing system are
ρ
=
λ/(sµ)
p0
=
½· s−1
X
n=0
(sρ)n
n!
¸
+
·
(sρ)s
(s!)(1 −ρ)
¸¾−1
L
=
sρ +
(sρ)s+1p0
s(s!)(1 −ρ)2
Lq
=
(sρ)s+1p0
s(s!)(1 −ρ)2
W
=
L/λ
Wq
=
W −1
µ
In this problem, we have λ = 0.0416, µ = 0.025.
(a) ρ = 0.555 for s = 3.
(b) Plugging into the appropriate equation above, we ﬁnd that p0 = 0.1732.
Then Lq = 0.3725.
And ﬁnally, W = Wq + 1
µ =
Lq
0.0416 + 40 = 48.96 min
(c) ρ = 0.832 for s = 2.
Plugging into the appropriate equation above, we ﬁnd that p0 = 0.0917.
Then Lq = 3.742.
And ﬁnally, W =
Lq
0.0416 + 40 = 129.96 min.
18–12. λ = 18, µ = 30, φ = 0.6, ρ = 0.6/s.
p0 =
· s−1
X
j=0
φj
j! +
φs
s!(1 −ρ)
¸−1
.
s = 1 ⇒p0 = 0.4.

6
s = 2 ⇒p0 = 0.5384
and P(wait) = 1 −p0 −p1 = 1 −0.5384 −(0.6)1
1!
(0.5384) = 0.14.
18–13. λ = 50, µ = 20, φ = 2.5, ρ = 2.5/s.
(a)
Cj
=
λj−1λj−2 · · · λ0
µjµj−1 · · · µ1
,
where
λj =
½ λ,
j ≤s
0,
j > s
and
µj =
½ jµ,
j ≤s
sµ,
j > s
Therefore,
Cj
=
(λ/µ)j
j!
,
j = 0, 1, . . . , s.
pj =
(
(λ/µ)j
j!
p0
j = 0, 1, . . . , s
0
otherwise
,
where
p0 =
·
s
X
j=0
(λ/µ)j
j!
¸−1
(b) ps = (λ/µ)s
s!
·
s
X
j=0
(λ/µ)j
j!
¸−1
≤0.05
Try s = 5 ⇒ps = 0.065
s = 6 ⇒ρ = 0.416, ps = 0.0354 (which works).
(c) p6 = 0.0354
(d) Utilization would change from 41.6% to 100ρ = 100(0.5/6) = 8.33%.
(e) φ = 50/12 = 4.166. p6 = 0.377 fraction getting busy tone.

1
Chapter 19
19–1. Let n denote the number of coin ﬂips.
Then the number of heads observed is
X ∼Bin(n, 0.5). Therefore, we can expect to see about n/2 heads over the long
term.
19–2. If ˆπn denotes the estimator for π after n darts have been thrown, then it is easy
to see that ˆπn ∼(4/n)Bin(n, π/4). Then E(ˆπn) = π, and we can expect to see the
estimator converge towards π as n becomes large.
19–3. By the Law of the Unconscious Statistician,
E(ˆIn)
=
b −a
n
E
µ n
X
i=1
f(a + (b −a)Ui)
¶
=
(b −a)E[f(a + (b −a)Ui)]
=
(b −a)
Z 1
0
f(a + (b −a)u) · 1 du
=
I
19–4.
(a) The exact answer is Φ(2) −Φ(0) = 0.4772. The n = 1000 result will tend to
be closer than the n = 10.
(b) We can instead integrate over
R 4
0 , say, since
R 10
4
≈0. This strategy will prevent
the “waste” of observations on the trivial tail region.
(c) The exact answer is 0.
19–5.
(a)
customer
arrival time
begin service
service time
depart time
wait
1
3
3.0
6.0
9.0
0.0
2
4
9.0
5.5
14.5
5.0
3
6
14.5
4.0
18.5
8.5
4
7
18.5
1.0
19.5
11.5
5
13
19.5
2.5
22.0
6.5
6
14
22.0
2.0
24.0
8.0
7
20
24.0
2.0
26.0
4.0
8
25
26.0
2.5
28.5
1.0
9
28
28.5
4.0
32.5
0.5
10
30
32.5
2.5
35.0
2.5

2
Time
Event
Customers in System
3
Cust 1 arrival
1
4
Cust 2 arrival
1 2
6
Cust 3 arrival
1 2 3
7
Cust 4 arrival
1 2 3 4
9
Cust 1 depart
2 3 4
13
Cust 5 arrival
2 3 4 5
14
Cust 6 arrival
2 3 4 5 6
14.5
Cust 2 depart
3 4 5 6
18.5
Cust 3 depart
4 5 6
19.5
Cust 4 depart
5 6
20
Cust 7 arrival
5 6 7
22
Cust 5 depart
6 7
24
Cust 6 depart
7
25
Cust 8 arrival
7 8
26
Cust 7 depart
8
28
Cust 9 arrival
8 9
28.5
Cust 8 depart
9
30
Cust 10 arrival
9 10
32.5
Cust 9 depart
10
35
Cust 10 depart
Thus, the last customer leaves at time 35.
(b) The average waiting time for the 10 customers is 4.75.
(c) The maximum number of customers in the system is 5 (between times 14 and
14.5).
(d) The average number of customers over the ﬁrst 30 minutes is calculated by
adding up all of the customer minutes from the second table — one customer
from times 3 to 4, two customers from times 4 to 6, etc.
1
30
Z 30
0
L(t) dt = 79.5
30
= 2.65

3
19–6. The following table gives a history for this (S, s) inventory system.
day
initial stock
customer order
end stock
reorder?
lost orders
1
20
10
10
no
0
2
10
6
4
yes
0
3
20
11
9
no
0
4
9
3
6
yes
0
5
20
20
0
yes
0
6
20
6
14
no
0
7
14
8
6
no
0
By using s = 6, we had no lost orders.
19–7.
(a) Here is the complete table for the generator.
i
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Xi
0
1
16
15
12
13
2
11
8
9
14
7
4
5
10
3
0
Thus, U1 = 1/16, U2 = 6/16.
(b) Yes (see the table).
(c) Since the generator cycles, we have X0 = X16 = · · · = X144 = 0.
Then
X145 = 1, X146 = 6, . . . , X150 = 2.
19–8.
(a) Using the algorithm given in Example 19–8 of the text, we ﬁnd that X1 =
1422014746 and X2 = 456328559. Since Ui = Xi/(231 −1), we have U1 =
0.6622, U2 = 0.2125.
19–9.
(a) X = −(1/2)ℓn(1 −U).
(b) X = −(1/2)ℓn(0.025) = 0.693.
19–10.
(a) Z = Φ−1(0.25) = −0.6745.
(b) X = µ + σZ = 1 + 3Z = −1.0235.
19–11. f(x) = |x/4|, −2 < x < 2.
(a) If −2 < x < 0, then F(x) =
Z x
−2
−t
4 dt = 1
2 −x2
8 .
If 0 < x < 2, then F(x) = 1
2 +
Z x
0
t
4 dt = 1
2 + x2
8 .

4
Thus, for 0 < U < 1/2, we set F(X) = 1
2 −X2
8 = U.
Solving, we get X = −
√
4 −8U.
For 1/2 < U < 1, we set F(X) = 1
2 + X2
8 = U.
Solving this time, we get X =
√
8U −4.
Recap:
X =
½ −
√
4 −8U,
0 < U < 1/2
√
8U −4
1/2 < U < 1
(b) X =
p
8(0.6) −4 = 0.894.
19–12.
(a)
x
p(x)
F(x)
U
−2.5
0.35
0.35
[0,0.35)
1.0
0.25
0.60
[0.35,0.60)
10.5
0.40
1.0
[0.60,1.0)
(b) U = 0.86 yields X = 10.5.
19–13.
(a) F(X) = 1 −e−(X/α)β = U.
Solving for X, we obtain X = α[−ℓn(1 −U)]1/β.
(b) X = (1.5)[−ℓn(0.34)]1/2 = 1.558.
19–14. We have
Z1 =
p
−2ℓn(0.45) cos(2π(0.12)) = 0.921
and
Z2 =
p
−2ℓn(0.45) sin(2π(0.12)) = 0.865.

5
19–15.
12
X
i=1
Ui −6 = 1.07.
19–16. Since the Xi’s are IID exponential(λ) random variables, we know that their m.g.f.
is
MXi(t) =
λ
λ −t,
t < λ,
i = 1, 2 . . . , n.
Then the m.g.f. of Y = Pn
i=1 Xi is
MY (t) =
n
Y
i=1
MXi(t) =
µ
λ
λ −t
¶n
.
We will be done as soon as we can show that this m.g.f. matches that corresponding
to the p.d.f. from Equation (19–4), namely,
MY (t)
=
Z ∞
0
etyλne−λyyn−1/(n −1)! dy
=
λn
(n −1)!
Z ∞
0
e−(λ−t)y yn−1 dy
=
λn
(n −1)!
Z ∞
0
e−u
µ
u
λ −t
¶n−1
du
λ −t
=
λn
(λ −t)n(n −1)!
Z ∞
0
e−u un−1 du
=
λn
(λ −t)n(n −1)! Γ(n)
=
µ
λ
λ −t
¶n
.
Since both versions of MY (t) match, that means that the two versions of Y must
come from the same distribution — and we are done.
19–17. X = −1
λ ℓn
µ n
Y
i=1
Ui
¶
= −1
3 ℓn((0.73)(0.11)) = 0.841.
19–18.
(a) To get a Bernoulli(p) random variable Xi, simply set
Xi =
½ 1,
if Ui ≤p
0,
if Ui > p

6
(Note that there are other allocations of the uniforms that will do the trick
just as well.)
(b) Suppose X1, X2, . . . , Xn are IID Bernoulli’s, generated according to (a). To
get a Binomial(n, p), let Y = Pn
i=1 Xi.
19–19. Suppose success (S) on trial i corresponds to Ui ≤0.25, and failure (F) corresponds
to Ui > 0.25. Then, from the sequence of uniforms in Problem 19–15, we have
FFFFS, i.e., we require X = 5 trials before observing the ﬁrst success.
19–20. The grand sample mean is
¯Zb = 1
b
b
X
i=1
Zi = 4,
while
ˆVB =
1
b −1
b
X
i=1
(Zi −¯Zb)2 = 1.
So the 90% batch means CI for µ is
µ
∈
¯Zb ± tα/2,b−1
q
ˆVB/b
=
4 ± t0.05,2
p
1/3
=
4 ± 2.92
p
1/3
=
4 ± 1.686
19–21. The 90% conﬁdence interval is of the form
[−2.5, 3.5]
=
¯X ± tα/2,b−1 y
=
0.5 ± t0.05,4 y
=
0.5 ± 2.132 y
Since the half-length of the CI is 3, we must have that y = 3/2.132 = 1.407.
The 95% CI will therefore be of the form
¯X ± t0.025,4 y = 0.5 ± (2.776)(1.407) = 0.5 ± 3.91 = [−3.41, 4.41].

7
19–23. The 95% batch means CI for µ is
µ
∈
¯Zb ± tα/2,b−1
q
ˆVB/b
=
100 ± t0.025,4
p
250/5
=
100 ± 2.776
√
50
=
100 ± 19.63
19–25.
(a) (i) Both exponential(1).
(ii) Cov(Ui, 1 −Ui) = −V (Ui) = −1/12.
(iii) Yes.
(b) After a little algebra,
V (( ¯Xn + ¯Yn)/2)
=
1
4
£
V ( ¯Xn) + V (¯Yn) + 2Cov( ¯Xn, ¯Yn)
¤
=
1
2
£
V ( ¯Xn) + Cov( ¯Xn, ¯Yn)
¤
=
1
2n [V (Xi) + Cov(Xi, Yi)]
≤
1
2nV (Xi)
=
V ( ¯X2n).
So the variance decreases compared to V ( ¯X2n).
(c) You get zero, which is the correct answer.
19–26.
(a) E(C) = E( ¯X) −E[k(Y −E(Y ))] = µ −[k(E(Y ) −E(Y ))] = µ.
(b) V (C) = V ( ¯X) + k2V (Y ) −2k Cov( ¯X, Y ). Comment: It would be nice, in
terms of minimizing variance, if k Cov( ¯X, Y ) > 0.
(c)
d
dkV (C)
=
2kV (Y ) −2 Cov( ¯X, Y ) = 0.
This implies that the critical (minimizing) point is
k = Cov( ¯X, Y )
V (Y )
.

8
Thus, the optimal variance is
V (C)
=
V ( ¯X) +
µCov( ¯X, Y )
V (Y )
¶2
V (Y ) −2
µCov( ¯X, Y )
V (Y )
¶
Cov( ¯X, Y )
=
V ( ¯X) −Cov2( ¯X, Y )
V (Y )
.
19–27. They are exponential(1).
19–28. They should look normal.
19–29. You should have a bivariate normal distribution (with correlation 0), centered at
zero with symmetric tails in all directions.


Klaus Schoeffmann 
Thanarat H. Chalidabhongse 
Chong Wah Ngo · Supavadee Aramvith 
Noel E. O’Connor · Yo-Sung Ho 
Moncef Gabbouj · Ahmed Elgammal (Eds.)
 123
LNCS 10705
24th International Conference, MMM 2018
Bangkok, Thailand, February 5–7, 2018
Proceedings, Part II
MultiMedia Modeling

Lecture Notes in Computer Science
10705
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zürich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7409

Klaus Schoeffmann
• Thanarat H. Chalidabhongse
Chong Wah Ngo
• Supavadee Aramvith
Noel E. O’Connor
• Yo-Sung Ho
Moncef Gabbouj
• Ahmed Elgammal (Eds.)
MultiMedia Modeling
24th International Conference, MMM 2018
Bangkok, Thailand, February 5–7, 2018
Proceedings, Part II
123

Editors
Klaus Schoeffmann
Alpen-Adria-Universität Klagenfurt
Klagenfurt
Austria
Thanarat H. Chalidabhongse
Chulalongkorn University
Bangkok
Thailand
Chong Wah Ngo
City University of Hong Kong
Hong Kong
China
Supavadee Aramvith
Chulalongkorn University
Bangkok
Thailand
Noel E. O’Connor
Dublin City University
Dublin
Ireland
Yo-Sung Ho
Gwangju Institute of Science
and Technology
Gwangju
Korea (Republic of)
Moncef Gabbouj
Tampere University of Technology
Tampere
Finland
Ahmed Elgammal
Rutgers University
Piscataway, NJ
USA
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-73599-3
ISBN 978-3-319-73600-6
(eBook)
https://doi.org/10.1007/978-3-319-73600-6
Library of Congress Control Number: 2017963755
LNCS Sublibrary: SL3 – Information Systems and Applications, incl. Internet/Web, and HCI
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
These proceedings contain the papers presented at MMM 2018, the 24th International
Conference on MultiMedia Modeling, held in Bangkok, Thailand, during February
5–7, 2018. MMM is a leading international conference for researchers and industry
practitioners to share new ideas, original research results, and practical development
experiences from all MMM-related areas, broadly falling into three categories: multi-
media content analysis; multimedia signal processing and communications; and mul-
timedia applications and services.
MMM 2018 received 185 submissions across four categories; 158 full research
paper submissions, six special session paper submissions, 12 demonstration submis-
sions, and nine submissions to the Video Browser Showdown (VBS 2018). Of the
submissions, 75% were from Asia, 17% from Europe, 5% from North America, 2%
from South America, and 1% each from Oceania and Africa.
Of the 158 full papers submitted, 46 were selected for oral presentation and 28 for
poster presentation, which equates to a 47% acceptance rate overall. Of the six special
session papers submitted, ﬁve were selected for oral presentation, which equates to a
83% acceptance rate overall. In addition, all 12 demonstrations and nine VBS sub-
missions were accepted. The overall acceptance percentage across the conference was
thus 54%, but 40% for full papers and 25% of full papers for oral presentation.
The submission and review process was coordinated using the EasyChair confer-
ence management system. All full-paper submissions were assigned for review to at
least three members of the Program Committee. We owe a debt of gratitude to all these
reviewers for providing their valuable time to MMM 2018.
We also wish to thank our organizational team: Special Session Chairs Suree
Pumrin and Benoit Huet; Demonstration Chairs Wolfgang Huerst, Joemon Jose, and
Suvit Nakpeerayuth; Video Browser Showdown Chairs Klaus Schoeffmann, Werner
Bailer, Cathal Gurrin, Jakub Lokoč, and Kunwadee Sripanidkulchai; Publicity Chairs
Kiyoharu Aizawa, Norliza Binti Mohd Noor, Celia Shahnaz, Karlsten Mueller, and
Alexander Loui; Finance Chair Nisachon Tangsangiumvisai; Sponsorship Chairs
Natawut Nupairoj and Widhayakorn Asdornwised; Registration Chairs Sukree
Sinthupinyo and Charnchai Pluempitiwiriyawej; Publication Chairs Twittie Senivongse
and Peerapon Vateekul; Local Arrangements Chairs Nakornthip Prompoon and Kultida
Rojviboonchai; Web Chair Krerk Piromsopa; Webmaster Apinun Intarachaiya; Con-
ference Secretariat Thittaporn Ganokratanaa, Tasaporn Intarachaiya, Itsara Wichakam,
Kankawin Kowsrihawat, and Araya Pudtal.
We would like to thank Chulalongkorn University for hosting MMM 2018. Finally,
special thanks go to our supporting team at Chulalongkorn University (Watchara
Ruengsang, Sirinthra Chantharaj, Pitchayut Chitsinpchayakun, Ammarin Jetthakun,
Kawin Liaowongphuthorn, Kissada Pornratthanapong, Chanatip Saetia, and Chavisa
Thamjarat), as well as to student volunteers, for all their contributions and valuable
support.

The accepted research contributions represent the state of the art in multimedia
modeling research and cover a very diverse range of topics. We wish to thank all
authors who spent their valuable time and effort to submit their work to MMM 2018.
And, ﬁnally, we thank all those who made the trip to Bangkok to attend MMM 2018
and VBS 2018.
February 2018
Supavadee Aramvith
Yo-Sung Ho
Noel E. O’Connor
Thanarat Chalidabhongse
Klaus Schoeffmann
Chong Wah Ngo
Moncef Gabbouj
Ahmed Elgammal
VI
Preface

Organization
MMM 2018 was organized by the Faculty of Engineering, Chulalongkorn University,
Thailand.
Steering Committee
Phoebe Chen (Chair)
La Trobe University, Australia
Tat-Seng Chua
National University of Singapore, Singapore
Kiyoharu Aizawa
University of Tokyo, Japan
Cathal Gurrin
Dublin City University, Ireland
Benoit Huet
Eurecom, France
R. Manmatha
University of Massachusetts, USA
Noel E. O’Connor
Dublin City University, Ireland
Klaus Schoeffmann
Alpen-Adria-Universität Klagenfurt, Austria
Yang Shiqiang
Tsinghua University, China
Cees G. M. Snoek
University of Amsterdam, The Netherlands
Meng Wang
Hefei University of Technology, China
Organizing Committee
Honorary Chairs
Supot Teachavorasinskun
Chulalongkorn University, Thailand
Ming-Ting Sun
University of Washington, USA
Tat-Seng Chua
National University of Singapore, Singapore
General Chairs
Supavadee Aramvith
Chulalongkorn University, Thailand
Yo-Sung Ho
Gwangju Institute of Science and Technology,
South Korea
Noel E. O’Connor
Dublin City University, Ireland
Program Chairs
Thanarat Chalidabhongse
Chulalongkorn University, Thailand
Klaus Schoeffmann
Alpen-Adria-Universität Klagenfurt, Austria
Chong Wah Ngo
City University of Hong Kong, SAR China
Moncef Gabbouj
Tempere University of Technology, Finland
Ahmed Elgammal
Rutgers University, USA

Special Session Chairs
Suree Pumrin
Chulalongkorn University, Thailand
Benoit Huet
Eurecom, France
Demonstration Chairs
Suvit Nakpeerayuth
Chulalongkorn University, Thailand
Wolfgang Huerst
Utrecht University, The Netherlands
Joemon Jose
University of Glasglow, UK
Video Browser Showdown Chairs
Kunwadee Sripanidkulchai
Chulalongkorn University, Thailand
Cathal Gurrin
Dublin City University, Ireland
Werner Bailer
Joanneum University, Austria
Klaus Schoeffmann
Alpen-Adria-Universität Klagenfurt, Austria
Jakub Lokoč
Charles University in Prague, Czech Republic
Publicity Chairs
Kiyoharu Aizawa
University of Tokyo, Japan
Norliza Binti Mohd Noor
Universiti Teknologi Malaysia, Malaysia
Celia Shahnaz
BUET, Bangladesh
Karlsten Mueller
Franhofer Heinrich Hertz Institute, Germany
Alexander Loui
Kodak Alaris, USA
Finance Chair
Nisachon Tangsangiumvisai
Chulalongkorn University, Thailand
Sponsorship Chairs
Natawut Nupairoj
Chulalongkorn University, Thailand
Widhayakorn Asdornwised
Chulalongkorn University, Thailand
Registration Chairs
Charnchai
Pluempitiwiriyawej
Chulalongkorn University, Thailand
Sukree Sinthupinyo
Chulalongkorn University, Thailand
Publication Chairs
Peerapon Vateekul
Chulalongkorn University, Thailand
Twittie Senivongse
Chulalongkorn University, Thailand
Local Arrangements Chairs
Nakornthip Prompoon
Chulalongkorn University, Thailand
Kultida Rojviboonchai
Chulalongkorn University, Thailand
VIII
Organization

Web Chair
Krerk Piromsopa
Chulalongkorn University, Thailand
Webmaster
Apinun Intarachaiya
Chulalongkorn University, Thailand
Conference Secretariat
Thittaporn Ganokratanaa
Chulalongkorn University, Thailand
Tasaporn Intarachaiya
Chulalongkorn University, Thailand
Itsara Wichakam
Chulalongkorn University, Thailand
Kankawin Kowsrihawat
Chulalongkorn University, Thailand
Araya Pudtal
Chulalongkorn University, Thailand
Special Session Organizers
SS: Multimedia Analytics: Perspectives, Techniques, and Applications
Laurent Amsaleg
IRISA Lab, France
Cathal Gurrin
Dublin City University, Ireland
Björn Þór Jónsson
IT University of Copenhagen, Denmark
Stevan Rudinac
University of Amsterdam, The Netherlands
Program Committee
Thumrongrat Amornraksa
KMUTT, Thailand
Laurent Amsaleg
IRISA Lab, France
Le An
University of North Carolina at Chapel Hill, USA
Ognjen Arandjelovic
University of St. Andrews, UK
Sansanee Auephanwiriyakul
Chiang Mai University, Thailand
Esra Açar
Middle East Technical University, Germany
Werner Bailer
Joanneum Research, Austria
Amr M. Bakry
Alexandria University, USA
Ilaria Bartolini
University of Bologna, Italy
Jenny Benois-Pineau
LaBRI, University of Bordeaux, France
Benjamin Bustos
University of Chile, Chile
K. Selcuk Candan
Arizona State University, USA
Premysl Cech
Charles University, Czech Republic
Thanarat Chalidabhongse
Chulalongkorn University, Thailand
Savvas Chatzichristoﬁs
Neapolis University Pafos, Cyprus
Edgar Chavez
CICESE, Mexico
Zhineng Chen
Chinese Academy of Sciences, China
Wen-Huang Cheng
Academia Sinica, Taiwan
Wei-Ta Chu
National Chung Cheng University, Taiwan
Tat-Seng Chua
National University of Singapore, Singapore
Vincent Claveau
IRISA – CNRS, France
Kathy Clawson
University of Sunderland, UK
Organization
IX

Claudiu Cobarzan
Alpen-Adria Universität Klagenfurt, Austria
Michel Crucianu
CNAM, France
Rossana Damiano
Università di Torino, Italy
Duc Tien Dang Nguyen
Dublin City University, Ireland
Petros Daras
Information Technologies Institute, Greece
Wesley De Neve
Ghent University – iMinds, KAIST, Belgium
Jianping Fan
UNC Charlotte, USA
Weijie Fu
Hefei University of Technology, China
Lianli Gao
University of Electronic Science and Technology
of China, China
Ivan Giangreco
University of Basel, Switzerland
Guillaume Gravier
CNRS, IRISA, France
Ziyu Guan
Northwest University of China, China
Silvio Guimaraes
Pontifícia Universidade Católica de Minas Gerais,
Brazil
Cathal Gurrin
Dublin City University, Ireland
Shijie Hao
Hefei University of Technology, China
Nicolas Hervé
INA, France
Richang Hong
Hefei University of Technology, China
Frank Hopfgartner
University of Glasgow, UK
Michael E. Houle
National Institute of Informatics, Japan
Jen-Wei Huang
National Cheng Kung University, Taiwan
Wolfgang Huerst
Utrecht University, The Netherlands
Ichiro Ide
Nagoya University, Japan
Yuji Iwahori
Chubu University, Japan
Adam Jatowt
Kyoto University, Japan
Yu-Gang Jiang
Fudan University, China
Peiguang Jing
Tianjin University, China
Björn Thór Jónsson
IT University of Copenhagen, Denmark
Sabrina Kletz
Alpen-Adria-Universität Klagenfurt, Austria
Thananop Kobchaisawat
Chulalongkorn University, Thailand
Yiannis Kompatsiaris
CERTH – ITI, Greece
Markus Koskela
CSC, IT Center for Science Ltd., Finland
Worapan Kusakunniran
Mahidol University, Thailand
Michael S. Lew
Leiden University, The Netherlands
Yingbo Li
Institut Eurecom, France
Xueliang Liu
Hefei University of Tech, China
Jakub Lokoč
Charles University in Prague, Czech Republic
Stephane Marchand-Maillet
University of Geneva, Switzerland
Jean Martinet
CRIStAL, Lille 1 University, France
Sanparith Marukatat
NECTEC, Thailand
Kevin Mcguinness
Dublin City University, Ireland
Robert Mertens
HSW University of Applied Sciences, Germany
Vasileios Mezaris
Informatics and Telematics Institute/Centre
for Research and Technology Hellas, Greece
Dalibor Mitrovic
Vienna University of Technology, Austria
X
Organization

Bernd Muenzer
Alpen-Adria-Universität Klagenfurt, Austria
Phivos Mylonas
National Technical University of Athens, Greece
Henning Müller
HES-SO, Switzerland
Liqiang Nie
National University of Singapore, Singapore
Naoko Nitta
Osaka University, Japan
Neil O’Hare
Yahoo! Spain
Tse-Yu Pan
National Cheng Kung University, Taiwan
Sang Phan
National Institute of Informatics, Japan
Manfred Jürgen Primus
Alpen-Adria-Universität, Austria
Yannick Prié
LINA, University of Nantes, France
Jianjun Qian
Nanjing University of Science and Technology, China
Miloš Radovanović
University of Novi Sad, Serbia
Michael Riegler
Simula Research Laboratory, Norway
Luca Rossetto
University of Basel, Switzerland
Puripant Ruchikachorn
Chulalongkorn University, Thailand
Stevan Rudinac
University of Amsterdam, The Netherlands
Mukesh Saini
Indian Institute of Technology Ropar, India
Shin’Ichi Satoh
National Institute of Informatics, Japan
Klaus Schoeffmann
Alpen-Adria-Universität Klagenfurt, Austria
Heiko Schuldt
University of Basel, Switzerland
Jie Shao
University of Electronic Science and Technology
of China, China
Xiaobo Shen
Nanjing University of Science and Technology, China
Koichi Shinoda
The University of Tokyo, Japan
Mei-Ling Shyu
University of Miami, USA
Tomas Skopal
Charles University, Czech Republic
Alan Smeaton
Dublin City University, Ireland
Lifeng Sun
Tsinghua University, China
Sheng Tang
Chinese Academy of Sciences, China
Shuhei Tarashima
NTT, Japan
El-Gaaly Tarek
Voyage, USA
Georg Thallinger
Joanneum Research, Austria
Christian Timmerer
Alpen-Adria-Universität Klagenfurt, ITEC – MMC,
Austria
Shingo Uchihashi
Fuji Xerox Co., Ltd., Japan
Lucia Vadicamo
ISTI-CNR, Italy
Marie-Luce Viaud
INA, France
Lai Kuan Wong
Multimedia University, Malaysia
Marcel Worring
University of Amsterdam, The Netherlands
Hong Wu
UESTC, China
Xiao Wu
Southwest Jiaotong University, China
Toshihiko Yamasaki
The University of Tokyo, Japan
Keiji Yanai
The University of Electro-Communications, Japan
Ting Yao
Microsoft, China
Matthias Zeppelzauer
University of Applied Sciences St. Pölten, Austria
Hanwang Zhang
Columbia University, USA
Organization
XI

Wei Zhang
Chinese Academy of Sciences, China
Wan-Lei Zhao
Xiamen University, China
Marissa Zhou
Dublin City University, Ireland
Xiaofeng Zhu
Guangxi Normal University, China
Arthur Zimek
University of Southern Denmark, Denmark
Roger Zimmermann
National University of Singapore, Singapore
External Reviewers
Konstantinos Avgerinakis
Information Technologies Institute, Greece
Long Chen
Northwest University of China, China
Klitos Christodoulou
Neapolis University of Pafos, Cyprus
Hannes Fassold
Joanneum Research, Austria
Frank Hopfgartner
The University of Glasgow, UK
Andreas Leibetseder
Alpen-Adria Universität Klagenfurt, Austria
Xinhui Li
Tianjin University, China
Meng Liu
Shandong University China
Eva Mohedano
Insight Centre for Data Analytics, Ireland
Eleftherios
Spyromitros-Xiouﬁs
Information Technologies Institute, Greece
Filareti Tsalakanidou
Information Technologies Institute, Greece
Stefanie Wechtitsch
Joanneum Research, Austria
Wolfgang Weiss
Joanneum Research, Austria
Jun Zhang
University of North Carolina at Chapel Hill, USA
Yihong Zhang
Kyoto University, Japan
Wanqing Zhao
Northwest University, China
XII
Organization

Sponsors
Chulalongkorn University
Faculty of Engineering, Chulalongkorn University
Thailand Convention & Exhibition Bureau
Springer Publishing
Organization
XIII

Contents – Part II
Full Papers Accepted for Poster Presentation
A New Accurate Image Denoising Method Based on Sparse
Coding Coefficients. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Kai Lin, Ge Li, Yiwei Zhang, and Jiaxing Zhong
A Novel Frontal Facial Synthesis Algorithm Based on Individual
Residual Face . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
Xin Ding, Ruimin Hu, Zhen Han, and Zhongyuan Wang
A Text Recognition and Retrieval System for e-Business
Image Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Jiang Zhou, Kevin McGuinness, and Noel E. O’Connor
Accurate Detection for Scene Texts with a Cascaded CNN Networks . . . . . .
36
Jianjun Li, Chenyan Wang, Zhenxing Luo, Zhuo Tang, and Haojie Li
Cloud of Line Distribution and Random Forest Based Text Detection
from Natural/Video Scene Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Wenhai Wang, Yirui Wu, Palaiahnakote Shivakumara, and Tong Lu
CNN-Based DCT-Like Transform for Image Compression . . . . . . . . . . . . . .
61
Dong Liu, Haichuan Ma, Zhiwei Xiong, and Feng Wu
Coarse-to-Fine Image Super-Resolution Using Convolutional
Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Liguo Zhou, Zhongyuan Wang, Shu Wang, and Yimin Luo
Data Augmentation for EEG-Based Emotion Recognition
with Deep Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . .
82
Fang Wang, Sheng-hua Zhong, Jianfeng Peng, Jianmin Jiang,
and Yan Liu
Domain Invariant Subspace Learning for Cross-Modal Retrieval . . . . . . . . . .
94
Chenlu Liu, Xing Xu, Yang Yang, Huimin Lu, Fumin Shen,
and Yanli Ji
Effective Action Detection Using Temporal Context and Posterior
Probability of Length. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
Xinran Liu, Yan Song, and Jinhui Tang

Efficient Two-Layer Model Towards Cover Song Identification . . . . . . . . . .
118
Xiaoshuo Xu, Yao Cheng, Xiaoou Chen, and Deshun Yang
Food Photo Recognition for Dietary Tracking: System and Experiment . . . . .
129
Zhao-Yan Ming, Jingjing Chen, Yu Cao, Ciarán Forde,
Chong-Wah Ngo, and Tat Seng Chua
Fusion Networks for Air-Writing Recognition. . . . . . . . . . . . . . . . . . . . . . .
142
Buntueng Yana and Takao Onoye
Global and Local C3D Ensemble System for First Person Interactive
Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Lingling Fa, Yan Song, and Xiangbo Shu
Implicit Affective Video Tagging Using Pupillary Response. . . . . . . . . . . . .
165
Dongdong Gui, Sheng-hua Zhong, and Zhong Ming
k-Labelsets for Multimedia Classification with Global and Local
Label Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
Yan Yan, Shining Li, Xiao Zhang, Anyi Wang, Zhigang Li,
and Jingyu Zhang
LVFS: A Lightweight Video Storage File System for IP Camera-Based
Surveillance Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Chong Wang, Ke Zhou, Zhongying Niu, Ronglei Wei, and Hongwei Li
Person Re-id by Incorporating PCA Loss in CNN. . . . . . . . . . . . . . . . . . . .
200
Kaixuan Zhang, Yang Xu, Li Sun, Song Qiu, and Qingli Li
Robust and Real-Time Visual Tracking Based
on Complementary Learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Xingzhou Luo, Dapeng Du, and Gangshan Wu
Room Floor Plan Generation on a Project Tango Device . . . . . . . . . . . . . . .
226
Vincent Angladon, Simone Gasparini, and Vincent Charvillat
Scalable Bag of Selected Deep Features for Visual Instance Retrieval . . . . . .
239
Yue Lv, Wengang Zhou, Qi Tian, and Houqiang Li
SeqSense: Video Recommendation Using Topic Sequence Mining . . . . . . . .
252
Chidansh Bhatt, Matthew Cooper, and Jian Zhao
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
Based on Autoencoder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
Yunjie Wu, Zhengxing Sun, Youcheng Song, and Hongyan Li
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding . . . . .
277
Zhenhua Tang, Sunguo Huang, and Hongbo Jiang
XVI
Contents – Part II

SRN: The Movie Character Relationship Analysis via Social Network. . . . . .
289
Jingmeng He, Yuxiang Xie, Xidao Luan, Lili Zhang, and Xin Zhang
The Long Tail of Web Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
Luca Rossetto and Heiko Schuldt
Vehicle Semantics Extraction and Retrieval for Long-Term Carpark
Video Surveillance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
Clarence Weihan Cheong, Ryan Woei-Sheng Lim, John See,
Lai-Kuan Wong, Ian K. T. Tan, and Azrin Aris
Venue Prediction for Social Images by Exploiting Rich Temporal Patterns
in LBSNs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
Jingyuan Chen, Xiangnan He, Xuemeng Song, Hanwang Zhang,
Liqiang Nie, and Tat-Seng Chua
Demonstrations
A Virtual Reality Interface for Interactions with Spatiotemporal 3D Data . . . .
343
Hunter Quant, Sean Banerjee, and Natasha Kholgade Banerjee
ActionVis: An Explorative Tool to Visualize Surgical Actions in
Gynecologic Laparoscopy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
Stefan Petscharnig and Klaus Schoeffmann
AR DeepCalorieCam: An iOS App for Food Calorie Estimation
with Augmented Reality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
Ryosuke Tanno, Takumi Ege, and Keiji Yanai
Auto Accessory Segmentation and Interactive Try-on System . . . . . . . . . . . .
357
Yi-Xuan Zeng, Yu-Hang Kuo, and Hsu-Yung Cheng
Automatic Smoke Classification in Endoscopic Video . . . . . . . . . . . . . . . . .
362
Andreas Leibetseder, Manfred Jürgen Primus, and Klaus Schoeffmann
Depth Representation of LiDAR Point Cloud with Adaptive Surface
Patching for Object Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
Kanokphan Lertniphonphan, Satoshi Komorita, Kazuyuki Tasaka,
and Hiromasa Yanagihara
ImageX - Explore and Search Local/Private Images. . . . . . . . . . . . . . . . . . .
372
Nico Hezel, Kai Uwe Barthel, and Klaus Jung
Lifelog Exploration Prototype in Virtual Reality . . . . . . . . . . . . . . . . . . . . .
377
Aaron Duane and Cathal Gurrin
Contents – Part II
XVII

Multi-camera Microenvironment to Capture Multi-view Time-Lapse
Videos for 3D Analysis of Aging Objects . . . . . . . . . . . . . . . . . . . . . . . . .
381
Lintao Guo, Hunter Quant, Nikolas Lamb, Benjamin Lowit,
Natasha Kholgade Banerjee, and Sean Banerjee
Ontlus: 3D Content Collaborative Creation via Virtual Reality . . . . . . . . . . .
386
Chien-Wen Chen, Jain-Wei Peng, Chia-Ming Kuo, Min-Chun Hu,
and Yuan-Chi Tseng
Programmatic 3D Printing of a Revolving Camera Track to Automatically
Capture Dense Images for 3D Scanning of Objects . . . . . . . . . . . . . . . . . . .
390
Nikolas Lamb, Natasha Kholgade Banerjee, and Sean Banerjee
Video Browsing on a Circular Timeline. . . . . . . . . . . . . . . . . . . . . . . . . . .
395
Bernd Münzer and Klaus Schoeffmann
Video Browser Showdown
Competitive Video Retrieval with vitrivr . . . . . . . . . . . . . . . . . . . . . . . . . .
403
Luca Rossetto, Ivan Giangreco, Ralph Gasser, and Heiko Schuldt
Enhanced VIREO KIS at VBS 2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Phuong Anh Nguyen, Yi-Jie Lu, Hao Zhang, and Chong-Wah Ngo
Fusing Keyword Search and Visual Exploration for Untagged Videos . . . . . .
413
Kai Uwe Barthel, Nico Hezel, and Klaus Jung
Revisiting SIRET Video Retrieval Tool . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
Jakub Lokoč, Gregor Kovalčík, and Tomáš Souček
Sketch-Based Similarity Search for Collaborative Feature Maps . . . . . . . . . .
425
Andreas Leibetseder, Sabrina Kletz, and Klaus Schoeffmann
Sloth Search System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
431
Sitapa Rujikietgumjorn, Nattachai Watcharapinchai,
and Sanparith Marukatat
The ITEC Collaborative Video Search System at the Video Browser
Showdown 2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
Manfred Jürgen Primus, Bernd Münzer, Andreas Leibetseder,
and Klaus Schoeffmann
VERGE in VBS 2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
444
Anastasia Moumtzidou, Stelios Andreadis, Foteini Markatopoulou,
Damianos Galanopoulos, Ilias Gialampoukidis, Stefanos Vrochidis,
Vasileios Mezaris, Ioannis Kompatsiaris, and Ioannis Patras
XVIII
Contents – Part II

Video Search Based on Semantic Extraction and Locally Regional
Object Proposal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
451
Thanh-Dat Truong, Vinh-Tiep Nguyen, Minh-Triet Tran,
Trang-Vinh Trieu, Tien Do, Thanh Duc Ngo, and Dinh-Duy Le
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
457
Contents – Part II
XIX

Contents – Part I
Full Papers Accepted for Oral Presentation
A Markov Network Based Passage Retrieval Method for Multimodal
Question Answering in the Cultural Heritage Domain . . . . . . . . . . . . . . . . .
3
Shurong Sheng, Aparna Nurani Venkitasubramanian,
and Marie-Francine Moens
A Method of Weather Radar Echo Extrapolation Based on Convolutional
Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
En Shi, Qian Li, Daquan Gu, and Zhangming Zhao
A Motion-Driven Approach for Fine-Grained Temporal Segmentation
of User-Generated Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
Konstantinos Apostolidis, Evlampios Apostolidis, and Vasileios Mezaris
A Novel 3D Human Action Recognition Framework for Video
Content Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
Lianglei Wei, Yirui Wu, Wenhai Wang, and Tong Lu
Adaptive Image Representation Using Information Gain and Saliency:
Application to Cultural Heritage Datasets. . . . . . . . . . . . . . . . . . . . . . . . . .
54
Dorian Michaud, Thierry Urruty, François Lecellier,
and Philippe Carré
AGO: Accelerating Global Optimization for Accurate Stereo Matching . . . . .
67
Peng Yao, Hua Zhang, Yanbing Xue, and Shengyong Chen
An RNN-Based Speech-Music Discrimination Used for Hybrid
Audio Coder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Wanzhao Yang, Weiping Tu, Jiaxi Zheng, Xiong Zhang,
Yuhong Yang, and Yucheng Song
Co-occurrent Structural Edge Detection for Color-Guided Depth Map
Super-Resolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Jiang Zhu, Wei Zhai, Yang Cao, and Zheng-Jun Zha
Collision-Free LSTM for Human Trajectory Prediction . . . . . . . . . . . . . . . .
106
Kaiping Xu, Zheng Qin, Guolong Wang, Kai Huang,
Shuxiong Ye, and Huidi Zhang
Convolution with Logarithmic Filter Groups for Efficient Shallow CNN . . . .
117
Tae Kwan Lee, Wissam J. Baddar, Seong Tae Kim, and Yong Man Ro

Cost-Sensitive Deep Metric Learning for Fine-Grained Image
Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
Junjie Zhao and Yuxin Peng
Crowd Distribution Estimation with Multi-scale Recursive Convolutional
Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
Meng Wei, Yu Kang, Weiguo Song, and Yang Cao
Deep Convolutional Neural Network for Correlating Images
and Sentences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
Yuhua Jia, Liang Bai, Peng Wang, Jinlin Guo, and Yuxiang Xie
Deep Pedestrian Detection Using Contextual Information
and Multi-level Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
Weijie Kong, Nannan Li, Thomas H. Li, and Ge Li
Dual-Way Guided Depth Image Inpainting with RGBD Image Pairs . . . . . . .
178
Hua Yuan, Yuanyuan Zhou, Yun Sheng, and Guixu Zhang
Efficient and Interactive Spatial-Semantic Image Retrieval . . . . . . . . . . . . . .
190
Ryosuke Furuta, Naoto Inoue, and Toshihiko Yamasaki
Evaluation of Visual Content Descriptors for Supporting Ad-Hoc Video
Search Tasks at the Video Browser Showdown. . . . . . . . . . . . . . . . . . . . . .
203
Sabrina Kletz, Andreas Leibetseder, and Klaus Schoeffmann
Find Me a Sky: A Data-Driven Method for Color-Consistent Sky Search
and Replacement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
Saumya Rawat, Siddhartha Gairola, Rajvi Shah, and P. J. Narayanan
Font Recognition in Natural Images via Transfer Learning . . . . . . . . . . . . . .
229
Yizhi Wang, Zhouhui Lian, Yingmin Tang, and Jianguo Xiao
Frame-Based Classification of Operation Phases in Cataract
Surgery Videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
Manfred Jüergen Primus, Doris Putzgruber-Adamitsch,
Mario Taschwer, Bernd Münzer, Yosuf El-Shabrawi,
Laszlo Böszörmenyi, and Klaus Schoeffmann
High-Precision 3D Coarse Registration Using RANSAC
and Randomly-Picked Rejections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
Jong-Hee Back, Sunho Kim, and Yo-Sung Ho
Image Aesthetic Distribution Prediction with Fully
Convolutional Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
Huidi Fang, Chaoran Cui, Xiang Deng, Xiushan Nie,
Muwei Jian, and Yilong Yin
XXII
Contents – Part I

Improving the Quality of Video-to-Language Models by Optimizing
Annotation of the Training Material . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Laura Pérez-Mayos, Federico M. Sukno, and Leo Wanner
Iterative Active Classification of Large Image Collection . . . . . . . . . . . . . . .
291
Mofei Song, Zhengxing Sun, Bo Li, and Jiagao Hu
Learning to Index in Large-Scale Datasets . . . . . . . . . . . . . . . . . . . . . . . . .
305
Amorntip Prayoonwong, Cheng-Hsien Wang, and Chih-Yi Chiu
Light Field Foreground Matting Based on Defocus and Correspondence . . . .
317
Jianshe Zhou, Tuya Naren, Xianyu Chen, Yike Ma, Jie Liu,
and Feng Dai
LOCO: Local Context Based Faster R-CNN for Small Traffic
Sign Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
Peng Cheng, Wu Liu, Yifan Zhang, and Huadong Ma
Multi-hypothesis-Based Error Concealment for Whole Frame Loss
in HEVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
Yongfei Zhang and Zhe Li
Multi-stream Fusion Model for Social Relation Recognition from Videos. . . .
355
Jinna Lv, Wu Liu, Lili Zhou, Bin Wu, and Huadong Ma
Multimodal Augmented Reality – Augmenting Auditory-Tactile Feedback
to Change the Perception of Thickness . . . . . . . . . . . . . . . . . . . . . . . . . . .
369
Geert Lugtenberg, Wolfgang Hürst, Nina Rosa, Christian Sandor,
Alexander Plopski, Takafumi Taketomi, and Hirokazu Kato
Parameter Selection for Denoising Algorithms Using NR-IQA
with CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
Jianjun Li, Lanlan Xu, Haojie Li, Chin-chen Chang, and Fuming Sun
Real-Time Polyps Segmentation for Colonoscopy Video Frames Using
Compressed Fully Convolutional Network . . . . . . . . . . . . . . . . . . . . . . . . .
393
Itsara Wichakam, Teerapong Panboonyuen, Can Udomcharoenchaikit,
and Peerapon Vateekul
Recursive Pyramid Network with Joint Attention
for Cross-Media Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
Yuxin Yuan and Yuxin Peng
Reinforcing Pedestrian Parsing on Small Scale Dataset . . . . . . . . . . . . . . . .
417
Qi Zheng, Jun Chen, Junjun Jiang, and Ruimin Hu
Remote Sensing Image Fusion Based on Two-Stream Fusion Network . . . . .
428
Xiangyu Liu, Yunhong Wang, and Qingjie Liu
Contents – Part I
XXIII

REVT: Robust and Efficient Visual Tracking by Region-Convolutional
Regression Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
440
Peng Wu, Di Huang, and Yunhong Wang
Shallow-Water Image Enhancement Using Relative Global Histogram
Stretching Based on Adaptive Parameter Acquisition . . . . . . . . . . . . . . . . . .
453
Dongmei Huang, Yan Wang, Wei Song, Jean Sequeira,
and Sébastien Mavromatis
Spatiotemporal 3D Models of Aging Fruit from Multi-view
Time-Lapse Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
466
Lintao Guo, Hunter Quant, Nikolas Lamb, Benjamin Lowit,
Sean Banerjee, and Natasha Kholgade Banerjee
Stitch-Based Image Stylization for Thread Art Using Sparse Modeling . . . . .
479
Kewei Yang, Zhengxing Sun, Shuang Wang, and Bo Li
Teacher and Student Joint Learning for Compact Facial Landmark
Detection Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
493
Hong Joo Lee, Wissam J. Baddar, Hak Gu Kim, Seong Tae Kim,
and Yong Man Ro
Text Image Deblurring via Intensity Extremums Prior . . . . . . . . . . . . . . . . .
505
Zhengcai Qin, Bin Wu, and Meng Li
The CAMETRON Lecture Recording System: High Quality Video
Recording and Editing with Minimal Human Supervision . . . . . . . . . . . . . .
518
Dries Hulens, Bram Aerts, Punarjay Chakravarty, Ali Diba,
Toon Goedemé, Tom Roussel, Jeroen Zegers, Tinne Tuytelaars,
Luc Van Eycken, Luc Van Gool, Hugo Van Hamme, and Joost Vennekens
Towards Demographic-Based Photographic Aesthetics Prediction
for Portraitures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
Magzhan Kairanbay, John See, and Lai-Kuan Wong
Triplet Convolutional Network for Music Version Identification . . . . . . . . . .
544
Xiaoyu Qi, Deshun Yang, and Xiaoou Chen
Two-Level Segment-Based Bitrate Control for Live ABR Streaming . . . . . . .
556
Yujing Chen, Jing Xiao, Gen Zhan, Xu Wang, and Zhongyuan Wang
Uyghur Text Localization with Fast Component Detection . . . . . . . . . . . . . .
565
Jianjun Chen, Hongtao Xie, Yue Hu, and Chenggang Yan
SS: Multimedia Analytics: Perspectives, Techniques and Applications
Approaches for Event Segmentation of Visual Lifelog Data . . . . . . . . . . . . .
581
Rashmi Gupta and Cathal Gurrin
XXIV
Contents – Part I

Category Specific Post Popularity Prediction . . . . . . . . . . . . . . . . . . . . . . .
594
Masoud Mazloom, Iliana Pappi, and Marcel Worring
Image Aesthetics and Content in Selecting Memorable Keyframes
from Lifelogs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
608
Feiyan Hu and Alan F. Smeaton
On the Traceability of Results from Deep Learning-Based
Cloud Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
620
Werner Bailer
Rethinking Summarization and Storytelling for Modern
Social Multimedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
632
Stevan Rudinac, Tat-Seng Chua, Nicolas Diaz-Ferreyra,
Gerald Friedland, Tatjana Gornostaja, Benoit Huet, Rianne Kaptein,
Krister Lindén, Marie-Francine Moens, Jaakko Peltonen, Miriam Redi,
Markus Schedl, David A. Shamma, Alan Smeaton, and Lexing Xie
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
645
Contents – Part I
XXV

Full Papers Accepted for Poster
Presentation

A New Accurate Image Denoising Method
Based on Sparse Coding Coeﬃcients
Kai Lin, Ge Li(B), Yiwei Zhang, and Jiaxing Zhong
Digital Media R&D Center, SECE, Shenzhen Graduate School,
Peking University, Shenzhen, China
1701213612@sz.pku.edu.cn, geli@ece.pku.edu.cn,
{yuriyzhang,jxzhong}@pku.edu.cn
Abstract. Although sparse coding error has been introduced to improve
the performance of sparse representation-based image denoising, how-
ever, the sparse coding noise is not tight enough. To suppress the sparse
coding noise, we exploit a couple of images to estimate unknown sparse
code. There are two main contributions in this paper: The ﬁrst is to use
a reference denoised image and an intermediate denoised image to esti-
mate the sparse coding coeﬃcients of the original image. The second is
that we set a threshold to rule out blocks of low similarity to improve the
accuracy of estimation. Our experimental results have shown improve-
ments over several state-of-the-art denoising methods on a collection of
12 generic natural images.
Keywords: Keywords-image denoising · Sparse representation model
Sparse coding coeﬃcients · Sparse coding noise · Noise level
1
Introduction
Image denoising is an important tool for many applications, such as remote sens-
ing imaging, object recognition and medical imaging, etc. As previous literatures
supposed, the noise in the contaminated image obeys independent identically
distributed Gaussian. In this paper, we mainly focus on the zero mean additive
white Gaussian noise (AWGN). The problem of image denoising can be generally
formulated by
y = x + n
where y is the observed corrupted image vector, x is the original image vector and
n is the AWGN vector with a standard deviation σ. Owing to the ill-posed nature
of image denoising, it is of great importance to make a good assumption about
original image, and many prior representation models have been introduced.
Traditional image denoising methods can be divided into transform domain,
spatial domain, and dictionary learning based method. Transform domain based
methods transform image into other domains and image patches are represented
by the orthonormal basis(e.g., contourlets [1] and wavelets [2]). Portilla et al. [3]
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 3–13, 2018.
https://doi.org/10.1007/978-3-319-73600-6_1

4
K. Lin et al.
Fig. 1. Image denoising performance comparison on woman image (σ = 20). From
left to right and top to bottom: original image, noise image, the reconstructed images
by SAPCA-BM3D (PSNR = 32.10 dB), CSR (PSNR = 32.59 dB), NCSR (PSNR =
32.65 dB), and our proposed method (PSNR = 33.01 dB).
use mixture of Gaussians to model the wavelet coeﬃcients. Donoho [4] devides
image into some wavelet subbands and then applies soft-thresholding to the
coeﬃcients. Spatial domain based methods apply similarities between patches
in an image. For a given patch, a series of similar patches will be searched to
denoise the patch. According to the searching scope, Spatial domain methods
can be categorized as local ﬁlters (e.g., TF [5] and SKR [6]) and nonlocal ﬁlters
(e.g., NLM [7] and INLM [8]). Recently, Xiong et al. [9] exploit the nonlocal
correlation in images to estimate the expectation and variance, thus achieving
a good denoising eﬀect. Ma et al. [10] discuss their vision for the future of in-
loop ﬁlter research by exploring the potential of non-local similarities. Dictionary
learning based method apply each patch in the image. The sparsity-based reg-
ularization has been used to achieve good results. They perform denoising by
letting each patch in the denoised image expressed as a linear combination of
only a few atoms from a redundant dictionary. Representative methods include
K-SVD [11], LSSC [12], and CSR [13]. Owing to the fact that dictionary learn-
ing is highly non-convex [14], Chen et al. propose PCLR [15] which employs the
GMMs learned from clean images to guide patch clustering of the input noisy
images.
Our proposed method is established on the sparsity-based model. NCSR [16]
has consider sparse coding error (the diﬀerence between the sparse coding of
original image and denoised image). Owing to the fact that x is unknown, they

A New Accurate Image Denoising Method
5
Fig. 2. The iteration process of our method
make use of the self-similarity of intermediate denoised image to estimate the
sparse coding of original image αx. However, the estimation of αx is not accurate
enough, which will result in over-smoothing. In this paper we focus on improving
the estimation accuracy of αx. Our proposed model is composed of two stages:
In the ﬁrst step, we use a reference denoised image and an intermediate denoised
image to compute weights of similar blocks simultaneously. In the second step,
we set the threshold ε to rule out low similar blocks, where ε is determined by
the noise level σ adaptively. The extensive experiments demonstrate that our
proposed method outperforms several state-of-the-art denoising methods.
The rest of this paper is organized as follows. Section 2 presents the details
of our method. Section 3 presents the experimental results. The conclusion is
drawn in Sect. 4.
2
The Proposed Method
We denote an image as a vector x ∈ℜM acquired by reforming pixels of the
corresponding matrix in lexicographical order. Let a patch be represented as
vector xi ∈ℜN2, which is a √n × √n size block at location i from x through
xi = Rix, where Ri is a matrix extracting pixels in the patch from the image.
In order to suppress the block artifacts, patches are overlapped and redundant
patch-based representation are used in this paper.
Based on the assumption of sparse coding, xi can be sparsely represented
from D ∈ℜn×M(n ≪M) as xi ≈Dαi
x, where D ∈ℜN×M(N ≪M) is an over-
complete dictionary, and α is a sparse coding coeﬃcient vector, whose elements
are zero or close to zero. Given a dictionary D, the optimization process is as
follows:
min
α ∥α∥0 s.t. ∥x −Dα∥2 ≤ε
(1)
In the task of image denoising, only noise image y is available, thus we obtain
the sparse coding of y as follows:
αy = arg min
α {∥y −Dα∥2
2 + γ

i
∥αi −βi∥1}
(2)

6
K. Lin et al.
where (αy −β) is the sparse coding noise, β is the estimation of αx and constant
γ denotes the regularization parameter. Then we reconstruct x by ˆx = Dαy
under the assumption of αx ≈αy. We get better denoising eﬀect than NCSR by
improving the estimation accuracy of β. In the following section we will detail
how to estimate β and improve its accuracy.
2.1
Estimation of Unknown Sparse Code
Based on the fact that an image contains a lot of nonlocal redundancies and
strong nonlocal correlation between the sparse coding coeﬃcients exists, NCSR
obtain β from intermediate denoised image (I im) directly. I im is the interme-
diate iteration denoising results of algorithm. However, owing to the fact that
an image y is disturbed by noise, in the ﬁrst few iterations of the algorithm,
I im is far from the original image x. Therefore, the estimation of β is not accu-
rate enough, resulting in αy deviate from αx. To solve this problem, we use an
intermediate denoised image (I im) and a reference denoised image (R im) to
estimate β simultaneously, where R im is an image preliminarily denoised by
some kinds of denoising algorithm (in this paper we adopt CSR).
Fig. 3. The restored image quality
at each iteration on the house image
(σ = 20)
Fig. 4. An scatter plot between sigma
and epsilon
Figure 3 shows the restored image quality at each iteration. By comparison,
we ﬁnd that in the ﬁrst few iterations, the PSNR value of the R im is higher
(closer to the original image) than I im. However, after some iterations, the
PSNR of I im is higher than R im. The same trend is observed in other test
images. To make full use of R im and I im simultaneously, we estimate β based
on the two images. Given a patch I imi and a patch R imi, where R imi is
the patch with the same position as I imi. We search I imi’s nonlocally similar
patches in a large window centered at pixel i in I im and congregate them by set
Φi. Similarly, we obtain R imi’s similar patches Ωi. Assume the sparse coding
of Φi’s pth patch I imi,p denoted by αi,p
I im, the sparse coding of Ωi’s qth patch

A New Accurate Image Denoising Method
7
Fig. 5. Iterative regularization
R imi,q denoted by αi,q
R im. Then βi is calculated based on the weighted average
of αi,p
I im and αi,q
R im as follows:
βi =

p∈Φi
ωi,p
I imαi,p
I im +

q∈Ωi
ωi,q
R imαi,q
R im
(3)
where
ωi,p
I im = 1
Wi
exp
−∥I imi −I imi,p∥2
2
h

(4)
and
ωi,q
R im = 1
Wi
exp
−∥R imi −R imi,q∥2
2
h

(5)
where W is a normalizing term, Wi = 
p∈Φi ωi,p
I im+
q∈Ωi ωi,q
R im, and h acts as
a ﬁlter parameters. In the lth iteration, based on the result of previous iteration
I im(l−1), we search for the similar patches to each patch i, and use Eqs. (3) to
update βi. Then α(l)
y
is obtained by using Eqs. (2). The entire iteration process
of our method is shown in Fig. 2 and Algorithm 1. The restored quality of our
method is presented in Fig. 3. From Fig. 3 we can see that the restored image
quality of our method outperform the other two methods.
2.2
Determine the Threshold ε by Noise Level σ Adaptively
In Sect. 2.1, by ﬁnding similar patches of xi and calculating sparse coding of
each similar patches, β is estimated as the weight average of them. Although
for some very low similarity patches the weights of them are very small. Lots
of them will reduce the accuracy of β. For this purpose, We remove some very
low similarity patches by setting the corresponding weight to zero when it is less
than ε. Mathematically, it can be represented by
ωi,k
new =

0,
ωi,k < ε
ωi,k,
ωi,k ≥ε
(6)

8
K. Lin et al.
Fig. 6. PSNR comparison under diﬀerent block
sizes and diﬀerent noise levels.
Table 1. Block size settings
Noise level
Block size
σ ≤15
6
15 < σ ≤30
7
30 < σ ≤50
9
σ > 50
8
where ωi,k is kth similar patch of xi. Therefore, the value of ε is critical. By
the grid search method, we obtain the corresponding ε value when the PSNR of
test image is optimal. We do this at diﬀerent noise levels (σ) and diﬀerent test
images. And then, ¯ε is obtained by averaging ε at the same noise level. From
Fig. 4, we can see that there is an approximate linear relationship between ¯ε and
σ. By regression ﬁtting, the following approximate relationship is obtained:
¯ε = 0.000198 ∗σ
(7)
By (7), the value of ε is determined by the level of noise adaptively rather
than a constant. Therefore, the ﬁlter process is more robust.
Algorithm 1
Input: observed image y, noise level σ
1: Initialization: ˆx(0) = y, β(−1) = 0.
2: Iterate over l = 0, 1, 2, . . . , L
(I) update the dictionary D on ˆx(l) via Kmeans++ and PCA;
(II) update β(l) using (3) (4) (5) (6);
(III) Compute α(l+1)
y
by solving Eq.(2) using Shrinkage Algorithm [17];
(IV) Estimate intermediated denoised result: ˆx(l+1) = Dα(l+1)
y
;
Output: ˆx = ˆx(L)
2.3
Find Back Some Image Structures in the Residual Image
In order to avoid over-smoothing during the iteration, we ﬁnd back some image
structures in the residual image by applying a LOG ﬁlter to the residual image.
LOG ﬁlter is less sensitive to noise than other edge extraction ﬁlters due to
the presence of Gaussian low pass ﬁltering of the signal before the computation
of spatial derivatives. So it allows us to introduce as little noise as possible. As
illustrated in Fig. 5, in the lth iteration, we get residual image by (y−ˆx(l)), where

A New Accurate Image Denoising Method
9
y is the noise image and ˆx(l) is the result of the lth iteration of our method. And
then we extract image structures by using LOG ﬁlter to the residual image.
Finally, the edge image will be added back to ˆx(l) and the result will be applied
in the (l + 1)th iteration of our method.
3
Experiment Results
The basic parameter setting is as follows: L = 9, γ = 0.2, and the ﬁltering
parameter h is selected as h = 0.85 ∗σ. the value of block size is determined by
Fig. 7. The test images used in the paper
Fig. 8. Image denoising performance comparison on ground image (σ = 20). From
left to right and top to bottom: original image, noise image, the reconstructed images
by SAPCA-BM3D (PSNR = 27.23 dB), CSR (PSNR = 27.47 dB), NCSR (PSNR =
26.91 dB), and our proposed method (PSNR = 28.16 dB).

10
K. Lin et al.
the image noise level σ. We set diﬀerent block sizes and diﬀerent noise levels on
several test images, and the observed PSNR are shown in the Fig. 6. According
to the trend of the line in Fig. 6, we set the value of block size according to the
image noise level σ adaptively. The setting of block size are shown in Table 1.
And we speed up our algorithm by separable nonlocal means ﬁltering [18]. To
evaluate the eﬀect of image denoising, we use peak signal-to-noise ratio (PSNR)
and structural similarity (SSIM) [19]. In order to verify the image denoising
ability of our proposed algorithm, we compare the proposed method with three
competitive state-of-the-art image denoising algorithm, including CSR, NCSR
and SAPCA-BM3D [20]. We apply 12 commonly used image in our experiments
under a wide range of noise levels. The test images are displayed in Fig. 7. And
the experiment results are presented in Tables 2 and 3 (the highest values are
highlighted in each cell). From Tables 2 and 3, we can see that our proposed
algorithm outperforms the other three algorithms in most cases. For the average
PSNR, our algorithm is the best.
Table 2. PSNR (dB) results by diﬀerent denoising methods. In each cell, the results of
four denoising methods are reported. Top Left: NCSR; Top Right: CSR; Bottom Left:
SAPCA-BM3D; Bottom Right: our method.
σ
5
10
15
20
50
100
Lena
38.74
38.70
35.90
35.81
34.20
34.09
32.96
32.92
28.89
28.94
25.66
25.87
38.86
39.16
36.07
36.43
34.43
34.80
33.20
33.61
29.07
29.46
25.37
26.25
Monarch
38.43
38.49
34.49
34.57
32.25
32.34
30.71
30.69
25.68
25.74
22.05
22.08
38.69
39.08
34.74
35.35
32.46
33.19
30.92
31.53
26.28
26.50
22.31
22.61
Barbara
38.43
38.36
35.10
34.98
33.17
33.02
31.78
31.72
27.10
27.20
23.30
23.60
38.38
38.87
35.07
35.66
33.27
33.80
31.97
32.44
27.51
27.64
23.05
23.86
Boat
37.31
37.35
33.88
33.90
32.05
32.03
30.78
30.74
26.60
26.66
23.64
23.88
37.50
37.83
34.10
34.53
32.29
32.77
31.02
31.48
26.89
27.26
23.71
24.13
C. Man
38.18
38.17
34.06
34.12
31.89
31.99
30.49
30.48
26.16
26.21
22.89
22.92
38.54
38.67
34.52
34.80
32.31
32.67
30.86
31.15
26.59
26.87
22.91
23.49
Couple
37.41
37.44
33.95
33.94
32.00
31.95
30.60
30.56
26.21
26.22
23.22
23.47
37.60
37.94
34.13
34.60
32.20
32.69
30.83
31.30
26.48
26.80
23.19
23.73
F.Print
36.85
36.81
32.70
32.70
30.47
30.46
28.97
28.99
24.53
24.52
21.29
21.47
36.67
37.40
32.65
33.52
30.46
31.40
28.97
29.83
24.53
25.30
21.07
21.71
Hill
37.12
37.17
33.66
33.69
31.87
31.86
30.65
30.61
26.86
26.95
24.13
24.47
37.31
37.65
33.84
34.26
32.06
32.46
30.85
31.20
27.13
27.41
24.10
24.81
House
39.98
39.91
36.88
36.80
35.11
35.11
33.86
33.97
29.63
29.53
25.65
25.84
40.13
40.44
37.06
37.36
35.31
35.62
34.03
34.40
29.53
30.12
25.20
26.14
Man
37.78
37.78
33.96
33.96
31.91
31.89
30.56
30.52
26.60
26.65
23.97
24.15
37.99
38.34
34.18
34.66
32.12
32.64
30.73
31.23
26.84
27.15
23.86
24.43
Peppers
38.03
38.06
34.64
34.66
32.69
32.70
31.25
31.26
26.53
26.52
22.64
22.81
38.30
38.05
34.94
34.70
33.01
32.78
31.61
31.30
26.94
26.55
23.05
22.84
Straw
35.89
35.87
31.51
31.50
29.14
29.13
27.50
27.50
22.48
22.44
19.23
19.51
35.81
35.92
31.46
31.50
29.13
29.01
27.52
27.40
22.79
22.17
19.42
19.44
Average
37.85
37.84
34.23
34.24
32.23
32.21
30.84
30.83
26.44
26.47
23.14
23.34
37.98
38.28
34.40
34.78
32.42
32.82
31.04
31.40
26.71
26.93
23.10
23.62

A New Accurate Image Denoising Method
11
Table 3. SSIM results by diﬀerent denoising methods. In each cell, the results of
four denoising methods are reported. Top Left: NCSR; Top Right: CSR; Bottom Left:
SAPCA-BM3D; Bottom Right: our method.
σ
5
10
15
20
50
100
Lena
0.9440
0.9375
0.9145
0.9132
0.8923
0.8917
0.8735
0.8659
0.7986
0.7894
0.5737
0.5105
0.9532
0.9620 0.9187
0.9195 0.9008
0.9087 0.8820
0.8973 0.8010
0.8143 0.5847
0.6013
Monarch 0.9754
0.9655
0.9571
0.9533
0.9391
0.9320
0.9211
0.9017
0.8252
0.8197
0.6015
0.5954
0.9210
0.9782 0.9580
0.9612 0.9418
0.9532 0.9289
0.9311 0.8410
0.8560 0.6143
0.6355
Barbara 0.9640
0.9531
0.9412
0.9385
0.9217
0.9187
0.9032
0.8972
0.7984
0.7732
0.4964
0.4890
0.9774
0.9825 0.9600
0.9712 0.9305
0.9410 0.9105
0.9337 0.8143
0.8259 0.5013
0.5115
Boat
0.9380
0.9312
0.8857
0.8802
0.8488
0.7990
0.8190
0.8098
0.6962
0.6152
0.4013
0.3957
0.9385
0.9400 0.8910
0.8997 0.8613
0.8705 0.8235
0.8245 0.7013
0.7187 0.4218
0.4330
C. Man
0.9597
0.9442
0.9272
0.9195
0.8977
0.8754
0.8736
0.8659
0.7875
0.7653
0.5328
0.5327
0.9620
0.9655 0.9447
0.9503 0.9003
0.0110 0.8810
0.8935 0.7963
0.8013 0.5427
0.5488
Couple
0.9482
0.9411
0.9049
0.8995
0.8691
0.8438
0.8364
0.8210
0.6926
0.6730
0.5013
0.4989
0.9513
0.9580 0.9107
0.9188 0.8847
0.8895 0.8410
0.8610 0.7018
0.7087 0.5130
0.5205
F.Print
0.9881
0.9825
0.9696
0.9620
0.9610
0.9578
0.9310
0.9280
0.8273
0.8265
0.6780
0.6680
0.9897
0.9903 0.9828
0.9834 0.9697
0.9715 0.9337
0.9395 0.8410
0.8423 0.6954
0.7013
Hill
0.9404
0.9387
0.8828
0.8747
0.8376
0.7998
0.8009
0.7985
0.6652
0.6540
0.4702
0.4680
0.9486
0.9512 0.8942
0.9012 0.8410
0.8425 0.8142
0.8207 0.6431
0.6613 0.4850
0.4987
House
0.9563
0.9483
0.9238
0.9213
0.8916
0.8580
0.8707
0.8665
0.8096
0.8073
0.6533
0.6329
0.9608
0.9673 0.9550
0.9574 0.9012
0.9117 0.8848
0.8910 0.8143
0.8297 0.6010
0.6738
Man
0.9523
0.9444
0.9042
0.9010
0.8632
0.8210
0.8286
0.8198
0.7010
0.6995
0.5080
0.4983
0.9630
0.9710 0.9135
0.9213 0.8742
0.8796 0.8410
0.8497 0.7137
0.7335 0.5138
0.5142
Peppers
0.9544
0.9440
0.9260
0.9253
0.9039
0.9105
0.8842
0.8798
0.7951
0.8001
0.6880
0.6843
0.9651 0.9538
0.9273 0.9159
0.9532 0.9321
0.7955 0.7857
0.8137 0.8280
0.7013 0.6790
Straw
0.9865
0.9807
0.9626
0.9610
0.9344 0.9012
0.9031
0.9142
0.6981
0.6887
0.4870
0.4905
0.9760
0.9890 0.9786
0.9711 0.9610
0.9140
0.9110
0.9273 0.6784
0.6990 0.4872
0.4774
Average 0.9589
0.9510
0.9250
0.9208
0.8967
0.8802
0.8704
0.8687
0.7579
0.7498
0.5621
0.5658
0.9583
0.9674 0.9288
0.9310 0.8983
0.9001 0.8732
0.8790 0.7600
0.7654 0.5525
0.5688
In Figs. 1 and 8, we show the denoising performance comparison on diﬀerent
images. It can be seen that our proposed method not only do well in remov-
ing noise, but also has a good preservation on the image edge and texture. In
addition, our restored image is more visually pleasant than NCSR.
4
Conclusions
In this paper, we propose a new method to estimate the sparse coding of the
original image. Diﬀerent from NCSR, we estimate αx by a couple of images: a ref-
erence denoised image and an intermediate denoised image. Meanwhile, we rule
out low similar blocks by setting the threshold. The threshold is determined by
the noise level adaptively. Experimental results demonstrated that our proposed
method outperformed other leading image denoising methods.
Acknowledgment. This work was supported by Science and Technology Plan-
ning
Project
of
Guangdong
Province,
China
(No.
2014B090910001
and
No.
2014B010117007), Shenzhen Key Laboratory for Intelligent Multimedia and Virtual
Reality (ZDSYS201703031405467), Shenzhen Peacock Plan (20130408-1830 03656),
and the grant of National Science Foundation of China (No. U1611461).

12
K. Lin et al.
References
1. Do, M.N., Vetterli, M.: The contourlet transform: an eﬃcient directional multireso-
lution image representation. IEEE Trans. Image Process. 14(12), 2091–2106 (2005)
2. Simoncelli, E.P., Adelson, E.H.: Noise removal via Bayesian wavelet coring. In:
Proceedings of International Conference on Image Processing, vol. 1, pp. 379–382
(1996)
3. Portilla, J., Strela, V., Wainwright, M.J., Simoncelli, E.P.: Image denoising using
scale mixtures of Gaussians in the wavelet domain. IEEE Trans. Image Process. A
Publ. IEEE Sig. Process. Soc. 12(11), 1338–1351 (2003)
4. Donoho, D.L.: De-noising by soft-thresholding. IEEE Press (1995)
5. Mallat, S., Yu, G.: Super-resolution with sparse mixing estimators. IEEE Trans.
Image Process. 19(11), 2889–2900 (2010)
6. Chen, F., Zhang, L., Yu, H.: External patch prior guided internal clustering for
image denoising. In: Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 603–611 (2015)
7. Shao, L., Zhang, H., De Haan, G.: An overview and performance evaluation
of classiﬁcation-based least squares trained ﬁlters. IEEE Trans. Image Process.
17(10), 1772–1782 (2008)
8. Takeda, H., Farsiu, S., Milanfar, P.: Kernel regression for image processing and
reconstruction. IEEE Trans. Image Process. 16(2), 349–366 (2007)
9. Xiong, R., Liu, H., Zhang, X., Zhang, J., Ma, S., Wu, F., Gao, W.: Image denoising
via bandwise adaptive modeling and regularization exploiting nonlocal similarity.
IEEE Trans. Image Process. 25(12), 5793–5805 (2016)
10. Ma, S., Zhang, X., Zhang, J., Jia, C., Wang, S., Gao, W.: Nonlocal in-loop ﬁlter: the
way toward next-generation video coding? IEEE Multimedia 23(2), 16–26 (2016)
11. Buades, A., Coll, B., Morel, J.M.: A non-local algorithm for image denoising. In:
IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
CVPR 2005, vol. 2, pp. 60–65 IEEE (2005)
12. Goossens, B., Luong, H., Piurica, A., Philips, W.: An improved non-local denoising
algorithm. In: 2008 International Workshop on Local and Non-local Approximation
in Image Processing (LNLA 2008), pp. 143–156 (2008)
13. Elad, M., Aharon, M.: Image denoising via sparse and redundant representations
over learned dictionaries. IEEE Trans. Image Process. 15(12), 3736–3745 (2006)
14. Mairal, J., Bach, F., Ponce, J., Sapiro, G., Zisserman, A.: Non-local sparse models
for image restoration. In: 2009 IEEE 12th International Conference on Computer
Vision, pp. 2272–2279. IEEE (2009)
15. Dong, W., Li, X., Zhang, L., Shi, G.: Sparsity-based image denoising via dictionary
learning and structural clustering. In: Computer Vision and Pattern Recognition,
pp. 457–464 (2011)
16. Dong, W., Zhang, L., Shi, G., Li, X.: Nonlocally centralized sparse representation
for image restoration. IEEE Trans. Image Process. 22(4), 1620–1630 (2013)
17. Daubechies, I., Defrise, M., De Mol, C.: An iterative thresholding algorithm for
linear inverse problems with a sparsity constraint. Commun. Pure Appl. Math.
57(11), 1413–1457 (2004)
18. Bhujle, H.: Feature-preserving 3D ﬂuorescence image sequence denoising. In: Tenth
Indian Conference on Computer Vision, Graphics and Image Processing, p. 45
(2016)

A New Accurate Image Denoising Method
13
19. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13(4),
600–612 (2004)
20. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: BM3D image denoising with
shape-adaptive principal component analysis. In: Proceedings of Workshop on Sig-
nal Processing with Adaptive Sparse Structured Representation, Saint-Malo (2009)

A Novel Frontal Facial Synthesis Algorithm
Based on Individual Residual Face
Xin Ding1,2(B), Ruimin Hu1,2, Zhen Han1,2, and Zhongyuan Wang1,2
1 National Engineering Research Center for Multimedia Software,
Wuhan University, Wuhan, China
ding-xin@whu.edu.cn, hurm1964@gmail.com, hanzhen 1980@163.com,
wzy hope@163.com
2 Hubei Key Laboratory of Multimedia and Network Communication Engineering,
Wuhan University, Wuhan 430072, China
Abstract. The frontal facial synthesis results of current main methods
tend to be smooth and lack personal characteristics, which greatly inﬂu-
ence the subjective impression. Especially aiming at large-scale deﬂect-
ing faces, the shielding side just provides few features for reconstructing
and the deformation of facial elements enhance the diﬃculty to obtain
exact features of target frontal face, making the synthesis result seem
to be same and as mean face of database. In this paper, to solve these
problems, we propose a novel two-step face synthesis method. In the
ﬁrst step, we utilize the basic symmetry of human face to predict the
missing patches according to the other side and generate interim facial
image. And in the following step, we introduce the individual residual
facial image between interim result and mean face to compensate for the
lost personal features of the synthesis result because the residual image
carries more individual characteristic of the input face. We show that
experimental results of proposed method outperform in the objective
and subjective eﬀects with other state-of-the-art methods.
Keywords: Frontal view synthesis · Residual facial image
Manifold learning · Facial symmetry
1
Introduction
As an active topic of research in image processing and computer vision, frontal
facial image synthesis has plenty of potential applications in video surveillance
and face recognition in recent years. Generally speaking, frontal face synthe-
sis means to generate ones frontal image from his single or multiple arbitrary
faces [1]. In real surveillance environmentpose variation of target faces frequently
obtained induces the invalidation of face recognition, which greatly reduces value
of surveillance videos. In these two decades, a lot of related approaches have been
proposed in tackling the problem of non-frontal facial images in arbitrary poses.
Current available face synthesis methods can be roughly divided into two cate-
gories: 3D models and 2D techniques [2]. The ﬁrst category is to build 3D model
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 14–22, 2018.
https://doi.org/10.1007/978-3-319-73600-6_2

A Novel Frontal Facial Synthesis Algorithm
15
based on human head. The main steps are to ﬁrst align the 2D face image with
a 3D face model on account of facial landmarks, and then map the texture of
the 2D image to the 3D model, last rotate the texture one to a desired pose and
generate a new 2D image, such as the wire frame model [3,4], 3D morphable
models (3DMMs) [5–9], and some further optimized model, 3DDFA [10].
        (a)                      (b)                      (c)                    (d)                      (e)
Fig. 1. Synthesis results of current methods rushing to the mean face, (a) are the
original non-frontal faces, (b) and (c) are synthesis results of LOC and URF, (d) is
mean face and (e) are the corresponding frontal faces
On the other hand, 2D techniques try to learn the statistical relationship
by means of linear or nonlinear mathematical forms between the frontal faces
and arbitrary faces. Inchoate studies [1,11] created linear models to describe the
relationship of facial poses, such as LOC (Linear object classes) based method,
which was applied to shape vector and texture vector separated from input face.
Chai et al. [12] exploded facial images into local rectangular patches, and each
patch was predicted and synthesized singly for solving the linear inconsistence of
global face. Han et al. [13] utilized a novel synthesis method draw lessons from
LOC, in which they separated input image into shape and shape-free texture,
and apply Neighbor Embedding (NE) [14] to them respectively. Hao and Qi [15]
developed a Uniﬁed Regularization Framework (URF) for face synthesis, which
incorporates local similarity prior and embraces neighbor patches of the target
patch as constraint for the manifold between these diﬀerent poses. Wei et al. [16]
proposed a stepwise algorithm transferring the large scale rotation to multiple
small scale deﬂection to solve the manifold inconsistence of large deformation,
yet this process need multi-pose facial databases, which is not easy to obtain.
In fact, shown in Fig. 1(b) and (c), the results of LOC and URF are too
smooth and lack personal features, they look nearly same and even like mean
face of database (as Fig. 1(d)). This is because the large-scale rotated faces lead
to the missing of individual features. The shielding side just provides little infor-
mation and the patches are so small that they couldn’t oﬀer strong characteristic

16
X. Ding et al.
for reconstruction. For another, even that the exposed side contains much infor-
mation, the deformation of facial elements weakens the personal features com-
pared with the target frontal facial image (as Fig. 2). These problems bring in
so much common redundance of facial database within the synthesis process, so
that the results look as the mean face.
Fig. 2. Symmetrical patches of frontal and non-frontal facial model, the same colour
patches are mapping with each other
In this paper, to solve above problems, we present a novel two-step frontal
facial synthesis method, and provide powerful personal characteristic enhance-
ment technique for synthesis. A ﬂowchart of our face image synthesis approach
is shown in Fig. 3. In the ﬁrst step, we introduce Locality-Constrained Represen-
tation (LcR) [17] for reconstructing the frontal face. Especially for the missing
features of the shielding part, we retrieve the tiny patches according to the other
side based on facial symmetry, which maintains the relevancy with the origi-
nal frontal face. Because the non-frontal input image weakens target personal
characteristic, the generated result take some common information of database,
the feature of mean face. So we compute the residual facial image between the
temporary result of the ﬁrst step and mean face. The residual face getting rid of
the mean face is considered as taking more features of the target one. So in the
second step, we generate two training residual databases, the one corresponding
to the test residual face which gains from the database faces as the ﬁrst step and
the other being the minus between the database frontal faces and mean face, to
compensate for facial features to enhance the subjective eﬀect. The ﬁnal output
is formed by adding the residual face of the second step to the mean face. In
the following experiment, results demonstrate that the proposed method outper-
forms in the objective and subjective eﬀects with other mainstream methods.

A Novel Frontal Facial Synthesis Algorithm
17
Fig. 3. Flowchart of our facial synthesis approach
2
Proposed Method
In this method, we separate every face image into T triangle patches. Sup-
posed we have training sets of frontal face images Io = {I1
o, I2
o, . . ., IN
o } ∈Rσ×N
and their corresponding non-frontal versions In = {I1
n, I2
n, . . ., IN
n } ∈Rσ×N,
the facial synthesis can be formulated as the estimation of frontal face I from
one input non-frontal image In, which has the same angle of Ipn. Each image
of database can be divided into T triangle patches x(1,0), x(2,0), . . ., x(T,0), with
In into x(1,n), x(2,n), . . ., x(T,n), and each patch has the corresponding texture
information.
2.1
Step1: Frontal Facial Synthesis based on Facial Symmetry
As shown in Fig. 2, we can ﬁnd the patches in shielding side gather together
and are too small to provide enough information and features for the neighbor
patches in the database. Based on the symmetry of human face, we provide the
correspondence from left patches to right ones as Fig. 2:
Nj =

Map(j), j ∈Mapkj, else
(1)
where Map means mapping operation between the left patch and the right one,

Mapk
represents the set of left patches, and Nj is the mapping patch when it

18
X. Ding et al.
traverses to the jth patch xj of the given non-frontal facial image In, which can
be expressed by corresponding neighbor patches xk
(j,p0) of database faces Ipn:
x∗
Nj =
N

j=1
wk
Njxk
(Nj,p0)
(2)
where wk
Nj is the weight for representing the target patch while it would match
N
j=1 wk
Nj = 1, and k means the patch’s K nearest neighbors in the database
Ipn. The optimal weight wk
Nj can be solved by the linear relationship between the
jth patch x(Nj,pn) and the same position patches of non-frontal facial database
D(Nj,pn) = {x1
Nj,pn, x2
Nj,pn, . . . , xN
Nj,pn}:
w∗k
Nj = argmin||x(Nj,pn) −wk
NjD(Nj,pn)∥2
2
(3)
And after all the triangle patches are reconstructed, the temporary facial
image I∗can be obtained by integrating them.
2.2
Step2: Residual Face Compensation for the Individual Features
Since the temporary facial image I∗looks smooth lacking detailed facial features,
we have intention to calculate the residual face from mean face to compensate the
personal characteristic [2]. The residual synthesis face RM and original residual
face RH have similar texture and strong relevance, the diﬀerences of which are
the weaken information and the original features. We can map RM to RH to
enhance the subjective eﬀect of the input facial image.
In the training phase, we obtain the temporary faces

IN
t

for non-frontal
training set based on the method in the ﬁrst step, deﬁning the residual faces as
RM = IN
t −Imean, where Imean is the mean face of the frontal facial database.
Similarly, we also deﬁne residual faces of the original faces as RH = IN
po −Imean.
And then

RM
is the set of weakening personal feature components, while

RH
is one of the strong personal feature components providing exact facial
information. Each image of

RH
and

RM
is divided into a set of small
overlapping square patches

IH
i,j

and

IM
i,j

, where i, j is the initial coordinate
of each patch.
In the test phase, for the input non-frontal facial image In, we gain its tem-
porary result I∗. Following the above method, we also acquire the residual face
R∗
n = I∗−Imean as the input in the second step, meanwhile dividing into
overlapping patches, and the patch set is

I∗
i,j

. We apply NE to decide the
reconstructed residual patch of the temporary face, and then we can gain the
residual patch {R∗
n} by following steps: (1) Finding K nearest neighbors in the
set of

IM
i,j

for

I∗
i,j

:
Ki,j =

IM
i1j, IM
i2j, . . . , IM
iKj

(4)

A Novel Frontal Facial Synthesis Algorithm
19
(2) Calculating the reconstruction weights wM
j
of the neighbors that minimize
the error of reconstructing IM
i,j:
wM
j
= argmin||I∗
i,j −
K

k=1
wkjMIM
ikj∥2
2
(5)
where the weights wM
j
meets K
k=1 wkj = 1.
(3) Construct the residual patch Rijf using the same weights for the corre-
sponding residual patches

IH
i,j

:
Rijf =
K

k=1
wkjMIH
ikj
(6)
After all the residual patches are reconstructed, the residual image Rf
t can
be obtained by integrating them. The ﬁnal HR face image is formed by the sum
of the temporary result and residual image:
If
t = Rijf + Imean
(7)
3
Experiments and Results
3.1
Database
In this experiment, we use CAS-PEAL-R1 database. It contains 30863 images of
1040 subjects with controlled pose, expression, accessory, and lighting variations.
In our experiments, we chose 495 images of 165 subjects with three poses (0◦,
+45◦, +67.5◦) under a neutral expression, no accessory, and ﬁxed illumination.
We used 150 persons for training and left the other 15 for testing. The size of each
image is 192 × 228. We manually located 60 landmark points on all of the face
images, and then a set of triangles was connected by the delaunay triangulation
algorithm. With the help of these triangulations, we morphed each one using
aﬃne transform. And we set the pose of +45◦and +67.5◦of the ﬁve faces of
test images as input.
3.2
Synthesis Results and Evaluation
In this section, we compare the synthesis results of our method with other meth-
ods in Fig. 5. In each group, the compared algorithms include SFFS (Stepwise
Frontal Face Synthesis method) [16], LLR [12], LLE [13], URF [15]. As shown in
Figs. 4 and 5, our synthesis results generated more strong personal features to
more similar facial images with the original images both +45◦and +67.5◦facial
images. The larger and more clear faces shown as Fig. 6 can prove that our results
provided much better subjective eﬀectiveness, while the other approaches’ faces
were smooth and rushed to the mean face.

20
X. Ding et al.
Fig. 4. (a) is the given +45◦face image, and from (b) to (f), methods for comparison
respectively are SFFS, LLR, LLE, URF, and (g) are ours and (h) are the original faces
Table 1. Average PSNR and SSIM of these ﬁve faces
Methods
Indexes
Pose angle +45◦Pose angle +67◦
SFFS
PSNR (db) 24.42
24.02
SSIM
0.8515
0.8464
LLR
PSNR (db) 23.82
22.82
SSIM
0.8404
0.8272
LLE
PSNR (db) 23.62
22.97
SSIM
0.8145
0.8010
URF
PSNR (db) 24.19
23.19
SSIM
0.8408
0.8310
Ours
PSNR (db) 24.35
23.81
SSIM
0.8486
0.8423
Improvement
PSNR (db)
0.16
0.62
(Compared with best results) SSIM
0.0078
0.0113
Furthermore, to illustrate the performance of our method, we introduce the
PSNR (peak signal to noise ratio) and SSIM (structural similarity). Table 1
presents the averaged PSNR and SSIM of the randomly chose ﬁve synthesized
frontal facial images, and shows the improvement of the objective index. Among
these methods, SFFS (Stepwise Frontal Face Synthesis method) [16] has better
PSNR and SSIM. Whereas this algorithm need gradual changing facial pose
databases, and more samples could provide more synthesis information, which

A Novel Frontal Facial Synthesis Algorithm
21
Fig. 5. (a) is the given +67◦face image, and from (b) to (f), methods for comparison
respectively are SFFS, LLR, LLE, URF, and (g) are ours and (h) are the original faces
Fig. 6. Subjective comparing of our and other methods
can promote the value of PSNR and SSIM. Meanwhile as Fig. 6, the synthesis
facial images couldn’t demonstrate the feature of target faces better in the aspect
of subjective.
4
Conclusion
Pose variations, especially large angles, was the major challenge in the current
face recognition technology. However the facial synthesis results of current main
methods are too smooth and lack individual features. In this paper, we propose
a novel synthesis approach focusing the reconstruction of the personal charac-
teristic, which improves the subjective eﬀect a lot. In the future, our work will
focus on enhancing facial details and utilizing multiple non-frontal facial images
of the same person to jointly synthesize the frontal facial image.

22
X. Ding et al.
Acknowledgments. The research was supported by the National High Technology
Research and Development Program of China (863 Program) No. 2015AA-016306,
National Nature Science Foundation of China (61231015, 61671336, 61671332), the EU
FP7 QUICK project under Grant Agreement No. PIRSES-GA-2013-612652*, Hubei
Province Technological Innovation Major Project (No. 2016AAA015, 2017AAA123),
Natural Science Foundation of Hubei Province (2016CFB573).
References
1. Vetter, T., Poggio, T.: Linear object classes and image synthesis from a single
example image. IEEE Trans. Pattern Anal. Mach. Intell. 19(7), 733–742 (1995)
2. Ding, C., Tao, D.: A comprehensive survey on pose-invariant face recognition. ACM
Trans. Intell. Syst. Technol. 7(3) (2015)
3. Lee, M.W., Ranganath, S.: Pose-invariant face recognition using a 3D deformable
model. Pattern Recogn. 36(8), 1835–1846 (2003)
4. Zhang, X., Gao, Y., Zhang, B.-L.: Recognizing rotated faces from two orthogonal
views in mugshot databases, vol. 1, pp. 195–198 (2006)
5. Aldrian, O., Smith, W.A.P.: Inverse rendering of faces with a 3D morphable model.
IEEE Trans. Pattern Anal. Mach. Intell. 35(5), 1080–1093 (2013)
6. Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In: Con-
ference on Computer Graphics and Interactive Techniques, pp. 187–194 (1999)
7. Blanz, V., Vetter, T.: Face recognition based on ﬁtting a 3D morphable model.
IEEE Trans. Pattern Anal. Mach. Intell. 25(9), 1063–1074 (2003)
8. Zhu, X., Lei, Z., Yan, J., Yi, D.: High-ﬁdelity pose and expression normalization
for face recognition in the wild, pp. 787–796 (2015)
9. Hassner, T.: Viewing real-world faces in 3D. In: International Conference on Com-
puter Vision (ICCV), December 2013
10. Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3D
solution. Computer Science (2015)
11. Vetter, T., Jones, M.J., Poggio, T.: A bootstrapping algorithm for learning linear
models of object classes. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 40–46 (1997)
12. Chai, X., Shan, S., Chen, X., Gao, W.: Locally linear regression for pose-invariant
face recognition. IEEE Trans. Image Process. 16(7), 1716–1725 (2007)
13. Han, Z., Jiang, J., Hu, R., Lu, T.: A novel frontal view synthesis method based on
neighbor embedding. In: International Conference on Image Analysis and Signal
Processing, pp. 128–132 (2011)
14. Chang, H., Yeung, D.Y., Xiong, Y.: Super-resolution through neighbor embed-
ding. In: IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pp. 275–282 (2004)
15. Hao, Y., Qi, C.: A uniﬁed regularization framework for virtual frontal face image
synthesis. IEEE Sig. Process. Lett. 22(22), 559–563 (2015)
16. Wei, X., Hu, R., Han, Z., Chen, L., Ding, X.: A stepwise frontal face synthesis
approach for large pose non-frontal facial image. In: Chen, E., Gong, Y., Tie, Y.
(eds.) PCM 2016. LNCS, vol. 9917, pp. 439–448. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-48896-7 43
17. Jiang, J., Ruimin, H., Wang, Z., Han, Z.: Noise robust face hallucination via
locality-constrained representation. IEEE Trans. Multimedia 16(5), 1268–1281
(2014)

A Text Recognition and Retrieval System
for e-Business Image Management
Jiang Zhou(B), Kevin McGuinness, and Noel E. O’Connor
Dublin City University, Dublin, Ireland
{jiang.zhou,kevin.mcguinness,noel.oconnor}@dcu.ie
Abstract. The on-going growth of e-business has resulted in companies
having to manage an ever increasing number of product, packaging and
promotional images. Systems for indexing and retrieving such images
are required in order to ensure image libraries can be managed and fully
exploited as valuable business resources. In this paper, we explore the
power of text recognition for e-business image management and pro-
pose an innovative system based on photo OCR. Photo OCR has been
actively studied for scene text recognition but has not been exploited for
e-business digital image management. Besides the well known diﬃculties
in scene text recognition such as various size, location, orientation in text
and cluttered background, e-business images typically feature text with
extremely diverse fonts, and the characters are often artistically modiﬁed
in shape, colour and arrangement. To address these challenges, our sys-
tem takes advantage of the combinatorial power of deep neural networks
and MSER processing. The cosine distance and n-gram vectors are used
during retrieval for matching detected text to queries to provide toler-
ance to the inevitable transcription errors in text recognition. To evaluate
our proposed system, we prepared a novel dataset designed speciﬁcally
to reﬂect the challenges associated with text in e-business images. We
compared our system with two other approaches for scene text recogni-
tion, and the results show our system outperforms other state-of-the-art
on the new challenging dataset. Our system demonstrates that recogniz-
ing text embedded in images can be hugely beneﬁcial for digital asset
management.
Keywords: Image management · Image retrieval · OCR
1
Introduction
As a consequence of the popularity and the fast growth of e-business and on-
line shopping, an ever growing number of product images are uploaded, viewed,
and shared. These images can be organized based on tags, captions, or text
descriptions if such information is given. However, many images are created or
uploaded without any textual annotation. Indexing and searching these images
would require laborious manual tagging, which could be impractical if the col-
lection is very large, motivating content-based retrieval approaches [4,14].
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 23–35, 2018.
https://doi.org/10.1007/978-3-319-73600-6_3

24
J. Zhou et al.
Product packaging images or promotional posters generally have rich text
containing category information such as the brands, ingredients, pricing infor-
mation, etc. which is diﬃcult to extract using standard content-based image
retrieval methods. In this paper, we propose an innovative photo OCR-based
system for e-business image management. By indexing each image with words
recognized from the image automatically, retrieval can be achieved using simple
keyword searching.
Conventional OCR methods do not work well on these types of e-business
images as such methods were typically designed for scanned documents which
generally have structured layouts, horizontal text-lines, clear character blocks
and uniform text patterns. As a result, photo OCR techniques are required [1,12].
However, most photo OCR methods were developed for text recognition in
generic scenes, and have not been speciﬁcally designed for e-business image man-
agement. Scene text recognition usually addresses challenges such as low reso-
lution, blurry images, strong lighting and low contrast, etc. E-business images
are usually professionally produced and have good image quality in general but
they present other challenges for text recognition, such as the cluttered back-
ground, various text sizes, location and orientation. Moreover, text in e-business
images usually features extremely diverse fonts, and in fact characters are often
artistically modiﬁed in shape, colour and arrangement.
To address these challenges in e-business images, we take advantage of the
combinatorial power of deep neural networks and maximally stable extremal
regions (MSERs) processing. Text regions are identiﬁed with a convolutional
neural network (CNN) model. Letter regions are then extracted as MSERs and
grouped as words by pair grouping within the text regions. The word regions are
veriﬁed by a word/non-word CNN model. A word region growing technique is
proposed to complete partially detected word regions. Word regions are then con-
verted to machine coded transcription by a CNN-trained character recognizer.
At query time, n-gram vectors are computed for the text query and the tran-
scription. The cosine distance is calculated for searching a pair match between
the query and transcription of a detected region and any image that contains a
match will be returned from the system.
Several datasets have been published for scene text recognition. However,
testing on these datasets could not fully evaluate our proposed system given
the speciﬁc characteristics of e-business images. Therefore, we prepared a novel
dataset1 specﬁcally designed to reﬂect the type of text typically encountered in e-
business images. The dataset contains 500 e-business images that are categorized
into 13 categories. Words in images are annotated with bounding boxes, which
provides more than 4000 annotations and approx. 2500 unique words in total.
The rest of the paper is organized as follows: in Sect. 2 we review techniques
of text recognition for images. Section 3 details our proposed system. In Sect. 4.1,
we describe our dataset and present the challenges of text recognition for
e-business images. In Sect. 4.2, we compare our system with two other
1 The dataset is available for download from https://github.com/jiang-public/
mmm2018.

A Text Recognition and Retrieval System for e-Business Image Management
25
state-of-the-art approaches for text recognition. Finally, Sect. 5 ends the paper
with some concluding remarks.
2
Related Work
Converting printed or handwritten text in images to machine encoded text has
a long history and is usually referred to as optical character recognition (OCR).
Conventional OCR methods are designed primarily for black-white scanned doc-
uments and typically rely on brittle techniques such as binarization [1,21]. Strong
assumptions are usually made in the processing such as the presence of horizon-
tal text lines and that characters have the same size, therefore these methods
perform poorly on general images.
Recently, many methods have been proposed for recognizing text in images
of natural scenes [12]. Such approaches are usually referred to as PhotoOCR
[1]. Some of these methods address sub-tasks such as text detection [8,13] or
text recognition [20], while others attempt to combine both to have an end-to-
end solution [10,19]. In text recognition, methods typically assume perfect text
localization has been achieved and words are “cut-out”. However, in real-world
applications, the assumption that text is localized 100% accurately is usually
invalid. Text in natural images may exhibit signiﬁcant diversity of text patterns
in highly complicated backgrounds. Therefore, much eﬀort has been devoted to
the task of text detection.
Depending on the processing ﬂow for text detection, methods can generally
be categorized into two groups, connected component-based methods, and sliding
window-based methods. The stroke width transform (SWT) [5] and maximally
stable extremal regions (MSERs) [6,16] are two common connected component-
based methods. MSER-based detectors exploit the fact that characters normally
have strong contrast with the background to allow for easy reading [3,19]. SWT
uses the assumption that characters are regions of similar stroke width [10,
17]. The same assumption is also exploited in the FASTex detector [2] where
text fragments are extracted by local thresholding properties of stroke-speciﬁc
keypoints. There are also some methods leveraging the beneﬁts of both SWT and
MSERs that attempt to achieve improved text localization performance [15,18].
Sliding window based methods process text as a standard task of object detec-
tion and recognition [1,11]. Wang et al. [22] detected characters with Random
Ferns and grouped them into words with a pictorial structure model. Jaderberg
et al. [10] generated candidate word bounding boxes with Edge Box proposals
and pruned them with a Random Forest classiﬁer. Each word is then recognized
with a CNN model trained on a large synthetic data set [9].
However, no methods have yet achieved suﬃcient accuracy for practical end-
to-end applications. Although promising results have been achieved for scene
text recognition [12], the datasets evaluated are not fully realistic – the text
patterns are simple, the word orientations are only horizontal, they occupy a
signiﬁcant part of the image, and there is no perspective distortion or signiﬁcant
noise [19]. Therefore, these methods are not expected to perform well on the real

26
J. Zhou et al.
challenges aforementioned from the Internet e-business images, as shown later
in our experiments.
In contrast, our system is designed to tackle those challenges from e-business
images. Speciﬁcally, (a) an approach of combining the merits from both deep
neural networks and MSER processing is developed; (b) A region growing tech-
nique is proposed to complete partial detected word regions; (c) we use cosine
distances with n-grams vectors instead of keywords comparison with edit dis-
tances in retrieval to provide tolerance to inevitable transcription errors in text
recognition.
3
The Proposed System
The core of our proposed e-business image management system is its photo
OCR-based backend which consists of image indexing and searching. Figure 1(a)
depicts the process pipeline of the image indexing stage. During indexing, text
embedded in each image is recognized and its transcription is saved in a database.
At query time, a text query is given and a matching transcription is searched in
the database based on the cosine distance – sample search terms and their dis-
tances are illustrated in Fig. 1(b). Any images containing matched transcriptions
are retrieved and returned to the user.
Fig. 1. The backend of our proposed system
3.1
Indexing
Text-line region detection. Our system detects text-line regions based on
a pretrained CNN model from Oxford [11] and Hough line detection. A text
saliency map, shown in Fig. 2(b), is generated by evaluating the text/background
CNN model, which is trained on cropped 24×24 pixel case-insensitive characters,
in a sliding window fashion across the image. Hysteresis thresholding (thigh =
0.2, tlow = −0.5) is subsequently applied to the saliency map to obtain a binary
text/background map. With the binary map, we apply Hough line detection
to extract the potential text line regions and remove those sparse text spots
which are very likely to be noise detected from the CNN model. Our Hough line
detection is a probabilistic Hough line transform modiﬁed as follows. Once a

A Text Recognition and Retrieval System for e-Business Image Management
27
line segment is identiﬁed, all pixels from the regions that the line traverses are
suppressed and are not used in subsequent line voting. This text line detection
is used only to detect the text line regions rather than estimating the baseline
of the text, thus curved text line regions can also be identiﬁed. Figure 2 shows
an example of the detection.
Fig. 2. An example of text-line region detection
MSERs extraction and grouping. The output from text-line region detec-
tion is not suﬃcient for creating accurate word bounding boxes. As the detection
for word “kellogg’s” in Fig. 2(b), the detected region is only around the central
area of the text. Therefore, we compute MSERs from images for precisely extract-
ing the word regions at a later stage. The MSERs that overlap with a text-line
region are pair grouped to form a potential word. Each MSER i searches for its
nearest neighbour MSER j. If the normalized distance between i and j is small
enough, MSER i and j are considered as a pair of characters from the same
word. Given a small evaluation image set, we empirically choose this distance
as 1.15.
Word region veriﬁcation. To remove false detections, a word/non-word CNN
classiﬁer is trained with Jaderberg’s synthetic data [9]. We change the last fully
connected layer of Jaderberg’s network [9] to have a binary word/non-word out-
put. Each potential word region is classiﬁed and empirically those with proba-
bility higher than 0.98 are retained.
Word region growing. A group of retained MSERs may only present a part of
a word due to inaccuracies in detection. We propose a word growing technique to
address this. As shown in Fig. 3, for a retained word region we extend the width
of the word region from both sides, and search candidate MSERs from all MSERs
extracted in the entire image. If any MSER has more than 30% region overlap
with the potential word, this MSER becomes a candidate for being attached to
the word region. However, among all candidates only the one with the highest
overlap ratio is chained to the potential word in any given search. We repeat

28
J. Zhou et al.
Fig. 3. An example of word region growing. White components in (c) and (d) are
MSERS conﬁrmed to be part of a word, green components are candidates being
searched, and the orange component in (d) was the winning MSER attached to a
word region in the ﬁnal search
this search procedure on both left and right directions and grow the potential
word continuously until no more candidate MSERs can be found.
Word recognition. After word growing, the word regions are ready for recog-
nition and Jaderberg’s character sequence encoding model [9] is applied. We ﬁnd
that the model is sensitive to the padding around the text in the word region. A
slight diﬀerence in padding could result in diﬀerent character sequences, espe-
cially for cursive characters. Therefore, for each word region we expand the region
with 6 and 12 pixel paddings respectively, and rotate the 6 pixels padding region
through +5 and −5 degrees such that the word will not be partially cut out
because of the rotation. Together with the originally detected region, 5 region
patches are extracted and resized to size 32 × 100 individually for translation by
the character sequence encoding model as shown in Fig. 4. Transcriptions from
all detected word regions in an image are saved in a database and this image is
then indexed.
Fig. 4. Region patches of “Kellogg’s” for word recognition. The top row text is the
output from the recognition model in each case
3.2
Searching
The transcription from the character sequence encoding model may not always
correspond to meaningful words as illustrated in Fig. 4. Therefore, the cosine
distance with n-gram features is used for string comparison. The n in n-gram is
chosen to be 1 to 4. A pre-prepared n-gram look-up table [9] is used to generate

A Text Recognition and Retrieval System for e-Business Image Management
29
a 10K dimensional n-gram histogram vector. It should be noted that training
a CNN model to convert region patches to n-grams feature vectors directly as
described in Jaderberg’s paper [9] is inappropriate for our system because this
would require our queries to be pictures as well.
Transcription from each detected word is compared to the query for matching
based on the cosine distance. 5 cosine distances of 5 patches of a detected word
are computed. We take the average of these 5 distances to be the distance of the
detected word to the query. We iterate through all detected words in an image
and then any image containing a matched word is retrieved.
3.3
Frontend
Our proposed system is a web-based management system. Images in a database
are pre-indexed asynchronously. When a user submits a query, the system
retrieves all the images containing the query word and returns them back to
the users through the interface as shown in Fig. 5.
Fig. 5. The frontend of the image management system
4
Evaluation
4.1
Dataset
To evaluate our system, a novel dataset is prepared. Comparing to the other
public datasets for text recognition evaluation, our dataset is much closer to e-
business applications and reveals the challenges of text recognition for e-business
images as shown by the examples in Fig. 6. The dataset contains 500 product
packaging and promotional images that are categorized into 13 categories which
are artworks, beauty, biscuits, books, boy tops, cereal and porridge, grocery,
health and personal care, homecare cleaning, pet supplies, sweets and chocolate,
toys, and vhs. Words in images are annotated manually with bounding boxes
as the ground truth. One or two character words including numbers are deemed
unreadable and not annotated. Words with repeat patterns and unreadable small

30
J. Zhou et al.
Fig. 6. Examples from the evaluation dataset. (a) artworks, (b) beauty, (c) biscuits,
(d) books, (e) boy tops, (f) cereal and porridge, (g) grocery, (h) health and personal care,
(i) homecare cleaning, (j) pet supplies, (k) sweets and chocolate, (l) toys, (m) vhs
words are also ignored. This gives us in total 4, 123 annotations and 2, 579 unique
words in the dataset.
For detailed evaluation, 390 item queries and 110 document queries were
prepared. The item queries includes 6 query sets with 65 words in each set
respectively. The “small” words are deﬁned as non-cursive horizontal words
with character size around 12 × 12 pixels, which are usually used for ingredient
information, terms and conditions, etc. The “middle” words are those charac-
ters around 24 × 24 pixels, which usually give supplementary information about
products such as “all skin types”, “chocolate”, “international”, etc. Characters
in “big” words are bigger than 48 × 48 pixels which are generally used for brand
names, product names, book names, etc. Characters in the “orientation” set are
not horizontally aligned. The words in the “cursive” set are words with hand-
written script fonts. The “art” set is the most challenging set, and words have
strong artistic decoration, e.g. unequal character size, gradually changed font
colour or almost transparent characters.
The 110 document queries were prepared for image retrieval evaluation. The
query words were randomly selected from the annotated dataset. Given a query
word, all images containing the word are considered relevant. In this evaluation,
even if the query word appears in multiple locations in a relevant image, having
only one location detected and read out correctly is suﬃcient for the target
application.
4.2
Comparison with Other Approaches
Two other state-of-the-art scene text recognition approaches are implemented
for comparison, which detect and convert word regions to machine-coded tran-
scription. The comparison shares the same searching procedure.
The ﬁrst approach is Chen’s robust text detection in natural images [3].
Chen’s approach applies both MSERs and stroke width in character detection

A Text Recognition and Retrieval System for e-Business Image Management
31
and uses the conventional OCR engine Tesseract [21] for word recognition, which
provides a baseline for comparison. We adapted Saburo Okita’s implementation
of the approach2 and kept most parameters as default except the minimum and
maximum MSER size which are set to 10 and 10000 pixels to keep most MSERs
for further processing.
The second approach is Neumann’s lexicon-free method [19], which has been
implemented in OpenCV3. Neumann’s approach drops the stability requirement
of MSERs and selects suitable extremal regions (ERs) by a sequential classiﬁer
trained for character detection. Gomez’s method [7] is used for grouping arbitrary
oriented text, and characters are recognized by a nearest-neighbour classiﬁer
trained with features of chain-code bitmaps such that its word recognition is
dictionary free.
4.3
Results
Item detection. In item detection, the goal is to localize the query word in
an image. Taking each annotated word in the dataset as a query, we obtain
an overall detection performance. Figure 7 shows the detection recall change by
increasing the detected/non-detected threshold to the overlapping ratio α, where
α and the recall Rdet are deﬁned as in Eq. (1). Gi stands for the ground truth
bounding box of a word and Di is the bounding box of a detected item. For the
many-to-one case where many items are detected within a true word bounding
box, we only count this once in the correctly detected items calculation.
α = Area(Gi ∩Di)
Area(Gi)
,
Rdet = No. correctly detected items
No. relevant items in dataset
(1)
As shown in Fig. 7, our proposed method performs better when 50% overlap is
applied, which is the general criterion in object detection. Our method is penal-
ized when high overlap ratio is required, however this could be compensated in
recognition by applying the n-gram based matching. We also run our system and
the comparison approaches on the item queries with a 50% overlap criterion. As
shown in Table 1, our recall performance outperforms the other two approaches
in most cases except for the orientation and the art query sets. This is proba-
bly because stroke width is rotation invariant and the distance transform is less
aﬀected by the colour changes within characters, while the character spotting
CNN model deployed in our system would require more orientated and art char-
acter samples during training. It can also be seen that our text region detection is
scale sensitive. The system performs well when the character size is similar to the
training samples. Small characters tend to be regarded as noise in our detection,
and the word region growing technique compensates more for the big characters
than for small ones. Both Chen’s and Neumann’s methods encounter diﬃculties
in detecting cursive characters. Chen’s method assumes characters have low vari-
ation in stroke width, and Neumman’s method was designed mainly for block
2 https://github.com/subokita/Robust-Text-Detection.
3 http://docs.opencv.org/3.0-beta/modules/text/doc/text.html.

32
J. Zhou et al.
Fig. 7.
Item
detection
recall
change by increasing α
Table 1. Item detection recall & precision on 6
item query sets
Recall
small middle
big
orientation cursive
art
Proposed 0.369 0.692 0.615
0.338
0.492 0.338
Chen’s
0.231
0.169
0.385
0.430
0.277 0.385
Neumann’s 0.354
0.415
0.154
0.292
0.046
0.308
Precision
small middle big orientation cursive
art
Proposed 0.042 0.070 0.086
0.046
0.090 0.053
Chen’s
0.025 0.019 0.038
0.039
0.028 0.039
Neumann’s 0.105 0.097 0.055
0.102
0.019 0.113
Fig. 8.
Item retrieval precision-
recall curve
Table 2. Item retrieval recall & precision on 6
item query sets
Recall
small middle
big
orientation cursive
art
Proposed 0.385 0.662 0.585
0.292
0.262 0.354
Chen’s
0.015
0.015
0.031
0.0
0.0
0.015
Neumann’s
0.0
0.0
0.0
0.0
0.0
0.0
Precision
small middle big orientation cursive
art
Proposed 0.641 0.878 0.792
0.559
0.85
0.622
Chen’s
1.0
0.167 0.222
0.0
0.0
1.0
Neumann’s
0.0
0.0
0.0
0.0
0.0
0.0
print characters. In contrast, our proposed method can still detect a certain
amount of cursive words. The precision table shows that all methods suﬀer from
a high false detection rate, with Neumann’s method performing a little better
than others overall.
Item retrieval. Detected regions are converted to machine-coded transcription.
An item is recognized and retrieved if its averaged cosine distance to a query is
less than a threshold β. We ﬁx the overlap ratio as 0.1 in detection such that
all three methods can keep most regions for query matching. The retrieval per-
formance will thus be strongly impacted by performance of the text recognition.
As with item detection, we obtain an overall retrieval performance by iterating
through all annotated words in the dataset. Figure 8 plots the Precision-Recall
curves of the three methods. The item retrieval recall Rret and precision Pret
are deﬁned similar to the deﬁnition in item detection by replacing the detected
items with retrieved items.
As shown in the plot, Chen’s and Neumann’s approaches do not perform well
on the dataset. This can also be seen from the results in Table 2, running the
evaluation on the 6 item query sets with distance threshold β 0.5. In Chen’s
approach, Tesseract was used for text recognition, which was originally designed
for scanned document OCR. It assumes print letters on white background and

A Text Recognition and Retrieval System for e-Business Image Management
33
fonts are general sizes used in documents, which is not the case in the proposed
dataset. Neumann’s approach was mainly designed for block characters in scene
text recognition. The character recognition classiﬁer is trained with chain-code
features extracted from synthetic black print letters on white background with
no distortion, blurring, scaling or rotation introduced, while in our dataset there
are a large number of cursive characters and various text patterns.
Image Retrieval. To retrieve images, each returned image contains at least
one detected word matching the input query. Although the retrieval performance
of the system has been demonstrated in item retrieval evaluation, we test our
system with the 110 document queries and obtain a 0.394 mean average precision.
Figure 9 shows retrieved images for an example query “free”. It can be seen that,
even though some detection is not perfect, our system is still capable of ﬂagging
a match owing to the n-grams based string comparison.
Fig. 9. Correctly retrieved image examples for the query “free”
5
Conclusion
In this paper, we address the challenge text recognition for e-business image man-
agement. A method combining neural networks and MSERs for image indexing
is proposed. A novel dataset is prepared specially for text recognition evalua-
tion in e-business images, and our system outperforms other two state-of-the-art
scene text recognition approaches. In future work, the system will be integrated
with other content based image retrieval methods to make image management
even easier.
Acknowledgements. This research has been supported by Science Foundation Ire-
land under grant number SFI/12/RC/2289 which is co-funded by the European
Regional Development Fund.
References
1. Bissacco, A., Cummins, M., Netzer, Y., Neven, H.: PhotoOCR: reading text in
uncontrolled conditions. In: Proceedings of the 2013 IEEE International Con-
ference on Computer Vision. ICCV 2013, pp. 785–792. IEEE Computer Society,
Washington, DC (2013)

34
J. Zhou et al.
2. Buˇsta, M., Neumann, L., Matas, J.: FASText: eﬃcient unconstrained scene text
detector. In: 2015 IEEE International Conference on Computer Vision (ICCV
2015), pp. 1206–1214. IEEE, California, December 2015
3. Chen, H., Tsai, S.S., Schroth, G., Chen, D.M., Grzeszczuk, R., Girod, B.: Robust
text detection in natural images with edge-enhanced maximally stable extremal
regions. In: 2011 IEEE International Conference on Image Processing. Brussels,
September 2011
4. Deselaers, T., Keysers, D., Ney, H.: Features for image retrieval: an experimental
comparison. Inf. Retrieval 11(2), 77–107 (2008)
5. Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke
width transform. In: CVPR, pp. 2963–2970. IEEE (2010)
6. Forss´en, P.E.: Maximally stable colour regions for recognition and matching. In:
IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer
Society, IEEE, Minneapolis, June 2007
7. G´omez, L., Karatzas, D.: Multi-script text extraction from natural scenes. In: Pro-
ceedings of the 2013 12th International Conference on Document Analysis and
Recognition, pp. 467–471. ICDAR 2013. IEEE Computer Society, Washington,
DC (2013)
8. He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural network
for scene text detection. Trans. Img. Proc. 25(6), 2529–2541 (2016)
9. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and arti-
ﬁcial neural networks for natural scene text recognition. In: NIPS Deep Learning
Workshop (2014)
10. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild
with convolutional neural networks. Int. J. Comput. Vis. 116(1), 1–20 (2016)
11. Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting. In:
Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol.
8692, pp. 512–528. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10593-2 34
12. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S.K., Bagdanov, A.D.,
Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F.,
Uchida, S., Valveny, E.: ICDAR 2015 competition on robust reading. In: ICDAR,
pp. 1156–1160. IEEE Computer Society (2015). Relocated from Tunis, Tunisia
13. Koo, H.I., Kim, D.H.: Scene text detection via connected component clustering
and nontext ﬁltering. IEEE Trans. Image Process. 22(6), 2296–2305 (2013)
14. Lew, M.S., Sebe, N., Djeraba, C., Jain, R.: Content-based multimedia informa-
tion retrieval: state of the art and challenges. ACM Trans. Multimedia Comput.
Commun. Appl. 2(1), 1–19 (2006)
15. Li, Y., Lu, H.: Scene text detection via stroke width. In: Proceedings of the
21st International Conference on Pattern Recognition (ICPR2012), pp. 681–684,
November 2012
16. Matas, J., Chum, O., Urban, M., Pajdla, T.: Robust wide baseline stereo from
maximally stable extremal regions. In: Proceedings of the British Machine Vision
Conference, pp. 36.1–36.10. BMVA Press (2002). https://doi.org/10.5244/C.16.36
17. Neumann, L., Matas, J.: Scene text localization and recognition with oriented
stroke detection. In: 2013 IEEE International Conference on Computer Vision
(ICCV 2013), pp. 97–104. IEEE, California, December 2013
18. Neumann, L., Matas, J.: Eﬃcient scene text localization and recognition with local
character reﬁnement. In: 2015 13th International Conference on Document Anal-
ysis and Recognition (ICDAR), pp. 746–750. IEEE, California, August 2015

A Text Recognition and Retrieval System for e-Business Image Management
35
19. Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recog-
nition. IEEE Trans. Pattern Anal. Mach. Intell. 38(9), 1872–1885 (2016)
20. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-
based sequence recognition and its application to scene text recognition. CoRR
abs/1507.05717 (2015)
21. Smith, R.: An overview of the Tesseract OCR engine. In: Proceedings of the Ninth
International Conference on Document Analysis and Recognition - Volume 02, pp.
629–633. ICDAR 2007. IEEE Computer Society, Washington, DC (2007)
22. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: 2011
International Conference on Computer Vision, pp. 1457–1464, November 2011

Accurate Detection for Scene Texts
with a Cascaded CNN Networks
Jianjun Li1(B), Chenyan Wang1, Zhenxing Luo2, Zhuo Tang2, and Haojie Li3
1 School of Computer, Hangzhou Dianzi University, Hangzhou 310018, China
lijjcan@gmail.com
2 The 36th Institute of China Electronics Technology Group Corporation,
Jiaxing, China
3 School of Software, Dalian University of Technology, Dalian 140023, China
Abstract. We propose an algorithm of text detection to accurately and
reliably determine the bounding regions of texts in a natural scene. The
cascaded convolutional neural networks are aggregated in our system in
order to obtain accurate Precision, Recall and F-score (PRF) of text
detection. The ﬁrst fully convolutional network, as a coarse detector, is
in charge of detecting and segmenting areas of text-like. And the sec-
ond network ﬁlters the segment blocks of non-text and accurately deter-
mines each text lines of the segment blocks. In order to make best use of
the advantages of two networks, we proposed an intermediate-processing
mechanism. The whole system has powerful capability of detecting those
squeezed lines with very tiny words and also those texts with diﬀerent
sizes, especially for small size text. Our experimental system is based on
a Titan X GPU and achieves precision of 0.92, recall of 0.83 and F-score
of 0.87, which is listed in the 22nd place among all the published results
of the ICDAR 2013 Focused Scene Text dataset benchmark.
Keywords: Scene text detection
Cascaded convolutional neural networks · Squeezed text lines
1
Introduction
In recent years, text detection and localization in the nature scenes has become a
very hot topic in research areas with rich application scenarios, such as product
packages, a license plate number, and place name. However, due to the diversity
of natural scenes, variation of text contents and fonts in the scene, and uncon-
trollable environmental interference, this topic subject is still a huge challenging
problem, especially for the tiny text detection in the scene.
There are three key contributions in this paper. We proposed a cascaded
CNN networks from coarse to ﬁne. First, the coarse detection network, based
on fully convolutional network for text object, is charging for determining
text block candidates as much as possible, even for those candidates with a
lower possibility. Second, after resolving the issues from the ﬁrst-stage network,
the intermediate-processing mechanism combines strength of the both networks
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 36–47, 2018.
https://doi.org/10.1007/978-3-319-73600-6_4

Accurate Detection for Scene Texts with a Cascaded CNN Networks
37
with smooth connection. Third, small size text, squeezed texts, and text with
illumination interference can be detected in accuracy location of bounding boxes.
2
Relative Work
Traditional text detection methods are on the basis of hand-designed feature,
which is used to distinguish the text and non-text. There are always multiple
substeps in those methods. Tu et al. [1] detect texts through substeps of com-
ponent extraction, analysis, linking and ﬁlter, while MSER and SWT (Stroke
Width Transform) are used to extract components, such as the works in [2,3].
With the development of Convolutional Neural Network (CNN), the mode
of CNN and sliding window replaces manual work. [4,5] use CNN classiﬁers
to elimate false detection. Present progress in object detection and semantic
segmentation brings the new idea for understanding scene texts. TextBoxes [6]
proposed the architecture based Short Semantic Detection (SSD) network [7],
while Gupta et al. [8] used the YOLO’s idea [9]. Tian et al. [10] and Zhong
et al. [11] develop the RPN mechanism. Qin and Manduchi ﬁrstly presented
cascaded idea in [12], combining with Fully Convolutional Network (FCN) [13]
and YOLO. Zhang et al. extended the idea by combining FCN and MSER in
[14]. Nowadays, the mainstream method can be divided into two types: word level
detection and character level detection. The detection of the word level treats
a word as a detection object. Most of the existing methods involve the target
detection technology and the character of word is treated as an independent
target resulting in the existence of multiple bounding boxes for a word, which
greatly aﬀects the accuracy of text detection.
There are some comprehensive surveys can be read, such as [15–19].
3
Methodology
This section presents details of the proposal networks. It includes three keys
that make it reliable and accurate for text localization: coarse locations of text-
like blocks; intermediate-processing mechanism of text-like blocks from the ﬁrst
network; and reﬁnement of text detection as shown in the Fig. 1.
Fig. 1. Pipeline architecture of the method

38
J. Li et al.
3.1
Coarse Location of Text-Like Blocks with First Network
The aim of the ﬁrst network is to obtain the approximate position of the text
blocks including the potential texts from input images. We apply the architecture
of the Holistically-Nested Edge Detection (HED) [20] to train the ﬁrst network
model. HED is based on FCN and popular used to pixel-level text classiﬁcation.
In training phase with HED, we redesign the label map. Figure 2(a) is the original
image in the training set. Figure 2(b) is the label map of ground truth. The label
map is deﬁned by setting all the pixels in the text area of the original image to the
positive sample pixels. The generation of label maps above is relatively simple,
because the text areas in training images is surrounded by regular quadrilateral
bounding boxes, which is given in advance. In test phase, we can obtain text
score map, which records score of the text possibility of per-pixel through trained
model of the ﬁrst network. Then, through intermediate-processing mechanism,
we transform the text score map into a text binary map, as shown in Fig. 1(b).
Fig. 2. Label of coarse location
Fig. 3. Label in reﬁne detection
3.2
Intermediate-Processing Mechanism
Mixture of multi-scale text score maps. We deﬁne that two modes of input
images in the ﬁrst network. Mode I and II represent that the original image has a
height adjustment of 500 and 1000 pixels in constant aspect ratios, just shown in
Fig. 1(a). With the ﬁrst network, we can obtain text score maps in two modes.
According to text score maps, if the probability score (T1) of a pixel for the
test image is greater than 0.2 (T1 > 0.2), then the position is set to 1 (means
a text pixel), otherwise it is set to 0 (means non-text pixels). Label bounding
boxes contain not only word pixels but also background pixels. To retain as much
candidate as possible in the coarse stage, the threshold is low. In this processing,
we transform the text score maps into text binary maps in two modes. As shown
in Fig. 1 (b). And Mode III is mixture of Mode I and Mode II. Finally, we choose
Mode III in our system because it is better to extract small size text.

Accurate Detection for Scene Texts with a Cascaded CNN Networks
39
Text blocks segmentation and merging. As shown in Fig. 1(b), the white
areas of the binary map refers to text areas. The white areas does not look
like the ones of the label map of ground truth in Fig. 2(b), which are reg-
ular rectangles. Therefore, we process the binary map using morphological
methods. With a simple open operation on the binary map, to smooth the
outline of the object, disconnect the narrow connection, remove some promi-
nent small parts, and ﬁll some holes in the binary map. Then, the binary
map is divided into multiple connected blocks according to the connectivity
between the white areas. The shape of these connected blocks is not regular
and they will be surrounded by minimum enclosing rectangles. It can be deter-
mined by the coordinates of two points. The known conditions can be set as:
(BoxMinX, BoxMinY, BoxMaxX, BoxMaxY, BoxHeight, BoxWidth)
The known conditions of binary map is (ImgWidth, ImgHeight). To ensure
that text words in the box is relatively complete, you need to expand the bound-
aries of the bounding box. The ﬁnal expansion bounding box formula is:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
extendBoxMinX = max(1, BoxMinX −BoxWidth ∗T2)
extendBoxMinY = max(1, BoxMinY −BoxHeight ∗T2)
extendBoxMaxX = max(ImgWidth, BoxMaxX + BoxWidth ∗T2)
extendBoxMaxY = max(ImgHeight, BoxMaxY + BoxHeight ∗T2)
(1)
In expression (1), T2 is the expansion threshold. We have done many exper-
iments in T2 = 0.005, 0.01, 0.05, 0.1, ﬁnally, it can better ensure the integrity of
words in the condition when T2 = 0.1.
There are several problems between the text blocks from the ﬁrst network.
The text is repeated and a word is divided into multiple text blocks. So, we need
to merge text blocks after segmentation. Here, we use the graphical approach.
The text blocks set from multiple scales is treated as a set of graph P. The blocks
of text in the collection can be treated as vertices in the graph, expressed as,
V = {v1, v2, · · · , vi, · · · , vj, · · · , vn}
(2)
Expression (2) shows that there are n text blocks in graph P, that is, n vertices. If
there is an overlap between the text block vi and text block vj, we can ﬁnd that
the two vertices are interrelated, that is, the edge vivj exists, and the position
corresponding to the adjacency matrix is set to 1. According to the adjacency
matrix, these text blocks with overlapping relationship in the image are merged
into a group. Thus, a set of P will be divided into multiple groups and each
group will be merged into a larger text block as shown in Fig. 1(c).
Flexibly adjusted for text blocks size. The section, as input of the reﬁne-
ment of text detection, smoothly connect the front jobs. Although the size of
text blocks from the ﬁrst network are various, it can be better handled in the
second detection network when the text blocks ﬂexibly adjusted are in the range
of the receptive ﬁeld. It is more accuracy for detecting squeezed texts through
making reasonable use of this advantage to adjust text blocks sizes. In our sys-
tem, we take the width and height of the text block as the axis, and draw the

40
J. Li et al.
plane space. According to the aspect ratios and the height of text blocks, we
divided space into multiple relationships, classify the text blocks and adaptively
adjust the size of diﬀerent text blocks.
3.3
Reﬁnement of Text Detection
The design of label and anchor. In the section, the network of reﬁnement
of text detection is a strong character-level detector to detect “text blocks”,
as shown in Fig. 4. We use the usual 13 layers of VGG-16 to extract the text
features of input images, removing the ﬁfth pooling layer and the subsequent
full connection layer, then adding the context information features with long
short-term memory (LSTM) architecture and two full convolutional layer, that
is regression layer and classiﬁcation layer. Finally, the trained model is used to
obtain areas (Rois) and scores. The detector is similar as CTPN [10], while the
diﬀerence in our system is following. (1) Redesign Training samples label and
anchor. (2) Remove CTPN’s side-reﬁnement layer and update the loss function.
The detector network is based on anchor ideas in Faster R-CNN [21]. Its
basic detection object is character part level and the detection result is text
line level. Anchor is an important innovation, as the link between the input
image and the feature map. Anchors are distributed on the input image. Feature
map represents the depth feature of the corresponding anchor and determines
the possibility of the anchor contains text. The power of the network carrying
out the back-propagation is to constantly adjust the oﬀset loss of the predicted
bounding box, the ground truth bounding box (label), and the anchor.
Our network has 4 pooling layers in total, and the feature map width will
be 1/16 of the input map. Accordingly, we design the anchor distributed on the
input image with 16-pixel width and 16-pixel stride. The label here is designed
to divide the real word label into a series of 16-pixel wide and the height of the
original words strips, as shown from Fig. 3(a) to (b).
As shown in Fig. 4(a), the 3*3 window is skipped on the feature map, and
the feature vector of each 3*3 window represents the center point of the window,
whose receptive ﬁeld is 228*228. Because of the ﬁxed width of the anchor and
Fig. 4. The network of reﬁnement of text detection

Accurate Detection for Scene Texts with a Cascaded CNN Networks
41
the label, the network is simpliﬁed to regress the center deviation and height
deviation. After extensive observations and experiments, the height of the anchor
is designed as: 11, 16, 23, 33, 48, 68, 97, 139, 198, and 283 pixels of anchor height.
This can detect small target objects, the maximum height of the anchor is also
close to the range of the receptive ﬁeld. The relationship between the anchor,
the predicted and the ground truth bounding box (GT box) is as follows:
vj = (vc, vh)
vc = (cy −ca
y)/ha
vh = log h
ha
v∗
j = (v∗
c, v∗
h)
v∗
c = (c∗
y −ca
y)/ha
v∗
h = log h∗
ha
(3)
In the expression (3), (vc,vh) is the central deviation and height deviation
between the prediction and the anchor. (v∗
c,v∗
h) is the center deviation and height
deviation between the ground truth label and the anchor.
LSTM mechanism. The processes shown from Fig. 4(a) to (b) represent the
increased LSTM mechanism. It helps to solve illumination interference.
At present, LSTM can be widely used to text recognition [22] due to the fact
that the characters are not independent individuals and the use of contextual
relationships can help text recognition.
Here, contextual information is taken into text detection by LSTM. From
Fig. 4(a) to (b), the width of the feature map in (a) is w, and the feature vector
at each location of each line of the feature map is the input string of LSTM, so the
input sequence is expressed as (x1, x2, · · · , xt, · · · , xw). xt is a one-dimensional
vector feature that is obtained by transforming the 3*3 window feature centered
on the t-th point in the row. (x1, x2, · · · , xt, · · · , xw) also represents the deep
feature of the corresponding anchor on the same horizontal direction of the input
image. The score predicted for text of xt is aﬀected by xt itself and the prior (x1,
x2, · · · , xt−1). The new feature map with contextual information is generated
by bi-directional LSTM.
The design of loss function in training. From Fig. 4(b) to (c), the new
feature map from the bi-directional LSTM network layer is the input of two full
convolutional layer at the same level. We call these two layers as the bounding
box regression layer (reg) and classiﬁcation layers (cls) to obtain score and the
regression of central and height deviation of predicted bounding box.
From Fig. 4(c), this is a multi-task learning network including regression layer
and classiﬁcation layers, the loss function is designed as follows:
L(si, vj) = 1/Ns

i Lcl
s (si, s∗
i ) + λ1/Nv

j Lre
v

vj, v∗
j

(4)
Lcl
s , loss function of the score, and we use Softmax loss to distinguish between
text and non-text for the classiﬁcation loss. Lre
v , the loss function that represents
the height and center regression and we use the regression loss, smooth L1 to
calculate it.
si, the prediction of i-th anchor. s∗
i = 0, 1, i-th anchor will be set to 1 or 0,
representing positive anchor (text) and negative anchor (non-text) respectively,

42
J. Li et al.
according to the overlap mechanism, intersection-over-union (IoU), between real
label with anchor. Only if s∗
i = 1, the second item of loss function need to be
calculated. We deﬁne that, overlap mechanism need to meet in the following
constrains (similar with Faster R-CNN): (1) For a GT box, ﬁnd the anchor
corresponding to the GT box with the largest IoU; (2) The IoU between an
anchor and some GT box is greater than 0.7*IoU.
The anchor that satisﬁes any of the above two conditions is a positive anchor.
The negative anchor needs to satisfy that the IoU between it and all GT boxes
should be less than 0.3*IoU.
The generation of the text line in the test. Figure 4(d) is a simple post-
processing that groups the character strips into text lines. We get scores and
Rois of all predicted strips of each “text blocks” by inputting each “text blocks”
of a test image into the second network. We deﬁne that, if the score is bigger
than threshold smin, the prediction is text; otherwise is non-text (smin = 0.7).
In the Fig. 5(a), the predictions meeting score > smin are hold and displayed.
With the non-maximum suppression (NMS) and the similarity of location and
height information for character strips of the same group, the character strips of
same line are grouped and text lines are determined, as shown in Fig. 5(b).
Fig. 5. The generation of the text line
4
Experiment
4.1
Datasets
We evaluated our method on the ICDAR2013 Dataset. It is from the Challenge 2
(Focused Scene Text challenge) of the ICDAR 2013 Robust Reading Competition
[15], including 229 images of training, 233 images of testing, as a typical dataset
for text detection and recognition. Most of the texts are close to horizontal.

Accurate Detection for Scene Texts with a Cascaded CNN Networks
43
4.2
Implementation Details
In our proposed method, two model need be trained. All training images are har-
vested from the ICDAR2017 Multi-lingual scene text (MLT), where we selected
nearly 3000 (2982) Chinese and English images as the basic training samples.
Momentum and weight both are set to 0.9 and 5 × 10−4.
The ﬁrst network is used to generate text blocks, we augment the basic train-
ing samples by means of four angles rotations, three scales and mirror transfor-
mation, and we select about 70K images for training. Initial learning rate is 10−6,
and are multiplied by 1/10 after 30K and 60K iterations. The total number of
iterations is 100K, which consumes an average of 20 h.
As for network of reﬁnement, Training label is created on resized training
samples, 600 pixels short side with an unchanged aspect ratio, so that most
text areas are in range of receptive ﬁelds. Stochastic Gradient Descent (SGD)
is used. Learning rate starts from 10−3, and learning rate becomes 1/10 of the
current learning rate every 20K. The total number of iterations is 60K, which
takes about 7 h.
4.3
Experimental Results
Assessment of multi-scale text score maps. As shown in Table 1, which is
the PRF comparison of three modes, we get three modes of “text blocks” based
on ICDAR13 test images without ﬂexible adjustment, as input of the second
network. Because of the extract of small text blocks from Mode II and big text
blocks from Mode I, Mode III is better to ensure the accuracy, but also improve
the recall rate, therefore, it is be selected in our method.
Assessment of reﬁnement of text detection. In Table 2, line 3 is only using
the reﬁne character-level detector and line 6 is the PRF of CTPN, and we ﬁnd
that the simulation results is closer to CTPN, but there is also signiﬁcant gap.
However, from line 4 and line 5, respectively adding the coarse detector network
based on method of line 3 and adding intermediate-processing into method of
line 4, the PRF of the two lines shows that combine them improves the three
indicators of detection with great eﬀect. Meantime, we use the ﬁrst three lines
of data to prove that the small strips and LSTM are more eﬀective than Faster
R-CNN, the word-level detection method.
Assessment ﬂexible adjustment mechanism for text blocks size. The
ICDAR13 dataset has various scales, but the CTPN uniﬁes the scales at a
short side of 600 pixels. Although some small texts are detected by CTPN,
the localization is not accuracy. In order to make better use of the advantage of
character-level detector network, ﬂexible adjustment mechanism for the size of
“text blocks” is presented as follows.
As shown in Table 3, we obtain ﬂexible adjustment mechanism by analyz-
ing “text blocks” from ICDAR13 training images. There are 791 “text blocks”

44
J. Li et al.
Table 1. Three modes of perfor-
mance comparison after ﬁrst stage
Method
Recall
Precision F-score
Mode I
75.80% 89.12%
81.92%
Mode II
76.44% 88.13%
81.87%
Mode III 78.05% 89.64%
83.44%
Table 2. PRF assessment of cascaded CNN
network
Method
Recall
Precision F-score
Faster RCNN
66.50% 74.01%
70.06%
without LSTM
74.85% 78.80%
76.77%
only reﬁne detector 79.89% 86.47%
83.05%
two detectors
78.05% 89.64%
83.44%
our whole method
82.61% 92.13%
87.11%
CTPN
82.98% 92.98%
87.69%
obtained according to the text binary map of mode III. Because some “text
blocks” in CT0 are too big to input in the second stage, we need to shrink “text
blocks” in CT0 by t0 times; “Text blocks” in CT1 and CT2 contain small size of
texts. In order to obtain better detection, we enlarge the “text blocks” in CT1
by t1 times, while CT2 by t2 time; “Text blocks” in CT3 have topic text lines
but the height is relatively small, so we enlarge the “text block” by t3 times;
“text blocks” in CT5 may be beyond of the range of receptive ﬁeld, and these
are resized by t4 times. Other “text block” are not processed. Through adaptive
learning and extensive experimentation, t0 = t1 = 3, t2 = t4 = 2, t3 = 1.5. These
thresholds ensure that under each CT condition, the size of the text block around
the range of receptive ﬁeld size. We have chosen the most reasonable one in mul-
tiple group options to meet the conditions.
Fig. 6. Comparison of squeezed text detection
Comparisons with state-of-the-art result. The mechanism is used to detect
ICDAR13 test images. The last two lines in Table 2 show that our approach
is very close to CTPN. Our system has some advantages relative to CTPN.
Firstly, as visually shown in Fig. 6, the dense text lines in images can be handled
better, and the bounding boxes are more accurate. And it’s better for detecting
small and squeezed text lines. We deﬁne that small size text. For quantitatively
analyses in our evaluation dataset, the sizes of images are no-uniform, such as
3888*2592, 640*480, 480*640, and 1600*1200, etc. Here, we set the maximum

Accurate Detection for Scene Texts with a Cascaded CNN Networks
45
height is the uniform height of scene images. According to the labels of ground
truth words, if the height of word is smaller than 100 pixel, the word is treat
as small size text. In the experiments, we ﬁnd 361 small size text in evaluation
dataset. We can detect 311 words and the precision is 86.1% in our system, while
CTPN only detect 287 words and the precision is 79.5%. Secondly, CTPN takes
140 ms for testing an image under the scale of 600. In Our method, input image
in the second network is the image block, the test time is about 15–20 ms (an
image is divided into no more than 5 text blocks). Our method is more time-
consuming in the coarse detection, but it makes the obtained positioning more
accurate to follow-up work, such as text recognition.
Table 3. Statistics of “text block”
proportional relationship (ICDAR2013
training set)
Condition Relationship
Number
CT0
w > 1000
49
CT1
0 < h < 50, 0 < w < 50
202
CT2
50 < h < 150, 50 < w < 200 104
CT3
w/h > 2, 0 < h < 100
163
CT4
w/h > 2, 100 < h < 300
154
CT5
w/h > 2, h >= 300
4
CT6
w/h < 1
58
CT7
2 =< w/h <= 2
106
Total
/
791
Table 4. Comparison with other methods
on the ICDAR2013 benchmark (evaluation
criteria: DetEval)
Method
Year Recall Precision F-score
FasText [23]
2015 0.69
0.84
0.77
Neumann and Matas [24] 2015 0.72
0.82
0.77
Neumann and Matas [25] 2016 0.71
0.82
0.76
Text ﬂow [26]
2016 0.76
0.85
0.80
Zhang et al. [27]
2015 0.74
0.88
0.80
Text-CNN [28]
2016 0.73
0.93
0.82
Gupta et al. [8]
2016 0.76
0.92
0.83
TextBoxes [6]
2016 0.83
0.89
0.86
CTPN [10]
2016 0.83
0.93
0.88
Zhang et al. [14]
2016 0.78
0.88
0.83
Zhong et al. [11]
2015 0.83
0.87
0.85
Qin and Manduchi [12]
2017 0.83
0.90
0.86
Proposed
2017 0.83
0.92
0.87
We compare our performance against published results in recent years. As
shown in Table 4, the proposed method achieves 0.83 of Recall, 0.92 of Precision,
and 0.87 of F-score. Compared to the literatures [23–26] in 2015 and 2016, our
approach has a great improvement in the recall rate. Comparing Gupta et al.
[8] and TextBoxes [6], they train the model on a large number of synthetic text
datasets, and our method uses only a small portion of the synthetic data set
in the ﬁrst stage, and only 2982 training samples are used in the second stage.
Comparing Qin and Manduchi [12], the text blocks used in the second stage
needs to be normalized into a uniform size. Diversity of dimensions of “text
blocks” are allowed in our approach.
However, there are great challenges for big size text, single character and the
situation where Contrast to background is very small, as shown in Fig. 7.

46
J. Li et al.
Fig. 7. Examples of failure cases: missing words in yellow boxes (Color ﬁgure online)
5
Conclusion
We have proposed a new approach for text detection and localization, especially
for squeezed tiny and diﬀerent sizes of texts. The system is formed by the cas-
caded of a fully convolutional neural network (FCN) of segmentation and a deep
text detection network, where the latter operates on regions segmented out by
the former. By combining coarse segmentation of FCN in the ﬁrst stage and
reﬁnement detection in the last stage, meanwhile, with the help of Intermediate-
processing mechanism behind the ﬁrst stage, we leverage on the strengths of
each network. Our method is able to very robustly detect the presence of text
in the image, especially for those challenging situations, such as, small size text,
squeezed texts, and text with illumination interference.
References
1. Tu, Z., Ma, Y., Liu, W., et al.: Detecting texts of arbitrary orientations in natural
images. In: Computer Vision and Pattern Recognition, pp. 1083–1090. IEEE (2012)
2. Neumann, L., Matas, J.: Real-time scene text localization and recognition. In:
IEEE Conference on Computer Vision and Pattern Recognition, pp. 3538–3545.
IEEE Computer Society (2012)
3. Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke
width transform. In: Computer Vision and Pattern Recognition, pp. 2963–2970.
IEEE (2010)
4. Jaderberg, M., Simonyan, K., Vedaldi, A., et al.: Synthetic data and artiﬁcial neural
networks for natural scene text recognition. Eprint Arxiv (2014)
5. Jaderberg, M., Simonyan, K., Vedaldi, A., et al.: Reading text in the wild with
convolutional neural networks. Int. J. Comput. Vis. 116(1), 1–20 (2016)
6. Liao, M., Shi, B., Bai, X., et al.: TextBoxes: a fast text detector with a single deep
neural network (2016)
7. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C.:
SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46448-0 2
8. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat-
ural images. In: IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2315–2324. IEEE Computer Society (2016)

Accurate Detection for Scene Texts with a Cascaded CNN Networks
47
9. Redmon, J., Divvala, S., Girshick, R., et al.: You only look once: uniﬁed, real-time
object detection. In: Computer Vision and Pattern Recognition, pp. 779–788. IEEE
(2016)
10. Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with
connectionist text proposal network. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9912, pp. 56–72. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46484-8 4
11. Zhong, Z., Jin, L., Zhang, S., et al.: DeepText: a uniﬁed framework for text proposal
generation and text detection in natural images. Archit. Sci. 12, 1–18 (2015)
12. Qin, S., Manduchi, R.: Cascaded segmentation-detection networks for word-level
text spotting (2017)
13. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3431–3440. IEEE Computer Society (2015)
14. Zhang, Z., Zhang, C., Shen, W., et al.: Multi-oriented Text Detection with Fully
Convolutional Networks. In: Computer Vision and Pattern Recognition. IEEE
(2016)
15. Karatzas, D., Shafait, F., Uchida, S., et al.: ICDAR 2013 robust reading compe-
tition. In: International Conference on Document Analysis and Recognition, pp.
1484–1493. IEEE (2013)
16. Ye, Q., Doermann, D.: Text detection and recognition in imagery: a survey. IEEE
Trans. Pattern Anal. Mach. Intell. 37(7), 1480–1500 (2015)
17. Karatzas, D., Gomez-Bigorda, L., et al.: ICDAR 2015 competition on robust read-
ing. In: 2015 13th International Conference on Document Analysis and Recognition
(ICDAR), pp. 1156–1160 (2015)
18. Zhu, Y., Yao, C., Bai, X.: Scene text detection and recognition: recent advances
and future trends. Front. Comput. Sci. 10(1), 19–36 (2016)
19. Wang, S., Fu, C., Li, Q.: Text detection in natural scene image: a survey. In: Huang,
X.-L. (ed.) MLICOM 2016. LNICST, vol. 183, pp. 257–264. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-52730-7 26
20. Xie, S., Tu, Z.: Holistically-nested edge detection, pp. 1395–1403 (2015)
21. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell.
39(6), 1137–1149 (2017)
22. He, P., Huang, W., Qiao, Y., et al.: Reading scene text in deep convolutional
sequences. 116(1), 3501–3508 (2015)
23. Buta, M., Neumann, L., Matas, J.: FASText: eﬃcient unconstrained scene text
detector. In: IEEE International Conference on Computer Vision, pp. 1206–1214.
IEEE (2015)
24. Neumann, L., Matas, J.: Eﬃcient Scene text localization and recognition with
local character reﬁnement. In: International Conference on Document Analysis
and Recognition, pp. 746–750. IEEE (2015)
25. Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recog-
nition. IEEE Trans. Pattern Anal. Mach. Intell. 38(9), 1872–1885 (2016)
26. Tian, S., Pan, Y., Huang, C., et al.: Text ﬂow: a uniﬁed text detection system in
natural scene images, pp. 4651–4659 (2016)
27. Zhang, Z., Shen, W., Yao, C., et al.: Symmetry-based text line detection in natural
scenes. In: Computer Vision and Pattern Recognition, pp. 2558–2567. IEEE (2015)
28. He, T., Huang, W., Qiao, Y., et al.: Text-attentional convolutional neural network
for scene text detection. IEEE Trans. Image Process. 25(6), 2529–2541 (2016)

Cloud of Line Distribution and Random Forest
Based Text Detection from Natural/Video
Scene Images
Wenhai Wang1, Yirui Wu2, Palaiahnakote Shivakumara3,
and Tong Lu1(&)
1 National Key Lab for Novel Software Technology,
Nanjing University, Nanjing, China
wangwenhai362@163.com, lutong@nju.edu.cn
2 College of Computer and Information, Hohai University, Nanjing, China
wuyirui@hhu.edu.cn
3 Department of Computer System and Information Technology,
University of Malaya, Kuala Lumpur, Malaysia
shiva@um.edu.my
Abstract. Text detection in natural and video scene images is still considered
to be challenging due to unpredictable nature of scene texts. This paper presents
a new method based on Cloud of Line Distribution (COLD) and Random Forest
Classiﬁer for text detection in both natural and video images. The proposed
method extracts unique shapes of text components by studying the relationship
between dominant points such as straight or cursive over contours of text
components, which is called COLD in polar domain. We consider edge com-
ponents as text candidates if the edge components in Canny and Sobel of an
input image share the COLD property. For each text candidate, we further study
its COLD distribution at component level to extract statistical features and angle
oriented features. Next, these features are fed to a random forest classiﬁer to
eliminate false text candidates, which results representatives. We then perform
grouping using representatives to form text lines based on the distances between
edge components in the edge image. The statistical and angle orientated features
are ﬁnally extracted at word level for eliminating false positives, which results in
text detection. The proposed method is tested on standard database, namely,
SVT, ICDAR 2015 scene, ICDAR2013 scene and video databases, to show its
effectiveness and usefulness compared with the existing methods.
Keywords: COLD  Random forest  Text detection in natural scene image
Text detection in video image
1
Introduction
Text detection in natural scene video images is important for many real time appli-
cations such as driving vehicle without pilots and robotic applications to identify place
or person in ofﬁce automatically [1, 2]. It is true that a real dataset usually consists of
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 48–60, 2018.
https://doi.org/10.1007/978-3-319-73600-6_5

mixed data, namely, images and videos captured by a variety of devices. For example,
on Facebook and WhatsApp, we can see both images and videos. This makes the text
detection problem more challenging. Many methods have been developed in the past
years for solving the issues of text detection in natural scene and video images in
multimedia community [1, 2]. However, these methods usually focus on one type of
dataset and may not perform well for different datasets as they report inconsistent
results. In addition, the images uploaded in Facebook and Instagram usually suffer from
contrast variation and poor quality. Achieving better detection rate for such images and
videos is challenging. These factors motivated us to propose a new method for text
detection in both natural and video images in this work, which should also be robust to
multi-type texts, scripts and distortions to some extent.
2
Related Work
The methods of text detection can be classiﬁed broadly into the method which focus on
text in natural scene images, the methods which focus on text in video images and the
methods which focus both on text in natural and video images. In this work, we review
the above three categories to highlight the gap between the state-of-the-art and the
problem mentioned in the Introduction section.
Feng et al. [3] proposed scene text detection based on multi-scale SWT and edge
ﬁltering, which explores edge density and stroke width information. The main focus of
this method is to detect texts from natural scene images, so it expects high contrast
images for achieving better results. Pei et al. [4] proposed multi-orientation scene text
detection with multi-information fusion, which explores a convolutional neural net-
work classiﬁer and an Adaboost classiﬁer for text detection from natural scene images.
In addition, poor results are reported for blurred or low contrast images. Wu et al. [5]
proposed natural scene text detection by multi-scale adaptive color clustering and
non-text ﬁltering. Zheng et al. [6] proposed a cascaded method for text detection in
natural scene images. This method uses two classiﬁers at character level for text
detection. However, the focus of the method is only natural scene images. In addition,
the method does not work well for multi-oriented text images. Yin et al. [7] proposed
robust text detection in natural scene images based on MSER and classiﬁers. Neumann
and Mattas [8] proposed real time scene text localization and recognition, which
explores MSER concept. Li et al. [9] proposed a method for text detection in natural
scene images, which explores a Bayesian classiﬁer and the Markov Random Field
model. Mosleh et al. [10] proposed an automatic inpainting scheme for video text
detection and removal, which explores stroke width transform for text detection in
video. Mittal et al. [11] proposed rotation and script independent text detection from
video frames using sub-pixel mapping. This method explores super resolution concept
for enhancing low contrast text information, and then using descriptor to extract fea-
tures. Despite addressing the challenges of multi-oriented, low contrast and
multi-script, the method reports inconsistent results on different types of data. This is
due to variation in contrast and background for different types of datasets.
Cloud of Line Distribution and Random Forest Based Text Detection
49

In the same way, there are methods for detecting texts in both natural scene and
video images. For example, Wu et al. [12] proposed contour restoration of text com-
ponents for recognition in video and scene images, which restores the loss of edges in
video for text detection and recognition. Shivakumara et al. [13] proposed a fractals
based multi-oriented text detection system for recognition in mobile video images.
Shivakumara et al. [14] also proposed a new multi-modal approach to bib number/text
detection and recognition in Marathon images. These approaches are good for images
which do not suffer much from illumination effect and blur. Recently, deep learning
methods are also explored for text detection in natural scene images. For instance,
Huang et al. [15] proposed robust scene text detection with convolutional neural net-
works induced MSER trees. Jaderberg et al. [16] proposed reading texts in the wild
with convolutional neural networks. It is true that the use of deep learning still has
inherent limitations, such as labeling a large number of samples especially for
non-texts, for which there is no boundary or limit to choose good samples for training.
In addition, setting optimal parameters for different datasets is not so easy. Further-
more, it requires good infrastructure to execute deep learning.
It is noted from the above review that most methods focus on particular data type
but not both natural and video type images which affected by poor quality and contrast
variations. Hence, there is a need to develop robust method which ﬁlls the gap.
Therefore, in this work, we propose a new method based on Cloud of Line
Distribution (COLD) [17], which extracts shapes of text components according to
linearity and non-linearity of contours in polar domain. The proposed method has two
major contributions. Firstly, since the proposed method considers the distribution of
text components in polar domain, the proposed method is robust to distortion to some
extent and is independent of scripts, orientations and text types. Secondly, the com-
putation cost for extracting COLD and constructing related classiﬁers is quite low,
which makes the proposed method highly effective for text detection tasks. Therefore,
the proposed method is appropriate to deploy in embedded systems, which often lack
powerful computation resources. These advantages of the proposed method result in
achieving better results for both natural and video images with less computation cost.
Fig. 1. The COLD for different kinds of components, where (a), (b) and (c) show text
components and their COLD, and (d) gives a non-text component and its COLD, respectively.
50
W. Wang et al.

3
The Proposed Method
It is true that text components in images of different types have different characteristics on
shape, contrast, stroke information, or uniform spacing. Well-designed features are
expected to be robust for the same character even there are rotations, scaling and dis-
tortions, meanwhile distinguishable for different characters. We show several examples
of the proposed COLD for different kinds of components in Fig. 1. It can be observed
from Fig. 1 that the same character (Two “A”) tends to own similar distributions, while
the two different characters (“A” and “E”) have different distributions. Motivated by the
COLD for different kinds of components, we apply COLD for text detection.
Figure 2 gives the overview of the proposed method. The details of the proposed
approach are as follows. Firstly, we propose a method based on COLD for detecting
text candidates. Next, we study the COLD of different k at component level to extract
statistical features and angle oriented features, where k refers to the parameter which
denotes the difference of array indexes between two dominant points on the dominant
points array. Figure 3(b) and (c) show the process of deﬁning dominant point pairs with
k ¼ 1 and k ¼ 2, respectively. Figure 3(d) and (e) show the corresponding result
distributions with log-polar space of (b) and (c). We then feed these features to a
random forest classiﬁer to get character components. After that, we use the character
components to form text lines by aligning the spacing between character components.
Finally, we extract statistical and angle orientated features at text line level and feed
these features to a random forest classiﬁer to eliminate false text lines.
input 
image
text candidate 
detection
random 
forest
training set at 
component level
random 
forest
training set at 
text line level
text 
object
text lines 
construction
character components
extraction
text line construction and 
filtering
Fig. 2. The overview of the proposed method.
(a)
(b)
(d)
(c)
(e)
k=1
k=2
Fig. 3. Illustration of the process of the dominate point pair detection and the corresponding
result distributions respectively, where (a) is the example text region, (b) and (c) describe the
process of deﬁning dominant point pairs with k ¼ 1 and k ¼ 2, respectively, (d) and (e) show the
corresponding result distributions with log-polar space of (b) and (c).
Cloud of Line Distribution and Random Forest Based Text Detection
51

3.1
Text Candidate Detection
For an input image, we extract text candidates by the method based on COLD. We
propose to use the method in [17] to compute COLD. It is observed from Fig. 4 that the
COLD of Canny and Sobel images of character components are almost the same, and
the COLD of Canny and Sobel images of background region are very different.
Therefore, we generate text candidates according to the following steps:
(i)
Get Canny and Sobel images for the input image, compute the COLD
(k ¼ 1; 2; 3) of Canny image, which is named as CC, and similarly compute the
COLD of Sobel image, which is named CS.
(ii)
Find common points between CC and CS. In other word, compute the inter-
section set CI of CC and CS.
(iii)
Compute the corresponding dominant points fdig of CI, and restore the edge image
corresponding to the dominant points di
f g and Canny image of the input image.
(iv)
Get edge components of the restored edge image. The edge components can be
regards as text candidates of the input image.
The results of these four steps are illustrated in Fig. 5, where (a) gives an input
image, (b) and (c) show its Canny and Sobel images, (d) and (e) respectively give the
COLD of (b) and (c), (f) shows the common information between (d) and (e), (g) is the
restored image, and (h) gives edge components.
3.2
Representatives Detection for Text Line Construction
In this subsection, we aim to extract statistical features and angle oriented features from
COLD at component level. For each text candidate, we calculate its COLD distribution
of different k, which is named as Ck (k ¼ 1; 2; 3; . . .; 10), where k refers to the parameter
that denotes the distance on the dominant sequence. We perform experiments to select
k ¼ 10. For each Ck, we calculate distances between every point pairs, which are named
as d1; d2; . . .; dm. Then we extract features flk; rk; hkg as in Eq. 1:
Fig. 4. The result of COLD for Canny and Sobel images of different component. (a) Is character
component, (b) and (c) are Canny and Sobel images for (a), (g) and (h) are COLD of (b) and (c).
Where (d) is background region, (e) and (f) are Canny and Sobel images for (d), (i) and (j) are
COLD of (e) and (f).
52
W. Wang et al.

lk ¼
Pm
i¼1 di
m
rk ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n
Pm
i¼1 di  lk
ð
Þ2
q
hk ¼ PCAðCkÞ
8
>
>
<
>
>
:
ð1Þ
where lk and rk refer to the mean and standard deviation value of distances, respec-
tively, hk represents the angle of Ck, function PCAðÞ represents the operation of
Principal Component Analysis (PCA). Here we use PCA to calculate the angle of Ck.
(a) (b)
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
(c)
(d) (e)
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
(f)
Fig. 6. The feature construction at component level, where (a) is a text candidate, (b) is restored
edge image of (a), (c) is COLD for (b) of different k k ¼ 1; 2; 3; . . .; 10
ð
Þ, (d) is a false text
candidate, (e) is restored edge image of (d), (f) is COLD for (e) of different k k ¼ 1; 2; 3; . . .; 10
ð
Þ.
(a)
(h)
(b)
(c)
(g)
(d)
(e)
(f)
Fig. 5. The overview of text candidates detection based on COLD, where (a) is the input image,
(b) and (c) are Canny and Sobel images of the input image, (d) and (e) are COLD of (b) and (c),
(f) is common information between (d) and (e), (g) is the restored edge image corresponding to
the common information in COLD, (h) gives edge components of the restored edge image.
Cloud of Line Distribution and Random Forest Based Text Detection
53

Next, we combine the mean, standard deviation and angle of each Ck to combine
them into a 30 dimensional feature vector. Feature construction at component level for
text and non-text examples are shown in Fig. 6, where we can ﬁnd that with the
increasing of k, the h of text components changes less than non-text components, and
the distribution of text components will be more scattered and then tend towards
stability, while that of non-text components cannot be stabilized. At last, we apply the
feature vector at component level in a random forest classiﬁer to assign labels for each
candidate. Example results after eliminating false text candidates are shown in Fig. 7.
3.3
Text Lines Construction
This subsection presents text line extraction by grouping text candidates given by the
previous step. Let the text candidate set be S, which provides coarse locations of texts.
To draw bounding boxes for extracted text lines, the proposed method groups text
candidates which share the common shape properties such as height, scale and ori-
entation. Speciﬁcally, the proposed method extracts multi-oriented texts with the
conditions in Eq. 2:
fq p; q
ð
Þ ¼ 1; if
2=3  Hp=Hq

  3=2
hp  hq

  p=8
fd p; q
ð
Þ  ðWp þ WqÞ=2

  Hp þ Hq
8
<
:
ð2Þ
where H, W and h represent height, width and orientation of a text region, respectively,
and fdðp; qÞ refers to the distance between the center of p and that of q. Note that the
parameters in Eq. 2 are determined experimentally. If a text candidate satisﬁes this
property, the method groups it. In this way, the proposed method groups text candi-
dates into text lines. Then we extract the feature vector from COLD of every text line as
the previous step. Feature construction at text line level is shown in Fig. 8. At last, we
feed the feature vector at text line level to a random forest classiﬁer to eliminate false
text lines. The effect of these steps is shown in Fig. 9.
(a)
(b)
Fig. 7. The result after eliminating false text candidates, where (a) is the result of text
candidates, and (b) is the result after eliminating false text candidates.
54
W. Wang et al.

4
Experiments
To evaluate the proposed method, we consider four benchmark databases, namely,
ICDAR 2013 scene, ICDAR 2015 scene, ICDAR 2013 video, Street View Text (SVT).
To calculate measures for text detection in both natural scene images and video frames,
we follow the standard evaluation scheme as in the ICDAR robust competition [18].
According to the instructions given in [18], we calculate Recall, Precision and
F-measure to evaluate the performance of the proposed method. Moreover, we record
time-cost per image on a PC (2.9 GHz 2-core CPU, 8G RAM, no GPU device
involved) to compare the computation cost. In order to show the effectiveness of the
proposed method, we use available codes of Yin et al. [7], Neumann and Mattas [8] and
Li et al. [9], which use the MSER concept for text detection. In the same way, we
implement Wu et al.’s method [12], which explores character shape restoration for text
detection in both natural and video images, and Huang et al.’s method [15] which uses
the concepts of MSER and convolutional neural networks for text detection. On top of
this, we also reported the results listed in [19] for the database of ICDAR 2015.
(c)
(d) (e)
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
(f)
(a)
(b)
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
Fig. 8. The feature construction at text line level, where (a) is a text line, (b) is restored edge
image of (a), (c) is COLD distribution of different k (k = 1, 2, 3, …, 10), (d) is a false text line,
(e) is restored edge image of (d), and (f) is COLD distribution of different k (k = 1, 2, 3, …, 10).
(a)
(b)
(c)
Fig. 9. The result of text line, where (a) is the result after eliminating false text candidates, (b) is
the result of text lines after grouping text candidates by heuristic method, (c) is the result after
eliminating false text lines.
Cloud of Line Distribution and Random Forest Based Text Detection
55

Note that the training set for random forest at component level is extracted from
several benchmark datasets, namely, ICDAR 2013 scene/video, ICDAR 2015 scene,
and SVT. There are totally 109545 components (25980 text components and 83565
non-text components) for training. The training dataset for random forest at text line
level is extracted from these datasets as well. There are totally 9045 text lines (8242
true text lines and 803 false text lines) for training.
We train the random forest classiﬁer with the parameters as follows. The number of
the trees in the forest is set as 10. The function to measure the quality of a split is Gini
impurity. The number of features to consider when looking for the best split is set as 8.
The maximum depth of the tree is set as 6, and the minimum number of samples
required to split an internal node is set as 2. Finally, the minimum number of samples
required to be at a leaf node is set as 1.
Fig. 10. The ROC curve of different k. The blue curve is the ROC curve of k = 10. (Color ﬁgure
online)
Table 1. Performance of text detection on ICDAR 2013 scene.
Method
Precision Recall F-measure Time-cost (s)
Proposed
0.87
0.66
0.75
1.42
Yin et al. [7]
0.84
0.65
0.73
1.73
Neumann and Matas [8] 0.74
0.63
0.68
0.40
Wu et al. [12]
0.78
0.62
0.69
1.93
Huang et al. [15]
0.86
0.67
0.75
3.23
Table 2. Performance of text detection on SVT.
Method
Precision Recall F-measure Time-cost (s)
Proposed
0.77
0.64
0.70
1.47
Yin et al. [7]
0.41
0.66
0.51
1.89
Neumann and Matas [8] 0.65
0.63
0.64
0.61
Wu et al. [12]
0.77
0.65
0.70
2.05
Huang et al. [15]
0.74
0.67
0.70
4.03
56
W. Wang et al.

4.1
Experiments on Feature of Different k
We choose 100 images randomly from all the databases. For these 100 images, we
calculate ROC curves of the random forest classiﬁer, which is trained on different
feature vectors of different k k ¼ 2; 4; 6; 8; 10; 12
ð
Þ. It is observed from Fig. 10 that the
random forest classiﬁer performs the best when k ¼ 10.
4.2
Experiments on Natural Scene Images
Quantitative results on natural scene images of the proposed and the existing methods
are reported in Tables 1, 2 and 3 for the ICDAR 2013 scene data, SVT and ICDAR
2015 scene data, respectively. Table 1 shows that the proposed method is the best at
F-measure and Precision, while Recall and time-cost is the second best for ICDAR
2013 scene database. This shows the effectiveness of the proposed method upon tra-
ditional text detection methods and some of the CNN-based methods. Neumann and
Matas method [8] is designed for real-time text detection, which achieves the best result
(0.4 s) in computation cost. However, it achieves a much lower F-score (0.48) than the
proposed method (0.75). Huang et al.’s method [15] is as good as the proposed method
in F-score. However, running a CNN-based architecture could be time-consuming
without powerful GPU device, which could be proved by its largest time-cost (2.83 s).
Similarly, Table 2 shows the proposed method scores the best results for SVT dataset
with reasonable computation cost. For ICDAR 2015 scene, the existing methods offer
less information about both codes and cost-time. We simply copy the result of the
existing methods to make a comparison. It is observed from Table 3 that the proposed
method scores the third best results at Recall, while Precision and F-measure are the
second best for ICDAR 2015 scene database compared with the existing methods.
Considering that most of the existing methods list in Table 2 are CNN-based methods,
we can notice the high computation cost without the support of GPU device, which are
often true in embedded systems. Detection examples of the proposed method on
ICDAR 2013 scene, ICDAR 2015 scene and SVT are shown in Fig. 11. We can notice
the proposed method is robust to distortion to some extent and is independent of
scripts, orientations and text types since we use the distribution of text components in
polar domain for features.
Table 3. Performance of text detection on ICDAR 2015 scene.
Method
Precision Recall F-measure
Proposed
0.70
0.38
0.49
CNN Pro.
0.35
0.34
0.35
Deep2Text
0.50
0.32
0.39
HUST
0.44
0.38
0.41
AJOU
0.47
0.47
0.47
NJU-Text
0.70
0.36
0.47
StradVision1 0.53
0.46
0.50
StradVision2 0.77
0.37
0.50
Cloud of Line Distribution and Random Forest Based Text Detection
57

4.3
Experiments on Video Images
Quantitative results on video images of the proposed and the existing methods are
reported in Table 4 for ICDAR 2013 video data. Since video images are greatly
affected by low-contrast, multi orientation and non-uniform illumination compared to
scene images, we can notice the drop in F-measure for the proposed method and Yin
et al. [7] in Table 4. The proposed method is still the best at Precision and F-measure,
while Recall is the second best for the ICDAR 2013 video data. We can notice the
proposed method achieves the best detection result with the reasonable time-cost. This
also shows the consistence of the proposed method when dealing with datasets of
various features. Detection examples of the proposed method on ICDAR 2013 video
are shown in Fig. 12.
Fig. 11. Detection examples of the proposed method on ICDAR 2013 scene, ICDAR 2015
scene and SVT.
Table 4. Performance of text detection on ICDAR 2013 video
Method
Precision Recall F-measure Time-cost (s)
Proposed
0.65
0.63
0.64
1.56
Mosleh et al. [10] 0.50
0.49
0.49
0.81
Yin et al. [7]
0.64
0.57
0.60
1.76
Li et al. [9]
0.46
0.70
0.56
1.98
Fig. 12. Detection examples of the proposed method on ICDAR 2013 video.
58
W. Wang et al.

5
Conclusion
In this paper, we have proposed a new method for text detection in natural scene, video
images. We propose COLD for detecting text candidates. For text candidates, the
proposed method ﬁrst extracts statistical features and angle oriented features of text
candidates. To eliminate false text candidates, we then feed feature vectors at com-
ponent level to a random forest. Next, we perform grouping useful representatives to
form text lines based on the distance between edge components in the edge image. The
statistical and angle orientated features are ﬁnally extracted at text line level for
eliminating false positives. We have tested the proposed method on different databases
to show the effectiveness of the proposed method. Our future work includes the further
improvement of the proposed method and the discovery on distribution patterns of
more text types.
Acknowledgements. This work was supported by the Natural Science Foundation of China
under Grant 61672273, Grant 61272218, and Grant 61321491, the Science Foundation for
Distinguished Young Scholars of Jiangsu under Grant BK20160021, the Science Foundation of
JiangSu under Grant BK20170892, the Fundamental Research Funds for the Central Universities
under Grant 2013/B16020141, and the open Project of the National Key Lab for Novel Software
Technology in NJU under Grant KFKT2017B05.
References
1. Ye, Q., Doermann, D.: Text detection and recognition in imagery: a survey. IEEE Trans.
PAMI 1480–1500 (2015)
2. Yin, X.C., Zuo, Z.Y., Tian, S., Liu, C.L.: Text detection, tracking and recognition in video: a
comprehensive survey. IEEE Trans. Image Process. 2752–2773 (2016)
3. Feng, Y., Song, Y., Zhang, Y.: Scene text detection based on multi-scale SWT and edge
ﬁltering. In: Proceedings of ICPR, pp. 634–639 (2016)
4. Pei, W.Y., Yang, C., Kau, L.J., Yin, X.C.: Multi-orientation scene text detection with
multi-information fusion. In: Proceedings of ICPR, pp. 646–651 (2016)
5. Wu, H., Zou, B., Zhao, Y.Q., Chen, Z., Zhu, C., Guo, J.: Natural scene text detection by
multi-scale adaptive color clustering and non-text ﬁltering. Neurocomputing 1011–1025
(2016)
6. Zheng, Y., Li, Q., Ju, J., Hu, H., Li, G., Zhang, S.: A cascaded method for text detection in
natural scene images. Neurocomputing 1–9 (2017)
7. Yin, X., Yin, X., Huang, K., Hao, H.: Robust text detection in natural scene images. IEEE
Trans. PAMI 36(5), 970–983 (2014)
8. Neumann, L., Matas, J.: Real-time scene text localization and recognition. In: Proceedings of
CVPR, pp. 3538–3545 (2012)
9. Li, Y., Jia, W., Shen, C., Hengel, A.V.D.: Characterness: an indicator of text in the wild.
IEEE Trans. IP 23(4), 1666–1677 (2014)
10. Mosleh, A., Bouguila, N., Hamza, A.B.: Automatic inpainting scheme for video text
detection and removal. IEEE Trans. IP 22(11), 4460–4472 (2013)
11. Mittal, A., Roy, P.P., Singh, P., Raman, B.: Rotation and script independent text detection
from video using sub pixel mapping. Vis. Commun. Image Represent. (2017, to appear)
Cloud of Line Distribution and Random Forest Based Text Detection
59

12. Wu, Y., Shivakumara, P., Lu, T., Lim Tan, C., Blumenstein, M., Kumar, G.H.: Contour
restoration of text components for recognition in video/scene images. IEEE Trans. IP 25(12),
5622–5634 (2016)
13. Shivakumara, P., Wu, L., Lu, T., Tan, C.L.: Fractals based multi-oriented text detection
system for recognition in mobile video images. Pattern Recogn. 158–174 (2017)
14. Shivakumara, P., Raghavendra, R., Qin, L., Raja, K.B., Lu, T., Pal, U.: A new multi-modal
approach to bib number/text detection and recognition in Marathon images. Pattern Recogn.
479–491 (2017)
15. Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolution neural network
induced MSER trees. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014.
LNCS, vol. 8692, pp. 497–511. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10593-2_33
16. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with
convolutional neural networks. IJCV 116(1), 1–20 (2016)
17. He, S., Schomaker, L.: Beyond OCR: multi-faceted understanding of handwritten document
characteristics. Pattern Recogn. 321–333 (2017)
18. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., Boorda, L.G.I., Mestre, S.R., Mas, J.,
Mota, D.F., Almazan, J.A., De las Heras, L.P.: ICDAR 2013 robust reading competition. In:
Proceedings of ICDAR, pp. 1115–1124 (2013)
19. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanow, A., Iwamura, M.,
Matas, J., Neumann, L., Chandrsekhar, V.R.: ICDAR 2015 competition on robust reading.
In: Proceedings of ICDAR, pp. 1156–1160 (2015)
60
W. Wang et al.

CNN-Based DCT-Like Transform for Image
Compression
Dong Liu(B), Haichuan Ma, Zhiwei Xiong, and Feng Wu
CAS Key Laboratory of Technology in Geo-Spatial Information Processing
and Application System, University of Science and Technology of China, Hefei, China
dongeliu@ustc.edu.cn
Abstract. This paper presents a block transform for image compres-
sion, where the transform is inspired by discrete cosine transform (DCT)
but achieved by training convolutional neural network (CNN) models.
Speciﬁcally, we adopt the combination of convolution, nonlinear map-
ping, and linear transform to form a non-linear transform as well as a
non-linear inverse transform. The transform, quantization, and inverse
transform are jointly trained to achieve the overall rate-distortion opti-
mization. For the training purpose, we propose to estimate the rate by the
l1-norm of the quantized coeﬃcients. We also explore diﬀerent combina-
tions of linear/non-linear transform and inverse transform. Experimen-
tal results show that our proposed CNN-based transform achieves higher
compression eﬃciency than ﬁxed DCT, and also outperforms JPEG sig-
niﬁcantly at low bit rates.
Keywords: Convolutional neural network (CNN)
Discrete cosine transform (DCT) · Rate-distortion optimization
Transform
1
Introduction
Transform coding is a fundamental technique in modern image and video compres-
sion solutions. Almost all of the existing lossy compression schemes adopt trans-
form, but in diﬀerent forms. For example, JPEG adopts 8 × 8 discrete cosine trans-
form (DCT) [1], JPEG2000 adopts several kinds of discrete wavelet transform [2],
MPEG-4 AVC/H.264 adopts integer cosine transform at diﬀerent block sizes like
4 × 4 and 8 × 8 [3], which is inherited and extended to more block sizes in HEVC [4].
The pursuit of higher compression eﬃciency continuously inspires the emergence
of new transform tools, such as graph-based transform [5].
Traditionally, transform is derived based on the signal processing theory,
which often assumes stationary signal to claim the optimum of the derived
transform. However, the image/video signal is non-stationary and usually too
complicated to be analyzed precisely. Then, how to derive transform that works
better for image/video is a diﬃcult problem that probably goes beyond the cur-
rent signal processing theory.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 61–72, 2018.
https://doi.org/10.1007/978-3-319-73600-6_6

62
D. Liu et al.
In recent years, deep learning receives more and more attention especially in
the ﬁelds of computer vision and image processing. There have been some works
studying deep learning-based image/video compression, which is demonstrated
to be a promising direction [6–14].
A large portion of previous works, such as [6–11], are devoted to end-to-end
training of deep network models for image compression. Indeed, these works were
inspired by the paradigm of auto-encoder, which originates from the well-known
work of Hinton and Salakhutdinov [15]. The original auto-encoder network con-
sists of two parts that perform “encoding” and “decoding,” respectively, and is
pre-trained layer by layer and then ﬁne-tuned end to end. In the original auto-
encoder network, the dimension of “encoded” features is conﬁgured to be less
than the dimension of input signal, so that the “encoding” part of the network
performs non-linear dimensionality reduction; but the features are ﬂoating-point
numbers and thus the network is not quite useful for practical compression. In
the following works [6–11], a quantization layer is inserted into auto-encoder to
convert the “encoded” features into bits or integers that are then collected into
bit-stream. These works can be categorized into two approaches. The ﬁrst app-
roach works at block level, converts blocks of ﬁxed size (32 × 32) into bits [6–8].
The second approach works at image level by converting the entire input image
into integers [9–11]. The diﬀerence between these two approaches are similar to
that between JPEG and JPEG2000.
Although the existing works [6–11] have demonstrated the great potential of
using auto-encoder-like schemes for image compression, there are several limita-
tions in these works. First, the networks using binary quantization [6–8] cannot
provide arbitrary bitrate since the possible bitrates are predeﬁned by the net-
work structure. Second, the networks working at image level lack the ﬂexibility
of block-based image/video compression, i.e. enabling adaptive mode selection
at block level [3,4]. Third, the new schemes are totally diﬀerent from the existing
standards (JPEG, JPEG2000, H.264, HEVC, etc.), so it is diﬃcult to integrate
them into the standards, or to reuse the existing techniques in the standards.
In this paper, we study a deep learning-based method for transform coding.
We also follow the paradigm of auto-encoder to train an end-to-end convolu-
tional neural network (CNN) model. But diﬀerent from the previous works, our
key idea is to ﬁnd ways to improve the existing DCT rather than to develop
a totally new tool. Thus, our trained transform can replace DCT so as to be
seamlessly integrated into the block-based image/video compression standards,
such as HEVC. Our network works at block level to embrace the ﬂexibility of
block-based schemes, and our network adopts multi-level quantization rather
than binary quantization so as to enable arbitrary bitrate. In addition, we make
the following new contributions compared to previous works. First, we inves-
tigate asymmetric auto-encoder schemes in order to reduce the computational
complexity of encoding or decoding. Second, we propose to use the l1-norm
of quantized coeﬃcients to estimate rate when training the network for rate-
distortion optimization.

CNN-Based DCT-Like Transform for Image Compression
63
The remainder of this paper is organized as follows. Section 2 reviews the
related works. Section 3 discusses the proposed method. Section 4 presents the
experimental results, followed by conclusions in Sect. 5.
2
Related Works
Deep learning-based image/video compression has become an emerging topic
and received much attention in the recent two years [6–14]. Most of the previous
works are devoted to auto-encoder-like schemes for image compression. As men-
tioned above, these works can be categorized into two approaches, block-level
and image-level.
For networks working at block level, Toderici et al. proposed a scheme based
on recurrent neural network (RNN) models [6]. The network has a binary quan-
tization layer inserted after the “encoding” part of auto-encoder, so the bitrate
is predeﬁned by the network structure. In order to achieve variable bitrates,
either multiple auto-encoders are cascaded, or RNN is run for multiple itera-
tions. Later on, they improved the scheme by further compressing the network
output bits using a carefully designed RNN-based arithmetic coding method [7].
More recently, Johnston et al. further improved the scheme by enabling priming
in RNN and allocating diﬀerent bitrates to diﬀerent blocks [8]. All these works
adopt binary quantization, so the bitrate cannot be made arbitrary.
For networks working at image level, Balle et al. proposed a scheme based on
fully convolutional network models [9]. The network adopts multi-level quantiza-
tion (i.e. quantization to integers) rather than binary quantization, so diﬀerent
quantization steps naturally lead to arbitrarily variable bitrates. In addition, to
train the network including quantization, they proposed a new loss function to
approximate rate-distortion cost. Theis et al. [10] and Rippel and Bourdev [11]
also proposed similar schemes but with diﬀerent network structures and diﬀer-
ent ways to approximate rate-distortion cost. These schemes are not easy to be
integrated into the existing standards like HEVC.
Besides the auto-encoder-like schemes, there are some other explorations of
deep learning-based image/video compression. For example, Jiang et al. pro-
posed a CNN-based framework for image compression, where an input image is
down-scaled by a CNN, compressed, and then up-scaled by another CNN [12];
Baig and Torresani proposed a deep learning-based method for colorization, and
adopted the trained network to compress the chrominance components of images
[13]; Prakash et al. proposed an image compression method with CNN-based pre-
diction of perceptual distortion [14]. Nonetheless, there are only a few researches
of deep learning-based predictive coding and transform coding, which are worthy
of further investigation.
3
The Proposed Method
3.1
Mathematical Formulations
We ﬁrst deﬁne some symbols for describing block transform for image com-
pression. An original image block B ∈RN×N is transformed into coeﬃcients

64
D. Liu et al.
C ∈RN×N, and then quantized into integers I ∈ZN×N. The quantized coef-
ﬁcients are inversely transformed to reconstruct the image block ˆB ∈RN×N.
There are three steps performing transform, quantization, and inverse trans-
form, respectively:
C = fE(B)
(1)
I = fQ(C)
(2)
ˆB = fD(I)
(3)
In this paper we study the case of N = 32, to be compared with the previous
works [6–8]. Please note that the amount of coeﬃcients is identical to the amount
of pixels, i.e. no dimensionality reduction is performed, which is diﬀerent from
most of the previous works that follow [15].
Our aim is to derive a good transform for natural images via deep learning.
That is, the transform, quantization, and inverse transform functions are inte-
grated into a uniﬁed deep network; given a set of image blocks {Bi}, the deep
network can be trained end to end in order to optimally compress the blocks. We
consider to optimize the joint rate-distortion cost, where distortion is measured
by mean-squared-error (MSE):
D =

i
||Bi −ˆBi||2
2
(4)
There is a diﬃculty to measure the rate, which is dependent on the entropy
of Ii, but the entropy is not diﬀerentiable and thus cannot be put directly into
the deep network for training. There have been several proposals to estimate
the entropy with mathematical tractable functions [9–11], but these functions
are computationally expensive. In this paper, we are motivated by the well-
known rate-ρ relation in image/video coding, which states the rate is highly
dependent on ρ, the amount of non-zero quantized coeﬃcients [16]. Actually, ρ
is the l0-norm of the quantized coeﬃcients, and l0-norm minimization implies
sparsity constraint [17], but l0-norm is mathematically intractable. We further
replace l0-norm with l1-norm since l1-norm minimization also implies sparsity
constraint but is much easier to solve [17]. Therefore, we propose to measure the
l1-norm of the quantized coeﬃcients to estimate the rate:
R =

i
||Ii||1
(5)
Then, the joint rate-distortion cost is used as the total loss function to train the
deep network,
{f ∗
E, f ∗
Q, f ∗
D} = arg
min
fE,fQ,fD (D + λR)
(6)
where λ is the Lagrangian multiplier. Note that for diﬀerent λ values, the optimal
network parameters can be diﬀerent.
In this paper we focus on the transform and the inverse transform, so for
simplicity we ﬁx the quantization function to be rounding, i.e. fQ(·) = [·]. Since
the rounding function is not diﬀerentiable, as a workaround, we adopt the trick
proposed in [6], i.e. f ′
Q(·) ≈1.

CNN-Based DCT-Like Transform for Image Compression
65
Fig. 1. The network to train a non-linear transform and a non-linear inverse transform,
called symmetric network hereafter. For each convolutional layer, the shown numbers
indicate kernel size (such as 3 × 3) and the amount of feature maps (such as 64). Each
convolutional layer is followed by a non-linear mapping, which is rectiﬁed linear unit
(ReLU) [18] in this paper (i.e. f(x) = max(0, x)). The full-connection layers have 1024
inputs and 1024 outputs. The quantization layer performs rounding.
3.2
Non-linear Transform and Non-linear Inverse Transform
We now introduce the transform and the inverse transform realized by deep
network. Firstly, we investigate the combination of a non-linear transform and
a non-linear inverse transform, which is depicted in Fig. 1. The network can
be interpreted as to perform sequentially: pre-processing on the original block
(with four convolutional layers), linear transform (with full-connection layer),
quantization, linear inverse transform (with full-connection layer), and post-
processing to produce the reconstructed block (with four convolutional layers).
For this network, if we omit the pre- and post-processing parts, and initialize
the full-connection layers as DCT and inverse DCT, respectively, then the entire
network actually performs DCT, quantization, and inverse DCT. When adding
the pre- and post-processing parts and training the entire network end to end,
we expect it to outperform the original DCT for image compression, because the
trained network is tuned according to the given training data to minimize the
joint rate-distortion cost.
Compared our network with the previously designed auto-encoder-like
schemes, there are several notable diﬀerences. First, there is no change of
scale/resolution throughout our network, but down- and up-scaling are exten-
sively used in [7,9]. Second, there is full-connection layers in our network, but pre-
vious works adopt merely convolutions [7,9]. We would like to remark that con-
volutions do not naturally remove the spatial redundancy among pixels, and thus
the combination of convolutions and down-scaling is necessary like in JPEG2000.
However in our network, we use the full-connection layer to remove the spatial
redundancy, thus avoid down-scaling.

66
D. Liu et al.
Fig. 2. Two asymmetric networks with simpliﬁed encoder and simpliﬁed decoder,
respectively.
3.3
Linear and Non-linear Transform and Inverse Transform
We also investigate two diﬀerent networks that are depicted in Fig. 2. Compared
to the network in Fig. 1, the two networks in Fig. 2 either omit the pre-processing
parts so that the original block is directly linearly transformed to coeﬃcients
(Fig. 2(a)), or omit the post-processing parts so that the quantized coeﬃcients
are directly linearly transformed to the reconstructed block (Fig. 2(b)). We call
these two networks asymmetric networks and in contrast the network in Fig. 1
is called symmetric. To the best of our knowledge, we are the ﬁrst to study
asymmetric networks in the paradigm of auto-encoder. For practical compres-
sion, one obvious advantage of asymmetric auto-encoder is the reduction of com-
putational complexity in either encoder or decoder. For example, the simpliﬁed
encoder network (Fig. 2(a)) is suitable for usage when an image is uploaded from
a mobile device to the cloud, while the simpliﬁed decoder network (Fig. 2(b)) is
suitable when an image is downloaded from the cloud to a mobile device, where

CNN-Based DCT-Like Transform for Image Compression
67
the mobile device has limited computational resource but the cloud has much.
Thus, we would like to observe their compression performance experimentally.
(a) Our simpliﬁed encoder, 0.272 bpp,
31.95 dB
(b) Toderici et al. , 0.250 bpp, 27.63 dB
(c) Fixed 32×32 DCT, 0.245 bpp, 30.46
dB
(d) JPEG, 0.265 bpp, 29.73 dB
Fig. 3. Reconstructed images for kodim07 at around 0.25 bpp.
4
Experimental Results
4.1
Training
We use the Uncompressed Colour Image Database (UCID) that consists of 1,338
natural images [19] to prepare the training data. The images are converted to
grayscale and divided into 32 × 32 blocks, and pixel values are divided by 255
to fall into the range [0, 1]. No other processing step is involved. We use Caﬀe
[20] and stochastic gradient descent (SGD) to train the networks depicted in
Figs. 1 and 2. The full-connection layers are initialized by the weights of DCT
and inverse DCT, respectively. We observed that such initialization helps to
converge very fast. Each network is ﬁrst trained by setting λ = 0, i.e. only
considering distortion, until convergence. At this time, the reconstruction quality
is usually very high (more than 45 dB), showing that the network is well trained.
Next, for a given λ, we proceed to train the network until convergence. Thus,
for diﬀerent λ values the trained network parameters are diﬀerent, which can

68
D. Liu et al.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
bitrate (bpp)
22
24
26
28
30
32
34
36
PSNR (dB)
Toderici et al.
JPEG
DCT 32x32
Our Simp Dec
Our Simp Enc
Our Sym Net
JPEG2000
HEVC intra
(a) kodim07
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
bitrate (bpp)
22
24
26
28
30
32
34
PSNR (dB)
(b) kodim14
Fig. 4. Rate-distortion curves of diﬀerent methods on two test images.
be used to provide variable bitrates for the same input. In this paper, the λ
values are chosen from {10−4, 8 × 10−5, 4 × 10−5, 2 × 10−5, 10−5, 5 × 10−6}. As
λ decreases, both reconstruction quality and bitrate increase, showing the loss
function indeed plays the role of joint rate-distortion cost.
4.2
Image Compression with the Trained Networks
When using a trained network for compression, the network is split into encoder
and decoder at the quantization (rounding) layer. An image to compress is
divided into 32 × 32 blocks, with its pixel values divided by 255, then input into
the encoder to achieve integer codes. The integer codes are fed into decoder to
reconstruct blocks. To further compress the integer codes, note that our trained
transform is improved upon DCT (and the full-connection layer is initialized
by DCT), we assume the transform coeﬃcients are corresponding to DCT coef-
ﬁcients, and thus we can reuse the entropy coding method designed for DCT
coeﬃcients. In experiments, we implement such an entropy coding method: for
the ﬁrst coeﬃcient, corresponding to direct current (DC), it is predicted from
the DC of the previous block, and the prediction residue is coded; for the other
coeﬃcients, corresponding to alternating current (AC), they are arranged in
the zigzag scanning order, and then the run-length of 0’s is coded followed by
each non-zero coeﬃcient; all the syntax elements are collected by a multi-level
arithmetic encoder [21].
4.3
Compared Methods
We use the 24 images in the Kodak image dataset1 to test the performance. The
raw color images are converted to grayscale before compression. We compare
the three networks presented in this paper, i.e. the symmetric network (shown
1 http://r0k.us/graphics/kodak/.

CNN-Based DCT-Like Transform for Image Compression
69
in Fig. 1), the simpliﬁed encoder and the simpliﬁed decoder (shown in Fig. 2(a)
and (b), respectively). Besides, we compare with ﬁxed 32 × 32 DCT: for fair
comparison, it uses the same entropy coding method designed by ourselves. We
also compare with JPEG, JPEG2000, HEVC intra, and the method proposed by
Toderici et al. [7]. For JPEG and JPEG2000 we use the implementation provided
in MATLAB, for HEVC intra we use the HM codec2, and for Toderici et al. we
use the trained network provided by the authors3.
4.4
Results
Figure 3 presents the reconstructed images using diﬀerent methods for kodim07
at around 0.25 bpp. The results of ﬁxed 32 × 32 DCT and JPEG suﬀer from
severe ringing artifacts in the area of leaves and ﬂowers. In our result, the ringing
is greatly reduced. The result of Toderici et al. is more smooth in the area of
leaves, but has strange texture in the area of window frame. Overall, our result
has the best visual quality among them. Note that our result has the highest
PSNR at similar bit rates.
Figure 4 depicts the rate-distortion curves of diﬀerent methods for two rep-
resentative test images, where distortion is measured by PSNR. Our method
achieves better performance than ﬁxed DCT in a wide range of bitrates. Both
our method and ﬁxed 32 × 32 DCT outperform JPEG signiﬁcantly, especially
at low bitrates. However, note that Toderici et al. performs worse than JPEG,
and JPEG2000 and HEVC intra are the best performers among the compared
methods. Thus, it is not trivial to outperform the existing highly optimized
standards.
When comparing the three networks proposed in this paper, we ﬁnd the sym-
metric network usually performs the best among the three, but the simpliﬁed
encoder network also performs very well, and even surpasses the symmetric net-
work on several test images (e.g. kodim07, shown in Fig. 4(a)). On the contrary,
the simpliﬁed decoder network does not compete favorably with the other two,
and is often worse than ﬁxed DCT. Therefore, it seems critical to adopt non-
linear inverse transform to pursue higher compression eﬃciency. The simpliﬁed
encoder network, though performing worse than the symmetric network, is still
better than ﬁxed DCT, and thus can be useful in scenarios where the encoding
complexity is concerned more than compression eﬃciency.
Table 1 summarizes the BD-rate results of our symmetric network compared
with diﬀerent anchors including ﬁxed 32 × 32 DCT, JPEG, and Toderici et al.
When comparing with ﬁxed 32×32 DCT, our method achieves BD-rate reduction
for 22 out of 24 test images, except for kodim15 and kodim17. The average BD-
rate reduction of 24 images is 8.83%. When comparing with JPEG and Toderici
et al., our method achieves signiﬁcant BD-rate reduction for all of the test images,
with on average 38.03% and 56.66% BD-rate reduction, respectively.
2 https://hevc.hhi.fraunhofer.de/svn/svn HEVCSoftware/tags/HM-16.15/.
3 https://github.com/tensorﬂow/models/tree/master/compression. This network has
no entropy coding since the authors do not provide.

70
D. Liu et al.
Table 1. BD-rate results of our symmetric network compared with diﬀerent anchors
Ours vs. DCT (32 × 32) Ours vs. JPEG Ours vs. Toderici et al.
kodim01 −17.74%
−28.45%
−36.71%
kodim02
−3.15%
−56.38%
−79.21%
kodim03 −10.28%
−47.22%
−67.99%
kodim04
−2.86%
−50.96%
−65.11%
kodim05 −18.31%
−24.65%
−28.60%
kodim06 −13.29%
−35.05%
−59.77%
kodim07 −11.10%
−39.13%
−54.94%
kodim08 −11.63%
−24.42%
−36.11%
kodim09
−8.09%
−41.15%
−61.45%
kodim10
−5.46%
−42.18%
−61.39%
kodim11 −10.91%
−33.09%
−55.52%
kodim12
−8.84%
−43.60%
−69.24%
kodim13 −13.35%
−23.27%
−41.25%
kodim14 −15.91%
−30.20%
−46.09%
kodim15
7.17%
−37.82%
−60.90%
kodim16
−7.95%
−46.79%
−67.92%
kodim17
11.88%
−35.22%
−44.26%
kodim18 −15.79%
−34.46%
−48.31%
kodim19
−9.84%
−47.52%
−65.65%
kodim20
−3.26%
−35.89%
−62.65%
kodim21 −17.32%
−34.89%
−60.27%
kodim22 −12.52%
−39.39%
−57.85%
kodim23
−3.48%
−54.09%
−77.02%
kodim24 −10.00%
−26.80%
−51.71%
Average
−8.83%
−38.03%
−56.66%
Table 2 presents the BD-rate results of diﬀerent methods compared with
JPEG. It can be observed that our symmetric network performs better than
the simpliﬁed encoder and the simpliﬁed decoder networks. However, JPEG2000
and HEVC intra perform even better. For comparison purpose, we include into
Table 2 the BD-rate results reported in [8]. Please note that our results are
achieved with grayscale images converted from the Kodak image dataset, and
the results in [8] are achieved with the color images, so the BD-rate results are
not directly comparable. All the results show that deep learning-based methods
including ours and those in [8,10] are better than JPEG but still worse than
JPEG2000 and HEVC intra. Thus, it may be not wise to replace the existing
standards with an entirely new deep learning-based scheme. Finally, we would
like to remark again that our method provides an improved transform than

CNN-Based DCT-Like Transform for Image Compression
71
Table 2. BD-rate results of diﬀerent schemes compared with JPEG
Test set
Method
BD-rate
Kodak (grayscale) Our symmetric network −38.03%
Our simpliﬁed encoder
−31.13%
Our simpliﬁed decoder
−18.31%
JPEG2000
−54.89%
HEVC intra
−71.00%
Kodak (color)a
Toderici et al. [7]
13.34%
Johonston et al. [8]
−27.14%
Theis et al. [10]
−29.04%
JPEG2000b
−38.28%
HEVC intrac
−54.85%
aThese results are quoted from Table 3 of [8]
bUsing OpenJPEG codec (http://www.openjpeg.org/)
cUsing BPG codec (https://bellard.org/bpg/) and YUV
420 format
ﬁxed DCT, and thus our method can be seamlessly integrated into block-based
schemes like HEVC. We will study the integration in the future.
5
Conclusion
This paper presents a block transform inspired by DCT and achieved by training
convolutional neural network models. Our network consists of convolution, non-
linear mapping and linear transform so as to provide non-linear transform and
non-linear inverse transform. We propose to estimate the rate by the l1-norm
of the quantized coeﬃcients, and thus the network can be trained end to end.
We also study asymmetric structures of the network. Experimental results show
that the trained transform achieves better compression performance than ﬁxed
DCT.
Our future work will proceed in three directions. First, we plan to investigate
other network structures to achieve higher compression eﬃciency. Second, we will
extend the transform to other block sizes like 16 × 16 and 8 × 8. Third, we will
integrate the trained transform into the state-of-the-art video coding scheme
such as HEVC.
Acknowledgment. This work was supported by the Natural Science Foundation of
China (NSFC) under Grant 61772483, Grant 61390512, and Grant 61425026, and by the
Fundamental Research Funds for the Central Universities under Grant WK3490000001.
References
1. Wallace, G.K.: The JPEG still picture compression standard. IEEE Trans. Con-
sum. Electron. 38(1), xviii–xxxiv (1992)

72
D. Liu et al.
2. Christopoulos, C., Skodras, A., Ebrahimi, T.: The JPEG2000 still image coding
system: an overview. IEEE Trans. Consum. Electron. 46(4), 1103–1127 (2000)
3. Wiegand, T., Sullivan, G.J., Bjontegaard, G., Luthra, A.: Overview of the
H.264/AVC video coding standard. IEEE Trans. Circ. Syst. Video Technol. 13(7),
560–576 (2003)
4. Sullivan, G.J., Ohm, J., Han, W.J., Wiegand, T.: Overview of the high eﬃciency
video coding (HEVC) standard. IEEE Trans. Circ. Syst. Video Technol. 22(12),
1649–1668 (2012)
5. Hu, W., Cheung, G., Ortega, A., Au, O.C.: Multiresolution graph fourier transform
for compression of piecewise smooth images. IEEE Trans. Image Process. 24(1),
419–433 (2015)
6. Toderici, G., O’Malley, S.M., Hwang, S.J., Vincent, D., Minnen, D., Baluja, S.,
Covell, M., Sukthankar, R.: Variable rate image compression with recurrent neural
networks. In: ICLR (2016)
7. Toderici, G., Vincent, D., Johnston, N., Hwang, S.J., Minnen, D., Shor, J., Covell,
M.: Full resolution image compression with recurrent neural networks. In: CVPR,
pp. 5306–5314 (2017)
8. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Hwang,
S.J., Shor, J., Toderici, G.: Improved lossy image compression with priming and
spatially adaptive bit rates for recurrent networks. arXiv preprint arXiv:1703.10114
(2017)
9. Ball´e, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression.
In: ICLR (2017)
10. Theis, L., Shi, W., Cunningham, A., Husz´ar, F.: Lossy image compression with
compressive autoencoders. In: ICLR (2017)
11. Rippel, O., Bourdev, L.: Real-time adaptive image compression. In: ICML, pp.
2922–2930 (2017)
12. Jiang, F., Tao, W., Liu, S., Ren, J., Guo, X., Zhao, D.: An end-to-end compression
framework based on convolutional neural networks. IEEE Trans. Circ. Syst. Video
Technol. (2017). https://doi.org/10.1109/TCSVT.2017.2734838
13. Baig, M.H., Torresani, L.: Multiple hypothesis colorization and its application to
image compression. Comput. Vis. Image Underst. (2017)
14. Prakash, A., Moran, N., Garber, S., DiLillo, A., Storer, J.: Semantic perceptual
image compression using deep convolution networks. In: DCC, pp. 250–259 (2017)
15. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neu-
ral networks. Science 313(5786), 504–507 (2006)
16. Wong, C.W., Au, O.C., Lam, H.K.: Rate control using probability of non-zero
quantized coeﬃcients. In: ICME (2004)
17. Candes, E.J., Tao, T.: Decoding by linear programming. IEEE Trans. Inf. Theory
51(12), 4203–4215 (2005)
18. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann
machines. In: ICML, pp. 807–814 (2010)
19. Schaefer, G., Stich, M.: UCID: an uncompressed color image database. In: Elec-
tronic Imaging 2004, International Society for Optics and Photonics, pp. 472–480
(2004)
20. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: convolutional architecture for fast feature embedding.
In: ACM Multimedia, pp. 675–678. ACM (2014)
21. Said, A.: Introduction to arithmetic coding - theory and practice. Technical report
HPL-2004-76, Hewlett Packard Laboratories Palo Alto (2004)

Coarse-to-Fine Image Super-Resolution Using
Convolutional Neural Networks
Liguo Zhou1,2, Zhongyuan Wang1,2(&), Shu Wang3, and Yimin Luo4
1 NERCMS, Computer School of Wuhan University, Wuhan, China
948202396@qq.com, wzy_hope@163.com
2 Research Institute of Wuhan University in Shenzhen, Shenzhen, China
3 Life Science and Biomedical Engineering of King’s College London,
London, UK
shu.1.wang@kcl.ac.uk
4 Remote Sensing Information Engineering School of Wuhan University,
Wuhan, China
1464355846@qq.com
Abstract. Convolutional neural networks (CNNs) have been widely applied to
computer vision ﬁelds due to its excellent performance. CNN-based single
image super resolution (SR) methods are also put into practice and outperform
previous methods. In this paper, we propose a coarse-to-ﬁne CNN method to
boost the existing CNN-based SR methods. We design a cascaded CNN
architecture with three stages. The ﬁrst stage takes the low-resolution (LR) im-
age as the input and outputs a high-resolution (HR) image, then the next stage
similarly takes the high-resolution result as the input and produces a ﬁner HR
image. Finally, the last stage can obtain the ﬁnest HR image. Our architecture is
trained as one entire CNN which combines three loss functions to optimize the
gradient descent procedure. Experiments on ImageNet-based training samples
validates the effectiveness of our method on the public benchmark datasets.
Keywords: Image super resolution  Convolutional neural networks
Coarse-to-ﬁne  Deep learning
1
Introduction
Single image super-resolution (SR) [1–6], a classical problem in computer vision ﬁelds,
is a reconstruction technique for recovering a high-resolution image from a single
low-resolution image. Generally speaking, image SR is an underdetermined inverse
issue, as it has multiple solutions for a given low-resolution image [7]. As for the
advancement of this technique, current researches mainly focus on three parts:
interpolation-based SR [8], reconstruction-based SR [9] and learning-based SR [6].
Among them, learning-based SR enjoys the highest attention owning to the develop-
ment of machine learning algorithms. Learning-based SR, as the name implies, learns
representation through constantly training high-resolution and low-resolution image
pairs to obtain a super-resolution model. There are a variety of learning-based methods
for SR, like sparse representation [10] and manifold learning [11] and deep Laplacian
pyramid networks [12].
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 73–81, 2018.
https://doi.org/10.1007/978-3-319-73600-6_7

Inspired by the recent successes achieved with convolutional neural networks
(CNNs) in vision-related tasks [7, 13–15] pioneered an innovative deep learning
method for SR, named super-resolution convolutional neural network (SRCNN). Since
this method learns an end-to-end mapping between pairs of low-resolution (LR) and
high-resolution (HR) images via three hidden layers, it visibly outperforms previous
methods and reconstructs more high-frequency details.
SRGAN [16] employed generative adversarial networks (GANs) [17] for
super-resolution, which trained a super-resolution network in opposition to a secondary
network that attempts to discriminate whether a synthesized high-resolution image is
real or fake. Although SRGAN can obtain high perceptual quality, it suffers from
several drawbacks, such as unstable network and mode collapse.
Considering the limitations of SRCNN with shallow network and motivated by
VGGNet [18, 19] further proposes a rectiﬁed convolutional neural network based SR
method using very deep network (VDSR). This method deepens the CNN architecture
for obtaining more meticulous high-frequency details and optimizes its network
architecture by residual learning [20], and it substantially outperforms SRCNN and
other previous methods.
Admittedly, the VDSR architecture is validated to be helpful to image SR. This
paper makes a further exploration on these CNN-based image SR methods. Based on a
coarse-to-ﬁne idea, our architecture comprises a three-stage convolutional neural net-
work which has ten convolutional layers in each stage. Consequently, the obtained
high-resolution image can be further reﬁned to retain ﬁner result. Experimentally,
trained with ImageNet [21] dataset, the reconstruction results on Set5 [22], Set14 [23]
and BSD100 [24] all validate the effectiveness of our method. Furthermore, in terms of
the stage number of our architecture, we provide a detailed analysis on this design, and
extensive experiments show the advantages of our network.
This paper is organized as follows: Sect. 2 gives a detailed introduction on previous
deep learning SR methods (SRCNN and VDSR). After that, Sect. 3 gives a further
discussion on CNN-based image SR methods and introduces our method. Next, Sect. 4
displays the experimental results of our method and analyzes its effectiveness by
comparing to previous methods. Finally, Sect. 5 concludes this paper.
2
Related Works
SRCNN [7] is the ﬁrst method to apply the convolutional neural network to image
super resolution. It only contains the convolution layers, so the trained model can
directly process the image with any size. In the training process, due to the absence of
image’s border padding before convolution, the SRCNN’s output image is always
smaller than the input image, so does the ground truth image of the training samples.
SRCNN is a shallow convolutional neural network which only consists of three layers,
and the number of feature maps and convolution kernel size of each layer are not
consistent. The resolution of training samples is also very low, and only suitable for
shallow network training without border padding. SRCNN compares the output of the
last layer to the ground truth image directly and calculates their Euclidean distance loss,
then do back propagation to regulate the network’s parameters. Through constantly
74
L. Zhou et al.

iteration, SRCNN ﬁnally gains a super resolution model with good performance until
the loss convergences.
VDSR [15] has done a lot of improvements on SRCNN, and its main method is to
increase the number of network layers to 20. To solve the problem of images’ size
reduction caused by the increase of network depth, VDSR does zero-padding before
each convolution operation. Its convolution kernel size of each layer is 3  3, in
addition to the consistency between the output layer’s channel number and input
image’s channel number, the number of other convolutional layers’ feature maps is 64.
As directly calculating the loss between the last convolutional layer’s output and the
ground truth image increases the training time and decreases the network’s perfor-
mance, the author puts forward a method named residual learning as optimization for
above problems, that is to let the network learn the difference between the ground truth
image and LR image, then add this difference to LR image to get the HR image. This
method not only makes the network converge faster, but also improves the quality of
the results.
3
Proposed Methodology
As single image super-resolution can be viewed as a mapping problem from the
low-resolution image to the high-resolution image, we can use Y = f (X) to represent
the process, where X represents the LR image, Y represents the HR image, and the
function f is the mapping relationship between them. For the deep learning based SR
method, f stands for a nonlinear mapping, and our network structure is f (•), the training
process is to solve the nonlinear regression problem. Next, we will explain our method
in terms of the network structure and training procedure.
3.1
Framework
SRCNN and VDSR have shown that ﬁne high-resolution images can be obtained from
coarse interpolated images. We design a cascaded convolutional neural network, which
makes the quality of the image can be reﬁned from coarse to meticulous, and ﬁnally,
the quality of the image is better than that of one-stage convolutional neural network.
Fig. 1. The core network of our coarse-to-ﬁne super resolution method. This network contains
three stages with 10 convolutional layers in each stage, and the output image’s quality increases
gradually.
Coarse-to-Fine Image Super-Resolution Using CNNs
75

Our network consists of three stages which are shown in Fig. 1: the ﬁrst stage takes
the interpolated image as the input and the output image’s quality can be slightly
improved, the second and third stage take the last stage’s output as the input respec-
tively, then the image quality is gradually reﬁned. Each CNN stage contains ten
convolutional layers with 3  3 kernel size, and each convolutional layer produces 64
feature maps followed by an activation ReLU [25] except for its output layer. The
output layers’ channel number should be equivalent to the channel number of input
image. To keep the output image size unchanged, we pad zeros around the border.
In order to speed up the training process, inspired by VDSR, we also use residual
learning. Each CNN stage’s output is the high frequency detail of the predicted
high-resolution image, which is then added to the stage’s input to synthesize the SR
image.
3.2
Training
For a given training sample pair (Xi, Yi), Xi represents the coarse low-resolution image
processed by bicubic interpolation, and Yi represents the ground truth high-resolution
image. Putting Xi into the network, a predictive high-resolution image Y0
ij j ¼ 1; 2; 3
ð
Þ
will be synthesized in each stage. The purpose of training is to make the difference
between Y0
ij and Yi as small as possible. To judge the similarity between two images,
Peak Signal to Noise Ratio (PSNR) (1) is the most popular evaluation standard, the
larger the PSNR value is, the more similar the two images are. Therefore, our training is
to increase PSNR value between the Y0
i and Yi, and this is equivalent to minimize the
two images’ Mean Squared Error (MSE) (2), or minimize their Euclidean distance, so
our loss function in each stage can be expressed in (3).
PSNR ¼ 10  log10ðð2n  1Þ2
MSE
Þ
ð1Þ
MSE ¼
1
width  height Yi  Y0
i

2
ð2Þ
Lossj ¼ 1
2N
XN
i¼1 Yi  Y0
ij


2
ð3Þ
Where n is the number of bits of the image pixel, generally n = 8, and N is the
batch size.
3.3
Joint Loss Function
During the training process, the loss needs to be calculated in each stage, so our
network has to calculate 3 loss values in every iteration, and jointly uses them to
regulate parameters. To be more speciﬁc, the ﬁrst stage’s loss value back propagates to
regulate the parameter of the ﬁrst stage’s each convolutional layer, the second stage’s
loss is used to regulate the last 2 stages’ parameters, and the third stage’s loss regulates
all previous parameters.
76
L. Zhou et al.

4
Experiments
4.1
Experiment Implementation Details
The experiment concrete steps can be expressed below:
Similar to VDSR, we also train a general model that can zoom in the LR image at
different ratios.
ImageNet dataset is used as our training set. Original image is cut into 96  96
non-overlapping image block as ground truth set {Yi}, then we down sample each
image block by 1/2, 1/3, 1/4 ratio of {Yi} respectively, and use bicubic interpolation to
recover them to the original size, thus, low resolution image set {Xi} is obtained. We
utilize the classic Caffe [26] library to train our network. The input image is in RGB
color space, its batch size is set to 160, and Adam gradient descent algorithm is applied
here to optimize the network. Three loss weights are all set to 1.
Then we come to the step of training the three-stage CNN network. Its initial
learning rate of the network is set to 0.0001, when the loss is no longer dropped, it is
reduced at a rate of 0.5 and stops training until the loss converges.
Experimental results show that the image quality of our network can exceed VDSR.
Except on PSNR standard, the output images of our network also perform well when
evaluated by structural similarity index (SSIM) [27].
4.2
Results and Analysis
We compare bicubic, SRCNN, VDSR and our method by computing the PSNR &
SSIM and the results are shown in Table 1, there is no question that our method
outperforms the other three. To be speciﬁc, on the smallest set Set5 (only 5 images) and
upscaling by the factor of 4, the PSNR value is improved by 3.25 dB compared to
bicubic interpolation, 1.19 dB compared to SRCNN, 0.33 dB compared to VDSR. For
other scales and datasets, our method still has the best performance.
Figure 3 shows the detailed recovery effects of the four mentioned algorithms, from
the upscaled image blocks, we can clearly see that bicubic interpolation is the most
blurred one, SRCNN & VDSR improve the image quality gradually, and the results
based on our method tells the best reconstruction.
Also, by observing the output of three stages’ loss in our training network, we ﬁnd
that the quality improvement from the ﬁrst stage to the second stage is signiﬁcantly
higher than that from the second to the third. We try to increase the network depth to
four and ﬁve stages in the training process, and ﬁnd it more difﬁcult to get signiﬁcant
improvement while the increase of the training & testing time complexity is huge.
From the comparison between the VDSR and VDSR (30 layers), we can see that
just increasing the number of VDSR layers to 30 doesn’t improve the performance of
the network, but cascading by our method can boost the reconstruction results. And as
showed in Fig. 2, the highest PSNR can be obtained when the number of stages is 3,
the testing set is Set5 and the upscale factor is 4, but other sets have similar results. So,
there is no need to deepen the current network anymore, the modiﬁcation should be
found in other aspects.
Coarse-to-Fine Image Super-Resolution Using CNNs
77

Table 1. Comparison of Bicubic, A+ [28], SelfExSR [29], SRCNN [7], VDSR [15] and our
algorithm on public benchmark data. (PSNR [dB], SSIM)
Dataset
Set5
Set14
BSD100
Scale
2
3
4
2
3
4
2
3
4
Bicubic
PSNR 33.68
30.41
28.43
30.24
27.54
26
29.56
27.21
25.96
SSIM
0.9306 0.8688 0.8103 0.8691 0.7738 0.7015 0.8437 0.7391 0.668
A+
PSNR 36.55
32.59
30.29
32.28
29.13
27.32
30.78
28.18
26.77
SSIM
0.9544 0.9088 0.8603 0.9056 0.8188 0.7491 0.8863 0.7835 0.7087
SelfExSR
PSNR 36.5
32.62
30.33
32.23
29.16
27.4
31.18
28.3
26.85
SSIM
0.9537 0.9094 0.8623 0.9036 0.8197 0.7518 0.8855 0.7843 0.7108
SRCNN
PSNR 36.66
32.75
30.49
32.45
29.3
27.5
31.36
28.41
26.9
SSIM
0.9542 0.909
0.8628 0.9067 0.8215 0.7513 0.8879 0.7863 0.7101
VDSR
PSNR 37.53
33.66
31.35
33.03
29.77
28.01
31.9
28.82
27.29
SSIM
0.9587 0.9213 0.8838 0.9124 0.8314 0.7674 0.896
0.7976 0.7251
VDSR
(30 layers)
PSNR 37.58
33.76
31.36
32.99
29.73
27.92
31.89
28.79
27.2
SSIM
0.9592 0.923
0.8853 0.9127 0.8314 0.7662 0.8962 0.7979 0.7248
Ours
PSNR 37.69
34.01
31.68
33.48
30.14
28.37
32.01
28.95
27.41
SSIM
0.9596 0.9241 0.8874 0.9164 0.8361 0.7724 0.8976 0.8008 0.7292
Number of Stages 
PSNR(dB)
29.5
30
30.5
31
31.5
32
1
2
3
4
5
Fig. 2. Reconstruction PSNR with our network on Set5 along different number of stages with
scale 4. Three stages gains the best performance, deepening the network afterwards does not
guarantee higher PSNR.
78
L. Zhou et al.

Original
Bicubic      
SRCNN      
VDSR
Ours
Fig. 3. Comparison between the 4 algorithms on testing images from Set14. The ﬁrst column is
the original image, the second is bicubic interpolation results, the third is SRCNN results, the
fourth is VDSR and the last is ours.
Coarse-to-Fine Image Super-Resolution Using CNNs
79

5
Conclusion and Future Works
This paper has proposed a three-stage CNN architecture to improve CNN’s perfor-
mance for image SR based on a coarse-to-ﬁne idea. Experimentally, it is proven that the
proposed method substantially outperforms state-of-the-art CNN-based SR methods.
The reconstructed images of our method enjoy a higher quality and display ﬁner
details. Because our framework consumes relatively higher training time, further
optimization on its architecture is needed.
Acknowledgement. The research was supported by National Natural Science Foundation of
China (61671332), Basic Research Program of Shenzhen City (JCYJ20170306171431656) and
Hubei Province Technological Innovation Major Projects (2017AAA123, 2016AAA015).
References
1. Irani, M., Peleg, S.: Improving resolution by image registration. Graph. Models Image
Process. 53(3), 231–239 (1991)
2. Wang, H., Gao, X., Zhang, K., Li, J.: Single image super-resolution using Gaussian process
regression with dictionary-based sampling and student-t likelihood. IEEE Trans. Image
Process. 26(7), 3556–3568 (2017)
3. Bevilacqua, M., Roumy, A., Guillemot, C., Alberimorel, M.: Low complexity single-image
super-resolution based on nonnegative neighbor embedding. In: British Machine Vision
Conference, Guildford, Surrey, UK (2012)
4. Park, S.C., Park, M.K., Kang, M.G.: Super-resolution image reconstruction: a technical
overview. IEEE Signal Process. Mag. 20(3), 21–36 (2003)
5. Camponez, M.O., Salles, E.O.T., Sarcinelliﬁlho, M.: Super-resolution image reconstruction
using nonparametric Bayesian INLA approximation. IEEE Trans. Image Process. 21(8),
3491–3501 (2012)
6. Zeng, K., Yu, J., Wang, R., Li, C., Tao, D.: Coupled deep auto-encoder for single image
super-resolution. IEEE Trans. Cybern. 47(1), 27–37 (2015)
7. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convolutional
networks. IEEE Trans. Pattern Anal. Mach. Intell. 38(2), 295–307 (2015)
8. Zhou, F., Yang, W., Liao, Q.: Interpolation-based image super-resolution using multisurface
ﬁtting. IEEE Trans. Image Process. 21(7), 3312–3318 (2012)
9. Tai, W., Liu, S., Brown, M.S., Lin, S.: Super resolution using edge prior and single image
detail synthesis. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2400–2407. IEEE, San Francisco, CA, USA (2010)
10. Yang, J., Wright, J., Huang, T.S., Ma, Y.: Image super-resolution via sparse representation.
IEEE Trans. Image Process. 19(11), 2861–2873 (2010)
11. Chang, H., Yeung, D., Xiong, Y.: Super-resolution through neighbor embedding. In: IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1063–6919. IEEE,
Washington, DC, USA (2004)
12. Lai, W.-S., Huang, J.-B., Ahuja, N., Yang, M.-H.: Deep Laplacian pyramid networks for fast
and accurate super-resolution. arXiv:1704.03915 [cs.CV]
13. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep convolutional
neural networks. In: International Conference on Neural Information Processing Systems,
Lake Tahoe, Nevada, USA, pp. 1097–1105 (2012)
80
L. Zhou et al.

14. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,
V., Rabinovich, A.: Going deeper with convolutions. In: 28th IEEE Conference on
Computer Vision and Pattern Recognition. IEEE, Boston, MA, USA (2015)
15. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks
for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37(9), 1904–1916 (2015)
16. Ledig, C., Theis, L., Huszar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A.P.,
Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-resolution using a
generative adversarial network. In: 28th IEEE Conference on Computer Vision and Pattern
Recognition. IEEE, Las Vegas, NV, USA (2016)
17. Goodfellow, I.J., Pougetabadie, J., Mirza, M., Xu, B., Wardefarley, D., Ozair, S., Courville,
A.C., Bengio, Y.: Generative adversarial nets. In: International Conference on Neural
Information Processing Systems, Montréal, Canada, pp. 2672–2680 (2014)
18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image
recognition. In: 5th International Conference on Learning Representations, San Diego,
California, USA (2015)
19. Kim, J., Lee, J.K., Lee, K.M.: Accurate image super-resolution using very deep
convolutional networks. In: 29th IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1646–1654. IEEE, Las Vegas, Nevada, USA (2016)
20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: 29th
IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778. IEEE, Las
Vegas, Nevada, USA (2016)
21. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,
Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet large scale visual recognition
challenge. IJCV 115(3), 211–252 (2015)
22. Bevilacqua,
M.,
Roumy,
A.,
Guillemot,
C.,
Alberimorel,
M.L.:
Low-complexity
single-image super-resolution based on nonnegative neighbor embedding. In 23rd British
Machine Vision Conference, Guildford, Surrey, UK, pp. 1–10 (2012)
23. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-representations.
Curves Surf. 6920, 711–730 (2012)
24. Martin, D.R., Fowlkes, C.C., Tal, D., Malik, J.: A database of human segmented natural
images and its application to evaluating segmentation algorithms and measuring ecological
statistics. In: International Conference on Computer Vision, Vancouver, Canada, pp. 416–
423 (2001)
25. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted Boltzmann machines. In:
27th International Conference on Machine Learning, Haifa, Israel, pp. 807–814 (2010)
26. Yangqing, J., Evan, S., Jeff, D., Sergey, K., Jonathan, L., Ross, G., Sergio, G., Trevor, D.:
Caffe: convolutional architecture for fast feature embedding. In: 22nd ACM Multimedia,
Orlando, Florida, USA, pp. 675–678 (2014)
27. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error
visibility to structural similarity. IEEE Trans. Image Process. 4(13), 600–612 (2004)
28. Timofte, R., De Smet, V., Van Gool, L.: A+: adjusted anchored neighborhood regression for
fast super-resolution. In: Cremers, D., Reid, I., Saito, H., Yang, M.-H. (eds.) ACCV 2014.
LNCS, vol. 9006, pp. 111–126. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
16817-3_8
29. Huang, J., Singh, A., Ahuja, N.: Single image super-resolution from transformed self-
exemplars. In: 28th IEEE Conference on Computer Vision and Pattern Recognition. IEEE,
Boston, MA, USA (2015)
Coarse-to-Fine Image Super-Resolution Using CNNs
81

Data Augmentation for EEG-Based Emotion
Recognition with Deep Convolutional Neural
Networks
Fang Wang1, Sheng-hua Zhong1(&), Jianfeng Peng1, Jianmin Jiang1,
and Yan Liu2
1 College of Computer Science and Software Engineering,
Shenzhen University, Shenzhen 518000, People’s Republic of China
{wangfang20161,pengjianfeng2017}@email.szu.edu.cn,
{csshzhong,jianmin.jiang}@szu.edu.cn
2 Department of Computing, The Hong Kong Polytechnic University,
Kowloon, Hong Kong
csyliu@comp.polyu.edu.hk
Abstract. Emotion recognition is the task of recognizing a person’s emotional
state. EEG, as a physiological signal, can provide more detailed and complex
information for emotion recognition task. Meanwhile, EEG can’t be changed
and hidden intentionally makes EEG-based emotion recognition achieve more
effective and reliable result. Unfortunately, due to the cost of data collection,
most EEG datasets have small number of EEG data. The lack of data makes it
difﬁcult to predict the emotion states with the deep models, which requires
enough number of training data. In this paper, we propose to use a simple data
augmentation method to address the issue of data shortage in EEG-based
emotion recognition. In experiments, we explore the performance of emotion
recognition with the shallow and deep computational models before and after
data augmentation on two standard EEG-based emotion datasets. Our experi-
mental results show that the simple data augmentation method can improve the
performance of emotion recognition based on deep models effectively.
Keywords: Emotion recognition  Data augmentation  EEG
1
Introduction
Emotional recognition is the process of identifying human emotional state. As an
interdisciplinary ﬁeld, the research of emotional recognition is beneﬁted from the
development of psychology, modern neuroscience, cognitive science, and computer
science as well [1]. For computer science, the emotion recognition based on computer
system aspires to enhance human-machine interaction across a wide range of appli-
cation domains including clinical, industrial, military, gaming and so on [2].
Various approaches have been proposed for emotional recognition and can be
divided into two categories [2]. The ﬁrst category is using the features of emotional
behavior, such as facial expression, the tone of voice, body gestures and so on, to detect
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 82–93, 2018.
https://doi.org/10.1007/978-3-319-73600-6_8

a speciﬁc emotion. The second category is taking advantage of the physiological
signals to recognize emotions. These physiological signals include the electroen-
cephalograph (EEG), electrocardiograph (ECG), pulse rate, respiration signals, etc.
Compared with the former, the physiological signals not only provide more detailed
and complex information for estimating emotional states but also yield more effective
and reliable recognition results.
Up to now, a number of EEG-based emotion recognition methods have been
studied. Emotional recognition mainly includes the two modules: feature extraction and
emotion classiﬁcation. As a time-series signal, the EEG original signal has potential
information and can’t be directly used for emotion recognition [3]. In order to fully
exploit the potential information, the classiﬁer uses the feature data as input to identify
the emotional states. In the traditional EEG-based emotion recognition methods, many
studies explored the effects of emotional recognition using the traditional machine
learning models as the classiﬁers, such as Support Vector Machine [4–6], K Nearest
Neighbors [7–9], and so on.
In recent years, deep learning techniques have received widespread attention and
achieved remarkable results in many ﬁelds. Using the deep learning model as a clas-
siﬁer to complete the classiﬁcation task, especially in the image recognition task, show
an appreciable performance [10, 11]. Recently, the deep learning method has been
applied to the study of the EEG-based emotional recognition [12–14]. Zheng and Lu
compared the performance of the deep learning models with the feature-based shallow
models on EEG-based emotion recognition and showed the superior performance of the
deep learning models [1]. Based on the above achievements, it is necessary for us to
promote exploration on the EEG-based emotion recognition with deep learning models.
However, compared with the shallow models, the deep learning models, for example,
the deep convolution neural network, have more model parameters, which makes the
training of deep learning models requires a large number of labeled training samples.
But due to the expensive cost, for EEG-related task, only a limited amount of labeled
data can be obtained to train the model. For most public EEG datasets, they only have a
few hundred samples, which are collected for different tasks with different speciﬁca-
tions. Thus, if we try to further explore deep learning models on the task of EEG-based
emotional recognition, getting sufﬁcient and effective labeled training data based on
existing datasets is the primary issue needs to be addressed.
In this paper, we focus on applying simple data augmentation method to generate
more EEG training samples. After that, we compare the performance of EEG-based
emotion recognition with the shallow and the deep models before and after data
augmentation.
The rest of this paper is organized as followings. Section 2 brieﬂy reviews the
representative work on EEG-based emotion recognition. In Sect. 3, we introduce our
proposed method in detail. In Sect. 4, we provide a series of experiments to validate the
proposed method on the standard datasets. The paper is closed with the conclusion and
future work.
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
83

2
Related Work
EEG-based emotion recognition, as an important branch of emotion recognition, has
received much attention in the past decades. Davidson and Fox investigated that infants
show greater activation of the left frontal than of the right frontal area in response to the
happy segments [15]. Klimesch et al. proposed the beta frequency band of EEG reﬂects
the emotional and cognitive patterns [16]. Li and Lu indicated that gamma band of
EEG can be used for classifying happiness and sadness [17]. In the early 21st century,
the rapid development of dry electrode technologies and wearable devices allows us to
record and analyze the EEG signals from the laboratories to the real environment. At
the same time, the application of EEG-based emotion recognition in real-world has also
been further promoted. Sourina et al. proposed to combine music therapy process with
the real-time EEG-based human emotion recognition algorithm [18]. Kothe and
Makeig collected the real-time EEG signals to perform two-class discrimination task
between “high” and “low” workload levels [19]. Shi et al. proposed to use the extreme
learning machine to perform the EEG-based vigilance estimation [3].
As can be seen from the above, the study of EEG-based emotional recognition has
never stopped. Recent years, various studies about using machine learning techniques
to establish emotion recognition model have been presented. Chanel et al. used EEG
time-frequency information as features and SVM as a classiﬁer to distinguish three
emotional states [20]. Heraz and Frasson reported using the amplitudes of four EEG
components as features and KNN as a classiﬁer to characterize EEG signals into eight
emotional states [21]. Zheng et al. used some commonly used features obtained from
various frequency bands and discriminative Graph regularized Extreme Learning
Machine as a classiﬁer to build EEG-based emotion recognition systems [22].
Recently, deep learning models are applied to process EEG signals. Li et al. applied a
DBN model to detect the emotional states from EEG signals. They compared the result
with ﬁve baselines and got the improvement from 11.5% to 24.4% [23]. Mao et al.
proposed two new architectures of convolutional neural networks and proved the
potential of deep learning methods for real-life EEG-based biometric identiﬁcation
[24]. Li et al. designed a hybrid deep learning model that combined the CNN and RNN
for mining inter-channel correlation and contextual information from EEG frames [25].
So far, using deep learning methods to identify emotions from EEG signals is still in its
infancy. Due to the limitation of the cost of data collection, the labeled EEG samples
that can be used to train the deep learning models are signiﬁcantly insufﬁcient.
Therefore, augmenting the labeled EEG samples is the key to promoting the use of
deep learning techniques for EEG-based emotional recognition.
3
The EEG-Based Emotional Recognition Framework Based
on Data Augmentation
In this paper, we propose to use data augmentation on EEG-based emotion recognition
task. The whole process including training and testing stages is demonstrated in Fig. 1.
The EEG signals are recorded while subjects are watching emotional videos. The
differential entropy (DE) feature is extracted from the recorded EEG signal. In the
84
F. Wang et al.

training stage, we use the data augmentation method to generate more EEG data. These
data are input to the classiﬁer to train a learned model. And this model is used to predict
the label of EEG data in the test stage.
3.1
Feature Extraction
In our method, we extract the differential entropy (DE) feature from the recorded EEG
signal segment. It has been shown that DE features can obtain the superior performance
in comparison with other commonly used features [1, 22]. For the time series
X obeying the Gauss distribution N (l, r2), its DE can be deﬁned as the following
formulation:
hðXÞ ¼ 
Z 1
1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2pr2
p
exp xl
ð
Þ2
2r2 log
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2pr2
p
exp xl
ð
Þ2
2r2


dx ¼ 1
2 log 2per2
ð1Þ
where l is the mean value and r is the standard deviation. For a ﬁx length EEG signal,
DE feature is equivalent to the logarithm energy spectrum in a certain frequency band
[2, 3]. Assume that the EEG signal is recorded with n-channel and the length of an EEG
signal segment is l s. First, we apply the band pass ﬁlter to obtain the 5 frequency
bands of each channel. The ranges of the ﬁlter are delta (1–3 Hz), theta (4–7 Hz), alpha
(8–13 Hz), beta (14–30 Hz), gamma (31–50 Hz). Then, we use a 256-point short-time
Fourier transform with a non-overlapped Hamming window of 1 s to obtain the energy
spectrum of each speciﬁed frequency band. After that, we extract the DE feature for
each frequency band by calculating the logarithm energy spectrum. Thus, the DE
feature for each second consists of n-channel DE calculated across 5 frequency bands.
After feature extraction, the size of each EEG feature sample is n  l  5.
Fig. 1. The EEG-based emotional recognition framework based on data augmentation.
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
85

3.2
Data Augmentation
Data augmentation is the process of generating new samples by transforming training
data, with the aim of improving the accuracy and robustness of classiﬁers [26].
Improper data augmentation methods are likely to increase the amount of the samples
that are not informative enough, which will do not even reduce the recognition
accuracy and robustness of recognition systems. In our work, we consider using the
commonly used data augmentation methods in images processing to increase the
number of EEG samples.
There are two basic data augmentation approaches used in images processing:
geometric transformation and noise addition. Geometric transformations, including
shift, scale, rotation/reﬂection and etc., are not directly suitable for enlarging our EEG
data. Compared with the image, the EEG signal is a continuous signal that changes
over time. Even if the feature extraction is performed, its features are still a time series.
Thus, if we rotate or shift the EEG data, the feature of time domain will be destroyed.
To avoid this problem, we further consider using the noise addition method to augment
the EEG samples. In theory, there are many ways for us to add noise (Gaussian,
Poisson, Salt, Pepper, etc.) into the EEG data. But EEG signal has a strong randomness
and non-stationarity. If we randomly add some local noises, such as Poisson noise, Salt
noise, or Pepper noise, which will change the features of EEG data locally.
Based on these considerations, in our work, we focus on adding Gaussian noise to
each feature sample of the original training data to obtain new training samples. The
probability density function P of a Gaussian random variable z is deﬁned by:
PGðzÞ ¼
1
r
ﬃﬃﬃﬃﬃﬃ
2p
p
e zl
ð
Þ2
2r2
ð2Þ
where z represents the grey level, l is the mean value and r is the standard deviation. In
our work, in order to ensure that the amplitude value of the sample will not be changed
with the addition of noise, we generate the Gaussian noise with l = 0. Under this
premise, in order to explore the effect of different noise intensity (standard deviation)
on the work of EEG data augmentation, the value of r is set to 0.001, 0.01, 0.02, 0.1,
0.2, and 0.5. We use m to represent the augmented multiple. When m = 1, it means
there is no data augmentation stage, and the train samples input into the classiﬁer only
include the original training data.
3.3
Machine Learning Models
In this paper, we use three machine learning models as the classiﬁers to recognize
EEG-based emotion category. These three models are support vector machine (SVM),
LeNet, and ResNet.
SVM. As a commonly used machine learning model, the basic idea of SVM is to map
the input data to a high-dimensional feature space via a kernel transfer function, in this
new space, these input data will be easier to be separated than that in the original
feature space [1]. SVM is often used in EEG-based emotional recognition tasks [27–
29]. In our work, the input of SVM model is the DE feature of EEG signal segment
86
F. Wang et al.

with duration of 1s. In order to meet the demand of SVM for the input, we transform
the size of DE feature sample from n  1  5 to 1  (n  5). Each EEG signal
segment will be divided into l samples. In training, all these l samples of each EEG
signal segment have the same emotional labels with the corresponding EEG signal
segment. In test, we obtain the predictive emotional label of an EEG signal segment by
counting the frequency of the predictive labels of the l test samples corresponding to
this EEG segment. The most frequently emotional predictive label will be identiﬁed as
the predictive label for this EEG segment.
LeNet. LeNet is a typical convolutional network for recognizing characters. It can learn
complex, high-dimensional, nonlinear mappings from large collections of examples
[10]. In our work, the architecture of LeNet is C32@32  32- P32@16  16-
C32@16  16- P32@8  8- C64@8  8- P64@4  4- C64@1  1- F3, which
contains four convolutional layers (denoted by C with the number and output size of the
feature map), three pooling layers (denoted by P with the number and output size of the
feature map) and one fully-connected layer (denoted by F with the number of output).
The input sample of this model is the DE feature of EEG signal segment with duration of
62 s. The size of an input sample is n  62  5. Each EEG signal segment will be
divided into l/62 samples. We use the same method of counting the frequency with SVM
to obtain the predictive label for each EEG test segment.
ResNet. ResNet can ease the training of deeper network with a residual learning
framework. It has been proven that this network is easier to optimize, and can gain
accuracy from considerably increased depth [30]. In our work, we use the ResNet to
establish the EEG-based emotion recognition model. The architecture of this network is
C64@62  62- P64@56  56- C64@56  56- C64@56  56- C256@56  56-
C128@28  28- C128@28  28- C512@28  28- C256@14  14- C256@14
 14- C1024@14  14- C512@7  7- C512@7  7- C2048@7  7- P2048@1 
1- F3. It contains thirteen convolutional layers, two pooling layers, and one
fully-connected layer. It has been proven that alpha, beta and gamma bands of EEG are
more predictive to the emotional states compared with the delta and theta bands [1, 17].
In our experiment, we use the feature of the three high frequency bands (alpha, beta,
and gamma) as the input of ResNet. The size of an input sample is n  62  3. We use
the same voting method with LeNet to obtain the predictive label for each EEG test
segment.
4
Experiments
4.1
Experimental Setting
In this paper, we investigate the performance of EEG-based emotion recognition with
the shallow and deep model before and after data augmentation on two standard
EEG-based emotional datasets: SJTU Emotion EEG Dataset (SEED) [1] and
MAHNOB-HCI dataset [31]. Two classical convolutional neural networks are utilized
in this work, including LeNet and ResNet. SVM, which is a representative shallow
model, is included in our comparison. We use LIBSVM Toolbox [32] to implement
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
87

SVM classiﬁer and use the linear kernel and the grid optimization to ﬁnd the optimal
value of C in the parameter space [2−10:210] with a step of one. The LeNet and ResNet
are implemented by the MATCONVNET Toolbox [33]. To the parameters for LeNet
and ResNet, we follow the general setting in MATCONVNET. For example, the
learning rate is set to 0.1, and the batch size is set to 100. All statistical experiments are
repeated for ﬁve times, and the average results are reported.
4.2
Experimental Results on SEED Dataset
As a standard dataset for emotion recognition, the SEED dataset consists of 630 EEG
segments of 14 subjects while watching 15 emotional ﬁlm clips for three sessions,
which were recorded by the EEG cap according to the international 10–20 system for
62 channels. For every EEG segment, the duration is about 4 min. After perform the
processing, the raw EEG data is downsampled to 200 Hz sampling. Three basic
emotion states are included in this dataset: positive, neutral and negative. Each emo-
tional state is with an equal number of segments. In our experiment, we segment every
original segment into three short EEG samples with the length of 62 s. Totally, there
are 1890 samples for the experiments. For these samples, all the samples of the ﬁrst
nine segments in each session are utilized as the training data, the rest are used as the
test data.
In this section, we ﬁrst compare the accuracies of EEG-based emotion recognition
based on SVM, LeNet and ResNet models without data augmentation. In addition, we
also explore the effect of dimensionality reduction on emotion recognition. In the
experiment, we use the Principle component analysis (PCA) algorithm to reduce the
original feature dimension. Table 1 shows the emotion recognition accuracies of these
models without data augmentation. Among them, SVM is selected as the compared
model because it is also applied as the classiﬁer for emotion recognition on the same
dataset [1, 22]. LeNet and ResNet for EEG-based emotion recognition are applied and
implemented by ourselves.
From these results, we can ﬁnd that the accuracies of three models are different. For
the SVM model, the accuracy is 74.2%, which is better than the performance of LeNet
and ResNet. When we use PCA to reduce the original 310 dimensional feature to 160
dimensions, 210 dimensions and the number of PCA dimensions retaining the top 95%
energy, the accuracies drop from 74.2% to 56.9%, 58.1% and 49.8%. We can see that
dimensionality reduction will affect the accuracy of emotion recognition to a certain
extent. That is because the complexity of EEG data makes the simple dimension
reduction method may not be able to preserve the important discriminative information
Table 1. The average accuracy of three models on the original data without data augmentation.
Models
SVM
[1]
PCA + SVM
(95% energy)
PCA [22]
+ SVM (160
dimensions)
PCA [22]
+ SVM (210
dimensions)
LeNet ResNet
Accuracy (%) 74.2
49.8
56.9
58.1
49.6
34.2
88
F. Wang et al.

of original domain information. This result is also consistent with the conclusion of
existing work [22]. However, it can be found that these results are still better than the
results of two deep learning models. We believe that the consistency of the number of
training samples and the number of free parameters in the model is the main factor that
affects the experimental results. For the LeNet used in our work, there are about 4000
network parameters that need to be learned. For the ResNet, the number of the network
parameters in it is more than 20000. Unfortunately, in this experiment, only 1134
training samples can be used in the training stage of the recognition model. The number
of training samples can’t meet the requirement of the deep learning models. Compared
with these deep learning models, SVM shows better emotional accuracy because SVM
is not sensitive to the number of sample.
As we have described in Table 1, the lack of EEG training samples may affect the
accuracy of emotional recognition of those deep models. In order to prove this con-
jecture and solve this problem, we use the proposed data augmentation method in
Sect. 3.2 to increase the amount of training samples and test its effect on LeNet.
Table 2 shows the recognition accuracies of LeNet trained by the augmented training
data. From the table, we ﬁnd that data augmentation can effectively improve the
performance of LeNet. We can achieve the best accuracy of 74.3% when the standard
deviation is 0.2 and the number of training samples is augmented to 30 times. The
standard deviation of Gaussian noise used in data augmentation and the scale of the
augmented training data will affect the performance. Too small or too large standard
deviation can’t generate the effective new samples.
We have already proved that data augmentation method can improve the perfor-
mance of EEG-based emotion recognition with LeNet. In order to further explore
whether this method is also effective for other machine learning models, we also
conduct experiments on SVM and ResNet with the augmented data. In Fig. 2, the
emotion recognition accuracies of SVM, LeNet and ResNet obtained before and after
data augmentation are demonstrated. In experiments, we follow the best parameter
setting of data augmentation obtained in Table 2. For each model, the mean value l is
set to be 0, and the standard deviation is set to be 0.2. For LeNet and ResNet, the
training-data is augmented to 30 times. Owing to the extremely high computational
complexity, the LIBSVM Toolbox only permits the number of training samples to be
augmented 5 times. Hence, in this experiment, for SVM, the training data is only
augmented to 5 times.
Table 2. The average accuracy (%) of data augmentation on LeNet.
Augmented
multiple (m)
Standard deviation of Gaussian
noise (r)
0.001 0.01 0.02 0.1
0.2
0.5
5
67.4
68.8 66.6 68.1 69.0 61.6
20
69.9
70.2 68.3 71.7 73.4 70.8
30
68.9
69.8 68.9 71.4 74.3 70.9
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
89

From Fig. 2, we can see that increasing the amount of input training data cannot
improve the recognition accuracy of SVM (74.2% vs. 73.4%). As we known, SVM is
insensitive to the number of training data [34]. As with LeNet, data augmentation can
effectively improve the accuracy of ResNet. Compared with LeNet, the input sample of
ResNet only includes the DE feature of three frequency bands of EEG signals. Under
this situation, the accuracy of ResNet is improved from 34.2% to 75.0%, better than the
LeNet (from 49.6% to 74.3%). These results also evidence that data augmentation is a
useful method to address the issue of the lack of training samples.
In the last experiment of this dataset, we have explored the contribution of low and
high frequency bands for emotion recognition. In detail, the high frequency bands
include: alpha, beta, and gamma. The low frequency bands include: delta, theta, and
alpha. The emotion recognition accuracies of the low and high frequency bands with
ResNet before and after data augmentation are shown in Fig. 3. From the ﬁgure, we
can ﬁnd that the accuracy of the three low frequency bands before and after data
augmentation is 33.3% and 57.6%, respectively, both of which are lower than the
high-frequency bands.
4.3
Experimental Results on MAHNOB-HCI Dataset
The MAHNOB-HCI dataset, a multi-model dataset for affect recognition and implicit
tagging, includes the EEG data recorded by the EEG cap according to the international
10–20 system for 32 channels. The length of the emotional video as the stimulus is
between 34.9 and 117 s. Before the feature extraction, we remove the artifacts from the
original EEG data and use the average reference as the virtual reference. In our
experiments, we divide the EEG segments into three classes according the self-reported
feeling of the subjects in valence space, negative (1–3), neutral (4–6) and positive
(7–9). Totally, there are 188 negative, 208 neutral samples and 131 positive samples.
In Sect. 4.2, we have explored the effect of data augmentation on the SEED dataset.
In this section, in order to verify the universality of our proposed data augmentation
method, we continue to perform experiments on the MAHNOB-HCI dataset and
compare the performance before and after data augmentation on shallow and deep
Fig. 2. The average accuracy of three models
trained before and after data augmentation.
Fig. 3. The accuracies of the high and
low frequency bands on ResNet.
90
F. Wang et al.

models. In Table 3, we show the best emotion recognition results obtained by different
models on the MAHNOB-HCI dataset. For SVM, the training data is augmented to 30
times, the mean value l is set to be 0, and the standard deviation is set to be 0.2. For
ResNet, the training data is augmented to 30 times, the mean value l is set to be 0, and
the standard deviation is set to be 0.01. In the confusion matrices of Table 3, the row
represents the predicted label, the column represents the ground truth, and the number
denotes the recognition accuracy in percentage.
From Table 3, we can learn the details of SVM and ResNet before and after data
augmentation on the MAHNOB-HCI dataset. For SVM, we can see that the average
accuracy is improved from 42.5% to 44.3% after data augmentation. Among the three
categories, the neutral emotion can be recognized with high accuracy, and after data
augmentation, the classiﬁcation performance of the neutral emotion and positive
emotion are improved. However, for the shallow model, SVM, the proposed data
augmentation method does not improve the recognition accuracy of negative emotions.
For ResNet, we can ﬁnd that data augmentation can also improve the performance of
emotion recognition (from 40.8% to 45.4%). Before data augmentation, ResNet con-
fuses the positive emotion with the neutral and negative emotions. Data augmentation
can signiﬁcantly improve the accuracies of positive and negative emotion for ResNet.
5
Conclusion
In this paper, we propose to use the data augmentation method to solve the problem
that the amount of EEG data can’t meet the needs of EEG-based emotional recognition
with deep models. In order to explore the effect of data augmentation, we conduct a
series of experiments on two standard EEG-based emotion recognition datasets. On the
SJTU dataset, we ﬁrst compare the accuracy of shallow (SVM) and deep models
(LeNet, ResNet) without data augmentation. We ﬁnd that the shallow model can
achieve better performance than the deep model. Then, we use the proposed data
augmentation method to generate new training samples. By analyzing the experimental
result, we ﬁnd that the data augmentation method can effectively improve the perfor-
mance of deep models. In order to further conﬁrm the effectiveness and universality of
Table 3. The confusion matrices for recognition accuracy (%) of different models on the
MAHNOB-HCI dataset before and after data augmentation (row: predicted label; column:
ground truth)
SVM
ResNet
Before
After
Before
After
Neg. Neu. Pos. Neg. Neu. Pos. Neg. Neu. Pos. Neg. Neu. Pos.
Neg. 0
0
0
0
0
0
8.1
4.3
9.3
56.5
44.9
41.9
Neu. 93.5
95.7
81.4 96.8
98.6
79.1 91.9
95.7
90.7 16.1
34.8
11.6
Pos.
6.5
4.3
18.6 3.2
1.4
20.9 0
0
0
27.4
20.3
46.5
Acc. 42.5
44.3
40.8
45.4
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
91

our proposed method, we validate the experiment on the MAHNOB-HCI dataset. Our
results show that the data augmentation is a useful method to address the issue of the
lack of training samples in EEG data for deep learning models. In future, we will seek
to use other data augmentation methods, such as generative adversarial networks, to
generate more effective samples of EEG data and improve the performance of
EEG-based emotion recognition.
Acknowledgements. This work was supported by the National Natural Science Foundation of
China (No. 61502311, No. 61373122, No. 61620106008), the Natural Science Foundation of
Guangdong Province (No. 2016A030310053, No. 2016A030310039), the Special Program for
Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase)
(No. U1501501), Shenzhen Emerging Industries of the Strategic Basic Research Project under
Grant (No. JCYJ20160226191842793), the Shenzhen high-level overseas talents program, and
the Tencent “Rhinoceros Birds” - Scientiﬁc Research Foundation for Young Teachers of
Shenzhen University (2015, 2016).
References
1. Zheng, W.L., Lu, B.L.: Investigating critical frequency bands and channels for EEG-based
emotion recognition with deep neural networks. IEEE Trans. Auton. Ment. Dev. 7(3), 1 (2015)
2. Duan, R.N., Zhu, J.Y., et al.: Differential entropy feature for EEG-based emotion
classiﬁcation. In: International IEEE/EMBS Conference on Neural Engineering, vol. 8588,
pp. 81–84 (2013)
3. Shi, L.C., Jiao, Y.Y., et al.: Differential entropy feature for EEG-based vigilance estimation.
In: International Conference of the IEEE Engineering in Medicine & Biology Society,
p. 6627 (2013)
4. Lin, Y.P., Wang, C.H., et al.: EEG-based emotion recognition in music listening. IEEE
Trans. Biomed. Eng. 57(7), 1798 (2010)
5. Petrantonakis, P.C., Hadjileontiadis, L.J.: A novel emotion elicitation index using frontal
brain asymmetry for enhanced EEG-based emotion recognition. IEEE Trans. Inf. Technol.
Biomed. 15(5), 737 (2011). A Publication of the IEEE Engineering in Medicine & Biology
Society
6. Wang, X.W., Nie, D., et al.: Emotional state classiﬁcation from EEG data using machine
learning approach. Neurocomputing 129(4), 94–106 (2014)
7. Heraz, A., Frasson, C.: Predicting the three major dimensions of the learner’s emotions from
brainwaves. Int. J. Comput. Sci. 2(3), 187–193 (2007)
8. Murugappan, M., Ramachandran, N., et al.: Classiﬁcation of human emotion from EEG
using discrete wavelet transform. J. Biomed. Sci. Eng. 03(4), 390–396 (2010)
9. Brown, L., Grundlehner, B., et al.: Towards wireless emotional valence detection from EEG.
In: International Conference of the IEEE Engineering in Medicine & Biology Society,
pp. 2188–2191 (2011)
10. Lecun, Y., Bottou, L., et al.: Gradient-based learning applied to document recognition. Proc.
IEEE 86(11), 2278–2324 (1998)
11. Krizhevsky, A., Sutskever, I., et al.: ImageNet classiﬁcation with deep convolutional neural
networks. In: International Conference on Neural Information Processing Systems,
pp. 1097–1105 (2012)
12. Soleymani, M., Esfeden, S.A., et al.: Analysis of EEG signals and facial expressions for
continuous emotion detection. IEEE Trans. Affect. Comput. 7(1), 17–28 (2017)
92
F. Wang et al.

13. Li, M.G., Zhu, W., et al.: The novel recognition method with optimal wavelet packet and
LSTM based recurrent neural network. In: IEEE International Conference on Mechatronics
and Automation, pp. 584–589 (2017)
14. Kumar, S., Sharma, A., et al.: A deep learning approach for motor imagery EEG signal
classiﬁcation. In: Computer Science and Engineering, pp. 34–39 (2017)
15. Davidson, R.J., Fox, N.A.: Asymmetrical brain activity discriminates between positive and
negative affective stimuli in human infants. Science 218(4578), 1235–1237 (1983)
16. Klimesch, W., Doppelmayr, M., et al.: Induced alpha band power changes in the human
EEG and attention. Neurosci. Lett. 244(2), 73 (1998)
17. Li, M., Lu, B.L.: Emotion classiﬁcation based on gamma-band EEG. In: International
Conference of the IEEE Engineering in Medicine & Biology Society, vol. 2009, pp. 1323–
1326 (2009)
18. Sourina, O., Liu, Y., et al.: Real-time EEG-based emotion recognition for music therapy.
J. Multimodal User Interfaces 5(1–2), 27–35 (2011)
19. Kothe, C.A., Makeig, S.: Estimation of task workload from EEG data: new and current tools
and perspectives. In: International Conference of the IEEE Engineering in Medicine and
Biology Society, pp. 6547–6551 (2011)
20. Chanel, G., Kierkels, J.J.M., et al.: Short-term emotion assessment in a recall paradigm. Int.
J. Hum.-Comput. Stud. 67(8), 607–627 (2009)
21. Heraz, A., Frasson, C.: Predicting the three major dimensions of the learner’s emotions from
brainwaves. Int. J. Comput. Sci. 2(3), 187–193 (2008)
22. Zheng, W.L., Zhu, J.Y., et al.: Identifying stable patterns over time for emotion recognition
from EEG. IEEE Trans. Affect. Comput. PP(99), 1 (2016)
23. Li, K., Li, X., et al.: Affective state recognition from EEG with deep belief networks. In:
IEEE International Conference on Bioinformatics and Biomedicine, pp. 305–310 (2013)
24. Mao, Z.J., Yao, W.X, et al.: EEG-based biometric identiﬁcation with deep learning. In:
International IEEE EMBS Conference on Neural Engineering, pp. 25–28 (2017)
25. Li, X., Song, D., et al.: Emotion recognition from multi-channel EEG data through
convolutional recurrent neural network. In: IEEE International Conference on Bioinformat-
ics and Biomedicine, pp. 352–359 (2017)
26. Fawzi, A., Samulowitz, H., et al.: Adaptive data augmentation for image classiﬁcation. In:
IEEE International Conference on Image Processing, pp. 3688–3692 (2016)
27. Hadjidimitriou, S.K., Hadjileontiadis, L.J.: Toward an EEG-based recognition of music
liking using time-frequency analysis. IEEE Trans. Biomed. Eng. 59(12), 3498 (2012)
28. Tibdewal, M.N., Tale, S.A.: Multichannel detection of epilepsy using SVM classiﬁer on
EEG signal. In: International Conference on Computing Communication Control and
Automation, pp. 1–6 (2016)
29. Chong, Y.S., Mokhtar, N., et al.: Automated classiﬁcation and removal of EEG artifacts with
SVM and wavelet-ICA. IEEE J. Biomed. Health Inform. PP(99), 1 (2017)
30. He, K., Zhang, X., et al.: Deep residual learning for image recognition. In: Conference on
Computer Vision and Pattern Recognition, pp. 770–778 (2015)
31. Soleymani, M., Lichtenauer, J., et al.: A multimodal database for affect recognition and
implicit tagging. IEEE Trans. Affect. Comput. 3(1), 42–55 (2014)
32. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines. ACM Trans. Intell.
Syst. Technol. 2(3), 27 (2011)
33. Vedaldi, A., Lenc, K.: MatConvNet: convolutional neural networks for matlab. In: ACM
International Conference on Multimedia, pp. 689–692 (2015)
34. Zhang, G., Ge, H.: Prediction of xylanase optimal temperature by support vector regression.
Electron. J. Biotechnol. 15(1), 7 (2012)
Data Augmentation for EEG-Based Emotion Recognition with Deep CNNs
93

Domain Invariant Subspace Learning
for Cross-Modal Retrieval
Chenlu Liu1, Xing Xu1(B), Yang Yang1, Huimin Lu2, Fumin Shen1,
and Yanli Ji3
1 Center for Future Media & School of Computer Science and Engineering,
University of Electronic Science and Technology of China, Chengdu, China
lcl 1208@163.com, xing.xu@uestc.edu.cn, dlyyang@gmail.com,
fumin.shen@gmail.com
2 Kyushu Institute of Technology, Kitakyushu, Japan
luhuimin@ieee.org
3 School of Automation Engineering,
University of Electronic Science and Technology of China, Chengdu, China
yanliji@uestc.edu.cn
Abstract. Due to the rapid growth of multimodal data, cross-modal
retrieval has drawn growing attention in recent years, which aims to take
one type of data as the query to retrieve relevant data of another type.
To enable directly matching between diﬀerent modalities, the key issue in
cross-modal retrieval is to eliminate the heterogeneity between modal-
ities. A bundle of existing approaches directly project the samples of
multimodal data into a common latent subspace with the supervision of
class label information, and diﬀerent samples within the same class con-
tribute uniformly to the subspace construction. However, the subspace
constructed by these methods may not reveal the true importance of each
sample as well as the discrimination of diﬀerent class label. To tackle this
problem, in this paper we regard diﬀerent modalities as diﬀerent domains
and propose a Domain Invariant Subspace Learning (DISL) method to
associate multimodal data. Speciﬁcally, DISL simultaneously minimize
the classiﬁcation error with sample-wise weighting coeﬃcients and pre-
serve the structure similarity within and across modalities with the graph
regularization. Therefore, the subspace learned by DISL can well reﬂect
the sample-wise importance and capture the discrimination of diﬀerent
class labels in multi-modal data. Compared with several state-of-the-art
algorithms, extensive experiments on three public datasets demonstrate
the superiority of the proposed method for cross-modal retrieval tasks
such as image-to-text and text-to-image.
Keywords: Cross-modal retrieval · Subspace learning
1
Introduction
The rapid development of Internet contributes to the explosion of online multi-
modal data. Much attention has been paid to analyse diﬀerent modal data [1–3],
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 94–105, 2018.
https://doi.org/10.1007/978-3-319-73600-6_9

Domain Invariant Subspace Learning for Cross-Modal Retrieval
95
thus emerge wide applications, like image labeling, image retrieval, multimedia
data management and etc. The traditional retrieval methods mainly focus on
single modality, like image retrieval [4] and text retrieval, where queries and its
related results are in the same type. Since diﬀerent forms of data may present
the same content, which means they may have some inner relationship, people
tend to use multimedia data to study information acquisition and processing.
Since queries and its search results must share the same modalities in tradi-
tional single-modal retrieval, cross-modal retrieval show a big advantage in this
respect, which provide more ﬂexible and various search results on multi-media.
The challenging problem in cross-modal retrieval is that the distribution and rep-
resentation of diﬀerent modalities are inconsistent. In other words, the feature
of image and text are diﬀerent in dimension and have heterogeneous properties,
thus we cannot directly compute the similarities between them.
Fig. 1. The general framework of our proposed domain invariant subspace learning for
cross-modal retrieval. After projecting original features into subspace, we use weighting
matrix to minimize the classiﬁcation error and ﬁne tune the space we learned. In order
to enhance the similarity between both inter-modality and intra-modality, we use the
multimodal graph as a discriminative regularization term to make the distance among
the related samples as small as possible in the learned space.
Early methods [5,6] consider data of various forms as multimedia documents
to construct relation graphs on retrieval task, which is not eﬀective on separately
existed data. Accordingly, many work devote to the latent space learning [7,8],
which eliminate the diversity among diﬀerent dimensional features and preserve
the structure and relationship in paired instances as well. The main idea of
subspace learning method is to project diﬀerent modal data features into one
common space, where features of various type have common representation, thus
we can directly measure similarities among them.
In this paper, we propose a novel method that is composed of two parts
to construct an eﬀective common subspace. The overall ﬂowchart of the pro-
posed method is shown in Fig. 1. With the purpose of preserving structure of

96
C. Liu et al.
intra-modality, we minimize the classiﬁcation error in the ﬁrst, which restricts
the distance among similar samples within same modality. For the labeled
multimodal data, image and its paired text share the same label information.
In order to ensure the distance of relevant instances among diverse modalities to
be as small as possible, we use a multimodal graph in the second part, aiming at
preserving the structure of both intra-modality and inter-modality. We conduct
our experiment on three benchmark multimodal datasets, and the results show
that our method is an eﬀective way for cross-modal retrieval.
Our contributions in this work can be summarized as follows:
– We propose a novel Domain Invariant Subspace Learning (DISL) method that
learns a common subspace which can truly reﬂect the sample-wise importance
and capture the discrimination of diﬀerent class labels in multimodal data.
– We employ eﬀective regularization terms in our framework to simultaneously
minimize the classiﬁcation error and to preserve the structure within and
across modalities.
– We develop an eﬃcient optimization algorithm to solve the complex minimiza-
tion problem in the proposed DISL, and extensive results on three benchmark
multimodal datasets show that the proposed DISL outperforms several rele-
vant state-of-the-art approaches.
The rest of paper is organized as follows. In Sect. 2, we discusses previous
work in cross-modal retrieval. We describe details of the proposed method in
Sect. 3 and present the experimental results in Sect. 4. At last, we summarize
the conclusion in Sect. 5.
2
Related Work
In this section, we brieﬂy review some representative methods of cross-modal
retrieval, which generally utilize four types of techniques: subspace learning, prob-
abilistic models, metric learning and deep learning. Since subspace learning plays
an important role in cross-modal matching problem, many approaches propose
various methods focusing on it, including unsupervised methods and supervised
methods. The former methods ignore the label information while the latter ones
utilize labels to guide the subspace learning. In most cases, supervised methods
can achieve better results. Among unsupervised methods, the Canonical Corre-
lation Analysis (CCA) algorithm [9] is famous for its simplicity and eﬃciency,
which aims at ﬁnding projection vectors for two sets of data and maximizing
the correlations between the mapped features. Recently, many extensions based
on CCA method have been proposed to better learn semantic subspace, like
[10,11]. The work in [10] extends the standard CCA to multi-label approaches
and did not need pairing samples. Moreover, in order to achieve a better sepa-
ration for multimodal data of diﬀerent classes in the learned common subspace,
Gong et al. [11] proposed a three-view CCA model which introduces a semantic
view obtained by supervised information or clustering analysis. Considering CCA
method is not valid on those nonlinear data, Hardoon and Shawe-Taylor [12]

Domain Invariant Subspace Learning for Cross-Modal Retrieval
97
applied kernel methods on CCA and proposed kernel canonical correlation anal-
ysis (KCCA) to learn a common semantic space for nonlinear real world data.
On the other hand, supervised methods establish diﬀerent modal relation-
ships between data by further exploiting label information. Wang et al. [13] pro-
posed a joint feature selection and subspace learning algorithm termed JFSSL,
which uses the multimodal graph regularization to explicitly preserve the sim-
ilarity between inter-modality and intra-modality. The results of cross-modal
retrieval experiment show it further improves the performance. What’s more,
the algorithm of [14] considered generalizing multiview analysis as an eigenvalue
decomposition problem. Its extensions GMLDA [14] and GMMFA [14] have show
good performances on the cross-modal retrieval problem, too. Besides those sub-
space learning methods, Jia et al. [15] used probabilistic model to learn a set of
shared topics across various modalities, which deﬁnes a Markov random ﬁeld on
the document level, the result show it is a good way to ﬂexibly model document
similarities. In addition, Srivastava and Salakhutdinov used a deep Boltzmann
machine [16] to learn a common representation for multimodal data. Metric
learning methods usually learn a distance metric between diﬀerent modalities of
data, which is a new perspective for cross-modal retrieval. Simply, the distance
is small when they are similar pairs. Mignon and Jurie [17] considered both pos-
itive and negative constraints to learn the metric. Quadrianto and Lampert [18]
learned projections in a shared feature space, where we can measure the similar-
ity between intra-modality and inter-modality by the Euclidean distance. With
the development of deep learning, many new methods based on deep learning
are springing up, which extract high-level features from the raw data. Frome
et al. [19] presented a deep visual-semantic embedding model to identify visual
objects, which used both labeled image data and semantic information from the
unannotated text documents.
In short, our work is a supervised method in subspace learning, which not
only preserve the original data structure but also reduce mismatch of the data
distribution. Thus we can obtain the common subspace where additionally incor-
porates the information of similarity between inter-modality and intra-modality.
3
Proposed Method
3.1
Problem Formulation
Suppose there are n labeled instances with image-text pairs, represented as:
X = {(Xv
i , Xt
i)}n
i=1, where Xv
i is paired with Xt
i. Here Xv
i ∈Rd1 is the feature
of the ith image and Xt
i ∈Rd2 is the feature of the ith text, d1 and d2 are the
dimensionality of image and text feature space, respectively. In addition, assume
image and text share c class labels {li}c
i=1. And the class label are denoted as
Y = {yi}n
i=1, the corresponding label of <Xv
i , Xt
i> is yi, where each yi ∈Rc is
a binary class indicator vector. That is, if a sample belongs to the jth class, then
its corresponding label is composed of all zeros except for the representation
place of the lj class being one.

98
C. Liu et al.
Inspired by the idea of subspace learning and domain adaptation, we use the
linear predictive functions to ﬁnd a subspace where only diﬀerent modal samples
within the same class are close to each other. For the cross matching problem
of two modalities, we project the original features into the low-dimensional
subspace propose to minimize modalities divergence and explore the invariant
structures across modalities. Accordingly, the objective function is formulated
as following:
L(wv, wt, bv, bt, mv, mt) = ∥Xvmvwv + bv −Y∥+ αv∥wv∥2
(1)
+ ∥Xtmtwt + bt −Y∥+ αt∥wt∥2 + γtr(A⊤LA)
s.t.
m⊤
v mv = I,
m⊤
t mt = I
In this loss function, mv, mt are the feature mapping matrices, where mv ∈
Rd1×p and mt ∈Rd2×p, with p denotes the dimension of the subspace, wv, wt and
bv, bt are the model weight and bias parameters respectively. Here in order to
make each mapping basis uncorrelated to each other, we use orthogonal condition
to constrain the mapping matrices mv and mt, and I is the identify matrix.
Moreover, we add a regularization penalty over the model parameters wv, wt to
achieve a smaller classiﬁcation error, and αv, αt are the tradeoﬀparameters.
Our goal is to ﬁnd a joint latent space, where corresponding samples in dif-
ferent modalities are mapped to nearby locations. Considering the shift between
diﬀerent modalities, only preserve the structure of intra-modalities may not
eﬃcient, so we add a discriminative regularization term tr(A⊤LA) for inter-
modalities. That is, we hope similar instances in one modality should have sim-
ilar mappings, and within the same class diﬀerent modal samples should be as
near as possible after mapping.
The intra and inter similarity between diﬀerent modalities is measured by
tr(A⊤LA), where A =

Xvmv
Xtmt

, L = D −J, here J is a 2n × 2n dimensional
matrix, with its elements being one or zero. For example, if the ith image shares
the same label with the jth image, then the cross of the ith row and jth column
is ﬁlled with one; if the ith image shares the same label with the jth text, then
the cross of the ith row and (n + j)th column is ﬁlled with one. D is a diagonal
matrix that contains the row sums of J.
3.2
Optimization
We solve the minimization problem by randomly choosing one parame-
ter and optimizing it with the others ﬁxed. What’s more, the parameter
wv, wt, bv, bt

can be expressed by mv and mt. To optimize the objective
function, we set the derivative of wv, wt, bv, bt to zero, and get the following
result:

bs = 1
n1⊤(Y −Xsmsws)

s∈(v,t) ,
(2)

ws =

(Xsms)⊤HsXsms + αsI
−1
m⊤
s Zs
	
s∈(v,t)
,
(3)

Domain Invariant Subspace Learning for Cross-Modal Retrieval
99

Zs = (Xs)⊤HsYs, Hs = I −1
n11⊤

s∈(v,t) .
(4)
Note that 1 is a column vector that all elements are ﬁlled with one, and we
use s ∈(v, t) for simplicity, in other words, s can be replaced by v or t.
Next we replace those parameters

wv, wt, bv, bt

in Eq. 1 by using
Eq. (2–4), we obtain the loss function:
L(mv, mt) = Y⊤HvY −Z⊤
v mv

m⊤
v Mvmv + αvI
−1 m⊤
v Zv
+ Y⊤HtY −Z⊤
s ms

m⊤
s Msms + αsI
−1 m⊤
s Zs
+ γtr

Xsms + Xtmt
⊤
L

Xsms + Xtmt

s.t.
m⊤
v mv = I,
m⊤
t mt = I
(5)
We deﬁne Mv = (Xv)⊤HvXv,
Mt = (Xt)⊤HtXt,
Xs =
Xs
0

and
Xt =

0
Xt

. We can easily compute the gradient of the objective function with
respect to mv and mt. Here is the gradient expression and s ∈(v, t):
▽msL (mv, mt) = −2ZsZ⊤
s ms

m⊤
s Msms + αsI
−1
+ 2γ

Xs⊤
L

Xvmv + Xtmt

+ 2Msms

m⊤
s Msms + αsI
−1
m⊤
s ZsZ⊤
s ms

m⊤
s Msms + αsI
−1
(6)
Since this is a non convex problem with the orthogonal constrains, we use the
gradient descent optimization [20] with curvilinear search, the detail can be
found in the references.
After obtain the local optimal solution mv and mt, we can get the other
parameters
wv, wt, bv, bt

by using Eq. (2–4). Those parameters are used
to predict the label of data. We project the original features
Xv, Xt
into
the common subspace via
Xvmv, Xtmt

to compute the similarity between
image and text. The whole algorithm is given in Algorithm 1, of which the
step 10/11 is often numerically cheaper than computing SVDs or geodesics. In
many cases, dimension p is much smaller than n/2, hence it follows from the
Sherman-Morrison-Woodbury (SMW) theorem that one only needs to invert a
smaller 2p × 2p matrix. The total ﬂops for computing Qs(τ) is 4np2 + O(p3),
and updating Qs(τ) for a diﬀerent τ needs 2np2 + O(p3) ﬂops.
4
Experiment
In this section, we conduct the experiment on three datasets: Wiki [21], Pascal
VOC [22] and Pascal Sentence databases [23], which have been widely used in
the cross-media retrieval problem. For each dataset, we compare our proposed
method with several state-of-the-art methods to verify its eﬀectiveness.

100
C. Liu et al.
Algorithm 1. Domain invariant subspace learning for cross-modal retrieval.
1: Input: 0 < μ < 1, ϵ ≥0, τ, Xv, Xt, Y.
2: Initialize the mapping matrices mv, mv by using Principal Component Analysis
(PCA).
3: for iter = 1 to Tmax do
4:
Compute gradients:
Gv = ▽mvL(mv, mt)
Gt = ▽mtL(mv, mt)
5:
Compute skew-symmetric matrices:
6:
Pv = Gvm⊤
v −mvG⊤
v
Pt = Gtm⊤
t −mtG⊤
t
7:
repeat
8:
τ = μτ
9:
Compute new trial point:
10:
Qv(τ) =

I + τ
2 Pv
−1 
I −τ
2 Pv

mv
11:
Qt (τ) =

I + τ
2 Pt
−1 
I −τ
2 Pt

mt
12:
until Armijo-Wolfe conditions meet
13:
Update the transformation matrices: mv = Qv(τ), mt = Qt(τ)
14: end for
15: Compute wv, wt, bv, bt via Eq. (2 ∼4)
4.1
Experimental Setup
Datasets and Features. As shown in Table 1, we use both shallow features
and convolutional neural network (CNN) features in our experiment. To make it
clear, we take the Wiki (shallow) as an example. We use 2,173 pairs for training,
462 pairs for testing and 231 pairs for validation, all those pairs belong to 10
classes. Notably, SIFT descriptor and latent Dirichlet allocation model are used
for image feature and text feature extraction, respectively.
Table 1. The statistics of the three datasets used in the experiments, where “/” in
columns “Img-txt Pairs” and “Img Feature” (“Text Feature”) stand for the number of
train/text/validation pairs.
Datasets
Img-txt pairs
Classes Img feature
Text feature
Wiki(shallow)
2173/462/231 10
128d SIFT
10d LDA
Pascal VOC
2808/2841/0
20
512d Gist
399d bag of words
vector
Wiki(CNN)
2173/462/231 10
4096d VGG-fc7 1000d bag of words
vector
Pascal sentence 800/100/100
20
4096d VGG-fc7 1000d bag of words
vector
Evaluation metrics. Specially, we use the mean average precision (MAP) [24]
for the evaluation, which can be calculated by Eq. 7. P(k) is the precision of
the total k retrieved results and R denotes the number of the retrieved results.
If the item at rank k is relevant to the query, we deﬁne rel(k) = 1, otherwise,

Domain Invariant Subspace Learning for Cross-Modal Retrieval
101
rel(k) = 0. We get the MAP score by averaging AP for all queries in the query
set.
AP =
R
k=1 P(k)rel(k)
R
i=1 rel(k)
.
(7)
In addition, we also use precision-scope curve [21] on Wiki and Pascal VOC
to clearly show the eﬀectiveness of our method. The scope is speciﬁed by the
number of top-ranked query related results.
Table 2. MAP scores on Wiki (shallow) and Pascal VOC datasets.
Method
Shallow Wiki
Pascal VOC
I-query T-query Aver
I-query T-query Aver
PLS [25]
0.215
0.171
0.193
0.320
0.251
0.286
CCA [9]
0.246
0.192
0.219
0.163
0.150
0.157
BLM [26]
-
-
-
0.301
0.224
0.263
GMMFA [14] 0.242
0.180
0.211
0.319
0.256
0.288
GMLDA [14]
0.181
0.138
0.160
0.308
0.244
0.276
KCCA [12]
0.269
0.213
0.241
-
-
-
LCFS [27]
0.268
0.218
0.243
0.344
0.267
0.306
DISL
0.279
0.228
0.254 0.400
0.304
0.352
4.2
Performance on Diﬀerent Datasets
Shallow features on Wiki. Due to the 10 dimensional textual features, we add
a step in the pretreatment, which use kernel methods to extend the dimension of
both image and text. Speciﬁcally, we use RBF kernel for image and linear kernel
for text. We compare our method with several famous methods, the results are
in the Table 2, for simplicity, we use I-query, T-query and Aver represent Image
query, Text query and Average MAP, respectively. Our method is more compact
and eﬀective by exploiting the inter-modality and intra-modality similarity rela-
tionships, with the MAP scores of image and text query are 0.279 and 0.228,
respectively. Moreover, the performance of KCCA is very close to LCFS, which
may because low features lose much useful information.
In addition, corresponding precision-scope curve of both Image query and
Text query are plotted in Fig. 2. It can be clearly observed that our method has
a good performance in both forms of cross-modal retrieval.
On Pascal VOC Dataset. Table 2 shows the MAP scores of various methods
on Pascal VOC. Since CCA only use pairwise information, it performs worse
than those methods that combine class information as well. Since our method
preserves the similarity relationship of both inter and intra modality by using
graph constraint, it is in excess of 5% higher than other methods. We also show
the precision-scope curve of those methods in Fig. 2, with the same scope, it is

102
C. Liu et al.
Scope
0
200
400
600
800
1000
Precision
0.1
0.15
0.2
0.25
0.3
CCA
PLS
GMMFA
GMLDA
KCCA
LCFS
DISL
(a)Image query
Scope
0
200
400
600
800
1000
Precision
0.1
0.15
0.2
0.25
0.3
0.35
CCA
PLS
GMMFA
GMLDA
KCCA
LCFS
DISL
(b)Text query
Scope
0
200
400
600
800
1000
Precision
0.1
0.15
0.2
0.25
0.3
0.35
0.4
CCA
PLS
BLM
GMLDA
GMMFA
DISL
(c)Image query
Scope
0
200
400
600
800
1000
Precision
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
CCA
PLS
BLM
GMLDA
GMMFA
DISL
(d)Text query
Fig. 2. (a) and (b) are the precision-scope curves on shallow Wiki dataset, (c) and (d)
are on the Pascal VOC dataset.
Table 3. MAP scores on Wiki(CNN) and Pascal sentence dataset.
Method
CNN Wiki
Pascal sentence
I-query T-query Aver
I-query T-query Aver
CCA [9]
0.189
0.184
0.187
0.154
0.129
0.142
CFA [28]
0.334
0.297
0.316
0.351
0.34
0.346
KCCA [12]
0.326
0.268
0.297
0.361
0.325
0.343
Corr-AE [29] 0.402
0.395
0.399
0.489
0.484
0.487
JRL [30]
0.453
0.40
0.427
0.504
0.489
0.497
LGCFL [31]
0.481
0.427
0.454
0.539
0.525
0.532
CMDN [32]
0.488
0.427
0.458
0.534
0.534
0.534
DISL
0.505
0.475
0.490 0.538
0.537
0.537
obvious shown that our method ﬁnds more correct matches in both forms of
cross-modal retrieval.
CNN features on Wiki. From Table 3, the proposed DISL achieves the best
overall performance with the average MAP at 0.49, which outperforms the exist-
ing state-of-the-art algorithm with MAP 0.458 for CMDN that uses deep neural
network with metric learning. The performance of LGCFL is very close to CMDN
since they model the correlation of both intra-modality and inter-modality.
On Pascal Sentence Dataset. We show the detail in Table 3. Among those
semantic methods, our algorithm gives the best performance both on image query
and text query. Though it perform a little better than the existing state-of-the-
art algorithms with MAP score 0.5% higher than the LGCFL and 0.3% higher
than CMDN, showing the eﬀectiveness of the proposed algorithm. CCA and
CFA are two traditional methods in cross-modal retrieval, whose accuracies are
limited. Since KCCA extends standard CCA to better model nonlinear datasets
by using kernel functions, the average MAP of KCCA is about 20% higher than
that of CCA.

Domain Invariant Subspace Learning for Cross-Modal Retrieval
103
4.3
Parameter Sensitivity and Convergency Analysis
There are ﬁve parameters in the supervised algorithm: μ, τ, αv, αt, γ. To ensure
the performance of the proposed method, we empirically set μ = 0.1, τ = 0.001
in the curvilinear search, and the parameters αv, αt, γ are selected from {0.001,
0.01, 0.1, 1, 10, 100, 1000}. We use the validation set to get the optimal values.
1000
100
10
1
αt
0.1
0.01
0.001
1000
100
10
αv
1
0.1
0.01
0.001
0.5
0.1
0.2
0.4
0.3
MAP
(a) Image query
1000
100
10
1
αt
0.1
0.01
0.001
1000
100
10
αv
1
0.1
0.01
0.001
0.3
0.2
0.1
0.4
MAP
(b) Text query
Interation number
0
50
100
150
200
250
Objective Loss
3800
4000
4200
4400
4600
4800
5000
(c) Convergence curve
1000
100
10
1
αt
0.1
0.01
0.001
1000
100
10
αv
1
0.1
0.01
0.001
0.5
0.6
0.2
0.1
0.3
0.4
MAP
(d) Image query
1000
100
10
1
αt
0.1
0.01
0.001
1000
100
10
αv
1
0.1
0.01
0.001
0.5
0.6
0.2
0.1
0.3
0.4
MAP
(e) Text query
Interation number
0
50
100
150
200
250
Objective Loss
580
600
620
640
660
680
700
720
740
760
780
(f) Convergence curve
Fig. 3. On the top row, subﬁgures (a)–(c) are the performance variation with respect
to αv and αt when ﬁxing γ, and the convergence curve on Pascal VOC datasets,
respectively. Similarly, the subﬁgures (d)–(f) shows the corresponding results on Pascal
Sentence dataset.
Taking the results on Pascal VOC for instance, from Fig. 3(a)–(b), we can
clearly see our method is insensitive to parameters, showing the robustness of
our method. In particularly, our method obtains better performance when αv is
1 to 10, the reason lies on that if the αv is too large, it may remove some useful
features. If the αv is too small, it cannot eﬀectively select out the representative
features. In Fig. 3(c), it can be obviously see that the algorithm generally con-
verges within about forty iterations. Moreover, the running time of convergence
on the Pascal VOC dataset takes only ﬁve minutes, which show the rapidity
of our method. Actually, consistent observations can be obtained according to
the results from Fig. 3(d)–(f) on Pascal Sentence dataset. Therefore, our method
can achieve stable performance eﬃciently on benchmark datasets for practical
usage.
5
Conclusion
In this paper, we have explored a new Domain Invariant Subspace Learning
(DISL) method for cross-modal retrieval. In the proposed method, we preserve

104
C. Liu et al.
the structure of original data by minimizing the classiﬁcation error. Particularly,
we use the multimodal graph to enhance the similarity between both inter-
modality and intra-modality. Furthermore, the experimental results on three
widely used datasets show the proposed DISL could not only reduce the data
distribution mismatch across diﬀerent modalities but also preserve structure
properties of the original data. Indeed, the proposed DISL can be extended to a
semi-supervised setting where unlabeled multimodal samples can be leveraged to
construct more compact multimodal graph that improves the subspace learning
result. This would be investigated in our future work.
Acknowledgments. This paper is partially supported by NSFC grants No. 61602089,
61572108, 61632007, 61673088; Fundamental Research Funds for Central Universities
ZYGX2014Z007 and ZYGX2016KYQD114; LEADER of MEXT-Japan (16809746),
The Telecommunications Foundation, REDAS and SCAT.
References
1. Shen, X., Shen, F., Sun, Q., Yang, Y., Yuan, Y., Shen, H.T.: Semi-paired discrete
hashing: learning latent hash codes for semi-paired cross-view retrieval. In: TCYB
(2016)
2. He, L., Xu, X., Lu, H., Yang, Y., Shen, F., Shen, H.T.: Unsupervised cross modal
retrieval through adversarial learning. In: ICML (2017)
3. Zhang, H., Zha, Z., Yang, Y., Yan, S., Gao, Y., Chua, T.: Attribute-augmented
semantic hierarchy: towards bridging semantic gap and intention gap in image
retrieval. In: ACM MM, pp. 33–42 (2013)
4. Zhang, H., Shen, F., Liu, W., He, X., Luan, H., Chua, T.: Discrete collaborative
ﬁltering. In: SIGIR, pp. 325–334 (2016)
5. Zhuang, Y., Yang, Y., Wu, F.: Mining semantic correlation of heterogeneous multi-
media data for cross-media retrieval. IEEE Trans. Multimedia 10, 221–229 (2008)
6. Yang, Y., Zhuang, Y., Wu, F., Pan, Y.: Harmonizing hierarchical manifolds for
multimedia document semantics understanding and cross-media retrieval. IEEE
Trans. Multimedia 10, 437–446 (2008)
7. Xu, X., Shimada, A., Taniguchi, R., He, L.: Coupled dictionary learning and feature
mapping for cross-modal retrieval. In: ICME, pp. 1–6 (2015)
8. Xu, X., Shen, F., Yang, Y., Shen, H.T., Li, X.: Learning discriminative binary
codes for large-scale cross-modal retrieval. IEEE Trans. Image Proces. 26, 2494–
2507 (2017)
9. Hardoon, D.R., Szedm´ak, S., Shawe-Taylor, J.: Canonical correlation analysis: an
overview with application to learning methods. Neural Comput. 16, 2639–2664
(2004)
10. Ranjan, V., Rasiwasia, N., Jawahar, C.V.: Multi-label cross-modal retrieval. In:
ICCV, pp. 4094–4102 (2015)
11. Gong, Y., Ke, Q., Isard, M., Lazebnik, S.: A multi-view embedding space for mod-
eling internet images, tags, and their semantics. Int. J. Comput. Vis. 106, 210–233
(2014)
12. Hardoon, D.R., Shawe-Taylor, J.: KCCA for diﬀerent level precision in content-
based image retrieval. In: CIBM (2003)

Domain Invariant Subspace Learning for Cross-Modal Retrieval
105
13. Wang, K., He, R., Wang, L., Wang, W., Tan, T.: Joint feature selection and sub-
space learning for cross-modal retrieval. IEEE Trans. Pattern Anal. Mach. Intell.
38, 2010–2023 (2016)
14. Sharma, A., Kumar, A., Daume, H., Jacobs, D.W.: Generalized multiview analysis:
a discriminative latent space. In: CVPR, pp. 2160–2167 (2012)
15. Jia, Y., Salzmann, M., Darrell, T.: Learning cross-modality similarity for multino-
mial data. In: ICCV, pp. 2407–2414 (2011)
16. Srivastava, N., Salakhutdinov, R.: Multimodal learning with deep Boltzmann
machines. In: NIPS, pp. 2231–2239 (2012)
17. Mignon, A., Jurie, F.: CMML: a new metric learning approach for cross modal
matching, pp. 1–14 (2012)
18. Quadrianto, N., Lampert, C.H.: Learning multi-view neighborhood preserving pro-
jections. In: ICML, 425–432 (2011)
19. Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., Mikolov,
T.: Devise: a deep visual-semantic embedding model. In: NIPS, pp. 2121–2129
(2013)
20. Wen, Z., Yin, W.: A feasible method for optimization with orthogonality con-
straints. Math. Program. 142, 397–434 (2013)
21. Rasiwasia, N., Moreno, P.J., Vasconcelos, N.: Bridging the gap: query by semantic
example. IEEE Trans. Multimedia 9, 923–938 (2007)
22. Hwang, S.J., Grauman, K.: Reading between the lines: object localization using
implicit cues from image tags. IEEE Trans. Pattern Anal. Mach. Intell. 34, 1145–
1158 (2012)
23. Simon, M., Rodner, E., Denzler, J.: Imagenet pre-trained models with batch nor-
malization. CoRR (2016)
24. Rasiwasia, N., Pereira, J.C., Coviello, E., Doyle, G., Lanckriet, G.R.G., Levy, R.,
Vasconcelos, N.: A new approach to cross-modal multimedia retrieval. In: MM, pp.
251–260 (2010)
25. Li, A., Shan, S., Chen, X., Gao, W.: Cross-pose face recognition based on partial
least squares. Pattern Recogn. Lett. 32, 1948–1955 (2011)
26. Tenenbaum, J.B., Freeman, W.T.: Separating style and content with bilinear mod-
els. In: Neural Computation, pp. 1247–1283 (2000)
27. Wang, K., He, R., Wang, W., Wang, L., Tan, T.: Learning coupled feature spaces
for cross-modal matching. In: ICCV, pp. 2088–2095 (2013)
28. Li, D., Dimitrova, N., Li, M., Sethi, I.K.: Multimedia content processing through
cross-modal association. In: MM, pp. 604–611 (2003)
29. Feng, F., Wang, X., Li, R.: Cross-modal retrieval with correspondence autoencoder.
In: MM, pp. 7–16 (2014)
30. Zhai, X., Peng, Y., Xiao, J.: Learning cross-media joint representation with sparse
and semisupervised regularization. IEEE Trans. Circ. Syst. Video Technol. 24,
965–978 (2014)
31. Kang, C., Xiang, S., Liao, S., Xu, C., Pan, C.: Learning consistent feature represen-
tation for cross-modal multimedia retrieval. IEEE Trans. Multimedia, 17, 370–381
(2015)
32. Peng, Y., Huang, X., Qi, J.: Cross-media shared representation by hierarchical
learning with multiple deep networks. IJCA I, 3846–3853 (2016)

Eﬀective Action Detection Using Temporal
Context and Posterior Probability of Length
Xinran Liu, Yan Song(B), and Jinhui Tang
Nanjing University of Science and Technology, Nanjing, China
songyan@njust.edu.cn
Abstract. In this paper, we focus on human action detection for untrim-
med long videos. We propose an eﬀective action detection system aiming
at solving two diﬃculties in existing works. Firstly, we propose to take
into account the temporal context information in model learning to tackle
with the problem of high-quality proposal generation. Secondly, we pro-
pose to utilize the posterior probability of proposal length to adjust the
selection criterion of action proposals. This can eﬀectively encourage the
proposals with reasonable lengths and suppress the high-classiﬁcation-
score proposals with unreasonable lengths. We test our method on the
THUMOS14 Dataset and the experiment results show that our action
detection system improve the performance by about 4% compared with
the state-of-art methods.
Keywords: Action detection · Temporal-context-aware sparse coding
Posterior probability of length
1
Introduction
With the rapid development of computer vision, analyzing and understanding
human actions in videos is becoming more and more popular. Action recognition
usually deals with trimmed videos that have a single action. In recent literatures,
it has achieved great progress that many feasible algorithms have been developed.
Diﬀerently, action detection deals with long untrimmed videos that contain more
than one action and plenty of backgrounds, which is more practical in reality.
Action detection not only labels actions, but also detects the temporal position
and duration of the actions [7,11–15,17].
Exiting methods of action detection still have many shortcomings because of
two main diﬃculties. Firstly, how to generate high-quality proposals? A sample
proposal is shown in Fig. 1. High-quality proposals should contain as much as the
ground truth actions with high recall. However, most of the candidate proposals
contain certain proportion of non-action parts. Existing methods [5] usually
applied models learned from trimmed actions to select proposals, which may not
be generalized enough. Secondly, how to classify the proposals accurately? Many
existing works [13,14] applied recognition methods to action detection directly.
However, the selection criterion of action proposals is inconsistent with that in
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 106–117, 2018.
https://doi.org/10.1007/978-3-319-73600-6_10

Eﬀective Action Detection Using Temporal Context
107
action recognition. For example, the classiﬁer scores can be high, even if the
proposal has only a small overlap with the ground truth, as shown in Fig. 2.
For action detection, this proposal may not be a good choice. Shou et al. [11]
addressed this problem and modiﬁed the loss function to solve it.
Fig. 1. A sample of temporal action proposals for a long video. The blue bar reprensents
the ground truth. The green segment is a true proposal and the red one is a false
proposal. (Color ﬁgure online)
Fig. 2. A sample of proposal has only a small overlap with the ground truth, but the
classiﬁer scores can be high.
In order to solve the above problems, we propose an eﬀective action detection
system shown in Fig. 3. With respect to the ﬁrst problem, we propose a temporal-
context-aware sparse coding based proposal generation method. It takes the
temporal context of actions into consideration to learn dictionaries for sparse
representation, which can make the model more generalized and the proposal
set more complete. For the second problem, we propose to utilize the posterior
probability of proposal length to adjust the selection criterion of action propos-
als. This can eﬀectively encourage the proposals with reasonable lengths and
suppress the high-classiﬁcation-score proposals with unreasonable lengths. In
addition, we also propose several eﬀective steps to improve the detection perfor-
mance, such as the binary classiﬁcation of action and non-action.
The rest of this paper is organized as follows. Section 2 introduces the related
works on human action detection. Section 3 describes our temporal-context-
aware sparse coding method for proposal generation. Section 4 describes our
proposal classiﬁcation method. The experiment results are reported in Sect. 5.
Section 6 concludes this paper.

108
X. Liu et al.
Fig. 3. The framework of the proposed system for action detection.
2
Related Work
In action detection, temporal sliding window is usually used to generate can-
didate video segments, the number of which is unavoidable huge. So we need
to select proposals which should contain as much as the true actions from the
clipped segment set. On the other hand, the number of selected proposal should
be as few as possible for the consideration of subsequent computational burden.
Recent works adopted many methods for proposal generation, such as action
tubes [4], proposals from dense trajectories [3], the fast proposal method [16],
bag of fragments [10] and sparse coding proposal [5]. Most of them relied on the
dense trajectories or used hierarchical grouping methods. Heilbron et al. [5] used
sparse coding to proposal generation for the ﬁrst time, which adopted a proposal
learning function that can be adapted to varying activity types of interest.
Proposal classiﬁcation usually took advantage of the methods used in action
recognition. Most earlier action methods relied on manually selected features like
STIP [9] and dense trajectories [8]. In recent years, many state-of-the-art action
detection methods used the iDT [7,12]. Yuan et al. [18] constructed the MPII
Cooking Activities Dataset and adopted the trajectory features with integrated
histogram for eﬃcient computation.
On the other hand, the neural networks have achieved great success in com-
puter vision applications. Du et al. [2] proposed a spatiotemporal feature learned
by deep 3-dimensional convolutional networks, which is called C3D, which is
conceptually very simple and is easy to train. Wang et al. [14] proposed to com-
bine motion and appearance features for action detection. Karaman et al. [7]

Eﬀective Action Detection Using Temporal Context
109
proposed a fast saliency based pooling of ﬁsher encoded dense trajectories for
action detection. Singh and Cuzzolin [13] used C3D features and binary random
forest classiﬁer to detect actions. Shou et al. [11] proposed a multi-stage CNNs to
solve the second problem we mentioned before. They used a novel loss function
for the localization network. Yuan et al. [17] proposed a pyramid of score dis-
tribution features with Recurrent Neural Networks (RNN) for action detection.
Yeung et al. [15] proposed an end-to-end learning method of action detection
from frame glimpses in videos.
3
Proposal Generation
Given a long untrimmed video, we ﬁrst extract a large set of candidate segments
by sliding windows. Action proposals are generated from the candidate set by
deleting the candidates that are unlikely to contain actions. There is a trade-oﬀ
in proposal generation. The generated proposals should contain as many actions
as possible. On the other hand, large number of proposals means high subsequent
computational burden.
Figure 4 shows the method that we propose to generate action proposals
from an untrimmed video, which is based on a temporal-context-aware sparse
coding method. In dictionary learning, C3D features are extracted from segments
which include temporal context of actions, and then they are used to learn sparse
dictionaries. In proposal generation, the candidate segments are extracted from
the input untrimmed videos, then proposals are selected from the candidate set
based on the reconstruction error of sparse representation.
3.1
Feature Extraction
We adopt the C3D feature [2] which is generated from a deep 3-dimensional
convolution network. It is well suited for spatiotemporal feature learning for
videos. The network input is a video clip containing 16 frames. The C3D network
has 8 convolution layers, 5 max-pooling layers, 2 fully connected layers, followed
by a softmax output layer. It uses convolution kernels. Each fully connected
layer has 4096 output units. We use the fully connected layer fc-7 output as the
feature and reduce its dimension to 500 by PCA.
3.2
Candidate Segments Generation
For an long untrimmed video, temporal sliding windows are adopted to clip a
large number of video segments as candidates. The clipping is performed uni-
formly over time, but the sliding window lengths are decided based on the length
distribution of the trimmed action videos in the training set. Thus, we clip the
input video into a large number of candidate segments, which can be overlapped
in time. However, we observe that short lengths are usually not included in the
selection of clusters by MeanShift [5]. So we supplement a short length for the
sliding window by experience. This makes the candidate set more complete.

110
X. Liu et al.
Fig. 4. The process of proposal generation. The left represents the dictionary learning
step, and the right represents the proposal generation step.
3.3
Temporal-Context-Aware Sparse Coding for Proposal
Generation
Sparse coding is widely used in the computer vision ﬁeld, which represents the
original signal as a sparse linear combination of dictionary elements. Heilbron et
al. [5] proposed to adopt sparse learning for fast proposal generation. They used
trimmed action segments for dictionary learning.
As we observe, it happens with a low probability that the action parts are
exactly clipped as candidates. More often, candidates are comprised of part of
actions and the temporal context of them. Thus, we propose a temporal-context-
aware sparse coding method for proposal generation. Speciﬁcally, we utilize the
segments which contain temporal context information of actions for dictionary
learning, instead of using the trimmed action segments. The temporal-context-
aware segments are those sequences whose temporal Intersection over Union
(IoU) with the action segments is smaller than 1.
We testify two methods for dictionary learning. For the ﬁrst one, segments
whose IoUs with the action segments are larger than a threshold are selected for
learning a dictionary. For the second one, we use more than one set of temporal-
context-aware segments to learn multiple dictionaries. Each set is comprised of
segments whose IoUs are in a certain range.
Then we compute the reconstruction error based on the learned dictionaries
to rank the candidates. Based on the ranking, a subset of the candidates is

Eﬀective Action Detection Using Temporal Context
111
selected as proposals. Specially, for the second method, we integrate the results
from multiple dictionaries as the ﬁnal proposals.
4
Proposal Classiﬁcation and Action Detection
After obtaining a large number of action proposals, we have to classify them
to decide whether they are considered as action detections and what kind of
actions they are. Firstly, we propose to adopt a binary classiﬁer to diﬀerentiate
action and non-action preliminarily. Then, action proposals are recognized by a
multi-class classiﬁer. Furthermore, we propose to use the posterior probability of
segment length to adjust the conﬁdence score. Finally, we use the Non-Maximum
Suppression (NMS) to remove redundant detections.
4.1
Action v.s. Non-action Binary Classiﬁcation
Many proposals still have high overlaps with the background. Before the multi-
class classiﬁcation, we propose to preliminarily classify the proposals into two
categories, the action and the non-action. This step is able to remove the pro-
posals with large portion of typical backgrounds, which reduces the subsequent
computational burden and improve the detection performance.
We adopt binary SVM as the classiﬁer. We construct the positive training
set by the segments of trimmed actions and the negative training set by the
candidate segments whose IoUs with the ground truth are 0. At last, we make
the ratio of the positive set and negative set near 1 by randomly selecting samples
from the negative set.
A proposal usually contains many clips. If the proportion of clips which are
classiﬁed as background is more than 30%, the proposal is labeled as background,
otherwise it is labeled as action.
4.2
Proposal Classiﬁcation
For the proposal classiﬁcation, we train an one-v.s.-all SVM classiﬁer. We con-
struct the training set following a similar strategy as the binary classiﬁcation.
Moreover, in order to balance the number of training data for each class, we
reduce the number of background samples to keep it roughly equal to the aver-
age number of samples per category.
4.3
Adjustment by the posterior probability of length
As mentioned in the introduction, there is a common observation in action detec-
tion, which is that the conﬁdence score may be high even if the proposal has
only a small overlap with the action. As a matter of fact, we should encourage
to remain the proposals with large IoUs with the ground truth and suppress
the ones with smaller IoUs. This problem was also mentioned in [11] and they
proposed to solve it by adjusting the loss function of the classiﬁer.

112
X. Liu et al.
In this work, we propose a simpler yet eﬀective method to solve this prob-
lem based on the posterior probability of segment length. Speciﬁcally, we adopt
the naive bayesian posterior probability to adjust the conﬁdence score of the
proposal by:
P(Ci|f, Θ, L) = P(L|Ci)P(Ci|f, Θ)
P(L)
.
(1)
P(L|Ci) is the posterior probability of the action length according to the ith
category, which we adopt GMM to ﬁt. Figure 5 shows the action length dis-
tribution by GMM for “JavelinThrow” and “CleanAndJerk”. P(Ci|f, Θ) is the
category probability of the proposal we inferred from the svm model, i.e. the
average score of the clips in the proposal. f is the features of clips and Θ is the
parameters in svm. P(L) is the prior probability which is set to 1. P(Ci|f, Θ, L)
is the proposal’s new score, which we use to rank.
Fig. 5. The action length distribution of “CleanAndJerk” (left) and “JavelinThrow”
(right).
4.4
Action Detection
Finally, we use NMS to remove the redundant detections. We set the overlap
threshold in NMS slightly smaller than the overlap threshold θ (measured by
IoU) in the evaluation. As a result we obtain the ﬁnal action detection results,
which contain the action labels as well as the starting frames and the ending
frames.
5
Experiments
5.1
Dataset
THUMOS 2014. THUMOS 2014 dataset [6] can be used for action recognition
and detection. The detection task includes twenty categories, such as “Baseball-
Pitch”, “Diving”, “HighJump”, etc. It provides a large amount of untrimmed

Eﬀective Action Detection Using Temporal Context
113
videos annotated by the starting and ending time and the label of the actions.
The validation set includes 200 untrimmed videos. The testing set includes 213
untrimmed videos. We use the validation set as the training set for dictionary
learning and classiﬁer training. We use all the 213 untrimmed videos for test.
5.2
Evaluation Metrics
In proposal generation step, as the conventional methods, we evaluate recall of
the proposed method. To determine whether a proposal is correct, we use the
IoU of the proposal and the ground truth. If the IoU is larger than the threshold
θ, it is considered as a true proposal.
In action detection step, we adopt the conventional metric of mAP. A detec-
tion result is considered as correct only when it has the correct label and its IoU
with the ground truth is larger than the overlap threshold θ.
5.3
Experimental Details
C3D feature. The general C3D network takes a clip of 16 consecutive frames
as input. Here, we extract 16 frames at an interval of two frames to generate the
Interval-C3D feature. A clip actually covers a length of 32 frames.
Sliding window length supplement. We add a short sliding window length
for candidate generation, which is set as 17 frames by experience.
Temporal-context-aware sparse coding. For the ﬁrst method, we use the
segments whose IoUs are larger than 0.6 to learn a dictionary for sparse coding.
For the second method, we adopt ﬁve IoU ranges which are (0.6, 0.7), (0.7,
0.8), (0.8, 0.9), (0.9, 1), and 1. The results generated from the ﬁve dictionaries
are combined to form the ﬁnal candidate set. All the dictionaries we use 256
elements.
Weighted Non-Maximum Suppression (WNMS). Diﬀerent from other
methods which used normal NMS, we use the weighted NMS to reduce the
proposals number to an average of 1000. The weighted NMS for diﬀerent lengths
of proposals uses diﬀerent suppression overlaps. We divide proposal length into
six ranges, which are (0, 50), (50, 500), (500, 600), (600, 800), (800, 1200), (1200,
the video length). These six ranges correspond to six overlaps, which are 0.55,
0.65, 0.55, 0.45, 0.35 and 0.25 respectively.
5.4
Results
5.4.1
Proposal Generation
Comparison results of the proposed proposal generation methods. We
compare our three methods and the Class-Induced method [5].
1. Class-Induced method [5]
2. 1 + adding small sliding windows
3. 2 + the ﬁrst temporal-context-aware sparse coding method
4. 2 + the second temporal-context-aware sparse coding method.

114
X. Liu et al.
All the methods use the weighted NMS. We list the comparison results of recall
at the average of 1000 proposals in Fig. 6. The result shows the method (4)
achieves the best performance. The average increase of recall is about 2%.
Fig. 6. The comparison results of recall at the average of 1000 proposals for the pro-
posed proposal generation methods.
Comparison with the state-of-the-art methods. We compare our best
method, the second temporal-context-aware sparse coding method, with sev-
eral state-of-the-art methods including APT [3], BoFrag [10], Sparse-prop [5] and
SCNN-prop [11]. We list the comparison results in Fig. 7. Obviously, our method
signiﬁcantly outperforms other methods. Speciﬁcally, our method achieves the
recall of 97.6% when the tIoU is 0.5.
5.4.2
Action Detection
The impact of the C3D feature processing method. We compare the
General-C3D feature with the Interval-C3D feature and list the results in Table 1.
The results show that the Interval-C3D feature performs better than the General-
C3D feature.
The impact of the binary classiﬁcation and the posterior probabil-
ity of proposal length. We compare three methods which are (1) multi-class
classiﬁcation, (2) binary classiﬁcation and multi-class classiﬁcation, (3) binary
classiﬁcation, multi-class classiﬁcation and posterior probability. All the meth-
ods use the Interval-C3D feature. We list the comparison results in Table 2. The
results show that the third method achieves the best performance. The binary
classiﬁcation brings in an increase of 3.52% for mAP and the posterior proba-
bility of proposal length brings in an increase of 11.87% for mAP.
Comparison with the state-of-the-art methods. We compare our best
method with several state-of-the-art methods including Karaman et al. [7],

Eﬀective Action Detection Using Temporal Context
115
Fig. 7. The comparison results of recall at the average of 1000 proposals for the pro-
posed proposal generation method with several state-the-art methods.
Table 1. Comparison results of action detection with diﬀerent C3D features and dif-
ferent classiﬁcation schemes.
Method
Feature
mAP (θ = 0.5)
Multi-class classiﬁcation
General-C3D 0.0718
Interval-C3D 0.0836
Binary classiﬁcation + multi-class classiﬁcation General-C3D 0.0753
Interval-C3D 0.1188
Table 2. Comparison results of action detection of our three methods.
Method
mAP (θ = 0.5)
Multi-class classiﬁcation
0.0836
Binary classiﬁcation + multi-class
classiﬁcation
0.1188
Binary classiﬁcation + multi-class
classiﬁcation + posterior probability
0.2375
Wang et al. [14], Oneata et al. [1], Yuan et al. [17], Yeunget et al. [15] and
Shou et al. [11]. We list the comparison results in Table 3. Our method improves
the action detection performance by more than 4% on average.

116
X. Liu et al.
Table 3. Comparison of mAPs of our method and several state-of-the-art methods.
θ
0.5
0.4
0.3
0.2
0.1
Karaman et al. [7]
0.2
0.3
0.5
0.9
1.5
Wang et al. [14]
8.5
12.1
14.6
17.8
19.2
Oneata et al. [1]
15.0
21.8
28.8
36.2
39.8
Yuan et al. [17]
18.8
26.1
33.6
42.6
51.4
Yeunget et al. [15] 17.1
26.4
36.4
44.0
48.9
Shou et al. [11]
19.0
28.7
36.3
43.5
47.7
Ours
23.8 32.7 41.8 48.2 52.1
6
Conclusions
We propose an eﬀective action detection system for long untrimmed videos.
Our method includes the proposal generation and proposals classiﬁcation steps.
For the proposals generation step, we learn a sparse dictionary by temporal-
context-aware segments. For the proposal classiﬁcation step, we propose to clas-
sify actions and non-actions before the multi-class classiﬁcation, followed by a
posterior probability adjusted step. Experiments demonstrate the eﬀectiveness
of these methods, especially the posterior probability of proposal length.
Acknowledgments. This work was supported in part by the 973 Program under
Grant 2014CB347600; in part by the National Nature Science Foundation of China
under Grants 61672285.
References
1. Oneata, D., Verbeek, J., Schmid, C.: Action and event recognition with ﬁsher
vectors on a compact feature set. In: IEEE International Conference on Computer
Vision, pp. 1817–1824 (2013)
2. Du, T., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotemporal
features with 3D convolutional networks. In: IEEE International Conference on
Computer Vision, pp. 4489–4497 (2016)
3. Van Gemert, J.C., Jain, M., Gati, E., Snoek, C.G.M.: APT: action localization
proposals from dense trajectories. In: BMVC (2015)
4. Gkioxari, G., Malik, J.: Finding action tubes, pp. 759–768 (2014)
5. Heilbron, F.C., Niebles, J.C., Ghanem, B.: Fast temporal activity proposals for
eﬃcient detection of human actions in untrimmed videos. In: IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1914–1923 (2016)
6. Jiang, Y.G., Liu, J., Roshan Zamir, A., Toderici, G., Laptev, I., Shah, M., Suk-
thankar, R.: Thumos challenge: action recognition with a large number of classes
(2014). http://crcv.ucf.edu/THUMOS14/
7. Karaman, S., Seidenari, L., Bimbo, A.D.: Fast saliency based pooling of ﬁsher
encoded dense trajectories. In: European Conference on Computer Vision (2014)

Eﬀective Action Detection Using Temporal Context
117
8. Klaser, A., Schmid, C.: Action recognition by dense trajectories. In: Computer
Vision and Pattern Recognition, pp. 3169–3176 (2011)
9. Laptev, I., Lindeberg, T.: On space-time interest points. Int. J. Comput. Vis. 64(2–
3), 107–123 (2005)
10. Mettes, P., Van Gemert, J.C., Cappallo, S., Mensink, T., Snoek, C.G.M.: Bag-of-
fragments: selecting and encoding video fragments for event detection and recount-
ing. In: ACM on International Conference on Multimedia Retrieval, pp. 427–434
(2015)
11. Shou, Z., Wang, D., Chang, S.F.: Temporal action localization in untrimmed videos
via multi-stage CNNs, pp. 1049–1058 (2016)
12. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-
nition in videos. Adv. Neural Inf. Process. Syst. 1(4), 568–576 (2014)
13. Singh, G., Cuzzolin, F.: Untrimmed video classiﬁcation for activity detection: sub-
mission to activitynet challenge (2016)
14. Wang, L., Qiao, Y., Tang, X.: Action recognition and detection by combining
motion and appearance features (2014). http://crcv.ucf.edu/THUMOS14/papers/
CUHK&SIAT.pdf
15. Yeung, S., Russakovsky, O., Mori, G., Li, F.F.: End-to-end learning of action detec-
tion from frame glimpses in videos. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2678–2687 (2016)
16. Yu, G., Yuan, J.: Fast action proposals for human action detection and search.
In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 1302–1311
(2015)
17. Yuan, J., Ni, B., Yang, X., Kassim, A.A.: Temporal action localization with pyra-
mid of score distribution features. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3093–3102 (2016)
18. Yuan, J., Liu, Z., Wu, Y.: Discriminative subvolume search for eﬃcient action
detection. In: IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2009, pp. 2442–2449 (2009)

Eﬃcient Two-Layer Model Towards Cover Song
Identiﬁcation
Xiaoshuo Xu(B), Yao Cheng, Xiaoou Chen, and Deshun Yang
Institute of Computer Science and Technology, Peking University,
128 Zhongguancun North Street, Haidian District,
Beijing 100871, People’s Republic of China
{xuxiaoshuo,chengyao,chenxiaoou,yangdeshun}@pku.edu.cn
Abstract. So far, few cover song identiﬁcation systems aim at prac-
tical application. On one hand, existing sequence alignment methods
achieve a high precision at the expense of high time cost. On the other
hand, for large-scale identiﬁcation, researchers attempt to exploit ﬁxed
low-dimensional features to reduce time cost. However, such highly com-
pressed representations often result in a worse accuracy. In this paper,
we propose an eﬃcient two-layer system which takes advantage of the
two kinds of methods. The proposed approach outperforms existing
approaches and achieves high precision with relatively small time com-
plexity.
Keywords: Cover song · Two-layer model · Practical system
1
Introduction
Cover song identiﬁcation has played an increasingly important role in music
information retrieval over the past ten years because of its potential commercial
values in music copyright protection or music management.
In the early stage, many works represented songs as sequences of feature
vectors and concentrated on sequence alignment algorithms. Popular features
like chroma [7], which represents the intensity of 12 pitch classes and has many
enhanced versions, have been widely used to represent music. Then some elabo-
rate matching techniques are used to measure the similarity between songs. For
instance, Ellis and Poliner [5] applied Dynamic Programming (DP) to extract
Beat-chroma features and cross-related the representations to ﬁnd the optimal
match. In [12], the authors adapted the ideas of the Basic Local Alignment Search
Tool (BLAST) to match chroma features. Dynamic Time Warp (DTW), a suc-
cessful algorithm for measuring the similarity among sequences, was applied and
further explored in cover song identiﬁcation [10,25]. Qmax [20], which embedded
chroma vectors into high-dimensional vectors and applied Cross Recurrence Plot
to cover song identiﬁcation, won Mirex Audio Cover Song Identiﬁcation contest
from 2007 to 2009. However, even though these matching approaches obtained
good retrieval accuracy, they were inadequate for large-scale databases due to
high time cost.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 118–128, 2018.
https://doi.org/10.1007/978-3-319-73600-6_11

Eﬃcient Two-Layer Model Towards Cover Song Identiﬁcation
119
Apart from this, with the advent of Million Song Dataset [3], some scholars
attempted to explore some compact and ﬁxed-dimensional features instead of
sequences of feature vectors for cover song identiﬁcation. For example, Serr`a et
al. [18] obtained ﬁxed-dimensional representations of songs through time series
modeling. In [11], the authors exploited Chord Proﬁles of songs to calculate the
distances between diﬀerent songs. [2] extracted 2D Fourier Magnitude (2DFM)
from Beat-chroma and attained ﬁxed low-dimensional representations. [9] pro-
jected 2DFM onto very sparse space and exploited supervised learning for cover
song identiﬁcation. Foster et al. [6] considered an information-theoretic approach
to measure music similarity. After extracting short representations of music,
some simple measures like Euclidean distance could be directly used to compute
music similarity. Since the representation was highly compact in general, it could
be easily applied to large-scale identiﬁcation. However, such simpliﬁcation often
lost tremendous temporal information and thus caused poor performance.
Indeed, it is hard to achieve high precision and high eﬃciency at the same
time. There appears to be a tradeoﬀbetween eﬃciency and precision. In this
paper, we aim at developing a balanced and eﬃcient model for applications. To
achieve this goal, we propose an approach to recognize cover songs by taking
advantage of the two types of methods we have discussed above. Our system is a
two-layer model based on 2DFM [2] and Qmax [20]. The ﬁrst layer uses 2DFM to
generate potential candidates from a database. For the sake of eﬃciency, we uti-
lize KD-Tree and Ball-Tree to accelerate this procedure. Then, Qmax is utilized
to reﬁne the ranking of the candidates in the second layer. Our contributions lie
on two points. First, we empirically explore diﬀerent kinds of chroma features
to improve the performance of Qmax and achieve good performances compared
to existing methods. Second, by accelerating 2DFM with KD-Tree and combin-
ing Qmax and 2DFM, we build an eﬀective system for application, which could
process a query within 8 s on large-scale datasets.
The rest of the paper is structured as follows. In Sect. 2, we introduce the pro-
cedures of our approach. Then, the experimental setting is described in Sect. 3.
In Sect. 4, the results of experiments are presented and discussed. Finally, we
conclude this article.
2
Approach
Our system is a two-layer model, as shown in Fig. 1. 2DFM [2] is used to generate
candidates of a query in the ﬁrst layers. Then, the top k candidates are further
processed by Qmax [20]. Besides, compared to the original implementation of
2DFM, we exploit KD-Tree or Ball-Tree to enhance the eﬃciency. For Qmax,
diﬀerent sequential features are explored to build a more eﬀective reﬁnement
procedure. These improvements are discussed in the following section.

120
X. Xu et al.
Query
2DFM-KDT
Top k candidates
Final ranking
Qmax
First Layer:
Second Layer:
Fig. 1. Framework of two-layer model
Audio
Chroma Sequence
Chroma ExtracƟon
2DFM Sequence
2DFM Feature
2D Fourier TransformaƟon
Median Filtering
12
T
M
T
M
1
Fig. 2. Main procedures of extracting
2DFM feature
2.1
Generating Potential Candidates
2DFM is implemented to roughly measure the distance between the query and
the references in the database. As shown in Fig. 2, this approach ﬁrstly applies
2D Fourier Transform to chroma sequences. The resulting sequence could be
regarded as a matrix with the shape of M × T, where T is the length of the
sequence, and M is the dimension of 2DFM feature. With the property of
Fourier Transform, the transformation brings key invariance to chroma bins,
which avoids further key transposition and thus improves the eﬃciency. Finally,
the median ﬁlter is applied to each row of the matrix, and we obtain a compact
representation of the original recording. Principal component analysis could be
used to extract more compressed features, see [2] for more details. Note that
2DFM may be either referred to the method or the feature. In the rest of this
paper, without explicit indication, 2DFM is referred to the method.
Then, when processing a query, the algorithm uses Euclidean distance to mea-
sure the similarity between the query and the recordings in the database and
returns a ranking list according to the similarity. One considerable improvement
for this method is to adopt Nearest Neighbor Search (NNS) algorithms to compute
the distance. This adaptation is based on two reasons. First, searching the whole
database is indeed demanding and unnecessary. Even if 2DFM cannot retrieve all
cover songs of a query very precisely, it could still return good results to some
extent. Hence, we do not necessarily process all candidates in the second layer.
Instead, we only need to reﬁne some top-ranking candidates. Of course, this might
lose some relevant candidates. We will discuss the tradeoﬀbetween recall and eﬃ-
ciency in the following section. Second, it is easy to adopt eﬀective NNS algo-
rithms to ﬁnd some top-rank candidates with low complexity. Given a database
with N recordings and the dimension of feature M, eﬀective NNS methods return
the result with O(MlogN) compared to O(MN) for brute-force search.

Eﬃcient Two-Layer Model Towards Cover Song Identiﬁcation
121
In our experiments, we utilize two kinds of data structures, KD-Tree and Ball-
Tree, to perform the nearest neighbors search. The two accelerated algorithms
are both precise search methods and denoted as 2DFM-KDT and 2DFM-BT.
When processing a query, the system generates the top k candidates through
these eﬀective search algorithms. Then, a more sophisticated method is used to
reﬁne the ranking of these candidates.
2.2
Reﬁning the Result
Qmax is used to enhance the performance. Basically, Qmax is a dynamic program-
ming algorithm. By embedding chroma vectors into high-dimensional vectors and
applying a well-designed transition function, this approach shows great robust-
ness against temporal variations and even structural changes in cover songs.
Originally, Qmax used Harmonic Pitch Class Proﬁles (HPCP [8]) as a feature.
However, this method could be combined with any sequential representations
actually. It is possible that combined with other features, Qmax could achieve
better performances.
In our experiments, we compare several kinds of chroma features that have
been applied to cover song identiﬁcation. HPCP, ﬁrst used in Qmax [19,20,22],
is a kind of enhanced chroma feature. We follow the parameter setting of [20]
and use essentia1 to extract HPCP features. More speciﬁcally, we resample the
audio with 44100 Hz and map spectral energy within 4096-point windows to
chroma bins with the hop size of 1024. Then, 12-dimensional HPCP vectors are
normalized and downsampled with a factor of 20. Finally, we attain a sequence of
12-dimensional vectors—2.15 vectors per second, each vector spanning roughly
460 ms.
CENS, which is originally developed to deal with audio matching and music
retrieval in [13], has been successfully applied to cover song identiﬁcation recently
[23,24]. In our case, we use Chroma Toolbox [14] to extract CENS with the
default setting except for applying a Hanning window with a length of 21 and
a downsampled factor of 5. Under this setting, we obtain a sequence of 12-
dimensional vectors—2 vectors per second, each vector corresponding to 2100 ms.
Beat-chroma, which represents the intensity of twelve semitones per beat,
was ﬁrstly applied in Mirex 2006 Audio Cover Song Identiﬁcation contest [5].
Then, some later works also used this feature [1,2,16]. In our experiments, we
run the code provided by the authors to extract Beat-chroma under the default
setting.
3
Experimental Setup
3.1
Dataset
Covers80 2 is commonly used by researchers for evaluation in the literature.
It contains 80 songs that are performed by two artists, and thus it has 160
1 https://github.com/MTG/essentia/.
2 https://labrosa.ee.columbia.edu/projects/coversongs/covers80/.

122
X. Xu et al.
recordings in total. Besides, this dataset provides audio ﬁles. In our experiments,
we use this dataset to explore what chroma features combined with Qmax and
2DFM would achieve the best performance.
SHS 3 is a subset of Million Song Dataset [3] for cover song recognition. To
the best of our knowledge, this dataset is the largest database for cover song
identiﬁcation. It contains 18,196 tracks and splits into SHS training set with
12,960 tracks and SHS Test set of 5236 tracks without overlap. In our case, we
do experiments on SHS training set. Besides, this dataset only provides chroma
features instead of raw audio; thus we only extract Beat-chroma on it.
Covers2473 is constructed by our lab, which contains 165 diﬀerent songs
with 2473 recordings. This database is mainly composed of Chinese, Japanese
and Korean pop music. The reason for using this database is that we want to
explore the performance of our system on music with diﬀerent styles and genres
thoroughly since existing databases like Covers80 and SHS only contain western
music but no eastern music.
3.2
Metrics
When evaluating our system, we do not split the data into a training set and
a test set. Instead, for each recording in the dataset, we compute the similarity
between it and the remaining recordings in the dataset. Then, some metrics are
used to evaluate the performance of our system. Many works are based on this
manner [18,21], while some researchers split the dataset into a training set and
a test set [23,24]. As we have discussed above, we aim at establishing a balanced
and eﬃcient system for applications. Obviously, we mainly care about eﬃciency,
recall and precision of the system. In our experiments, average query time T/Q
is used to estimate the eﬃciency of the system. As for precision and recall, we
will discuss them in the following sections.
MAP@k. This metric is essentially very similar to Mean Average Precision
(MAP), a widely used measure in cover song identiﬁcation. However, since we
focus on the retrieved result of k candidates, we merely calculate the precision of
the system within the k candidates. Given a dataset with N recordings, the sys-
tem outputs k candidates for a query Qi. Consider that there are Ck
i cover songs
of Qi within the k candidates, and the ranks of these songs are Rk
i1, Rk
i2 . . . Rk
iCk
i .
Then, MAP@k is deﬁned as follows:
MAP@k = 1
N
N

i=1
Ck
i

j=1
j
Ck
i Rk
ij
(1)
Note that when k is equal to N, MAP@k degrades into MAP. Please pay atten-
tion to distinguish these notations. In the following sections, we will use both
MAP@k and MAP to evaluate the performance of our approach.
3 https://labrosa.ee.columbia.edu/millionsong/secondhand.

Eﬃcient Two-Layer Model Towards Cover Song Identiﬁcation
123
Recall. For investigating how many relevant songs the system could ﬁnd given
a query, it is straightforward to use recall as a measure. Given that the system
returns k candidates, recall is deﬁned as the proportion of the retrieved cover
songs to the cover songs in the database on average.
Recall = 1
N
N

i=1
Ck
i
Ci
(2)
where Ci is the number of cover songs of Qi in the dataset. Obviously, with
the increment of k, this metric goes higher and higher. It is trivial to attain
100% recall by returning all candidates. Thus, this metric alone is not enough
for evaluation. In our experiments, both MAP@k and recall are taken into con-
sideration.
4
Result and Discussion
We design several experiments to verify the validity of our system. Our approach
is mainly implemented in Python and C++. Besides, our experimental environ-
ment is a Dell PowerEdge R730 server with an Intel Xeon E5-2640v3 processor
and 128 GB of memory.
4.1
Exploring Optimal Chroma Feature
First, we explore what chroma features achieve the best performance combined
with Qmax or 2DFM. Parameters of these methods like the embedding length
m (review [21] for more details) are carefully tuned to obtain great recognition.
Table 1 shows the results of diﬀerent combinations of features and methods under
the optimal setting. It seems that CENS shows consistent beneﬁts combined with
Qmax or 2DFM. Based on CENS, 2DFM achieves the highest MAP of 38.26%,
and Qmax combined with CENS achieves a MAP of 67.49% with an improvement
of 1.34% and 1.59% over HPCP and Beat-chroma respectively. Note that it is a
very good result on Covers80. In comparison, some preceding works achieved a
MAP of 62.4% [4], 55% [17] and 52.5% [25].
Table 1. Performance of diﬀerent kinds of chroma features combined with (a) Qmax
and (b) 2DFM on Covers80
(a)
Feature
MAP T/Q(s)
HPCP
0.6615
30.64
CENS
0.6749 25.10
Beat-chroma 0.6590
115.6
(b)
Feature
MAP T/Q(ms)
HPCP
0.3809
7.325
CENS
0.3826
7.115
Beat-chroma 0.3326
7.696

124
X. Xu et al.
As for the reason why CENS surpasses the two features consistently, we think
it is because CENS takes more sophisticated measures to reduce the eﬀect of vari-
ations in cover songs. Even though the length of CENS sequences is the shortest
among the three features as described in Sect. 2.2, in fact each vector of CENS
sequences spans the longest period of time. Under the setting of our experiment,
each vector represents chroma energy within 2.1 s. Additionally, CENS quanti-
ﬁes chroma energy and convolves the vector through frames, which further helps
reduce variations of timbre, dynamics and articulation in music.
Besides, one may note that Qmax based on CENS turns out to be the most
eﬃcient among the methods in Table 1(a). This is because under the parame-
ter setting, the length of CENS sequence is shorter than that of Beat-chroma
and HPCP. Especially, since the length of Beat-chroma sequence is much longer
than that of CENS and HPCP, Qmax based on Beat-chroma is even more time-
consuming. For 2DFM, as the features are transformed into ﬁxed-dimensional
vectors, theoretically it would take the same time for these methods to compute.
The nuances of query time might be due to some random factors during the run-
time. Furthermore, comparing average query time of Qmax with that of 2DFM,
one could intuitively grasp the idea that sequence matching methods like Qmax
work far slower than methods exploiting low-dimensional features like 2DFM,
even though the former could achieve much higher performance.
4.2
Accelerating 2DFM
Then, we design experiments to tune parameters of KD-Tree and Ball-Tree
to accelerate 2DFM method. For these data structures, the eﬃciency mainly
depends on leaf size parameter, which controls the number of samples at which a
query switches to brute-force search. In our experiments, we iterate this param-
eter through {10, 50, 100, 300, 500, 600, 700, 800, 900, 1000} to ﬁnd the optimal
setting. Besides, the brute-force search is provided as a baseline. We use scikit-
learn [15] to implement KD-Tree and Ball-Tree. The result is displayed in Fig. 3.
0
200
400
600
800
1000
leaf size
0
5
10
15
20
T/Q (ms)
2DFM-KDT
2DFM-BT
Brute-force
(a)
0
200
400
600
800
1000
leaf size
0
20
40
60
80
100
T/Q (ms)
2DFM-KDT
2DFM-BT
Brute-force
(b)
Fig. 3. Comparision of eﬃciency among NNS algorithms and brute-force search:
(a) Covers2473 and (b) SHS training set

Eﬃcient Two-Layer Model Towards Cover Song Identiﬁcation
125
Without surprising, KD-Tree and Ball-Tree run faster than exhaustive search
whatever the leaf size is. Especially, 2DFM-KDT and 2DFM-BT run about 5
times as fast as 2DFM under the optimal parameter. Indeed, such improvement
is promising for a practical retrieval system. In addition, we observe that when
the leaf size ranges from 500 to 1000, average query time is approximately one
ﬁfth of that of the exhaustive search. It indicates that it would be easy to tune the
parameter in practice because these methods achieve comparably high eﬃciency
within a broad range of parameter.
4.3
Analyzing Recall
Then, we conduct experiments to explore how the number of candidates k inﬂu-
ences the recall of the system. As shown in Fig. 4, the curve rises up drastically
with the increasing number of candidates in the beginning. It shows that given
a small k, the system could actually have a relatively high recall. For instance,
given k = 50, the two-layer model achieves a recall of 32% on Covers2473 and a
recall of 17% on SHS training set. Note that in this situation, only 2% of record-
ings in Covers2473 and 0.3% of recordings in SHS training set are taken to the
second layer. Since the eﬃciency of the system is mainly subject to the second
layer, a small k could lead to a relatively high recall and eﬃciency. Indeed, this
result is beneﬁcial to practical uses.
0
500
1000
1500
2000
2500
Number of candidates
0
20
40
60
80
100
Recall (%)
(a)
0
5000
10000
15000
Number of candidates
0
20
40
60
80
100
Recall (%)
(b)
Fig. 4. Recall of the ﬁrst layer on (a) Covers2473 and (b) SHS training set
In addition, one might note that when the recall is higher than 60%, the
curve goes up slowly. Under the circumstance, the system has to generate as
many candidates as possible in order to achieve a high recall. For example, the
system needs to generate 11000 candidates, 84% of the recordings in SHS train-
ing set, to achieve a recall of 96%. Undoubtedly, dealing with these candidates
in the second layer would waste tremendous time. Hence, to build a balanced
and eﬃcient system, we need to ﬁnd a tradeoﬀbetween eﬃciency and recall.

126
X. Xu et al.
One could carefully adjust the number of candidates to meet their demands
according to Fig. 4. In our ﬁnal system, only 50 candidates are generated through
the ﬁrst layer, and then Qmax is used to reﬁne the ranking.
4.4
Exploring Two-Layer Model
We use Qmax to reﬁne the ranking after generating candidates through 2DFM-
KDT. The comparison among Qmax, 2DFM-KDT and the two-layer model is
shown in Table 2. Noticeably, with the help of Qmax, the two-layer model achieves
a MAP@50 of 0.9190 on Covers2473 and a MAP@50 of 0.2514 on SHS training
set, which signiﬁcantly improves the result of 2DFM-KDT.
Table 2. Result of two-layer model on (a) Covers2473 and (b) SHS training set (note
that we do not report the result of Qmax on SHS training set, as it would take roughly
198.1 days to ﬁnish the calculation)
(a)
Method
MAP@50
T/Q
Qmax
0.8260
4.201min
2DFM-KDT
0.4493
2.103ms
two-layer
0.9190
8.372s
(b)
Method
MAP@50
T/Q
Qmax
-
-
2DFM-KDT
0.1281
12.08ms
two-layer
0.2514
7.997s
In addition, it is surprising to note that the two-layer model even outper-
forms Qmax in terms of MAP@50 with shorter running time. We think that the
improvement is due to the combination of Qmax and 2DFM. With the help of
2DFM, some irrelevant songs that Qmax cannot correctly rank might be ﬁltered
out in the ﬁrst layer. Hence, the combination of two methods helps improve the
performance of Qmax, and the reranking process returns very good results.
With respect to time complexity, average query time of the two-layer model
is longer than that of 2DFM-KDT, but we still think it acceptable in practice.
By carefully implementing the system and applying programming optimizations,
it is possible that query time could be reduced to several seconds, which might
be suitable for applications.
5
Conclusion
We have presented a highly eﬃcient retrieval system for cover song identiﬁcation.
By exploring the optimal chroma feature for existing successful methods, our
method outperforms preceding approaches on Covers80 and achieves a MAP of
67.49%. In addition, through exploiting NNS algorithms, the proposed system
achieves great performances on an in-house database and a public database.
For each query, the system spends about 8 s on retrieval. This system would be
suitable for cover song identiﬁcation system aiming at practical use.

Eﬃcient Two-Layer Model Towards Cover Song Identiﬁcation
127
Acknowledgments. This work was supported by the Natural Science Foundation of
China (No. 61370116).
References
1. Bertin-Mahieux, T., Ellis, D.P.W.: Large-scale cover song recognition using hashed
chroma landmarks. In: IEEE Workshop on Applications of Signal Processing to
Audio and Acoustics, pp. 117–120 (2011)
2. Bertin-Mahieux, T., Ellis, D.P.: Large-scale cover song recognition using the 2D
Fourier transform magnitude. In: International Society for Music Information
Retrieval Conference (2012)
3. Bertin-Mahieux, T., Ellis, D.P., Whitman, B., Lamere, P.: The million song
dataset. In: International Society for Music Information Retrieval Conference
(2011)
4. Chen, N., Li, W., Xiao, H.: Fusing similarity functions for cover song identiﬁcation.
Multimed. Tools Appl., 1–24 (2017)
5. Ellis, D.P., Poliner, G.E.: Identifying cover songs with chroma features and dynamic
programming beat tracking. In: IEEE International Conference on Acoustics,
Speech and Signal Processing (2007)
6. Foster, P., Dixon, S., Klapuri, A.: Identifying cover songs using information-
theoretic measures of similarity. IEEE/ACM Trans. Audio Speech Lang. Process.
23(6), 993–1005 (2015)
7. Fujishima, T.: Realtime chord recognition of musical sound: a system using com-
mon Lisp music. In: ICMC, pp. 464–467 (1999)
8. G´omez, E.: Tonal description of polyphonic audio for music content processing.
INFORMS J. Comput. 18, 294–304 (2006)
9. Humphrey, E.J., Nieto, O., Bello, J.P.: Data driven and discriminative projections
for large-scale cover song identiﬁcation. In: International Society for Music Infor-
mation Retrieval Conference, pp. 149–154 (2013)
10. Julia, J.S.: Music similarity based on sequences of descriptors: tonal features
applied to audio cover song identiﬁcation. Master’s thesis (2007)
11. Khadkevich, M., Omologo, M.: Large-scale cover song identiﬁcation using chord
proﬁles. In: International Society for Music Information Retrieval Conference, pp.
233–238 (2013)
12. Martin, B., Brown, D.G., Hanna, P., Ferraro, P.: Blast for audio sequences align-
ment: a fast scalable cover identiﬁcation. In: International Society for Music Infor-
mation Retrieval Conference (2012)
13. M¨uller, M.: Information Retrieval for Music and Motion. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-74048-3
14. M¨uller, M., Ewert, S.: Chroma Toolbox: MATLAB implementations for extract-
ing variants of chroma-based audio features. In: International Society for Music
Information Retrieval Conference, Miami, USA (2011)
15. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: machine
learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011)
16. Ravuri, S., Ellis, D.P.: Cover song detection: from high scores to general classiﬁca-
tion. In: IEEE International Conference on Acoustics, Speech and Signal Process-
ing, pp. 65–68 (2010)

128
X. Xu et al.
17. Seetharaman, P., Raﬁi, Z.: Cover song identiﬁcation with 2D Fourier transform
sequences. In: IEEE International Conference on Acoustics, Speech and Signal
Processing (2017)
18. Serr`a, J., Kantz, H., Serra, X.: Predictability of music descriptor time series and
its application to cover song detection. IEEE Trans. Audio Speech Lang. Process.
20(2), 514–525 (2012)
19. Serr`a, J., Serra, X., Andrzejak, R.G.: Cross recurrence quantiﬁcation for cover song
identiﬁcation. New J. Phys. 11(9), 093017 (2009)
20. Serr`a, J.: Identiﬁcation of versions of the same musical composition by processing
audio descriptions. Ph.D. thesis (2011)
21. Serr`a, J., G´omez, E., Herrera, P.: Audio cover song identiﬁcation and similarity:
background, approaches, evaluation, and beyond. In: Ra´s, Z.W., Wieczorkowska,
A.A. (eds.) Advances in Music Information Retrieval. SCI. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-11674-2 14
22. Serr`a, J., G´omez, E., Herrera, P., Serra, X.: Chroma binary similarity and local
alignment applied to cover song identiﬁcation. IEEE Trans. Audio Speech Lang.
Process. 16(6), 1138–1151 (2008)
23. Silva, D.F., Yeh, C.C.M., Batista, G.E.A.P.A., Keogh, E., et al.: SiMPle: assess-
ing music similarity using subsequences joins. In: International Society for Music
Information Retrieval Conference (2016)
24. Silva, D.F., Souza, V.M.A.D., Batista, G.E.A.P.A., et al.: Music shapelets for fast
cover song regognition. In: International Society for Music Information Retrieval
Conference (2015)
25. Tralie, C., Paul, B.: Cover song identiﬁcation with timbral shape sequences. In:
16th International Society for Music Information Retrieval Conference (2015)

Food Photo Recognition for Dietary Tracking:
System and Experiment
Zhao-Yan Ming1(B), Jingjing Chen2, Yu Cao1, Ciar´an Forde3,4,
Chong-Wah Ngo2, and Tat Seng Chua1
1 School of Computing, National University of Singapore, Singapore, Singapore
mingzhaoyan@gmail.com, caoyu.cy@u.nus.edu, chuats@comp.nus.edu.sg
2 Department of Computer Science, City University of Hong Kong,
Kowloon Tong, Hong Kong
chenjingjing.tju@gmail.com, cscwngo@cityu.edu.hk
3 Physiology, National University of Singapore, Singapore, Singapore
phscgf@u.nus.edu
4 Singapore Institute for Clinical Sciences, Singapore, Singapore
Abstract. Tracking dietary intake is an important task for health man-
agement especially for chronic diseases such as obesity, diabetes, and car-
diovascular diseases. Given the popularity of personal hand-held devices,
mobile applications provide a promising low-cost solution to tackle the
key risk factor by diet monitoring. In this work, we propose a photo based
dietary tracking system that employs deep-based image recognition algo-
rithms to recognize food and analyze nutrition. The system is beneﬁcial
for patients to manage their dietary and nutrition intake, and for the
medical institutions to intervene and treat the chronic diseases. To the
best of our knowledge, there are no popular applications in the market
that provide a high-performance food photo recognition like ours, which
is more convenient and intuitive to enter food than textual typing. We
conducted experiments on evaluating the recognition accuracy on labo-
ratory data and real user data on Singapore local food, which shed light
on uplifting lab trained image recognition models in real applications.
In addition, we have conducted user study to verify that our proposed
method has the potential to foster higher user engagement rate as com-
pared to existing apps based dietary tracking approaches.
Keywords: Food image recognition · Dietary app · User food photo
1
Introduction
Chronic diseases such as diabetes, obesity, and cardiovascular diseases are becom-
ing the dominant sources of mortality and morbidity worldwide and recently an
epidemic in many Asia Paciﬁc countries [4,25]. Unhealthy diet is one of the key
common modiﬁable risk factors in preventing and managing chronic diseases [28].
Personalized dietary intake intervention showed signiﬁcant impact on inﬂuencing
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 129–141, 2018.
https://doi.org/10.1007/978-3-319-73600-6_12

130
Z.-Y. Ming et al.
people’s choice and promoting their health [4]. The feedback on nutrition intake
is substantial and behavioral changing when patients track their dietary intake
for a considerable length of time. However, the burden of logging food makes
compliance a challenge. Clinical studies rely on patients to recall dietary intake,
which is time-consuming and prone to underestimation [18]. On the other hand,
the ubiquitous usage of mobile devices makes it possible for one to track the
dietary intake on personal devices. Indeed there are thousands of applications
available on food logging and calorie counting. According to [3], people who self-
track diet and lifestyle with mobile devices have experienced “strong behavior
change”.
The convenience of food entering methods plays a key role in the usability of
dietary tracking applications [14]. Existing methods include food database search
and free text diary, both requiring typing. Bar code scanning is eﬃcient but
limited to packaged food. The recent advancement in computer vision and deep
learning makes photo based dietary tracking possible through automatic food
image recognition [7,9,22,24]. Photo based dietary tracking is more intuitive,
more faithful, and easier to perform than text based approaches [19]. To perform
subsequent nutrition value analysis, the recognized food category is used to look
up nutrition databases [24]. When a food type is not covered, nutrition can still
be estimated by ingredient recognition [9].
Despite the enormous success of image recognition methods in recent years,
their performance in a real dietary tracking system is relatively less explored.
In this work, we propose to use photo taking and food image recognition as the
dietary entering method. The following questions need to be answered:
1. Can the photo based dietary tracking method improve the eﬃciency and user
satisfaction of food entering?
2. Does lab trained image recognition model suﬃce when applied in a real
dietary tracking system?
3. In case there is substantially diﬀerent in performance between the lab and the
real usage settings, what are the underlying reasons? What are the actions
to improve the real usage performance?
In this work, we built a photo based dietary tracking system named DietLens
on mobile devices. We ﬁrst experimented a deep-based food image recognition
model in the lab on Singapore food, which has one of the most diverse food
environment sharing inﬂuences from Chinese, Malay, Indian and Western cul-
tures [18]. We then applied the recognition model in our prototype system, the
DietLens. Through the prototype system, we tested the model on real user food
images. Moreover, we conducted user study that compares the usability of sev-
eral popular food tracking applications and our prototype system with focus on
the food entering methods. The contributions of this paper are three folds:
– We have developed a dietary tracking mobile application, which has been
shown to have higher usability than some of the popular applications.
– We experimented food image recognition on laboratory data and real user
data, which sheds light on approaches to improve lab trained image recogni-
tion models on food recognition.

Food Photo Recognition for Dietary Tracking
131
– Within the photo based tracking methods, we introduced a novel photo-based
portion selection method, which has been veriﬁed scientiﬁcally in medical
research and demonstrated empirically to be better than existing approaches.
2
Background and Literature Review
2.1
Deep-Based Food Recognition
Food recognition for health-oriented applications has started to capture more
research attention in recent years. Existing works include simultaneous estima-
tion of food categories and calories [10], multi-food recognition [23], context-
based recognition by GPS and restaurant menus [6], multi-modal food cate-
gorization [15], and multi-task learning of food and ingredient recognition [9].
Owing to the success of deep learning technologies in image classiﬁcation, most
recent works employ deep features extracted directly from neural networks for
image-level food categorization [16,29]. For example, [16] extract features from
AlexNet [17], and then utilize SVM for food classiﬁcation. The DCNN features
perform signiﬁcantly better than hand-crafted features. Similar conclusion is also
reported by [7,9] on Food-101 and VIREO Food-172 data set respectively.
Diﬀerent deep architectures have also been exploited for food recognition
[9,12,20,21]. [9] modiﬁes VGG for multi-task food and ingredient recognition;
[12,20] exploit and revise inception modules while [21] modiﬁes Residual Network
(ResNet) [13] for food recognition. As reported in [9,21], deeper networks such
as VGG, GoogleNet and ResNet tend to achieve better recognition performance
than AlexNet.
2.2
Dietary Tracking Mobile Applications
There are many food tracking mobile applications, e.g., Foodlog [5,27], MyFit-
nessPal and FatSecret. To understand the available dietary tracking methods,
we surveyed a number of food tracking mobile applications sorted by their pop-
ularity as shown in Table 1. We identify that food entering and portion size
speciﬁcation are two key steps in dietary tracking.
Table 1. Comparison of popular dietary tracking mobile applications. Note that the
number of downloads is taken from Android Play Store because iOS App Store does
not provide this number.
App name
Record
by
photo
Food
photo
recognition
Record
by
search
Record by
barcode
scan
Record by
free text
Rating
out of 5
# rating
Launch
time
# download
MyFitnessPal N
NA
Y
Y
Y
4.6
1,000,000 2005
50,000,000
FatSecret
Y
N
Y
Y
Y
4.4
193,000 2007
10,000,000
Noom Coach N
NA
Y
Y
Y
4.3
166,000 2010
10,000,000
Lose It!
Y
Y
Y
Y
Y
4.4
55,000 2011
5,000,000
Sparkpeople
N
N A
Y
Y
Y
4.4
22,000 2012
1,000,000
MyNetDiary
N
NA
Y
Y
Y
4.5
18,000 2010
1,000,000
MyPlate
N
NA
Y
Y
Y
4.6
16,000 2010
1,000,000

132
Z.-Y. Ming et al.
Photo Based Food Entering: There are generally four ways of entering a food
item into one’s food log: search a textual database, scan bar code of packaged
food, type free text of food name and nutrition value, and take a food photo.
The photo recording approach has the advantages of easy to use, high cov-
erage, and promising accuracy. However, according to our study, food photo
recording is not adopted in most of the food apps that are intended for nutri-
tion analysis and health management. Among a few apps that use photos as a
way to record food, most of them use the photos for social networking purpose
(such as SnapDish) or a means to communicate with nutritionists to get manual
feedbacks (See How You Eat Diary/Coach).
To the best of our knowledge, Lose It! is the only popular app that has a
food image recognition function (SnapIt) among the food apps that have more
than 1 million downloads in the Play Store. Compared to our system, Lose It!
covers 100 types of generic food. Therefore it requires an additional step to select
the ﬁne-grained food category after the image recognition.
Using a food recognition engine that covers 100 types of speciﬁc food, a study
based on Nibble [19] found that low accuracy in food image recognition made
users revert to search based entering method. In our system, we cover 250 types
of food and the number is increasing. We believe that the coverage should be
targeted alongside the accuracy, or the latter will suﬀer too.
Portion Size Recording Methods: Determining the portion size is another
key step in dietary tracking. The amount of nutrition in the food has to be
analyzed based on the size of the dish. The existing approaches to size recording
are done in two ways. The ﬁrst is to select the size along the food name. For
example, one can choose from “avocado (one fruit)”, “avocado (half)”, “avocado
(a cup)”, “avocado (50 g)” when avocado is consumed. In this approach, the
portion is part of the identiﬁer in the food knowledge database.
The second is to choose or type a number to specify the weight, the number of
servings, or the volume. The problem here is that the units may not be perceived
correctly, for example, some users may not know what a 100 g apple looks like.
Another problem is that the deﬁnitions of some size speciﬁers are not common
knowledge for some people. For example, “a serving” is deﬁned by nutritionists
and it is distinct for diﬀerent types of food that normal users usually do not
know. In Southeast Asia context, a bowl of rice is a standard serving unit, but it
is counted as around two servings of carbohydrate. Noom Coach tries to educate
the users by using reference items to provide portion guidelines, such as one’s
ﬁst is around the size of a cup and the thumb is the size of a table spoon.
Portion selection needs to be contextualized as the measuring systems may
be diﬀerent in diﬀerent social-geometric settings. In this work, we complete the
photo based entering method by proposing a portion photo based size speciﬁca-
tion method, which is more intuitive and applicable in various context.

Food Photo Recognition for Dietary Tracking
133
Fig. 1. Overview of system
3
DietLens: A Mobile App for Dietary Tracking
and Management
DietLens is a prototype system that we developed for the dietary tracking and
analysis application. As shown in Fig. 1, the app links users, doctors, and social
media to form a computational wellness ecosystem. For users, the app will cap-
ture diet and physical activities. It may also include medical data such as chronic
health conditions to complete the wellness proﬁle of the users. The app will pro-
vide real time feedback based on users’ proﬁle and input. The knowledge base
forms the basis of automatic feedback that the app pushes to the users. The users
and doctor can interact through the app. The doctors will be able to manage all
the users in a centralized manner. The users will also be able to interact with
their friends on social media.
Figure 2 shows the main pages of the food photo taking function and the
recognition result. The user is prompted to a list of candidate food names if the
ﬁrst one is incorrect. The back-end deep-based image recognition model can be
deployed in the cloud or the mobile phone itself thanks to the development of
the state-of-art deep learning algorithm [2].
In addition to the weight, serving, volume based portion selection method,
we propose a reference food image based method, as shown in Fig. 3. The users
need only to choose from a series of pre-compiled varying portion size photos of
the food that is of the same type as the target food. The portion size photo is
coded with the amount of nutrition [8,11], for example, the calories. It has been
shown in [8,11] that image based portion selection maps well with the actual
food intake.
Photo based portion selection requires no prior knowledge of the context: the
communication is performed by comparing the standard portion size photos and
the actual food. The challenge is two fold: the ﬁrst is to pre-compile the portion
size photos that have high coverage of the types of target food; the second is

134
Z.-Y. Ming et al.
Fig. 2. Food image recognition in DietLens
Fig. 3. Portion selection by example food photos.
to map the actual food to a similar type of food with the portion size photos.
Currently, our system covers about 60 types of food using the sliding bar portion
selection method.
With the food properly logged, the nutrition databases are looked up to
provide the nutrition facts. A clinically veriﬁed nutrition information database
is essential. To ensure the quality and coverage of nutrition information, we
resort to the following databases. The ﬁrst is from a local nutrition research
institute where popular local food nutrition is studied [18]. When the food item
is not covered by the ﬁrst, we check the Singapore Health Promotion Board [26]
database of energy and nutrition composition of food. For general food items,
we also use FatSecret API [1] as a reference.
4
Food Image Recognition Performance
4.1
Food Image Recognition Data and Method
We construct a large food data set speciﬁcally for Singapore hawker food. The
data set is crawled from Google by using the food name as query. The data set
contains 249 food categories, which covers most Singapore hawker food, including
Chinese food, Western food, Indian food, Malay food, snacks, fruits, desserts

Food Photo Recognition for Dietary Tracking
135
and beverage etc. Apart from fruit images, which are obtained from ImageNet
data set, most of the images in the data set were crawled from Google image
search. For each category, the name was issued as keywords to search engines.
We manually checked up to 1,300 crawled images in each category. We exclude
images with resolution lower than 256 × 256 or suﬀer from blurring, images with
more than one dishes, and the false positives. After this process, each category
has around 300 clean images on average. In total, the data set contains 87,470
images. In each food category, 80% of images are randomly picked for training,
while 10% for validation and the remaining 10% for testing. For performance
evaluation, the average top-1 and top-5 accuracies are adopted.
As ResNet is the state-of-the art deep model for image classiﬁcation, we adopt
deep residual network (ResNet-50) [13] for food recognition. ResNet-50 contains
50 convolutional layers and 1 fully connected layers. We ﬁne-tune the ResNet-
50 on our data set. The network is trained using stochastic gradient descent
with momentum set as 0.9 and the initial learning rate as 0.01. The size of
mini-batch is 25 and the learning rate decays after every 8000 iterations. During
the training process, we adopt data augmentation by horizontal mirroring and
random cropping of input images. To prevent over-ﬁtting, dropout is used. To
evaluate the recognition performance, we report top-1 and top-5 accuracy.
4.2
Experimental Results and Analysis
Table 2 shows the recognition performances of diﬀerent food groups. Compared to
western food, the performances on Chinese food are much lower. This is mainly
due to the facts that Chinese food have diverse appearances of dishes and wild
composition of ingredients, which have been discussed in [9]. From the results,
Malay food recognition is also challenging, because the food are always covered
by sauces, like “sambal” and “satay sauce”. As the western food, cafeteria food,
desserts and snacks are usually cooked in standard style, their performance is rel-
atively higher. We also have an additional classiﬁer for non-food images. For non-
food image classiﬁcation, the top-1 accuracy is 0.682 and top-5 accuracy is 0.861.
The non-food classiﬁer is trained on the residual images of the food categories,
which are removed from the food categories and are not food. This training set is
not comprehensive as it does not cover many of that non-food types.
Table 2. Recognition performances of diﬀerent food groups
Groups
#Num Top-1 Top-5 Groups
#Num Top-1 Top-5
Chinese
78
0.642 0.879 Desserts 15
0.794 0.931
Fruit
61
0.658 0.898 Snack
10
0.769 0.954
Western 25
0.786 0.928 Indian
7
0.777 0.921
Japanese 6
0.683 0.912 Malay
40
0.611 0.889
Thai
1
0.700 0.980 Non-food -
0.682 0.861
Beverage 5
0.932 0.984 All
250
0.681 0.899

136
Z.-Y. Ming et al.
Among the 249 food categories, 29 are labeled by our annotators as popular
dishes, including “bak chor mee”,“black pepper crab” and “cereal prawns” etc.
Table 3 presents the recognition performance on these popular food types. The
average recognition accuracy are 0.752 on Top-1 and 0.931 on Top-5, which are
higher than the overall averaged performances reported on Table 2.
Table 3. Recognition accuracy on 29 popular local food types.
#Num Top-1 Top-5
Popular food 29
0.752
0.931
The current performance on all the food and the popular food is considered
satisfactory, as it is convenient to show 5 candidates as the recognition results
in the mobile app. A top-5 accuracy of 0.95 and above is desirable. To further
improve the performance, more training data is required. In our current model,
some categories such as Chinese food have fewer training examples and hence
suﬀer from lower top-5 performance.
4.3
Real User Photo Performance
As our experimental data was crawled from Google, the performance on real user
photo may be diﬀerent. To quantify the diﬀerence, we released the prototype
system to the research lab which collects 100 photos taken at meal time. The
accuracy on the recognized photos is 0.53 on Top-1 and 0.71 on Top-5. About
25% of user-uploaded photos are not in our food type list, which is mainly
homemade food.
A set of 100 photos is too small to get conclusive insight. We thus turn to
Instagram to ﬁnd real user photos for testing. We crawled images from Instagram
by using the food name as the queries, and manually selected those from real
eating scenarios. In total, we collected 4,697 images in 94 food categories with
more than 50 positive samples each. We used these categories as testing data.
The comparison of performance on the laboratory test set and the real user test
set is shown in Table 4. From the results, the performance on real user data is
around 20% lower than that on our data set in terms of top-1 accuracy.
Table 4. Recognition accuracy on laboratory test photos and real user photos.
Num
(test image)
Macro
Micro
Top-1 Top-5 Top-1 Top-5
Laboratory test photos 16,349
0.681
0.899
0.703
0.910
Real user photos
4,697
0.476
0.698
0.488
0.708

Food Photo Recognition for Dietary Tracking
137
A few categories that have large performance gap between our data set and
Instagram are listed in Table 5. These categories include “Nasi padang”, “Bread-
fruit” and “Fish bee hoon” etc. The performance gap is mainly caused by two
reasons. Firstly, the photo uploaded to Instagram may contain multiple dishes,
while the images in our data set only contain one dish. Examples include “nasi
padang” and “yong tau foo” in Fig. 4. Secondly, some dishes in Instagram are
composed of ingredients diﬀerent from the dishes in our data set. There dishes
maybe prepared at home, like “granola” and “satay chicken” in Fig. 4.
The multi-dishes and home-made dishes are the main reasons for degraded
recognition performance. In a way, it shows that food image recognition at food
type level cannot capture the variation in ingredient composition. To address
this issue, the addition of ingredient-level recognition is a promising approach
to tackle the food recognition problem.
Nasi padang
Yong taufoo
Granola
Satay chicken
Instagram
Our dataset
Fig. 4. Example images from Instagram and our data set.
Table 5. The recognition performances of 10 categories that have large performance
gap between our data set and Instagram images.
Top-1
Top-5
Instagram laboratory Instagram laboratory
Nasi padang
0.30
0.42
0.67
0.75
Breadfruit
0.19
0.82
0.26
0.97
Satay chicken
0.21
0.58
0.72
0.95
Yong tau foo
0.22
0.53
0.45
0.76
Fish been hoon 0.24
0.48
0.76
0.87
Top-1
Top-5
Instagram laboratory Instagram laboratory
Lemon
0.25
0.68
0.38
0.95
Apricot
0.12
0.57
0.25
0.93
Lamb chop 0.07
0.82
0.13
0.93
Mango
0.23
0.56
0.45
0.86
Granola
0.27
0.96
0.55
0.99
5
App Usability Study
To get user feedback on the usability of the food entering method, we conducted a
user study involving 15 participants, including some from the labs of the authors’
institute and some from people working in a local hospital. We assembled a list
of 30 popular local food which the participants can choose as a meal that they

138
Z.-Y. Ming et al.
will eat. Each participant chose to log ﬁve meals using each of the six mobile
applications including ours as listed in Table 1. The food photos are hidden from
the participants until they requested. This is to avoid bias towards photo based
entering method.
The food logging operations of all the participants on the provided mobile
phones are recorded by a screen recorder software. We checked the videos manu-
ally about the duration of operations that we are interested in and the correctness
of the logged information. In total, there are 174 valid loggings after removing
incomplete logging and logging by history items. The comparison is summarized
in Table 6.
Table 6. Usability study on popular dietary tracking applications and our prototype
system. The applications statistics can be found in Table 1.
App name
# items
Entering methods
ratio
Avg. time on
entering one
meal (s)
Accuracy of
logged food
type
Accuracy of
logged portion
size
Percentage of
logging default
portion size
DietLens
45
Photo (45)
11.58
98%
85%
56%
MyFitnessPal 36
Search/barcode
(35/1)
17.72
100%
78%
75%
Lose It!
23
Search/photo
(19/4)
16.87
96%
63%
65%
Fatsecret
32
Search/photo/
barcode (29/2/1)
20.75
91%
57%
78%
Spark people 28
Search (28)
17.25
100%
38%
71%
Noom Coach
10
Search/barcode
(9/1)
13.69
90%
57%
30%
In terms of food entering method, DietLens is superior in both speed and
accuracy. It takes an average of 11.58 s for DietLen users to log one meal. This
time is close to the speed of logging using bar code scanning based on individual
records. Among the other applications, the higher the ratio of photo logging,
the faster the entering speed. FatSecret is the slowest among all the apps, as
it provides photo logging but no image recognition. Another reason is that the
response speed of text search in FatSecret is not as good as the other applications.
For portion size selection, DietLens photo based approach is the most accu-
rate as shown in Table 6. DietLens is the only app that provides a sample portion
image approach. It is notable that DietLens users tend to log the portion size
rather than to skip this step (the default size is logged in such case) as com-
pared to the other applications. A subjective questionnaire which is not reported
here also indicates that users prefer the example photo based portion selection
method most.
In summary, the user study shows that DietLens’ photo based dietary track-
ing method, including the automatic food recognition and the example photo
based portion selection, provides a convenient and accurate approach for food
logging and size speciﬁcation.

Food Photo Recognition for Dietary Tracking
139
6
Conclusions
Dietary tracking is an essential task in chronic disease management and interven-
tion. Food photo taking and image recognition signiﬁcantly reduce the burden
of food entering on personal mobile devices. In this work, we have developed a
dietary tracking system that applies the deep-based image recognition to accu-
rately and eﬃciently log food and nutrition intake. Through real user food photo
testing and user study, we found that laboratory models form the foundation of
the solution but miss out some of the key challenges. The diversity of real food
photos is higher than the lab trained model. An ingredient based recognition
is a promising way of tracking the free style and homemade food recognition
problems in which training data is sparse and not representative. Moreover, the
proposed photo based portion selection method is shown to be more accurate
and engages the users better than the existing methods.
Acknowledgments. This research is part of NExT++ project, supported by
the National Research Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@Singapore Funding Initiative.
References
1. Fatsecret platform api. https://platform.fatsecret.com/api/
2. Integrating caﬀe2 on ios/android. https://caﬀe2.ai/docs/mobile-integration.html
3. Every day big data statistics 2.5 quintillion bytes of data created daily (2015).
http://www.vcloudnews.com/every-day-big-data-statistics-2-5-quintillion-bytes-
of-data-created-daily/
4. Singapore Health Fact: Disease Burden. Principal Causes of Death, Ministry of
Health Singapore (2016)
5. Aizawa, K., Ogawa, M.: FoodLog: multimedia tool for healthcare applications.
IEEE Multimed. 22(2), 4–8 (2015)
6. Bettadapura, V., Thomaz, E., Parnami, A., Abowd, G.D., Essa, I.: Leveraging
context to support automated food recognition in restaurants. In: 2015 IEEE Win-
ter Conference on Applications of Computer Vision (WACV), pp. 580–587. IEEE
(2015)
7. Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 – mining discriminative com-
ponents with random forests. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.
(eds.) ECCV 2014. LNCS, vol. 8694, pp. 446–461. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-10599-4 29
8. Brunstrom, J.M.: Mind over platter: pre-meal planning and the control of meal
size in humans. Int. J. Obes. 38(Suppl 1), S9 (2014)
9. Chen, J., Ngo, C.-W.: Deep-based ingredient recognition for cooking recipe
retrieval. In: Proceedings of the 2016 ACM on Multimedia Conference, pp. 32–
41. ACM (2016)
10. Ege, T., Yanai, K.: Simultaneous estimation of food categories and calories with
multi-task CNN. In: 2017 Fifteenth IAPR International Conference on Machine
Vision Applications (MVA), pp. 198–201. IEEE (2017)
11. Forde, C.G., Almiron-Roig, E., Brunstrom, J.M.: Expected satiety: application to
weight management and understanding energy selection in humans. Curr. Obes.
Rep. 4(1), 131–140 (2015)

140
Z.-Y. Ming et al.
12. Hassannejad, H., Matrella, G., Ciampolini, P., De Munari, I., Mordonini, M.,
Cagnoni, S.: Food image recognition using very deep convolutional networks. In:
Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary
Management, pp. 41–49. ACM (2016)
13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
14. Hingle, M., Patrick, H.: There are thousands of apps for that: navigating mobile
technology for nutrition education and behavior. J. Nutr. Educ. Behav. 48(3),
213.e1–218.e1 (2016)
15. Hoashi, H., Joutou, T., Yanai, K.: Image recognition of 85 food categories by
feature fusion. In: 2010 IEEE International Symposium on Multimedia (ISM), pp.
296–301. IEEE (2010)
16. Kawano, Y., Yanai, K.: Food image recognition with deep convolutional features.
In: Proceedings of the 2014 ACM International Joint Conference on Pervasive and
Ubiquitous Computing: Adjunct Publication, pp. 589–593. ACM (2014)
17. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems, pp. 1097–1105 (2012)
18. Lau, E., Goh, H.J., Quek, R., Lim, S.W., Henry, J.: Rapid estimation of the energy
content of composite foods: the application of the calorie answer. Asia Pac. J. Clin.
Nutr. 25(1), 18–25 (2016)
19. Lim, B.Y., Chng, X., Zhao, S.: Trade-oﬀbetween automation and accuracy in
mobile photo recognition food logging. In: Proceedings of the Fifth International
Symposium of Chinese CHI 2017, pp. 53–59. ACM, New York (2017)
20. Liu, C., Cao, Y., Luo, Y., Chen, G., Vokkarane, V., Ma, Y.: DeepFood: deep
learning-based food image recognition for computer-aided dietary assessment. In:
Chang, C.K., Chiari, L., Cao, Y., Jin, H., Mokhtari, M., Aloulou, H. (eds.) ICOST
2016. LNCS, vol. 9677, pp. 37–48. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-39601-9 4
21. Martinel, N., Foresti, G.L., Micheloni, C.: Wide-slice residual networks for food
recognition. arXiv preprint arXiv:1612.06543 (2016)
22. Matsuda, Y., Hoashi, H., Yanai, K.: Recognition of multiple-food images by detect-
ing candidate regions. In: 2012 IEEE International Conference on Multimedia and
Expo, pp. 25–30. IEEE, July 2012
23. Matsuda, Y., Yanai, K.: Multiple-food recognition considering co-occurrence
employing manifold ranking. In: 2012 21st International Conference on Pattern
Recognition (ICPR), pp. 2017–2020. IEEE (2012)
24. Myers, A., Johnston, N., Rathod, V., Korattikara, A., Gorban, A., Silberman, N.,
Guadarrama, S., Papandreou, G., Huang, J., Murphy, K.: Im2Calories: towards an
automated mobile vision food diary. In: 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 1233–1241. IEEE, December 2015
25. World Health Organization: preventing chronic diseases: a vital investment. World
Health Organization (2005)
26. Health Promotion Board Singapore: energy and nutrition composition of food.
http://focos.hpb.gov.sg/eservices/ENCF/. Accessed 18 July 2017
27. Waki, K., Aizawa, K., Kato, S., Fujita, H., Lee, H., Kobayashi, H., Ogawa, M.,
Mouri, K., Kadowaki, T., Ohe, K.: Dialbetics with a multimedia food recording
tool, foodlog: smartphone-based self-management for type 2 diabetes. J. Diab. Sci.
Technol. 9(3), 534–540 (2015)

Food Photo Recognition for Dietary Tracking
141
28. Willett, W.C., Koplan, J.P., Nugent, R., Dusenbury, C., Puska, P., Gaziano, T.A.:
Prevention of chronic disease by means of diet and lifestyle changes (2006)
29. Yanai, K., Kawano, Y.: Food image recognition using deep convolutional network
with pre-training and ﬁne-tuning. In: 2015 IEEE International Conference on Mul-
timedia & Expo Workshops (ICMEW), pp. 1–6. IEEE (2015)

Fusion Networks for Air-Writing Recognition
Buntueng Yana(B) and Takao Onoye
Graduate School of Information Science and Technology,
Osaka University, Osaka, Japan
mr.buntueng@gmail.com
Abstract. This paper presents a fusion framework for air-writing recog-
nition. By modeling a hand trajectory using both spatial and tempo-
ral features, the proposed network can learn more information than the
state-of-the-art techniques. The proposed network combines elements of
CNN and BLSTM networks to learn the isolated air-writing characters.
The performance of proposed network was evaluated by the alphabet and
numeric databases in the public dataset namely 6DMG. We ﬁrst eval-
uate the accuracy of fusion network using CNN, BLSTM, and another
fusion network as the references. The results conﬁrmed that the average
accuracy of fusion network outperforms all of the references. When the
BLSTM unit was set at 40, the best accuracy of proposed network is
99.27% and 99.33% in the alphabet and numeric gesture, respectively.
When compared this result with another work, the accuracy of proposed
network improves 0.70% and 0.34% in the alphabet and numeric gesture,
respectively. We also examine the performance of the proposed network
by varying the number of BLSTM units. The experiments demonstrate
that while increasing the number of BLSTM units, the accuracy also
improves. When the BLSTM unit is greater than 20, the accuracy main-
tains even though the BLSTM unit increases. Despite adding more learn-
ing features, the accuracy of proposed network insigniﬁcantly improves.
Keywords: Air-writing recognition · Human machine interface
Gesture recognition · Convolutional neural network · BLSTM
1
Introduction
Motion gesture has been widely used in many applications, especially for the
smart devices where the size is small and may not ﬁt with the traditional inputs.
Most of the applications deploy the simple gestures, such as swipe, open palm,
and closed palm because it is easy to perform and manage. Even the simple
gestures work quite well in practice, the communicating power is limited by
the set of commands and it is diﬃcult to extend the set of gestures. Writing a
command in the air is an alternative choice to overcome this problem.
Air-writing is a dynamic hand gesture. It refers to writing the alphabet or
numeric gestures in free space. Even we can use many sensors to track or capture
the motion, deploying the image sensing device is the most intuitive because it
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 142–152, 2018.
https://doi.org/10.1007/978-3-319-73600-6_13

Fusion Networks for Air-Writing Recognition
143
does not need special device attached to the user’s hand. The air-writing can
commonly be classiﬁed into two types of writing depending on the consecutive
written characters. The ﬁrst style is over-writing and another one is writing-
toward style. In the over-writing style, the characters are written in an imagi-
nary box which is ﬁxed the height and width in the ﬁeld of view of an image
sensor. The next character will be written over the previous one. In the writing-
toward style, the characters are written from left to write which is similar to the
traditional writing in many aspects. This type of writing is more complicated
than the over-writing because the size and shape of the character may vary.
Moreover, writing characters in an unconstrained area will cause a slant which
aﬀects the accuracy of the system. In general, the over-writing style is a good
choice for many applications because it is easy to manage, even though this type
of writing is not natural. Even many techniques have been proposed to recog-
nize the air-writing, creating a robust system for a real-life application is still
challenging because the air-writing is diﬀerent from the traditional writing in
many aspects. By drawing the characters in the air, it does not have any visual
or haptic feedback. Even the character is written by the same writer, the shape
and size of gesture are inconsistency. This work, we present a fusion networks for
air-writing recognition. Instead of modeling the writing trajectory using either
spatial or temporal feature as the previous study, we deploy both features which
cause the performance of the proposed network is better than the other.
2
Related Work
Even the air-writing recognition has been studied more than 20 years, it is still
active research for a human-computer interaction application. The air-writing
trajectory is usually represented by either spatial or temporal space as depicted
in Fig. 1 where the blue dots represent the gesture trajectory in three-dimensional
space, the red and green lines are the temporal signals that projected on x-plane
and y-plane, respectively.
y-plane
x-plane
Fig. 1. Air-writing trajectory in spatial and temporal spaces (Color ﬁgure online)
From the previous study, the air-writing recognition normally modeled by either
spatial or temporal space. In the spatial space, the writing trajectory is repre-
sented by a two-dimensional image which is similar to the traditional writing.

144
B. Yana and T. Onoye
Each character is generated by projecting the position of reference point into the
two-dimensional visual plane. By viewing the air-writing as an image, the state-
of-the-art techniques such as Convolutional Neural Network (CNN) [1,2] can be
applied to recognize the character. In a temporal space, the trajectory is repre-
sented by multiple signals contained information about the writing sequence. The
commonly used features are a sequence of hand position, the writing velocity,
and the angle between the consecutive points in the trajectory. The conventional
algorithms such as Hidden Markov Model (HMM) [3–5], Dynamic Time Warp-
ing (DTW) [2], and Conditional Random Field (CRF) [6,9], model the writing
trajectory as transition of the reference point in the temporal space. These tech-
niques are work well with a simple gesture such as the numeric gesture because
each gesture is clearly diﬀerent from the others. When applied these techniques
with an alphabet gesture which is more complicated than the numeric gesture,
the accuracy of the network may drop. For example, by using only the temporal
features, the certain gesture “B” may be predicted as “D” or “P” as depicted in
Fig. 2. Because the candidate gestures consist of the similar sequence with the
certain gesture.
certain gesture
candidate1
candidate2
candidate3
Fig. 2. Example of confusing gestures
The fusion technique for air-writing recognition was ﬁrstly proposed by Yang
et al. [7]. They examined their network with a public dataset namely 6DMG [8].
By learning both spatial and temporal features, their network outperforms all
of the previous works. We also consider a fusion network which is quite familiar
with Yang’s work. Instead of using the CRF technique as Yang’s work, we deploy
the BLSTM network because it showed a good performance when applied to the
writing trajectory [10–12,14] and the unsegmented time sequence data [17].
3
Proposed Framework
This work, we propose the fusion network for air-writing recognition by using
both spatial and temporal features. By deploying the fusion framework, we can
acquire more information than the conventional techniques. We deploy the Con-
volutional Neural Network which is shown a high performance in the handwritten
recognition [13] while the BLSTM network is chosen for learning the temporal
features. The complete structure of the proposed network is shown in Fig. 3
The CNN part comprises with three convolution layers, two subsampling lay-
ers, and two fully connected layers as depicted in Fig. 4. All convolutional layers

Fusion Networks for Air-Writing Recognition
145
CNN
BLSTM
Fusion layer
F1 layer
F2 layer
Decision layer
output
Fusion network
time
Amplitude
32
32
Fig. 3. The structure of proposed network
utilized the 5 × 5 ﬁxed convolution kernel size. The kernel size in all subsam-
pling layers is 2 × 2 which reduce the previous by four. For a fair comparison
with other works, the trajectory image size is ﬁxed at 32 × 32 pixels. An image-
like data is created by plotting the normalized trajectory on a two-dimensional
grayscale image. Then two consecutive points are connected by linear approx-
imation. Finally, the edge of trajectory image was smoothed by the Gaussian
ﬁlter. The kernel size of the ﬁlter is ﬁxed with 3 × 3 pixels while the variance
(σ) of the Gaussian function is 0.5.
CNN part
Total trajectory
C1
S1
C2
S2
C3
Fig. 4. CNN part in the proposed network
The BLSTM part comprises with 3 main gates namely, input gate (it), forget
gate (ft), and an output gate (ot). The subscript t denotes the computation at
time step t. The output of each gate is squashed with the sigmoid (σ) activation
function. The learning parameters of the LSTM cell are computed using these
equations.
it = σ(Wixt + Uiht−1 + bi),
ft = σ(Wfxt + Ufht−1 + bf),
ot = σ(Woht + Uoht−1 + bo),
(1)
ˆct = tanh(Wcxt + Ucht−1 + bc),
ct = ft ⊙ct−1 + it ⊙ˆct,
ht = ot ⊙tanh(ct),
where W∗is the input hidden weight matrix, the U∗is the internal hidden weight
matrix in the recurrent part, and b∗is the bias vector in each layer. The operator

146
B. Yana and T. Onoye
⊙denotes the element-wise product of the vector. The main diﬀerence between
the fusion networks for learning alphabet and numeric gesture is a number of
neurons in the decision layer. For learning the alphabet gesture, the decision
layer comprises with 26 fully connected neurons. While learning the numeric
gesture, the neuron in decision layer is 10. The fusion score at the decision layer
of the proposed framework is computed by the Softmax function as
Sq(I, F, w) =
exp(zq(I, F, w))

q exp(zq(I, F, w)),
(2)
where zq(I, F, w) is a conﬁdence function of the qth class. I is an input image
size 32×32, F is the observed features which were normalized, and w is a matrix
of learning parameters from CNN and BLSTM networks. The prediction class
of air-writing (ˆq) is computed as
ˆq = arg max
q (Sq(I, F, w)),
(3)
In the learning process, we choose a categorical cross entropy with Adam
optimizer to minimize the loss function. We start examining the performance of
the fusion network by setting the BLSTM unit equals to 40. Then we investi-
gate the relationship between the number of BLSTM units and the recognition
accuracy by varying the BLSTM unit from 1 to 40. We also examine the eﬀect
of learning features as shown in next section.
4
Experiments and Results
The fusion framework was evaluated by the alphabet and numeric gestures in
the 6DMG dataset. The alphabet gesture is a set of the uppercase “A” to “Z”
motion characters. It was collected from 25 participants. Each type of gesture
was written 10 times by each participant which cause the total sample to be
6500. The numeric gesture was collect from 6 participants. Each character was
written 10 times. Before we feed these samples in the fusion network, we clean
the sample gesture by removing the redundant points and normalizing it with
simple processes as follow.
4.1
Preprocessing Data
Writing a character in the air is diﬃcult for the writer to control a speed of
writing because it does not have any feedback. By capturing the gesture with a
ﬁxed sampling rate sensor, the sampling point in a gesture is signiﬁcant varied
even the same character written by the same writer. A commonly used tech-
nique is resampling the trajectory with ﬁxed distance [15]. Instead of using ﬁxed
distance resampling technique, we deploy a simple method to remove the redun-
dant points by using a threshold of consecutive points. The observed gesture is
denoted as P = {p0, p1, . . . , pN} where pi = (xi, yi) is the ith instance of the

Fusion Networks for Air-Writing Recognition
147
hand reference, and N is a number of sampling points in a gesture. The observed
gesture is computed the Euclidean distance between the consecutive points as
D(pi, pi−1) =

(xi −xi−1)2 + (yi −yi−1)2,
(4)
where pi and pi−1 is a current and previous position of the hand, respectively. If
the distance of the consecutive points is less than a threshold value the current
point will be removed from the observed sequence. This work, the threshold
value was set at 5 cm. This technique does not only improve the recognition
accuracy, it also reduces the time for training the network. Before feeding the
gesture in the fusion networks, we normalized with a simple process similar to
Jaeger’s work [16]. The writing trajectory was ﬁrst split into x and y sequences.
The normalized trajectory pni is calculated as
pni =
pi
max(Δx, Δy),
(5)
where the Δx and Δy are the diﬀerences between the lowest and highest values of
the position in x-axis and y-axis, respectively. The Δx and Δy are computed as
Δx =
N
max
i=1 (xi) −
N
min
i=1 (xi),
Δy =
N
max
i=1 (yi) −
N
min
i=1 (yi),
This normalized method is simple and the shape of the gesture in spatial space
is kept unchanged. The other types of features that we interest are velocity and
angular features. The angular features is denoted by Ot = {o1, o2, . . . , oN−1}
where each element in this feature is computed by
oi = arctan
 yi −yi−1
xi −xi−1

,
(6)
After computing the total points in the feature, every single element of the angu-
lar feature is normalized by 2π. The last feature that we have studied is velocity
which is denoted by V = {v1, v2, . . . , vN−1}. Each element of the angular fea-
ture is derived from the normalized hand position using the Euclidean distance
in (4). It is ﬁnally normalized by the maximum velocity in each gesture which is
similar to the method used to normalize the position features.
4.2
Performance Evaluation with Base References
We ﬁrst examined a performance of the fusion technique by using both CNN
and BLSTM network as the base references. Total trajectory image and hand
position are the basic features in the ﬁrst experiment. To avoid the overﬁtting
in the fusion network, we had applied the dropout in F1 and F2 layers. In the
BLSTM part, the number of BLSTM nodes is set to 40 and the dropout method
is also applied in the BLSTM layer. The alphabet gesture examined using 20 fold

148
B. Yana and T. Onoye
cross-validation technique. The average recognition accuracy of all experiments
was recorded in the Table 1.
Table 1. Average accuracy of the fusion network
Dataset
CNN
BLSTM Fusion networks Yang’s [7]
Alphabet 97.83% 99.03%
99.27 %
98.57%
Numeric
98.00% 99.33%
99.33%
98.99%
From the record in the Table 1, it conﬁrmed that the recognition accuracy of
the proposed technique is better than both CNN and BLSTM networks. When
compared the result with Yang’s [7] work, the accuracy of proposed network
improves 0.70% on the alphabet gestures and 0.34% on the numeric gestures.
For more detailed evaluation, the accuracy of each class of gesture in an alphabet
and numeric gestures are plotted on the graph in Figs. 5 and 7, respectively.
Fusion network
CNN
BLSTM
Accuracy
Fig. 5. Accuracy on each class of alphabet gesture
The graph in Fig. 5 illustrate the recognition accuracy of fusion network is
better than the CNN in all classes. In some classes, “C”, “E”, “H”, “M”, “P”,
“V”, “W” , and “Y”, the recognition accuracy of proposed network is lower
than BLSTM network. The lowest accuracy of the proposed network is 95.60%
which states at a “P” gesture. For a precise interpreting the performance of the
proposed network, we also plot the result in the confusion matrix as depicted in
the Fig. 6. The two most misclassiﬁed are stated at the “D” and “P” gestures.
The actual “D” gesture was predicted as “P” gesture for 2.8% and the actual
“P” gesture was classiﬁed as “D” gesture for 4.4%.
The graph in Fig. 7 illustrates the accuracy of the proposed network when
applied with the numeric gesture. The results showed the accuracy of the fusion

Fusion Networks for Air-Writing Recognition
149
Fig. 6. Confusion matrix from the alphabet dataset
network is lower than CNN and BLSTM in some classes. The lowest accuracy of
the proposed network is 96.70% which states at the “7” gesture. The misclassiﬁed
gesture are stated at the “6”, “8”, and “9” gestures. The actual “6” gesture was
predicted as “4” and “8” gestures. The actual “8” gesture was classiﬁed as “0”
gesture for 1.67%. And the “9” gesture was predicted as “7” for 1.67%.
4.3
The Eﬀect of Learning Features
We examine the eﬀect of the BLSTM unit in the fusion network by varying the
number of the BLSTM units from 1 to 40. Then we set the training parameters
and use the optimization technique same as we do in the previous experiment.
The proposed network has been trained with the combination of hand position,
angular, and velocity features. The results of the alphabet gesture are plotted in
Fig. 8.

150
B. Yana and T. Onoye
Fusion network
CNN
BLSTM
Accuracy
Fig. 7. Accuracy on each class of numeric gesture
position
position+angular
position+velocity
position+angular+velocity
Fig. 8. Relationship of BLSTM unit and the average accuracy in alphabet gesture
To analyze the eﬀect of the BLSTM unit in the fusion network, we also plot
the line graph by ﬁtting the result of each group of the feature using a 3rd order
polynomial with the minimum mean squared error. The trend of the accuracy
of all group of features increases until the number of BLSTM is 20. After that
point, the accuracy maintains at the same level. Even increase the number of
features in the learning process, the accuracy improves insigniﬁcantly.
The results from the numeric dataset are similar to the previous one. The
recognition accuracy of proposed network increase when the number of BLSTM
units increased. When the number of BLSTM units are small, increasing the
BLSTM unit is much eﬀect on the accuracy. While the BLSTM unit is greater
than 20, increasing the BLSTM unit can slightly improve the accuracy.

Fusion Networks for Air-Writing Recognition
151
position
position+angular
position+velocity
position+angular+velocity
Fig. 9. Relationship of BLSTM unit and the average accuracy in numeric gesture
5
Conclusion
This paper has presented a fusion network for air-writing recognition. We model
the hand trajectory using both spatial and temporal features in the same manner
as Yang’s work. From the experiment, the accuracy of proposed outperforms both
CNN and BLSTM networks. When compare with Yang’s work, the accuracy
of proposed network improves 0.70% and 0.34% in the alphabet and numeric
gesture, respectively. By varying the number of BLSTM units in the network, the
accuracy increase while the BLSTM unit increase. The performance maintains
when the BLSTM unit is more than 20. In the last experiment, it is obvious that
the fusion network did not learn more information even adding more training
features.
References
1. LeCun, Y.: Neural networks and gradient-based learning in OCR. In: Proceedings
of the 1997 IEEE Workshop Neural Networks for Signal Processing, USA, p. 255,
September 1997
2. Hu, J.T., Fan, C.X., Ming, Y.: Trajectory image based dynamic gesture recognition
with convolutional neural networks. In: 2015 15th International Conference on
Control, Automation and Systems, Korea, pp. 1885–1889, October 2015
3. Xu, S., Xue, Y.: Air-writing characters modelling and recognition on modiﬁed
CHMM. In: 2016 IEEE International Conference on Systems, Man, and Cybernet-
ics, Hungary, pp. 001510–001513, October 2016
4. Agarwal, C., Dogra, D.P., Saini, R., Roy, P.P.: Segmentation and recognition of text
written in 3D using Leap motion interface. In: 2015 3rd IAPR Asian Conference
on Pattern Recognition, Malaysia, pp. 539–543, November 2015
5. Hameed, M.Z., Garcia-Hernando, G.: Novel spatio-temporal features for ﬁnger-
tip writing recognition in egocentric viewpoint. In: 2015 14th IAPR International
Conference on Machine Vision Applications, Japan, pp. 484–488, May 2015
6. Hsu, Y.L., Chu, C.L., Tsai, Y.J., Wang, J.S.: An inertial pen with dynamic time
warping recognizer for handwriting and gesture recognition. IEEE Sens. J. 15(1),
154–163 (2015)

152
B. Yana and T. Onoye
7. Yang, C., Ku, B., Han, D.K., Ko, H.: Alpha-numeric hand gesture recognition based
on fusion of spatial feature modelling and temporal feature modelling. Electron.
Lett. 52(20), 1679–1681 (2016)
8. Chen, M., AlRegib, G., Juang, B.: 6DMG: a new 6D motion gesture database. In:
Proceedings of the 3rd Multimedia Systems Conference, USA, pp. 83–88, February
2012
9. Ma, L., Zhang, J., Wang, J.: Modiﬁed CRF algorithm for dynamic hand gesture
recognition. In: 2014 33rd Chinese Control Conference, China, pp. 4763–4767, July
2014
10. Graves, A., Schmidhuber, J.: Framewise phoneme classiﬁcation with bidirectional
LSTM networks. In: 2005 IEEE International Joint Conference on Neural Net-
works, Canada, vol. 4, pp. 2047–2052, August 2005
11. Frinken, V., Uchida, S.: Deep BLSTM neural networks for unconstrained contin-
uous handwritten text recognition. In: ICDAR 2015 Proceedings of the 2015 13th
International Conference on Document Analysis and Recognition, USA, pp. 911–
915, August 2015
12. Zhang, X.Y., Yin, F., Zhang, Y.M., Liu, C.L., Bengio, Y.: Drawing and recogniz-
ing Chinese characters with recurrent neural network. Computer Vision Pattern
Recognition arXiv:1606.06539, June 2016
13. Lecun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to
document recognition. Proc. IEEE 86(11), 2278–2324 (1998)
14. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
15. Holzinger, A., Stocker, C., Peischl, B., Simonic, K.M.: On using entropy for enhanc-
ing handwriting preprocessing. Entropy 14(11), 2324–2350 (2012)
16. Jaeger, S., Manke, S., Reichert, J., Waibel, A.: Online handwriting recognition:
the NPen++ recognizer. Int. J. Doc. Anal. Recogn. 3(3), 169–180 (2001)
17. Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal
classiﬁcation: labelling unsegmented sequence data with recurrent neural networks.
In: The 23rd International Conference on Machine Learning, New York, USA, pp.
369–376, June 2006

Global and Local C3D Ensemble System
for First Person Interactive Action Recognition
Lingling Fa, Yan Song(B), and Xiangbo Shu
School of Computer Science and Engineering,
Nanjing University of Science and Technology, Nanjing, China
linglingfa12@gmail.com, {songyan,shuxb}@njust.edu.cn
Abstract. Action recognition in ﬁrst person videos is diﬀerent from that
in third person videos. In this paper, we aim to recognize interactive
actions in ﬁrst person videos. First person interactive actions contain
two kinds of motion which are the ego-motion from the observer and
the motion from the actor. To enable an observer to understand “what
activity others are doing to me”, we propose a twin stream network
architecture based on 3D convolution networks. The global action C3D
learns interactions with ego-motion and the local salient motion C3D
analyzes the motion from the actor in a salient region, especially when
the action happens at a distance from the observer. We also propose
a sampling method to extract clips as input to the C3D models and
investigate diﬀerent C3D architectures to improve the performance of
C3D. We carry out experiments on the benchmark of JPL ﬁrst-person
interaction dataset. Experiment results prove that the ensemble of global
and local networks can increase the accuracy over the state-of-the-art
methods by 3.26%.
Keywords: First person action recognition · Global C3D
Local C3D · Global and local C3D ensemble system
1
Introduction
Recently, with the development of wearable devices, the head mounted cameras
have generated more and more egocentric videos which bring about the require-
ment of understanding human activity in the ﬁrst person viewpoint. Action
recognition in the ﬁrst person view can be divided into two cases. In the ﬁrst
case, the action is only performed by the camera wearer. In the other case, the
action is performed by another person which is possibly interactive. In this case,
the wearer is called the observer and the other person is called the actor. For
example, a robot has to understand “what people doing to me” in its ﬁrst person
viewpoint. In the third person perspective, the camera is set at a distance away
from the actors, and is not involved when interactive actions are analyzed. Fur-
thermore, the camera is static or smoothly moving, while in the egocentric videos
there are a huge amount of ego-motions like shaking presented as the camera
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 153–164, 2018.
https://doi.org/10.1007/978-3-319-73600-6_14

154
L. Fa et al.
is wore on the observer’s head. However, there is a characteristic in ﬁrst person
video that the actor always appears in the center of the vision ﬁeld. Hence, the
occlusion problem seldom occurs.
Traditionally, in the task of human activity understanding in the ﬁrst person
view, previous methods focused on the handcrafted local features to represent
videos. Many approaches maked use of visual features like HOG [1], HOF [1],
MBH [2], STIP [3] and 3D SIFT [4] to encode appearance information. Recently,
deep neural networks is becoming attractive because of its ability of learning high
level visual feature representations automatically from raw data by end-to-end
training manner. Inspired by the success of convolutional neural networks in the
image understanding tasks, Tran et al. [5] proposed a deep 3D ConvNet to learn
spatio-temporal features in a stream for video recognition. The features extracted
from C3D encapsulate information related to objects, scenes and actions in a
video, which makes it perform well on various video analysis tasks. Singh et al. [6]
used a fusion of 3D ConvNet and 2D ConvNet for hand-object interactions in
egocentric videos. Poleg et al. proposed a compact 3D ConvNet architecture for
long-term outdoor activity recognition. However, these methods are not suitable
for recognizing human-robot interactions.
Fig. 1. The left column shows actions containing large amount of ego-motion. The
right column shows the motion region from the actor is relatively small.
As we observe, the ﬁrst person interactive actions contain two kinds of
motion, i.e. the ego-motion comes from the observer, and the motion from the
actor. Usually, interactions impact the observer physically, which results in the
ego-motion. For example, in the interaction of “shaking hands with the observer”,
the observer usually performs some shaking motion. The ego-motion inevitably
results in the global motion in the ﬁrst person video, as shown in Fig. 1(a). On
the other hand, the actor’s action usually comes from a local salient region in the
vision ﬁeld, which is sometimes of a relatively small size as shown in Fig. 1(b).
Based on the above observation, we propose to ensemble a twin stream net-
work architecture based on the C3D network to recognize interactive actions

Global and Local C3D Ensemble System
155
in ﬁrst person videos, which includes a global action C3D and a local salient
motion C3D. The global action C3D is designed to capture the ego-motion from
the observer. The local salient motion C3D focuses on recognizing the salient
motion from the actor, which is a necessary supplement. Speciﬁcally, the local
region is obtained by dominant ﬂow after two denoising steps. Besides, for the
generation of the input clip of C3D networks, we verify the sampling methods
with diﬀerent sampling parameters. It is shown in the experiment that diﬀerent
sampling parameters can aﬀect the performance of the method. Clips generated
with larger sampling interval can help the networks learn discriminative fea-
tures and improve their generalization capability. We also verify diﬀerent C3D
architectures such as deep C3D [7] and resnet-18 C3D [8] for the local C3D.
The rest of this paper is organized as follows. Section 2 introduces the related
work on ﬁrst person action recognition. Section 3 describes the algorithm in
global and local C3D ensemble system. The experiment results are reported in
Sect. 4. Section 5 concludes this paper.
2
Related Work
First person video datasets. First person video datasets are collected from
the ﬁrst viewpoint, which can be roughly divided into three types. The ﬁrst one
includes the GTEA [9], the GTEA gaze [10] and the GTEA gaze+ [10]. The
person wears a head-mounted camera and most activities involve hand-object
interactions. Thus, object detection, hand recognition and other visual cues are
usually used to infer the activity category [11]. The second type contains long-
term activities performed by the camera wearer doing outdoor sports in the
wild, like the Huji dataset [12]. As the video content is mainly the scenery that
the person sees, motion representation and scene understanding are useful for
recognizing actions in this kind of videos. The third type is the observer-actor
interactions. Action recognition for this kind of videos enables a robot to learn to
analyze human activities in order to make reactions [13]. As mentioned before,
our work focuses on the third type of ﬁrst person videos.
First person action recognition methods. Handcraft features were exten-
sively adopted in action analysis for ﬁrst person videos as for third person videos.
Kitani et al. [14] used optical ﬂow based global motion descriptors to discover
ego-actions in sports videos. Because of the unavailability of the actor’s poses,
Fathi et al. realized the importance of hands in the ﬁrst person action recog-
nition and proposed a representation utilizing the optical ﬂow, pose, size and
location of the subject’s hands for egocentric hand-object interactions.
Deep network features for ﬁrst person action recognition have also been
explored. Ma et al. [11] explored a twin stream network architecture to integrate
the appearance information like hand appearance and object attributes, with
the motion information like local hand motion and ego-motion. Lee et al. [13]
designed a new approach to make a robot learn the temporal structure of the

156
L. Fa et al.
activity by a regression network. While these networks focused on hand-object
interactions in the ﬁrst person videos, we propose a method designed for the
action recognition in the videos including social interactions between human
and a robot.
Fig. 2. The architecture of deep C3D.
3
Algorithm
3.1
Preliminary
In 2D ConvNets which are applied on images, convolution and pooling operations
are performed only spatially. When they are applied on videos, 2D convolution
and pooling treat diﬀerent frames as diﬀerent channels. As a consequence, 2D
ConvNets lose temporal information of the input videos. Tran et al. [5] extended
2D convolution networks to perform 3D convolution and 3D pooling operations,
which are veriﬁed eﬀective for learning spatio-temporal features in videos by
preserving the temporal information of the input signals. The deep C3D [7] and
the resnet-18-C3D [8] are the modiﬁed 3D convolution network. The deep C3D
has ﬁve 3D convolution layers and ﬁve 3D pooling layers, where each pooling
layer follows a convolution layer. Three fully connected layers and a softmax loss
layer are adopted to predict action labels. The network is set up to take short
video clips as input. Resnet-18-C3D is composed of several deeper branches
which can learn additional residual functions. Figure 2 shows the architecture of
deep C3D.
3.2
Global and Local C3D Ensemble System
Observer-actor interactions in ﬁrst person videos can be naturally decomposed
into the ego-motion from the observer and the action from the actor. For the
third person videos captured by hand-held cameras, the camera motion is usu-
ally removed before analyzing actions. For ﬁrst person action videos, ego-motion
coming from the camera may be the most important cue for recognizing interac-
tive actions. On the other hand, action from the actor sometimes occupies a small
region and most of the background is irrelevant. This characteristics of ﬁrst per-
son interactive actions poses serious challenges to single action classiﬁer. Based
on these insights, we propose a twin deep learning architecture designed for the

Global and Local C3D Ensemble System
157
Fig. 3. The ensemble system of the twin networks for ﬁrst person action recognition.
ﬁrst person interaction videos, which takes both global motion and local motion
into account. Figure 3 visualizes the ensemble system of the twin networks for
ﬁrst person action recognition.
For the global C3D, we take the entire frames in the video clip as input,
which contains large amount of ego-motion in the global appearance. For the
local C3D, following two denoising processes, we detect the salient motion region
of the actor, then crop the detected local regions as input to recognize the action
of the actor. The two classiﬁers are trained separately. Finally, we combine the
results of the global and local C3D classiﬁers to predict the action type in the
video.
For the ensemble, we adopt two voting methods, i.e. choosing the label with
the highest conﬁdence score and choosing the label which is predicted the most
times. In detail, we sample several clips for each video based on the sampling
method introduced in Sect. 4, then we get the labels and scores for them respec-
tively from two networks, and ensemble the results to recognize the action in the
video.
3.3
Network Input
Global C3D input. All video frames are resized to 128 × 171. Videos are divi-
ded into clips of 8 or 16 frames as input to the networks. For data augmentation
and overﬁtting reduction, videos are sampled based on the proposed sampling
method in Sect. 4. Then we train the global deep C3D network and use softmax
as the loss function.
Local C3D input. We use dominant ﬂow to locate a region where salient
motion occurs. First, we extract dominant ﬂow in the global frame, then remove
the irrelevant movement as noise and set the bounding box of the salient motion

158
L. Fa et al.
region based on the remaining ﬂow. To detect noise, we track the ﬂow points for
the latter check. It contains two steps to perform the denoising preprocess. The
noises of movement in ﬁrst person videos can be roughly categorized into two
types. The ﬁrst type comes from the weak camera motion which is irrelevant to
the salient motion. We ﬁlter them out by checking the distance of tracked points
from consecutive frames. The other type of noise is the motion from other person
who does not participate in the interaction. As we observe, they usually appear
on the edge of the image. Thereby, we use the number of tracked points on the
edge of the frame to decide whether they are the noises from irrelevant person.
Figure 4 shows the process of salient motion region extraction.
Fig. 4. (a) shows the original image with optical ﬂow noises. (b) shows the image after
the denoising of the weak camera motion in the ﬁrst step. (c) shows the image after
the two denoising steps.
Fig. 5. The local salient regions of diﬀerent types of interaction. The salient region in
‘petting the observer’ (c), where the actor is close to the observer, is larger than the
regions in ‘throwing objects to the observer’ (a) and ‘punching the observer’ (b).
As the 3D kernel block is sliding among all the frames in the video clip input
to the deep 3D network, the frames from one clip should be aligned. We use the
maximal region to crop all the frames in one video. Since the regions cropped
from diﬀerent videos may be of diﬀerent sizes, we resize all of the clips as the
input to the local 3D convolution network. Figure 5 shows diﬀerent region sizes
from diﬀerent classes of actions.

Global and Local C3D Ensemble System
159
3.4
Sampling Scheme for Clip Generation
The 3D convolution network takes a clip comprised of several frames as input
due to the limited capacity. The sampling method for clip generation has a great
impact on the generalization ability of the classiﬁer. Videos in the training set
are of diﬀerent durations, thus the balance of the numbers of training samples
should be considered in sampling. Besides, consecutive frames are usually quite
similar so that there are a lot of redundancy. Thus, we propose a sampling scheme
based on the above two considerations.
Fig. 6. The sampling method for clip generation. S i is the starting frame, R is the
sampling interval, and L is the length of a clip.
Suppose an action video sequence has N
frames denoted by I
=
{I1, I2, · · · , IN} and its action label is denoted by y, from which we extract
T clips. The number of starting frames equals to the number of clips extracted
from each video. We set U as the clip interval, so the indexes of the starting
frames are denoted by S = {1, 1 + U, 1 + 2U, · · · , 1 + (T −1) × U}, where U is
computed as N/T. To cover more key frames for each clip, we sample L frames
with the sampling interval R. Thus, for the ith sampled clip, the indexes of the
sampled frames are Xi = {Si, Si + R, Si + 2R, · · · , Si + (L −1) × R}, where Si
is its starting frame index. Each clip sampled from the video is labeled by y.
Figure 6 shows the sampling method.
4
Experiment
4.1
Dataset and Data Preprocess
JPL First Person Interaction Dataset. The dataset [15] is composed of
human activity videos taken from a ﬁrst person viewpoint. It contains interac-
tions between actors and observers by attaching a GoPro camera to the head
of a model robot (the observer). The robot has wheels below so it displays a
large amount of ego-motion in its videos particularly during the activities. The
dataset contains 7 types of activities performed by 8 participants wearing 10

160
L. Fa et al.
diﬀerent clothings in 5 diﬀerent environments. The activities include 4 positive
interactions with the observer, 1 neutral interaction, and 2 negative interac-
tions. “shaking hands with the observer”, “hugging the observer”, “petting the
observer”, “waving a hand to the observer” are positive interactions. “pointing
to the observer when two people have a conversation” is the neutral interaction.
“punching the observer” and “throw objects to the observer” are the two nega-
tive interactions. There are signiﬁcant ego-motion in some actions. For example,
the robot collapses in “punching”, and the robot shakes in “shaking hands”.
Figure 7 shows the seven interactions in the dataset.
Fig. 7. The seven interactions in the dataset.
Data Preprocess. The video dataset is composed of 12 sets. We adopt the
ﬁrst 10 sets as the training, and the remaining 2 sets as the testing. To aug-
ment the dataset, we multi-sample video frames in each video by the sampling
method introduced in Sect. 3.4. In order to balance the number of training sam-
ples from each class, we set the clip interval according to the duration of the
videos. For instance, “punching” is often short, while “petting” records rela-
tively long videos. Finally, we obtain 1420 samples for training with about 200
samples for each class, and 357 samples for testing with about 50 samples for
each class. For the experiments in Sects. 4.2 and 4.3, the results are counted on
the clip sample level. The results in Sect. 4.4 are counted on the video level.
4.2
Global Action C3D
Our network implementation is based on C3D modiﬁed by caﬀe. We train the
network on NVIDIA Tesla GPU K20 with 4 GB memory.
We train the global network by deep C3D from scratch and set the ﬁxed
learning rate to 0.0001. We use the batch size of 5 for training dataset, and

Global and Local C3D Ensemble System
161
batch size of 3 for the testing dataset. All video frames are resized to 128 × 171.
A 112 × 112 sub-image is randomly cropped from the selected frame. For training
the global action C3D, we sample 8 or 16 frames with the interval of 1, 2, 3,
and 4 for each clip. Table 1 shows the results of global recognition using diﬀerent
sampling parameters. The accuracy achieves 75.67% with 16 frames per clip and
the sampling interval of 2.
The experiment shows that consecutive sampling is not representative enough
for C3D to learn the eﬀective spatio-temporal features. When the sampling inter-
val is larger than 1, the global action C3D can achieve better performance no
matter the sampling frames per clip is 8 or 16. However, if the sampling interval
becomes too large, the accuracy decreases. Moreover, 16 frames per clip is better
than 8 frames because more frames can provide more information.
Table 1. The results of global recognition using diﬀerent sampling parameters.
Sampled frames per clip Sampling interval Clip accuracy (%)
8
2
66
8
4
71.33
16
1
66
16
2
75.67
16
3
72.22
4.3
Local Salient Motion C3D
We use the sparse optical ﬂow based on LK to detect the local salient region. The
noises come from two aspects. To ﬁlter out the noise from weak camera motion,
we use the threshold of 3 for the distance of tracked points from consecutive
frames. To ﬁlter out the motion from irrelevant person, we use the threshold
of 10 for the number of tracked key points on the edge of the frame. After the
two denoising steps, we crop the salient motion regions for each video. Then, we
verify two C3D networks for the local salient motion recognition.
For the deep C3D network, all the local images are resized to 116 × 144 and
cropped to 112 × 112 randomly for data augmentation. We set the learning rate
to 0.0001. Table 2 shows the performance of deep C3D network for local motion
recognition using diﬀerent sampling intervals. It shows that the accuracy achieves
the highest when the sampling interval is 4.
For the resnet-18-C3D network, all the local images are resized to 64 × 96, and
cropped to 64 × 64 randomly for data augmentation. We set the batch size to 3
and the base learning rate to 0.0001. We adopt the ‘step’ to update the leaning
rate every 5000 iterations, and gamma is set to 0.1. The local salient motion
C3D is ﬁnetuned on the sports1m model [5] that is trained through 2800000
iterations. Because of the high cost of memory, we sample 8 frames per clip.
The clip in resnet-18-C3D covers the same amount of frames in the deep C3D.

162
L. Fa et al.
Table 2. The performance of deep C3D network for local motion recognition using
diﬀerent sampling intervals.
Sampled frames per clip Sampling interval Clip accuracy (%)
16
2
69.74
16
3
69.05
16
4
74.42
Table 3 shows the performance of resnet-18-C3D for local motion recognition
using diﬀerent sampling intervals. It performs best when the sampling interval
is 8.
Table 3. The performance of resnet-18-C3D for local motion recognition using diﬀerent
sampling intervals.
Sampled frames per clip Sampling interval Clip accuracy (%)
8
4
67.5
8
6
65.95
8
8
67.78
4.4
Global and Local C3D Ensemble System
For the ensemble system, we sample 16 frames per clip with the sampling interval
of 2 for the global action C3D, and sample 16 frames per clip with sampling
interval of 4 for the local salient motion C3D. We sample 10 clips from each video
in the testing dataset. The ﬁnal recognition result for a video is determined by
two voting methods introduced in Sect. 3.2. We compare the two voting methods
and list the results in Table 4. The results show that the local C3D performs
better than the global one. The voting method does not impact the performance
of the local C3D, but it performs better for the global C3D when the most-times
voting method is adopted. The ensemble of the two C3D models achieves the
best performance with the most-times voting method, which surpasses the single
model by at least 7.15%.
We compare the proposed method with several existing methods and list
the comparison results in Table 5. Ryoo and Matthies [15] proposed structure
match, a new kernel function to compute video distances, for ﬁrst person action
recognition. They also proposed a local motion descriptor and a global motion
descriptor. They reported the comparison result of their matching scheme with
several other matching methods [16,17], which are listed in Table 5. We observe
that the C3D ensemble system outperforms the state-of-art methods by 3.26%.

Global and Local C3D Ensemble System
163
Table 4. The ensemble results by two voting methods.
Method
Video accurancy (%)
Max score Most times
Global C3D
71.43
78.57
Local C3D
85.71
85.71
C3D ensemble system 85.71
92.86
Table 5. Comparison results of diﬀerent recognition approaches on the JPL dataset.
Method
Video accurancy (%)
Local (%) Local and global (%)
x2 kernel
82.4
84.3
Histogram Intersect
82.4
84.4
ST-Pyramid match [16]
82.6
86.0
Dynamic BoW [17]
82.8
87.1
Structure match [15]
83.1
89.6
C3D ensemble system 85.71
92.86
5
Conclusion
In this paper, we propose a twin stream C3D network architecture, which
includes a global action C3D and a local salient motion C3D. This method
is to recognize interaction level activities in ﬁrst person perspective. The global
action C3D is able to capture the ego-motion from the observer while the local
salient motion C3D focuses on the salient motion from the actor. We testify
several sampling parameters for clip generation and ﬁnd out that using larger
sampling intervals can increase the recognition accuracy. Experiments show that
the ensemble system of global and local C3D networks performs better than the
single C3D network. Besides, it outperforms the state-of-art methods by 3.26%.
Acknowledgement. This work was supported in part by the 973 Program (Project
No. 2014CB347600), the Natural Science Foundation of Jiangsu Province (Grant No.
BK20170856), the National Nature Science Foundation of China (Grant Nos. 61672285
and 61702265) and CCF-Tencent Open Research Fund.
References
1. Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning realistic human
actions from movies. In: IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2008, pp. 1–8 (2008)
2. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Dense trajectories and motion bound-
ary descriptors for action recognition. Int. J. Comput. Vis. 103(1), 60–79 (2013)

164
L. Fa et al.
3. Laptev, I., Lindeberg, T.: On space-time interest points. Int. J. Comput. Vis. 64(2–
3), 107–123 (2005)
4. Scovanner, P., Ali, S., Shah, M.: A 3-dimensional sift descriptor and its application
to action recognition, pp. 357–360 (2007)
5. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-
poral features with 3D convolutional networks, pp. 4489–4497 (2014)
6. Singh, S., Arora, C., Jawahar, C.V.: First person action recognition using deep
learned descriptors. In: Computer Vision and Pattern Recognition (2016)
7. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. Computer Science (2014)
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
9. Fathi, A., Ren, X., Rehg, J.M.: Learning to recognize objects in egocentric activi-
ties. In: Computer Vision and Pattern Recognition, pp. 3281–3288 (2011)
10. Fathi, A., Li, Y., Rehg, J.M.: Learning to recognize daily actions using gaze. In:
Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012.
LNCS, vol. 7572, pp. 314–327. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-33718-5 23
11. Ma, M., Fan, H., Kitani, K.M.: Going deeper into ﬁrst-person activity recognition,
pp. 1894–1903 (2016)
12. Poleg, Y., Ephrat, A., Peleg, S., Arora, C.: Compact CNN for indexing egocentric
videos. Computer Science, pp. 1–9 (2016)
13. Lee, J., Ryoo, M.S.: Learning robot activities from ﬁrst-person human videos using
convolutional future regression (2017)
14. Kitani, K.M., Okabe, T., Sato, Y., Sugimoto, A.: Fast unsupervised ego-action
learning for ﬁrst-person sports videos. In: Computer Vision and Pattern Recogni-
tion, pp. 3241–3248 (2011)
15. Ryoo, M.S., Matthies, L.: First-person activity recognition: what are they doing
to me? In: IEEE Conference on Computer Vision and Pattern Recognition, pp.
2730–2737 (2013)
16. Choi, J., Jeon, W.J., Lee, S.C.: Spatio-temporal pyramid matching for sports
videos. In: ACM International Conference on Multimedia Information Retrieval,
pp. 291–297 (2008)
17. Ryoo, M.S.: Human activity prediction: early recognition of ongoing activities from
streaming videos. In: IEEE International Conference on Computer Vision, pp.
1036–1043 (2012)

Implicit Aﬀective Video Tagging Using
Pupillary Response
Dongdong Gui, Sheng-hua Zhong(B), and Zhong Ming
College of Computer Science and Software Engineering, Shenzhen University,
Shenzhen 518060, China
ddgui@email.szu.edu.cn, {csshzhong,mingz}@szu.edu.cn
Abstract. The psychological research found that human eyes could
serve as a sensitive indicator of emotional response. Pupillary response
has been used to analyze the aﬀective video content in previous studies,
but the performance is not good enough. In this paper, we propose a novel
method for implicit aﬀective video tagging using pupillary response. The
issue of pupil size diﬀerence between subjects has not been eﬀectively
solved, which seriously aﬀected the performance of the implicit aﬀec-
tive video tagging. In our method, we ﬁrst deﬁne the pupil diameter
baseline of each subject to diminish individual diﬀerence on pupil size.
Besides, the probabilistic support vector machine (SVM) and long short
term memory (LSTM) network are used to extract valuable informa-
tion and output the probability estimates based on the proposed global
features and sequence features obtained from the pupil dilation ratio
time-series data, respectively. The ﬁnal decision is made by combining
the probability estimates from these two models based on the sum rule.
In empirical validation, we evaluate the proposed method on a standard
dataset MAHNOB-HCI. The experimental results show that the pro-
posed method achieves better classiﬁcation accuracy compared with the
existing method.
Keywords: Implicit aﬀective video tagging · Pupillary response
Emotion recognition
1
Introduction
With the explosive growth of video data, automatic video content analysis plays
an increasingly signiﬁcant role in various video-based applications, such as video
retrieval [31], video summarization [28], video saliency detection [9], and so on.
Most of traditional content-based video analysis concentrates on the main event
happened in a video. The aﬀective level is an especially important measure of
the viewers’ attitude toward video content. Recently, video aﬀective content
analysis has been an active research area. Video aﬀective content analysis seeks
to automatically identify the emotions elicited by videos [32]. The development
of it is beneﬁt to both the users and businesses. Users could utilize the emotional
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 165–176, 2018.
https://doi.org/10.1007/978-3-319-73600-6_15

166
D. Gui et al.
information to retrieve certain videos, and ﬁlmmakers may change their editing
to make a more stimulating movie that meet the emotional ﬂow of audiences
through the help of video aﬀective content analysis [27].
The success of this task will crucially hinge on how the eﬀective features
are deﬁned and extracted. The direct approaches mainly rely on the automatic
feature extraction from video data. The extracted audio-visual features include
color histogram, the number of scene cuts, Mel- Frequency Cepstrum Coeﬃ-
cients (MFCC) [2], and spatio-temporal features [23]. As we known, aﬀective
information in videos is closely related to the viewer’s feelings and emotions.
Thus, the emerging research ﬁeld of video aﬀective analysis aims at exploiting
human emotions based on the analysis of the spontaneous reactions for view-
ers while watching the video content, which can be called as the implicit video
aﬀective content analysis.
Since most of the psychological theories agree that physiological activity is an
important component of emotional experience, and facial expression is the pri-
mary channel for emotion communication, implicit video aﬀective content anal-
ysis mainly adopts viewers’ physiological signals and spontaneous visual behav-
iors, especially the facial behaviors [21]. Some important physiological signals,
such as: electroencephalography (EEG), electrocardiography (ECG), electromyo-
graphy (EMG), skin temperature (ST), heart rate (HR), and blood volume pulse
(BVP), which are controlled by the sympathetic nervous system, are capable to
reﬂect unconscious body changes [11,12,22,25]. Compared with facial behaviors,
these signals are more evident and reliable [17,21]. Unfortunately, it is diﬃcult
to obtain these physiological signals. Users are required to wear complex appa-
ratuses to obtain these physiological signals. Moreover, the physiological signals
are also sensitive to many artifacts, such as involuntary eye-movements and
irregular muscle movements [15,18]. On the contrary, the visual behaviors of
participants are much easier to be obtained and recorded. Only one remote vis-
ible camera is required to record the visual behaviors of viewers. Furthermore,
the recorded visual behaviors of participants are undisturbed by the movements
or other body conditions. Compared with the physiological signals, spontaneous
visual behaviors are more convenient to measure, although it is susceptible to
some environmental noises, such as lighting conditions [4], occlusion [20], and
etc.
Several studies have demonstrated the feasibility of using spontaneous visual
behaviors for video aﬀective content analysis. Zhao et al. [30] extracted view-
ers’ facial expressions to predict the aﬀective curve to describe the process of
aﬀect changes. Their experimental results veriﬁed that both eye movements and
facial expressions could be helpful in video aﬀective content analysis and other
related applications. Yeasin et al. [29] employed the recognized facial expres-
sions to detect the emotional levels for diﬀerent videos. Rather than focusing on
subjects’ whole facial activity, Ong and Kameyama [19] analyzed the aﬀective
video content by calculating the viewers’ pupil sizes and gazing points. Katti et
al. [16] employed users’ pupillary dilation response for the task of the aﬀective
video summarization and the storyboard generation.

Implicit Aﬀective Video Tagging Using Pupillary Response
167
Although the pupillary response has been used to recognize the video emo-
tion in previous studies, the performance is not good enough. The ﬁrst rea-
son is that the issue of the pupil size diﬀerence between subjects has not been
solved so far. Pupillary changes during watching aﬀective pictures could reﬂect
human’s emotion state [5]. However, some subjects have larger pupil sizes than
others [26], thus absolute pupil diameter couldn’t accurately reﬂect the emo-
tional state of diﬀerent subjects. To address this issue, we ﬁrst deﬁne the pupil
diameter baseline (PDB) of each subject. Then, the absolute pupil diameter of
each subject can be converted to pupil dilation ratio based on his own PDB
value. Besides, no studies have used recurrent neural network to learn temporal
relations from pupillary response. Long short-term memory (LSTM) network is a
variant of recurrent neural network capable of learning temporal representations
from sequence data, but it requires enough training samples. Support vector
machine (SVM) has good generalization ability, even if the samples are limited.
But compared with LSTM, SVM couldn’t capture the temporal information of
the sequence data well. Therefore, under the condition of limited samples, we try
to combine the SVM and LSTM to improve the classiﬁcation accuracy. In our
method, the probabilistic SVM and LSTM are used to extract valuable informa-
tion and output the probability estimates based on the proposed global features
and sequence features, respectively. Finally, the ﬁnal decision is made by com-
bining the output probabilities from these two models based on the sum rule.
The results suggest that the probability estimates between SVM and LSTM are
complementary, and the classiﬁcation accuracy could be further improved after
we combine the probability estimates of SVM and LSTM.
2
Method
2.1
Preprocessing
In the preprocessing part, we ﬁrst exclude the missing samples of the pupil
diameter due to the eye blinking. Then, the average pupil diameter of the left
and the right pupil is deﬁned as the pupil diameter time-series data. Then,
we remove the pupil diameter time-series data from 0 to 2 s, which contains the
initial light reﬂex [5]. The initial light reﬂex generally appears within 2 s after the
video starts playing. As shown in Fig. 1, the red part of the pupillary response is
the initial light reﬂex. We can ﬁnd the pupil diameter decreases signiﬁcantly due
to the brightness of video clip. To eliminate the interference from the initial light
reﬂex, we remove this part from the pupil diameter time-series data. At last, a
local regression smoothing method named LOESS (Locally Weighted Scatterplot
Smoothing) [8] is applied to smooth the pupil diameter time-series data. Figure 2
shows an example of the original and the smoothed pupil diameter time-series
data.
2.2
Pupil Diameter Baseline Specifying
The issue of pupil size diﬀerence between subjects has never been eﬀectively
solved. To address this problem, we deﬁne the pupil diameter baseline of each

168
D. Gui et al.
0
1
2
3
4
5
6
Time (s)
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
3.6
3.7
Pupil Diameter (mm)
Fig. 1. An example of the pupil diameter time-series data in the ﬁrst 6 s. The red part
is the initial light reﬂex. We can ﬁnd the pupil diameter decreases signiﬁcantly due to
the high brightness of the video.
0
20
40
60
80
100
120
Time (s)
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
Pupil Diameter (mm)
Original Data
Smoothed Data
Fig. 2. An example of the original and the smoothed pupil diameter time-series data.
subject before feature extraction. In the standard emotional experiments, a col-
lection of neutral video clips was usually prepared in advance in each trial. And a
neutral video clip randomly selected from the collection was shown to the human
subject before each emotional video clip. Thus, the pupil diameter baseline of
the subject j is simply deﬁned as the average pupil diameter of this subject in
response to the neutral video clip:
PDBj = 1
n

v∈C
dv
j
(1)

Implicit Aﬀective Video Tagging Using Pupillary Response
169
where PDBj denotes the pupil diameter baseline of the subject j, n is the
number of video clips in the neutral video clip collection C, dv
j is the average
pupil diameter of the subject j in response to the video clip v in the neutral
video clip collection C.
After we construct the pupil diameter baseline of each subject, we could
use these baseline values to diminish the individual diﬀerence on the pupil size.
For each subject, we transform the absolute pupil diameter time-series data
(see Fig. 3a) to the pupil dilation ratio time-series data based on his own pupil
diameter baseline. (see Fig. 3b). Here, the pupil dilation ratio is the ratio of the
absolute pupil diameter to the pupil diameter baseline.
0
20
40
60
80
100
120
Time (s)
2.6
2.8
3
3.2
3.4
3.6
3.8
4
Pupil Diameter (mm)
Xs1
Xs2
(a) Pupil diameter time-series signals from two subjects
0
20
40
60
80
100
120
Time (s)
0.95
1
1.05
1.1
1.15
1.2
1.25
Pupil Dialtion Ratio
Ys1
Ys2
(b) Pupil dilation ratio time-series signals from two subjects
Fig. 3. (a) Xs1 and Xs2 represent the pupil diameter time-series from subject s1 and
s2 in response to the same video clip, respectively. Due to the pupil size diﬀerence
between subjects, the absolute pupil diameters of s1 and s2 ﬂuctuate around two very
diﬀerent values, even though s1 and s2 have the same emotional state. (b) Ys1 and Ys2
represent the corresponding pupil dilation ratio time-series of Xs1 and Xs2, respectively.
The PDB of s1 is 3.29 mm, and the PDB of s2 is 2.73 mm. Intuitively, after the absolute
pupil diameters of s1 and s2 are divided by their own PDB, the obtained pupil dilation
ratios of s1 and s2 ﬂuctuate around two very close values.

170
D. Gui et al.
2.3
Feature Extraction
A pupillary response signal can be represented as two diﬀerent feature types:
global feature and sequence feature. The global feature is used to train an SVM
classiﬁer, the sequence feature is used to train an LSTM classiﬁer (see Sect. 2.4).
Global Feature. Given a pupil dilation ratio time-series, some emotion related
features can be extracted from the whole time-series. Table 1 shows the features
extracted from the pupillary response signal. The time domain features include
the average value and the standard deviation of the pupillary response. The
average value can be used to distinguish diﬀerent arousal levels [5]. The frequency
domain features include the spectral power from four bands (0 Hz < f ≤0.2 Hz,
0.2 Hz < f ≤0.4 Hz, 0.4 Hz < f ≤0.6 Hz, 0.6 Hz < f ≤1 Hz), these spectral
features are related to the mental activity [24].
Sequence Feature. For the same pupil dilation ratio time-series, it can also be
represented as a sequence feature. A time window divides the time-series data
into segments with the length λ, and the segments are with 50% overlap. The
average shot length (ASL) of most ﬁlms is less than 10 s [3]. Hence, we set the
length λ to 10 s. Then, the features listed in Table 1 can be extracted from each
segment and combined as a sequence. Finally, the time-series data is represented
as a m×n sequence feature, where m is the sequence length, and n is the feature
dimension of each segment.
Table 1. The emotion-related features extracted from the pupillary response.
Domain
Extracted features
Time
Average, standard deviation
Frequency Spectral power in the following bands:
(0, 0.2]Hz, (0.2, 0.4]Hz, (0.4, 0.6]Hz,
(0.6, 1]Hz
2.4
Recognition Method
This section describes the proposed recognition method, which includes three
parts: (1) probabilistic support vector machine; (2) long short-term memory
network; (3) decision level fusion.
Probabilistic Support Vector Machine. Support vector machine is a pow-
erful algorithm, even if the training samples are limited. However, the output
of SVM cannot be directly used as a probability estimate of multi-class. For
this reason, Huang et al. [14] proposed a probabilistic SVM model, which could
output the probability estimate of multi-class.

Implicit Aﬀective Video Tagging Using Pupillary Response
171
Long Short-Term Memory Network. Long short-term memory network
is a type of recurrent neural network capable of learning order dependence in
sequence prediction problems [13]. To obtain the probability estimate of each
category, the softmax function is applied to the output layer.
Decision Level Fusion. In our method, the probabilistic SVM outputs the
probability estimate based on the global feature, and the LSTM network outputs
the probability estimate based on the sequence feature. The ﬁnal probability
estimate is then made by combining the output probabilities from the two models
with the sum rule [6]. For a given trial, the sum rule is deﬁned as follows:
gi =

m∈M Pm(ci|f)
K
i=1

m∈M Pm(ci|f)
=
1
|M|

m∈M
Pm(ci|f)
(2)
where M is the ensemble of the classiﬁcation models chosen for fusion, |M| is
the number of these models in M, and K is the number of classes. Pm(ci|f) is
the posterior probability of feature f belongs to class ci obtained by model m.
The ﬁnal decision is made by selecting the class ci with the highest gi.
In our work, (2) can be simpliﬁed as:
gi = 1
2(Psvm(ci|fv) + Plstm(ci|fs))
(3)
In (3), Psvm(ci|fv) is the posterior probability of the vector feature fv belongs
to class ci obtained by a learned probabilistic SVM, Plstm(ci|fs) is the posterior
probability of the sequence feature fs belongs to class ci by a learned LSTM
network. The class ci with the highest gi is the ﬁnal decision.
3
Experiments
3.1
Experimental Setting
In this paper, we evaluate the performance of the proposed method on the
MAHNOB-HCI database [24]. The MAHNOB-HCI database is a multimodal
database recorded in response to aﬀective stimuli with the purpose of implicit
video aﬀective tagging. The eye gaze data recorded from four participants (P3,
P16, P25, and P26) of the total 27 participants are not used in our experiment
due to the incompleteness of the data collection. After watching a video clip, all
participants reported their felt emotions as keywords feedback. In our experi-
ments, we use the responded keyword with the highest proportion of all partic-
ipants as the label of each corresponding video clip. As shown in Table 2, these
emotional labels can also be mapped into the commonly used two-dimensional
arousal-valence space, containing arousal and valence dimensions [10].
We compare the proposed method with the only existing method [24] which
used this standard dataset, and we use the classiﬁcation accuracy and F1 score
as the evaluation metrics. Leave-one-participant-out cross-validation is applied

172
D. Gui et al.
to validate the performance. The LibSVM [7] with RBF kernel is used as an
implementation of probabilistic SVM. A parameter selection tool included in
LibSVM automatically select the parameters C and γ for the RBF kernel. The
LSTM implementation of an open source neural network library Keras [1] is
used in our experiments. We use one hidden layer of 128 LSTM units. In order
to prevent overﬁtting of the network, the dropout fraction of the input is set to
0.3. The batch size for LSTM is set to 8, and the number of epochs is set to 100.
Table 2. The emotional labels and the arousal-valence dimension labels for the video
clips in MAHNOB-HCI database.
Emotional label Arousal
Valence
Video clips sources
Neutral
Calm
Neutral
AccuWeather New York, Dallas,
Detroit weather report
Sadness
Calm
Unpleasant Gangs of New York, the thin red
line, American history X
Disgust
Calm
Unpleasant Hannibal, ear worm
Joy
Medium
Pleasant
Mr. Bean’s holiday, love actually,
the thin red line
Amusement
Medium
Pleasant
Kill Bill VOL I, Mr. Bean’s holiday,
love actually, funny cats, funny
Fear
Activated Unpleasant The shining (2), silent hill
Anger
Activated Unpleasant Hannibal, ear worm
3.2
The Eﬀect of Individual Diﬀerence Reduction
Table 3 presents the average classiﬁcation accuracies of diﬀerent models with or
without individual diﬀerence reduction. Without individual diﬀerence reduction,
all features are extracted from the absolute pupil diameter time-series. With
individual diﬀerence reduction, all features are extracted from the pupil dilation
ratio time-series. From this table, we could ﬁnd that all models could obtain
better accuracies with the features extracted from the pupil dilation ratio time-
series data. Notably, the average accuracies with SVM are improved more than
9%, but the average accuracies with LSTM are only improved about 4%. The
possible reason for this result is that LSTM is not as sensitive as SVM to the
individual diﬀerence. Although the pupil size diﬀerence exists between subjects,
LSTM is still capable of learning the temporal representations from pupillary
responses. On the whole, the results demonstrate that the proposed method can
eﬀectively reduce the pupil size diﬀerence between subjects.
3.3
The Comparison of Classiﬁcation Results
In this section, we compare the proposed method with the only existing method
[24] which used this standard dataset. Table 4 presents the average classiﬁcation

Implicit Aﬀective Video Tagging Using Pupillary Response
173
Table 3. The average classiﬁcation accuracies of diﬀerent models with or without
individual diﬀerence reduction. Without individual diﬀerence reduction, all features
are extracted from the original pupil diameter time-series. With individual diﬀerence
reduction, all features are extracted from the pupil dilation ratio time-series.
Model
Without individual
diﬀerence reduction
With individual
diﬀerence reduction
Arousal Valence
Arousal Valence
SVM
60.0%
66.3%
69.3%
76.3%
LSTM
62.4%
66.1%
66.1%
70.4%
Decision level fusion 63.9%
69.1%
73.0%
78.5%
accuracies and F1 scores of diﬀerent methods. The results demonstrate that
our method could achieve better performance. In Fig. 4a, the average F1 scores
of diﬀerent arousal classes are shown. The proposed method obtains higher F1
scores for all the arousal classes. Besides, we can ﬁnd that the proposed method
signiﬁcantly improves the F1 score of the activated class. In general, most of
the participants have signiﬁcant pupil dilation when watching the climax of a
high arousal video. The proposed method is capable of capturing the temporal
information to recognize the activated emotions.
Table 4. The average classiﬁcation accuracies and the F1 scores of diﬀerent methods.
Method
Classiﬁcation accuracy Average F1
Arousal Valence
Arousal Valence
Existing method [24] 63.5%
68.8%
0.60
0.68
Proposed method
73.0%
78.5%
0.72
0.77
For the valence classes, our method could obtain higher F1 scores on the
unpleasant and pleasant classes. But we can also ﬁnd our method obtains a
slightly lower F1 score on the neutral category (see Fig. 4b). The bad performance
on the neutral class of the proposed method is primarily due to the sample
imbalance. The number of the neutral samples is only 15% of the total. Table 5
presents the average F1 scores of all valence classes obtained by diﬀerent models.
Compared with the classical shallow model SVM, as a deep learning model,
LSTM has more model parameters, which makes the training of it requires a large
number of labeled training samples. Therefore, the lack of the neutral training
samples leads to a low F1 score 0.19 of the neutral category. In future, we will
integrate the imbalance techniques into our method to address the imbalance
problem in implicit aﬀective video tagging. Despite LSTM has this limitation,
it is still good at extracting eﬀective information from a sequence. Hence, from
Table 5, we can ﬁnd the ﬁnal fusion result in most of the cases is better than the
respective output of SVM and LSTM.

174
D. Gui et al.
Calm
Medium arousal
Activated
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
F1 score
Existing method
Proposed method
(a) F1 scores of arousal classes
Unpleasant
Neutral
Pleasant
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
F1 score
Existing method
Proposed method
(b) F1 scores of valence classes
Fig. 4. The average F1 scores of diﬀerent arousal and valence classes.
Table 5. The average F1 scores of diﬀerent valence classes obtained by diﬀerent models
in the proposed method.
Model
Unpleasant Neutral Pleasant
SVM
0.80
0.79
0.69
LSTM
0.79
0.19
0.71
Decision level fusion 0.84
0.59
0.76
4
Conclusion
In this paper, we propose a novel method for implicit aﬀective video tagging
using pupillary response. Our method includes three parts. First, we construct
the pupil diameter baseline of each subject to reduce the individual diﬀerence
on pupil size. Then, the probabilistic SVM and LSTM network are used to
obtain the probability estimates based on the proposed global features and the
sequence features, respectively. Finally, the ﬁnal classiﬁcation decision is made
by combining the probability estimates from these two models. We evaluate our
method on a standard dataset MAHNOB-HCI. The experimental results show
that the proposed method is eﬀective to reduce the pupil size diﬀerence between
subjects. Compared with the existing method, our method could achieve better
classiﬁcation accuracy. Moreover, from our results, we can also ﬁnd the proba-
bility estimates between SVM and LSTM are complementary. By combining the
probability estimates of SVM and LSTM, the classiﬁcation accuracy could be
further improved.
In the future, we will investigate the computational emotion recognition using
multimodal physiological signals, such as EEG (electroencephalography).
Acknowledgements. This work was supported by the National Natural Science
Foundation of China (No. 61502311, No. 61672358), the (Key) Project of Depart-
ment of Education of Guangdong Province (No. 2014GKCG031, No. 12JGXM-MS29,

Implicit Aﬀective Video Tagging Using Pupillary Response
175
No.
2015SQXX0),
the
Natural
Science
Foundation
of
Guangdong
Province
(No. 2016A030310053), the Special Program for Applied Research on Super Com-
putation of the NSFC-Guangdong Joint Fund (the second phase) (No. U1501501), the
Shenzhen high-level overseas talents program, and the Tencent “Rhinoceros Birds” Sci-
entiﬁc Research Foundation for Young Teachers of Shenzhen University (2015, 2016).
References
1. Keras: an open source neural network library. http://keras.io
2. Acar, E., Hopfgartner, F., Albayrak, S.: Understanding aﬀective content of music
videos through learned representations. In: Gurrin, C., Hopfgartner, F., Hurst,
W., Johansen, H., Lee, H., O’Connor, N. (eds.) MMM 2014. LNCS, vol. 8325, pp.
303–314. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-04114-8 26
3. Baxter, M.: Notes on cinemetric data analysis (2014)
4. Belhumeur, P.N., Kriegman, D.J.: What is the set of images of an object under all
possible illumination conditions? Int. J. Comput. Vis. 28(3), 245–260 (1998)
5. Bradley, M.M., Miccoli, L., Escrig, M.A., Lang, P.J.: The pupil as a measure of emo-
tional arousal and autonomic activation. Psychophysiology 45(4), 602–607 (2008)
6. Chanel, G., Kierkels, J.J., Soleymani, M., Pun, T.: Short-term emotion assessment
in a recall paradigm. Int. J. Hum.-Comput. Stud. 67(8), 607–627 (2009)
7. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines. ACM
Trans. Intell. Syst. Technol. (TIST) 2(3), 1–27 (2011)
8. Cleveland, W.S.: Robust locally weighted regression and smoothing scatterplots.
J. Am. Stat. Assoc. 74(368), 829–836 (1979)
9. Fang, Y., Lin, W., Chen, Z., Tsai, C., Lin, C.: A video saliency detection model in
compressed domain. IEEE Trans. Circ. Syst. Video Technol. 24(1), 27–38 (2014)
10. Fontaine, J.R., Scherer, K.R., Roesch, E.B., Ellsworth, P.C.: The world of emotions
is not two-dimensional. Psychol. Sci. 18(12), 1050–1057 (2007)
11. Gajraj, R., Doi, M., Mantzaridis, H., Kenny, G.: Analysis of the EEG bispectrum,
auditory evoked potentials and the EEG power spectrum during repeated transi-
tions from consciousness to unconsciousness. Br. J. Anaesth. 80(1), 46–52 (1998)
12. Guggisberg, A.G., Hess, C.W., Mathis, J.: The signiﬁcance of the sympathetic
nervous system in the pathophysiology of periodic leg movements in sleep. Sleep
30(6), 755–766 (2007)
13. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
14. Huang, T., Weng, R.C., Lin, C.J.: Generalized Bradley-Terry models and multi-
class probability estimates. J. Mach. Learn. Res. 7(Jan), 85–115 (2006)
15. Iwasaki, M., Kellinghaus, C., Alexopoulos, A.V., Burgess, R.C., Kumar, A.N., Han,
Y.H., L¨uders, H.O., Leigh, R.J.: Eﬀects of eyelid closure, blinks, and eye movements
on the electroencephalogram. Clin. Neurophysiol. 116(4), 878–885 (2005)
16. Katti, H., Yadati, K., Kankanhalli, M., Tat-Seng, C.: Aﬀective video summarization
and story board generation using pupillary dilation and eye gaze. In: Proceedings
of the International Symposium on Multimedia, pp. 319–326. IEEE (2011)
17. Kreibig, S.D.: Autonomic nervous system activity in emotion: a review. Biol. Psy-
chol. 84(3), 394–421 (2010)
18. Lins, O.G., Picton, T.W., Berg, P., Scherg, M.: Ocular artifacts in EEG and event-
related potentials I: scalp topography. Brain Topogr. 6(1), 51–63 (1993)
19. Ong, K., Kameyama, W.: Classiﬁcation of video shots based on human aﬀect. J.
Inst. Image Inf. Telev. Eng. 63(6), 847–856 (2009)

176
D. Gui et al.
20. Poursaberi, A., Araabi, B.N.: Iris recognition for partially occluded images:
methodology and sensitivity analysis. EURASIP J. Appl. Sig. Process. 2007(1),
20 (2007)
21. Rainville, P., Bechara, A., Naqvi, N., Damasio, A.R.: Basic emotions are associated
with distinct patterns of cardiorespiratory activity. Int. J. Psychophysiol. 61(1),
5–18 (2006)
22. Robinson, B.F., Epstein, S.E., Beiser, G.D., Braunwald, E.: Control of heart rate
by the autonomic nervous system. Circ. Res. 19(2), 400–411 (1966)
23. Shao, L., Zhen, X., Tao, D., Li, X.: Spatio-temporal Laplacian pyramid coding for
action recognition. IEEE Trans. Cybern. 44(6), 817–827 (2014)
24. Soleymani, M., Lichtenauer, J., Pun, T., Pantic, M.: A multimodal database for
aﬀect recognition and implicit tagging. IEEE Trans. Aﬀect. Comput. 3(1), 42–55
(2012)
25. Tang, Y.Y., Ma, Y., Fan, Y., Feng, H., Wang, J., Feng, S., Lu, Q., Hu, B., Lin,
Y., Li, J., et al.: Central and autonomic nervous system interaction is altered by
short-term meditation. Proc. Natl. Acad. Sci. 106(22), 8865–8870 (2009)
26. Tsukahara, J.S., Harrison, T.L., Engle, R.W.: The relationship between baseline
pupil size and intelligence. Cogn. Psychol. 91, 109–123 (2016)
27. Wang, S., Ji, Q.: Video aﬀective content analysis: a survey of state-of-the-art meth-
ods. IEEE Trans. Aﬀect. Comput. 6(4), 410–430 (2015)
28. Wu, J., Zhong, S.H., Jiang, J., Yang, Y.: A novel clustering method for static video
summarization. Multimedia Tools Appl., 1–17 (2016)
29. Yeasin, M., Bullot, B., Sharma, R.: Recognition of facial expressions and mea-
surement of levels of interest from video. IEEE Trans. Multimedia 8(3), 500–508
(2006)
30. Zhao, S., Yao, H., Sun, X., Xu, P., Liu, X., Ji, R.: Video indexing and recommen-
dation based on aﬀective analysis of viewers. In: Proceedings of the 19th ACM
international conference on Multimedia, pp. 1473–1476. ACM (2011)
31. Zhu, Y., Huang, X., Huang, Q., Tian, Q.: Large-scale video copy retrieval with
temporal-concentration sift. Neurocomputing 187, 83–91 (2016)
32. Zhu, Y., Jiang, Z., Peng, J., Zhong, S.: Video aﬀective content analysis based on
protagonist via convolutional neural network. In: Chen, E., Gong, Y., Tie, Y. (eds.)
PCM 2016. LNCS, vol. 9916, pp. 170–180. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-48890-5 17

k-Labelsets for Multimedia Classiﬁcation
with Global and Local Label Correlation
Yan Yan, Shining Li(B), Xiao Zhang, Anyi Wang, Zhigang Li,
and Jingyu Zhang
School of Computer Science, Northwestern Polytechnical University, Xi’an, China
yanyan.nwpu@gmail.com, {lishining,lizhigang}@nwpu.edu.cn,
{zhang xiao,hay,zhangjingyu}@mail.nwpu.edu.cn
Abstract. Multimedia data, e.g., text and images, can be associated
with more than one label. Existing methods for multimedia data clas-
siﬁcation either consider label correlation globally by assuming that it
is shared by all the instances; or consider label correlations locally by
assuming that it is a pairwise label correlation and shared only in a local
group of instances. In fact, both global and local correlations may occur
in the real-world applications; and the label correlation cannot be con-
ﬁned to pairwise labels. In this paper, a novel and eﬀective multi-label
learning approach named GLkEL is proposed for multimedia data cate-
gorization. Brieﬂy, a H igh-Order Label Correlation Assessment strategy
named HOLCA is proposed by using approximated joint mutual infor-
mation; and then GLkEL, which breaks the original label set into several
of the most correlated and distinct combination of k labels (called k-
labELsets) according to the HOLCA strategy, learns Global and Local
label correlations simultaneously based on label correlation matrix. Com-
prehensive experiments across 8 data sets from diﬀerent multimedia
domains indicate that, it manifests competitive performance against
other well-established multi-label learning methods.
Keywords: Multimedia data · Global and local label correlations
Multi-label classiﬁcation
1
Introduction
With rich information presented in multimedia data, many classiﬁcation tasks
require to assign more than one label to each instance under real-world scenarios.
For example, an image can be annotated with many tags [1], a magazine may be
associated with multiple topics [2], a piece of music may belong to several genres
[3]. Thus, multi-label classiﬁcation problem, where each instance corresponds to
a set of class labels, has received an increasing attention in recent years [4].
Current studies on multi-label learning try to incorporate label correlation
of diﬀerent orders to facilitate the learning process [4]. However, the existing
approaches mostly concentrate on global label correlation by assuming that it is
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 177–188, 2018.
https://doi.org/10.1007/978-3-319-73600-6_16

178
Y. Yan et al.
shared by all the instances [1,5]. For example, ‘bird’ and ‘sky’ are highly corre-
lated labels, and so are ‘Yao Ming’ and ‘basketball star’. On the other hand, some
label correlations, which can be distinct in diﬀerent data groups, may be only
shared by a local data group [6,7]. For example, ‘Jordan’ is related to ‘shoe’ in
life magazines, but ‘basketball star’ in basketball magazines. ML-LOC [6], which
is the ﬁrst work trying to model local label correlations, demonstrates that label
correlations may be only shared by a local data group. It gets a satisfactory
performance by exploiting label correlations locally, while may suﬀer from the
fact that label correlations could not be conﬁned locally. The previous research
described above considers label correlation either from a global or local perspec-
tive, however, both of them may occur under real-world scenarios. So, taking
both global and local label correlations into consideration is more desirable and
practical.
Label correlations, which may change in diﬀerent locales, are hard to mine
manually. Several methods mine label correlations by label clustering based on
the assumption of label hierarchies [8]; or construct a Bayesian network model
based on the Bayesian assumption [9]. However, the hierarchical structure, which
is not a universal architecture, only exists in few real-world applications. Oth-
ers learn label dependency by the co-occurrence of labels in training data [10].
For example, GCC [7] estimates label correlations by the co-occurrence of each
pair of labels. It is eﬀective to some extent by considering pairwise label cor-
relations locally, while may suﬀer from the fact that the label correlations may
go beyond second-order. What’s worse, co-occurrence is less meaningful, since
there exists an intrinsic characteristic of multimedia data, i.e. the widely-existing
class-imbalance among labels.
In this paper, we propose a novel and eﬀective approach called ‘k-LabELsets
for Multimedia Classiﬁcation with Global and Local Label Correlation’ (GLkEL).
GLkEL tackles multi-label classiﬁcation problem with four simple steps. Firstly, a
H igh-Order Label Correlation Assessment strategy named HOLCA is proposed,
which can estimate the correlation among multiple labels based on approximated
joint mutual information. Secondly, GLkEL learns local label correlation for each
group respectively on the base of clustering, and then combines global label corre-
lation with it linearly. Thirdly, GLkEL exploits global and local label correlations
simultaneously based on the integration of label correlation above, through break-
ing the original label set into several of the most correlated and distinct combina-
tion of k labels (called k-labELsets) according to the HOLCA strategy in diﬀerent
groups; and then, employs Label Powerset (LP) to train the linear classiﬁer using
speciﬁc k-labelsets for each group. Lastly, the classiﬁer, which corresponds to the
nearest group for a test instance, is used to predict.
2
Related Works
During the last decade, many multi-label learning methods have been witnessed
[4]. The existing approaches, which mainly focus on exploiting label correlation,
can be grouped into three categories based on the degree of label correlation
used [9]:

k-Labelsets for Multimedia Classiﬁcation
179
First-order methods refer to addressing the problem of multi-label classiﬁca-
tion by breaking up it into several independent binary ones. For example, BR
[1] trains a classiﬁer for each label independently. Although simple and easy to
implement, these methods are not so eﬀective due to lack of label correlation.
Second-order methods deal with multi-label learning problem by taking rel-
evance between pairwise labels into account. For example, CLR [11] deals with
multi-label learning problem by the pairwise label ranking. Although be used
to some extent and relatively eﬀective, the correlation cannot be conﬁned to
pairwise labels under real-world scenarios.
High-order methods tackle multi-label learning problem by taking high-order
relation among labels into account. For example, RAkEL [5] transforms multi-
label learning problem into an ensemble of multi-class classiﬁcation problems,
where each multi-class classiﬁcation problem is generated by adopting the LP
[1] on a randomly selected k-labelset from label space. High-order methods could
address more realistic label correlations, while it may have high model complex-
ities.
All the studies mentioned above focus on learning label correlations globally.
However, ML-LOC [6] presents that label correlations may be local and only
shared by a local data group. Speciﬁcally, it exploits local label correlations by
augmenting the original features with a LOcal Correlation (LOC) code for each
instance. It performs well with considering local label correlations, while may
not get rid of the following drawbacks. Firstly, the LOC code may be less use-
ful for high-dimensional data. Secondly, ML-LOC only learns label correlations
locally, does not take global ones into account. GCC [7], which estimates pairwise
label dependency by using co-occurrence pattern, enhances the feature represen-
tation of each instance by embedding the pairwise label correlations code into
feature space; and then learns the label correlation graph locally in each group
by clustering. It has a certain advantage in some cases by taking pairwise local
label correlations into account, while may suﬀer from the following limitations.
Firstly, co-occurrence is meaningless, since few positive instances exist in real-
world scenes. Secondly, label correlations may be high-order and not be limited
locally.
It is easily seen that considering both global and local label correlations
simultaneously is more practical and beneﬁcial.
3
The Proposed Approach
In this section, details of the proposed approach GLkEL will be presented. First,
we propose a method named HOLCA to model high-order correlation among
labels, and then give the details of the GLkEL framework.
3.1
Preliminaries
In multi-label learning, an instance can be associated with more than one label.
Let D = Rn be the n-dimensional sample space and L = {l1, l2, · · ·, lq} be the

180
Y. Yan et al.
ﬁnite set of q possible labels. T = {(xi, Yi)
i = 1, 2, ···, d} denotes the multi-label
training set with d samples, where xi ∈D is a n-dimensional feature vector so
that xi = [x1
i , x2
i , · · ·, xn
i ]. We denote Y = [Y1, · · ·, Yd]T ∈{−1, 1}d×q as the
ground truth label matrix, where Yi = [li1, li2, · · ·, liq] is the set of label vector
associated with xi. Each element lij = 1 if the label lij is associated with xi,
otherwise lij = 0. k is the size of the labelsets for data set. g is the number of
clusters for one data set.
3.2
HOLCA Strategy
In this subsection, a simple but eﬀective H igh-Order Label Correlation
Assessment strategy named HOLCA is proposed, which evaluates high-order
label correlation based on approximated joint mutual information. The target
of label correlation estimate strategy is to select the most correlated k-labelset
S from label space L. The S with k labels {l1, l2, · · ·, lk} can be chosen from
L through maximizing the Interaction Information shared by all k labels. The
process is deﬁned as:
arg max
S⊆L
I(S) ≡arg max
S⊆L
I(l1; l2; · · ·; lk) = arg max
S⊆L

Γ ⊆S
−(−1)|S|−|Γ |H(Γ)
(1)
It is an alternating (inclusion-exclusion) sum over all the subsets Γ ⊆S, where
|S| = k.
However, direct calculation of Eq. (1) is impractical, and an exhaustive strat-
egy has an unacceptable time complexity when k is large. To address the issue, a
simple iterative greedy strategy is adopted: given a k-labelset S of k - 1 selected
labels {l1, l2, · · ·, lk−1}, the next label lk is chosen by maximizing Eq. (2) in an
incremental fashion, as follows:
arg max
lkϵL\S
I(lk; S)
(2)
Simply, let us describe the approach for choosing the most correlated label
li from L, so that it maximizes the mutual information (MI ) between li and
S, I(li; S) = H(li) + H(S) −H(li; S), where H(x) = −
x
p(x) log(p(x)) is the
entropy of x. When S = {l1, l2} has two labels, I(li; S) can be computed accord-
ing to Eq. (3),
I(li; S) = I(li; l1, l2) = H(li) + H(l1, l2) −H(li, l1, l2)
(3)
It is intractable to calculate H(x) accurately when x = (x1, x2, · · ·, xn) is a
multivariate discrete random variable. To estimate H(x) eﬀectively, we approx-
imate it by Shearer’s inequality [12], H(li, l1, l2) ≤
1
2(H(li, l1) + H(li, l2) +
H(l1, l2)); and then we can describe the lower bound of I(fi; S) as follows,
I(li; S) = I(li; l1, l2) ≥1
2(I(li; l1) + I(li; l2)) + θ,
(4)

k-Labelsets for Multimedia Classiﬁcation
181
where θ = −1
2I(l1; l2) is a constant w.r.t li. It is easily seen that the lower bound
of I(li; S) has the same variation tendency with the correlation between the
candidate label and the selected labels. The correlation between li and S can be
approximated as Eq. (5) according to Eq. (4),
ˆI(li; S) =

l∈S,li∈L\S
I(li; l)
(5)
After li with highest ˆI(li; S) is chosen as the most correlated label with S,
the next label li+1 is chosen by maximizing MI when it is included. Using the
approximation in Eq. (5) for the term, we can get the following estimate,
J(li+1) = ˆI(li+1; li, S) =

l∈S,li+1∈L\(S,li)
I(li+1; l) + I(li+1; li),
(6)
The label li+1 is chosen as subsequent labels by maximizing J(li+1). Suppose
a labelset S with i labels {l1, l2, · · ·, li} has been chosen, then we select the next
label li+1 in an incremental fashion by optimizing the following formula,
arg max
li+1∈L\S

l∈S
I(li+1; l)
(7)
3.3
GLkEL Framework
Cluster the data into groups: To exploit label correlations locally, we cluster
the data set into diﬀerent groups. Suppose that multi-label training dataset T can
be decomposed into g groups {T1, ···, Tg}, where Tm ∈Rdm×n has dm instances.
In our experiments, we simply choose kmeans as the clustering method, and the
similarity is calculated by Euclidean distance. Since instances from distinct data
groups may share diﬀerent label correlations, so the local label correlations may
vary from group T1 to Tg. Moreover, both local and global label correlations,
which may occur in real-world applications, will be learned simultaneously in
the next step.
Learn global and local label correlations: It is well known that exploit-
ing label correlations is an essential ingredient in multi-label learning. Let
M0 = [Mij] ∈Rq×q be the global label correlation matrix of Y . GLkEL exploits
high-order global label correlation using the matrix M0, through partitioning L
into α = ⌈q/k⌉most correlated and disjoint k-labelset Rj according to HOLCA
strategy, where j = 1 · · · α, α
j=1Rj = ∅.
As mentioned in the cluster stage, label correlations may vary from group T1
to Tg. Let Ym be the label submatrix in Y corresponding to Tm, and Mm ∈Rq×q
is the local label correlation matrix of group m. Similar to global label correlation,
we learn high-order local label correlations according to the same strategy by
using the corresponding matrix Mm.
By combining global and local label correlation matrix, we have the Eq. (8)
as follows,

182
Y. Yan et al.
ˆ
Mm = λ1M0 + λ2Mm,
(8)
where m = 1 · · · g, and λ1, λ2 are trade-oﬀparameters for controlling the weight
between global and local label correlations.
Then, we can learn global and local label correlation structures simultane-
ously, through choosing α k-labelsets according to HOLCA strategy using the
corresponding matrix { ˆ
Mm}g
m=1 which is a linear combination of label correla-
tion matrices using Eq. (8) for each group. Namely, g label correlation structures
({Gi}g
i=1) can be obtained.
Build the multi-label classiﬁer: In this stage, we build g multi-label classi-
ﬁcation models (Hi(x)g
i=1) based on the corresponding learned label correlation
structure Gi and the training subset Ti.
Hi(x) = {hi1(x), hi2(x), · · ·, hiα(x)}
(9)
Each binary classiﬁer hiˆα(x), which confronts a single-label classiﬁcation job
having the label values (namely, all the k-labelset of Rj found in Ti), is learned
by using LP. The training set for hiˆα(x), denoted as ti, only contains a group
of the training set associated with the intersection of their original annotations
and Rj: ti = {(xi, Yi
 Rj)
i = 1, 2, · · ·, di}.
It is noted that the label correlation structure {Gi}g
i=1 is constructed by α
distinct k-labelsets according to HOLCA strategy, which means that multi-label
classiﬁcation model {Hi(x)}g
i=1 for all the labels need to be trained only once.
Predict: Intuitively, similar instances may have similar labels. So, we ﬁnd the
nearest group Ti of the test instance xt by computing Euclidean distance. Sup-
pose that xt shares similar label dependency with the instances in group Ti.
Classiﬁcation model Hi(x) is learned by the label correlation structure Gi which
is composed of the local label correlations on group Ti and global label correla-
tion. Hi(x) is used to predict xt, and we expect that Hi(x) works better than
other classiﬁcation models which are learned on the group-speciﬁc local label
correlations and global label correlation simultaneously.
4
Experiments
4.1
Data Sets
In order to evaluate the eﬀectiveness of the proposed multi-label learning app-
roach, 8 real-world multimedia data sets have been adopted in this paper. For
each data set D = {(Xi, Yi)
1 ⩽i ⩽p}, we use
D
, dim(D), L(D) and F(D)
to denote the number of examples, number of features, number of possible class
labels, and feature type for D respectively. Moreover, some other multi-label
properities [4] are denoted as follows:

k-Labelsets for Multimedia Classiﬁcation
183
– LCard(D) =
1
p
t
i=1
Yi
: label cardinality measures the average number of
labels for each instance;
– LDen(D) = LCard(D)
L(D)
: label density normalizes LCard(D) via the size of label
space;
– DL(D) =
{Y
(X, Y ) ∈D}
: distinct label sets counts the size of distinct label
combinations in D;
– PDL(D) = DL(D)
|D|
: proportion of distinct label sets which normalizes DL(D)
by the size of instances.
Table 1 summarizes some detailed characteristics of multimedia data sets
adapted in our experiments. The 8 data sets, which are ordered by
D
, are
chosen from four diﬀerent application areas, such as text, images, audio and
music. Therefore, comprehensive data sets with a broad range of multimedia
properties are used in our experiments. For each dataset, we randomly select
90% of the instances for training, and the rest for testing.
Table 1. Characteristics of the experimental data sets
Data set
|D|
dim(D) L(D)
F (D)
LCard(D) LDen(D) DL(D) P DL(D) Domain URL∗
Flags
194
10
7
Numeric
3.392
0.485
54
0.278
Images
URL 1
Emotion
593
72
19
Numeric
1.869
0.311
27
0.046
Music
URL 1
Birds
645
258
6
Numeric
1.014
0.053
133
0.206
Audio
URL 1
Enron
1702
1001
53
Nominal
3.378
0.064
753
0.442
Text
URL 2
Image
2000
294
5
Numeric
1.236
0.247
20
0.010
Images
URL 2
Scene
2407
294
6
Numeric
1.074
0.179
15
0.006
Images
URL 1
Rev1 (subset 1) 6000
944
101
Numeric
2.880
0.029
1028
0.171
Text
URL 1
Bibtex
7395
1836
159
Nominal
2.402
0.015
2856
0.386
Text
URL 1
∗URL 1: http://mulan.sourceforge.net/datasets.html
URL 2: http://cse.seu.edu.cn/people/zhangml/Resources.htm#data
4.2
Performance Evaluation
To evaluate the performance of multi-label learning algorithms eﬀectively, four
popular evaluation metrics in multi-label learning are adopted in this paper.
Let τ = {(xi, Yi)
1 ⩽k ⩽t} be a multi-label testing set, where Yi ∈{0, 1}q is
the ground truth labels of the ith instance, and ˆYi is the predicted labels. Let
{f1, f2, · · ·, fq} be a group of q learned functions, and P +
j , N −
j
be the sets of
positive and negative instances belonging to the jth label.
– Average AUC:
avgauc = 1
q
q
j=1
 ˆ
Pj

P +
j
P −
j

where ˆPj = {(i
′, i
′′)
fj(xi′ ) ≥fj(xi′′), (i
′, i
′′) ∈P +
j × N −
j }. Average AUC eval-
uates the quality of the prediction for each class label pairs and returns the
averaged value across all the class labels.

184
Y. Yan et al.
– Macro F1:
macroF1 = 1
q
q
i=1
2piri
pi+ri
where pi and ri are the precision and recall for the ith label. MacroF1 is deﬁned
as the harmonic mean of precision and recall.
– Coverage:
coverage = 1
q

1
t
t
i=1
max
Lk∈Y rank(xi, Lk) −1

where rank(xi, Lk) =
q
j=1
τ (fj(xi) ≥fk(xi)) returns the sequence of Lk when all
class labels in Y are ranked by descending order according to {f1(xi), f2(xi), · ·
·, fq(xi)}. Coverage evaluates the average depth to move down the ranked label
list so as to cover all the possible labels of instance.
– Accuracy:
accuracy = 1
t
t
i=1
Yi∧ˆYi

Yi∨ˆYi

Accuracy evaluates Jaccard similarity between the ground truth labels Yi and
the predicted labels ˆYi.
It is noted that Coverage metric is normalized by the number of possible
class labels (i.e. q) in this paper; and the values of the four multi-label metrics
vary in [0, 1]. For Average AUC, Macro F1, Accuracy, the larger value the better
performance; While for Coverage, the smaller value the better performance.
4.3
Compared Methods
To validate the eﬀectiveness of the proposed approach, we compare GLkEL
against the following state-of-art multi-label learning algorithms:
1. BR [1], which is a ﬁrst-order method, trains a binary classiﬁer for each label
independently;
2. RAkEL [5], which is a high-order method, transforms multi-label learning
problem into several multi-class learning problems by exploiting high-order
global label correlation;
3. ML-LOC [6], which is a high-order method, exploits label correlations locally
by embedding them into instances’ feature space;
4. GCC [7], which is a second-order method, exploits pairwise label correlations
locally by learning a local label dependency graph.
Note that BR does not consider label correlation. RAkEL only takes high-
order label correlation globally into account. Both ML-LOC and GCC only
consider local label correlations.
For simplicity, we set λ1 = 1 and k = 3 in GLkEL. The other parameters, as
well as the compared methods, are chosen by ten-fold cross validation (10-CV)
on the training set.

k-Labelsets for Multimedia Classiﬁcation
185
In order to ensure the fair comparison, LIBSVM [13] is adopted as the base
learner for classiﬁer induction to instantiate GLkEL, BR, RAkEL, ML-LOC,
GCC. 3 is set to the size of labelsets k and distinct fashion is used in RAkEL.
All the methods are performed using Matlab.
Table 2. Experimental result of each comparison algorithm (mean±std. deviation) on
the eight multimedia data sets
Evaluation criterion Algorithm
ﬂags
emotion
birds
enron
image
scene
rev1-s1
bibtex
Average AUC ↑
Glkel
0.707±0.016 0.762±0.009 0.755±0.012 0.734±0.002 0.711±0.007 0.835±0.007 0.688±0.002
0.673±0.002
Br
0.581±0.006
0.699±0.008
0.581±0.006
0.732±0.003
0.608±0.007
0.785±0.003
0.603±0.002
0.656±0.001
Rakel
0.678±0.010
0.754±0.012
0.677±0.010
0.733±0.004
0.700±0.005
0.797±0.003
0.683±0.002
0.683±0.002
Ml-loc
0.644±0.004
0.755±0.086
0.707±0.036 0.734±0.002 0.804±0.045 0.787±0.082
0.632±0.003
0.696±0.005
Gcc
0.706±0.004
0.735±0.031
0.752±0.000 0.734±0.031 0.701±0.061
0.792±0.061 0.871±0.005 0.888±0.007
Macro F1 ↑
Glkel
0.530±0.020 0.651±0.016 0.407±0.018 0.193±0.007
0.550±0.007 0.742±0.011 0.237±0.007
0.309±0.006
Br
0.117±0.009
0.526±0.124
0.117±0.009
0.177±0.003
0.442±0.011
0.667±0.005
0.236±0.006
0.305±0.000
Rakel
0.288±0.016
0.643±0.018
0.288±0.016
0.190±0.005
0.538±0.007
0.686±0.007
0.237±0.006
0.309±0.006
Ml-loc
0.288±0.038
0.634±0.049
0.388±0.034 0.505±0.017 0.545±0.055
0.712±0.030 0.298±0.005 0.366±0.008
Gcc
0.016±0.010
0.621±0.031
0.250±0.000
0.137±0.013 0.551±0.048 0.691±0.048
0.236±0.004
0.247±0.010
Coverage ↓
Glkel
0.029±0.001 0.387±0.009 0.161±0.012 0.570±0.008 0.054±0.002 0.028±0.001 0.429±0.005
0.457±0.003
Br
0.276±0.014
0.435±0.012
0.276±0.161
0.573±0.006
0.282±0.013
0.170±0.005
0.442±0.004
0.473±0.005
Rakel
0.228±0.013
0.393±0.120
0.228±0.132
0.577±0.007
0.285±0.012
0.158±0.004
0.442±0.003
0.458±0.003
Ml-loc
0.257±0.023
0.390±2.959
0.204±0.034
0.571±0.006
0.271±0.012
0.068±0.006
0.430±0.010
0.464±0.003
Gcc
0.185±0.012
0.394±2.959
0.173±0.026 0.226±0.020 0.106±0.013
0.088±0.009 0.190±0.005 0.138±0.008
Accuracy ↑
Glkel
0.540±0.025 0.553±0.014 0.222±0.011 0.426±0.006 0.520±0.011 0.725±0.013 0.303±0.005 0.329±0.005
Br
0.073±0.005
0.422±0.016
0.073±0.005
0.382±0.006
0.498±0.013
0.575±0.006
0.285±0.004
0.323±0.002
Rakel
0.171±0.008
0.540±0.020
0.171±0.008
0.387±0.006
0.512±0.009
0.632±0.007
0.291±0.003
0.322±0.002
Ml-loc
0.204±0.045
0.533±0.043
0.188±0.049
0.135±0.015
0.519±0.047
0.647±0.005
0.294±0.003
0.320±0.004
Gcc
0.157±0.033
0.481±0.041
0.158±0.033
0.161±0.026 0.524±0.054 0.594±0.043
0.297±0.030
0.323±0.006
4.4
Experiment Results and Analysis
In this paper, various multimedia data sets shown in Table 1, such as images,
audio, music and text, are adopted to evaluate the eﬀectiveness of GLkEL. Per-
formance on the multimedia data sets is shown in Table 2, which demonstrates
the detailed experimental results about the comparison of the proposed approach
with other four multi-label learning approaches on the 8 data sets. For each eval-
uation measure, “↓” denotes “the smaller, the better” while “↑” denotes “the
larger, the better”. In addition, the best performance among the ﬁve compared
algorithms is highlighted in boldface.
Across all the 32 conﬁgurations (i.e. 8 data sets × 4 criteria as shown in
Table 2), GLkEL ranks in 1st place among the ﬁve comparison algorithms at
65.6% cases, in 2nd place at 31.3% cases, and only 3.1% cases rank after 2nd.
By comparing with the most popular multi-label learning methods (BR,
RAkEL, ML-LOC, and GCC), we can see that BR is the worst of all, since
it does take label correlations into account. Compared with RAkEL which is a
degenerated version of GLkEL without considering label correlations eﬀectively,
GLkEL is always superior to it for all the four evaluation measures under each
dataset. We can see that GLkEL gets the satisfactory predictive results on most
of the multimedia data sets, which signiﬁes that the performance of classiﬁer can
be improved by exploiting label correlations; and the methods (GLkEL, ML-LOC
and GCC) which consider label correlations locally get the better performance
to some extent than the method (RAkEL) which exploits label correlation glob-
ally. It indicates that label correlations may be only shared in a local group, and

186
Y. Yan et al.
exploiting the group-speciﬁc label correlations can perform better. It is worth to
mention that GLkEL performs better than ML-LOC and GCC in most cases,
as it models high-order label correlation and takes both global and local label
correlations into account.
4.5
Sensitivity to Parameters
To evaluate the inﬂuence of parameters, we study the number of clusters g and
the weight of local label correlation parameter λ2 in our experiment. We vary
one parameter, while maintaining the other ﬁxed at its best set. Due to the page
limit, we only report the results on the scene dataset in Figs. 1 and 2, whereas
others get similar results.
Inﬂuence of Parameter g (The Number of Clusters)
Figure 1 shows the inﬂuence on the scene dataset. When there is only one cluster,
global label correlation is considered. With the number of clusters g becomes
larger, the performance of GLkEL improves as more local label correlations are
considered. When overmuch clusters are adopted, local label correlations, which
cannot be estimated reliably by few instances contained in each group, may be
less meaningful. Thus, the performance of GLkEL begins to decrease.
(a) Average AUC
(b) Macro F1
(c) Coverage
(d) Accuracy
Fig. 1. Varying the number of clusters g on the scene dataset
Inﬂuence of Label Correlation Parameter λ2
A larger λ2 means higher weight of local label correlation. Figure 2 shows the
inﬂuence on the scene dataset. When λ2 = 0 which means that only global label
correlation is considered, the performance is poor. With the increase of λ2, the
(a) Average AUC
(b) Macro F1
(c) Coverage
(d) Accuracy
Fig. 2. Varying the local label correlation parameter λ2 on the scene dataset

k-Labelsets for Multimedia Classiﬁcation
187
performance improves as the local label correlation joins and works gradually.
However, when λ2 is too large, performance deteriorates as the local label cor-
relations dominate.
5
Conclusion
In multimedia classiﬁcation tasks, both global and local label correlations may
occur under real-world scenarios. Contrasting to the existing methods which
either consider label correlation globally or locally, the paper proposes a novel
and eﬀective multi-label learning approach GLkEL for multimedia data cat-
egorization by exploiting both global and local label correlations simultane-
ously. Compared with the previous studies, it is eﬀective to exploit both global
and local label correlations in multimedia classiﬁcation, which learns high-order
label correlations by using a simple but eﬀective H igh-Order Label Correlation
Assessment strategy HOLCA. Experimental results show that GLkEL performs
better than the state of art multi-label learning methods which do not consider
label correlation and only exploit label correlation globally or locally. In our
work, we handle the case that label correlation is positive without considering
negative correlation. In many situations, the labels can be mutually exclusive
with each other, e.g., label “sea” and “train”, label “ship” and “car”. So it is
desirable to take negative label correlation into account in our future work.
Acknowledgments. This work was supported by R&D program of Shannxi Province
Grant No. 2017ZDXM-GY-018, National Key Technologies R&D Program of China
Grant No. 2014BAH14F01.
References
1. Boutell, M.R., Luo, J., Shen, X.P., Brown, C.M.: Learning multi-label scene clas-
siﬁcation. Pattern Recogn. 37(9), 1757–1771 (2004)
2. Ueda, N., Saito, K.: Parametric mixture models for multi-label text. In: Becker,
S., Thrun, S., Obermayer, K. (eds.) Advances in Neural Information Processing
Systems, vol. 15, pp. 721–728. MIT Press, Cambridge (2003)
3. Turnbull, D., Barrington, L., Torres, D., Lanckriet, G.: Semantic annotation and
retrieval of music and sound eﬀects. TASLP 16(2), 467–476 (2008)
4. Zhang, M.L., Zhou, Z.H.: A review on multi-label learning algorithms. IEEE Trans.
Knowl. Data Eng. 26(8), 1819–1837 (2014)
5. Tsoumakas, G., Katakis, I., Vlahavas, I.: Random k-labelsets for multi-label clas-
siﬁcation. IEEE Trans. Knowl. Discov. Data Eng. 23(7), 1079–1089 (2010b)
6. Huang, S.J., Zhou, Z.H., Zhou, Z.H.: Multi-label learning by exploiting label cor-
relations locally. In: AAAI (2012)
7. Huang, J., et al.: Group sensitive classiﬁer chains for multi-label classiﬁcation. In:
2015 IEEE International Conference on Multimedia and Expo, pp. 1–6 (2015)
8. Punera, K., Rajan, S., Ghosh, J.: Automatically learning document taxonomies for
hierarchical classiﬁcation. In: Proceeding WWW 2005, Special Interest Tracks and
Posters of the 14th International Conference on World Wide Web, pp. 1010–1011
(2005)

188
Y. Yan et al.
9. Zhang, M.L., Zhang, K.: Multi-label learning by exploiting label dependency. In:
Proceedings of the 16th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 999–1008 (2010)
10. Gibaja, E., Ventura, S.: Multi-label learning: a review of the state of the art and
ongoing research. Wiley Interdisc. Rev. Data Min. Knowl. Discov. 4(6), 411–444
(2014)
11. F¨urnkranz, J., H¨ullermeier, E., Loza Menc´ıa, E., Brinker, K.: Multilabel classiﬁca-
tion via calibrated label ranking. Mach. Learn. 73(2), 133–153 (2008)
12. Chung, F.R.K., Frankl, P., Graham, R.L., Shearer, J.B.: Some intersection theo-
rems for ordered sets and graphs. J. Comb. Theory Ser. A 43, 23–37 (1986)
13. Chang, C.C., Lin, C.J.: LIBSVM: A library for support vector machines. ACM
Trans. Intell. Syst. Technol. (TIST) 2, 27:1–27:27 (2011)

LVFS: A Lightweight Video Storage File System
for IP Camera-Based Surveillance Applications
Chong Wang1, Ke Zhou1(B), Zhongying Niu2, Ronglei Wei1, and Hongwei Li1
1 Wuhan National Laboratory for Optoelectronics, School of Computer,
Huazhong University of Science and Technology, Wuhan, China
{c wang,k.zhou,leeawei,hust wrl}@hust.edu.cn
2 Beijing Computer Institute of Technology and Application, Beijing, China
niuzhy@163.com
Abstract. Surveillance video data are characterized by a high-volume
write-oriented workload and cyclic use of storage space at full capacity.
Besides, the video data needs indexing to support accurate-query. Data
archiving for such applications, however, is complicated by the increas-
ing demands of higher-resolution cameras and longer video-retention
times. Due to the constant data steaming and nearly 100% write activ-
ity, general-purpose ﬁle systems will not suﬃce for present purposes.
Thus, we specially design and implement a lightweight video storage
ﬁle system (LVFS) for IP camera-based surveillance applications. LVFS
provides a recycled storage platform to meet the retention requirement
of surveillance applications, and delivers high performance for concur-
rent stream data uploading from multiple cameras and accurate data
retrieval. Results of a multi-workload experiment show that LVFS is
able to archive 40 HD or 16 full-HD cameras for an individual hard disk
while processing nearly constant time queries, indicating that LVFS suc-
cessfully ﬁts the system requirement and performs better than general-
purpose ﬁle system.
Keywords: Video surveillance · Storage system · File system
IP cameras
1
Introduction
Video surveillance [1] has become a security tool [11,16] with growing popular-
ity for years. It has constituted an ubiquitous aspect of the modern urban land-
scape, situated in varieties of environments [13] including airports, train stations,
schools, and some other public or private places. These systems typically consist
of a large number of cameras, which can either be active or static, transmit-
ting real-time video streams to underlying storage systems. With the explosive
growth of the monitoring data [9], storage systems for video data are becom-
ing a major challenge when customers demand more cameras, higher-resolution
frames and longer retention times.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 189–199, 2018.
https://doi.org/10.1007/978-3-319-73600-6_17

190
C. Wang et al.
To archive data for surveillance systems, most video applications store video
data on general-purpose ﬁle system [15], such as Ext2, Ext4. Video data are
commonly split into smaller ﬁles, with ﬁxed time range or data size. However,
from the storage system’s perspective, the characteristics of video data in surveil-
lance applications diﬀer substantially from those of normal business data. The
workloads of video surveillance are write-dominated and mainly sequential [18].
Consequently, the general-purpose ﬁle system cannot retain the storage access
pattern well. Speciﬁcally, there are three reasons.
First, a video surveillance storage system must receive new data at high rate.
The visual surveillance platforms typically consist of a large number of cameras,
which concurrently upload data to the underlying storage devices continuously.
The storage device needs to record each stream at high rate without dropping
frames. For each camera, the workloads are dependent on resolution, frame rate,
and frame size. For instance, the recommended transfer rate for a HD (High
Deﬁnition) camera is 2 MB/s. In long-term operation, the overhead of writing
to a ﬁle system is larger than in the case of a raw disk. Furthermore, designed
for common application, general-purpose ﬁle systems have to maintain complex
metadata information, which may be useless for video surveillance applications.
Second, in a streaming storage system, storage generally runs at full capacity,
recording until the disks are full, and then being overwritten in a circular mode.
As a policy, most organizations store surveillance video for at least 30 days and
many for 90 days or longer. Out of the retention time, however, the data make
less sense. Then, it needs to remove the oldest data to free up space for new data.
This calls for a mechanism for reclaiming and reusing storage due to the limited
storage space. As a consequence, a large amount of reclaiming operations will be
needed to free up old space for new ﬁles, leading to growing fragmentation in ﬁle
system and decreased writing performance. Moreover, much poorer performance
will result from the combined eﬀect of complex metadata maintained by the
general-purpose ﬁle systems.
Last but not least, data indexing for video surveillance data poses a unique
challenge. Since the archive data are stored for future or immediate use, it is
sometime necessary to query data for prooﬁng investigation and tracking events.
Therefore, besides archiving data in real-time, these systems also need to index
data at the same rates. For visual surveillance, time and geospatial location are
major factors in event querying. Most of the existing systems store the video
data in standard ﬁles (JPEG, MPEG, etc.) [15], which is not eﬃcient to search
the content of the data in speciﬁc moments. To deal with video ﬁles which are
much larger and demand a longer retention time, timestamp-based indexing [5]
rather than the stream itself is necessary for accurate data retrieval [17]. This is
in direct contrast to a general-purpose ﬁle system, where video ﬁles are named by
the starting and ending time, or even associated with timestamps in a database
[3] for the sake of accurate data retrieval. Therefore, when storing recorded video
on a hard disk, it is important to have a well-organized structure for video data
and maintain a timestamp-based index.

LVFS: A Lightweight Video Storage File System
191
Accordingly, conventional ﬁle servers and associated storage systems are not
well suited to support real-time video surveillance; instead, appropriative storage
systems which are capable of handling the speciﬁc characteristics of video data
stream are needed. For this reason, we design and implement LVFS, a speciﬁc
storage system for video surveillance applications. The architecture of LVFS is
shown in Fig. 1. In LVFS, video streams are stored as Record Segments, and
managed in Record Volume, Logical Volume hierarchically. Due to the limited
storage space, LVFS is running on a recycled storage model. Besides archiving
data in real-time, LVFS also generates timestamp-based indexing at the same
time. In sum, the three major contributions in the paper are: (1) A lightweight
ﬁle system is designed for large-scale video data. It has a well-organized structure
for video, and manages data directly on the raw disks. (2) The ﬁle system exhibits
a large degree of parallelism for handling intensive stream traﬃc generated by a
number of high-resolution cameras and producing query results within a given
time bound in case of incident occurrences. (3) It implements speciﬁc interfaces
and provides a software development kit (SDK) both in Windows and Linux to
support extension applications for video surveillance systems.
Fig. 1. Architecture of LVFS
The remainder work is organized as follows. Related works are reviewed in
Sect. 2. Section 3 presents the design and implementation of LVFS. Experimen-
talresults are given in Sect. 4. Finally, Sect. 5 concludes the paper.
2
Related Work
The increasing security concerns from society leads to a growing need for video
surveillance applications, such as security surveillance or remote monitoring,

192
C. Wang et al.
accident detection [11,16], etc. Over the past three decades, Video-based surveil-
lance systems have evolved in three generations [1] and shifted from analog
CCTV surveillance to fully digital, network-based video surveillance systems.
Unlike analog video systems, in a network video application, digitized video
and audio streams are sent over wired or wireless IP networks, enabling video
monitoring and recoding from anywhere on the network.
Large-scale video surveillance platform is an emerging research area. Due
to the on-demand data outsourcing service model, cloud storage is proposed in
video surveillance systems [2,6–8,12,14]. In cloud-based surveillance, the storage
infrastructure is provided by the cloud vendor on a pay as you go basis. Such
systems, known as Video Surveillance as a Service (VSaaS) [12], has been inves-
tigated from several aspects, such as cloud-adopted architectures [2,12,14] or
platforms [7], resource allocation [8], and the suitability [6], etc. Cloud storage,
although being elastic and having lower maintenance cost, is not application-
oriented, simply for the reason that underlying stored data is transparent.
Most existing multimedia systems store video data in ﬁles, directly on ﬁle
systems. Because of the sequential nature, log-structured ﬁle system is recom-
mended in such high-volume stream storage. Adopting the log-structured ﬁle
system, Hyperion [4] is a stream archiving system, specially designed for network
monitoring. In an energy-eﬃcient storage, NILFS [10] ﬁle system is eligible for
video surveillance according to the sequential access pattern of video surveillance
[18]. However, in NILFS ﬁle system, garbage collection is an obvious obstacle
because of b-tree structured directory.
3
System Design and Implementation
Focusing on the reasons outlined in the previous sections, our design goals are:
to record each video stream without loss, guarantee system performance on the
premise of re-use storage at full capacity, and maintain timestamp-based index
for ﬁne-grained time ranged data queries. In this section, we ﬁrst discuss the
recycled storage mode, then describe LVFS, including the layout of the disks,
data organizations, and the interfaces.
3.1
Recycled Storage
According to [4], there are three major re-use modes:
First, the system creates a single large ﬁle for each corresponding camera
and write in append mode, as illustrated in Fig. 2(a). When the storage space is
insuﬃcient, the oldest segments are overwritten by the new data. Performance
of this method is poor because of the reiterative track seeking, aroused by the
multiple simultaneous streams. Moreover, when creating a new ﬁle, it has to
allocate enough space, which depends on the stream rate and retention time.
Second, the system splits video data into smaller ﬁles with ﬁxed size, rotated
in an interleaved fashion, as shown in Fig. 2(b). It is a ﬂexible approach both
in storage allocation and reclamation in comparison with the previous method.

LVFS: A Lightweight Video Storage File System
193
When a ﬁle reaches the given size, the new data are altered to the next corre-
sponding ﬁle; then, the oldest ﬁles are deleted to free space for new data.
Third, from the sequential write perspective, log-structured ﬁle system is
appropriate. Storage is sequentially allocated in the data arrival order, to guar-
antee interleaved writes to the contiguous disk space. As shown in Fig. 2(c), the
highest write throughput will be obtained in this approach.
Fig. 2. Three disk re-use modes: (a) File-per-stream. Write arrivals and disk accesses
for single ﬁle per stream; (b) Round-robin rotation. Data arrives for stream A, B,
and C in an interleaved fashion; (c) Log-structured append mode. Data arrives in an
interleaved fashion and is written to disk in the same order.
On the contrary, obtaining sequential writing will inevitably result in the
discontiguous segments of data from the same video capture device. In spite of
the lower frequency, read performance is crucial for data export or retrospec-
tive playbacks if incidents occur. Consequently, it is as far as possible to store
stream data in contiguous region of address space. The trade-oﬀbetween write
performance and the read sequential is discussed in the following paragraph.
3.2
Storage Organization and Disk Layout
We organize and manage video data directly on the raw disks, and adopt the
round-robin rotation mode with larger size for the following reasons. First, the
video data is much larger than the streams in network monitoring [4]. Appro-
priate buﬀering strategy, caching stream data into a suitable size before writing,
may keep the system in certiﬁed performance. Second, our data is organized
directly on the raw disks with lightweight metadata information. Therefore, the
overhead of data writing is smaller than that in conventional ﬁle systems. More-
over, the log-structured ﬁle system has to maintain complex metadata informa-
tion, for example, b-tree structured directory in NILFS ﬁle system. Maintaining

194
C. Wang et al.
b-tree index over the entire stream would require one or more disk operations,
the sequential rewrite performs weakly in NILFS [10] ﬁle system. At last, in our
approach, the storage space of a stream is sectional contiguous, aﬀecting little
in barely-occurred reading. Accordingly, our design is rational.
Fig. 3. Disk layout of the LVFS. The logical volume is the basis of LVFS, divided into
the metadata and the data blocks.
In this paragraph, we introduce the storage organization ﬁrst. In LVFS, video
data are organized hierarchically:
Logical Volume: Logical volume is the basis level of LVFS, and is used for
managing the underlying storage devices. It provides a logical storage pool for
the upper layers. A logical volume is divided into record volumes and allocates
sectional contiguous space for each steam.
Record Volume: Each record volume corresponds to a assigned camera, and
stores the stream data in a circular buﬀer fashion; for each camera, video data
in diﬀerent time intervals are stored into diﬀerent record segments, as illustrated
in Fig. 3. Each record volume corresponds to an assigned video capture device,
the storage space of the record volume is guaranteed for cyclic use.
Record Segment: Record segment stores video data over a period of con-
tinuous time in a contiguous region of the disks. The size of record volume is
dependent on the stream rate and recording time. For example, if the camera
resolution is HD whose recommended transfer rate is 2 MB/s, the amount of
data generated in 5 min is nearly 600 MB.
The disk layout of LVFS is shown in Fig. 3, dividing the logical volume into
volume header and data blocks. The volume header holds the global information
of the logical volume, and it consists of superblock, block map, vnode bitmap
and vnode array.
Superblock: Superblock has the description of the devices making up the
ﬁle system, the data block size, the total number of data blocks, the number of
the free blocks, etc.
Block map: This map tabs each data block allocated or not.
Vnode bitmap: This map holds the record volume node (Vnode) directory,
designed for vnode positioning.
Vnode array: The array keeps the metadata for each recode volume (the
information of associated video capture device, the data block address of the

LVFS: A Lightweight Video Storage File System
195
latest segment being written, the latest timestamp index address being written,
the video retention time, the data overwritten policy, etc.) Indeed, Vnode array
is an index of record volumes, namely the video capture devices directory. Along
with the timestamp-based index in the following paragraph, LVFS forms spatio-
temporal index in two dimensions.
Fig. 4. Timestamp-based index in LVFS.
3.3
Timestamp-Based Indexing
To support eﬃcient retrospective queries, LVFS employs a two-level index, the
structure of which is illustrated in Fig. 4. The timestamp index is associated
corresponding to the record segments per seconds. The range index is a directory
table of the timestamp index and lists the start time and end time of each record
segment. Both the timestamp index and range index are maintained in the header
of the record volume. When querying a video data in a speciﬁc moment, we will
ﬁrstly position the video volume to which the video data belongs, then determine
the timestamp index that includes the queried temporal boundaries by using
range index. At last, return the storage address found in the timestamp index.
Since the content of the two-level index is time ordered, timestamp-based data
positioning will be eﬃcient by using linear operations.
3.4
Interface
LVFS provides a stream data oriented interface for video surveillance applica-
tions, rather than a standard API such as POSIX. It supports the usual oper-
ations on record volume, record segment, such as create, delete; and the data
operations, including write, read and delete.
3.5
Implementation
We have implemented a stability system of the video surveillance storage on
Linux, running on commodity servers. As shown in Fig. 1, rounded boxes are
implemented as LVFS. The modules, Logical Volume management, Record

196
C. Wang et al.
volume management and Segment management, manage the storage space hier-
archically. In HD or Full-HD network video surveillance systems, multiple high-
volume video streams with concurrent write may easily induce I/O congestion.
In order to avoid the congestion, the task management module employs the
single-thread non-blocking I/O multiplexing technology. In the buﬀer manage-
ment module, we assign an independent buﬀer for each record volume, optimizing
the data writing and time index generating. Moreover, we have implemented a
software development kit (SDK) both in Windows and Linux for client users.
4
Performance Evaluation
In this section we present operational measurements of the LVFS storage system.
The storage servers are conﬁgured in Table 1. The experiment is conducted in two
phases. First, we simulate numbers of cameras to evaluate the write performance.
Second, with the write operations, we test the response time of queries.
Table 1. Test environment
Components Parameters
CPU
Intel G540/2.5 GHZ processors
RAM
DDR3, 4 GB
OS
CentOS 6.0
Hard disk
WDC, 500G, 7200 rpm
Network
1 Gbps
Write: We simulate N cameras write simultaneously to the LVFS storage sys-
tem, ignoring data from read operations. Because of the bandwidth limitation,
the maximum data arrive rate in the LVFS is 80 MB/s, as shown in Fig. 5(a).
That is, the network environment supports at most 40 HD channels or 16 full-HD
channels. Exceeding it, the LVFS storage sever will encounter frame loss.
In this part of comparison, we create a 1G-ﬁle for each camera in both Ext2
and Ext4 with contiguous storage space separately. The number of cameras is
increasing from 1 to 45 in HD camera environment and from 1 to 18 for full-HD
cameras. As the results shown in Fig. 5(b) and (c), LVFS achieves the network-
limited throughput, namely, 80 MB/s. At the same time, the peak throughput in
Ext2 and Ext4 is about 60 MB/s 65 MB/s, whose theoretical limit for sequential
write is nearly 110 MB/s, as tested by Iometer. That is because general-purpose
ﬁle systems are designed for common applications, not optimized for such work-
loads. On the contrary, with the task scheduling and buﬀering schemes, LVFS
integrates larger data blocks and schedules the concurrent write operations.
Therefore, the LVFS can record video streams in high rate.

LVFS: A Lightweight Video Storage File System
197
0
5
10
15
20
25
30
35
40
45
0
10
20
30
40
50
60
70
80
90
100
Number of cameras N
Frame loss rate(100%)
HD camera
Full−HD camera
(a)
0
5
10
15
20
25
30
35
40
45
0
10
20
30
40
50
60
70
80
90
100
Number of cameras N
Write rate (MB/s)
LVFS
Ext2
Ext4
(b)
0
2
4
6
8
10
12
14
16
18
0
10
20
30
40
50
60
70
80
90
100
Number of cameras N
Write rate (MB/s)
LVFS
Ext2
Ext4
(c)
0
5
10
15
20
25
30
35
0
10
20
30
40
50
60
70
80
90
100
Number of cameras N
Query time (ms)
HD camera
F−HD camera
(d)
Fig. 5. (a) Frame loss rate in LVFS; (b) Ext2, Ext4 vs. LVFS write throughput from
1 to 45 HD cameras; (c) Ext2, Ext4 vs. LVFS write throughput from 1 to 18 full-HD
cameras; (d) Query response time.
Query: Concurrently with data writing, we test the overhead of queries. We
keep the system running for a 3-day before test. Then, with the increasing num-
ber of cameras, we randomly select videos from diﬀerent cameras with various
time intervals. The average response time is between 50 ms to 60 ms, regardless
of the number of cameras, as shown in Fig. 5(d). The nearly constant query time
proﬁt from our two-leveled index.
In summary, LVFS performs well in data writing and returns query results
in acceptable response time.
5
Conclusions
In video surveillance, large numbers of cameras transmit continuous video
streams to underlying storage systems. To capture high-volume streams in real-
time and archive the video data with timestamp-based index in valid reten-
tion time, a video storage system for IP camera-based surveillance applications,

198
C. Wang et al.
named LVFS, was proposed and its performance in data writing and video query-
ing was further evaluated. Results of a multi-workload experiment show that
LVFS is able to archive 40 HD or 16 full-HD cameras for an individual hard disk
while processing nearly constant time queries. Future work will need to optimize
the writing performance in hard disk and scale out the storage system to support
larger-scale video surveillance.
Acknowledgment. Firstly, we would like to thank the reviewers for the careful and
thorough reading of our manuscript and for the insightful comments and constructive
suggestions. Secondly, this work is supported in part by the National Basic Research
Program (973 Program) of China under Grant No. 2011CB302305, the National Nat-
ural Science Foundation of China under Grant No. 61232004. NSF-CNS-1116606.
References
1. Bramberger, M., Doblander, A., Maier, A., Rinner, B., Schwabach, H.: Distributed
embedded smart cameras for surveillance applications. computer 39(2), 68–75
(2006)
2. Chang, R.I., Wang, T.C., Wang, C.H., Liu, J.C., Ho, J.M.: Eﬀective distributed
service architecture for ubiquitous video surveillance. Inf. Syst. Front. 14(3), 499–
515 (2012)
3. Chen, X., Zhang, C.: A spatio-temporal database model on transportation surveil-
lance videos. In: Proceedings of the Third Workshop on Spatio-Temporal Database
Management STDBM 2006, p. 17 (2006)
4. Desnoyers, P., Shenoy, P.J.: Hyperion: high volume stream archival for retrospective
querying. In: USENIX Annual Technical Conference, pp. 45–58 (2007)
5. Diallo, O., Rodrigues, J.J., Sene, M.: Real-time data management on wireless sen-
sor networks: a survey. J. Netw. Comput. Appl. 35(3), 1013–1021 (2012)
6. Hossain, M.A.: Analyzing the suitability of cloud-based multimedia surveillance
systems. In: 2013 IEEE 10th International Conference on High Performance Com-
puting and Communications and 2013 IEEE International Conference on Embed-
ded and Ubiquitous Computing (HPCC EUC), pp. 644–650. IEEE (2013)
7. Hossain, M.A.: Framework for a cloud-based multimedia surveillance system. Int.
J. Distrib. Sens. Netw. 10(5), 135257 (2014)
8. Hossain, M.S., Hassan, M.M., Al Qurishi, M., Alghamdi, A.: Resource allocation
for service composition in cloud-based video surveillance platform. In: 2012 IEEE
International Conference on Multimedia and Expo Workshops (ICMEW), pp. 408–
412. IEEE (2012)
9. Huang, T.: Surveillance video: the biggest big data. Comput. Now 7(2), 82–91
(2014)
10. Layton, J.B.: Nilfs: a ﬁle system to make SSDs scream. Linux Mag., 6 (2009)
11. Li, S., Liu, W., Ma, H., Fu, H.: Multi-attribute based ﬁre detection in diverse
surveillance videos. In: Amsaleg, L., Guðmundsson, G.Þ., Gurrin, C., J´onsson,
B.Þ., Satoh, S. (eds.) MMM 2017. LNCS, vol. 10132, pp. 238–250. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-51811-4 20
12. Limna, T., Tandayya, P.: A ﬂexible and scalable component-based system archi-
tecture for video surveillance as a service, running on infrastructure as a service.
Multimedia Tools Appl. 75(4), 1765–1791 (2016)

LVFS: A Lightweight Video Storage File System
199
13. Liu, H., Chen, S., Kubota, N.: Intelligent video systems and analytics: a survey.
IEEE Trans. Ind. Inf. 9(3), 1222–1233 (2013)
14. Lo, W.T., Chang, Y.S., Sheu, R.K., Yang, C.T., Juang, T.Y., Wu, Y.S.: Imple-
mentation and evaluation of large-scale video surveillance system based on P2P
architecture and cloud computing. Int. J. Distrib. Sens. Netw. 10(4), 375871 (2014)
15. Piroska, H., Gyula, F., Ioan-Cosmin, S.: Data storage for smart environment using
non-SQL databases. In: 2012 IEEE International Conference on Intelligent Com-
puter Communication and Processing (ICCP), pp. 305–308. IEEE (2012)
16. R¨aty, T.D.: Survey on contemporary remote surveillance systems for public safety.
IEEE Trans. Syst. Man Cybern. Part C (Appl. Rev.) 40(5), 493–515 (2010)
17. Ye, G., Liao, W., Dong, J., Zeng, D., Zhong, H.: A surveillance video index and
browsing system based on object ﬂags and video synopsis. In: He, X., Luo, S.,
Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM 2015. LNCS, vol. 8936, pp.
311–314. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14442-9 36
18. Zhizhuo, S., Yu-An, T., Yuanzhang, L.: An energy-eﬃcient storage for video surveil-
lance. Multimedia Tools Appl. 73(1), 151–167 (2014)

Person Re-id by Incorporating PCA Loss
in CNN
Kaixuan Zhang, Yang Xu, Li Sun(B), Song Qiu, and Qingli Li
Shanghai Key Laboratory of Multidimensional Information Processing,
East China Normal University, Shanghai, China
sunli@ee.ecnu.edu.cn
Abstract. This paper proposes an algorithm, particularly a loss func-
tion and its end to end learning manner, for person re-identiﬁcation task.
The main idea is to take full advantage of the labels in a batch during
training, and to employ PCA to extract discriminative features. Deriving
from the classic eigenvalue computation problem in PCA, our method
incorporates an extra term in loss function with the purpose of minimiz-
ing those relative large eigenvalues. And the derivative with respect to
the designed loss can be back-propagated in deep network by stochas-
tic gradient descent (SGD). Experiments show the eﬀectiveness of our
algorithm on several re-id datasets.
1
Introduction
Person re-identiﬁcation, or re-id, aims to ﬁnd the person of interest from images
captured by other cameras, and it has became quite popular in the ﬁeld of
computer vision due to it’s extensive applications, especially in surveillance and
security applications [1,2]. Re-id task is extremely challenging because of the
complex variations of lighting, pose, viewpoint, occlusion, background clutter
across diﬀerent camera views, etc. In the early time, person re-id algorithms were
dominantly based on hand-crafted features [3–6], while with the growth of the
scale of person re-id datasets and the improvement of computational capabilities,
algorithms based on convolutional neural network (CNN) are prevalently used
[7–13].
Cumulative Match Curve (CMC) is employed to evaluate the performance
of re-id algorithms. Since the calculation of CMC depends on Euclidean metric,
extracted features should be not only separable but also discernable, as shown
in Fig. 1. Generally, two types of CNN models are commonly employed in re-
id tasks, which are classiﬁcation model [11] with softmax loss and veriﬁcation
model based on siamese or triplet loss [7–9,12]. But both of them have their
limitations. Classiﬁcation model simply ﬁnds hyperplanes to separate features,
K. Zhang and Y. Xu—Contributed equally to this work.
L. Sun—This work was supported in part by the National Natural Science Foun-
dation of China under Project 61302125, 61671376 and in part by Natural Science
Foundation of Shanghai under Project 17ZR1408500.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 200–212, 2018.
https://doi.org/10.1007/978-3-319-73600-6_18

Person Re-id by Incorporating PCA Loss in CNN
201
regardless of the relation between samples of the same pedestrian. Thus, features
speciﬁed by it may not be discernable enough. Veriﬁcation model only exploits
a little information from labels, which may leads to a degraded performance.
Some recent works [14,15], trying to combine these two models, have achieved
good performance on tasks similar with re-id.
In this paper, we propose a loss function, namely PCA loss, and its end-to-end
training method in CNN. Based on the classiﬁcation model, we combine softmax
loss and PCA loss to jointly supervise a CNN and extract discernable features.
Unlike siamese and triplet loss which need careful training sample selection,
our method is easy to implement, and the only requirement of training data
is that in a mini-batch the number of samples is more than that of identities.
Since we formulate the derivative of our loss function’s gradient, our method
can be trained directly by standard SGD. Another aspect of our contribution is
that extensive experiments have been presented on several person re-id datasets.
Improvement of the performance of CNN has been observed, which validates the
eﬀectiveness of PCA loss and its end-to-end training manner.
(a) Separable Features (b) Discernable Features
Fig. 1. Under both conditions the classiﬁcation accuracy is 100%, but in a person re-id
task (a) gives a worse result than (b), because in (a) the solid points are so close that
they are regarded as the same identity while in fact they are not.
2
Related Work
In the ﬁeld of person re-id, traditional methods focus on feature learning
[3,4,16] and metric learning [4,17] were dominant in the early time. But since
Krizhevsky et al. [18] won ILSVRC’12 by a large margin, CNN-based models
gradually become popular in this ﬁeld [7–9,11,12]. Most CNN-based methods
focus on classiﬁcation model and triplet loss. For instance, Hermans et al. [19]
improved tripletloss and achieved good results. However, except the problem of
sample selection, triplet loss also suﬀers from data imbalance and dramatic data
expansion.
Recently, CNN-based models which focus on the discriminative ability
have been proposed. Dorfer et al. [20] proposed an end-to-end fashion named
DeepLDA to learn linearly latent representations, which achieved impressive
results on several datasets. Wen et al. [14] proposed center loss to minimize
the intra-class variance of the same identity, enhancing the discriminative power
of CNN.

202
K. Zhang et al.
Although our method is CNN-based and also focuses on extracting discern-
able features, our approach is signiﬁcantly diﬀerent from that of DeepLDA [20]
and center loss [14]. Speciﬁcally, DeepLDA requires a suﬃciently large batch-size
to ﬁnd an optimal subspace, which is impractical facing a dataset with a large
number of classes, but our approach do not have this drawback. For center loss,
it minimizes the intra-class variance by simultaneously learning centers of each
class and penalizing the distance between deep features and their corresponding
class centers. However, it is unreasonable that the learned centers and weights
of neural network are up-dated respectively. In our method, we overcome this
problem by optimizing the eigenvalues instead of learning a center. A mathemat-
ical discussion and experiments to compare PCA loss and center loss are given
in Sects. 3.4 and 4.1.
In our loss function PCA is employed, which has already been used in neural
networks in recent researches [21,22]. Deep PCA (DPCA) was proposed by Liong
et al. [21], who utilized PCA to reduce the dimension of features and discard
noisy. Besides, Chan et al. [22] proposed PCANet, employing PCA to learn
multistage ﬁlter banks. But as far as we know, our work is the ﬁrst attempt
to optimize the eigenvalues of PCA and help CNN extract discernable features.
The details of our loss function are given in Subsect. 3.2.
3
The Proposed Approach
A general introduction to PCA [23] is provided in this section. Based on this
introduction, we propose PCA loss which optimizes intra-class variance of deep
learned features. Finally, the relation between our loss function and center loss
is discussed. The employed framework is shown in Fig. 2.
Fig. 2. Overview of our framework. CNN inside the box is proposed in [11]. In our
method, softmax loss and PCA loss are combined to jointly supervise the learning of
CNN so as to get a more discriminative feature.
3.1
Principal Component Analysis
Given a set of features (h1, . . . , hN) = H ∈RN×d exacted by CNN network
in a batch, where (h1, . . . , hN) belong to C diﬀerent classes c ∈{1, . . . , C}.

Person Re-id by Incorporating PCA Loss in CNN
203
Hi denotes the i-th row in H and also a deep-learned feature of a pedestrian.
Hc denotes the set of Hi belongs to the same class c, and yi denotes the label of
Hi. ¯Hc = Hc −mc are the mean-centered observations of class c with per-class
mean vector mc.
PCA is mathematically deﬁned as an orthogonal linear transformation that
transforms the data to a new coordinate system:
xi = W Thi, i = 1, . . . , N
(1)
columns of W are the eigenvectors ei obtained by solving the eigenstructure
decomposition:
Qei = viei
(2)
where Q =
1
N−1 ¯HT ¯H is the covariance matrix and vi the eigenvalue associated
with the eigenvector ei (∥ei∥= 1). In the following subsection, we will cast PCA
as an objective function for CNN.
3.2
Optimization Target
The loss function of our proposed framework is formulated as:
L = Ls + λLP CA
(3)
where Ls is softmax loss and LP CA is the optimization target in Eq. 7.
Since each eigenvalue vi represents the variance of ¯H on the direction of cor-
responding eigenvector ei, we can minimize the intra-class variance of each class
by individually minimizing their eigenvalues. Nevertheless, the eigendecomposi-
tion is quite time-consuming. Thus, instead of optimizing the eigenvalues of each
class in a batch individually, we compute the mean value of all the covariance
matrix in a batch and eigendecompose the average covariance matrix Sw:
Sc =
1
Nc −1
¯HcT ¯Hc
(4)
Sw = 1
C
C

c=1
Sc
(5)
except saving time, this resort also produces more representative gradient
because Sw contains information from a variety of samples, more than that
of a single covariance matrix Sc. Our initial optimization target is formulated
as:
arg min 1
N
N

i=1
vi,
(6)
where N is the feature dimension of the last hidden layer and vi is an eigenvalue
of Sw. Intuitively, large eigenvalues of Sw represent common features, and small

204
K. Zhang et al.
eigenvalues, on the contrary, reﬂect noise, individual characters, and mislabelled
data. On the basis of this intuition, we reformulate our PCA loss as:
LP CA = 1
k
k

i=1
vi
(7)
In Eq. 7, vi ∈{vj | vj ≥β · max {v1, . . . , vd}} , β ∈[0, 1]. By this resort,
CNN focuses less on common features, extracting more discernable features.
Besides, the negative inﬂuence of mislabelled data is mitigated by discarding
small eigenvalues. Experiment results concerned with hyperparameter β will be
presented in Sect. 4.3.
3.3
Gradient of Optimization Target
In this subsection, we provide partial derivatives of the optimization target pro-
posed in Eq. 7 with respect to the last hidden layer representation H, which
allows PCA loss to train with standard SGD. For convenience, we change the sub-
scripts of the scatter matrices to superscripts in this subsection (e.g.Sw →Sw).
Sw
ij addresses the element in row i and column j in matrix Sw. Starting from
the PCA eigenvalue problem:
Swei = viei
(8)
Diﬀerentiate Eq. 8 to get:
∂Sw
∂H ei + Sw ∂ei
∂H = ∂vi
∂H ei + vi
∂ei
∂H
(9)
and collection terms gives:
∂vi
∂H ei = ∂Sw
∂H ei + (Sw −vi)∂ei
∂H
(10)
Premultiplying both sides by eT
i , and using Eq. 8, we have:
∂vi
∂H = ei
T ∂Sw
∂H ei
(11)
We can write the partial derivative of Sc [20] on hidden representation H as:
∂Sc
ab
∂Hij
=
 ∂Sc
ab
∂Hc
i′j if c = yi
0
if c ̸= yi
(12)
∂Sc
ab
∂Hc
ij
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2
Nc−1(Hc
ij −
1
Nc

n Hc
nj) if a = j, b = j
1
Nc−1(Hc
ib −
1
Nc

n Hc
nb) if a = j, b ̸= j
1
Nc−1(Hc
ia −
1
Nc

n Hc
na) if a ̸= j, b = j
0
if a ̸= j, b ̸= j
(13)

Person Re-id by Incorporating PCA Loss in CNN
205
in Eq. 12, Hij and Hc
i′j represent the same element, despite their row numbers
may be diﬀerent. Based on Eq. 5, we can write the partial derivatives of Sw with
respect to the latent representation H as:
∂Sw
ab
∂Hij
= 1
C

c
∂Sc
ab
∂Hij
(14)
The partial derivative of our optimization target introduced in Sect. 3.2 with
respect to hidden representation H is then deﬁned as:
∂
∂H
1
k
k

i=1
vi = 1
k
k

i=1
∂vi
∂H = 1
k
k

i=1
ei
T ∂Sw
∂H ei
(15)
3.4
Relation with Center Loss
Although our optimization target is quite diﬀerent from that of center loss [14],
we discovered that PCA loss is equivalent center loss multiplied by a constant in
a single iteration if certain conditions hold. A mathematical proof is given here.
Let us start from an easy condition. Assuming that all the data in a batch
come from pedestrian c and the number of samples is Nc. The formulation of
center loss and our loss function are:
Lc
cl =

i
1
2 ∥Hc
i −cyi∥2
(16)
Lc
P CA = 1
k
k

i=1
vc
i
(17)
where yi represents the label of sample Hi and cyi is the learned center of class
yi. The C is deﬁned as the total number of classes, meaning that yi ∈{0, 1, 2 . . .
C −1}. Since vi is the eigenvalue of matrix Sw in Eq. 5, we have:
vc
i =
1
Nc −1eT
i ¯HcT ¯Hcei =
1
Nc −1
		 ¯Hcei
		2
(18)
In the above equation, ei donates the base vector. Then further decompose
		 ¯Hcei
		2:
vc
i =
1
Nc −1

j
		 ¯Hc
j ei
		2
(19)
With Eqs. 17 and 19, we can get:
Lc
P CA =
1
k (Nc −1)
k

i=1

j
		 ¯Hc
j ei
		2 =
1
k (Nc −1)

j

 k

i=1
		 ¯Hc
j ei
		2

(20)

206
K. Zhang et al.
Notice that ei is orthogonal to ej (i ̸= j) and ∥ei∥= 1. If k = r

¯HcT ¯Hc
, we
can orthogonal decompose
		 ¯Hc
j
		2:
		 ¯Hc
j
		2 =
k

i=1
		 ¯Hc
j ei
		2
(21)
Put Eq. 21 in 20:
Lc
P CA =
1
k (Nc −1)

j
		 ¯Hc
j
		2
(22)
In Eq. 16, if cyi equals to the class mean vector mc, then based on the deﬁnition
of ¯
Hc:
Lc
cl =

i
1
2 ∥Hc
i −cyi∥2 =

i
1
2 ∥Hc
i −mc∥2 =

j
1
2
		 ¯Hc
j
		2 = k (Nc −1)
2
Lc
P CA
(23)
With the conclusion above, it is easy to discuss the condition where a batch
contains many classes, for the only diﬀerence is in vi. Diﬀerent from Eq. 18, now
we have:
vi = eT
i ( 1
C

c
1
Nc −1
¯HcT ¯Hc)ei = 1
C

c
1
Nc −1
		 ¯Hcei
		2
(24)
if k = r(
c ¯HcT ¯Hc), cyi = mc and Nc is a constant, we have a conclusion similar
to Eq. 23:
Lcl = Ck (Nc −1)
2
LP CA
(25)
By the way, in center loss [14] the update equation of cyi is:
ct+1
yi
= ct
yi −αΔct
yi
(26)
where t and t + 1 means the number of iteration. Supposing Hi belongs to class
c, Δct
yi can be represented as:
Δcyi = Δcc =

i δ(yi −c) · (cyi −Hi)
1 + 
i δ(yi −c)
(27)
where δ(yi −c) = 1 only if yi = c, otherwise δ(yi −c) = 0. When α = 1 and

i δ(yi −c) ≈1 + 
i δ(yi −c), we can simplify ct+1
yi
as:
ct+1
yi
= ct
yi −Δct
yi =

i δ(yi −c) · Hi

i δ(yi −c)
= mc
(28)
By choosing the β in our method as 0, k = r(
c ¯HcT ¯Hc) can be satisﬁed and all
the eigenvalues are optimized. cyi approximates to mc when the α in center loss
[14] is ﬁxed at 1 and the batch-size is much larger than 1. This approximation
has been proved in Eqs. 26, 27, and 28. Nc would be a constant if in each mini-
batch the number of samples of each pedestrian are same. If these prerequisites
are satisﬁed, PCA loss degenerates into center loss.

Person Re-id by Incorporating PCA Loss in CNN
207
4
Experiments
Since the motivations of PCA loss and center loss are similar, we compare them
in Sect. 4.1 through a toy example. Moreover, in order to prove the eﬀectiveness
of our approach, we evaluate our method on seven person re-id datasets. Factors
like scale, background, illumination, etc. vary greatly among these datasets. In
Sect. 4.2, we introduce the test protocols and some details of our experiments.
Then experiments are conducted with diﬀerent hyperparameters in Sect. 4.3.
Finally, our results are compared on CUHK03 with the state-of-the-art tech-
nologies in Sect. 4.4.
(a) Softmax loss
(b) Softmax loss + center loss
(c) Softmax loss + PCA loss
Fig. 3. The distribution of deep learned features with diﬀerent loss functions. The
points with diﬀerent colors denote features of diﬀerent classes. (Color ﬁgure online)
4.1
A Toy Example
We combine softmax loss, center loss, and PCA loss with LeNet respectively,
in which we reduce the output dimension of the embedding layer to 2 [14]. In
this case, we can plot the features for visualization directly. Considering that
PCA loss is designed for the re-identiﬁcation task, we select ten identities from
MARS [24] dataset to train the CNN and illustrate the distribution. Figure 3
straightforwardly exempliﬁes that, as we expect, center loss and PCA loss yield
more discernable features, which is favored in re-id tasks as we have illustrated
in Sect. 1.
In order to quantitatively describe the extracted features, we average the
intra-class variance of all classes, which equals to the summation of all the
eigenvalues of Sw in Eq. 5. The results are listed in Table 1. We hypothesis that
PCA loss achieves a smaller intra-class variance than center loss because PCA
loss directly optimizes the eigenvalues, which represent the intra-class variance.
Therefore, generally PCA loss extracts features which has a smaller intra-class
variance than that of center loss.

208
K. Zhang et al.
Table 1. Intra-class variance of
features extracted by diﬀerent loss
functions.
Loss function
Averaged intra-class
variance
Softmax loss
204.916
Softmax loss
+ center loss
2.472
Softmax loss
+ PCA loss
2.444
Table 2. Statistics of the datasets and eval-
uation protocols
Dataset
ID
Train
Val
Prb Gal
CUHK03 [8]
1467
21886 4377
100 100
CHUK01 [25]
971
1559
389 485 485
PRID [26]
385
2992
748 100 649
VIPeR [27]
632
506
126 316 316
3DPeS [28]
193
408
101
96
96
i-LIDS [29]
119
194
48
60
60
MARS [24]
1261 424928 84986 636 636
4.2
Datasets and Protocols
Seven person re-id datasets are chosen in our experiments to validate our method.
CUHK03 [8] is one of the most largest published person re-identiﬁcation datasets,
with more than 14,000 images of 1467 pedestrians from ﬁve diﬀerent pairs of
camera views. CUHK01 [25] is captured on the same campus as CUHK03, but
only has two camera views and 3884 images of 971 identities. PRID [26] has two
camera views, each contains 385 and 749 identities. But only 200 of them appear
in both views, and the extracted pedestrian images are from recorded trajectory
video frames. MARS [24] dataset contains 1261 diﬀerent pedestrians captured
by at least 2 cameras, including 509914 training boxes and 681089 testing boxes.
The scale of rest datasets are relatively small. VIPeR [27] is one of the most
challenging datasets with 2 images per identity. 3DPeS [28] has 193 identities
but the number of images for each person is not ﬁxed. iLIDS [29] captures 119
individuals by surveillance cameras in an airport, and thus consists of large
occlusions caused by luggage and other passengers.
It has been demonstrated in [11] that CUHK03 [8] is large enough to train
a deep CNN, so in Sect. 4.3 the other datasets are ﬁne-tuned on a model pre-
trained only by the data from CUHK03. We mainly follow the settings in [11]
to generate the training set, validation set, test set, probe and gallery set. The
diﬀerence is that we sample 16% of images for validation in CUHK03. The statis-
tics of all the datasets and evaluation protocols are summarized in Table 2. In
our experiments, the commonly used CMC [30] top-1 accuracy is employed to
evaluate the performance of all methods.
For MARS dataset which is not included in [11], we sample 16% of images
for validation, the rest of them is served as training data. Probe and gallery sets
are captured by diﬀerent cameras. We randomly sample 636 probe images and
636 gallery images. Probe images and gallery images are totally diﬀerent, each
of them covers 636 identities.

Person Re-id by Incorporating PCA Loss in CNN
209
4.3
Experiments on Hyperparameters of PCA Loss
PCA loss includes two essential hyperparameters, β and λ. β decides which
eigenvalues to optimize while λ dominates the intra-class variance. It is necessary
to conduct experiments to investigate the sensitiveness of the two parameters.
Finetune: First, we use the images of CUHK03 to pretrain the CNN with
iteration of 40,000. Later, we employ each dataset to ﬁne-tune this model with
softmax loss only and record the times of iteration each dataset needs to achieve
best performance. Without changing any parameters, we ﬁne-tuned the pre-
trained model again with the same iteration times, under a joint supervision of
softmax loss and PCA loss. The results are listed in Table 3 and Fig. 4.
Choice of β: Although β ∈[0, 1] theocratically, we do not choose β as 0 or
1 in experiments. In practice, it is safe to choose β as 0.01 or 0.99 instead of 0
or 1 considering the ﬁnite-precision numerical eﬀects.
Training data: Considering PCA does not work if all the samples in a
mini-batch comes from diﬀerent pedestrians, the training data need an extra
preprocessing to make sure in each mini-batch the number of samples is more
than that of identities. Usually, training data is shuﬄed to generate set A and
sorted to generate set B. For each mini-batch, half samples come from set A and
the rest are from set B.
According to the results in Table 3, generally β = 0.99 brings the highest
CMC top-1 accuracy in most datasets except CUHK01 and 3DPeS datasets. We
argue that the scale of CUHK01 and 3DPeS are relative small, which lead to
overﬁt. Additionally, the number of samples per identity is insuﬃcient in these
two datasets, which is hard for PCA loss to extract common features and increase
the accuracy. While for CUHK03 and MARS datasets which have large scales
and suﬃcient intra-class samples, PCA loss enhance the top-1 accuracy greatly
and a higher β brings a higher accuracy.
Based on Fig. 4, a relative large λ deteriorates the results. Since a large λ
leads to a rather small intra-class variance [14], we hypothesize that a very small
intra-class variance leads to overﬁt and decrease the generalization ability of
CNN. Generally, setting λ between 10−4 and 10−2 is a safe choice.
Fig. 4. CMC top-1 accuracies of CUHK01, CUHK03, and MARS datasets. β = 0.01
for CUHK01, β = 0.75 for CUHK03 and β = 0.99 for MARS.

210
K. Zhang et al.
Table 3. CMC top-1 accuracies of each
datasets. λ = 10−3 for CUHK01, PRID,
3DPeS, and i-LIDS. λ
=
5 × 10−3 for
CUHK03 and MARS. λ = 5 × 10−4 for
VIPeR.
Dataset
Without
PCA
With PCA
β = 0.01 β = 0.5 β = 0.99
CUHK03 75.2
79.4
80.3
80.9
CUHK01 52.3
56.1
54.6
52.6
PRID
27.0
27.0
29.0
29.0
VIPeR
13.3
14.0
12.7
15.8
3DPeS
42.3
42.5
43.1
41.1
i-LIDS
43.9
45.8
45.4
46.7
MARS
63.6
67.5
68.2
70.1
Table 4. Comparison with state-
of-the-art methods.
Methods
CUHK03
r = 1 r = 5 r = 10
NullReid [31]
58.9
85.6
92.5
Ensembles [32] 62.1
89.1
94.3
DeepLDA [33] 63.2
90.0
92.7
GOG [3]
67.3
91.0
96.0
Gated Siamese
[34]
68.1
88.1
94.6
SSM [35]
76.6
94.6
98.0
Quadruplet +
MargOHNM
[36]
75.5
95.2 99.2
Softmax +
Centerloss [14]
77.5
91.3
94.2
Softmax +
Tripet [10]
80.0
93.9
96.9
DGD [11]
80.5
94.9
97.1
Softmax +
PCA (Ours)
80.9 95.0
97.3
4.4
Comparison with the State of the Arts
On Table 4, we compare our results on CUHK03 with current person re-id meth-
ods, for [11] proved that CUHK03 is large enough to train such a deep network.
Our method is very eﬀective on this dataset, achieving the best CMC top-1
accuracy. Perhaps it’s because PCA loss learns from intra-class information and
CUHK03 provides 20 pictures per pedestrian. Notice that our performance is
better than DGD [11], an algorithm which uses more training data than ours.
Besides, PCA loss is easy to implement, unlike the triplet loss which has prob-
lems like data imbalance and dramatic data expansion.
5
Conclusion
In this paper, a new loss function is proposed, namely PCA loss, which takes full
advantage of the labels in a mini-batch so as to minimize intra-class variance. By
a jointly supervision of softmax loss and PCA loss, more discernable features are
extracted through CNN. Extensive experiments on several person re-id datasets
have convincingly demonstrated the eﬀectiveness of our approach.

Person Re-id by Incorporating PCA Loss in CNN
211
References
1. Li, Y., Wu, Z., Karanam, S., Radke, R.J.: Real-world re-identiﬁcation in an airport
camera network. In: International Conference on Distributed Smart Cameras, p.
35 (2014)
2. Camps, O., Gou, M., Hebble, T., Karanam, S., Lehmann, O., Li, Y., Radke, R.J.,
Wu, Z., Xiong, F.: From the lab to the real world: re-identiﬁcation in an airport
camera network. IEEE Trans. Circ. Syst. Video Technol. 27(3), 540–553 (2017)
3. Matsukawa, T., Okabe, T., Suzuki, E., Sato, Y.: Hierarchical Gaussian descriptor
for person re-identiﬁcation. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1363–1372 (2016)
4. Liao, S., Hu, Y., Zhu, X., Li, S.Z.: Person re-identiﬁcation by local maximal occur-
rence representation and metric learning, vol. 8, no. 4, pp. 2197–2206 (2015)
5. Ma, B., Su, Y., Jurie, F.: Covariance descriptor based on bio-inspired features for
person re-identiﬁcation and face veriﬁcation. Image Vis. Comput. 32(67), 379–390
(2014)
6. K¨ostinger, M., Hirzer, M., Wohlhart, P., Roth, P.M.: Large scale metric learning
from equivalence constraints. In: Computer Vision and Pattern Recognition, pp.
2288–2295 (2012)
7. Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identiﬁcation by
multi-channel parts-based CNN with improved triplet loss function. In: IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 1335–1344 (2016)
8. Li, W., Zhao, R., Xiao, T., Wang, X.: DeepReID: deep ﬁlter pairing neural network
for person re-identiﬁcation. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 152–159 (2014)
9. Yi, D., Lei, Z., Liao, S., Li, S.Z.: Deep metric learning for person re-identiﬁcation.
In: International Conference on Pattern Recognition, pp. 34–39 (2014)
10. Schroﬀ, F., Kalenichenko, D., Philbin, J.: FaceNet: a uniﬁed embedding for face
recognition and clustering, pp. 815–823 (2015)
11. Xiao, T., Li, H., Ouyang, W., Wang, X.: Learning deep feature representations
with domain guided dropout for person re-identiﬁcation (2016)
12. Ding, S., Lin, L., Wang, G., Chao, H.: Deep feature learning with relative distance
comparison for person re-identiﬁcation. Pattern Recogn. 48(10), 2993–3003 (2015)
13. Mclaughlin, N., Rincon, J.M.D., Miller, P.: Recurrent convolutional network for
video-based person re-identiﬁcation. In: 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 1325–1334, June 2016
14. Wen, Y., Zhang, K., Li, Z., Qiao, Y.: A discriminative feature learning approach
for deep face recognition. In: IEEE International Conference on Multimedia and
Expo, vol. 47, no. 9, pp. 11–26 (2016)
15. Ranjan, R., Castillo, C.D., Chellappa, R.: L2-constrained softmax loss for discrim-
inative face veriﬁcation. CoRR abs/1703.09507 (2017)
16. Zheng, W.S., Li, X., Xiang, T., Liao, S.: Partial person re-identiﬁcation. In: IEEE
International Conference on Computer Vision, pp. 4678–4686 (2015)
17. Zheng, W.S., Gong, S., Xiang, T.: Towards open-world person re-identiﬁcation by
one-shot group-based veriﬁcation. IEEE Trans. Pattern Anal. Mach. Intell. 38(3),
1 (2016)
18. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks (2012)
19. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-
identiﬁcation. CoRR abs/1703.07737 (2017)

212
K. Zhang et al.
20. Dorfer, M., Kelz, R., Widmer, G.: Deep linear discriminant analysis. In: NBER
Chapters, vol. 5 (2015)
21. Liong, V.E., Lu, J., Wang, G.: Face recognition using Deep PCA (2013)
22. Chan, T.H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y.: PCANet: a simple deep
learning baseline for image classiﬁcation? IEEE Trans. Image Process. 24(12),
5017–5032 (2015)
23. Fukunaga, K.: Introduction to Statistical Pattern Recognition, 2nd edn., pp. 2133–
2143. Academic Press, San Diego (1990)
24. Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S., Tian, Q.: MARS: a video
benchmark for large-scale person re-identiﬁcation. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9910, pp. 868–884. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46466-4 52
25. Li, W., Wang, X.: Locally aligned feature transforms across views. In: Computer
Vision and Pattern Recognition, pp. 3594–3601 (2013)
26. Hirzer, M., Beleznai, C., Roth, P.M., Bischof, H.: Person re-identiﬁcation by
descriptive and discriminative classiﬁcation. In: Heyden, A., Kahl, F. (eds.) SCIA
2011. LNCS, vol. 6688, pp. 91–102. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-21227-7 9
27. Gray, D., Brennan, S., Tao, H.: Evaluating appearance models for recognition,
reacquisition, and tracking (2007)
28. Baltieri, D., Vezzani, R., Cucchiara, R.: 3DPeS: 3D people dataset for surveil-
lance and forensics. In: Joint ACM Workshop on Human Gesture and Behavior
Understanding, pp. 59–64 (2011)
29. Zheng, W.S., Gong, S., Xiang, T.: Associating groups of people. In: Proceedings of
the British Machine Vision Conference, BMVC 2009, London, UK, 7–10 September
2009
30. Moon, H., Phillips, P.J.: Computational and performance aspects of PCA-based
face-recognition algorithms. Perception 30(3), 303–321 (2001)
31. Zhang, L., Xiang, T., Gong, S.: Learning a discriminative null space for person re-
identiﬁcation. In: IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1239–1248 (2016)
32. Paisitkriangkrai, S., Shen, C., Hengel, A.V.D.: Learning to rank in person re-
identiﬁcation with metric ensembles, pp. 1846–1855 (2015)
33. Wu, L., Shen, C., Hengel, A.V.D.: Deep linear discriminant analysis on ﬁsher net-
works: a hybrid architecture for person re-identiﬁcation. Pattern Recogn. 65, 238–
250 (2017)
34. Varior, R.R., Haloi, M., Wang, G.: Gated siamese convolutional neural network
architecture for human re-identiﬁcation. In: Leibe, B., Matas, J., Sebe, N., Welling,
M. (eds.) ECCV 2016. LNCS, vol. 9912, pp. 791–808. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-46484-8 48
35. Bai, S., Bai, X., Tian, Q.: Scalable person re-identiﬁcation on supervised smoothed
manifold (2017)
36. Chen, W., Chen, X., Zhang, J., Huang, K.: Beyond triplet loss: a deep quadruplet
network for person re-identiﬁcation (2017)

Robust and Real-Time Visual Tracking Based
on Complementary Learners
Xingzhou Luo, Dapeng Du, and Gangshan Wu(B)
State Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing, China
xzluo@smail.nju.edu.cn, dudp.nju@gmail.com, gswu@nju.edu.cn
Abstract. Correlation ﬁlter based tracking methods have achieved
impressive performance in recent years, showing high eﬃciency and
robustness to challenging situations which exhibit illumination varia-
tions and motion blur. However, how to reduce model drift phenomenon
which is usually caused by object deformation, abrupt motion, heavy
occlusion and out-of-view, is still an open problem. In this paper, we
exploit the low dimensional complementary features and an adaptive
online detector with the average peak-to-correlation energy to improve
tracking accuracy and time eﬃciency. Speciﬁcally, we appropriately inte-
grate several complementary features in the correlation ﬁlter based dis-
criminative framework and combine with the global color histogram to
further boost the overall performance. In addition, we adopt the average
peak-to-correlation energy to determine whether to activate and update
an online CUR ﬁlter for re-detecting the target. We conduct extensive
experiments on challenging OTB-15 benchmark datasets, and experi-
mental results demonstrate that the proposed method achieves promis-
ing results in terms of eﬃciency, accuracy and robustness while running
at 46 FPS.
Keywords: Visual tracking · Correlation ﬁlter
Dimension reduction · Online detection
1
Introduction
Visual object tracking is one of the most challenging tasks in the ﬁeld of computer
vision and has a wide range of applications such as video surveillance, human-
computer interaction, autonomous driving and robotics. In generic visual track-
ing, given an initial state of the object in the ﬁrst frame, the goal is to estimate
the trajectory of the target throughout video sequences. Despite the signiﬁcant
progress in visual tracking, model drift problem and scale estimation usually
lead to tracking failure in challenging situations such as illumination variation,
deformation, fast motion, occlusion, out of view, scale variation and background
clusters.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 213–225, 2018.
https://doi.org/10.1007/978-3-319-73600-6_19

214
X. Luo et al.
In recent years, correlation ﬁlter based discriminative methods have shown
excellent performance in terms of accuracy and robustness. However, Discrimi-
native Correlation Filter based methods [1,4,7,10,11,14,15,22] learn a correla-
tion ﬁlter from raw pixels, Histogram of Oriented Gradients (HOG) [3] or Color
Names (CN) [16] features on a set of training samples. Owing to the limitation
of each type of feature in some certain scenes, those methods which employ a
type of feature fail to handle scale variation accurately and deal with complex
conditions. Although several methods fuse multiple features or models to learn
the target appearance model, the online models tend to drift due to fast motion
and occlusion. Figure 1 presents some examples of tracking failure. In addition,
several complicated tracking algorithms improve performance at the price of
reducing tracking speed, which limits their real-time performance in real-world
applications.
Fig. 1. Examples of tracking failure in visual object tracking (from left to right are
Basketball, Jogging-1, Car24 and ClifBar). The red and green rectangles denote the
incorrect tracking and ground truth bounding boxes, respectively. Model drift (column
1, 2 and 4) happens due to background clusters, fast motion and occlusion; Scale
variation (column 3) occurs because of illumination change and scale variation. (Color
ﬁgure online)
To overcome the aforementioned issues, we propose a robust tracking method
by extending the Staple tracker with a low dimensional complementary features
and use an adaptive online detector with the average peak-to-correlation energy
(APCE) [17] to achieve tracking robustness and real-time performance. First, we
appropriately integrate complementary features including HOG, Color Names
and intensity with dimension reduction in the correlation ﬁlter based discrim-
inative framework and combine with color histogram-based model to further
boost the accuracy and eﬃciency of visual tracking. In addition, complemen-
tary features are extended to learn the scale ﬁlter for accurate scale estimation.
Finally, we employ APCE to determine whether to activate and update an online
CUR ﬁlter for re-detecting the target.
To evaluate the performance of the proposed method, we evaluate our method
on the large-scale benchmark OTB15 datasets [20] with 100 challenging video
sequences. Compared to a variety of state-of-the-art trackers, extensive exper-
iments show that our method achieves appealing performance in terms of eﬃ-
ciency, accuracy and robustness. The contributions of this paper is brieﬂy sum-
marized as follows:

Robust and Real-Time Visual Tracking Based on Complementary Learners
215
– We extend the Staple tracker by integrating the complementary features
to enhance the discriminative ability to illumination variation. Besides, we
employ a dimension reduction strategy to improve robustness to noise inter-
ference and tracking speed.
– We adopt the average-peak-to-correlation to determine whether to activate
and update an online CUR ﬁlter, which is conducive to dealing with tracking
failure eﬀectively.
2
Related Work
Visual object tracking has been studied extensively with a variety of applications
and achieved extremely excellent performance in the ﬁeld of computer vision.
We brieﬂy review the relevant researches on correlation ﬁlter based tracking and
tracking-by-detection approaches.
Correlation Filter. Correlation ﬁlter based tracking has widely captured
researcher’s attention in recent years. Minimum Output Sum of Squared Error
(MOSSE), proposed by Bolme et al. [2], adopts raw pixels to model the target
appearance by adaptive correlation ﬁlters. Henriques et al. [10] propose Circular
Structure with Kernels tracker (CSK) that utilizes the structure of the circulant
patch to learn a kernelized least squares classiﬁer of the target from a single
image patch with dense sampling, and then extend CSK by using the kernelized
ridge regression and apply HOG features instead of raw pixels to Kernelized
Correlation Filters (KCF) [11] to boost the performance of visual tracking. To
obtain more superior performance than the CSK tracker, Danelljan et al. [7]
introduce the sophisticated color features called Color Names into the frame-
work of the CSK tracker for color sequences. Adaptive low-dimensional variant
of color attributes is mainly used for accelerating tracking. Danelljan et al. [4]
propose DSST to learn separate discriminative correlation ﬁlters using HOG
features for translation estimation and handling the scale changes of the tar-
get during visual tracking. Scale Adaptive with Multiple Features (SAMF) [14]
learns appearance model of the target by fusing both HOG and Color Names
features together to facilitate robust tracking with the scale adaptive scheme.
Sum of Template And Pixel-wise (Staple) [1] exploits complementary learners
including the template-related learner and the histogram-related learner in the
ridge regression framework to preserve robustness to color changes and deforma-
tions. However, the above mentioned CFT trackers are less eﬀective for dealing
with scale variation and model drift problem due to various challenging caused
by fast motion, background cluster, long-term occlusion and out-of-view.
Tracking-by-detection.
Tracking-by-detection approaches are exceedingly
popular due to their high eﬃciency and performance. These tracking algorithms
generally adopt the binary classiﬁer which segregates the target from background

216
X. Luo et al.
to perform visual tracking. To alleviate the stability-plasticity dilemma regard-
ing online update in visual tracking, Kalal et al. [12] propose a novel Tracking-
Learning-Detection (TLD) framework that explicitly decomposes the long-term
tracking task into three components: tracking, learning and detection where the
tracker provides labeled training data for training and updating detector and the
detector re-initializes the tracker when tracking failure happens. Hare et al. [9]
consider the spatial distribution of training samples, and integrate features and
kernels into an online structured output SVM learning framework to predict the
object location. Zhu et al. [22] propose the collaborative correlation tracker that
jointly employ multi-scale kernelized correlation ﬁlter to learn the target appear-
ance and introduce an eﬃcient online CUR ﬁlter for detection which alleviates
the model drift. Ma et al. [15] use discriminative correlation ﬁlters for translation
and scale estimation, and develop an online random ferns classiﬁer to redetect
the target in case of tracking failure. Diﬀerent from the above trackers, we utilize
APCE to determine whether to activate and update the online detector.
3
Tracking Components
We aim to build a robust and real-time tracking method. Recently, the Staple [1]
tracker achieves appealing performance with high speed. Due to the competitive
performance and eﬃciency, we base our approach on the Staple tracker. In this
section, we ﬁrstly review the Staple tracker. Secondly, we introduce the com-
plementary feature used in our method to enhance robustness to illumination
change. Moreover, we introduce a dimension reduction strategy to remove noise
interference for target estimation and improve eﬃciency. Finally, to eﬀectively
deal with tracking failure, we utilize an adaptive online detection scheme with the
average peak-to-correlation energy. Figure 2 shows an overview of our proposed
method.
3.1
The Staple Tracker
The Staple [1] tracker combines two responses—the template response is learnt
from HOG feature that is insensitive to color changes, and the histogram
response is learnt from the global color histogram that is robustness to shape
deformation. The models are learnt by solving two independent ridge regression
problems, which retains the eﬃciency of the correlation ﬁlter and avoids ignoring
the information captured by the color histogram response.
The template response is learnt under the least-squares correlation ﬁlter for-
mulation. Multi-channel correlation ﬁlters are learnt from a single sample of the
target that consists of d-dimensional feature maps f. The optimal correlation
ﬁlter h is achieved by minimizing the objective
min
h

d

i
hi ⋆f i −y

2
+ λ
d

i
hi2 ,
(1)

Robust and Real-Time Visual Tracking Based on Complementary Learners
217
Fig. 2. The framework of our proposed method. At each frame, an image patch at esti-
mated position and size from the previous frame is cropped as current input. First, the
template response is calculated from the correlation between low dimensional comple-
mentary features and the previous learned ﬁlter (⊙denotes element-wise computation).
Meanwhile, the histogram response is computed by using the integral image from the
target likelihood map. Then, the ﬁnal response map is obtained by the linear combi-
nation of the template and histogram responses. Next, we use APCE calculated from
the ﬁnal response to consider whether to activate the online detector and re-detect the
target. Finally, we update the template and histogram-related model parameters at
estimated target state.
where f i is the feature map of the i-th channel of f, λ is a regularization term
that prevents over-ﬁtting, y is the desired correlation output and the star ⋆
denotes the circular correlation. Further, the correlation operation is performed
in the Fourier domain. As the method presented in [4], the solution of the ﬁlter
is given by
Hi =
Y F i
d
j=1 F jF j + λ
,
(2)
where F i is the discrete Fourier transform (DFT) of f i, F j is the DFT of f j and
Y denotes the complex conjugate of the DFT of y.
For Eq. (2), an optimal ﬁlter Hi can be achieved by solving a d × d linear
system of equation per pixel, which triggers a computational bottleneck for online
learning step in Discriminative Correlation Filter based tracking algorithm. To
obtain a robust approximation, instead of performing expensive computation,
the numerator Bi
t and the denominator Γ i
t of the optimal ﬁlter Hi
t are updated
separately as
Bi
t = (1 −αcf) Bi
t−1 + αcfY F i
t
Γt = (1 −αcf) Γt−1 + αcf
d

j=1
F j
t F j
t .
(3)

218
X. Luo et al.
Here, αcf is the learning rate parameter of the template response. Moreover, the
correlation score ycf is estimated by using the inverse DFT:
ycf = F −1
d
i=1 Bi
t−1Zi
t
Γt−1 + λ

.
(4)
The histogram response is learnt under the color histogram based Bayes
framework. Given the object region O and its surrounding region S, both the
color histogram of the object and the background are calculated to obtain the
histogram response. According to Bayes theorem, the object likelihood at loca-
tion x is denoted as
P (x ∈O|O, B) ≈
P (bx|x ∈O) P (x ∈O)

Ω∈{O,B} P (bx|x ∈Ω) P (x ∈Ω).
(5)
Let HΩ(·) and bx denote the color histogram which is calculated over the region
Ω. Then, Eq. (5) simpliﬁes to
P (x ∈O|O, B) =
HO(bx)
HO(bx) + HB(bx).
(6)
In addition, the model parameters HO (b) and HB (b) are updated online as
HO,t(b) = (1 −αch) HO,t−1(b) + αchHO,t(b)
HB,t(b) = (1 −αch) HB,t−1(b) + αchHB,t(b).
(7)
Finally, the response of color histogram ych is calculated by employing the
integral image from P (x ∈O|O, B). The ﬁnal response map g is obtained by the
linear combination of the template response and histogram response as
g = γych + (1 −γ) ycf.
(8)
3.2
Multiple Feature Fusion
Generally, the correlation ﬁlter only performs dot-product operation on multiple
features and sums over image features in Fourier domain. The popular HOG
features have been successfully applied in various practical applications [3,8,11].
Color attributes, or Color Names [16], which are linguistic color labels assigned
by human to describe colors in the real world, have shown excellent results
for object recognition [13]. HOG features mainly analyze image gradients while
Color Names focus on color representation. To address the limitation of HOG
features under illumination variation and deformation, we concatenate HOG
features with complementary features Color Names and intensity into a vector
to represent the target appearance model in our method.
Furthermore, we extend multiple features into the scale search procedure to
achieve more accurate scale estimation. To handle scale variation, we follow the
scale search strategy that the scale ﬁlter is learnt by constructing scale feature
pyramid proposed by Danelljan et al. [4] at estimated target location.

Robust and Real-Time Visual Tracking Based on Complementary Learners
219
3.3
Dimension Reduction
The FFT operations consume the expensive computation for the template
response and scales linearly with the feature dimension. To reduce the compu-
tation cost of FFT, we introduce a dimension reduction strategy [5] that retains
useful information to boost the speed of our approach.
Instead of updating the target appearance vt, we use the learned appearance
vt(l) to construct a ¯d × d project matrix Qt. The project matrix Qt is used to
reconstruct the compressed target template vt as ˆvt(l) = Qtvt(l), where l is the
tuple index that covers all elements in the target appearance vt. The project
matrix Qt is estimated by minimizing the reconstruction error of the target
template vt
ϵ =

l
∥vt(l) −QT
t Qtvt(l)∥2.
(9)
Here, the Eq. 9 is minimized under constraint QtQT
t
= I. This is solved by
performing an eigenvalue decomposition of the matrix Jt = 
l vt(l)vt(l)T . The
rows of the project matrix Qt is selected as the d-eigenvectors of Jt corresponding
to the largest eigenvalues. Therefore, the ﬁlters are derived as:
ˆBi
t = Y ˆV i
t
ˆΓt = (1 −αcf) ˆΓt−1 + αcf
ˆd

j=1
ˆF j
t ˆF j
t .
(10)
Here, the compressed training sample ˆFt = F{Qtft} and target appearance
ˆVt = F{Qtvt}. The template response is obtained by employing the compressed
test sample ˆZt = F{Qt−1zt}
ycf = F −1
 ˆd
i=1 ˆBi
t−1 ˆZi
t
ˆΓt−1 + λ

.
(11)
3.4
Online Detection
It is obvious that introducing a re-detection component is favorable for improving
the robust long-term tracking algorithm in case of tracing failure. However, if
the re-detection procedure is carried out at each frame in videos, the tracking
algorithm will inevitably suﬀer the high computational complexity. The CCT
[22] tracker utilizes the overlapping rate between the estimated target state and
the candidate bounding box detected by the CUR ﬁlter to detect the tracking
failure and alleviates model drift problem to some extent, but it hardly solves
the problem of tracking failure due to the inaccuracy of translation estimation.
We propose an eﬀective method to tackle this problem.
Conventionally, correlation ﬁlters are designed to produce strong peaks for
the target and the conﬁdence degree of the response map is measured by

220
X. Luo et al.
APCE [17]. To enable the tracker to detect tracking failure and activate re-
detection module, we estimate the target state in the t-th frame by
APCE =
|gmax −gmin|
mean
 
w(g(w) −gmin)
,
(12)
where gmax and gmin is the maximum and minimum value on the correlation
map, and w denotes the tuple index that covers all elements of the response
map. We determine whether to activate re-detection module with the criteria
APCE. In addition, the learning rates αcf and αch are adjusted to καcf and
καch respectively if APCE is smaller than a predeﬁned threshold, where κ is the
penalty coeﬃcient.
We adopt an online CUR ﬁlter which is ﬁrstly used in the collaborative
correlation tracker for re-detect the target. Diﬀerent from previous CCT, whether
we update the online CUR ﬁlter depends entirely on APCE. Specially, the CUR
decomposition algorithm [18,21] of a matrix A ∈Rm×n aims to ﬁnd a matrix
C ∈Rc×r with a subset of c columns of A, a matrix R ∈Rr×n with a subset of
r rows of A, and a low-rank matrix U ∈Rc×n such that ∥A −CUR∥ξ achieves
minimum, where ∥· ∥ξ is 2-norm or Frobenius norm. If APCE is above the
threshold τ during tracking, we add the target appearance representation At
into the historical object template pool A. We achieve the CUR ﬁlter Dt in
current frame as follows
Dt = 1
c

i=1,...,c
C(i),
(13)
where C is a subset generated by the historical object template pool A with
random sampling and the size c of C is approximately obtained by c = 2k
ε (1 +
o(1)), where k is the target rank and ε is the error probability. If APCE is
below τ, we estimate the similarity between the CUR ﬁlter Dt and each possible
candidate image regions in the image with convolution theorem to detect the
top-k conﬁdent image regions. The target state is ﬁnally identiﬁed to locate at
the maximum value among these response maps. Finally, the online CUR ﬁlter
for detection is updated only when APCE exceeds the threshold τ.
4
Experiments and Analysis
4.1
Datasets and Experimental Setup
We evaluate our approach on the recently published benchmark which is widely
used: the OTB15 datasets [20]. The datasets consist of 100 videos including
many challenging situations: illumination variation, scale variation, motion blur,
occlusion, etc. To fully evaluate our method, we follow the evaluation protocol as
suggested in [19] as well as several standard evaluation metrics, namely distance
precision (DP), overlap precision (OP) and tracking speed in frames per second
(FPS). DP is deﬁned as the percent of frames in a video where the Euclidean

Robust and Real-Time Visual Tracking Based on Complementary Learners
221
distance between the estimated center location and the ground-truth of the tar-
get is below than a threshold. We present the result at the threshold of 20 pixels
[19]. OP is computed as the percent of frames where the intersection-over-union
overlap between the predicted bounding box and the ground-truth suppress a
threshold. We report the result at the threshold of 0.5. We also provide the FPS
for each tracker. In addition, the precision and success plots [19] of the results are
given over all 100 videos. The precision and success plots show the mean distance
and overlap precision over a range of thresholds, respectively. In the legend, the
trackers are ranked using the average DP score at 20 pixels in precision plots
and the area under the curve (AUC) in success plots.
Our approach is implemented in MATLAB 2015a on a desktop PC with an
Intel R⃝CoreTM i7-3770 3.4 GHz CPU and 8 GB RAM. The regulation param-
eter is set to 0.001. The learning rates αcf and αch are set to 0.01 and 0.04,
respectively. We set the merge factor γ to 0.3. For online detection, we set the
parameter τ to 10 to determine when to activate the detector, the penalty coef-
ﬁcient κ to 0.1 and the size c of template pool to 20.
4.2
Experimental Results
We compare our algorithm with 9 diﬀerent state-of-the-art methods to present
the excellent performance. The methods used for comparison contain Staple [1],
DSST [4], SRDCF [6], CN [7], Struck [9], KCF [11], SAMF [14], LCT [15] and
CCT [22]. The code or binaries for all trackers are provided by authors or the
OTB datasets [20].
Table 1. Quantitative comparison of our approach with the state-of-the-art trackers
on 100 challenging sequences. The results of the trackers are presented in median OP
at the threshold of 0.5 and DP at threshold of 20 pixels. We also reported the average
frames per second (FPS) as well. The best two results are highlighted by bold and
underline. Our method performs favorably with the existing trackers.
CN [7]KCF [11]Struck [9]SAMF [14]CCT [22]LCT [15]DSST [4]SRDCF [6]Staple [1] Ours
OP 0.475
0.552
0.534
0.636
0.667
0.700
0.672
0.728
0.699
0.774
DP 0.595
0.697
0.655
0.740
0.739
0.762
0.696
0.788
0.784
0.836
FPS 193.81 251.79
24.37
19.30
40.12
19.43
38.6
6.99
57.63
46.05
Table 1 presents a comparison with the state-of-the-art methods above on 100
challenging sequences using OP and DP. We report the speed of the methods in
average frames per second (FPS) as well. The best two results are highlighted by
bold and underline in each metric. Compared to the baseline method Staple, our
method improves the median OP from 69.9% to 77.4% and DP from 78.4% to
83.6%. Among the trackers in the literature, SRDCF has shown to achieve the
best performance with the median OP of 72.8% and DP of 78.8%. In addition
to the performance advantage with 4.6% in median OP and 4.8% in median DP,
our tracker is nearly 7 times faster than SRDCF. In terms of three evaluation

222
X. Luo et al.
metrics, our method achieves the best compared to CCT and LCT with re-
detection module. Although CN and KCF obtain higher frame rate than 46.05,
the proposed method is able to reach better performance with respect to them.
Fig. 3. The distance precision and overlap success plots using one-pass evaluation
(OPE), temporal robustness evaluation (TRE) and spatial robustness evaluation (SRE)
over all the 100 videos. The values indicate the mean DP score at the threshold of 20
pixels in the legend of precision plots and the legend contains the area under the curve.
Figure 3 shows the precision and success plots illustrating the median dis-
tance and overlap precision over all the 100 sequences. Our approach performs
favorably against the mentioned trackers in OPE, TRE and SRE [8] evaluation
schemes. The trackers are ranked using the median distance precision at the
threshold of 20 pixels for precision plot and the area under the curve (AUC)
for success plot. In precision plots, our method outperforms SRDCF by 4.8%
and the baseline algorithm Staple by 5.2%. In success plots, our approach pro-
vides an improvement of 3% and 4.9% in AUC scores compared to SRDCF and
Staple respectively. Additionally, we evaluate the robustness of our approach on
two diﬀerent types of initialization criteria temporal robustness (TRE) and spa-
tial robustness (SRE). In both robustness evaluations, our algorithm achieves a
consistent gain in performance compared to SRDCF and the baseline algorithm
Staple.
4.3
Qualitative Evaluation
Here we provide a qualitative comparison of our approach with existing state-of-
the-art trackers (Struck [9], KCF [11], SAMF [14], DSST [4] and Staple [1]) on
eight challenging sequences in Fig. 4. The Struck uses the kernelized structured
output SVM classiﬁer and does not deal with well out-of-view, occlusion (Tiger2

Robust and Real-Time Visual Tracking Based on Complementary Learners
223
and Jogging-2), scale variation (Dog1 and Doll), rotation (Rubick) and back-
ground clutters (Shaking). The KCF tracker based on correlation ﬁlter learned
from HOG features does not perform well in scale variation (Dog1 and Doll)
since it is not able to estimate scale changes. The KCF tracker fails to deal
with background clusters (Shaking) because of the property of HOG feature and
occlusion (Jogging-2 and Tiger2) due to the lack of the re-detection module in
case of tracking failure. Although the SAMF tracker integrates HOG and CN
features in the correlation ﬁlter framework, it is less eﬀective in handling model
drift problem caused by multiple factors (BlurOwl, Shaking and Diving). The
Staple tracker performs well in scale variation (Dog1 and Doll) and out-of-view
(Tiger2) due to complementary learners. However, it fails to eﬀectively track the
target for background clusters (Shaking). In addition, The Staple tracker leads
to model drift (Jogging-2) since it does not deal with the partially or fully occlu-
sion. Overall, our proposed approach performs remarkably in most challenging
situations. The main reasons are as follows. First, complementary features are
extracted to learn the template response which is combined with the color his-
togram response to improve performance. The feature fusion strategy exhibits
powerful ability for handling fast motion and motion blur (BlurOwl and Tiger2),
deformation (Diving) and background clusters (Shaking). Besides, our approach
does well in scale variation (Dog1 and Doll) since we extend multiple powerful
features to handle scale change. Finally, the online detector eﬀectively activates
re-detection module in case of tracking failure for out-of-view (Tiger2) and occlu-
sion (Jogging-2).
Fig. 4. A qualitative comparison of our algorithm with the ﬁve state-of-the-art trackers
on eight challenging sequences (from left to right and top to down are BlurOwl, Tiger2,
Dog1, Doll, Rubik, Shaking, Jogging-2 and Diving, respectively).

224
X. Luo et al.
5
Conclusion
In this paper, we propose a robust and real-time visual tracking object method.
We extend the Staple tracker by integrating the complementary features to
enhance the discriminative power to illumination change. We also extend mul-
tiple features to the procedure of learning scale ﬁlter to achieve accurate scale
estimation and improve robustness and eﬃciency by reducing feature in dimen-
sion. In addition, we employ APCE to determine whether to activate and update
the CUR ﬁlter to improve robustness to tracking failure. Extensive experiments
demonstrate that our method achieves superior performance in terms of accu-
racy, robustness and speed.
Acknowledgments. This work is supported by the National Science Foundation of
China under Grant No. 61321491, and Collaborative Innovation Center of Novel Soft-
ware Technology and Industrialization.
References
1. Bertinetto, L., Valmadre, J., Golodetz, S., Miksik, O., Torr, P.H.S.: Staple: com-
plementary learners for real-time tracking. In: CVPR, pp. 1401–1409 (2016)
2. Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking using
adaptive correlation ﬁlters. In: CVPR, pp. 2544–2550. IEEE (2010)
3. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
CVPR, vol. 1, pp. 886–893. IEEE (2005)
4. Danelljan, M., H¨ager, G., Khan, F.S., Felsberg, M.: Accurate scale estimation for
robust visual tracking. In: BMVC. BMVA Press (2014)
5. Danelljan, M., H¨ager, G., Khan, F.S., Felsberg, M.: Discriminative scale space
tracking. TPAMI 39(8), 1561–1575 (2017)
6. Danelljan, M., H¨ager, G., Shahbaz Khan, F., Felsberg, M.: Learning spatially reg-
ularized correlation ﬁlters for visual tracking. In: ICCV, pp. 4310–4318 (2015)
7. Danelljan, M., Shahbaz Khan, F., Felsberg, M., Van de Weijer, J.: Adaptive color
attributes for real-time visual tracking. In: CVPR, pp. 1090–1097 (2014)
8. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection
with discriminatively trained part-based models. TPAMI 32(9), 1627–1645 (2010)
9. Hare, S., Saﬀari, A., Torr, P.H.S.: Struck: structured output tracking with kernels,
pp. 263–270 (2011)
10. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: Exploiting the circulant struc-
ture of tracking-by-detection with kernels. In: Fitzgibbon, A., Lazebnik, S., Perona,
P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7575, pp. 702–715. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-33765-9 50
11. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with
kernelized correlation ﬁlters. TPAMI 37(3), 583–596 (2015)
12. Kalal, Z., Mikolajczyk, K., Matas, J.: Tracking-learning-detection. TPAMI 34(7),
1409–1422 (2012)
13. Khan, F.S., Van de Weijer, J., Vanrell, M.: Modulating shape features by color
attention for object recognition. IJCV 98(1), 49–64 (2012)

Robust and Real-Time Visual Tracking Based on Complementary Learners
225
14. Li, Y., Zhu, J.: A scale adaptive kernel correlation ﬁlter tracker with feature inte-
gration. In: Agapito, L., Bronstein, M.M., Rother, C. (eds.) ECCV 2014. LNCS,
vol. 8926, pp. 254–265. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
16181-5 18
15. Ma, C., Yang, X., Zhang, C., Yang, M.H.: Long-term correlation tracking. In:
CVPR, pp. 5388–5396 (2015)
16. Van De Weijer, J., Schmid, C., Verbeek, J., Larlus, D.: Learning color names for
real-world applications. TIP 18(7), 1512–1523 (2009)
17. Wang, M., Liu, Y., Huang, Z.: Large margin object tracking with circulant feature
maps. In: CVPR (2017)
18. Wang, S., Zhang, Z.: Improving CUR matrix decomposition and the Nystr¨om
approximation via adaptive sampling. JMLR 14(1), 2729–2769 (2013)
19. Wu, Y., Lim, J., Yang, M.H.: Online object tracking: a benchmark. In: CVPR, pp.
2411–2418 (2013)
20. Wu, Y., Lim, J., Yang, M.H.: Object tracking benchmark. TPAMI 37(9), 1834–
1848 (2015)
21. Xu, M., Jin, R., Zhou, Z.: CUR algorithm for partially observed matrices. In:
ICML, pp. 1412–1421 (2015)
22. Zhu, G., Wang, J., Wu, Y., Lu, H.: Collaborative correlation tracking. In: BMVC,
p. 184-1 (2015)

Room Floor Plan Generation on a Project
Tango Device
Vincent Angladon1,2(B), Simone Gasparini1, and Vincent Charvillat1
1 Universit´e de Toulouse, INPT – IRIT, 118 Route de Narbonne,
31062 Toulouse, France
{vincent.angladon,simone.gasparini,vincent.charvillat}@irit.fr
2 Telequid, Toulouse, France
Abstract. This article presents a method to ease the generation of room
ﬂoor plans with a Project Tango device. Our method takes as input range
images as well as camera poses. It is based on the extraction of vertical
planar surfaces we label as wall or clutter. While the user is scanning the
scene, we estimate the room layout from the labeled planar surfaces by
solving a shortest path problem. The user can intervene in this process,
and aﬀect the estimated layout by changing the label of the extracted
planar surfaces. We compare our approach with other mobile applications
and demonstrate its validity.
Keywords: Floor plan · Mobile device · RGB-D cameras
2D/3D mapping
1
Introduction
Floor plan generation is the problem of generating a drawing or a digital model
to scale of an existing room or building. A common workﬂow consists in taking
individual measurements reported on a freehand sketch of the ﬂoor, then in
using a CAD software to draw the ﬂoor plan. This approach is mostly manual
and requires the user to collect all the measurements, typically with a laser range
ﬁnder. The task is not trivial, as it requires the correct use of the measurement
tool and the collection of all the necessary measurements to fully deﬁne the
ﬂoor plan: missing measurements may lead to incomplete plans, thus requiring
costly do-overs on site. Moreover, the task can be challenging when dealing with
furnished or cluttered environments, preventing, e.g., the direct measurement of
certain distances. In order to provide more automatic and eﬃcient solutions, the
use of desktop or mobile applications performing the drawing on-site has gained
a lot of interest over the last years.
With the latest advancement in 3D scanning technologies, 3D scanners placed
on tripods have become an interesting solution for the generation of the ﬂoor
plan. The generated 3D point cloud enables to perform Building Information
Modeling (BIM) and ﬂoor plan generation in a semi-automatic way, through
the use of a dedicated software. The operating cost is generally higher than the
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 226–238, 2018.
https://doi.org/10.1007/978-3-319-73600-6_20

Room Floor Plan Generation on a Project Tango Device
227
previous method, due to the investment of the scanner and the scanning time
which can be quite long when there are several small rooms. The automatic
creation of ﬂoor plans or BIM from such scanners has been extensively researched
for single rooms [1,2] and multiple rooms [3–8]. These methods, are called oﬄine
because they start the processing after the scan is complete and all the data is
available.
At the opposite, for handheld scanners, online methods provide results incre-
mentally during the scanning process. Several online approaches have been pro-
posed [9–11] to extract some walls of indoor scenes, but none of them focus on
the creation of single or multiple room ﬂoor plans.
On common smartphones, user-driven approaches have been proposed [12–
14]. Knowing the phone orientation and the vertical distance of the device to
the ﬂoor (assumed constant and calibrated), they estimate the distances between
the user and the room corners. More recently, as mobile devices started to sport
depth sensors, user-driven approaches have been proposed for the Project Tango
mobile devices: the user is required to manually select the walls with FloorPlanEx
[15] or the edge corners of the indoor scene [16].
In this paper, we propose an online approach for the Tango Tablet Develop-
ment Kit(TDK) Fig. 1a, which is a tablet equipped with a depth sensor and a ﬁsh
eye camera running on Android. It oﬀers dedicated libraries to perform localiza-
tion and 3D reconstructions in the form of 3D textured meshes. Our method
can generate a room ﬂoor plan with optional user interaction. We rely on the
ability of this device to localize itself and capture depth maps, in order to have
a better understanding of the scene and hence reduce the user eﬀorts. Given
the complexity of the task and the diversity of the scenes, the user interaction
is yet helpful to improve the robustness of the approach: large occlusions, lack
of physical separation between rooms, missing data, incorrect depth perception
(e.g. glasses and mirrors), and other issues that could be challenging for a fully
automatic system, can be easily solved by the operator. [17] enumerates and
classiﬁes all the issues related to the problem of indoor mapping and modeling.
While the Tango TDK addresses most of the acquisitions and sensors problems
(variable lighting conditions, sensor fusion, mobility support), our works also
tackles acquisition problems (variable occupancy support), data structure and
modeling issues (real-time modeling, dynamic abstraction), visualization prob-
lems (on mobile visualization, real-time change visualization) and legal issues
(user privacy).
2
Overview of the Method
Hypotheses. We assume the considered rooms are made of a horizontal ceiling
and ﬂoor, and vertical planar walls, the latters not necessarily orthogonal w.r.t.
each other. They can contain clutter (furniture, movable objects, . . . ) occluding
the walls. With the Tango TDK, we observed objects located 4.2 m away suﬀer
from a depth uncertainty over 25 ± 9 mm, and lack of texture (e.g. walls and
ceiling of uniform color) can lead to incorrect camera poses from the Tango

228
V. Angladon et al.
motion tracking. Therefore, our approach is limited to rooms with reasonable
texture and where all the walls and the ceiling can be observed by the depth
sensor without too many eﬀorts (medium size room with a 5 m ceiling height
maximum).
(a)
Planar patch
extraction
and fusion
Planar patches
model
Visibility
Polygon creation
Wall/Clutter
labeling
Layout
generation
Depth
map
Camera
pose
User interaction
Screen
Labeled
patches
Labeled planar patches
(b)
Fig. 1. (a) Tango Tablet Development Kit image sensors: RGB-IR camera (1), Fisheye
camera (2), IR pattern emitter (3). (b) Our pipeline.
Pipeline. Figure 1b summarizes the proposed pipeline. The Tango Tablet Develop-
ment Kit middleware provides at each iteration the depth map of the scene, as well
as the camera pose. The depth map is processed in order to extract sets of 3D
points corresponding to candidate walls, what we call planar patches (detailed in
Sect. 3). Thanks to the known camera pose, the planar patches are brought into
a global world coordinate system, so that they can be associated and then fused
with the existing patches of the model (Sect. 4). Whenever the model is updated,
the visibility polygon of the discovered area(s) is also updated (Sect. 5), which
can be used as a visual feedback for the user. Furthermore, the visibility polygon
is used by the labeling module to automatically classify the planar patches as
wall or clutter (Sect. 6), thus allowing the disambiguation between actual walls
and other objects that may be lying inside the room. This classiﬁer was trained
beforehand against 900 manually labeled vertical planar regions from our train-
ing dataset. This last task can take advantage of the interaction of the user, who
can correct and change the automatic labeling of the vertical planar patches
into wall or clutter (Sect. 7). Finally, the layout generation module computes
the room layout from the labeled vertical planes and the boundary given by the
visibility polygon (Sect. 8).
3
Planar Patches Extraction
A planar patch is deﬁned as a list of 3D points associated with the equation of
the ﬁtted inﬁnite plane and the boundaries of this patch. We deﬁne the planar
patches extraction problem as a function which takes as input a range image
(also called depth map) and returns a list of planar patches.

Room Floor Plan Generation on a Project Tango Device
229
The common strategies to extract planar patches are the Hough Transform
approaches [18], the region growing algorithms [19], the normal map segmenta-
tion [20] and the split and merge approaches [21].
In our preliminary tests, we noticed that normal map segmentation
approaches were too sensitive to the data noise and required higher computa-
tional time. We chose instead a region growing approach, similar to the algorithm
described by Poppinga et al. [19]. In this approach, a random seed point and
its neighbors are picked and extended by taking into consideration neighboring
points. A plane is estimated on this set of points and a new point is considered
valid when its distance to the plane is small enough. The planar patch keeps
growing iteratively until no valid point can be found in the neighborhood of
the patch. A new seed point is picked until all points have been considered. We
made some modiﬁcations to the original algorithm in order to get better results
with the point cloud provided by the Tango TDK. We ignore the 3D points with
a distance to the camera farther to 3.7 m as they revealed to be very noisy. To
cope with the low density of the depth map, the neighboring selection is not
pixel-wise but performed on a 3 × 3 pixel window. In a post-processing step, we
remove isolated points of the planar patches with a neighborhood pixel analysis
performed in the segmented depth map. This ensure our planar patches are more
compact. Finally, we perform a validation of the planar patch with the RANSAC
algorithm. Without implementing the optimizations suggested by [19], we can
process 10k points in 47 ms with our C++ implementation.
The ceiling and the ﬂoor are incrementally estimated with the minimal and
maximal height of the considered vertical planar patches and validated with the
ﬁtting of horizontal planes at the considered heights. In the following sections,
we consider only the vertical planar patches for which we compute a rectangle
boundary.
4
Planar Patches Model Update
We create a model of planar patches in the world coordinates which is initialized
with the patches of the ﬁrst frame. For each successive frame, we compute planar
patches which are associated with the model with the help of an association
function. We update the associated planar patches of the model with a fusion
function.
Planar Patch Association. Given a list of planar patches from the model and a
list of planar patches from the current frame the association function computes
a list of planar patches pairs where each patch appears only once. We consider
a distance between planar patches which takes into account the normal angle
diﬀerence of the planes and the mean distance of the points-to-plane distances,
using the points of the rectangle boundary of the planar patches. This distance
deﬁnes the candidate pairs of associated patches, which are validated with an
overlapping test between the rectangle boundaries.

230
V. Angladon et al.
Planar Patch Fusion. After two planar patches have been associated, we need
to create a new planar patch combining the two. We compute and update the
covariances matrices of the planes using [22]. The plane equations of the fused
patch is updated with a Principal Component Analysis, in order to avoid the
costly storage of the 3D points associated with the planar patches. We then
update the rectangular boundary to include the two planar patches.
5
Visibility Polygon Computation
During a scan, the user needs to know which parts of the scene he visited or not.
We provide this information with a top-down 2D view representation of the area
observed by the camera in the form of a visibility polygon, similarly to [9], but
computed diﬀerently from the vertical planar patches model and the history of
the camera positions. We denote sv the line segment obtained from the projection
of a vertical planar patch on a horizontal plane. We consider the visibility view
polygon P s associated to each line segment sv, which is formed by the union of
all the triangles formed by the camera position and the two extremities of sv,
see Fig. 2. Each triangle is made of one wall segment and two frustum segments.
The geometric union of all the polygons P s forms the visibility polygon.
Fig. 2. Left: the three blue, cyan and green triangles represent the part of the camera
frustum viewing sv and associated with the camera poses c1, c2 and c3 respectively.
Their union form P s: the visibility view polygon associated with the line segment
sv. Right: the visibility polygon is the union of the visibility view polygons P s, here
represented with diﬀerent colors. (Color ﬁgure online)
We implemented the computation of the visibility polygon with the help of
the geometry engine GEOS [23].
6
Wall-Clutter Separation
While clutter often consists of irregular shapes, such as plants, sofas, etc., which
are eliminated in planar primitive approaches, it can also consist of piecewise
planar shapes such as cupboards, radiators, etc. In this section, we propose a
method to classify the planar primitive as wall or clutter in order to reduce

Room Floor Plan Generation on a Project Tango Device
231
the number of potential irrelevant planes considered by the room layout estima-
tions component. The segmentation of the primitives can be performed individ-
ually [4,6,24], i.e. using features on each primitive considered independently, or
globally [2,7], i.e. considering the adjacent primitives, which provide contextual
information.
Our objective is to build the room layout incrementally, during the scan
progress, which means all the adjacencies are only known at the end of the scan.
Therefore we favored a classiﬁcation with an individual approach. Our features
take the form of a vector (dc, df, l, dv). They include the distances dc between
the highest point of the vertical planar patch and the estimated ceiling and df
between the lowest point of the vertical planar patch and the estimated ﬂoor.
Intuitively, a planar patch both close to the ceiling and ﬂoor is likely to be a
wall. Similarly, a segment sv with a longer length l has a higher probability
to correspond to a wall. We also consider the distance dv = maxp∈sv d(p, ∂Pv)
between a segment sv and the exterior boundary of the visibility polygon ∂Pv.
A high distance dv corresponds to a segment with at least one extremity far
from the visibility polygon, which is likely to correspond to clutter.
We compute a wall probability P(sv) for each segment sv of the model with
a Multi-layer Perceptron classiﬁer using one hidden layer and a logistic sigmoid
activation, trained on our dataset. When a new frame is ingested, we compute
P(sv) if sv was modiﬁed since the previous frame or the ceiling/ﬂoor estimation
changed. Our implementation based on the Python Scikit-learn module can label
50 planar patches in 29 ms on average.
Fig. 3. Visualization of the scan progress of the scene House2. Planar patches classiﬁed
as wall and clutter are represented in green and blue respectively. Intermediate colors
represent intermediate probabilities. Left: augmented view of the device camera with
the detected and classiﬁed planar patches. The estimated room layout is displayed with
black lines. Right: visibility polygon in light gray, camera view polygon in black. (Color
ﬁgure online)
7
User Interaction
Automatic approaches for ﬂoor plan generation or scan to BIM can be prone to
errors. These errors can come from missing data, clutter, sensor noise or some
speciﬁcities in the scene (e.g. small walls, large openings, cavities in the walls,

232
V. Angladon et al.
obstacles fully covering a wall, etc.). Depending on the progress of the scan,
it may not be possible for an algorithm to determine whether a planar patch
corresponds to a wall or clutter.
The user interaction schemes proposed in the literature are usually correc-
tive [7,25,26], where the user intervenes at the end to correct a proposed solu-
tion (commonly considered for oﬄine approaches), or user-driven [12–16] where
no solution can be computed without user interaction (mostly found in online
approaches).
We wanted to propose a collaborative interaction scheme where the interac-
tion would be optional and could be performed at any time. The ﬁrst way to
interact is to move the Tango TDK to make it observe new parts of the scene.
A ﬁrst view displays the camera image augmented with the detected planar
patches colored relatively to their wall probability and the estimated layout,
while a second view provides a top view of the scene with the visibility polygon
and the estimated layout too. The two views shown in Fig. 3 are updated at the
frame rate of the depth sensor, giving an immediate feedback to the user who
can decide to visit the area(s) with missing data. The user can interchange the
wall/clutter labels of the detected patches by touching them in the augmented
view, which directly aﬀects the room layout estimation component. At the end
of the scan, we let the user perform further editions: suppress forgotten clutter
patches and merge planar patches. At the time of writing, we only designed a
desktop prototype which can replay a recorded scan or process live data trans-
mitted by the application. The presented results were obtained by performing
the interactions on replays of the scans.
8
Room Layout Estimation
The Room Layout Estimation takes as input the model of line segments sv
coarsely classiﬁed as wall or clutter and generates a simpliﬁed ﬂoor plan (see
Fig. 1b). In order to compute the room layout, we need to retrieve the topological
relationships between the walls, i.e. their adjacencies.
Related Work. A simple approach is to consider the topology of the room can be
recovered by connecting adjacent segments [2,24,27], which works well for rooms
with low clutter, and when the walls are well separated from the clutter. The use
of cell-complex is very popular for both single rooms [1,28] and multiple rooms
[5–8] layout estimation. The line segments are replaced by inﬁnite lines which
partition the 2D space into polygonal cells. A graph connecting the adjacent
cells is deﬁned: instead of considering topological information on the segments,
adjacency relationship between the cells is taken into account. The inside/outside
label of the cells are computed with a graph-cut algorithm. This approach is
robust to missing data because the extension of the segments is automatically
considered, but it does not simpliﬁes the layout.

Room Floor Plan Generation on a Project Tango Device
233
Weighted graph construction. In our approach, the visibility polygon provides the
topology, and we try to compute a simple chain of segments sv which explains the
visibility polygon. We create a graph from the adjacency relationships between
the segments. For simpliﬁcation purpose, the segments with a low wall prob-
ability (inferior to 0.5 in practice) are discarded. The remaining segments S
are very likely to be close to the boundary of the visibility polygon, but they
might not cover this boundary completely due to missing data. We complete the
boundary with segments sc from the relative complement of S with respect to
the visibility polygon. A node is created at each segment extremity and each
segment intersection as illustrated in Fig. 4a. The nodes belonging to the same
segments are linked by an edge. We assign to each edge e a weight proportional
to 1 −P(sv), where sv is the segment associated to e. The edges corresponding
to frustum segments and completion segments sc are assigned a penalty weight.
With this design, the shortest path, which minimizes the sum of the weights of
its edges, gives more importance to segments with a high wall probability and
favors solutions with a low number of planar patches.
Fig. 4. Toy examples illustrating the graph creation and the solving. Faded colors
represent the segments which are not part of the solution. Red edges correspond to
frustum edges, purple for the other edges. (Color ﬁgure online)
Solving and discussion. We use the Dijkstra’s algorithm to ﬁnd a cycle in our
graph which minimizes the sum of our weights. To avoid trivial solutions, we
use a start point on the segment with a high P(sv), an endpoint which is adja-
cent, and we remove the edge between them. As shown in Fig. 4a, the graph
may contain several cycles which can lead to incorrect layouts. In order to avoid
ﬁnding an incorrect cycle, we impose the segments along the visibility polygon

234
V. Angladon et al.
to correspond to directed edges following a clockwise orientation in the visibility
polygon. This method cannot handle the cycles non adjacent to the visibility
polygon, which revealed to be non-existent in our dataset. In a post-processing
step, we replace, whenever possible, the chains of frustum/completion segments
by lines segments extending their adjacent segments, otherwise we simply join
them. Figure 4b illustrates the solution computed from a simpliﬁed scene. We
also apply the Ramer-Douglas-Peucker algorithm [29] with a low threshold (1 cm)
to simplify near parallel adjacent segments. The diﬀerent steps of this process are
summarized in Fig. 5. Contrary to the oﬄine approaches mentioned earlier, ours
can deliver an immediate room layout. Our Python implementation of this com-
ponent takes 200 ms to generate the graph, and 11 ms to compute the shortest
path and apply the post-processing steps.
For a satisfying user experience, the layout of previously seen areas should
not change when the user visits a new part of the scene. This behavior cannot be
guaranteed with an oﬄine approach. When the ceiling and the ﬂoor are detected,
our method is suitable for incremental changes of the model: the wall probability
P(sv) of the previously observed segments sv does not change, which means the
computed path restricted to the previously seen segments is the same and has
the same cost.
Fig. 5. Four steps of our room layout estimation: the extracted segments, the generated
graph, the shortest path solution, and the layout obtained after post-processing. See
Figs. 3 and 4 for the signiﬁcation of the colors. (Color ﬁgure online)
9
Experimental Results
In this section, we compare –the geometry accuracy of– the room ﬂoor plans
generated with our approach, Magic Plan [14] (run on iPad Air 1, prior to ARKit
release) and FloorPlanEx from Google [15].
Evaluation
protocol.
We
considered
ﬁve
indoor
scenes
Lab1MW,
Lab2,
House1MW, House2MW and House3MW, where MW denotes the scenes respect-
ing the Manhattan World assumption. The ground truth room layouts of these
scenes were created with a Bosh DLE 50 laser ranger ﬁnder. We evaluated the
geometry accuracy of the obtained layouts with the ground truth and the repro-
ducibility of the measurements by repeating the measurements ﬁve times. Each
estimated layout was aligned with the ground truth by computing the trans-
formation which minimizes the distances between their corresponding vertices.

Room Floor Plan Generation on a Project Tango Device
235
The mean of these distances deﬁnes our residual error. We also evaluated the
user eﬀort during the use of the considered mobile applications. Magic Plan and
FloorPlanEx are user-driven applications where the user selects the walls and
the corners, respectively. The number of interactions is equal to the number of
corners (plus one for Magic Plan). We did not count the interactions required
for Magic Plan calibration processes. For our approach, we evaluated the num-
ber of labels corrections on the planar patches and the number of post-scan
modiﬁcations.
Results and analysis. Table 1 and Fig. 6 show the obtained results. Magic Plan esti-
mates the camera-to-corner distances from the device orientation instead of taking
advantage of a localization module or a depth sensor. Consequently, even the best
results of the application are less accurate than the results obtained with the other
approaches. Due to the amount of clutter, most of the corners were captured on the
ceiling, which reduces the accuracy of the measurements. Magic Plan assumes the
angle between two consecutive walls is 90◦or 45◦, for this reason, the results are
unsatisfactory on the scene Lab2 which does not follow the MW assumption. We
can also observe the residual increases with the area of the room, which is coherent
when there is a small error with the device height estimation.
For selling or renting a property in France, the Alur law deﬁnes the maximal
error of the measured area to 5%. The area errors from the FloorPlanEx appli-
cation and our approach are inferior or equal to this threshold, which may not
be enough for some oﬃcial uses. The results show our method is generally more
accurate and provides more repeatable results than FloorPlanEx. One expla-
nation is that we consider the 3D points from multiple frames to estimate the
planes of the walls when the FloorPlanEx only considers the points from one
frame.
Table 1. Results of the geometry accuracy and reproducibility comparison experiment.
Scene
Method
Mean
area err.
Max
area err.
Mean
residual
σresid.
Min
residual
Max
residual
Number
interact.
Lab1MW (25 m2)
Ours
2.3%
4.2%
29 mm
14 mm
14 mm
48 mm
3.25
FloorPlanEx
2.2%
4.5%
47 mm
26 mm
18 mm
84 mm
4
Magic Plan
12%
17%
164 mm
52 mm
106 mm
231 mm
5
Lab2 (47 m2)
Ours
1.1%
2.4%
38 mm
3 mm
35 mm
43 mm
0.75
FloorPlanEx
3.3%
4.3%
73 mm
19 mm
45 mm
100 mm
6
Magic Plan
15%
24%
264 mm
65 mm
199 mm
329 mm
7
House1MW
(11 m2)
Ours
1.8%
2.4%
30 mm
12 mm
14 mm
44 mm
0.5
FloorPlanEx
2.6%
4.5%
53 mm
9 mm
38 mm
60 mm
6
Magic Plan
4.9%
8.8%
66 mm
18 mm
46 mm
87 mm
7
House2MW
(13 m2)
Ours
3.0%
5.0%
29 mm
12 mm
17 mm
49 mm
0
FloorPlanEx
3.3%
4.2%
41 mm
11 mm
28 mm
58 mm
4
Magic Plan
5.4%
9.2%
50 mm
23 mm
22 mm
89 mm
5
House3MW
(48 m2)
Ours
1.9%
2.3%
32 mm
5 mm
27 mm
37 mm
7.5
FloorPlanEx
2.8%
4.0%
105 mm
23 mm
78 mm
144 mm
6
Magic Plan
15.4%
24.9%
233 mm
79 mm
163 mm
344 mm
7

236
V. Angladon et al.
The last column of Table 1 describes the degree of interaction. It con-
ﬁrms our approach generally requires fewer screen interactions than user-driven
approaches, except for the scene House3MW which contained a ﬁreplace, high
furnitures and many curtains. The Lab1MW was also quite challenging because
of the presence of a high cupboard and pillars which were incorrectly labeled as
wall.
Fig. 6. First row: the point clouds and the labeled planar patches (blue: clutter, green:
walls) of the scanned rooms: Lab1MW, Lab2, House1MW, House2MW and House3MW
(from left to right). Second row: comparison of our approach in orange with the ground
truth in blue. Third and fourth row: comparison of the room layout obtained with
FloorPlanEx and Magic Plan in orange and the ground truth in blue. (Color ﬁgure
online)
10
Conclusion
We presented an online approach to estimate and measure the room layout of
an indoor scene using a Project Tango mobile device. The various components of
our pipeline were evaluated on a desktop computer, using scans recorded from
a Project Tango mobile device. Compared to existing online works, the proposed
method relies on scene understanding to compute the room layout. In order to

Room Floor Plan Generation on a Project Tango Device
237
cope with the complexity of some rooms, the user can interact in a coopera-
tive way to add or remove walls among the observed planes. The comparison
with other mobile applications demonstrates that in general, we achieve higher
accuracy. In term of eﬃciency, the number of user interactions depends on the
complexity of the scenes, which may contain clutter data incorrectly classiﬁed
as walls. Some limitations of the proposed method are implicitly associated to
the Tango TDK: drift of the device pose and surfaces not handled by the depth
sensors, such as mirrors and glasses. Applications of our works include ﬂoor plan
generation, area estimation and room reconstruction for virtual reality games.
References
1. Budroni, A., Boehm, J.: Automated 3D reconstruction of interiors from point
clouds. Int. J. Archit. Comput. 8(1), 55–73 (2010)
2. Xiong, X., Adan, A., Akinci, B., Huber, D.: Automatic creation of semantically rich
3D building models from laser scanner data. Autom. Constr. 31, 325–337 (2013)
3. Xiao, J., Furukawa, Y.: Reconstructing the world’s museums. Int. J. Comput. Vis.
110(3), 243–258 (2014)
4. Stambler, A., Huber, D.: Building modeling through enclosure reasoning. In: 2014
2nd International Conference on 3D Vision (3DV), vol. 2, pp. 118–125. IEEE (2014)
5. Oesau, S., Lafarge, F., Alliez, P.: Indoor scene reconstruction using feature sensitive
primitive extraction and graph-cut. ISPRS J. Photogram. Remote Sens. 90, 68–82
(2014)
6. Mura, C., Mattausch, O., Villanueva, A.J., Gobbetti, E., Pajarola, R.: Automatic
room detection and reconstruction in cluttered indoor environments with complex
room layouts. Comput. Graph. 44, 20–32 (2014)
7. Mura, C., Mattausch, O., Pajarola, R.: Piecewise-planar reconstruction of multi-
room interiors with arbitrary wall arrangements. In: Proceedings of the 24th Paciﬁc
Conference on Computer Graphics and Applications, PG 2016, Goslar, Germany,
pp. 179–188. Eurographics Association (2016)
8. Ochmann, S., Vock, R., Wessel, R., Klein, R.: Automatic reconstruction of para-
metric building models from indoor point clouds. Comput. Graph. 54, 94–103
(2016)
9. Zhang, Y., Luo, C., Liu, J.: Walk&sketch: create ﬂoor plans with an RGB-D cam-
era. In: UbiComp (2012)
10. Zhang, Y., Xu, W., Tong, Y., Zhou, K.: Online structure analysis for real-time
indoor scene reconstruction. ACM Trans. Graph. (TOG) 34(5), 159 (2015)
11. Dzitsiuk, M., Sturm, J., Maier, R., Ma, L., Cremers, D.: De-noising, stabilizing
and completing 3D reconstructions on-the-go using plane priors. In: 2017 IEEE
International Conference on Robotics and Automation (ICRA), pp. 3976–3983.
IEEE (2017)
12. Rosser, J., Morley, J., Smith, G.: Modelling of building interiors with mobile phone
sensor data. ISPRS Int. J. Geo-Inf. 4(2), 989–1012 (2015)
13. Pintore, G., Agus, M., Gobbetti, E.: Interactive mapping of indoor building struc-
tures through mobile devices. In: 2014 2nd International Conference on 3D Vision
(3DV), vol. 2, pp. 103–110. IEEE (2014)
14. Sensopia: MagicPlan, Create a ﬂoor plan in just a few minutes (2012). http://
www.magic-plan.com

238
V. Angladon et al.
15. Google: Java ﬂoor plan example create a ﬂoor plan by using the depth sensor of a
Google Tango device to detect and measure walls in a room (2016). https://goo.
gl/3Ns3XY
16. Google: Measure: augmented reality measurement application for Google Tango
devices (2016). https://goo.gl/2RyPnz
17. Zlatanova, S., Sithole, G., Nakagawa, M., Zhu, Q.: Problems in indoor mapping
and modelling. In: Acquisition and Modelling of Indoor and Enclosed Environments
2013, Cape Town, South Africa, 11–13 December 2013, ISPRS Archives Volume
XL-4/W4 (2013)
18. Borrmann, D., Elseberg, J., Lingemann, K., N¨uchter, A.: The 3D Hough Transform
for plane detection in point clouds a review and a new accumulator design. 3D Res.
2(2), 32:1–32:13 (2011)
19. Poppinga, J., Vaskevicius, N., Birk, A., Pathak, K.: Fast plane detection and polyg-
onalization in noisy 3D range images. In: 2008 IEEE/RSJ International Conference
on Intelligent Robots and Systems, pp. 3378–3383, September 2008
20. Holz, D., Holzer, S., Rusu, R.B., Behnke, S.: Real-time plane segmentation using
RGB-D cameras. In: R¨ofer, T., Mayer, N.M., Savage, J., Saranlı, U. (eds.) RoboCup
2011. LNCS (LNAI), vol. 7416, pp. 306–317. Springer, Heidelberg (2012). https://
doi.org/10.1007/978-3-642-32060-6 26
21. Feng, C., Taguchi, Y., Kamat, V.R.: Fast plane extraction in organized point clouds
using agglomerative hierarchical clustering. In: 2014 IEEE International Confer-
ence on Robotics and Automation (ICRA), pp. 6218–6225, May 2014
22. Lee, T.K., Lim, S., Lee, S., An, S., Oh, S.Y.: Indoor mapping using planes extracted
from noisy RGB-D sensors. In: 2012 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (2012)
23. OSGeo: GEOS, Geometry Engine (2000). https://trac.osgeo.org/geos/
24. Murali, S., Speciale, P., Oswald, M.R., Pollefeys, M.: Indoor Scan2BIM: building
information models of house interiors. In: IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) (2017)
25. Arikan, M., Schw¨arzler, M., Fl¨ory, S., Wimmer, M., Maierhofer, S.: O-snap:
optimization-based snapping for modeling architecture. ACM Trans. Graph.
(TOG) 32(1), 6 (2013)
26. Pintore, G., Ganovelli, F., Gobbetti, E., Scopigno, R.: Mobile mapping and visual-
ization of indoor structures to simplify scene understanding and location awareness.
In: Hua, G., J´egou, H. (eds.) ECCV 2016. LNCS, vol. 9914, pp. 130–145. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-48881-3 10
27. Valero, E., Ad´an, A., Cerrada, C.: Automatic method for building indoor boundary
models from dense point clouds collected by laser scanners. Sensors 12(12), 16099–
16115 (2012)
28. Previtali, M., Barazzetti, L., Brumana, R., Scaioni, M.: Towards automatic indoor
reconstruction of cluttered building rooms from point clouds. ISPRS Ann. Pho-
togram. Remote Sens. Spat. Inf. Sci. 2(5), 281 (2014)
29. Ramer, U.: An iterative procedure for the polygonal approximation of plane curves.
Comput. Graph. Image Process. 1(3), 244–256 (1972)

Scalable Bag of Selected Deep Features
for Visual Instance Retrieval
Yue Lv1, Wengang Zhou1(B), Qi Tian2, and Houqiang Li1
1 University of Science and Technology of China,
Hefei, Anhui, People’s Republic of China
lvyue@mail.ustc.edu.cn, {zhwg,lihq}@ustc.edu.cn
2 University of Texas at San Antonio, San Antonio, USA
qi.tian@utsa.edu
Abstract. Recent studies show that aggregating activations of convo-
lutional layers from CNN models together as a global descriptor leads to
promising performance for instance retrieval. However, due to the global
pooling strategy adopted, the generated feature representation is lack
of discriminative local structure information and is degraded by irrele-
vant image patterns or background clutter. In this paper, we propose a
novel Bag-of-Deep-Visual-Words (BoDVW) model for instance retrieval.
Activations of convolutional feature maps are extracted as a set of indi-
vidual semantic-aware local features. An energy-based feature selection
is adopted to ﬁlter out features on homogeneous background with poor
distinction. To achieve the scalability of local feature-level cross match-
ing, the local deep CNN features are quantized to adapt to the inverted
index structure. A new cross-matching metric is deﬁned to measure image
similarity. Our approach achieves respectable performance in compari-
son to other state-of-the-art methods. Especially, it is proved to be more
eﬀective and eﬃcient on large scale datasets.
Keywords: Instance retrieval · Local deep features
Feature selection · Bag-of-Deep-Visual-Words
1
Introduction
In instance retrieval, the goal is to eﬃciently identify relevant images to a visual
instance query from a very large image database. Unlike semantic classiﬁca-
tion, visual instance retrieval focuses more on local visual patterns matching.
Due to the success of local invariant features (e.g., SIFT [10]), local visual pat-
tern matching based on handcrafted descriptors has been widely used for visual
instance retrieval [21,22,24,25]. With SIFT feature for image representation,
thousands of features can be extracted from a single image. To achieve search
eﬃciency with large image corpus, Bag-of-Words (BoW) model with inverted
index structure is introduced and has achieved great success.
Recent studies have shown visual features extracted from convolutional lay-
ers of pre-trained convolutional neural networks (CNN) achieve promising per-
formance for image retrieval [1,7,9,17,20,23]. Comparing to the conventional
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 239–251, 2018.
https://doi.org/10.1007/978-3-319-73600-6_21

240
Y. Lv et al.
local descriptors, these deep features capture more semantic information as they
are learned from massive amount of data in a supervised manner. Most exist-
ing approaches [1,7] usually aggregate these features over the whole image or
some predeﬁned regions into a compact representation, and evaluate the query-
to-image similarity at image or region level. However, due to the global pool-
ing strategy adopted, the discriminative local structure information is not fully
utilized. Besides, it also remains a problem to avoid the inﬂuence of cluttered
backgrounds.
In this paper, we propose a Bag-of-Deep-Visual-Words (BoDVW) approach
for eﬀective and eﬃcient instance retrieval. Figure 1 illustrates the framework
of the proposed method. Following the direction of the CNN-based methods,
we extract image features from the convolutional layer in a deep CNN model.
These deep features capture high-level semantic information of local patches,
and can be viewed as densely sampled local features like dense SIFT in some
way. To ﬁlter out those features from homogeneous background which are not
distinctive, we propose an energy-based feature selection scheme. Unlike previous
work, which aggregate features into a compact descriptor, we represent an image
by a set of carefully selected local deep CNN features in order to keep the locality
information to address. The similarity of two images is measured by the matching
score of their selected features. We use improved BoW method as a fast matching
technique. Instead of applying TF-IDF score as in the traditional BoW model,
we propose a new similarity metric based on local deep feature matching, and
only preserve the best matched pairs for the ﬁnal image similarity computing. To
further reduce the computational complexity, we generate binary signature for
each feature with the iterative quantization (ITQ) [3] algorithm. Experiments
on the public datasets demonstrate the eﬀectiveness of the proposed algorithm.
Fig. 1. Framework of the Bag-of-Deep-Visual-Words approach
2
Related Work
Recently, witnessing the great success of deep convolutional neural networks
(CNN) in image classiﬁcation, lots of researchers have leveraged the features
learned from deep CNN model to other visual applications. In instance retrieval,
descriptors generated by CNN achieve promising performance. Early CNN-based
methods [2,4,13,16] proposed to use the outputs of the fully-connected layer

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
241
activations as global image descriptors. Razavian et al. [13] investigated the use
of CNN models trained on ImageNet dataset as black box descriptor extractors
for image retrieval and achieved considerable results. Babenko et al. [2] compared
features extracted from diﬀerent layers as a global image representation and
achieved a state-of-the-art performance with very short codes. Gong et al. [4]
used VLAD-embedding to aggregate the activations of a fully-connected layer
extracted from diﬀerent patches of the image.
Features extracted from convolutional layers have demonstrated with bet-
ter performance than extracted from fully-connected layers in recent works
[1,7,14,17]. Compared with the fully-connected layer features with global recep-
tive ﬁeld, convolutional layer features can preserve more local characteristics
of the input image and are more robust to image transformations. Babenko et
al. [1] investigated possible ways to aggregate local deep features to produce
compact global descriptors and proposed to use a simple but eﬀective sum-
pooling aggregation strategy. Kalantidis et al. [7] extended [1] by introducing
cross-dimensional weighting scheme. Tolias et al. [17] proposed an integral max-
pooling method over multiple regions and achieved state-of-the-art performance.
Our approach is similar with [13,14] in that the cross-matching strategy
is adopted. Both [13,14] extracted the deep features from multi-scale regions,
performed region cross-matching and accumulated the maximum similarity per
query region. Given a query image, they cross-matched all sub-patches which
results in too much computational overhead to handle large-scale datasets. Dif-
ferent from these methods, we extract local deep features without feeding mul-
tiple inputs to the network. Besides, we propose BoDVW approach with a new
feature selection scheme to accelerate the speed of matching.
3
The Proposed Method
In this section, we give a formal description of our image retrieval pipeline.
Firstly, we propose an image-level similarity criterion based on local feature
cross-matching in Sect. 3.1. After that, we introduce an energy-based fea-
ture selection approach to ﬁlter out features from homogeneous background
in Sect. 3.2. Finally, in Sect. 3.3, we present our Bag-of-Deep-Visual-Words
(BoDVW) model for eﬀective and eﬃcient instance retrieval.
3.1
Feature Cross-Matching
Given a pre-trained CNN network with the fully-connected layers discarded, an
input image is passed through the network. The activations of a convolutional
layer form a 3D tensor of H × W × K dimensions, where W and H denote
the height and width of each feature map. K denotes the number of the output
feature maps. At each location (i, j) in the output feature maps, where 1 ≤i ≤H
and 1 ≤j ≤W, we obtain a K-dimensional vector fi,j ∈RK corresponding to a
certain image region according to the layer’s receptive ﬁeld. In this way, we obtain
H × W feature vectors for each image at the convolutional layers, denoted as

242
Y. Lv et al.
F = {f1, f2, · · · , fH×W }. In our experiment, we do not resize the input images.
Features extracted from convolutional layers correspond to local image regions,
capturing the local detail information of the input image, and thus can be viewed
as local features sampled from a dense sampling grid.
0.715                 0.703                0.695                 0.690                0.685
0.517                0.511                0.501                0.501                0.500
0.312                 0.309              0.308                 0.304                0.301
query
Fig. 2. The visual similarity between query patch and dataset image patches. The
image on the left denotes the query, while those on the right denote patches from the
database. The similarity score are shown below each image.
We perform an experiment to demonstrate the discriminative power of the
proposed feature representation. The features are taken from images randomly
selected in Paris Dataset [12], and are extracted from the last pooling layer of
AlexNet [8]. We compute the cosine similarities between each pair of features.
The matching scores and the corresponding image patches are illustrated in
Fig. 2. For each feature, we crop the 100 × 100 image patch at center of its
receptive ﬁeld for demonstration.
As we can see from Fig. 2, image patches with higher cosine similarity scores
to the query are more visually similar as well. The results indicate that simi-
larity of convolutional features is a suitable metric to quantify the similarity of
local visual patterns. Further, we can deﬁne the image-level similarity criterion
based on local feature cross-matching. The distance between query image Iq and
reference image Ir is deﬁned as the sum of Euclidean distances between query
features and their nearest features from reference image:
d(Iq, Ir) =
Nq

i=1
min
1≤j≤Nr d(f q
i , f r
j ),
(1)
where N q and N r are feature number of the query image and the reference image,
respectively. {f q
i } and {f r
i } denote the feature vector set of image Iq and Ir,
respectively. All features are l2-normalized before the distance computation.
There are two problems to be addressed when adopting cross-matching
directly between image Ii and image Ij. Firstly, features from homogeneous

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
243
background will distract similarity computing. To address this problem, in
Sect. 3.2, an energy based feature selection scheme is introduced to ﬁlter out
unnecessary features from background regions. Secondly, we need to match
N i×N j pairs of individual local deep features when applying the cross-matching
similarity criterion. Typically, for an image of size 1024 × 768, N i is around 350,
resulting in high computational complexity. To address this problem, we pro-
pose a Bag-of-Deep-Visual-Words (BoDVW) model in Sect. 3.3, which combines
a binary hashing based on ITQ algorithm to boost the matching procedure.
Fig. 3. Query images of Oxford and heatmaps of their convolutional feature maps. The
heatmaps are generated according to each feature’s L2 norm. Top: images. Middle: heat
maps. Bottom: positions of selected features in convolutional feature maps, where the
white regions are locations of the selected features.
3.2
Feature Selection
Typically, those local regions containing distinctive visual patterns obtain high
activations at the corresponding locations of convolutional feature maps. On the
contrary, features with low activations usually correspond to the homogeneous
background. To illustrate this fact, we select several images from Oxford dataset
and extract features from the last pooling layer on AlexNet. Then we compute
the energy (L2 norm) of each feature and show the heatmaps of the corresponding
convolutional feature maps in the second row of Fig. 3. We see that the locations
of high activations in heatmaps correspond well to the distinctive visual patterns
in images.
We use L2 norm to measure the importance of a feature and perform feature
selection to reduce the impact of background and irrelevant image patterns.
Feature selection is applied to each image independently. For the feature set F
of an image, the selected features are T = {fi|fi ∈F and ∥fi∥2 ≥ε}, where
ε is the median of {∥fi∥2}, fi ∈F. As a result, only half of those local deep
features are kept. This process is similar to rejecting keypoints of low contrast

244
Y. Lv et al.
in the accurate keypoint localization step of SIFT, except that our method uses
energy as a metric for feature selection.
The selected features of some sample images are illustrated in the bottom row
of Fig. 3. Obviously, feature selection ﬁlters out less important regions like grass,
wall, ground, and sky. The selected features carry discriminative semantic fea-
tures. Besides, feature selection can also cut down the computational cost. In the
follow-up experiments, we will demonstrate that feature selection signiﬁcantly
boosts the performance of instance retrieval.
3.3
Bag-of-Deep-Visual-Words (BoDVW) Model
Feature Index and Distance Measurement. To eﬃciently compute the
cross-matching of local deep features between images, we make some improve-
ments to the BoW model and inverted index structure. In the classical BoW
model, we count the number of features that are assigned to the same visual
word, and compute TF-IDF scores accordingly, which is a voting method in
practice. Our improved BoW model computes matching scores in a diﬀerent
way, which is an approximation of the cross-matching.
To build the codebook of BoW, we perform Approximate K-means clustering
on extracted local deep features from the cleaned Landmarks dataset [5]. In the
indexing and search phases, we assign all features to k-nearest visual words in
the codebook with Approximate Nearest Neighbors (ANN) algorithm. Multiple
assignment is applied here to improve recall. In the experiment, k is set to 3.
In online retrieval, query features are only compared with database features
that are assigned to the same visual word. Considering that it is both memory-
consuming and time-consuming to store features and compute their distances,
we transform them to distance-preserving compact binary signatures in Ham-
ming space. In our method, iterative quantization (ITQ) [3] algorithm is used to
generate the binary signature which is called ITQ code. We store the feature’s
ITQ code as well as its image ID in our inverted index structure (Fig. 1). The
detail of binary signature generation is discussed in the following subsection.
Let Iq and Ir be the query image and reference image, respectively. The
distance between their features f q
i and f r
j is deﬁned as:
dH(f q
i , f r
j ) =
g(f q
i ,f r
j ),
if vw(f q
i )=vw(f r
i ),
α,
otherwise,
(2)
where vw(f q
i ) = vw(f r
i ) denotes that f q
i and f r
j are assigned to the same visual
word, g(f q
i , f r
j ) denotes the Hamming distance between the ITQ codes of f q
i
and f r
j , and α is a constant.
We deﬁne the distance of Iq and Ir as:
d(Iq, Ir) =
Nq

i=1
min
1≤j≤Nr dH(f q
i , f r
j ).
(3)
This equation is essentially an approximation to Eq. (1), with the only diﬀerence
that we compute Hamming distance of the features assigned to the same visual
words, which eﬀectively reduces the computational complexity.

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
245
Binary Signature Generation. We generate a compact binary code for each
K-dim ﬂoating feature by the iterative quantization (ITQ) [3] algorithm. ITQ is
one of the state-of-the-art distance preserving hashing methods and is suitable for
feature matching. We speculate the other state-of-the-art hashing algorithms [18]
are potential alternatives as well. With ITQ, the ITQ code of a feature fi is
computed as:
f IT Q
i
= sign(fi
W),
(4)
where the projection matrix 
W ∈RK×B is learnt by ITQ. 
W = WR, where
W ∈RK×B is the PCA projection matrix and R ∈RB×B is learned iteratively.
B denotes the length of the generated ITQ code. In our experiment, B is set to
128-bit as a trade-oﬀbetween retrieval accuracy and speed. Besides, K equals
to 128 as we perform PCA to reduce the dimension of our local feature vectors.
For features extracted from the last pooling layer of VGG19 model [15],
the original feature is 512-D ﬂoating point vector, which is 16384-bit. After
iterative quantization, the new compact representation saves 99.2% storage cost.
Besides, the distance of two binary codes can be more eﬃciently computed using
Hamming distance.
4
Experiment
4.1
Experimental Setting
We evaluate the proposed method on three public benchmark datasets: Oxford5k
[11], Paris6k [12] and Holidays [6]. The Oxford5k and Paris6k datasets contain
5,063 and 6,412 images, respectively, both with 55 queries coming from 11 classes.
For each query in these datasets, bounding box is used to identify the target
object. Additional 100k Flickr [11] images are added to compose Oxford105k
and Paris106k, respectively. The Holidays dataset contains 1,491 images of per-
sonal holiday photos, of which 500 images are used as queries. Following the
standard evaluation protocol, we use mean Average Precision (mAP) to mea-
sure the retrieval performance.
The focus of our work is to fully use the features extracted from the pre-
trained CNN models. Two widely used networks, AlexNet [8] and the very deep
network (VGG19) [15] pre-trained on ImageNet are evaluated in our experiment.
Without resizing the input images, we extract the features from the last pooling
layer, which has 256 feature channels for AlexNet and 512 for VGG19. Then
we conduct the feature selection and post-process features with signed square
normalization, PCA-whitening, and L2-normalization. For all experiments in
this paper, we reduce the dimensionality of the feature vector to 128 with PCA
to reduce the time cost when assigning features to the visual words. The cleaned
Landmarks dataset [5] is related to our task and has no overlapped classes with
the Oxford5k, Paris6k, and Holidays. So we use it to learn our parameters,
including PCA in feature post-processing and projection matrix in ITQ. Besides,
it is also used to train vocabularies in our BoDVW model.

246
Y. Lv et al.
70
75
80
85
40
45
50
55
60
65
70
mAP
α
Paris6K
Oxford5k
Holidays
60
65
70
75
80
85
40
45
50
55
60
65
70
mAP
α
Paris6k
Oxford5k
Holidays
(a) VGG19
(b) AlexNet
Fig. 4. The impact of α on retrieval performance. (a) and (b) denote features extracted
from VGG19 and AlexNet, respectively.
4.2
Impact of Feature Selection
In the previous description, we show that features with low L2 norm values
are more likely to correspond to the homogeneous background. In our experi-
ments, we select the features with large L2 norm values, which preserves about
90% energy of all the features for each image. Here we conduct an experiment
to verify the eﬀectiveness of feature selection. We compare the performance
through direct cross-matching of features with and without the feature selection
on Oxford5k, Paris6k and Holidays, respectively. The results are shown in Table 1
(DCM and DCM+FS). It is seen that the feature selection signiﬁcantly boosts
the performance on instance retrieval. Speciﬁcally, with features extracted from
VGG19, 9%, 7.5% and 4.1% performance improvement are achieved after feature
selection on Oxford5k, Paris6k and Holidays, respectively.
4.3
Impact of Parameters
As described in Eq. (2), α is a parameter which measures the distance of two
features belonging to diﬀerent visual words. We compare the performance on
three benchmarks with diﬀerent α values. The vocabulary size is set to 10k. As
shown in Fig. 4, the best performance is obtained when α is 55. Thus, we ﬁx α
to 55 in the following experiments.
Vocabulary size is a factor aﬀecting the vector quantization loss. Figure 5
shows the performance with diﬀerent vocabulary sizes. Better performance is
obtained with smaller codebook size, in which case more features are compared
with each query feature. Therefore, the similarity computed between query and
database image is more accurate. However, this will slow the matching process.
We set the codebook size to 10k as a trade-oﬀbetween speed and performance.

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
247
60
65
70
75
80
85
5K
10K
20K
50K
mAP
(a) VGG19
Paris6k
Oxford5k
Holidays
60
65
70
75
80
85
5K
10K
20K
50K
mAP
(b) AlexNet
Paris6k
Oxford5k
Holidays
Fig. 5. The impact of vocabulary size on retrieval performance. The horizontal axis
denotes the vocabulary size varying from 5k to 50k. (a) and (b) denote features
extracted from VGG19 and AlexNet, respectively.
4.4
Impact of Quantization
As analyzed above, direct cross-matching leads to too much computational over-
head to handle large-scale datasets. In order to reduce the cost of the storage
and computation, we propose a BoDVW model, which is involved with two-step
quantizations, i.e., Approximate K-means (AKM) based vector quantization and
ITQ-based binary hashing. Here we discuss their impact on performance and
computational cost.
Table 1. Performance (mAP) comparison among diﬀerent settings. DCM: direct cross-
matching with all features. DCM+FS: DCM with selected features. DCM+FS+ITQ:
DCM with binary selected features. BoDVW: our ﬁnal model with feature selection
and two-step quantizations.
Dataset
VGG19
AlexNet
DCM DCM+FS DCM+FS+ITQ BoDVW DCM DCM+FS DCM+FS+ITQ BoDVW
Oxford5k 73.4
82.4
79.7
77.6
64.8
76.6
73.8
70.9
Paris6k
80.7
88.2
86.3
85.0
73.6
82.3
79.3
76.9
Holidays
82.0
86.1
84.9
83.8
81.8
86.6
85.2
83.9
We compare the impact of quantization in terms of retrieval accuracy in
Table 1, and in terms of computational and memory cost in Table 2. For accu-
racy, slight performance loss is observed when applying quantization with both
CNN models on the three benchmarks. Concretely, the ITQ-based binary hash-
ing introduces less than 3% accuracy loss, and AKM-based vector quantization
leads to around 1% further loss with VGG19 and 2% with AlexNet. However,
we observe considerable complexity alleviation after two-step quantizations. For
time cost, the complexity of our method is about O( N·M 2
K
), where N denotes the
image database size, K denotes the codebook size, and M denotes the average
number of the selected features for each image. Speciﬁcally, Hamming distance

248
Y. Lv et al.
Table 2. The computational cost per image on average for diﬀerent methods, including
the searching time (TIME, in seconds) and the memory footprints (MEM, in KB).
The experiments are conducted on Oxford105k dataset with 3.6 GHz i7-4790 CPU in
single core mode. DCM: direct cross-matching with all features. DCM+FS: DCM with
selected features. DCM+FS+ITQ: DCM with binary selected features. BoDVW: our
ﬁnal model with feature selection and two-step quantizations.
DCM
DCM+FS DCM+FS+ITQ BoDVW
TIME 1863.4 472.7
93.1
0.05
MEM
370
185
5.6
17
computing of ITQ codes reduces the average query time from 472.7 s to 93.1 s,
while applying BoDVW further reduces the time cost to only 0.05 s. For memory
cost, 185KB memory is required to store each database image without quanti-
zation, but we only need 5.6 KB to store the ITQ codes, and 17 KB for BoDVW
due to applying multiple assignment. Moreover, our ﬁnal model requires 5MB
memory to store the codebook, and the ﬁnal index of Oxford105k dataset takes
about 1.8 GB memory. As a result, there are about 10,000 times faster and 10
times storage reduction with less than 5% performance degradation.
4.5
Comparison to the State-of-the-Arts
We compare the proposed method with the state-of-the-art algorithms based on
CNN features. The results are shown in Table 3. Considering the methods with-
out re-ranking and query expansion, our method beats all other approaches on
Oxford5k and Paris6k, especially outperforming the best competing method [7]
by 9.4% on Oxford5k. In large scale experiments, our approach is signiﬁcantly
better than other methods, outperforming the best competing method by 11.7%
on Oxford105k and 3.7% on Paris106k, respectively. The reason of our good
performance is the full use of the rich local semantic information provided by
local deep features and our approach is robust to image clutter thanks to the
feature selection strategy. [14] achieves astounding performance, more speciﬁ-
cally, 84.3% on Oxford5k, 87.9% on Paris6k, 89.6% on Holidays. However, in
our experiments, we did not compare with this method because its complexity
is very high, e.g., it takes 30–40 s on a single CPU to process a single query on
Oxford5k.
We also compare computational cost with the state-of-the-art methods in an
one-million dataset. As shown in Table 3, it is obverse that we spend less time
than others due to introduction of inverted indexing structure. Typically, our
algorithm is 5 times faster than the best competing method CroWs [7] and R-
MAC [17]. Besides, those methods use exhaustive matching scheme which will
result in a linear increase in time overhead with the increase of dataset. However,
our approach will not suﬀer as much as these methods due to introduction of
inverted indexing structure, which makes our approach has a greater advantage
in scalability for large-scale database.

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
249
Table 3. Performance (mAP) and computational cost comparison with the state-of-
the-arts. The column TIME (in seconds) is the average retrieving time per image on
an one million dataset and the experiments are conducted on Flickr-MIR dataset with
3.6 GHz i7-4790 CPU in single core mode.
Method
Network
Dim
Oxford5k Oxford105k Paris6k Paris106k Holidays TIME
Neural codes [2]
AlexNet
128
55.7
52.3
-
-
78.9
0.5
R-MAC [17]
AlexNet
256
56.1
47.0
72.9
60.1
-
1
BoDVW
AlexNet
BoW-10k 70.9
68.0
76.9
69.1
83.9
0.4
CNNaug-ss [13]
OverFeat 4–15k
68.0
-
79.5
-
84.3
281.3
Conv-VLAD [19] VGG16
128
59.3
-
67.0
-
81.6
0.5
SPoC [1]
VGG19
256
53.1
50.1
-
-
80.2
1
R-MAC [17]
VGG16
512
66.9
61.6
83.0
75.7
-
2
CroW [7]
VGG16
512
68.2
63.2
79.6
71.0
84.9
2
BoDVW
VGG19
BoW-10k 77.6
74.9
85.0
79.4
83.8
0.4
5
Conclusions
In this work, we propose a scalable local deep feature matching method for
visual instance retrieval. Local deep CNN features are extracted and ﬁltered
with an energy-based selection scheme. To achieve the scalability of local deep
feature matching, inverted index with quantization is adopted. To eﬃcient verify
this distance of matched features, distance-preserving binary hashing by ITQ is
exploited. We obtain state-of-the-art performance on several retrieval datasets.
The performance will be further improved by other post-processing algorithms,
like re-ranking and query expansion. In our future work, we will explore various
post-processing approaches to further boost our retrieval performance.
Acknowledgement. This work was supported in part to Prof. Houqiang Li by 973
Program under contract No. 2015CB351803, NSFC under contract No. 61325009 and
No. 61390514, in part to Dr. Wengang Zhou by NSFC under contract No. 61472378 and
No. 61632019, the Young Elite Scientists Sponsorship Program by CAST under Grant
2016QNRC001, and the Fundamental Research Funds for the Central Universities, and
in part to Dr. Qi Tian by ARO grant W911NF-15-1-0290 and Faculty Research Gift
Awards by NEC Laboratories of America and Blippar. This work was supported in
part by NSFC under contract No. 61429201.
References
1. Babenko, A., Lempitsky, V.: Aggregating local deep features for image retrieval.
In: ICCV (2015)
2. Babenko, A., Slesarev, A., Chigorin, A., Lempitsky, V.: Neural codes for image
retrieval. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014.
LNCS, vol. 8689, pp. 584–599. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-10590-1 38

250
Y. Lv et al.
3. Gong, Y., Lazebnik, S., Gordo, A.: Iterative quantization: a procrustean approach
to learning binary codes for large-scale image retrieval. TPAMI 35(12), 2916–2929
(2013)
4. Gong, Y., Wang, L., Guo, R., Lazebnik, S.: Multi-scale orderless pooling of deep
convolutional activation features. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars,
T. (eds.) ECCV 2014. LNCS, vol. 8695, pp. 392–407. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-10584-0 26
5. Gordo, A., Almazan, J., Revaud, J., Lualus, D.: End-to-end learning of deep visual
representations for image retrieval. arXiv preprint arXiv:1610.07940 (2016)
6. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometric con-
sistency for large scale image search. In: Forsyth, D., Torr, P., Zisserman, A. (eds.)
ECCV 2008. LNCS, vol. 5302, pp. 304–317. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-88682-2 24
7. Kalantidis, Y., Mellina, C., Osindero, S.: Cross-dimensional weighting for aggre-
gated deep convolutional features. In: Hua, G., J´egou, H. (eds.) ECCV 2016. LNCS,
vol. 9913, pp. 685–701. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46604-0 48
8. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems, pp. 1097–1105 (2012)
9. Li, Y., Kong, X., Zheng, L., Tian, Q.: Exploiting hierarchical activations of neural
network for image retrieval. In: ACM MM, pp. 132–136 (2016)
10. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2),
91–110 (2004)
11. Philbin, J., Chum, O., Isard, M.: Object retrieval with large vocabularies and fast
spatial matching. In: CVPR (2007)
12. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Lost in quantization:
improving particular object retrieval in large scale image databases. In: CVPR
(2008)
13. Razavian, A.S., Azizpour, H., Sullivan, J.: CNN features oﬀ-the-shelf: an astound-
ing baseline for recognition. In: CVPRW (2014)
14. Razavian, A.S., Sullivan, J., Carlsson, S.: Visual instance retrieval with deep con-
volutional networks. ITE Trans. Media Technol. Appl. 4(3), 251–258 (2016)
15. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
16. Sun, S., Zhou, W., Tian, Q., Li, H.: Scalable object retrieval with compact image
representation from generic object regions. TOMM 12(2), 29 (2016)
17. Tolias, G., Sicre, R., J´egou, H.: Particular object retrieval with integral max-
pooling of CNN activations. In: ICLR (2016)
18. Wang, M., Zhou, W., Tian, Q., Li, H.: A general framework for linear distance
preserving hashing. TIP (2017)
19. Ng, J.Y.-H., Yang, F., Davis, L.S.: Exploiting local features from deep networks
for image retrieval. In: CVPRW, pp. 53–61 (2015)
20. Zheng, L., Yang, Y., Tian, Q.: SIFT meets CNN: a decade survey of instance
retrieval. TPAMI (2017)
21. Zhou, W., Li, H., Lu, Y., Tian, Q.: Large scale partial-duplicate image retrieval
with bi-space quantization and geometric consistency. In: ICASSP, pp. 2394–2397
(2010)
22. Zhou, W., Li, H., Yijuan, L., Tian, Q.: Principal visual word discovery for auto-
matic license plate detection. TIP 21(9), 4269–4279 (2012)

Scalable Bag of Selected Deep Features for Visual Instance Retrieval
251
23. Zhou, W., Li, H., Sun, J., Tian, Q.: Collaborative index embedding for image
retrieval. TPAMI (2017)
24. Zhou, W., Lu, Y., Li, H., Song, Y., Tian, Q.: Spatial coding for large scale partial-
duplicate web image search. In: ACM MM (2010)
25. Zhou, W., Yang, M., Wang, X., Li, H., Lin, Y., Tian, Q.: Scalable feature matching
by dual cascaded scalar quantization for image retrieval. TPAMI 38(1), 159–171
(2016)

SeqSense: Video Recommendation
Using Topic Sequence Mining
Chidansh Bhatt, Matthew Cooper(B), and Jian Zhao
FX Palo Alto Laboratory, Palo Alto, CA 94304, USA
{bhatt,cooper,zhao}@fxpal.com
http://www.fxpal.com
Abstract. This paper examines content-based recommendation in
domains exhibiting sequential topical structure. An example is edu-
cational video, including Massive Open Online Courses (MOOCs) in
which knowledge builds within and across courses. Conventional content-
based or collaborative ﬁltering recommendation methods do not exploit
courses’ sequential nature. We describe a system for video recommen-
dation that combines topic-based video representation with sequential
pattern mining of inter-topic relationships. Unsupervised topic modeling
provides a scalable and domain-independent representation. We mine
inter-topic relationships from manually constructed syllabi that instruc-
tors provide to guide students through their courses. This approach also
allows the inclusion of multi-video sequences among the recommendation
results. Integrating the resulting sequential information with content-
level similarity provides relevant as well as diversiﬁed recommendations.
Quantitative evaluation indicates that the proposed system, SeqSense,
recommends fewer redundant videos than baseline methods, and instead
emphasizes results consistent with mined topic transitions.
Keywords: Content-based video recommendation · Educational video
1
Introduction
The explosive growth of expository content in the modern video sharing era is
well documented. This content ranges from one-oﬀhow-to videos to profession-
ally produced, multi-course certiﬁcation programs. One prominent example is
the Massive Open Online Course (MOOC) videos distributed on platforms such
as Coursera, edX, and Udacity.
A course may consist of 30–90 videos each targeting speciﬁc learning objec-
tives. The videos are sequenced by the instructor in a syllabus to facilitate stu-
dents’ comprehension. Syllabi are frequently partitioned hierarchically into sec-
tions containing subsets of videos covering closely related concepts. The syllabi
thus encode sequential relationships between speciﬁc videos. Because the videos
themselves are often short and topically coherent, the syllabus also reﬂects inter-
topic sequential relationships.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 252–263, 2018.
https://doi.org/10.1007/978-3-319-73600-6_22

SeqSense: Video Recommendation Using Topic Sequence Mining
253
However, the syllabus remains a prescribed “one size ﬁts all” approach. Its
inability to accommodate the diversity of learner proﬁles is cited as a factor
contributing to low MOOC student retention rates [1–3]. One approach to this
challenge is to aggregate content across related courses available on multiple plat-
forms and use recommendation to enable users to ﬂexibly navigate the expanded
collection of videos. Recommendations drawn from multiple courses can provide
additional perspectives on concepts of interest.
Recently, MOOC providers have recognized that their consumers include
many professionals or“lifelong learners” who are no longer students [4]. These
learners often consume online videos to achieve professional and career growth
rather than complete a certiﬁcation or degree program [5].
In our view, these learners will require more ﬂexible access to a broader range
of content. We hypothesize that these users will seek ﬁner grained information
closer to the individual video level than the course level. As well, we believe
these users will beneﬁt from the ability to choose among a set of related content
across multiple courses to best address their information needs.
We propose recommendation methods that aim to better support this broader
set of users, and ﬁrst aggregate data from various MOOC platforms. We build
a common topic-based representation of the course content based on text tran-
scripts. Next, we identify sequential relationships between videos’ topics across
the corpus. For this, we use the most prominent topics detected per video and
the (partial) ordering of videos in the syllabi as input to a sequence mining mod-
ule. The output is two sets of signiﬁcant inter-topic transitions observed in the
course syllabi. Finally, we re-rank recommendations generated by a traditional
text processing pipeline using the sequential information. Our experiments show
the resulting recommendations originate from a more diverse set of courses, and
also better reﬂect inter-topic orderings in the course syllabi.
2
Related Work
This approach builds on two areas of related work. The ﬁrst is topic modeling
of text document collections. Other works have used Latent Dirichlet Allocation
(LDA) [6] for video recommendation, including [7]. In social media applications,
temporal variants of LDA have been proposed such as Wang et al. [8]. On-line
LDA [9] proceeds by working with a partitioned corpus. However, application of
these methods require a global ordering across the corpus such as time. There
is no natural sequencing of content across independently created courses that
are organized conceptually. Seeded LDA [10] has shown promise in applications
with educational data, but requires data labeling. [11] is a system for educational
video aggregation which focused on improved access to content by searching text
displayed within videos, but did consider sequential relationships between videos.
Similarly, many applications of sequence mining are distinct from video rec-
ommendation. Kinnebrew et al. [12] studies sequence mining approaches to mod-
eling student behavior, but this is not focused on modeling instructional content.
[13] argues that topic mining alone is insuﬃcient for linking web videos to sup-
plement digital textbooks. We augment course data with corresponding syllabi.

254
C. Bhatt et al.
More recently, [14] studied the role of topic sequencing in student performance in
the context of a tutoring system. Their analysis indicates that the topic sequenc-
ing can critically impact student performance. Thus, the sequential organization
of topics by experts within course syllabi may provide valuable information for
improving recommendation. [15] facilitates information discovery in educational
hypermedia systems by applying sequential pattern mining to user logs, but not
the content. The system of [16] enables content adaptation based on user model-
ing within a single course. Incorporating user modeling is an important direction
in which we plan to extend the work presented here.
We combine established topic modeling and sequential pattern mining meth-
ods for educational content recommendation. Sequence learning can be achieved
with several methods including Markov models, recurrent neural networks
(RNNs), and sequential pattern mining (SPM). We select sequential pattern
mining here as it typically requires less parameter tuning or training data [17].
We apply the Top-K sequential pattern mining (TKS) [18] and the Top-K non-
redundant sequential rules (TNS) algorithms [19] to discover important sequen-
tial rules within our syllabus database. The discovered rules indicate which spe-
ciﬁc course topics exhibit sequential relationships across our collected courses.
3
System Description
We integrate automatically mined inter-topic relationships into a conventional
content-based recommendation engine using re-ranking. The data ﬂow appears
in Fig. 1. Pre-processing is illustrated above the dashed line. These steps are
performed oﬀ-line and updated when the corpus changes. The lower portion
shows processing at recommendation time.
Fig. 1. Data ﬂow. Oﬄine pre-processing is depicted above the dashed line. Processing
at recommendation time appears below it.

SeqSense: Video Recommendation Using Topic Sequence Mining
255
3.1
Data Pre-processing
We assemble content from a number of courses, including videos, available text
transcripts, and other meta-data. The courses are designed from varied per-
spectives by each instructor. Generating recommendations across the breadth
of content requires a common data representation. Useful visual attributes such
as whether the format is a classroom lecture, khan-academy style electronic
ink, or Coursera style slide-based videos, etc. are largely captured by associated
meta-data. Most platforms provide semantically rich closed caption transcripts
enabling the identiﬁcation of videos’ prominent topics.
Explicit topics within videos are diﬃcult to align across courses due to dif-
ferences in vocabulary and the contexts in which material is presented. Also,
any manual mapping is not scalable or generalizable to alternate domains. We
thus discover latent topics present across the collection using LDA [6], an estab-
lished unsupervised topic modeling method. In the LDA model, each document
is generated by a mixture of a low dimensional set of topics. We estimate the
appropriate number of topics [20] for the corpus and perform visualization to ver-
ify topic quality [21]. Each video is modeled by its distribution over the discrete
set of Z latent topics with Z = 30 throughout.
3.2
Sequential Pattern Mining
Sequential pattern mining (SPM) algorithms identify prominent subsequences
within a sequence database [22]. Our database contains sets of topics correspond-
ing to each video and the partial orderings derived from each course syllabus.
Detected subsequences are associated with support measures indicating their
prevalence within the database.
Our aim is to use the currently watched video to recommend videos covering
concepts users are likely to watch next. However, frequent topic subsequence
detection alone is not suﬃcient for prediction. Sequential rule mining addresses
prediction by discovering rules of the form X ⇒Y , where X and Y are two
sets of topics. X ⇒Y denotes the rule “topic(s) Y appear in the sequence
after topic(s) X”. Intuitively, the support of frequent patterns corresponds to
the marginal probability of the subsequence, whereas sequential rules have cor-
responding conﬁdence measures analogous to conditional probabilities P(Y |X).
For additional details see [19].
Denote the topic signature of the ith video by Vi = {k : P (i)(zk) ≥0.1}
which is the discrete set of topic indices weighted at least 0.1 in the video’s topic
distribution. We construct an ordered sequence of video topic signatures accord-
ing to each course’s syllabus. These sequences are aggregated into a sequence
database. Course syllabi often employ a hierarchical structure in which related
videos are grouped into course sections. Both the sections and the videos within
a section are ordered. In syllabi with multiple hierarchical levels, we use the level
immediately above individual videos for section grouping.
The Top-K non-redundant sequential patterns (TNS) algorithm [19] detects
sequential rules that reﬂect global analysis of inter-topic transitions. TNS elimi-
nates rules that are deemed “redundant” (rules that are implied by other rules

256
C. Bhatt et al.
having the same support and conﬁdence) to capture more varied sequences and
automatically ﬁne-tunes the minimum support parameter.
Some topic sets observed in our videos are relatively infrequent and will be
overlooked in the global analysis. The Top-K sequential pattern mining (TKS)
algorithm [18] ﬁnds sequential patterns within a given minimum and maximum
length such that a set of items must appear within a deﬁned allowed gap. For local
analysis, we collect the sequences that include each video’s signature. We apply
TKS with the constraint that each video’s topic set appears within a distance of
3 to 6 in the sequence. We then apply TNS algorithm on these derived sequences
to ﬁnd signiﬁcant sequential rules within this local data subset. By this design,
each video’s topic signature is described by sequential rules in the local analysis.
Each sequential rule in the local analysis has a corresponding conﬁdence score.
This application of sequential pattern mining produces sets of prominent topic
transitions describing the sequence database both globally and locally.
3.3
Recommendation
In the scenario of interest, a user watches a video which comprises the query
to the system. The ﬁrst step depicted below the dotted line in Fig. 1 is to issue
the query against our baseline content-based recommendation system. This cur-
rently uses standard tf/idf vectorspace retrieval [23] based on the video tran-
scripts with ranking according to cosine similarity. The initial results emphasize
videos with similar content. These videos are appropriate in cases when users
wish to augment their understanding of a topic. However, when users are ready
to advance to related topics, topic-transition knowledge can enhance recommen-
dation. Denote the latent topic distribution of the video with index r by P (r).
We have examined a variety of scoring criteria detailed below:
Topic similarity score (TS): The signatures of the query video Vq, and recom-
mended video Vr, are matched in terms of overlap and probability values. The TS
score combines the Jaccard similarity between the signatures, and the number
of probabilities for common topics that are within a threshold diﬀerence of 0.2.
SimTS(Vq, Vr) = Jaccard(Vq, Vr) +
1
Z
Z

z=1
δ

|P (q)(z) −P (r)(z)| ≤0.2

.
(1)
Global sequence score (GS): We retrieve N support and conﬁdence score val-
ues, {(sn, cn)} from the mined global sequential patterns with antecedent values
matching Vq and consequent values matching subsets of Vr. The GS score is
SimGS(Vq, Vr) = 1
N
N

n=0
cn
yn
|Vr| + sn
DG
.
(2)
DG is the total number of global sequences, and yn is the length of the matched
subset of Vr. This score emphasizes results consistent with topic transitions
mined in the global analysis of the corpus.

SeqSense: Video Recommendation Using Topic Sequence Mining
257
Local sequence score (LS): We retrieve M additional support and conﬁdence
score values from mined local sequential patterns with antecedent matching a
subset of Vq and consequent matching a subset of Vr. The LS score is
SimLS(Vq, Vr) = 1
M
M

m=0
cm
ym
|Vr| + sm
Dq
.
(3)
Dq is total number of mined local sequences with antecedent matching any subset
of Vq, and ym is the length of the matched subset of Vr. The LS score preferen-
tially weights results with topics that appear in close proximity in the sequence
database to the speciﬁc topics in Vq. We apply feature scaling to bring all scores
into the range [0, 1] before linearly fusing the scores with uniform weights in the
experiments.
3.4
Sub-sequence Recommendations
Most recommendation systems generate lists of highly ranked results. For MOOC
data, it is useful to consider the context of videos (i.e., adjacent videos in the
syllabus). We observe that many of the top recommendation results are in close
proximity within their original course sequences. We thus generate sub-sequence
results with videos in sequential order subject to constraints including total
subsequence duration, proximity in the original sequence (allowable gaps), and
sequence length. We consider the top 20 results and group videos by their original
courses and sort these videos based on their sequence lengths. We generate sub-
sequences of length 2–3 from any sequences longer than 4. The sub-sequences
are ordered according to their course syllabus. The sub-sequence similarity score
is the per-video average of the constituent similarity scores. Finally, we ﬁlter the
results to retain only the highest scoring subsequence from each course.
4
Evaluation
4.1
Data
We report pilot experiments using a pilot corpus of ﬁve MOOCs related to
machine learning, as shown in Table 1. We performed LDA with Z = 30 latent
dimensions to represent the content. For sequence analysis, the parameter K =
8, 000 for TNS on the pilot database with a minimum conﬁdence value of 10%.
TNS on the 53 sequence pilot database generated 62,688 maximum sequential
pattern candidates with minimum support of 2 from which we retain the top
8,000 rules. For the local rules, the number of latent topics associated with each
video signature is much smaller (avg. 3.27), and we thus set K = 100 to avoid
extraneous patterns.
Additionally, we extend the experiments to a larger corpus with 4,186 videos
from 42 courses from additional MOOC platforms including edX and Udacity.
When TNS is run on the full database with 537 sequences, it generated 237,846
maximum sequential pattern candidates with minimum support of 20% from
which we retain the top 8,000 rules. For local rules, the number of latent topics
associated with each video signature is on average 4.1, and we again set K = 100.

258
C. Bhatt et al.
Table 1. Five MOOC courses on various aspects of machine learning comprising the
pilot experimental corpus.
MOOC title
Videos Sections
Machine learning
112
41
Neural nets for machine learning
78
16
Machine learning with big data
28
14
Machine learning: regression
120
44
Machine learning foundations
117
35
Total
455
150
4.2
Metrics
We validate the recommendation system in the absence of user evaluation by
contrasting its recommendations with those produced by a conventional text-
based pipeline. We employ several measures of the topical characteristics of the
highest ranked recommendations, and also use of the original syllabus containing
the query video. We denote the video index that follows the query with index q
in its original syllabus by the index q nxt.
Matching topics: indicates the average number of the top ten results with
signatures Vr overlapping with Vq or Vq nxt. Ideally, we want the recommendation
to be relevant to both the query video and the next video to smoothly traverse
through related content. Vectorspace retrieval provides more granular, word-
based similarity while LDA can provide topic similarity.
Coverage of topics: indicates the average number of topics from Vq appearing
in at least one of the top ten results. Here, we assume that the topics that appear
in the sequel video, but not in the query video are desirable for recommendation.
We denote this topic set Vq nxt d = Vq nxt\Vq to assess how much any conceptual
gap between Vq and Vq nxt can be bridged by recommended videos.
Novelty within section: measures the redundancy of recommendations
between query videos belonging to the same section in a syllabus. We believe
that emphasizing topic transitions in our ranking can account for ﬁner grained
diﬀerences between related queries. Our intuition is that as users move through
proximate videos in a course syllabus, generated recommendations that include
the same results repeatedly limit users’ ability to expand their understanding.
Diversity of courses: provides statistics on the average number of distinct
courses from which the top ten recommendations originate. We assume that dif-
ferent courses are prepared with diﬀerent objectives, viewpoints, and constraints
(e.g. experience, time). Thus, recommendations spanning diﬀerent sources can
provide a more ﬂexible user experience. Overall, these metrics enable comparing
the proposed system incorporating sequential information with the content-based
recommender in terms of relevance, redundancy, and source diversity.

SeqSense: Video Recommendation Using Topic Sequence Mining
259
Table 2. Summary comparing several system variants in terms of matching, coverage,
diversity and novelty as described in Sect. 4.2.
Ranking
Matching
Coverage
Diversity Novelty
Vq Vq nxt Vq
Vq nxt Vq nxt d
Pilot results
CB
7
7
0.6 0.6
0.5
1
4
GS
7
7
0.6 0.6
0.5
2
9
LS
8
8
0.6 0.6
0.4
2
8
GS + LS
8
8
0.6 0.6
0.4
2
9
TS + GS + LS 9
8
0.7 0.6
0.5
2
9
Complete results
CB
9
9
0.6 0.7
0.4
1
7
GS
10 9
0.7 0.7
0.4
2
9
LS
9
9
0.5 0.5
0.2
2
8
GS + LS
9
9
0.5 0.6
0.3
2
9
TS + GS + LS 10 9
0.8 0.7
0.4
2
9
4.3
Quantitative Results
The experiments compare content-based (CB) recommendations to variants of
our system which use re-ranking functions described in Sect. 3. Results from the
pilot dataset appear in the upper portion of Table 2 and the bottom portion
shows results for the complete dataset. The CB recommendations concentrate
within the query video’s course based on the distinctiveness of each instructor’s
word usage. Using CB on the pilot dataset we found that the ﬁrst recommen-
dation from any diﬀerent course appears on average at rank position twelve on
average. To provide more diverse recommendations, we ﬁlter out results from
the same course as Vq for this evaluation and assess the ability of CB and the
proposed methods to direct users to related content from other courses.
Table 2 shows that the matching and coverage measures for all methods are
comparable with some diﬀerences between the pilot dataset and the complete
dataset. GS and LS perform similarly in the pilot study, but with the extended
dataset, GS outperforms LS. In terms of the diversity and novelty measures, the
integration of sequence mining shows a clear impact for both datasets. The nov-
elty measure examines recommendation results within sections of the courses.
Using the CB baseline on the pilot dataset, the sets of the top ten results for
videos within a section produced an average overlap of 6 (stdev = 3.7) or novelty
of 4. Using the sequence based scoring functions GS and LS, the average overlap
for the recommendations was 1 (stdev = 3.8) as in Table 2. Using the complete
dataset, the CB recommender produces an average overlap of 3 (stdev = 3.9)
compared to sequence based scoring (TS + GS + LS) with the average overlap
for the recommendations 1 (stdev = 3.0) or novelty of 9. Thus, the proposed

260
C. Bhatt et al.
approach uses the sequential relationships to provide more diverse recommenda-
tions while preserving essential similarity to the query.
4.4
Example Results
Table 3 compares recommendation results using the pilot dataset from CB with
those from GS + LS for the example query video “Unsupervised learning intro-
duction” with Vq = [4 15 16 21] from the “Machine Learning” MOOC. The sequel
video in the syllabus is “k-means algorithm” with Vq nxt = [7 9 15 21]. CB pro-
vides two recommendations (ranks 4 and 5) unrelated to unsupervised learning.
The re-ranked results span varied aspects (e.g., algorithms, tools, applications)
of unsupervised learning from more courses. Also, GS + LS shows greater topic
matching and coverage between the query, sequel, and recommended videos.
We also use multidimensional scaling (MDS) to further examine our recom-
mendations. MDS [24] is a standard dimension reduction technique that uses
proximity to communicate similarity in an abstract low dimensional space. We
use it here to visualize the relationships between recommended results. We use
the TS + GS + LS similarity scores between the query video, the top ten rec-
ommendations, and the adjacent (previous and next) videos to the query from
its course syllabus. We convert these to dissimilarity values by negation. MDS
takes this set of dissimilarities and returns a layout of points corresponding to
each data instance such that the distances between the points spatially reﬂect
the videos’ pairwise relationships.
Figure 2 shows example MDS layouts of recommendation results for the query
video for lecture 77 in the 112 video course titled “Machine learning”. The query
Table 3. Example recommendation results comparing CB (left) and GS + LS (right).
The query video is “Unsupervised learning introduction” with Vq = [4 15 16 21] and
sequel video “k-means algorithm” with Vq nxt = [7 9 15 21].
Rank CB
GS + LS
Video title
Vr
Video title
Vr
1
Clustering
documents an
unsupervised
learning task
[2 10 15 22]
Spark mllib
clustering
[9 21 22]
2
k means a clustering
algorithm
[15 21 22]
The k-means
clustering algorithm
[9 13 15 17 21]
3
Other examples of
clustering
[15 21 22 28] Spark mllib frequent
item associated sets
[0 9 21]
4
From weighted k-nn
to kernel regression
[2 13 21 22]
Loading exploring
wikipedia data
[16 21 22]
5
Weighted k nearest
neighbors
[2 15 22]
Unsupervised
learning with
clustering
[9 10 15 21]

SeqSense: Video Recommendation Using Topic Sequence Mining
261
Fig. 2. MDS visualizations of recommendations using CB scores in panel (a),
TS + GS + LS scores in panel (b) and sub-sequence recommendations based on
TS + GS + LS scores in panel (c). Each circle represents a video labeled with its lec-
ture sequence number. Circles with the same color indicate videos in the same course.
(Color ﬁgure online)
title is “K means algorithm.” The color code indicates the diﬀerent courses from
which the individual videos originate. Arrows connect lectures according to the
course syllabus of the query. We include the neighboring videos in the syllabus
(i.e., lectures 76 “Unsupervised learning introduction” and 78 “Optimization
objective”).
Panel (a) shows the MDS visualization of the CB recommendations, and the
videos from the query’s course are closely clustered. The recommended videos
are also clustered but the layout doesn’t readily reveal any relationship between
the recommendations and the query or its neighbors. Panel (b) shows the MDS
layout using TS + GS + LS scores. The layout suggests stronger relationships
between the recommendations and the query context. Speciﬁcally, various recom-
mendations show more relative similarity to the previous, query, or subsequent
videos. This context is not evident in the layout of the CB recommendations.
This visualization suggests the various recommendations can address a broader
range of user information needs. Panel (c) shows the MDS layout of the sub-
sequence recommendations using TS + GS + LS scores. Also, when we convert
the recommendation list to the sub-sequences it is interesting to observe that sub-
sequence with lecture ids [49, 50: “Estimating parameters from soft assignment”,
51] is rated highest in Panel (b). Both that sub-sequence and the sub-sequence
[29: “Hope for unsupervised learning”, 30: “the K means algorithm”, 31: “k
means as coordinate”] generated from CB recommendations are from the course
“Machine learning clustering and retrieval”. The CB results represents the pre-
liminary part of the course whereas the TS + GS + LS sub-sequence occurs later
in that course and is more relevant to the discussion of clustering in the query
video from the course “Machine learning”. Considering sub-sequences provides
ﬂexibility in navigating the recommendations and can use more complete context
across multiple adjacent videos.

262
C. Bhatt et al.
5
Conclusion
We presented a recommendation system, SeqSense, that exploits sequential rela-
tionships between topics. These inter-topic relations occur commonly in edu-
cational content as well as corpora documenting structured, ordered processes.
Our initial experiments indicate SeqSense makes fewer redundant recommen-
dations than a conventional content-based method. Instead, recommendations
reﬂect topic transitions observed in course syllabi created by the instructors.
This provides a natural means by which users can explore collections of related
videos originating from independent, often siloed, content platforms. Recom-
mendations optionally include short sub-sequences of videos to more completely
convey content beyond video-to-video level recommendation. The use of a global
topic model and sequence-based recommendation can enhance the learning expe-
rience by allowing users to more ﬂexibly navigate videos across independent
siloed platforms while avoiding redundant conceptual content.
While collaborative ﬁltering based recommendation systems process user
behavioral data to drive recommendation, many scenarios like education may not
readily share user data outside their institutions. However, user logs or models
remain a powerful complementary source of information to improve the proposed
system. More generally, the query can be expanded to integrate viewing history
information, facets based on meta-data, or extracted keyphrases [25]. Beyond
incorporating auxiliary data, our future work continues on several fronts. We
are designing an interface to enable users to playback content and explore our
corpus via recommendation and so that we can perform a more complete study
of the system’s utility. This work will also involve further visualization of recom-
mendation results to facilitate ﬂexible exploration and navigation in the larger
corpus.
References
1. Onah, D.F., Sinclair, J., Boyatt, R.: Dropout rates of massive open online courses:
behavioural patterns. In: EDULEARN 2014 Proceedings, pp. 5825–5834 (2014)
2. UIL Policy Brief: Making large-scale literacy campaigns and programmes work
(2016)
3. Cope, B., Kalantzis, M.: e-Learning Ecologies: Principles for New Learning and
Assessment. Routledge, Abingdon (2017)
4. Levin, R.: Announcing Coursera for Business (2016). https://blog.coursera.org/
announcing-coursera-for-business/. Accessed 19 July 2017
5. Zheng, S., Rosson, M.B., Shih, P.C., Carroll, J.M.: Understanding student motiva-
tion, behaviors and perceptions in MOOCs. In: Proceedings of the 18th ACM Con-
ference on Computer Supported Cooperative Work & Social Computing. CSCW
2015, New York, NY, USA, pp. 1882–1895. ACM (2015)
6. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn.
Res. 3(Jan), 993–1022 (2003)
7. Zhu, Q., Shyu, M.L., Wang, H.: Videotopic: content-based video recommendation
using a topic model. In: ISM, pp. 219–222. IEEE Computer Society (2013)

SeqSense: Video Recommendation Using Topic Sequence Mining
263
8. Wang, Y., Agichtein, E., Benzi, M.: TM-LDA: eﬃcient online modeling of latent
topic transitions in social media. In: KDD, pp. 123–131. ACM (2012)
9. AlSumait, L., Barbar´a, D., Domeniconi, C.: On-line LDA: adaptive topic models for
mining text streams with applications to topic detection and tracking. In: ICDM,
pp. 3–12. IEEE (2008)
10. Ramesh, A., et al.: Understanding MOOC discussion forums using seeded LDA.
In: Proceedings of the Ninth Workshop on Innovative Use of NLP for Building
Educational Applications (2014)
11. Adcock, J., Cooper, M., Denoue, L., Pirsiavash, H., Rowe, L.A.: TalkMiner: a
lecture webcast search engine. In: ACM Multimedia, pp. 241–250. ACM (2010)
12. Kinnebrew, J.S., Loretz, K.M., Biswas, G.: A contextualized, diﬀerential sequence
mining method to derive students’ learning behavior patterns. J. Educ. Data Min.
5(1), 190–219 (2013)
13. Agrawal, R., Christoforaki, M., Gollapudi, S., Kannan, A., Kenthapadi, K., Swami-
nathan, A.: Mining videos from the web for electronic textbooks. In: Glodeanu,
C.V., Kaytoue, M., Sacarea, C. (eds.) ICFCA 2014. LNCS, vol. 8478, pp. 219–234.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-07248-7 16
14. Doroudi, S., Holstein, K., Aleven, V., Brunskill, E.: Sequence matters, but how
exactly? A method for evaluating activity sequences from data. In: International
Conference on Educational Data Mining EDM (2016)
15. Morales, C.R., P´erez, A.P., Soto, S.V., Martınez, C.H., Zafra, A.: Using sequen-
tial pattern mining for links recommendation in adaptive hypermedia educational
systems. Curr. Dev. Technol.-Assist. Educ. 2, 1016–1020 (2006)
16. Pardos, Z.A., Tang, S., Davis, D., Le, C.V.: Enabling real-time adaptivity in
MOOCs with a personalized next-step recommendation framework. In: ACM Con-
ference on Learning @ Scale, pp. 23–32. ACM (2017)
17. Fournier-Viger, P., Faghihi, U., Nkambou, R., Nguifo, E.M.: CMRules: mining
sequential rules common to several sequences. Knowl.-Based Syst. 25, 63–76
(2012)
18. Fournier-Viger, P., Gomariz, A., Gueniche, T., Mwamikazi, E., Thomas, R.: TKS:
eﬃcient mining of top-k sequential patterns. In: Motoda, H., Wu, Z., Cao, L.,
Zaiane, O., Yao, M., Wang, W. (eds.) ADMA 2013. LNCS, vol. 8346, pp. 109–120.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-53914-5 10
19. Fournier-Viger, P., Tseng, V.S.: TNS: mining top-k non-redundant sequential rules.
In: SAC, pp. 164–166. ACM (2013)
20. Guille, A., Soriano-Morales, E.-P.: TOM: a library for topic modeling and browsing.
In: Conf´erence sur l’Extraction et la Gestion des Connaissances, Reims, France,
January 2016. Actes de la 16`eme Conf´erence sur l’Extraction et la Gestion des
Connaissances (2016). https://hal.archives-ouvertes.fr/hal-01442868/
21. Sievert, C., Shirley, K.E.: LDAvis: a method for visualizing and interpreting topics.
In: Proceedings of the Workshop on Interactive Language Learning, Visualization,
and Interfaces, pp. 63–70 (2014)
22. Fournier-Viger, P., Lin, J.C.W., Kiran, R.U., Koh, Y.S.: A survey of sequential
pattern mining. Data Sci. Pattern Recogn. 1, 54–77 (2017)
23. Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Information Retrieval.
Cambridge University Press, Cambridge (2008)
24. Kruskal, J.B.: Multidimensional scaling by optimizing goodness of ﬁt to a non-
metric hypothesis. Psychometrika 29(1), 1–27 (1964)
25. Chuang, J., Manning, C.D., Heer, J.: “Without the clutter of unimportant words”:
descriptive keyphrases for text visualization. ACM Trans. Comput.-Hum. Interact.
19(3), 19 (2012)

ShapeCreator: 3D Shape Generation
from Isomorphic Datasets Based
on Autoencoder
Yunjie Wu, Zhengxing Sun(&), Youcheng Song, and Hongyan Li
State Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing, China
szx@nju.edu.cn
Abstract. With the development of 3D digital geometry media, the creation of
3D content is becoming more and more important. This paper presents a novel
method for 3D shape generation from the isomorphic examples in terms of
Autoencoder. A structure-aware shape representation is ﬁrstly built from the
given examples with same category. The representation describes the shapes in a
uniﬁed manner no matter how the shape structure varies. Then, an Autoencoder
model is introduced to establish a bidirectional mapping between the
high-dimensional representing space and a 2D latent space. This bridges the
existed examples and the latent generated shapes. In the one hand, the sample
data in representation space is transferred to a lower dimension by the encoder
of Autoencoder to form a latent space. Then the latent space is checked and
visualized to guarantee created shapes meaningful. In the other hand, decoder of
Autoencoder is able to transform new data from the latent space to isomorphic
representation, and novel structures are constructed from the decoded repre-
sentation. This scheme facilitate the operation of 3D creation a lot. Experimental
results prove the effectiveness of the proposed approach.
1
Introduction
3D modeling is a fundamental and signiﬁcant task in the ﬁeld of computer graphics.
With much application in many aspects, the creation of new 3D shapes becomes more
and more important. In recent years, generating novel shapes from existed ones has
attracted a lot of attention. However, there are some problems with this process. As
many 3D shapes, especially of man-made artifacts, exhibit rich structural variations,
even within the same object class (e.g. the diversity of chairs), the challenge is what
information can be learned from the shape set and how to make up new shapes with the
extracted knowledge. Some existing researches have focused on co-analyzing sets of
the shapes from the same class [1–4]. However, the typical outcome of these methods,
a correspondence between different shapes, does not really summarize essences of the
object class. On the other hand, many proposed works rely on the correspondence of
different shapes, and synthesize new shapes by replacing some parts of origin shapes
[14, 15, 19]. For these methods, the generated shapes are severely limited in the variety.
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 264–276, 2018.
https://doi.org/10.1007/978-3-319-73600-6_23

To solve previous problems, we are aimed at learning a continuous, generative
model for 3D objects’ structure. To establish such a model, the main challenges faced
are two-fold. The ﬁrst is to ﬁnd an appropriate shape structure representation that can
adapt the intra-class structural variations. The typical method is represent the shape
structures as graphs [7, 12, 14]. However, the structural variations may lead to the
heterogeneity of graphs, which makes the following process difﬁcult. The other chal-
lenge is how to construct such a continuous, generative model. The previous modeling
methods usually generate discrete shape instances, because they constructed new
shapes by replacing parts with other existing parts [14, 15, 19]. That requires us to
employ a different way to generate new shapes.
Accordingly, this paper presents a method of 3D shape generation from isomorphic
data, named as ShapeCreator. With consistently segmented and corresponded, shapes
from a same class are represented as vectors called isomorphic data. Take extracted
isomorphic data from shapes as training data, a neural network based Autoencoder is
constructed. The trained Autoencoder builds a bridge between origin existing shapes
and generated shapes, which enables the creation of new shapes.
Extracting the isomorphic data from shapes requires overcoming the varieties of
shape structure. Our key insight is that, no matter how shape structures varies, they still
follow the rules of constitution of the category, e.g. chairs are usually consist of the
back, the seat, the arm and the leg. Given a set of shapes, an isomorphic graph is
constructed by adding null node representing missing components. Then, by combing
parameters of each node in graph, we extract the isomorphic data for each shape. The
vectorization process is invertible. With a representing vector, we can reconstruct the
structure of the origin shape easily.
To create new shapes requires generating new meaningful isomorphic data from the
extracted isomorphic data. As shapes are from the same class, some potential char-
acteristics of the object class lie in the extracted isomorphic data. To capture the
essences of data, we introduce the neural network-based Autoencoder, which is known
as an unsupervised feature learning model. An Autoencoder is trained with the iso-
morphic data to learn a bidirectional mapping between the isomorphic space and a 2D
latent space. Then, new shapes can be generated with isomorphic data decoded from
the latent space. The validity of the 2D data is checked by some pre-set rules, and the
checking result is visualized to ensure created shapes meaningful.
The main contribution of our work can be summarized as follows.
1. A structure-aware shape representation based on isomorphic graph is designed. The
representation encodes various shape structure into a ﬁxed-dimensional vector. The
vectorization procedure is invertible, allows us to reconstruct novel structures with
new vectors. The representation can also be used in other applications, such as
ﬁne-grained classiﬁcation.
2. A method of extraction the distribution of sample sets in terms of Autoencoder is
proposed. The representing space of example shape structures is transfered to a
compact latent space, which captures a more abstract distribution of shapes.
Meanwhile, representing data point than can be generated from the latent space in
the decode stage, which bridges the existing shapes and the generated shapes.
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
265

3. A structure synthesis method based on Autoencoder is presented. After established,
the latent space is visualized to guarantee the created shapes meaningful. The
visualization is realized via checking the validity of sampling data from latent space
and rendering the checking results. Selected data from the latent space is decoded to
isomorphic representation by Autoencoder. Then, new structures are created from
the isomorphic representation.
2
Overview
We take as input a collection of same class shapes and our goal is to learn a continuous,
generative model for shape structure. In [5], shape structure is deﬁned by the
arrangements of and the relations between shape components. Under this deﬁnition, we
use shape parts’ oriented bounding box (OBB) to capture shape structures, and our
generative model’s outcome is the parameters of new shape’s each part’s OBB. We
assume that the shapes are consistently segmented and corresponded. Our method has
three stages, shown in Fig. 1. In this section, we provide an interview of our iso-
morphic representation construction, bidirectional mapping building, space visualiza-
tion and novel structure synthesis.
Isomorphic representation construction. In the ﬁrst stage, we construct a uniﬁed
representation for each shape structure. Shape structure includes two aspects, the
constitution of the shape and spatial parameters for each part of the shape. We construct
an isomorphic graph (see Sect. 4) to represent the various constitution of shapes uni-
formly. To describe the spatial information of shape parts, oriented bounding boxes
(OBBs) are computed as the abstraction. After the graph construction and OBB
extraction, we compress them into a ﬁxed vector. Each shape than is represented by its
own vector. The vector construction process is invertible. Origin shape structure can be
derived with the vector.
Fig. 1. An overview of our framework, including the three stages: (a) constructing the
isomorphic representation for the shape family and generate a ﬁxed dimension vector
representing each shape, (b) using an Autoencoder to learn the birdirectional mapping between
representing vector space and 2D space, and (c) visualizing the 2D space and synthesizing new
shape structure.
266
Y. Wu et al.

Bidirectional Mapping Building. With the training shapes’ representing vector, we
construct an Autoencoder to learn a distribution over vector space. The bidirectional
mapping between representing vector space and 2D space are also learned. Each shape
than can be embedded into 2D space. Given a 2D data in the space, we decode it and a
new structure can be synthesized.
Space Visualization and Structure Synthesis. To make our framework more practical,
we visualize the 2D space, and provide a method to synthesis with simple interaction.
Two validity criterion are set up to guarantee the quality of synthesized structure:
connectivity criterion and symmetry criterion. The 2D space is checked with the two
criterion. Then the checking result is visualized. User can click on the 2D space and
new structure will be synthesized with the picked 2D point.
3
Isomorphic Representation Construction
We demonstrate the isomorphic representation construction process in this section. Our
goal is to encode the various shape structure into a ﬁxed-dimension vector. The vector
is invertible, allowing the origin structure reconstructed from the vector. The challenge
is twofold. First, the structure of shapes may vary signiﬁcantly, so it’s difﬁcult to
represent every structure with a ﬁxed-dimension vector. Second, the vectorization
procedure needs to be invertible.
Pre-process. In our work, we require input shapes to be consistent segmented, cor-
responded and oriented. Much 3D shape repositories provide the segment and corre-
spond information (e.g. ShapeNet, COSEG). For shape sets without these information,
much segmentation methods [1, 2] and corresponding methods [6, 22] can be applied
to meet our requirements. Manual processing is also an alternative.
Consolidated graph. To represent the structure of shapes, we refer to the spatial graph
[16]. In their approach, each individual part is represented as a node, and two parts’
connectivity is represented as an edge between the two relevant nodes. Their approach
can represent a pair of shapes. With their node’s and edge’s deﬁnition, we expand their
spatial graph to consolidated graph to represent set of shapes.
To construct our consolidated graph, we extract spatial graph for each shape ﬁrst.
Then, with the correspondence of shape parts, we merge the corresponding nodes into a
single one, as well as the corresponding edges. After merging, each node deﬁnes a type
of components and each edge deﬁnes a connectivity relation between two types of
components. The consolidated graph reveal the rules of constitution of the shape family
by enumerating all possible structure. Therefore, any spatial graph is a subgraph of the
consolidated graph.
Isomorphic graph. Our goal is to construct isomorphic representation for various
structures. We extract the isomorphic graph of each structure with its spatial graph and
the consolidated graph. As mentioned before, the spatial graph is a subgraph of con-
solidated graph. For each node in consolidated, if it exists in the spatial graph, it is
copied into the isomorphic. Otherwise, a special null node is deﬁned to take the place
of the missing node in spatial graph. We process edges in the same way. The iso-
morphic graph construction process is shown in Fig. 2.
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
267

Representing vector. We need to represent each shape with a representing vector,
which encodes the structure information of the shape. The structure information is
composed of two parts: each component’s spatial information and relationships
between connected components. We compute the oriented bounding box (OBB) for
each component as its Information summary. Each bounding box is described by its
center c, three primary axes (ax; ay; az), and the length of each axis (lx; ly; lz), so the
parameters of each part is 15-dimensional. To establish meaningful connection rela-
tionships, we ﬁrst detect the contact point for every two connected parts Pi; Pj, denoted
as Ci;j. Then, we express Ci;j by the local frame of reference Pi and Pj to describe the
connection relationship between Pi and Pj, so the parameters of each relationship
between two parts is 6-dimensional.
Now, we need to encode the component’s spatial information and relationships
between them into a single vector. Our representing vector is composed of four parts:
NodeInd, EdgeInd, NodePara and EdgePara. Let n and m be the number of isomorphic
graph’s nodes and edges respectively. NodeInd is a 0–1 vector whose length is n. If
isomorphic graph’s i-th node is a null node, then the i-th component of NodeInd is 0,
otherwise is 1. The EdgeInd is the similar to NodeInd, indicating whether every edge is
null or not. NodePara is a 15*n-dimensional vector, made up of each node’s param-
eters. For the null node, its parameters is a 15-dimensional zero vector. Similarly,
EdgePara consists of each edge’s parameters. Figure 3 shows an example of repre-
senting vector. Various structure can be uniformly represented by the representing
vector. What’s more, with one representing vector, the origin structure can be recon-
structed directly (see Sect. 6).
(a) origin shapes
(b) spatial graph
(c)consolidated graph
(d) isomorphic graph
Fig. 2. An example of isomorphic graph construction process. From left to right: (a) origin
shapes, (b) the extracted spatial graph, (c) the consolidated graph after node and edge merging,
(d) the isomorphic graph for origin shapes. Notice that although origin shapes vary a lot in
structure, the ﬁnal graph representation is isomorphic.
Fig. 3. An example of representing vector. From left to right: (a) the isomorphic graph, (b) four
components of representing vector. The four vectors are concatenated to make up the
representing vector.
268
Y. Wu et al.

4
Bidirectional Mapping Building
In the previous section, we construct a uniﬁed representation for shapes. The repre-
sentation consists of isomorphic graph structure, a high-dimensional node’s parameters
and edge’s parameters. The degree of freedom of this representation is high. As all
shapes are from the same object class, the structure of them implicit signiﬁcant simi-
larity. Our goal is to lower the dimensions of representation space to achieve a more
compact
manifold.
In
this
section,
we
present
the
method
of
learning
a
low-dimensional, non-linear mapping for our isomorphic representation.
The bidirectional mapping learning process is actually a regression task. We want
to learn an invertible function f ðÞ that maps the set of input vectors xi to the corre-
sponding output vectors yi. Once the mapping learned, with a new input, we can predict
the output.
Classical Autoencoder is a three-layered neural network [23]. It only has one
hidden layer. The input layer and the hidden layer can be seen as an encoder, which
transform the input vector to the latent code. The hidden layer and the output layer
make up a decoder, which reconstruct a vector based on the latent code. Autoencoder
training is a self-supervised process, the training goal is to make the reconstructed
vectors by decoder similar to the origin ones.
We construct an Autoencoder with three full-connected hidden layers for our task,
which structure is shown in Fig. 4. More hidden layers means stronger ability to cover
the data structure. For i-th hidden layer, there are two parameters to be trained: the
weight matrix Wi and bias vector bi. Let xi be the input for i-th hidden layer, the output
yi can be computed as
yi ¼ Sf ðWixi þ biÞ
Where Sf is the encoder’s activation function. This process is single layer encoding
process. After encoded layer-by-layer, the output is the ﬁnal code, which dimension is
set to two. When we want to perform the decode process, the process is just the reverse.
Fig. 4. The structure of our autoencoder. Let the dimension of input vector be n, the number of
neurons of three hidden layers is 1.2*n, 0.5*n, 0.25*n respectively.
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
269

Let ~yi be the output for i-th hidden layer in decoder, ~Wi be the weight matrix and bi be
the bias vector, the decoded process is as follows:
~xi ¼ Sgð ~Wi~yi þ ~biÞ
Where Sg is the decoder’s activation function. Usually, ~Wi is set to WT
i . During
training, we take reconstruction error as our loss function, deﬁned as
L xi;~xi
ð
Þ ¼ xi  ~xi
k
k2
Minimize the sum of loss function in the dataset can complete the training process.
We train our Autoencoder layer-by-layer with back propagation, and take stochastic
gradient descent algorithm to optimize our loss function. In our implementation, we set
the Sf and Sg to sigmoid function.
5
Space Visualization and Structure Synthesis
In this section, we introduce the ﬁnal stage of our framework: space visualization and
structure synthesis. Our goal is to allow users synthesize new shape structure by
clicking the 2-D space. To make this process more effective, we visualize the validity
of 2-D space. With the visualized space, users are guided to click the pixels whose
corresponding synthesized structures are more plausible.
Structure synthesis. First, we demonstrate how to synthesize a structure with the 2-D
vector from the 2-D space. As we have trained the Autoencoder in Sect. 4, the cor-
responding representing vector can be decoded from the 2-D vector by the Autoen-
coder. Then the synthesizing process is straightforward. As previous, the representing
vector consists of four parts: NodeInd, EdgeInd, NodePara, EdgePara. With the
NodeInd and EdgeInd, we ﬁlter out the null nodes and null edges. For the rest valid
nodes and edges, we take out the parameters from NodePara and EdgePara for them.
With the NodePara, each component’s OBB can be reconstructed easily. Finally, we
adjust the arrangements of components by lapping the contact point coordinates con-
tained in EdgePara.
Validity constraint. To make our synthesized structure more plausible, the invalid
results should be ﬁltered out. We set two validity constraint to check whether one
synthesized structure is valid or not, shown as Fig. 5. The ﬁrst one is connectivity
constraint. That means, if two types of components are spatial connected in most of
training shapes, and don’t maintain the connection relation in the synthesized shape,
then the shape is considered as an invalid one. The other constraint is symmetry
constraint. If in training shapes, some components show the symmetry, then the
symmetry ought to be maintained in the synthesized ones, otherwise the synthesized
shapes are discarded.
270
Y. Wu et al.

Space visualization. We now demonstrate the latent space visualization process.
A range of latent space containing all mapped training representing vectors is deter-
mined ﬁrst. Then, we sample in the latent space, decode the sampled 2-D data,
reconstruct shape from the decoded vector and check the validity of the shape. In the
sampling phase, we take each pixel in screen, and compute the corresponding 2-D data
as a sampled data. If one synthesized shape is judged as a valid one, its corresponding
pixel in screen is set to blue, otherwise is set to red. With the visualization of each
pixel’s validity, users are guided to click the meaningful region.
6
Result
In this section, we describe the experiments performed to evaluate the isomorphic
representation and the Autoencoder-based structure generative model.
For our experiments, we collected 500 3D models from three categories. The
dataset contains 200 chairs, 100 lamps, 100 airplanes and 100 tables. All models are
from the ShapeNet [25]. The segmentation and corresponding information is also
provided. In the pre-process stage, each shape’s OBB is computed.
Structure-aware embedding. Embedding data into a low dimensional manifold is
meaningful. To see the results clearly, the chair dataset is chosen and devided into four
sub-class: armchair, ball chair, four-leg chair and swivel chair. We compare our
Fig. 5. Two invalid structure examples. (a): The leg and the seat are disconnected, so this
structure violates the connectivity constraint. (b): The nearest basal leg is not symmetrical with
other four legs, the symmetry constraint is broken.
Fig. 6. Comparison between (a) our latent space and (b) LFD’s. It can be seen that our latent
space has a clearer line between different sub-classes.
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
271

isomorphic representation with LFD (a multi-view shape feature) [24] in the matter of
data embedding. This is not an equal comparison: our isomorphic require a
pre-segmentation and corresponding while LFD does not. Nevertheless, we ignore the
detail geometry feature, which is taken into consideration in LFD. From the result in
Fig. 6, we can conclude that for man-made objects, the varies of structure are more
representative to geometrical characteristics.
Shape abstraction. A common data abstraction method is taking average as abstrac-
tion, which is difﬁculty for 3D shapes. The isomorphic representation can be utilized to
solve this. The abstraction of shapes is presented as follows: given a set of shapes, the
isomorphic representation of each shape is constructed ﬁrst. Then, in the representing
space, the average vector of all data points is computed. After that, we construct a new
shape structure from the average vector, and take this as the abstraction of shapes.
Figure 7 shows some experimental results.
Training datasets scale. The scale of training datasets usually has a signiﬁcant effect
on the trained model. We use four different scale datasets to train our generative model
respectively. The results are shown in Fig. 8. It can be seen that, even with small-scale
datasets (30 training shapes in total), our method remains valid.
Fig. 7. The average shapes for the four sub-classes of chairs. From left to right: (a) armchair,
(b) ball chair, (c) four-leg chair, (d) swivel chair
Fig. 8. Different scale of training datasets. (a) The Mean Square Error (MSE) during the training
process. (b): Some generated shapes from different amounts of training examples. Even with 30
training shapes, our method can reach a convergence, and generate meaningful shape structures.
272
Y. Wu et al.

Segmentation hierarchies. In the pre-process stage, the shapes are consistently seg-
mented. However, the segmentation hierarchies are not speciﬁed. A more reﬁned
segmentation is performed to check our model’s compatibility for various hierarchies
of segmentation. Some generated shapes with different segmentation hierarchies are
shown in Fig. 9.
Shape interpolation. Given two structurally different shapes, interpolation can be
performed between them with our method. The two shapes ﬁrstly are represented by
our isomorphic representation and projected into the latent space by Autoencoder.
Linear interpolation then is applied in the latent space. Then, we construct new
structures with the above synthesis process. Examples are demonstrated in Fig. 10. The
interpolation shows smooth even with different structures.
Shape synthesis. Our shape synthesis framework facilitates 3D modeling operation.
With clicking in the visualized space, topologically-various shape structure can be
synthesized. Several synthesized examples are shown in Fig. 11. From the visualized
result, we can see most latent space near origin data points is valid. It’s interesting that
the representation and learning process doesn’t contain any symmetry information, but
most synthesized structure conform to the symmetry constraint. There are also some
failure cases, framed by the red box. These synthesized structures are detected as valid,
actually invalid, e.g. a chair never be equipped with four upright legs and a swivel leg
at the same time.
Fig. 9. Two hierarchies of generated shapes. Note the difference of chair’s back, swivel leg and
plane’s empennage. The experimental results prove that our method can adapt to various
segmentation hierarchies.
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
273

7
Limitation and Future Work
The main limitation of our method is the dependence of pre-segmentation and corre-
sponding. The consistently segmentation and corresponding may be hard to accomplish
for some special object class. Another drawback is that the our representation only
takes into account the shape structures. Without the consideration of geometry, some
meaningful information may be missing. The validity checking process can also be
improved, some meaningless examples may be synthesized even they conform to our
validity constraints.
Fig. 10. Some shape interpolation examples. The shapes in green box are source and target.
Notice how the structure varies, including existence, size and arrangements of components
(Color ﬁgure online)
274
Y. Wu et al.

Our work still has a long way to go. In future, it is worthy to explore a repre-
sentation containing the geometric information. Another interesting future work is to
introduce the GAN framework. With the adversarial training process, the generated
instances may be improved.
Acknowledgement. This work was supported by National High Technology Research and
Development Program of China (No. 2007AA01Z334), National Natural Science Foundation of
China (Nos. 61321491 and 61272219), Innovation Fund of State Key Laboratory for Novel
Software Technology (Nos. ZZKT2013A12 and ZZKT2016A11), and Program for New Century
Excellent Talents in University of China (NCET-04-04605).
References
1. Kalogerakis, E., Hertzmann, A., Singh, K.: Learning 3D mesh segmentation and labeling.
ACM Trans. Graph. (TOG) 29(4), 102 (2010). ACM
2. Huang, Q., Koltun, V., Guibas, L.: Joint shape segmentation with linear programming. ACM
Trans. Graph. (TOG) 30(6), 125 (2011). ACM
3. Sidi, O., et al.: Unsupervised co-segmentation of a set of shapes via descriptor-space spectral
clustering. 30(6), 126 (2011). ACM
4. Kim, V.G., Li, W., Mitra, N.J., DiVerdi, S., Funkhouser, T.: Learning part-based templates
from large collections of 3D shapes. ACM Trans. Graph. (TOG) 32, 70 (2013)
5. Mitra, N., Wand, M., Zhang, H.R., Cohen-Or, D., Kim, V., Huang, Q.X.: Structure-aware
shape processing. In: SIGGRAPH Asia 2013 Courses, p. 1. ACM (2013)
6. Laga, H., Mortara, M.: Geometry and context for semantic correspondences and
functionality recognition in man-made 3D shapes. ACM Trans. Graph. (TOG) 32, 150
(2013)
Fig. 11. Space visualization and structure synthesis results. The red area is detected as invalid
and the blue is opposite. The structures in red box are the failure examples, which are
meaningless but pass the valid checking. (Color ﬁgure online)
ShapeCreator: 3D Shape Generation from Isomorphic Datasets
275

7. Hu, R., van Kaick, O., Wu, B., Huang, H., Shamir, A., Zhang, H.: Learning how objects
function via co-analysis of interactions. ACM Trans. Graph. (TOG) 35, 47 (2016)
8. Kim, V.G., Chaudhuri, S., Guibas, L., Funkhouser, T.: Shape2pose: human-centric shape
analysis. ACM Trans. Graph. (TOG) 33(4), 120 (2014)
9. Chaudhuri, S., Kalogerakis, E., Guibas, L., Koltun, V.: Probabilistic reasoning for
assembly-based 3D modeling. ACM Trans. Graph. TOG 30, 35 (2011)
10. Kalogerakis, E., Chaudhuri, S., Koller, D., Koltun, V.: A probabilistic model for
component-based shape synthesis. ACM Trans. Graph. (TOG) 31(4), 55 (2012)
11. Fish, N., Averkiou, M., Kaick, Van, Cohen-Or, D., Mitra, N.J.: Meta-representation of shape
families. ACM Trans. Graph. (TOG) 33(4), 34 (2014)
12. Hilaga, M., et al.: Topology matching for fully automatic similarity estimation of 3D shapes.
In: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive
Techniques. ACM (2001)
13. Barra, V., Biasotti, S.: 3D shape retrieval using kernels on extended Reeb graphs. Pattern
Recogn. 46(11), 2985–2999 (2013)
14. Liu, H., Vimont, U., Wand, M., Cani, M.P., Mitra, N.J.: Replaceable substructures for
efﬁcient part-based modeling. Comput. Graph. Forum 34, 503–513 (2015)
15. Huang, S.S., Fu, H., Wei, L.Y., Hu, S.M.: Support substructures: support-induced part-level
structural representation. IEEE Trans. Vis. Comput. Graph. 22(8), 2024–2036 (2016)
16. Alhashim, I., Li, H., Xu, K., Cao, J., Ma, R., Zhang, H.: Topology-varying 3D shape
creation via structural blending. ACM Trans. Graph. (TOG) 33(4), 158 (2014)
17. Kreavoy, V., Julius, D., Sheffer, A.: Model composition from interchangeable components.
In: Computer Graphics and Applications, pp. 129–138 (2007)
18. Xu, K., Zhang, H., Cohen-Or, D., Chen, B.: Fit and diverse: set evolution for inspiring 3D
shape galleries. ACM Trans. Graph. (TOG) 31(4), 57 (2012)
19. Zheng, Y., Cohen-Or, D., Mitra, N.J.: Smart variations: functional substructures for part
compatibility. Comput. Graph. Forum 32(2pt2), 195–204 (2013)
20. Huang, H., Kalogerakis, E., Marlin, B.: Analysis and synthesis of 3D shape families via
deep-learned generative models of surfaces. Comput. Graph. Forum 34, 25–38 (2015)
21. Li, J., Xu, K., Chaudhuri, S., Yumer, E., Zhang, H., Guibas, L.: GRASS: Generative
Recursive Autoencoders for Shape Structures (2017). arXiv preprint arXiv:1705.02090
22. Alhashim, I., Xu, K., Zhuang, Y., Cao, J., Simari, P., Zhang, H.: Deformation-driven
topology-varying 3D shape correspondence. ACM Trans. Graph. TOG 34, 236 (2015)
23. Williams, D.R.G.H.R., Hinton, G.: Learning representations by back-propagating errors.
Nature 323(6088), 533–538 (1986)
24. Chen, D.Y., Tian, X.P., Shen, Y.T., Ouhyoung, M.: On visual similarity based 3D model
retrieval. Comput. Graph. Forum 22(3), 223–232 (2003)
25. Chang, A.X., et al.: Shapenet: an information-rich 3D model repository. arXiv preprint
arXiv:1512.03012 (2015)
276
Y. Wu et al.

Source Distortion Estimation for Wyner-Ziv
Distributed Video Coding
Zhenhua Tang1,2(&), Sunguo Huang1, and Hongbo Jiang3
1 School of Computer and Electronic Information,
Guangxi University, Nanning, China
tangedward@126.com
2 Guangxi Key Laboratory of Multimedia Communications and Network
Technology, Guangxi University, Nanning, China
3 School of Electronic Information and Communications,
Huazhong University of Science and Technology, Wuhan, China
Abstract. Distributed video coding (DVC), which can move the computational
complexity burden from the encoder to the decoder, is an effective source coding
paradigm for promising video applications over wireless networks, e.g. wireless
video surveillance and wireless video sensor networks. For these video appli-
cations, it is crucial to provide an efﬁcient way to assess the quality of recon-
structed videos accurately. However, due to absence of original frames at the
decoder, how to estimate the reconstructed video quality of DVC remains a
challenging task. In this paper, we propose a source distortion estimation
method for DVC, in which the distortion incurred by the quantization and
reconstruction is taken into account. Focusing on the statistical distortion of a
transformed coefﬁcient in each Wyner-Ziv (WZ) frame, the proposed method
measures the average distortion of WZ frames utilizing only the coding infor-
mation available at the decoder, i.e. the coefﬁcients of side information
(SI) frames and the decoded coefﬁcients outputted from a decoder of low density
parity code (LDPC). Besides, we propose an estimation algorithm of probability
distribution parameters to deal with the case that all the coefﬁcients of a
sub-band are zero values by using an approximate principle. Experiments have
been conducted to validate the accuracy of our estimation method. For no
requirement of original WZ frames at the decoder, the presented method can be
suitable for real-time video applications.
Keywords: Source distortion estimation  Distributed video coding
Wyner-Ziv coding  Quantization  Reconstruction
1
Introduction
Distributed video coding (DVC) provides an effective source coding paradigm for
promising video applications, e.g. wireless video surveillance, wireless video sensor
networks, and mobile cameras phones. Inspired by the theorem of Slepian-Wolf
(SW) [1] and Wyner-Ziv (WZ) theorem [2], DVC can perform independent encod-
ing and joint decoding for correlated videos captured by single and multiple camera.
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 277–288, 2018.
https://doi.org/10.1007/978-3-319-73600-6_24

As one of the most typical DVC paradigms, WZ video coding solutions [3–6] aims at
moving the complexity burden, namely motion prediction, from the encoder to the
decoder. Consequently, they are appropriate for video applications over mobile
networks.
With the emerging of video applications over mobile communication networks, a
desired goal is to guarantee the best video quality possible for end-users [7]. To that
end, it is crucial to provide an efﬁcient way to assess the quality of reconstructed videos
accurately at the receiver. A number of methods have been proposed to measure the
quality of received videos. These approaches can be roughly divided into three cate-
gories: no-reference (NR) [8–10], reduced reference (RR) and full references (FR) [11].
The FR and RR approaches need original video information wholly or partially, thus
they are applicable for off-line video scenarios. In contrast, the NR methods can be
suitable for real-time or on-line applications because of its useless of original video
signals.
Since the quality of received videos in a practical system relies on various factors,
e.g. source coding, network conditions, and error correction, video distortions can be
measured individually for different functional units for a communication system, such
as video coding [7]. For traditional video coding solutions, such as MPEG-X and
H.26L, compression distortion is caused primarily by the quantization at the encoder.
While the distortion of WZ video coding is also introduced by the reconstruction
function and estimation errors of side information (SI) at the decoder besides quanti-
zation. Speciﬁcally, only parity bits instead of source frames are transmitted from the
DVC encoder to decoder, thus the substitute of source information called as SI are
needed to be estimated when performing decoding. Hence, how to estimate the
reconstructed video quality of DVC at the decoder accurately remains a challenging
task, especially for real-time scenarios.
Several pieces of work have been focus on analyzing or modeling the distortion of
DVC schemes. An analytical distortion model for WZ video coding that concentrates
on the distortion caused by the quantization and the reconstruction was developed by
Xiang [12]. But this model requires original WZ frames and their corresponding SI
information to ﬁt the model parameters at the encoder, which might increase the
burden of encoding and break the principle of simple encoding for WZ video coding.
In [13], Slowack et al. proposed a mode selection algorithm at the DVC decoder based
on a rate-distortion (R-D) model. The distortion incurred by quantization and recon-
struction in terms of the mean absolute difference (MAD) measure is considered in the
presented distortion model. However, the quantization errors of original key frames are
needed to be transmitted from the encoder to the decoder to obtain the distribution
parameter of the correlation errors. They may be suitable only for off-line video
applications. An R-D model based adaptive quantization scheme for DVC system in
transform domain was developed by [14]. In this scheme, the distortion incurred by
quantization is estimated at the decoder, but the probability distribution of original WZ
frames is required to be provided. This might be unrealistic for on-line video
applications.
To solve the above mentioned problems, we propose a source distortion estimation
method for WZ video coding, in which the distortion incurred by the quantization and
reconstruction is taken into account. Focusing on the statistical distortion of a
278
Z. Tang et al.

transformed coefﬁcient for each WZ frame based on the reconstruction portion at the
decoder, the proposed method measure the average distortion of WZ video frames
utilizing only the coding information available at the decoder, i.e. the coefﬁcients of SI
frames and the decoded coefﬁcients outputted from a decoder of low density parity
code (LDPC). For no requirement of original WZ frames, the presented estimation
method belongs to one of the NR algorithms.
The contributions of our work are as follows. First, we present a source distortion
estimation method for WZ video coding using only the coding information available at
the decoder unlike the exiting methods [12–14] which need source information from
the encoder. For no requirement of original WZ frames at the decoder, our method can
be easily applicable for real-time video applications as an advantage. Second, we
propose an estimation algorithm of probability distribution parameters, which uses an
approximate principle to deal with the case that all the coefﬁcients of a sub-band are
zero values. This method can improve the accuracy of distribution parameters.
The remainder of this paper is organized as follows. Section 2 presents a brief
description of the WZ video codec in transform domain adopted by this paper. Sec-
tion 3 introduces the proposed distortion estimation method for WZ coding solution,
and Sect. 4 describes the estimation of distribution parameter. In Sects. 5, experimental
results are presented. Finally, Sect. 6 concludes the paper.
2
Architecture of WZ Video Coding in Transform Domain
The purpose of this paper is to present a distortion estimation method for WZ video
coding, thus we should ﬁrst determine the architecture of a WZ video codec. Perfor-
mance studies have demonstrated that WZ video coding in transform domain is
superior to other schemes in pixel domain [3]. Hence in this paper we employ a typical
WZ video codec in transform domain illustrated in Fig. 1.
At the encoder, input video frames are divided into two types: key frames and WZ
frames. Key frames exploited to generate SI information are encoded by making use of
the traditional intra-frame coding approaches, e.g. H.264/AVC [15], MPEG-4 [16] or
JPEG [17]. While WZ frames are ﬁrst transformed by using 8  8 discrete cosine
transform (DCT), and then the DCT coefﬁcients are quantized. After that, the quantized
DCT coefﬁcients are encoded by a LDPC encoder. Finally, only parity bits outputted
from the LDPC encoder are transmitted to the decoder, while the systematic bits
representing original source information are discarded.
For the decoder, bit stream of key frames are decoded by utilizing the corre-
sponding intra-frame decoding methods. Then the decoded key frames are used to
generate SI frames for adjacent WZ frames. For a WZ frame, the LDPC decoder
outputs the decoded DCT coefﬁcients by utilizing the parity bits from the encoder and
the probability information of SI frame as inputs. Then the decoded DCT coefﬁcients
are reconstructed with the aid of the SI information. Eventually, inverse DCT transform
is performed to obtain the reconstructed WZ frame.
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding
279

3
Proposed Source Distortion Estimation Method
In this section, we describe the proposed source distortion estimation method for WZ
video coding. Since key frames are encoded by utilizing traditional intra-frame coding
approaches, source distortion estimation can be realized by using the algorithm of [18].
And source distortion of WZ frames is distinct from that of the traditional inter-frame
coding due to different coding process, hence we mainly focus on the distortion esti-
mation for WZ frames in this paper.
When decoding a WZ frame, the corresponding SI is required because only parity
bits are transmitted to the decoder. This may introduce some errors between the original
WZ frames and their corresponding SI frames, which are called correlation channel
errors [19]. On the other hand, the reconstruction portion at the decoder obtains the
ﬁnal signals by comparing the decoded DCT coefﬁcients outputted from the LDPC
decoder with the coefﬁcients of the corresponding SI frame. Though this may probably
decrease quantization errors, new errors between original WZ frames and SI might be
introduced. Therefore, source distortion of a WZ frame is primarily incurred by the
following factors: quantization, errors between the original WZ frames and their cor-
responding SI, and reconstruction. As the LDPC decoder performs full decoding in
case of enough parity bits, all the errors between the original WZ frames and their SI
frames will be corrected. Consequently, it is reasonable to assume that the distortions
caused only by the quantization and reconstruction in this paper. The objective of our
work is to estimate the source distortion between the original and reconstructed WZ
frames at the decoder. And we measure the distortion by using mean square error
(MSE) principle.
Let xi;j and ^xi;j be the original and reconstructed DCT coefﬁcients at the position
ði; jÞ of a WZ frame, respectively. And Dm represents the quantization step at the
position m of a given quantization matrix. The upper and lower bound of the rth
quantization interval can be denoted by qH
r ¼Dmðr þ 0:5Þ and qL
r ¼Dmðr  0:5Þ,
respectively. If a DCT coefﬁcient ~xi;j outputted from the LDPC decoder belongs to the
rth quantization interval, while the corresponding SI coefﬁcient of xi;j belong to the kth
quantization interval, then a typical reconstruction function can be formulated as
DCT 
Transform
Quantizer
LDPC 
Encoder
Inverse 
Transform
Reconstruction
LDPC 
Decoder
Conventioanl Intra-frame 
Encoding
Conventioanl Intra-frame 
Decoding
SI 
Generation
Encoder
Decoder
Key 
Frames
WZ 
Frames
Parity 
Bits
Fig. 1. The architecture of WZ video codec in transform domain.
280
Z. Tang et al.

^xi;j ¼
qH
r ;
r\k
yi;j ;
r ¼ k
qL
r ;
r [ k
8
>
<
>
:
ð1Þ
where yi;j is the corresponding SI coefﬁcient of xi;j.
Actually, it is very difﬁcult to calculate the distortion of each DCT coefﬁcient
accurately because of absence of original WZ coefﬁcients. Consequently, we focus on
modeling the statistical distortion of DCT coefﬁcients in this paper. The distortion of all
coefﬁcients in a WZ frame can be deﬁned as
DWZ ¼
X
i;j
E
Xi;j  ^Xi;j

2 Yi;j

h
i
:
ð2Þ
Thus, the distortion of a coefﬁcient can be derived as
DC ¼ E
Xi;j  ^Xi;j

2 Yi;j

h
i
¼
X
1
r¼1
Z qH
r
qLr
fXi;j Yi;j
j
ðxi;j yj i;jÞðxi;j  ^xi;jÞ2dxi;j ;
ð3Þ
where fXi;j Yi;j
j
ðxi;j yj i;jÞ represents the probability distribution function (PDF), which
equals to the PDF fXi;jYi;jðxi;j  yi;jÞ [12]. Moreover, the difference between X and Y
often accords with a Laplacian distribution [19], hence we can obtain
fX Y
j ðx yj Þ ¼ a
2 ea xy
j
j ;
ð4Þ
where a is the parameter of a Laplacian distribution.
For notation convenience, we drop the subscripts i; j in (3) and then rewrite (3) as
DC ¼
X
1
r¼1
Z qH
r
qLr
fX Y
j ðx yj Þðx  ^xÞ2dx:
ð5Þ
By incorporating (1) and (4), the expression (5) can be derived as
DC ¼
X
k1
r¼1
Z qH
r
qLr
a
2 eaðyxÞðx  qH
r Þ2dx
þ
Z y
qL
k
a
2 eaðyxÞðx  yÞ2dx
þ
Z qH
k
y
a
2 eaðxyÞðx  yÞ2dx
þ
X
1
r¼k þ 1
Z qH
r
qLr
a
2 eaðxyÞðx  qL
r Þ2dx :
ð6Þ
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding
281

The ﬁrst item
P
k1
r¼1
R qH
r
qLr
a
2 eaðyxÞðx  qH
r Þ2dx is derived as
I1 ¼
1
a2 eaDm
2  ðD2
m
2 þ Dm
a þ 1
a2Þe3aDm
2


 eaDmyN
1  eaDm :
ð7Þ
where yN is deﬁned as yN ¼
y
Dm  k, yN 2 ½0:5; 0:5.
Similarity, we can obtain the results of other three items as follows.
I2 ¼
 ð1 þ 2yNÞ2D2
m
8
 ð1 þ 2yNÞDm
2a
 1
a2
"
#
 eað1
2 þ yNÞDm þ 1
a2 :
ð8Þ
I3 ¼
 ð1  2yNÞ2D2
m
8
 ð1  2yNÞDm
2a
 1
a2
"
#
 eað1
2yNÞDm þ 1
a2 :
ð9Þ
I4 ¼
1
a2 eaDm
2  ðD2
m
2 þ Dm
a þ 1
a2Þe3aDm
2



eaDmyN
1  eaDm :
ð10Þ
Finally, the distortion of a coefﬁcient can be achieved by adding (7), (8), (9),
and (10)
Dc ¼
ðaDm þ 2ÞDmyN
a
sinhðaDmyNÞ

 ðaDm þ 2ÞDmeaDm
að1  eaDmÞ
þ aD2
mð1 þ 4y2
NÞ þ 4Dm
4a


coshðaDmyNÞ

 eaDm
2 þ 2
a2 :
ð11Þ
From Eq. (11), it can be seen that the quantization step, the parameter of Laplacian
distribution, and SI coefﬁcients should be provided when estimating the distortion of
coefﬁcients in a WZ frame. The quantization step and SI coefﬁcients can be obtained
easily at the decoder, while the parameters of Laplacian distribution need to be esti-
mated at the decoder, which will be discussed in Sect. 4.
4
Estimation of Distribution Parameter
In this section, we present an estimation method for the Laplacian distribution
parameter a at the decoder. Since the original WZ frames are not provided at the
decoder, how to estimate the distribution parameter of the errors between original WZ
frames and the corresponding SI frames accurately remains a challenge task. The goal
of our work is to estimate the distribution parameter a by using the decoded coefﬁcients
of a WZ frame outputted from LDPC decoder and the coefﬁcients of SI frame.
282
Z. Tang et al.

Let xL
i;j be a coefﬁcient at the coordinate ði; jÞ outputted from the LDPC decoder,
and we can obtain a residue coefﬁcient as
Ri;j ¼ xL
i;j  yi;j :
ð12Þ
The residual coefﬁcients are grouped into different 8  8 sub-bands in DCT
domain. Then we can estimate the parameter as of each sub-band by using
as ¼ 2 lnð1  NzðsÞ
NtðsÞÞ
	
Dm :
ð13Þ
where NzðsÞ and NtðsÞ denote the number of zero values and the total number of
coefﬁcients in the sub-band s.
However, if all the coefﬁcients in a sub-band are values of zero, it will fail to use
expression (13) to obtain a reasonable parameter obviously. The main reason is that the
estimated parameter would tend towards an inﬁnite value in this case. Therefore, to
solve this problem, we propose an estimation approach that utilizes an approximate
principle. And the steps of the presented approach are as follows.
Step 1: we calculate the value NzðsÞ=NtðsÞ for each sub-band of a current pro-
cessing WZ frame, and ﬁnd the best nearest value to the maximum, i.e.1. The corre-
sponding sub-band is called a sub-max band in this paper.
Step 2: we obtain the corresponding distribution parameter asub
max and quanti-
zation step Dsub
max of the sub-max band, respectively.
Step 3: we achieve the distribution parameter of the sub-band that are all zero
values by computing with
azero ¼ asub
max
Dsub
max
 Dzero :
ð14Þ
Where Dzero represents the quantization step of a sub-band in which all of the coefﬁ-
cients are values of zero.
To sum up, we can formulate the expression of estimating the distribution
parameter by combining (13) with (14)
as ¼
2 lnð1  NzðsÞ
NtðsÞÞ
	
Dm ;
Nz
Nt
\1
aSub max
DSub max
 Dzero ;
Nz
Nt
¼ 1
8
>
>
<
>
>
:
:
ð15Þ
5
Experimental Results
We verify the performance of the proposed source distortion estimation method making
use of the DVC codec provided by [20]. All the tests are conducted on the typical video
sequences dataset [21] which are commonly used in video coding, processing and
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding
283

communication. The resolutions of videos are 176  144 with 4:2:0 format, and the
total number of video frames for each sequence is 130. In the experiments, we use the
JPEG quantization matrix and four quantization parameters (QP): 0.5, 1.0, 2.0, and 4.0
employed by [18]. Thus the videos can be encoded with different bitrates by switching
various QP to multiple the quantization matrixes. And the frame rate is set as 15 frames
per second. The group of picture (GOP) is set as 2, and the ﬁrst frame of each GOP is a
Table 1. Practical PSNR and estimated PSNR values.
Video
sequence
Average
rate
(kbps)
①
Practical
values
(dB)
②
Our
method
(dB)
③
Approach
[13]
(dB)
Deviations of our
method
Deviations of approach
[13]
Absolute
error (dB)
|②−①|
Relative
error
|②−①|/
①
Absolute
error (dB)
|③−①|
Relative
error
|③−①|/
①
Stefan
96.42
31.76
31.66
30.93
0.10
0.31%
0.83
2.61%
65.82
28.84
28.48
25.39
0.36
1.25%
3.45
11.96%
44.54
26.46
26.14
19.53
0.32
1.21%
6.93
26.19%
30.11
24.47
23.89
13.48
0.58
2.37%
10.99
44.91%
Football
82.69
34.47
34.21
30.83
0.26
0.75%
3.64
10.56%
56.51
31.97
31.15
26.10
0.82
2.56%
5.87
18.36%
37.41
29.66
28.63
18.89
1.03
3.47%
10.77
36.31%
23.98
27.49
26.73
14.58
0.76
2.76%
12.91
46.96%
Soccer
70.44
35.72
35.21
30.72
0.51
1.43%
5.00
14.00%
48.10
33.34
33.55
25.17
0.21
0.63%
8.17
24.51%
31.37
31.07
30.05
19.82
1.02
3.28%
11.25
36.21%
20.28
28.76
28.09
13.45
0.67
2.33%
15.31
53.23%
Harbor
91.19
33.69
33.47
31.03
0.22
0.65%
2.66
7.90%
62.81
30.81
31.07
25.47
0.26
0.84%
5.34
17.33%
41.68
28.21
28.14
19.85
0.07
0.25%
8.36
29.63%
27.24
25.82
26.14
13.12
0.32
1.24%
12.70
49.19%
Mobile
116.54
30.78
30.90
30.70
0.12
0.39%
0.08
0.26%
76.18
27.82
27.74
25.52
0.08
0.29%
2.30
8.27%
49.81
25.46
25.33
19.79
0.13
0.51%
5.67
22.27%
33.13
23.53
22.80
13.79
0.73
3.10%
9.74
41.39%
Coast-guard
76.55
34.16
33.75
31.30
0.41
1.20%
2.86
8.37%
52.15
31.71
30.64
25.33
1.07
3.37%
6.38
20.12%
33.61
29.46
28.39
19.87
1.07
3.63%
9.59
32.55%
21.61
27.24
26.64
14.20
0.60
2.20%
13.04
47.87%
Hall
53.57
36.76
35.76
31.74
1.00
2.72%
5.02
13.66%
35.87
33.68
32.61
25.83
1.07
3.18%
7.85
23.31%
33.84
30.73
30.33
20.11
0.40
1.30%
10.62
34.56%
17.90
28.18
27.79
13.52
0.39
1.38%
14.66
52.02%
Akiyo
37.11
38.95
38.37
32.40
0.58
1.49%
6.55
16.82%
25.73
35.93
35.82
25.71
0.11
0.31%
10.22
28.44%
17.76
33.21
34.21
20.17
1.00
3.01%
13.04
39.27%
13.76
30.84
32.12
15.48
1.28
4.15%
15.36
49.81%
284
Z. Tang et al.

key frame. To evaluate the performance of the proposed method, we compare the
proposed method with the approach developed by [13]. And we employ the peak signal
noise ratio (PSNR) to measure the distortion values. Let vP and vM denote the values of
PSNR and MSE, respectively. Then the transformable relation between the two values
can be described as
vP ¼ 10  log10
2552
vM


:
ð16Þ
Table 1 shows the average values of practical distortion and estimated distortion.
Here we calculate the average distortion values over all of WZ frames for each video
sequence. From Table 1 it can be seen that the distortion values estimated by our
method is very close to the practical distortion. Speciﬁcally, the average absolute errors
between them are almost within 1 dB, and the average relative errors are within 5%. In
contrast, the distortion values produced by the approach of [13] are far away the
practical distortion values. The main reason is that the quantization noise variances
introduced by [13] may enlarge the standard deviation of the Laplace distribution. This
might lead to lower the distribution parameter, and affect the accuracy of estimated
distortion as a result. These results indicate that the performance of our method is
superior to the approach of [13] clearly.
As shown in Fig. 2 and Fig. 3, we compare the estimated PSNR values of our
method with the practical PSNR values. Figure 2 illustrates the results of three video
sequences: stefan, soccer, and mobile. And Fig. 3 demonstrates the results of other
three video sequences: football, harbor, and coastguard. As can be seen from the plots,
our proposed estimation method is quite accurate.
To further demonstrate the performance of our proposed method, we illustrate the
estimated results for each WZ frame in Figs. 4, 5, 6, and 7. Speciﬁcally, the results for
sequences soccer, football, stefan, and coastguard are depicted in Figs. 4, 5, 6, and 7,
respectively. From these results, we can see that the distortion value of each WZ frame
estimated by our method approximates to that of practical distortion very closely. These
results indicate that the estimation accuracy of our method is very high.
15
20
25
30
35
40
15
20
25
30
35
40
Estimated PSNR(dB)
True PSNR(dB)
Fig. 2. PSNR estimation versus True PSNR for stefan, soccer and mobile sequences.
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding
285

15
20
25
30
35
40
15
20
25
30
35
40
Estimated PSNR(dB)
True PSNR(dB)
Fig. 3. PSNR estimation versus True PSNR for football, coastguard and harbor sequences.
0
20
40
60
80
100
120
28
30
32
34
36
38
40
42
Frame number
PSNR(dB)
 
 
Pracitcal distortion
Approach of literature [13]
Proposed method
Fig. 4. Distortion estimation of WZ frames for soccer video sequence (QP = 0.5).
0
20
40
60
80
100
120
30
35
40
45
Frame number
PSNR(dB)
 
 
Pracitcal distortion
Approach of literature [13]
Proposed method
Fig. 5. Distortion estimation of WZ frames for football video sequence (QP = 0.5).
0
20
40
60
80
100
120
29
30
31
32
33
34
35
36
Frame number
PSNR(dB)
 
 
Pracitcal distortion
Approach of literature [13]
Proposed method
Fig. 6. Distortion estimation of WZ frames for stefan video sequence (QP = 0.5).
286
Z. Tang et al.

6
Conclusion
In this paper, we propose a source distortion estimation method for WZ video coding,
in which the distortion incurred by the quantization and reconstruction is taken into
account. The proposed method can estimate the average distortion of WZ frames
utilizing only the coding information available at the decoder. Experimental results
show that the estimation accuracy of our method is very high. For no requirement of
original WZ frames at the decoder, the advantage of our method is that it can be easily
applicable for real-time video applications. In the proposed method, we assume the
errors between the original WZ frames and their corresponding SI are corrected by
LDPC decoder in case of enough parity bits. And we will further consider the distortion
caused by the case that some parts of errors are not corrected in the future work.
Acknowledgments. This work is supported in part by the National Natural Science Foundation
of
China
(No.
61461006);
by
the
Guangxi
Natural
Science
Foundation
Project
(No. 2016GXNSFAA380216). This research is also supported by the fund of Guangxi Colleges
and Universities Key Laboratory of Multimedia Communications and Information Processing.
References
1. Slepian, D., Wolf, J.: Noiseless coding of correlated information sources. IEEE Trans. Inf.
Theory 19(4), 471–480 (1973)
2. Wyner, A., Ziv, J.: The rate-distortion function for source coding with side information at the
decoder. IEEE Trans. Inf. Theory 22(1), 1–10 (1976)
3. Girod, B., Aaron, A., Rane, S., Rebollo-Monedero, D.: Distributed video coding. Proc. IEEE
93(1), 71–83 (2005)
4. Ascenso, J., Brites, C., Pereira, F.: Content adaptive Wyner-Ziv video coding driven by
motion activity. In: Proceedings of IEEE International Conference on Image Processing,
Atlanta, GA, pp. 605–608 (2006)
5. Aaron, A., Rane, S., Setton, E., Girod, B.: Transform-domain Wyner-Ziv codec for video.
In: Proceedings of the SPIE, pp. 520–528 (2004)
6. Shen, Y., Cheng, H., Luo, J., Lin, Y., Wu, J.: Efﬁcient real-time distributed video coding by
parallel progressive side information regeneration. IEEE Sens. J. 17(6), 1872–1883 (2017)
7. Apostolopoulos, J.G., Reibman, A.R.: The challenge of estimating video quality in video
communication applications. IEEE Sig. Process. Mag. 29(2), 156–160 (2012)
0
20
40
60
80
100
120
30
32
34
36
38
Frame number
PSNR(dB)
 
 
Pracitcal distortion
Approach of literature [13]
Proposed method
Fig. 7. Distortion estimation of WZ frames for coastguard video sequence (QP = 0.5)
Source Distortion Estimation for Wyner-Ziv Distributed Video Coding
287

8. Zhu, K., Li, C., Asari, V., Saupe, D.: No-reference video quality assessment based on artifact
measurement and statistical analysis. IEEE Trans. Circ. Syst. Video Technol. 25(4), 533–546
(2015)
9. Søgaard, J., Forchhammer, S., Korhonen, J.: No-reference video quality assessment using
codec analysis. IEEE Trans. Circ. Syst. Video Technol. 25(10), 1637–1650 (2015)
10. Huang, X., Søgaard, J., Forchhammer, S.: No-reference pixel based video quality assessment
for HEVC decoded video. J. Vis. Commun. Image Represent. 43, 173–184 (2017)
11. Chikkerur, S., Sundaram, V., Reisslein, M., Karam, L.-J.: Objective video quality
assessment methods: a classiﬁcation, review, and performance comparison. IEEE Trans.
Broadcast. 57(2), 165–182 (2011)
12. Xiang, S., Cai, L.: Distortion analysis of Wyner-Ziv distributed video coding. In:
Proceedings of IEEE Global Telecommunications Conference, pp. 1–5 (2010)
13. Slowack, J., Mys, S., Skorupa, J., Deligiannis, N., Lambert, P., Munteanu, A., Walle, R.:
Rate-distortion driven decoder-side bitplane mode decision for distributed video coding. Sig.
Process. Image Commun. 25(9), 660–673 (2010)
14. Chien, W.-J., Karam, L.J.: Transform-domain distributed video coding with rate–
distortion-based adaptive quantisation. IET Image Process. 3(6), 340–354 (2009)
15. Ostermann, J.: Video coding with H.264/AVC: tools, performance, and complexity. IEEE
Circ. Syst. Mag. 4(1), 7–28 (2004)
16. Information Technology—Coding of Audio/Visual Objects. ISO/IEC14496-2:1999 (1999)
17. Pennebaker, W.B., Mitchell, J.L.: JPEG Still Image Data Compression Standard. Van
Nostrand Reinhold, New York (1993)
18. He, Z., Mitra, S.K.: A uniﬁed rate-distortion analysis framework for transform coding. IEEE
Trans. Circ. Syst. Video Technol. 11(12), 1221–1235 (2001)
19. Brites, C., Pereira, F.: Correlation noise modeling for efﬁcient pixel and transform domain
Wyner-Ziv video coding. IEEE Trans. Circ. Syst. Video Technol. 18(9), 1177–1190 (2008)
20. Varodayan, D., Chen, D., Flierl, M., Girod, B.: Wyner-Ziv coding of video with
unsupervised motion vector learning. Sig. Process. Image Commun. 23(5), 369–378 (2008)
21. Xiph.org Video Test Media. http://media.xiph.org/video/derf/
288
Z. Tang et al.

SRN: The Movie Character Relationship
Analysis via Social Network
Jingmeng He1, Yuxiang Xie1(&), Xidao Luan2, Lili Zhang1,
and Xin Zhang1
1 National University of Defense Technology, Changsha 410072, China
{hejingmeng11,yxxie}@nudt.edu.cn
2 Changsha University, Changsha 410072, China
Abstract. Video character relationship mining, as a kind of video semantic
analysis, has become a hot topic. Based on the temporal and spatial context and
video semantic information of the scene, this paper proposes a method of
exploiting character co-occurrence relationship, and constructs character’s social
network called SRN through the quantitative relationship among the characters.
Based on SRN network, we can get rid of the limitation of the traditional
feature-based approach and carry out more in-depth video semantic analysis. By
analyzing the characters in the SRN network, a community identiﬁcation
method based on the core characters is proposed, which can automatically
conﬁrm the core characters in the video and dig out the communities around the
core characters. In this paper, lots of movie videos are used to experiment, and
experimental results show the effectiveness of SRN model and community
identiﬁcation method.
Keywords: Video character relationship mining  Temporal and spatial context
Character co-occurrence relationship  SRN  Community identiﬁcation
1
Introduction
With the gradual change of peopel’s audio-visual consumption in the information age,
the number of TV, movies and other video resources increases rapidly. Meanwhile,
with the continuous development of video analysis technology, the demand for auto-
matic access to the information such as video story line, characters, etc. is also growing.
Recently, there has been methods and technologies proposed to meet these needs,
among which social network analysis is an effective video story analysis technique.
Social network technology is applied to video story analysis by designing the video
as a weighted graph, where the nodes represent the characters, the edges represent
relationships among characters and the weights represent closeness of the relationships.
Vinciarelli et al. [1] was the ﬁrst to use social network analysis technology for the
segmentation of TV programs. Park et al. [2] construct the characters relationship
network called character-net through the dialogue relationship between the characters,
and dig the character relationship through social network analysis. In [3], the RoleNet
is constructed by co-occurrence relationship of the characters in the scene, which is
used to analyze the core characters and identify the communities. The method in [4] is
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 289–301, 2018.
https://doi.org/10.1007/978-3-319-73600-6_25

an extension of one in [3], Yeh et al. proposed the method on the basis of taking full
account of the characters relationship existing between adjacent shots. Compared with
the methods in [2–4], which mine character relationship through the characters
co-occurrence state in the video scenes and shots, the Literature [5, 6] calculated the
time interval which characters appeare in the video, as a basis to conﬁrm co-occurrence
relationship between the characters, which can effectively avoid missed judgment.
Social networking technology is also used in other video analysis areas. The social
network proposed in [6, 7, 12] is constructed for video summary analysis. The [13]
proposed a learning-based movie summarization framework via role-community social
network analysis and feature fusion. The social network analysis is applied to the video
recommendation in [14]. There are many kinds of methods to construct video social
network, and the application ﬁeld is very extensive. This paper will continue to study
video character relationship based on the methods proposed in the above literatures.
This paper presents a SRN-based video character community analysis method. The
algorithm ﬂow is demonstrated in Fig. 1. As shown in the ﬁgure, the method proposed
in this paper is divided into three stages: the character recognition stage, the SRN
network construction stage and the community analysis stage. The structure of chapters
is also organized according to the above three stages.
2
Character Identiﬁcation
As shown in Fig. 1, before constructing the SRN for a movie, we perform character
identiﬁcation, which is carried out to identify the individual main characters appearing
in each scene. Character identiﬁcation is a very important step for character-based
Fig. 1. Overview of the proposed algorithm ﬂow
290
J. He et al.

video analysis, which can be divided into two stages: (1) video structured analysis,
(2) human-face clustering. The former includes scene boundary detection and shot
detection. Subsequently, we perform face detection and face feature extraction and use
clustering algorithm for human-face clustering in the latter stage.
In our implementation, the method proposed in [9] is used to detect scene
boundaries for a movie. After the completion of scene boundaries detection, we use the
libfacedetection1 engine for face detection, which can quickly and efﬁciently achieve
face detection. Based on it, we perform face feature extraction using SeetaFace2
open-source face identiﬁcation engine, in which the outputs of the 2048 nodes of the
FC2 layer in the VIPLFaceNet [8] are exploited as the feature of the input face. It is
worth mentioning that the model shipped in with the codes is trained with 1.4M face
images of 16K subjects including both Mongolians and Caucasians. Finally, the
hierarchical clustering method is used to cluster the human faces. Note, face occlusion,
angle changes, light change and other factors may lead to incorrect human face clus-
tering. Therefore, after the completion of a round of clustering, we select the faces,
clustering number of which is 1. Since these faces are most likely to be misclassiﬁed,
we use them to perform the next round of clustering by reducing the similarity
threshold, and cycle several times. The approach can effectively improve the clustering
accuracy.
In this paper, only the major characters in the video are analyzed, so after the
completion of face clustering, it is necessary to exclude faces with fewer clusters in the
results, and the remaining ones were judged as the major character. According to the
experimental results, the overall accuracy of face clustering reaches about 75%.
3
Construction of SRN
3.1
The Deﬁnition of SRN
A SRN is an undirected weighted graph expressed by
G ¼
C; E; W
h
i
ð1Þ
where C ¼
c1; c2; . . .; cn
f
g denotes the nodes in graph G, just the set of characters
that appear in the video. E denotes the set of edges in graph G, where eij represents the
edge between ci and cj. The element in W represents the strength of the relationship
between characters.
In order to construct an SRN, it is necessary to quantify the “relationship” between
the characters, i.e. wij. RoleNet [3] quantify characters’ relationship as the number of
co-occurrence between roles in the scene. However, this model can not accurately
reﬂect the relationship between characters, since the characters appear in the same
scene may have different importance, and should not be given the same weight.
1 https://github.com/ShiqiYu/libfacedetection
2 https://github.com/seetaface/SeetaFaceEngine
SRN: The Movie Character Relationship Analysis via Social Network
291

The SRN proposed in this paper can quantify the importance of the characters in the
scene as a basis for calculating the co-occurrence matrix for the whole video. Next,
model description will be performed.
3.2
The Importance Analysis of Scene Characters
To determine whether the character is important, the most intuitive factor is the number
of times it appears in the scene, but this evaluation method is one-sided and inaccurate.
In this paper, we analyze the characters’ relationship in the spatio-temporal context to
get the inﬂuence coefﬁcient of the characters in a scene, and then take acount of both
inﬂuence coefﬁcient and the number of appearing times to decide the character’s
importance degree in the scene.
Two or more characters appear in the same shot, which often indicate that they have
a close associations. Taking into account the continuity of video content, character
relationships may be implied in sequential video shots. Therefore, character relation-
ship could be analyzed in each shot and the shots around it.
Given a shot sequence in a scene, we can express the status of occurrence by a
matrix S ¼ ski
½
mn, where the element
ski ¼
1;
if character ci appears in the k-th shot
0;
otherwise

ð2Þ
After analyzing characters appearing in the shot sequence within a surrounding
span k  r; k þ r
½
 around the target shot (k-th), we can get the character co-occurrence
matrix Rk ¼ ½rk
ijnn based on the k-th shot. The speciﬁc method is as follows:
Gaussian content is used to quantify the co-occurrence relationship between the
character ci and cj which appear in the target shot and the surrounding shots respec-
tively. If the character cj appears in the shot sequence within a surrounding span
k  r; k þ r
½
, the following formula can calculate the elements of the matrix Rk:
rk
ij ¼
X
k þ r
n¼kr
ðeðnkÞ2
r2
 ski  snjÞ
ð3Þ
where k  r  n  k þ r, and rk
ij ¼ 0, if character ci is absent in the k-th shot. The n-th
shot is nearer to the k-th one in temporal sequence, which demonstrates that the closer
concurrence extent can be built.
After obtaining the matrix Rk, we further analyze character relationships in scene
level. In this step, we cumulate matrix Rk of each shot in a scene to calculate the
co-occurrence matrix R based on scenes, which is deﬁned as follows:
rij ¼
X
m
k¼1
rk
ij
ð4Þ
292
J. He et al.

where rij denotes the co-occurrence relationship value between character ci and cj, and
m is the number of shots in scene.
Through the value of the matrix, we can grasp the closeness of relationship between
the character ci and other characters, the stronger which is, indicating the more
inﬂuence coefﬁcient the character owns in the scene. Moreover, after taking acount of
both inﬂuence coefﬁcient and the number of occurrences of the character ci, we can
quantify the importance of the character, which is expressed as followed:
Ip
i ¼
X
j
rij þ kNumðiÞ
ð5Þ
where NumðiÞ is the number of occurrences of the character, and k is the balance
parameter. After normalization, the importance vector ImpðpÞ of the p-th scene can be
obtained as followed:
ImpðpÞ ¼ fIp
1; Ip
2; . . .; Ip
ng
ð6Þ
where n is the number of characters in the whole video.
3.3
The Construction of SRN
After the scene characters importance analysis, we futher perform the construction of
SRN. As shown in Fig. 2(a), the edge denotes that the character ci is present in the j-th
scene. For a movie composed of z scenes and n different characters, we can express the
status of occurrence by a matrix A ¼ ½aijzn, where the element is deﬁned as followed:
aij ¼
1;
if character ci appears in the j-th shot
0;
otherwise

ð7Þ
As shown in Fig. 2(b), the matrix A reﬂects the characters co-occurrence rela-
tionship in the scene, also contains the importance distribution of the different char-
acters in the scene. Based on the matrix A, we quantify the relationship between the
character ci and cj as follows:
wij ¼ P
z
c¼1
aciacj ¼ aT
i aj
for i 6¼ j
ð8Þ
where wij represents the quantitative relationship value between the character ci and cj.
Extending to the entire matrix, we can get the relationship among characters in the
whole movie as follows:
W ¼ ATA
ð9Þ
where wij is set as 0 while i ¼ j. The W is mathematical representation of SRN, and the
corresponding graphical representation is shown in Fig. 2(c), where the edge between
two nodes represent that there exists relationship between two characters, and is
SRN: The Movie Character Relationship Analysis via Social Network
293

weighted based on the closeness between them. The thickness of the edge represents
the strength of the relationship among characters.
In summary, we have transformed the character relationship in the video into SRN
and will use this as a basis for more in-depth video character analysis.
4
Community Identiﬁcation Method Based on Core
Characters
After the construction of SRN, we perform community analysis, which is divided into
the two steps: (1) the core character determination; (2) community identiﬁcation. Core
characters are the persons who have the most signiﬁcant impact and dominate the
progress of stories in a movie. A community is a group of characters that relatively
have similar relationships to a core character [3].
We introduce the SRN-based community analysis approach in this section. The ﬁrst
step is to determine the core nodes, and then clustering of SRN nodes will be per-
formed to obtain associated nodes that relatively have close relationships to core nodes.
Shot 1
Shot 2
Shot 3
Shot 4
Shot 5
    SRN
Co-occurrence Relationship Matrix W
Shot 1
Shot 2
Shot 3
Shot 4
Shot 5
Time derecƟon
1c
2c
3c
4c
5c
6c
7c
Shots sequence
Characters
1c
2c
3c
4c
5c
6c
7c
1
1
1
1
2
4
2
2
2
2
1
3
5
6
3
3
3
2
4
7
4
4
4
1
5
6
5
5
7
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
z n
I
I
I
I
I
I
I
A
I
I
I
I
I
I
I
I
×
⎤
⎡
⎥
⎢
⎥
⎢
⎥
⎢
=
⎥
⎢
⎥
⎢
⎥
⎢
⎦
⎣
,
0
T
ii
W
A A
w
=
=
12
17
21
67
71
76
0
0
0
w
w
w
W
w
w
w
⎡
⎤
⎢
⎥
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
(a)
(b)
(c)
    SRN
 Co-occurrence Relationship Matrix W
Fig. 2. The construction of SRN
294
J. He et al.

4.1
Core Characters Determination
In order to evaluate the inﬂuence of each SRN node and determine the core node in the
network, we introduce the centrality value of nodes. Generally speaking, the larger the
centrality value is, the greater inﬂuence the node has in the network. One way to
calculate the centrality value is to count the number of all connected edges, but SRN is
a weighted graph, which conveys much information in edge weights, so that way can
not accurately reﬂect the information of SRN. Based on SRN, the centrality value CðciÞ
of the node (character) ci is calculated as
CðciÞ ¼
X
j
wij
ð10Þ
where wij is the edge weight of SRN. Generally, the node with the largest centrality
value can be determined as the core one. But there may be more than one core character
in the video and the number can not be determined in advance. Therefore, we need an
automatic core node determination method.
As the core character has a much greater inﬂuence than other characters, the
centrality value of the other nodes has a signiﬁcant gap with the core nodes. Based on
this, the method to automatically deternime the core nodes is shown in Table 1.
According to Algorithm 1, we can get the set of core characters deﬁned as
LC ¼ flc1; lc2; ::; lclg, where l denotes the number of core characters.
4.2
Community Identiﬁcation
Community identiﬁcation is actually SRN nodes clustering. In view that each core
character corresponds to a community, we proposed a SRN nodes clustering method
based on core nodes to excavate the hidden community information.
Each community consists of a core SRN node and its associated SRN nodes.
Therefore, after obtaining the core node, we can achieve SRN nodes clustering after
determining the associated nodes of each core nodes.
The algorithm iteratively performs the following two steps: (1) the core node
selection; (2) the associated node conﬁrmation, as elaborated below:
Table 1. The core node determination method
Algorithm 1: the core node determination
1 Calculate the centrality value of each node in the SRN;
2 Sort all nodes in descending order according to the centrality value
3 Calculate the centrality value difference of adjacent nodes, and get the 
node centrality difference distribution;
4 According to the difference distribution, the maximum difference is 
obtained. The two nodes corresponding to the value are the boundary 
between the core node and other nodes.
SRN: The Movie Character Relationship Analysis via Social Network
295

(1) The core node selection: Find the core node with the largest centrality value in set
LC obtained in the previous section, expressed is as followed:
lrcðpÞ ¼ arg
max
lck2LCðpÞ CðlckÞ
ð11Þ
where LCðpÞ denotes LC at the p-th iteration. When p is set as 1, LCðpÞ is just LC.
(2) The associated node conﬁrmation: According to the core node lrcðpÞ, its asso-
ciated node will be conﬁrmed into the p-th community. To realize this case, the
ﬁrst step is ﬁnding out all the nodes connected to the core node lrcðpÞ in the
SRNðpÞ, which denotes SRN at the p-th iteration. These nodes are divided into two
types: one is the node set only connected with lrcðpÞ instead of other core nodes,
represented by olkðlrcðpÞÞ; the other is the node set still connected with other core
nodes, represented by mlkðlrcðpÞÞ. The former is directly selected into the p-th
community. The latter should be judged which have closer relationship with
lrcðpÞ, and the closer node will be regarded as associated one. The mathematical
expression of the judgment method is as followed:
flkðlrcðpÞÞ ¼
ci 2 mlkðlrcðpÞÞ
lc ¼ arg
max
lck2LCðpÞ wðci; lckÞ;
lc 2 flrcðpÞg

(
)
ð12Þ
where flkðlrcðpÞÞ is a screening result. The associated node set alrcðpÞ ¼
folkðlrcðpÞÞ; flkðlrcðpÞÞg. Therefore, the clustering result after the p-th iteration is
gclðpÞ ¼ flrcðpÞ; alrcðpÞg.
After getting the gclðpÞ, we peform the p + 1-th iteration. Set SRNðp þ 1Þ ¼
SRNðpÞngclðpÞ and LCðp þ 1Þ ¼ LCðpÞnlrcðpÞ where the symbol “\” represents the set
difference, which removes the nodes included in gclðpÞ from SRNðpÞ and remove the
lrcðpÞ included in LCðpÞ. Repeat the above-mentioned the core node selection and the
associated node conﬁrmation to get gclðp þ 1Þ. Clustering is ﬁnished until all the nodes
in SRN are clustered.
After the node clustering, we transform SRN into multiple sub-networks, each of
which denotes a community and is centered on a core node.
5
Experimental Results Analysis
In the experiment, we choose 8 movies as test videos to evaluate the proposed model
and method, and the total length is 16 h. Table 2 demonstrates the speciﬁc information
of the selected videos. As shown in the table, each movie has long time, numerous
scenes and rich characters, which are conducive to analyze the characters social rela-
tionship. We can reveal the effectiveness of the proposed model and method through
analyzing the experimental results of the core character determinnation and community
identiﬁcation.
296
J. He et al.

Before the experiments, we need to obtain the ground truth, that is, to learn who the
core characters are and who are within the community, which provides a factual basis
for later experimental veriﬁcation. Here we use manual analysis: we organize more
than one person to watch the movies and analyze the movie reviews, and determine the
relationship among the characters based on their judgments. If there are differences in
their analysis results about a certain character’s position, the way of voting will be used
to judge to conﬁrm the ground truth.
5.1
Core Characters Determination
Taking the movie “You’ve Got Mail” as an example, we evaluate the core character
determination method. First of all, construct the SRN model, and calculate the centrality
value of each character in the movie, as shown in Fig. 3(a). Then sort all characters in
descending order according to the centrality value, as illustrated in Fig. 3(b). As can be
seen from the ﬁgure, since the character 1 and 2 have a signiﬁcant gap with the other
characters, we can determine character 1 and 2 as the core character according to the
centrality difference distribution.
Besides, we design a method to evaluate the performance of core characters
determination. There are the core characters set LC ¼ flc1; lc2; ::; lclg in a movie
where l denotes the set number, and the core characters set determined by the proposed
Table 2. The speciﬁc information of the selected videos
ID
Movie
Length Characters number Scenes number
M1 You’ve Got Mail (1998)
119 m
14
62
M2 21 Grams (2003)
124 m
20
120
M3 Mission Impossible 4 (2011)
132 m
11
71
M4 Catch Me If You Can (2002) 141 m
15
101
M5 The Lake House (2006)
105 m
9
76
M6 Hidden Figures (2016)
127 m
13
65
M7 Four Brothers (2005)
109 m
17
59
M8 Broken Flowers (2005)
106 m
13
69
(a)
(b)
Fig. 3. The character centrality value
SRN: The Movie Character Relationship Analysis via Social Network
297

method can be represented by LC ¼ fuc1; uc2; ::; uchg where h denotes the set
number. So the core characters determination accuracy rate D is calculated as followed:
D ¼
P
h
i¼1
diCðuciÞ
P
l
i¼1
CðlciÞ
di ¼ 1
if uci 2 LC
di ¼ 1
otherwise
(
ð13Þ
where C ð Þ represents the centrality value deﬁned in Sect. 4.1, and di indicates whether
uci is included in the LC, if true, set as 1, otherwise 0; The evaluation index D fully
considers the emergence of misjudgment and missed judgment of core characters, and
accurately quantiﬁes the performance of determination through combining the cen-
trality value in the SRN network. The larger D is, more accurate the determination
results are.
After repeating the above-mentioned process to all the movies, the performance of
core character determination can be obtained shown in Table 3. The second column of
the table shows the core character identiﬁed in the proposed method, and the third
column shows the ground truth by the manual analysis, and the fourth column shows
the determination accuracy rate D of each video quantiﬁed by the Formula (13). From
the table, we can see that the performance of the method is excellent and satisfactory.
Except M7 and M8, all the core characters can accurately comﬁrmed. We attribute the
prospective result that core characters almost appear in all the scenes of the movie and
have a close relationship with others. This proposed method exactly makes use of the
characteristic.
5.2
Community Identiﬁcation
After the core characters determination, we perform the community identiﬁcation
subsequently, which is a process to deal with the experimental data M1–M8 through
the proposed method. First of all, we designs a method to quantify the effectiveness of
community identiﬁcation by calculating community identiﬁcation accuracy rate, which
is deﬁned below:
Table 3. Determined core characters
ID
Core charcters Ground truth Value of Δ
M1 1, 2
1, 2
100%
M2 1, 2, 6
1, 2, 6
100%
M3 1
1
100%
M4 1, 2
1, 2
100%
M5 1, 2
1, 2
100%
M6 1, 2, 4
1, 2, 4
100%
M7 1, 2, 3
1, 2, 3, 5
77.5%
M8 1, 2, 7
1, 2
85.4%
298
J. He et al.

For a movie, there are the core characters set LC ¼ flc1; lc2; ::; lclg and the
associated characters set RC ¼ frc1; rc2; ::; rcmg where l and m denotes the set
number respectively. If lci and rcj are in the same community, \lci; rcj [ is a
community element, and the indicative value aij is set as 1, otherwide 0. The value of
aij indicates whether the identiﬁcation result of \lci; rcj [ is the same as the ground
truth. The rate r is calculated as:
r ¼
P
l
i¼1
P
m
j¼1
aij
m
 D
ð14Þ
where D denotes core characters determination accuracy rate deﬁned above, which has
a great impact on community identiﬁcation accuracy rate. The value of r indicates the
degree of similarity between the identiﬁcation result and the ground truth, and the
larger the ratio is, more accurate the identiﬁcation results are.
Based on the above evaluation method, we can quantify the effectiveness of
community identiﬁcation. The Table 4 shows the performance for all the test datas
based on the proposed measurement. The superiority of the proposed community
identiﬁcation method can be seen from the table. The average identiﬁcation accuracy
rate reaches 80.7%. In the experimental results, the identiﬁcation rates of M3, M7 and
M8 were relatively abnormal, and the one of M3 even reaches 100%, mainly because
Table 4. The speciﬁc results of community identiﬁcation
ID
Ground truth
Result of community identiﬁcation
Core charcters
Communities
Core charcters
Communities
Rate r
M1
1, 2
{1, 3, 5, 7, 10, 12, 13, 14}
{2, 4, 6, 8, 9, 11}
1, 2
{1, 3, 5, 7, 8, 10, 13, 14}
{2, 4, 6, 9, 11, 12}
0.833
M2
1, 2, 6
{1, 5, 7, 9, 19, 20}
{2, 3, 4, 12, 15, 17, 18}
{6, 8, 10, 11, 13, 14, 16}
1, 2, 6
{1, 7, 9, 12, 19, 20}
{2, 3, 4, 5, 11, 15, 17, 18}
{6, 8, 9, 13, 14, 16}
0.823
M3
1
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
1
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
1
M4
1, 2
{1, 3, 4, 5, 8, 10, 12, 13, 15}
{2, 6, 7, 9, 11, 14}
1, 2
{1, 3, 4, 7, 8, 10, 13, 15}
{2, 5, 6, 9, 11, 12, 14}
0.769
M5
1, 2
{1, 3, 6, 7, 8}
{2, 4, 5, 9}
1, 2
{1, 6, 7, 8}
{2, 3, 4, 5, 9}
0.857
M6
1, 2, 4
{1, 3, 6, 7, 9}
{2, 5, 8, 12, 13}
{4, 10, 11}
1, 2, 4
{1, 3, 5, 6, 7, 9}
{2, 8, 12, 13}
{4, 10, 11}
0.9
M7
1, 2, 3, 5
{1, 6, 7, 11, 17}
{2, 4, 8, 15}
{3, 9, 12, 13, 16}
{5, 10, 14}
1, 2, 3
{1, 5, 6, 7, 11, 17}
{2, 4, 8, 10, 15}
{3, 9, 12, 13, 14, 16}
0.656
M8
1, 2
{1, 4, 5, 6, 8, 9, 13}
{2, 3, 7, 10, 11, 12}
1, 2, 7
{1, 5, 6, 8, 9, 13}
{2, 3, 4, 10, 11}
{7, 12}
0.621
SRN: The Movie Character Relationship Analysis via Social Network
299

there are only one core chacrater determined and one community identiﬁed in M3. And
the identiﬁcation rates of M7 and M8 are signiﬁcantly lower than the average, it is
because that not all the core characters in M7 and M8 are determined correctly,
resulting in a lower value of D, which directly causes a poor performance of com-
munity identiﬁcation. Therefore the limitation of community identiﬁcation method
proposed in this paper can be apparently seen from the results that when core characters
determination is ineffective, it fails to achieve the desired results. Next, we will con-
tinue to improve the algorithm, and further reﬁne the community classiﬁcation to deal
with the above two cases.
6
Conclusion
In this paper, we propose a SRN-based video character relationship analysis method,
which introduces social network technology to video analysis. In this regards, a movie
is seen as a small society, and is meticulously transformed to a character social network
called SRN through analyzing the character relationship. Based on SRN, a community
identiﬁcation method based on core characters is presented, which can accurately
determine the core characters and execute community identiﬁcation. In the experiment,
we used more than 16 h of movie videos as test datas to evaluate the performance of
proposed model and method. The experimental results show that the community
identiﬁcation accuracy rate is 80.7%. The results are satisfactory, and meet the
expected requirements. The next step, we will carry out the following two aspects of
the work focused on the algorithm and model: (1) improve the community identiﬁ-
cation algorithm to solve the two cases in the experiment through reﬁnement of the
community classiﬁcation; (2) explore the possibility of fusion of the audio-visual
features and SRN, and apply it to other aspects of work.
Acknowledgment. This work is supported by National Science Foundation of China
(No. 61571453) and Natural Science Foundation of Hunan, China (No. 14JJ3010). The authors
are grateful for the anonymous reviewers who made constructive comments.
References
1. Vinciarelli, A., Fernàndez, F., Favre, S.: Semantic segmentation of radio programs using
social network analysis and duration distribution modeling. pp. 779 – 782. IEEE (2007)
2. Park, S.B., Oh, K.J., Jo, G.S.: Social network analysis in a movie using character-net.
Kluwer Academic Publishers (2012)
3. Weng, C.Y., Chu, W.T., Wu, J.L.: RoleNet: movie analysis from the perspective of social
networks. IEEE Trans. Multimed. 11(2), 256–271 (2009)
4. Yeh, M.C., Tseng, M.C., Wu, W.P.: Automatic social network construction from movies
using ﬁlm-editing cues, vol. 131, no. 5, pp. 242–247 (2012)
5. Tran, Q.D., Jung, J.E.: CoCharNet: extracting social networks using character co-occurrence
in movies. J. Univ. Comput. 21(6), 796–815 (2015)
6. Tran, Q.D., Hwang, D., Lee, O.J., et al.: Exploiting character networks for movie
summarization. Multimed. Tools Appl. 1–13 (2016)
300
J. He et al.

7. Tsai, C.M., Kang, L.W., Lin, C.W., et al.: Scene-based movie summarization via role -
community networks. IEEE Trans. Circuits Syst. Video Technol. 23(11), 1927–1940 (2013)
8. Liu, X., Kan, M., Wu, W., et al.: VIPLFaceNet: an open source deep face recognition SDK.
Front. Comput. Sci. 11(2), 208–218 (2017)
9. Rasheed, Z., Shah, M.: Detection and representation of scenes in videos. IEEE Trans.
Multimed. 7(6), 1097–1105 (2005)
10. Li, Y., Yang, X., Luo, J.: Semantic video entity linking based on visual content and
metadata. In: IEEE International Conference on Computer Vision. IEEE Computer Society,
Article in a Conference Proceedings
11. Tran, Q.D., Hwang, D., Lee, O.J., Jung, J.J.: A novel method for extracting dynamic
character network from movie. In: Jung, J., Kim, P. (eds.) BDTA 2016. LNICST, vol. 194,
pp. 48–53. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-58967-1_6
12. Sang, J., Xu, C.: Character-based movie summarization. In: ACM International Conference
on Multimedia. pp. 855–858. ACM (2010)
13. Li, J.Y., Kang, L.W., Tsai, C.M., et al.: Learning-based movie summarization via
role-community analysis and feature fusion. In: IEEE, International Workshop on
Multimedia Signal Processing, pp. 1–6. IEEE (2015)
14. Xuan, H.P., Jung, J.J., Le, A.V., et al.: Exploiting social contexts for movie recommen-
dation. Malays. J. Comput. Sci. 27(1), 68–79 (2014)
SRN: The Movie Character Relationship Analysis via Social Network
301

The Long Tail of Web Video
Luca Rossetto(B) and Heiko Schuldt
Databases and Information Systems Research Group,
Department of Mathematics and Computer Science,
University of Basel, Basel, Switzerland
{luca.rossetto,heiko.schuldt}@unibas.ch
Abstract. Web Video continues to gain importance not only in many
areas of computer science but in society in general. With the growth in
numbers, both of videos, viewers, and views, there arise several technical
challenges. In order to address them eﬀectively, the properties of Web
Video in general need to be known. There is however comparatively little
analysis of these properties. In this paper, we present insights gained from
the analysis of a data set containing the meta data of over 100 million
videos from YouTube. We were able to conﬁrm common wisdom about
the relationship between video duration and user engagement and show
the extreme long tail of the distribution of video views overall. Such data
can be beneﬁcial in making informed decisions regarding strategies for
large scale video storage, delivery, processing and retrieval.
1
Introduction
Web video continues to grow, not only in quantity but also in importance. With
the continued improvements in video recording technology, recording devices
not only become cheaper and therefore more common, the videos created by
these devices also increase in frame rate and resolution, further amplifying the
data growth. This growth produces challenges across many ﬁelds, not only for
video storage but also for processing, analysis, delivery, and retrieval. While a
lot of research is conducted to overcome these challenges, little is known about
the root cause, the videos as a whole. In this paper, we present an analysis
based on a large set on web video meta data from YouTube1. We show that
many properties of such web video as found in the wild show a long-tail dis-
tribution which has relevant consequences for many applications involving such
videos.
The remainder of this paper is structured as follows: Sect. 2 presents other
analyses of YouTube video done in the past and Sect. 3 outlines the methods
employed in our analysis. Section 4 presents the results which are discussed in
greater detail in Sect. 5. Section 6 concludes.
1 https://youtube.com.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 302–314, 2018.
https://doi.org/10.1007/978-3-319-73600-6_26

The Long Tail of Web Video
303
2
Related Work
There are a few datasets which are built from video material on YouTube, and
these are mostly built with a very speciﬁc purpose in mind. The newest and
largest one as of September 2017 is the YouTube-8M [1] dataset published by
Google Research. It was compiled with the intention to further the research in
the ﬁeld of video understanding and it contains various features extracted from
labeled videos. It does not, however, include the videos themselves nor their
meta data which renders the dataset unsuitable for our analysis.
There have been eﬀorts in the past to analyze overall properties of the videos
on YouTube [3–5,9]. For all of those, a custom crawler was used to gather video
meta data from the site itself which was subsequently analyzed. Most of these
eﬀorts used the data from one or several million videos and are already a few
years old. The most recent of these analysis is based on meta data from roughly
100 million videos [9].
Additionally, the network implications of streaming video from YouTube have
been analyzed several times over the years [2,6–8,10]. Some of those works also
considered the video meta data in a similar way to the ones mentioned above.
Since these measurements are however made from a point close to the edge of the
Internet, the data produced does not have a strong claim for being representative
of the distribution found on the entire platform since it is biased by the video
watching habits of the users of the particular network in question.
3
Methods
The data used in this paper was ﬁrst published in [9] and consists of the meta
data collected from 20 million videos from vimeo2 as well as from 100 million
videos from YouTube. This data was collected in 2016 by a purpose-built dis-
tributed crawling setup and made available3 together with the paper [9] as Post-
greSQL4 database export. For our analysis, we limit ourselves to the YouTube
part of the data, not only because it is the larger part but also because it con-
tains richer meta information including view- and like-count which the vimeo
part does not. Table 1 shows the schema of the used data. The ﬁelds we focus on
in this analysis are the numerical values duration, views, likes, dislikes as well as
the age5 of the video. In what follows, we show analysis results and aggregations
on the basis of this data. Some of the relations between these properties were
already analyzed in [9] which we will not repeat here.
2 https://vimeo.com.
3 http://download-dbis.dmi.unibas.ch/WWIN/.
4 https://www.postgresql.org/.
5 With ‘age’, we denote the diﬀerence in days between the date the video was uploaded
and the date the metadata of the video was harvested.

304
L. Rossetto and H. Schuldt
Table 1. The schema of the dataset used for the analysis
Property name
Description
id
11 character string which uniquely identiﬁes a video on
YouTube
Name
Name of the video as shown on the page
Description
Description of the video as shown on the page
License
Binary value: true for a creative commons licensed video
Duration
The video duration in seconds as an integer
Upload date
The day on which the video was uploaded to YouTube
Views
Number of views for the video at the time of crawling
Likes
Number of likes for the video or -1 if disabled
Dislikes
Number of dislikes for the video or -1 if disabled
Subtitles
Number of subtitles available for the video
Unlisted
Binary value: true if the site reports the video as not
publicly listed
Family friendly false if the video contains content deemed oﬀensive
Crawl date
The day on which the crawler generated this particular
entry
Channel
24 character string uniquely identifying the channel of the
video
Genre
The selected genre (out of the 18 available)
4
Results
In this section, we present the performed analysis as well as the results obtained
by them. First, we will analyze how certain metrics behave with respect to the
age of a video. Next, we will look at the relationship between a video’s popularity
and its duration and ﬁnally, we will see how long-tailed the distributions in video
popularity actually are.
4.1
Changes over Time
We start by observing the relationship between the age of a video and the number
of times it has been viewed within this time period. If one assumed a similar
popularity for all videos and therefore a uniform random distribution of views
across all videos, one would expect a roughly linear relationship between the
number of days for which a video was available for viewing and the number of
views it collected. As can clearly be seen in Fig. 1, this is not the case.
The ﬁgure shows a density heat-map with respect to video age and the
number of views accumulated during that time. The color is proportional to
the logarithm of the density, changing from blue for small values to red for
large values. With the exception of some artifacts on the left side of the ﬁgure,

The Long Tail of Web Video
305
Fig. 1. Distribution of views per video with respect to video age
the distribution of views per video appears to change very little over time. For
the most recent 4 years, it appears to be mostly independent of the actual age of
the video. The changes which can be seen on the right side of the ﬁgure are less
due to the video age as such and more due to the comparatively fewer videos in
existence (or at least present within the used dataset) which are of such high age.
This interpretation is further supported by Fig. 2 which shows the daily mean
(blue) and median (green) aggregation of views per video, again with respect
to the videos’ age. It can be seen that a vast majority of views must be pro-
duced relatively shortly after a video is uploaded. After this initial period, the
aggregated view count stays relatively stable with only a slight increase over
time. The median aggregation rendered in green shows a more prominent trend
towards view accumulation over time.
Fig. 2. Daily mean and median aggregation of views per video with respect to video
age for the entire time covered by the dataset
The dip in both traces in Fig. 2 which can be seen on the left of the plot
can be explained by the higher number of relatively recent videos in the dataset
which also caused the artifact in Fig. 1.
Since videos appear to already start out with most of the views they will
receive, we can focus our attention to Fig. 3 which shows the same data as Fig. 2
but zoomed in on the ﬁrst 100 days after a video’s initial upload. It can be seen
that the vast majority of views is accumulated in the ﬁrst few days after its

306
L. Rossetto and H. Schuldt
upload. The data is rather sparse for the ﬁrst few days, which is probably due
to the way the crawler used to collect it found videos to visit. It is therefore
not possible to make meaningful statements about the ﬁrst few days of a video’s
presence on the platform.
Fig. 3. Daily mean and median aggregation of views per video with respect to video
age, limited to videos at most 100 days old
The data used there is not perfectly suited for the analysis presented up until
now as it only contains the view count for a video at exactly one point in time.
In order to have a more reliable analysis, a dataset would ideally contain several
view counts of a video from diﬀerent points in time. In the absence of such a
dataset, we will content ourselves with this analysis.
When looking at the accumulation of likes over the time, we see a similar
independence of video age as with the views. Figure 4 shows the distribution
of video likes with respect to the age of the video. As with the views depicted
above, the distribution of likes appears to be mostly independent of the age of
the video, at least when only considering videos with an age suﬃciently large to
put them out of the initial accumulation period.
Fig. 4. Distribution of likes per video with respect to video age
This initial accumulation period is indeed the same as for the views, which
can be seen in Fig. 5 which shows the mean views, likes and dislikes of a video

The Long Tail of Web Video
307
with respect to its age. Note that the blue points are the same as the ones in
Fig. 2. The cyan and purple points show the aggregated likes and dislikes per
video, respectively.
Fig. 5. Aggregated mean of views, likes and dislikes per video with respect to video
age
We can see that the shapes of the three traces are very similar and none of
them shows a substantial dependency on the video age. There also appears to
be a close to constant factor between the three measures. Independently of the
age of the video, there is approximately one like for every 331 views6 and one
dislike for every 17 likes7.
4.2
Duration
In this section, we will look into the inﬂuence of a video’s duration over how
often it is watched or liked. Figure 6 shows the distribution of video likes with
respect to video duration.
Fig. 6. Distribution of likes per video with respect to video duration
6 Average(views/likes): 332.9592, median(views/likes): 331.0722.
7 Average(likes/dislikes): 17.2896, median(likes/dislikes): 16.8257.

308
L. Rossetto and H. Schuldt
It can be seen that, in accordance with commonly repeated recommendations
often found in the context of YouTube video creation, the most-liked videos are
somewhere between 3 and 4 min in length. Note that this distribution is not
normalized with respect to views per video or numbers of videos per duration.
Figure 7 shows the aggregated average of views, likes, and dislikes with respect
to the video duration, similar to the way shown in Fig. 5 with respect to video
age. In contrast to Fig. 5 which shows the entire available data range, the data
shown in Fig. 7 was limited to videos with a total duration of at most 150 min,
after which the data becomes too sparse for a meaningful aggregation.
Fig. 7. Aggregated mean of views, likes and dislikes per video with respect to video
duration
Like before, we can see three traces with similar shapes and a seemingly
constant factor between them. If we again estimate this factor, we get roughly
200 views per like8 and 15 likes per dislike9. The fact that these values not
only diﬀer from the ones above but also from the ratios of the entire dataset
which has on average 162.25 views per like and 18.54 likes per dislike indicates
that in both cases, the data was not entirely uncorrelated with respect to the
aggregation. Figure 7 also conﬁrms that videos with a duration of between 3
and 4 min appear to receive more views than both longer and shorter videos.
The diﬀerence in expected likes and dislikes is even more substantial for shorter
videos.
4.3
Genres
Tables 2 and 3 show the breakdown of views, likes, and dislikes grouped by genre
and aggregated by average and median, respectively. It can be seen that there
are substantial diﬀerences between genres in all three measures. The most ‘liked’
videos come from the Gaming genre, according to both average and median
aggregations. Most ‘disliked’ are Shows and Movies which form the top two in
number of dislikes per video in both aggregation, albeit not in a consistent order.
8 Average(views/likes): 201.6157, median(views/likes): 195.7886.
9 Average(likes/dislikes): 15.4702, median(likes/dislikes): 14.6817.

The Long Tail of Web Video
309
Table 2. Average views, likes and dislikes per video with respect to Genre
Genre
Videos
Views
Likes
Dislikes
V iews
Likes
V iews
Dislikes
Likes
Dislikes
People & blogs 21’507’972 33’496.2
211.49
14.13
158.38 2’370.46 14.97
Entertainment 13’789’270 84’829.4
5’11.52
29.86
165.84 2’840.63 17.13
Music
11’256’684 184’613.8 738.22
34.26
250.08 5’388.29 21.55
News &
politics
107’30’269 15’341.5
57.57
7.22
266.47 2’123.66
7.97
Education
6’619’603
33’029.5
114.38
10.85
288.77 3’044.13 10.54
Gaming
6’598’415
78’590.74 1’351.64 46.99
58.14 1672.46
28.76
Sports
6’040’181
35’518.5
141.19
8.43
251.57 4’212.42 16.74
Howto & style 4’285’544
54’007.3
542.14
24.42
99.62 2’211.75 22.20
Film &
animation
4’202’044
105’460.2 345.33
29.11
305.39 3’622.93 11.86
Autos &
vehicles
3’926’800
36’142.3
104.10
9.18
347.18 3’935.76 11.33
Travel &
events
3’083’467
18’039.8
72.23
4.66
249.76 3’874.88 15.51
Science &
technology
2’900’186
36’040.6
220.03
15.69
163.80 2’296.72 14.02
Comedy
2’630’365
135’632.6 1’184.19 59.40
114.54 2’283.21 19.93
Nonproﬁts &
activism
2’536’371
13’154.1
74.28
5.79
177.10 2’273.59 12.84
Pets &
animals
1’235’834
59’092.5
169.90
13.46
347.80 4’390.05 12.62
Shows
323’115
288’280.4 1’181.55 99.45
243.99 2’898.63 11.88
Movies
18’681
194’249.4 307.42
69.01
631.86 2’814.99
4.46
Trailers
7’407
196’615.2 226.94
24.60
866.39 7’993.99
9.23
Interestingly, those genres, together with the Trailers form the top three in terms
of views per video in both aggregations, followed by Music on place four.
While not towards the top in terms of views or likes per video, members of
the Howto & Style genre show a high view to like ratio in both aggregations,
indicating that these videos while not excessively likely to be viewed are more
likely to be ‘liked’ by their viewers. This discrepancy could also be due to the fact
that any user with an account on YouTube can watch a video multiple times and
therefore generate an arbitrary amount of views but only ‘like’ or ‘dislike’ a video
once. A low view to like ratio could therefore either mean that videos are more
likely to be liked when watched or that they are less likely to be watched several
times by the same user. The data at hand does not enable us do diﬀerentiate
between these cases.

310
L. Rossetto and H. Schuldt
Table 3. Median views, likes and dislikes per video with respect to Genre
Genre
Videos
Views
Likes Dislikes
V iews
Likes
V iews
Dislikes
Likes
Dislikes
People & blogs 21’507’972 1’962
10
1
196.20
1’962.00
10.00
Entertainment
13’789’270 4’894
18
1
271.89
4’894.00
18.00
Music
11’256’684 10’309 40
1
257.73
10’309.00 40.00
News &
politics
10’730’269 1’206
4
0
301.50
-
-
Education
6’619’603
1’888
9
0
209.78
-
-
Gaming
6’598’415
7’203
74
4
97.34
1’800.75
18.50
Sports
6’040’181
3’013
10
0
301.30
-
-
Howto & style
4’285’544
5’615
37
2
151.76
2’807.50
18.50
Film &
animation
4’202’044
5’620
19
1
295.79
5’620.00
19.00
Autos &
vehicles
3’926’800
4’156
11
1
377.82
4’156.00
11.00
Travel &
events
3’083’467
1’766
6
0
294.33
-
-
Science &
technology
2’900’186
3’448
11
1
313.45
3’448.00
11.00
Comedy
2’630’365
7’961
30
2
265.37
3’980.50
15.00
Nonproﬁts &
activism
2’536’371
976
1
0
976.00
-
-
Pets &
animals
1’235’834
3’621
11
1
329.18
3’621.00
11.00
Shows
323’115
19’416 52
5
373.38
3’883.20
10.40
Movies
18’681
22’458 39
9
575.85
2’495.33
4.33
Trailers
7’407
25’888
1
0
25’888.00 -
-
Table 4 shows the average and median video duration by genre. The longest
videos are unsurprisingly labeled as Movies while Trailers are the shortest. Com-
pared to these two extremes, the diﬀerence in duration between the other genres
is relatively minor. There is however a diﬀerence between the two aggregations.
The last column in Table 4 shows the time diﬀerence between the average and
the median duration. For all genres except Movies, the average video is longer
than the median one, often by a signiﬁcant fraction.
Such discrepancies between the two aggregations can also be observed in
Tables 2 and 3 where the values generated by the average are usually substan-
tially larger than the median values. This indicated a skewed distribution with
a small number of large values and a large number of small values, commonly
referred to as a long-tail distribution.

The Long Tail of Web Video
311
Table 4. Average and median video duration by Genre
Genre
Average Median
Diﬀerence
Trailers
00:02:34 00:02:08 00:00:26
Autos & vehicles
00:08:12 00:04:35 00:03:37
Pets & animals
00:08:50 00:03:32 00:05:18
Comedy
00:09:31 00:04:35 00:04:56
Howto & style
00:10:24 00:06:50 00:03:34
Music
00:10:29 00:04:34 00:05:55
Travel & events
00:12:04 00:05:54 00:06:10
Sports
00:12:30 00:05:12 00:07:18
News & politics
00:14:04 00:04:31 00:09:33
Entertainment
00:14:12 00:05:29 00:08:43
Science & technology
00:14:44 00:06:14 00:08:30
People & blogs
00:15:24 00:06:24 00:09:00
Film & animation
00:17:28 00:05:52 00:11:36
Shows
00:18:22 00:11:08 00:07:14
Gaming
00:23:24 00:13:47 00:09:37
Education
00:24:06 00:10:00 00:14:06
Nonproﬁts & activism 00:25:43 00:10:00 00:15:43
Movies
01:54:06 02:03:35 −00:09:29
4.4
Long Tails
In this last part of our analysis, we look at how views, likes, and dislikes are
distributed across all videos. We have already seen above that it would be incor-
rect to assume a completely uniform distribution of views, etc. since this would
lead to a strong correlation between video age and view count which we did not
observe.
Figure 8 shows what fraction of videos, sorted by most viewed in descending
order, is needed to account for that part of all views within the dataset. It is
rather surprising to see that already the most viewed 0.07‰ videos account for
10% of all views. Increasing this fraction to 1‰ encompasses 26.68% of the view
total, meaning that the remaining 99.9% of videos only produced 73.32% of all
views. With 0.73%, we can even account for half of all views and rounding up to
1% of videos gives us an additional 4.4% of views, meaning that the bottom 99%
of videos are accountable for 45.56% and therefore not even half the views.
Increasing this ratio further to 10% of videos gives us 87.64% of views and
in order to get nine in ten views, we would need the top 12.29% of videos.
The picture for likes and dislikes is rather similar, as depicted in Figs. 9 and 10
respectively.

312
L. Rossetto and H. Schuldt
Fig. 8. Cumulative fraction of views per fraction of videos
Fig. 9. Cumulative fraction of likes per fraction of videos
Fig. 10. Cumulative fraction of dislikes per fraction of videos
5
Discussion
Multiple conclusions can be drawn from the analysis presented above. The data
appears to conﬁrm that new content is more relevant than old content, which, by
itself, is not very surprising. Having this intuition empirically conﬁrmed however
enables informed decisions about data storage and caching policies. It can be
argued that it is not only more important to treat recently accessed data as hot
data but also to prime the caches with newly added content in anticipation of its
consumption. Since the used dataset holds no information about global access
patterns for individual videos over time, let alone regional ones, we cannot draw
any conclusions about useful distribution strategies from this data.

The Long Tail of Web Video
313
We were able to conﬁrm a long standing common recommendation about
video duration since, for this dataset, videos with a duration of about three min-
utes show on average more views and ‘engagement’ as measured by the number
of likes and dislikes. This trend, while not very substantial, can again be used
to inform decisions about video caching and processing. Since video duration
is strongly correlated with its ﬁle size, choosing a caching policy which favors
videos in this popular duration range could lead to more eﬃcient use of storage.
Similarly, since video processing eﬀort is commonly dependent on video duration,
employing a similar strategy could also prove beneﬁcial.
A further interesting insight gained from this data is the extremely long-
tailed distribution of views, likes, and dislikes per video. If YouTube were for
some reason unable to deliver 9 out of every 10 videos, they could still retain
over 87% of views. Since shorter videos produce commonly more views than
longer ones and longer videos require more storage space, deleting these 90%
would presumably free up substantially more than 90% of the current storage
requirements. Since we have no way to predict a video’s popularity in advance,
we would however advise against this data deletion strategy.
6
Conclusion
In this paper, we have presented insights gained from the analysis of a large set
of web video meta data. We have shown that, at least in the case of YouTube,
video popularity as expressed by views and likes (as well as dislikes) exhibits
a substantially long-tailed distribution in which a small percentage of the most
popular videos accounts for a vastly over-proportional fraction of views. The
data also suggests that video age might be a usable indicator for interest in a
particular video as most views appear to be generated shortly after the video
publication. Further analysis of data which contains multiple samples per video
from diﬀerent points in time would however be required to make more conclusive
statements regarding this topic. This analysis aims at helping to make informed
decisions when working with large scale multimedia in general and video in
particular, not only in the areas of video storage and distribution but also for
video processing, analysis, and retrieval.
Acknowledgements. This work was partly supported by the Chist-Era project IMO-
TION with contributions from the Swiss National Science Foundation (SNSF, contract
no. 20CH21 151571).
References
1. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,
Vijayanarasimhan, S.: YouTube-8M: a large-scale video classiﬁcation benchmark.
CoRR, abs/1609.08675 (2016)
2. Ameigeiras, P., Ramos-Munoz, J.J., Navarro-Ortiz, J., Lopez-Soler, J.M.: Analysis
and modelling of YouTube traﬃc. Trans. Emerg. Telecommun. Technol. 23(4),
360–377 (2012)

314
L. Rossetto and H. Schuldt
3. Che, X., Ip, B., Lin, L.: A survey of current YouTube video characteristics. IEEE
Multimedia 22(2), 56–63 (2015)
4. Cheng, X., Dale, C., Liu, J.: Statistics and social network of YouTube videos. In:
16th International Workshop on Quality of Service. IWQoS 2008, pp. 229–238.
IEEE (2008)
5. Cheng, X., Liu, J., Dale, C.: Understanding the characteristics of internet short
video sharing: a YouTube-based measurement study. IEEE Trans. Multimedia
15(5), 1184–1194 (2013)
6. Gill, P., Arlitt, M., Li, Z., Mahanti, A.: YouTube traﬃc characterization: a view
from the edge. In: Proceedings of the 7th ACM SIGCOMM Conference on Internet
Measurement, pp. 15–28. ACM (2007)
7. Horv´ath, G., Fazekas, P.: Characterization and modelling of YouTube traﬃc in
mobile networks. In: ICN 2015, p. 127 (2015)
8. Ramos-Mu˜noz, J.J., Prados-Garzon, J., Ameigeiras, P., Navarro-Ortiz, J., L´opez-
Soler, J.M.: Characteristics of mobile YouTube traﬃc. IEEE Wirel. Commun.
21(1), 18–25 (2014)
9. Rossetto, L., Schuldt, H.: Web video in numbers - an analysis of web-video meta-
data. CoRR, abs/1707.01340 (2017)
10. Zink, M., Suh, K., Gu, Y., Kurose, J.: Characteristics of YouTube network traﬃc
at a campus network - measurements, models, and implications. Comput. Netw.
53(4), 501–514 (2009)

Vehicle Semantics Extraction and Retrieval
for Long-Term Carpark Video Surveillance
Clarence Weihan Cheong1(B), Ryan Woei-Sheng Lim1, John See1,
Lai-Kuan Wong1, Ian K. T. Tan1, and Azrin Aris2
1 Center for Visual Computing, Multimedia University,
Persiaran Multimedia, 63100 Cyberjaya, Malaysia
clarence han@hotmail.com, ryanlim0616@gmail.com,
{johnsee,lkwong,ian}@mmu.edu.my
2 VADS Lyfe, Telekom Malaysia Berhad, 60000 Kuala Lumpur, Malaysia
azrin.aris@tm.com.my
Abstract. Car park video surveillance data provides plenty of semantic
rich data such as vehicle color, trajectory, speed, and type which can be
tapped into and extracted for video and data analytics. We present meth-
ods for extracting and retrieving color and motion semantics from long
term carpark video surveillance. This is a challenging task in outdoor sce-
narios due to ever-changing illumination and weather conditions, while
retrieval time also increases as data size grows. To address these chal-
lenges, we subdivided the search space into smaller chunks by introduc-
ing spatio-temporal cubes or atoms, which can store and retrieve these
semantics at ease. The proposed method was tested on 2 days of contin-
uous data from an outdoor carpark under various lighting and weather
conditions. We report the precision, recall and F1 scores to determine
the overall performance of the system.
Keywords: Vehicle semantic extraction · Retrieval systems
Carpark surveillance
1
Introduction
The use of video-based traﬃc surveillance is becoming increasingly popular due
to the low implementation cost. However, majority of these data are left unpro-
cessed and kept in storage devices. Rich semantic data such as vehicle color,
trajectory and type can be exploited for video and data analytics to provide
deeper insights for surveillance and retrieval purposes.
Traditionally, to perform retrieval on surveillance videos, users need to pro-
vide description of the vehicle such as the time, place of the incident, vehicle
registration plate, and color of the vehicle. Next, users would ﬁlter through all
the retrieved results to manually identify the target event. This entire process is
undoubtedly time consuming and labor intensive.
To overcome the ineﬃciency of such laborious methods, we propose a long-
term surveillance analytics system that extracts and stores the semantics data
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 315–326, 2018.
https://doi.org/10.1007/978-3-319-73600-6_27

316
C. W. Cheong et al.
into a database and allow video clips of speciﬁc events to be retrieved using
user-described queries.
First, our method performs background subtraction to extract foreground
blobs that represents vehicles. Next, a ﬁltering process is applied to remove any
unwanted blobs such as pedestrians. They are then tracked frame by frame to
generate their individual trajectories and to extract vehicle-speciﬁc semantics.
Lastly, these semantics are segmented into spatio-temporal cubes (atoms) and
stored in the database. We evaluated the reliability and performance of the
proposed method over a span of 20 h.
2
Related Works
While Intelligent Transportation System (ITS) is a popular research topic, there
has been little research done for carpark scenes. When viewed regardless of the
intended scene, vehicle semantics extraction and trajectory retrieval for surveil-
lance video is a wide research ﬁeld.
For vehicle color semantics, there are many diﬀerent school of thoughts that
arise from it. Authors in [2,5] approach this challenge by obtaining the histogram
in HSV/HSL color space while other works [4,6,9] addressed it by designing deep
learning methods. In an interesting work [8], color spaces were also used to detect
moving shadows from urban surveillance video.
In the area of vehicle motion extraction and trajectory grouping, the authors
in [2,11] quantized the moving direction of the objects into 4 and 9 directional
bins respectively. In [3], a novel method of indexing trajectories into spatio-
temporal cubes is introduced. A recent work by Casta˜n´on et al. [2] used other
vehicle semantics such as size and persistence to query for anomalous and typical
events.
3
Framework
The framework of our proposed method adheres to the typical top-down app-
roach for automated video surveillance in carparks [7] which includes background
subtraction, blob ﬁltering, vehicle detection vehicle tracking. The semantic infor-
mation from these vehicle blobs are extracted, segmented into atom-based cubes
and stored in the database. This information can then be queried through a
search interface. The overview of our framework is shown in Fig. 1.
3.1
Background Subtraction, Vehicle Detection and Tracking
We describe a number of preparatory steps that were taken prior to the extrac-
tion of semantics. Firstly, background subtraction with a combination of adap-
tive learning and frame diﬀerencing [7] is performed to extract foreground blobs
from each video frame. This strategy is computationally cheaper than optical
ﬂow, hence it improves on the overall eﬃciency of segmenting moving objects.

Vehicle Semantics Extraction and Retrieval for Long-Term Surveillance
317
Fig. 1. Framework diagram
Since the focus is on carpark surveillance where large portions of the video
footages may not contain any substantial movement, a frame skipping method
was deployed to speed up the overall processing. The blobs then go through a
series of morphological operations such as dilation and erosion to ﬁlter out noise
and to ﬁll up gaps in the blobs to generate the ﬁnal foreground blobs.
Next, the blobs are ﬁltered according to their sizes, positions and aspect
ratios, where each of these parameters were determined empirically to suit the
scene geometry. After that, the YOLO real-time object detector [10] is applied
to diﬀerentiate between vehicles or non-vehicle blobs. This two-step approach is
designed to ﬁlter out objects other than vehicles that are not of interest such as
pedestrians and motorcycles. Finally, each blob is matched back to the trajec-
tories using a tracking state machine proposed in [7].
3.2
Object Speciﬁc Semantic Extraction
As object speciﬁc semantics from the scene provides deeper insights for surveil-
lance purposes, this work currently focuses on two types of object speciﬁc seman-
tics, namely the color and motion information.
I. Color Information plays a signiﬁcant role in the retrieval process as it is
often one of the most common information given when a user tries to describe an
object from an event in a scene. Extracting color information accurately is partic-
ularly challenging for outdoor scenes as the color information varies throughout
the day due to ambient illumination and weather changes. Algorithm 1 summa-
rizes our strategy for extracting color information.
When a vehicle is detected in the scene, a bounding box of the foreground
blob is usually used to mark the location of the vehicle. However, due to the
background subtraction method used, the ﬁnal foreground blob appears slightly
larger than the actual footprint of the vehicle. In order to obtain a closer esti-
mation of the vehicle’s dominant color, the bounding box is cropped by 30% to
reduce some background information such as the road or vehicles around it.

318
C. W. Cheong et al.
Subsequently, our strategy determines the dominant color of each vehicle blob
by ﬁrst undertaking a task to determine if the vehicle’s dominant color belongs
to the achromatic scale (black, gray and white color) or chromatic scale (other
hues). We determine the absolute diﬀerence between the cropped image and its
grayscale version, and then threshold each channel in RGB at an empirically-
found intensity value of 35. The hint of signiﬁcant values from this step indi-
cates a substantial presence of chromatic hue. Then, we convert the thresholded
image to a grayscale image, and determine the ratio of non-zero pixel values over
total pixels. This process allows us to deduce the presence of strong chromatic
hues and estimate if the vehicle belongs to the achromatic or chromatic subsets.
A threshold pivot, Tpivot is empirically set at the 0.18 where if the ratio of non-
zero pixel values is more than Tpivot, we can assume that the particular vehicle
blob contains a strong chromatic hue, as illustrated in Fig. 2.
(a) Achromatic vehicle (Gray/White)
(b) Chromatic vehicle (Red)
Fig. 2. (From left) original image; grayscale image; absolute diﬀerence; binary thresh-
old absolute diﬀerence; threshold diﬀerence in grayscale (Color ﬁgure online)
Achromatic and chromatic color processing. Upon determining if the vehicle
belongs to the achromatic scale, we then subjected the cropped image to both
the black and white ﬁlters individually by applying binary thresholds set at
empirically determined intensity levels of 50 and 170 respectively. Next, in similar
fashion, the ratio of non-zero pixels upon ﬁltering is used to determine if the
vehicle is assigned to black, white or gray color term. Figure 3a shows how a
white vehicle responds to a black and white ﬁlter.
As for the chromatic colors, we chose to utilize the HSV color space as it
is visually more intuitive than the RGB color space. Here, we generated a 3-
dimensional HSV histogram with 15 Hue bins, 8 Saturation bins and 8 Value
bins. Based on the generated histogram, the maximum value of each bin from
all 3 channels is assumed to correspond to the dominant color of the vehicle.
However, since the vehicle is moving in the outdoor scene, the ambient and
directional lighting (from sun and other light sources) contribute to slight vari-
ation of colors. To suit our problem, the dominant color for each frame of a
tracked vehicle is averaged out throughout its trajectory.

Vehicle Semantics Extraction and Retrieval for Long-Term Surveillance
319
(a)
(b)
(c)
Fig. 3. (a) Black & white ﬁlter responses, (b) directional bins, (c) 11 color categories
[1] (Color ﬁgure online)
Algorithm 1. Color Term Extraction
1: for Each blob in object do
2:
Shrink bounding box (crop image)
3:
Create a copy of the cropped image in grayscale
4:
Calculate absolute diﬀerence between cropped image & grayscale image
5:
Perform threshold on absolute diﬀerence to amplify diﬀerence
6:
Convert results into grayscale & calculate no. of non-zero pixels
7:
if Ratio of non-zero pixels > Tpivot then
8:
Calculate 3D HSV histogram
//Chromatic Vehicle
9:
Locate maximum bin location of each channel
10:
Map the highest bin from each channel to Color Term
11:
else
12:
Perform black & white ﬁlter
//Achromatic Vehicle
13:
Obtain ratio of non-zero pixels from both ﬁlters
14:
Determine Color Term
15:
end if
16: end for
17: Obtain average dominant color & return Color Term
We also note that the achromatic algorithm is an essential step because the
8 Values bins are insuﬃcient to accurately represent vehicles with borderline
dominant color as the brightness values may be widely distributed.
Color terms. Next, we addressed the problem of deﬁning color terms by adopting
the eleven common terms in English as described by a study done in 1969 by
Berlin and Kay [1]. The color categories are white, black, red, green, yellow, blue,
brown, purple, pink, orange, and gray. This deﬁnition enables us to quantize the
range of colors to a ﬁxed number of color categories while taking advantage of
the atom-based structure (Refer to Sect. 3.3).
II. Motion Information also plays an important role in the retrieval pro-
cess as users would often describe the trajectory of a vehicle from a particular
incident. Instead of generating a ﬁne representation of the vehicle’s trajectory

320
C. W. Cheong et al.
(conventional motion vectors), we can store a coarser representation of motion
information in the form of directional categories. We achieve this by quantiz-
ing the extracted motion information of the vehicle trajectories into 9 bins – 8
directional bins as well as one bin to denote minuscule and negligible motion,
as shown in Fig. 3b. The motion vectors are extracted from the centroid of the
vehicle, with respect to its previous location one second ago; a minimum dis-
placement of 5 pixels determines the presence of motion.
The advantage of such a method is that we are able to fully utilize the atom-
based structure (Refer to Sect. 3.3) to locate motions of interest in an eﬃcient
manner. With this approach, we do not have to consider the various combinations
of ﬁne-grained motion trajectories which may occur in realistic outdoor scenes
such as carparks.
3.3
Semantic Segmentation and Indexing
I. Semantic Segmentation. In the proposed method, we adopted the concept
of using video cubes or atoms from [2] as a high-level data structure that frames
the data into a spatio-temporal search space. An atom is deﬁned as a group of
cells at a similar spatial location, that spans a certain ﬁxed number of frames;
hence forming a spatio-temporal ‘cube’. Figure 4b illustrates the atom structure.
Contrary to [2] which uses these atoms in a tree-like structure that associates
each atom with its neighbors as the child nodes, we consider each atom discretely
and independently of one another.
(a) 2D Atom Grids
(b) Atom
Fig. 4. Atom structure
Since our video data (see Sect. 4.1) has a resolution of 640 × 480 pixels
and frame rate of 10 fps, we analytically set the dimensions of each atom, α
to αwidth = 32 pixels, αheight = 24 pixels and αt = 10 frames, which represents
the temporal duration of one second. We selected the resolution of the atom
(αwidth, αheight) as such so that the video resolution can be uniformly divide our
video into 20 atoms across both its width and height. This approach allows us to
distinctly identify each atom from a video by an index (αwidth, αheight, αt) (see

Vehicle Semantics Extraction and Retrieval for Long-Term Surveillance
321
Fig. 4). Similarly, we can store speciﬁc occurrences of each semantic type (color,
motion) with the same atom index. Though a vehicle blob may be encapsulated
by several neighboring atoms, only the atom which corresponds to the centroid of
the blob is stored. This is done with the consideration that the precise bounding
box location is not essential for retrieving video shots.
II. Semantic Indexing. In order to index the extracted data, we borrowed the
idea similar to that of Locality-sensitive Hashing (LSH) used in [2] by grouping
similar semantics together for quicker retrieval. Our proposed method espouses
this by creating unique database tables for each of the 20 semantics (11 colors
and 9 motion bins) with the source video, vehicle ID along with the individual
atom indices as columns for each table. This allows us to rapidly locate speciﬁc
atoms with the queried semantic without going through the entire database, as
illustrated in Fig. 5. The atom structure enables us to make queries based on a
speciﬁc region of interest, or time slice.
(a) Region of Interest
(b) Time Slicing
Fig. 5. Types of queries
3.4
Semantic Retrieval
When a color query C is issued, all vehicles with matching colors are returned.
When a trajectory query Q is issued, the possible atoms that are matched will
be retrieved in temporal order. These retrieved atoms need to be merged to
build video shots that will be returned. Assume A = {αn, αn+1, . . . , αN} is a
time-ordered set of retrieved atoms, we intend to piece together relevant atoms
to build video shots Si. This is achieved by performing atom merging if the
following condition is met:
αn+1 ⊂Si
if
tαn+1 −tαn < (5 ∗f)
(1)
where f is the video frame rate.
We also introduce a Conﬁdence Value (CV) which sets the sensitivity level of
accepting a video shot as among the retrieved results. For each shot, we accept
each retrieved shot Si if it fulﬁlls the following condition:
CV < length(Si)
length(Q) × 100%
(2)

322
C. W. Cheong et al.
This provides a margin of error when performing the query which acts as a
trade-oﬀfunction. A lower CV results in returning a larger set of results but at
the expense of an increase in retrieved shots, and vice versa.
Search Interface. The proposed methods were realized in a form of a search
interface, which was designed to allow users to construct a query by tracing
the trajectory and selecting colors which ﬁt their intended vehicle description.
Figure 6 shows the interface, with the green lines showing the user-selected tra-
jectory query. The underlying atom-based structure allows queries to be formed
in a way which emulates the semantics extraction process, eliminating the need
for query parsing.
(a) Motion Test Case 1 (TQ1)
(b) Motion Test Case 2 (TQ2)
Fig. 6. Search interface for the proposed framework (Color ﬁgure online)
4
Experiments
4.1
Dataset
This section describes the video data used in the development and evaluation
of the proposed method. We collected a new video dataset consisting of videos
recorded from a university’s outdoor carpark area over a duration of several
months. A single stationary camera was set up to record the video on weekdays
throughout the week from 8:30AM to 6:30PM. These videos were recorded in
a compressed H.264 MPEG-4 format with a resolution of 640 × 480 pixels and
frame rate of 10 fps. Figure 7a shows a wide range of challenges found in the
recorded video data: severe morning and afternoon shadows, rainy weather, and
reﬂections. Due to the scale of experiment, we selected 2 days of video data
(totaling 20 h) for the work in this paper.
4.2
Experiment Methodology
To validate our method for vehicle color and motion retrieval, the ground truth
states were manually labeled by a few annotators and cross-checked to arrive at
a consensus. This allows us to validate the eﬃcacy of our proposed automated

Vehicle Semantics Extraction and Retrieval for Long-Term Surveillance
323
Fig. 7. (a) Various carpark scene challenges throughout the day (severe shadow,
weather condition and reﬂections), (b) sample screenshots of 3 retrieved shots (left,
center, right columns) for query TQ2 (turning into junction)
method against human observations. Our system was implemented on an Intel
i7 machine with 16 GB RAM, GeForce GTX 1060 GPU. In order to analyze
both color and motion semantics individually they are evaluated separately to
measure their individual performances.
Color Retrieval
To measure the performance of the color retrieval module, we follow through
the pipeline by extracting unique objects from each of the 11 color tables. The
retrieved results are then compared against the ground truth. The ground truth
distribution of vehicle colors is shown in Table 1.
Table 1. Ground truth distribution vehicle colors ordered by occurrence
Color Gray Black White Red Blue Orange Yellow Green Pink Purple Brown
#
365
182
150
60
19
15
13
10
9
7
7
%
43.6
21.7
17.9
7.2
2.3
1.8
1.6
1.2
1.1
0.8
0.8
Motion Retrieval
We speciﬁed 2 speciﬁc motion paths or trajectory queries (TQ) that we intend
to validate: (TQ1) Heading southward (see Fig. 6a) & (TQ2) Turning in a junc-
tion (see Fig. 6b). The distribution of these test cases are 252 (86.3%) and 40
(13.7%) trajectories for TQ1 and TQ2 respectively.
To measure the performance of the motion retrieval method, we performed
evaluation on both TQ1 and TQ2 without consideration for the vehicle color.
These experiments were tested on a few CV values (70%, 80%, 90%) and diﬀerent
number of atom query inputs to test the impact of trajectory details.
Evaluation Metrics: We used 3 evaluation metrics - Precision, Recall as well
as the F1 score to determine the overall performance of the system. Correct

324
C. W. Cheong et al.
matches will be regarded as true positives, tp. False positives, fp is the total
number of retrieved results minus the true positives, while false negatives, fn is
the total number of correct results minus true positives. Precision, Recall and
F1 score is computed as:
Precision =
tp
tp + fp; Recall =
tp
tp + fn; F1-score = 2 · Precision · Recall
Precision + Recall (3)
4.3
Experiment Results and Discussion
The performance of the proposed method is computed by comparing the anno-
tated ground truth against the retrieved results.
Color Retrieval
Table 2 reports the confusion matrix of the color retrieval task. The overall
precision stands at 54%, with a recall of 36% and F1 score of 39%. The cells in
Table 2 marked in green indicate the highest count of correctly predicted colors,
while the cells marked in red indicate the highest count of incorrect predictions
for each color. Our method is able to predict correctly a majority of cases for
seven out of eleven colors.
Based on the obtained results, we learn that the Tpivot needs to be adjusted
as too many chromatic vehicles were classiﬁed as achromatic vehicles which in
turned aﬀected the overall performance of the proposed method. We hypothesize
that better results can be obtained by careful adjustment of Tpivot or attempt
to learn a suitable color model as in [6,9] for this particular scene. We observe
that vehicles with lighter and darker shades were particularly diﬃcult as they
do not contain enough chromatic hues to arrive at a correct prediction.
While the classiﬁcation of achromatic versus chromatic vehicles faced con-
siderable diﬃculties, the black & white ﬁlters provide a considerably good result
when determining the diﬀerent categories of achromatic vehicles which may be
useful for processing in grayscale. Based on our observation, most errors usu-
ally occur when the vehicles are in locations where the intensity of shadows
overpowered the lighter-shade vehicles in terms of coverage area.
Motion Retrieval
The retrieved trajectory shots are validated against the annotated ground
truth labels to obtain our results. As it is diﬃcult to pinpoint the exact “scene”
where the test cases occur, we use a time window of ±T seconds to indicate a
range whereby a retrieved motion can be correctly matched to the ground truth
label. We ﬁxed T = 5 in our experiments, similar to that used in the tracking
evaluation of [7].
Table 3 shows the results of the motion retrieval task with varying Conﬁdence
Values (CV) and varying number of atom inputs in the trajectory query. For
TQ1, the total number of atom inputs varied from 5 to 8 inputs while TQ2 is
represented by a shorter trajectory of 4 to 6 inputs as it concerns a junction
turning query. For TQ1 & TQ2, the overall average precision is around 89% &
50% while the recall is at 27% & 59% respectively. Based on the F1 scores, the

Vehicle Semantics Extraction and Retrieval for Long-Term Surveillance
325
Table 2. Confusion matrix for color retrieval task
Predicted Color
Gray Black White Red
Blue Orange Yellow Green Pink Purple Brown
Gray
236
61
68
0
0
0
0
0
0
0
0
Black
48
134
0
0
0
0
0
0
0
0
0
White
26
4
120
0
0
0
0
0
0
0
0
Red
27
25
0
2
0
0
0
0
4
2
0
Blue
3
10
0
0
6
0
0
0
0
0
0
Orange
8
3
0
0
0
3
0
0
0
1
0
Yellow
3
1
2
0
0
0
7
0
0
0
0
Green
5
1
4
0
0
0
0
0
0
0
0
Pink
1
0
0
3
0
0
0
0
5
0
0
Purple
3
3
0
0
0
0
0
0
0
1
0
Actual Color
Brown
3
4
0
0
0
0
0
0
0
0
0
Precision 65.01 54.47 61.86 40.00 100.00 100.00 100.00 N/A 55.56 25.00
N/A
Recall
64.66 73.63 80.00
3.33
31.58
20.00
53.85
0.00 55.56 14.29
0.00
Result
F1 Score 64.84 62.62 69.77
6.15
48.00
33.33
70.00
N/A 55.56 18.18
N/A
results show that the proposed retrieval method works best when the CV is at
the lowest (70%) with an atom input length of 5. Figure 7b shows some sample
snapshots representative of the retrieved shots for TQ2.
We analyzed these results from various perspectives and we ﬁnd that our
proposed method performs reasonably well at retrieving a user described trajec-
tory motion at high precision, but at the cost of a lower recall rate when CV
increases. This is likely due to its over-sensitivity towards the exact query given.
From the experiment, we also learnt that the queries should be expanded to
include neighboring atoms so as to provide a better chance at obtaining a higher
recall rate with good precision. This appeals towards the subjective nature of
trajectory-based querying where the users of such an interface would naturally
draw a general direction of the query instead of a precise path.
Table 3. Results of motion retrieval task with varying CV and number of atom inputs
No. of Input CV: 70%
CV: 80%
CV: 90%
Precision Recall F1 Score Precision Recall F1 Score Precision Recall F1 Score
TQ1 5
93.82
61.53
74.32
95.34
33.19
49.24
95.34
33.19
49.24
6
90.09
36.84
52.29
90.09
36.84
52.29
89.13
16.59
27.98
7
87.27
38.86
53.78
88
17.81
29.62
87.87
11.74
20.71
8
86.88
21.45
34.41
84.61
13.36
23.07
89.65
10.52
18.84
TQ2 4
16.28
80
27.06
28.69
73.33
41.25
28.69
73.33
41.25
5
65.3
71.11
68.08
73.33
48.88
58.66
73.33
48.88
58.66
6
55.81
53.33
54.54
55.81
53.33
54.54
57.69
33.33
42.25

326
C. W. Cheong et al.
5
Conclusion and Future Work
This paper proposes a framework for extracting and retrieving color and motion
semantics from an outdoor long-term car park setting. We demonstrated meth-
ods that were able to retrieve queries to a good measure of precision under various
lighting and weather conditions. However, there is room for improvement in the
recall ability for both the color and motion semantics.
Our future directions are aimed at ﬁne tuning the proposed method for bet-
ter performance over a longer span of time, i.e. weeks or months. With that,
alternative methods that are more data-dependent may be plausible, such as
learning a scalable color term extraction model.
Acknowledgment. This work is supported in part by Telekom Malaysia Research &
Development Grant No. RDTC/160903 (SHERLOCK) and Multimedia University.
References
1. Brent, B., Kay, P.: Basic Color Terms: Their Universality and Evolution. University
of California Press, Berkeley (1991)
2. Casta˜n´on, G., Elgharib, M., Saligrama, V., Jodoin, P.M.: Retrieval in long-
surveillance videos using user-described motion and object attributes. IEEE Trans.
Circ. Syst. Video Technol. 26(12), 2313–2327 (2016)
3. d’Acierno, A., Saggese, A., Vento, M.: Designing huge repositories of moving
vehicles trajectories for eﬃcient extraction of semantic data. IEEE Trans. Intell.
Transp. Syst. 16(4), 2038–2049 (2015)
4. Dehghan, A., Masood, S.Z., Shu, G., Ortiz, E., et al.: View independent vehi-
cle make, model and color recognition using convolutional neural network. arXiv
preprint arXiv:1702.01721 (2017)
5. Feris, R.S., Siddiquie, B., Petterson, J., Zhai, Y., Datta, A., Brown, L.M., Pankanti,
S.: Large-scale vehicle detection, indexing, and search in urban surveillance videos.
IEEE Trans. Multimed. 14(1), 28–42 (2012)
6. Hu, C., Bai, X., Qi, L., Chen, P., Xue, G., Mei, L.: Vehicle color recognition with
spatial pyramid deep learning. IEEE Trans. Intell. Transp. Syst. 16(5), 2925–2934
(2015)
7. Lim, R.W.S., Cheong, C.W., See, J., Tan, I.K.T., Wong, L.K., Khor, H.Q.: On
vehicle state tracking for long-term carpark video surveillance. In: IEEE Interna-
tional Conference on Signal and Image Processing Applications (2017, to appear)
8. Moghimi, M.K., Pourghassem, H.: Shadow detection based on combinations of
HSV color space and orthogonal transformation in surveillance videos. In: 2014
Iranian Conference on Intelligent Systems (ICIS), pp. 1–6. IEEE (2014)
9. Rachmadi, R.F., Purnama, I.: Vehicle color recognition using convolutional neural
network. arXiv preprint arXiv:1510.07391 (2015)
10. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed,
real-time object detection. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 779–788 (2016)
11. Zhang, T., Liu, S., Xu, C., Lu, H.: Mining semantic context information for intel-
ligent video surveillance of traﬃc scenes. IEEE Trans. Industr. Inf. 9(1), 149–160
(2013)

Venue Prediction for Social Images by
Exploiting Rich Temporal Patterns in LBSNs
Jingyuan Chen1(B), Xiangnan He1, Xuemeng Song2, Hanwang Zhang3,
Liqiang Nie2, and Tat-Seng Chua1
1 School of Computing, National University of Singapore, Singapore, Singapore
jingyuanchen91@gmail.com, xiangnanhe@gmail.com, dcscts@nus.edu.sg
2 School of Computer Science and Technology, ShanDong University, Jinan, China
sxmustc@gmail.com, nieliqiang@gmail.com
3 Department of Computer Science, Columbia University, New York, USA
hanwangzhang@gmail.com
Abstract. Location (or equivalently, “venue”) is a crucial facet of user
generated images in social media (aka. social images) to describe the
events of people’s daily lives. While many existing works focus on pre-
dicting the venue category based on image content, we tackle the grand
challenge of predicting the speciﬁc venue of a social image. Simply
using the visual content of a social image is insuﬃcient for this purpose
due its high diversity. In this work, we leverage users’ check-in histories
in location-based social networks (LBSNs), which contain rich tempo-
ral movement patterns, to complement the limitations of using visual
signals alone. In particular, we explore the transition patterns on suc-
cessive check-ins and periodical patterns on venue categories from users’
check-in behaviors in Foursquare. For example, users tend to check-in to
cinemas nearby after having meals at a restaurant (transition patterns),
and frequently check-in to churches on every Sunday morning (periodi-
cal patterns). To incorporate such rich temporal patterns into the venue
prediction process, we propose a generic embedding model that fuses the
visual signal from image content and various temporal signal from LBSN
check-in histories. We conduct extensive experiments on Instagram social
images, demonstrating that by properly leveraging the temporal patterns
latent in Foursquare check-ins, we can signiﬁcantly boost the accuracy
of venue prediction.
Keywords: Venue prediction · Transition pattern · Temporal pattern
1
Introduction
The unprecedented growth of smart mobile devices allows people to easily take
pictures of their life events and post them on online social networks. As a result,
This research is part of NExT++ project, supported by the National Research Foun-
dation, Prime Ministers Oﬃce, Singapore under its IRC@Singapore Funding Initia-
tive and National Natural Science Foundation of China under Grant No.: 61702300.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 327–339, 2018.
https://doi.org/10.1007/978-3-319-73600-6_28

328
J. Chen et al.
the current social Web is experiencing a tremendous volume of user generated
images, which we term as social images [6]. According to a recent study by
Chen et al. [7], over 45% of tweets are associated with images in Weibo — the
largest micro-blog service in China. To facilitate understanding and use of such
social images, it is crucial to address the fundamental problem of “where did it
happen”.
Fig. 1. Examples of social images with venue tags to show the challenge of venue
prediction based on visual content only.
Several existing works have explored the prediction of venue category for
social images [22] and micro-videos [21], such as to predict whether a photo is
taken at a restaurant or a theme park. In this work, we move one step further, to
predict the speciﬁc venue1 of a social image, which permits us to infer a user’s
footprint more accurately so as to provide better location-based services. Specif-
ically, we aim to predict the exact venue where the user was taking the photo,
such as the Los Tacos No. 1 restaurant or Universal Studios Singapore, rather
than the general categories of restaurant or theme park. Nevertheless, existing
approaches rely solely on images’ visual contents [21,22] for venue category pre-
diction, which is far from being suﬃcient to predict the speciﬁc venue accurately
due to the high diversity of social images. Figure 1 gives an illustrative example.
The ﬁrst row shows that simply relying on visual content of the image itself, it
is diﬃcult to distinguish whether the image is taken in the Universal Studio of
Singapore, Osaka or Hollywood. The second row shows that images taken at the
same venue can be very diﬀerent visually.
To alleviate the diﬃculty, an intuitive idea is to utilize a user’s histori-
cal locations to restrict the venue prediction candidates and discover possible
1 In this work, we use point-of-interest (POI), venue, and location interchangeably,
which all refer to a speciﬁc venue.

Venue Prediction for Social Images
329
movement patterns. As an example, the visual content may show that the image
was taken in a McDonald’s restaurant, while the recent movement history of
the user can help to identify which speciﬁc store of McDonald’s. However, the
high sparsity of venue-tagging activities of social network sites (e.g., Twitter
and Instagram) makes it challenging to implement the idea based on the data
from one site only. Fortunately, the emerging of location-based social networks
(LBSNs) provides an excellent alternative source of data to tackle the problem.
For example, users mainly use Foursquare2 to check-in to POIs, leaving us with
valuable spatial-temporal trails of users’ historical movements. In this work, we
explore the possibilities of mining check-in histories of LBSNs to address the
problem of predicting the exact venues of images in a social media site with
sparse check-in histories.
Fig. 2. Overview of our proposed framework. Given a social image and the user’s
check-in history, our framework predicts the speciﬁc venue of the image by inferring a
probabilistic distribution. (a) We train a deep convolutional neural network to learn
venue information (both category and speciﬁc venue) from visual content. (b) We mine
various signals from the check-in histories of a LBSN to complement the visual signals.
Using Foursquare as a check-in–rich LBSN for our case study, we ﬁrst con-
ducted comprehensive statistical analysis to understand users’ movement behav-
iors. Our analysis reveals some promising patterns that support our premise of
using LBSN check-in histories for venue prediction. For example, we found that
people typically move within a bounded region and seldom travel long distances
within a short time; and moreover, successive check-ins usually exhibit certain
correlations and strong periodical patterns. Guided by these phenomenon, we
developed an end-to-end probabilistic solution for social image venue prediction
(Sect. 3). Figure 2 shows an overview of our proposed framework. Speciﬁcally, our
2 https://foursquare.com.

330
J. Chen et al.
solution uniﬁes the inference of venue category and speciﬁc venue, and both com-
ponents carefully fuse the visual signal from image content and various temporal
signal from LBSN check-in histories. We evaluate our solution on social images
of Instagram (Sect. 4). Extensive experiments demonstrate that by exploiting
the rich temporal patterns in Foursquare check-ins, we can signiﬁcantly enhance
the venue prediction accuracy by 5.7% on average.
2
Related Work
Image venue prediction, also called image geotagging, aims to identify the venue,
landmark or location that an image refers to from a set of candidates [1,18,19].
Most methods extract a rich set of visual features from images and leverage the
visual features to train either shallow or deep models to estimate the venues
of the given images. As reported in [12], the landmark identiﬁcation [3] and
scene classiﬁcation [2] of images are the key factors to recognize the locations.
In addition to visual content, Crandall et al. [9] combine both visual and textual
information to map photos on Flickr into diﬀerent venue categories based on
landmarks. These approaches are based on the observation that there exists
strong correlation between the content of images with certain venue categories.
However, most of these methods ignore the high diversity of the content of social
images. For example, visually similar images can be taken at diﬀerent venues,
while images taken at the same places can have diﬀerent visual appearance.
In such cases, utilizing only the content of images is far from being suﬃcient
to predict the speciﬁc venue accurately. Our work diﬀers from these studies
by utilizing a user’s historical movement behaviors on LBSN to discover users’
possible movement patterns and adjust the prediction accordingly.
3
Proposed Method
In this section, we begin by formulating the venue prediction problem, followed
by elaborating the design of solution components one by one. In terms of tech-
niques, we wish to develop models that are expressive enough to capture the
various relevant signals and temporal dynamics, while at the same time can gen-
eralize well with a controllable number of parameters. To achieve these design
goals, we resort to embedding-based models, which encode various features and
patterns in the latent space.
3.1
Problem Formulation
Let Su = {s1
u, s2
u, · · · , snu
u } be the historical check-in sequence for user u, where
nu denotes the number of u’s historical check-ins; if a user has check-in behaviors
on multiple social networks, we can merge them together to form Su. Each check-
in entry sn
u is represented as sn
u = (ln
u, in
u, tn
u), meaning the location, image, and
time of the check-in. Note that in
u is an optional ﬁeld for sn
u, as not all check-ins

Venue Prediction for Social Images
331
are associated with images. Each location ln
u corresponds to a venue category
cn
u. Given a set of images with venue categories, we train a ResNet to predict
the venue category of the image. In this work, we aim to solve the problem of
predicting the speciﬁc venue lnu+1
u
of the next check-in snu+1
u
, given its check-in
image inu+1
u
, timestamp tnu+1
u
, and the user’s check-in history. From a proba-
bilistic point of view, the task can be addressed by inferring the probability that
user u would visit lnu+1
u
at time point tnu+1
u
:
p(lnu+1
u
|Su, inu+1
u
).
(1)
Apparently, it is not advisable to predict the speciﬁc venue directly. On one
hand, it is more challenging to predict the speciﬁc venue than the venue category
solely based on the visual content of social images due to their high diversity
(example see Fig. 1). On the other hand, it has been found that there exists a
strong correlation between the categories of two successive check-ins for a user in
a short period according to our pilot study. As such, we tackle the problem in two
steps. First, for each social image, we predict its venue category by taking into
account multiple aspects including the visual content, user’s personal interests,
global popularity of the category and the transition probability between succes-
sive check-ins. Second, given the predicted venue category, we predict the speciﬁc
venue based on the similar aspects. Mathematically, we decompose Eq. (1) into
two parts:
p(lnu+1
u
|Su, inu+1
u
) = p(lnu+1
u
|Su, inu+1
u
, cnu+1
u
) · p(cnu+1
u
|Su, inu+1
u
).
(2)
In what follows, we ﬁrst detail how to predict the venue category, and then
discuss the venue prediction model. We pay special attention to temporal mod-
eling, which is used in both the category and venue prediction.
3.2
Venue Category Prediction
Although there are several work on predicting venue category of multimedia
posts on social networks, such as the micro-videos on Vine [5,21], these work are
merely based on the multimedia (and textual) content while overlooking other
relevant signals. One important signal is the sequential pattern between two suc-
cessive check-in categories. For example, if a user just checked in at a shopping
mall, then the user is more likely to visit a restaurant as compared to oﬃce
or school. Such sequential patterns naturally motivate us to adopt the Markov
chain [8] modeling. Secondly, venue categories usually show varying global pop-
ularity, for instance, users tend to check in at oﬃce more frequently than hospi-
tal. Apart from the global popularity, they may also express diﬀerent personal
interests in diﬀerent venue categories. To tackle the venue category prediction
problem in a comprehensive way, we summarize the key factors to be accounted
for as follows:

332
J. Chen et al.
– the global popularity of venue categories;
– the consistency between the user’s personal interest and the given venue
category;
– the correlation between the venue category of the last check-in and that of
the current check-in;
– the matching of the visual content of social image and the given venue
category.
Based on the ﬁrst-order Markov chain property, the venue category proba-
bility conditioning on the full check-in sequence can be approximated as condi-
tioning on the last check-in:
p(cnu+1
u
|Su, inu+1
u
) = p(cnu+1
u
|cnu
u , inu+1
u
).
(3)
By incorporating the proposed factors, we further parameterize the proba-
bility as:
p(cnu+1
u
|cnu
u , inu+1
u
) = αnu+1
u
+ ˜vu · ˜cnu+1
u
+ ˜cnu
u · ˜cnu+1
u
+ θ1p(cnu+1
u
|inu+1
u
),
(4)
where αnu+1
u
denotes the global popularity of category cnu+1
u
; vectors ˜vu ∈RD1
and ˜cnu
u
∈RD1 denote the embedding vector for user u and category cnu
u , respec-
tively. The ﬁrst inner product ˜vu · ˜cnu+1
u
encodes the personalized preference of
user u on venue category cnu+1
u
, and the second inner product ˜cnu
u ·˜cnu+1
u
encodes
the transition probability between the two successive check-ins. Note that we
have intentionally chosen inner product to model the interaction between two
entities, which although is simple, but shown to be very competitive compared
to more complex neural network functions recently [16].
The last term p(cnu+1
u
|inu+1
u
) denotes the probability that image inu+1
u
belongs to category cnu+1
u
, which is inferred from the visual content of the image.
In our implementation, we employ the state-of-the-art deep convolutional neural
network ResNet [13], pre-training it on ImageNet data and ﬁne-tuning it to our
venue category data (more details see Sect. 4.1). The hyper-parameter θ1 should
be non-negative that controls the weight of the visual signal. Note that we have
further normalized the probabilities such that  p(·|cnu
u , inu+1
u
) = 1, to make it
a valid conditional probability in the strict sense.
3.3
Speciﬁc Venue Prediction
As have mentioned before, most of user movements are constrained to a relatively
small geographical region in a short period. As such, a social image is more likely
to be taken at the venues near the user’s last check-in. We similarly adopt the
ﬁrst-order Markov chain assumption and model the venue probability as:
p(lnu+1
u
|Su, inu+1
u
) = p(lnu+1
u
|snu
u , inu+1
u
)
= βnu+1
u
+ ˆvu ·ˆlnu+1
u
+ˆlnu
u ·ˆlnu+1
u
+ θ2p(lnu+1
u
|inu+1
u
),
(5)

Venue Prediction for Social Images
333
where βnu+1
u
represents the global popularity of venue lnu+1
u
, vectors ˆvu ∈RD2
and ˆlnu
u
∈RD2 denote the embedding vector for user u and venue lnu
u , respec-
tively. Note that the speciﬁc venue model shares a similar formulation with the
venue category model, as users’ movement patterns on venue categories can
be smoothly transferred to speciﬁc venues. Speciﬁcally, the two inner product
terms encode the personalized user preference and transition pattern, respec-
tively. The last term p(lnu+1
u
|inu+1
u
) denotes the probability that image inu+1
u
is taken at venue lnu+1
u
judging from the visual content. The hyper-parameter
θ2 should be non-negative that controls the weight of the visual signal. As it
is computationally expensive and ineﬃcient to train deep learning models over
hundreds of thousands venues, we estimate the probability as the cosine similar-
ity between location features and image features. Speciﬁcally, the image features
are extracted by the ResNet (trained on venue categories), and the location
features are the average of extracted features of images that are taken at the
location.
3.4
Modeling Temporal Dynamics
We now present how to incorporate other temporal dynamics into our prediction
model. We discuss the details in the context of venue category prediction model.
As the encoding of temporal dynamics to speciﬁc venue model can be achieved
in a similar way, we omit these details here to avoid repetition.
Periodical patterns
According to our statistics, users’ visits to some venue categories, such as concert
hall, oﬃce, pub and restaurant, are highly dependent on the time (e.g., day of
the week) and exhibit periodical patterns. For example, users typically visit pub
more frequently during the night of weekends than weekdays. Moreover, within
the span of a day, the category distribution also varies temporally. For example,
users prefer to check in more at train station in the morning, while check in more
at cafe in the afternoon. To capture the inﬂuence of temporal information, it is
natural to extend the user embedding vectors ˜vu and ˆvu to be time-dependent.
Speciﬁcally, we introduce three time-related categorical variables to interpret a
timestamp:
– t1 indicates whether the image is uploaded on weekday or weekend, e.g., 0
means weekday and 1 means weekend.
– t2 indicates the day of the week that the image is uploaded, e.g., 0 represents
Monday, 1 represents Tuesday, 2 represents Wednesday, etc.
– t3 indicates the time of the day the image is uploaded, where a day is divided
into 24 h from 0 to 23.
Accordingly, we introduce three types of embedding vectors to capture the
temporal dynamics in user representation as,
˜vu(t) = ˜vu + E1(t1) + E2(t2) + E3(t3),
(6)

334
J. Chen et al.
where E1 ∈RD1×2, E2 ∈RD1×7 and E3 ∈RD1×24 denote the embedding matrix
for the three time indicators, which are weekday or weekend, day of the week
and time of the day, respectively. The symbol Ei(ti) returns the ti-th column
of Ei, and similarly for other notations. Here the stationary user preference is
modelled by ˜vu, while the time-dependent periodical patterns are accounted by
remaining parts.
Temporal weighting drift
In addition to the periodical patterns, we also consider the temporal dependence
between two successive check-ins. As the time interval between two successive
check-ins generally follows a Poisson distribution and may range from several
minutes to days, it is natural to assume that smaller interval leads to higher
dependence. To capture this eﬀect, we introduce a time decay component similar
to [15] to adjust the transition probability:
p(cnu+1
u
|cnu
u , inu+1
u
) = αnu+1
u
+ ˜vu(tnu+1
u
) · ˜cnu+1
u
+ δ(Δt)˜cnu
u · ˜cnu+1
u
+ θ1p(cnu+1
u
|inu+1
u
).
(7)
where Δt = tnu+1
u
−tnu
u represents the time interval between two successive check-
ins and δ(Δt) = e−λΔt is the rate of the decay. Through this, the dependency
on previous check-in can be gradually weakened as time goes by.
Table 1. Number of data records in NUS-MSS.
City
users# ch-ins# images# venues# venue cats#
London
2,860
63,273
11,661
5,857
394
Singapore 5,677
284,258
10,711
14,010
450
New York 5,122
189,152
23,925
14,891
485
4
Experiments
We ﬁrst present the experimental settings, followed by studying the performance
of our proposed solution in venue prediction and exploring the eﬀectiveness of
temporal check-in patterns. Lastly, we perform some micro-level analysis by
showing some illustrative examples.
4.1
Preliminaries
In this work, all the experiments are conducted based on the NUS-MSS
dataset [11], which provides a set of users’ behaviors on multiple social net-
works. In particular, we utilize users’ check-in sequences on Foursquare and
social images on Instagram. Given a social image i posted by user u on
Instagram at time point t, user u’s last check-in venue is his/her lastest check-
in on Foursquare before time point t. To ensure the quality of the dataset, we

Venue Prediction for Social Images
335
retain only users with at least 3 check-ins, and venues that have been visited
at least 3 times. Finally, we obtain a dataset consisting of 2860, 5677 and 5122
users, 63273, 284258 and 18912 check-ins, and 11661, 10711 and 23925 images
in London, Singapore and New York respectively, as shown in Table 1.
Dataset Alignment. Due to that the POI tags provided by Instagram cannot
be directly aligned with those on Foursquare, we ﬁrst need to tackle the prob-
lem of venue alignment. For each venue of Instagram, we crawl its proﬁle to
obtain the name and location information (i.e. longitude and latitude), based on
which we utilize the Foursquare venue/search api endpoint3 to link each venue
in Instagram to that in Foursquare.
Visual Feature Extraction. As certain objects should frequently appear in
certain venue categories or venues, the multimedia content of social images plays
an important role in venue prediction. To build the deep model based on the
visual contents of social images as mentioned in Sect. 3.2 and feature extraction
of speciﬁc venue matching as mentioned in Sect. 3.3, for each venue category,
we collected extra 200 images from the venue proﬁle on Foursquare. We then
employ the ResNet-50 [13] model, which has been extensively studied in com-
puter vision domain, as the architecture of deep model. This model is trained on
ImageNet [10] and then ﬁne-tuned on the venue category dataset we collected.
Finally, we adopt the output after softmax as the prediction score for each venue
category and pool5 layer output as the features for speciﬁc venue matching. It
is noted that the venue feature vector is generated by averaging all the image
features belongs to that venue.
Evaluation Metrics. To evaluate the performance of venue prediction, we
adopt the leave-one-out evaluation strategy, where for each user, we select the
latest social image as the testing sample and the remaining data for training.
Regarding the evaluation metrics, on one hand, we use the average top-1 and
top-5 accuracies, which are the standard measures for the single-label task [4].
On the other hand, to assess the position of the hit, we adopt another widely
used metric—Normalized Discounted Cumulative Gain (NDCG) [17]. Finally,
we report the average score for all testing samples. It is worth mentioning that
to save the time cost, for each testing sample, we randomly sample 200 negative
samples as the venue candidates rather than the whole samples.
4.2
Model Comparison
Since our proposed model is derived from the matrix factorization method [20]
with temporal patterns, we term it as MFTP for short. To evaluate the proposed
method, we compare it with the following baselines:
– VenuePop. Venues are ranked by their popularity, which is measured by the
number of check-ins. This is a non-personalized method to benchmark the
prediction performance.
3 https://developer.foursquare.com/docs/.

336
J. Chen et al.
Table 2. Average top-1 and top-10 accuracy, and NDCG-10 for venue prediction.
City
Method
VenuePop ContentBased NearestNeigh FPMC-LR MFTP
London
Top-1(%)
8.25
11.01
28.71
35.21
38.25
Top-10(%)
25.31
39.09
49.48
63.81
65.98
NDCG-10(%) 15.95
23.01
38.29
48.68
51.16
Singapore Top-1(%)
13.38
12.76
19.13
31.27
32.50
Top-10(%)
39.35
41.29
41.59
69.60
71.66
NDCG-10(%) 25.04
24.86
29.14
48.84
50.52
NewYork
Top-1(%)
11.03
10.07
27.72
38.99
40.96
Top-10(%)
25.79
37.72
46.60
65.25
66.97
NDCG-10(%) 17.26
21.86
36.52
51.08
52.91
– ContentBased. This method is solely based on the content of social images.
Due to the fact that the number of images for unpopular venues is not enough
for training the SVM classiﬁers, we simply calculate the cosine similarity
between the venue features and the given social image feature.
– NearestNeigh. According to our statistics, users usually move within a
bounded region. Therefore, for each social image, we select the nearest neigh-
bor venue to the user’s last check-in as the venue tag.
– FPMC-LR [8]. This method mainly exploits the personalized Markov chains
in the inter check-in sequence. The diﬀerence from our model lies in that
FPMC-LR predict the speciﬁc venue in one step and overlooks the importance
of the periodical patterns.
Parameter Settings. We randomly initialized model parameters with a Gaus-
sian distribution (with a mean of 0 and standard deviation of 0.01) and optimized
the model with stochastic gradient descent (SGD) until convergence. Finally, we
tested the batch size of 32, the latent feature dimension of 32, the learning rate
of [0.01, 0.05] and the regularizer of 0.01.
Table 2 shows the performance for diﬀerent models. The results show that:
– The general trend is that MFTP signiﬁcantly outperforms the rule-based
baselines, such as VenuePop and NearestNeigh. Despite the relatively bad
performance of NearestNeigh method, it is still better than expected, espe-
cially on TOP-1 accuracy. This maybe due to the fact that after data cleaning
there are only about 10,000 venues left in each city, which are more likely to
be scattered throughout the city and thus have relatively large inter dis-
tances. Moreover, as users tend to move within a bounded region, the large
inter distance would narrow down the number of venue candidates and hence
boost the performance of venue prediction. Therefore, we hypothesize that
NearestNeigh would work well on datasets with low density of venues.
– The performance of content-based method ContentBased is unsatisfactory,
due to the high diversity of content of social images. For the same venue, peo-
ple may take photos of diﬀerent objects or from diﬀerent angles. For example,

Venue Prediction for Social Images
337
it is common that users would take photo of a dish, upload it to the social
network and then check-in to the restaurant. In such a case, the visual content
cannot reﬂect the unique characteristics of the speciﬁc restaurant.
– MFTP shows superiority over the strong baseline—FPMC-LR, which is based
on a similar generic embedding framework. The possible reasons are as follows:
(1) FPMC-LR directly predicts the speciﬁc venue tags in which the category
of the venue is not utilized; (2) although FPMC-LR considers the transition
patterns between successive check-ins, it ignores the important periodical
patterns of users’ check-in behaviors; and (3) FPMC-LR is a POI recommen-
dation framework in which no visual content information is utilized.
4.3
Illustrative Examples
To gain insights on the inﬂuential factors on the task of venue prediction, we
comparatively illustrate a few representative examples in Fig. 3. From this ﬁgure,
we have the following observations:
– The image in Fig. 3(a) refers to a famous landmark in Singapore and the
visual content is clean and distinctive. In such cases, the visual content will
dominate the prediction.
– From the visual content of the image in Fig. 3(b), we can easily tell that the
image was taken in some library. However, it is diﬃcult to ﬁgure out which
speciﬁc library it refers to. Fortunately, the user’s last check-in was to a bar
which is quite near the speciﬁc library; this helps us to get the right answer.
Fig. 3. Illustration of prediction results. They respectively justify the importance of
visual content, transition patterns on successive check-ins, and periodical patterns.

338
J. Chen et al.
– The image in Fig. 3(c) reﬂects a church in New York. However, the visual
signals derived from CNNs mistakenly indicate that this image is a concert
hall. Fortunately, we found that the time stamp of this image is Sunday
moring and the user periodically visit church around the same time every
week. Based on these temporal signals, the model could correctly generate
the prediction.
Due to space limitation, we only show the positive results in Fig. 3. It is
actually a very hard problem to get the speciﬁc venue prediction right in some
cases. For example, for users that travel frequently, the prediction performance
would be quite low. To tackle such problems, we will investigate how to include
GPS information to extend our model for further improvement.
5
Conclusion
In this work, we studied the novel problem of speciﬁc venue prediction of social
images. We ﬁrst conducted exploratory analysis on real-world datasets, based on
which we found strong evidence of transition patterns on successive check-ins and
periodical patterns on venue categories. We then developed a generic embedding
model based on matrix factorization to capture the interactions between visual
content and temporal patterns. To the best of our knowledge, this is the ﬁrst
work on time-aware social image venue prediction. Experimental results on a
real-world dataset demonstrate the eﬀectiveness of our proposed solution, where
the accuracy of venue prediction was improved by more than 5% by leveraging
LBSN check-ins. Apart from quantitative analysis, we highlight two qualita-
tive insights gained from this work. First, it is promising to exploit the venue
category information for location-related tasks. Second, transition patterns and
periodical patterns are strong signals in predicting users’ movements and activ-
ities. In future, we plan to investigate the eﬀect of GPS information for venue
prediction of multimedia content. Further, we are interested in exploring the
recently developed neural factorization machines [14] for modelling the higher-
order interactions between users, venues, and temporal patterns.
References
1. Avrithis, Y.S., Kalantidis, Y., Tolias, G., Spyrou, E.: Retrieving landmark and
non-landmark images from community photo collections. In: MM. ACM (2010)
2. Cao, S., Snavely, N.: Graph-based discriminative learning for location recognition.
IJCV 112(2), 239–254 (2015)
3. Chen, D.M., Baatz, G., K¨oser, K., Tsai, S.S., Vedantham, R., Pylv¨an¨ainen, T.,
Roimela, K., Chen, X., Bach, J., Pollefeys, M., Girod, B., Grzeszczuk, R.: City-
scale landmark identiﬁcation on mobile devices. In: CVPR. IEEE (2011)
4. Chen, J., Ngo, C.: Deep-based ingredient recognition for cooking recipe retrieval.
In: MM. ACM (2016)
5. Chen, J., Song, X., Nie, L., Wang, X., Zhang, H., Chua, T.: Micro tells macro:
predicting the popularity of micro-videos via a transductive model. In: MM, pp.
898–907. ACM (2016)

Venue Prediction for Social Images
339
6. Chen, J., Zhang, H., He, X., Nie, L., Liu, W., Chua, T.: Attentive collaborative
ﬁltering: multimedia recommendation with item- and component-level attention.
In: SIGIR, pp. 335–344. ACM (2017)
7. Chen, T., He, X., Kan, M.: Context-aware image Tweet modelling and recommen-
dation. In: MM. ACM (2016)
8. Cheng, C., Yang, H., Lyu, M.R., King, I.: Where you like to go next: successive
point-of-interest recommendation. In: IJCAI. IJCAI/AAAI (2013)
9. Crandall, D.J., Backstrom, L., Huttenlocher, D.P., Kleinberg, J.M.: Mapping the
world’s photos. In: WWW. ACM (2009)
10. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: a large-scale
hierarchical image database. In: CVPR. IEEE (2009)
11. Farseev, A., Nie, L., Akbari, M., Chua, T.: Harvesting multiple sources for user
proﬁle learning: a big datastudy. In: ICMR. ACM (2015)
12. Hays, J., Efros, A.A.: IM2GPS: estimating geographic information from a single
image. In: CVPR. IEEE Computer Society (2008)
13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. IEEE Computer Society (2016)
14. He, X., Chua, T.-S.: Neural factorization machines for sparse predictive analytics.
In: SIGIR, pp. 355–364 (2017)
15. He, X., Gao, M., Kan, M.-Y., Liu, Y., Sugiyama, K.: Predicting the popularity of
web 2.0 items based on user comments. In: SIGIR, pp. 233–242 (2014)
16. He, X., Liao, L., Zhang, H., Nie, L., Hu, X., Chua, T.-S.: Neural collaborative
ﬁltering. In: WWW, pp. 173–182 (2017)
17. J¨arvelin, K., Kek¨al¨ainen, J.: Cumulated gain-based evaluation of IR techniques.
TOIS 20(4), 422–446 (2002)
18. Li, X., Pham, T.N., Cong, G., Yuan, Q., Li, X., Krishnaswamy, S.: Where you
Instagram? Associating your Instagram photos with points of interest. In: CIKM.
ACM (2015)
19. Li, Y., Crandall, D.J., Huttenlocher, D.P.: Landmark classiﬁcation in large-scale
image collections. In: ICCV. IEEE (2009)
20. Zhang, H., Shen, F., Liu, W., He, X., Luan, H., Chua, T.-S.: Discrete collaborative
ﬁltering. In: SIGIR, pp. 325–334 (2016)
21. Zhang, J., Nie, L., Wang, X., He, X., Huang, X., Chua, T.: Shorter-is-better: venue
category estimation from micro-video. In: MM. ACM (2016)
22. Zhou, B., Lapedriza, `A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features
for scene recognition using places database. In: NIPS (2014)

Demonstrations

A Virtual Reality Interface for Interactions
with Spatiotemporal 3D Data
Hunter Quant, Sean Banerjee, and Natasha Kholgade Banerjee(B)
Clarkson University, Potsdam, NY 13699, USA
{quanthd,sbanerje,nbanerje}@clarkson.edu
Abstract. Traditional interfaces for interacting with 3D models in vir-
tual environments lack support for spatiotemporal 3D models such as
point clouds and meshes generated by markerless capture systems. We
present a virtual reality (VR) interface that enables the user to per-
form spatial and temporal interactions with spatiotemporal 3D models.
To accommodate the high volume of spatiotemporal data, we provide a
data format for spatiotemporal 3D models which has an average speedup
of 3.84 and a space reduction of 43.9% over traditional model ﬁle for-
mats. We enable the user to manipulate spatiotemporal 3D data using
gestures intuitive from real-world experience or by using a VR user inter-
face similar to traditional 2D visual interactions.
Keywords: Virtual reality · Spatiotemporal 3D models
User interface
1
Introduction
With the ubiquity of motion capture systems and the rise of multi-camera sys-
tems for markerless motion capture, a large quantity of spatiotemporal 3D data
is being generated in the form of point clouds [7] and meshes [2,3]. While soft-
ware such as AutoDesk MotionBuilder enables the user to visualize motion
capture points, there exist limited approaches [1,2] to visualize spatiotemporal
point cloud and mesh data. The interactions provided by these approaches are
restricted to spatial manipulations and temporal playback using traditional user
interface devices such as mouse, keyboard, and screen. Approaches in augmented
reality [4] allow for temporal interactions on animated 3D models, however this
does not accommodate non-rigged spatiotemporal 3D data obtained by marker-
less motion capture, such as sequenced point clouds and meshes.
In this demo, we present a virtual reality (VR) interface for interacting with
spatiotemporal 3D models. In addition to spatial transformations such as rota-
tion, scaling, and translation provided by traditional approaches of VR interac-
tion on objects [5,6,9], our approach provides novel temporal transformations
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-319-73600-6 29) contains supplementary material, which is
available to authorized users.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 343–347, 2018.
https://doi.org/10.1007/978-3-319-73600-6_29

344
H. Quant et al.
in VR such as rewind, fastforward, and playback on spatiotemporal 3D models
such as point cloud and mesh collections, where the object has a separate model
speciﬁcation at each time instant. Our work is distinct from current temporal
approaches for VR interaction that focus on editing of VR footage [8]. Our inter-
face allows users to interact with spatiotemporal 3D models either using gestures
intuitive through real-world experience or using interactions with a user inter-
face (UI) intuitive through traditional interactions with 2D screen content. Our
VR interface is implemented in the Unity 3D Game Engine [10] and runs on the
HTC Vive and the Oculus Rift CV1 with Oculus Touch.
To load high volumes of spatiotemporal 3D data into memory for VR inter-
actions, we create a novel data ﬁle format that provides an average speedup of
3.84 and a space reduction of 43.9% over traditional model ﬁle formats such as
OBJ and PLY. We report load time and space usage results for temporal 3D
point cloud and 3D mesh collections on a Windows 10 desktop with an NVIDIA
GTX 1080 Ti graphics processing unit (GPU) and an Intel i7 7700K processor.
2
VR Interface to Interact with Spatiotemporal 3D Data
Gestural Interactions. As shown in Fig. 1, the user can perform spatial and
temporal interactions on spatiotemporal 3D data using swipe gestures on the
main controller. The user switches between transformation modes by swiping
the auxiliary controller. Our approach allows the user to perform translation
along the axis of the pointer using vertical swipes. We also provide a freeform
translation mode, where the user grabs the object by pulling the trigger on the
main controller while positioning its pointer on the object, moves the controller,
and releases the trigger to set the object at a desired destination.
Our approach provides horizontal swipes to rewind and fast-forward temporal
changes and to scale the object size. To perform rotation the user swipes in the
desired rotation direction, and rotation occurs along the axis vrot calculated as
vrot = xvr −yvup. Here, vup is the up vector given as

0 1 0
T , x ∈[−1, 1] and
y ∈[−1, 1] are coordinates of a 2D vector vinput along the swipe direction, and
vr is a unit vector perpendicular to the up vector vup and a unit vector vp in
the direction of the pointer through the controller into VR space. The value of
vr is calculated as vup × vp/ ∥vup × vp∥. The rotation occurs around the origin
of the model coordinate system.
(a) Translation by moving controller and vertical swipe
(b) Rotation by horizontal swipe
(c) Scaling by horizontal swipe
(d) Temporal navigation by horizontal swipe
Before
Before
Before
After
After
Before
After
After
Fig. 1. Gestural interactions for (a) translation, (b) rotation, (c) scaling, and (d) tem-
poral navigation.

A Virtual Reality Interface for Interactions with Spatiotemporal 3D Data
345
Before
After
Before
After
Before
After
Before
After
(a) Translation using UI buttons
(b) Rotation using UI buttons
(c) Fast forward using UI slider
(d) Rewind using UI buttons
Fig. 2. UI interactions for (a) translation, (b) rotation, (c) temporal navigation slider,
and (d) temporal navigation buttons. Translation and rotation is performed on data
associated with [2].
Interactions with user interface (UI). As shown in Fig. 2, the user can
also use the pointer and trigger on the main controller to interact with a UI
displayed to the user in VR whose motion in VR space is attached to the auxiliary
controller. The user can point to and trigger the single and double arrow buttons
respectively to perform small and large increments of spatial transformation
along the three axes or temporal transformation forward and backward in time.
We provide a UI slider that enables the user to perform navigation back and
forth in the temporal domain. The user triggers the pause/play button on the
UI to pause or playback the spatiotemporal 3D data in the virtual space.
3
3D Spatiotemporal Data Format
To load spatiotemporal 3D models with high vertex and face counts of between
10,000 to 500,000 vertices that are typical outputs of modern markerless motion-
capture systems, we provide a ﬁle structure that consists of a header ﬁle to spec-
ify the number of frames in the object, a list of properties present in the model
such as vertices, faces, and color, and a ﬂag that speciﬁes whether a property
is shared by all frames or not. Properties are stored as binary ﬁles. In the sup-
plementary video, we show meshes of fruit aging that contain vertices and a
face structure shared by all frames, meshes of single person captures from data
associated with [2] that contain individual vertices and faces for each frame, and
point clouds of single and multi-person captures that contain individual vertices
with no faces.
Table 1 shows counts of properties such as vertices and faces for the fruit
meshes and the point clouds, together with space usage in MB and runtimes in
milliseconds per frame (ms/frame) for our ﬁle format as compared to traditional
ﬁle formats such as OBJ for the fruit, and ASCII PLY for the point clouds.
As shown by the table, our ﬁle format uses lesser space with an average space
reduction of 54.2% from the OBJ format, 38.7% from the PLY format, and
43.9% overall. Our format also loads faster into memory, with an average loading
speedup of 2.70 over the OBJ format, 4.40 over the PLY format, and 3.84 overall.
By using a global header ﬁle, we minimize per-frame header reading time, and
provide faster load time per MB, with an average speedup normalized for storage
of 1.24 for OBJ ﬁles, 2.7 for PLY ﬁles, and 2.21 overall.

346
H. Quant et al.
Table 1. Vertex and face counts, space usage (MB), space reduction, load time
(ms/frame), speedup, and speedup normalized for space usage for our ﬁle format ver-
sus traditional formats such as OBJs for pear and banana meshes and PLYs for point
clouds of paper plane, guitar, rock-paper-scissors (RPS), and origami.
Name
Verts
Faces Space usage Space
reduction
Load time
Speedup Normalized
speedup
Ours Trad.
Ours Trad.
Pear
22633
45262 103
225
54.2%
76.6
238.6 3.12
1.43
Banana
18226
36448 83.4
182
54.2%
82.0
188.0 2.29
1.03
Plane
82609
-
189
309
38.9%
70.9
295.7 4.17
2.55
Guitar
82079
-
187
315
40.6%
84.7
302.9 3.57
2.12
RPS
134778
-
308
482
36.1%
94.8
427.2 4.51
2.88
Origami 146198
-
334
552
39.5%
97.5
522.6 5.36
3.24
4
Discussion
We have provided a virtual reality interface that expands the gamut of VR inter-
actions to handle temporal transformations to spatiotemporal 3D data such as
point clouds and meshes in conjunction with traditional spatial transformations.
Our current data ﬁle format provide support for vertices and mesh faces. We will
expand the ﬁle format to include support for geometric properties such as nor-
mals and mesh smoothness values, and reﬂection models with their associated
material properties. In future, we will provide visualization of multiple forms of
textural information such as thermal and multispectral data, depth and shadow
maps, and ambient occlusion. Future work will also include enabling creation
and editing of spatiotemporal 3D data through our interface.
Acknowledgements. This work was partially supported by the National Science
Foundation (NSF) grant #1730183.
References
1. Akhter, I., Simon, T., Khan, S., Matthews, I., Sheikh, Y.: Bilinear spatiotemporal
basis models. ACM Trans. Graph. 31(2) (2012)
2. Alexiadis, D., Zarpalas, D., Daras, P.: Fast and smooth 3D reconstruction using
multiple RGB-depth sensors. In: 2014 IEEE VCIP, pp. 173–176, December 2014
3. Collet, A., Chuang, M., Sweeney, P., Gillett, D., Evseev, D., Calabrese, D., Hoppe,
H., Kirk, A., Sullivan, S.: High-quality streamable free-viewpoint video. ACM
Trans. Graph. 34(4) (2015)
4. Dong, S., Behzadan, A.H., Chen, F., Kamat, V.R.: Collaborative visualization of
engineering processes using tabletop augmented reality. Adv. Eng. Softw. 55, 45–55
(2013). https://doi.org/10.1016/j.advengsoft.2012.09.001
5. Google: Blocks. https://vr.google.com/blocks/
6. Han, S., Lee, H., Park, J., Chang, W., Kim, C.: Remote interaction for 3D manip-
ulation. In: Proceedings CHI (Extended Abstracts) (2010)

A Virtual Reality Interface for Interactions with Spatiotemporal 3D Data
347
7. Kowalski, M., Naruniec, J., Daniluk, M.: Live scan3D: a fast and inexpensive 3D
data acquisition system for multiple kinect v2 sensors. In: IEEE 3DV (2015)
8. Nguyen, C., DiVerdi, S., Hertzmann, A., Liu, F.: Vremiere: in-headset virtual real-
ity video editing. In: Proceedings of the CHI (2017)
9. Oculus: Medium. https://www.oculus.com/medium/
10. Unity: Unity 3D. https://unity3d.com/

ActionVis: An Explorative Tool to Visualize
Surgical Actions in Gynecologic Laparoscopy
Stefan Petscharnig(B)
and Klaus Schoeﬀmann
Alpen-Adria-Universit¨at Klagenfurt, Universit¨atsstraße 65-67,
9020 Klagenfurt, Austria
{stefan.petscharnig,klaus.schoeffmann}@itec.aau.at
Abstract. Appropriate visualization of endoscopic surgery recordings
has a huge potential to beneﬁt surgical work life. For example, it enables
surgeons to quickly browse medical interventions for purposes of docu-
mentation, medical research, discussion with colleagues, and training of
young surgeons. Current literature on automatic action recognition for
endoscopic surgery covers domains where surgeries follow a standardized
pattern, such as cholecystectomy. However, there is a lack of support in
domains where such standardization is not possible, such as gynecologic
laparoscopy. We provide ActionVis, an interactive tool enabling surgeons
to quickly browse endoscopic recordings. Our tool analyses the results
of a post-processing of the recorded surgery. Information on individual
frames are aggregated temporally into a set of scenes representing fre-
quent surgical actions in gynecologic laparoscopy, which help surgeons
to navigate within endoscopic recordings in this domain.
Keywords: Endoscopic video · Visualization · Temporal aggregation
1
Introduction
Innovations in medical technology, such as minimally invasive surgery methods
(MIS), beneﬁt patients by allowing for reduced postoperative pain and hospital
stay [3]. Moreover, MIS show an unexploited potential for beneﬁting surgeons
by post-processing recordings of endoscopic surgeries. In current practice, the
procedure of revisiting endoscopic multimedia material is to locate the videos
of interest within the video database and browse through hours of endoscopic
recordings manually. These videos only contain a fraction of content that is inter-
esting to medical experts. Hence, manually browsing the whole video is a tedious,
time-consuming task that is not supported by current tools. Yet, revisiting sur-
gical interventions beneﬁts several groups of medical personnel. Figure 1 shows
three diﬀerent groups of medical personnel that could beneﬁt from automatic
support when revisiting endoscopic surgery recordings: surgeons who perform
the surgery, their colleagues, and medical students. Surgeons, who conduct the
intervention themselves, are enabled to provide a more detailed surgery docu-
mentation and the possibility to add video sequences of interest. Our tool saves
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 348–351, 2018.
https://doi.org/10.1007/978-3-319-73600-6_30

ActionVis: An Explorative Tool to Visualize Surgical Actions
349
Fig. 1. ActionVis use case description. The tool is intended to be used by surgeons
who perform the surgery, their colleagues, and medical students.
time for the task of locating such sequences and thus supports a complete and
precise documentation of every case. Moreover, such a rigorous documentation
is a basic step towards advances in medical research, such as improvement of
surgical techniques. Such accompanying multimedia material furthermore yields
beneﬁts for team meetings with colleagues. Moreover, surgeons, who have to
take over a speciﬁc patient for post-operative care or follow-up surgeries, are
enabled to reassess the patients’ last interventions more easily. Eventually, such
pre-selected sequences provide support for the training of medical students. Cur-
rent literature, approaches the recognition of surgery phases for domains that
have standardized operation routines, such as cholecystectomy [2,6] or cataract
surgery [1,5]. In contrast to the aforementioned use-cases, the domain of gyneco-
logic laparoscopy generally does not allow for precisely deﬁned surgical phases.
Thus, we base our work on classiﬁcation into medically motivated semantic con-
tent classes, i.e. surgical actions, as proposed in our previous work [4]. This work
shows how single-frame predictions can be temporally aggregated and visual-
ized, in order to provide an overview on the surgical actions of a single surgical
intervention allowing fast navigation within recordings of endoscopic surgeries.
2
System Description
ActionVis is designed as a web-based visualization tool for surgical actions in
gynecologic laparoscopy. Thus, it shall display the video and important meta-
data about the surgical actions. We pre-compute CNN classiﬁcation probability
vectors using a model from [4]. We store predictions for every video frame and
the semantics of the individual classes in a JSON ﬁle. We base the visualization
for an individual playback position on the following adjustable window-based
temporal (average) aggregation: p(x) = 
f∈W α(f) ∗p′
f ,where p denotes the
processed vector of class probabilities for a frame on playback position x, a set of
frames (window) W (corresponding to x), a weight coeﬃcient α which depends
on the position of f relative to x, and the predicted vector of class probabilities

350
S. Petscharnig and K. Schoeﬀmann
Fig. 2. ActionVis user interface screen shot with points of interest.
p′
f for frame f. For the window-based calculations, ActionVis provides three kinds
of aggregation windows: backward, forward, and center. In a backward window
the current frame is the last frame of the window, whereas in the forward win-
dow the current frame is the ﬁrst window frame. Within center windows, the
current frame is in the center of the window. For the sake of simplicity, the
window’s size (without the current frame) is restricted to be a multiple of 2,
such that the current frame has the same number of frames on its right as on its
left. Weights deﬁne how much individual frames inﬂuence the prediction with
respect to their position relative to the current frame. ActionVis supports linear,
quadratic, and cubic weights. When using linear weights, each frame in the win-
dow contributes the same amount to the prediction for the window. Considering
the fact that past and future frames are not necessarily showing the action of
the current frame, quadratic and cubic weights allow for ﬂexible weighting set-
tings of frames near to the current one. This processing step provides a smooth
transition of class probabilities from frame to frame, which is our assumption
for the following step. We aggregate the single frames to scenes with a simple
thresholding scheme which is explained in the following. We consider the maxi-
mum probability of an individual frame (starting at the beginning of the video)
and check whether is exceeds a manually set threshold (t1). If not, we conclude
that there is not enough evidence for a surgical action visible. Otherwise, this is
the ﬁrst frame of the corresponding action class. Until the ﬁrst frame that falls
under a second threshold (t2), consecutive frames are labeled as the respective
class. Please note that t2 should not be lower than 1−t1, because we then poten-
tially miss new scenes. A screen shot of the user interface is given in Fig. 2. We
direct the reader to the following points of interest regarding the UI. The Video
Panel (1) shows the video to the user. The recognized class is shown in the Class
Panel (2). For more detailed information on the prediction, the Class Histogram
(3) shows conﬁdences of the prediction system. The Class Legend (4) visual-
izes the colors used for the respective classes. Standardized Video Controls (5)
allow for controlling the video playback. Its main components are a play/pause
button and a slider where the playback position can be altered. Classiﬁcation

ActionVis: An Explorative Tool to Visualize Surgical Actions
351
Overview (coarse) (6) gives a coarse overview on the surgical actions within the
whole surgery video. The bar is colored in the respective action colors for recog-
nized actions. The left side of the bar stands for the video start, the right side
for the end. Thus the coarse overview bar visualizes the whole intervention. A
black cursor indicates the actual playback position. Clicking on the bar jumps
to the respective position in the video. The Classiﬁcation Overview (ﬁne) (7) is
a dynamic bar showing the predicted classes in a 5 second window around the
current playback time. The current playback time is indicated by a black cursor,
which is positioned statically in the middle of the bar. Parameter Settings (8)
allow for exploratory changes to the recognition process. ActionVis allows for
changing the recognition threshold t1, the end of prediction threshold (t2), the
window size (how much frames in a window around the current playback posi-
tion aﬀect the prediction), weight (how the frames in the window inﬂuence the
recognition), and window alignment (prediction forwards, backwards, or both).
The recalculate button initiates recalculation of the whole prediction which can
be a time-consuming task considering surgeries lasting for hours.
3
Conclusions
ActionVis provides surgical action detection and temporal segmentation within
endoscopic recordings in the domain of laparoscopic gynecology by smoothing
and temporal aggregation of predictions for individual frames. This demonstra-
tion supports use cases such as surgery documentation, medical research, infor-
mation exchange with medical colleagues, as well as training of young surgeons.
While we evaluated how good individual frame predictions match to the surgical
action visible [4] for selected actions, we declare the evaluation of the surgical
action detection and segmentation from full surgery videos as future work.
References
1. Hajj, H.A., Lamard, M., Charri`ere, K., Cochener, B., Quellec, G.: Surgical tool
detection in cataract surgery videos through multi-image fusion inside a convolu-
tional neural network. In: 2017 39th Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC), pp. 2002–2005, July 2017
2. Loukas, C., Nikiteas, N., Schizas, D., Georgiou, E.: Shot boundary detection in
endoscopic surgery videos using a variational bayesian framework. Int. J. Comput.
Assist. Radiol. Surg. 11(11), 1937–1949 (2016)
3. McCrory, B., LaGrange, C.A., Hallbeck, M.: Quality and safety of minimally invasive
surgery: past, present, and future. Biomed. Eng. Comput. Biol. 6, 1 (2014)
4. Petscharnig, S., Sch¨oﬀmann, K.: Learning laparoscopic video shot classication for
gynecological surgery. Multimedia Tools Appl. 1–19 (2017). https://doi.org/10.
1007/s11042-017-4699-5
5. Quellec, G., Lamard, M., Cochener, B., Cazuguel, G.: Real-time segmentation and
recognition of surgical tasks in cataract surgery videos. IEEE Trans. Med. Imaging
33(12), 2352–2360 (2014)
6. Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., de Mathelin, M., Padoy,
N.: Endonet: a deep architecture for recognition tasks on laparoscopic videos. IEEE
Trans. Med. Imaging 36(1), 86–97 (2017)

AR DeepCalorieCam: An iOS App for Food
Calorie Estimation with Augmented Reality
Ryosuke Tanno(B), Takumi Ege, and Keiji Yanai(B)
Department of Informatics, The University of Electro-Communications,
1-5-1 Chofugaoka, Chofu-shi, Tokyo 182-8585, Japan
{tanno-r,ege-t,yanai}@mm.inf.uec.ac.jp
Abstract. A food photo generally includes several kinds of food dishes.
In order to recognize multiple dishes in a food photo, we need to detect
each dish in a food image. Meanwhile, in recent years, the accuracy of
object detection has improved drastically by the appearance of Convolu-
tional Neural Network (CNN). In this demo, we present two automatic
calorie estimation apps, DeepCalorieCam and AR DeepCalorieCam, run-
ning on iOS. DeepCalorieCam can estimate food calories by detecting
dishes from the video stream captured from the built-in camera of an
iPhone. We use YOLOv2 [1] which is the state-of-the-art object detec-
tor using CNN, as a dish detector to detect each dish in a food image,
and the food calorie of each detected dish is estimated by image-based
food calorie estimation [2,3]. AR DeepCalorieCam is a combination of
calorie estimation and augmented reality (AR) which is an AR version
of DeepCalorieCam.
Keywords: Food calorie estimation · Object detection
Food image recognition · Convolutional Neural Network (CNN)
Augmented reality (AR)
1
Introduction
In recent years, due to growing of health consciousness, various food photo recog-
nition applications for recording everyday meals have been proposed. Although
some applications use image-based classiﬁcation for estimating food categories,
most of them do not leverage object detection. This means that human assistance
is necessary to detect dishes individually for the case that multiple dishes are
included in a food photo. In fact, we often encounter the situation with multiple
food dishes.
On the other hand, in the ﬁeld of image recognition, a lot of methods using
CNN have achieved various improvements. Currently, its applied technology is
used for various applications. Therefore, in this demo, we use YOLOv2 [1] which
is the state-of-the-art object detection system using CNN, to detect food dishes
from a food photo. YOLOv2 is the latest system on object detection using CNN,
and achieved high-speed and highly accurate detection. Since YOLOv2 outputs
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 352–356, 2018.
https://doi.org/10.1007/978-3-319-73600-6_31

AR DeepCalorieCam: An iOS App
353
rectangular bounding boxes of objects with class labels, it is possible to aware
individual objects of the same category. In case of a food photo of multiple
dishes, this means it is possible to estimate each dish area in the whole image.
Therefore, if we combine a system that works for a single-label image and object
detection, it is possible to recognize more detail from a detected rectangular area
of each object.
In this demo, we present two automatic calorie estimation app, DeepCalo-
rieCam and AR DeepCalorieCam, running on iOS. Ege and Yanai [2] proposed
image-based food calorie estimation which is simultaneous estimation of food
categories and calories for food photos. However, this is limited to food photos
which contains only one dish, so it is eﬀective to combine with this dish detec-
tor for estimating food calories from food photos of multiple dishes. Figure 1
shows an example usage of DeepCalorieCam which is running on an iPhone 7
Plus. In addition, AR DeepCalorieCam is a combination of calorie estimation
and augmented reality (AR) instead of detecting dishes as shown in Fig. 2. By
annotating calories of foods on the AR space, the system can recognize all the
foods in the AR space by moving a smartphone.
Fig. 1. DeepCalorieCam
Fig. 2. AR DeepCalorieCam
2
Proposed System
2.1
DeepCalorieCam
We estimate food calories from food photos according to [2,3]. They proposed
image-based food calorie estimation which is simultaneous estimation of food
categories and calories for food photos. They collected calorie annotated recipe
data from the online cooking recipe sites, and trained CNN that output food
calories directly from a food photo that contained only one dish.
The network they use for food calorie estimation is based on VGG16 [4]. The
fc6 layer is shared by both tasks, and the fc7 layer is branched to each task, so
that each task has the fc7 layer and the output layer independently. The food
calorie estimation task has the fc7 layer with 4096 dimension and an output-layer

354
R. Tanno et al.
composed of one unit which outputs food calorie. The food categorization task
has the fc7 layer with 4096 dimension and an output layer composed of units
corresponding to each category.
However, considering the mobile implementation, we think that VGG16 is
not suitable for the following reasons. (Please also see Fig. 4 which shows the
relationship between Inference time on iPhone 8 Plus, ImageNet Top-1 Accuracy
and the size of the model weights expressed in the size of circles.)
1. The size of the model weights (553 MB) is too large for mobile implementa-
tion.
2. The inference time is longer.
In particular, for mobile implementation, the memory capacity and processing
time of the device is an important factor in implementing deep learning. Also,
from the research of [2,3], it is suggested that the result of calorie estimation
tends to depend on classiﬁcation accuracy. For this time, we decided to use
inception-v3 [5] which has the lighter memory, faster inference and higher clas-
siﬁcation accuracy.
Figure 3 shows a ﬂow of dish detection and calorie estimation system. We
combine this food calorie estimation network and YOLOv2 [1] for food photos
of multiple dishes. Firstly, we extract bounding boxes of food dishes by YOLOv2
from a food photo of multiple dishes, and obtain a cropped image corresponding
to each bounding box which contains only one dish. Then, we provide the cropped
dish images to the food calorie estimation network one by one. Finally, the total
amount of food calories are calculated from food calories estimated for all the
cropped dishes.
Fig. 3. Flow of dish detection and calorie estimation

AR DeepCalorieCam: An iOS App
355
Fig. 4. Inference time on iPhone 8 Plus, ImageNet Top-1 accuracy and the size of the
model weights expressed as the size of circles.
2.2
AR DeepCalorieCam
We propose a new food recognition method using AR technology. Since AR
DeepCalorieCam omits YOLOv2 which performs dish detection, it recognizes
only one food at the same time. However, since the recognition results remain
in the 3D AR-view space, we realized sequential multiple food recognition by
moving a smartphone.
3
Demo Video
We prepared the videos recorded that DeepCalorieCam and AR DeepCalo-
rieCam app were running in the two kinds of the settings.
– DeepCalorieCam
https://www.youtube.com/watch?v=FOho7WyntOw
– AR DeepFoodCam
https://www.youtube.com/watch?v=iXJL8OO4Eqg
– Project Pages
http://foodcam.mobi/deepcaloriecam/
We will release DeepCalorieCam and AR DeepCalorieCam at iOS App Store by
the time of the MMM2018 conference.
References
1. Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)
2. Ege, T., Yanai, K.: Simultaneous estimation of food categories and calories with
multi-task CNN. In: Proceedings of ACPR International Conference on Machine
Vision Applications (MVA) (2017)

356
R. Tanno et al.
3. Ege, T., Yanai, K.: Estimating food calories for multiple-dish food photos. In: Pro-
ceedings of Asian Conference on Pattern Recognition (ACPR) (2017)
4. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image
recognition. In: Proceedings of International Conference on Learning Representa-
tions (2015)
5. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the Inception
Architecture for Computer Vision. In: Proceedings of arXiv:1512.00567 (2015)

Auto Accessory Segmentation and Interactive
Try-on System
Yi-Xuan Zeng(&), Yu-Hang Kuo, and Hsu-Yung Cheng
National Central University, No. 300, Jhongda Rd., Jhongli City,
Taoyuan County, Taiwan
105522015@cc.ncu.edu.tw
Abstract. The convenience and diversity of online shopping makes many
consumers willing to buy apparel or accessories on the web. In order to make
products more attractive to users, many virtual try-on systems are developed for
e-commerce applications. This paper proposes an interactive virtual try-on
system combined with automatic accessory segmentation. Our system auto-
matically retrieves the hat from images and store them in the try-on system to
provide users with subsequent selection. When a user selects the hat that he or
she wants to try on, the hat is placed on the proper position of the user in the
image. In the stage of accessories segmentation, we perform background
elimination and super-pixel segmentation. According to the color information on
the hat image, the feature vector generated by the color histogram is used to
select super-pixels that belong to the accessories. In the stage of try-on system,
we use Kinect, which provides skeleton information, to track the user’s face and
gestures. When a user selects the hat, the proposed system reads the corre-
sponding hat information and places the hat in the appropriate location based on
the results of the face tracking. The proposed try-on system can reach 30 fps
real-time speed in a personal computer.
Keywords: Image segmentation  Try-on system  Kinect
1
Introduction
The main purpose of this paper is to develop an interactive virtual try-on system using a
single accessory image and an image with a model wearing the accessory. In this paper,
we focus on hats as the accessory to construct the platform. A user can select the hat he
or she wants to try on. The try-on system analyzes the pose of the user and place the hat
in the appropriate location based on the results of the face tracking. The system
framework is divided into two parts. The ﬁrst part is accessary segmentation and the
second part is the try-on system. The goal of the accessary segmentation is to auto-
matically extract the hat from the image with a model wearing the hat, as shown in
Fig. 1. The proposed accessory segmentation system and try-on system are illustrated
Electronic supplementary material The online version of this chapter (https://doi.org/10.1007/
978-3-319-73600-6_32) contains supplementary material, which is available to authorized users.
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 357–361, 2018.
https://doi.org/10.1007/978-3-319-73600-6_32

in Figs. 2 and 3, respectively. The details of these two sub-systems are elaborated in
Sects. 2 and 3, respectively.
2
Accessory Segmentation
To achieve accessory segmentation, we analyze the color and texture information
obtained from the hat map. However, if we only use the color information to ﬁnd the
location of the hat on the image, the noises will be included in the segmentation results
when the background is complicated. Similarly, if we only use texture to ﬁnd the
location of hat, we cannot accurately extract the hat. As shown in Fig. 2, it ﬁrst searches
the color map of the hat area and capture the hat in the region of interest automatically
determined according to face analysis. This step can signiﬁcantly reduce the color
interference outside of the hat area and improve the accuracy of segmentation results.
2.1
Accessory Region Searching
Usually, we can assume that the area of the hat is above the face. First of all, we use
face detection [1] to ﬁnd the region of the face. The width of ROI’s left and right
boundary is 2.5 times from middle to face and the height of top boundary is 5 times
from middle to mouse and bottom boundary is to the mouth of the model. As shown in
Fig. 4, the blue rectangle is the automatically determined ROI. To deal with the face
Fig. 1. Accessory segmentation ﬂow chart
Fig. 2. (a) Image with a model wear-
ing the hat. (b) Segmented hat
Fig. 3. Try-on system ﬂow chart
Fig. 5. Coordinate
conversion
Fig. 6. ROI
modiﬁcation
Fig. 7. Super-pixel
selection
Fig. 4. ROI (Color
ﬁgure online)
358
Y.-X. Zeng et al.

rotation problem, we use face alignment with ensemble of regression tree [2] to
overcome the problem. We perform the coordinate conversion between camera coor-
dinate and image coordinate shown in Fig. 5. Finally, we can get the angle and modify
the ROI shown in Fig. 6.
In general, the background color within the ROI has a similar color distribution
with the background beyond the ROI and will not produce signiﬁcant faults at the ROI
boundary. So we can use the region outside of the ROI to establish background
Gaussian Mixture Models and apply Grabcut [3] algorithm to iteratively update the
background model. In this way, the pixels belonging to the background model in the
ROI can be removed.
2.2
Obtaining Color Features
A super-pixel refers to the sub-region of an image which has consistency and maintains
the local structure of the image. In comparison with a single pixel, super-pixels are
more convenient to extract local features and local structure. In this paper, we use
SEEDS [4] to merge super-pixels. We can control the maximum number of
super-pixels generated by SEEDS. Next, we use HSV color space to vote the histogram
by dividing Saturation and Hue into each 16 bins. By matching the color histograms of
the target hat and the histogram extracted from the super-pixels, we can get the seg-
mented hat image as shown in Fig. 7.
2.3
Edge Repairing by Matting and Guided Filter
After the above mentioned steps, there might be some noises on the segmented
accessory. Morphological operations can improve the contour, but the results could still
be not satisfying. In this paper, we use the global matting [5] and guided ﬁlter [6] to
improve the shortcomings of local sampling through global sampling.
3
Try-on System
The try-on system is divided into two parts, the menu mode is shown in Fig. 8(a) and
the try-on mode is shown in Fig. 8(b). When the user has not selected any hats, the
system is in the menu mode. When the user selects a hat, the system enters the try-on
mode. The information column shows the user’s head position, the selected hat
number, and the three-axis rotation vector of the user’s head. In addition, the image
with a model wearing the hat is displayed. When the user’s angle is similar to the
model, the system will automatically show the photo for the user and display it in the
lower right corner and save it, as shown in Fig. 8(c). The details are explained in the
following subsections.
Auto Accessory Segmentation and Interactive Try-on System
359

3.1
Skeleton Tracking
Kinect skeleton tracking captures the image by its infrared sensor. It provides us with
accurate detection results which is unaffected by lighting changing. We can get 25
joints points by Kinect.
3.2
Fingertip Detection and Gesture Identiﬁcation
We extract hand areas from the coordinates of the joints of the palm. The system
performs ﬁngertip detection according to the contour curvature and the distance from
contour boundaries to the center of the hand area. The gestures recognized by the
system include sliding up or down the menu, selecting a hat, switching back from
try-on mode to menu mode.
3.3
Face Alignment and Accessories Try-on
We obtain the head node and its expanded area by Kinect skeleton information. After
detecting the head ROI, we perform face detection. When the face cannot be detected,
the system uses the face detection result of the previous frame facilitate the subsequent
face contour point detection. Face tracking and alignment [2] is performed to get the
landmarks on the face, as shown in Fig. 9(a). With the information of the face land-
marks, we can obtain the height of the upper boundary of the hat from the eye center
hmodel, and the left and right boundary position Lmodel and Rmodel, as shown in Fig. 9(b)
and (d). Using the information of hmodel, Lmodel and Rmodel, the position of the segmented
accessory huser, Luser and Ruser can be determined, as shown in Fig. 9(c) and (e).
Fig. 8. Try-on system: (a) Menu mode (b) Try-on mode (c) Saved try-on images
Fig. 9. Examples of face alignment accessories try-on
360
Y.-X. Zeng et al.

4
Conclusions and Future Work
This paper realizes an auto accessory segmentation and interactive try-on system.
Without any additional human operation, hats can be segmented from the image with a
model wearing the hat. The proposed system automatically tracks the user’s ﬁngertips
to recognize the gestures of selecting a hat from the list. It also tracks the user’s face
information to adjust the size and position of the hat to be tried on. The proposed
system can successfully achieve the purpose of low-cost try-on systems to stimulate
users’ desire to purchase commercial products. For future works, we would like to
extend the try-on system for other accessories in additional to hats.
References
1. Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple features. In:
Computer Vision and Pattern Recognition 2001, Kauai, pp. 1–511 (2001)
2. Kazemi, V., Sullivan, J.: One millisecond face alignment with an ensemble of regression
trees. In: Conference on Computer Vision and Pattern Recognition, pp. 1867–1874. IEEE,
Columbus (2014)
3. Rother, C.: “GrabCut”—interactive foreground extraction using iterated graph cuts. Int.
J. Comput. Vis. 23(3), 309–314 (2004)
4. Van den Bergh, M.: SEEDS: superpixels extracted via energy-driven sampling. Int.
J. Comput. Vis. 3(111), 298–314 (2015)
5. He, K., Rhemann, C., Rother, C., Tang, X., Sun, J.: A global sampling method for alpha
matting. In: Conference on Computer Vision and Pattern Recognition, pp. 2049–2056. IEEE,
Colorado (2011)
6. He, K.: Guided image ﬁltering. IEEE Trans. Pattern Anal. Mach. Intell. 35(6), 1397–1409
(2013)
Auto Accessory Segmentation and Interactive Try-on System
361

Automatic Smoke Classiﬁcation
in Endoscopic Video
Andreas Leibetseder(B), Manfred J¨urgen Primus, and Klaus Schoeﬀmann
Institute of Information Technology,
Alpen-Adria University, 9020 Klagenfurt, Austria
{aleibets,mprimus,ks}@itec.aau.at
Abstract. Medical smoke evacuation systems enable proper, ﬁltered
removal of toxic fumes during surgery, while stabilizing internal pres-
sure during endoscopic interventions. Typically activated manually, they,
however, are prone to ineﬃcient utilization: tardy activation enables
smoke to interfere with ongoing surgeries and late deactivation wastes
precious resources. In order to address such issues, in this work we
demonstrate a vision-based tool indicating endoscopic smoke – a ﬁrst
step towards automatic activation of said systems and avoiding human
misconduct. In the back-end we employ a pre-trained convolutional neu-
ral network (CNN) model for distinguishing images containing smoke
from others.
Keywords: Endoscopy · Smoke detection · User interface
1
Introduction
Endoscopy, a widespread form of minimally invasive surgery (MIS), is conducted
via inserting a camera – the endoscope – as well as various instruments into a
patient’s body, often through purposefully created apertures but also using nat-
ural oriﬁces. Interventions performed in this way are observed on an external
monitor displaying the camera feed, enabling operating physicians to apply nec-
essary treatments. Most endoscopic surgeries require some kind of dissection,
which can involve severing organs or simply separating tissue. To prevent such
actions from creating bleeding wounds or even simply to help accelerating natu-
ral hemostasis, cauterization instruments are used for cutting and sealing bodily
regions. Burning tissue, however, introduces another undesirable side eﬀect –
surgical smoke.
Emerging smoke during endoscopic interventions not only constitutes a seri-
ous health hazard to patients as well as surgical staﬀ, it also very likely impedes
ongoing treatment procedures. In order to prevent such deﬁciencies, present-day
medical institutions employ evacuation systems that allow for properly dispos-
ing smoke and at the same time prevent pressure loss by introducing surgical
gas. The circumstance that these systems usually are activated manually leaves
much room for ineﬃciencies: tardy activation enables smoke to interfere with the
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 362–366, 2018.
https://doi.org/10.1007/978-3-319-73600-6_33

Automatic Smoke Classiﬁcation in Endoscopic Video
363
surgery and late deactivation wastes precious resources. In this work, therefore,
we demonstrate a vision-based attempt for automatically detecting smoke by
utilizing a pre-trained convolutional neural network (CNN) model, which ulti-
mately could alleviate the medical staﬀfrom having to react to emerging smoke.
Fig. 1. Smoke detection tool architecture (Abdominal laparoscopy illustration by
Blausen.com staﬀ(2014). “Medical gallery of Blausen Medical 2014”. WikiJournal
of Medicine 1 (2). https://doi.org/10.15347/wjm/2014.010. ISSN 2002-4436.)
The demonstrated tool, schematically depicted in Fig. 1, performs a binary
classiﬁcation task on endoscopic images or videos: does the current input contain
smoke or not? Corresponding media can be retrieved from several sources and
a color-coded smoke indication portrays the conﬁdence of the underlying CNN
model. We thus formulate our contributions in the following manner:
– Visualizing smoke detection in real-time via the CNN model developed in [1]
– Oﬀering a variety of inputs for live classiﬁcation: stored image/video ﬁles and
captured external video signals from endoscopes, cameras or other devices.
Since corresponding research is not yet in a state that allows for applying all
ﬁndings to medical grade systems (cf. [1,2]), the goal of this demonstration is
merely to outline the feasibility for automatic smoke evacuation systems. Fur-
thermore, besides real-time analysis the presented methodology would as well
possibly be an interesting addition for surgical video post-processing: burning
tissue scenes can signify key points for patient case revisitations, hence, visually
indicating video segments showing smoke could greatly alleviate a physician’s
administrative eﬀort. The remainder of this article describes our tool in detail.
2
Smoke Detection Tool
At its core the presented tool utilizes Google’s GoogLeNet (Inception v3) [3]
model, which has been trained on 1000 classes from ImageNet and ﬁne-tuned

364
A. Leibetseder et al.
to perform binary smoke classiﬁcation using 20K speciﬁc endoscopic images, as
more thoroughly described in [1]. The resulting net is used for binary classify-
ing images by employing Berkley AI’s popular deep learning framework Caﬀe1
with GPU-acceleration. We build our prototype for a workstation with following
hardware specs: Intel Core i7-5820K CPU @ 3.30 GHz × 6, 32 GiB DDR3 @ 1333
MHz, Nvidia GeForce GTX Titan X. 6.
Fig. 2. Smoke detection tool interface (Color ﬁgure online)
2.1
Implementation
The application is implemented in Python2 using Pyside3 for handling all GUI
elements, OpenCV4 for processing media and PyCaﬀe5 for operating the CNN
model predicting smoke. For the most part the tool consists of a main thread
managing user inputs together with a capture thread conducting video playback
1 http://caﬀe.berkeleyvision.org.
2 https://www.python.org.
3 https://wiki.qt.io/PySide.
4 https://opencv.org.
5 https://github.com/BVLC/caﬀe/blob/master/python/caﬀe/pycaﬀe.py.

Automatic Smoke Classiﬁcation in Endoscopic Video
365
and frame extraction and a classiﬁcation thread producing CNN-releated predic-
tion conﬁdences. Since classifying an image requires about 100–150 ms (cf. [1,2]),
despite exploiting GPU-acceleration, for video streams not every frame is classi-
ﬁed, i.e. a frame is dropped in case the classiﬁcation thread is busy resulting in
predictions for about every second to third frame. This measure only marginally
impacts real-time classiﬁcation because emerging smoke does not ﬂuctuate much
across few frames or in other words: there are slim chances of ﬁnding relevant
smoke sequences with fewer than 5 frames. Finally, in order to smooth out the
tool’s visual classiﬁcation feedback CNN predictions for streams are calculated
over a sliding window of 10 frames.
2.2
Interface
A complete view of the tool’s interface is portrayed in Fig. 2 and divided into
seven sections. Section 1 and 4 oﬀer a user multiple possibilities of specifying
the input: “Select Media” or “File -> Open” are used to browse for image or
video ﬁles on locally accessible drives. “Select Source” enables exploring the
available streaming sources, such as webcams or devices attached to a capture
card. Section 3 indicates the type of source selected: image, video or stream. The
display window of section 6 shows the current image or video stream being classi-
ﬁed. Additional video controls provided in section 7 enable a user to play/pause
a prerecorded video, quickly navigate through it via a scrollbar and change the
playback speed, while as well displaying the current position. Optionally, the
checkbox in section 5 oﬀers a color correction for streaming input devices trans-
mitting color channels in RGB order instead of BGR. Finally, section 2 displays
the CNN model’s conﬁdence for smoke being contained in the displayed media of
section 6. The indication is color-coded: green marks no smoke or a low, yellow
a medium and red a high conﬁdence for smoke. Additionally the last image’s
classiﬁcation time in milliseconds is displayed.
3
Conclusion
We present a tool for visualizing image-based smoke detection using a convolu-
tional neural network for making real-time predictions. The tool oﬀers a variety
of inputs including stored image/video ﬁles and captured streams from various
devices. Our goal is to demonstrate the feasibility of automatically activating
medical grade smoke evacuation devices upon detecting its emergence. We fur-
thermore deem smoke visualization useful for post-surgery inspection of endo-
scopic videos, as identifying actions producing smoke could likely aid physicians
in ﬁnding relevant scenes during case revisitations.
Acknowledgements. This work was supported by Universit¨at Klagenfurt and Lake-
side Labs GmbH, Klagenfurt, Austria and funding from the European Regional Devel-
opment Fund and the Carinthian Economic Promotion Fund (KWF) under grant KWF
20214 u. 3520/26336/38165.

366
A. Leibetseder et al.
References
1. Leibetseder, A., Primus, M.J., Petscharnig, S., Schoeﬀmann, K.: Image-based smoke
detection in laparoscopic videos. In: Cardoso, M.J., et al. (eds.) CARE/CLIP -2017.
LNCS, vol. 10550, pp. 70–87. Springer, Cham (2017). https://doi.org/10.1007/978-
3-319-67543-5 7
2. Leibetseder, A., Primus, M.J., Petscharnig, S., Schoeﬀmann, K.: Real-time image-
based smoke detection in endoscopic videos. In: ACM Multimedia Conference on
Multimedia 2017 - Thematic Workshops, Mountain View, CA, USA (2017)
3. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9 (2015)

Depth Representation of LiDAR Point Cloud
with Adaptive Surface Patching for Object
Classiﬁcation
Kanokphan Lertniphonphan(&), Satoshi Komorita, Kazuyuki Tasaka,
and Hiromasa Yanagihara
KDDI Research, Inc., 2-1-15 Ohara, Fujimino-shi, Saitama, Japan
le-kanokphan@kddi-research.jp
Abstract. Object segmentation and classiﬁcation from point cloud light
detection and ranging (LiDAR) are increasingly important in 3D mapping and
autonomous mobile systems. Even though the distance measurement and object
localization from laser pulses are accurate and robust to environmental varia-
tions better than an image, the reﬂected points in each frame are sparse and lack
semantic information. The appropriate representation that can extract object
characteristics from a single frame point cloud is important for segmenting a
moving object before it makes a trail in the reconstruction. We propose depth
projection and an adaptive surface patch to extract and emphasize shape, curve,
and some texture of the object point cloud for classiﬁcation. The projection
plane is based on the sensor position to ensure that the projected image contains
ﬁne details of the object surface. An adaptive surface patch is used to construct
an object surface from a sparse point cloud at any distance. The experimental
results indicate that the object representation can be used to classify an object by
means of an existing image classiﬁcation method [1].
Keywords: LiDAR point cloud  Object detection  Object classiﬁcation
Depth representation  Depth projection  Computer vision
1
Introduction
Object classiﬁcation from a single frame point cloud is important in many approaches
especially in 3D reconstruction. In a terrestrial scenario, a reconstructed point cloud
might include trails from moving objects such as cars. Removing moving objects in
each frame before the reconstruction can solve the problem. However, objects in the
single frame point cloud are sparse and lack semantic information. For these reasons,
object segmentation and classiﬁcation from a point cloud is challenging.
There are several frameworks for object detection and classiﬁcation from a LiDAR
point cloud. Based on the point cloud characteristics, some researchers [2, 3] combine
the RGB image and point cloud for object detection. A depth map from the point cloud
is used in [2] to create a multimodal representation. In [3], multi-view data, which
consist of a bird’s eye view and front view features from a point cloud and RGB image,
are used to localize and detect the vehicle. However, depth mapping from the whole
point cloud consumes a lot of processing resources while the target objects occur in a
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 367–371, 2018.
https://doi.org/10.1007/978-3-319-73600-6_34

small set of points in the point cloud. Also, image information might be affected by
environmental variations such as illumination and weather.
There are approaches [4, 5], which use depth projection from a segmented object
point cloud to create an object representation. The projection is based on distance
between a candidate point and an estimated plane. In [4], 2D depth images using
principal component analysis (PCA) interpolating patches of point cloud are presented.
The approach in [5] also uses the depth image from the side view of the target object
for appearance-based object classiﬁcation. However, the projected points on the plane,
which are based on the point distribution of the object, contain noise from the surface
reﬂectivity and can be affected by occlusion and the connected blob problem. Also,
point overlapping, from the point projection and inappropriate interpolation, can cause
some loss of texture and shape information lost.
In our system, we propose an object point cloud representation by projecting depth
information on a 2D image. The projected plane is based on the sensor and the target
object position. The proposed adaptive surface patch is used to create the object surface
in the depth representation from the sparse point cloud. Our projection method and
surface reconstruction emphasize semantic information, which can be used to classify
the object category by the existing image object classiﬁcation framework.
2
Preprocessing
The preprocessing consists of ground removal and object segmentation. The point
cloud coordinate is in a 3D Cartesian coordinate system (XYZ) as shown in Fig. 1(a).
From Fig. 1(b), the terrestrial point cloud contains lots of objects, which may include
roads, footpaths, trafﬁc islands, cars, pedestrians, cyclists, walls, etc. Since a point
cloud has no object boundary or discriminating edge, we remove the ground plane to
separate objects. First, we apply region growing with a smoothness constraint to collect
plane regions. Then, the plane perpendicular to the z-axis is classiﬁed as the ground
plane by RANSAC [6] as shown in Fig. 1(c).
After ground removal as shown in Fig. 1(d), points of an object are close to each
other and can be easily grouped using the distance constraint. Euclidean distance is
applied to measure the distance between points. Based on our experiments, we set the
distance constraint to be equal to 0.1 m. Points, which are close to each other, are
grouped into the same cluster as shown in Fig. 1(e).
Fig. 1. (a) LiDAR coordinate system, (b) input point cloud, (c) ground plane, (d) point cloud
after ground removal, and (e) object point cloud with color indicating a different cluster. (Color
ﬁgure online)
368
K. Lertniphonphan et al.

3
Object Representation and Classiﬁcation
Object point cloud representation in our system is created by depth projection from a
segmented object point cloud onto a 2D image. From the point cloud and sensor
information, we obtain the position of each point pn(xn,yn,zn), sensor position s(xs,ys,zs),
and angular resolution on the vertical a axis and the horizontal b axis. For projection,
the system requires the position of the corresponding pixels gn(in,jn) on our projection
plane, and a surface patch w.
Our depth projection starts by ﬁnding a point that has the shortest distance to the
sensor on the xy plane as shown in Fig. 2(a). Given p0(x0,y0,z0) as an object point that
has the shortest distance R0 from s, the projected plane is perpendicular to vector R0 for
which the distance from s is equal to r0 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx0  xsÞ2 þ ðy0  ysÞ2


r
. For point pn,
which is the next shortest distance from s, the distance between pn and s is
rn ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðxn  xsÞ2 þ ðyn  ysÞ2


r
. The angle between r0 and rn is hn as shown in Fig. 2
(b). Given dn as the distance between pn and the projection plane, gn(in,jn) and intensity
dn of pn(xn,yn,zn) on the projection plane is computed by
dn ¼ rn cos hn
ð
Þ  r0
ð1Þ
in ¼ rn sin hn
ð2Þ
jn ¼ zn
ð3Þ
However, the point cloud is not dense enough to project on every pixel. The surface
patch is applied to create an object surface from point candidates as shown in Fig. 2(c).
For the object point cloud in the difference distance, we adjust the surface patch width
ww and height wh based on the distance between s and p0(x0,y0,z0) as represented by the
following equation.
wh ¼ r0  tan a
ð Þ
ð4Þ
ww ¼ r0  tan b
ð Þ
ð5Þ
Since the system segments objects before the projection, our target object size
occurs in a small area in the whole point cloud. To reduce the processing time, the
patch size is ﬁxed for every point within the segmented object point cloud. A Gaussian
ﬁlter is applied to smooth and reduce the noise of the depth image. The normalized
result of the projection is shown in the bottom image of Fig. 2(c).
For object classiﬁcation, faster R-CNN [1] is used to classify the depth represen-
tations into car, cyclist, pedestrian, and background.
Depth Representation of LiDAR Point Cloud
369

4
Experimental
A KITTI object dataset [7] is used for testing our system. The dataset contains 7,481
training samples. Since the testing dataset does not provide ground truth, we split the
training data in half; one half for training and the other half for validation. For training,
we segmented and gathered the target objects within the projection area of the provided
bounding boxes from 2D images while the other objects are used as the negative
training samples. Objects that are outside the sensor range or contain a small number of
points are excluded.
For evaluation, the overall number of extracted object point clouds for classiﬁcation
is 22,167 objects, containing 4,501 cars, 232 cyclists, and 767 pedestrians. The average
precision for car, cyclist, and pedestrian classiﬁcation is 89.61%, 53.38%, and 81.63%,
respectively. Our system can correctly detect and classify a point cloud object with
occlusion and missing points when it belongs to the car category. In the cyclist cate-
gory, most of the cyclists were mistakenly identiﬁed as pedestrians since these two
objects have many similar characteristics. Bicycles have thin structures that barely
reﬂect the sensor light. For some of the cyclist point clouds, only a cyclist without a
bicycle was reﬂected, which leads to low precision. However, we indicated that our
depth image can be used to detect some human postures and can be used to classify the
difference between walking and biking at some distances.
5
Conclusion
This paper presented depth projection of the segmented object point cloud for object
classiﬁcation. By taking the characteristics of the sensor and object point cloud into
consideration, the representation can extract object characteristics and can be applied to
the existing image object classiﬁcation framework. The experimental results indicate
that our depth representation can be used to accurately classify cars, cyclists, and
pedestrians with a mean average precision of 74.87%.
Fig. 2. Depth projection (a) a projection plane and a sensor position, (b) vector projection, and
(c) an object point cloud (top) and a depth projection (bottom).
370
K. Lertniphonphan et al.

References
1. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with
region proposal networks. In: Proceedings of NIPS (2015)
2. González, A., Vázquez, D., Lóopez, A.M., Amores, J.: On-board object detection: multicue
multimodal and multiview random forest of local experts. IEEE TSMC PP(99), 1–11 (2016)
3. Chen, X., Ma, H., Wan, J., Xia, T.: Multi-view 3D object detection network for autonomous
driving. In: Proceedings of CVPR (2017)
4. Deuge, M.D., Quadros, A., Hung, C., Douillard, B.: Unsupervised feature learning for
classiﬁcation of outdoor 3D scans. In: Proceedings of ACRA (2013)
5. Börcs, A., Nagy, B., Benedek, C.: Instant object detection in LiDAR point clouds.
IEEE GRSL 14(7), 992–996 (2017)
6. Fisher, M., Bolles, R.: Random sample consensus: a paradigm for model ﬁtting with
applications to image analysis and automated cartography. Commun. ACM 24(6), 381–395
(1981)
7. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI dataset. IJRR
32, 1231–1237 (2013)
Depth Representation of LiDAR Point Cloud
371

ImageX - Explore and Search Local/Private
Images
Nico Hezel(&), Kai Uwe Barthel, and Klaus Jung
Visual Computing Group, HTW Berlin - University of Applied Sciences,
Wilhelminenhofstraße 75a, 12459 Berlin, Germany
hezel@htw-berlin.de
Abstract. In this paper we present a system to visually explore and search large
sets of untagged images, running on common operating systems and consumer
hardware. High quality image descriptors are computed using activations of a
convolutional neural network. By applying normalization and a principal
component analysis of the activations compact feature vectors of only 64 bytes
are generated. The L1-distances for these feature vectors can be calculated very
fast using a novel computation approach and allows search-by-example queries
to be processed in fractions of a second. We further show how entire image
collections can be transferred into hierarchical image graphs and describe a
scheme to explore this complex data structure in an intuitive way. To enable
keyword search for untagged images, reference features for common keywords
are generated. These features are constructed by collecting and clustering
examples images from the web.
Keywords: Image exploration  Visualization  CBIR  Keyword search
1
Introduction
Taking and storing pictures has become an easy process due to the advances in mobile
technologies. For searching and exploring personal image collections only few
approaches are worth to be mentioned. In the consumer market most images are stored
without annotations, making it difﬁcult to search them semantically. Google, Microsoft
and Apple apply machine learning techniques to predict missing keywords. Google and
Microsoft require the images to be uploaded in the cloud to process them, which is a
data privacy concern. Apple runs algorithms directly on the device and tries to classify
the images with relatively few categories. We think classiﬁcation should only be done
in scenarios of unknown collections. If the user is aware of the images in the collection,
not ﬁnding particular images due to misclassiﬁcations is inexcusable. To avoid this
problem content based image retrieval can be applied. Instead of ﬁltering images, all
images are ranked by their similarity to a query image or a classiﬁcation concept.
To assist the tedious manual scanning process of search results or even the entire
image collection, various attempts have been made. Displaying long lists of images is
straining the eyes. On the other hand, graph based visualizations like Google Swirl [1]
do not utilize the screen space well and are hard to navigate.
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 372–376, 2018.
https://doi.org/10.1007/978-3-319-73600-6_35

In [2] we have proposed a system allowing visual navigation of millions of images
by using a hierarchical pyramid structure. www.picsbuffet.com is an example using this
approach. Such a scheme is good for static image collections but requires a complete
rearrangement if images are added or removed. To overcome this problem, we intro-
duced construction and visualization schemes for hierarchical image graphs [2]. These
graphs can be updated quickly, they allow to be ﬁltered by metadata and can be
displayed and navigated in a way similar to picsbuffet.
This paper introduces ImageX an image explorer wrapping these algorithms in an
end-user application. The goal is a general-purpose software which is compatible with
common operating systems and is still performant enough to handle thousands of local
images. It provides exploration capabilities superior to simple image lists with the help
of dynamic hierarchical image graphs and keyword search for untagged images. The
software runs locally, so there are no privacy issues.
2
ImageX
To be able to develop easy maintainable and multi-OS compatible software we use the
programming language Java despite its performance limitations. Handling large image
sets is challenging, therefore small and high-quality image descriptors are important. Like
many other researchers we rely on machine learning. A ResNet152 [3] trained on the
entire ImageNet data set [4] is utilized to generate feature vectors. The 2048 activations
before the fully connected layer were taken as initial feature vectors, based on their good
image retrieval properties [5]. Normalization further improves the retrieval quality.
Reducing the dimensionality to 64 with a PCA decreases the mean average precision
(MAP) [6]. Applying another normalization to the compressed features leads to even
higher MAP-values than the unprocessed 2048-dimensional original feature vectors, see
Fig. 1 for details. In order to store the values compactly we multiply them by 127, scale
them by 18, limit their range to −128 and +127 and cast them to bytes.
Fig. 1. MAP of the “Holiday” set [7] for different normalization and compression approaches
ImageX - Explore and Search Local/Private Images
373

3
Keyword Search for Untagged Images
A set of supported search terms is collected from Fotolia’s most frequent keywords. For
the top 20000 keywords we crawl Pixabay to collect corresponding images. The
compressed feature vectors are computed. For each keyword 10 image-clusters are
generated using the Generalized Lloyd algorithm. A cluster is represented by the
feature vector of the median image. Thus, each keyword is described by ten reference
features covering different common contexts. The size of the entire database is only
13 MB (= 20000 keywords * 10 feature vectors * 64 bytes). For a keyword search the
results from 10 queries of the reference features are aggregated. Due to this, small
features vectors and a fast similarity calculation are crucial.
Since Java has limited support for Single Instruction, Multiple Data (SIMD)
operations, we propose an algorithm to speed up the L1-distance computation by a
factor of four. The 64 bytes of a feature vector are packed into 8 longs such that the ﬁrst
byte occupies the least signiﬁcant bit of each long and the second byte the second least
signiﬁcant bit, etc. After the conversion the 64 bits in every long represent bits with the
same signiﬁcance from all 64 bytes and can therefore be processed equally. With the
help of Boolean algebra, the L1-distance can be computed by applying half- and
full-subtractors to the bits of the 8 long values (Fig. 2).
The result of the search process is visually arranged on a 2D grid using an improved
version of a Self-Sorting-Map algorithm. By sub-sampling this grid a pyramid like
structure is generated [2].
4
Constructing and Navigating an Image Graph
The presented system features a graph optimization background-thread building a
hierarchical image graph as described in [2]. All images in user deﬁned directories are
added to the graph after having been analyzed. While the distortion of the graph
gradually decreases over time it only takes a second to produce a sufﬁciently sorted
Fig. 2. L1-distance for 2  8 longs (X, Y) using logic gates. All operations work bitwise.
374
N. Hezel et al.

graph consisting of 100 images. The growing graph enables search queries and image
set modiﬁcations at any time.
Visualizing such an image graph becomes possible by dynamically querying the
graph images and projecting them onto a regular 2D plane according to their simi-
larities. Dragging the plane moves some images out of the viewport. New images are
retrieved from the graph and they are mapped to the most suitable empty places of the
map.
5
Interface
The user interface of the software is rather simple. First the user chooses the directories
that should be scanned and indexed. The corresponding images are analyzed and their
features are generated. At the same time the construction process of the image graph
begins. After a few seconds the user can switch to navigation mode and explore the
graph/images (Fig. 3). The displayed image grid is drag- and zoomable like Google
Maps or picsbuffet. A keyword search presents the most similar images arranged as a
2D map on the right and some interesting locations of this map on the left.
References
1. Jing, Y., et al.: Google image Swirl, a large-scale content-based image browsing system. In:
ICME, p. 267. IEEE Computer Society (2010)
2. Barthel, K.U., Hezel, N.: Visually exploring millions of images using image maps and graphs.
In: Big Data Analytics for Large-Scale Multimedia Search. Wiley, Hoboken (2018, to appear)
3. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 630–645.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0_38
4. Russakovsky, O., et al.: ImageNet large scale visual recognition challenge. Int. J. Comput.
Vis. 115(3), 211–252 (2015)
Fig. 3. ImageX UI. Left: the image indexing screen. Right: the image navigation screen.
ImageX - Explore and Search Local/Private Images
375

5. Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: CNN features off-the-shelf: an
astounding baseline for recognition. In: CVPR Workshops, pp. 512–519. IEEE Computer
Society (2014)
6. Babenko, A., Slesarev, A., Chigorin, A., Lempitsky, V.: Neural codes for image retrieval. In:
Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8689,
pp. 584–599. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10590-1_38
7. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometric consistency for
large scale image search. In: Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV 2008. LNCS,
vol. 5302, pp. 304–317. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-
88682-2_24
376
N. Hezel et al.

Lifelog Exploration Prototype in Virtual Reality
Aaron Duane(B) and Cathal Gurrin
Insight Centre for Data Analytics, Dublin City University, Dublin 9, Ireland
{aaron.duane,cathal.gurrin}@insight-centre.org
Abstract. Eﬃciently exploring large lifelog datasets is the subject of
much research in the lifelogging community. In this paper we describe
a pioneer lifelog interaction prototype developed for virtual reality. This
prototype was created as part of a larger research eﬀort to explore the
feasibility and potential of exploring visual lifelogs in virtual environ-
ments. In this paper we describe the prototype and its design.
Keywords: Virtual reality · Lifelog
1
Introduction
Dodge and Kitchin [1] refer to lifelogging as ‘a form of pervasive computing, con-
sisting of a uniﬁed digital record of the totality of an individual’s experiences,
captured multimodally through digital sensors and stored permanently as a per-
sonal multimedia archive’. The prevalence of modern technology and sensors
has enabled people to capture this digital trove of life experiences automatically
and continuously with newfound ease and eﬃciency [4] and ongoing research
is constantly optimising the user experience on these systems. However, virtual
reality, which has seen a recent resurgence in popularity due to advancements
in technology, has gone largely unexplored in terms of supporting lifelog access.
In this paper we describe a prototype lifelog exploration tool developed for the
HTC Vive which was built as part of a pilot study to investigate the feasibility
of lifelog exploration in virtual reality. The tool supports a user to query a large
visual lifelog in VR and to browse temporally-ordered result sets to ﬁnd items
of interest. To the best of our knowledge, this is the ﬁrst lifelog VR access tool.
When we consider the complex and multifaceted nature of lifelog datasets
in the context of virtual reality’s multidimensional axes of exploration, there
are numerous applications we can consider. For example, the SAS Institute [3]
states that we are limited to processing less than 1 kilobit of information per
second when reading from a screen yet the human optic nerve has an estimated
bandwidth of about 8 megabits per second. In virtual reality it becomes possible
to harness multiple axes and depths of information and we are able to fully utilise
our peripheral vision, providing new opportunities for immersive information
access.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 377–380, 2018.
https://doi.org/10.1007/978-3-319-73600-6_36

378
A. Duane and C. Gurrin
2
Dataset
The lifelog exploration prototype described in this paper was designed based
on a lifelog dataset [4] released for the NTCIR-12 conference [5] in Tokyo in
2016. This dataset consisted of 1000–1500 images per day generated by wear-
able cameras and captured by three lifeloggers (about one month each). These
images were further enriched by an automatic image classiﬁcation algorithm
which described the content of each image using keywords (e.g. laptop, coﬀee,
train, etc.) which we refer to in this paper as ‘lifelogging concepts’. Additionally
there were semantic locations and semantic activities, based from sensor read-
ings on mobile devices, but we did not employ these for this research. Associated
with the dataset were a number of topics (information needs) that formed the
basis for our experimentation.
3
Prototype
The primary function of the prototype is to support a user to query a lifelog
archive in the VR environment and to browse the result sets. In order to support
directed exploration, users of our VR lifelog prototype were given topics (from
the NTCIR-12 lifelog dataset) which required them to ﬁnd certain moments
that were known to exist in the collection. For example, moments in which the
lifelogger was getting onto a ﬂight or taking a photo of a lake.
Fig. 1. VR lifelog prototype - clipboard menu
The system allowed a user to generate faceted queries via a customised VR
query interface (described below) and then explore the search results (a set of
lifelog images). A typical query will consist of a selection of dates, a selection

Lifelog Exploration Prototype in Virtual Reality
379
of lifelogging concepts and an optional range of time (e.g. 9am to 6pm). To
accomplish this, the user interface is presented as a virtual menu in the virtual
environment and is divided into three sections: a date selector, a concept selector
and a time range selector. The interface can be presented to the user in two
diﬀerent orientations depending on their preference. One orientation is directly
attached to the user’s wireless controller and is interacted with by pointing the
opposing controller at the menu which will generate a blue beam that highlights
the interactive elements. We refer to this as the ‘clipboard’ menu (see Fig. 1). The
other orientation works in a similar fashion, however the interface is presented
at a distance in front of the user and in a magniﬁed form. We refer to this as
the ‘billboard’ menu (see Fig. 2).
Fig. 2. VR lifelog prototype - billboard menu
Once the user has chosen a selection of dates and a selection of lifelogging
concepts, the corresponding image results will appear in front of them at an
adjustable size. The user has the option at this point to further ﬁlter the results
by selecting a range of time. The image results can be explored by swinging
the wireless controllers left or right in a momentum-based scrolling mechanism.
An individual image can be further explored by pointing a controller directly at
the image and enabling the blue beam. This will reveal a tooltip in front of the
image result which displays the relevant metadata (see Fig. 3). Once the relevant
images for a topic are found, the user moves onto the next topic and begins the
search again.

380
A. Duane and C. Gurrin
Fig. 3. VR lifelog prototype - tooltip (Color ﬁgure online)
4
Conclusion
Our lifelog prototype was developed as part of a research eﬀort to explore the
feasibility and potential of virtual reality as a platform for visual lifelog explo-
ration. This was the ﬁrst prototype developed and was part of an evaluation to
compare the speed and eﬃciency of executing typical lifelog interaction queries
on a conventional platform compared to a virtual reality platform.
References
1. Dodge, M., Kitchin, R.: Outlines of a world coming into existence: pervasive com-
puting and the ethics of forgetting. Environ. Plann. B: Plann. Des. 34(3), 431–445
(2007)
2. Yang, Y., Lee, H., Gurrin, C.: Visualizing lifelog data for diﬀerent interaction plat-
forms. In: CHI 2013 Extended Abstracts on Human Factors in Computing Systems
on - CHI EA 2013, p. 1785 (2013)
3. SAS Institute. http://blogs.sas.com/content/sascom/2014/04/25/using-virtual-real
ity-understand-big-data/. Accessed Aug 2017
4. Gurrin, C., Joho, H., Hopfgartner, F., Zhou, L., Albatal, R.: NT-CIR lifelog: the
ﬁrst test collection for lifelog research (2016)
5. NTCIR-12. http://research.nii.ac.jp/ntcir/ntcir-12. Accessed Apr 2017

Multi-camera Microenvironment to Capture
Multi-view Time-Lapse Videos for 3D Analysis
of Aging Objects
Lintao Guo, Hunter Quant, Nikolas Lamb, Benjamin Lowit,
Natasha Kholgade Banerjee, and Sean Banerjee(B)
Clarkson University, Potsdam, NY 13699, USA
{linguo,quanthd,lambne,lowitbp,nbanerje,sbanerje}@clarkson.edu
Abstract. We present a microenvironment of multiple cameras to cap-
ture multi-viewpoint time-lapse videos of objects showing spatiotempo-
ral phenomena such as aging. Our microenvironment consists of four
synchronized Raspberry Pi v2 cameras triggered by four corresponding
Raspberry Pi v3 computers that are controlled by a central computer.
We provide a graphical user interface for users to trigger captures and
visualize multiple viewpoint videos. We show multiple viewpoint cap-
tures for objects such as fruit that depict shape changes due to water
volume loss and appearance changes due to enzymatic browning.
Keywords: Multi-camera · Time-lapse · Multiple viewpoint
Time-varying
1
Introduction
Natural and man-made objects exhibit time-varying transformations at varying
time scales. An ice cube melts in a few minutes when left outside, while a banana
darkens over several days, and an iron pipe corrodes over several months. While
there exist approaches to study time-varying changes to appearance in objects
[3,5,7,8], approaches to model 3D time-varying shape are limited to human faces
and bodies [1,2] or use turntables with single scanner setups that may introduce
unwanted deformations to the object [6]. As discussed in [4], we provide an
approach to reconstruct spatiotemporal 3D models of time-varying shape and
appearance changes in fruit using multi-view time-lapse videos. In this paper,
we describe our work on providing multi-camera microenvironments to capture
multi-view time-lapse videos.
The microenvironment images objects from multiple viewpoints using a
set of Raspberry Pi v2 cameras controlled by corresponding Raspberry Pi v3
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-319-73600-6 37) contains supplementary material, which is
available to authorized users.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 381–385, 2018.
https://doi.org/10.1007/978-3-319-73600-6_37

382
L. Guo et al.
computers. Our microenvironment uses modularized hardware to install the
Raspberry Pi cameras and computers that can be printed on a consumer grade
3D printer. We provide a Java graphical user interface to capture and visual-
ize multi-viewpoint time-lapse captures. Using the microenvironment, users can
perform time-lapse captures of tabletop objects showing shape and appearance
changes over time with customized lapse durations and capture lengths.
Fig. 1. Microenvironment to capture multi-view time-lapse images consisting of four
Raspberry Pi v2 cameras connected to four Raspberry Pi v3 computers.
2
Multi-camera Microenvironment
As shown in Fig. 1, our microenvironment consists of a 24”× 24” plywood base
with four 3D printed adjustable height camera towers. We modularize the cam-
era towers to allow the user to to raise or lower the height of the cameras by
adding or removing tower pieces. Each camera tower consists of a Raspberry Pi
v3 computer and a Raspberry Pi v2 camera. The Raspberry Pi computers are
connected to an Asus ESC4000-G3 central computer using a Gigabit Netgear
switch. We use the central computer to start and stop captures, and to store
and visualize the captured data.
The user interacts with the microenvironment using a graphical user interface
(GUI) on the central computer implemented using the Swing package in Java.
Our interface provides the user with a “Capture” and a “View” tab, as shown in
Fig. 2. To capture image sequences, the user navigates to the “Capture” tab. As
shown in Fig. 2(a), we allow the user to begin capturing image sequences after all
cameras have been tested and synchronized. During testing, we capture a sample
image to ensure the cameras are functional. We mark non-functional cameras in
red to enable rapid identiﬁcation and replacement. During synchronization, we
update the local time on each Raspberry Pi computer to the time on the central
computer to reduce time drift.

Multi-camera Microenvironment to Capture Multi-view Time-Lapse Videos
383
As shown in Fig. 2(b), the user speciﬁes the number of images and cap-
ture interval and presses “Capture” button on our interface to start a capture.
We send a parallelized capture signal to the four Raspberry Pi computers at the
user speciﬁed capture interval. We store the image captured by each Raspberry
Pi camera on its corresponding Raspberry Pi computer. We transfer all images
to the central computer before triggering the capture of images at the next time-
lapse instant. The capture stops once all images have been captured or if the
user interrupts the capture by click the “Stop” button.
Fig. 2. Java based graphical user interface to allow users to (a) test and synchronize
cameras, (b) set up a capture, and (c) visualize capture results.
To visualize a capture the user uses the “View” tab on the interface to load
time-lapse image sequences from the multiple viewpoints as shown in Fig. 2(c).
Once the capture is loaded, we provide the user with the number of images in
the capture sequence, and “Prev” (or ‘previous’) and “Next” buttons to navigate
through the image sequence. The user navigates to a picture by entering the
image index and pressing enter, by scrolling forward and backward using the
“Prev” and “Next” buttons, or by using the left and right arrow keys on the
keyboard. We provide keyboard shortcuts “X” and “D” to the user to export
and delete images from the capture sequence. Figures 3, 4, and 5 show multi-view
time-lapse captures for a banana, a green bell pepper, and a Gala apple.
Camera 1
Camera 2
Camera 3
Camera 4
Fig. 3. Banana blackening over one week at 5 min intervals.

384
L. Guo et al.
Camera 1
Camera 2
Camera 3
Camera 4
Fig. 4. Green bell pepper shrinking and reddening over ﬁve days at 5 min intervals.
(Color ﬁgure online)
Camera 1
Camera 2
Camera 3
Camera 4
Fig. 5. Apple browning over ﬁve days at 2 min intervals.
3
Future Work
In future work, we will integrate calibration of the geometric and photometric
properties of the cameras using a variety of calibration targets into the GUI. We
will also include controls for temporal operations found in traditional multimedia
viewers. To accommodate increasing camera counts in the microenvironment, we
will use network latency and shutter lag statistics to minimize diﬀerences in cap-
ture times at each frame for captures of phenomena occurring on the order of
seconds to a few minutes such as melting candles, cooking food, insect motions,
and movements of mechanical parts. Future work will include providing ﬁne-
grained control over individual camera parameters such as exposure compensa-
tion, white balance gains, region of interest, and ISO, and providing the option
to switch between JPEG ﬁles and the original RAW images.
Acknowledgements. This work was partially supported by the National Science
Foundation (NSF) grant #1730183.

Multi-camera Microenvironment to Capture Multi-view Time-Lapse Videos
385
References
1. de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.P., Thrun, S.: Perfor-
mance capture from sparse multi-view video. ACM Trans. Graph 27(3), 98 (2008)
2. Beeler, T., Hahn, F., Bradley, D., Bickel, B., Beardsley, P., Gotsman, C., Sum-
ner, R.W., Gross, M.: High-quality passive facial performance capture using anchor
frames. In: SIGGRAPH (2011)
3. Enrique, S., Koudelka, M., Belhumeur, P., Dorsey, J., Nayar, S., Ramamoorthi,
R.: Time-varying textures: deﬁnition, acquisition, and synthesis. In: SIGGRAPH
Sketches (2005)
4. Guo, L., Quant, H., Lamb, N., Lowit, B., Banerjee, S., Banerjee, N.K.: Spatiotem-
poral 3D models of aging fruit from multi-view time-lapse videos. In: MMM (2018)
5. Langenbucher, T., Merzbach, S., M¨oller, D., Ochmann, S., Vock, R., Warnecke, W.,
Zschippig, M.: Time-varying BTFs. In: CESCG (2010)
6. Li, Y., Fan, X., Mitra, N.J., Chamovitz, D., Cohen-Or, D., Chen, B.: Analyzing
growing plants from 4D point cloud data. ACM Trans. Graph 32(6), 157 (2013)
7. Lu, J., Georghiades, A.S., Glaser, A., Wu, H., Wei, L.Y., Guo, B., Dorsey, J., Rush-
meier, H.: Context-aware textures. ACM Trans. Graph 26(1), 3 (2007)
8. Sun, B., Sunkavalli, K., Ramamoorthi, R., Belhumeur, P.N., Nayar, S.K.: Time-
varying BRDFs. TVCG (3), 595–609 (2007)

Ontlus: 3D Content Collaborative Creation
via Virtual Reality
Chien-Wen Chen1, Jain-Wei Peng1, Chia-Ming Kuo2, Min-Chun Hu1(B),
and Yuan-Chi Tseng2
1 Department of Computer Science and Information engineering,
National Cheng Kung University, Tainan, Taiwan, ROC
ai281918@gmail.com, andersonpeng190@gmail.com, cazimy@gmail.com
2 Department of Industrial Design, National Cheng Kung University,
Tainan, Taiwan, ROC
p900372012@gmail.com, yuanchi.tseng@gmail.com
Abstract. 3D content creation is usually done by commercial software
like Maya and 3ds Max, or shareware like Blender. However, most of the
current 3D modeling software tend to edit the content via 2D screen,
which is not eﬃcient and intuitive. In this work, we develop a virtual
reality (VR) system, in which the user can not only create 3D content
easily and smoothly but also collaborate with other users in the VR
environment to accomplish their 3D design project. We use HTC VIVE to
display the immersive designing environment and realize the interaction
between the user and the 3D content. Through VIVE controllers, the
users can perform painting, sculpturing, coloring, transformation, and
multi-editor collaboration in the 3D virtual space. Compared with the
traditional 3D modeling software, directly editing 3D content in the 3D
VR environment is more user-friendly.
1
Introduction
VR technology has been applied in many ﬁelds, such as gaming, sports training,
education, medical cares, arts and industrial design. To create various 3D content
for diﬀerent applications, many 3D model editors (e.g., Maya, 3ds Max, Blender)
have been developed for users to modify the target content via 2D screen. How-
ever, it is not easy and intuitive to create 3D models via 2D screen for amateurs.
Researches have shown that editing 3D content in VR can dramatically reduce
the quantity of the entity prototype in the process of product design and evalu-
ation [1]. We hence aim to develop a VR system named Ontlus, which enables
the users to edit or create 3D content in an immersive virtual 3D space. Similar
to the currently existing VR systems like Tilt Brush [2], Blocks [3], and Oculus
Medium [4], the Ontlus system have the functionalities of painting, sculpturing,
coloring, and transformation. Through intuitive brush operations, the user can
quickly make the prototype of a 3D model, modify it in the VR environment,
and print the ﬁnal 3D output. In addition, multi-editor mode is provided for the
user to collaborate with others in the VR environment to accomplish their 3D
design project.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 386–389, 2018.
https://doi.org/10.1007/978-3-319-73600-6_38

Ontlus: 3D Content Collaborative Creation via Virtual Reality
387
(a)
(b)
(c)
Fig. 1. (a) Reconstructed 3D mesh by using the conventional Marching Cube algo-
rithm. (b) Reconstructed 3D mesh by using the modiﬁed Marching Cube algorithm.
(c) Paint the surface of the 3D model.
2
Methods
Our VR system is developed based on Unity engine and we implement our algo-
rithms and user interfaces with C#. HTC VIVE is used to display the immersive
designing environment and realize the interaction between the user and the 3D
content. Please refer to our demo video at https://goo.gl/xS6tZk. The system
implementation details are described as follows.
To realize a VR system capable of drawing arbitrary shapes in the 3D virtual
space, we divide the 3D virtual space into voxels of ﬁxed size to store the point
cloud information. According to the brush/eraser operation input by the user,
the vertex information around the position of the brush/eraser is updated in
real time and the corresponding 3D content is established immediately. Note
that applying conventional Marching Cube algorithm [5] to generate the 3D
mesh would result in lumpy surfaces as shown in Fig. 1(a). In this work, we
apply the concept of a modiﬁed 2D Marching Square algorithm [6] and extend it
to 3D Marching Cube for establishing a smooth 3D mesh as shown in Fig. 1(b).
When editing the 3D content, each vertex of the 3D model will be stored in
the corresponding voxel along with the color information. The color information
is updated in the same way as updating the vertex information (i.e., the system
updates the color of the voxels near the position of the controller). The ﬁnal color
is interpolated through the shader and rendered on the head-mounted display,
as shown in Fig. 1(c). We also design friendly and concise user interfaces (as
shown in Fig. 2) so that the user can easily know how to operate the editing
tools including brush, eraser, palette, shapes, straight line, particles, layers, and
object transformation. In the Brush and Eraser UI modes, the system provides
diﬀerent shapes of brushes/erasers for the user to create 3D content with high
ﬂexibility. In the Palette UI mode, the user can select color via the palette and
paint the surface of the 3D model. Or the user can select color by sampling from
part of the 3D model. The Transformation UI mode allows the user to move,
rotate, and adjust the scale of the model. The Straight Line UI mode allows the
user to form an accurate straight model by setting points. The user can also
create new layers to prevent modifying the previous content.

388
C.-W. Chen et al.
Our system also supports multi-editor mode for collaborative creation. By
simply setting the IP and Port information, multiple PCs can transmit action
messages of each user and multiple users can collaborate on their 3D design
project. As shown in Fig. 3, one of the computers is chosen to be the server and
the others will be the clients. All the action messages of the users are ﬁrst sent
to the server based on TCP/IP Protocol and enqueued to a buﬀer. The server
decides when to dequeue the action messages from the buﬀer and broadcasts
these messages to each client. With the aforementioned rendering and message
transmission mechanisms, our system can achieve 90 fps for each user without
noticeable latency. Figure 4(a) shows two users cooperating in the multi-user
mode.
(a)
(b)
(c)
(d)
Fig. 2. User interfaces of the proposed VR system. (a) Brush UI mode. (b) Eraser UI
mode. (c) Palette UI mode. (d) Straight Line UI mode.
3
Applications and Future Work
Our VR system can be used to build the prototypes of 3D design projects. More-
over, novices can practice sculpturing without fear of making wrong by using our
system. Figure 4(b) shows a 3D doll created by using the Ontlus system, and it
can be output directly for 3D printing. In the future, we will focus on automatic
rigging and animation based on the 3D content created by the user, and further
use the system for interactive storytelling and gaming.

Ontlus: 3D Content Collaborative Creation via Virtual Reality
389
Fig. 3. Our multi-editor collaboration system.
(a)
(b)
Fig. 4. (a) Multi-user cooperation for simultaneously editing. (b) A 3D model created
by using our system.
Acknowledgement. This research was supported by the Ministry of Science and
Technology (contract MOST 105-2511-S-006-020-MY3 and MOST 105-2221-E-006-066-
MY3), Taiwan.
References
1. Tariq, S.M., Tamas, S., Mohammed, S.J.H.: Virtual reality applications in manu-
facturing process simulation. J. Mater. Process. Technol. 155, 1834–1838 (2004)
2. Tilt Brush. https://www.tiltbrush.com
3. Blocks. https://vr.google.com/blocks
4. Oculus Medium. https://www.oculus.com/medium
5. William, E.L., Harvey, E.C.: Marching cubes: a high resolution 3D surface construc-
tion algorithm. In: ACM Siggraph Computer Graphics, pp. 163–169. ACM (1987)
6. Marching squares algorithm. http://catlikecoding.com/unity/tutorials/marching-
squares-2

Programmatic 3D Printing of a Revolving
Camera Track to Automatically Capture Dense
Images for 3D Scanning of Objects
Nikolas Lamb, Natasha Kholgade Banerjee, and Sean Banerjee(B)
Clarkson University, Potsdam, NY 13699, USA
{lambne,nkholgad,sbanerje}@clarkson.edu
Abstract. Low-cost 3D scanners and automatic photogrammetry soft-
ware have brought digitization of objects into 3D models to the level of
the consumer. However, the digitization techniques are either tedious,
disruptive to the scanned object, or expensive. We create a novel 3D
scanning system using consumer grade hardware that revolves a camera
around the object of interest. Our approach does not disturb the object
during capture and allows us to scan delicate objects that can deform
under motion, such as potted plants. Our system consists of a Raspberry
Pi camera and computer, stepper motor, 3D printed camera track, and
control software. Our 3D scanner allows the user to gather image sets for
3D model reconstruction using photogrammetry software with minimal
eﬀort. We scale 3D scanning to objects of varying sizes by designing our
scanner using programmatic modeling, and allowing the user to change
the physical dimensions of the scanner without redrawing each part.
Keywords: 3D scanning · 3D printing · Multi-view imaging
Photogrammetry
1
Introduction
3D models today are instrumental in steamlining tasks such as physics simula-
tions [3], animation of articulated characters [4], and preservation of historical
monuments [9]. While traditional 3D models have been manually designed, con-
sumer 3D scanners and automatic photogammetry software provide lower-cost
and more dimensionally accurate digitizations of complex or irregular objects.
Commonly available 3D scanners, such as the MakerBot Digitizer and Matter
and Form 3D scanner, use one or more stationary cameras to capture an object
rotating on a turntable [2]. Rigid or non-rigid motions of the object due to
vibrations of the turntable may yield an inaccurate 3D model. Instead of rotating
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-319-73600-6 39) contains supplementary material, which is
available to authorized users.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 390–394, 2018.
https://doi.org/10.1007/978-3-319-73600-6_39

Programmatic 3D Printing of a Revolving Camera Track
391
the object, we create a 3D scanner that revolves a camera around the stationary
object. Our approach prevents the scanner from disturbing the object during
the capture process, allowing our system to capture fragile or delicate objects,
such as the potted plant shown in Fig. 1.
Fig. 1. (a) Our 3D scanner showing Raspberry Pi 3B computer, Raspberry Pi v2
camera, and Nema 23 stepper motor, (b) Images of a plant captured by the scanner
using the camera, and (c) Reconstruction of the potted plant using Autodesk ReMake.
2
Related Work
While the emergence of consumer grade 3D scanners such as the Matter and
Form 3D scanner are recent phenomena, the technology that enables them has
been present and evolving for many decades [5]. Most contactless 3D scan-
ners rely on using either depth or color cameras to scan objects placed on a
turntable [8,10]. Commercial photogrammetry software, such as Autodesk 123D
Catch, Trnio, and Autodesk ReMake [1], allow users to capture and reconstruct
everyday objects with consumer grade cameras or mobile phones. While the
approaches provide low-cost digitizations, they require the user to manually cap-
ture a dense set of images from multiple angles. This task may prove tedious
for an average user as it often requires careful monitoring of image uniformity
and placement. Multi-camera approaches require dense arrays of 25 or more
RGB cameras for high ﬁdelity stereo matching, or when sparse require RGB-D
sensors that either need shaker rigs to reduce interference [7] or fail to capture
over short distances [6]. Our approach addresses the issues inherent in turntable
3D scanners by keeping the object of interest static while revolving the camera
around the object. It also circumvents the tediousness and imprecision of manual
capture by capturing and importing all images automatically.
3
Programatically Modeled 3D Printed Scanner
Our 3D scanner consists of a Raspberry Pi v2 camera and Raspberry Pi 3B
computer installed via an adjustable height tower on a circular camera track,
shown in Fig. 1. The track is made by connecting twelve segmented arc pieces
which are geared to interface with a Nema 23 stepper motor. The geared track

392
N. Lamb et al.
moves over ball bearings placed in bearing holders on a lower grooved support
track. We prevent the camera track from wandering during motion by using
guide clips and a double helical gear design for the geared arc and motor gear.
The motor is controlled by the general purpose input/output (GPIO) pins of
the Raspberry Pi computer via a bipolar stepper driver. The Raspberry Pi is
powered using a 2.5 A 5 V DC power supply, while the stepper motor is powered
using a 36 V DC power supply. The Raspberry Pi and the host desktop computer
which activates the Raspberry Pi are networked together using a Gigabit switch.
Fig. 2. Mechanical hardware of our 3D scanner in OpenSCAD. The mechanical hard-
ware consists of a double helical motor gear, geared revolving camera track, guide clip,
bearing holder, static lower track, stepper holder, and support feet.
We use a 3D printer to generate the mechanical parts of the 3D scanner, i.e.,
the double helical motor gear, the geared revolving camera track arcs, the guide
clip, the bearing holders, the support track, the stepper holder, and support feet.
As shown in Fig. 2, we design the parts using OpenSCAD, an open-source appli-
cation for programmatic 3D model creation. By programmatically modeling the
parts, we provide users with the ability to render scanners of varying dimensions
for scanning objects of varying size. Our approach allows the user to change the
radius, width, and heights of the upper and lower tracks, the diameter of the
friction bearings, the elevation of the platform riders, and the width of the step-
per motor by changing the variables in our OpenSCAD program. We modularize
the camera tower to allow the user to re-position the camera to image objects
of varying heights by adding or removing tower pieces.
4
Software Interface
The software interface for our 3D scanner consists of a MATLAB script on the
host computer which is used to set the capture parameters, and a Python script
on the Raspberry Pi which is used to control the motion of the scanner and to
capture images of the object. Our MATLAB control script allows the user to
specify the IP address of the Pi, the capture name, and the number of capture
images. We transmit the capture parameters via Secure Shell (SSH) to the Pi.
We then remotely call the Python script to control the motion of the camera.
The Python script sends motor control commands to the stepper to execute a

Programmatic 3D Printing of a Revolving Camera Track
393
Fig. 3. Reconstructed 3D meshes of objects for natural objects such as (a) a potted
plant, (b) a bitten apple, and (c) a banana, and man-made objects such as (d) a
horsehead, (e) a Buddha statue, and (f) a tape measure scanned using our 3D scanner.
sequence of small rotations. After each rotation we capture and store an image
to the Raspberry Pi before making the next rotation. We continue this process
until the camera makes a complete revolution around the object, after which
the motor returns to its starting point. We send a signal from the Raspberry
Pi to the host computer indicating termination of the Python script. We then
automatically transfer each image from the Raspberry Pi to the scanner con-
trol computer. We manually import the image set into Autodesk ReMake and
generate a textured 3D mesh of the object [1]. In Fig. 3, we show the results
of automatically capturing dense images of a variety of natural and man-made
objects using our 3D scanner and reconstructing them using Autodesk ReMake.
5
Discussion and Future Work
We provide a 3D scanning system that captures delicate objects, such as pot-
ted plants and fruit, without impacting the integrity of the object of interest.
Our 3D scanner is designed programmatically using OpenSCAD to allow users
to quickly change the dimensional properties without needing to remodel the
parts manually. To capture larger objects our system requires the user to add
additional camera towers, or reprint the mechanical components. In future we
will remove these constraints by designing a system that drives the camera inde-
pendently of the camera track, and making the camera tracks interchangeable
by using interlocking straight and curved pieces.
Acknowledgements. This work was partially supported by the National Science
Foundation (NSF) grant #1730183.
References
1. Autodesk ReMake. http://remake.autodesk.com/about
2. Matter and form. https://matterandform.net/scanner
3. Baraﬀ, D., Witkin, A.: Dynamic simulation of non-penetrating ﬂexible bodies. In:
SIGGRAPH (1992)
4. Baran, I., Popovi´c, J.: Automatic rigging and animation of 3D characters. ACM
Trans. Graph. 26(3) (2007)

394
N. Lamb et al.
5. Borghese, N.A., Ferrigno, G., Baroni, G., Pedotti, A., Ferrari, S., Savar`e, R.:
Autoscan: a ﬂexible and portable 3D scanner. CGA 18(3), 38–41 (1998)
6. Butkiewicz, T.: Low-cost coastal mapping using Kinect v2 time-of-ﬂight cameras.
In: Oceans (2014)
7. Butler, D.A., Izadi, S., Hilliges, O., Molyneaux, D., Hodges, S., Kim, D.:
Shake’n’sense: reducing interference for overlapping structured light depth cam-
eras. In: SIGCHI (2012)
8. Callieri, M., Fasano, A., Impoco, G., Cignoni, P., Scopigno, R., Parrini, G., Biagini,
G.: Roboscan: an automatic system for accurate and unattended 3D scanning. In:
3DPVT (2004)
9. Levoy, M., Pulli, K., Curless, B., Rusinkiewicz, S., Koller, D., Pereira, L., Ginzton,
M., Anderson, S., Davis, J., Ginsberg, J., et al.: The digital michelangelo project:
3D scanning of large statues. In: CGIT (2000)
10. Tong, J., Zhou, J., Liu, L., Pan, Z., Yan, H.: Scanning 3D full human bodies using
kinects. IEEE TVCG 18(4), 643–650 (2012)

Video Browsing on a Circular Timeline
Bernd M¨unzer(B) and Klaus Schoeﬀmann
Institute of Information Technology,
Alpen-Adria-Universit¨at Klagenfurt/Lakeside Labs,
Klagenfurt, Austria
{bernd,ks}@itec.aau.at
Abstract. The emerging ubiquity of videos in all aspects of society
demands for innovative and eﬃcient browsing and navigation mecha-
nisms. We propose a novel visualization and interaction paradigm that
replaces the traditional linear timeline with a circular timeline. The
main advantages of this new concept are (1) signiﬁcantly increased and
dynamic navigation granularity, (2) minimized spacial distances between
arbitrary points on the timeline, as well as (3) the possibility to eﬃciently
utilize the screen space for bookmarks or other supplemental information
associated with points of interest. The demonstrated prototype imple-
mentation proves the expedience of this new concept and includes addi-
tional navigation and visualization mechanisms, which altogether create
a powerful video browser.
Keywords: Video browsing · Video navigation · Video interaction
Visualization
1
Introduction
As recent years show a massive increase in video production and consumption,
the issue of eﬃcient video browsing and interaction is steadily gaining impor-
tance. The need for innovative user interfaces for eﬃcient and intuitive interac-
tion is undeniable. This is also reﬂected in the growing popularity of scientiﬁc
video search competitions [2,5].
In the last decade, numerous innovative ideas for improved browsing, inter-
action and retrieval have been proposed [3]. However, most of the proposed
interfaces use a linear timeline that is typically located below the actual video
and allows the user to navigate in the video. Some approaches augment the
timeline with various auxiliary sources of information [1], but so far the time-
line always remained linear. The idea of using a circular structure is frequently
used for visualization of various kinds of data (e.g., historical timelines or surgi-
cal workﬂow) and has been proposed for indirect navigation support on mobile
devices in the form of a scroll wheel [6], but – to the best of our knowledge –
has not been considered as basic element for video navigation in a desktop tool
so far.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 395–399, 2018.
https://doi.org/10.1007/978-3-319-73600-6_40

396
B. M¨unzer and K. Schoeﬀmann
In this paper, we propose a completely new approach for visualization of
the video player timeline. Instead of relying on the traditional linear shape, we
arrange the timeline around a circle, such that it forms a ring that is super-
imposed on the video. We argue that this design has a number of advantages
over the linear approach. We demonstrate this new concept with a prototype
implementation that furthermore includes various useful browsing and naviga-
tion features.
2
Circular Timeline
In the following, we describe the conceptual characteristics and fundamental
advantages of the proposed circular timeline, before we provide some details
about our implementation. A screenshot of the user interface is depicted in Fig. 1.
Fig. 1. Screenshot of the user interface. (a) start position, (b) playback progress indi-
cator, (c) preview image, (d) bookmark, (e) bookmark preview image, (f) button for
setting a bookmark, (g) speed adaptation pad, (h) minimum center distance for navi-
gation (25% of the radius)
Intuitive orientation. The circular timeline is arranged in analogy to a clock
and therefore allows for intuitive orientation regarding the relative playback time
of the current position or various markers on the timeline. The playback progress
indicator (Fig. 1b) starts at 12 o’clock (Fig. 1a) and runs clockwise.
Navigation on the entire screen. Linear timelines typically restrict the used
screen region to a rectangular area at the bottom of the screen. By contrast, our
circular timeline uses the entire screen as navigation area and thus facilitates

Video Browsing on a Circular Timeline
397
much more ﬂexible interaction. This ﬂexibility is due to the mapping from mouse
position to video position. It is determined by the angle of the straight line
between mouse position and circle center. Hence, the mouse does not need to
be positioned on the timeline, but can be anywhere on the screen. In order to
visualize the corresponding position on the timeline, our implementation always
displays the according line (see Fig. 1c).
Navigation granularity. The timeline resolution or granularity, i.e., the abso-
lute number of distinct points that can be addressed, is often a limiting factor for
precise video navigation. This is particularly relevant in a search scenario where
the user wants to ﬁnd a speciﬁc scene in a long video based on a preview image
that is visualized in accordance with the mouse position on the timeline. If we
assume the common screen resolution of 1920 × 1080 (FullHD) and consequently
an aspect ratio of 16:9, a linear timeline can have a maximum resolution of 1920
pixels, i.e., 1920 distinct preview images can be addressed. With a circular time-
line, the granularity is signiﬁcantly enhanced to 3142 pixels (166%) in the middle
of the timeline ring (if we assume a ring width of 60 pixels and a top and bottom
margin of 10 pixels, i.e., a circle radius of 500px). Moreover, the granularity is
not constant (as for linear timelines), but changes dynamically depending on the
distance to the circle center. Coarser navigation can be accomplished by mov-
ing the mouse closer to the center, while very ﬁne navigation is possible close
to the screen borders (up to a theoretical maximum of 6920 pixels or 360% of
linear timeline granularity in the screen corners, where the maximum distance
from the circle center can be reached). On displays with a non-widescreen aspect
ratio (e.g., 4:3 or portrait mode), the relative gain of granularity is even more
signiﬁcant.
Short distances. A further advantage is that the distances between two arbi-
trary points on the timeline are reduced. This means that the user can reach
other sections of the video with smaller mouse movements. If we consider the
timeline ring, the maximum distance between two points is 2 ∗r, where r is
the circle radius, i.e., 1000 pixels according to the example above. For the lin-
ear timeline, the maximum distance is 1920 pixels. However, as the user is not
restricted to the actual ring but can also move the mouse towards the center,
these distances can become even much shorter. If we consider a distance to the
center of r
4 (illustrated by Fig. 1h), each point on the timeline can be reached
within 250 pixels (albeit with lower granularity).
Screen space utilization. Many advanced video players exploit the concept of
bookmarks, i.e., markers that either correspond to points of interest or indicate
the beginning of a new section and thus help to structure the video into semantic
units. Such bookmarks are usually simply highlighted on linear timelines and
only show additional information on mouse-over. With a circular timeline, the
area left and right outside the circle (or top and bottom in portrait mode) is
predestinated for permanent visualization of supplemental information like a
preview image, which can be directly linked to the marker on the timeline.

398
B. M¨unzer and K. Schoeﬀmann
Potential drawbacks. The most obvious potential argument against the cir-
cular timeline would be the fact that a certain portion of the video is inevitably
occluded. We argue that this problem can be largely mitigated if the circular
timeline is not displayed all the time, but only on demand. In our implementa-
tion, we hide the navigation ring if the mouse is not moved for n seconds (by
default, n = 1). A similar strategy is also pursued in many players with linear
timelines, but typically with longer time intervals. We deliberately chose a quite
short interval due to the higher occlusion extent. Moreover, most relevant con-
tent usually tends to be located in the image center and therefore remains visible
anyway. Another point could be that this novel interface might be unfamiliar for
users and might take a little time to become accustomed. We plan to investigate
these aspects in future work with extensive user studies.
3
Demonstration System
We implemented the circular timeline as a web application, i.e., mainly using
Javascript at the client and PHP on the server side. The application is currently
optimized for usage on a desktop computer, but will also be adapted for mobile
devices.
Apart from common playback control, the user can basically interact with
the system by (1) moving the mouse anywhere on screen in order to see a preview
image of the corresponding video position (Fig. 1c), (2) clicking to jump to this
position and (3) right clicking on the timeline to add a bookmark (Fig. 1d) or
other supplemental information. Clicking on a bookmark preview image (Fig. 1e)
also jumps to the according position. The timeline ring can be conﬁgured to visu-
alize various types of automatically extracted information from content-based
analysis methods (e.g., scene segmentation) that is represented in a well-deﬁned
JSON format. The screenshot illustrates a further automatically extracted ele-
ment for navigation support: the Stripe Image [4], a compact representation of
an entire video, which is created by concatenating the center columns of all
frames, and provides a good overview of the video content. For instance, in the
illustrated soccer example we can clearly distinguish between scenes showing the
actual game and scenes showing close-ups. Furthermore, the playback rate can
easily be modiﬁed by using the speed pad (Fig. 1g).
4
Conclusions
We present the circular timeline, a novel visualization and interaction paradigm
for video navigation. We show that the circular arrangement has a number of
advantages compared to the traditional linear timeline and demonstrate the fea-
sibility with a prototype implementation. In our future work we plan to conduct
user studies to obtain empirical evidence about the beneﬁt of this approach. We
also want to adapt the interface for mobile devices, which will require several
diﬀerent interaction concepts. Moreover, we intend to use this interface for visu-
alization of results from various content based analysis techniques, like temporal
scene segmentation or event detection.

Video Browsing on a Circular Timeline
399
References
1. Del Fabro, M., M¨unzer, B., B¨osz¨ormenyi, L.: Smart video browsing with augmented
navigation bars. In: Li, S., El Saddik, A., Wang, M., Mei, T., Sebe, N., Yan, S., Hong,
R., Gurrin, C. (eds.) MMM 2013. LNCS, vol. 7733, pp. 88–98. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-35728-2 9
2. Schoeﬀmann, K., Ahlstr¨om, D., Bailer, W., Cobˆarzan, C., Hopfgartner, F., McGuin-
ness, K., Gurrin, C., Frisson, C., Le, D.-D., Del Fabro, M., Bai, H., Weiss, W.: The
video browser showdown: a live evaluation of interactive video search tools. Int. J.
Multimedia Inf. Retrieval 3(2), 113–127 (2014)
3. Schoeﬀmann, K., Hudelist, M.A., Huber, J.: Video interaction tools: a survey of
recent work. ACM Comput. Surv. 48(1), 14:1–14:34 (2015)
4. Schoeﬀmann, K., Taschwer, M., Boeszoermenyi, L.: The video explorer: a tool for
navigation and searching within a single video based on fast content analysis. In:
Proceedings of the First Annual ACM SIGMM Conference on Multimedia Systems,
MMSys 2010, New York, NY, USA, pp. 247–258. ACM (2010)
5. Smeaton, A.F., Over, P., Kraaij, W.: Evaluation campaigns and TRECVid. In:
Proceedings of the 8th ACM International Workshop on Multimedia Information
Retrieval, MIR 2006, New York, NY, USA, pp. 321–330. ACM (2006)
6. Sun, Q., H¨urst, W.: Video browsing on handheld devices - interface designs for the
next generation of mobile video players. IEEE MultiMedia 15(3), 76–83 (2008)

Video Browser Showdown

Competitive Video Retrieval with vitrivr
Luca Rossetto(B), Ivan Giangreco, Ralph Gasser, and Heiko Schuldt
Databases and Information Systems Research Group,
Department of Mathematics and Computer Science,
University of Basel, Basel, Switzerland
{luca.rossetto,ivan.giangreco,ralph.gasser,heiko.schuldt}@unibas.ch
Abstract. This paper presents the competitive video retrieval capabil-
ities of vitrivr. The vitrivr stack is the continuation of the IMOTION
system which participated to the Video Browser Showdown competi-
tions since 2015. The primary focus of vitrivr and its participation in
this competition is to simplify and generalize the system’s individual
components, making them easier to deploy and use. The entire vitrivr
stack is made available as open source software.
1
Introduction
In this paper we present the current iteration of vitrivr [6], an open-source
content-based multimedia retrieval stack. The vitrivr stack is the continuation
of the IMOTION system [3,5,7,8] which participated in previous iterations of
the Video Browser Showdown [1]. Despite oﬀering some new functionality, the
primary focus for this years participation lies in the simpliﬁcation and general-
ization of the retrieval stack in order to make it easier to adapt, deploy, and use
by both experts and laymen. The vitrivr stack is available in its entirety from
https://vitrivr.org.
The remainder of this paper is structured as follows: Sect. 2 provides a brief
overview of the overall system architecture and Sect. 3 summarizes all query
types supported by vitrivr. Section 4 provides details on the functionalities intro-
duced in the current version. In Sect. 5, we brieﬂy outline our reasoning behind
the open sourcing of vitrivr and Sect. 6 concludes.
2
Architectural Overview
The vitrivr stack – like its predecessor IMOTION – consists of three primary sys-
tem components: the storage layer ADAMpro [2], the retrieval engine Cineast [4],
and a browser-based user interface. Additionally, a web server is used to serve
static content such as videos and thumbnail images. Additional details on the
architecture of the entire stack can be found in [6].
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 403–406, 2018.
https://doi.org/10.1007/978-3-319-73600-6_41

404
L. Rossetto et al.
3
Interaction and Query Types
The vitrivr stack oﬀers various ways in which queries can be speciﬁed. Basically,
they can for the most part be grouped into two categories: visual and textual.
The visual query modes include Query-by-Sketch and Query-by-Example as well
as Relevance Feedback which are based on visual input such as user generated
sketches of a scene or one or multiple previously retrieved scenes. These queries
are performed based on data extracted directly from the video frames. The
textual queries are based on information which can be extracted from the video
content and represented as text, such as spoken language, text on screen, or
the provided textual video meta data. For this we use the ASR data provided
with the video data set as well as several object detectors to produce labels for
the shots. OCR is applied in order to make text which might appear on screen
searchable as well.
4
New Functionality
While the IMOTION system that has participated in previous instances of VBS
has always been a specialized piece of software, purpose-built for the competition,
the functionality we added to vitrivr in preparation for this iteration of VBS are
such that they are also useful for other use cases of vitrivr.
4.1
New User Interface
The most salient diﬀerence to the IMOTION System of the previous year is
the new user interface. While still browser-based, the latest iteration of the UI
is based upon the Angular framework1. Its modular structure makes it easy to
customize the entire UI or parts thereof to shift its focus from general purpose
multimedia retrieval to, in this case, competitive video retrieval.
As in the past, the UI enables result streaming in order to be able to already
present partial results to the user while the query is still being processed by
the backend. It, however, achieves this no longer via AJAX requests but rather
uses a WebSocket connection to Cineast. A REST-API is also available. The
stack still includes a web server which provides the static content such as shot
thumbnails and the videos themselves, but it is no longer required to act as a
proxy between the browser and Cineast. The screenshot in Fig. 1 depicts the
current version of the UI.
4.2
Approximate Retrieval
The underlying storage engine ADAMpro [2] supports multiple index structures
for eﬃcient vector space retrieval. Many of these index structures achieve their
high eﬃciency by approximating results rather than producing the true nearest
1 https://angular.io/.

Competitive Video Retrieval with vitrivr
405
Fig. 1. Screenshot of the vitrivr UI for large-scale competitive video search
neighbors of a query vector. In previous system iterations, we only made use of
exact query results which lead to longer query times. In the current iteration
of the system, the choice as to whether exact or approximate queries should be
used can be made at query time. Hence, the user can sacriﬁce some accuracy to
gain major speed-ups.
5
Open Source
The entire vitrivr stack [6] is published under the MIT license, the source code
of all its components is available from their individual GitHub2 repositories,
additional documentation can be found on https://vitrivr.org. Being a general-
purpose multimedia retrieval stack, vitrivr has many applications outside of com-
petitive video retrieval as it also supports other domains such as Images, Audio,
and 3D-models. With this ﬂexible open source stack, we hope to oﬀer the com-
munity the basis for future research in many areas and domains of multimedia
retrieval.
6
Conclusions
With the competitive video retrieval version of vitrivr, we plan to continue the
successful participations with we had with the IMOTION system in the past. It
is our hope that by publishing the entire retrieval stack as open source software,
we lower the entry hurdle for future participants and provide some acceleration
for the testing of new ideas in the context of large-scale video retrieval.
2 https://github.com/vitrivr.

406
L. Rossetto et al.
Acknowledgements. This work was partly supported by the Chist-Era project
IMOTION with contributions from the Swiss National Science Foundation (SNSF,
contract no. 20CH21 151571).
References
1. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.W., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimedia Tools Appl. 76(4), 5539–
5571 (2017)
2. Giangreco, I., Schuldt, H.: ADAMpro: database support for big multimedia retrieval.
Datenbank-Spektrum 16(1), 17–26 (2016)
3. Rossetto, L., Giangreco, I., Heller, S., T˘anase, C., Schuldt, H., Dupont, S., Sed-
dati, O., Sezgin, M., Altıok, O.C., Sahillio˘glu, Y.: IMOTION - searching for video
sequences using multi-shot sketch. In: Tian, Q., Sebe, N., Qi, G.-J., Huet, B., Hong,
R., Liu, X. (eds.) MMM 2016. LNCS, vol. 9517, pp. 377–382. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-27674-8 36
4. Rossetto, L., Giangreco, I., Schuldt, H.: Cineast: a multi-feature sketch-based video
retrieval engine. In: Proceedings of the 2014 IEEE International Symposium on
Multimedia (ISM 2014), Taichung, Taiwan, pp. 18–23. IEEE Computer Society,
December 2014
5. Rossetto, L., Giangreco, I., Schuldt, H., Dupont, S., Seddati, O., Sezgin, M.,
Sahillio˘glu, Y.: IMOTION — a content-based video retrieval engine. In: He, X., Luo,
S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM 2015. LNCS, vol. 8936, pp.
255–260. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14442-9 24
6. Rossetto, L., Giangreco, I., T˘anase, C., Schuldt, H.: vitrivr: a ﬂexible retrieval stack
supporting multiple query modes for searching in multimedia collections. In: Pro-
ceedings of the 2016 ACM Conference on Multimedia Conference (ACM MM 2016),
Amsterdam, The Netherlands, pp. 1183–1186. ACM, October 2016
7. Rossetto, L., Giangreco, I., T˘anase, C., Schuldt, H.: Multimodal video retrieval
with the 2017 IMOTION system. In: Proceedings of the 2017 ACM International
Conference on Multimedia Retrieval (ICMR 2017), Bucharest, Romania, pp. 457–
460. ACM, June 2017
8. Rossetto, L., Giangreco, I., T˘anase, C., Schuldt, H., Dupont, S., Seddati, O.:
Enhanced retrieval and browsing in the IMOTION system. In: Amsaleg, L.,
Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017. LNCS,
vol. 10133, pp. 469–474. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
51814-5 43

Enhanced VIREO KIS at VBS 2018
Phuong Anh Nguyen(&), Yi-Jie Lu, Hao Zhang,
and Chong-Wah Ngo
City University of Hong Kong, Kowloon, Hong Kong
{panguyen2-c,yijie.lu,hzhang57-c}@my.cityu.edu.hk,
cscwngo@cityu.edu.hk
Abstract. The VIREO Known-Item Search (KIS) system has joined the Video
Browser Showdown (VBS) [1] evaluation benchmark for the ﬁrst time in year
2017. With experiences learned, the second version of VIREO KIS is presented
in this paper. Considering the color-sketch based retrieval, we propose a simple
grid-based approach for color query. This method allows the aggregation of
color distributions in video frames into a shot representation, and generates the
pre-computed rank list for all available queries which reduces computational
resources and favors a recommendation module. With focusing on concept
based retrieval, we modify our multimedia event detection system at TRECVID
2015 in VIREO KIS 2017. In this year, the concept bank of VIREO KIS has
been upgraded to 14K concepts. An adaptive concept selection, combination
and expansion mechanism, which assists the user in picking the right concepts
and logically combining concepts to form more expressive query, has been
developed. In addition, metadata is included for textual query and some inter-
face designs are also revised for providing a ﬂexible view of results to the user.
Keywords: Video search  Known-Item Search  Color sketch query
Concept query  Concept selection  Concept combination
1
Introduction
In VBS 2017, the ﬁrst version of VIREO KIS system [2] has achieved the 4th rank over
6 participants on Known-Item Search task and got the best result on Adhoc Video
Search (AVS) task. This system includes three modalities: query by color sketch, by
edge sketch, by concept that mainly relies on the detection of audio-visual objects.
Based on the lesson learnt from VBS 2017, we have investigated the limitations of our
system and proposed solutions in this paper.
First, query by sketch is relied on the color signatures feature which was originally
proposed and implemented by SIRET team [3, 4]. The main idea is extraction of color
circles from video frames for matching with the color circles sketched by the user using
a spatial grid indexing technique. The effectiveness of color signatures feature depends
on the temporal sampling rate for feature extraction: dense sampling brings accurate
results with the expense of low efﬁciency due to dramatic increase of candidates for
matching; master shot key-frames sampling processes less number of frames for color
circles extraction which reduces retrieval accuracy. To deal with this problem, we
propose a new method to capture the color information of video frames and enable
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 407–412, 2018.
https://doi.org/10.1007/978-3-319-73600-6_42

query by color sketch. In this approach, a uniform grid is placed over a video frame and
the color distribution of each cell is calculated. For retrieval, the user needs to formulate
a cell-based query including both cell’s location and dominant color for matching.
Using this method, the color distribution of video frames can be aggregated into
shot-based representation which signiﬁcantly reduces the matching expense comparing
to frame-based representation. Moreover, a set of colors available for query is deﬁned
enabling pre-computing the matching result for every single cell-based query. The
pre-computed result then favors a recommendation module which provides to the user
the color distribution information of the dataset. This recommendation module has been
inspired by our previous work on a simulation framework for color sketch [5].
Second, concept selection is one of the critical part of the concept based KIS
system. Previously, our system relied on an automatic concept selection module
combined with the user’s evaluation for concept re-selection and concept-weight
estimation. In practice, the concept selection can be done on the ﬂy with a simple
recommendation module which directly maps the user input to the concept bank. To
enrich the query, we follow the Waseda team’s report in AVS task 2016 [6] and
provide the AND, OR, NOT combination of concepts. Also, a recommendation module
has been developed to provide a list of co-occurrence concepts for ﬁltering. We also
refer to an existing tags tokenizing interface in website development to implement the
user interface speciﬁcally for the concept selection.
Third, to improve the searchability of textual query, we include all metadata
including video name, video description, speech and on-screen text for retrieval. Also,
a compact video key-frame representation is also developed using the idea of video
thumbnail preview.
2
Simpliﬁed Color Sketch Query
As aforementioned, the color signatures feature provides a ﬂexible mechanism for
deﬁning a query with arbitrary position and color. Because the user can freely put color
circles in any position with any color, the matching calculation must be done on the ﬂy
which requires computing and sorting distances from the query color circles to color
circles in the database. This approach leads to a tedious matching calculation when the
size of the dataset grows larger. Based on a study from Johns Hopkin University which
shows that the user’s memories for colors are biased in favor of his/her “best” versions
of basic colors [7], we combine with the grid-based color sketch in [8] to deﬁne a ﬁxed
set of queries based on the basic colors. Relying on this set of queries, the user can
construct an approximate version of their visual memory as an input to the video
retrieval system. The advantage of using this set of queries is that the matching phase
can be done in advance for all pre-deﬁned queries. Hence, it creates more room to
calculate result for the combination of multiple queries and enables recommendation to
the user.
At the ﬁrst step, we select a ﬁxed set of colors C ¼ c1; . . .; cn
f
g as the available
query colors. A uniform grid is placed over the color sketch area generating a list of
cells P ¼ p1; . . .; pm
f
g. To construct a query q, the user needs to specify both the cell
position p and the color c of that cell, i.e. q ¼ ðp; cÞ.
408
P. A. Nguyen et al.

In the matching phase, the distance from a query q to a video frame f is calculated
by taking the average of the pixel-wise Euclidean distances between the query color qc
and the color at the same query cell p in frame f .
D q; f
ð
Þ ¼ 1
np
Xnp
x¼1 d cx; qc
ð
Þ
ð1Þ
where cx is the color of pixel x in cell p of f and np is the number of pixels in cell p.
As a query’s cell position and color have been ﬁxed, we have totally Q ¼
q1; . . .; qk
f
g available queries with k ¼ n  m, where n is the number of selected
colors and m is the number of cells generated by the uniform gird. Using (1), the
distances from each query in Q to all video frames in the database can be calculated in
advance and indexed.
Based on (1), we aggregate the frames in the same shot to generate the distances
from each query in Q to all shots. The aggregation can be done by taking the average or
the minimum of the distances over all frames in a shot s ¼ f1; . . .; fh
f
g. In details,
DS q; s
ð
Þ ¼ 1
h
Xh
i¼1 D q; fi
ð
Þ
ð2Þ
or
DS q; s
ð
Þ ¼ minh
i¼1D q; fi
ð
Þ
ð3Þ
where D q; fi
ð
Þ is the distance from a query q to the i-th frame and h is the number of
frames in shot s. The average or the minimum function can be actively selected by the
user depending on his/her attention. For example, if the user focuses in the areas where
the color does not change in the shot, the average function will be the best choice. In
contrast, if the user focuses on the continuously changing color in the shot, the min-
imum function should be used. As a result, we have pre-computed distances to all video
shots from all queries in Q using both (2) and (3) approaches.
Using these pre-computed distances, the result can be quickly retrieved as well as
recommendation can be made. Assuming that the user has already put a query q and
intends to put a second query q0, we require the user to specify the cell’s color c0 ﬁrst,
followed by the location p0 of q0. Then, the pre-computed distances for all combinations
of q0 ¼ ðp0; c0Þ can be loaded and combined with the existing distances from q. Finally,
a distance threshold is deﬁned which can help counting the number of shots that lie in a
First query
Query area
Color picker area
Fig. 1. Dynamic update of color query area for system recommendation. In this example, the user
picks yellow color as the second color after specifying the grid position of green color. The color area
is updated with each cell indicating the available number of shots to be retrieved. The lighter color
means a more unique cell with less number of candidate shots to be browsed. (Color ﬁgure online)
Enhanced VIREO KIS at VBS 2018
409

speciﬁc range from those queries. These numbers provide the user the ideas about the
distribution of the result that he/she will get in the next query.
According to our interface design for color sketch (see Fig. 1), the user needs to
select a color from the color picker area ﬁrst and then click on the query area to place
the query, i.e. for every query q0, the user will specify the color c0 as the input to the
recommendation module. The distribution of the result that the user gets from q0 is
shown in all non-placed query cells in the query area indicated by the intensity of each
cell. The darkness of a query cell is calculated by normalizing the number of shots in
deﬁned distance threshold by the maximal value of these numbers over all query cells.
3
Enhanced Concept Selection, Combination and Query
Expansion
To improve the effectiveness of the concept based modality, we ﬁrst replace the pre-
vious concept bank [9] by the larger one with 14,046 concepts including: ImageNet
with 12,988 concepts, MIT Places with 205 concepts [10], TRECVID SIN Task with
346 concepts [11] and Research collection with 497 concepts [12]. With a large number
of concepts, the process of selecting the query concepts becomes more important. In the
previous study, the retrieval accuracy is improved by the user’s engagement in the
concept selection process. With the advantages of an interactive system, the perfor-
mance of concept selection can be signiﬁcantly improved by letting the user pick the
right concepts from the start. Then, with the understanding of the co-occurrence
concepts from the retrieved results, the user can reﬁne the query by adding more
speciﬁc concepts or removing un-related concepts. To accomplish these two tasks, we
implement the system as follows:
Concept selection. The target of this step is to force the user to directly select concepts
from the concept bank. This part has been done by two steps: when the user inputs the
text, the system directly show a list of concepts which shows the concepts start with the
input text using a drop-down list; when the user ﬁnishes typing but the concept does
not exist in the concept bank, a list of nearest concepts in WordNet is updated to the
drop-down list for use selection.
Concept combination. This function allows the user to deﬁne a speciﬁc query based
on three logic combinations: AND, OR, NOT. The proposed method of Waseda team
[6] for these logic combinations has been implemented in VIREO KIS. In order to
provide a user-friendly interface, an interface for multi-tags selection which is com-
monly used by search engines, is developed to help the user formulate query easily.
Instead of treating every concept as a tag, we consider each logic combination of
concepts as a tag, each tag comes with a background color which shows the meaning of
the combination. All the stand-alone tags are combined using the AND combination.
The user can easily add more concepts combination or remove existing combination in
a tokenizing area (see Fig. 2).
Concept query expansion. After feeding the query into the retrieval systems, the rank
list of video shots is retrieved together with a list of related concepts which largely
410
P. A. Nguyen et al.

impact the result. The related concepts are generated by evaluating the co-occurrence
concepts [13] based on their inﬂuence and frequency in the top-k shots retrieved. From
these concepts, the user can pick more concepts and update query. We expect that the
user will mostly use the AND function to create more detail query and the NOT
function to ﬁlter out unrelated results.
4
Metadata for Textual Query and Interface Design
Metadata. From VBS 2017, we ﬁgured out that the metadata is very helpful in many
cases. Hence, we deploy a text-based retrieval module using Lucence to index the text
extracted from ﬁle name, video description, automation speech provided with the video
data and the optical character recognized by Tesseract OCR Engine [14].
Interface design. The existing interface is based on the design of SIRET team [3, 4]
with each video being shown as a row accompanied with its temporal context. The
display occupies a lot of screen space resulting in only 10 videos being shown on
screen. This way of presentation reduces the effectiveness of the browsing phase in the
early stages of retrieval for large-scale dataset, so we go back to the traditional rep-
resentation which shows the candidate master-shot key-frames only. To provide a
quick judgement to the user, we let each master-shot key-frame become a dynamical
image where frames in shot are shown continuously on mouse hover. We also employ a
preview panel which allows the user to view the summarization of a video using master
shot key-frames as well as preview the video content with fast forward speed.
5
Conclusion
The 2018 version of VIREO KIS has signiﬁcant improvement in term of resource con-
sumption and query latency comparing to the last year version. With a simpliﬁed color
sketch query with recommendation module, the user now has more understanding on the
dataset as well as has more guidance in formulating queries. Using a large concept bank
with a ﬂexible concept selection, combination and expansion mechanism provides an
interesting playground for the user in adaptingqueries especially in dealing with large-scale
dataset. And ﬁnally, metadata brings more options in revising queries as well as the new
browsing interface is more compact for providing a broader view on retrieval results.
Fig. 2. The combination of concepts. After picking a list of concepts, the user can choose the
logical combination by clicking on the corresponding button. Each tag comes along with a
removal button (“X”) such that the user can modify the query interactively.
Enhanced VIREO KIS at VBS 2018
411

Acknowledgment. The work described in this paper was supported by two grants from the
Research Grants Council of the Hong Kong Special Administrative Region, China (CityU
11210514, 11250716).
References
1. Cobârzan, C., Schoeffmann, K., Bailer, W., Hürst, W., Blažek, A., Lokoč, J., Vrochidis, S.,
Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed analysis of the video
browser showdown 2015. Multimed. Tools Appl. 76(4), 5539–5571 (2017)
2. Lu, Y.-J., Nguyen, P.A., Zhang, H., Ngo, C.-W.: Concept-based interactive search system.
In: Amsaleg, L., Guðmundsson, G.Þ., Gurrin, C., Jónsson, B.Þ., Satoh, S. (eds.) MMM
2017. LNCS, vol. 10133, pp. 463–468. Springer, Cham (2017). https://doi.org/10.1007/978-
3-319-51814-5_42
3. Lokoč, J., Blažek, A., Skopal, T.: Signature-based video browser. In: Gurrin, C., Hopfgartner,
F., Hurst, W., Johansen, H., Lee, H., O’Connor, N. (eds.) MMM 2014. LNCS, vol. 8326,
pp. 415–418. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-04117-9_49
4. Blažek, A., Lokoč, J., Matzner, F., Skopal, T.: Enhanced signature-based video browser. In:
He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM 2015. LNCS, vol.
8936, pp. 243–248. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14442-9_22
5. Lokoč, J., Phuong, A.N., Vomlelová, M., Ngo, C.-W.: Color-sketch simulator: a guide for
color-based visual known-item search. In: Cong, G., Peng, W.-C., Zhang, W.E., Li, C., Sun,
A. (eds.) ADMA 2017. LNCS, vol. 10604, pp. 754–763. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-69179-4_53
6. Ueki, K., Kikuchi, K., Saito, S., Kobayashi, T.: Waseda at TRECVID 2016: ad-hoc video
search. In: TRECVID 2016 Workshop, Gaithersburg, MD, USA (2016)
7. Bae, G.Y., Olkkonen, M., Allred, S.R., Flombaum, J.I.: Why some colors appear more
memorable than others: a model combining categories and particulars in color working
memory. J. Exp. Psychol. Gen. 144(4), 744–763 (2015)
8. Wang, J., Hua, X.-S.: Interactive image search by color map. ACM Trans. Intell. Syst.
Technol. 3(1), Article Id 12 (2011)
9. Lu, Y.J., Zhang, H., de Boer, M., Ngo, C.W.: Event detection with zero example: select the
right and suppress the wrong concepts. In: ACM ICMR (2016)
10. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene
recognition using places database. In: Advances in Neural Information Processing Systems,
pp. 487–495 (2014)
11. Zhang, W., Zhang, H., Yao, T., Lu, Y., Chen, J., Ngo, C.-W.: VIREO @ TRECVID 2014:
instance search and semantic indexing. In: NIST TRECVID Workshop (2014)
12. Strassel, S., Morris, A., Fiscus, J., Caruso, C., Lee, H., Over, P., Fiumara, J., Shaw, B.,
Antonishek, B., Michel, M.: Creating HAVIC: heterogeneous audio-visual internet
collection. In: Chair, N.C.C., Choukri, K., Declerck, T., Dogan, M.U., Maegaard, B.,
Mariani, J., Odijk, J., Piperidis, S. (eds.) LREC, Istanbul, Turkey, May 2012. ELRA (2012)
13. Sigurbjornsson, B., Zwol, R.V.: Flickr tag recommendation based on collective knowledge.
In: Proceeding of ACM Intelligent World Wide Web Conference, pp. 327–336 (2008)
14. Smith, R.: An overview of the tesseract OCR engine. In: Proceeding of 9th International
Conference on Document Analysis & Recognition (2007)
412
P. A. Nguyen et al.

Fusing Keyword Search and Visual
Exploration for Untagged Videos
Kai Uwe Barthel(&), Nico Hezel, and Klaus Jung
Visual Computing Group, HTW Berlin, University of Applied Sciences,
Wilhelminenhofstraße 75a, 12459 Berlin, Germany
barthel@htw-berlin.de
Abstract. Video collections often cannot be searched by keywords because
most videos are poorly annotated. We present a system that allows to search
untagged videos by sketches, example images and keywords. Having analyzed
the most frequent search terms and the corresponding images from the Pixabay
stock photo agency we derived visual features that allow to search for 20000
keywords. For each keyword we use several image features to be able to cope
with large visual and conceptual variations. As the intention of a user searching
for an image is unknown, we retrieve thousands of result images (video scenes),
which are shown as a visually sorted hierarchical image map. The user can
easily ﬁnd images of interest by dragging and zooming. The visual arrangement
of the images is performed with an improved version of a self-sorting map,
which allows organizing thousands of images in fractions of a second. If an
image similar to the search query has been found, further zooming will show
more related images, retrieved from a precomputed image graph. The new
approach helps to ﬁnd untagged images very quickly in an exploratory, incre-
mental way.
Keywords: Content-based video retrieval  Exploration  Image browsing
Visualization  Navigation  Convolutional neural networks
1
Introduction
Searching for particular scenes in large unannotated video collections is not trivial and
can be very time consuming. For the Video Browser Showdown 2018 its participants
have to develop interactive video browsing systems able to ﬁnd speciﬁc videos shown
as visual clips or as a textual description of such a clip. VBS2018 will collaborate with
TRECVID 2017 (the Ad-Hoc Video Search (AVS) Task) and use the same data set,
consisting of 4593 Internet Archive videos (144 GB, a total of 600 h) with durations
between 6.5 min and 9.5 min.
The results of past Video Browser Showdowns [1] have shown an overall better
performance in the visual and a lot of space for improvements in the textual tasks. The
latter would beneﬁt from conceptual searches using keywords. In addition, a clean
interface, able to display many video frames without losing overview, and the support
for real-time interaction to quickly explore large video collections are needed.
© Springer International Publishing AG 2018
K. Schoeffmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 413–418, 2018.
https://doi.org/10.1007/978-3-319-73600-6_43

In this paper we present a keyword search technique for untagged videos. For
realistic search scenarios many different keywords are needed. Sections 2 and 3
describe how to generate visual features for 20000 keywords. The following sections
explain how the images are collected to form a search result, and how this result is
visualized. We describe a new approach by combining the idea of the image pyramid
and the graph-based image exploration. Figure 1 shows the principle of the organi-
zation of a search result. Finally we give an overview of the user interaction with the
search system.
2
Generating Semantic Image Features
Convolutional neural networks (CNNs) are not only able to achieve high accuracy rates
for image classiﬁcation tasks, they can also be used for feature extraction. The outputs
of a neural network trained for a particular set of categories do not perform well in
general image retrieval tasks. However, the activations of a hidden layer can serve as
feature vectors for image retrieval [2]. We performed an extensive evaluation to
determine which CNNs, which layers, and what kind of transformations are suited to
generate feature vectors that perform well for image retrieval tasks.
For determining how to generate good feature vectors, we tested different
state-of-the-art CNNs [3–6] regarding their image retrieval quality. The models were
chosen by their classiﬁcation accuracy and their diversity. The networks were trained
using the ILSVRC2012 data set [7] or the entire ImageNet set. To evaluate the retrieval
quality, we used the “holiday” image set from INRIA [8]. Neither the images nor the
categories were part of the training image sets. Best results in regard to the mean
Fig. 1. A search result is visually arranged and shown as an image pyramid. The user can
explore the search results by dragging. Zooming will display more related images. At the lowest
level similar images are retrieved from a precomputed image graph. On the right the
corresponding viewports of the different levels are shown.
414
K. U. Barthel et al.

average precision were obtained using a Deep Residual Learning Network [4].
A pre-trained ResNet-200 was provided by Wei via GitHub [9]. It was trained with the
full ImageNet dataset, except for classes with less than 500 images.
The 2048 activations before the fully connected layer were taken as initial feature
vectors. By applying a L1-normalization followed by a PCA we could improve the
retrieval quality and compress the feature vectors to only 64 dimensions stored as byte
values. The best metric for comparing these feature vectors was the L1 distance.
A deeper discussion of this topic is beyond the scope of this paper. More details can be
found in a paper submitted to the demo session of MMM2018 [10]. To cope with
different objects in the images we store several feature vectors of various image regions
suggested by a R-CNN [11].
3
Generating Visual Features for Keywords
In order to produce visual feature vectors for keywords we use the freely available 1.1
million images from the Pixabay stock photo agency. In many cases these images have
only few keywords, however they often correctly describe and represent the main
content of the image.
Typically, a particular keyword covers many different concepts and visual varia-
tions or even ambiguities. For the keyword “apple” there will be images such as
isolated apples, people eating apples, apple trees and other things. We do not try to
separate these concepts but rather to cover the great variability by allowing several
different feature vectors per keyword.
The feature vectors were generated in the following manner: First we determined a
list of all keywords used in Pixabay and sorted them by their frequency. For each of the
top 20000 keywords we randomly retrieved up to 4096 images and computed their
visual feature vectors. From all these feature vectors 10 clusters were calculated using
the Generalized Lloyd algorithm. For each cluster we determined the most representing
feature vector (the generalized median) and assigned it to the list of feature vectors for
that keyword.
4
Search Result Visualization
A major problem is the presentation and navigation of very large image sets. In [12] we
have proposed an image exploration tool and demonstrated its potential with www.
picsbuffet.com allowing to visually search and browse more than one million stock
images from Pixabay [13]. All images are presented as a hierarchical 2D image map
arranged according to their similarities. However, by projecting large amounts of
images onto a two-dimensional space the complex relationships between all images
cannot be preserved. We provided a solution for this problem by using a hierarchical
set of image graphs [14]. Lower layers of the graph connect very similar images while
higher layers connect less related images.
But even with such a graph-based approach related images (e.g. a photo and a
drawing of an apple) may still be far away from each other and it will not be obvious
Fusing Keyword Search and Visual Exploration for Untagged Videos
415

how to navigate to related concepts. Graph navigation in general is very challenging,
because the user is either confused with too many connections of the graph or is
confronted with a low-dimensional projection that does not reﬂect the properties of the
graph accurately.
In our proposed video browsing scheme searches can be started with sketches,
example images or keywords. Other typical image or video search systems use very
limited result sets, showing only about 20 to 50 images per screen. This amount is too
small to represent the large variation of an entire search result. An image pyramid with
not too many images allows an easy overview and navigation, whereas the image graph
guarantees access to all images in the dataset.
In order not to miss any matching results, for each search query we retrieve a much
larger set of images. By presenting these images as a visually sorted hierarchical image
map, up to several thousands of images can easily be perceived. The user can explore
these search results by dragging and zooming. If an image close to the desired query
has been found, further zooming will display more related images. At the lowest level
this is performed by “leaving the image pyramid” and switching to a mode, where
similar images are retrieved from a precomputed image graph. Within this graph all
images are connected according to their similarity. The user will not be aware of this
change as the retrieved images are displayed by projecting them onto a visually sorted
2D map. Our approach enables users to ﬁnd images very quickly and helps to avoid
time consuming new searches. Figure 1 shows the principle of the organization of a
search result. The top three layers consist of the visually sorted hierarchical image map
of the search result images, below the image-graphs of all images is shown.
5
Search System
Our video search system needs several ofﬂine preparation steps. Initially we extract two
frames per second from every video clip. These frames are used to separate different
scenes. Each scene is represented by the generalized median of all feature vectors of the
scene.
Next a hierarchical image graph has to be constructed such that similar images can
be retrieved very quickly by traversing the edges of the graph. For building the graph
the following requirements need to be met: Connected images should be very similar.
The number of connections per image should neither be too high nor too low. A de-
tailed description how to build such a graph efﬁciently can be found in [14]. A graph of
one million images can be built in one hour on a single core of a Xeon E3-1230 v3
CPU.
When doing a search we always retrieve 4096 result images. This number turned
out to be large enough to cover many variations of a search or a keyword concept. On
the other hand, this number of images is not too large and can still be perceived on a
three level image pyramid with 16  16 images on the top level. Using a modiﬁed
version of the self-sorting map (SSM) algorithm, 4096 images can be sorted in less than
250 ms [15, 16]. As keywords are represented with 10 feature vectors, also 10 searches
need to be performed for a single keyword search. These 10 search results have to be
aggregated. We use GPU acceleration to further speed up the search.
416
K. U. Barthel et al.

6
Search System User Interaction
After the search result has been retrieved and the image pyramid has been generated the
user can start browsing to ﬁnd the desired images. When dragging the pyramid only the
viewport needs to be changed. Zooming changes the pyramid level. As described
before zooming on the lowest pyramid level will retrieve related images from the image
graph. Starting from the image in focus, its connected edges (images) are recursively
followed until the desired amount of neighboring images has been retrieved. In a next
step these images are sorted with the modiﬁed SSM, which maps the images according
to their similarities onto a regular 2D image map. If the user drags the canvas new
images are retrieved in the same way and get mapped to the most suitable empty map
places. The positions of the previously displayed images remain unchanged. To
emphasize realistic navigation experience previous arrangements are cached.
Figure 2 shows a preliminary screenshot of our video search system. On the left the
result of a keyword search can be seen. At the top 10 images representing the search
result clusters are shown. These images can be used to jump to the corresponding
position on the image map. Double clicking an image will trigger a new search and
display the result on the map. In the middle the most similar video clips and their ﬁve
successive scenes are shown. On the right additional elements for drawing sketches can
be found.
Fig. 2. The video search interface. On the left the result of a keyword search can be seen. At the
top 10 images representing the search result clusters are shown.
Fusing Keyword Search and Visual Exploration for Untagged Videos
417

References
1. Cobârzan, C., et al.: Interactive video search tools: a detailed analysis of the video browser
showdown 2015. Multimed. Tools Appl. 76(4), 5539–5571 (2017)
2. Krizhevsky, A. et al.: Imagenet classiﬁcation with deep convolutional neural networks. In:
Advances in Neural Information Processing Systems, pp. 1097–1105 (2012)
3. He, K. et al.: Deep residual learning for image recognition. In: CVPR, pp. 770–778. IEEE
Computer Society (2016)
4. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 630–645.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0_38
5. Ioffe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing
internal covariate shift. In: Bach, F.R., Blei, D.M. (eds.) ICML, pp. 448–456 (2015). JMLR.org
6. Szegedy, C. et al.: Going deeper with convolutions. In: CVPR, pp. 1–9. IEEE Computer
Society (2015)
7. Russakovsky, O., et al.: Imagenet large scale visual recognition challenge. Int. J. Comput.
Vis. 115(3), 211–252 (2015)
8. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometric consistency
for large scale image search. In: Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV 2008.
LNCS, vol. 5302, pp. 304–317. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-
540-88682-2_24
9. Wie, W.: GitHub ResNet. https://github.com/tornadomeet/ResNet. Accessed 8 Oct 2017
10. Hezel, N., et al.: ImageX – Explore and search local/private images, submitted to
MMM2018
11. Girshick, R.: Fast R-CNN. CoRR. abs/1504.08083 (2015)
12. Barthel, K.U., Hezel, N., Mackowiak, R.: ImageMap - visually browsing millions of images.
In: He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM 2015. LNCS, vol.
8936, pp. 287–290. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14442-9_30
13. Picsbuffet Homepage. http://www.picsbuffet.com. Accessed 15 Oct 2017
14. Barthel, K.U., Hezel N.: Visually exploring millions of images using image maps and
graphs. In: Big Data Analytics for Large-Scale Multimedia Search (2018, to appear). (to be
published by Wiley & Sons)
15. Barthel, K.U. et al.: Visually browsing millions of images using image graphs. In: Ionescu,
B. et al. (eds.) ICMR, pp. 475–479. ACM (2017)
16. Strong, G., Gong, M.: Self-sorting map: an efﬁcient algorithm for presenting multimedia data
in structured layouts. IEEE Trans. Multimed. 16(4), 1045–1058 (2014)
418
K. U. Barthel et al.

Revisiting SIRET Video Retrieval Tool
Jakub Lokoˇc(B), Gregor Kovalˇc´ık, and Tom´aˇs Souˇcek
SIRET Research Group, Department of Software Engineering,
Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic
lokoc@ksi.mff.cuni.cz, gregor.kovalcik@gmail.com, tomas.soucek1@gmail.com
Abstract. The known-item and ad-hoc video search tasks still rep-
resent challenging problems for the video retrieval community. Dur-
ing last years, the Video Browser Showdown identiﬁed several promis-
ing approaches that can improve the eﬀectiveness of interactive video
retrieval tools focusing on the tasks. We present a major revision of the
SIRET interactive video retrieval tool that follows these ﬁndings. The
new version employs three diﬀerent query initialization approaches and
provides several result visualization methods for eﬀective navigation and
browsing in sets of ranked keyframes.
1
Introduction
Since 2012, the Video Browser Showdown (VBS) competition [7,12] brings
together research teams focusing on interactive video retrieval [13]. After ﬁrst few
years, the competition started to focus on three types of tasks – visual/textual
known item search (KIS) and ad-hoc video search (AVS). The competing teams
have no restrictions on the design of their interactive tools, except disabled cam-
eras for solving visual KIS tasks. During the last three events, the provided
dataset has grown from 100 h in 2015 to 250 h in 2016 and 600 h in 2017, when
VBS started to cooperate with TRECVID [2]. With the growing dataset, the
tasks have become more and more challenging. In 2015, the winner presented a
tool based on color sketches and similarity search using semantic descriptors. In
2016, the winner [3] utilized also sorted image maps for eﬀective visualization
of larger result sets, while in 2017 the winner [10] presented a tool with addi-
tional ﬁltering and ranking approaches. Since 2015, all winning tools relied in
multiple ways on deep convolutional neural networks. In this work, we present
a tool based on three diﬀerent query formulation approaches – query by key-
words, query by color-sketch and query by example. Although these approaches
have been utilized also in the previous versions of our tool, we have changed
the underlying retrieval models. We have also added an additional ranked result
visualization approach to the visualizations used in the previous versions of our
tool.
2
Revisited Retrieval Models
In the following, the underlying retrieval models for all the utilized querying
approaches are detailed. The tool supports also symmetric and asymmetric late
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 419–424, 2018.
https://doi.org/10.1007/978-3-319-73600-6_44

420
J. Lokoˇc et al.
fusion strategies of multiple modalities [6]. Note that the retrieval models search
only in a selected set of representative keyframes (e.g., TRECVID Master shot
reference), while visualizations of the best matching results can utilize an addi-
tional set of keyframes extracted uniformly (four frames per second).
2.1
Query by Keywords
The previous version of our tool [4] assigned labels automatically to keyframes
using an ImageNet [11] classiﬁcation model. The Deep Convolutional Neural
Network by Simonyan and Zisserman [14] was considered, where the classiﬁca-
tion model is restricted to a limited set of 1000 labels (image classes selected
to challenge mainly classiﬁcation tasks). In order to connect the automatically
assigned labels to user queries, the set of labels was further extended by a set
of hypernyms. However, according to our experience the set of labels was not
suﬃcient for the most of the KIS tasks.
In our revisited keyword search retrieval model, we have focused on our
own set of labels manually selected from the set of ImageNet classes containing
more than 500 example images. The selection was based on a human judge
without the knowledge of the TRECVID dataset, who focused on common and
frequently appearing nouns. So far, 1390 labels were selected, but this number
can grow in the future versions of our tool. As a classiﬁcation model, we consider
a retrained GoogLeNet inception architecture [15]. The retraining was performed
in a standard way as follows. The weights were initialized to ILSVRC values
except for the last layer, which was trained with the new labels and corresponding
example images until a training data accuracy stopped improving. This prevents
destroying network’s learned values by propagation of random gradients in the
ﬁrst iterations. Then, the whole network was retrained. Learning rate decay,
strong regularization and artiﬁcially enlarged dataset resulted in network’s small
variance without much eﬀort. In addition to the manually selected labels, again
a set of hypernyms is used to connect the user input to the set of automatically
assigned labels. Since the number of automatically assigned labels could be still
unsatisfactory (due to omitted classes), we consider also a much wider collection
of labels provided by an external automatic image annotation system [1]. The
system performs similarity search (based on deep features) in about 100M images
included in the YFCC100M dataset and assigns labels based on the available
metadata of the most similar images to the actually annotated keyframe. Our
system supports both models for automatic annotation, users can switch the
models in settings. Provided video metadata are considered as well.
Each query consists of selected labels1 connected by logical AND and OR.
Speciﬁcally, users specify sets of labels Ni ⊆{label1, . . . , label|L|}, where in each
set the labels are connected by logical OR, while the sets are connected by logical
AND. For k such sets, the query is represented as QK = {Ni}k
i=1. The ranking
of each selected keyframe o is calculated as
1 Only the supported set of labels L of a selected model M can be used to form the
query (our UI keyword search element prompts the labels).

Revisiting SIRET Video Retrieval Tool
421
rank keyword(QK, o) =

∀Ni∈QK
(

∀labelj∈Ni
score(labelj, o, M)),
where score(labelj, o, M) assigns a relevance score that the keyframe o contains
labelj in a selected automatic annotation model M. The scores are evaluated
and stored in the preprocessing phase.
The most relevant images are then presented to the user. To speed up the
computation and reduce the amount of memory for the employed inverted index,
only a set of labels with the highest score are considered for each keyframe o.
For the GoogLeNet model, the user can input also any WordNet hypernym h
that is treated as a disjunction of all its available hyponyms in the supported
set of labels L.
2.2
Query by Color-Sketch
In order to query the database by colors, our tool still considers a simple-to-use
sketch drawing canvas, where users place/edit colored points depicted as circles
for easier manipulation. Each colored point is modeled as a vector cp ∈F, where
the feature space F ⊂R5 considers position and color information. The ﬁrst two
dimensions of F represent (x, y) position and the last three dimensions corre-
spond to a selected color (perceptually uniform CIE Lab color space is employed).
As the frames for color-based search are no longer densely sampled, small thumb-
nails are utilized instead of heavily compressed clustering-based position color
feature signatures used in the previous versions [5,8]. Since the memorized loca-
tion of a moving object does not have to be covered by the list of selected set of
representative keyframes, the ranking model puts less stress on the position of
the colored point. More speciﬁcally, users can specify (based on their conﬁdence)
a radius ri ∈R+
0 in the query sketch for a given colored point cp ∈F. The radius
determines the inspected pixels in the keyframe thumbnail, where the pixel with
the most similar color to the query color is searched.
Formally, the query sketch is deﬁned as a set QS = {(qi, ri)}k
i=1 of k colored
points qi ∈F with a given search radius ri ∈R+
0 and each database frame is
represented as a set OS = {oj}300
j=1 of colored points oj ∈F obtained after an
image resize operation and transformation of colors to the Lab color space. The
ranking of a database object OS with respect to a query sketch QS is computed
as
rank sketch(QS, OS) =

∀(qi,ri)∈QS
( min
∀oj∈OS(f(qi, ri, oj))),
where f(q, r, o) = (q.L −o.L)2 + (q.a −o.a)2 + (q.b −o.b)2 for close neighboring
tuples satisfying (q.x −o.x)2 + (q.y −o.y)2 ≤r2, while f(q, r, o) = ∞otherwise.
The x, y position error can be added to the ranking score if the user is conﬁdent
with the position. Note that a normalization is necessary to match positions
from the query sketch to positions in the uniﬁed keyframe thumbnail resolution.
Pixel locations inside the radius ri can be indeed determined in advance, while
the remaining pixel locations can be ﬁltered out which speeds up the ranking of

422
J. Lokoˇc et al.
all the keyframes. To speed up the ranking further, we again employ caching of
partial results min∀oj∈OS(f(qi, ri, oj)) for a given (qi, ri), as users always modify
only one colored circle in the query sketch at once. With these optimizations, our
tool can eﬃciently sequentially rank and sort all the selected keyframes from the
actual video collection. The color-based model enables also an advanced mode,
where users can specify ellipse axes instead of radius, which aﬀects the set of
inspected pixel locations.
Based on the observations from our recently presented color-sketch simulator
framework [9], our system presents also density based statistics for the entered
colored circles. Hence the user can observe how many selected frames contain a
pixel (or a subset of pixels) similar to the entered colored point (or a subset of
entered colored points).
2.3
Query by Example Image
In cases where keyword or color-sketch search are not successful, users can still
select promising candidate keyframes from ranked result sets (or images from an
external image search engine) and try to ﬁnd similar keyframes. This option was
already included in the previous version of our tool [4]. As a similarity model,
normalized semantic feature vectors v ∈Rn are frequently used in connection
with a distance function δ : Rn ×Rn →R+
0 (e.g., Euclidean or Cosine distance).
As the semantic vectors, selected neuron activations from a deep convolutional
neural network represent a popular eﬀective choice, for example FC7 features
from an improved version of the 19-layer model used by the VGG team in the
ILSVRC-2014 competition [14]. In the new version of our tool, we focus also
on multi-query retrieval, where users select multiple query objects to determine
the ranking of the keyframes. Given k query feature vectors QE = {vq
i }k
i=1 and
database object feature vector vo, the ranking for vo is deﬁned as
rank example(QE, vo) =

∀vq
i ∈QE
δ(vq
i , vo),
where the Cosine distance is considered as eﬀective and eﬃcient δ.
In order to boost eﬃciency of the ranking that aggregates multiple distances
δ, we consider two optimizations. The ﬁrst optimization is straightforward. As
the semantic vectors from deep neural networks are often sparse, we select only
dimensions of vq
i with non-zero values for the Cosine distance computation. Fur-
thermore, the tool supports an approximation of the distance by ignoring addi-
tional dimensions from vq
i that contain a value lower than a threshold θ ∈R.
The second optimization is again based on the caching of already evaluated dis-
tances δ(vq
i , vo). With these optimizations our tool can eﬃciently search (even
sequentially) the selected keyframes from the actual video collection.
3
Tool Interface Overview
The presented tool follows a classical arrangement with a query initialization
panel on the left and results presentation panels in the remaining area.

Revisiting SIRET Video Retrieval Tool
423
The left panel consists of several query formulation panels. The ﬁlters and
settings panel enables users to manually turn on/oﬀvarious ﬁlters (e.g., black
and white frames) and the combinations of retrieval models and their settings.
The color sketch panel serves as a sketch drawing canvas, where users place/edit
colored circles and their radii. The keyword search panel accepts textual queries
and provides also suggestions on all possible inputs by searching over category
names and descriptions acquired from WordNet. The suggestions try to eliminate
a downside of a limited text input. The example images panel serves as a multi-
query container of images from external servers or collected during the search.
The visualization part consists of a sorted image grid (either by color or
semantic descriptors) presenting the actual top ranked results and a panel pre-
senting a representative subset of a video corresponding to a selected frame in
the grid. Thus, users can quickly observe a higher number of top matching frames
in the early stages of the search with fast inspection of the corresponding video
context for a selected frame. The tool supports also two additional visualizations
of the ranking – the inspection of top ranked videos and the inspection of the
top ranked frames, potentially accompanied with the temporal context.
4
Conclusion
In this paper, we present a revisited version of the SIRET video retrieval tool, a
follow up of the previously introduced approaches of the SIRET team. We present
new versions of keyword search, sketch-based retrieval and multi-querying. In
the future, we would like to further investigate additional database inspection
approaches and employ novel classiﬁcation models.
Acknowledgments. This paper has been supported by Czech Science Foundation
(GAˇCR) project Nr. 17-22224S and by grant SVV-2017-260451.
References
1. Amato, G., Falchi, F., Gennaro, C., Rabitti, F.: Searching and annotating 100M
images with YFCC100M-HNFC6 and MI-ﬁle. In: Proceedings of the 15th Interna-
tional Workshop on Content-Based Multimedia Indexing, CBMI 2017, New York,
NY, USA, pp. 26:1–26:4. ACM (2017)
2. Awad, G., Butt, A., Fiscus, J., Michel, M., Joy, D., Kraaij, W., Smeaton, A.F.,
Qu´enot, G., Eskevich, M., Ordelman, R., Jones, G.J.F., Huet, B.: TRECVID 2017:
evaluating ad-hoc and instance video search, events detection, video captioning and
hyperlinking. In: Proceedings of TRECVID 2017. NIST (2017)
3. Barthel, K.U., Hezel, N., Mackowiak, R.: Navigating a graph of scenes for exploring
large video collections. In: Tian, Q., Sebe, N., Qi, G.-J., Huet, B., Hong, R., Liu, X.
(eds.) MMM 2016. LNCS, vol. 9517, pp. 418–423. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-27674-8 43
4. Bla˘zek, A., Loko˘c, J., Kubo˘n, D.: Video hunter at VBS 2017. In: Amsaleg, L.,
Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017. LNCS,
vol. 10133, pp. 493–498. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-51814-5 47

424
J. Lokoˇc et al.
5. Blaˇzek, A., Lokoˇc, J., Skopal, T.: Video retrieval with feature signature sketches. In:
Traina, A.J.M., Traina, C., Cordeiro, R.L.F. (eds.) SISAP 2014. LNCS, vol. 8821,
pp. 25–36. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11988-5 3
6. Bud´ıkov´a, P., Batko, M., Zezula, P.: Fusion strategies for large-scale multi-modal
image retrieval. Ttans. Large-Scale Data Knowl.-Centered Syst. 33, 146–184 (2017)
7. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimed. Tools Appl. 76(4), 5539–
5571 (2016)
8. Lokoˇc, J., Blaˇzek, A., Skopal, T.: Signature-based video browser. In: Gurrin, C.,
Hopfgartner, F., Hurst, W., Johansen, H., Lee, H., O’Connor, N. (eds.) MMM
2014. LNCS, vol. 8326, pp. 415–418. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-04117-9 49
9. Lokoˇc, J., Phuong, A.N., Vomlelov´a, M., Ngo, C.-W.: Color-sketch simulator: a
guide for color-based visual known-item search. In: Cong, G., Peng, W.-C., Zhang,
W.E., Li, C., Sun, A. (eds.) ADMA 2017. LNCS (LNAI), vol. 10604, pp. 754–763.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-69179-4 53
10. Rossetto, L., Giangreco, I., T˘anase, C., Schuldt, H., Dupont, S., Seddati, O.:
Enhanced retrieval and browsing in the IMOTION system. In: Amsaleg, L.,
Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017. LNCS,
vol. 10133, pp. 469–474. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-51814-5 43
11. Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large
scale visual recognition challenge. Int. J. Comput. Vis. 115(3), 211–252 (2015)
12. Schoeﬀmann, K.: A user-centric media retrieval competition: the video browser
showdown 2012–2014. IEEE Multimed. 21(4), 8–13 (2014)
13. Schoeﬀmann, K., Hudelist, M.A., Huber, J.: Video interaction tools: a survey of
recent work. ACM Comput. Surv. 48(1), 14 (2015)
14. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. CoRR, abs/1409.1556 (2014)
15. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan,
D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston,
MA, USA, 7–12 June 2015, pp. 1–9 (2015)

Sketch-Based Similarity Search for Collaborative
Feature Maps
Andreas Leibetseder(B), Sabrina Kletz, and Klaus Schoeﬀmann
Institute of Information Technology, Klagenfurt University (AAU),
9020 Klagenfurt, Austria
{aleibets,sabrina,ks}@itec.aau.at
Abstract. Past editions of the annual Video Browser Showdown (VBS)
event have brought forward many tools targeting a diverse amount of
techniques for interactive video search, among which sketch-based search
showed promising results. Aiming at exploring this direction further, we
present a custom approach for tackling the problem of ﬁnding similarities
in the TRECVID IACC.3 dataset via hand-drawn pictures using color
compositions together with contour matching. The proposed methodol-
ogy is integrated into the established Collaborative Feature Maps (CFM)
system, which has ﬁrst been utilized in the VBS 2017 challenge.
Keywords: Interactive video search · Collaboration
Sketch-based search
1
Introduction
Collocated at the International Conference on Multimedia Modeling (MMM),
VBS1 is challenging participating teams to compete in solving former TRECVID
tasks of known-item search (KIS) [10] and a currently also include ad-hoc video
search (AVS) [1] in a large video dataset2. Past VBS events showed that the
concept of drawing simple sketches for relevant shot retrieval can be leveraged
to achieve excellent results [5], even in cases surpassing all other approaches
and coming out victorious, as has been the case in 2014 [6] as well as 2015 [2].
CFM [9], ﬁrst introduced in VBS 2017, oﬀer a great variety of collaborative
search modalities, yet, other than a simple dominant color ﬁlter, it does not
oﬀer the possibility of sketch-based similarity search. Further improving the sys-
tem, we thus incorporate custom sketch-based similarity search as an extension,
as illustrated in Fig. 1. Among several already built-in extensions, the newly
integrated feature takes a hand-drawn image as an input and in a ﬁrst ﬁltering
step retrieves TRECVID keyframes exhibiting a similar color composition. In a
second step these results are further reﬁned by matching selected contours from
the sketch. Following sections describe the proposed addition in more detail:
1 VBS has ﬁrst been organized in 2012 [8].
2 The current IACC.3 dataset contains 600 h of video.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 425–430, 2018.
https://doi.org/10.1007/978-3-319-73600-6_45

426
A. Leibetseder et al.
Sect. 2 gives an overview over the proposed approach’s architecture, starting out
with Sect. 2.1 describing the user interface and ending in Sect. 2.2 detailing the
underlying methodologies. Finally, Sect. 3 concludes this work.
Fig. 1. Collaborative Feature Maps extension: sketch-based workﬂow. (Color ﬁgure
online)
2
Sketch-Based Similarity Search
As indicated above, the extension’s main workﬂow can be split up into two
separate tasks: drawing and reviewing results. Although created sketches should
be fairly rudimentary, as is shown in Fig. 2 portraying the VBS 2017 query
topics “palm tree” and “playing guitar outdoor”, the activity can be much more
time-consuming than for example text-based search. Hence, it is best for the
collaborating team members to dynamically assume diﬀerent roles as needed.
Accordingly, a plausible scenario for handling a given query could well be all
team members starting out drawing and gravitating towards either continuing
this task, i.e. illustrating an alternate perspective or adding more details, or
processing result lists of already created sketches.
Fig. 2. Sample sketches drawn by an artist. (Color ﬁgure online)

Sketch-Based Similarity Search for Collaborative Feature Maps
427
Since tablets, smart phones or any other pen input device are growing in
illustration usage, these devices provide simple sketch-based drawing tools, which
are suﬃcient to create rudimentary sketches. Therefore, the system does not
provide a supplementary interface to create sketches. The artists simply use
tools of their choice to create digital images, which they transfer to a location
visible for the system. Upon receiving completed sketches the system conducts
implemented similarity searches (see Sect. 2.2), to retrieve result lists for each of
the provided paintings, displayed in the provided result processing interface.
2.1
Interface
We extend the traditional interface of the system by an additional view, shown
in Fig. 3. After choosing a particular drawing from any available input device,
a user is provided with a simple sketch visualization and ranked result lists
according to several partially user-deﬁned criteria.
Fig. 3. Screenshot of the extended color sketch view - prototype UI (Color ﬁgure online)
The main focus of the sketch view is the reﬁnement of paintings to narrow
down the result set. A reduced set of dominant colors, i.e. color composition,
are extracted for the sketch, which initially is treated entirely as belonging to
an image background [4,7]. Optionally, a user can deﬁne parts of the drawing
as a foreground region, which is useful for bringing focus to moving objects.

428
A. Leibetseder et al.
All detected dominant colors are displayed according to their visibility in the
sketched scene. The presence of colors is indicated by the percentage of their
occurrence including the unpainted or transparent area, intentionally left blank
by the artist. Collaborators are able to change, add or remove colors using a
preset reduced color palette and even re-weight color dominance.
Beside the reﬁnement of the color compositions, contours are an essential
feature outlining shapes. In the majority of cases a rough color sketch consists of
color regions, which do not necessarily contain meaningful contours. Therefore
coherent areas, such as the crown of the palm tree can be selected and used as
expanded matching criteria. Since the sketch view is integrated into the CFM
system and in the case that shapes do not result with the desired content, the
underlying query can as well be searched via textual ﬁltering (see ‘Tags’ ﬁltering
option in ﬁgure), which utilizes the indexed concepts already provided by the
system.
Finally, when a new sketch is processed, i.e. all ﬁltering criteria have been
applied, the system interface displays the results in a storyboard arrangement.
The result set is split up into shots with one storyboard each. The storyboards
are arranged in sub views line by line and consist of frames corresponding to the
individual shots. In each sub view, the frames are aligned from left to right and
from top to bottom. The visible part of the sub view starts with the keyframe
containing the sketched scene or object and the following frames are arranged by
time stamp. To get an overview of the whole shot, the corresponding sub view
can be expanded and still remaining frames will be loaded. The sub views are
ranked according to their similarity to the input sketch and can be re-ranked by
various matching criteria, as described previously.
2.2
Details
While traditional sketch-based retrieval systems provide line-based sketches,
either with colored lines or simple black lines, color sketches that outline a scene
with color regions lack in detail to use shapes as matching criteria [3,11]. Moti-
vated by this drawback, the system extension exploits a two-fold approach, where
for any given sketch ﬁrst color compositions are matched against precalculations
on the TRECVID dataset and then selected coherent contours are used for shape
matching. Whereas drawing objects with contours is often problematic due to
the vast amount of diﬀerent potential perspectives, dominant colors are consid-
ered the best choice to describe a scene independent of its position in an image
and contained movement. Even black and white scenes can be outlined by means
of diﬀerent grayscale regions. Since shots contain several dominant color compo-
sitions changing in time and space, the dominant colors of each contained frame
are separately addressed and used as a matching criteria.
The implementation of color compositions is based on image segmentation
approaches [4,7]. This has the advantage that due to the color reduction, the
most dominant colors and as well roughly outlined shapes can be identiﬁed.
In case the scene contains moving objects, they are indicated as belonging to
the foreground when applying background subtraction. Therefore, user-deﬁned

Sketch-Based Similarity Search for Collaborative Feature Maps
429
foreground objects in drawings can merely match TRECVID scenes containing
moving objects, i.e. with an existing foreground. Regardless of background or
foreground objects, edge detection approaches provide at least some contours
that can be used to align outlines using distance transformations [11]. How-
ever, drawing color regions and objects and matching them to their most similar
occurrence in shots requires several computationally intensive steps. Therefore,
dominant colors and objects in the dataset are pre-indexed, while merely a sketch
is processed online, i.e. dominant colors and contours are extracted for applying
two-fold similarity measures. Thus, an initial result list can be produced accord-
ing to color composition and then narrowed down through matching contours.
3
Conclusion
We present an extension for the Collaborative Feature Maps system used in the
Video Browser Showdown 2017 challenge based on hand-drawn sketches of col-
laborating artists. The extension employs a two-fold strategy for ﬁnding similar
shots to corresponding drawings: matching dominant colors on the pre-indexed
TRECVID dataset and allowing for further result reﬁnement via determining
similar object contours. Segmentation techniques further allow for a more ﬁne-
grained sketch partitioning and retrieval improvement.
References
1. Awad, G., Fiscus, J., Michel, M., Joy, D., Kraaij, W., Smeaton, A.F., Qu´enot,
G., Eskevich, M., Aly, R., Ordelman, R.: Evaluating video search, video event
detection, localization, and hyperlinking. In: Proceedings of TRECVID, TRECVID
2016, vol. 2016 (2016)
2. Blaˇzek, A., Lokoˇc, J., Matzner, F., Skopal, T.: Enhanced signature-based video
browser. In: He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM
2015. LNCS, vol. 8936, pp. 243–248. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-14442-9 22
3. Bui, T., Collomosse, J.: Scalable sketch-based image retrieval using color gradi-
ent features. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 1012–1019 (2016)
4. Chen, J., Pappas, T.N., Mojsilovi´c, A., Rogowitz, B.E.: Adaptive perceptual color-
texture image segmentation. IEEE Trans. Image Process. 14(10), 1524–1536 (2005)
5. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimedia Tools Appl. 76(4), 5539–
5571 (2017)
6. Lokoˇc, J., Blaˇzek, A., Skopal, T.: Signature-based video browser. In: Gurrin, C.,
Hopfgartner, F., Hurst, W., Johansen, H., Lee, H., O’Connor, N. (eds.) MMM
2014. LNCS, vol. 8326, pp. 415–418. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-04117-9 49
7. Mojsilovi´c, A., Hu, J., Soljanin, E.: Extraction of perceptually important colors
and similarity measurement for image matching retrieval and analysis. IEEE Trans.
Image Process. 11, 1238–1248 (2002)

430
A. Leibetseder et al.
8. Schoeﬀmann, K.: A user-centric media retrieval competition: the video browser
showdown 2012–2014. IEEE Multimedia 21(4), 8–13 (2014)
9. Schoeﬀmann, K., Primus, M.J., Muenzer, B., Petscharnig, S., Karisch, C., Xu, Q.,
Huerst, W.: Collaborative feature maps for interactive video search. In: Amsaleg,
L., Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017.
LNCS, vol. 10133, pp. 457–462. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-51814-5 41
10. Smeaton, A.F., Over, P., Kraaij, W.: Evaluation campaigns and TRECVid. In:
Proceedings of the 8th ACM International Workshop on Multimedia Information
Retrieval, MIR 2006, pp. 321–330. ACM, New York (2006)
11. Sun, X., Wang, C., Xu, C., Zhang, L.: Indexing billions of images for sketch-based
retrieval. In: Proceedings of the 21st ACM international conference on Multimedia,
MM 2013, pp. 233–242 (2013)

Sloth Search System
Sitapa Rujikietgumjorn(B), Nattachai Watcharapinchai,
and Sanparith Marukatat
National Electronics and Computer Technology Center (NECTEC),
Pathum Thani, Thailand
{sitapa.ruj,nattachai.wat,sanparith.mar}@nectec.or.th
Abstract. In this paper, we present the Sloth Search System (SSS) for
large scale video browsing. Our key concept is to apply object recogni-
tion and scene classiﬁcation to generate keyword tags from video images.
This indexing process is performed only on selected frames for faster pro-
cessing. The keyword tags are used to retrieve videos from a text-based
query. Additional feature signatures are also used to extract spatial and
color information. These proposed signatures are stored as binary codes
for a compact representation and for fast search. Such a representation
allows users to search by drawing a sketch or a bounding box of a speciﬁc
object.
Keywords: Content-based video retrieval · Video search
Convolutional neural networks · Known Item Search · Sketch search
1
Introduction
Video search is still a challenging problem due to the scale of video data and
image representations that are needed to index for fast retrieval. Several infor-
mations such as object types, scenes or actions, can be used for indexing the
video. Other relevant information such as colors in an image or object relations
and interactions may also be useful. For video search, indexing is crucial and
challenging because we need to deﬁne features for the image representation and
there should be enough features for a satisfactory search result. In addition, this
indexing process should perform on an extensive video data set with a query run-
time acceptable for a real application scenario. Another challenge is to retrieve
relevant results matching a user query and how to retrieve it within an acceptable
time limit.
Interactive video search tools integrate the idea of user interaction and
(visual) content analysis into the search process. The Video Browser Showdown
(VBS) is a video search competition where participating teams try to retrieve
relevant videos in a large video dataset as fast as possible [1]. The IMOTION sys-
tem [2] focuses on sketching-based retrieval which supports several types of query
consisting of query-by-sketch, query-by-example, query-by-motion and querying
using semantic concepts. This system is the winner of VBS 2017. Hierarchi-
cal graphs and visually sorted image maps are used by [3]. This video search
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 431–437, 2018.
https://doi.org/10.1007/978-3-319-73600-6_46

432
S. Rujikietgumjorn et al.
Fig. 1. Overview of proposed indexing framework.
system is designed for navigation by a graph of the scene, which makes it easier
to browse large sets of video and it was the winner in 2016. The 2015 winner sys-
tem [4] uses color and position features from the selected frame in the indexing
process and query-by-sketch is applied for retrieval process.
Deep learning has been widely used for object recognition and also yields good
results. But concerning processing time, there is still an issue for large scale video
data. We propose a video search framework called Sloth Search System (SSS)
that uses deep learning to generate keyword tags for the text-based query. For
retrieving a video from a user-sketched input image, we propose to use features
generated by converting the object location into spatial feature vectors and by
converting the sketch-like images into color-based sketch feature vectors.
The rest of the paper is organized as follows. First, we give details of our
framework overview and our video indexing structure. Then, we describe our
approach for extracting objects and scenes information in Sect. 2.3. The spatial
and sketch-based color indexing are explained in Sect. 2.4 and the sketch search
is explained in Sect. 2.5. Finally, we outline the user interaction procedures in
Sect. 2.6.
2
Sloth Search System (SSS)
2.1
Framework Overview
Our video search system mainly consists of three parts: feature indexing, search,
and user interaction. The indexing framework is shown in Fig. 1. In this frame-
work, keyword tags and feature vectors are used as our indexes. The keyword
tags are keywords that describe each video shot or frame for video retrieval
based on text query. To generate these tags, we use object recognition to get an

Sloth Search System
433
Fig. 2. Entity relationship diagram of our Sloth Search System database
object tag and use scene or category classiﬁcation to get a scene or category tag.
As for feature vectors, they are extracted from object spatial information and
sketch-like images. Our search engine retrieves video based on these keyword
tags and feature vectors. For user interaction, the user is required to input one
or multiple queries such as keywords, categories, a bounding box drawing, or an
image sketch. Multiple relevant results are displayed and are ranked based on
their search similarity and the user is required to select the ﬁnal result.
2.2
Video Indexing Structure
Our video indexing structure is designed based on our query-by-text and query-
by-sketch framework. Our database system design is shown in Fig. 2 where each
video is stored in the Video Level table consisting of a unique video number
(video id) and video keyword tags (video tag). These tags are summarized from
scene tags and object name. The videos are segmented into multiple video shots
stored in the Shot Level table consisting of a unique shot number (shot id),
scene types (scene tags) and their timestamps (position time). A unique frame
is selected and is stored in the Selected Frame table consisting of a unique
frame number (frame id) and their timestamps (position time). In these selected
frames, object types and their positions are stored in the Object Annotation
table. For each object, the spatial feature vector is stored as a binary ﬁle (spa-
tial binary code). For the Sketching Level table, the sketch-based color vector is

434
S. Rujikietgumjorn et al.
stored as a binary ﬁle (color binary code). These feature vectors of spatial and
sketch-based color indexing will be described in Sect. 2.4.
2.3
Objects and Scenes for Video Indexing
We use a deep learning framework for object recognition and localization. Since
deep learning framework has a high computational time, it is not feasible to
perform the object recognition or scene classiﬁcation on every frame of a large
video data set such as the TRECVID IACC.3 dataset [5]. As shown in Fig. 1,
we perform these two tasks only on selected frames to generate keyword tags for
each frame. An automatic video shot segmentation is applied to divide the whole
video sequences into multiple video shots and some frames from those shots are
selected.
Object Recognition and Localization
Object types and their positions can be used to retrieve a relevant video. In this
framework, we use the F-RCNN architecture [6] to perform object recognition
and localization. The model we use is pre-trained on the COCO dataset that
has 90 object classes [7]. The positions and bounding boxes of each object are
outputs that will be further used for spatial feature extraction as explained in
Sect. 2.4.
Scene and Category Classiﬁcation
The type of place can be an important information to retrieve a desired video
sequence. Thus, scene and category are used as a text-based index in our system.
We apply the Places365-standard pre-trained CNN model [8] to our framework.
The Places365 standard model has been trained with 365 scene categories and
with 3,068 to 5,000 images per class. This model uses the VGG 16 CNN. For
our indexing framework, the Place365 model is used to deﬁne a place or scene
in the selected frame of a video shot. These scene tags of the selected frames are
also summarized as one or multiple categories for each video shot indexing.
2.4
Spatial and Sketch-Based Color Indexing
The Sloth Search System (SSS) constructs binary indexes for each object cat-
egory. Given an image, the object recognition and localization module outputs
several object categories along with their bounding boxes. Figures 3(a) and (b)
show examples of detection and localization results. For each object category,
we construct a binary mask having the value one inside the bounding boxes of
the considered category and zero elsewhere. This mask is split into 16×8 blocks
and thresholded. This produces 128 binary values. Hence, 16 bytes are needed
to encode the spatial position of each category per image.
As an example, consider the ‘dog’ category that appears in Figs. 3(a) and
(b). The ‘dog’ binary codes from the two images are ‘[0, 0, 0, 0, 0, 0, 0, 0, 112, 0,

Sloth Search System
435
112, 0, 112, 0, 112, 0]’ and ‘[0, 0, 255, 31, 255, 31, 255, 31, 255, 31, 255, 31, 255,
31, 255, 31]’. The overlapped area for ‘dog’ category between these two images
can be computed using bitwise AND and counting the number of ones in the
result. For this example, this yields 12.
Fig. 3. Example of two images (a) and (b) with the detected categories. The mask
associated with the “dog” category is shown.
Besides object category, SSS also performs color indexing for sketch-based
search. Each frame is quantized using a ﬁxed color palette. Then for each color,
the 128 binary values are computed from 16×8 blocks similar to object category
above. Given a palette of 16 color and 5 objects per frame on average, the size
of the index is 336 bytes per frame. This is quite compact and suitable for large
scale video indexing.
2.5
Sketch Search
For sketch search, the user roughly draws an image with colors and/or objects
at desired location. Binary codes are computed from the query image using the
same procedure described in previous section. The obtained codes for objects
and colors that appear in the sketch are compared against the stored codes. To
further speed up the search, the number of ones obtained from bitwise AND
between two bytes is precomputed and stored in a lookup table. Thus, the sim-
ilarity calculation between two binary codes requires only addition making it
extremely fast.

436
S. Rujikietgumjorn et al.
2.6
User Interaction
We design our user interface to support easy searching and browsing for videos.
The query procedure is explained in more detail below.
1. A user deﬁnes a query using the types of scenes or categories and our system
will return some candidate videos that are relevant to the user’s query.
2. From these candidate videos, we can repeat the keyword queries to browse
for more speciﬁc candidate shots.
3. The user selects a shot to be submitted as a retrieval result when the relevant
shot is found.
4. If there are no relevant shots, the sketch search or bounding box search can
also be applied.
5. If no relevant shot is found, the user has to change the query by repeating
the step 1 or 4 again.
3
Conclusion
We propose the Sloth Search System (SSS) as a video browsing tool for the
Video Browser Showdown. This system can be an assistive video searching tool
for Known Item Search (KIS) or Ad-hoc Video Search (AVS) tasks. High-level
semantic features, object spatial information, and color feature signatures are
used to generate compact features for indexing. There are basically two types
of queries through our user interface: query-by-text, and query-by-sketch. Our
search system is capable of ﬁnding video frames with keyword tags, unique color
or object spatial information. The search result is displayed as a list of candidates
that user can browse for a speciﬁc item.
References
1. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimedia Tools Appl. 76(4), 5539–
5571 (2017)
2. Rossetto, L., Giangreco, I., T˘anase, C., Schuldt, H., Dupont, S., Seddati, O.:
Enhanced retrieval and browsing in the IMOTION system. In: Amsaleg, L.,
Guðmundsson, G., Gurrin, C., J´onsson, B., Satoh, S. (eds.) MMM 2017. LNCS,
vol. 10133, pp. 469–474. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
51814-5 43
3. Barthel, K.U., Hezel, N., Mackowiak, R.: Navigating a graph of scenes for exploring
large video collections. In: Tian, Q., Sebe, N., Qi, G.-J., Huet, B., Hong, R., Liu, X.
(eds.) MMM 2016. LNCS, vol. 9517, pp. 418–423. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-27674-8 43
4. Blaˇzek, A., Lokoˇc, J., Matzner, F., Skopal, T.: Enhanced signature-based video
browser. In: He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM
2015. LNCS, vol. 8936, pp. 243–248. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-14442-9 22

Sloth Search System
437
5. Awad, G., Butt, A., Fiscus, J., Joy, D., Delgado, A., Michel, M., Smeaton, A.F., Gra-
ham, Y., Kraaij, W., Qu´enot, G., Eskevich, M., Ordelman, R., Jones, G.J.F., Huet,
B.: Evaluating ad-hoc and instance video search, events detection, video caption-
ing and hyperlinking. In: Proceedings of TRECVID 2017, TRECVID 2017. NIST,
Gaithersburg (2017)
6. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Neural Information Processing Systems
(NIPS) (2015)
7. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,
Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-oﬀs for
modern convolutional object detectors. CoRR abs/1611.10012 (2016)
8. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: a 10 mil-
lion image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell.
PP(99), 1 (2017). https://doi.org/10.1109/TPAMI.2017.2723009

The ITEC Collaborative Video Search System
at the Video Browser Showdown 2018
Manfred J¨urgen Primus(B), Bernd M¨unzer, Andreas Leibetseder,
and Klaus Schoeﬀmann
Institute of Information Technology, Klagenfurt University, Klagenfurt, Austria
{juergen.primus,bernd,aleibets,ks}@itec.aau.at
Abstract. We present our video search system for the Video Browser
Showdown (VBS) 2018 competition. It is based on the collaborative sys-
tem used in 2017, which already performed well but also revealed high
potential for improvement. Hence, based on our experience we introduce
several major improvements, particularly (1) a strong optimization of
similarity search, (2) various improvements for concept-based search, (3)
a new ﬂexible video inspector view, and (4) extended collaboration fea-
tures, as well as numerous minor adjustments and enhancements, mainly
concerning the user interface and means of user interaction. Moreover,
we present a spectator view that visualizes the current activity of the
team members to the audience to make the competition more attractive.
Keywords: Video retrieval · Interactive search · Collaboration
1
Introduction and Related Work
The Collaborative Video Search System (CoViSS) [10] has been proven to work
very well for the challenges of the 6th Video Browser Showdown (VBS), where
it achieved the second place. The VBS is an annual event comparing interactive
video retrieval tools in a live evaluation. The 2017 edition featured three diﬀerent
challenges: (i) visual Known-Item Search (KIS), where the goal is to ﬁnd a
speciﬁc short segment of a video that is visually presented to the participants, (ii)
textual KIS, where the content of the target segment is only described textually
(without any visual information), and (iii) Ad-hoc Video Search (AVS), where
a more general query is presented as a textual description, and multiple target
segments should be submitted. The competing teams have to solve each task
within ﬁve minutes. The video archive used in this contest consists of 4,593
videos with an overall duration of 600 h (TRECVID IACC.3 dataset [1]). The
videos are segmented into approximately 340,000 shots. Detailed information
about the challenges are given by Schoeﬀmann [9] and Cobˆarzan et al. [4].
Our system for the 7th VBS is based on our highly successful video retrieval
system of the previous year, which combines many ideas we experienced over
the years with previously developed systems. It also adapts ideas presented by
competitors, e.g., the 3D perspective map proposed by Barthel et al. [2] (winner
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 438–443, 2018.
https://doi.org/10.1007/978-3-319-73600-6_47

The ITEC Collaborative Video Search System
439
2016) or the Feature Signatures used by Blaˇzek et al. [3] (winner 2015). More-
over, we include a Storyboard interface with many small thumbnails for rapid
visual inspection, inspired by H¨urst et al. [5], who scored third in 2015 - without
any content analysis. The main view of the graphical user interface allows to
hierarchically browse through the shots on a 3D perspective map. The shots are
arranged according to their similarity, based on diﬀerent features (e.g., color, tex-
ture, concept, motion). The system provides a manifold set of search and ﬁltering
mechanisms that enable diﬀerent search strategies, and thus support diﬀerent
types of queries. This includes, amongst others, concept search, search by web
example, search in a storyboard and search with an HSV color ﬁlter. Another
core aspect in our system is the support of collaboration between users, e.g.,
indicating the current search position as well as search history of collaborators
on a minimap. This helps to exclude areas, which have already been scanned by
co-workers. Moreover, we use the information of the users for adapting a ranked
list of videos, which is used for showing more important videos in the front or
less important videos at the end of a view, e.g., in the storyboard view.
We provide a brief overview of beneﬁts and shortcomings of the 2017 ver-
sion of CoViSS, and detailed information about our changes and improvements
in Sect. 2. Section 3 describes the spectator view that allows the audience to
observe all actions performed in the collaborative system at a single point of
view. Section 4 summarizes the improvements of our system.
2
Lessons Learned and Proposed Improvements
During the preparation and especially at VBS 2017 several characteristics of the
system have been identiﬁed that worked very well for the search tasks. Never-
theless, there were also various shortcomings of the system, which are described
in the next section, before we present the main adjustments that have been
introduced in CoViSS for VBS 2018.
2.1
Addressing the Limitations of CoViSS 2017
A motion-sensitive shot detection algorithm has been used to select appropriate
keyframes. The shots found by this algorithm have some advantages over the
master shot reference, as described in [8]. One adverse consequence is that the
number of shots and the alignment of the shot boundaries do not match. While
this is not a problem for KIS tasks, which require detailed inspection, it is a
problem for AVS tasks, where the goal is to ﬁnd all matching shots. This is
cumbersome if the provided shots do not ﬁt to the given shots.
The web example search is used to ﬁnd sample images in a third-party image
search engine. If a good example image has been found externally (e.g., with
Google), a similarity search to the internal dataset is performed. To increase
the search speed, a hierarchical arrangement of the feature map is exploited.
Therefore, the similarity search is performed in the ﬁrst layer of the map that
contains only 1,600 keyframes. After that it is repeated in the corresponding area

440
M. J. Primus et al.
of the matching keyframe in a lower, reﬁned map. This simple solution reduces
the search time to a practical level and allows the user to start a further search
in a reasonable area in the presorted feature map. Practice, however, has shown
that the quality of the similarity search results are not always satisfactory.
The concept-based search uses two diﬀerent convolutional neural networks
(CNNs)—the well-known AlexNet [6] and a self-tuned version of it. Both CNNs
allow to index the keyframes with 1,077 diﬀerent concepts. The concept search
assumes that the user know exactly which class names are available. Although the
1,000 classes of AlexNet have been increased by 77 additional missing concepts
it turned out that a greater variability of search terms would be desirable.
Feature maps are a core utility of CoViSS. In a feature map all keyframes are
arranged in a two-dimensional grid, based on their similarity. Additionally, the
feature maps are halved multiple times in size. For every adjacent layer a single
image from a 2 × 2 image area is chosen as representative one – a method that
leads to a pyramid image arrangement. An essential requirement is a suitable
layout based on a given similarity metric. For the 6th VBS we used 15 diﬀerent
similarity metrics (based on concepts, color, texture, motion, and combinations).
Primary, only concept similarity and color maps have been used by the users.
Further, it revealed that the arrangement within the maps is still not optimal.
2.2
Improvements and New Features
Similarity search is a core functionality of an image and video retrieval system.
The usefulness of such a function depends largely on the time it needs to return
the result set. Our implementation is based on the work of Mic´o et al. [7], which
exploits a pivot table for fast search. At search time this pivot table is used to
ﬁnd the heuristically best candidate and to prune all objects in the database
that can not be nearer to the target than the current candidate. This very fast
nearest neighbor search algorithm is used to ﬁnd the most similar keyframe(s)
to a query image, which can be either an image from a web search or each other
image (keyframe or other frames from the video archive). In particular, any view
of the system allows a similarity search for any shown image.
The video inspector view is a new view that visualizes the content of a single
video. Keyframes of the video are shown within a grid. The keyframes can be
the keyframes from the mastershots, keyframes from shots found by the motion-
sensitive shot detection algorithm, or uniform sampled keyframes with a variable
granularity. This allows the user to select a visualization of the whole content of
a video that ﬁts to his current needs. E.g, the user can switch between diﬀerent
sets of keyframes or adjust the uniform sample rate.
The concept search is a powerful search instrument. To overcome some limita-
tions of the previous system more CNN models are incorporated. More precisely,
we use models from the Caﬀe model zoo, e.g., Inception-BN trained for 21,841
ImageNet classes, Places-CNN and Sun397.
For the detailed inspection of a certain video an advanced video player has
been implemented as shown in Fig. 1. The functions of the video view are con-
trolled by keyboard and mouse. Such functions are, e.g., the possibility to jump

The ITEC Collaborative Video Search System
441
Fig. 1. The interface provides diﬀerent content features for presentation the similarity
arrangement in the feature map (in the background), a video player with indications of
shot boundaries according to diﬀerent methods (lower part of playback window), and
a browsing history (on the right).
between shots, fast-forward and reverse, the change of the playback speed of the
video, and the visualization of diﬀerent timelines. Each timeline can visualize
diﬀerent sets of shots. For instance, in Fig. 1 the upper timeline shows shots
from the motion-sensitive shot detection algorithm. The lower one shows the
shots of the mastershot reference.
The minimap is extended to provide information about collaborating team
members and positions of images in the current feature map. Collaborating team
members are shown in the minimap with diﬀerent colored points. These points
provide information at which position in a feature map the other members are
currently searching. If a similarity search is performed, the positions of the
images of the result list are visualized in the minimap. If the storyboard of
a single video is opened or if the video player is used, the minimap visualizes the
distribution of the shots that belongs to this video (lower left in Fig. 1).
In the context of collaboration we introduce positive and negative suggestions
as well as a hot video list. If a user ﬁnd videos or shots that might be interesting
for collaborating users—that could be interesting starting points for a search
or entire videos to inspect—he can send the IDs of these objects to the other
users. The GUI shows a list with these suggestions in the upper right area of the
GUI. The entries of this list can be used by every collaborating user. A user can
make also negative suggestions, which means that a single shot or the shots of an
entire video are marked with a semitransparent cross in every view or optionally
these shots are hidden in ﬁlter results (hiding is not possible in the feature maps,
because they are pre-rendered). Additionally, the system calculates a hot video
list. During the search each collaborator visits shots explicitly but also implicitly,

442
M. J. Primus et al.
e.g., due a concept search they get back a result list of 1,000 shots, where they
only recognize the ﬁrst part of the shots. The system keep account of these visits
for each shot at each view. In the background the shots are ranked accordingly
to some values (e.g., place in a list, retention time at a video) and presented
at the upper left-hand part of the system. A user can inspect these entries and
remove inappropriate hints from the list, giving them a negative suggestion.
3
Spectator View
The spectator view provides an overview of all collaborators’ actions in order
to better portray their ongoing eﬀort to bystanders. Figure 2 illustrates the pre-
sentation of a few CoViSS search modalities. Just as collaborators in CoViSS
communicate to each other via WebSocket, a spectator view assumes a similar
but inactive role. Multiple spectator views can be opened receiving continuous
updates on the team members’ activities. In its default setting such a view auto-
matically cycles through all known collaborators displaying their present choice
of search strategy as well as current results. Additionally, single collaborators
can be pinned, meaning that the view exclusively focuses on the selected person.
Regardless, important actions such as viewing videos will always be displayed for
all participants, albeit in smaller scale within a sidebar. Finally, special attention
is payed to the most crucial action, i.e. submitting video shots, which is triggered
when any collaborator hands in a discovered result and displays the submission
as a timed overlay to the spectator view’s main window. Successive submissions
are queued and presented in order, yet, they as well are assigned a time to live
as to prevent huge queue lengths on submission intensive queries.
(a) Color Picker
(b) Concept search
(c) Web Example
(d) Storyboard
Fig. 2. Diﬀerent modalities visualized in the spectator view.
4
Summary
We present an iterative improvement of our ﬂexible and collaborative video
search system that was used for the VBS 2017 competition and scored second
there. Based on our experience we optimized many diﬀerent components (such as
concept-based search and similarity search) and introduced several new features
(such as a video inspector view and a spectator view). Our changes of the CoViSS

The ITEC Collaborative Video Search System
443
system comprise also many small optimization and tunings that should further
improve the search performance of the tool and provide an even higher ﬂexibility
in terms of interaction features for both visual and textual search tasks.
Acknowledgment. This work was supported by Universit¨at Klagenfurt and Lakeside
Labs GmbH, Klagenfurt, Austria and funding from the European Regional Develop-
ment Fund and the Carinthian Economic Promotion Fund (KWF) under grant KWF-
20214 U. 3520/26336/38165.
References
1. Awad, G., Butt, A., Fiscus, J., Joy, D., Delgado, A., Michel, M., Smeaton, A.F.,
Graham, Y., Kraaij, W., Qu´enot, G., Eskevich, M., Ordelman, R., Jones, G.J.F.,
Huet, B.: Trecvid 2017: evaluating ad-hoc and instance video search, events detec-
tion, video captioning and hyperlinking. In: Proceedings of TRECVID 2017, NIST,
USA (2017)
2. Barthel, K.U., Hezel, N., Mackowiak, R.: Navigating a graph of scenes for exploring
large video collections. In: Tian, Q., Sebe, N., Qi, G.-J., Huet, B., Hong, R., Liu, X.
(eds.) MMM 2016. LNCS, vol. 9517, pp. 418–423. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-27674-8 43
3. Blaˇzek, A., Lokoˇc, J., Matzner, F., Skopal, T.: Enhanced signature-based video
browser. In: He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.) MMM
2015. LNCS, vol. 8936, pp. 243–248. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-14442-9 22
4. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimed. Tools Appl. 76(4), 5539–
5571 (2017)
5. H¨urst, W., van de Werken, R., Hoet, M.: A storyboard-based interface for mobile
video browsing. In: He, X., Luo, S., Tao, D., Xu, C., Yang, J., Hasan, M.A. (eds.)
MMM 2015. LNCS, vol. 8936, pp. 261–265. Springer, Cham (2015). https://doi.
org/10.1007/978-3-319-14442-9 25
6. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: IEEE Conference
on Computer Vision and Pattern Recognition 2014 (CVPR), pp. 1725–1732. June
2014
7. Mic´o, M.L., Oncina, J., Vidal, E.: A new version of the nearest-neighbour approx-
imating and eliminating search algorithm (AESA) with linear preprocessing time
and memory requirements. Pattern Recogn. Lett. 15(1), 9–17 (1994)
8. Primus, M.J., Muenzer, B., Petscharnig, S., Schoeﬀmann, K.: ITEC-UNIKLU: Ad-
hoc video search submission 2016. In: Proceedings of TRECVID 2017. NIST, USA
(2016)
9. Schoeﬀmann, K.: A user-centric media retrieval competition: the video browser
showdown 2012–2014. MultiMed. IEEE 21(4), 8–13 (2014)
10. Schoeﬀmann, K., Primus, M.J., Muenzer, B., Petscharnig, S., Karisch, C., Xu, Q.,
Huerst, W.: Collaborative feature maps for interactive video search. In: Amsaleg,
L., Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017.
LNCS, vol. 10133, pp. 457–462. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-51814-5 41

VERGE in VBS 2018
Anastasia Moumtzidou1(B), Stelios Andreadis1, Foteini Markatopoulou1,2,
Damianos Galanopoulos1, Ilias Gialampoukidis1, Stefanos Vrochidis1,
Vasileios Mezaris1, Ioannis Kompatsiaris1, and Ioannis Patras2
1 Information Technologies Institute,
Centre for Research & Technology Hellas, Thessaloniki, Greece
{moumtzid,andreadisst,markatopoulou,dgalanop,heliasgj,
stefanos,bmezaris,ikom}@iti.gr
2 School of Electronic Engineering and Computer Science,
QMUL, London, UK
i.patras@qmul.ac.uk
Abstract. This paper presents VERGE interactive video retrieval
engine, which is capable of browsing and searching into video content.
The system integrates several content-based analysis and retrieval mod-
ules including concept detection, clustering, visual and textual similarity
search, query analysis and reranking, as well as multimodal fusion.
1
Introduction
VERGE interactive video search engine integrates several multimodal indexing
and retrieval modules that allow for eﬃcient browsing, and retrieval of video col-
lections. VERGE has participated in several video retrieval related conferences
and showcases such as TRECVID [1], and Video Browser Showdown (VBS)
[2] and has evolved in order to support Known Item Search (KIS), Instance
Search (INS) and Ad-Hoc Video Search tasks (AVS). The VERGE system par-
ticipating in VBS 2018 incorporates several changes related to the user interface
and the functionalities supported compared to the previous version. Section 2
presents the updated modules of the VERGE system and Sect. 3 describes the
new VERGE user interface.
2
Video Retrieval System
VERGE combines advanced browsing and retrieval functionalities with a user-
friendly interface, and supports the submission of queries, the fusion and the
ﬁltering of relevant results. The following indexing and retrieval modules are
integrated in the developed search application: (a) Visual Similarity Search; (b)
High Level Concepts Retrieval; (c) Automatic Query Formulation and Expan-
sion; (d) ColorMap and Video Clustering; (e) Text Based Search; and (f) Mul-
timodal Fusion for Multimedia Retrieval. The above modules allow the user to
search through a collection of images and/or video keyframes and the user is
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 444–450, 2018.
https://doi.org/10.1007/978-3-319-73600-6_48

VERGE in VBS 2018
445
Fig. 1. VERGE framework.
presented with the results of each module through the graphical user interface.
Moreover, the system allows the reranking of the top-N results returned, combin-
ing also multiple modalities to deliver the ﬁnal reranked list of retrieved results.
Figure 1 depicts the general framework of the VERGE system.
2.1
Visual Similarity Search
The visual similarity search module performs content-based retrieval using deep
convolutional neural networks (DCNNs). First, a GoogleNet [3] is trained on
5055 ImageNet concepts, and then, the output of the last pooling layer, with
dimension 1024, is used as a global keyframe representation. Fast retrieval of
visual similar images is achieved through an IVFADC index database vectors.
Then, the K-Nearest Neighbours are computed for the query image [4].
2.2
High Level Concepts Retrieval
This module indexes the video shots based on 1000 ImageNet concepts, 345
TRECVID SIN concepts, 500 event-related concepts, and 205 place-related con-
cepts [5]. To obtain scores regarding the 1000 ImageNet concepts, we applied
ﬁve pre-trained ImageNet deep convolutional neural networks (DCNNs) on the
AVS test keyframes [5]. The output of these networks was averaged in terms of
arithmetic mean to obtain a single score for each of the 1000 concepts. To obtain
the scores regarding the 345 TRECVID SIN concepts we ﬁne-tuned (FT) the
ResNet pretrained ImageNet network on the 345 concepts using the TRECVID
AVS development dataset and the extension strategy proposed in [6]. We applied
the ﬁne-tuned network on the AVS development dataset and we used as a fea-
ture (i.e., a global keyframe representation) the output of the last hidden layer to
train one Support Vector Machine (SVM) per concept. Subsequently, we applied

446
A. Moumtzidou et al.
this FT network on the AVS test keyframes to extract features, and used them
as input to the trained SVM classiﬁers in order to gather scores for each of
the 345 concepts. Finally, to obtain scores for the event- and place-related con-
cepts we applied the publicly available DCNNs that have been ﬁne-tuned on the
EventNet [7] and Places [8] dataset, respectively.
2.3
Automatic Query Formulation and Expansion Using High-Level
Concepts
This module formulates and expands an input query in order to translate it into
a set of high-level concepts CQ, as proposed in [9]. First, we search for one or
more high-level concepts that are semantically similar to the entire query, using
the Explicit Semantic Analysis (ESA) measure [10]. If such concepts are found
(according to a threshold θ) we assume that the query is well described by them,
the selected concept(s) is (are) added in the set CQ (which is initially an empty
set), and no further action is taken. If this is not the case, we examine if any
of the concepts in our concept pool appears in any part of the query by string
matching, and if so these concepts are added in CQ. Then, we transform the
original query to a set of elementary “subqueries”, using Part-of-Speech tagging
and a task-speciﬁc set of NLP rules. For example, if the original query contains
a sequence in the form “Noun Verb Noun”, this triad is considered to be a
“subquery”; the motivation is that such a sequence is much more characteristic
of the original query than any of these three words alone would be, and moreover
it is easier to ﬁnd correspondences between this and the concepts in our pool,
compared to doing so for a very long and complex query. Subsequently, we check
if any of the “subqueries” exists in our concept pool by calculating the ESA
relatedness between each “subquery” and each of the concepts in the pool. If
there are concepts that exceed the threshold θ, these are added into the set
CQ. Otherwise, the original query and all the subqueries are used as input to
the zero-example event detection pipeline [11], which jointly considers all this
input and attempts to ﬁnd the concepts that are most closely related to it. The
outcome of this pipeline is a set of concepts CQ that describe the input query.
2.4
Clustering
Two clustering approaches are applied for the eﬀective visualization of the
dataset:
ColorMap clustering: Video keyframes are clustered by color using Self Orga-
nizing Maps into color classes, updating the corresponding previous version [12].
Three MPEG-7 descriptors related to color (i.e. Color Layout, Color Structure,
Scalable Color) are extracted from each video frame, and each color cluster pro-
vides a representative image for visualization in the VERGE GUI.
Video clustering: This method clusters videos by using the visual features of
their keyframes. Speciﬁcally, for each video, we retrieve the top-N most similar
keyframes of each video keyframe. Then, a simple majority vote algorithm is

VERGE in VBS 2018
447
applied, which counts the frequencies of the returned videos. In the sequel, the
frequency values are normalized per video, and we consider as similar to a video
the top-M videos that are linked with a similarity value exceeding a pre-deﬁned
value. The videos and the links among them are visualized as a network.
2.5
Text Based Search
The text based search module allows for text retrieval in the metadata text
describing each video, in the text extracted from the ASR (provided with the
video data), and ﬁnally in captions extracted from the keyframes using Dense-
Cap [13] using Apache Lucene. It should be noted that textual concepts are
also extracted using the DBpedia Spotlight annotation tool, which annotates
automatically DBpedia entities in natural language text [14].
2.6
Multimodal Fusion and Search
The fusion module combines high-level concepts and low-level features from
visual content with video textual metadata and ASR output, by fusing the sim-
ilarities per modality in a non-linear graph-based approach [15]. The best per-
forming modality among visual descriptors (Sect. 2.1), visual concepts (Sect. 2.2)
and textual concepts (Sect. 2.5) is initially used to ﬁlter-out irrelevant retrieved
results, keeping only the top-l keyframes, which are then reranked. The l × l
similarity matrices and the l × 1 similarity vectors are combined to obtain the
ﬁnal fused similarity score to rank the l keyframes for the ﬁnal output.
3
VERGE User Interface and Interaction Modes
This year we introduce a novel graphical user interface (Fig. 2) with an alterna-
tive look and feel, always designed to oﬀer the end users an intuitive experience
while searching for an image shot or a video. The main diﬀerence from previ-
ous versions is the absence of links and navigation through pages, since now all
retrieval utilities are displayed in a dashboard-like manner and results are shown
in the same page.
The VERGE user interface1 consists of three components: a vertical dash-
board menu on the left, a results panel that covers most of the screen and a
ﬁlm-strip on the bottom right. Details for each component follow.
Describing the menu from top to bottom, it contains the application’s logo,
a countdown that applies to the contest showing the remaining time for submis-
sion, a slider to adjust the size of the shots and an undo button to restore the
previous results. A novel widget is a switch that toggles between two states, New
and Rerank; in the ﬁrst option any search module will retrieve fresh results, while
in the second option resulted shots will be reranked by a selected retrieval app-
roach. Next, all diﬀerent search capabilities follow as sliding boxes. Namely, Text
1 http://mklab-services.iti.gr/vbs2017/.

448
A. Moumtzidou et al.
Fig. 2. Screenshot of VERGE video retrieval engine. (Color ﬁgure online)
Search is a one-line text input ﬁeld that looks for given keywords in the videos’
metadata, e.g. description of content, (Sect. 2.5), Search for Concepts converts
a sentence to recommended concepts (Sect. 2.2), while Concepts present the full
list oﬀering auto-complete suggestion and multiple selection (Sect. 2.3). Further-
more, Events refer to queries that combine persons, objects, locations, and activi-
ties, and Videos provide a video-based representation of results. Finally, Clusters
give a visual grouping of shots that are more similar, and Colors include a palette
with some basic shades that results can be mapped to (Sect. 2.4).
The main component of the user interface displays results as images in a grid
view, ranked according to retrieval scores. Hovering on a single shot allows users
to run the Visual Similarity modality (Sect. 2.1) or submit it to the contest.
Clicking on a shot ﬁlls the ﬁlm-strip on the bottom with every frame of the
video where this speciﬁc frame belongs to, in a chronological order.
To illustrate the capabilities of the VERGE system, follows a simple usage
scenario where we are interested in ﬁnding a woman working on the computer.
We can initiate the search procedure by selecting the concepts “Adult Female
Human” and “Computers” or the relevant event “Sitting with a laptop”. Alterna-
tively, we can type the query in natural language and receive proposed concepts.
After the ﬁrst results are retrieved, we are able to perform visual similarity to
the most relative shot or rerank the images based on a speciﬁc color, e.g. to
detect shots with a blue background. If the above results are not satisfying, we
can always refer to the other modalities, such as clustering or text search in the
metadata. Anytime, the full video of a shot is available in the ﬁlm-strip, just
with a click.

VERGE in VBS 2018
449
4
Future Work
Future work includes developing a color sketching module that provides the user
the ability to draw a sketch using colors. Another feature would be to minimize
the time response of the system in order to improve the user experience.
Acknowledgements. This work was supported by the EU’s Horizon 2020 research
and innovation programme under grant agreements H2020-645012 KRISTINA, H2020-
779962 V4Design, H2020-732665 EMMA, and H2020-687786 InVID.
References
1. Awad, G., Butt, A., Fiscus, J., et al.: Trecvid 2017: Evaluating ad-hoc and instance
video search, events detection, video captioning and hyperlinking. In: Proceedings
of TRECVID 2017. NIST (2017)
2. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., et al.: Interactive video search tools:
a detailed analysis of the video browser showdown 2015. Multimed. Tools Appl.
76(4), 5539–5571 (2017)
3. Szegedy, C., Liu, W., Jia, Y., et al.: Going deeper with convolutions. In: 2015 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 00:1–00:9
(2015)
4. Jegou, H., Douze, M., Schmid, C.: Product quantization for nearest neighbor
search. IEEE Trans. Pattern Anal. Mach. Intell. 33(1), 117–128 (2011)
5. Markatopoulou, F., Moumtzidou, A., Galanopoulos, D., et al.: ITI-CERTH partic-
ipation in TRECVID 2016. In: TRECVID 2016 Workshop, USA (2016)
6. Pittaras, N., Markatopoulou, F., Mezaris, V., Patras, I.: Comparison of ﬁne-tuning
and extension strategies for deep convolutional neural networks. In: Amsaleg, L.,
Guðmundsson, G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017. LNCS,
vol. 10132, pp. 102–114. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-51811-4 9
7. Ye, G., Li, Y., Xu, H., et al.: Eventnet: a large scale structured concept library for
complex event detection in video. In: ACM MM (2015)
8. Zhou, B., Lapedriza, A., Xiao, J., et al.: Learning deep features for scene recognition
using places database. In: NIPS, pp. 487–495 (2014)
9. Markatopoulou, F., Galanopoulos, D., Mezaris, V., et al.: Query and keyframe
representations for ad-hoc video search. In: ICMR, pp. 407–411 (2017)
10. Gabrilovich, E., Markovitch, S.: Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In: Proceedings of 20th International Joint Con-
ference on Artiﬁcial Intelligence, pp. 1606–1611. Morgan Kaufmann Publishers Inc.
(2007)
11. Galanopoulos, D., Markatopoulou, F., Mezaris, V., Patras, I.: Concept language
models and event-based concept number selection for zero-example event detection.
In: ICMR, pp. 397–401. ACM (2017)
12. Moumtzidou, A., et al.: VERGE in VBS 2017. In: Amsaleg, L., Guðmundsson,
G.Þ., Gurrin, C., J´onsson, B.Þ., Satoh, S. (eds.) MMM 2017. LNCS, vol. 10133, pp.
486–492. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-51814-5 46
13. Johnson, J., Karpathy, A., Fei-Fei, L.: DenseCap: fully convolutional localization
networks for dense captioning. In: 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 4565–4574 (2016)

450
A. Moumtzidou et al.
14. Daiber, J., Jakob, M., Hokamp, C., Mendes, P.N.: Improving eﬃciency and
accuracy in multilingual entity extraction. In: Proceedings of the 9th Interna-
tional Conference on Semantic Systems, I-SEMANTICS 2013, pp. 121–124. ACM,
New York (2013)
15. Gialampoukidis, I., Moumtzidou, A., Liparas, D., et al.: Multimedia retrieval based
on non-linear graph-based fusion and partial least squares regression. Multimed.
Tools Appl. 76(21), 22383–22403 (2017)

Video Search Based on Semantic Extraction
and Locally Regional Object Proposal
Thanh-Dat Truong1,2(B), Vinh-Tiep Nguyen1, Minh-Triet Tran2,
Trang-Vinh Trieu1, Tien Do1, Thanh Duc Ngo1, and Dinh-Duy Le1
1 Multimedia Communications Laboratory, University of Information Technology,
Vietnam National University-HCMC, Ho Chi Minh City, Vietnam
{dattt,tiepnv,tiendv,thanhnd,ldduy}@uit.edu.vn, 14521097@gm.uit.edu.vn
2 University of Science, Vietnam National University-HCMC,
Ho Chi Minh City, Vietnam
tmtriet@fit.hcmus.edu.vn
Abstract. In this paper, we propose a semantic concept-based video
browsing system which mainly exploits the spatial information of both
object and action concepts. In a video frame, we soft-assign each locally
regional object proposal into cells of a grid. For action concepts, we
also collect a dataset with about 100 actions. In many cases, actions
can be predicted from a still image, not necessarily from a video shot.
Therefore, we consider actions as object concepts and use a deep neural
network based on YOLO detector for action detection. Moreover, instead
of densely extracting concepts of a video shot, we focus on high-saliency
objects and remove noisy concepts. To further improve the interaction,
we develop a color-based sketch board to quickly remove irrelevant shots
and an instant search panel to improve the recall of the system. Finally,
metadata, such as video’s title and summary, is integrated into our sys-
tem to boost its precision and recall.
Keywords: Semantic extraction · Locally regional object proposal
Saliency detection · Action detection · Color based search
1
Introduction
With the rapid growth of video data from many sources such as social networks,
ﬁlms, broadcast TVs, etc., it is an urgent need to provide users with utilities
for various video retrieval tasks. In this paper, we focus on two scenarios. In the
ﬁrst scenario, we need to search for video shots related to an ad-hoc description
(in text format) of video content. The task for the second scenario is to ﬁnd the
video clip that exactly contains a video shot of interest. These are the two query
types in Video Browser Showdown (VBS) [1]. In this challenge, the dataset used
is IACC.3 video dataset which contains 4593 videos, altogether 600 h in duration.
Participants are required to solve two tasks: Ah-hoc Video Search (AVS) and
Known-Item Search (KIS) corresponding to the two query scenarios mentioned
above.
c
⃝Springer International Publishing AG 2018
K. Schoeﬀmann et al. (Eds.): MMM 2018, Part II, LNCS 10705, pp. 451–456, 2018.
https://doi.org/10.1007/978-3-319-73600-6_49

452
T.-D. Truong et al.
For each AVS task, a verbal expression is used to describe video segments
to be retrieved in a large-scale dataset. With the rapid growth of deep learning,
there are many concept detectors developed with high accuracy. Although there
are thousands of concepts, only a few of them are used as the query in reality.
In contrast to the last year system, we only use about an object detector for
80 common concepts with high accuracy to describe video shots. The YOLO
object detector [2] is used to extract concepts with location information. For fast
indexing, we quantize concept locations into a grid of 7×7 non-overlapping cells
with a soft assignment strategy. To describe the overall properties of a video
shot, we also use video metadata such as its title and the summary of scene
attributes with the models trained from MIT Places [3] and SUN attribute [4]
datasets.
For each KIS task, our system must point out exactly the video shot that was
presented to a user. To solve this problem, we propose to use a locally regional
object proposal in a video frame. To enhance the performance, we use a high-
saliency object detector based on DHSNet [5] to reduce noise for video retrieval.
This method not only removes some noisy objects but also reduces the time of
the oﬄine processing stage. Besides, we use color-based search which allows us
to search video shots based on a raw color sketch. To further improve the search
result, we may use a text-based query to further ﬁlter out irrelevant objects.
2
Semantic Concept Based Search
To process an ad-hoc query (in text), we propose to use semantic concept extrac-
tors and the inverted index structure. Although there are many concepts that
can be expressed in an image, we may need to focus on some of them to process
a given query. For instance, Fig. 2 can be described by independent objects (such
as a dog, a bicycle, a car) or relationships between objects (such as a dog is in
front of a bike).
Since the number of concepts in the real world is unlimited, queries are
unpredictable. To increase the recall of our system, we propose to extract as
much background and foreground information as possible. The purpose of this
work is to cover popular concepts of a video shot that might be useful for video
retrieval. Our proposed system includes two main parts: semantic extraction
using deep models trained on large-scale datasets and semantic feature indexing
using an inverted ﬁle. Figure 1 illustrates our proposed framework with these
two main stages.
2.1
Semantic Extraction
This is the most important part of our proposed system to detect main concepts
in a video frame. Inspired by the rapid growth and success of deep learning
techniques, we combine and attempt to leverage the powerful deep features in a
semantic search task through the usage of advanced machine learning methods,
speciﬁcally Convolutional Neural Network (CNN). In this paper, we focus on the
following types of semantics:

Video Search Based on Semantic Extraction
453
Fig. 1. Proposed system for searching based on semantic description.
– Main Objects: We extract objects in regions that users may be interested in
using saliency map [5]. To classify objects in such regions, we use VGG-16
network proposed by Simonyan and Zisserman [6]. We sample an original
video frame to overlapping 224 × 224 patches, then transfer them to our
pre-trained feedforward network. Feature maps from the output activation
are aggregated with the average pooling approach. The ﬁve objects with the
highest scores are used to represent the video frame.
– Scene Attributes: We can use descriptions including scenes, e.g. indoor, out-
door, building, park, kitchen, etc., to query video shots. We use the state-of-
the-art method [3] to extract scene attributes. This method was trained on
MIT scene and SUN attribute datasets.
– Object Relationships: We may need to express a complicated query with dense
relationships between objects. To deal with the problem, we propose to use
Convolutional Neural Network-Recurrent Neural Network (CNN-RNN) [7] to
generate many sentences from detected objects.
– Metadata: The metadata of a video, such as its title, content summary, or
tags, may be available. Such data often reﬂects the main topic of a video but
does not provide many details. In some cases, we can exploit such information
to improve the performance of our system by combining that information with
other semantic concepts as mentioned above.
2.2
Action Detection from Static Image
A video frame contains not only objects but also relationships and actions of
objects. To solve a complex retrieval task corresponding to a query sentence
related to actions and/or object relationships, we propose an action detector
which describes relationships between objects or actions of an object. In this way,
we can search video shots based on complex sentences which contain actions. To
learn the model for the action detection, we use the end-to-end network proposed
in YOLO [2]. Figure 4 illustrates an example of a handshaking action. In this
ﬁgure, we can see that the action is represented in a small region in a video
frame. As most action datasets do not point out exactly which regions contain

454
T.-D. Truong et al.
actions like other object datasets such as ImageNet or Pascal VOC, we manually
create our own action dataset with about 100 popular actions.
2.3
Building Inverted Index
After extracting semantic features, the searching task now becomes to a text-
based retrieval one. This stage is to index the semantic text returned from
the previous stage. A standard tf-idf scheme is used to calculate the weight of
each word. In the online searching stage, the system computes similarity scores
between the query text and video semantic features using the inverted index
structure.
3
Locally Regional Object Proposal
3.1
High-Saliency Object Filtering
One of the diﬃcult problems in video search is that there can be too many
objects in a video frame. Therefore, we do not know which objects are really
focused on by users. To tackle this problem, we propose to use a high-saliency
Object Detection. The purpose of this work is that we just focus on high-saliency
objects in a video frame. By selecting regions of interest from complex scenes,
we can reduce noise in the result. Figure 3 shows an example of a saliency map
which illustrates regions of an image that users may be interested in.
3.2
Locally Regional Object Proposal
Finding an exact scene requires speciﬁc and discriminative cues. One of the most
discriminative cues in a scene is object instances. Therefore, we suggest a Locally
Regional Object Proposal to search by object instances.
Fig. 2. Objects detected from YOLO object detector and indexed by dividing video
frame into 7 × 7 grid.

Video Search Based on Semantic Extraction
455
First, we extract objects from a video frame with OLO (version 2) [2], one of
the state-of-the-art object detectors. The YOLO network was trained on COCO
datasets [8], which comprises of 80 concepts. We employ YOLO to get bounding
box information of objects in each video frame.
Second, we proceed soft-indexing by dividing a video frame into a regular
7 × 7 grid. Each object detected by YOLO object detector is in one or several
cells. We index each object with two attributes: the object name and the cells
containing that object. For instance, Fig. 2 shows the detected objects, each of
which lays on some cells. For example, the dog is indexed with the following
cells: (2, 1), (2, 2), (3, 1), (3, 2), (4, 1), (4, 2), (5, 1), (5, 2), (6, 1), and (6, 2).
Similarly, we can index the bicycle and the truck. Note: rows are numbered from
0 (top to bottom) and columns are numbered from 0 (left to right).
To search for a video frame with an object, we create a sketch in a blank
7 × 7 grid to represent a collection of objects in the video frame of interest. For
each object, we mark all the possible cells in the grid that contain the object.
Each cell in our sketch can contain multiple objects. Since all data was indexed
before, we can search for a video frame quickly. By this way, we can search for
a frame with an object (in one of the 80 classes) and its position.
4
Post Processing
4.1
Color Based Filtering
We aim at the Known-Item Search scenario in which users search for a short video
segment known either visually or by a textual description. Based on raw color
sketching, we can search all video frames that have a color distribution similar to
a raw color sketching distribution. To deal with the problem, we consider using
Color Based Searching [9]. The retrieval model is based on feature signatures, a
ﬂexible image descriptor capturing distinct color regions in video key-frames.
Fig. 3. Example of saliency map.
Fig. 4. Action detection example.
4.2
Instance Search
In AVS tasks, the output of the system is a ranked list with many shots that
are relevant to a given verbal expression. After using the semantic concept to
retrieve an initial ranked list, we propose to extend the query using an instant

456
T.-D. Truong et al.
search system. In this paper, we use the framework that leverages the advantage
of a local feature based representation model and a deep feature based object
detector [10]. In the TRECVID INS task, our method achieve about 42.42% in
MAP.
5
Conclusion
We propose a new hybrid method that takes advantages of semantic concept
detectors, an action detector from a static image, and a locally regional object
proposal. To ﬁlter irrelevant shots, we use color-based signatures and spatial
information of concepts. We also propose an instance search panel to expand
the query and improve the recall of the system.
References
1. Cobˆarzan, C., Schoeﬀmann, K., Bailer, W., H¨urst, W., Blaˇzek, A., Lokoˇc, J.,
Vrochidis, S., Barthel, K.U., Rossetto, L.: Interactive video search tools: a detailed
analysis of the video browser showdown 2015. Multimedia Tools Appl. 76(4), 5539–
5571 (2017)
2. Redmon, J., Farhadi, A.: Yolo9000: Better, faster, stronger. arXiv preprint
arXiv:1612.08242 (2016)
3. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for
scene recognition using places database. In: Ghahramani, Z., Welling, M., Cortes,
C., Lawrence, N.D., Weinberger, K.Q. (eds.) Advances in Neural Information Pro-
cessing Systems 27, pp. 487–495. Curran Associates, Inc., New York (2014)
4. Patterson, G., Xu, C., Su, H., Hays, J.: The sun attribute database: beyond cate-
gories for deeper scene understanding. Int. J. Comput. Vis. 108(1–2), 59–81 (2014)
5. Liu, N., Han, J.: Dhsnet: deep hierarchical saliency network for salient object
detection. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016
6. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. CoRR abs/1409.1556 (2014)
7. Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: fully convolutional localization
networks for dense captioning. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (2016)
8. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft COCO: common objects in context. In: Fleet, D., Pajdla,
T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 740–755.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48
9. Blaˇzek, A., Lokoˇc, J., Skopal, T.: Video retrieval with feature signature sketches. In:
Traina, A.J.M., Traina, C., Cordeiro, R.L.F. (eds.) SISAP 2014. LNCS, vol. 8821,
pp. 25–36. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11988-5 3
10. Nguyen, V.T., Le, D.D., Salvador, A., Zhu, C., Nguyen, D.L., Tran, M.T., Duc,
T.N., Duong, D.A., Satoh, S., i Nieto, X.G.: Nii-hitachi-uit at trecvid 2015. In:
TRECVID 2015 Workshop, Gaithersburg, MD, USA (2015)

Author Index
Aerts, Bram
I-518
Andreadis, Stelios
II-444
Angladon, Vincent
II-226
Apostolidis, Evlampios
I-29
Apostolidis, Konstantinos
I-29
Aris, Azrin
II-315
Back, Jong-Hee
I-254
Baddar, Wissam J.
I-117, I-493
Bai, Liang
I-154
Bailer, Werner
I-620
Banerjee, Natasha Kholgade
I-466, II-343,
II-381, II-390
Banerjee, Sean
I-466, II-343, II-381, II-390
Barthel, Kai Uwe
II-372, II-413
Bhatt, Chidansh
II-252
Böszörmenyi, Laszlo
I-241
Cao, Yang
I-93, I-142
Cao, Yu
II-129
Carré, Philippe
I-54
Chakravarty, Punarjay
I-518
Chang, Chin-chen
I-381
Charvillat, Vincent
II-226
Chen, Chien-Wen
II-386
Chen, Jianjun
I-565
Chen, Jingjing
II-129
Chen, Jingyuan
II-327
Chen, Jun
I-417
Chen, Shengyong
I-67
Chen, Xianyu
I-317
Chen, Xiaoou
I-544, II-118
Chen, Yujing
I-556
Cheng, Hsu-Yung
II-357
Cheng, Peng
I-329
Cheng, Yao
II-118
Cheong, Clarence Weihan
II-315
Chiu, Chih-Yi
I-305
Chua, Tat Seng
II-129
Chua, Tat-Seng
I-632, II-327
Cooper, Matthew
II-252
Cui, Chaoran
I-267
Dai, Feng
I-317
Deng, Xiang
I-267
Diaz-Ferreyra, Nicolas
I-632
Diba, Ali
I-518
Ding, Xin
II-14
Do, Tien
II-451
Du, Dapeng
II-213
Duane, Aaron
II-377
Ege, Takumi
II-352
El-Shabrawi, Yosuf
I-241
Fa, Lingling
II-153
Fang, Huidi
I-267
Forde, Ciarán
II-129
Friedland, Gerald
I-632
Furuta, Ryosuke
I-190
Gairola, Siddhartha
I-216
Galanopoulos, Damianos
II-444
Gasparini, Simone
II-226
Gasser, Ralph
II-403
Gialampoukidis, Ilias
II-444
Giangreco, Ivan
II-403
Goedemé, Toon
I-518
Gornostaja, Tatjana
I-632
Gu, Daquan
I-16
Gui, Dongdong
II-165
Guo, Jinlin
I-154
Guo, Lintao
I-466, II-381
Gupta, Rashmi
I-581
Gurrin, Cathal
I-581, II-377
Han, Zhen
II-14
He, Jingmeng
II-289
He, Xiangnan
II-327
Hezel, Nico
II-372, II-413
Ho, Yo-Sung
I-254
Hu, Feiyan
I-608
Hu, Jiagao
I-291
Hu, Min-Chun
II-386
Hu, Ruimin
I-417, II-14
Hu, Yue
I-565
Huang, Di
I-440

Huang, Dongmei
I-453
Huang, Kai
I-106
Huang, Sunguo
II-277
Huet, Benoit
I-632
Hulens, Dries
I-518
Hürst, Wolfgang
I-369
Inoue, Naoto
I-190
Ji, Yanli
II-94
Jia, Yuhua
I-154
Jian, Muwei
I-267
Jiang, Hongbo
II-277
Jiang, Jianmin
II-82
Jiang, Junjun
I-417
Jung, Klaus
II-372, II-413
Kairanbay, Magzhan
I-531
Kang, Yu
I-142
Kaptein, Rianne
I-632
Kato, Hirokazu
I-369
Kim, Hak Gu
I-493
Kim, Seong Tae
I-117, I-493
Kim, Sunho
I-254
Kletz, Sabrina
I-203, II-425
Komorita, Satoshi
II-367
Kompatsiaris, Ioannis
II-444
Kong, Weijie
I-166
Kovalčík, Gregor
II-419
Kuo, Chia-Ming
II-386
Kuo, Yu-Hang
II-357
Lamb, Nikolas
I-466, II-381, II-390
Le, Dinh-Duy
II-451
Lecellier, François
I-54
Lee, Hong Joo
I-493
Lee, Tae Kwan
I-117
Leibetseder, Andreas
I-203, II-362, II-425,
II-438
Lertniphonphan, Kanokphan
II-367
Li, Bo
I-291, I-479
Li, Ge
I-166, II-3
Li, Haojie
I-381, II-36
Li, Hongwei
II-189
Li, Hongyan
II-264
Li, Houqiang
II-239
Li, Jianjun
I-381, II-36
Li, Meng
I-505
Li, Nannan
I-166
Li, Qian
I-16
Li, Qingli
II-200
Li, Shining
II-177
Li, Thomas H.
I-166
Li, Zhe
I-342
Li, Zhigang
II-177
Lian, Zhouhui
I-229
Lim, Ryan Woei-Sheng
II-315
Lin, Kai
II-3
Lindén, Krister
I-632
Liu, Chenlu
II-94
Liu, Dong
II-61
Liu, Jie
I-317
Liu, Qingjie
I-428
Liu, Wu
I-329, I-355
Liu, Xiangyu
I-428
Liu, Xinran
II-106
Liu, Yan
II-82
Lokoč, Jakub
II-419
Lowit, Benjamin
I-466, II-381
Lu, Huimin
II-94
Lu, Tong
I-42, II-48
Lu, Yi-Jie
II-407
Luan, Xidao
II-289
Lugtenberg, Geert
I-369
Luo, Xingzhou
II-213
Luo, Yimin
II-73
Luo, Zhenxing
II-36
Lv, Jinna
I-355
Lv, Yue
II-239
Ma, Haichuan
II-61
Ma, Huadong
I-329, I-355
Ma, Yike
I-317
Markatopoulou, Foteini
II-444
Marukatat, Sanparith
II-431
Mavromatis, Sébastien
I-453
Mazloom, Masoud
I-594
McGuinness, Kevin
II-23
Mezaris, Vasileios
I-29, II-444
Michaud, Dorian
I-54
Ming, Zhao-Yan
II-129
Ming, Zhong
II-165
Moens, Marie-Francine
I-3, I-632
Moumtzidou, Anastasia
II-444
Münzer, Bernd
I-241, II-395, II-438
Narayanan, P. J.
I-216
Naren, Tuya
I-317
458
Author Index

Ngo, Chong-Wah
II-129, II-407
Ngo, Thanh Duc
II-451
Nguyen, Phuong Anh
II-407
Nguyen, Vinh-Tiep
II-451
Nie, Liqiang
II-327
Nie, Xiushan
I-267
Niu, Zhongying
II-189
O’Connor, Noel E.
II-23
Onoye, Takao
II-142
Panboonyuen, Teerapong
I-393
Pappi, Iliana
I-594
Patras, Ioannis
II-444
Peltonen, Jaakko
I-632
Peng, Jain-Wei
II-386
Peng, Jianfeng
II-82
Peng, Yuxin
I-130, I-405
Pérez-Mayos, Laura
I-279
Petscharnig, Stefan
II-348
Plopski, Alexander
I-369
Prayoonwong, Amorntip
I-305
Primus, Manfred Jüergen
I-241
Primus, Manfred Jürgen
II-362, II-438
Putzgruber-Adamitsch, Doris
I-241
Qi, Xiaoyu
I-544
Qin, Zheng
I-106
Qin, Zhengcai
I-505
Qiu, Song
II-200
Quant, Hunter
I-466, II-343, II-381
Rawat, Saumya
I-216
Redi, Miriam
I-632
Ro, Yong Man
I-117, I-493
Rosa, Nina
I-369
Rossetto, Luca
II-302, II-403
Roussel, Tom
I-518
Rudinac, Stevan
I-632
Rujikietgumjorn, Sitapa
II-431
Sandor, Christian
I-369
Schedl, Markus
I-632
Schoeffmann, Klaus
I-203, I-241, II-348,
II-362, II-395, II-425, II-438
Schuldt, Heiko
II-302, II-403
See, John
I-531, II-315
Sequeira, Jean
I-453
Shah, Rajvi
I-216
Shamma, David A.
I-632
Shen, Fumin
II-94
Sheng, Shurong
I-3
Sheng, Yun
I-178
Shi, En
I-16
Shivakumara, Palaiahnakote
II-48
Shu, Xiangbo
II-153
Smeaton, Alan F.
I-608
Smeaton, Alan
I-632
Song, Mofei
I-291
Song, Wei
I-453
Song, Weiguo
I-142
Song, Xuemeng
II-327
Song, Yan
II-106, II-153
Song, Youcheng
II-264
Song, Yucheng
I-81
Souček, Tomáš
II-419
Sukno, Federico M.
I-279
Sun, Fuming
I-381
Sun, Li
II-200
Sun, Zhengxing
I-291, I-479, II-264
Taketomi, Takafumi
I-369
Tan, Ian K. T.
II-315
Tang, Jinhui
II-106
Tang, Yingmin
I-229
Tang, Zhenhua
II-277
Tang, Zhuo
II-36
Tanno, Ryosuke
II-352
Tasaka, Kazuyuki
II-367
Taschwer, Mario
I-241
Tian, Qi
II-239
Tran, Minh-Triet
II-451
Trieu, Trang-Vinh
II-451
Truong, Thanh-Dat
II-451
Tseng, Yuan-Chi
II-386
Tu, Weiping
I-81
Tuytelaars, Tinne
I-518
Udomcharoenchaikit, Can
I-393
Urruty, Thierry
I-54
Van Eycken, Luc
I-518
Van Gool, Luc
I-518
Van Hamme, Hugo
I-518
Vateekul, Peerapon
I-393
Venkitasubramanian, Aparna Nurani
I-3
Vennekens, Joost
I-518
Vrochidis, Stefanos
II-444
Author Index
459

Wang, Anyi
II-177
Wang, Cheng-Hsien
I-305
Wang, Chenyan
II-36
Wang, Chong
II-189
Wang, Fang
II-82
Wang, Guolong
I-106
Wang, Peng
I-154
Wang, Shu
II-73
Wang, Shuang
I-479
Wang, Wenhai
I-42, II-48
Wang, Xu
I-556
Wang, Yan
I-453
Wang, Yizhi
I-229
Wang, Yunhong
I-428, I-440
Wang, Zhongyuan
I-556, II-14, II-73
Wanner, Leo
I-279
Watcharapinchai, Nattachai
II-431
Wei, Lianglei
I-42
Wei, Meng
I-142
Wei, Ronglei
II-189
Wichakam, Itsara
I-393
Wong, Lai-Kuan
I-531, II-315
Worring, Marcel
I-594
Wu, Bin
I-355, I-505
Wu, Feng
II-61
Wu, Gangshan
II-213
Wu, Peng
I-440
Wu, Yirui
I-42, II-48
Wu, Yunjie
II-264
Xiao, Jianguo
I-229
Xiao, Jing
I-556
Xie, Hongtao
I-565
Xie, Lexing
I-632
Xie, Yuxiang
I-154, II-289
Xiong, Zhiwei
II-61
Xu, Kaiping
I-106
Xu, Lanlan
I-381
Xu, Xiaoshuo
II-118
Xu, Xing
II-94
Xu, Yang
II-200
Xue, Yanbing
I-67
Yamasaki, Toshihiko
I-190
Yan, Chenggang
I-565
Yan, Yan
II-177
Yana, Buntueng
II-142
Yanagihara, Hiromasa
II-367
Yanai, Keiji
II-352
Yang, Deshun
I-544, II-118
Yang, Kewei
I-479
Yang, Wanzhao
I-81
Yang, Yang
II-94
Yang, Yuhong
I-81
Yao, Peng
I-67
Ye, Shuxiong
I-106
Yin, Yilong
I-267
Yuan, Hua
I-178
Yuan, Yuxin
I-405
Zegers, Jeroen
I-518
Zeng, Yi-Xuan
II-357
Zha, Zheng-Jun
I-93
Zhai, Wei
I-93
Zhan, Gen
I-556
Zhang, Guixu
I-178
Zhang, Hanwang
II-327
Zhang, Hao
II-407
Zhang, Hua
I-67
Zhang, Huidi
I-106
Zhang, Jingyu
II-177
Zhang, Kaixuan
II-200
Zhang, Lili
II-289
Zhang, Xiao
II-177
Zhang, Xin
II-289
Zhang, Xiong
I-81
Zhang, Yifan
I-329
Zhang, Yiwei
II-3
Zhang, Yongfei
I-342
Zhao, Jian
II-252
Zhao, Junjie
I-130
Zhao, Zhangming
I-16
Zheng, Jiaxi
I-81
Zheng, Qi
I-417
Zhong, Jiaxing
II-3
Zhong, Sheng-hua
II-82, II-165
Zhou, Jiang
II-23
Zhou, Jianshe
I-317
Zhou, Ke
II-189
Zhou, Liguo
II-73
Zhou, Lili
I-355
Zhou, Wengang
II-239
Zhou, Yuanyuan
I-178
Zhu, Jiang
I-93
460
Author Index


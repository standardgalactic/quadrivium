

MATRIX ANALYSIS
FOR STATISTICS

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice,
Geof H. Givens, Harvey Goldstein, Geert Molenberghs, David W. Scott,
Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg
Editors Emeriti: J. Stuart Hunter, Iain M. Johnstone, Joseph B. Kadane,
Jozef L. Teugels
A complete list of the titles in this series appears at the end of this volume.

MATRIX ANALYSIS
FOR STATISTICS
Third Edition
JAMES R. SCHOTT

Copyright © 2017 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ
07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and specifically disclaim any implied warranties of
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a professional where appropriate. Neither the publisher nor
author shall be liable for any loss of profit or any other commercial damages, including but not limited to
special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at
(317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic formats. For more information about Wiley products, visit our web site at
www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Names: Schott, James R., 1955- author.
Title: Matrix analysis for statistics / James R. Schott.
Description: Third edition. | Hoboken, New Jersey : John Wiley & Sons, 2016.
| Includes bibliographical references and index.
Identifiers: LCCN 2016000005| ISBN 9781119092483 (cloth) | ISBN 9781119092469
(epub)
Subjects: LCSH: Matrices. | Mathematical statistics.
Classification: LCC QA188 .S24 2016 | DDC 512.9/434–dc23 LC record available at
http://lccn.loc.gov/2016000005
Cover image courtesy of GettyImages/Alexmumu.
Typeset in 10/12pt TimesLTStd by SPi Global, Chennai, India
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

To Susan, Adam, and Sarah


CONTENTS
Preface
xi
About the Companion Website
xv
1
A Review of Elementary Matrix Algebra
1
1.1
Introduction, 1
1.2
Definitions and Notation, 1
1.3
Matrix Addition and Multiplication, 2
1.4
The Transpose, 3
1.5
The Trace, 4
1.6
The Determinant, 5
1.7
The Inverse, 9
1.8
Partitioned Matrices, 12
1.9
The Rank of a Matrix, 14
1.10
Orthogonal Matrices, 15
1.11
Quadratic Forms, 16
1.12
Complex Matrices, 18
1.13
Random Vectors and Some Related Statistical Concepts, 19
Problems, 29
2
Vector Spaces
35
2.1
Introduction, 35
2.2
Definitions, 35
2.3
Linear Independence and Dependence, 42

viii
CONTENTS
2.4
Matrix Rank and Linear Independence, 45
2.5
Bases and Dimension, 49
2.6
Orthonormal Bases and Projections, 53
2.7
Projection Matrices, 58
2.8
Linear Transformations and Systems of Linear Equations, 65
2.9
The Intersection and Sum of Vector Spaces, 73
2.10
Oblique Projections, 76
2.11
Convex Sets, 80
Problems, 85
3
Eigenvalues and Eigenvectors
95
3.1
Introduction, 95
3.2
Eigenvalues, Eigenvectors, and Eigenspaces, 95
3.3
Some Basic Properties of Eigenvalues and Eigenvectors, 99
3.4
Symmetric Matrices, 106
3.5
Continuity of Eigenvalues and Eigenprojections, 114
3.6
Extremal Properties of Eigenvalues, 116
3.7
Additional Results Concerning Eigenvalues Of Symmetric
Matrices, 123
3.8
Nonnegative Definite Matrices, 129
3.9
Antieigenvalues and Antieigenvectors, 141
Problems, 144
4
Matrix Factorizations and Matrix Norms
155
4.1
Introduction, 155
4.2
The Singular Value Decomposition, 155
4.3
The Spectral Decomposition of a Symmetric Matrix, 162
4.4
The Diagonalization of a Square Matrix, 169
4.5
The Jordan Decomposition, 173
4.6
The Schur Decomposition, 175
4.7
The Simultaneous Diagonalization of Two Symmetric Matrices, 178
4.8
Matrix Norms, 184
Problems, 191
5
Generalized Inverses
201
5.1
Introduction, 201
5.2
The Moore–Penrose Generalized Inverse, 202
5.3
Some Basic Properties of the Moore–Penrose Inverse, 205
5.4
The Moore–Penrose Inverse of a Matrix Product, 211
5.5
The Moore–Penrose Inverse of Partitioned Matrices, 215
5.6
The Moore–Penrose Inverse of a Sum, 219
5.7
The Continuity of the Moore–Penrose Inverse, 222
5.8
Some Other Generalized Inverses, 224

CONTENTS
ix
5.9
Computing Generalized Inverses, 232
Problems, 238
6
Systems of Linear Equations
247
6.1
Introduction, 247
6.2
Consistency of a System of Equations, 247
6.3
Solutions to a Consistent System of Equations, 251
6.4
Homogeneous Systems of Equations, 258
6.5
Least Squares Solutions to a System of Linear Equations, 260
6.6
Least Squares Estimation For Less Than Full Rank Models, 266
6.7
Systems of Linear Equations and The Singular Value
Decomposition, 271
6.8
Sparse Linear Systems of Equations, 273
Problems, 278
7
Partitioned Matrices
285
7.1
Introduction, 285
7.2
The Inverse, 285
7.3
The Determinant, 288
7.4
Rank, 296
7.5
Generalized Inverses, 298
7.6
Eigenvalues, 302
Problems, 307
8
Special Matrices and Matrix Operations
315
8.1
Introduction, 315
8.2
The Kronecker Product, 315
8.3
The Direct Sum, 323
8.4
The Vec Operator, 323
8.5
The Hadamard Product, 329
8.6
The Commutation Matrix, 339
8.7
Some Other Matrices Associated With the Vec Operator, 346
8.8
Nonnegative Matrices, 351
8.9
Circulant and Toeplitz Matrices, 363
8.10
Hadamard and Vandermonde Matrices, 369
Problems, 373
9
Matrix Derivatives and Related Topics
387
9.1
Introduction, 387
9.2
Multivariable Differential Calculus, 387
9.3
Vector and Matrix Functions, 390
9.4
Some Useful Matrix Derivatives, 396
9.5
Derivatives of Functions of Patterned Matrices, 400

x
CONTENTS
9.6
The Perturbation Method, 402
9.7
Maxima and Minima, 409
9.8
Convex and Concave Functions, 413
9.9
The Method of Lagrange Multipliers, 417
Problems, 423
10
Inequalities
433
10.1
Introduction, 433
10.2
Majorization, 433
10.3
Cauchy-Schwarz Inequalities, 444
10.4
H¨older’s Inequality, 446
10.5
Minkowski’s Inequality, 450
10.6
The Arithmetic-Geometric Mean Inequality, 452
Problems, 453
11
Some Special Topics Related to Quadratic Forms
457
11.1
Introduction, 457
11.2
Some Results on Idempotent Matrices, 457
11.3
Cochran’s Theorem, 462
11.4
Distribution of Quadratic Forms in Normal Variates, 465
11.5
Independence of Quadratic Forms, 471
11.6
Expected Values of Quadratic Forms, 477
11.7
The Wishart Distribution, 485
Problems, 496
References
507
Index
513

PREFACE
As the field of statistics has developed over the years, the role of matrix methods
has evolved from a tool through which statistical problems could be more conve-
niently expressed to an absolutely essential part in the development, understanding,
and use of the more complicated statistical analyses that have appeared in recent
years. As such, a background in matrix analysis has become a vital part of a graduate
education in statistics. Too often, the statistics graduate student gets his or her matrix
background in bits and pieces through various courses on topics such as regression
analysis, multivariate analysis, linear models, stochastic processes, and so on. An
alternative to this fragmented approach is an entire course devoted to matrix methods
useful in statistics. This text has been written with such a course in mind. It also could
be used as a text for an advanced undergraduate course with an unusually bright group
of students and should prove to be useful as a reference for both applied and research
statisticians.
Students beginning in a graduate program in statistics often have their previous
degrees in other fields, such as mathematics, and so initially their statistical back-
grounds may not be all that extensive. With this in mind, I have tried to make the
statistical topics presented as examples in this text as self-contained as possible.
This has been accomplished by including a section in the first chapter which cov-
ers some basic statistical concepts and by having most of the statistical examples
deal with applications which are fairly simple to understand; for instance, many of
these examples involve least squares regression or applications that utilize the sim-
ple concepts of mean vectors and covariance matrices. Thus, an introductory statistics
course should provide the reader of this text with a sufficient background in statistics.
An additional prerequisite is an undergraduate course in matrices or linear algebra,
while a calculus background is necessary for some portions of the book, most notably,
Chapter 8.

xii
PREFACE
By selectively omitting some sections, all nine chapters of this book can be covered
in a one-semester course. For instance, in a course targeted at students who end their
educational careers with the masters degree, I typically omit Sections 2.10, 3.5, 3.7,
4.8, 5.4-5.7, and 8.6, along with a few other sections.
Anyone writing a book on a subject for which other texts have already been written
stands to benefit from these earlier works, and that certainly has been the case here.
The texts by Basilevsky (1983), Graybill (1983), Healy (1986), and Searle (1982), all
books on matrices for statistics, have helped me, in varying degrees, to formulate my
ideas on matrices. Graybill’s book has been particularly influential, since this is the
book that I referred to extensively, first as a graduate student, and then in the early
stages of my research career. Other texts which have proven to be quite helpful are
Horn and Johnson (1985, 1991), Magnus and Neudecker (1988), particularly in the
writing of Chapter 8, and Magnus (1988).
I wish to thank several anonymous reviewers who offered many very helpful
suggestions, and Mark Johnson for his support and encouragement throughout this
project. I am also grateful to the numerous students who have alerted me to various
mistakes and typos in earlier versions of this book. In spite of their help and my
diligent efforts at proofreading, undoubtedly some mistakes remain, and I would
appreciate being informed of any that are spotted.
Jim Schott
Orlando, Florida
PREFACE TO THE SECOND EDITION
The most notable change in the second edition is the addition of a chapter on
results regarding matrices partitioned into a 2 × 2 form. This new chapter, which
is Chapter 7, has the material on the determinant and inverse that was previously
given as a section in Chapter 7 of the first edition. Along with the results on the
determinant and inverse of a partitioned matrix, I have added new material in this
chapter on the rank, generalized inverses, and eigenvalues of partitioned matrices.
The coverage of eigenvalues in Chapter 3 has also been expanded. Some additional
results such as Weyl’s Theorem have been included, and in so doing, the last section
of Chapter 3 of the first edition has now been replaced by two sections.
Other smaller additions, including both theorems and examples, have been made
elsewhere throughout the book. Over 100 new exercises have been added to the prob-
lems sets.
The writing of a second edition of this book has also given me the opportunity
to correct mistakes in the first edition. I would like to thank those readers who have

PREFACE TO THE THIRD EDITION
xiii
pointed out some of these errors as well as those that have offered suggestions for
improvement to the text.
Jim Schott
Orlando, Florida
September 2004
PREFACE TO THE THIRD EDITION
The third edition of this text maintains the same organization that was present in
the previous editions. The major changes involve the addition of new material. This
includes the following additions.
1. A new chapter, now Chapter 10, on inequalities has been added. Numerous
inequalities such as Cauchy-Schwarz, Hadamard, and Jensen’s, already appear
in the earlier editions, but there are many important ones that are missing, and
some of these are given in the new chapter. Highlighting this chapter is a fairly
substantial section on majorization and some of the inequalities that can be
developed from this concept.
2. A new section on oblique projections has been added to Chapter 2. The previous
editions only covered orthogonal projections.
3. A new section on antieigenvalues and antieigenvectors has been added to
Chapter 3.
Numerous other smaller additions have been made throughout the text. These
include some additional theorems, the proofs of some results that previously had
been given without proof, and some more examples involving statistical applica-
tions. Finally, more than 70 new problems have been added to the end-of-chapter
problem sets.
Jim Schott
Orlando, Florida
December 2015


ABOUT THE COMPANION WEBSITE
This book is accompanied by a companion website:
www.wiley.com/go/Schott/MatrixAnalysis3e
The instructor’s website includes:
• A solutions manual with solutions to selected problems
The student’s website includes:
• A solutions manual with odd-numbered solutions to selected problems


1
A REVIEW OF ELEMENTARY MATRIX
ALGEBRA
1.1
INTRODUCTION
In this chapter, we review some of the basic operations and fundamental properties
involved in matrix algebra. In most cases, properties will be stated without proof, but
in some cases, when instructive, proofs will be presented. We end the chapter with a
brief discussion of random variables and random vectors, expected values of random
variables, and some important distributions encountered elsewhere in the book.
1.2
DEFINITIONS AND NOTATION
Except when stated otherwise, a scalar such as α will represent a real number. A
matrix A of size m × n is the m × n rectangular array of scalars given by
A =
⎡
⎢⎢⎢⎣
a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
am1
am2
· · ·
amn
⎤
⎥⎥⎥⎦,
and sometimes it is simply identified as A = (aij). Sometimes it also will be conve-
nient to refer to the (i, j)th element of A, as (A)ij; that is, aij = (A)ij. If m = n,
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

2
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
then A is called a square matrix of order m, whereas A is referred to as a rectangular
matrix when m ̸= n. An m × 1 matrix
a =
⎡
⎢⎢⎢⎣
a1
a2
...
am
⎤
⎥⎥⎥⎦
is called a column vector or simply a vector. The element ai is referred to as the ith
component of a. A 1 × n matrix is called a row vector. The ith row and jth column
of the matrix A will be denoted by (A)i· and (A)·j, respectively. We will usually use
capital letters to represent matrices and lowercase bold letters for vectors.
The diagonal elements of the m × m matrix A are a11, a22, . . . , amm. If all
other elements of A are equal to 0, A is called a diagonal matrix and can be
identified as A = diag(a11, . . . , amm). If, in addition, aii = 1 for i = 1, . . . , m
so that A = diag(1, . . . , 1), then the matrix A is called the identity matrix of
order m and will be written as A = Im or simply A = I if the order is obvious.
If A = diag(a11, . . . , amm) and b is a scalar, then we will use Ab to denote the
diagonal matrix diag(ab
11, . . . , ab
mm). For any m × m matrix A, DA will denote
the diagonal matrix with diagonal elements equal to those of A, and for any m × 1
vector a, Da denotes the diagonal matrix with diagonal elements equal to the
components of a; that is, DA = diag(a11, . . . , amm) and Da = diag(a1, . . . , am).
A triangular matrix is a square matrix that is either an upper triangular matrix or a
lower triangular matrix. An upper triangular matrix is one that has all of its elements
below the diagonal equal to 0, whereas a lower triangular matrix has all of its elements
above the diagonal equal to 0. A strictly upper triangular matrix is an upper triangular
matrix that has each of its diagonal elements equal to 0. A strictly lower triangular
matrix is defined similarly.
The ith column of the m × m identity matrix will be denoted by ei; that is, ei is
the m × 1 vector that has its ith component equal to 1 and all of its other components
equal to 0. When the value of m is not obvious, we will make it more explicit by
writing ei as ei,m. The m × m matrix whose only nonzero element is a 1 in the
(i, j)th position will be identified as Eij.
The scalar zero is written 0, whereas a vector of zeros, called a null vector, will be
denoted by 0, and a matrix of zeros, called a null matrix, will be denoted by (0). The
m × 1 vector having each component equal to 1 will be denoted by 1m or simply 1
when the size of the vector is obvious.
1.3
MATRIX ADDITION AND MULTIPLICATION
The sum of two matrices A and B is defined if they have the same number of rows
and the same number of columns; in this case,
A + B = (aij + bij).

THE TRANSPOSE
3
The product of a scalar α and a matrix A is
αA = Aα = (αaij).
The premultiplication of the matrix B by the matrix A is defined only if the number
of columns of A equals the number of rows of B. Thus, if A is m × p and B is p × n,
then C = AB will be the m × n matrix which has its (i, j)th element, cij, given by
cij = (A)i·(B)·j =
p

k=1
aikbkj.
A similar definition exists for BA, the postmultiplication of B by A, if the number
of columns of B equals the number of rows of A. When both products are defined,
we will not have, in general, AB = BA. If the matrix A is square, then the product
AA, or simply A2, is defined. In this case, if we have A2 = A, then A is said to be an
idempotent matrix.
The following basic properties of matrix addition and multiplication in Theorem
1.1 are easy to verify.
Theorem 1.1
Let α and β be scalars and A, B, and C be matrices. Then, when the
operations involved are defined, the following properties hold:
(a) A + B = B + A.
(b) (A + B) + C = A + (B + C).
(c) α(A + B) = αA + αB.
(d) (α + β)A = αA + βA.
(e) A −A = A + (−A) = (0).
(f) A(B + C) = AB + AC.
(g) (A + B)C = AC + BC.
(h) (AB)C = A(BC).
1.4
THE TRANSPOSE
The transpose of an m × n matrix A is the n × m matrix A′ obtained by interchang-
ing the rows and columns of A. Thus, the (i, j)th element of A′ is aji. If A is m × p
and B is p × n, then the (i, j)th element of (AB)′ can be expressed as
((AB)′)ij = (AB)ji = (A)j·(B)·i =
p

k=1
ajkbki
= (B′)i·(A′)·j = (B′A′)ij.
Thus, evidently (AB)′ = B′A′. This property along with some other results involving
the transpose are summarized in Theorem 1.2.

4
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
Theorem 1.2
Let α and β be scalars and A and B be matrices. Then, when defined,
the following properties hold:
(a) (αA)′ = αA′.
(b) (A′)′ = A.
(c) (αA + βB)′ = αA′ + βB′.
(d) (AB)′ = B′A′.
If A is m × m, that is, A is a square matrix, then A′ is also m × m. In this case, if
A = A′, then A is called a symmetric matrix, whereas A is called a skew-symmetric
if A = −A′.
The transpose of a column vector is a row vector, and in some situations, we may
write a matrix as a column vector times a row vector. For instance, the matrix Eij
defined in Section 1.2 can be expressed as Eij = eie′
j. More generally, ei,me′
j,n
yields an m × n matrix having 1, as its only nonzero element, in the (i, j)th position,
and if A is an m × n matrix, then
A =
m

i=1
n

j=1
aijei,me′
j,n.
1.5
THE TRACE
The trace is a function that is defined only on square matrices. If A is an m × m
matrix, then the trace of A, denoted by tr(A), is defined to be the sum of the diagonal
elements of A; that is,
tr(A) =
m

i=1
aii.
Now if A is m × n and B is n × m, then AB is m × m and
tr(AB) =
m

i=1
(AB)ii =
m

i=1
(A)i·(B)·i =
m

i=1
n

j=1
aijbji
=
n

j=1
m

i=1
bjiaij =
n

j=1
(B)j·(A)·j
=
n

j=1
(BA)jj = tr(BA).
This property of the trace, along with some others, is summarized in Theorem 1.3.

THE DETERMINANT
5
Theorem 1.3
Let α be a scalar and A and B be matrices. Then, when the appropriate
operations are defined, we have the following properties:
(a) tr(A′) = tr(A).
(b) tr(αA) = α tr(A).
(c) tr(A + B) = tr(A) + tr(B).
(d) tr(AB) = tr(BA).
(e) tr(A′A) = 0 if and only if A = (0).
1.6
THE DETERMINANT
The determinant is another function defined on square matrices. If A is an m × m
matrix, then its determinant, denoted by |A|, is given by
|A| =

(−1)f(i1, ... ,im)a1i1a2i2 · · · amim
=

(−1)f(i1, ... ,im)ai11ai22 · · · aimm,
where the summation is taken over all permutations (i1, . . . , im) of the set of inte-
gers (1, . . . , m), and the function f(i1, . . . , im) equals the number of transpositions
necessary to change (i1, . . . , im) to an increasing sequence of components, that is,
to (1, . . . , m). A transposition is the interchange of two of the integers. Although f
is not unique, it is uniquely even or odd, so that |A| is uniquely defined. Note that the
determinant produces all products of m terms of the elements of the matrix A such
that exactly one element is selected from each row and each column of A.
Using the formula for the determinant, we find that |A| = a11 when m = 1. If A
is 2 × 2, we have
|A| = a11a22 −a12a21,
and when A is 3 × 3, we get
|A| = a11a22a33 + a12a23a31 + a13a21a32
−a11a23a32 −a12a21a33 −a13a22a31.
The following properties of the determinant in Theorem 1.4 are fairly straightfor-
ward to verify using the definition of a determinant.
Theorem 1.4
If α is a scalar and A is an m × m matrix, then the following prop-
erties hold:
(a) |A′| = |A|.
(b) |αA| = αm|A|.

6
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
(c) If A is a diagonal matrix, then |A| = a11 · · · amm = 	m
i=1 aii.
(d) If all elements of a row (or column) of A are zero, |A| = 0.
(e) The interchange of two rows (or columns) of A changes the sign of |A|.
(f) If all elements of a row (or column) of A are multiplied by α, then the deter-
minant is multiplied by α.
(g) The determinant of A is unchanged when a multiple of one row (or column) is
added to another row (or column).
(h) If two rows (or columns) of A are proportional to one another, |A| = 0.
An alternative expression for |A| can be given in terms of the cofactors of A.
The minor of the element aij, denoted by mij, is the determinant of the (m −1) ×
(m −1) matrix obtained after removing the ith row and jth column from A. The
corresponding cofactor of aij, denoted by Aij, is then given as Aij = (−1)i+jmij.
Theorem 1.5
For any i = 1, . . . , m, the determinant of the m × m matrix A can
be obtained by expanding along the ith row,
|A| =
m

j=1
aijAij,
(1.1)
or expanding along the ith column,
|A| =
m

j=1
ajiAji.
(1.2)
Proof.
We will just prove (1.1), as (1.2) can easily be obtained by applying (1.1) to
A′. We first consider the result when i = 1. Clearly
|A| =

(−1)f(i1, ... ,im)a1i1a2i2 · · · amim
= a11b11 + · · · + a1mb1m,
where
a1jb1j =

(−1)f(i1, ... ,im)a1i1a2i2 · · · amim,
and the summation is over all permutations for which i1 = j. Since (−1)f(j,i2, ... ,im)
= (−1)j−1(−1)f(i2, ... ,im), this implies that
b1j =

(−1)j−1(−1)f(i2, ... ,im)a2i2 · · · amim,
where the summation is over all permutations (i2, . . . , im) of (1, . . . , j −1, j +
1, . . . , m). If C is the (m −1) × (m −1) matrix obtained from A by deleting its
1st row and jth column, then b1j can be written

THE DETERMINANT
7
b1j = (−1)j−1 
(−1)f(i1, ... ,im−1)c1i1 · · · cm−1im−1 = (−1)j−1|C|
= (−1)j−1m1j = (−1)1+jm1j = A1j,
where the summation is over all permutations (i1, . . . , im−1) of (1, . . . , m −1) and
m1j is the minor of a1j. Thus,
|A| =
m

j=1
a1jb1j =
m

j=1
a1jA1j,
as is required. To prove (1.1) when i > 1, let D be the m × m matrix for which
(D)1· = (A)i·, (D)j· = (A)j−1·, for j = 2, . . . , i, and (D)j· = (A)j· for j = i +
1, . . . , m. Then Aij = (−1)i−1D1j, aij = d1j and |A| = (−1)i−1|D|. Thus, since
we have already established (1.1) when i = 1, we have
|A| = (−1)i−1|D| = (−1)i−1
m

j=1
d1jD1j =
m

j=1
aijAij,
and so the proof is complete.
□
Our next result indicates that if the cofactors of a row or column are matched with
the elements from a different row or column, the expansion reduces to 0.
Theorem 1.6
If A is an m × m matrix and k ̸= i, then
m

j=1
aijAkj =
m

j=1
ajiAjk = 0.
(1.3)
Example 1.1
We will find the determinant of the 5 × 5 matrix given by
A =
⎡
⎢⎢⎢⎢⎣
2
1
2
1
1
0
0
3
0
0
0
0
2
2
0
0
0
1
1
1
0
1
2
2
1
⎤
⎥⎥⎥⎥⎦
.
Using the cofactor expansion formula on the first column of A, we obtain
|A| = 2








0
3
0
0
0
2
2
0
0
1
1
1
1
2
2
1








,

8
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
and then using the same expansion formula on the first column of this 4 × 4 matrix,
we get
|A| = 2(−1)






3
0
0
2
2
0
1
1
1






.
Because the determinant of the 3 × 3 matrix above is 6, we have
|A| = 2(−1)(6) = −12.
Consider the m × m matrix C whose columns are given by the vectors
c1, . . . , cm; that is, we can write C = (c1, . . . , cm). Suppose that, for some m × 1
vector b = (b1, . . . , bm)′ and m × m matrix A = (a1, . . . , am), we have
c1 = Ab =
m

i=1
biai.
Then, if we find the determinant of C by expanding along the first column of C, we
get
|C| =
m

j=1
cj1Cj1 =
m

j=1
 m

i=1
biaji

Cj1
=
m

i=1
bi
⎛
⎝
m

j=1
ajiCj1
⎞
⎠=
m

i=1
bi|(ai, c2, . . . , cm)|,
so that the determinant of C is a linear combination of m determinants. If B is an
m × m matrix and we now define C = AB, then by applying the previous derivation
on each column of C, we find that
|C| =





 m

i1=1
bi11ai1, . . . ,
m

im=1
bimmaim





=
m

i1=1
· · ·
m

im=1
bi11 · · · bimm|(ai1, . . . , aim)|
=

bi11 · · · bimm|(ai1, . . . , aim)|,
where this final sum is only over all permutations of (1, . . . , m), because Theorem
1.4(h) implies that
|(ai1, . . . , aim)| = 0
if ij = ik for any j ̸= k. Finally, reordering the columns in |(ai1, . . . , aim)| and
using Theorem 1.4(e), we have

THE INVERSE
9
|C| =

bi11 · · · bimm(−1)f(i1, ... ,im)|(a1, . . . , am)| = |B||A|.
This very useful result is summarized in Theorem 1.7.
Theorem 1.7
If both A and B are square matrices of the same order, then
|AB| = |A||B|.
1.7
THE INVERSE
An m × m matrix A is said to be a nonsingular matrix if |A| ̸= 0 and a singular matrix
if |A| = 0. If A is nonsingular, a nonsingular matrix denoted by A−1 and called the
inverse of A exists, such that
AA−1 = A−1A = Im.
(1.4)
This inverse is unique because, if B is another m × m matrix satisfying the inverse
formula (1.4) for A, then BA = Im, and so
B = BIm = BAA−1 = ImA−1 = A−1.
The following basic properties of the matrix inverse in Theorem 1.8 can be easily
verified by using (1.4).
Theorem 1.8
If α is a nonzero scalar, and A and B are nonsingular m × m matri-
ces, then the following properties hold:
(a) (αA)−1 = α−1A−1.
(b) (A′)−1 = (A−1)′.
(c) (A−1)−1 = A.
(d) |A−1| = |A|−1.
(e) If A = diag(a11, . . . , amm), then A−1 = diag(a−1
11 , . . . , a−1
mm).
(f) If A = A′, then A−1 = (A−1)′.
(g) (AB)−1 = B−1A−1.
As with the determinant of A, the inverse of A can be expressed in terms of the
cofactors of A. Let A#, called the adjoint of A, be the transpose of the matrix of
cofactors of A; that is, the (i, j)th element of A# is Aji, the cofactor of aji. Then
AA# = A#A = diag(|A|, . . . , |A|) = |A|Im,
because (A)i·(A#)·i = (A#)i·(A)·i = |A| follows directly from (1.1) and (1.2), and
(A)i·(A#)·j = (A#)i·(A)·j = 0, for i ̸= j follows from (1.3). The equation above
then yields the relationship
A−1 = |A|−1A#

10
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
when |A| ̸= 0. Thus, for instance, if A is a 2 × 2 nonsingular matrix, then
A−1 = |A|−1

a22 −a12
−a21
a11

.
Similarly when m = 3, we get A−1 = |A|−1A#, where
A# =
⎡
⎣
a22a33 −a23a32
−(a12a33 −a13a32)
a12a23 −a13a22
−(a21a33 −a23a31)
a11a33 −a13a31
−(a11a23 −a13a21)
a21a32 −a22a31
−(a11a32 −a12a31)
a11a22 −a12a21
⎤
⎦.
The relationship between the inverse of a matrix product and the product of the
inverses, given in Theorem 1.8(g), is a very useful property. Unfortunately, such a nice
relationship does not exist between the inverse of a sum and the sum of the inverses.
We do, however, have Theorem 1.9 which is sometimes useful.
Theorem 1.9
Suppose A and B are nonsingular matrices, with A being m × m and
B being n × n. For any m × n matrix C and any n × m matrix D, it follows that if
A + CBD is nonsingular, then
(A + CBD)−1 = A−1 −A−1C(B−1 + DA−1C)−1DA−1.
Proof.
The proof simply involves verifying that (A + CBD)(A + CBD)−1 = Im
for (A + CBD)−1 given above. We have
(A + CBD){A−1 −A−1C(B−1 + DA−1C)−1DA−1}
= Im −C(B−1 + DA−1C)−1DA−1 + CBDA−1
−CBDA−1C(B−1 + DA−1C)−1DA−1
= Im −C{(B−1 + DA−1C)−1 −B
+ BDA−1C(B−1 + DA−1C)−1}DA−1
= Im −C{B(B−1 + DA−1C)(B−1 + DA−1C)−1 −B}DA−1
= Im −C{B −B}DA−1 = Im,
and so the result follows.
□
The expression given for (A + CBD)−1 in Theorem 1.9 involves the inverse of
the matrix B−1 + DA−1C. It can be shown (see Problem 7.12) that the conditions
of the theorem guarantee that this inverse exists. If m = n and C and D are identity
matrices, then we obtain Corollary 1.9.1 of Theorem 1.9.

THE INVERSE
11
Corollary 1.9.1
Suppose that A, B and A + B are all m × m nonsingular matrices.
Then
(A + B)−1 = A−1 −A−1(B−1 + A−1)−1A−1.
We obtain Corollary 1.9.2 of Theorem 1.9 when n = 1.
Corollary 1.9.2
Let A be an m × m nonsingular matrix. If c and d are both m × 1
vectors and A + cd′ is nonsingular, then
(A + cd′)−1 = A−1 −A−1cd′A−1/(1 + d′A−1c).
Example 1.2
Theorem 1.9 can be particularly useful when m is larger than n and
the inverse of A is fairly easy to compute. For instance, suppose we have A = I5,
B =
1 1
1 2

,
C =
⎡
⎢⎢⎢⎢⎣
1 0
2 1
−1 1
0 2
1 1
⎤
⎥⎥⎥⎥⎦
,
D′ =
⎡
⎢⎢⎢⎢⎣
1 −1
−1
2
0
1
1
0
−1
1
⎤
⎥⎥⎥⎥⎦
,
from which we obtain
G = A + CBD =
⎡
⎢⎢⎢⎢⎣
1
1
1
1
0
−1
6
4
3
1
−1
2
2
0
1
−2
6
4
3
2
−1
4
3
2
2
⎤
⎥⎥⎥⎥⎦
.
It is somewhat tedious to compute the inverse of this 5 × 5 matrix directly. However,
the calculations in Theorem 1.9 are fairly straightforward. Clearly, A−1 = I5 and
B−1 =

2 −1
−1
1

,
so that
(B−1 + DA−1C) =
 2 −1
−1
1

+
−2 0
3 4

=
0 −1
2
5

and
(B−1 + DA−1C)−1 =

2.5 0.5
−1
0

.
Thus, we find that
G−1 = I5 −C(B−1 + DA−1C)−1D

12
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
=
⎡
⎢⎢⎢⎢⎣
−1
1.5 −0.5 −2.5
2
−3
3
−1
−4
3
3 −2.5
1.5
3.5 −3
2
−2
0
3 −2
−1
0.5 −0.5 −1.5
2
⎤
⎥⎥⎥⎥⎦
.
1.8
PARTITIONED MATRICES
Occasionally we will find it useful to partition a given matrix into submatrices. For
instance, suppose A is m × n and the positive integers m1, m2, n1, n2 are such that
m = m1 + m2 and n = n1 + n2. Then one way of writing A as a partitioned matrix
is
A =
A11
A12
A21
A22

,
where A11 is m1 × n1, A12 is m1 × n2, A21 is m2 × n1, and A22 is m2 × n2. That
is, A11 is the matrix consisting of the first m1 rows and n1 columns of A, A12 is the
matrix consisting of the first m1 rows and last n2 columns of A, and so on. Matrix
operations can be expressed in terms of the submatrices of the partitioned matrix. For
example, suppose B is an n × p matrix partitioned as
B =
B11
B12
B21
B22

,
where B11 is n1 × p1, B12 is n1 × p2, B21 is n2 × p1, B22 is n2 × p2, and p = p1 + p2.
Then the premultiplication of B by A can be expressed in partitioned form as
AB =

A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22

.
Matrices can be partitioned into submatrices in other ways besides this 2 × 2 par-
titioned form. For instance, we could partition only the columns of A, yielding the
expression
A =

A1 A2

,
where A1 is m × n1 and A2 is m × n2. A more general situation is one in which the
rows of A are partitioned into r groups and the columns of A are partitioned into c
groups so that A can be written as
A =
⎡
⎢⎢⎢⎣
A11
A12
· · ·
A1c
A21
A22
· · ·
A2c
...
...
...
Ar1
Ar2
· · ·
Arc
⎤
⎥⎥⎥⎦,

PARTITIONED MATRICES
13
where the submatrix Aij is mi × nj and the integers m1, . . . , mr and n1, . . . , nc
are such that
r

i=1
mi = m
and
c

j=1
nj = n.
This matrix A is said to be in block diagonal form if r = c, Aii is a square matrix for
each i, and Aij is a null matrix for all i and j for which i ̸= j. In this case, we will
write A = diag(A11, . . . , Arr); that is,
diag(A11, . . . , Arr) =
⎡
⎢⎢⎢⎣
A11
(0)
· · ·
(0)
(0)
A22
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Arr
⎤
⎥⎥⎥⎦.
Example 1.3
Suppose we wish to compute the transpose product AA′, where the
5 × 5 matrix A is given by
A =
⎡
⎢⎢⎢⎢⎣
1
0
0
1
1
0
1
0
1
1
0
0
1
1
1
−1
−1
−1
2
0
−1
−1
−1
0
2
⎤
⎥⎥⎥⎥⎦
.
The computation can be simplified by observing that A may be written as
A =

I3
131′
2
−121′
3
2I2

.
As a result, we have
AA′ =

I3
131′
2
−121′
3
2I2
  I3
−131′
2
121′
3
2I2

=
 I3 + 131′
2121′
3
−131′
2 + 2131′
2
−121′
3 + 2121′
3
121′
3131′
2 + 4I2

=

I3 + 2131′
3
131′
2
121′
3
3121′
2 + 4I2

=
⎡
⎢⎢⎢⎢⎣
3
2
2
1
1
2
3
2
1
1
2
2
3
1
1
1
1
1
7
3
1
1
1
3
7
⎤
⎥⎥⎥⎥⎦
.

14
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
1.9
THE RANK OF A MATRIX
Our initial definition of the rank of an m × n matrix A is given in terms of submatri-
ces. We will see an alternative equivalent definition in terms of the concept of linearly
independent vectors in Chapter 2. Most of the material we include in this section can
be found in more detail in texts on elementary linear algebra such as Andrilli and
Hecker (2010) and Poole (2015).
In general, any matrix formed by deleting rows or columns of A is called a
submatrix of A. The determinant of an r × r submatrix of A is called a minor of
order r. For instance, for an m × m matrix A, we have previously defined what we
called the minor of aij; this is an example of a minor of order m −1. Now the rank
of a nonnull m × n matrix A is r, written rank(A) = r, if at least one of its minors
of order r is nonzero while all minors of order r + 1 (if there are any) are zero. If A
is a null matrix, then rank(A) = 0. If rank(A) = min(m, n), then A is said to have
full rank. In particular, if rank(A) = m, A has full row rank, and if rank(A) = n,
A has full column rank.
The rank of a matrix A is unchanged by any of the following operations, called
elementary transformations:
(a) The interchange of two rows (or columns) of A.
(b) The multiplication of a row (or column) of A by a nonzero scalar.
(c) The addition of a scalar multiple of a row (or column) of A to another row (or
column) of A.
Thus, the definition of the rank of A is sometimes given as the number of nonzero
rows in the reduced row echelon form of A.
Any elementary transformation of A can be expressed as the multiplication of A by
a matrix referred to as an elementary transformation matrix. An elementary transfor-
mation of the rows of A will be given by the premultiplication of A by an elementary
transformation matrix, whereas an elementary transformation of the columns corre-
sponds to a postmultiplication. Elementary transformation matrices are nonsingular,
and any nonsingular matrix can be expressed as the product of elementary transfor-
mation matrices. Consequently, we have Theorem 1.10.
Theorem 1.10
Let A be an m × n matrix, B be an m × m matrix, and C be an
n × n matrix. Then if B and C are nonsingular matrices, it follows that
rank(BAC) = rank(BA) = rank(AC) = rank(A).
By using elementary transformation matrices, any matrix A can be transformed
into another matrix of simpler form having the same rank as A.

ORTHOGONAL MATRICES
15
Theorem 1.11
If A is an m × n matrix of rank r > 0, then nonsingular m × m
and n × n matrices B and C exist, such that H = BAC and A = B−1HC−1,
where H is given by
(a) Ir
if r = m = n,
(b)

Ir
(0)

if r = m < n,
(c)
 Ir
(0)

if r = n < m,
(d)
 Ir
(0)
(0)
(0)

if r < m, r < n.
Corollary 1.11.1 is an immediate consequence of Theorem 1.11.
Corollary 1.11.1
Let A be an m × n matrix with rank(A) = r > 0. Then an m × r
matrix F and an r × n matrix G exist, such that rank(F) = rank(G) = r and A =
FG.
1.10
ORTHOGONAL MATRICES
An m × 1 vector p is said to be a normalized vector or a unit vector if p′p = 1. The
m × 1 vectors, p1, . . . , pn, where n ≤m, are said to be orthogonal if p′
ipj = 0 for
all i ̸= j. If in addition, each pi is a normalized vector, then the vectors are said to
be orthonormal. An m × m matrix P whose columns form an orthonormal set of
vectors is called an orthogonal matrix. It immediately follows that
P ′P = Im.
Taking the determinant of both sides, we see that
|P ′P| = |P ′||P| = |P|2 = |Im| = 1.
Thus, |P| = +1 or −1, so that P is nonsingular, P −1 = P ′, and PP ′ = Im in
addition to P ′P = Im; that is, the rows of P also form an orthonormal set of
m × 1 vectors. Some basic properties of orthogonal matrices are summarized in
Theorem 1.12.
Theorem 1.12
Let P and Q be m × m orthogonal matrices and A be any m × m
matrix. Then
(a) |P| = ±1,
(b) |P ′AP| = |A|,
(c) PQ is an orthogonal matrix.

16
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
One example of an m × m orthogonal matrix, known as the Helmert matrix, has
the form
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
√m
1
√m
1
√m
· · ·
1
√m
1
√
2
−1
√
2
0
· · ·
0
1
√
6
1
√
6
−2
√
6
· · ·
0
...
...
...
...
...
1
√
m(m−1)
1
√
m(m−1)
1
√
m(m−1) · · ·
−
(m−1)
√
m(m−1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
For instance, if m = 4, the Helmert matrix is
H =
⎡
⎢⎢⎣
1/2
1/2
1/2
1/2
1/
√
2
−1/
√
2
0
0
1/
√
6
1/
√
6
−2/
√
6
0
1/
√
12
1/
√
12
1/
√
12
−3/
√
12
⎤
⎥⎥⎦.
Note that if m ̸= n, it is possible for an m × n matrix P to satisfy one of the
identities, P ′P = In or PP ′ = Im, but not both. Such a matrix is sometimes referred
to as a semiorthogonal matrix.
An m × m matrix P is called a permutation matrix if each row and each column
of P has a single element 1, while all remaining elements are zeros. As a result, the
columns of P will be e1, . . . , em, the columns of Im, in some order. Note then that
the (h, h)th element of P ′P will be e′
iei = 1 for some i, and the (h, l)th element
of P ′P will be e′
iej = 0 for some i ̸= j if h ̸= l; that is, a permutation matrix is a
special orthogonal matrix. Since there are m! ways of permuting the columns of Im,
there are m! different permutation matrices of order m. If A is also m × m, then PA
creates an m × m matrix by permuting the rows of A, and AP produces a matrix by
permuting the columns of A.
1.11
QUADRATIC FORMS
Let x be an m × 1 vector, y an n × 1 vector, and A an m × n matrix. Then the
function of x and y given by
x′Ay =
m

i=1
n

j=1
xiyjaij
is sometimes called a bilinear form in x and y. We will be most interested in the spe-
cial case in which m = n, so that A is m × m, and x = y. In this case, the function
above reduces to the function of x,
f(x) = x′Ax =
m

i=1
m

j=1
xixjaij,

QUADRATIC FORMS
17
which is called a quadratic form in x; A is referred to as the matrix of the quadratic
form. We will always assume that A is a symmetric matrix because, if it is not, A
may be replaced by B = 1
2(A + A′), which is symmetric, without altering f(x);
that is,
x′Bx = 1
2x′(A + A′)x = 1
2(x′Ax + x′A′x)
= 1
2(x′Ax + x′Ax) = x′Ax
because x′A′x = (x′A′x)′ = x′Ax. As an example, consider the function
f(x) = x2
1 + 3x2
2 + 2x2
3 + 2x1x2 −2x2x3,
where x is 3 × 1. The symmetric matrix A satisfying f(x) = x′Ax is given by
A =
⎡
⎣
1
1
0
1
3 −1
0 −1
2
⎤
⎦.
Every symmetric matrix A and its associated quadratic form is classified into one
of the following five categories:
(a) If x′Ax > 0 for all x ̸= 0, then A is positive definite.
(b) If x′Ax ≥0 for all x and x′Ax = 0 for some x ̸= 0, then A is positive
semidefinite.
(c) If x′Ax < 0 for all x ̸= 0, then A is negative definite.
(d) If x′Ax ≤0 for all x and x′Ax = 0 for some x ̸= 0, then A is negative
semidefinite.
(e) If x′Ax > 0 for some x and x′Ax < 0 for some x, then A is indefinite.
Note that the null matrix is actually both positive semidefinite and negative semidef-
inite.
Positive definite and negative definite matrices are nonsingular, whereas positive
semidefinite and negative semidefinite matrices are singular. Sometimes the term
nonnegative definite will be used to refer to a symmetric matrix that is either pos-
itive definite or positive semidefinite. An m × m matrix B is called a square root of
the nonnegative definite m × m matrix A if A = BB′. Sometimes we will denote
such a matrix B as A1/2. If B is also symmetric, so that A = B2, then B is called the
symmetric square root of A.
Quadratic forms play a prominent role in inferential statistics. In Chapter 11, we
will develop some of the most important results involving quadratic forms that are of
particular interest in statistics.

18
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
1.12
COMPLEX MATRICES
Throughout most of this text, we will be dealing with the analysis of vectors and
matrices composed of real numbers or variables. However, there are occasions in
which an analysis of a real matrix, such as the decomposition of a matrix in the form
of a product of other matrices, leads to matrices that contain complex numbers. For
this reason, we will briefly summarize in this section some of the basic notation and
terminology regarding complex numbers.
Any complex number c can be written in the form
c = a + ib,
where a and b are real numbers and i represents the imaginary number √−1. The
real number a is called the real part of c, whereas b is referred to as the imaginary
part of c. Thus, the number c is a real number only if b is 0. If we have two complex
numbers, c1 = a1 + ib1 and c2 = a2 + ib2, then their sum is given by
c1 + c2 = (a1 + a2) + i(b1 + b2),
whereas their product is given by
c1c2 = a1a2 −b1b2 + i(a1b2 + a2b1).
Corresponding to each complex number c = a + ib is another complex number
denoted by c and called the complex conjugate of c. The complex conjugate of c
is given by c = a −ib and satisfies cc = a2 + b2, so that the product of a complex
number and its conjugate results in a real number.
A complex number can be represented geometrically by a point in the complex
plane, where one of the axes is the real axis and the other axis is the complex or
imaginary axis. Thus, the complex number c = a + ib would be represented by the
point (a, b) in this complex plane. Alternatively, we can use the polar coordinates
(r, θ), where r is the length of the line from the origin to the point (a, b) and θ is the
angle between this line and the positive half of the real axis. The relationship between
a and b, and r and θ is then given by
a = r cos(θ), b = r sin(θ).
Writing c in terms of the polar coordinates, we have
c = r cos(θ) + ir sin(θ),
or, after using Euler’s formula, simply c = reiθ. The absolute value, also sometimes
called the modulus, of the complex number c is defined to be r. This is, of course,
always a nonnegative real number, and because a2 + b2 = r2, we have
|c| = |a + ib| =

a2 + b2.

RANDOM VECTORS AND SOME RELATED STATISTICAL CONCEPTS
19
We also find that
|c1c2| =

(a1a2 −b1b2)2 + (a1b2 + a2b1)2
=

(a2
1 + b2
1)(a2
2 + b2
2) = |c1||c2|.
Using this identity repeatedly, we also see that for any complex number c and any
positive integer n, |cn| = |c|n.
A useful identity relating a complex number c and its conjugate to the absolute
value of c is
cc = |c|2.
Applying this identity to the sum of two complex numbers c1 + c2 and noting that
c1c2 + c1c2 ≤2|c1||c2|, we get
|c1 + c2|2 = (c1 + c2)(c1 + c2) = (c1 + c2)(c1 + c2)
= c1c1 + c1c2 + c2c1 + c2c2
≤|c1|2 + 2|c1||c2| + |c2|2
= (|c1| + |c2|)2.
From this result, we get the important inequality, |c1 + c2| ≤|c1| + |c2|, known as
the triangle inequality.
A complex matrix is simply a matrix whose elements are complex numbers. As a
result, a complex matrix can be written as the sum of a real matrix and an imaginary
matrix; that is, if C is an m × n complex matrix then it can be expressed as
C = A + iB,
where both A and B are m × n real matrices. The complex conjugate of C, denoted
C, is simply the matrix containing the complex conjugates of the elements of C;
that is,
C = A −iB.
The conjugate transpose of C is C∗= C
′. If the complex matrix C is square and
C∗= C, so that cij = cji, then C is said to be Hermitian. Note that if C is Hermitian
and C is a real matrix, then C is symmetric. The m × m matrix C is said to be unitary
if C∗C = Im, which is the generalization of the concept of orthogonal matrices to
complex matrices because if C is real, then C∗= C′.
1.13
RANDOM VECTORS AND SOME RELATED STATISTICAL
CONCEPTS
In this section, we review some of the basic definitions and results in distribution
theory that will be needed later in this text. A more comprehensive treatment of this

20
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
subject can be found in books on statistical theory such as Casella and Berger (2002)
or Lindgren (1993). To be consistent with our notation in which we use a capital letter
to denote a matrix, a bold lowercase letter for a vector, and a lowercase letter for a
scalar, we will use a lowercase letter instead of the more conventional capital letter
to denote a scalar random variable.
A random variable x is said to be discrete if its collection of possible values, Rx,
is a countable set. In this case, x has a probability function px(t) satisfying px(t) =
P(x = t), for t ∈Rx, and px(t) = 0, for t /∈Rx. A continuous random variable x,
on the other hand, has for its range, Rx, an uncountably infinite set. Associated with
each continuous random variable x is a density function fx(t) satisfying fx(t) > 0,
for t ∈Rx, and fx(t) = 0, for t /∈Rx. Probabilities for x are obtained by integration;
if B is a subset of the real line, then
P(x ∈B) =

B
fx(t) dt.
For both discrete and continuous x, we have P(x ∈Rx) = 1.
The expected value of a real-valued function of x, g(x), gives the average observed
value of g(x). This expectation, denoted E[g(x)], is given by
E[g(x)] =

t∈Rx
g(t)px(t),
if x is discrete, and
E[g(x)] =
 ∞
−∞
g(t)fx(t) dt,
if x is continuous. Properties of the expectation operator follow directly from prop-
erties of sums and integrals. For instance, if x is a random variable and α and β are
constants, then the expectation operator satisfies the properties
E(α) = α
and
E[αg1(x) + βg2(x)] = αE[g1(x)] + βE[g2(x)],
where g1 and g2 are any real-valued functions. The expected values of a random vari-
able x given by E(xk), k = 1, 2, . . . are known as the moments of x. These moments
are important for both descriptive and theoretical purposes. The first few moments
can be used to describe certain features of the distribution of x. For instance, the first
moment or mean of x, μx = E(x), locates a central value of the distribution. The
variance of x, denoted σ2
x or var(x), is defined as
σ2
x = var(x) = E[(x −μx)2] = E(x2) −μ2
x,

RANDOM VECTORS AND SOME RELATED STATISTICAL CONCEPTS
21
so that it is a function of the first and second moments of x. The variance gives a
measure of the dispersion of the observed values of x about the central value μx.
Using properties of expectation, it is easily verified that
var(α + βx) = β2 var(x).
All of the moments of a random variable x are embedded in a function called the
moment generating function of x. This function is defined as a particular expectation;
specifically, the moment generating function of x, mx(t), is given by
mx(t) = E(etx),
provided this expectation exists for values of t in a neighborhood of 0. Otherwise, the
moment generating function does not exist. If the moment generating function of x
does exist, then we can obtain any moment from it because
dk
dtk mx(t)




t=0
= E(xk).
More importantly, the moment generating function characterizes the distribution of x
in that, under certain conditions, no two different distributions have the same moment
generating function.
We now focus on some particular families of distributions that we will encounter
later in this text. A random variable x is said to have a univariate normal distribution
with mean μ and variance σ2, indicated by x ∼N(μ, σ2), if the density of x is given
by
fx(t) =
1
√
2πσ e−(t−μ)2/2σ2,
−∞< t < ∞.
The corresponding moment generating function is
mx(t) = eμt+σ2t2/2.
A special member of this family of normal distributions is the standard normal dis-
tribution N(0, 1). The importance of this distribution follows from the fact that if
x ∼N(μ, σ2), then the standardizing transformation z = (x −μ)/σ yields a random
variable z that has the standard normal distribution. By differentiating the moment
generating function of z ∼N(0, 1), it is easy to verify that the first six moments of
z, which we will need in Chapter 11, are 0, 1, 0, 3, 0, and 15, respectively.
If r is a positive integer, then a random variable v has a chi-squared distribution
with r degrees of freedom, written v ∼χ2
r, if its density function is
fv(t) = t(r/2)−1e−t/2
2r/2Γ(r/2) ,
t > 0,
where Γ(r/2) is the gamma function evaluated at r/2. The moment generating
function of v is given by mv(t) = (1 −2t)−r/2, for t < 1
2. The importance of

22
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
the chi-squared distribution arises from its connection to the normal distribution.
If z ∼N(0, 1), then z2 ∼χ2
1. Further, if z1, . . . , zr are independent random
variables with zi ∼N(0, 1) for i = 1, . . . , r, then
r

i=1
z2
i ∼χ2
r.
(1.5)
The chi-squared distribution mentioned above is sometimes referred to as a central
chi-squared distribution because it is actually a special case of a more general family
of distributions known as the noncentral chi-squared distributions. These noncentral
chi-squared distributions are also related to the normal distribution. If x1, . . . , xr are
independent random variables with xi ∼N(μi, 1), then
r

i=1
x2
i ∼χ2
r(λ),
(1.6)
where χ2
r(λ) denotes the noncentral chi-squared distribution with r degrees of free-
dom and noncentrality parameter
λ = 1
2
r

i=1
μ2
i;
that is, the noncentral chi-squared density, which we will not give here, depends not
only on the parameter r but also on the parameter λ. Since (1.6) reduces to (1.5)
when μi = 0 for all i, we see that the distribution χ2
r(λ) corresponds to the central
chi-squared distribution χ2
r when λ = 0.
A distribution related to the chi-squared distribution is the F distribution with r1
and r2 degrees of freedom, denoted by Fr1,r2. If y ∼Fr1,r2, then the density function
of y is
fy(t) = Γ{(r1 + r2)/2}
Γ(r1/2)Γ(r2/2)
r1
r2
r1/2
t(r1−2)/2

1 + r1
r2
t
−(r1+r2)/2
,
t > 0.
The importance of this distribution arises from the fact that if v1 and v2 are indepen-
dent random variables with v1 ∼χ2
r1 and v2 ∼χ2
r2, then the ratio
t = v1/r1
v2/r2
has the F distribution with r1 and r2 degrees of freedom.
The concept of a random variable can be extended to that of a random vector.
A sequence of related random variables x1, . . . , xm is modeled by a joint or mul-
tivariate probability function px(t) if all of the random variables are discrete, and
a multivariate density function fx(t) if all of the random variables are continuous,
where x = (x1, . . . , xm)′ and t = (t1, . . . , tm)′. For instance, if they are continuous
and B is a region in Rm, then the probability that x falls in B is

RANDOM VECTORS AND SOME RELATED STATISTICAL CONCEPTS
23
P(x ∈B) =

· · ·

B
fx(t) dt1 · · · dtm,
whereas the expected value of the real-valued function g(x) of x is given by
E[g(x)] =
 ∞
−∞
· · ·
 ∞
−∞
g(t)fx(t) dt1 · · · dtm.
The random variables x1, . . . , xm are said to be independent, a concept we have
already referred to, if and only if the joint probability function or density function
factors into the product of the marginal probability or density functions; that is, in the
continuous case, x1, . . . , xm are independent if and only if
fx(t) = fx1(t1) · · · fxm(tm),
for all t.
The mean vector of x, denoted μ, is the vector of expected values of the xi’s; that
is,
μ = (μ1, . . . , μm)′ = E(x) = [E(x1), . . . , E(xm)]′.
A measure of the linear relationship between xi and xj is given by the covariance of
xi and xj, which is denoted cov(xi, xj) or σij and is defined by
σij = cov(xi, xj) = E[(xi −μi)(xj −μj)] = E(xixj) −μiμj.
(1.7)
When i = j, this covariance reduces to the variance of xi; that is, σii = σ2
i =
var(xi). When i ̸= j and xi and xj are independent, then cov(xi, xj) = 0 because
in this case E(xixj) = μiμj. If α1, α2, β1 and β2 are constants, then
cov(α1 + β1xi, α2 + β2xj) = β1β2 cov(xi, xj).
The matrix Ω, which has σij as its (i, j)th element, is called the variance–covariance
matrix, or simply the covariance matrix, of x. This matrix will be also denoted some-
times by var(x) or cov(x, x). Clearly, σij = σji so that Ω is a symmetric matrix.
Using (1.7), we obtain the matrix formulation for Ω,
Ω = var(x) = E[(x −μ)(x −μ)′] = E(xx′) −μμ′.
If α is an m × 1 vector of constants and we define the random variable y = α′x,
then
E(y) = E(α′x) = E
 m

i=1
αixi

=
m

i=1
αiE(xi)
=
m

i=1
αiμi = α′μ.

24
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
If, in addition, β is another m × 1 vector of constants and w = β′x, then
cov(y, w) = cov(α′x, β′x) = cov
⎛
⎝
m

i=1
αixi,
m

j=1
βjxj
⎞
⎠
=
m

i=1
m

j=1
αiβj cov(xi, xj) =
m

i=1
m

j=1
αiβjσij = α′Ωβ.
In particular, var(y) = cov(y, y) = α′Ωα. Because this holds for any choice of α
and because the variance is always nonnegative, Ω must be a nonnegative definite
matrix. More generally, if A is a p × m matrix of constants and y = Ax, then
E(y) = E(Ax) = AE(x) = Aμ,
(1.8)
var(y) = E[{y −E(y)}{y −E(y)}′] = E[(Ax −Aμ)(Ax −Aμ)′]
= E[A(x −μ)(x −μ)′A′] = A{E[(x −μ)(x −μ)′]}A′
= AΩA′.
(1.9)
Thus, the mean vector and covariance matrix of the transformed vector, Ax, is Aμ
and AΩA′. If v and w are random vectors, then the matrix of covariances between
components of v and components of w is given by
cov(v, w) = E(vw′) −E(v)E(w)′.
In particular, if v = Ax and w = Bx, then
cov(v, w) = A cov(x, x)B′ = A var(x)B′ = AΩB′.
A measure of the linear relationship between xi and xj that is unaffected by the
measurement scales of xi and xj is called the correlation. We denote this by ρij and
it is defined as
ρij =
cov(xi, xj)

var(xi) var(xj)
=
σij
√σiiσjj
.
When i = j, ρij = 1. The correlation matrix P, which has ρij as its (i, j)th element,
can be expressed in terms of the corresponding covariance matrix Ω and the diagonal
matrix D−1/2
Ω
= diag(σ−1/2
11
, . . . , σ−1/2
mm ); specifically,
P = D−1/2
Ω
ΩD−1/2
Ω
.
(1.10)
For any m × 1 vector α, we have
α′Pα = α′D−1/2
Ω
ΩD−1/2
Ω
α = β′Ωβ,

RANDOM VECTORS AND SOME RELATED STATISTICAL CONCEPTS
25
where β = D−1/2
Ω
α, and so P must be nonnegative definite because Ω is. In partic-
ular, if ei is the ith column of the m × m identity matrix, then
(ei + ej)′P(ei + ej) = (P)ii + (P)ij + (P)ji + (P)jj
= 2(1 + ρij) ≥0
and
(ei −ej)′P(ei −ej) = (P)ii −(P)ij −(P)ji + (P)jj
= 2(1 −ρij) ≥0,
from which we obtain the inequality, −1 ≤ρij ≤1.
Typically, means, variances, and covariances are unknown and so they must be
estimated from a sample. Suppose x1, . . . , xn represents a random sample of a ran-
dom variable x that has some distribution with mean μ and variance σ2. These quan-
tities can be estimated by the sample mean and the sample variance given by
x = 1
n
n

i=1
xi,
s2 =
1
n −1
n

i=1
(xi −x)2 =
1
n −1
 n

i=1
x2
i −nx2

.
In the multivariate setting, we have analogous estimators for μ and Ω; if x1, . . . , xn
is a random sample of an m × 1 random vector x having mean vector μ and covari-
ance matrix Ω, then the sample mean vector and sample covariance matrix are given
by
x = 1
n
n

i=1
xi,
S =
1
n −1
n

i=1
(xi −x)(xi −x)′ =
1
n −1
 n

i=1
xix′
i −nxx′

.
The sample covariance matrix can be then used in (1.10) to obtain an esti-
mator of the correlation matrix, P; that is, if we define the diagonal matrix
D−1/2
S
= diag(s−1/2
11
, . . . , s−1/2
mm ), then the correlation matrix can be estimated by
the sample correlation matrix defined as
R = D−1/2
S
SD−1/2
S
.
One particular joint distribution that we will consider is the multivariate normal
distribution. This distribution can be defined in terms of independent standard normal

26
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
random variables. Let z1, . . . , zm be independently distributed as N(0, 1), and put
z = (z1, . . . , zm)′. The density function of z is then given by
f(z) =
m

i=1
1
√
2π exp

−1
2z2
i

=
1
(2π)m/2 exp

−1
2z′z

.
Because E(z) = 0 and var(z) = Im, this particular m-dimensional multivariate
normal distribution, known as the standard multivariate normal distribution, is
denoted as Nm(0, Im). If μ is an m × 1 vector of constants and T is an m × m
nonsingular matrix, then x = μ + Tz is said to have the m-dimensional multivariate
normal distribution with mean vector μ and covariance matrix Ω = TT ′. This
is indicated by x ∼Nm(μ, Ω). For instance, if m = 2, the vector x = (x1, x2)′
has a bivariate normal distribution and its density, induced by the transformation
x = μ + Tz, can be shown to be
f(x) =
1
2π

σ11σ22(1 −ρ2)
exp

−
1
2(1 −ρ2)
(x1 −μ1)2
σ11
−2ρ
x1 −μ1
√σ11
 x2 −μ2
√σ22

+ (x2 −μ2)2
σ22

,
(1.11)
for all x ∈R2, where ρ = ρ12 is the correlation coefficient. When ρ = 0, this density
factors into the product of the marginal densities, so x1 and x2 are independent if and
only if ρ = 0. The cumbersome-looking density function given in (1.11) can be more
conveniently expressed by using matrix notation. It is straightforward to verify that
this density is identical to
f(x) =
1
2π|Ω|1/2 exp

−1
2(x −μ)′Ω−1(x −μ)

.
(1.12)
The density function of an m-variate normal random vector is very similar to the
function given in (1.12). If x ∼Nm(μ, Ω), then its density is
f(x) =
1
(2π)m/2|Ω|1/2 exp

−1
2(x −μ)′Ω−1(x −μ)

,
(1.13)
for all x ∈Rm.
If Ω is positive semidefinite, then x ∼Nm(μ, Ω) is said to have a singular normal
distribution. In this case, Ω−1 does not exist and so the multivariate normal density
cannot be written in the form given in (1.13). However, the random vector x can still
be expressed in terms of independent standard normal random variables. Suppose that
rank(Ω) = r and U is an m × r matrix satisfying UU ′ = Ω. Then x ∼Nm(μ, Ω) if
x is distributed the same as μ + Uz, where now z ∼Nr(0, Ir).
An important property of the multivariate normal distribution is that a linear trans-
formation of a multivariate normal vector yields a multivariate normal vector; that
is, if x ∼Nm(μ, Ω) and A is a p × m matrix of constants, then y = Ax has a
p-variate normal distribution. In particular, from (1.8) and (1.9), we know that y ∼
Np(Aμ, AΩA′).

RANDOM VECTORS AND SOME RELATED STATISTICAL CONCEPTS
27
We next consider spherical and elliptical distributions that are extensions of mul-
tivariate normal distributions. In particular, a spherical distribution is an extension of
the standard multivariate normal distribution Nm(0, Im), whereas an elliptical distri-
bution is an extension of the multivariate normal distribution Nm(μ, Ω). An m × 1
random vector x has a spherical distribution if x and Px have the same distribu-
tion for all m × m orthogonal matrices P. If x has a spherical distribution with a
density function, then this density function depends on x only through the value of
x′x; that is, the density function of x can be written as g(x′x) for some function
g. The term spherical distribution then arises from the fact that the density function
is the same for all points x that lie on the sphere x′x = c, where c is a nonnegative
constant. Clearly z ∼Nm(0, Im) has a spherical distribution because for any m × m
orthogonal matrix P, Pz ∼Nm(0, Im). An example of a nonnormal spherical dis-
tribution is the uniform distribution; that is, if u is a randomly selected point on the
surface of the unit sphere in Rm, then u has a spherical distribution. In fact, if the
m × 1 random vector x has a spherical distribution, then it can be expressed as
x = wu,
(1.14)
where u is uniformly distributed on the m-dimensional unit sphere, w is a nonnega-
tive random variable, and u and w are independently distributed. It is easy to verify
that when z has the distribution Nm(0, Im), then (1.14) takes the form
z = vu,
where v2 ∼χ2
m. Thus, if the m × 1 random vector x has a spherical distribution, then
it can also be expressed as
x = wu = wv−1z = sz,
where again z has the distribution Nm(0, Im), s = wv−1 is a nonnegative random
variable, and z and s are independently distributed. The contaminated normal distri-
butions and the multivariate t distributions are other examples of spherical distribu-
tions. A random vector x having a contaminated normal distribution can be expressed
as x = sz, where z ∼Nm(0, Im) independently of s, which takes on the values σ
and 1 with probabilities p and 1 −p, respectively, and σ ̸= 1 is a positive constant.
If z ∼Nm(0, Im) independently of v2 ∼χ2
n, then the random vector x = n1/2z/v
has a multivariate t distribution with n degrees of freedom.
We generalize from spherical distributions to elliptical distributions in the same
way that Nm(0, Im) was generalized to Nm(μ, Ω). An m × 1 random vector y has
an elliptical distribution with parameters μ and Ω if it can be expressed as
y = μ + Tx,
where T is m × r, TT ′ = Ω, rank(Ω) = r, and the r × 1 random vector x has a
spherical distribution. Using (1.14), we then have
y = μ + wTu,
where the random variable w ≥0 is independent of u, which is uniformly distributed
on the r-dimensional unit sphere. If Ω is nonsingular and y has a density, then it

28
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
depends on y only through the value of (y −μ)′Ω−1(y −μ); that is, the density is
the same for all points y that lie on the ellipsoid (y −μ)′Ω−1(y −μ) = c, where c
is a nonnegative constant. A more detailed discussion about spherical and elliptical
distributions can be found in Fang et al. (1990).
One of the most widely used procedures in statistics is regression analysis. We will
briefly describe this analysis here and later use regression analysis to illustrate some
of the matrix methods developed in this text. Some good references on regression are
Kutner et al. (2005), Rencher and Schaalje (2008), and Sen and Srivastava (1990).
In the typical regression problem, one wishes to study the relationship between some
response variable, say y, and k explanatory variables x1, . . . , xk. For instance, y
might be the yield of some product of a manufacturing process, whereas the explana-
tory variables are conditions affecting the production process, such as temperature,
humidity, pressure, and so on. A model relating the xj’s to y is given by
y = β0 + β1x1 + · · · + βkxk + ϵ,
(1.15)
where β0, . . . , βk are unknown parameters and ϵ is a random error, that is, a
random variable, with E(ϵ) = 0. In what is known as ordinary least squares
regression, we also have the errors as independent random variables with common
variance σ2; that is, if ϵi and ϵj are random errors associated with the responses
yi and yj, then var(ϵi) = var(ϵj) = σ2 and cov(ϵi, ϵj) = 0. The model given
in (1.15) is an example of a linear model because it is a linear function of the
parameters. It need not be linear in the xj’s so that, for instance, we might have
x2 = x2
1. Because the parameters are unknown, they must be estimated and this will
be possible if we have some observed values of y and the corresponding xj’s. Thus,
for the ith observation, suppose that the explanatory variables are set to the values
xi1, . . . , xik yielding the response yi, and this is done for i = 1, . . . , N, where
N > k + 1. If model (1.15) holds, then we should have, approximately,
yi = β0 + β1xi1 + · · · + βkxik
for each i. This can be written as the matrix equation
y = Xβ
if we define
y =
⎡
⎢⎢⎢⎣
y1
y2
...
yN
⎤
⎥⎥⎥⎦,
β =
⎡
⎢⎢⎢⎣
β0
β1
...
βk
⎤
⎥⎥⎥⎦,
X =
⎡
⎢⎢⎢⎣
1
x11
· · ·
x1k
1
x21
· · ·
x2k
...
...
...
1
xN1
· · ·
xNk
⎤
⎥⎥⎥⎦.
One method of estimating the βj’s, which we will discuss from time to time in this
text, is called the method of least squares. If ˆβ = ( ˆβ0, . . . , ˆβk)′ is an estimate of the

PROBLEMS
29
parameter vector β, then ˆy = X ˆβ is the vector of fitted values, whereas y −ˆy gives
the vector of errors or deviations of the actual responses from the corresponding fitted
values, and
f(ˆβ) = (y −X ˆβ)′(y −X ˆβ)
gives the sum of squares of these errors. The method of least squares selects as ˆβ
any vector that minimizes the function f(ˆβ). We will see later that any such vector
satisfies the system of linear equations, sometimes referred to as the normal equations,
X′X ˆβ = X′y.
If X has full column rank, that is, rank(X) = k + 1, then (X′X)−1 exists and so the
least squares estimator of β is unique and is given by
ˆβ = (X′X)−1X′y.
PROBLEMS
1.1 Show that the scalar properties ab = 0 implies a = 0 or b = 0, and ab = ac for
a ̸= 0 implies that b = c do not extend to matrices by finding
(a) 2 × 2 nonnull matrices A and B for which AB = (0),
(b) 2 × 2 matrices A, B, and C, with A being nonnull, such that AB = AC,
yet B ̸= C.
1.2 Let A be an m × m idempotent matrix. Show that
(a) Im −A is idempotent,
(b) BAB−1 is idempotent, where B is any m × m nonsingular matrix.
1.3 Let A and B be m × m symmetric matrices. Show that AB is symmetric if and
only if AB = BA.
1.4 Prove Theorem 1.3(e); that is, if A is an m × n matrix, show that tr(A′A) = 0
if and only if A = (0).
1.5 Show that
(a) if x and y are m × 1 vectors, tr(xy′) = x′y,
(b) if A and B are m × m matrices and B is nonsingular, tr(BAB−1) =
tr(A).
1.6 Suppose A is m × n and B is n × m. Show that tr(AB) = tr(A′B′).
1.7 Suppose that A, B, and C are m × m matrices. Show that if they are symmetric
matrices, then tr(ABC) = tr(ACB).
1.8 Prove Theorem 1.4.
1.9 Show that any square matrix can be written as the sum of a symmetric matrix
and a skew-symmetric matrix.
1.10 Let A and B be m × m symmetric matrices. Show that AB −BA is a
skew-symmetric matrix.

30
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
1.11 Suppose that A is an m × m skew-symmetric matrix. Show that −A2 is a non-
negative definite matrix.
1.12 Define the m × m matrices A, B, and C as
A =
⎡
⎢⎢⎢⎣
b11 + c11
b12 + c12
· · ·
b1m + c1m
a21
a22
· · ·
a2m
...
...
...
am1
am2
· · ·
amm
⎤
⎥⎥⎥⎦,
B =
⎡
⎢⎢⎢⎣
b11
b12
· · ·
b1m
a21
a22
· · ·
a2m
...
...
...
am1
am2
· · ·
amm
⎤
⎥⎥⎥⎦,
C =
⎡
⎢⎢⎢⎣
c11
c12
· · ·
c1m
a21
a22
· · ·
a2m
...
...
...
am1
am2
· · ·
amm
⎤
⎥⎥⎥⎦.
Prove that |A| = |B| + |C|.
1.13 Verify the results of Theorem 1.8.
1.14 Suppose that A and B are m × m nonnull matrices satisfying AB = (0). Show
that both A and B must be singular matrices.
1.15 Consider the 4 × 4 matrix
A =
⎡
⎢⎢⎣
1
2
1
1
0
1
2
0
1
2
2
1
0
−1
1
2
⎤
⎥⎥⎦.
Find the determinant of A by using the cofactor expansion formula on the first
column of A.
1.16 Using the matrix A from the previous problem, verify (1.3) when i = 1 and
k = 2.
1.17 Prove Theorem 1.6.
1.18 Let λ be a variable, and consider the determinant of A −λIm, where A is an
m × m matrix, as a function of λ. What type of function of λ is this?
1.19 Find the adjoint matrix of the matrix A given in Problem 1.15. Use this to obtain
the inverse of A.
1.20 Using elementary transformations, determine matrices B and C so that
BAC = I4 for the matrix A given in Problem 1.15. Use B and C to compute
the inverse of A; that is, take the inverse of both sides of the equation
BAC = I4 and then solve for A−1.

PROBLEMS
31
1.21 Compute the inverse of
(a) Im + 1m1′
m,
(b) Im + e11′
m.
1.22 Show that
(a) the determinant of a triangular matrix is the product of its diagonal
elements,
(b) the inverse of a lower triangular matrix is a lower triangular matrix.
1.23 Let a and b be m × 1 vectors and D be an m × m diagonal matrix. Use Corol-
lary 1.9.2 to find an expression for the inverse of D + αab′, where α is a scalar.
1.24 Let A# be the adjoint matrix of an m × m matrix A. Show that
(a) |A#| = |A|m−1,
(b) (αA)# = αm−1A#, where α is a scalar.
1.25 Consider the m × m partitioned matrix
A =

A11 (0)
A21 A22

,
where the m1 × m1 matrix A11 and the m2 × m2 matrix A22 are nonsingular.
Obtain an expression for A−1 in terms of A11, A22, and A21.
1.26 Let
A =

A11 A12
A′
12 A22

,
where A11 is m1 × m1, A22 is m2 × m2, and A12 is m1 × m2. Show that if A
is positive definite, then A11 and A22 are also positive definite.
1.27 Find the rank of the 4 × 4 matrix
A =
⎡
⎢⎢⎣
2
0
1
−1
1
−1
1
−1
1
−1
2
0
2
0
0
−2
⎤
⎥⎥⎦.
1.28 Use elementary transformations to transform the matrix A given in Problem
1.27 to a matrix H having the form given in Theorem 1.11. Consequently, deter-
mine matrices B and C so that BAC = H.
1.29 Prove parts (b) and (c) of Theorem 1.12.
1.30 List all permutation matrices of order 3.
1.31 Consider the 3 × 3 matrix
P =
1
√
6
⎡
⎣
√
2
√
2
√
2
√
3
−
√
3
0
p31
p32
p33
⎤
⎦.
Find values for p31, p32, and p33 so that P is an orthogonal matrix. Is your
solution unique?

32
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
1.32 Give the conditions on the m × 1 vector x so that the matrix H = Im −2xx′
is orthogonal.
1.33 Suppose the m × m orthogonal matrix P is partitioned as P = [P1 P2], where
P1 is m × m1, P2 is m × m2, and m1 + m2 = m. Show that P ′
1P1 = Im1,
P ′
2P2 = Im2, and P1P ′
1 + P2P ′
2 = Im.
1.34 Let A, B, and C be m × n, n × p, and n × n matrices, respectively, while x is
an n × 1 vector. Show that
(a) Ax = 0 for all choices of x if and only if A = (0),
(b) Ax = 0 if and only if A′Ax = 0,
(c) A = (0) if A′A = (0),
(d) AB = (0) if and only if A′AB = (0),
(e) x′Cx = 0 for all x if and only if C′ = −C.
1.35 For each of the following, find the 3 × 3 symmetric matrix A so that the given
identity holds:
(a) x′Ax = x2
1 + 2x2
2 −x2
3 + 4x1x2 −6x1x3 + 8x2x3.
(b) x′Ax = 3x2
1 + 5x2
2 + 2x2
3 + 2x1x2 + 2x1x3 + 4x2x3.
(c) x′Ax = 2x1x2 + 2x1x3 + 2x2x3.
1.36 Let x be a 4 × 1 vector. Find symmetric matrices A1 and A2 such that
x′A1x = (x1 + x2 −2x3)2 + (x3 −x4)2,
x′A2x = (x1 −x2 −x3)2 + (x1 + x2 −x4)2.
1.37 Let A be an m × m matrix, and suppose that a real n × m matrix T exists such
that T ′T = A. Show that A must be nonnegative definite.
1.38 Prove that a nonnegative definite matrix must have nonnegative diagonal
elements; that is, show that if a symmetric matrix has any negative diagonal
elements, then it is not nonnegative definite. Show that the converse is not true;
that is, find a symmetric matrix that has nonnegative diagonal elements but is
not nonnegative definite.
1.39 Let A be an m × m nonnegative definite matrix, while B is an n × m matrix.
Show that BAB′ is a nonnegative definite matrix.
1.40 Define A as
A =
5 1
1 4

.
Find an upper triangular square root matrix of A; that is, find a 2 × 2 upper
triangular matrix B satisfying BB′ = A.
1.41 Use the standard normal moment generating function, mz(t) = et2/2, to show
that the first six moments of the standard normal distribution are 0, 1, 0, 3, 0,
and 15.

PROBLEMS
33
1.42 Use properties of expectation to show that for random variables x1 and x2, and
scalars α1, α2, β1, and β2,
cov(α1 + β1x1, α2 + β2x2) = β1β2 cov(x1, x2).
1.43 Let S
be the sample covariance matrix computed from the sample
x1, . . . , xn, where each xi is m × 1. Define the m × n matrix X to be
X = (x1, . . . , xn). Find a matrix expression for the symmetric matrix A
satisfying S = (n −1)−1XAX′.
1.44 Show that if x ∼Nm(μ, Ω), where Ω is positive definite, then (x −μ)′Ω−1
(x −μ) ∼χ2
m.
1.45 Suppose x ∼N3(μ, Ω), where
μ =
⎡
⎣
1
2
3
⎤
⎦,
Ω =
⎡
⎣
2
1
−1
1
2
1
−1
1
3
⎤
⎦,
and let the 3 × 3 matrix A and 2 × 3 matrix B be given by
A =
⎡
⎣
2
2
1
1
0
−1
0
1
−1
⎤
⎦,
B =

1
1
1
−1
1
0

.
(a) Find the correlation matrix of x.
(b) Determine the distribution of u = 1′
3x.
(c) Determine the distribution of v = Ax.
(d) Determine the distribution of
w =

Ax
Bx

.
(e) Which, if any, of the distributions obtained in (b), (c), and (d) are singular
distributions?
1.46 Suppose x is an m × 1 random vector with mean vector μ and covariance
matrix Ω. If A is an n × m matrix of constants and c is an m × 1 vector of
constants, give expressions for
(a) E[A(x + c)],
(b) var[A(x + c)].
1.47 Let x1, . . . , xm be a random sample from a normal population with mean μ
and variance σ2, so that x = (x1, . . . , xm)′ ∼Nm(μ1m, σ2Im).
(a) What is the distribution of u = Hx, where H is the Helmert matrix?
(b) Show that m
i=1 (xi −x)2 = m
i=2 u2
i, and use this to establish that s2 is
distributed independently of x.

34
A REVIEW OF ELEMENTARY MATRIX ALGEBRA
1.48 Use the stochastic representation given in Section 1.13 for a random vector
x having a contaminated normal distribution to show that E(x) = 0 and
var(x) = {1 + p(σ2 −1)}Im.
1.49 Show that if x has the multivariate t distribution with n degrees of freedom as
given in Section 1.13, then E(x) = 0 and var(x) =
n
n−2Im if n > 2.

2
VECTOR SPACES
2.1
INTRODUCTION
In statistics, observations typically take the form of vectors of values of different
variables; for example, for each subject in a sample, one might record height, weight,
age, and so on. In estimation and hypotheses testing situations, we are usually inter-
ested in inferences regarding a vector of parameters. As a result, the topic of this
chapter, vector spaces, has important applications in statistics. In addition, the con-
cept of linearly independent and dependent vectors, which we discuss in Section 3,
is very useful in the understanding and determination of the rank of a matrix.
2.2
DEFINITIONS
A vector space is a collection of vectors that satisfies some special properties; in
particular, the collection is closed under the addition of vectors and under the multi-
plication of a vector by a scalar.
Definition 2.1
Let S be a collection of m × 1 vectors satisfying the following:
(a) If x1 ∈S and x2 ∈S, then x1 + x2 ∈S.
(b) If x ∈S and α is any real scalar, then αx ∈S.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

36
VECTOR SPACES
Then S is called a vector space in m-dimensional space. If S is a subset of T, which
is another vector space in m-dimensional space, then S is called a vector subspace of
T, which will be indicated by writing S ⊆T.
The choice of α = 0 in Definition 2.1(b) implies that the null vector 0 ∈S; that
is, every vector space must contain the null vector. In fact, the set S = {0} consisting
of the null vector only is itself a vector space. Note also that the two conditions (a)
and (b) are equivalent to the one condition that says if x1 ∈S, x2 ∈S, and α1 and
α2 are any real scalars, then (α1x1 + α2x2) ∈S. This can be easily generalized to
more than two, say n, vectors; that is, if α1,. . ., αn are real scalars and x1,. . ., xn
are vectors such that xi ∈S, for all i, then for S to be a vector space, we must have
n

i=1
αixi ∈S.
(2.1)
The left-hand side of (2.1) is called a linear combination of the vectors x1,. . ., xn.
Because a vector space is closed under the formation of linear combinations, vector
spaces are sometimes also referred to as linear spaces.
The concept of linear spaces can be generalized to sets that contain elements that
are not vectors. For example, if S is a set of m × n matrices, it is a linear space as
long as α1X1 + α2X2 ∈S for all choices of X1 ∈S, X2 ∈S, and scalars α1 and α2.
We will use Rm×n to denote the linear space consisting of all m × n matrices with
real components.
Example 2.1
Consider the sets of vectors given by
S1 = {(a, 0, a)′ : −∞< a < ∞},
S2 = {(a, b, a + b)′ : −∞< a < ∞, −∞< b < ∞},
S3 = {(a, a, a)′ : a ≥0}.
Let x1 = (a1, 0, a1)′ and x2 = (a2, 0, a2)′, where a1 and a2 are arbitrary scalars.
Then x1 ∈S1, x2 ∈S1, and
α1x1 + α2x2 = (α1a1 + α2a2, 0, α1a1 + α2a2)′ ∈S1,
so that S1 is a vector space. By a similar argument, we find that S2 is also a vector
space. Further, S1 consists of all vectors of S2 for which b = 0, so S1 is a subset of
S2, and thus S1 is a vector subspace of S2. On the other hand, S3 is not a vector space
because, for example, if we take α = −1 and x = (1, 1, 1)′, then x ∈S3 but
αx = −(1, 1, 1)′ /∈S3.

DEFINITIONS
37
Every vector space with the exception of the vector space {0} has infinitely many
vectors. However, by using the process of forming linear combinations, a vector space
can be associated with a finite set of vectors as long as each vector in the vector space
can be expressed as some linear combination of the vectors in this set.
Definition 2.2
Let {x1,. . ., xn} be a set of m × 1 vectors in the vector space
S. If each vector in S can be expressed as a linear combination of the vectors
x1,. . ., xn, then the set {x1,. . ., xn} is said to span or generate the vector space S,
and {x1,. . ., xn} is called a spanning set of S.
A spanning set for a vector space S
is not uniquely defined unless
S = {0}.
Because
(a, b, a + b)′ = a(1, 0, 1)′ + b(0, 1, 1)′,
it
is
easy
to
see
that {(1, 0, 1)′, (0, 1, 1)′} is a spanning set for the vector space S2 defined in
Example 2.1. However, any set of at least two vectors of this form will also be a
spanning set as long as at least two of the vectors are nonnull vectors that are not
scalar multiples of each other. For instance, {(1, 1, 2)′, (1, −1, 0)′, (2, 3, 5)′} is also
a spanning set for S2.
Suppose we select from the vector space S a set of vectors {x1,. . ., xn}. In gen-
eral, we cannot be assured that every x ∈S is a linear combination of x1,. . ., xn,
and so it is possible that the set {x1,. . ., xn} does not span S. This set must, however,
span a vector space, which is a subspace of S.
Theorem 2.1
Let {x1,. . ., xn} be a set of m × 1 vectors in the vector space S, and
let W be the set of all possible linear combinations of these vectors; that is,
W =

x : x =
n

i=1
αixi, −∞< αi < ∞for all i

.
Then W is a vector subspace of S.
Proof.
Clearly, W is a subset of S because the vectors x1 . . ., xn are in S, and S is
closed under the formation of linear combinations. To prove that W is a subspace of
S, we must show that, for arbitrary vectors u and v in W and scalars a and b, au + bv
is in W. Because u and v are in W, by the definition of W, scalars c1,. . ., cn and
d1,. . ., dn must exist such that
u =
n

i=1
cixi,
v =
n

i=1
dixi.
It then follows that
au + bv = a
 n

i=1
cixi

+ b
 n

i=1
dixi

=
n

i=1
(aci + bdi)xi,
so that au + bv is a linear combination of x1,. . ., xn and thus au + bv ∈W.
□

38
VECTOR SPACES
The notions of the size or length of a vector or the distance between two vec-
tors are important concepts when dealing with vector spaces. Although we are most
familiar with the standard Euclidean formulas for length and distance, there are a vari-
ety of ways of defining length and distance. These measures of length and distance
sometimes involve a product of vectors called an inner product.
Definition 2.3
Let S be a vector space. A function, ⟨x, y⟩, defined for all x ∈S
and y ∈S, is an inner product if for any x, y, and z in S, and any scalar c:
(a) ⟨x, x⟩≥0 with ⟨x, x⟩= 0 if and only if x = 0.
(b) ⟨x, y⟩= ⟨y, x⟩.
(c) ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.
(d) ⟨cx, y⟩= c⟨x, y⟩.
For a given inner product, define the m × m matrix A to have (i, j)th element
aij = ⟨ei,m, ej,m⟩. Then, using properties (c) and (d) of Definition 2.3, we find that
for any m × 1 vectors x and y,
⟨x, y⟩=
 m

i=1
xiei,m

, y

=
m

i=1
xi⟨ei,m, y⟩
=
m

i=1
xi

ei,m,
m

j=1
yjej,m

=
m

i=1
m

j=1
xiyj⟨ei,m, ej,m⟩
=
m

i=1
m

j=1
xiyjaij = x′Ay.
That is, every inner product defined on Rm can be expressed as a bilinear form
⟨x, y⟩= x′Ay, for some m × m matrix A. Due to properties (a) and (b) of Definition
2.3, the matrix A must be positive definite.
A useful result regarding inner products is given by the Cauchy–Schwarz
inequality.
Theorem 2.2
If x and y are in the vector space S and ⟨x, y⟩is an inner product
defined on S, then
⟨x, y⟩2 ≤⟨x, x⟩⟨y, y⟩,
(2.2)
with equality if and only if one of the vectors is a scalar multiple of the other.
Proof.
The result is trivial if x = 0 because it is easily shown that, in this case,
⟨x, y⟩= ⟨x, x⟩= 0 so that (2.2) holds with equality and x = αy, where α = 0.

DEFINITIONS
39
Suppose that x ̸= 0, and let a = ⟨x, x⟩, b = 2⟨x, y⟩, and c = ⟨y, y⟩. Then using
Definition 2.3, we find that for any scalar t,
0 ≤⟨tx + y, tx + y⟩= ⟨x, x⟩t2 + 2⟨x, y⟩t + ⟨y, y⟩
= at2 + bt + c.
(2.3)
Consequently, the polynomial at2 + bt + c either has a repeated real root or no real
roots. This means that the discriminant b2 −4ac must be nonpositive, and this leads
to the inequality
b2 ≤4ac,
which simplifies to ⟨x, y⟩2 ≤⟨x, x⟩⟨y, y⟩as is required. Now if one of the vectors
is a scalar multiple of the other, then y = αx must hold for some α, and clearly this
leads to equality in (2.2). Conversely, equality in (2.2) corresponds to equality in (2.3)
for some t. This can only happen if tx + y = 0, in which case y is a scalar multiple
of x, so the proof is complete.
□
The most common inner product is the Euclidean inner product given by ⟨x, y⟩=
x′y. Applying the Cauchy–Schwarz inequality to this inner product, we find that
 m

i=1
xiyi
2
≤
 m

i=1
x2
i
  m

i=1
y2
i

holds for any m × 1 vectors x and y, with equality if and only if one of these vectors
is a scalar multiple of the other.
A vector norm and a distance function provide us with the means of measuring
the length of a vector and the distance between two vectors.
Definition 2.4
A function ||x|| is a vector norm on the vector space S if, for any
vectors x and y in S, we have
(a) ||x|| ≥0,
(b) ||x|| = 0 if and only if x = 0,
(c) ||cx|| = |c| ||x|| for any scalar c,
(d) ||x + y|| ≤||x|| + ||y||.
Definition 2.5
A function d(x, y) is a distance function defined on the vector space
S if for any vectors x, y, and z in S, we have
(a) d(x, y) ≥0,
(b) d(x, y) = 0 if and only if x = y,
(c) d(x, y) = d(y, x),
(d) d(x, z) ≤d(x, y) + d(y, z).

40
VECTOR SPACES
Property (d) in both Definition 2.4 and Definition 2.5 is known as the tri-
angle inequality because it is a generalization of the familiar relationship in
two-dimensional geometry. One common way of defining a vector norm and a dis-
tance function is in terms of an inner product. The reader can verify that for any inner
product, ⟨x, y⟩, the functions, ||x|| = ⟨x, x⟩1/2 and d(x, y) = ⟨x −y, x −y⟩1/2
satisfy the conditions given in Definition 2.4 and Definition 2.5.
We will use Rm to denote the vector space consisting of all m × 1 vectors with
real components; that is, Rm = {(x1,. . ., xm)′ : −∞< xi < ∞, i = 1,. . ., m}.
We usually have associated with this vector space the Euclidean distance function
dI(x, y) = ||x −y||2, where ||x||2 is the Euclidean norm given by
||x||2 = (x′x)1/2 =
 m

i=1
x2
i
1/2
and based on the Euclidean inner product
⟨x, y⟩= x′y. This distance formula
is a generalization of the familiar formulas that we have for distance in two- and
three-dimensional geometry. The space with this distance function is called Euclidean
m-dimensional space. Whenever this text works with the vector space Rm, the
associated distance will be this Euclidean distance unless stated otherwise. However,
in many situations in statistics, non-Euclidean distance functions are appropriate.
Example 2.2
Suppose we wish to compute the distance between the m × 1 vec-
tors x and μ, where x is an observation from a distribution having mean vector μ
and covariance matrix Ω. If we want to take into account the effect of the covari-
ance structure, then Euclidean distance would not be appropriate unless Ω = Im. For
example, if m = 2 and Ω = diag(0.5, 2), then a large value of (x1 −μ1)2 would be
more surprising than a similar value of (x2 −μ2)2 because the variance of the first
component of x is smaller than the variance of the second component; that is, it seems
reasonable in defining distance to put more weight on (x1 −μ1)2 than on (x2 −μ2)2.
A more appropriate distance function is given by
dΩ(x, μ) = {(x −μ)′Ω−1(x −μ)}1/2,
and it is called the Mahalanobis distance between x and μ. This function is some-
times also referred to as the distance between x and μ in the metric of Ω and is
useful in a multivariate procedure known as discriminant analysis (see McLachlan,
2005, or Huberty and Olejnik, 2006). Note that if Ω = Im, then this distance func-
tion reduces to the Euclidean distance function. For Ω = diag(0.5, 2), this distance
function simplifies to
dΩ(x, μ) = {2(x1 −μ1)2 + 0.5(x2 −μ2)2}1/2.
As a second illustration, suppose that again m = 2, but now
Ω =
	
1
0.5
0.5
1

.

DEFINITIONS
41
a
b
c
x
y
θ
θ∗
Figure 2.1
The angle between x and y
Because of the positive correlation, (x1 −μ1) and (x2 −μ2) will tend to have the
same sign. This is reflected in the Mahalanobis distance,
dΩ(x, μ) =
4
3{(x1 −μ1)2 + (x2 −μ2)2 −(x1 −μ1)(x2 −μ2)}
1/2
,
through the last term, which increases or decreases the distance according to whether
(x1 −μ1)(x2 −μ2) is negative or positive. In Chapter 4, we will take a closer look
at the construction of this distance function.
We next consider the angle between two m × 1 nonnull vectors x and y, where
m = 2. We will always choose this angle to be the smaller of the two angles that can
be constructed between the vectors so, for instance, in Figure 2.1, this angle is taken
to be θ as opposed to θ∗= 2π −θ. In Figure 2.1, x coincides with the first axis so
x = (a, 0)′, whereas y = (b cos θ, b sin θ)′, where a = (x′x)1/2 and b = (y′y)1/2
are the lengths of x and y. The squared distance between x and y is then given by
c2 = (y −x)′(y −x) = (b cos θ −a)2 + (b sin θ −0)2
= a2 + b2(cos2 θ + sin2 θ) −2ab cos θ
= a2 + b2 −2ab cos θ.
This identity is called the law of cosines. Solving for cos θ, we get
cos θ = a2 + b2 −c2
2ab
.
Substituting the expressions for a, b, and c in terms of x and y, we obtain
cos θ =
x′y
(x′x)1/2(y′y)1/2 ,
an expression that is valid for all values of m and regardless of the orientation of the
vectors x and y.

42
VECTOR SPACES
We end this section with examples of some other commonly used vector norms.
The norm ||x||1, called the sum norm, is defined by
||x||1 =
m

i=1
|xi|.
Both the sum norm and the Euclidean norm ||x||2 are members of the family of norms
given by
||x||p =
 m

i=1
|xi|p
1/p
,
where p ≥1. Yet another example of a vector norm, known as the infinity norm or
max norm, is given by
||x||∞= max
1≤i≤m |xi|.
Although we have been confining attention to real vectors, these norms also serve as
norms for complex vectors. However, in this case, the absolute values appearing in the
expression for ||x||p are necessary even when p is even. In particular, the Euclidean
norm, valid for complex as well as real vectors, is
||x||2 =
 m

i=1
|xi|2
1/2
= (x∗x)1/2.
2.3
LINEAR INDEPENDENCE AND DEPENDENCE
We have seen that the formation of linear combinations of vectors is a fundamental
operation of vector spaces. This operation is what establishes a link between a span-
ning set and its vector space. In many situations, our investigation of a vector space
can be reduced simply to an investigation of a spanning set for that vector space. In
this case, it will be advantageous to make the spanning set as small as possible. To do
this, it is first necessary to understand the concepts of linear independence and linear
dependence.
Definition 2.6
The set of m × 1 vectors {x1,. . ., xn} is said to be a linearly inde-
pendent set if the only solution to the equation
n

i=1
αixi = 0
is given by α1 = · · · = αn = 0. If there are other solutions, then the set is called a
linearly dependent set.

LINEAR INDEPENDENCE AND DEPENDENCE
43
Example 2.3
Consider the three vectors x1 = (1, 1, 1)′, x2 = (1, 0, −1)′, and x3 =
(3, 2, 1)′. To determine whether these vectors are linearly independent, we solve the
system of equations α1x1 + α2x2 + α3x3 = 0 or, equivalently,
α1 + α2 + 3α3 = 0,
α1 + 2α3 = 0,
α1 −α2 + α3 = 0.
These equations yield the constraints α2 = 0.5α1 and α3 = −0.5α1. Thus, for any
scalar α, a solution will be given by α1 = α, α2 = 0.5α, and α3 = −0.5α, and so the
vectors are linearly dependent. On the other hand, any pair of these vectors are linearly
independent; that is, {x1, x2}, {x1, x3}, and {x2, x3} is each a linearly independent
set of vectors.
The proof of Theorem 2.3 is left to the reader.
Theorem 2.3
Let {x1,. . ., xn} be a set of m × 1 vectors. Then the following state-
ments hold:
(a) The set is linearly dependent if the null vector 0 is in the set.
(b) If this set of vectors is linearly independent, any nonempty subset of it is also
linearly independent.
(c) If this set of vectors is linearly dependent, any other set containing this set as
a subset is also linearly dependent.
Note that in Definition 2.6, if n = 1, that is, only one vector is in the set, then
the set is linearly independent unless that vector is 0. If n = 2, the set is linearly
independent unless one of the vectors is the null vector, or each vector is a nonzero
scalar multiple of the other vector; that is, a set of two vectors is linearly dependent
if and only if at least one of the vectors is a scalar multiple of the other. In general,
we have the following result.
Theorem 2.4
The set of m × 1 vectors {x1,. . ., xn}, where n > 1, is a linearly
dependent set if and only if at least one vector in the set can be expressed as a linear
combination of the remaining vectors.
Proof.
The result is obvious if one of the vectors in the set is the null vector because
then the set must be linearly dependent and the m × 1 null vector is a linear combina-
tion of any set of m × 1 vectors. Now assume the set does not include the null vector.
First suppose one of the vectors, say xn, can be expressed as a linear combination
of the others; that is, we can find scalars α1,. . ., αn−1 such that xn = α1x1 + · · · +
αn−1xn−1. But this implies that
n

i=1
αixi = 0,
(2.4)

44
VECTOR SPACES
if we define αn = −1, so the vectors x1,. . ., xn are linearly dependent. Conversely,
now suppose that the vectors x1,. . ., xn are linearly dependent so that (2.4) holds
for some choice of α1, · · · , αn with at least one of the αi’s, say αn, not equal to zero.
Thus, we can solve (2.4) for xn, in which case, we get
xn =
n−1

i=1
−αi
αn

xi,
so that xn is a linear combination of x1,. . ., xn−1. This completes the proof.
□
We end this section by proving two additional results that we will need later. Note
that the first of these theorems, although stated in terms of the columns of a matrix,
applies as well to the rows of a matrix.
Theorem 2.5
Consider the m × m matrix X with columns x1,. . ., xm. Then
|X| ̸= 0 if and only if the vectors x1,. . ., xm are linearly independent.
Proof.
If |X| = 0, then rank(X) = r < m, and so it follows from Theorem 1.11
that nonsingular m × m matrices U and V = [V1
V2] exist, with V1 m × r, such
that
XU = V
	 Ir
(0)
(0)
(0)

= [V1
(0)].
But then the last column of U will give coefficients for a linear combination of
x1,. . ., xm, which equals the null vector. Thus, if these vectors are to be linearly
independent, we must have |X| ̸= 0. Conversely, if x1,. . ., xm are linearly depen-
dent, we can find a vector u ̸= 0 satisfying Xu = 0 and then construct a nonsingular
matrix U with u as its last column. In this case, XU = [W
0], where W is an
m × (m −1) matrix and, because U is nonsingular,
rank(X) = rank(XU) = rank([W
0]) ≤m −1.
Consequently, if |X| ̸= 0, so that rank(X) = m, then x1,. . ., xm must be linearly
independent.
□
Theorem 2.6
The set {x1,. . ., xn} of m × 1 vectors is linearly dependent if
n > m.
Proof.
Consider the subset of vectors {x1,. . ., xm}. If this is a linearly dependent
set, then it follows from Theorem 2.3(c) that so is the set {x1,. . ., xn}. Thus, the
proof will be complete if we can show that when x1,. . ., xm are linearly independent,
then one of the other vectors, say xm+1, can be expressed as a linear combination of
x1,. . ., xm. When x1,. . ., xm are linearly independent, it follows from the previous
theorem that if we define X as the m × m matrix with x1,. . ., xm as its columns,
then |X| ̸= 0 and so X−1 exists. Let α = X−1xm+1 and note that α ̸= 0 unless

MATRIX RANK AND LINEAR INDEPENDENCE
45
xm+1 = 0, in which case, the theorem is trivially true because of Theorem 2.3(a).
Thus, we have
m

i=1
αixi = Xα = XX−1xm+1 = xm+1,
and so the set {x1,. . ., xm+1} and hence also the set {x1,. . ., xn} is linearly
dependent.
□
2.4
MATRIX RANK AND LINEAR INDEPENDENCE
We have seen that we often work with a vector space through one of its spanning sets.
In many situations, our vector space has, as a spanning set, vectors that are either
the columns or rows of some matrix. In Definition 2.7, we define the terminology
appropriate for such situations.
Definition 2.7
Let X be an m × n matrix. The subspace of Rn spanned by the m
row vectors of X is called the row space of X. The subspace of Rm spanned by the
n column vectors of X is called the column space of X.
The column space of X is sometimes also referred to as the range of X, and we
will identify it by R(X); that is, R(X) is the vector space given by
R(X) = {y : y = Xa, a ∈Rn}.
Note that the row space of X may be written as R(X′).
A consequence of Theorem 2.5 is that the number of linearly independent col-
umn vectors in a matrix is identical to the rank of that matrix when it is nonsingular.
Theorem 2.7 shows that this connection between the number of linearly independent
columns of a matrix and the rank of that matrix always holds.
Theorem 2.7
Let X be an m × n matrix. If r is the number of linearly independent
rows of X and c is the number of linearly independent columns of X, then rank(X) =
r = c.
Proof.
We will only need to prove that rank(X) = r because this proof can be
repeated on X′ to prove that rank(X) = c. We will assume that the first r rows of
X are linearly independent because, if they are not, elementary row transformations
on X will produce such a matrix having the same rank as X. It then follows that the
remaining rows of X can be expressed as linear combinations of the first r rows; that
is, if X1 is the r × n matrix consisting of the first r rows of X, then some (m −r) × r
matrix A exists, such that
X =
	
X1
AX1

=
	
Ir
A

X1.

46
VECTOR SPACES
Now from Theorem 2.6 we know that there can be at most r linearly independent
columns in X1 because these are r × 1 vectors. Thus, we may assume that the last
n −r columns of X1 can be expressed as linear combinations of the first r columns
because, if this is not the case, elementary column transformations on X1 will produce
such a matrix having the same rank as X1. Consequently, if X11 is the r × r matrix
with the first r columns of X1, then an r × (n −r) matrix B exists satisfying
X =

Ir
A

[X11
X11B] =

Ir
A

X11[Ir
B].
If we define the m × m and n × n matrices U and V by
U =

Ir
(0)
−A
Im−r

and
V =

Ir
−B
(0)
In−r

,
then we have
UXV =

X11
(0)
(0)
(0)

.
Because the determinant of a triangular matrix is equal to the product of its diagonal
elements, we find that |U| = |V | = 1, so that U and V are nonsingular and thus
rank(X) = rank(UXV ) = rank(X11).
Finally, we must have rank(X11) = r, because if not, by Theorem 2.5, the rows of
X11 would be linearly dependent and this would contradict the already stated linear
independence of the rows of X1 = [X11
X11B].
□
The formulation of matrix rank in terms of the number of linearly independent
rows or columns of the matrix is often easier to work with than is our original defini-
tion in terms of submatrices. This is evidenced in the proof of Theorem 2.8 regarding
the rank of a matrix.
Theorem 2.8
Let A be an m × n matrix. Then the following properties hold:
(a) If B is an n × p matrix, rank(AB) ≤min{rank(A), rank(B)}.
(b) If B
is an m × n matrix, rank(A + B) ≤rank(A) + rank(B) and
rank(A + B) ≥|rank(A) −rank(B)|.
(c) rank(A) = rank(A′) = rank(AA′) = rank(A′A).
Proof.
Note that if B is n × p, we can write
(AB)·i =
n

j=1
bji(A)·j ;

MATRIX RANK AND LINEAR INDEPENDENCE
47
that is, each column of AB can be expressed as a linear combination of the columns
of A, and so the number of linearly independent columns in AB can be no more
than the number of linearly independent columns in A. Thus, rank(AB) ≤rank(A).
Similarly, each row of AB can be expressed as a linear combination of the rows of B
from which we get rank(AB) ≤rank(B), and so property (a) is proven. To prove
(b), note that by using partitioned matrices, we can write
A + B = [A
B]

In
In

.
So using property (a), we find that
rank(A + B) ≤rank([A
B]) ≤rank(A) + rank(B),
where the final inequality follows from the easily established fact (Problem 2.26) that
the number of linearly independent columns of [A
B] cannot exceed the sum of the
numbers of linearly independent columns in A and in B. This establishes the first
inequality in (b). Note that if we apply this inequality to A and −B, we also get
rank(A −B) ≤rank(A) + rank(B)
(2.5)
because rank(−B) = rank(B). Replacing A in (2.5) by A + B yields
rank(A + B) ≥rank(A) −rank(B),
and replacing B in (2.5) by A + B leads to
rank(A + B) ≥rank(B) −rank(A).
Combining these two inequalities, we get the second inequality in (b). In proving (c),
note that it follows immediately that rank(A) = rank(A′). It will suffice to prove that
rank(A) = rank(A′A) because this can then be used on A′ to prove that rank(A′) =
rank{(A′)′A′} = rank(AA′). If rank(A) = r, then a full column rank m × r matrix
A1 exists, such that, after possibly interchanging some of the columns of A, A =
[A1
A1C] = A1[Ir
C], where C is an r × (n −r) matrix. As a result, we have
A′A =

Ir
C′

A′
1A1[Ir
C].
Note that
EA′AE′ =

A′
1A1
(0)
(0)
(0)

if
E =

Ir
(0)
−C′
In−r

,
and because the triangular matrix E has |E| = 1, E is nonsingular, so rank(A′A) =
rank(EA′AE′) = rank(A′
1A1). If A′
1A1 is less than full rank, then by Theorem
2.5, its columns are linearly dependent, so we can find an r × 1 vector x ̸= 0 such

48
VECTOR SPACES
that A′
1A1x = 0, which implies that x′A′
1A1x = (A1x)′(A1x) = 0. However, for
any real vector y, y′y = 0 only if y = 0 and hence A1x = 0. But this contradicts
rank(A1) = r, and so we must have rank(A′A) = rank(A′
1A1) = r.
□
Theorem 2.9 gives some relationships between the rank of a partitioned matrix
and the ranks of its submatrices. The proofs, which are straightforward, are left to the
reader.
Theorem 2.9
Let A, B, and C be any matrices for which the partitioned matrices
below are defined. Then
(a)
rank([A
B]) ≥max{rank(A), rank(B)},
(b)
rank

A
(0)
(0)
B

= rank

(0)
B
A
(0)

= rank(A) + rank(B),
(c)
rank
	A
(0)
C
B


= rank
	C
B
A
(0)


= rank
	 B
C
(0)
A


= rank
	(0)
A
B
C


≥rank(A) + rank(B).
Theorem 2.10 gives a useful inequality for the rank of the product of three matrices.
Theorem 2.10
Let A, B, and C be p × m, m × n, and n × q matrices, respectively.
Then
rank(ABC) ≥rank(AB) + rank(BC) −rank(B).
Proof.
It follows from Theorem 2.9(c) that
rank
	 B
BC
AB
(0)


≥rank(AB) + rank(BC).
(2.6)
However, because
	 B
BC
AB
(0)

=
	Im
(0)
A
Ip

 	 B
(0)
(0)
−ABC

 	In
C
(0)
Iq

,

BASES AND DIMENSION
49
where, clearly, the first and last matrices on the right-hand side are nonsingular, we
must also have
rank

B
BC
AB
(0)

= rank

B
(0)
(0)
−ABC

= rank(B) + rank(ABC).
(2.7)
Combining (2.6) and (2.7) we obtain the desired result.
□
A special case of Theorem 2.10 is obtained when n = m and B is the m × m
identity matrix. The resulting inequality gives a lower bound for the rank of a matrix
product complementing the upper bound given in Theorem 2.8(a).
Corollary 2.10.1
If A is an m × n matrix and B is an n × p matrix, then
rank(AB) ≥rank(A) + rank(B) −n.
2.5
BASES AND DIMENSION
The concept of dimension is a familiar one from geometry. For example, we recognize
a line as a one-dimensional region and a plane as a two-dimensional region. In this
section, we generalize this notion to any vector space. The dimension of a vector space
can be determined by looking at spanning sets for that vector space. In particular, we
need to be able to find the minimum number of vectors necessary for a spanning set.
Definition 2.8
Let {x1,. . ., xn} be a set of m × 1 vectors in a vector space S. Then
this set is called a basis of S if it spans the vector space S and the vectors x1,. . ., xn
are linearly independent.
Every vector space, except the vector space consisting only of the null vector 0,
has a basis. Although a basis for a vector space is not uniquely defined, a consequence
of our next theorem is that the number of vectors in a basis is unique, and this is what
gives us the dimension of a vector space.
Theorem 2.11
Suppose {x1,. . ., xn} is a basis for the vector space S. Then
(a) any set of more than n vectors in S must be linearly dependent,
(b) any set of fewer than n vectors in S does not span S.
Proof.
Let {y1,. . ., yk} be a set of vectors in S with k > n. Since {x1,. . ., xn}
spans S and each yi is in S, there exists an n × k matrix A such that Y = XA,
where X = (x1,. . ., xn) and Y = (y1,. . ., yk). It follows from Theorem 2.8(a) that

50
VECTOR SPACES
rank(Y ) ≤rank(X) = n < k, and this implies that the columns of Y , that is, the
vectors y1,. . ., yk, are linearly dependent. Now assume that the set {z1,. . ., zp}
spans S. Then since each xi is in S, there exists an p × n matrix B such that X =
ZB, where Z = (z1,. . ., zp). Another application of Theorem 2.8(a) yields n =
rank(X) ≤rank(Z) ≤p, and this establishes (b).
□
Definition 2.9
If the vector space S is {0}, then the dimension of S, denoted by
dim(S), is defined to be zero. Otherwise, the dimension of the vector space S is the
number of vectors in any basis for S.
Example 2.4
Consider the set of m × 1 vectors {e1,. . ., em}, where for each i,
ei is defined to be the vector whose only nonzero component is the ith component,
which is one. Now, the linear combination of the ei’s,
m

i=1
αiei = (α1,. . ., αm)′,
will equal 0 only if α1 = · · · = αm = 0, so the vectors e1,. . ., em are linearly inde-
pendent. Also, if x = (x1,. . ., xm)′ is an arbitrary vector in Rm, then
x =
m

i=1
xiei,
so that {e1,. . ., em} spans Rm. Thus, {e1,. . ., em} is a basis for the m-dimensional
space Rm, and in fact, any linearly independent set of m m × 1 vectors will be a
basis for Rm. For instance, if the m × 1 vector γi has its first i components equal to
one while the rest are all zero, then {γ1,. . ., γm} is also a basis of Rm.
Example 2.5
An implication of Theorem 2.7 is that the dimension of the column
space of a matrix is the same as the dimension of the row space. However, this does
not mean that the two vector spaces are the same. As a simple example, consider the
matrix
X =
⎡
⎣
0
0
1
0
1
0
0
0
0
⎤
⎦,
which has rank 2. The column space of X is the two-dimensional subspace of R3
composed of all vectors of the form (a, b, 0)′, whereas the row space of X is the
two-dimensional subspace of R3 containing all vectors of the form (0, a, b)′. If X
is not square, then the column space and row space will be subspaces of different
Euclidean spaces. For instance, if
X =
⎡
⎣
1
0
0
1
0
1
0
1
0
0
1
1
⎤
⎦,

BASES AND DIMENSION
51
then the column space is R3, whereas the row space is the three-dimensional subspace
of R4 consisting of all vectors of the form (a, b, c, a + b + c)′.
If we have a spanning set for a vector space S and that spanning set is not a basis,
then we could eliminate at least one vector from the set so that the reduced set is still
a spanning set for S.
Theorem 2.12
If V = {x1,. . ., xr} spans a vector space S ̸= {0} and V is a lin-
early dependent set of vectors, then there is a subset of V that also spans S.
Proof.
Because V is a linearly dependent set, it follows from Theorem 2.4 that one
of the vectors can be expressed as a linear combination of the remaining vectors. For
notational convenience, we will assume that we have labeled the xi’s so that xr is
such a vector; that is, scalars α1,. . ., αr−1 exist, such that xr = r−1
i=1 αixi. Now
because V spans S, if x ∈S, scalars β1,. . ., βr exist, so that
x =
r

i=1
βixi = βrxr +
r−1

i=1
βixi
= βr
r−1

i=1
αixi +
r−1

i=1
βixi =
r−1

i=1
(βrαi + βi)xi.
Thus, {x1,. . ., xr−1} spans S and so the proof is complete.
□
Example 2.6
Consider the vector space S spanned by the vectors x1 = (1, 1, 1)′,
x2 = (1, 0, −1)′, and x3 = (3, 2, 1)′. We saw in Example 2.3 that {x1, x2, x3} is
a linearly dependent set of vectors so that this set is not a basis for S. Because
x3 = 2x1 + x2, we can eliminate x3 from the spanning set; that is, {x1, x2} and
{x1, x2, x3} must span the same vector space. The set {x1, x2} is linearly indepen-
dent, and so {x1, x2} is a basis for S and S is a two-dimensional subspace, that is, a
plane in R3. Note that x1 is a linear combination of x2 and x3, and x2 is a linear com-
bination of x1 and x3, so in this case it does not matter which xi is eliminated from the
original spanning set; that is, {x1, x3} and {x2, x3} are also bases for S. As a related
example, consider the vector space S∗spanned by the vectors y1 = (1, 1, 1, 0)′, y2 =
(1, 0, −1, 0)′, y3 = (3, 2, 1, 0)′, and y4 = (0, 0, 0, 1)′. The set consisting of these four
yi vectors is linearly dependent, so we will be able to eliminate one of the vectors.
However, although y1, y2, and y3 each can be written as a linear combination of
the other yi’s, y4 cannot. Thus, we can eliminate any one of the yi’s from the set
except for y4.
Every vector x in a vector space can be expressed as a linear combination of the
vectors in a spanning set. However, in general, more than one linear combination
may yield a particular x. Our next result indicates that this is not the case when the
spanning set is a basis.

52
VECTOR SPACES
Theorem 2.13
Suppose the set of m × 1 vectors {x1,. . ., xn} is a basis for the
vector space S. Then any vector x ∈S has a unique representation as a linear com-
bination of the vectors x1,. . ., xn.
Proof.
Because the vectors x1,. . ., xn span S and x ∈S, scalars α1,. . ., αn must
exist, such that
x =
n

i=1
αixi.
Thus, we only need to prove that the representation above is unique. Suppose it is not
unique so that another set of scalars β1,. . ., βn exists for which
x =
n

i=1
βixi.
But this result then implies that
n

i=1
(αi −βi)xi =
n

i=1
αixi −
n

i=1
βixi = x −x = 0.
Because {x1,. . ., xn} is a basis, the vectors x1,. . ., xn must be linearly indepen-
dent and so we must have αi −βi = 0 for all i. Thus, we must have αi = βi, for
i = 1,. . ., n and so the representation is unique.
□
Some additional useful results regarding vector spaces and their bases are summa-
rized in Theorem 2.14. The proofs are left to the reader.
Theorem 2.14
For any vector space S, the following properties hold:
(a) If {x1,. . ., xn} is a set of linearly independent vectors in a vector space S
and the dimension of S is n, then {x1,. . ., xn} is a basis for S.
(b) If the set {x1,. . ., xn} spans the vector space S and the dimension of S is n,
then the set {x1,. . ., xn} must be linearly independent and thus a basis for S.
(c) If the vector space S has dimension n and the set of linearly independent
vectors {x1,. . ., xr} is in S, where r < n, then there are bases for S that
contain this set as a subset.
Our final theorem in this section gives some intuitively appealing results regarding
a vector space which is a subspace of another vector space.
Theorem 2.15
Suppose that S and T are subspaces of Rm with S ⊂T. Then
(a) dim(S) ≤dim(T),
(b) S = T if dim(S) = dim(T).

ORTHONORMAL BASES AND PROJECTIONS
53
Proof.
Let n = dim(S), p = dim(T), and suppose that n > p. Then a basis for S
would contain n linearly independent vectors. But since S ⊂T, these vectors are
also in T leading to a contradiction of Theorem 2.11(a). Thus, we must have n ≤p.
Now if n = dim(S) = dim(T), then any basis for S contains exactly n vectors. But
S ⊂T, so these n linearly independent vectors are also in T and would have to form
a basis for T since dim(T) = n. This ensures that S = T.
□
2.6
ORTHONORMAL BASES AND PROJECTIONS
If each vector in a basis for a vector space S is orthogonal to every other vector in
that basis, then the basis is called an orthogonal basis. In this case, the vectors can be
viewed as a set of coordinate axes for the vector space S. We will find it useful also
to have each vector in our basis scaled to unit length, in which case, we would have
an orthonormal basis.
Suppose the set {x1,. . ., xr} forms a basis for the vector space S, and we wish
to obtain an orthonormal basis for S. Unless r = 1, an orthonormal basis is not
unique so that there are many different orthonormal bases that we can construct.
One method of obtaining an orthonormal basis from a given basis {x1,. . ., xr} is
called Gram–Schmidt orthonormalization. First, we construct the set {y1,. . ., yr}
of orthogonal vectors given by
y1 = x1,
y2 = x2 −x′
2y1
y′
1y1
y1,
...
yr = xr −x′
ry1
y′
1y1
y1 −· · · −x′
ryr−1
y′
r−1yr−1
yr−1,
(2.8)
and then the set of orthonormal vectors {z1,. . ., zr}, where for each i,
zi =
yi
(y′
iyi)1/2 .
Note that the linear independence of x1,. . ., xr guarantees the linear independence
of y1,. . ., yr. Thus, we have Theorem 2.16.
Theorem 2.16
Every r-dimensional vector space, except the zero-dimensional
space {0}, has an orthonormal basis.
If {z1,. . ., zr} is a basis for the vector space S and x ∈S, then from Theorem
2.13, we know that x can be uniquely expressed in the form x = α1z1 + · · · + αrzr.
When {z1,. . ., zr} is an orthonormal basis, each of the scalars α1,. . ., αr has a sim-
ple form; premultiplication of this equation for x by z′
i yields the identity αi = z′
ix.

54
VECTOR SPACES
Example 2.7
We will find an orthonormal basis for the three-dimensional vector
space S, which has as a basis {x1, x2, x3}, where
x1 =
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦,
x2 =
⎡
⎢⎢⎣
1
−2
1
−2
⎤
⎥⎥⎦,
x3 =
⎡
⎢⎢⎣
3
1
1
−1
⎤
⎥⎥⎦.
The orthogonal yi’s are given by y1 = (1, 1, 1, 1)′,
y2 =
⎡
⎢⎢⎣
1
−2
1
−2
⎤
⎥⎥⎦−(−2)
4
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
3/2
−3/2
3/2
−3/2
⎤
⎥⎥⎦,
and
y3 =
⎡
⎢⎢⎣
3
1
1
−1
⎤
⎥⎥⎦−(4)
(4)
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦−(6)
(9)
⎡
⎢⎢⎣
3/2
−3/2
3/2
−3/2
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
1
−1
−1
⎤
⎥⎥⎦.
Normalizing these vectors yields the orthonormal basis {z1, z2, z3}, where
z1 =
⎡
⎢⎢⎣
1/2
1/2
1/2
1/2
⎤
⎥⎥⎦,
z2 =
⎡
⎢⎢⎣
1/2
−1/2
1/2
−1/2
⎤
⎥⎥⎦,
z3 =
⎡
⎢⎢⎣
1/2
1/2
−1/2
−1/2
⎤
⎥⎥⎦.
Thus, for any x ∈S, x = α1z1 + α2z2 + α3z3, where αi = x′zi. For instance,
because x′
3z1 = 2, x′
3z2 = 2, x′
3z3 = 2, we have x3 = 2z1 + 2z2 + 2z3.
Now if S is a vector subspace of Rm and x ∈Rm, Theorem 2.17 indicates how
the vector x can be decomposed into the sum of a vector in S and another vector.
Theorem 2.17
Let {z1,. . ., zr} be an orthonormal basis for some vector subspace,
S, of Rm. Then each x ∈Rm can be expressed uniquely as
x = u + v,
where u ∈S and v is a vector that is orthogonal to every vector in S.
Proof.
It follows from Theorem 2.14(c) that we can find vectors zr+1,. . ., zm, so
that the set {z1,. . ., zm} is an orthonormal basis for the m-dimensional Euclidean
space Rm. It also follows from Theorem 2.13 that a unique set of scalars α1,. . ., αm
exists, such that
x =
m

i=1
αizi.

ORTHONORMAL BASES AND PROJECTIONS
55
x2
x1
x
u
v
Figure 2.2
Projection of x onto a one-dimensional subspace of R2
Thus, if we let u = α1z1 + · · · + αrzr and v = αr+1zr+1 + · · · + αmzm, we have,
uniquely, x = u + v, u ∈S, and v will be orthogonal to every vector in S because
of the orthogonality of the vectors z1,. . ., zm.
□
The vector u in Theorem 2.17 is known as the orthogonal projection of x onto
S. When m = 3, the orthogonal projection has a simple geometrical description that
allows for visualization. If, for instance, x is a point in three-dimensional space and
S is a two-dimensional subspace, then the orthogonal projection u of x will be the
point of intersection of the plane S and the line that is perpendicular to S and passes
through x.
Example 2.8
We will look at some examples of projections in R2 and R3. First
consider the space S1 spanned by the vector y = (1, 1)′, which is a line in R2.
Normalizing y, we get z1 = (1/
√
2, 1/
√
2)′, and the set {z1, z2} is an orthonormal
basis for R2, where z2 = (1/
√
2 −1/
√
2)′. We will look at the projection of
x = (1, 2)′ onto S1. Solving the system of equations x = α1z1 + α2z2, we find
that α1 = 3/
√
2 and α2 = −1/
√
2. The orthogonal projection of x onto S1, which
is illustrated in Figure 2.2, is then given by u = α1z1 = (3/2, 3/2)′, whereas
v = α2z2 = (−1/2, 1/2)′. Next consider the space S2 spanned by the vectors
y1 = (3, 1, 1)′ and y2 = (1, 7, 2)′, so that S2 is a plane in R3. It is easily verified
that {z1, z2} is an orthonormal basis for S2, where z1 = (3/
√
11, 1/
√
11, 1/
√
11)′
and
z2 = (−5/
√
198, 13/
√
198, 2/
√
198)′,
and
the
set
{z1, z2, z3}
is
an
orthonormal basis for R3 if we define z3 = (1/
√
18, 1/
√
18, −4/
√
18)′. We
will look at the projection of x = (4, 4, 4)′ onto S2. Solving the system of
equations x = α1z1 + α2z2 + α3z3, we find that α1 = 20/
√
11, α2 = 40/
√
198,
and α3 = −8/
√
18. The orthogonal projection of x onto S2, as depicted
in Figure 2.3, is given by u = α1z1 + α2z2 = (4.44, 4.44, 2.22)′, whereas
v = (−0.44, −0.44, 1.78)′.
The importance of the orthogonal projection u in many applications arises out
of the fact that it is the closest point in S to x. That is, if y is any other point in
S and dI is the Euclidean distance function, then dI(x, u) ≤dI(x, y). This is fairly

56
VECTOR SPACES
x3
x1
x2
u
x
v
Figure 2.3
Projection of x onto a two-dimensional subspace of R3
simple to verify. Since u and y are in S, it follows from the decomposition x = u + v
that the vector u −y is orthogonal to v = x −u and, hence, (x −u)′(u −y) = 0.
Consequently,
{dI(x, y)}2 = (x −y)′(x −y)
= {(x −u) + (u −y)}′{(x −u) + (u −y)}
= (x −u)′(x −u) + (u −y)′(u −y) + 2(x −u)′(u −y)
= (x −u)′(x −u) + (u −y)′(u −y)
= {dI(x, u)}2 + {dI(u, y)}2,
from which dI(x, u) ≤dI(x, y) follows because {dI(u, y)}2 ≥0.
Example 2.9
Simple linear regression relates a response variable y to one explana-
tory variable x through the model
y = β0 + β1x + ϵ ;
that is, if this model is correct, then observed ordered pairs (x, y) should be clustered
about some line in the x, y plane. Suppose that we have N observations, (xi, yi),
i = 1,. . ., N, and we form the N × 1 vector y = (y1,. . ., yN)′ and the N × 2 matrix
X =
⎡
⎢⎢⎢⎣
1
x1
1
x2
...
...
1
xN
⎤
⎥⎥⎥⎦= [1N
x].
The least squares estimator ˆβ of β = (β0, β1)′ minimizes the sum of squared errors
given by
(y −ˆy)′(y −ˆy) = (y −X ˆβ)′(y −X ˆβ).
In Chapter 9, we will see how to find ˆβ using differential methods. Here we will
use the geometrical properties of projections to determine ˆβ. For any choice of ˆβ,

ORTHONORMAL BASES AND PROJECTIONS
57
ˆy = X ˆβ gives a point in the subspace of RN spanned by the columns of X, that is,
the plane spanned by the two vectors 1N and x. Thus, the point ˆy that minimizes
the distance from y will be given by the orthogonal projection of y onto this plane
spanned by 1N and x, which means that y −ˆy must be orthogonal to both 1N and
x. This result leads to the two normal equations
0 = (y −ˆy)′1N = y′1N −ˆβ
′X′1N
=
N

i=1
yi −ˆβ0N −ˆβ1
N

i=1
xi,
0 = (y −ˆy)′x = y′x −ˆβ
′X′x
=
N

i=1
xiyi −ˆβ0
N

i=1
xi −ˆβ1
N

i=1
x2
i,
which when solved simultaneously for ˆβ0 and ˆβ1, yields
ˆβ1 =
N
i=1 xiyi −N x y
N
i=1 x2
i −N x2
,
ˆβ0 = y −ˆβ1x.
If we want to test the hypothesis that β1 = 0, we would consider the reduced model
y = β0 + ϵ,
and least squares estimation here only requires an estimate of β0. In this case, the
vector of fitted values satisfies ˆy = ˆβ01N, so for any choice of ˆβ0, ˆy will be given
by a point on the line passing through the origin and 1N. Thus, if ˆy is to minimize
the sum of squared errors and hence the distance from y, then it must be given by the
orthogonal projection of y onto this line. Consequently, we must have
0 = (y −ˆy)′1N = (y −ˆβ01N)′1N =
N

i=1
yi −ˆβ0N,
or simply
ˆβ0 = y.
The vector v in Theorem 2.17 is called the component of x orthogonal to S. It is
one vector belonging to what is known as the orthogonal complement of S.
Definition 2.10
Let S be a vector subspace of Rm. The orthogonal complement of
S, denoted by S⊥, is the collection of all vectors in Rm that are orthogonal to every
vector in S; that is, S⊥= {x : x ∈Rm and x′y = 0 for all y ∈S}.

58
VECTOR SPACES
Theorem 2.18
If S is a vector subspace of Rm, then its orthogonal complement S⊥
is also a vector subspace of Rm.
Proof.
Suppose that x1 ∈S⊥and x2 ∈S⊥, so that x′
1y = x′
2y = 0 for any y ∈S.
Consequently, for any y ∈S and any scalars α1 and α2, we have
(α1x1 + α2x2)′y = α1x′
1y + α2x′
2y = 0,
and so (α1x1 + α2x2) ∈S⊥, and thus S⊥is a vector space.
□
A consequence of Theorem 2.19 is that if S is a vector subspace of Rm and the
dimension of S is r, then the dimension of S⊥is m −r.
Theorem
2.19
Suppose
{z1,. . ., zm}
is
an
orthonormal
basis
for
Rm
and {z1 . . . , zr} is an orthonormal basis for the vector subspace S. Then
{zr+1,. . ., zm} is an orthonormal basis for S⊥.
Proof.
Let T be the vector space spanned by {zr+1,. . ., zm}. We must show that
this vector space is the same as S⊥. If x ∈T and y ∈S, then scalars α1,. . ., αm exist,
such that y = α1z1 + · · · + αrzr and x = αr+1zr+1 + · · · + αmzm. As a result of
the orthogonality of the zi’s, x′y = 0, so x ∈S⊥and thus T ⊆S⊥. Conversely, sup-
pose now that x ∈S⊥. Because x is also in Rm, scalars α1,. . ., αm exist, such that
x = α1z1 + · · · + αmzm. Now if we let y = α1z1 + · · · + αrzr, then y ∈S, and
because x ∈S⊥, we must have x′y = α2
1 + · · · + α2
r = 0. But this can only happen
if α1 = · · · = αr = 0, in which case x = αr+1zr+1 + · · · + αmzm and so x ∈T.
Thus, we also have S⊥⊆T, and so this establishes that T = S⊥.
□
2.7
PROJECTION MATRICES
The orthogonal projection of an m × 1 vector x onto a vector space S can be conve-
niently expressed in matrix form. Let {z1,. . ., zr} be any orthonormal basis for S,
whereas {z1,. . ., zm} is an orthonormal basis for Rm. Suppose α1,. . ., αm are the
constants satisfying the relationship
x = (α1z1 + · · · + αrzr) + (αr+1zr+1 + · · · + αmzm) = u + v,
where u and v are as previously defined. Write α = (α′
1, α′
2)′ and Z = [Z1
Z2],
where α1 = (α1,. . ., αr)′, α2 = (αr+1,. . ., αm)′, Z1 = (z1,. . ., zr), and Z2 =
(zr+1,. . ., zm). Then the expression for x given above can be written as
x = Zα = Z1α1 + Z2α2;

PROJECTION MATRICES
59
that is, u = Z1α1 and v = Z2α2. As a result of the orthonormality of the zi’s, we
have Z′
1Z1 = Ir and Z′
1Z2 = (0), and so
Z1Z′
1x = Z1Z′
1Zα = Z1Z′
1[Z1
Z2]
	α1
α2

= [Z1
(0)]
	α1
α2

= Z1α1 = u.
Thus, Theorem 2.20 results.
Theorem 2.20
Suppose the columns of the m × r matrix Z1 form an orthonormal
basis for the vector space S, which is a subspace of Rm. If x ∈Rm, the orthogonal
projection of x onto S is given by Z1Z′
1x.
The matrix Z1Z′
1 appearing in Theorem 2.20 is called the projection matrix for
the vector space S and sometimes will be denoted by PS. Similarly, Z2Z′
2 is the pro-
jection matrix for S⊥and ZZ′ = Im is the projection matrix for Rm. Since ZZ′ =
Z1Z′
1 + Z2Z′
2, we have the simple equation Z2Z′
2 = Im −Z1Z′
1 relating the projec-
tion matrices of a vector subspace and its orthogonal complement. Although a vector
space does not have a unique orthonormal basis, the projection matrix formed from
these orthonormal bases is unique.
Theorem 2.21
Suppose the columns of the m × r matrices Z1 and W1 each form
an orthonormal basis for the r-dimensional vector space S. Then Z1Z′
1 = W1W ′
1.
Proof.
Each column of W1 can be written as a linear combination of the columns of
Z1 because the columns of Z1 span S and each column of W1 is in S; that is, an r × r
matrix P exists, such that W1 = Z1P. However, Z′
1Z1 = W ′
1W1 = Ir, because each
matrix has orthonormal columns. Thus,
Ir = W ′
1W1 = P ′Z′
1Z1P = P ′IrP = P ′P,
so that P is an orthogonal matrix. Consequently, P also satisfies PP ′ = Ir, and
W1W ′
1 = Z1PP ′Z′
1 = Z1IrZ′
1 = Z1Z′
1,
so the proof is complete.
□
We will use projection matrices to take another look at the Gram–Schmidt
orthonormalization procedure. The procedure takes an initial linearly independent
set of vectors {x1,. . ., xr}, which is transformed to an orthogonal set {y1,. . ., yr},
which is then transformed to an orthonormal set {z1,. . ., zr}. It is easy to verify
that for i = 1,. . ., r −1, the vector yi+1 can be expressed as
yi+1 =
⎛
⎝Im −
i

j=1
zjz′
j
⎞
⎠xi+1;

60
VECTOR SPACES
that is, yi+1 = (Im −Z(i)Z′
(i))xi+1, where Z(i) = (z1,. . ., zi). Thus, the (i + 1)th
orthogonal vector yi+1 is obtained as the projection of the (i + 1)th original vector
onto the orthogonal complement of the vector space spanned by the first i orthogonal
vectors, y1,. . ., yi.
The Gram–Schmidt orthonormalization process represents one method of obtain-
ing an orthonormal basis for a vector space S from a given basis {x1,. . ., xr}. In
general, if we define the m × r matrix X1 = (x1,. . ., xr), the columns of
Z1 = X1A
(2.9)
will form an orthonormal basis for S if A is any r × r matrix for which
Z′
1Z1 = A′X′
1X1A = Ir.
The matrix A must be nonsingular because we must have rank(X1) = rank(Z1) = r,
so A−1 exists, and X′
1X1 = (A−1)′A−1 or (X′
1X1)−1 = AA′; that is, A is a square
root matrix of (X′
1X1)−1. Consequently, we can obtain an expression for the projec-
tion matrix onto the vector space S, PS, in terms of X1 as
PS = Z1Z′
1 = X1AA′X′
1 = X1(X′
1X1)−1X′
1.
(2.10)
Note that the Gram–Schmidt equations given in (2.8) can be written in matrix form
as Y1 = X1T, where Y1 = (y1,. . ., yr), X1 = (x1,. . ., xr), and T is an r × r upper
triangular matrix with each diagonal element equal to 1. The normalization to produce
Z1 can then be written as Z1 = X1TD−1, where D is the diagonal matrix with the
positive square root of y′
iyi as its ith diagonal element. Consequently, the matrix
A = TD−1is an upper triangular matrix with positive diagonal elements. Thus, the
Gram–Schmidt orthonormalization is the particular case of (2.9) in which the matrix
A has been chosen to be the upper triangular square root matrix of (X′
1X1)−1 having
positive diagonal elements. This is commonly known as the QR factorization of A.
Example 2.10
Using the basis {x1, x2, x3} from Example 2.7, we form the X1
matrix
X1 =
⎡
⎢⎢⎣
1
1
3
1
−2
1
1
1
1
1
−2
−1
⎤
⎥⎥⎦,
and it is easy to verify that
X′
1X1 =
⎡
⎣
4
−2
4
−2
10
4
4
4
12
⎤
⎦,
(X′
1X1)−1 = 1
36
⎡
⎣
26
10
−12
10
8
−6
−12
−6
9
⎤
⎦.

PROJECTION MATRICES
61
Thus, the projection matrix for the vector space S spanned by {x1, x2, x3} is given
by
PS = X1(X′
1X1)−1X′
1 = 1
4
⎡
⎢⎢⎣
3
1
1
−1
1
3
−1
1
1
−1
3
1
−1
1
1
3
⎤
⎥⎥⎦,
which, of course, is the same as Z1Z′
1, where Z1 = (z1, z2, z3) and z1, z2, z3 are
the vectors obtained by the Gram–Schmidt orthonormalization in Example 2.7. Now
if x = (1, 2, −1, 0)′, then the projection of x onto S is X1(X′
1X1)−1X′
1x = x; the
projection of x is equal to x because x = x3 −x1 −x2 ∈S. On the other hand,
if x = (1, −1, 2, 1)′, then the projection of x is given by u = X1(X′
1X1)−1X′
1x =
( 3
4, −3
4, 9
4, 3
4)′. The component of x orthogonal to S or, in other words, the orthogonal
projection of x onto S⊥, is {I4 −X1(X′
1X1)−1X′
1}x = x −X1(X′
1X1)−1X′
1x =
x −u = ( 1
4, −1
4, −1
4, 1
4)′, which gives us the decomposition
x =
⎡
⎢⎢⎣
1
−1
2
1
⎤
⎥⎥⎦= 1
4
⎡
⎢⎢⎣
3
−3
9
3
⎤
⎥⎥⎦+ 1
4
⎡
⎢⎢⎣
1
−1
−1
1
⎤
⎥⎥⎦= u + v
of Theorem 2.17.
Example 2.11
We will generalize some of the ideas of Example 2.9 to the multiple
regression model
y = β0 + β1x1 + · · · + βkxk + ϵ,
relating a response variable y to k explanatory variables, x1,. . ., xk. If we have N
observations, this model can be written as
y = Xβ + ϵ,
where y is N × 1, X is N × (k + 1), β is (k + 1) × 1, and ϵ is N × 1, whereas the
vector of fitted values is given by
ˆy = X ˆβ,
where ˆβ is an estimate of β. Clearly, for any ˆβ, ˆy is a point in the subspace of RN
spanned by the columns of X. To be a least squares estimate of β, ˆβ must be such
that ˆy = X ˆβ yields the point in this subspace closest to the vector y, because this
will have the sum of squared errors,
(y −X ˆβ)′(y −X ˆβ),

62
VECTOR SPACES
minimized. Thus, X ˆβ must be the orthogonal projection of y onto the space spanned
by the columns of X. If X has full column rank, then this space has projection matrix
X(X′X)−1X′, and so the required projection is
X ˆβ = X(X′X)−1X′y.
Premultiplying this equation by (X′X)−1X′, we obtain the least squares estimator
ˆβ = (X′X)−1X′y.
In addition, we find that the sum of squared errors (SSE) for the fitted model ˆy = X ˆβ
can be written as
SSE1 = (y −X ˆβ)′(y −X ˆβ)
= (y −X(X′X)−1X′y)′(y −X(X′X)−1X′y)
= y′(IN −X(X′X)−1X′)2y
= y′(IN −X(X′X)−1X′)y,
and so this sum of squares represents the squared length of the projection of y onto
the orthogonal complement of the column space of X. Suppose now that β and X
are partitioned as β = (β′
1, β′
2)′ and X = (X1, X2), where the number of columns
of X1 is the same as the number of elements in β1, and we wish to decide whether
β2 = 0. If the columns of X1 are orthogonal to the columns of X2, then X′
1X2 = (0)
and
(X′X)−1 =
	
(X′
1X1)−1
(0)
(0)
(X′
2X2)−1

,
and so ˆβ can be partitioned as ˆβ = (ˆβ
′
1, ˆβ
′
2)′, where ˆβ1 = (X′
1X1)−1X′
1y and ˆβ2 =
(X′
2X2)−1X′
2y. Further, the sum of squared errors for the fitted model, ˆy = X ˆβ, can
be decomposed as
(y −X ˆβ)′(y −X ˆβ) = y′(IN −X(X′X)−1X′)y
= y′(IN −X1(X′
1X1)−1X′
1 −X2(X′
2X2)−1X′
2)y.
On the other hand, the least squares estimator of β1 in the reduced model
y = X1β1 + ϵ
is ˆβ1 = (X′
1X1)−1X′
1y, whereas its sum of squared errors is given by
SSE2 = (y −X1ˆβ1)′(y −X1ˆβ1)
= y′(IN −X1(X′
1X1)−1X′
1)y.

PROJECTION MATRICES
63
Thus, the term SSE2 −SSE1 = y′X2(X′
2X2)−1X′
2y gives the reduction in the sum
of squared errors attributable to the inclusion of the term X2β2 in the model y =
Xβ + ϵ = X1β1 + X2β2 + ϵ, and so its relative size will be helpful in deciding
whether β2 = 0. If β2 = 0, then the N observations of y should be randomly clus-
tered about the column space of X1 in RN with no tendency to deviate from this
subspace in one direction more than in any other direction, whereas if β2 ̸= 0, we
would expect larger deviations in directions within the column space of X2 than in
directions orthogonal to the column space of X. Now, because the dimension of the
column space of X is k + 1, SSE1 is the sum of squared deviations in N −k −1
orthogonal directions, whereas SSE2 −SSE1 gives the sum of squared deviations
in k2 orthogonal directions, where k2 is the number of components in β2. Thus,
SSE1/(N −k −1) and (SSE2 −SSE1)/k2 should be of similar magnitudes if β2 =
0, whereas the latter should be larger than the former if β2 ̸= 0. Consequently, a
decision about β2 can be based on the value of the statistic
F = (SSE2 −SSE1)/k2
SSE1/(N −k −1).
(2.11)
Using results that we will develop in Chapter 11, it can be shown that F ∼Fk2,N−k−1
if ϵ ∼NN(0, σ2IN) and β2 = 0.
When X′
1X2 ̸= (0), (SSE2 −SSE1) does not reduce to y′X2(X′
2X2)−1X′
2y
because, in this case, ˆy is not the sum of the projection of y onto the column space
of X1 and the projection of y onto the column space of X2. To properly assess the
effect of the inclusion of the term X2β2 in the model, we must decompose ˆy into
the sum of the projection of y onto the column space of X1 and the projection of y
onto the subspace of the column space of X2 orthogonal to the column space of X1.
This latter subspace is spanned by the columns of
X2∗= (IN −X1(X′
1X1)−1X′
1)X2,
because (IN −X1(X′
1X1)−1X′
1) is the projection matrix of the orthogonal comple-
ment of the column space of X1. Thus, the vector of fitted values ˆy = X ˆβ can be
written as
ˆy = X1(X′
1X1)−1X′
1y + X2∗(X′
2∗X2∗)−1X′
2∗y.
Further, the sum of squared errors is given by
y′(IN −X1(X′
1X1)−1X′
1 −X2∗(X′
2∗X2∗)−1X′
2∗)y,
and the reduction in the sum of squared errors attributable to the inclusion of the term
X2β2 in the model y = Xβ + ϵ is
y′X2∗(X′
2∗X2∗)−1X′
2∗y.
Least squares estimators are not always unique as they have been throughout this
example. For instance, let us return to the least squares estimation of β in the model

64
VECTOR SPACES
y = Xβ + ϵ, where now X does not have full column rank. As before, ˆy = X ˆβ will
be given by the orthogonal projection of y onto the space spanned by the columns
of X, but the necessary projection matrix cannot be expressed as X(X′X)−1X′,
because X′X is singular. If the projection matrix of the column space of X is denoted
by PR(X), then a least squares estimator of β is any vector ˆβ satisfying
X ˆβ = PR(X)y.
Since X does not have full column rank, the columns of X are linearly dependent,
and so we will be able to find a nonnull vector a satisfying Xa = 0. In this case, if
ˆβ is a least squares estimator of β, so also is ˆβ + a because
X(ˆβ + a) = PR(X)y,
and so the least squares estimator is not unique.
We have seen that if the columns of an m × r matrix Z1 form an orthonormal basis
for a vector space S, then the projection matrix of S is given by Z1Z′
1. Clearly this
projection matrix is symmetric and, because Z′
1Z1 = Ir, it is also idempotent; that
is, every projection matrix is symmetric and idempotent. Theorem 2.22 proves the
converse. Every symmetric idempotent matrix is a projection matrix for some vector
space.
Theorem 2.22
Let P be an m × m symmetric idempotent matrix of rank r. Then
an r-dimensional vector space exists that has P as its projection matrix.
Proof.
From Corollary 1.11.1, an m × r matrix F and an r × m matrix G exist,
such that rank(F) = rank(G) = r and P = FG. Since P is idempotent, we have
FGFG = FG,
which implies that
F ′FGFGG′ = F ′FGG′.
(2.12)
Since F and G′ are full column rank, the matrices F ′F and GG′ are nonsingular.
Premultiplying (2.12) by (F ′F)−1 and postmultiplying by (GG′)−1, we obtain
GF = Ir. Using this and the symmetry of P = FG, we find that
F = FGF = (FG)′F = G′F ′F,
which leads to G′ = F(F ′F)−1. Thus, P = FG = F(F ′F)−1F ′. Comparing this
with (2.10), we see that P must be the projection matrix for the vector space spanned
by the columns of F. This completes the proof.
□

LINEAR TRANSFORMATIONS AND SYSTEMS OF LINEAR EQUATIONS
65
Example 2.12
Consider the 3 × 3 matrix
P = 1
6
⎡
⎣
5
−1
2
−1
5
2
2
2
2
⎤
⎦.
Clearly, P is symmetric and is easily verified as idempotent, so P is a projection
matrix. We will find the vector space S associated with this projection matrix. First,
note that the first two columns of P are linearly independent, whereas the third col-
umn is the average of the first two columns. Thus, rank(P) = 2, and so the dimension
of the vector space associated with P is 2. For any x ∈R3, Px yields a vector in S.
In particular, Pe1 and Pe2 are in S. These two vectors form a basis for S because
they are linearly independent and the dimension of S is 2. Consequently, S contains
all vectors of the form (5a −b, 5b −a, 2a + 2b)′.
2.8
LINEAR TRANSFORMATIONS AND SYSTEMS OF LINEAR
EQUATIONS
If S is a vector subspace of Rm, with projection matrix PS, then we have seen that
for any x ∈Rm, u = u(x) = PSx is the orthogonal projection of x onto S; that is,
each x ∈Rm is transformed into a u ∈S. The function u(x) = PSx is an example
of a linear transformation of Rm into S.
Definition 2.11
Let u be a function defined for all x in the vector space T, such
that for any x ∈T, u = u(x) ∈S, where S is also a vector space. Then the trans-
formation defined by u is a linear transformation of T into S if for any two scalars
α1 and α2 and any two vectors x1 ∈T and x2 ∈T,
u(α1x1 + α2x2) = α1u(x1) + α2u(x2).
We will be interested in matrix transformations of the form u = Ax, where x is
in the subspace of Rn denoted by T, u is in the subspace of Rm denoted by S, and
A is an m × n matrix. This defines a transformation of T into S, and the transforma-
tion is linear because for scalars α1 and α2, and n × 1 vectors x1 and x2, it follows
immediately that
A(α1x1 + α2x2) = α1Ax1 + α2Ax2.
(2.13)
In fact, every linear transformation can be expressed as a matrix transformation
(Problem 2.40). For the orthogonal projection described at the beginning of this
section, A = PS, so that n = m, and thus, we have a linear transformation of Rm
into Rm or, to be more specific, a linear transformation of Rm into S. In particular,
for the multiple regression problem discussed in Example 2.11, we saw that for any

66
VECTOR SPACES
N × 1 vector of observations y, the vector of estimated or fitted values was given by
ˆy = X(X′X)−1X′y. Thus, because y ∈RN and ˆy ∈R(X), we have a linear
transformation of RN into R(X).
It should be obvious from (2.13) that if S is actually defined to be the set {u :
u = Ax; x ∈T}, then T being a vector space guarantees that S will also be a vector
space. In addition, if the vectors x1, . . . , xr span T, then the vectors Ax1,. . ., Axr
span S. In particular, if T is Rn, then because e1,. . ., en span Rn, we find that
(A)·1,. . ., (A)·n span S; that is, S is the column space or range of A because it is
spanned by the columns of A.
When the matrix A does not have full column rank, then vectors x will exist, other
than the null vector, which satisfy Ax = 0. The set of all such vectors is called the
nullspace of the transformation Ax or simply the null space of the matrix A.
Theorem 2.23
Let the linear transformation of Rn into S be given by u = Ax,
where x ∈Rn and A is an m × n matrix. Then the null space of A, given by the set
N(A) = {x : Ax = 0, x ∈Rn},
is a vector space.
Proof.
Let x1 and x2 be in N(A) so that Ax1 = Ax2 = 0. Then, for any scalars
α1 and α2, we have
A(α1x1 + α2x2) = α1Ax1 + α2Ax2 = α1(0) + α2(0) = 0,
so that (α1x1 + α2x2) ∈N(A) and, hence, N(A) is a vector space.
□
The null space of a matrix A is related to the concept of orthogonal complements
discussed in Section 2.6. In fact, the null space of the matrix A is the same as the
orthogonal complement of the row space of A. Similarly, the null space of the matrix
A′ is the same as the orthogonal complement of the column space of A. Theorem
2.24 is an immediate consequence of Theorem 2.19.
Theorem 2.24
Let A be an m × n matrix. If the dimension of the row space of A
is r1 and the dimension of the null space of A is r2, then r1 + r2 = n.
Since the rank of the matrix A is equal to the dimension of the row space of A, the
result above can be equivalently expressed as
rank(A) = n −dim{N(A)}.
(2.14)
This connection between the rank of a matrix and the dimension of the null space of
that matrix can be useful to us in determining the rank of a matrix in certain situations.
Example 2.13
To illustrate the utility of (2.14), we will give an alternative
proof of the identity rank(A) = rank(A′A), which was given as Theorem 2.8(c).

LINEAR TRANSFORMATIONS AND SYSTEMS OF LINEAR EQUATIONS
67
Suppose x is in the null space of A so that Ax = 0. Then, clearly, we must have
A′Ax = 0, which implies that x is also in the null space of A′A, so it follows that
dim{N(A)} ≤dim{N(A′A)}, or equivalently,
rank(A) ≥rank(A′A).
(2.15)
On the other hand, if x is in the null space of A′A, then A′Ax = 0. Premultiplying
by x′ yields x′A′Ax = 0, which is satisfied only if Ax = 0. Thus, x is also in the
null space of A so that dim{N(A)} ≥dim{N(A′A)}, or
rank(A) ≤rank(A′A).
(2.16)
Combining (2.15) and (2.16), we get rank(A) = rank(A′A).
When A is an m × m nonsingular matrix and x ∈Rm, then u = Ax defines a
one-to-one transformation of Rm onto Rm. One way of viewing this transforma-
tion is as the movement of each point in Rm to another point in Rm. Alternatively,
we can view the transformation as a change of coordinate axes. For instance, if we
start with the standard coordinate axes, which are given by the columns e1,. . ., em
of the identity matrix Im, then, because for any x ∈Rm, x = x1e1 + · · · + xmem,
the components of x give the coordinates of the point x relative to these standard
coordinate axes. On the other hand, if x1,. . ., xm is another basis for Rm, then
from Theorem 2.13, scalars u1,. . ., um exist, so that with u = (u1,. . ., um)′ and
X = (x1,. . ., xm), we have
x =
m

i=1
uixi = Xu;
that is, u = (u1,. . ., um)′ gives the coordinates of the point x relative to the coordi-
nate axes x1,. . ., xm. The transformation from the standard coordinate system to the
one with axes x1,. . ., xm is then given by the matrix transformation u = Ax, where
A = X−1. Note that the squared Euclidean distance of u from the origin,
u′u = (Ax)′(Ax) = x′A′Ax,
will be the same as the squared Euclidean distance of x from the origin for every
choice of x if and only if A, and hence, also X, is an orthogonal matrix. In this
case, x1,. . ., xm forms an orthonormal basis for Rm, and so the transformation
has replaced the standard coordinate axes by a new set of orthogonal axes given by
x1,. . ., xm.
Example 2.14
Orthogonal transformations are of two types according to whether
the determinant of A is +1 or −1. If |A| = 1, then the new axes can be obtained by
a rotation of the standard axes. For example, for a fixed angle θ, let
A =
⎡
⎣
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎤
⎦,

68
VECTOR SPACES
so that |A| = cos2 θ + sin2 θ = 1. The transformation given by u = Ax trans-
forms the standard axes e1, e2, e3 to the new axes x1 = (cos θ, −sin θ, 0)′,
x2 = (sin θ, cos θ, 0)′, x3 = e3, and this simply represents a rotation of e1 and e2
through an angle of θ. If instead we have
A =
⎡
⎣
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
−1
⎤
⎦,
then |A| = (cos2 θ + sin2 θ) · (−1) = −1. Now the transformation given by
u = Ax transforms the standard axes to the new axes x1 = ( cos θ, −sin θ, 0)′,
x2 = ( sin θ, cos θ, 0)′, and x3 = −e3; these axes are obtained by a rotation of e1
and e2 through an angle of θ followed by a reflection of e3 about the x1, x2 plane.
Although orthogonal transformations are common, situations occur in which non-
singular, nonorthogonal transformations are useful.
Example 2.15
Suppose we have several three-dimensional vectors x1,. . ., xr that
are observations from distributions, each having the same positive definite covariance
matrix Ω. If we are interested in how these vectors differ from one another, then a plot
of the points in R3 may be helpful. However, as discussed in Example 2.2, if Ω is not
the identity matrix, then the Euclidean distance is not appropriate, and so it becomes
difficult to compare and interpret the observed differences among the r points. This
difficulty can be resolved by an appropriate transformation. We will see in Chapter 3
and Chapter 4 that because Ω is positive definite, a nonsingular matrix T exists, which
satisfies Ω = TT ′. If we let ui = T −1xi, then the Mahalanobis distance, which was
defined in Example 2.2, between xi and xj is
dΩ(xi, xj) = {(xi −xj)′Ω−1(xi −xj)}1/2
= {(xi −xj)′T −1′T −1(xi −xj)}1/2
= {(T −1xi −T −1xj)′(T −1xi −T −1xj)}1/2
= {(ui −uj)′(ui −uj)}1/2 = dI(ui, uj),
whereas the variance of ui is given by
var(ui) = var(T −1xi) = T −1{var(xi)}T −1′
= T −1ΩT −1′ = T −1TT ′T −1′ = I3.
That is, the transformation ui = T −1xi produces vectors for which the Euclidean
distance function is an appropriate measure of distance between points.
In Example 2.16 and Example 2.17, we discuss some transformations that are
sometimes useful in regression analysis.

LINEAR TRANSFORMATIONS AND SYSTEMS OF LINEAR EQUATIONS
69
Example 2.16
A simple transformation that is useful in some situations is one that
centers a collection of numbers at the origin. For instance, if x is the mean of the
components of x = (x1,. . ., xN)′, then the average of each component of
v = (IN −N −11N1′
N)x =
⎡
⎢⎢⎢⎣
x1 −x
x2 −x
...
xN −x
⎤
⎥⎥⎥⎦
is 0. This transformation is sometimes used in a regression analysis to center each of
the explanatory variables. Thus, the multiple regression model
y = Xβ + ϵ = [1N
X1]
	
β0
β1

+ ϵ
= β01N + X1β1 + ϵ
can be re-expressed as
y = β01N + {N −11N1′
N + (IN −N −11N1′
N)}X1β1 + ϵ
= γ01N + V1β1 + ϵ = V γ + ϵ,
where
V = [1N
V1] = [1N
(IN −N −11N1′
N)X1]
and
γ = (γ0, β′
1)′ =
(β0 + N −11′
NX1β1, β′
1)′. Because the columns of V1 are orthogonal to 1N, the
least squares estimator of γ simplifies to
ˆγ =

ˆγ0
ˆβ1

= (V ′V )−1V ′y
=

N −1
0′
0
(V ′
1V1)−1
  yi
V ′
1y

=

y
(V ′
1V1)−1V ′
1y

.
Thus, ˆγ0 = y. The estimator, ˆβ1, can be conveniently expressed in terms of the sample
covariance matrix computed from the N (k + 1) × 1 vectors that form the rows of
the matrix [y
X1]. If we denote this covariance matrix by S and partition it as
S =

s11
s′
21
s21
S22

,
then (N −1)−1V ′
1V1 = S22 and, because V ′
11N = 0,
(N −1)−1V ′
1y = (N −1)−1V ′
1(y −y1N) = s21.

70
VECTOR SPACES
Consequently, ˆβ1 = S−1
22 s21. Yet another adjustment to the original regression model
involves the standardization of the explanatory variables. In this case, the model
becomes
y = δ01N + Z1δ1 + ϵ = Zδ + ϵ,
where δ = (δ0, δ′
1)′, Z = [1N
Z1], δ0 = γ0, Z1 = V1D−1/2
S22 , and δ1 = D1/2
S22 β1. The
least squares estimators are ˆδ0 = y and ˆδ1 = s1/2
11 R−1
22 r21, where we have partitioned
the correlation matrix R, computed from the N (k + 1) × 1 vectors that form the
rows of the matrix [y
X1], in a fashion similar to that of S.
The centering of explanatory variables, discussed previously, involves a linear
transformation on the columns of X1. In some situations, it is advantageous to employ
a linear transformation on the rows of X1, V1, or Z1. For instance, suppose that T is a
k × k nonsingular matrix, and we define W1 = Z1T, α0 = δ0, and α1 = T −1δ1, so
that the model
y = δ01N + Z1δ1 + ϵ = Zδ + ϵ
can be written as
y = α01N + W1α1 + ϵ = Wα + ϵ,
where W = [1N
W1]. This second model uses a different set of explanatory vari-
ables than the first; its ith explanatory variable is a linear combination of the explana-
tory variables of the first model with the coefficients given by the ith column of T.
However, the two models yield equivalent results in terms of the fitted values. To see
this, let
T∗=
	
1
0′
0
T

,
so that W = ZT∗, and note that the vector of fitted values from the second model,
ˆy = W ˆα = W(W ′W)−1W ′y = ZT∗(T ′
∗Z′ZT∗)−1T ′
∗Z′y
= ZT∗T −1
∗(Z′Z)−1T −1′
∗
T ′
∗Z′y = Z(Z′Z)−1Z′y,
is the same as that obtained from the first model.
Example 2.17
Consider the multiple regression model
y = Xβ + ϵ,
where now var(ϵ) ̸= σ2IN. In this case, our previous estimator, ˆβ = (X′X)−1X′y,
is still the least squares estimator of β, but it does not possess certain optimality
properties, one of which is illustrated later in Example 3.14, that hold when var(ϵ) =
σ2IN. In this example, we will consider the situation in which the ϵi’s are still uncor-
related, but their variances are not all the same. Thus, var(ϵ) = Ω = σ2C, where

LINEAR TRANSFORMATIONS AND SYSTEMS OF LINEAR EQUATIONS
71
C = diag(c2
1,. . ., c2
N) and the ci’s are known constants. This special regression prob-
lem is sometimes referred to as weighted least squares regression. The weighted least
squares estimator of β is obtained by making a simple transformation so that ordi-
nary least squares regression applies to the transformed model. Define the matrix
C−1/2 = diag(c−1
1 ,. . ., c−1
N ) and transform the original regression problem by pre-
multiplying the model equation by C−1/2; the new model equation is
C−1/2y = C−1/2Xβ + C−1/2ϵ
or, equivalently,
y∗= X∗β + ϵ∗,
where y∗= C−1/2y, X∗= C−1/2X, and ϵ∗= C−1/2ϵ. The covariance matrix of
ϵ∗is
var(ϵ∗) = var(C−1/2ϵ) = C−1/2var(ϵ)C−1/2
= C−1/2{σ2C}C−1/2 = σ2IN.
Thus, for the transformed model, ordinary least squares regression applies, and so the
least squares estimator of β can be expressed as
ˆβ = (X′
∗X∗)−1X′
∗y∗.
Rewriting this equation in the original model terms X and y, we get
ˆβ = (X′C−1/2C−1/2X)−1X′C−1/2C−1/2y
= (X′C−1X)−1X′C−1y.
A common application related to linear transformations is one in which the matrix
A and vector u consist of known constants, whereas x is a vector of variables, and we
wish to determine all x for which Ax = u; that is, we want to find the simultaneous
solutions x1,. . ., xn to the system of m equations
a11x1 + · · · + a1nxn = u1
...
am1x1 + · · · + amnxn = um.
For instance, in Example 2.11, we saw that the least squares estimator of the
parameter vector β in the multiple regression model satisfies the equation,
X ˆβ = X(X′X)−1X′y; that is, here A = X, u = X(X′X)−1X′y, and x = ˆβ. In
general, if u = 0, then this system of equations is referred to as a homogeneous
system, and the set of all solutions to Ax = u, in this case, is simply given by the

72
VECTOR SPACES
null space of A. Consequently, if A has full column rank, then x = 0 is the only
solution, whereas infinitely many solutions exist if A has less than full column rank.
A nonhomogeneous system of linear equations is one that has u ̸= 0. Although a
homogeneous system always has at least one solution, x = 0, a nonhomogeneous
system might not have any solutions. A system of linear equations that has no
solutions is called an inconsistent system of equations, whereas a system with
solutions is referred to as a consistent system. If u ̸= 0 and Ax = u holds for
some x, then u must be a linear combination of the columns of A; that is, the
nonhomogeneous system of equations Ax = u is consistent if and only if u is in the
column space of A.
The mathematics involved in solving systems of linear equations is most conve-
niently handled using matrix methods. For example, consider one of the simplest
nonhomogeneous systems of linear equations in which the matrix A is square and
nonsingular. In this case, because A−1 exists, we find that the system Ax = u has a
solution that is unique and is given by x = A−1u. Similarly, when the matrix A is sin-
gular or not even square, matrix methods can be used to determine whether the system
is consistent, and if so, the solutions can be given as matrix expressions. The results
regarding the solution of a general system of linear equations will be developed in
Chapter 6.
The focus in this section has been on linear transformations of vectors, but the
concept is easily extended to matrices. Suppose T is a linear space of p × q matri-
ces, and let X and Z be m × p and q × n matrices, respectively. Then for B ∈T,
Y = XBZ is a linear transformation of the linear space T ⊂Rp×q onto the linear
space S = {Y : Y = XBZ, B ∈T} ⊂Rm×n. When T is Rp×q, this transformation
yields the range R(X, Z); that is, R(X, Z) = {Y : Y = XBZ, B ∈Rp×q}, so that
this linear space consists of all matrices Y whose columns are in R(X) and whose
rows are in R(Z′). The null space of the linear transformation Y = XBZ is given
by N(X, Z) = {B : XBZ = (0), B ∈Rp×q} ⊂Rp×q.
Our final example of this section concerns a statistical analysis that utilizes a
matrix transformation of the form Y = XBZ.
Example 2.18
In this example, we look at a statistical model commonly known
as a growth curve model. One response variable is recorded for each of n individ-
uals at m different points in time, t1,. . ., tm. The expected value of the response
for any individual at time ti is modeled as a polynomial in time of degree p −1 for
some choice of p; that is, this expected value has the form b0 + b1ti + · · · + bp−1tp−1
i
,
where b0,. . ., bp−1 are unknown parameters. The response matrix Y is m × n with
(i, j)th component given by the response for the jth individual at time ti, and the
growth curve model has the matrix form Y = XBZ + E. The (i, j)th element of the
m × p matrix X is tj−1
i
and the m × n matrix E is a matrix of random errors. Numer-
ous designs can be achieved through appropriate choices of the p × q parameter
matrix B and the q × n design matrix Z. For instance, if all n individuals come from
a common group, then we would take q = 1, B = (b0, b1,. . ., bp−1)′, and Z = 1′
n.
If, on the other hand, there are g groups and ni of the individuals are from the ith

THE INTERSECTION AND SUM OF VECTOR SPACES
73
group, we would have q = g,
B =
⎡
⎢⎣
b01
· · ·
b0g
...
...
bp−1,1
· · ·
bp−1,g
⎤
⎥⎦,
Z =
⎡
⎢⎢⎢⎣
1′
n1
0′
· · ·
0′
0′
1′
n2
· · ·
0′
...
...
...
0′
0′
· · ·
1′
ng
⎤
⎥⎥⎥⎦.
The method of least squares can be used to find an estimator for B. We will show
that this estimator is given by ˆB = (X′X)−1X′Y Z′(ZZ′)−1 when rank(X) = p and
rank(Z) = q. Note that the sum of squared errors corresponding to a particular B
matrix is tr{(Y −XBZ)(Y −XBZ)′} and
tr{(Y −XBZ)(Y −XBZ)′}
= tr[{(Y −X ˆBZ) + (X ˆBZ −XBZ)}{(Y −X ˆBZ) + (X ˆBZ −XBZ)}′]
= tr{(Y −X ˆBZ)(Y −X ˆBZ)′} + tr{(X ˆBZ −XBZ)(X ˆBZ −XBZ)′}
+2tr{(Y −X ˆBZ)(X ˆBZ −XBZ)′}
≥tr{(Y −X ˆBZ)(Y −X ˆBZ)′} + 2tr{(Y −X ˆBZ)(X ˆBZ −XBZ)′},
where the inequality follows from the fact that the trace of a nonnegative definite
matrix is nonnegative. But since X′X ˆBZZ′ = X′Y Z′, we have
tr{(Y −X ˆBZ)(X ˆBZ −XBZ)′} = tr{X′(Y −X ˆBZ)Z′( ˆB −B)′}
= tr{(X′Y Z′ −X′X ˆBZZ′)( ˆB −B)′}
= tr{(X′Y Z′ −X′Y Z′)( ˆB −B)′} = 0,
so it follows that
tr{(Y −XBZ)(Y −XBZ)′} ≥tr{(Y −X ˆBZ)(Y −X ˆBZ)′},
which confirms that ˆB is the least squares estimator of B.
2.9
THE INTERSECTION AND SUM OF VECTOR SPACES
In this section, we discuss some common ways of forming a vector subspace from
two or more given subspaces. The first of these uses a familiar operation from set
theory.
Definition 2.12
Let S1 and S2 be vector subspaces of Rm. The intersection of S1
and S2, denoted by S1 ∩S2, is the vector subspace given as
S1 ∩S2 = {x ∈Rm : x ∈S1 and x ∈S2}.

74
VECTOR SPACES
Note that this definition says that the set S1 ∩S2 is a vector subspace if S1 and S2
are vector subspaces, which follows from the fact that if x1 and x2 are in S1 ∩S2,
then x1 ∈S1, x2 ∈S1 and x1 ∈S2, x2 ∈S2. Thus, because S1 and S2 are vector
spaces, for any scalars α1 and α2, α1x1 + α2x2 will be in S1 and S2 and, hence, in
S1 ∩S2. Definition 2.12 can be generalized in an obvious fashion to the intersection,
S1 ∩· · · ∩Sr, of the r vector spaces S1,. . ., Sr.
A second set operation, which combines the elements of S1 and S2, is the union;
that is, the union of S1 and S2 is given by
S1 ∪S2 = {x ∈Rm : x ∈S1 or x ∈S2}.
If S1 and S2 are vector subspaces, then S1 ∪S2 will also be a vector subspace only if
S1 ⊆S2 or S2 ⊆S1. It can be easily shown that the following combination of S1 and
S2 yields the vector space containing S1 ∪S2 with the smallest possible dimension.
Definition 2.13
If S1 and S2 are vector subspaces of Rm, then the sum of S1 and
S2, denoted by S1 + S2, is the vector space given by
S1 + S2 = {x1 + x2 : x1 ∈S1, x2 ∈S2}.
Again our definition can be generalized to S1 + · · · + Sr, the sum of the r vector
spaces S1,. . ., Sr. The proof of Theorem 2.25 has been left as an exercise.
Theorem 2.25
If S1 and S2 are vector subspaces of Rm, then
dim(S1 + S2) = dim(S1) + dim(S2) −dim(S1 ∩S2).
Example 2.19
Let S1 and S2 be subspaces of R5 having bases {x1, x2, x3} and
{y1, y2}, respectively, where
x1 = (1, 0, 0, 1, 0)′,
x2 = (0, 0, 1, 0, 1)′,
x3 = (0, 1, 0, 0, 0)′,
y1 = (1, 0, 0, 1, 1)′,
y2 = (0, 1, 1, 0, 0)′.
We wish to find bases for S1 + S2 and S1 ∩S2. Now, clearly, S1 + S2 is spanned by
the set {x1, x2, x3, y1, y2}. Note that y2 = x1 + x2 + x3 −y1, and it can be easily
verified that no constants α1, α2, α3, α4 exist, except α1 = α2 = α3 = α4 = 0,
which satisfy α1x1 + α2x2 + α3x3 + α4y1 = 0. Thus, {x1, x2, x3, y1} is a
basis for S1 + S2, and so dim(S1 + S2) = 4. From Theorem 2.25, we know that

THE INTERSECTION AND SUM OF VECTOR SPACES
75
dim(S1 ∩S2) = 3 + 2 −4 = 1, and so any basis for S1 ∩S2 consists of one vector.
The dependency between the x’s and the y’s will indicate an appropriate vector, so
we seek solutions for α1, α2, α3, β1, and β2, which satisfy
α1x1 + α2x2 + α3x3 = β1y1 + β2y2.
As a result, we find that a basis for S1 ∩S2 is given by the vector y1 + y2 =
(1, 1, 1, 1, 1)′ because x1 + x2 + x3 = y1 + y2.
When S1 and S2 are such that S1 ∩S2 = {0}, then the vector space obtained as
the sum of S1 and S2 is sometimes referred to as the direct sum of S1 and S2 and
written S1 ⊕S2. In this special case, each x ∈S1 ⊕S2 has a unique representation
as x = x1 + x2, where x1 ∈S1 and x2 ∈S2. A further special case is one in which
S1 and S2 are orthogonal vector spaces; that is, for any x1 ∈S1 and x2 ∈S2, we have
x′
1x2 = 0. In this case, the unique representation x = x1 + x2 for x ∈S1 ⊕S2 will
have the vector x1 given by the orthogonal projection of x onto S1, whereas x2 will be
given by the orthogonal projection of x onto S2. For instance, for any vector subspace
S of Rm, Rm = S ⊕S⊥, and for any x ∈Rm,
x = PSx + PS⊥x.
In general, if a vector space S is the sum of the r vector spaces S1,. . ., Sr, and
Si ∩Sj = {0} for all i ̸= j, then S is said to be the direct sum of S1,. . ., Sr and is
written as S = S1 ⊕· · · ⊕Sr.
Example 2.20
Consider the vector spaces S1,. . ., Sm, where Si is spanned by {ei}
and, as usual, ei is the ith column of the m × m identity matrix. Consider a sec-
ond sequence of vector spaces, T1,. . ., Tm, where Ti is spanned by {ei, ei+1} if
i ≤m −1, whereas Tm is spanned by {e1, em}. Then it follows that Rm = S1 +
· · · + Sm, as well as Rm = T1 + · · · + Tm. However, although Rm = S1 ⊕· · · ⊕
Sm, it does not follow that Rm = T1 ⊕· · · ⊕Tm, because it is not true that Ti ∩Tj =
{0} for all i ̸= j. Thus, any x = (x1,. . ., xm)′ in Rm can be expressed uniquely as
a sum comprised of a vector from each of the spaces S1,. . ., Sm; namely,
x = x1e1 + · · · + xmem,
where ei ∈Si. On the other hand, the decomposition corresponding to T1,. . ., Tm is
not unique. For instance, we can get the same sum above by choosing e1 ∈T1, e2 ∈
T2,. . ., em ∈Tm. However, we also have x = y1 + · · · + ym and yi ∈Ti, where
yi = 1
2(xiei + xi+1ei+1) when i ≤m −1 and ym = 1
2(x1e1 + xmem). In addi-
tion, the sum of the orthogonal projections of x onto the spaces S1,. . ., Sm yields x,
whereas the sum of the orthogonal projections of x onto the spaces T1,. . ., Tm yields
2x. Consider as a third sequence of vector spaces, U1,. . ., ,Um, where Ui has the
basis {γi} and γi = e1 + · · · + ei. Clearly, Ui ∩Uj = {0} if i ̸= j, so Rm = U1 ⊕

76
VECTOR SPACES
b
a
x1
x
x2
Figure 2.4
Projection of x onto a along b
· · · ⊕Um and each x ∈Rm has a unique decomposition x = x1 + · · · + xm with
xi ∈Ui. However, in this case, because the Ui’s are not orthogonal vector spaces,
this decomposition of x is not given by the sum of the orthogonal projections of x
onto the spaces U1,. . ., Um.
2.10
OBLIQUE PROJECTIONS
The projections that we have considered so far have been orthogonal projections.
For any x ∈Rm and any subspace of Rm, S1, x can be uniquely expressed as x =
x1 + x2, where x1 ∈S1 and x2 ∈S2 = S⊥
1 ; x1 is the orthogonal projection onto S1
and x2 is the orthogonal projection onto S2, and in this text these are the projections
we are referring to when we simply use the term projection.
In this section, we generalize the idea of a projection to oblique projections,
that is, to situations in which the subspaces S1 and S2 are not orthogonal to one
another.
Definition 2.14
Let S1 and S2 be subspaces of Rm such that S1 ∩S2 = {0} and
S1 ⊕S2 = Rm. Suppose
x = x1 + x2,
where x ∈Rm, x1 ∈S1, and x2 ∈S2. Then x1 is called the projection of x onto S1
along S2, and x2 is called the projection of x onto S2 along S1.
An oblique projection in R2 is depicted in Figure 2.4. Here S1 is the line spanned
by a = (2, 1)′ and S2 is the line spanned by b = (1, 2)′. The projection of x = (1, 1)′
onto S1 along S2 is x1 = (2/3, 1/3)′, while the projection onto S2 along S1 is x2 =
(1/3, 2/3)′.
When S1 and S2 are not orthogonal subspaces, we can find linear transformations
of them such that these transformed subspaces are orthogonal. Suppose the columns
of the m × m1 matrix X1 form a basis for S1 and the columns of the m × m2 matrix
X2 form a basis for S2. Then since S1 ∩S2 = {0} and S1 ⊕S2 = Rm, it follows that

OBLIQUE PROJECTIONS
77
m1 + m2 = m and X = (X1, X2) is nonsingular. For i = 1, 2, the subspace Ti =
{y : y = X−1x, x ∈Si} has as a basis the columns of Yi = X−1Xi. Since Y =
(Y1, Y2) = (X−1X1, X−1X2) = X−1X = Im, we have
Y1 =

Im1
(0)

,
Y2 =

(0)
Im2

,
(2.17)
and hence Y ′
1Y2 = (0); that is, T1 and T2 are orthogonal subspaces and, in particu-
lar, T2 is the orthogonal complement of T1. It follows from Theorem 2.17 that any
y ∈Rm can be expressed uniquely as y = y1 + y2, where yi ∈Ti. Equivalently,
x = Xy = x1 + x2, where xi = Xyi ∈Si. This immediately leads to the follow-
ing generalization of Theorem 2.17.
Theorem 2.26
Let S1 and S2 be subspaces of Rm such that S1 ∩S2 = {0} and
S1 ⊕S2 = Rm. Then each x ∈Rm can be expressed uniquely as
x = x1 + x2,
where xi ∈Si.
As with orthogonal projections, oblique projections can be computed through the
use of a projection matrix. We will denote by PS1|S2 the m × m matrix for which
PS1|S2x is the projection of x onto S1 along S2. Note that if y1 is the projection of
y = X−1x onto T1 along T2, that is, the orthogonal projection onto T1, then Xy1
is the projection of x onto S1 along S2. We have seen in Section 2.7 that y1 =
PT1y = Y1(Y ′
1Y1)−1Y ′
1y, and so the required projection is XY1(Y ′
1Y1)−1Y ′
1X−1x.
Using (2.17), the projection matrix simplifies to the form given in the following
theorem.
Theorem 2.27
Let S1 and S2 be subspaces of Rm such that S1 ∩S2 = {0} and
S1 ⊕S2 = Rm. Suppose X = (X1, X2), where the columns of the m × mi matrix
Xi form a basis for Si. Then the projection matrix for the projection onto S1 along
S2 is given by
PS1|S2 = X

Im1
(0)
(0)
(0)

X−1.
When S1 and S2 are orthogonal subspaces, X′
1X2 = (0) and
X−1 =

(X′
1X1)−1X′
1
(X′
2X2)−1X′
2

.
In this case, PS1|S2 as given in Theorem 2.27 reduces to PS1 = X1(X′
1X1)−1X′
1 as
given in Section 2.7.

78
VECTOR SPACES
Example 2.21
For i = 1, 2, let Si be the space spanned by the columns of Xi, where
X1 =
⎡
⎣
1
−1
1
1
2
1
⎤
⎦,
X2 =
⎡
⎣
1
1
1
⎤
⎦.
Since X = (X1, X2) is nonsingular, S1 ∩S2 = {0} and S1 ⊕S2 = R3. The projec-
tion matrix for the projection onto S1 along S2 is
PS1|S2 = X
⎡
⎣
1
0
0
0
1
0
0
0
0
⎤
⎦X−1
=
⎡
⎣
1
−1
1
1
1
1
2
1
1
⎤
⎦
⎡
⎣
1
0
0
0
1
0
0
0
0
⎤
⎦
⎡
⎣
0
−1
1
−.5
.5
0
.5
1.5
−1
⎤
⎦
=
⎡
⎣
.5
−1.5
1
−.5
−.5
1
−.5
−1.5
2
⎤
⎦.
The projection matrix for the projection onto S2 along S1 is then
PS2|S1 = I3 −PS1|S2 =
⎡
⎣
.5
1.5
−1
.5
1.5
−1
.5
1.5
−1
⎤
⎦.
If
x = (1, 2, 3)′,
then
we
find
that
x = x1 + x2,
where
x1 = PS1|S2x =
(.5, 1.5, 2.5)′ ∈S1 and x2 = PS2|S1x = (.5, .5, .5)′ ∈S2.
The projection matrix for an oblique projection, like that of an orthogonal projec-
tion, is idempotent since clearly P 2
S1|S2 = PS1|S2. We saw in Section 2.7 that a matrix
P is a projection matrix for an orthogonal projection if and only if P is idempotent
and symmetric, implying then that P ′
S1|S2 ̸= PS1|S2 when S1 and S2 are not orthog-
onal subspaces. This is also evident from the form of PS1|S2 given in Theorem 2.27
since if P ′
S1|S2 = PS1|S2, then X−1P ′
S1|S2X = X−1PS1|S2X, or
(X′X)−1

Im1
(0)
(0)
(0)

(X′X) =

Im1
(0)
(0)
(0)

,
which holds only if X′
1X2 = (0).
We next consider another generalization of the projections discussed in
Section 2.6. Instead of using the usual inner product, ⟨x, y⟩= x′y, we use the inner
product ⟨x, y⟩A = x′Ay, where A is an m × m positive definite matrix. We refer
to this as the A inner product.

OBLIQUE PROJECTIONS
79
Definition 2.15
Let S1 be a subspace of Rm and suppose A is an m × m positive
definite matrix. If
x = x1 + x2,
where x ∈Rm, x1 ∈S1, and x′
1Ax2 = 0, then x1 is the orthogonal projection onto
S1 relative to the A inner product.
Let U1 = {z : z = Bx, x ∈S1}, where B
is any m × m matrix satis-
fying B′B = A. Then z = z1 + z2, where z = Bx, z1 = Bx1 ∈U1, and
z′
1z2 = x′
1B′Bx2 = 0, so that z2 = Bx2 ∈U ⊥
1 . Thus, the uniqueness of z1 and z2
guarantees the uniqueness of x1 and x2 in Definition 2.15. If the columns of X1
form a basis for S1, then the columns of BX1 form a basis for U1 and the projection
matrix for the orthogonal projection onto U1 is BX1(X′
1AX1)−1X′
1B′. From
this, we find that X1(X′
1AX1)−1X′
1A is the projection matrix for the orthogonal
projection onto S1 relative to the A inner poduct.
It is not difficult to see that there is a direct connection between the decompo-
sitions given in Definitions 2.14 and 2.15. In Definition 2.14, if X = (X1, X2) and
the columns of Xi form a basis for Si, then x′
1X−1′X−1x2 = 0. Thus, x1 is the
orthogonal projection onto S1 and x2 is the orthogonal projection onto S2, both
relative to the (XX′)−1 inner product. Similarly, in Definition 2.15, x1 is the projec-
tion onto S1 along S2, where S2 is the vector space which has the projection matrix
PS2|S1 = Im −X1(X′
1AX1)−1X′
1A; S2 is the orthogonal complement of S1, relative
to the A inner product, since X′
1APS2|S1 = (0).
Suppose x ∈Rm and we want to find a point in the subspace S1 ∈Rm so that the
distance between x and that point, relative to the A inner product, is minimized. Using
the decomposition in Definition 2.15 for x, note that for any y ∈S1, x1 −y ∈S1,
so that (x −x1)′A(x1 −y) = x′
2A(x1 −y) = 0. Thus,
(x −y)′A(x −y) = {(x −x1) + (x1 −y)}′A{(x −x1) + (x1 −y)}
= (x −x1)′A(x −x1) + (x1 −y)′A(x1 −y)
+2(x −x1)′A(x1 −y)
= (x −x1)′A(x −x1) + (x1 −y)′A(x1 −y)
≥(x −x1)′A(x −x1).
That is, x1 is the point in S1 that minimizes the distance, relative to the A inner
product, from x.
Example 2.22
In Example 2.17, we obtained the weighted least squares estimator
of β in the multiple regression model
y = Xβ + ϵ,
where var(ϵ) = σ2diag(c2
1,. . ., c2
N), c2
1,. . ., c2
N are known constants, and X has
full column rank. We now consider a more general regression problem, sometimes

80
VECTOR SPACES
referred to as generalized least squares regression, in which var(ϵ) = σ2C, where
C is a known N × N positive definite matrix. Thus, the random errors not only
may have different variances but also may be correlated, and weighted least squares
regression is simply a special case of generalized least squares regression. When
C = IN as it was in Example 2.11, we find ˆβ by minimizing the sum of squared
errors
(y −ˆy)′(y −ˆy) = (y −X ˆβ)′(y −X ˆβ),
where ˆy = X ˆβ. When C ̸= IN, we need to use the Mahalanobis distance function to
measure the distance between y and X ˆβ. That is, to find the generalized least squares
estimator ˆβ, we need to minimize
(y −X ˆβ)′C−1(y −X ˆβ).
Since X ˆβ is a point in the space spanned by the columns of X, and the closest point
in this space to y, relative to the C−1 inner product, is the orthogonal projection of y
onto this space, relative to the C−1 inner product, we must have
X ˆβ = X(X′C−1X)−1X′C−1y.
Thus, premultiplying by (X′X)−1X′, we get
ˆβ = (X′C−1X)−1X′C−1y.
2.11
CONVEX SETS
A special type of subset of a vector space is known as a convex set. Such a set has
the property that it contains any point on the line segment connecting any other two
points in the set. Definition 2.16 follows.
Definition 2.16
A set S ⊆Rm is said to be a convex set if for any x1 ∈S and
x2 ∈S,
cx1 + (1 −c)x2 ∈S,
where c is any scalar satisfying 0 < c < 1.
The condition for a convex set is similar to the condition for a vector space; for S
to be a vector space, we must have for any x1 ∈S and x2 ∈S, α1x1 + α2x2 ∈S for
all α1 and α2, whereas for S to be a convex set, this need only hold when α1 and α2
are nonnegative and α1 + α2 = 1. Thus, any vector space is a convex set. However,
many familiar sets that are not vector spaces are, in fact, convex sets. For instance,
intervals in R, rectangles in R2, and ellipsoidal regions in Rm are all examples of
convex sets. The linear combination of x1 and x2, α1x1 + α2x2, is called a convex

CONVEX SETS
81
combination when α1 + α2 = 1 and αi ≥0 for each i. More generally, α1x1 + · · · +
αrxr is called a convex combination of the vectors x1,. . ., xr when α1 + · · · + αr
= 1 and αi ≥0 for each i. Thus, by a simple induction argument, we see that a set S
is convex if and only if it is closed under all convex combinations of vectors in S.
Theorem 2.28 indicates that the intersection of convex sets and the sum of convex
sets are convex. The proof will be left as an exercise.
Theorem 2.28
Suppose that S1 and S2 are convex sets, where Si ⊆Rm for each i.
Then the set
(a) S1 ∩S2 is convex,
(b) S1 + S2 = {x1 + x2 : x1 ∈S1, x2 ∈S2} is convex.
For any set S, the set C(S) defined as the intersection of all convex sets contain-
ing S is called the convex hull of S. Consequently, because of a generalization of
Theorem 2.28(a), C(S) is the smallest convex set containing S.
A point a is a limit or accumulation point of a set S ⊆Rm if for any δ > 0, the set
Sδ = {x : x ∈Rm, (x −a)′(x −a) < δ} contains at least one point of S distinct
from a. A closed set is one that contains all of its limit points. If S is a set, then S will
denote its closure; that is, if S0 is the set of all limit points of S, then S = S ∪S0. In
Theorem 2.29, we see that the convexity of S guarantees the convexity of S.
Theorem 2.29
If S ⊆Rm is a convex set, then its closure S is also a convex set.
Proof.
It is easily verified that the set Bn = {x : x ∈Rm, x′x ≤n−1} is a convex
set, where n is a positive integer. Consequently, it follows from Theorem 2.28(b) that
Cn = S + Bn is also convex. It also follows from a generalization of the result given
in Theorem 2.28(a) that the set
A =
∞

n=1
Cn
is convex. The result now follows by observing that A = S.
□
One of the most important results regarding convex sets is a theorem known as the
separating hyperplane theorem. A hyperplane in Rm is a set of the form T = {x :
x ∈Rm, a′x = c}, where a is an m × 1 vector and c is a scalar. Thus, if m = 2,
T represents a line in R2 and if m = 3, T is a plane in R3. We will see that the
separating hyperplane theorem states that two convex sets S1 and S2 are separated by
a hyperplane if their intersection is empty; that is, a hyperplane exists that partitions
Rm into two parts so that S1 is contained in one part, whereas S2 is contained in the
other. Before proving this result, we will need to obtain some preliminary results. Our
first result is a special case of the separating hyperplane theorem in which one of the
sets contains the single point 0.

82
VECTOR SPACES
Theorem 2.30
Let S be a nonempty closed convex subset of Rm and suppose that
0 /∈S. Then an m × 1 vector a exists, such that a′x > 0 for all x ∈S.
Proof.
Let a be a point in S which satisfies
a′a = inf
x∈S x′x,
where inf denotes the infimum or greatest lower bound. It is a consequence of the
fact that S is closed and nonempty that such an a ∈S exists. In addition, a ̸= 0
because 0 /∈S. Now let c be an arbitrary scalar and x any vector in S except for a,
and consider the vector cx + (1 −c)a. The squared length of this vector as a function
of c is given by
f(c) = {cx + (1 −c)a}′{cx + (1 −c)a}
= {c(x −a) + a}′{c(x −a) + a}
= c2(x −a)′(x −a) + 2ca′(x −a) + a′a.
Since the second derivative of this quadratic function f(c) is positive, we find that it
has a unique minimum at the point
c∗= −
a′(x −a)
(x −a)′(x −a).
Note that because S is convex, xc = cx + (1 −c)a ∈S when 0 ≤c ≤1, and so
we must have x′
cxc = f(c) ≥f(0) = a′a for 0 ≤c ≤1 because of the way a was
defined. However, because of the quadratic structure of f(c), this implies that f(c) >
f(0) for all c > 0. In other words, c∗≤0, which leads to
a′(x −a) ≥0
or
a′x ≥a′a > 0,
and this completes the proof.
□
A point x∗is an interior point of S if a δ > 0 exists, such that the set Sδ = {x : x ∈
Rm, (x −x∗)′(x −x∗) < δ} is a subset of S. On the other hand, x∗is a boundary
point of S if for each δ > 0, the set Sδ contains at least one point in S and at least
one point not in S. Theorem 2.31 shows that the sets S and S have the same interior
points if S is convex.
Theorem 2.31
Suppose that S is a convex subset of Rm, whereas T is an open
subset of Rm. If T ⊂S, then T ⊂S.

CONVEX SETS
83
Proof.
Let x∗be an arbitrary point in T, and define the sets
S∗= {x : x = y −x∗, y ∈S},
T∗= {x : x = y −x∗, y ∈T}.
It follows from the conditions of Theorem 2.31 that S∗is convex, T∗is open, and
T∗⊂S∗. The proof will be complete if we can show that 0 ∈S∗because this will
imply that x∗∈S. Since 0 ∈T∗and T∗is an open set, we can find an ϵ > 0 such that
each of the vectors, ϵe1,. . ., ϵem, −ϵ1m are in T∗. Since these vectors also must be
in S∗, we can find sequences, xi1, xi2, . . . , for i = 1, 2,. . ., m + 1, such that each
xij ∈S∗and xij →ϵei for i = 1,. . ., m, and xij →−ϵ1m for i = m + 1, as j →
∞. Define the m × m matrix Xj = (x1j,. . ., xmj) so that Xj →ϵIm, as j →∞.
It follows that an integer N1 exists, such that Xj is nonsingular for all j > N1. For
j > N1, define
yj = X−1
j xm+1,j
(2.18)
so that
yj →(ϵIm)−1(−ϵ1m) = −1m.
Thus, some integer N2 ≥N1 exists, such that for all j > N2, all of the components
of yj are negative. But from (2.18), we have
xm+1,j −Xjyj = [Xj
xm+1,j]
	−yj
1

= 0.
This same equation holds if we replace the vector (−y′
j, 1)′ by the unit vector
(y′
jyj + 1)−1/2(−y′
j, 1)′. Thus, 0 is a convex combination of the columns of
[Xj
xm+1,j], each of which is in S∗, so because S∗is convex, 0 ∈S∗.
□
Theorem 2.32 is sometimes called the supporting hyperplane theorem. It states
that for any boundary point of a convex set S, a hyperplane passing through that
point exists, such that none of the points of S are on one side of the hyperplane.
Theorem 2.32
Suppose that S is a convex subset of Rm and that x∗either is not in
S or is a boundary point of S if it is in S. Then an m × 1 vector b ̸= 0 exists, such
that b′x ≥b′x∗for all x ∈S.
Proof.
It follows from the previous theorem that x∗also is not in S or must be a
boundary point of S if it is in S. Consequently, there exists a sequence of vectors,
x1, x2,. . . with each xi /∈S, such that xi →x∗as i →∞. Corresponding to each
xi, define the set Si = {y : y = x −xi, x ∈S}, and note that 0 /∈Si because xi /∈
S. Thus, because Si is closed and convex by Theorem 2.29, it follows from Theorem
2.30 that an m × 1 vector ai exists, such that a′
iy > 0 for all y ∈Si or, equiva-
lently, a′
i(x −xi) > 0 for all x ∈S. Alternatively, we can write this as b′
i(x −xi)
> 0, where bi = (a′
iai)−1/2ai. Now because b′
ibi = 1, the sequence b1, b2,. . . is a

84
VECTOR SPACES
bounded sequence, and so it has a convergent subsequence; that is, positive integers
i1 < i2 < · · · and some m × 1 unit vector b exist, such that bij →b as j →∞. Con-
sequently, b′
ij(x −xij) →b′(x −x∗) as j →∞, and we must have b′(x −x∗) ≥0
for all x ∈S because b′
ij(x −xij) > 0 for all x ∈S. This completes the proof. □
We are now ready to prove the separating hyperplane theorem.
Theorem 2.33
Let S1 and S2 be convex subsets of Rm with S1 ∩S2 = ∅. Then an
m × 1 vector b ̸= 0 exists, such that b′x1 ≥b′x2 for all x1 ∈S1 and all x2 ∈S2.
Proof.
Clearly, the set S2∗= {x : −x ∈S2} is convex because S2 is convex. Thus,
from Theorem 2.28, we know that the set
S = S1 + S2∗= {x : x = x1 −x2, x1 ∈S1, x2 ∈S2}
is also convex. In addition, 0 /∈S because S1 ∩S2 = ∅. Consequently, using
Theorem 2.32, we find that an m × 1 vector b ̸= 0 exists for which b′x ≥0 for all
x ∈S. However, this result implies that b′(x1 −x2) ≥0 for all x1 ∈S1 and all
x2 ∈S2, as is required.
□
Suppose that f(x) is a nonnegative function that is symmetric about x = 0 and
has only one maximum, occurring at x = 0; in other words, f(x) = f(−x) for all x
and f(x) ≤f(cx) if 0 ≤c ≤1. Clearly, the integral of f(x) over an interval of fixed
length will be maximized when the interval is centered at 0. This can be expressed as
 a
−a
f(x + cy) dx ≥
 a
−a
f(x + y) dx,
for any y, a > 0, and 0 ≤c ≤1. This result has some important applications regard-
ing probabilities of random variables. The following result, which is a generalization
to a function f(x) of the m × 1 vector x replaces the interval in R1 by a symmetric
convex set in Rm. This generalization is due to Anderson (1955). For some sim-
ple applications of the result to probabilities of random vectors, see Problem 2.68.
For additional extensions and applications of this result, see Anderson (1996) and
Perlman (1990).
Theorem 2.34
Let S be a convex subset of Rm, symmetric about 0, so that if x ∈
S, −x ∈S also. Let f(x) ≥0 be a function for which f(x) = f(−x), Sα = {x :
f(x) ≥α} is convex for any positive α, and

Sf(x) dx < ∞. Then

S
f(x + cy) dx ≥

S
f(x + y) dx,
for 0 ≤c ≤1 and y ∈Rm.

PROBLEMS
85
A more comprehensive discussion of convex sets can be found in Berkovitz (2002),
Kelly and Weiss (1979), Lay (1982), and Rockafellar (1970), whereas some applica-
tions of the separating hyperplane theorem to statistical decision theory can be found
in Ferguson (1967).
PROBLEMS
2.1 Determine whether each of the following sets of vectors is a vector space:
(a) {(a, b, a + b, 1)′ : −∞< a < ∞, −∞< b < ∞}.
(b) {(a, b, c, a + b −2c)′ : −∞< a < ∞, −∞< b < ∞, −∞< c < ∞}.
(c) {(a, b, c, 1−a −b −c)′: −∞< a < ∞, −∞< b < ∞, −∞< c < ∞}.
2.2 Consider the vector space
S = {(a, a + b, a + b, −b)′ : −∞< a < ∞, −∞< b < ∞}.
Determine which of the following sets of vectors are spanning sets of S:
(a) {(1, 0, 0, 1)′, (1, 2, 2, −1)′}.
(b) {(1, 1, 0, 0)′, (0, 0, 1, −1)′}.
(c) {(2, 1, 1, 1)′, (3, 1, 1, 2)′, (3, 2, 2, 1)′}.
(d) {(1, 0, 0, 0)′, (0, 1, 1, 0)′, (0, 0, 0, 1)′}.
2.3 Is the vector x = (1, 1, 1, 1)′ in the vector space S given in Problem 2.2? Is the
vector y = (4, 1, 1, 3)′ in S?
2.4 Let {x1,. . ., xr} be a set of vectors in a vector space S, and let W be the vector
subspace consisting of all possible linear combinations of these vectors. Prove
that W is the smallest subspace of S that contains the vectors x1,. . ., xr; that
is, show that if V is another vector subspace containing x1,. . ., xr, then W is
a subspace of V .
2.5 Suppose that x is a random vector having a distribution with mean vector μ and
covariance matrix Ω given by
μ =
	1
1

,
Ω =
	 1
−0.5
−0.5
1

.
Let x1 = (2, 2)′ and x2 = (2, 0)′ be two observations from this distribution.
Use the Mahalanobis distance function to determine which of these two obser-
vations is closer to the mean.

86
VECTOR SPACES
2.6 Use the Cauchy–Schwarz inequality to prove the triangle inequality; that is, for
m × 1 vectors x and y, show that
{(x + y)′(x + y)}1/2 ≤(x′x)1/2 + (y′y)1/2.
2.7 Using the law of cosines, find
(a) the angle between x = (1, 2, 1, 2)′ and y = (3, 0, 1, 1)′,
(b) x′y if x′x = 3, y′y = 2, and the angle between x and y is π/6.
2.8 Show that the functions ||x||p and ||x||∞defined in Section 2.2 are, in fact, vector
norms.
2.9 If a vector norm is derived from an inner product, that is, ||x|| = ⟨x, x⟩1/2, show
that the identity
||x + y||2 + ||x −y||2 = 2||x||2 + 2||y||2,
known as the parallelogram identity, holds for all x and y.
2.10 Prove Theorem 2.3.
2.11 Which of the following sets of vectors are linearly dependent?
(a) {(1, −1, 2)′, (3, 1, 1)′}.
(b) {(4, −1, 2)′, (3, 2, 3)′, (2, 5, 4)′}.
(c) {(1, 2, 3)′, (2, 3, 1)′, (−1, 1, 1)′}.
(d) {(1, −1, −1)′, (2, 4, 3)′, (3, 3, 5)′, (7, 0, −1)′}.
2.12 Show that the set of vectors {(1, 2, 2, 2)′, (1, 2, 1, 2)′, (1, 1, 1, 1)′} is a linearly
independent set.
2.13 Consider the set of vectors
{(2, 1, 4, 3)′, (3, 0, 5, 2)′, (0, 3, 2, 5)′, (4, 2, 8, 6)′}.
(a) Show that this set of vectors is linearly dependent.
(b) From this set of four vectors find a subset of two vectors that is a linearly
independent set.
2.14 Prove that the set of m × 1 vectors {x1,. . ., xn} is a linearly independent set
if and only if {x1, x1 + x2,. . ., n
i=1 xi} is a linearly independent set.
2.15 Which of the following sets of vectors are bases for R4?
(a) {(0, 1, 0, 1)′, (1, 1, 0, 0)′, (0, 0, 1, 1)′}.
(b) {(2, 2, 2, 1)′, (2, 1, 1, 1)′, (3, 2, 1, 1)′, (1, 1, 1, 1)′}.
(c) {(2, 0, 1, 1)′, (3, 1, 2, 2)′, (2, 1, 1, 2)′, (2, 1, 2, 1)′}.
2.16 Let x1 = (2, −3, 2)′ and x2 = (4, 1, 1)′. Find a vector x3 so that the set
{x1, x2, x3} forms a basis for R3.

PROBLEMS
87
2.17 Suppose that the vector space S is spanned by the vectors x1 = (1, −2, 1)′,
x2 = (2, 1, 1)′, and x3 = (8, −1, 5)′.
(a) Show that the dimension of S is two and find a basis, {z1, z2}, for S.
(b) Show that the vector x = (1, 3, 0)′ is in S by finding the scalars α1 and α2,
such that x = α1z1 + α2z2.
(c) For the x given in part (b), find two different sets of scalars α1, α2, α3, such
that x = α1x1 + α2x2 + α3x3.
2.18 Show that if {x1,. . ., xr} is a basis for a vector space S, then every set con-
taining more than r vectors from S must be linearly dependent. Note that this
result proves that the number of vectors in a basis for S is uniquely defined.
2.19 Prove the results of Theorem 2.9.
2.20 Prove that if a set of orthogonal vectors does not contain the null vector, then it
is a linearly independent set.
2.21 Suppose that x1 and x2 are linearly independent and define y1 = ax1 + bx2
and y2 = cx1 + dx2. Show that y1 and y2 are linearly independent if and only
if ad ̸= bc.
2.22 Find a basis for the vector space given in Problem 2.2. What is the dimension
of this vector space? Find a second different basis for this same vector space,
where none of the vectors in this second basis is a scalar multiple of a vector in
the first basis.
2.23 Show that the set of vectors {γ1,. . ., γm}, given in Example 2.4, is a basis for
Rm.
2.24 Let A be an m × n matrix and B be an n × p matrix. Show that
(a) R(AB) ⊆R(A).
(b) R(AB) = R(A) if rank(AB) = rank(A).
2.25 Suppose A and B are m × n matrices. Show that an n × n matrix C exists,
which satisfies AC = B if and only if R(B) ⊆R(A).
2.26 Let A be an m × n matrix and B be an m × p matrix. Prove that
rank([A
B]) ≤rank(A) + rank(B).
2.27 Prove the results of Theorem 2.14.
2.28 Suppose x is an m × 1 nonnull vector and y is an n × 1 nonnull vector. What
is the rank of the matrix xy′?
2.29 Let A, B, and C be p × n, m × q, and m × n matrices, respectively. Prove that
rank
	C
B
A
(0)


= rank(A) + rank(B)
if an m × p matrix F and a q × n matrix G exist, such that C = FA + BG.
2.30 Let A be an m × n matrix and B be an n × p matrix with rank(B) = n. Show
that rank(A) = rank(AB).

88
VECTOR SPACES
2.31 Show by example that if A and B are m × m matrices, then rank(AB) =
rank(BA) does not necessarily hold.
2.32 Refer to Example 2.7 and Example 2.10. Find the matrix A satisfying
Z1 = X1A, where Z1 = (z1, z2, z3) and X1 = (x1, x2, x3). Show that
AA′ = (X′
1X1)−1.
2.33 Let S be the vector space spanned by the vectors x1 = (1, 2, 1, 2)′,
x2 = (2, 3, 1, 2)′, x3 = (3, 4, −1, 0)′, and x4 = (3, 4, 0, 1)′.
(a) Find a basis for S.
(b) Use the Gram–Schmidt procedure on the basis found in (a) to determine an
orthonormal basis for S.
(c) Find the orthogonal projection of x = (1, 0, 0, 1)′ onto S.
(d) Find the component of x orthogonal to S.
2.34 Using (2.10), determine the projection matrix for the vector space S given in
Problem 2.33. Use the projection matrix to compute the orthogonal projection
of x = (1, 0, 0, 1)′ onto S.
2.35 Let S be the vector space spanned by the vectors x1 = (1, 2, 3)′ and x2 =
(1, 1, −1)′. Find the point in S that is closest to the point x = (1, 1, 1)′.
2.36 Let {z1,. . ., zr} be an orthonormal basis for a vector space S. Show that if
x ∈S, then
x′x = (x′z1)2 + · · · + (x′zr)2.
2.37 Suppose S is a vector subspace of R4 having the projection matrix
PS = 1
10
⎡
⎢⎢⎣
6
−2
−2
−4
−2
9
−1
−2
−2
−1
9
−2
−4
−2
−2
6
⎤
⎥⎥⎦.
(a) What is the dimension of S?
(b) Find a basis for S.
2.38 Let P1 and P2 be the projection matrices for the orthogonal projections onto
S1 ∈Rm and S2 ∈Rm, respectively. Show that
(a) P1 + P2 is a projection matrix if and only if P1P2 = P2P1 = (0),
(b) P1 −P2 is a projection matrix if and only if P1P2 = P2P1 = P2.
2.39 Consider the vector space S = {u : u = Ax, x ∈R4}, where A is the 4 × 4
matrix given by
A =
⎡
⎢⎢⎣
1
2
0
1
1
1
2
2
1
0
4
3
1
3
−2
0
⎤
⎥⎥⎦.

PROBLEMS
89
(a) Determine the dimension of S and find a basis.
(b) Determine the dimension of the null space N(A) and find a basis for it.
(c) Is the vector (3, 5, 2, 4)′ in S?
(d) Is the vector (1, 1, 1, 1)′ in N(A)?
2.40 Let x ∈Rn and suppose that u(x) defines a linear transformation of Rn into
Rm. Using the standard basis {e1,. . ., en} for Rn and the m × 1 vectors
u(e1),. . ., u(en), prove that an m × n matrix A exists, for which
u(x) = Ax,
for every x ∈Rn.
2.41 Let T be a vector subspace of Rn, and suppose that S is the subspace of Rm
given by
S = {u(x) : x ∈T},
where the transformation defined by u is linear. Show that an m × n matrix A
exists, which satisfies
u(x) = Ax,
for every x ∈T.
2.42 Let T be the vector space spanned by the two vectors x1 = (1, 1, 0)′ and
x2 = (0, 1, 1)′. Let S be the vector space defined as S = {u(x) : x ∈T},
where the function u defines a linear transformation satisfying u(x1) =
(2, 3, 1)′ and u(x2) = (2, 5, 3)′. Find a matrix A, such that u(x) = Ax, for all
x ∈T.
2.43 Consider the linear transformation defined by
u(x) =
⎡
⎢⎢⎢⎣
x1 −x
x2 −x
...
xm −x
⎤
⎥⎥⎥⎦,
for all x ∈Rm, where x = (1/m)  xi. Find the matrix A for which
u(x) = Ax, and then determine the dimension of the range and null spaces
of A.
2.44 In an introductory statistics course, students must take three 100-point exams
followed by a 150-point final exam. We will identify the scores on these exams
with the variables x1, x2, x3, and y. We want to be able to estimate the value of
y once x1, x2, and x3 are known. A class of 32 students produced the following
set of exam scores.

90
VECTOR SPACES
Student
x1
x2
x3
y
Student
x1
x2
x3
y
1
87
89
92
111
17
72
76
96
116
2
72
85
77
99
18
73
70
52
78
3
67
79
54
82
19
73
61
86
101
4
79
71
68
136
20
73
83
76
82
5
60
67
53
73
21
97
99
97
141
6
83
84
92
107
22
84
92
86
112
7
82
88
76
106
23
82
68
73
62
8
87
68
91
128
24
61
59
77
56
9
88
66
65
95
25
78
73
81
137
10
62
68
63
108
26
84
73
68
118
11
100
100
100
142
27
57
47
71
108
12
87
82
80
89
28
87
95
84
121
13
72
94
76
109
29
62
29
66
71
14
86
92
98
140
30
77
82
81
123
15
85
82
62
117
31
52
66
71
102
16
62
50
71
102
32
95
99
96
130
(a) Find the least squares estimator for β = (β0, β1, β2, β3)′ in the multiple
regression model
y = β0 + β1x1 + β2x2 + β3x3 + ϵ.
(b) Find the least squares estimator for β1 = (β0, β1, β2)′ in the model
y = β0 + β1x1 + β2x2 + ϵ.
(c) Compute the reduction in the sum of squared errors attributable to the inclu-
sion of the variable x3 in the model given in (a).
2.45 Suppose that we have independent samples of a response y corresponding to k
different treatments with a sample size of ni responses from the ith treatment.
If the jth observation from the ith treatment is denoted yij, then the model
yij = μi + ϵij
is known as the one-way classification model. Here μi represents the expected
value of a response from treatment i, whereas the ϵij’s are independent and
identically distributed as N(0, σ2).
(a) If we let β = (μ1,. . ., μk)′, write this model in matrix form by defining y,
X, and ϵ so that y = Xβ + ϵ.
(b) Find the least squares estimator of β, and show that the sum of squared
errors for the corresponding fitted model is given by
SSE1 =
k

i=1
ni

j=1
(yij −yi)2,

PROBLEMS
91
where
yi =
ni

j=1
yij/ni.
(c) If μ1 = · · · = μk = μ, then the reduced model
yij = μ + ϵij
holds for all i and j. Find the least squares estimator of μ and the sum of
squared errors SSE2 for the fitted reduced model. Show that SSE2 −SSE1,
referred to as the sum of squares for treatment and denoted SST, can be
expressed as
SST =
k

i=1
ni(yi −y)2,
where
y =
k

i=1
niyi/n,
n =
k

i=1
ni.
(d) Show that the F statistic given in (2.11) takes the form
F = SST/(k −1)
SSE1/(n −k).
2.46 Suppose that we have the model y = Xβ + ϵ and wish to find the estimator ˆβ,
which minimizes
(y −X ˆβ)′(y −X ˆβ),
subject to the restriction that ˆβ satisfies Aˆβ = 0, where X has full column rank
and A has full row rank.
(a) Show that S = {y : y = X ˆβ, Aˆβ = 0} is a vector space.
(b) Let C be any matrix whose columns form a basis for the null space of A;
that is, C satisfies the identity, C(C′C)−1C′ = I −A′(AA′)−1A. Using the
geometrical properties of least squares estimators, show that the restricted
least squares estimator ˆβ is given by
ˆβ = C(C′X′XC)−1C′X′y.
2.47 Let S1 and S2 be vector subspaces of Rm. Show that S1 + S2 also must be a
vector subspace of Rm.
2.48 Let S1 be the vector space spanned by {x1, x2, x3}, where x1 = (1, 1, 1, 1)′,
x2 = (1, 2, 2, 2)′, and x3 = (1, 0, −2, −2)′. Let S2 be the vector space
spanned by {y1, y2, y3}, where y1 = (1, 3, 5, 5)′, y2 = (1, 2, 3, 6)′, and
y3 = (0, 1, 4, 7)′. Find bases for S1 + S2 and S1 ∩S2.

92
VECTOR SPACES
2.49 Let S1 and S2 be vector subspaces of Rm. Show that S1 + S2 is the vector space
of smallest dimension containing S1 ∪S2. In other words, show that if T is a
vector space for which S1 ∪S2 ⊆T, then S1 + S2 ⊆T.
2.50 Prove Theorem 2.25.
2.51 Let S1 and S2 be vector subspaces of Rm. Suppose that {x1,. . ., xr} spans
S1 and {y1,. . ., yh} spans S2. Show that {x1,. . ., xr, y1,. . ., yh} spans the
vector space S1 + S2.
2.52 Let S1 be the vector space spanned by the vectors
x1 =
⎡
⎢⎢⎣
3
1
3
1
⎤
⎥⎥⎦,
x2 =
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦,
x3 =
⎡
⎢⎢⎣
2
1
2
1
⎤
⎥⎥⎦,
whereas the vector space S2 is spanned by the vectors
y1 =
⎡
⎢⎢⎣
3
0
5
−1
⎤
⎥⎥⎦,
y2 =
⎡
⎢⎢⎣
1
2
3
1
⎤
⎥⎥⎦,
y3 =
⎡
⎢⎢⎣
1
−4
−1
−3
⎤
⎥⎥⎦.
Find the following:
(a) Bases for S1 and S2.
(b) The dimension of S1 + S2.
(c) A basis for S1 + S2.
(d) The dimension of S1 ∩S2.
(e) A basis for S1 ∩S2.
2.53 Let S1 and S2 be vector subspaces of Rm with dim(S1) = r1 and dim(S2) =
r2.
(a) Find expressions in terms of m, r1, and r2 for the smallest and largest pos-
sible values of dim(S1 + S2).
(b) Find the smallest and largest possible values of dim(S1 ∩S2).
2.54 Let S1 and S2 be vector subspaces of Rm.
(a) Show that (S1 + S2)⊥= S⊥
1 ∩S⊥
2 .
(b) Show that (S1 ∩S2)⊥= S⊥
1 + S⊥
2 .
2.55 Let T be the vector space spanned by the vectors {(1, 1, 1)′, (2, 1, 2)′}. Find
a vector space S1, such that R3 = T ⊕S1. Find another vector space S2, such
that R3 = T ⊕S2 and S1 ∩S2 = {0}.
2.56 Let S1 be the vector space spanned by {(1, 1, −2, 0)′, (2, 0, 1, −3)′}, whereas
S2 is spanned by {(1, 1, 1, −3)′, (1, 1, 1, 1)′}. Show that R4 = S1 + S2. Is this
a direct sum? That is, can we write R4 = S1 ⊕S2? Are S1 and S2 orthogonal
vector spaces?

PROBLEMS
93
2.57 Let S1 and S2 be vector subspaces of Rm, and let T = S1 + S2. Show that this
sum is a direct sum, that is, T = S1 ⊕S2 if and only if
dim(T) = dim(S1) + dim(S2).
2.58 For i = 1, 2, let Si be the space spanned by the columns of Xi, where
X1 =
⎡
⎣
1
0
−1
1
−1
−2
⎤
⎦,
X2 =
⎡
⎣
1
−1
0
⎤
⎦.
(a) Find the projection matrix for the projection onto S1 along S2.
(b) Find the projection of x = (2, 2, 1)′ onto S1 along S2 and the projection of
x onto S2 along S1.
2.59 Consider the projection matrix
PS1|S2 =
⎡
⎣
1
−1
1
0
−2
3
0
−2
3
⎤
⎦.
(a) Find a basis for the vector space S1.
(b) Find a basis for the vector space S2.
2.60 For i = 1, 2, let Si and Ti be subspaces of Rm such that Si ⊕Ti = Rm and
Si ∩Ti = {0}. Show that S1 = S2 if and only if PS1|T1PS2|T2 = PS2|T2 and
PS2|T2PS1|T1 = PS1|T1.
2.61 For i = 1, 2, suppose Si and Ti are subspaces of Rm satisfying S1 ⊕S2 = T1 ⊕
T2 = Rm and S1 ∩S2 = T1 ∩T2 = {0}.
(a) Show that PS1|S2 + PT1|T2 is a projection matrix if and only if PS1|S2PT1|T2
= PT1|T2PS1|S2 = (0).
(b) When the condition in (a) holds, show that PS1|S2 + PT1|T2 is the projection
matrix for the projection onto S1 ⊕T1 along S2 ∩T2.
2.62 Determine which of the following sets are convex and which are not convex:
(a) S1 = {(x1, x2)′ : x2
1 + x2
2 ≤1}.
(b) S2 = {(x1, x2)′ : x2
1 + x2
2 = 1}.
(c) S3 = {(x1, x2)′ : 0 ≤x1 ≤x2 ≤1}.
2.63 Prove Theorem 2.28.
2.64 Show that if S1 and S2 are convex subsets of Rm, then S1 ∪S2 need not be
convex.
2.65 Show that for any positive scalar n, the set Bn = {x : x ∈Rm, x′x ≤n−1} is
convex.
2.66 For any set S ⊆Rm, show that its convex hull C(S) consists of all convex
combinations of the vectors in S.

94
VECTOR SPACES
2.67 Suppose that S is a nonempty subset of Rm. Show that every vector in the
convex hull of S can be expressed as a convex combination of m + 1 or fewer
vectors in S.
2.68 Let x be an m × 1 random vector with density function f(x) such that f(x) =
f(−x) and the set {x : f(x) ≥α} is convex for all positive α. Suppose that S
is a convex subset of Rm, symmetric about 0.
(a) Show that P(x + cy ∈S) ≥P(x + y ∈S) for any constant vector y and
0 ≤c ≤1.
(b) Show that the inequality in (a) also holds if y is an m × 1 random vector
distributed independently of x.
(c) Show that if x ∼Nm(0, Ω), its density function satisfies the conditions of
this exercise.
(d) Show that if x and y are independently distributed with x ∼Nm(0, Ω1)
and y ∼Nm(0, Ω2), such that Ω1 −Ω2 is nonnegative definite, then P(x ∈
S) ≤P(y ∈S).

3
EIGENVALUES AND EIGENVECTORS
3.1
INTRODUCTION
Eigenvalues and eigenvectors are special implicitly defined functions of the elements
of a square matrix. In many applications involving the analysis of a square matrix,
the key information from the analysis is provided by some or all of these eigenvalues
and eigenvectors, which is a consequence of some of the properties of eigenvalues
and eigevectors that we will develop in this chapter. However, before we get to these
properties, we must first understand how eigenvalues and eigenvectors are defined
and how they are calculated.
3.2
EIGENVALUES, EIGENVECTORS, AND EIGENSPACES
If A is an m × m matrix, then any scalar λ satisfying the equation
Ax = λx,
(3.1)
for some m × 1 vector x ̸= 0, is called an eigenvalue of A. The vector x is
called an eigenvector of A corresponding to the eigenvalue λ, and (3.1) is called
the eigenvalue-eigenvector equation of A. Eigenvalues and eigenvectors are also
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

96
EIGENVALUES AND EIGENVECTORS
sometimes referred to as latent roots and vectors or characteristic roots and vectors.
Equation (3.1) can be equivalently expressed as
(A −λIm)x = 0.
(3.2)
Note that if |A −λIm| ̸= 0, then (A −λIm)−1 would exist, and so premultiplication
of (3.2) by this inverse would lead to a contradiction of the already stated assumption
that x ̸= 0. Thus, any eigenvalue λ must satisfy the determinantal equation
|A −λIm| = 0,
which is known as the characteristic equation of A. Applying the definition of a
determinant, we readily observe that the characteristic equation is an mth degree poly-
nomial in λ; that is, scalars α1, . . . , αm−1 exist, such that the characteristic equation
above can be expressed equivalently as
(−λ)m + αm−1(−λ)m−1 + · · · + α1(−λ) + α0 = 0.
Since an mth degree polynomial has m roots, it follows that an m × m matrix has
m eigenvalues; that is, m scalars λ1, . . . , λm exist that satisfy the characteristic
equation. When all of the eigenvalues of A are real, we will sometimes find it nota-
tionally convenient to identify the ith largest eigenvalue of the matrix A as λi(A).
In other words, in this case, the ordered eigenvalues of A may be written as λ1(A)
≥· · · ≥λm(A).
The characteristic equation can be used to obtain the eigenvalues of the matrix A.
These can be then used in the eigenvalue-eigenvector equation to obtain correspond-
ing eigenvectors.
Example 3.1
We will find the eigenvalues and eigenvectors of the 3 × 3 matrix A
given by
A =
⎡
⎣
5
−3
3
4
−2
3
4
−4
5
⎤
⎦.
The characteristic equation of A is
|A −λI3| =

5 −λ
−3
3
4
−2 −λ
3
4
−4
5 −λ

= −(5 −λ)2(2 + λ) −3(4)2 −4(3)2
+ 3(4)(2 + λ) + 3(4)(5 −λ) + 3(4)(5 −λ)
= −λ3 + 8λ2 −17λ + 10
= −(λ −5)(λ −2)(λ −1) = 0,

EIGENVALUES, EIGENVECTORS, AND EIGENSPACES
97
so the three eigenvalues of A are 1, 2, and 5. To find an eigenvector of A corresponding
to the eigenvalue λ = 5, we must solve the equation Ax = 5x for x, which yields
the system of equations
5x1 −3x2 + 3x3 = 5x1,
4x1 −2x2 + 3x3 = 5x2,
4x1 −4x2 + 5x3 = 5x3.
The first and third equations imply that x2 = x3 and x1 = x2, which, when used in
the second equation, yields the identity x2 = x2. Thus, x2 is arbitrary, and so any
x having x1 = x2 = x3, such as the vector (1, 1, 1)′, is an eigenvector of A associ-
ated with the eigenvalue 5. In a similar fashion, by solving the equation Ax = λx,
when λ = 2 and λ = 1, we find that (1, 1, 0)′ is an eigenvector corresponding to the
eigenvalue 2, and (0, 1, 1)′ is an eigenvector corresponding to the eigenvalue 1.
Note that if a nonnull vector x satisfies (3.1) for a given value of λ, then so will
αx for any nonzero scalar α. Thus, eigenvectors are not uniquely defined unless
we impose some scale constraint; for instance, we might only consider eigenvec-
tors, x, which satisfy x′x = 1. In this case, for the previous example, we would
obtain the three normalized eigenvectors, (1/
√
3, 1/
√
3, 1/
√
3)′, (1/
√
2, 1/
√
2, 0)′,
and (0, 1/
√
2, 1/
√
2)′ corresponding to the eigenvalues 5, 2, and 1, respectively.
These normalized eigenvectors are unique except for sign, because each of these
eigenvectors, when multiplied by −1, yields another normalized eigenvector.
Example 3.2 illustrates the fact that a real matrix may have complex eigenvalues
and eigenvectors.
Example 3.2
The matrix
A =

1
1
−2
−1

has the characteristic equation
|A −λI2| =

1 −λ
1
−2
−1 −λ

= −(1 −λ)(1 + λ) + 2
= λ2 + 1 = 0,
so that the eigenvalues of A are i = √−1 and −i. To find an eigenvector correspond-
ing to the root i, write x = (x1, x2)′ = (y1 + iz1, y2 + iz2)′ and solve for y1, y2, z1, z2
using the equation Ax = ix. As a result, we find that for any real scalar α ̸= 0,
x = (α + iα, −2α)′ is an eigenvector corresponding to the eigenvalue i. In a sim-
ilar manner, it can be shown that an eigenvector associated with the eigenvalue −i
has the form x = (α −iα, −2α)′.

98
EIGENVALUES AND EIGENVECTORS
The m eigenvalues of a matrix A need not all be different because the characteris-
tic equation may have repeated roots. An eigenvalue that occurs as a single solution
to the characteristic equation will be called a simple or distinct eigenvalue. Other-
wise, an eigenvalue will be called a multiple eigenvalue, and its multiplicity, or more
precisely its algebraic multiplicity, will be given by the number of times this solution
is repeated.
In some situations, we will find it useful to work with the set of all eigenvectors
associated with a specific eigenvalue. This collection, SA(λ), of all eigenvectors cor-
responding to the particular eigenvalue λ, along with the trivial vector 0, is called
the eigenspace of A associated with λ; that is, SA(λ) is given by SA(λ) = {x : x ∈
Rm and Ax = λx}.
Theorem 3.1
If SA(λ) is the eigenspace of the m × m matrix A corresponding to
the eigenvalue λ, then SA(λ) is a vector subspace of Rm.
Proof.
By definition, if x ∈SA(λ), then Ax = λx. Thus, if x ∈SA(λ) and y ∈
SA(λ), we have for any scalars α and β
A(αx + βy) = αAx + βAy = α(λx) + β(λy) = λ(αx + βy).
Consequently, (αx + βy) ∈SA(λ), and so SA(λ) is a vector space.
□
Example 3.3
The matrix
A =
⎡
⎣
2
−1
0
0
1
0
0
0
1
⎤
⎦
has the characteristic equation

2 −λ
−1
0
0
1 −λ
0
0
0
1 −λ

= (1 −λ)2(2 −λ) = 0,
and so the eigenvalues of A are 1, with multiplicity 2, and 2. To find SA(1), the
eigenspace corresponding to the eigenvalue 1, we solve the equation Ax = x for x.
We leave it to the reader to verify that two linearly independent solutions result; any
solution to Ax = x will be a linear combination of the two vectors x1 = (0, 0, 1)′
and x2 = (1, 1, 0)′. Thus, SA(1) is the subspace spanned by the basis {x1, x2}; that
is, SA(1) is a plane in R3. In a similar fashion, we may find the eigenspace, SA(2).
Solving Ax = 2x, we find that x must be a scalar multiple of (1, 0, 0)′. Thus, SA(2)
is the line in R3 given by {(a, 0, 0)′ : −∞< a < ∞}.
In Example 3.3, for each of the two values of λ, we have dim{S(λ)}, which is
sometimes referred to as the geometric multiplicity of λ, being equal to the corre-
sponding algebraic multiplicity of λ. This is not always the case. In general, when

SOME BASIC PROPERTIES OF EIGENVALUES AND EIGENVECTORS
99
we simply use the term multiplicity, we will be referring to the algebraic multiplicity.
Example 3.4 illustrates that it is possible for dim{S(λ)} to be less than the multiplic-
ity of the eigenvalue λ.
Example 3.4
Consider the 3 × 3 matrix given by
A =
⎡
⎣
1
2
3
0
1
0
0
2
1
⎤
⎦.
Since |A −λI3| = (1 −λ)3, A has the eigenvalue 1 repeated three times. The
eigenvalue-eigenvector equation Ax = λx yields the three scalar equations
x1 + 2x2 + 3x3 = x1,
x2 = x2,
2x2 + x3 = x3,
which have as a solution only vectors of the form x = (a, 0, 0)′. Thus, although the
multiplicity of the eigenvalue 1 is 3, the associated eigenspace SA(1) = {(a, 0, 0)′ :
−∞< a < ∞} is only one-dimensional.
3.3
SOME BASIC PROPERTIES OF EIGENVALUES
AND EIGENVECTORS
In this section, we establish some useful results regarding eigenvalues. The proofs
of the results in Theorom 3.2, which are left to the reader as an exercise, are easily
obtained by using the characteristic equation or the eigenvalue-eigenvector equation.
Theorem 3.2
Let A be an m × m matrix. Then the following properties hold:
(a) The eigenvalues of A′ are the same as the eigenvalues of A.
(b) A is singular if and only if at least one eigenvalue of A is equal to 0.
(c) The diagonal elements of A are the eigenvalues of A, if A is a triangular
matrix.
(d) The eigenvalues of BAB−1 are the same as the eigenvalues of A, if B is a
nonsingular m × m matrix.
(e) The modulus of each eigenvalue of A is equal to 1 if A is an orthogonal matrix.
We saw in Example 3.4 that it is possible for the dimension of an eigenspace
associated with an eigenvalue λ to be less than the multiplicity of λ. Theorem
3.3 shows that if dim{SA(λ)} ̸= r, where r denotes the multiplicity of λ, then
dim{SA(λ)} < r.

100
EIGENVALUES AND EIGENVECTORS
Theorem 3.3
Suppose that λ is an eigenvalue, with multiplicity r ≥1, of the
m × m matrix A. Then
1 ≤dim{SA(λ)} ≤r.
Proof.
If λ is an eigenvalue of A, then by definition an x ̸= 0 satisfying the
eigenvalue-eigenvector equation Ax = λx exists, and so clearly dim{SA(λ)} ≥1.
Now let k = dim{SA(λ)}, and let x1, . . . , xk be linearly independent eigenvectors
corresponding to λ. Form a nonsingular m × m matrix X that has these k
vectors as its first k columns; that is, X has the form X = [X1
X2], where
X1 = (x1, . . . , xk) and X2 is m × (m −k). Since each column of X1 is an
eigenvector of A corresponding to the eigenvalue λ, it follows that AX1 = λX1, and
X−1X1 =

Ik
(0)

follows from the fact that X−1X = Im. As a result, we find that
X−1AX = X−1[AX1
AX2]
= X−1[λX1
AX2]
=
λIk
B1
(0)
B2

,
where B1 and B2 represent a partitioning of the matrix X−1AX2. If μ is an eigenvalue
of X−1AX, then
0 = |X−1AX −μIm| =

(λ −μ)Ik
B1
(0)
B2 −μIm−k

= (λ −μ)k|B2 −μIm−k|,
where the last equality can be obtained by repeated use of the cofactor expansion for-
mula for a determinant. Thus, λ must be an eigenvalue of X−1AX with multiplicity
of at least k. The result now follows because, from Theorem 3.2(d), the eigenvalues
of X−1AX are the same as those of A.
□
We now prove Theorem 3.4, which involves both the eigenvalues and the eigen-
vectors of a matrix.
Theorem 3.4
Let λ be an eigenvalue of the m × m matrix A, and let x be a corre-
sponding eigenvector. Then,
(a) if n ≥1 is an integer, λn is an eigenvalue of An corresponding to the eigen-
vector x,

SOME BASIC PROPERTIES OF EIGENVALUES AND EIGENVECTORS
101
(b) if A is nonsingular, λ−1 is an eigenvalue of A−1 corresponding to the eigen-
vector x.
Proof.
Part (a) can be proven by induction. Clearly (a) holds when n = 1 because
it follows from the definition of λ and x. Note that if (a) holds for n −1, that is,
An−1x = λn−1x, then
Anx = A(An−1x) = A(λn−1x)
= λn−1(Ax) = λn−1(λx) = λnx,
and so the induction proof of (a) is complete. To prove part (b), premultiply the
eigenvalue-eigenvector equation
Ax = λx
by A−1, which yields the equation
x = λA−1x.
(3.3)
Since A is nonsingular, we know from Theorem 3.2(b) that λ ̸= 0, and so dividing
both sides of (3.3) by λ yields
A−1x = λ−1x,
which is the eigenvalue-eigenvector equation for A−1, with eigenvalue λ−1 and eigen-
vector x.
□
The determinant and trace of a matrix have simple and useful relationships with
the eigenvalues of that matrix. These relationships are established in Theorem 3.5.
Theorem 3.5
Let A be an m × m matrix with eigenvalues λ1, . . . , λm. Then
(a) tr(A) = 	m
i=1 λi,
(b) |A| = 
m
i=1 λi.
Proof.
Recall that the characteristic equation, |A −λIm| = 0, can be expressed in
the polynomial form
(−λ)m + αm−1(−λ)m−1 + · · · + α1(−λ) + α0 = 0.
(3.4)
We will first identify the coefficients α0 and αm−1. We can determine α0 by eval-
uating the left-hand side of (3.4) at λ = 0; thus, α0 = |A −(0)Im| = |A|. To find
αm−1, recall that, from its definition, the determinant is expressed as a sum of terms

102
EIGENVALUES AND EIGENVECTORS
over all permutations of the integers (1, 2, . . . , m). Since αm−1 is the coefficient of
(−λ)m−1, to identify this term we only need to consider the terms in the sum that
involve at least m −1 of the diagonal elements of (A −λIm). However, each term
in the sum is the product of m elements from the matrix (A −λIm), multiplied by
the appropriate sign, with one element chosen from each row and each column of
(A −λIm). Consequently, the only term in the sum involving at least m −1 of the
diagonal elements of (A −λIm) is the term that involves the product of all of the
diagonal elements. Since this term involves an even permutation, the sign term will
equal +1, and so αm−1 will be the coefficient of (−λ)m−1 in
(a11 −λ)(a22 −λ) · · · (amm −λ),
which clearly is a11 + a22 + · · · + amm or simply tr(A). Now to relate α0 = |A| and
αm−1 = tr(A) to the eigenvalues of A, note that because λ1, . . . , λm are the roots
to the characteristic equation, which is an mth degree polynomial in λ, it follows that
(λ1 −λ)(λ2 −λ) · · · (λm −λ) = 0.
Multiplying out the left-hand side of this equation and then matching corresponding
terms with those in (3.4), we find that
|A| =
m

i=1
λi,
tr(A) =
m

i=1
λi,
and this completes the proof.
□
The utility of the formulas for the determinant and trace of a matrix in terms of its
eigenvalues is illustrated in the proof of Theorem 3.6.
Theorem 3.6
Let A be an m × m nonsingular symmetric matrix and c and d be
m × 1 vectors. Then
|A + cd′| = |A|(1 + d′A−1c).
Proof.
Since A is nonsingular, we have
|A + cd′| = |A(Im + A−1cd′)| = |A||Im + bd′|,
where b = A−1c. For any x orthogonal to d, we have
(Im + bd′)x = x,
so 1 is an eigenvalue of Im + bd′ with multiplicity at least m −1 because there are
m −1 linearly independent vectors orthogonal to d. However, tr(Im + bd′) = m +
d′b, which implies that the final eigenvalue of Im + bd′ is given by 1 + d′b. Taking

SOME BASIC PROPERTIES OF EIGENVALUES AND EIGENVECTORS
103
the product of these eigenvalues, we find that |Im + bd′| = (1 + d′b). The result now
follows because d′b = d′A−1c.
□
Theorem 3.7 gives a sufficient condition for a set of eigenvectors to be linearly
independent.
Theorem 3.7
Suppose x1, . . . , xr are eigenvectors of the m × m matrix A, where
r ≤m. If the corresponding eigenvalues λ1, . . . , λr are such that λi ̸= λj for all
i ̸= j, then the vectors x1, . . . , xr are linearly independent.
Proof.
Our proof is by contradiction; that is, we begin by assuming that the vectors
x1, . . . , xr are linearly dependent. Let h be the largest integer for which x1, . . . , xh
are linearly independent. Such a set can be found because x1, being an eigenvector,
cannot equal 0, and so it is linearly independent. The vectors x1, . . . , xh+1 must be
linearly dependent, so there exist scalars α1, . . . , αh+1, with at least two not equal
to zero because no eigenvector can be the null vector, such that
α1x1 + · · · + αh+1xh+1 = 0.
Premultiplying the left-hand side of this equation by (A −λh+1Im), we find that
α1(A −λh+1Im)x1 + · · · + αh+1(A −λh+1Im)xh+1
= α1(Ax1 −λh+1x1) + · · · + αh+1(Axh+1 −λh+1xh+1)
= α1(λ1 −λh+1)x1 + · · · + αh(λh −λh+1)xh
also must be equal to 0. But x1, . . . , xh are linearly independent, so it follows that
α1(λ1 −λh+1) = · · · = αh(λh −λh+1) = 0.
We know that at least one of the scalars α1, . . . , αh is not equal to zero, and if, for
instance, αi is one of these nonzero scalars, we then must have λi = λh+1. This result
contradicts the conditions of the theorem, so the vectors x1, . . . , xr must be linearly
independent.
□
If the eigenvalues λ1, . . . , λm of an m × m matrix A are all distinct, then
it follows from Theorem 3.7 that the matrix X = (x1, . . . , xm), where xi is
an eigenvector corresponding to λi, is nonsingular. It also follows from the
eigenvalue-eigenvector equation Axi = λixi that if we define the diagonal matrix
Λ = diag(λ1, . . . , λm), then AX = XΛ. Premultiplying this equation by X−1
yields the identity X−1AX = Λ. Any square matrix that can be transformed into
a diagonal matrix through the postmultiplication by a nonsingular matrix and
premultiplication by its inverse is said to be diagonalizable. Thus, a square matrix
with distinct eigenvalues is diagonalizable.

104
EIGENVALUES AND EIGENVECTORS
When X is nonsingular, the equation AX = XΛ can also be rearranged as A =
XΛX−1. That is, in this case, A can be determined from its eigenvalues and any set
of corresponding linearly independent eigenvectors.
Example 3.5
Suppose that A is a 2 × 2 matrix with eigenvalues 1 and 2, and cor-
responding eigenvectors given as the columns of the matrix
X =

5
3
3
2

.
Since |X| = 1, X is nonsingular and
X−1 =
 2
−3
−3
5

.
Thus, we have enough information to compute A; that is,
A = XΛX−1 =

5
3
3
2
 
1
0
0
2
 
2
−3
−3
5

=

−8
15
−6
11

.
Consider a second 2 × 2 matrix B that has both of its eigenvalues equal to 0, but
only a single linearly independent eigenvector being any nonzero scalar multiple of
e1 = (1, 0)′. The eigenvalue-eigenvector equation Be1 = 0 implies that b11 = b21 =
0, and so the characteristic equation for B is

−λ
b12
0
b22 −λ
 = −λ(b22 −λ) = 0.
Since both of the eigenvalues of B are 0, we must have b22 = 0, and so B is of the
form
B =

0
b12
0
0

,
where b12 ̸= 0, because otherwise B would have two linearly independent eigenvec-
tors. Note, however, that the value of b12 cannot be determined.
Clearly, when a matrix is diagonalizable, then its rank equals the number of its
nonzero eigenvalues, because
rank(A) = rank(X−1AX) = rank(Λ)
follows from Theorem 1.10. This relationship between the number of nonzero eigen-
values and the rank of a square matrix does not necessarily hold if the matrix is not
diagonalizable.

SOME BASIC PROPERTIES OF EIGENVALUES AND EIGENVECTORS
105
Example 3.6
Consider the 2 × 2 matrices
A =
1
1
0
0

and
B =
0
1
0
0

.
Clearly, both A and B have rank of 1. Now the characteristic equation of A simplifies
to λ(1 −λ) = 0 so that the eigenvalues of A are 0 and 1, and thus, in this case,
rank(A) equals the number of nonzero eigenvalues. The characteristic equation for
B simplifies to λ2 = 0, so B has the eigenvalue 0 repeated twice. Hence, the rank of
B exceeds its number of nonzero eigenvalues.
Theorem 3.8, known as the Cayley–Hamilton theorem, states that a matrix satisfies
its own characteristic equation.
Theorem 3.8
Let A be an m × m matrix with eigenvalues λ1, . . . , λm. Then
m

i=1
(A −λiIm) = (0);
that is, if (−λ)m + αm−1(−λ)m−1 + · · · + α1(−λ) + α0 = 0 is the characteristic
equation of A, then
(−A)m + αm−1(−A)m−1 + · · · + α1(−A) + α0Im = (0).
Proof.
If (A −λIm)# is the adjoint of A −λIm, then
(A −λIm)(A −λIm)# = |A −λIm|Im.
Since |A −λIm| is an mth degree polynomial in λ, it follows that (A −λIm)# is a
polynomial in λ of degree at most m −1. That is, we can write
(A −λIm)# =
m−1

i=0
(−λ)iBi,
where Bi is an m × m matrix for i = 0, . . . , m −1, and so
(A −λIm)(A −λIm)# = (A −λIm)
m−1

i=0
(−λ)iBi
= AB0 +
m−1

i=1
(−λ)i(ABi + Bi−1) + (−λ)mBm−1.

106
EIGENVALUES AND EIGENVECTORS
Equating this to |A −λIm|Im = 	m−1
i=0 (−λ)iαiIm + (−λ)mIm, we find that
AB0 = α0Im,
AB1 + B0 = α1Im,
...
ABm−1 + Bm−2 = αm−1Im,
Bm−1 = Im.
Adding these equations after multiplying the ith equation by (−A)i−1, where
(−A)0 = Im, yields the desired result.
□
3.4
SYMMETRIC MATRICES
Many of the applications involving eigenvalues and eigenvectors in statistics are ones
that deal with a symmetric matrix, and symmetric matrices have some especially nice
properties regarding eigenvalues and eigenvectors. In this section, we will develop
some of these properties.
We have seen that a matrix may have complex eigenvalues even when the matrix
itself is real. This is not the case for symmetric matrices.
Theorem 3.9
Let A be an m × m real symmetric matrix. Then the eigenvalues of
A are real, and corresponding to any eigenvalue, eigenvectors that are real exist.
Proof.
Let λ = α + iβ be an eigenvalue of A and x = y + iz a corresponding
eigenvector, where i = √−1. We will first show that β = 0. Substitution of these
expressions for λ and x in the eigenvalue-eigenvector equation Ax = λx yields
A(y + iz) = (α + iβ)(y + iz).
(3.5)
Premultiplying (3.5) by (y −iz)′, we get
(y −iz)′A(y + iz) = (α + iβ)(y −iz)′(y + iz),
which simplifies to
y′Ay + z′Az = (α + iβ)(y′y + z′z),
because y′Az = z′Ay follows from the symmetry of A. Now x ̸= 0 implies that
(y′y + z′z) > 0 and, consequently, we must have β = 0 because the left-hand side
of the equation above is real. Substituting β = 0 in (3.5), we find that
Ay + iAz = αy + iαz.

SYMMETRIC MATRICES
107
Thus, x = y + iz will be an eigenvector of A corresponding to λ = α as long as
y and z satisfy Ay = αy, Az = αz, and at least one is not 0, so that x ̸= 0 . A
real eigenvector is then constructed by selecting y ̸= 0, such that Ay = αy, and
z = 0.
□
We have seen that a set of eigenvectors of an m × m matrix A is linearly indepen-
dent if the associated eigenvalues are all different from one other. We will now show
that, if A is symmetric, we can say a bit more. Suppose that x and y are eigenvectors
of A corresponding to the eigenvalues λ and γ , where λ ̸= γ. Then, because A is
symmetric, it follows that
λx′y = (λx)′y = (Ax)′y = x′A′y
= x′(Ay) = x′(γy) = γx′y.
Since λ ̸= γ, we must have x′y = 0; that is, eigenvectors corresponding to different
eigenvalues must be orthogonal. Thus, if the m eigenvalues of A are distinct, then the
set of corresponding eigenvectors will form a group of mutually orthogonal vectors.
We will show that this is still possible when A has multiple eigenvalues. Before we
prove this, we will need Theorem 3.10.
Theorem 3.10
Let A be an m × m symmetric matrix, and let x be any nonzero
m × 1 vector. Then for some r ≥1, the vector space spanned by the vectors
x, Ax, . . . , Ar−1x contains an eigenvector of A.
Proof.
Let r be the smallest integer for which x, Ax, . . . , Arx form a linearly
dependent set. Then scalars α0, . . . , αr exist, not all of which are zero, such that
α0x + α1Ax + · · · + αrArx = (α0Im + α1A + · · · + Ar)x = 0,
where, without loss of generality, we have taken αr = 1, because the way r was cho-
sen guarantees that αr is not zero. The expression in the parentheses is an rth-degree
matrix polynomial in A, which can be factored in a fashion similar to the way scalar
polynomials are factored; that is, it can be written as
(A −γ1Im)(A −γ2Im) · · · (A −γrIm),
where
γ1, . . . , γr
are
the
roots
of
the
polynomial
which
satisfy
α0 =
(−1)rγ1γ2 · · · γr, . . . , αr−1 = −(γ1 + γ2 + · · · + γr). Let
y = (A −γ2Im) · · · (A −γrIm)x
= (−1)r−1γ2 · · · γrx + · · · + Ar−1x,
and note that y ̸= 0 because, otherwise, x, Ax, . . . , Ar−1x would be a linearly
dependent set, contradicting the definition of r. Thus, y is in the space spanned by
x, Ax, . . . , Ar−1x and
(A −γ1Im)y = (A −γ1Im)(A −γ2Im) · · · (A −γrIm)x = 0.

108
EIGENVALUES AND EIGENVECTORS
Consequently, y is an eigenvector of A corresponding to the eigenvalue γ1, and so
the proof is complete.
□
Theorem 3.11
If the m × m matrix A is symmetric, then it is possible to construct
a set of m eigenvectors of A such that the set is orthonormal.
Proof.
We first show that if we have an orthonormal set of eigenvectors, say
x1, . . . , xh, where 1 ≤h < m, then we can find another normalized eigenvector
xh+1 orthogonal to each of these vectors. Select any vector x that is orthogonal to
each of the vectors x1, . . . , xh. Note that for any positive integer k, Akx is also
orthogonal to x1, . . . , xh because, if λi is the eigenvalue corresponding to xi, it
follows from the symmetry of A and Theorem 3.4(a) that
x′
iAkx = {(Ak)′xi}′x = (Akxi)′x = λk
i x′
ix = 0.
From the previous theorem, we know that, for some r, the space spanned by the vec-
tors x, Ax, . . . , Ar−1x contains an eigenvector, say y, of A. This vector y also must
be orthogonal to x1, . . . , xh because it is from a vector space spanned by a set of vec-
tors orthogonal to x1, . . . , xh. Thus, we can take xh+1 = (y′y)−1/2y. The theorem
now follows by starting with any eigenvector of A, and then using the previous argu-
ment m −1 times.
□
If we let the m × m matrix X = (x1, . . . , xm), where x1, . . . , xm are the
orthonormal vectors described in the proof, and Λ = diag(λ1, . . . , λm), then the
eigenvalue-eigenvector equations Axi = λixi for i = 1, . . . , m can be expressed
collectively as the matrix equation AX = XΛ. Since the columns of X are
orthonormal vectors, X is an orthogonal matrix. Premultiplication of our matrix
equation by X′ yields the relationship X′AX = Λ, or equivalently
A = XΛX′,
which is known as the spectral decomposition of A. We will see in Section 4.2 that
there is a very useful generalization of this decomposition, known as the singular
value decomposition, which holds for any m × n matrix A; in particular, there exist
m × m and n × n orthogonal matrices P and Q and an m × n matrix D with dij = 0
if i ̸= j, such that A = PDQ′.
Note that it follows from Theorem 3.2(d) that the eigenvalues of A are the same as
the eigenvalues of Λ, which are the diagonal elements of Λ. Thus, if λ is a multiple
eigenvalue of A with multiplicity r > 1, then r of the diagonal elements of Λ are
equal to λ and r of the eigenvectors, say x1, . . . , xr, correspond to this eigenvalue
λ. Consequently, the dimension of the eigenspace of A, SA(λ), corresponding to λ,
is equal to the multiplicity r. The set of orthonormal eigenvectors corresponding to
this eigenvalue is not unique. Any orthonormal basis for SA(λ) will be a set of r
orthonormal vectors associated with the eigenvalue λ. For example, if we let X1 =

SYMMETRIC MATRICES
109
(x1, . . . , xr) and let Q be any r × r orthogonal matrix, the columns of Y1 = X1Q
also form a set of orthonormal eigenvectors corresponding to λ.
Example 3.7
One application of an eigenanalysis in statistics involves overcoming
difficulties associated with a regression analysis in which the explanatory variables
are nearly linearly dependent. This situation is often referred to as multicollinearity.
In this case, some of the explanatory variables are providing redundant information
about the response variable. As a result, the least squares estimator of β in the model
y = Xβ + ϵ
ˆβ = (X′X)−1X′y
will be imprecise because its covariance matrix
var(ˆβ) = (X′X)−1X′{var(y)}X(X′X)−1
= (X′X)−1X′{σ2I}X(X′X)−1
= σ2(X′X)−1
will tend to have some large elements because of the near singularity of X′X. If the
near linear dependence is simply because one of the explanatory variables, say xj,
is nearly a scalar multiple of another, say xl, then one could simply eliminate one of
these variables from the model. However, in most cases, the near linear dependence
is not this straightforward. We will see that an eigenanalysis will help reveal any of
these dependencies. Suppose that we standardize the explanatory variables so that we
have the model
y = δ01N + Z1δ1 + ϵ
discussed in Example 2.16. Let Λ = diag(λ1, . . . , λk) contain the eigenvalues of
Z′
1Z1 in descending order of magnitude, and let U be an orthogonal matrix that
has corresponding normalized eigenvectors of Z′
1Z1 as its columns, so that Z′
1Z1 =
UΛU ′. It was shown in Example 2.16 that the estimation of y is unaffected by a non-
singular transformation of the explanatory variables; that is, we could just as well
work with the model
y = α01N + W1α1 + ϵ,
where α0 = δ0, α1 = T −1δ1, W1 = Z1T, and T is a nonsingular matrix. A method,
referred to as principal components regression, deals with the problems associated
with multicollinearity by using the orthogonal transformations W1 = Z1U and α1 =
U ′δ1 of the standardized explanatory variables and parameter vector. The k new
explanatory variables are called the principal components; the variable correspond-
ing to the ith column of W1 is called the ith principal component. Since W ′
1W1 =
U ′Z′
1Z1U = Λ and 1′
NW1 = 1′
NZ1U = 0′U = 0′, the least squares estimate of α1
is
ˆα1 = (W ′
1W1)−1W ′
1y = Λ−1W ′
1y,

110
EIGENVALUES AND EIGENVECTORS
whereas its covariance matrix simplifies to
var(ˆα1) = σ2(W ′
1W1)−1 = σ2Λ−1.
If Z′
1Z1 and, hence, also W ′
1W1 is nearly singular, then at least one of the λi’s will be
very small, whereas the variances of the corresponding ˆαi’s will be very large. Since
the explanatory variables have been standardized, W ′
1W1 is N −1 times the sample
correlation matrix of the principal components computed from the N observations.
Thus, if λi ≈0, then the ith principal component is nearly constant from observation
to observation, and so it contributes little to the estimation of y. If λi ≈0 for i =
k −r + 1, . . . , k, then the problems associated with multicollinearity can be avoided
by eliminating the last r principal components from the model; in other words, the
principal components regression model is
y = α01N + W11α11 + ϵ,
where W11 and α′
11 are obtained from W1 and α′
1 by deleting their last r columns.
If we let Λ1 = diag(λ1, . . . , λk−r), then the least squares estimate of α11 can be
written as
ˆα11 = (W ′
11W11)−1W ′
11y = Λ−1
1 W ′
11y.
Note that because of the orthogonality of the principal components, ˆα11 is identical to
the first k −r components of ˆα1. The estimate ˆα11 can be used to find the principal
components estimate of δ1 in the original standardized model. Recall that δ1 and
α1 are related through the identity δ1 = Uα1. By eliminating the last r principal
components, we are replacing this identity with the identity δ1 = U1α11, where U =
[U1 U2] and U1 is k × (k −r). Thus, the principal components regression estimate
of δ1 is given by
ˆδ1∗= U1 ˆα11 = U1Λ−1
1 W ′
11y.
A set of orthonormal eigenvectors of a matrix A can be used to find what are known
as the eigenprojections of A.
Definition 3.1
Let λ be an eigenvalue of the m × m symmetric matrix A with mul-
tiplicity r ≥1. If x1, . . . , xr is a set of orthonormal eigenvectors corresponding to
λ, then the eigenprojection of A associated with the eigenvalue λ is given by
PA(λ) =
r

i=1
xix′
i.
The eigenprojection PA(λ) is simply the projection matrix for the vector space
SA(λ). Thus, for any x ∈Rm, y = PA(λ)x gives the orthogonal projection of x
onto the eigenspace SA(λ). If we define X1 as before, that is X1 = (x1, . . . , xr),

SYMMETRIC MATRICES
111
then PA(λ) = X1X′
1. Note that PA(λ) is unique even though the set of eigenvec-
tors x1, . . . , xr is not unique; for instance, if Y1 = X1Q, where Q is an arbitrary
r × r orthogonal matrix, then the columns of Y1 form another set of orthonormal
eigenvectors corresponding to λ, but
Y1Y ′
1 = (X1Q)(X1Q)′ = X1QQ′X′
1
= X1IrX′
1 = X1X′
1 = PA(λ).
The term spectral decomposition comes from the term spectral set of A for the set
of all eigenvalues of A excluding repetitions of the same value. Suppose the m × m
matrix A has the spectral set {μ1, . . . , μk}, where k ≤m, because some of the μi’s
may correspond to multiple eigenvalues. The set of μi’s may be different from our set
of λi’s in that we do not repeat values for the μi’s. Thus, if A is 4 × 4 with eigenvalues
λ1 = 3, λ2 = 2, λ3 = 2, and λ4 = 1, then the spectral set of A is {3, 2, 1}. Using X
and Λ as previously defined, the spectral decomposition states that
A = XΛX′ =
m

i=1
λixix′
i =
k

i=1
μiPA(μi),
so that A has been decomposed into a sum of terms, one correspond-
ing to each value in the spectral set. If mi is the multiplicity of μi and
λMi+1 = · · · = λMi+mi = μi, where M1 = 0 and Mi = 	i−1
j=1 mj for i = 2, . . . , k,
then we have PA(μi) = 	mi
j=1 xMi+jx′
Mi+j. Note that when we write the decom-
position as A = 	k
i=1 μiPA(μi), the terms in the sum are uniquely defined because
of the uniqueness of the projection matrix of a vector space. On the other hand, the
decomposition A = 	m
i=1 λixix′
i does not have uniquely defined terms unless the
λi’s are distinct.
Example 3.8
It can be easily verified by solving the characteristic equation for the
3 × 3 symmetric matrix
A =
⎡
⎣
5
−1
−1
−1
5
−1
−1
−1
5
⎤
⎦
that A has the simple eigenvalue 3 and the multiple eigenvalue 6, with multiplicity
2. The unique (except for sign) unit eigenvector associated with the eigenvalue 3 can
be shown to equal (1/
√
3, 1/
√
3, 1/
√
3)′, whereas a set of orthonormal eigenvectors
associated with 6 is given by (−2/
√
6, 1/
√
6, 1/
√
6)′ and (0, 1/
√
2, −1/
√
2)′. Thus,

112
EIGENVALUES AND EIGENVECTORS
the spectral decomposition of A is given by
⎡
⎣
5
−1
−1
−1
5
−1
−1
−1
5
⎤
⎦=
⎡
⎣
1/
√
3
−2/
√
6
0
1/
√
3
1/
√
6
1/
√
2
1/
√
3
1/
√
6
−1/
√
2
⎤
⎦
⎡
⎣
3
0
0
0
6
0
0
0
6
⎤
⎦
×
⎡
⎣
1/
√
3
1/
√
3
1/
√
3
−2/
√
6
1/
√
6
1/
√
6
0
1/
√
2
−1/
√
2
⎤
⎦,
and the two eigenprojections of A are
PA(3) =
⎡
⎣
1/
√
3
1/
√
3
1/
√
3
⎤
⎦[1/
√
3
1/
√
3
1/
√
3] = 1
3
⎡
⎣
1
1
1
1
1
1
1
1
1
⎤
⎦,
PA(6) =
⎡
⎣
−2/
√
6
0
1/
√
6
1/
√
2
1/
√
6
−1/
√
2
⎤
⎦
−2/
√
6
1/
√
6
1/
√
6
0
1/
√
2
−1/
√
2

= 1
3
⎡
⎣
2
−1
−1
−1
2
−1
−1
−1
2
⎤
⎦.
The relationship between the rank of a matrix and the number of its nonzero eigen-
values becomes an exact one for symmetric matrices.
Theorem 3.12
Suppose that the m × m matrix A has r nonzero eigenvalues. Then,
if A is symmetric, rank(A) = r.
Proof.
If A = XΛX′ is the spectral decomposition of A, then the diagonal matrix
Λ has r nonzero diagonal elements and
rank(A) = rank(XΛX′) = rank(Λ),
because the multiplication of a matrix by nonsingular matrices does not affect the
rank. Clearly, the rank of a diagonal matrix equals the number of its nonzero diagonal
elements, so the result follows.
□
Some of the most important applications of eigenvalues and eigenvectors in statis-
tics involve the analysis of covariance and correlation matrices.
Example 3.9
In some situations, a matrix has some special structure that when
recognized, can be used to expedite the calculation of eigenvalues and eigenvectors. In
this example, we consider a structure sometimes possessed by an m × m covariance

SYMMETRIC MATRICES
113
matrix. This structure is one that has equal variances and equal correlations; that is,
the covariance matrix has the form
Ω = σ2
⎡
⎢⎢⎢⎣
1
ρ
· · ·
ρ
ρ
1
· · ·
ρ
...
...
...
ρ
ρ
· · ·
1
⎤
⎥⎥⎥⎦.
Alternatively, Ω can be expressed as Ω = σ2{(1 −ρ)Im + ρ1m1′
m}, so that it is a
function of the vector 1m. This vector also plays a crucial role in the eigenanalysis
of Ω because
Ω1m = σ2{(1 −ρ)1m + ρ1m1′
m1m} = σ2{(1 −ρ) + mρ}1m.
Thus, 1m is an eigenvector of Ω corresponding to the eigenvalue σ2{(1 −ρ) + mρ}.
The remaining eigenvalues of Ω can be identified by noting that if x is any m × 1
vector orthogonal to 1m, then
Ωx = σ2{(1 −ρ)x + ρ1m1′
mx} = σ2(1 −ρ)x,
and so x is an eigenvector of Ω corresponding to the eigenvalue σ2(1 −ρ). Since there
are m −1 linearly independent vectors orthogonal to 1m, the eigenvalue σ2(1 −ρ)
is repeated m −1 times. The order of these two distinct eigenvalues depends on the
value of ρ; σ2{(1 −ρ) + mρ} will be larger than σ2(1 −ρ) only if ρ is positive.
Example 3.10
A covariance matrix can be any symmetric nonnegative definite
matrix. Consequently, for a given set of m nonnegative numbers and a given set
of m orthonormal m × 1 vectors, it is possible to construct an m × m covariance
matrix with these numbers and vectors as its eigenvalues and eigenvectors. On the
other hand, a correlation matrix has the additional constraint that its diagonal ele-
ments must each equal 1, and this extra restriction has an impact on the eigenanalysis
of correlation matrices; that is, a much more limited set of possible eigenvalues and
eigenvectors exists for correlation matrices. For the most extreme case, consider a
2 × 2 correlation matrix that must have the form,
P =

1
ρ
ρ
1

with −1 ≤ρ ≤1, because P must be nonnegative definite. The characteristic
equation |P −λI2| = 0 readily admits the two eigenvalues 1 + ρ and 1 −ρ. Using
these in the eigenvalue-eigenvector equation Px = λx, we find that regardless of the
value of ρ, (1/
√
2, 1/
√
2)′ must be an eigenvector corresponding to 1 + ρ, whereas
(1/
√
2, −1/
√
2)′ must be an eigenvector corresponding to 1 −ρ. Thus, ignoring
sign changes, only one set of orthonormal eigenvectors is possible for a 2 × 2
correlation matrix if ρ ̸= 0. This number of possible sets of orthonormal eigenvectors

114
EIGENVALUES AND EIGENVECTORS
increases as the order m increases. In some situations, such as simulation studies
of analyses of correlation matrices, one may wish to construct a correlation matrix
with some particular structure with regard to its eigenvalues or eigenvectors. For
example, suppose that we want to construct an m × m correlation matrix that has
three distinct eigenvalues with one of them being repeated m −2 times. Thus, this
correlation matrix has the form
P = λ1x1x′
1 + λ2x2x′
2 +
m

i=3
λxix′
i,
where λ1, λ2, and λ are the distinct eigenvalues of P, and x1, . . . , xm
are corresponding normalized eigenvectors. Since P is nonnegative definite,
we must have λ1 ≥0, λ2 ≥0, and λ ≥0, whereas tr(P) = m implies that
λ = (m −λ1 −λ2)/(m −2). Note that P can be written as
P = (λ1 −λ)x1x′
1 + (λ2 −λ)x2x′
2 + λIm,
so that the constraint (P)ii = 1 implies that
(λ1 −λ)x2
i1 + (λ2 −λ)x2
i2 + λ = 1
or, equivalently,
x2
i2 = 1 −λ −(λ1 −λ)x2
i1
(λ2 −λ)
.
These constraints can then be used to construct a particular matrix. For instance, sup-
pose that we want to construct a 4 × 4 correlation matrix with eigenvalues λ1 = 2,
λ2 = 1, and λ = 0.5 repeated twice. If we choose x1 = (0.5, 0.5, 0.5, 0.5)′, then we
must have x2
i2 = 0.25, and so because of the orthogonality of x1 and x2, x2 can be
any vector obtained from x1 by negating two of its components. For example, if we
take x2 = (0.5, −0.5, 0.5, −0.5)′, then
P =
⎡
⎢⎢⎣
1
0.25
0.50
0.25
0.25
1
0.25
0.50
0.50
0.25
1
0.25
0.25
0.50
0.25
1
⎤
⎥⎥⎦.
3.5
CONTINUITY OF EIGENVALUES AND EIGENPROJECTIONS
Our first result of this section is one that bounds the absolute difference between
eigenvalues of two matrices by a function of the absolute differences of the elements
of the two matrices. A proof of Theorem 3.13 can be found in Ostrowski (1973). For
some other similar bounds, see Elsner (1982).

CONTINUITY OF EIGENVALUES AND EIGENPROJECTIONS
115
Theorem 3.13
Let A and B be m × m matrices with eigenvalues λ1, . . . , λm and
γ1, . . . , γm, respectively. Define
M =
max
1≤i≤m,1≤j≤m(|aij|, |bij|)
and
δ(A, B) = 1
m
m

i=1
m

j=1
|aij −bij|.
Then
max
1≤i≤m min
1≤j≤m |λi −γj| ≤(m + 2)M 1−1/mδ(A, B)1/m.
Theorem 3.13 will allow us to establish a useful result regarding the eigenvalues of
any matrix A. Let B1, B2, . . . be a sequence of m × m matrices such that Bn →A,
as n →∞, and let δ(A, Bn) be as defined in Theorem 3.13. It follows from the fact
that Bn →A, as n →∞, that δ(A, Bn) →0, as n →∞. Hence, if γ1,n, . . . , γm,n
are the eigenvalues of Bn, then Theorem 3.13 tells us that
max
1≤i≤m min
1≤j≤m |λi −γj,n| →0,
as n →∞. In other words, if Bn is very close to A, then for each i, some j exists,
such that γj,n is close to λi, or more precisely, as Bn →A, the eigenvalues of Bn
are converging to those of A. This leads to Theorem 3.14.
Theorem 3.14
Let λ1, . . . , λm be the eigenvalues of the m × m matrix A. Then,
for each i, λi is a continuous function of the elements of A.
Theorem 3.15 addresses the continuity of the eigenprojection PA(λ) of a sym-
metric matrix A. A detailed treatment of this problem, as well as the more general
problem of the continuity of the eigenprojections of nonsymmetric matrices, can be
found in Kato (1982).
Theorem 3.15
Suppose that A is an m × m symmetric matrix and λ is one of its
eigenvalues. Then PA(λ), the eigenprojection associated with the eigenvalue λ, is a
continuous function of the elements of A.
Example 3.11
Consider the matrix
A =
⎡
⎣
2
0
0
0
1
0
0
0
1
⎤
⎦,
which clearly has the simple eigenvalue 2 and the repeated eigenvalue 1. Suppose
that B1, B2, . . . is a sequence of 3 × 3 matrices such that Bn →A, as n →∞.

116
EIGENVALUES AND EIGENVECTORS
Let γ1,n ≥γ2,n ≥γ3,n be the eigenvalues of Bn, whereas x1,n, x2,n, and x3,n
is a corresponding set of orthonormal eigenvectors. Theorem 3.14 implies that,
as n →∞,
γ1,n →2
and
γi,n →1,
for i = 2, 3.
On the other hand, Theorem 3.15 implies that, as n →∞,
P1,n →PA(2),
P2,n →PA(1),
where
P1,n = x1,nx′
1,n,
P2,n = x2,nx′
2,n + x3,nx′
3,n.
For instance, suppose that
Bn =
⎡
⎣
2
0
n−1
0
1
0
n−1
0
1
⎤
⎦,
so that, clearly, Bn →A. The characteristic equation of Bn simplifies to
λ3 −4λ2 + (5 −n−2)λ −2 + n−2 = (λ −1)(λ2 −3λ + 2 −n−2) = 0,
so that the eigenvalues of Bn are
1,
3
2 −
√
1 + 4n−2
2
,
3
2 +
√
1 + 4n−2
2
,
which do converge to 1, 1, and 2, respectively. It is left as an exercise for the reader
to verify that
P1,n →
⎡
⎣
1
0
0
0
0
0
0
0
0
⎤
⎦= PA(2),
P2,n →
⎡
⎣
0
0
0
0
1
0
0
0
1
⎤
⎦= PA(1).
3.6
EXTREMAL PROPERTIES OF EIGENVALUES
One of the reasons that eigenvalues play a prominent role in many applications is
because they can be expressed as maximum or minimum values of certain functions
involving a quadratic form. In this section, we derive some of these extremal proper-
ties of eigenvalues.
Let A be a fixed m × m symmetric matrix, and consider the quadratic form x′Ax
as a function of x ̸= 0. If α is a nonzero scalar, then (αx)′A(αx) = α2x′Ax, so
that the quadratic form can be made arbitrarily small or large, depending on whether
x′Ax is negative or positive, through the proper choice of α. Thus, any meaningful

EXTREMAL PROPERTIES OF EIGENVALUES
117
study of the variational properties of x′Ax as we change x will require the removal of
the effect of scale changes in x, which can be accomplished through the construction
of what is commonly called the Rayleigh quotient given by
R(x, A) = x′Ax
x′x .
Note that R(αx, A) = R(x, A). Our first result involves the global maximization and
minimization of R(x, A).
Theorem 3.16
Let A be a symmetric m × m matrix with ordered eigenvalues
λ1 ≥· · · ≥λm. For any m × 1 vector x ̸= 0,
λm ≤x′Ax
x′x ≤λ1,
(3.6)
and, in particular,
λm = min
x̸=0
x′Ax
x′x ,
λ1 = max
x̸=0
x′Ax
x′x .
(3.7)
Proof.
Let A = XΛX′ be the spectral decomposition of A, where the columns of
X = (x1, . . . , xm) are normalized eigenvectors of A and Λ = diag(λ1, . . . , λm).
Then, if y = X′x, we have
x′Ax
x′x = x′XΛX′x
xXX′x
= y′Λy
y′y =
m
	
i=1
λiy2
i
m
	
i=1
y2
i
,
so that (3.6) follows from the fact that
λm
m

i=1
y2
i ≤
m

i=1
λiy2
i ≤λ1
m

i=1
y2
i .
Now (3.7) is verified by choices of x for which the bounds in (3.6) are attained; for
instance, the lower bound is attained with x = xm, whereas the upper bound holds
with x = x1.
□
Note that, because for any nonnull x, z = (x′x)−1/2x is a unit vector, the min-
imization and maximization of z′Az over all unit vectors z will also yield λm and
λ1, respectively; that is,
λm = min
z′z=1 z′Az,
λ1 = max
z′z=1 z′Az.

118
EIGENVALUES AND EIGENVECTORS
Theorem 3.17 shows that each eigenvalue of a symmetric matrix A can be
expressed as a constrained maximum or minimum of the Rayleigh quotient,
R(x, A).
Theorem 3.17
Let A be an m × m symmetric matrix having eigenvalues λ1 ≥
λ2 ≥· · · ≥λm with x1, . . . , xm being a corresponding set of orthonormal eigen-
vectors. For h = 1, . . . , m, define Sh and Th to be the vector spaces spanned by the
columns of Xh = (x1, . . . , xh) and Yh = (xh, . . . , xm), respectively. Then
λh = min
x∈Sh
x̸=0
x′Ax
x′x =
min
Y ′
h+1x=0
x̸=0
x′Ax
x′x
and
λh = max
x∈Th
x̸=0
x′Ax
x′x =
max
X′
h−1x=0
x̸=0
x′Ax
x′x .
Proof.
We will prove the result concerning the minimum; the proof for the
maximum is similar. Let X = (x1, . . . , xm) and Λ = diag(λ1, . . . , λm). Note
that, because X′AX = Λ and X′X = Im, it follows that X′
hXh = Ih and
X′
hAXh = Λh, where Λh = diag(λ1, . . . , λh). Now x ∈Sh if and only if an h × 1
vector y exists, such that x = Xhy. Consequently,
min
x∈Sh
x̸=0
x′Ax
x′x = min
y̸=0
y′X′
hAXhy
y′X′
hXhy
= min
y̸=0
y′Λhy
y′y
= λh,
where the last equality follows from Theorem 3.16. The second version of the mini-
mization follows immediately from the first and the fact that the null space of Y ′
h+1
is Sh.
□
Example 3.12 and Example 3.13 give some indication of how the extremal prop-
erties of eigenvalues make them important features in many applications.
Example 3.12
Suppose that the same m variables are measured on individuals from
k different groups with the goal being to identify differences in the means for the k
groups. Let the m × 1 vectors μ1, . . . , μk represent the k group mean vectors, and
let μ = (μ1 + · · · + μk)/k be the average of these mean vectors. To investigate the
differences in group means, we will use the deviations (μi −μ) from the average
mean; in particular, we form the sum of squares and cross products matrix given by
A =
k

i=1
(μi −μ)(μi −μ)′.
Note that for a particular unit vector x, x′Ax will give a measure of the differences
among the k groups in the direction x; a value of zero indicates the groups have

EXTREMAL PROPERTIES OF EIGENVALUES
119
identical means in this direction, whereas increasingly large values of x′Ax indicate
increasingly widespread differences in this same direction. If x1, . . . , xm are nor-
malized eigenvectors of A corresponding to its ordered eigenvalues λ1 ≥· · · ≥λm,
then it follows from Theorem 3.16 and Theorem 3.17 that the greatest difference
among the k groups, in terms of deviations from the overall mean, occurs in the direc-
tion given by x1. Of all directions orthogonal to x1, x2 gives the direction with the
greatest difference among the k groups, and so on. If some of the eigenvalues are very
small relative to the rest, then we will be able to effectively reduce the dimension of
the problem. For example, suppose that λ3, . . . , λm are all very small relative to λ1
and λ2. Then all substantial differences among the group means will be observed in
the plane spanned by x1 and x2. In Example 4.12, we will discuss the statistical pro-
cedure, called canonical variate analysis, which puts this sort of dimension reducing
process into practice.
Example 3.13
In Example 3.12, the focus was on means. In this example, we will
look at a procedure that concentrates on variances. This technique, called principal
components analysis, was developed by Hotelling (1933). Some good references on
this subject are Jackson (1991) and Jolliffe (2002). Let x be an m × 1 random vector
having the covariance matrix Ω. Suppose that we wish to find the m × 1 vector a1 so
as to make the variance of a′
1x as large as possible. However, from Section 1.13, we
know that
var(a′
1x) = a′
1{var(x)}a1 = a′
1Ωa1.
(3.8)
Clearly, we can make this variance arbitrarily large by taking a1 = αc for some
scalar α and some vector c ̸= 0, and then let α →∞. We will remove this effect of
the scale of a1 by imposing a constraint. For example, we may consider maximizing
(3.8) over all choices of a1 satisfying a′
1a1 = 1. In this case, we are searching for
the one direction in Rm, that is, the line, for which the variability of observations
of x projected onto that line is maximized. It follows from Theorem 3.16 that
this direction is given by the normalized eigenvector of Ω corresponding to its
largest eigenvalue. Suppose we also wish to find a second direction, given by
a2 and orthogonal to a1, where a′
2a2 = 1 and var(a′
2x) is maximized. From
Theorem 3.17, this second direction is given by the normalized eigenvector of
Ω corresponding to its second largest eigenvalue. Continuing in this fashion,
we would obtain m directions identified by the set a1, . . . , am of orthonormal
eigenvectors of Ω. Effectively, what we will have done is to find a rotation of
the original axes to a new set of orthogonal axes, where each successive axis is
selected so as to maximize the dispersion among the x observations along that
axis. Note that the components of the transformed vector (a′
1x, . . . , a′
mx)′,
which are called the principal components of Ω, are uncorrelated because for
i ̸= j,
cov(a′
ix, a′
jx) = a′
iΩaj = a′
i(λjaj) = λja′
iaj = 0.

120
EIGENVALUES AND EIGENVECTORS
For some specific examples, first consider the 4 × 4 covariance matrix given by
Ω =
⎡
⎢⎢⎣
4.65
4.35
0.55
0.45
4.35
4.65
0.45
0.55
0.55
0.45
4.65
4.35
0.45
0.55
4.35
4.65
⎤
⎥⎥⎦.
The eigenvalues of Ω are 10, 8, 0.4, and 0.2, so the first two eigenvalues account for a
large proportion, actually 18/18.6 = 0.97, of the total variability of x, which means
that although observations of x would appear as points in R4, almost all of the disper-
sion among these points will be confined to a plane. This plane is spanned by the first
two normalized eigenvectors of Ω, (0.5, 0.5, 0.5, 0.5)′ and (0.5, 0.5, −0.5, −0.5)′. As
a second illustration, consider a covariance matrix such as
Ω =
⎡
⎣
59
5
2
5
35
−10
2
−10
56
⎤
⎦,
which has a repeated eigenvalue; specifically the eigenvalues are 60 and 30 with mul-
tiplicities 2 and 1, respectively. Since the largest eigenvalue of Ω is repeated, there is
no one direction a1 that maximizes var(a′
1x). Instead, the dispersion of x observa-
tions is the same in all directions in the plane given by the eigenspace SΩ(60), which
is spanned by the vectors (1, 1, −2)′ and (2, 0, 1)′. Consequently, a scatter plot of x
observations would produce a circular pattern of points in this plane.
Theorem 3.18, known as the Courant–Fischer min–max theorem, gives alternative
expressions for the intermediate eigenvalues of A as constrained minima and maxima
of the Rayleigh quotient R(x, A).
Theorem 3.18
Let A be an m × m symmetric matrix having eigenvalues λ1 ≥
λ2 ≥· · · ≥λm. For h = 1, . . . , m, let Bh be any m × (h −1) matrix and Ch any
m × (m −h) matrix satisfying B′
hBh = Ih−1 and C′
hCh = Im−h. Then
λh = min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x
(3.9)
as well as
λh = min
Ch
max
C′
hx=0
x̸=0
x′Ax
x′x
(3.10)
Proof.
We first prove the min–max result given by (3.9). Let Xh = (x1, . . . , xh),
where x1, . . . , xh is a set of orthonormal eigenvectors of A, corresponding to

EXTREMAL PROPERTIES OF EIGENVALUES
121
the eigenvalues λ1, . . . , λh. Since Xh−1 is an m × (h −1) matrix satisfying
X′
h−1Xh−1 = Ih−1, it follows that
min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x ≤
max
X′
h−1x=0
x̸=0
x′Ax
x′x = λh,
(3.11)
where the equality follows from Theorem 3.17. Now for arbitrary Bh satisfying
B′
hBh = Ih−1, the matrix B′
hXh is (h −1) × h, so that the columns must be
linearly dependent. Consequently, we can find an h × 1 nonnull vector y such that
B′
hXhy = 0. Since Xhy is one choice for x, we find that
max
B′
hx=0
x̸=0
x′Ax
x′x ≥y′X′
hAXhy
y′X′
hXhy
= y′Λhy
y′y
≥λh,
(3.12)
where Λh = diag(λ1, . . . , λh) and the last inequality follows from (3.6). Minimiz-
ing (3.12) over all choices of Bh gives
min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x ≥λh.
This, along with (3.11), proves (3.9). The proof of (3.10) is along the same lines. Let
Yh = (xh, . . . , xm), where xh, . . . , xm is a set of orthonormal eigenvectors of A,
corresponding to the eigenvalues λh, . . . , λm. Since Yh+1 is an m × (m −h) matrix
satisfying Y ′
h+1Yh+1 = Im−h, it follows that
max
Ch
min
C′
hx=0
x̸=0
x′Ax
x′x ≥
min
Y ′
h+1x=0
x̸=0
x′Ax
x′x = λh,
(3.13)
where the equality follows from Theorem 3.17. For an arbitrary Ch satisfying C′
hCh
= Im−h, the matrix C′
hYh is (m −h) × (m −h + 1), so the columns of C′
hYh must
be linearly dependent. Thus, an (m −h + 1) × 1 nonnull vector y exists, satisfying
C′
hYhy = 0. Since Yhy is one choice for x, we have
min
C′
hx=0
x̸=0
x′Ax
x′x ≤y′Y ′
hAYhy
y′Y ′
hYhy
= y′Δhy
y′y
≤λh,
(3.14)
where Δh = diag(λh, . . . , λm) and the last inequality follows from (3.6). Maximiz-
ing (3.14) over all choices of Ch yields
max
Ch
min
C′
hx=0
x̸=0
x′Ax
x′x ≤λh,
which together with (3.13) establishes (3.10).
□

122
EIGENVALUES AND EIGENVECTORS
Corollary 3.18.1
Let A be an m × m symmetric matrix having eigenvalues λ1 ≥
λ2 ≥· · · ≥λm. For h = 1, . . . , m, let Bh be any m × (h −1) matrix and Ch be
any m × (m −h) matrix. Then
λh ≤max
B′
hx=0
x̸=0
x′Ax
x′x ,
and
λh ≤min
C′
hx=0
x̸=0
x′Ax
x′x .
Proof.
If B′
hBh = Ih−1 and C′
hCh = Im−h, then the two inequalities follow
directly from Theorem 3.18. We need to establish them for arbitrary Bh and Ch.
When B′
hBh = Ih−1, the set SBh = {x : x ∈Rm, B′
hx = 0} is the orthogonal
complement of the vector space that has the columns of Bh as an orthonormal
basis. Thus, the first inequality holds when maximizing over all x ̸= 0 in any
(m −h + 1)-dimensional vector subspace of Rm. Consequently, this inequality also
will hold for any m × (h −1) matrix Bh because, in this case, rank(Bh) ≤h −1
guarantees that the maximization is over a vector subspace of dimension at least
m −h + 1. A similar argument applies to the second inequality.
□
The proof of the following extension of Theorem 3.18 is left to the reader as an
exercise.
Corollary 3.18.2
Let A be an m × m symmetric matrix having eigen-
values
λ1 ≥λ2 ≥· · · ≥λm,
and
let
i1, . . . , ik
be
integers
satisfying
1 ≤i1 < · · · < ik ≤m. Define the matrices Bi1, . . . , Bik, such that Bij
is
m × (ij −1), B′
ijBij = Iij−1, and BihB′
ihBij = Bij for h = j + 1, . . . , k. Define
the matrices Ci1, . . . , Cik, such that Cij is m × (m −ij), C′
ijCij = Im−ij, and
CihC′
ihCij = Cij for h = 1, . . . , j. Then
k

j=1
λij =
min
Bi1, ... ,Bik
max
B′
i1x1=···=B′
ik xk=0
x1̸=0, ... ,xk̸=0
x′
hxl=0,h̸=l
k

j=1
x′
jAxj
x′
jxj
and
k

j=1
λij =
max
Ci1, ... ,Cik
max
C′
i1x1=···=C′
ik xk=0
x1̸=0, ... ,xk̸=0
x′
hxl=0,h̸=l
k

j=1
x′
jAxj
x′
jxj
.

ADDITIONAL RESULTS CONCERNING EIGENVALUES OF SYMMETRIC MATRICES
123
3.7
ADDITIONAL RESULTS CONCERNING EIGENVALUES OF
SYMMETRIC MATRICES
Let A be an m × m symmetric matrix and H be an m × h matrix satisfying H′H =
Ih. In some situations, it is of interest to compare the eigenvalues of A with those of
H′AH. Some comparisons follow immediately from Theorem 3.18. For instance, it
is easily verified that from (3.9), we have
λ1(H′AH) ≥λm−h+1(A),
and from (3.10) we have
λh(H′AH) ≤λh(A).
Theorem 3.19, known as the Poincaré separation theorem (Poincaré, 1890; see also
Fan, 1949), provides some inequalities involving the eigenvalues of A and H′AH in
addition to the two given above.
Theorem 3.19
Let A be an m × m symmetric matrix and H be an m × h matrix
satisfying H′H = Ih. Then, for i = 1, . . . , h, it follows that
λm−h+i(A) ≤λi(H′AH) ≤λi(A).
Proof.
To establish the lower bound on λi(H′AH), let Yn = (xn, . . . , xm), where
n = m −h + i + 1, and x1, . . . , xm is a set of orthonormal eigenvectors of A cor-
responding to the eigenvalues λ1(A) ≥· · · ≥λm(A). Then it follows that
λm−h+i(A) = λn−1(A) = min
Y ′nx=0
x̸=0
x′Ax
x′x ≤min
Y ′nx=0
x=Hy
y̸=0
x′Ax
x′x
=
min
Y ′nHy=0
y̸=0
y′H′AHy
y′y
≤λh−(m−n+1)(H′AH)
= λi(H′AH),
where the second equality follows from Theorem 3.17. The last inequality follows
from Corollary 3.18.1, after noting that the order of H′AH is h and Y ′
nH is (m −
n + 1) × h. To prove the upper bound for λi(H′AH), let Xi−1 = (x1, . . . , xi−1),
and note that
λi(A) =
max
X′
i−1x=0
x̸=0
x′Ax
x′x ≥
max
X′
i−1x=0
x=Hy
y̸=0
x′Ax
x′x
=
max
X′
i−1Hy=0
y̸=0
y′H′AHy
y′y
≥λi(H′AH),

124
EIGENVALUES AND EIGENVECTORS
where the first equality follows from Theorem 3.17 and the final inequality follows
from Corollary 3.18.1.
□
Theorem 3.19 can be used to prove Theorem 3.20.
Theorem 3.20
Let A be an m × m symmetric matrix, and let Ak be its leading
k × k principal submatrix; that is, Ak is the matrix obtained by deleting the last m −k
rows and columns of A. Then, for i = 1, . . . , k,
λm−i+1(A) ≤λk−i+1(Ak) ≤λk−i+1(A).
Theorem 3.21, sometimes referred to as Weyl’s Theorem, gives inequalities relat-
ing the eigenvalues of two symmetric matrices to those of the sum of the matrices.
Theorem 3.21
Let A and B
be m × m symmetric matrices. Then for
h = 1, . . . , m,
λh(A) + λm(B) ≤λh(A + B) ≤λh(A) + λ1(B).
Proof.
Let Bh be an m × (h −1) matrix satisfying B′
hBh = Ih−1. Then using
(3.9), we have
λh(A + B) = min
Bh
max
B′
hx=0
x̸=0
x′(A + B)x
x′x
= min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x + x′Bx
x′x

≥min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x + λm(B)

= min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x

+ λm(B)
= λh(A) + λm(B),
where the inequality was introduced from an application of (3.6), whereas the final
equality used (3.9). The upper bound is obtained in a similar manner by using
(3.10).
□
The inequalities given in Theorem 3.21 can be generalized. Before obtaining these
generalized inequalities, we first give some inequalities relating the eigenvalues of
A + B to those of A when we have some information about the rank of B.

ADDITIONAL RESULTS CONCERNING EIGENVALUES OF SYMMETRIC MATRICES
125
Theorem 3.22
Let A and B be m × m symmetric matrices, and suppose that
rank(B) ≤r. Then for h = 1, . . . , m −r,
(a) λh+r(A) ≤λh(A + B),
(b) λh+r(A + B) ≤λh(A).
Proof.
Since B is symmetric and has rank at most r, it can be expressed as
B =
r

i=1
γiyiy′
i,
where y1, . . . , yr are orthonormal vectors. Let Bh and Bh+r be m × (h −1) and
m × (h + r −1) matrices satisfying B′
hBh = Ih−1 and B′
h+rBh+r = Ih+r−1, and
define Yr = (y1, . . . , yr) and B∗= (Bh, Yr). Using (3.9), if h = 1, . . . , m −r, we
have
λh(A + B) = min
Bh
max
B′
hx=0
x̸=0
x′(A + B)x
x′x
≥min
Bh
max
B′
hx=0,Y ′rx=0
x̸=0
x′(A + B)x
x′x
= min
Bh max
B′∗x=0
x̸=0
x′Ax
x′x
=
min
B′
hYr=(0) max
B′∗x=0
x̸=0
x′Ax
x′x
≥min
Bh+r
max
B′
h+rx=0
x̸=0
x′Ax
x′x
= λh+r(A)
The third equality can be justified as follows. First, the minimum can be restricted to
Bh for which B∗is full rank, because if B∗is not full rank, then {x : B′
∗x = 0} will
contain subspaces of the form {x : B′
∗x = 0} for choices of Bh for which B∗is full
rank. Secondly, for choices of Bh for which B∗is full rank, the null space of B′
∗, {x :
B′
∗x = 0}, will be identical to the null space of B′
∗for a choice of Bh that has B′
hYr =
(0). This establishes (a). We obtain (b) in a similar fashion by using (3.10). Let Ch and
Ch+r be m × (m −h) and m × (m −h −r) matrices satisfying C′
hCh = Im−h and
C′
h+rCh+r = Im−h−r, and define C∗= (Ch+r, Yr). Then for h = 1, . . . , m −r,
λh+r(A + B) = max
Ch+r
min
C′
h+rx=0
x̸=0
x′(A + B)x
x′x

126
EIGENVALUES AND EIGENVECTORS
≤max
Ch+r
min
C′
h+rx=0,Y ′rx=0
x̸=0
x′(A + B)x
x′x
= max
Ch+r min
C′∗x=0
x̸=0
x′Ax
x′x
=
max
C′
h+rYr=(0) min
C′∗x=0
x̸=0
x′Ax
x′x
≤max
Ch
min
C′
hx=0
x̸=0
x′Ax
x′x
= λh(A)
and so the proof is complete.
□
We are now ready to give a generalization of the inequalities given in Theorem
3.21.
Theorem 3.23
Let A and B be m × m symmetric matrices, and let h and i be
integers between 1 and m inclusive. Then
(a) λh+i−1(A + B) ≤λh(A) + λi(B), if h + i ≤m + 1,
(b) λh+i−m(A + B) ≥λh(A) + λi(B), if h + i ≥m + 1.
Proof.
Let x1, . . . , xm be a set of orthonormal eigenvectors corresponding to
the eigenvalues λ1(A) ≥· · · ≥λm(A) and y1, . . . , ym be a set of orthonor-
mal eigenvectors corresponding to the eigenvalues λ1(B) ≥· · · ≥λm(B). If
we define Ah = 	h−1
j=1 λj(A)xjx′
j
and Bi = 	i−1
j=1 λj(B)yjy′
j, then clearly
rank(Ah) ≤h −1, rank(Bi) ≤i −1, and rank(Ah + Bi) ≤h + i −2. Thus,
applying Theorem 3.22(a) with h = 1 and r = h + i −2, we have
λ1(A −Ah + B −Bi) = λ1((A + B) −(Ah + Bi))
≥λ1+h+i−2(A + B)
= λh+i−1(A + B).
(3.15)
Also
λ1(A −Ah + B −Bi) ≤λ1(A −Ah) + λ1(B −Bi)
(3.16)
from Theorem 3.21. Note that
λ1(A −Ah) = λh(A),
λ1(B −Bi) = λi(B)
(3.17)

ADDITIONAL RESULTS CONCERNING EIGENVALUES OF SYMMETRIC MATRICES
127
because
A −Ah =
m

j=h
λj(A)xjx′
j,
B −Bi =
m

j=i
λj(B)yjy′
j.
Combining (3.15), (3.16), and (3.17), we get
λh(A) + λi(B) = λ1(A −Ah) + λ1(B −Bi)
≥λ1(A −Ah + B −Bi)
= λ1((A + B) −(Ah + Bi))
≥λh+i−1(A + B),
so that we have proven the inequality given in (a). The inequality in (b) can be
obtained by applying the inequality in (a) to −A and −B; that is,
λh+i−1(−A −B) ≤λh(−A) + λi(−B),
which can be re-expressed as
−λm−(h+i−1)+1(A + B) ≤−λm−h+1(A) −λm−i+1(B),
or equivalently,
λm−h−i+2(A + B) ≥λm−h+1(A) + λm−i+1(B).
The last inequality is identical to the one given in (b) because if we let k = m −h + 1
and l = m −i + 1, then k + l −m = m −h −i + 2.
□
The inequalities given in the preceding theorems can be used to obtain bounds on
sums of eigenvalues. For instance, from Theorem 3.21 it immediately follows that
k

h=1
λh(A) + kλm(B) ≤
k

h=1
λh(A + B) ≤
k

h=1
λh(A) + kλ1(B).
Theorem 3.24, which is due to Wielandt (1955), provides tighter bounds on the sums
of eigenvalues of A + B.
Theorem 3.24
Let A and B be m × m symmetric matrices, and let i1, . . . , ik be
integers satisfying 1 ≤i1 < · · · < ik ≤m. Then for k = 1, . . . , m,
k

j=1
{λij(A) + λm−k+j(B)} ≤
k

j=1
λij(A + B) ≤
k

j=1
{λij(A) + λj(B)}.

128
EIGENVALUES AND EIGENVECTORS
Proof.
Note that it follows from Corollary 3.18.2 that there are particular matrices
Ci1, . . . , Cik, such that Cij is m × (m −ij), C′
ijCij = Im−ij, CihC′
ihCij = Cij
for h = 1, . . . , j, and
k

j=1
λij(A + B) =
min
C′
i1x1=···=C′
ik xk=0
x1̸=0, ... ,xk̸=0
x′
hxl=0,h̸=l
k

j=1
x′
j(A + B)xj
x′
jxj
.
(3.18)
Let y1, . . . , yk be m × 1 unit vectors such that y′
hyl = 0 for h ̸= l, C′
i1y1 = · · · =
C′
ikyk = 0, and
k

j=1
y′
jAyj =
min
C′
i1x1=···=C′
ik xk=0
x1̸=0, ... ,xk̸=0
x′
hxl=0,h̸=l
k

j=1
x′
jAxj
x′
jxj
.
(3.19)
It follows from (3.18) that
k

j=1
λij(A + B) ≤
k

j=1
y′
j(A + B)yj
=
k

j=1
y′
jAyj +
k

j=1
y′
jByj.
(3.20)
Since the yj’s were chosen to satisfy (3.19), a direct application of Corollary 3.18.2
yields
k

j=1
y′
jAyj ≤
k

j=1
λij(A).
(3.21)
Let yk+1, . . . , ym be unit vectors such that y1, . . . , ym is an orthonormal set of
vectors, and define the m × (m −i) matrix C∗i = (yi+1, . . . , ym) for i = 1, . . . , k.
Then it follows that
k

j=1
y′
jByj =
min
C′
∗1x1=···=C′
∗kxk=0
x1̸=0, ... ,xk̸=0
x′
hxl=0,h̸=l
k

j=1
x′
jBxj
x′
jxj
,
and so another application of Corollary 3.18.2 leads to
k

j=1
y′
jByj ≤
k

j=1
λj(B).
(3.22)

NONNEGATIVE DEFINITE MATRICES
129
Using (3.21) and (3.22) in (3.20), we then get the required upper bound. The lower
bound is established in a similar fashion by using the min–max identity given in
Corollary 3.18.2.
□
Many applications utilizing Theorem 3.24 will involve the sum of the k largest
eigenvalues of A + B. This special case of Theorem 3.24 is highlighted in the fol-
lowing corollary.
Corollary 3.24.1
Let A and B be m × m symmetric matrices. Then for
k = 1, . . . , m,
k

i=1
λi(A) +
k

i=1
λm−k+i(B) ≤
k

i=1
λi(A + B) ≤
k

i=1
λi(A) +
k

i=1
λi(B).
Some additional results regarding eigenvalues can be found in Bellman (1970) and
Horn and Johnson (2013).
3.8
NONNEGATIVE DEFINITE MATRICES
In Chapter 1, the conditions for a symmetric matrix A to be a positive definite or
positive semidefinite matrix were given in terms of the possible values of the quadratic
form x′Ax. We now show that these conditions also can be expressed in terms of the
eigenvalues of A.
Theorem 3.25
Let λ1, . . . , λm be the eigenvalues of the m × m symmetric matrix
A. Then
(a) A is positive definite if and only if λi > 0 for all i,
(b) A is positive semidefinite if and only if λi ≥0 for all i and λi = 0 for at least
one i.
Proof.
Let the columns of X = (x1, . . . , xm) be a set of orthonormal eigenvec-
tors of A corresponding to the eigenvalues λ1, . . . , λm, so that A = XΛX′, where
Λ = diag(λ1, . . . , λm). If A is positive definite, then x′Ax > 0 for all x ̸= 0, so in
particular, choosing x = xi, we have
x′
iAxi = x′
i(λixi) = λix′
ixi = λi > 0.
Conversely, if λi > 0 for all i, then for any x ̸= 0 define y = X′x, and note that
x′Ax = x′XΛX′x = y′Λy =
m

i=1
y2
i λi
(3.23)

130
EIGENVALUES AND EIGENVECTORS
has to be positive because the λi’s are positive and at least one of the y2
i ’s is positive
because y ̸= 0. This proves (a). By a similar argument, we find that A is nonnegative
definite if and only if λi ≥0 for all i. Thus, to prove (b), we only need to prove that
x′Ax = 0 for some x ̸= 0 if and only if at least one λi = 0. It follows from (3.23)
that if x′Ax = 0, then λi = 0 for every i for which y2
i > 0. On the other hand, if for
some i, λi = 0, then x′
iAxi = λi = 0.
□
Since a square matrix is singular if and only if it has a zero eigenvalue, it fol-
lows immediately from Theorem 3.25 that positive definite matrices are nonsingular,
whereas positive semidefinite matrices are singular.
Example 3.14
Consider the ordinary least squares estimator ˆβ = (X′X)−1X′y of
β in the model
y = Xβ + ϵ,
where β
is (k + 1) × 1, E(ϵ) = 0 and var(ϵ) = σ2IN.
For an arbitrary
(k + 1) × 1 vector c, we will prove that c′ˆβ is the best linear unbiased estimator of
c′β; an estimator t is an unbiased estimator of c′β if E(t) = c′β. Clearly, c′ˆβ is
unbiased because E(ϵ) = 0 implies that
E(c′ˆβ) = c′(X′X)−1X′E(y)
= c′(X′X)−1X′Xβ
= c′β.
To show that c′ˆβ is the best linear unbiased estimator, we must show that it has vari-
ance at least as small as the variance of any other linear unbiased estimator of c′β.
Let a′y be an arbitrary linear unbiased estimator of c′β, so that
c′β = E(a′y) = a′E(y) = a′Xβ,
regardless of the value of the vector β. However, this implies that
c′ = a′X.
Now we saw in Example 3.7 that var(ˆβ) = σ2(X′X)−1, so
var(c′ˆβ) = c′{var(ˆβ)}c = c′{σ2(X′X)−1}c
= σ2a′X(X′X)−1X′a,
whereas
var(a′y) = a′{var(y)}a = a′{σ2IN}a = σ2a′a.

NONNEGATIVE DEFINITE MATRICES
131
Thus, the difference in their variances is
var(a′y) −var(c′ˆβ) = σ2a′a −σ2a′X(X′X)−1X′a
= σ2a′(IN −X(X′X)−1X′)a.
However,
{IN −X(X′X)−1X′}2 = {IN −X(X′X)−1X′},
and so using Theorem 3.4, we find that each of the eigenvalues of IN −
X(X′X)−1X′
must be 0 or 1. Thus, from Theorem 3.25, we see that
IN −X(X′X)−1X′ is nonnegative definite, and so
var(a′y) −var(c′ˆβ) ≥0,
as is required.
Symmetric matrices are often obtained as the result of a transpose product; that
is, if T is an m × n matrix, then both T ′T and TT ′ are symmetric matrices. The
following two theorems show that their eigenvalues are nonnegative and their positive
eigenvalues are equal.
Theorem 3.26
Let T be an m × n matrix with rank(T) = r. Then T ′T has r pos-
itive eigenvalues. It is positive definite if r = n and positive semidefinite if r < n.
Proof.
For any nonnull n × 1 vector x, let y = Tx. Then clearly
x′T ′Tx = y′y =
m

i=1
y2
i
is nonnegative, so T ′T is nonnegative definite, and thus, by Theorem 3.25, all of its
eigenvalues are nonnegative. If x is an eigenvector of T ′T corresponding to a zero
eigenvalue, then the equation above must equal zero, and this can only happen if
y = Tx = 0. Since rank(T) = r, we can find a set of n −r linearly independent
x’s satisfying Tx = 0, that is, any basis of the null space of T, and so the number of
zero eigenvalues of T ′T is equal to n −r. The result now follows.
□
Theorem 3.27
Let T be an m × n matrix, with rank(T) = r. Then the positive
eigenvalues of T ′T are equal to the positive eigenvalues of TT ′.
Proof.
Let λ > 0 be an eigenvalue of T ′T with multiplicity h. Since the n × n
matrix T ′T is symmetric, we can find an n × h matrix X, whose columns are
orthonormal, satisfying
T ′TX = λX.

132
EIGENVALUES AND EIGENVECTORS
Let Y = TX and observe that
TT ′Y = TT ′TX = T(λX) = λTX = λY ,
so that λ is also an eigenvalue of TT ′. Its multiplicity is also h because
rank(Y ) = rank(TX) = rank((TX)′TX)
= rank(X′T ′TX) = rank(λX′X)
= rank(λIh) = h,
and so the proof is complete.
□
Example 3.15
In multivariate multiple regression, we have multiple explanatory
variables, x1, . . . , xk, as in the standard multiple regression model described in
Example 2.11, but the response y = (y1, . . . , ym)′ is a random vector instead of a
random variable. The model is
y = B′x + ϵ,
where B is k × m, x = (x1, . . . , xk)′, and the m × 1 vector ϵ denotes a random
error. If we have N observations of the response vector, y1, . . . , yN, and N corre-
sponding explanatory vectors, x1, . . . , xN, the model can be written as
Y = XB + E,
where Y ′ = (y1, . . . , yN), X′ = (x1, . . . , xN), and E is an N × m matrix
containing error terms. The least squares estimator of B is the k × m matrix
which minimizes the sum of squares of the matrix (Y −XB0), that is, minimizes
tr{(Y −XB0)′(Y −XB0)}, over all choices for B0. We now show that this least
squares estimator is ˆB = (X′X)−1X′Y when X has full column rank. Note that
(Y −XB0)′(Y −XB0)
= (Y −X ˆB + X ˆB −XB0)′(Y −X ˆB + X ˆB −XB0)
= {Y −X ˆB + X( ˆB −B0)}′{Y −X ˆB + X( ˆB −B0)}
= (Y −X ˆB)′(Y −X ˆB) + ( ˆB −B0)′X′X( ˆB −B0)
+ (Y −X ˆB)′X( ˆB −B0) + ( ˆB −B0)′X′(Y −X ˆB)
= (Y −X ˆB)′(Y −X ˆB) + ( ˆB −B0)′X′X( ˆB −B0),
since X′(Y −X ˆB) = X′{Y −X(X′X)−1X′Y } = (X′ −X′)Y = (0). Thus,
tr{(Y −XB0)′(Y −XB0)} ≥tr{(Y −X ˆB)′(Y −X ˆB)},
since ( ˆB −B0)′X′X( ˆB −B0) is nonnegative definite.

NONNEGATIVE DEFINITE MATRICES
133
Next we will use the Courant–Fischer min–max theorem to prove the following
important monotonicity property of the eigenvalues of symmetric matrices.
Theorem 3.28
Let A be an m × m symmetric matrix and B be an m × m nonneg-
ative definite matrix. Then, for h = 1, . . . , m, we have
λh(A + B) ≥λh(A),
where the inequality is strict if B is positive definite.
Proof.
For an arbitrary m × (h −1) matrix Bh satisfying B′
hBh = Ih−1, we have
max
B′
hx=0
x̸=0
x′(A + B)x
x′x
= max
B′
hx=0
x̸=0
x′Ax
x′x + x′Bx
x′x

≥max
B′
hx=0
x̸=0
x′Ax
x′x + min
B′
hx=0
x̸=0
x′Bx
x′x
≥max
B′
hx=0
x̸=0
x′Ax
x′x + min
x̸=0
x′Bx
x′x
= max
B′
hx=0
x̸=0
x′Ax
x′x + λm(B)
≥max
B′
hx=0
x̸=0
x′Ax
x′x ,
where the last equality follows from Theorem 3.16. The final inequality is strict if B
is positive definite because, in this case, λm(B) > 0. Now minimizing both sides of
the equation above over all choices of Bh satisfying B′
hBh = Ih−1 and using (3.9)
of Theorem 3.18, we get
λh(A + B) = min
Bh
max
B′
hx=0
x̸=0
x′(A + B)x
x′x
≥min
Bh
max
B′
hx=0
x̸=0
x′Ax
x′x = λh(A).
This completes the proof.
□
Note that there is not a general bounding relationship between λh(A + B) and
λh(A) + λh(B). For instance, if A = diag(1, 2, 3, 4) and B = diag(8, 6, 4, 2), then
λ2(A + B) = 8 < λ2(A) + λ2(B) = 3 + 6 = 9,

134
EIGENVALUES AND EIGENVECTORS
whereas
λ3(A + B) = 7 > λ3(A) + λ3(B) = 2 + 4 = 6.
In Example 3.12, we discussed a situation in which the eigenvalues and eigenvec-
tors of
A =
k

i=1
(μi −μ)(μi −μ)′
were used in analyzing differences among the group means μ1, . . . , μk. For instance,
an eigenvector x1, corresponding to the largest eigenvalue of A, gives the direction
of maximum dispersion among the group means in that
x′
1Ax1
x′
1x1
is maximized. The division here by x′
1x1, which removes the effect of scale, may not
be appropriate if the groups have covariance matrices other than the identity matrix.
Suppose, for example, that each group has the same covariance matrix B. If y is a
random vector with covariance matrix B, then the variability of y in the direction
given by x will be var(x′y) = x′Bx. Since differences among the groups in a direc-
tion with high variability will not be as important as similar differences in another
direction with low variability, we will adjust for these differences in variability by
constructing the ratio
x′Ax
x′Bx.
The vector x1 that maximizes this ratio will then identify the one-dimensional sub-
space of Rm in which the group means differ the most, when adjusting for differences
in variability. The next step after finding x1 would be to find the vector x2 that max-
imizes this ratio but has x′
2y uncorrelated with x′
1y; this would be the vector x2 that
maximizes the ratio above subject to the constraint that x′
1Bx2 = 0. Continuing in
this fashion, we would determine the m vectors x1, . . . , xm that yield the m extremal
values λ1, . . . , λm of the ratio. These extremal values are identified in the following
theorem.
Theorem 3.29
Let A and B
be m × m matrices, with A being sym-
metric
and
B
being
positive
definite.
Then
the
eigenvalues
of
B−1A,
λ1(B−1A) ≥· · · ≥λm(B−1A), are real and there exists a linearly indepen-
dent set of eigenvectors, x1, . . . , xm, corresponding to these eigenvalues.
In addition, if we define Xh = (x1, . . . , xh) and Yh = (xh, . . . , xm) for
h = 1, . . . , m, then
λh(B−1A) =
min
Y ′
h+1Bx=0
x̸=0
x′Ax
x′Bx

NONNEGATIVE DEFINITE MATRICES
135
and
λh(B−1A) =
max
X′
h−1B x=0
x̸=0
x′Ax
x′Bx ,
where the min and max are over all x ̸= 0 when h = m and h = 1, respectively.
Proof.
Let
B = PDP ′
be
the
spectral
decomposition
of
B,
so
that
D = diag(d1, . . . , dm),
where
the
eigenvalues
of
B,
d1, . . . , dm,
are
all
positive
because
of
Theorem
3.25.
If
we
let
T = PD1/2P ′,
where
D1/2 = diag(d1/2
1
, . . . , d1/2
m ), then B = TT = T 2 and T, like B, is symmet-
ric and nonsingular. Now it follows from Theorem 3.2(d) that the eigenvalues of
B−1A are the same as those of T −1AT −1, and these must be real because T −1AT −1 is
symmetric. Also because of its symmetry, T −1AT −1 must have an orthonormal set of
eigenvectors, y1, . . . , ym. Note that if we write λi = λi(B−1A) = λi(T −1AT −1),
then T −1AT −1yi = λiyi, so that
T −1T −1AT −1yi = λiT −1yi
or
B−1A(T −1yi) = λi(T −1yi).
Thus, xi = T −1yi is an eigenvector of B−1A corresponding to the eigenvalue
λi = λi(B−1A) and yi = Txi. Clearly the vectors x1, . . . , xm are linearly
independent because y1, . . . , ym are orthonormal. All that remains is to prove
the identities involving the minimum and maximum. We will just prove the result
involving the minimum; the proof for the maximum is similar. Putting y = Tx, we
find that
min
Y ′
h+1Bx=0
x̸=0
x′Ax
x′Bx =
min
Y ′
h+1T T x=0
x̸=0
x′TT −1AT −1Tx
x′TTx
=
min
Y ′
h+1T y=0
y̸=0
y′T −1AT −1y
y′y
.
(3.24)
Since the rows of Y ′
h+1T are the transposes of the eigenvectors Txh+1, . . . , Txm
of T −1AT −1, it follows from Theorem 3.17 that (3.24) equals λh(T −1AT −1), which
we have already established as being the same as λh(B−1A).
□
Note that if xi is an eigenvector of B−1A corresponding to the eigenvalue λi =
λi(B−1A), then
B−1Axi = λixi
or, equivalently,
Axi = λiBxi.
(3.25)

136
EIGENVALUES AND EIGENVECTORS
Equation (3.25) is similar to the eigenvalue-eigenvector equation of A, except for the
multiplication of xi by B on the right-hand side of the equation. The eigenvalues
satisfying (3.25) are sometimes referred to as the eigenvalues of A in the metric of
B. Note that if we premultiply (3.25) by x′
i and then solve for λi, we get
λi(B−1A) = x′
iAxi
x′
iBxi
;
that is, the extremal values given in Theorem 3.29 are attained at the eigenvectors of
B−1A.
The result given in Theorem 3.29 can be generalized just as the result in Theorem
3.17 was generalized to that given in Theorem 3.18.
Theorem 3.30
Let A and B be m × m matrices, with A being symmetric and B
being positive definite. For h = 1, . . . , m, let Bh be any m × (h −1) matrix and Ch
any m × (m −h) matrix satisfying, B′
hBh = Ih−1 and C′
hCh = Im−h. Then
λh(B−1A) = min
Bh
max
B′
h x=0
x̸=0
x′Ax
x′Bx ,
and
λh(B−1A) = max
Ch
min
C′
h x=0
x̸=0
x′Ax
x′Bx ,
where the inner max and min are over all x ̸= 0 when h = 1 and h = m, respectively.
The proof of Theorem 3.29 suggests a way of simultaneously diagonalizing the
matrices A and B. Since T −1AT −1 is a symmetric matrix, it can be expressed in
the form QΛQ′, where Q is an orthogonal matrix and Λ is the diagonal matrix
diag(λ1(T −1AT −1), . . . , λm(T −1AT −1)). The matrix C = Q′T −1 is nonsingular
because Q and T −1 are nonsingular, and
CAC′ = Q′T −1AT −1Q = Q′QΛQ′Q = Λ,
CBC′ = Q′T −1TTT −1Q = Q′Q = Im.
Equivalently, if G = C−1, we have A = GΛG′ and B = GG′. This simultaneous
diagonalization is useful in proving our next result in Theorem 3.31. For some other
related results, see Olkin and Tomsky (1981).
Theorem 3.31
Let A be an m × m symmetric matrix and B be an m × m positive
definite matrix. If F is any m × h matrix with full column rank, then for i = 1, . . . , h
λi((F ′BF)−1(F ′AF)) ≤λi(B−1A),

NONNEGATIVE DEFINITE MATRICES
137
and further
max
F
λi((F ′BF)−1(F ′AF)) = λi(B−1A).
Proof.
Note that the second equation implies the first, so our proof simply involves
the verification of the second equation. Let the nonsingular m × m matrix G be such
that B = GG′ and A = GΛG′, where Λ = diag(λ1(B−1A), . . . , λm(B−1A)). Then
max
F
λi((F ′BF)−1(F ′AF)) = max
F
λi((F ′GG′F)−1(F ′GΛG′F))
= max
E
λi((E′E)−1(E′ΛE)),
where this last maximization is also over all m × h matrices of rank h, because
E = G′F must have the same rank as F. Note that because E has rank h, the h × h
matrix E′E is a nonsingular symmetric matrix. As was seen in the proof of Theorem
3.29, such a matrix can be expressed as E′E = TT for some nonsingular symmetric
h × h matrix T. It then follows that
max
E
λi((E′E)−1(E′ΛE)) = max
E
λi((TT)−1(E′ΛE))
= max
E
λi(T −1E′ΛET −1),
where this last equality follows from Theorem 3.2(d). Now if we define the m × h
rank h matrix H = ET −1, then
H′H = T −1E′ET −1 = T −1TTT −1 = Ih.
Thus,
max
E
λi(T −1E′ΛET −1) = max
H λi(H′ΛH) = λi(B−1A),
where the final equality follows from Theorem 3.19 and the fact that equality is actu-
ally achieved with the choice of H′ = [Ih
(0)].
□
Example 3.16
Many multivariate analyses are simply generalizations or extensions
of corresponding univariate analyses. In this example, we begin with what is known
as the univariate one-way classification model in which we have independent samples
of a response y from k different populations or treatments, with a sample size of ni
from the ith population. The jth observation from the ith sample can be expressed as
yij = μi + ϵij,
where the μi’s are constants and the ϵij’s are independent and identically distributed
as N(0, σ2). Our goal is to determine if the μi’s are all the same; that is, we wish to
test the null hypothesis H0 : μ1 = · · · = μk against the alternative hypothesis H1 : at

138
EIGENVALUES AND EIGENVECTORS
least two of the μi’s differ. An analysis of variance compares (see Problem 2.45) the
variability between treatments,
SST =
k

i=1
ni(yi −y)2,
to the variability within treatments,
SSE =
k

i=1
ni

j=1
(yij −yi)2,
where
yi =
ni

j=1
yij/ni,
y =
k

i=1
niyi/n,
n =
k

i=1
ni.
SST is referred to as the sum of squares for treatment whereas SSE is called the sum
of squares for error. The hypothesis H0 is rejected if the statistic
F = SST/(k −1)
SSE/(n −k)
exceeds the appropriate quantile of the F distribution with k −1 and n −k degrees
of freedom. Now suppose that instead of obtaining the value of one response variable
for each observation, we obtain the values of m different response variables for each
observation. If yij is the m × 1 vector of responses obtained as the jth observation
from the ith treatment, then we have the multivariate one-way classification model
given by
yij = μi + ϵij,
where μi is an m × 1 vector of constants and ϵij ∼Nm(0, Ω), independently. Mea-
sures of the between treatment variability and within treatment variability are now
given by the matrices,
B =
k

i=1
ni(yi −y)(yi −y)′,
W =
k

i=1
ni

j=1
(yij −yi)(yij −yi)′.
One approach to testing the null hypothesis H0 : μ1 = · · · = μk against the
alternative hypothesis H1 : at least two of the μi’s differ, is by a method called
the union-intersection procedure. This technique is based on the following
decomposition of the hypotheses H0 and H1 into univariate hypotheses. If c is
any m × 1 vector, and we define the hypothesis H0(c) : c′μ1 = · · · = c′μk, then
the intersection of H0(c) over all c ∈Rm is the hypothesis H0. In addition, if we
define the hypothesis H1(c) : at least two of the c′μi’s differ, then the union of the
hypotheses H1(c) over all c ∈Rm is the hypothesis H1. Thus, we should reject

NONNEGATIVE DEFINITE MATRICES
139
the hypothesis H0 if and only if we reject H0(c) for at least one c. Now the null
hypothesis H0(c) involves the univariate one-way classification model in which
c′yij is the response, and so we would reject H0(c) for large values of the F statistic
F(c) = SST(c)/(k −1)
SSE(c)/(n −k),
where SST(c) and SSE(c) are the sums of squares for treatments and errors, respec-
tively, computed for the responses c′yij. Since H0 is rejected if H0(c) is rejected
for at least one c, we will reject H0 if F(c) is sufficiently large for at least one c or,
equivalently, if
max
c̸=0 F(c)
is sufficiently large. Omitting the constants (k −1) and (n −k) and noting that the
sums of squares SST(c) and SSE(c) can be expressed using B and W as
SST(c) = c′Bc,
SSE(c) = c′Wc,
we find that we reject H0 for large values of
max
c̸=0
c′Bc
c′Wc = λ1(W −1B),
(3.26)
where the right-hand side follows from Theorem 3.29. Thus, if u1−α is the (1 −α)th
quantile of the distribution of the largest eigenvalue λ1(W −1B) (see, for example,
Morrison, 2005) so that
P[λ1(W −1B) ≤u1−α|H0] = 1 −α,
(3.27)
then
we
would
reject
H0
if
λ1(W −1B) > u1−α.
One
advantage
of
the
union-intersection procedure is that it naturally leads to simultaneous confi-
dence intervals. It follows immediately from (3.26) and (3.27) that for any mean
vectors μ1, . . . , μk, with probability 1 −α, the inequality
k	
i=1
nic′{(yi −y) −(μi −μ)}{(yi −y) −(μi −μ)}′c
c′Wc
≤u1−α,
(3.28)
holds for all m × 1 vectors c, where
μ =
k

i=1
niμi/n.

140
EIGENVALUES AND EIGENVECTORS
Scheffé’s method (see, Scheffé, 1953, or Miller, 1981) can then be used on (3.28) to
yield the inequalities
k

i=1
m

j=1
aicjyij −



u1−αc′Wc
 k

i=1
a2
i/ni

≤
k

i=1
m

j=1
aicjμij
≤
k

i=1
m

j=1
aicjyij +



u1−αc′Wc
 k

i=1
a2
i/ni

,
which hold with probability 1 −α, for all m × 1 vectors c and all k × 1 vectors a
satisfying a′1k = 0.
The remaining results in this section relate the eigenvalues of a matrix product to
products of the eigenvalues of the individual matrices. Theorem 3.32, which is due
to Anderson and Das Gupta (1963), gives bounds for a single eigenvalue of a matrix
product.
Theorem 3.32
Let A be an m × m nonnegative definite matrix, and let B be an
m × m positive definite matrix. If i, j, and k are integers between 1 and m inclusive
and satisfying j + k ≤i + 1, then
(a) λi(AB) ≤λj(A)λk(B),
(b) λm−i+1(AB) ≥λm−j+1(A)λm−k+1(B).
Proof.
Let the columns of the m × (j −1) matrix H1 be orthonormal eigenvectors
of A corresponding to λ1(A), . . . , λj−1(A), and let the columns of the m × (k −1)
matrix H2 be orthonormal eigenvectors of B corresponding to λ1(B), . . . , λk−1(B).
Define the m × (j + k −2) matrix H as H = [H1
H2]. Then
λi(AB) ≤λj+k−1(AB)
≤max
H′x=0
x̸=0
x′Ax
x′B−1x
= max
H′x=0
x̸=0
x′Ax
x′x
x′x
x′B−1x
≤max
H′x=0
x̸=0
x′Ax
x′x max
H′x=0
x̸=0
x′x
x′B−1x

ANTIEIGENVALUES AND ANTIEIGENVECTORS
141
≤max
H′
1x=0
x̸=0
x′Ax
x′x max
H′
2x=0
x̸=0
x′x
x′B−1x
= λj(A)λk(B),
where the second inequality follows from Theorem 3.30 and the final equality follows
from Theorem 3.17. This establishes the inequality given in (a). The inequality in (b)
can be obtained from (a) as follows. Write A∗= TAT, where T is the symmetric
matrix satisfying B = T 2. Then A∗is nonnegative definite because A is. Applying (a)
to A∗and B−1 and using the fact that λi(A∗B−1) = λi(A) and λj(A∗) = λj(AB),
we get
λi(A) ≤λj(AB)λk(B−1).
Since λk(B−1) = λ−1
m−k+1(B), this leads to
λj(AB) ≥λi(A)λm−k+1(B).
(3.29)
For each (i, j, k) satisfying the constraint given in the theorem so also will (i∗, j∗, k),
where i∗= m −j + 1 and j∗= m −i + 1. Making these substitutions in (3.29)
yields (b).
□
Our next result, due to Lidskiˇi (1950), gives a bound for a product of eigenvalues
of a matrix product. For a proof of this result, see Zhang (2011).
Theorem 3.33
Let A and B be m × m nonnegative definite matrices. If i1, . . . , ik
are integers satisfying 1 ≤i1 < · · · < ik ≤m, then
k

j=1
λij(AB) ≤
k

j=1
λij(A)λj(B),
for k = 1, . . . , m, with equality for k = m.
Some additional inequalities for eigenvalues will be given in Sections 7.6
and 10.2.
3.9
ANTIEIGENVALUES AND ANTIEIGENVECTORS
Consider an m × m positive definite matrix A and an m × 1 nonnull vector x. If
y = Ax, it follows from the Cauchy-Schwarz inequality that
x′Ax = x′y ≤

(x′x)(y′y) =

(x′x)(x′A2x),

142
EIGENVALUES AND EIGENVECTORS
with equality if and only if one of the vectors, x and y, is a scalar multiple of the
other, that is, Ax = λx for some scalar λ. Another way of stating this is that the
function
ψ(x) =
x′Ax

(x′x)(x′A2x)
has a maximum value of 1, which is attained if and only if x is an eigenvector of A.
This is not surprising since ψ(x) is cos θ, where θ is the angle between x and Ax,
and clearly the eigenvectors of A minimize this angle.
While the eigenvectors of A maximize ψ(x), the vectors that minimize ψ(x) are
known as the antieigenvectors of A. The notion of antieigenvalues and antieigenvec-
tors was originated by Gustafson (1972).
Definition 3.2
Let A be an m × m positive definite matrix and x be an m × 1
vector. Then
μ1 = min
x̸=0
x′Ax

(x′x)(x′A2x)
is the first antieigenvalue of A and x is a corresponding antieigenvector if μ1 = ψ(x).
The quantity θ = cos−1(μ1) can be described as the largest turning angle of A
since it gives the maximum angle between x and Ax over all choices of x ̸= 0.
Let λ1 ≥· · · ≥λm > 0 be the eigenvalues of A and x1, . . . , xm corresponding
orthonormal eigenvectors. The following result shows that the first antieigenvalue
and corresponding normalized first antieigenvectors can be expressed in terms of
λ1, λm, x1, and xm.
Theorem 3.34
The m × m positive definite matrix A has first antieigenvalue
μ1 = 2

λ1λm
λ1 + λm
,
with corresponding normalized antieigenvectors given by
x1+ =

λm
λ1 + λm
1/2
x1 +

λ1
λ1 + λm
1/2
xm,
x1−=

λm
λ1 + λm
1/2
x1 −

λ1
λ1 + λm
1/2
xm.

ANTIEIGENVALUES AND ANTIEIGENVECTORS
143
Proof.
Let A = XΛX′
be the spectral decomposition of A, and define
y = Λ1/2X′x, z = (y′y)−1/2y, and γi = λi/(λ1λm)1/2. Then
x′Ax

(x′x)(x′A2x)
=
(x′XΛ1/2)(Λ1/2X′x)

(x′XΛ1/2)Λ−1(Λ1/2X′x)(x′XΛ1/2)Λ(Λ1/2X′x)
=
y′y

(y′Λ−1y)(y′Λy)
=
1

(z′Λ−1z)(z′Λz)
=
1
 m
	
i=1
λ−1
i z2
i
  m
	
i=1
λiz2
i

=
1
 m
	
i=1
γ−1
i z2
i
  m
	
i=1
γiz2
i
.
(3.30)
Now since the function g(γ) = γ + γ−1 has positive second derivative and g(γ1) =
g(γm), it follows that g(γj) ≤g(γ1) for j = 2, . . . , m −1, and so
1
2
 m

i=1
γiz2
i +
m

i=1
γ−1
i z2
i

= 1
2
m

i=1
(γi + γ−1
i )z2
i
≤1
2
m

i=1
(γ1 + γ−1
1 )z2
i
= 1
2(γ1 + γ−1
1 ) = 1
2(γ1 + γm).
(3.31)
An application of the arithmetic-geometric mean inequality (see Section 10.6) yields
1
2
 m

i=1
γiz2
i +
m

i=1
γ−1
i z2
i

≥
 m

i=1
γiz2
i
  m

i=1
γ−1
i z2
i
1/2
.
(3.32)
Combining (3.31) and (3.32), we have
 m

i=1
γiz2
i
  m

i=1
γ−1
i z2
i
1/2
≤1
2(γ1 + γm) = 1
2

λ1 + λm

λ1λm

= μ−1
1 ,
which establishes μ1 as a lower bound for (3.30). This bound is attained if we have
equality in (3.31) and (3.32), so that z2
1 = z2
m = 1
2 or y1 = ym, y2 = · · · = ym−1 = 0.

144
EIGENVALUES AND EIGENVECTORS
Consequently, the bound is attained when
x = XΛ−1/2y =
y1

λ1
x1 +
y1

λm
xm.
If λ1 > λ2 and λm−1 > λm, there are only two linearly independent vectors of this
form, which, when normalized, can be expressed as x1+ and x1−.
□
Additional antieigenvalues and associated antieigenvectors can be defined as fol-
lows. Let Xk = (x1, . . . , xk−1, xm−k+2, . . . , xm). Then for k = 2, . . . , r, where
r = m/2 if m is even and r = (m −1)/2 if m is odd, the kth antieigenvalue of A is
μk = min
X′
kx=0
x̸=0
x′Ax

(x′x)(x′A2x)
,
with associated antieigenvectors given by any x satisfying ψ(x) = μk. The proof of
the following result, which is omitted, is similar to the proof of Theorem 3.34.
Theorem 3.35
The m × m positive definite matrix A has kth antieigenvalue
μk = 2λkλm−k+1
λk + λm−k+1
,
for k = 2, . . . , r. Corresponding normalized antieigenvectors are given by
xk+ =

λm−k+1
λk + λm−k+1
1/2
xk +

λk
λk + λm−k+1
1/2
xm−k+1,
xk−=

λm−k+1
λk + λm−k+1
1/2
xk −

λk
λk + λm−k+1
1/2
xm−k+1.
Applications of antieigenvalues and antieigenvectors in statistics can be found in
Khattree (2003), Rao (2005), and Gustafson (2006).
PROBLEMS
3.1 Consider the 3 × 3 matrix
A =
⎡
⎣
9
−3
−4
12
−4
−6
8
−3
−3
⎤
⎦.
(a) Find the eigenvalues of A.

PROBLEMS
145
(b) Find a normalized eigenvector corresponding to each eigenvalue.
(c) Find tr(A10).
3.2 Consider the 3 × 3 matrix A given in Problem 3.1 and the vector x = (3, 3, 4)′.
(a) Show that x can be written as a linear combination of the eigenvectors of
A.
(b) Find A10x without actually computing A10.
3.3 Find the eigenvalues of A′, where A is the matrix given in Problem 3.1. Deter-
mine the eigenspaces for A′, and compare these with those of A.
3.4 Let the 3 × 3 matrix A be given by
A =
⎡
⎣
1
−2
0
1
4
0
0
0
2
⎤
⎦.
(a) Find the eigenvalues of A.
(b) For each different value of λ, determine the associated eigenspace
SA(λ).
(c) Describe the eigenspaces obtained in part (b).
3.5 Consider the 4 × 4 matrix
A =
⎡
⎢⎢⎣
0
0
2
0
1
0
1
0
0
1
−2
0
0
0
0
1
⎤
⎥⎥⎦.
(a) Find the eigenvalues of A.
(b) Find the eigenspaces of A.
3.6 If the m × m matrix A has eigenvalues λ1, . . . , λm and corresponding eigen-
vectors x1, . . . , xm, show that the matrix (A + γIm) has eigenvalues λ1 +
γ, . . . , λm + γ and corresponding eigenvectors x1, . . . , xm.
3.7 In Example 3.7, we discussed the use of principal components regression as a
way of overcoming the difficulties associated with multicollinearity. Another
approach, called ridge regression, replaces the ordinary least squares estimator
in the standardized model, ˆδ1 = (Z′
1Z1)−1Z′
1y by ˆδ1γ = (Z′
1Z1 + γIk)−1Z′
1y,
where γ is a small positive number. This adjustment will reduce the impact of
the near singularity of Z′
1Z1 because the addition of γIk increases each of the
eigenvalues of Z′
1Z1 by γ.
(a) Show that if N ≥2k + 1, there is an N × k matrix W, such that ˆδ1γ is the
ordinary least squares estimate of δ1 in the model
y = δ01N + (Z1 + W)δ1 + ϵ;

146
EIGENVALUES AND EIGENVECTORS
that is, ˆδ1γ can be viewed as the ordinary least squares estimator of δ1
after we have perturbed the matrix of values for the explanatory variables
Z1 by W.
(b) Show that a k × k matrix U exists, such that ˆδ1γ is the ordinary least
squares estimate of δ1 in the model

y
0

=

δ01N
0

+

Z1
U

δ1 +

ϵ
ϵ∗

,
where 0 is a k × 1 vector of zeros and ϵ∗∼Nk(0, σ2Ik), independently
of ϵ . Thus, the ridge regression estimator also can be viewed as the least
squares estimator obtained after adding k observations, each having zero
for the response variable and the small values in U as the values for the
explanatory variables.
3.8 Refer to Example 3.7 and the previous exercise.
(a) Find the expected values of the principal components regression estimator,
ˆδ1∗, and the ridge regression estimator, ˆδ1γ, thereby showing that each is a
biased estimator of δ1.
(b) Find the covariance matrix of ˆδ1∗and show that var(ˆδ1) −var(ˆδ1∗) is a
nonnegative definite matrix, where ˆδ1 is the ordinary least squares estimator
of δ1.
(c) Find the covariance matrix of ˆδ1γ and show that tr{var(ˆδ1) −var(ˆδ1γ)} is
nonnegative.
3.9 If A and B are m × m matrices and at least one of them is nonsingular, show
that the eigenvalues of AB and BA are the same.
3.10 If λ is a real eigenvalue of the m × m real matrix A, show that there exist real
eigenvectors of A corresponding to the eigenvalue λ.
3.11 For some angle θ, consider the 2 × 2 matrix
P =
cos θ
−sin θ
sin θ
cos θ

.
(a) Show that P is an orthogonal matrix.
(b) Find the eigenvalues of P.
3.12 Suppose that A is m × n and B is n × m. Show that if ABx = λx, where
λ ̸= 0 and x ̸= 0, then Bx is an eigenvector of BA corresponding to λ. Thus,
show that AB and BA have the same nonzero eigenvalues by showing that the
number of linearly independent eigenvectors of AB corresponding to λ ̸= 0 is
the same as the number of linearly independent eigenvectors of BA correspond-
ing to λ.
3.13 Prove the results given in Theorem 3.2.
3.14 Suppose A is an m × m skew symmetric matrix. Show that each eigenvalue of
A is zero or a pure imaginary number; that is, each eigenvalue is of the form
0 + bi for some scalar b.

PROBLEMS
147
3.15 We know from Theorem 3.2(d) that if m × m matrices A and C satisfy C =
BAB−1 for some nonsingular matrix B, then A and C have the same eigenval-
ues. Show by example that the converse is not true; that is, find matrices A and
C that have the same eigenvalues, but do not satisfy C = BAB−1 for any B.
3.16 Suppose that λ is a simple eigenvalue of the m × m matrix A. Show that
rank(A −λIm) = m −1.
3.17 If A is an m × m matrix and rank(A −λIm) = m −1, show that λ is an eigen-
value of A with multiplicity of at least one.
3.18 Let A be an m × m matrix.
(a) Show that if A is nonnegative definite, then A2 is also nonnegative definite.
(b) Show that if A is positive definite, then A−1 is positive definite.
3.19 Consider the m × m matrix
A =
⎡
⎢⎢⎢⎢⎢⎣
1
1
0
· · ·
0
0
1
1
· · ·
0
...
...
...
...
0
0
0
· · ·
1
0
0
0
· · ·
1
⎤
⎥⎥⎥⎥⎥⎦
,
which has each element on and directly above the diagonal equal to 1. Find the
eigenvalues and eigenvectors of A.
3.20 Let x and y be m × 1 vectors.
(a) Find the eigenvalues and eigenvectors of the matrix xy′.
(b) Show that if c = 1 + x′y ̸= 0, then Im + xy′ has an inverse and
(Im + xy′)−1 = Im −c−1xy′.
3.21 Let A be an m × m nonsingular matrix with eigenvalues λ1, . . . , λm and cor-
responding eigenvectors x1, . . . , xm. If Im + A is nonsingular, find the eigen-
values and eigenvectors of
(a) (Im + A)−1,
(b) A + A−1,
(c) Im + A−1.
3.22 Suppose that A is an m × m nonsingular matrix and the sum of the elements in
each row of A is 1. Show that the row sums of A−1 are also 1.
3.23 Let the m × m nonsingular matrix A be such that Im + A is nonsingular, and
define
B = (Im + A)−1 + (Im + A−1)−1.
(a) Show that if x is an eigenvector of A corresponding to the eigenvalue λ,
then x is an eigenvector of B corresponding to the eigenvalue 1.
(b) Use Theorem 1.9 to show that B = Im.

148
EIGENVALUES AND EIGENVECTORS
3.24 Consider the 2 × 2 matrix
A =
4
2
3
5

.
(a) Find the characteristic equation of A.
(b) Illustrate Theorem 3.8 by substituting A for λ in the characteristic equation
obtained in (a) and then showing the resulting matrix is the null matrix.
(c) Rearrange the matrix polynomial equation in (b) to obtain an expression for
A2 as a linear combination of A and Im.
(d) In a similar fashion, write A3 and A−1 as linear combinations of A and Im.
3.25 Consider the general 2 × 2 matrix
A =

a11
a12
a21
a22

.
(a) Find the characteristic equation of A.
(b) Obtain expressions for the two eigenvalues of A in terms of the elements
of A.
(c) When will these eigenvalues be real?
3.26 If A is m × m, then a nonnull m × 1 vector x satisfying Ax = λx for some
scalar λ is more precisely referred to as a right eigenvector of A corresponding
to λ. An m × 1 vector y satisfying y′A = μy′ is referred to as a left eigenvector
of A corresponding to μ. Show that if λ ̸= μ, then x is orthogonal to y.
3.27 Find the eigenvalues and eigenvectors of the matrix 1m1′
m.
3.28 A 3 × 3 matrix A has eigenvalues 1, 2, and 3, and corresponding eigenvectors
(1, 1, 1)′, (1, 2, 0)′, and (2, −1, 6)′. Find A.
3.29 Consider the m × m matrix A = αIm + β1m1′
m, where α and β are scalars.
(a) Find the eigenvalues and eigenvectors of A.
(b) Determine the eigenspaces and associated eigenprojections of A.
(c) For which values of α and β will A be nonsingular?
(d) Using (a), show that when A is nonsingular, then
A−1 = α−1Im −
β
α(α + mβ)1m1′
m.
(e) Show that the determinant of A is αm−1(α + mβ).
3.30 Consider the m × m matrix A = αIm + βcc′, where α and β are scalars and
c ̸= 0 is an m × 1 vector.
(a) Find the eigenvalues and eigenvectors of A.
(b) Find the determinant of A.

PROBLEMS
149
(c) Give conditions for A to be nonsingular and find an expression for the
inverse of A.
3.31 Let A be the 3 × 3 matrix given by
A =
⎡
⎣
2
−1
0
−1
1
1
0
1
2
⎤
⎦.
(a) Find the eigenvalues and associated normalized eigenvectors of A.
(b) What is the rank of A?
(c) Find the eigenspaces and associated eigenprojections of A.
(d) Find tr(A4).
3.32 Construct a 3 × 3 symmetric matrix having eigenvalues 18, 21, and 28, and
corresponding eigenvectors (1, 1, 2)′, (4, −2, −1)′, and (1, 3, −2)′.
3.33 Show that if A is an m × m symmetric matrix with eigenvalues λ1, . . . , λm,
then
m

i=1
m

j=1
a2
ij =
m

i=1
λ2
i.
3.34 Show that the matrix A = (1 −ρ)Im + ρ1m1′
m is positive definite if and only
if −(m −1)−1 < ρ < 1.
3.35 Show that if A is an m × m symmetric matrix with its eigenvalues equal to its
diagonal elements, then A must be a diagonal matrix.
3.36 Show that the converse of Theorem 3.28 is not true; that is, find symmetric
matrices A and B for which λi(A + B) ≥λi(A) for i = 1, . . . , m yet B is
not nonnegative definite.
3.37 Let A be an m × n matrix with rank(A) = r. Use the spectral decomposition
of A′A to show that an n × (n −r) matrix X exists, such that
AX = (0)
and
X′X = In−r.
In a similar fashion, show that an (m −r) × m matrix Y exists, such that
Y A = (0)
and
Y Y ′ = Im−r.
3.38 Let A be the 2 × 3 matrix given by
A =
6
4
4
3
2
2

.
Find matrices X and Y satisfying the conditions given in the previous exercise.
3.39 An m × m matrix A is said to be nilpotent if Ak = (0) for some positive
integer k.

150
EIGENVALUES AND EIGENVECTORS
(a) Show that all of the eigenvalues of a nilpotent matrix are equal to 0.
(b) Find a matrix, other than the null matrix, that is nilpotent.
3.40 Complete the details of Example 3.11 by showing that
P1,n →
⎡
⎣
1
0
0
0
0
0
0
0
0
⎤
⎦,
P2,n →
⎡
⎣
0
0
0
0
1
0
0
0
1
⎤
⎦,
as n →∞.
3.41 Prove Corollary 3.18.2.
3.42 Let A be an m × m symmetric matrix with eigenvalues λ1 ≥· · · ≥λm, and
suppose S is a k-dimensional subspace of Rm.
(a) Show that if x′Ax/x′x ≥b for all x ∈S, then λk ≥b.
(b) Show that if x′Ax/x′x ≤b for all x ∈S, then λm−k+1 ≤b.
3.43 Show by example that Theorem 3.21 need not hold if the matrices A and B are
not symmetric.
3.44 Prove Theorem 3.20.
3.45 Our proof of Theorem 3.28 utilized (3.9) of Theorem 3.18. Obtain an alternative
proof of Theorem 3.28 by using (3.10) of Theorem 3.18.
3.46 Let A be a symmetric matrix with λ1(A) > 0. Show that
λ1(A) = max
x′Ax=1
1
x′x.
3.47 Let A be an m × m symmetric matrix and B be an m × m positive definite
matrix. If F is any m × h matrix with full column rank, then show the follow-
ing:
(a) λh−i+1((F ′BF)−1(F ′AF)) ≥λm−i+1(B−1A), for i = 1, . . . , h.
(b) minF λ1((F ′BF)−1(F ′AF)) = λm−h+1(B−1A).
(c) minF λh((F ′BF)−1(F ′AF)) = λm(B−1A).
3.48 Suppose A is an m × m matrix with eigenvalues λ1, . . . , λm and associated
eigenvectors x1, . . . , xm, whereas B is n × n with eigenvalues γ1, . . . , γn
and eigenvectors y1, . . . , yn. What are the eigenvalues and eigenvectors of the
(m + n) × (m + n) matrix
C =

A
(0)
(0)
B

?
Generalize this result by giving the eigenvalues and eigenvectors of the matrix
C =
⎡
⎢⎢⎢⎣
C1
(0)
· · ·
(0)
(0)
C2
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Cr
⎤
⎥⎥⎥⎦

PROBLEMS
151
in terms of the eigenvalues and eigenvectors of the square matrices C1, . . . , Cr.
3.49 Let
T =

1
−1
2
2
1
1

.
(a) Find the eigenvalues and corresponding eigenvectors of TT ′.
(b) Find the eigenvalues and corresponding eigenvectors of T ′T.
3.50 Let A be an m × m symmetric matrix. Show that if k is a positive integer, then
A2k is nonnegative definite.
3.51 Show that if A is a nonnegative definite matrix and aii = 0 for some i, then
aij = aji = 0 for all j.
3.52 Let A be an m × m positive definite matrix and B be an m × m nonnegative
definite matrix.
(a) Use the spectral decomposition of A to show that
|A + B| ≥|A|,
with equality if and only if B = (0).
(b) Show that if B is also positive definite and A −B is nonnegative definite,
then |A| ≥|B| with equality if and only if A = B.
3.53 Suppose that A is an m × m symmetric matrix with eigenvalues λ1, . . . , λm
and associated eigenvectors x1, . . . , xm, whereas B is an m × m symmetric
matrix with eigenvalues γ1, . . . , γm and associated eigenvectors x1, . . . , xm;
that is, A and B have common eigenvectors.
(a) Find the eigenvalues and eigenvectors of A + B.
(b) Find the eigenvalues and eigenvectors of AB.
(c) Show that AB = BA.
3.54 Suppose that x1, . . . , xr is a set of orthonormal eigenvectors corresponding
to the r largest eigenvalues γ1, . . . , γr of the m × m symmetric matrix A and
assume that γr > γr+1. Let P be the total eigenprojection of A associated with
the eigenvalues γ1, . . . , γr; that is,
P =
r

i=1
xix′
i.
Let B be another m × m symmetric matrix with its r largest eigenvalues given
by μ1, . . . , μr, where μr > μr+1, and a corresponding set of orthonormal
eigenvectors given by y1, . . . , yr. Let Q be the total eigenprojection of B
associated with the eigenvalues μ1, . . . , μr so that
Q =
r

i=1
yiy′
i.

152
EIGENVALUES AND EIGENVECTORS
(a) Show that P = Q if and only if
r

i=1
{γi + μi −λi(A + B)} = 0.
(b) Let X = (x1, . . . , xm), where xr+1, . . . , xm is a set of orthonormal
eigenvectors corresponding to the smallest m −r eigenvalues of A. Show
that if P = Q, then X′BX has the block diagonal form

U
(0)
(0)
V

,
where U is r × r and V is (m −r) × (m −r).
3.55 Let λ1 ≥· · · ≥λm be the eigenvalues of the m × m symmetric matrix A and
x1, . . . , xm be a set of corresponding orthonormal eigenvectors. For some k,
define the total eigenprojection associated with the eigenvalues λk, . . . , λm as
P =
m

i=k
xix′
i.
Show that λk = · · · = λm = λ if and only if
P(A −λIm)P = (0).
3.56 Let A1, . . . , Ak be m × m symmetric matrices, and let τi be one of the eigen-
values of Ai. Let x1, . . . , xr be a set of orthonormal m × 1 vectors, and define
P =
r

i=1
xix′
i.
Show that if each of the eigenvalues τi has multiplicity r and has x1, . . . , xr
as associated eigenvectors, then
P
 k

i=1
(Ai −τiIm)2

P = (0).
3.57 Let λ1 ≥· · · ≥λm be the eigenvalues of the m × m symmetric matrix A.
(a) If B is an m × r matrix, show that
min
B′B=Ir tr(B′AB) =
r

i=1
λm−i+1
and
max
B′B=Ir tr(B′AB) =
r

i=1
λi.

PROBLEMS
153
(b) For r = 1, . . . , m, show that
r

i=1
λm−i+1 ≤
r

i=1
aii ≤
r

i=1
λi.
3.58 Let λ1 ≥· · · ≥λm be the eigenvalues of the m × m positive definite matrix
A.
(a) If B is an m × r matrix, show that
min
B′B=Ir |B′AB| =
r

i=1
λm−i+1
and
max
B′B=Ir |B′AB| =
r

i=1
λi.
(b) Let Ar be the r × r submatrix of A consisting of the first r rows and first
r columns of A. For r = 1, . . . , m, show that
r

i=1
λm−i+1 ≤|Ar| ≤
r

i=1
λi.
3.59 Let A be an m × m nonnegative definite matrix, whereas B and C are m × m
positive definite matrices. If i, j, and k are integers between 1 and m inclusive
and satisfying j + k ≤i + 1, show that
(a) λi(AB) ≤λj(AC−1)λk(CB),
(b) λm−i+1(AB) ≥λm−j+1(AC−1)λm−k+1(CB).
3.60 Let A and B be m × m positive definite matrices. Show that
λ2
i(AB)
λ1(A)λ1(B) ≤λi(A)λi(B) ≤
λ2
i(AB)
λm(A)λm(B),
for i = 1, . . . , m.
3.61 Let A be an m × m positive definite matrix with eigenvalues λ1 ≥· · · ≥λm.
If X is an m × k matrix with k ≤m/2, show that
min
X′X=Ik
|X′AX|

|X′A2X|
=
k

i=1
2λiλm−i+1
λi + λm−i+1
.

154
EIGENVALUES AND EIGENVECTORS
3.62 Consider the function ψ(x) given in Section 3.9, where the m × m matrix A is
positive definite.
(a) Show that the stationary points of ψ(x) subject to the constraint x′x = 1
satisfy
A2x
x′A2x −2Ax
x′Ax + x = 0.
(b) Show that the equation given in part (a) holds for all normalized eigenvec-
tors of A and all normalized antieigenvectors of A.

4
MATRIX FACTORIZATIONS AND
MATRIX NORMS
4.1
INTRODUCTION
In this chapter, we take a look at some useful ways of expressing a given matrix A
in the form of a product of other matrices having some special structure or canonical
form. In many applications, such a decomposition of A may reveal the key features
of A that are of interest to us. These factorizations are particularly useful in multivari-
ate distribution theory in that they can expedite the mathematical development and
often simplify the generalization of results from a special case to a more general sit-
uation. Our focus here will be on conditions for the existence of these factorizations
as well as mathematical properties and consequences of the factorizations. Details
on the numerical computation of the component matrices in these factorizations can
be found in texts on numerical methods. Some useful references are Golub and Van
Loan (2013), Press, et al. (2007), and Stewart (1998, 2001).
4.2
THE SINGULAR VALUE DECOMPOSITION
The first factorization that we consider, the singular value decomposition, could be
described as the most useful because this factorization is for a matrix of any size;
the subsequent decompositions will only apply to square matrices. We will find this
decomposition particularly useful in the next chapter when we generalize the concept
of an inverse of a nonsingular square matrix to any matrix.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

156
MATRIX FACTORIZATIONS AND MATRIX NORMS
Theorem 4.1
If A is an m × n matrix of rank r > 0, orthogonal m × m and n × n
matrices P and Q exist, such that A = PDQ′ and D = P ′AQ, where the m × n
matrix D is given by
(a)
Δ
if
r = m = n,
(b)
[Δ
(0)]
if
r = m < n,
(c)
 Δ
(0)

if
r = n < m,
(d)
 Δ
(0)
(0)
(0)

if
r < m, r < n,
and Δ is an r × r diagonal matrix with positive diagonal elements. The diagonal
elements of Δ2 are the positive eigenvalues of A′A and AA′.
Proof.
We will prove the result for the case r < m and r < n. The proofs of (a)–(c)
only require notational changes. Let Δ2 be the r × r diagonal matrix whose diago-
nal elements are the r positive eigenvalues of A′A, which are identical to the positive
eigenvalues of AA′ by Theorem 3.27. Define Δ to be the diagonal matrix whose diag-
onal elements are the positive square roots of the corresponding diagonal elements
of Δ2. Since A′A is an n × n symmetric matrix, we can find an n × n orthogonal
matrix Q, such that
Q′A′AQ =

Δ2
(0)
(0)
(0)

.
Partitioning Q as Q = [Q1
Q2], where Q1 is n × r, the identity above implies that
Q′
1A′AQ1 = Δ2
(4.1)
and
Q′
2A′AQ2 = (0).
(4.2)
Note that from (4.2), it follows that
AQ2 = (0).
(4.3)
Now let P = [P1
P2] be an m × m orthogonal matrix, where the m × r matrix
P1 = AQ1Δ−1 and the m × (m −r) matrix P2 is any matrix that makes P orthog-
onal. Consequently, we must have P ′
2P1 = P ′
2AQ1Δ−1 = (0) or, equivalently,
P ′
2AQ1 = (0).
(4.4)
By using (4.1), (4.3), and (4.4), we find that
P ′AQ =

P ′
1AQ1
P ′
1AQ2
P ′
2AQ1
P ′
2AQ2


THE SINGULAR VALUE DECOMPOSITION
157
=

Δ−1Q′
1A′AQ1
Δ−1Q′
1A′AQ2
P ′
2AQ1
P ′
2AQ2

=
Δ−1Δ2
Δ−1Q′
1A′(0)
(0)
P ′
2(0)

=
 Δ
(0)
(0)
(0)

,
and so the proof is complete.
□
The singular value decomposition as given in Theorem 4.1 is for real matrices A.
However, this decomposition easily extends to complex matrices. In particular, an
m × n complex matrix A can be expressed as A = PDQ∗, where the m × m and
n × n matrices P and Q are unitary, while D has the same form given in Theorem
4.1 with the diagonal elements of Δ given by the positive square roots of the nonzero
eigenvalues of A∗A.
The diagonal elements of the Δ matrix given in Theorem 4.1, that is, the positive
square roots of the positive eigenvalues of A′A and AA′, are called the singular values
of A. It is obvious from the proof of Theorem 4.1 that the columns of Q form an
orthonormal set of eigenvectors of A′A, and so
A′A = QD′DQ′.
(4.5)
It is important to note also that the columns of P form an orthonormal set of eigen-
vectors of AA′ because
AA′ = PDQ′QD′P ′ = PDD′P ′.
(4.6)
If we again partition P and Q as P = [P1
P2] and Q = [Q1
Q2], where P1 is
m × r and Q1 is n × r, then the singular value decomposition can be restated as
follows.
Corollary 4.1.1
If A is an m × n matrix of rank r > 0, then m × r and n × r matri-
ces P1 and Q1 exist, such that P ′
1P1 = Q′
1Q1 = Ir and A = P1ΔQ′
1, where Δ is an
r × r diagonal matrix with positive diagonal elements.
It follows from (4.5) and (4.6) that P1 and Q1 are semiorthogonal matrices
satisfying
P ′
1AA′P1 = Δ2,
Q′
1A′AQ1 = Δ2.
(4.7)
However, in the decomposition A = P1ΔQ′
1, the choice of the semiorthogonal matrix
P1 satisfying (4.7) is dependent on the choice of the Q1 matrix. This should be appar-
ent from the proof of Theorem 4.1 in which any semiorthogonal matrix Q1 satisfying

158
MATRIX FACTORIZATIONS AND MATRIX NORMS
(4.7) was first selected, but the choice of P1 was then given by P1 = AQ1Δ−1. Alter-
natively, we could have first selected a semiorthogonal matrix P1 satisfying (4.7) and
then have chosen Q1 = A′P1Δ−1.
A lot of information about the structure of a matrix A can be obtained from its
singular value decomposition. The number of singular values gives the rank of A,
whereas the columns of P1 and Q1 are orthonormal bases for the column space and
row space of A, respectively. Similarly, the columns of P2 span the null space of A′,
and the columns of Q2 span the null space of A.
Theorem 4.1 and Corollary 4.1.1 are related to Theorem 1.11 and its corollary,
Corollary 1.11.1, which were stated as consequences of the properties of elemen-
tary transformations. It is easily verified that Theorem 1.11 and Corollary 1.11.1 also
follow directly from Theorem 4.1 and Corollary 4.1.1.
Example 4.1
We will find the singular value decomposition for the 4 × 3 matrix
A =
⎡
⎢⎢⎣
2
0
1
3
−1
1
−2
4
1
1
1
1
⎤
⎥⎥⎦.
First, an eigenanalysis of the matrix
A′A =
⎡
⎣
18
−10
4
−10
18
4
4
4
4
⎤
⎦
reveals that it has eigenvalues 28, 12, and 0 with associated normalized eigenvectors
(1/
√
2, −1/
√
2, 0)′, (1/
√
3, 1/
√
3, 1/
√
3)′, and (1/
√
6, 1/
√
6, −2/
√
6)′, respec-
tively. Let these eigenvectors be the columns of the 3 × 3 orthogonal matrix Q.
Clearly, rank(A) = 2 and the two singular values of A are
√
28 and
√
12. Thus, the
4 × 2 matrix P1 is given by
P1 = AQ1Δ−1 =
⎡
⎢⎢⎣
2
0
1
3
−1
1
−2
4
1
1
1
1
⎤
⎥⎥⎦
⎡
⎣
1/
√
2
1/
√
3
−1/
√
2
1/
√
3
0
1/
√
3
⎤
⎦
×

1/
√
28
0
0
1/
√
12

=
⎡
⎢⎢⎣
1/
√
14
1/2
2/
√
14
1/2
−3/
√
14
1/2
0
1/2
⎤
⎥⎥⎦.
The 4 × 2 matrix P2 can be any matrix satisfying P ′
1P2 = (0) and P ′
2P2 = I2; for
instance, (1/
√
12, 1/
√
12, 1/
√
12, −3/
√
12)′ and (−5/
√
42, 4/
√
42, 1/
√
42, 0)′

THE SINGULAR VALUE DECOMPOSITION
159
can be chosen as the columns of P2. Then our singular value decomposition of A is
given by
⎡
⎢⎢⎣
1/
√
14
1/2
1/
√
12
−5/
√
42
2/
√
14
1/2
1/
√
12
4/
√
42
−3/
√
14
1/2
1/
√
12
1/
√
42
0
1/2
−3/
√
12
0
⎤
⎥⎥⎦
⎡
⎢⎢⎣
√
28
0
0
0
√
12
0
0
0
0
0
0
0
⎤
⎥⎥⎦
×
⎡
⎣
1/
√
2
−1/
√
2
0
1/
√
3
1/
√
3
1/
√
3
1/
√
6
1/
√
6
−2/
√
6
⎤
⎦,
or in the form of Corollary 4.1.1,
⎡
⎢⎢⎣
1/
√
14
1/2
2/
√
14
1/2
−3/
√
14
1/2
0
1/2
⎤
⎥⎥⎦
√
28
0
0
√
12
 
1/
√
2
−1/
√
2
0
1/
√
3
1/
√
3
1/
√
3

.
As an alternative way of determining the matrix P, we could have used the fact that
its columns are eigenvectors of the matrix
AA′ =
⎡
⎢⎢⎣
5
7
−3
3
7
11
−9
3
−3
−9
21
3
3
3
3
3
⎤
⎥⎥⎦.
However, when constructing P this way, one must check the decomposition
A = P1ΔQ′
1 to determine the correct sign for each of the columns of P1.
The singular value decomposition of a vector is very easy to construct. We illus-
trate this in the next example.
Example 4.2
Let x be an m × 1 nonnull vector. Its singular value decomposition
will be of the form
x = Pdq,
where P is an m × m orthogonal matrix, d is an m × 1 vector having only its first
component nonzero, and q is a scalar satisfying q2 = 1. The single singular value of
x is given by λ, where λ2 = x′x. If we define x∗= λ−1x, note that x′
∗x∗= 1, and
xx′x∗= xx′(λ−1x) = (λ−1x)x′x = λ2x∗,
so that x∗is a normalized eigenvector of xx′ corresponding to its single positive
eigenvalue λ2. Any nonnull vector orthogonal to x∗is an eigenvector of xx′

160
MATRIX FACTORIZATIONS AND MATRIX NORMS
corresponding to the repeated eigenvalue 0. Thus, if we let d = (λ, 0 . . . , 0)′, q = 1,
and P = (x∗, p2, . . . , pm) be any orthogonal matrix with x∗as its first column, then
Pdq = [x∗, p2, . . . , pm]
⎡
⎢⎢⎢⎣
λ
0
...
0
⎤
⎥⎥⎥⎦1 = λx∗= x,
as is required.
When A is m × m and symmetric, the singular values of A are directly related to
the eigenvalues of A. This follows from the fact that AA′ = A2, and the eigenvalues of
A2 are the squares of the eigenvalues of A. Thus, the singular values of A will be given
by the absolute values of the eigenvalues of A. If we let the columns of P be a set of
orthonormal eigenvectors of A, then the Q matrix in Theorem 4.1 will be identical to
P except that any column of Q that is associated with a negative eigenvalue will be −1
times the corresponding column of P. If A is nonnegative definite, then the singular
values of A will be the same as the positive eigenvalues of A and, in fact, the singular
value decomposition of A is simply the spectral decomposition of A discussed in the
next section. This nice relationship between the eigenvalues and singular values of a
symmetric matrix does not carry over to general square matrices.
Example 4.3
Consider the 2 × 2 matrix
A =

6
6
−1
1

,
which has
AA′ =

72
0
0
2

,
A′A =

37
35
35
37

.
Clearly, the singular values of A are
√
72 = 6
√
2 and
√
2. Normalized eigenvec-
tors corresponding to 72 and 2 are (1, 0)′ and (0, 1)′ for AA′, whereas A′A has
(1/
√
2, 1/
√
2)′ and (−1/
√
2, 1/
√
2)′. Thus, the singular value decomposition of A
can be written as

1
0
0
1
 
6
√
2
0
0
√
2
 
1/
√
2
1/
√
2
−1/
√
2
1/
√
2

.
On the other hand, an eigenanalysis of A yields the eigenvalues 4 and 3. Associated
normalized eigenvectors are (3/
√
10, −1/
√
10)′ and (2/
√
5, −1/
√
5)′.
We end this section with an example that illustrates an application of the singular
value decomposition to least squares regression. For more discussion of this and other
applications of the singular value decomposition in statistics, see Mandel (1982),
Eubank and Webster (1985), and Nelder (1985).

THE SINGULAR VALUE DECOMPOSITION
161
Example 4.4
In this example, we will take a closer look at the multicollinearity
problem that we first discussed in Example 3.7. Suppose that we have the standardized
regression model,
y = δ01N + Z1δ1 + ϵ.
We have seen in Example 2.16 that the least squares estimator of δ0 is y. The fitted
model ˆy = y1N + Z1ˆδ1 gives points on a hyperplane in Rk+1, where the (k + 1)
axes correspond to the k standardized explanatory variables and the fitted response
variable. Now let Z1 = V DU ′ be the singular value decomposition of the N × k
matrix Z1. Thus, V is an N × N orthogonal matrix, U is a k × k orthogonal matrix,
and D is an N × k matrix that has the square roots of the eigenvalues of Z′
1Z1 as
its diagonal elements and zeros elsewhere. We can rewrite the model y = δ01N +
Z1δ1 + ϵ as we did in Example 2.16 by defining α0 = δ0, α1 = U ′δ1 and W1 = V D,
so that y = α01N + W1α1 + ϵ. Suppose that exactly r of the diagonal elements of
D, specifically the last r diagonal elements, are zeros, and so by partitioning U, V ,
and D appropriately, we get Z1 = V1D1U ′
1, where D1 is a (k −r) × (k −r) diago-
nal matrix. This means that the row space of Z1 is a (k −r)-dimensional subspace of
Rk, and this subspace is spanned by the columns of U1; that is, the points on the fitted
regression hyperplane described above, when projected onto the k-dimensional stan-
dardized explanatory variable space, are actually confined to a (k −r)-dimensional
subspace. Also, the model y = α01N + W1α1 + ϵ simplifies to
y = α01N + W11α11 + ϵ,
(4.8)
where W11 = V1D1, α11 = U ′
1δ1, and the least squares estimator of the (k −r) × 1
vector α11 is given by ˆα11 = (W ′
11W11)−1W ′
11y = D−1
1 V ′
1y. This can be used to
find a least squares estimator of δ1 because we must have ˆα11 = U ′
1ˆδ1. Partitioning
ˆδ1 = (ˆδ
′
11, ˆδ
′
12)′ and U ′
1 = (U ′
11, U ′
12), where ˆδ11 is (k −r) × 1, we obtain the
relationship
ˆα11 = U ′
11ˆδ11 + U ′
12ˆδ12.
Premultiplying this equation by U ′−1
11
(if U11 is not nonsingular, then δ1 and U1 can
be rearranged so that it is), we find that
ˆδ11 = U ′−1
11 ˆα11 −U ′−1
11 U ′
12ˆδ12;
that is, the least squares estimator of δ1 is not unique because ˆδ1 = (ˆδ
′
11, ˆδ
′
12)′ is a
least squares estimator for any choice of ˆδ12, as long as ˆδ11 satisfies this identity.
Now suppose that we wish to estimate the response variable y corresponding to an
observation that has the standardized explanatory variables at the values given in
the k × 1 vector z. Using a least squares estimate ˆδ1 we obtain the estimate ˆy =
y + z′ˆδ1. This estimated response, like ˆδ1, may not be unique because, if we partition
z as z′ = (z′
1, z′
2) with z1 being (k −r) × 1,
ˆy = y + z′ˆδ1 = y + z′
1ˆδ11 + z′
2ˆδ12
= y + z′
1U ′−1
11 ˆα11 + (z′
2 −z′
1U ′−1
11 U ′
12)ˆδ12.

162
MATRIX FACTORIZATIONS AND MATRIX NORMS
Thus, ˆy does not depend on the arbitrary ˆδ12 and is therefore unique, only if
(z′
2 −z′
1U ′−1
11 U ′
12) = 0′,
(4.9)
in which case the unique estimated value is given by ˆy = y + z′
1U ′−1
11 ˆα11. It is easily
shown that the set of all vectors z = (z′
1, z′
2)′ satisfying (4.9) is simply the column
space of U1. Thus, y = δ0 + z′δ1 is uniquely estimated only if the vector of standard-
ized explanatory variables z falls within the space spanned by the collection of all
vectors of standardized explanatory variables available to compute ˆδ1.
In the typical multicollinearity problem, Z1 is full rank so that the matrix D has no
zero diagonal elements but instead has r of its diagonal elements very small relative
to the others. In this case, the row space of Z1 is all of Rk, but the points correspond-
ing to the rows of Z1 all lie very close to a (k −r)-dimensional subspace S of Rk,
specifically, the space spanned by the columns of U1. Small changes in the values of
the response variables corresponding to these points can substantially alter the posi-
tion of the fitted regression hyperplane ˆy = y + z′ˆδ1 for vectors z lying outside of
and, in particular, far from S. For instance, if k = 2 and r = 1, the points correspond-
ing to the rows of Z1 all lie very close to S, which, in this case, is a line in the z1, z2
plane, and ˆy = y + z′ˆδ1 will be given by a plane in R3 extended over the z1, z2 plane.
The fitted regression plane ˆy = y + z′ˆδ1 can be identified by the line formed as the
intersection of this plane and the plane perpendicular to the z1, z2 plane and passing
through the line S, along with the tilt of the fitted regression plane. Small changes in
the values of the response variables will produce small changes in both the location
of this line of intersection and the tilt of the plane. However, even a slight change in
the tilt of the regression plane will yield large changes on the surface of this plane
for vectors z far from S. The adverse effect of this tilting can be eliminated by the
use of principal components regression. As we saw in Example 3.7, principal com-
ponents regression utilizes the regression model (4.8), and so an estimated response
will be given by ˆy = y + z′U1D−1
1 V ′
1y. Since this regression model technically holds
only for z ∈S, by using this model for z /∈S, we will introduce bias into our esti-
mate of y. The advantage of principal components regression is that this bias may be
compensated for by a large enough reduction in the variance of our estimate so as to
reduce the mean squared error (see Problem 4.11). However, it should be apparent
that the predicted values of y obtained from both ordinary least squares regression
and principal components regression will be poor if the vector z is far from S.
4.3
THE SPECTRAL DECOMPOSITION OF A SYMMETRIC MATRIX
The spectral decomposition of a symmetric matrix, which was briefly discussed in
Chapter 3, is nothing more than a special case of the singular value decomposition.
We summarize this result in Theorem 4.2.
Theorem 4.2
Let A be an m × m symmetric matrix with eigenvalues λ1, . . . , λm,
and suppose that x1, . . . , xm is a set of orthonormal eigenvectors corresponding

THE SPECTRAL DECOMPOSITION OF A SYMMETRIC MATRIX
163
to these eigenvalues. Then, if Λ = diag(λ1, . . . , λm) and X = (x1, . . . , xm), it
follows that
A = XΛX′.
We can use the spectral decomposition of a nonnegative definite matrix A to find a
square root matrix of A; that is, we wish to find an m × m nonnegative definite matrix
A1/2 for which A = A1/2A1/2. If Λ and X are defined as in Theorem 4.2, and we let
Λ1/2 = diag(λ1/2
1
, . . . , λ1/2
m ) and A1/2 = XΛ1/2X′, then because X′X = Im,
A1/2A1/2 = XΛ1/2X′XΛ1/2X′ = XΛ1/2Λ1/2X′
= XΛX′ = A,
as is required. Note that (A1/2)′ = (XΛ1/2X′)′ = XΛ1/2X′ = A1/2; consequently,
XΛ1/2X′ is referred to as the symmetric square root of A. Note also that if we did not
require A to be nonnegative definite, then A1/2 would be a complex matrix if some
of the eigenvalues of A are negative.
It is easy to show that the nonnegative definite square root A1/2 is uniquely defined.
In Section 3.4, we saw that the spectral decomposition of A can be written as
A =
k

i=1
μiPA(μi),
where μ1, . . . , μk are the spectral values of A. The eigenprojections of A are
uniquely defined and satisfy {PA(μi)}′ = PA(μi), {PA(μi)}2 = PA(μi), and
PA(μi)PA(μj) = (0) if i ̸= j. Suppose B is another m × m nonnegative definite
matrix with its spectral decomposition given by
B =
r

j=1
γjPB(γj).
If A = B2 so that
A =
r

j=1
γ2
jPB(γj),
it follows that we must have r = k and for each i, μi = γ2
j and PA(μi) = PB(γj) for
some j. This implies that
B =
k

i=1
μ1/2
i
PA(μi),
and this is equivalent to the expression A1/2 = XΛ1/2X′ given above.
We can expand the set of square root matrices if we do not insist that A1/2 be
symmetric; that is, now let us consider any matrix A1/2 satisfying A = A1/2(A1/2)′.
If Q is any m × m orthogonal matrix, then A1/2 = XΛ1/2Q′ is such a square root

164
MATRIX FACTORIZATIONS AND MATRIX NORMS
matrix because
A1/2A1/2′ = XΛ1/2Q′QΛ1/2X′ = XΛ1/2Λ1/2X′
= XΛX′ = A.
If A1/2 is a lower triangular matrix with nonnegative diagonal elements, then the
factorization A = A1/2A1/2′ is known as the Cholesky decomposition of A. Theorem
4.3 establishes the existence of such a decomposition.
Theorem 4.3
Let A be an m × m nonnegative definite matrix. Then an m × m
lower triangular matrix T having nonnegative diagonal elements exists, such that
A = TT ′. Further, if A is positive definite, then the matrix T is unique and has posi-
tive diagonal elements.
Proof.
We will prove the result for positive definite matrices. Our proof is by induc-
tion. The result clearly holds if m = 1, because in this case, A is a positive scalar,
and so the unique T would be given by the positive square root of A. Now assume
that the result holds for all positive definite (m −1) × (m −1) matrices. Partition A
as
A =

A11
a12
a′
12
a22

,
where A11 is (m −1) × (m −1). Since A11 must be positive definite if A is, we
know there exists a unique (m −1) × (m −1) lower triangular matrix T11 having
positive diagonal elements and satisfying A11 = T11T ′
11. Our proof will be complete
if we can show that there is a unique (m −1) × 1 vector t12 and a unique positive
scalar t22, such that

A11
a12
a′
12
a22

=

T11
0
t′
12
t22
 
T ′
11
t12
0′
t22

=
T11T ′
11
T11t12
t′
12T ′
11
t′
12t12 + t2
22

;
that is, we must have a12 = T11t12 and a22 = t′
12t12 + t2
22. Since T11 must be nonsin-
gular, the unique choice of t12 is given by t12 = T −1
11 a12, and so t2
22 must satisfy
t2
22 = a22 −t′
12t12 = a22 −a′
12(T −1
11 )′T −1
11 a12
= a22 −a′
12(T11T ′
11)−1a12 = a22 −a′
12A−1
11 a12.
Note that because A is positive definite, a22 −a′
12A−1
11 a12 will be positive because,
if we let x = (x′
1, −1)′ = (a′
12A−1
11 , −1)′, then
x′Ax = x′
1A11x1 −2x′
1a12 + a22
= a′
12A−1
11 A11A−1
11 a12 −2a′
12A−1
11 a12 + a22

THE SPECTRAL DECOMPOSITION OF A SYMMETRIC MATRIX
165
= a22 −a′
12A−1
11 a12.
Consequently, the unique t22 > 0 is given by t22 = (a22 −a′
12A−1
11 a12)1/2.
□
The following decomposition, commonly known as the QR factorization, can be
used to establish the triangular factorization of Theorem 4.3 for positive semidefinite
matrices.
Theorem 4.4
Let A be an m × n matrix, where m ≥n. An n × n upper triangu-
lar matrix R with nonnegative diagonal elements and an m × n matrix Q satisfying
Q′Q = In exist, such that A = QR.
Proof.
Let r11 = (a′
1a1)1/2, where a1 is the first column of A. If r11 = 0, let
Q1 = Im, otherwise let Q1 be any m × m orthogonal matrix with its first row given
by a′
1/r11 so that
Q1A =

r11
b′
11
0
A2

for
some
(n −1) × 1
vector
b11
and
(m −1) × (n −1)
matrix
A2.
Let
r22 = (a′
2a2)1/2, where a2 is the first column of A2. If r22 = 0, let Q22 = Im−1,
otherwise let Q22 be any (m −1) × (m −1) orthogonal matrix with its first row
given by a′
2/r22 so that
Q2Q1A =
⎡
⎣
r11
r12
b′
12
0
r22
b′
22
0
0
A3
⎤
⎦, where
Q2 =
1
0′
0
Q22

,
b′
11 = (r12, b′
12), b22 is some (n −2) × 1 vector, and A3 is some (m −2) × (n −2)
matrix. Continuing in this fashion through n steps, we will have obtained m × m
orthogonal matrices Q1, . . . , Qn such that
Qn · · · Q1A =

R
(0)

,
where R is an n × n upper triangular matrix with diagonal elements r11, . . . , rnn, all
of which are nonnegative. Finally, partition the orthogonal matrix Q∗= Qn · · · Q1 as
Q′
∗= [Q, P], where Q is m × n so that Q′Q = In and A = QR.
□
If A is a positive semidefinite matrix and A = A1/2(A1/2)′, then the triangular
factorization of Theorem 4.3 for positive semidefinite matrices can be proven by using
the QR factorization of (A1/2)′.
Example 4.5
Suppose that the m × 1 random vector x has mean vector μ and
the positive definite covariance matrix Ω. By using a square root matrix of Ω, we
can determine a linear transformation of x so that the transformed random vector

166
MATRIX FACTORIZATIONS AND MATRIX NORMS
is standardized; that is, it has mean vector 0 and covariance matrix Im. If we let
Ω1/2 be any matrix satisfying Ω = Ω1/2(Ω1/2)′ and put z = Ω−1/2(x −μ), where
Ω−1/2 = (Ω1/2)−1, then by using (1.8) and (1.9) of Section 1.13, we find that
E(z) = E{Ω−1/2(x −μ)} = Ω−1/2{E(x −μ)}
= Ω−1/2(μ −μ) = 0
and
var(z) = var{Ω−1/2(x −μ)}
= Ω−1/2{var(x −μ)}(Ω−1/2)′
= Ω−1/2{var(x)}(Ω−1/2)′
= Ω−1/2Ω(Ω−1/2)′ = Im.
Since the covariance matrix of z is the identity matrix, the Euclidean distance func-
tion will give a meaningful measure of the distance between observations from this
distribution. By using the linear transformation defined above, we can relate dis-
tances between z observations to distances between x observations. For example,
the Euclidean distance between an observation z and its expected value 0 is
dI(z, 0) = {(z −0)′(z −0)}1/2 = (z′z)1/2
= {(x −μ)′(Ω−1/2)′Ω−1/2(x −μ)}1/2
= {(x −μ)′Ω−1(x −μ)}1/2
= dΩ(x, μ),
where dΩ is the Mahalanobis distance function defined in Section 2.2. Similarly, if
x1 and x2 are two observations from the distribution of x and z1 and z2 are the
corresponding transformed vectors, then dI(z1, z2) = dΩ(x1, x2). This relationship
between the Mahalanobis distance and the Euclidean distance makes the construction
of the Mahalanobis distance function more apparent. The Mahalanobis distance is
nothing more than a two-stage computation of distance; the first stage transforms
points so as to remove the effect of correlations and differing variances, whereas the
second stage simply computes the Euclidean distance for these transformed points.
Example 4.6
We consider the generalized least squares regression model which
was first discussed in Example 2.22. This model has the same form as the standard
multiple regression model that is,
y = Xβ + ϵ,
but now var(ϵ) = σ2C, where C is a known N × N positive definite matrix. In
Example 2.22, we found the least squares estimator of β by using properties of pro-
jections. In this example, we obtain that same estimator by a different approach. We
will transform the problem to ordinary least squares regression; that is, we wish to

THE SPECTRAL DECOMPOSITION OF A SYMMETRIC MATRIX
167
transform the model so that the vector of random errors in the transformed model has
σ2IN as its covariance matrix. This can be done by using any square root matrix of C.
Let T be any N × N matrix satisfying TT ′ = C or, equivalently, T ′−1T −1 = C−1.
Now transform our original regression model to the model
y∗= X∗β + ϵ∗,
where y∗= T −1y, X∗= T −1X, and ϵ∗= T −1ϵ, and note that E(ϵ∗) = T −1E(ϵ) = 0
and
var(ϵ∗) = var(T −1ϵ) = T −1{var(ϵ)}T ′−1
= T −1(σ2C)T ′−1 = σ2T −1TT ′T ′−1
= σ2IN.
Thus, the generalized least squares estimator ˆβ∗of β in the model y = Xβ + ϵ is
given by the ordinary least squares estimator of β in the model y∗= X∗β + ϵ∗, and
so it can be expressed as
ˆβ∗= (X′
∗X∗)−1X′
∗y∗= (X′T ′−1T −1X)−1X′T ′−1T −1y
= (X′C−1X)−1X′C−1y.
In some situations, a matrix A can be expressed in the form of the transpose prod-
uct, BB′, where the m × r matrix B has r < m, so that unlike a square root matrix,
B is not square. This is the subject of Theorem 4.5, the proof of which will be left to
the reader as an exercise.
Theorem 4.5
Let A be an m × m nonnegative definite matrix with rank(A) = r.
Then there exists an m × r matrix B having rank of r, such that A = BB′.
The transpose product form A = BB′ of the nonnegative definite matrix A is not
unique. However, if C is another matrix of order m × n where n ≥r and A = CC′,
then there is an explicit relationship between the matrices B and C. This is established
in the next theorem.
Theorem 4.6
Suppose that B is an m × h matrix and C is an m × n matrix, where
h ≤n. Then BB′ = CC′ if and only if an h × n matrix Q exists, such that QQ′ = Ih
and C = BQ.
Proof.
If C = BQ with QQ′ = Ih, then clearly
CC′ = BQ(BQ)′ = BQQ′B′ = BB′.
Conversely, now suppose that BB′ = CC′. We will assume that h = n because
if h < n, we can form the matrix B∗= [B
(0)], so that B∗is m × n and

168
MATRIX FACTORIZATIONS AND MATRIX NORMS
B∗B′
∗= BB′; then proving that an n × n orthogonal matrix Q∗exists, such that
C = B∗Q∗, will yield C = BQ if we take Q to be the first h rows of Q∗. Now
because BB′ is symmetric, an orthogonal matrix X exists, such that
BB′ = CC′ = X
 Λ
(0)
(0)
(0)

X′ = X1ΛX′
1,
where rank(BB′) = r and the r × r diagonal matrix Λ contains the positive
eigenvalues of the nonnegative definite matrix BB′. Here X has been partitioned as
X = [X1
X2], where X1 is m × r. Form the matrices
E =

Λ−1/2
(0)
(0)
Im−r

X′B
=

Λ−1/2X′
1B
X′
2B

=
E1
E2

,
(4.10)
F =

Λ−1/2
(0)
(0)
Im−r

X′C
=

Λ−1/2X′
1C
X′
2C

=

F1
F2

,
(4.11)
so that
EE′ = FF ′ =

Ir
(0)
(0)
(0)

;
that is, E1E′
1 = F1F ′
1 = Ir, E2E′
2 = F2F ′
2 = (0), and so E2 = F2 = (0). Now let E3
and F3 be any (h −r) × h matrices, such that E∗= [E′
1
E′
3]′ and F∗= [F ′
1
F ′
3]′
are both orthogonal matrices. Consequently, if Q = E′
∗F∗, then QQ′ = E′
∗F∗F ′
∗E∗=
E′
∗E∗= Ih, so Q is orthogonal. Since E∗is orthogonal, we have E1E′
3 = (0), and so
EQ = EE′
∗F∗=

E1
(0)

[E′
1
E′
3]

F1
F3

=

Ir
(0)
(0)
(0)
 
F1
F3

=

F1
(0)

= F.
However, using (4.10) and (4.11), EQ = F can be written as

Λ−1/2
(0)
(0)
Im−r

X′BQ =

Λ−1/2
(0)
(0)
Im−r

X′C.
The result now follows by premultiplying this equation by
X

Λ1/2
(0)
(0)
Im−r

,
because XX′ = Im.
□

THE DIAGONALIZATION OF A SQUARE MATRIX
169
4.4
THE DIAGONALIZATION OF A SQUARE MATRIX
From the spectral decomposition theorem, we know that every symmetric matrix can
be transformed to a diagonal matrix by postmultiplying by an appropriately chosen
orthogonal matrix and premultiplying by its transpose. This result gives us a very
useful and simple relationship between a symmetric matrix and its eigenvalues and
eigenvectors. In this section, we investigate a generalization of this relationship to
square matrices in general. We begin with Definition 4.1.
Definition 4.1
The m × m matrices A and B are said to be similar matrices if a
nonsingular matrix C exists, such that A = CBC−1.
It follows from Theorem 3.2(d) that similar matrices have identical eigenvalues.
However, the converse is not true. For instance, if we have
A =

0
1
0
0

,
B =

0
0
0
0

,
then A and B have identical eigenvalues because each has 0 with multiplicity 2.
Clearly, however, there is no nonsingular matrix C satisfying A = CBC−1.
The spectral decomposition theorem given as Theorem 4.2 tells us that every sym-
metric matrix is similar to a diagonal matrix. Unfortunately, the same statement does
not hold for all square matrices. If the diagonal elements of the diagonal matrix Λ are
the eigenvalues of A, and the columns of X are corresponding eigenvectors, then
the eigenvalue-eigenvector equation AX = XΛ immediately leads to the identity
X−1AX = Λ, if X is nonsingular; that is, the diagonalization of an m × m matrix
simply depends on the existence of a set of m linearly independent eigenvectors.
Consequently, we have the following result, previously mentioned in Section 3.3,
which follows immediately from Theorem 3.7.
Theorem 4.7
Suppose that the m × m matrix A has the eigenvalues λ1, . . . , λm,
which are distinct. If Λ = diag(λ1, . . . , λm) and X = (x1, . . . , xm), where
x1, . . . , xm are eigenvectors of A corresponding to λ1, . . . , λm, then
X−1AX = Λ.
(4.12)
Theorem 4.7 gives a sufficient but not necessary condition for the diagonalization
of a general square matrix; that is, some nonsymmetric matrices that have multiple
eigenvalues are similar to a diagonal matrix. The next theorem gives a necessary and
sufficient condition for a matrix to be diagonalizable.
Theorem 4.8
Suppose the eigenvalues λ1, . . . , λm of the m × m matrix A
consist of h distinct values μ1, . . . , μh having multiplicities r1, . . . , rh, so that
r1 + · · · + rh = m. Then A has a set of m linearly independent eigenvectors and,
thus, is diagonalizable if and only if rank(A −μiIm) = m −ri for i = 1, . . . , h.

170
MATRIX FACTORIZATIONS AND MATRIX NORMS
Proof.
First, suppose that A is diagonalizable, so that using the usual notation, we
have X−1AX = Λ or, equivalently, A = AΛX−1. Thus,
rank(A −μiIm) = rank(XΛX−1 −μiIm)
= rank{X(Λ −μiIm)X−1}
= rank(Λ −μiIm),
where the last equality follows from the fact that the rank of a matrix is unaltered by its
multiplication by a nonsingular matrix. Now, because μi has multiplicity ri, the diag-
onal matrix (Λ −μiIm) has exactly m −ri nonzero diagonal elements, which then
guarantees that rank(A −μiIm) = m −ri. Conversely, now suppose that rank(A −
μiIm) = m −ri, for i = 1, . . . , h. This implies that the dimension of the null space
of (A −μiIm) is m −(m −ri) = ri, and so we can find ri linearly independent
vectors satisfying the equation
(A −μiIm)x = 0.
However, any such x is an eigenvector of A corresponding to the eigenvalue μi.
Consequently, we can find a set of ri linearly independent eigenvectors associated
with the eigenvalue μi. From Theorem 3.7, we know that eigenvectors corresponding
to different eigenvalues are linearly independent. As a result, any set of m eigenvec-
tors of A, which has ri linearly independent eigenvectors corresponding to μi for
each i, will also be linearly independent. Therefore, A is diagonalizable, and so the
proof is complete.
□
The spectral decomposition previously given for a symmetric matrix can be
extended to diagonalizable matrices. Let A be an m × m diagonalizable matrix with
eigenvalues λ1, . . . , λm and the corresponding linearly independent eigenvectors
x1, . . . , xm. Denote the spectral set of A by {μ1, . . . , μh} with μi having
multiplicity ri and λMi+1 = · · · = λMi+ri = μi, where M1 = 0 and Mi = i−1
j=1 rj
for i = 2, . . . , h. If Y = (y1, . . . , ym) = X−1′, then A can be expressed as
A = XΛX−1 = XΛY ′ =
m

i=1
λixiy′
i =
h

i=1
μiPA(μi),
where PA(μi) = ri
j=1 xMi+jy′
Mi+j. If A is symmetric, then X is an orthogonal
matrix so Y = X and this, of course, reduces to the spectral decomposition given in
Section 4.3.
Example 4.7
In Example 3.3, we saw that the matrix
A =
⎡
⎣
2
−1
0
0
1
0
0
0
1
⎤
⎦

THE DIAGONALIZATION OF A SQUARE MATRIX
171
has eigenvalues λ1 = λ2 = 1 and λ3 = 2, and corresponding linearly inde-
pendent eigenvectors x1 = (0, 0, 1)′, x2 = (1, 1, 0)′, and x3 = (1, 0, 0)′. With
X = (x1, x2, x3), it is easily shown that
X−1 =
⎡
⎣
y′
1
y′
2
y′
3
⎤
⎦=
⎡
⎣
0
0
1
0
1
0
1
−1
0
⎤
⎦.
Thus, the spectral decomposition of A is
A = 1PA(1) + 2PA(2) = 1(x1y′
1 + x2y′
2) + 2x3y′
3
= 1
⎡
⎣
0
1
0
0
1
0
0
0
1
⎤
⎦+ 2
⎡
⎣
1
−1
0
0
0
0
0
0
0
⎤
⎦.
We saw in Chapter 3 that the rank of a symmetric matrix is equal to the number
of its nonzero eigenvalues. The diagonal factorization given in (4.12) immediately
yields the following generalization of this result in Theorem 4.9.
Theorem 4.9
Let A be an m × m matrix. If A is diagonalizable, then the rank of
A is equal to the number of nonzero eigenvalues of A.
The converse of Theorem 4.9 is not true; that is, a matrix need not be diagonaliz-
able for its rank to equal the number of its nonzero eigenvalues.
Example 4.8
Let A, B, and C be the 2 × 2 matrices given by
A =
1
1
4
1

,
B =
0
1
0
0

,
C =
1
1
0
1

.
The characteristic equation of A simplifies to (λ −3)(λ + 1) = 0, so its eigenvalues
are λ = 3, −1. Since the eigenvalues are simple, A is diagonalizable. Eigenvectors
corresponding to these two eigenvalues are x1 = (1, 2)′ and x2 = (1, −2)′, so the
diagonalization of A is given by

1/2
1/4
1/2
−1/4
 
1
1
4
1
 
1
1
2
−2

=

3
0
0
−1

.
Clearly, the rank of A is 2, which is the same as the number of nonzero eigenvalues
of A. The characteristic equation of B reduces to λ2 = 0, so B has the eigenvalue
λ = 0 with multiplicity r = 2. Since rank(B −λI2) = rank(B) = 1 ̸= 0 = m −r,
B will not have two linearly independent eigenvectors. The equation Bx = λx = 0
has only one linearly independent solution for x, namely, vectors of the form (a, 0)′.
Thus, B is not diagonalizable. Note also that the rank of B is 1, which is greater
than the number of its nonzero eigenvalues. Finally, turning to C, we see that it has

172
MATRIX FACTORIZATIONS AND MATRIX NORMS
the eigenvalue λ = 1 with multiplicity r = 2, because its characteristic equation sim-
plifies to (1 −λ)2 = 0. This matrix is not diagonalizable because rank(C −λI2) =
rank(C −I2) = rank(B) = 1 ̸= 0 = m −r. Any eigenvector of C is a scalar mul-
tiple of the vector x = (1, 0)′. However, notice that even though C is not diagonal-
izable, it has rank of 2, which is the same as the number of its nonzero eigenvalues.
Theorem 4.10 shows that the connection between the rank and the number of
nonzero eigenvalues of a matrix A hinges on the dimension of the eigenspace asso-
ciated with the eigenvalue 0.
Theorem 4.10
Let A be an m × m matrix, let k be the dimension of the eigenspace
associated with the eigenvalue 0 if 0 is an eigenvalue of A, and let k = 0 otherwise.
Then
rank(A) = m −k.
Proof.
From Theorem 2.24, we know that
rank(A) = m −dim{N(A)},
where N(A) is the null space of A. However, because the null space of A consists of
all vectors x satisfying Ax = 0, we see that N(A) is the same as SA(0), and so the
result follows.
□
We have seen that the number of nonzero eigenvalues of a matrix A equals the
rank of A if A is similar to a diagonal matrix; that is, A being diagonalizable is a suf-
ficient condition for this exact relationship between rank and the number of nonzero
eigenvalues. The following necessary and sufficient condition for this relationship to
exist is an immediate consequence of Theorem 4.10
Corollary 4.10.1
Let A be an m × m matrix, and let m0 denote the multiplicity of
the eigenvalue 0. Then the rank of A is equal to the number of nonzero eigenvalues
of A if and only if
dim{SA(0)} = m0.
Example 4.9
We saw in Example 4.8 that the two matrices
B =
0
1
0
0

,
C =
1
1
0
1

,
are not diagonalizable because each has only one linearly independent eigenvector
associated with its single eigenvalue, which has multiplicity 2. This eigenvalue is 0
for B, so
rank(B) = 2 −dim{SB(0)} = 2 −1 = 1.

THE JORDAN DECOMPOSITION
173
On the other hand, because 0 is not an eigenvalue of C, dim{SC(0)} = 0, and so the
rank of C equals the number of its nonzero eigenvalues, 2.
4.5
THE JORDAN DECOMPOSITION
Our next factorization of a square matrix A is one that could be described as an
attempt to find a matrix similar to A, which, if not diagonal, is as close to being
diagonal as is possible. We begin with Definition 4.2.
Definition 4.2
For h > 1, the h × h matrix Jh(λ) is said to be a Jordan block matrix
if it has the form
Jh(λ) = λIh +
h−1

i=1
eie′
i+1 =
⎡
⎢⎢⎢⎢⎢⎣
λ
1
0
· · ·
0
0
λ
1
· · ·
0
0
0
λ
· · ·
0
...
...
...
...
0
0
0
· · ·
λ
⎤
⎥⎥⎥⎥⎥⎦
,
where ei is the ith column of Ih. If h = 1, J1(λ) = λ.
The matrices B and C from Example 4.8 and Example 4.9 are both 2 × 2 Jordan
block matrices; in particular, B = J2(0) and C = J2(1). We saw that neither of these
matrices is similar to a diagonal matrix. This is true for Jordan block matrices in gen-
eral; if h > 1, then Jh(λ) is not diagonalizable. To see this, note that because Jh(λ)
is a triangular matrix, its diagonal elements are its eigenvalues, and so it has the one
value, λ, repeated h times. However, the solution to Jh(λ)x = λx has x1 arbitrary,
whereas x2 = · · · = xh = 0; that is, Jh(λ) has only one linearly independent eigen-
vector, which is of the form x = (x1, 0, . . . , 0)′.
We now state the Jordan decomposition theorem. For a proof of this result, see
Horn and Johnson (2013).
Theorem 4.11
Let A be an m × m matrix. Then a nonsingular matrix B exists,
such that
B−1AB = J = diag(Jh1(λ1), . . . , Jhr(λr))
=
⎡
⎢⎢⎢⎣
Jh1(λ1)
(0)
· · ·
(0)
(0)
Jh2(λ2)
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Jhr(λr)
⎤
⎥⎥⎥⎦,
where h1 + · · · + hr = m and λ1, . . . , λr are the not necessarily distinct eigenvalues
of A.

174
MATRIX FACTORIZATIONS AND MATRIX NORMS
The matrix J in Theorem 4.11 will be diagonal if hi = 1 for all i. Since the
hi × hi matrix Jhi(λi) has only one linearly independent eigenvector, it follows that
the Jordan canonical form J = diag(Jh1(λ1), . . . , Jhr(λr)) has r linearly indepen-
dent eigenvectors. Thus, if hi > 1 for at least one i, then J will not be diagonal; in
fact, J will not be diagonalizable. The vector xi is an eigenvector of J correspond-
ing to the eigenvalue λi if and only if the vector yi = Bxi is an eigenvector of A
corresponding to λi; for instance, if xi satisfies Jxi = λixi, then
Ayi = (BJB−1)Bxi = BJxi = λiBxi = λiyi.
Thus, r also gives the number of linearly independent eigenvectors of A, and A is
diagonalizable only if J is diagonal.
Example 4.10
Suppose that A is a 4 × 4 matrix with the eigenvalue λ having
multiplicity 4. Then A will be similar to one of the following five Jordan canonical
forms:
diag(J1(λ), J1(λ), J1(λ), J1(λ)) =
⎡
⎢⎢⎣
λ
0
0
0
0
λ
0
0
0
0
λ
0
0
0
0
λ
⎤
⎥⎥⎦,
diag(J2(λ), J1(λ), J1(λ)) =
⎡
⎢⎢⎣
λ
1
0
0
0
λ
0
0
0
0
λ
0
0
0
0
λ
⎤
⎥⎥⎦,
diag(J3(λ), J1(λ)) =
⎡
⎢⎢⎣
λ
1
0
0
0
λ
1
0
0
0
λ
0
0
0
0
λ
⎤
⎥⎥⎦,
diag(J2(λ), J2(λ)) =
⎡
⎢⎢⎣
λ
1
0
0
0
λ
0
0
0
0
λ
1
0
0
0
λ
⎤
⎥⎥⎦,
J4(λ) =
⎡
⎢⎢⎣
λ
1
0
0
0
λ
1
0
0
0
λ
1
0
0
0
λ
⎤
⎥⎥⎦.
The first form given is diagonal, so this corresponds to the case in which A has four
linearly independent eigenvectors associated with the eigenvalue λ. The second and
last forms correspond to A having three and one linearly independent eigenvectors,
respectively. If A has two linearly independent eigenvectors, then it will be similar to
either the third or the fourth matrix given.

THE SCHUR DECOMPOSITION
175
4.6
THE SCHUR DECOMPOSITION
Our next result can be viewed as another generalization of the spectral decomposi-
tion theorem to any square matrix A. The diagonalization theorem and the Jordan
decomposition were generalizations of the spectral decomposition in which our goal
was to obtain a diagonal or “nearly” diagonal matrix. Now, instead we focus on the
orthogonal matrix employed in the spectral decomposition theorem. Specifically, if
we restrict attention only to orthogonal matrices, X, what is the simplest structure that
we can get for X′AX? It turns out that for the general case of any real square matrix
A, we can find an X such that X∗AX is a triangular matrix, where we have broad-
ened the choice of X to include all unitary matrices. Recall that a real unitary matrix
is an orthogonal matrix, and in general, X is unitary if X∗X = I, where X∗is the
transpose of the complex conjugate of X. This decomposition, sometimes referred to
as the Schur decomposition, is given in Theorem 4.12.
Theorem 4.12
Let A be an m × m matrix. Then an m × m unitary matrix X exists,
such that
X∗AX = T,
where T is an upper triangular matrix with the eigenvalues of A as its diagonal
elements.
Proof.
Let λ1, . . . , λm be the eigenvalues of A, and let y1 be an eigenvector of A
corresponding to λ1 and normalized so that y∗
1y1 = 1. Let Y be any m × m unitary
matrix having y1 as its first column. Writing Y in partitioned form as Y = [y1
Y2],
we see that, because Ay1 = λ1y1 and Y ∗
2 y1 = 0,
Y ∗AY =

y∗
1Ay1
y∗
1AY2
Y ∗
2 Ay1
Y ∗
2 AY2

=

λ1y∗
1y1
y∗
1AY2
λ1Y ∗
2 y1
Y ∗
2 AY2

=
λ1
y∗
1AY2
0
B

,
where the (m −1) × (m −1) matrix B = Y ∗
2 AY2. Using the identity above and
the cofactor expansion formula for a determinant, it follows that the characteristic
equation of Y ∗AY is
(λ1 −λ)|B −λIm−1| = 0,
and, because by Theorem 3.2(d) the eigenvalues of Y ∗AY are the same as those of
A, the eigenvalues of B must be λ2, . . . , λm. Now if m = 2, then the scalar B must
equal λ2 and Y ∗AY is upper triangular, so the proof is complete. For m > 2, we
proceed by induction; that is, we show that if our result holds for (m −1) × (m −1)
matrices, then it must also hold for m × m matrices. Since B is (m −1) × (m −1),
we may assume that a unitary matrix W exists, such that W ∗BW = T2, where T2 is

176
MATRIX FACTORIZATIONS AND MATRIX NORMS
an upper triangular matrix with diagonal elements λ2, . . . , λm. Define the m × m
matrix U by
U =

1
0′
0
W

,
and note that U is unitary because W is. If we let X = Y U, then X is also unitary
and
X∗AX = U ∗Y ∗AY U =

1
0′
0
W ∗
 
λ1
y∗
1AY2
0
B
 
1
0′
0
W

=

λ1
y∗
1AY2W
0
W ∗BW

=

λ1
y∗
1AY2W
0
T2

,
where this final matrix is upper triangular with λ1, . . . , λm as its diagonal elements.
Thus, the proof is complete.
□
If all of the eigenvalues of A are real, then corresponding real eigenvectors exist.
In this case, a real matrix X satisfying the conditions of Theorem 4.12 can be found.
Consequently, we have the following result.
Corollary 4.12.1
If the m × m matrix A has real eigenvalues, then an m × m
orthogonal matrix X exists, such that X′AX = T, where T is an upper triangular
matrix.
Example 4.11
Consider the 3 × 3 matrix given by
A =
⎡
⎣
5
−3
3
4
−2
3
4
−4
5
⎤
⎦.
In Example 3.1, the eigenvalues of A were shown to be λ1 = 1, λ2 = 2, and λ3 = 5,
with eigenvectors, x1 = (0, 1, 1)′, x2 = (1, 1, 0)′, and x3 = (1, 1, 1)′, respectively.
We will find an orthogonal matrix X and an upper triangular matrix T so that A =
XTX′. First, we construct an orthogonal matrix Y having a normalized version of
x1 as its first column; for instance, by inspection, we set
Y =
⎡
⎣
0
0
1
1/
√
2
1/
√
2
0
1/
√
2
−1/
√
2
0
⎤
⎦.
Thus, our first stage yields
Y ′AY =
⎡
⎣
1
−7
4
√
2
0
2
0
0
−3
√
2
5
⎤
⎦.

THE SCHUR DECOMPOSITION
177
The 2 × 2 matrix
B =

2
0
−3
√
2
5

has a normalized eigenvector (1/
√
3,
√
2/
√
3)′, and so we can construct an orthogo-
nal matrix
W =
 1/
√
3
−
√
2/
√
3
√
2/
√
3
1/
√
3

for which
W ′BW =

2
3
√
2
0
5

.
Putting it all together, we have
X = Y

1
0′
0
W

=
1
√
6
⎡
⎣
0
2
√
2
√
3
1
−
√
2
√
3
−1
√
2
⎤
⎦
and
T = X′AX =
⎡
⎣
1
1/
√
3
22/
√
6
0
2
3
√
2
0
0
5
⎤
⎦.
The matrices X and T in the Schur decomposition are not unique; that is, if
A = XTX∗is a Schur decomposition of A, then A = X0T0X∗
0 is also, where
X0 = XP and P is any unitary matrix for which P ∗TP = T0 is upper triangular.
The triangular matrices T and T0 must have the same diagonal elements, possibly
ordered differently. Otherwise, however, the two matrices T and T0 may be quite
different. For example, it can be easily verified that the matrices
X0 =
⎡
⎣
1/
√
3
2/
√
6
0
1/
√
3
−1/
√
6
−1/
√
2
1/
√
3
−1/
√
6
1/
√
2
⎤
⎦,
T0 =
⎡
⎣
5
8/
√
2
20/
√
6
0
1
−1/
√
3
0
0
2
⎤
⎦
give another Schur decomposition of the matrix A.
In Chapter 3, by using the characteristic equation of the m × m matrix A, we were
able to prove that the determinant of A equals the product of its eigenvalues, whereas
the trace of A equals the sum of its eigenvalues. These results are also very easily
proven using the Schur decomposition of A. If the eigenvalues of A are λ1, . . . , λm
and A = XTX∗is a Schur decomposition of A, then it follows that
|A| = |XTX∗| = |X∗X||T| = |T| =
m

i=1
λi,

178
MATRIX FACTORIZATIONS AND MATRIX NORMS
because |X∗X| = 1 follows from the fact that X is a unitary matrix, and the deter-
minant of a triangular matrix is the product of its diagonal elements. Also, using
properties of the trace of a matrix, we have
tr(A) = tr(XTX∗) = tr(X∗XT) = tr(T) =
m

i=1
λi.
The Schur decomposition also provides a method of easily establishing the fact
that the number of nonzero eigenvalues of a matrix serves as a lower bound for the
rank of that matrix. This is the subject of our next theorem.
Theorem 4.13
Suppose the m × m matrix A has r nonzero eigenvalues. Then
rank(A) ≥r.
Proof.
Let X be a unitary matrix and T be an upper triangular matrix, such that
A = XTX∗. Since the eigenvalues of A are the diagonal elements of T, T must have
exactly r nonzero diagonal elements. The r × r submatrix of T, formed by deleting
the columns and rows occupied by the zero diagonal elements of T, will be upper tri-
angular with nonzero diagonal elements. This submatrix will be nonsingular because
the determinant of a triangular matrix is the product of its diagonal elements, so we
must have rank(T) ≥r. However, because X is unitary, it must be nonsingular, so
rank(A) = rank(XTX∗) = rank(T) ≥r,
and the proof is complete.
□
4.7
THE SIMULTANEOUS DIAGONALIZATION OF TWO SYMMETRIC
MATRICES
We have already discussed in Section 3.8 one manner in which two symmetric matri-
ces can be simultaneously diagonalized. We restate this result in Theorem 4.14.
Theorem 4.14
Let A and B be m × m symmetric matrices with B being posi-
tive definite. Let Λ = diag(λ1, . . . , λm), where λ1, . . . , λm are the eigenvalues of
B−1A. Then a nonsingular matrix C exists, such that
CAC′ = Λ, CBC′ = Im.
Example 4.12
One application of the simultaneous diagonalization described in
Theorem 4.14 is in a multivariate analysis commonly referred to as canonical variate
analysis (see Krzanowski, 2000 or Mardia, et al. 1979). This analysis involves data
from the multivariate one-way classification model discussed in Example 3.16, so that
we have independent random samples from k different groups or treatments, with the

THE SIMULTANEOUS DIAGONALIZATION OF TWO SYMMETRIC MATRICES
179
ith sample of m × 1 vectors given by yi1, . . . , yini. The model is
yij = μi + ϵij,
where μi is an m × 1 vector of constants and ϵij ∼Nm(0, Ω). In Example 3.16, we
saw how the matrices
B =
k

i=1
ni(yi −y)(yi −y)′,
W =
k

i=1
ni

j=1
(yij −yi)(yij −yi)′,
where
yi =
ni

j=1
yij/ni,
y =
k

i=1
niyi/n,
n =
k

i=1
ni,
could be used to test the hypothesis, H0 : μ1 = · · · = μk. Canonical variate analysis
is an analysis of the differences in the mean vectors, performed when this hypothe-
sis is rejected. This analysis is particularly useful when the differences between the
vectors μ1, . . . , μk are confined, or nearly confined, to some lower dimensional sub-
space of Rm. Note that if these vectors span an r-dimensional subspace of Rm, then
the population version of B,
Φ =
k

i=1
ni(μi −μ)(μi −μ)′,
where μ =  niμi/n, will have rank r; in fact, the eigenvectors of Φ corresponding
to its positive eigenvalues will span this r-dimensional space. Thus, a plot of the pro-
jections of μ1, . . . , μk onto this subspace will yield a reduced-dimension diagram of
the population means. Unfortunately, if Ω ̸= Im, it will be difficult to interpret the dif-
ferences in these mean vectors because Euclidean distance would not be appropriate.
This difficulty can be resolved by analyzing the transformed data Ω−1/2yij, where
Ω−1/2′Ω−1/2 = Ω−1, because Ω−1/2yij ∼Nm(Ω−1/2μi, Im). Thus, we would plot
the projections of Ω−1/2μ1, . . . , Ω−1/2μk onto the subspace spanned by the eigen-
vectors of Ω−1/2ΦΩ−1/2′ corresponding to its r positive eigenvalues; that is, if the
spectral decomposition of Ω−1/2ΦΩ−1/2′ is given by P1Λ1P ′
1, where P1 is an m × r
matrix satisfying P ′
1P1 = Ir and Λ1 is an r × r diagonal matrix, then we could simply
plot the vectors P ′
1Ω−1/2μ1, . . . , P ′
1Ω−1/2μk in Rr. The r components of the vector
vi = P ′
1Ω−1/2μi in this r-dimensional space are called the canonical variates means
for the ith population. Note that in obtaining these canonical variates, we have essen-
tially used the simultaneous diagonalization of Φ and Ω, because if C′ = (C′
1, C′
2)
satisfies

C1
C2

Φ[C′
1
C′
2] =

Λ1
(0)
(0)
(0)

,

180
MATRIX FACTORIZATIONS AND MATRIX NORMS

C1
C2

Ω[C′
1
C′
2] =

Ir
(0)
(0)
Im−r

,
then we can take C1 = P ′
1Ω−1/2. When μ1, . . . , μk are unknown, the canonical
variate means can be estimated by the sample canonical variate means, which are
computed using the sample means y1, . . . , yk and the corresponding simultaneous
diagonalization of B and W.
Theorem 4.14 is a special case of the more general result given next.
Theorem 4.15
Let A and B be m × m symmetric matrices, and suppose that there
is a linear combination of A and B that is positive definite. Then a nonsingular matrix
C exists, such that both CAC′ and CBC′ are diagonal.
Proof.
Let D = αA + βB be a linear combination of A and B for which D is
positive definite. If both α and β are zero, D would not be positive definite, so we
may assume without loss of generality that α ̸= 0. In this case, A may be written as
A = α−1(D −βB). Since D is positive definite, a nonsingular matrix T exists, such
that D = TT ′, or equivalently, T −1DT −1′ = Im. Further, T −1BT −1′ is symmetric,
so an orthogonal matrix P exists, for which P ′T −1BT −1′P = Δ is diagonal. Thus,
if we define C = P ′T −1, we have CDC′ = P ′P = Im and CBC′ = Δ; that is, B
is diagonalized by C and so also is A because CAC′ = α−1(CDC′ −βCBC′) =
α(Im −βΔ).
□
We can get another set of sufficient conditions for A and B to be simultaneously
diagonalizable by strengthening the condition on A given in Theorem 4.14 while
weakening the condition on B.
Theorem 4.16
Let A and B be m × m nonnegative definite matrices. Then a non-
singular matrix C exists, such that both CAC′ and CBC′ are diagonal.
Proof.
Let r1 = rank(A), r2 = rank(B), and assume without loss of generality
that r1 ≤r2. Let A = P1Λ1P ′
1 be the spectral decomposition of A, so that the
m × r1 matrix P1 satisfies P ′
1P1 = Ir1 and Λ1 is an r1 × r1 diagonal matrix
with positive diagonal elements. Define A1 = C1AC′
1 and B1 = C1BC′
1, where
C′
1 = [P1Λ−1/2
1
P2] and P2 is any m × (m −r1) matrix for which [P1
P2] is
orthogonal, and note that
A1 =

Ir1
(0)
(0)
(0)

,
so that C1 diagonalizes A. If any of the last m −r1 diagonal elements of B1 is zero,
then all elements in that row and column are zero because B1 is nonnegative definite
(Problem 3.51). Note that if the (r1 + i, r1 + i)th element of B1 is b ̸= 0 and the
(r1 + i, j)th and (j, r1 + i)th elements of B1 are a, and we define
T = Im −a
b eje′
r1+i,

THE SIMULTANEOUS DIAGONALIZATION OF TWO SYMMETRIC MATRICES
181
then TB1T ′ yields a matrix identical to B1 except that a multiple of row r1 + i has
been added to row j after which a multiple of column r1 + i has been added to col-
umn j so that the (r1 + i, j)th and (j, r1 + i)th elements are now each zero. We can
repeatedly use this process to get a nonsingular matrix C2, which is a product of
matrices of the form given for T so that
C2B1C′
2 =

B∗
(0)
(0)
D1

,
where B∗is an r1 × r1 matrix of rank r3, D1 is an (m −r1) × (m −r1) diagonal
matrix with r4 nonzero diagonal elements each of which is positive, and r3 + r4 = r2.
Since the matrix T, and hence also the matrix C2, when partitioned has the form

Ir1
E
(0)
F

for some r1 × (m −r1) and (m −r1) × (m −r1) matrices E and F, it follows that
C2A1C′
2 = A1. Finally, define C3 as
C3 =
Q′
(0)
(0)
Im−r1

,
where Q is an r1 × r1 orthogonal matrix satisfying B∗= QD2Q′ and D2 is an
r1 × r1 diagonal matrix with r3 nonzero diagonal elements each of which is
positive. Then with C = C3C2C1, we have CAC′ = diag(Ir1, (0)) and CBC′ =
diag(D2, D1).
□
We are now in a position to establish a useful determinantal inequality.
Theorem 4.17
Suppose A and B are m × m nonnegative definite matrices. Then
|A + B| ≥|A| + |B|,
with equality if and only if A = (0) or B = (0) or A + B is singular.
Proof.
Since A + B is also nonnegative definite, the inequality clearly holds when
|A| = |B| = 0, with equality if and only if A + B is singular. For the remainder of
the proof we assume, without loss of generality, that B is positive definite. Using
Theorem 4.14, we find that we can establish the result by showing that
m

i=1
(λi + 1) = |Λ + Im| ≥|Λ| + |Im| =
m

i=1
λi + 1,
(4.13)
with equality if and only if Λ = (0). We prove this by induction. For m = 2,
(λ1 + 1)(λ2 + 1) = λ1λ2 + λ1 + λ2 + 1 ≥λ1λ2 + 1

182
MATRIX FACTORIZATIONS AND MATRIX NORMS
since λ1 and λ2 are nonnegative, and we have equality if and only if λ1 = λ2 = 0.
Now if (4.13) holds for m −1, with equality if and only if λ1 = · · · = λm−1 = 0,
then
m

i=1
(λi + 1) =
m−1

i=1
(λi + 1)

(λm + 1) ≥
m−1

i=1
λi + 1

(λm + 1)
=
m

i=1
λi +
m−1

i=1
λi + λm + 1 ≥
m

i=1
λi + 1,
as is required. The first inequality is an equality if and only if λ1 = · · · = λm−1 = 0,
while the second is an equality if and only if m−1
i=1 λi = 0 and λm = 0, and so the
proof is complete.
□
The matrix C that diagonalizes A and B in Theorems 4.14, 4.15, and 4.16 is non-
singular but not necessarily orthogonal. Further, the diagonal elements of the two
diagonal matrices are not the eigenvalues of A nor B. This sort of diagonalization,
one which will be useful in our study of quadratic forms in normal random vectors in
Chapter 11, is what we consider next; we would like to know whether there exists an
orthogonal matrix that diagonalizes both A and B. Theorem 4.18 gives a necessary
and sufficient condition for such an orthogonal matrix to exist.
Theorem 4.18
Suppose that A and B are m × m symmetric matrices. Then an
orthogonal matrix P exists, such that P ′AP and P ′BP are both diagonal if and only
if A and B commute; that is, if and only if AB = BA.
Proof.
First suppose that such an orthogonal matrix does exist; that is, there is an
orthogonal matrix P such that P ′AP = Λ1 and P ′BP = Λ2, where Λ1 and Λ2 are
diagonal matrices. Then because Λ1 and Λ2 are diagonal matrices, clearly Λ1Λ2 =
Λ2Λ1, so we have
AB = PΛ1P ′PΛ2P ′ = PΛ1Λ2P ′ = PΛ2Λ1P ′
= PΛ2P ′PΛ1P ′ = BA,
and hence, A and B do commute. Conversely, now assuming that AB = BA, we
need to show that such an orthogonal matrix P does exist. Let μ1, . . . , μh be the
distinct values of the eigenvalues of A having multiplicities r1, . . . , rh, respectively.
Since A is symmetric, an orthogonal matrix Q exists, satisfying
Q′AQ = Λ1 = diag(μ1Ir1, . . . , μhIrh).
Performing this same transformation on B and partitioning the resulting matrix in the
same way that Q′AQ has been partitioned, we get

THE SIMULTANEOUS DIAGONALIZATION OF TWO SYMMETRIC MATRICES
183
C = Q′BQ =
⎡
⎢⎢⎢⎣
C11
C12
· · ·
C1h
C21
C22
· · ·
C2h
...
...
...
Ch1
Ch2
· · ·
Chh
⎤
⎥⎥⎥⎦,
where Cij is ri × rj. Note that because AB = BA, we must have
Λ1C = Q′AQQ′BQ = Q′ABQ = Q′BAQ
= Q′BQQ′AQ = CΛ1.
Equating the (i, j)th submatrix of Λ1C to the (i, j)th submatrix of CΛ1 yields the
identity μiCij = μjCij. Since μi ̸= μj if i ̸= j, we must have Cij = (0) if i ̸= j;
that is, the matrix C = diag(C11, . . . , Chh) is block diagonal. Now because C is
symmetric, so also is Cii for each i, and thus, we can find an ri × ri orthogonal
matrix Xi satisfying
X′
iCiiXi = Δi,
where Δi is diagonal. Let P = QX, where X is the block diagonal matrix
X = diag(X1, . . . , Xh), and note that
P ′P = X′Q′QX = X′X
= diag(X′
1X1, . . . , X′
hXh)
= diag(Ir1, . . . , Irh) = Im,
so that P is orthogonal. Finally, the matrix Δ = diag(Δ1, . . . , Δh) is diagonal and
P ′AP = X′Q′AQX = X′Λ1X
= diag(X′
1, . . . , X′
h) diag(μ1Ir1, . . . , μhIrh) diag(X1, . . . , Xh)
= diag(μ1X′
1X1, . . . , μhX′
hXh)
= diag(μ1Ir1, . . . , μhIrh) = Λ1
and
P ′BP = X′Q′BQX = X′CX
= diag(X′
1, . . . , X′
h) diag(C11, . . . , Chh) diag(X1, . . . , Xh)
= diag(X′
1C11X1, . . . , X′
hChhXh)
= diag(Δ1, . . . , Δh) = Δ,
and so the proof is complete.
□

184
MATRIX FACTORIZATIONS AND MATRIX NORMS
The columns of the matrix P are eigenvectors of A as well as B; that is, A and B
commute if and only if the two matrices have common eigenvectors. Also, note that
because A and B are symmetric, (AB)′ = B′A′ = BA, and so AB = BA if and only
if AB is symmetric. Theorem 4.18 easily generalizes to a collection of symmetric
matrices.
Theorem 4.19
Let A1, . . . , Ak be m × m symmetric matrices. Then an orthogonal
matrix P exists, such that P ′AiP = Λi is diagonal for each i if and only if AiAj =
AjAi for all pairs (i, j).
The two previous theorems involving symmetric matrices are special cases of more
general results regarding diagonalizable matrices. For instance, Theorem 4.19 is a
special case of the following result. The proof, which is similar to that given for
Theorem 4.18, is left as an exercise.
Theorem 4.20
Suppose that each of the m × m matrices A1, . . . , Ak is diagonal-
izable. Then a nonsingular matrix X exists, such that X−1AiX = Λi is diagonal for
each i if and only if AiAj = AjAi for all pairs (i, j).
4.8
MATRIX NORMS
In Chapter 2 , we saw that vector norms can be used to measure the size of a vector.
Similarly, we may be interested in measuring the size of an m × m matrix A or mea-
suring the closeness of A to another m × m matrix B. Matrix norms will provide the
means to do this. In a later chapter, we will need to apply some of our results on matrix
norms to matrices that are possibly complex matrices. Consequently, throughout this
section, we will not be restricting attention only to real matrices.
Definition 4.3
A function ||A|| defined on all m × m matrices A, real or complex,
is a matrix norm if the following conditions hold for all m × m matrices A and B:
(a) ||A|| ≥0.
(b) ||A|| = 0 if and only if A = (0).
(c) ||cA|| = |c| ||A|| for any complex scalar c.
(d) ||A + B|| ≤||A|| + ||B||.
(e) ||AB|| ≤||A|| ||B||.
Any vector norm defined on m2 × 1 vectors, when applied to the m2 × 1 vector
formed by stacking the columns of A, one on top of the other, will satisfy conditions
(a)–(d) because these are the conditions of a vector norm. However, condition (e),
which relates the sizes of A and B to that of AB, will not necessarily hold for vector
norms; that is, not all vector norms can be used as matrix norms.

MATRIX NORMS
185
Although a given vector norm might not be a matrix norm, it can always be used
to find a matrix norm.
Theorem 4.21
Suppose ||x|| is a vector norm defined on m × 1 vectors. Then
||A|| = max
||x||=1 ||Ax||
is a matrix norm defined on m × m matrices.
Proof.
We simply need to verify the five conditions of Definition 4.3. Condition (a)
follows immediately since the nonnegativity of ||A|| is guaranteed by the nonnegativ-
ity of ||Ax||. Clearly, A = (0) implies ||A|| = 0, while ||A|| = 0 implies ||Aei|| = 0,
that is, Aei = 0, for i = 1, . . . , m, so (b) holds. Condition (c) holds since for any
scalar c
||cA|| = max
||x||=1 ||cAx|| = |c| max
||x||=1 ||Ax|| = |c|||A||.
Next, for any unit vector x, we have
||(A + B)x|| = ||Ax + Bx|| ≤||Ax|| + ||Bx|| ≤||A|| + ||B||,
and so
||A + B|| = max
||x||=1 ||(A + B)x|| ≤||A|| + ||B||.
To establish (e), we first note that ||Ay|| ≤||A||||y|| holds, clearly when y = 0, and
when y ̸= 0 since
||A|| = max
||x||=1 ||Ax|| ≥||A y
||y|||| = ||Ay||/||y||
yields the required inequality. Thus, for any unit vector x,
||ABx|| = ||A(Bx)|| ≤||A||||Bx|| ≤||A||||B||,
so
||AB|| = max
||x||=1 ||ABx|| ≤||A||||B||.
□
The matrix norm, ||A||, given in Theorem 4.21 is sometimes referred to as the
matrix norm induced by the vector norm ||x||.
We now give examples of some commonly encountered matrix norms. We will
leave it to the reader to verify that these functions, in fact, satisfy the conditions

186
MATRIX FACTORIZATIONS AND MATRIX NORMS
of Definition 4.3. The Euclidean matrix norm is simply the Euclidean vector norm
computed on the stacked columns of A, and so it is given by
||A||E =
⎛
⎝
m

i=1
m

j=1
|aij|2
⎞
⎠
1/2
= {tr(A∗A)}1/2.
The maximum column sum matrix norm is given by
||A||1 = max
1≤j≤m
m

i=1
|aij|,
whereas the maximum row sum matrix norm is given by
||A||∞= max
1≤i≤m
m

j=1
|aij|.
The spectral norm uses the eigenvalues of A∗A; in particular, if μ1, . . . , μm are the
eigenvalues of A∗A, then the spectral norm is given by
||A||2 = max
1≤i≤m
√μi;
that is, the spectral norm of A is the largest singular value of A.
Example 4.13
In this example, we will show that the spectral matrix norm
||A||2 = √μ1,
where
μ1 ≥· · · ≥μm ≥0
are
the
eigenvalues
of
A∗A,
is
induced by the Euclidean vector norm ||x||2 = (x∗x)1/2. Let A = PDQ∗be
the singular value decomposition of A so that P and Q are m × m unitary
matrices and D = diag(√μ1, . . . , √μm). Put y = Q∗x so that x = Qy and
||x||2 = ||Qy||2 = (y∗Q∗Qy)1/2 = (y∗y)1/2 = ||y||2. Then
max
||x||2=1 ||Ax||2 = max
||x||2=1 ||PDQ∗x||2 = max
||x||2=1 {(PDQ∗x)∗(PDQ∗x)}1/2
= max
||x||2=1 (x∗QD2Q∗x)1/2 =
max
||Qy||2=1 (y∗D2y)1/2
= max
||y||2=1 (y∗D2y)1/2 = max
||y||2=1
 m

i=1
μi|yi|2
1/2
≤√μ1 max
||y||2=1
 m

i=1
|yi|2
1/2
= √μ1.
The inequality can be replaced by equality since if q1 is the first column of Q,
||Aq1||2 = √μ1 and ||q1||2 = 1.

MATRIX NORMS
187
We will find the following theorem useful. The proof, which simply involves the
verification of the conditions of Definition 4.3, is left to the reader as an exercise.
Theorem 4.22
Let ||A|| be any matrix norm defined on m × m matrices. If C is an
m × m nonsingular matrix, then the function defined by
||A||C = ||C−1AC||
is also a matrix norm.
The eigenvalues of a matrix A play an important role in the study of matrix norms
of A. Particularly important is the maximum modulus of this set of eigenvalues.
Definition 4.4
Let λ1, . . . , λm be the eigenvalues of the m × m matrix A. The
spectral radius of A, denoted ρ(A), is defined to be
ρ(A) = max
1≤i≤m |λi|.
Although ρ(A) does give us some information about the size of A, it is not a matrix
norm itself. To see this, consider the case in which m = 2 and
A =

0
1
0
0

.
Both of the eigenvalues of A are 0, so ρ(A) = 0 even though A is not the null matrix;
that is, ρ(A) violates condition (b) of Definition 4.3. Theorem 4.23 shows that ρ(A)
actually serves as a lower bound for any matrix norm of A.
Theorem 4.23
For any m × m matrix A and any matrix norm ||A||, ρ(A) ≤||A||.
Proof.
Suppose that λ is an eigenvalue of A for which |λ| = ρ(A), and let x be a
corresponding eigenvector, so that Ax = λx. Then x1′
m is an m × m matrix satis-
fying Ax1′
m = λx1′
m, and so using properties (c) and (e) of matrix norms, we find
that
ρ(A)||x1′
m|| = |λ| ||x1′
m|| = ||λx1′
m|| = ||Ax1′
m|| ≤||A|| ||x1′
m||.
The result now follows by dividing the equation above by ||x1′
m||.
□
Although the spectral radius of A is at least as small as every norm of A, our next
result shows that we can always find a matrix norm so that ||A|| is arbitrarily close to
ρ(A).

188
MATRIX FACTORIZATIONS AND MATRIX NORMS
Theorem 4.24
For any m × m matrix A and any scalar ϵ > 0, there exists a matrix
norm, ||A||A,ϵ, such that
||A||A,ϵ −ρ(A) < ϵ.
Proof.
Let A = XTX∗be the Schur decomposition of A, so that X is a unitary
matrix and T is an upper triangular matrix with the eigenvalues of A, λ1, . . . , λm, as
its diagonal elements. For any scalar c > 0, let the matrix Dc = diag(c, c2, . . . , cm)
and note that the diagonal elements of the upper triangular matrix DcTD−1
c
are also
λ1, . . . , λm. Further, the ith column sum of DcTD−1
c
is given by
λi +
i−1

j=1
c−(i−j)tji.
Clearly, by choosing c large enough, we can guarantee that
i−1

j=1
|c−(i−j)tij| < ϵ
for each i. In this case, because |λi| ≤ρ(A), we must have
||DcTD−1
c ||1 < ρ(A) + ϵ,
where ||A||1 denotes the maximum column sum matrix norm previously defined. For
any m × m matrix B, define ||B||A,ϵ as
||B||A,ϵ = ||(XD−1
c )−1B(XD−1
c )||1.
Since
||A||A,ϵ = ||(XD−1
c )−1A(XD−1
c )||1 = ||DcTD−1
c ||1,
the result follows from Theorem 4.22.
□
Often we will be interested in the limit of a sequence of vectors or the limit of a
sequence of matrices. The sequence of m × 1 vectors x1, x2, . . . converges to the
m × 1 vector x if the jth component of xk converges to the jth component of x,
as k →∞, for each j; that is, |xjk −xj| →0, as k →∞, for each j. Similarly, a
sequence of m × m matrices, A1, A2, . . . converges to the m × m matrix A if each
element of Ak converges to the corresponding element of A as k →∞. Alternatively,
we can consider the notion of the convergence of a sequence with respect to a specific
norm. Thus, the sequence of vectors x1, x2, . . . converges to x, with respect to the
vector norm ||x||, if ||xk −x|| →0 as k →∞. Theorem 4.25 indicates that the actual
choice of a norm is not important. For a proof of this result, see Horn and Johnson
(2013).

MATRIX NORMS
189
Theorem 4.25
Let ||x||a and ||x||b be any two vector norms defined on any m × 1
vector x. If x1, x2, . . . is a sequence of m × 1 vectors, then xk converges to x as
k →∞with respect to ||x||a if and only if xk converges to x as k →∞with respect
to ||x||b.
Since the first four conditions of a matrix norm are the conditions of a vector norm,
Theorem 4.25 immediately leads to the following.
Corollary 4.25.1
Let ||A||a and ||A||b be any two matrix norms defined on any
m × m matrix A. If A1, A2, . . . is a sequence of m × m matrices, then Ak converges
to A as k →∞with respect to ||A||a if and only if Ak converges to A as k →∞with
respect to ||A||b.
A sequence of matrices that is sometimes of interest is the sequence A, A2, A3, . . .
formed from a fixed m × m matrix A. A sufficient condition for this sequence of
matrices to converge to the null matrix is given next.
Theorem 4.26
Let A be an m × m matrix, and suppose that for some matrix norm,
||A|| < 1. Then lim Ak = (0) as k →∞.
Proof.
By repeatedly using condition (e) of a matrix norm, we find that ||Ak|| ≤
||A||k, and so ||Ak|| →0 as k →∞, because ||A|| < 1. Thus, Ak converges to (0)
with respect to the norm ||A||. However, by Corollary 4.25.1, Ak also converges to
(0) with respect to the matrix norm (see Problem 4.51)
||A||∗= m( max
1≤i,j≤m |aij|).
But this implies that |ak
ij| →0 as k →∞for each (i, j), and so the proof is complete.
□
Our next result relates the convergence of Ak to (0), to the size of the spectral
radius of A.
Theorem 4.27
Suppose that A is an m × m matrix. Then Ak converges to (0) as
k →∞if and only if ρ(A) < 1.
Proof.
Suppose that Ak →(0), in which case, Akx →0 for any m × 1 vector x.
Now if x is an eigenvector of A corresponding to the eigenvalue λ, we must also have
λkx →0, because Akx = λkx. This can only happen if |λ| < 1, and so ρ(A) < 1,
because λ was an arbitrary eigenvalue of A. On the other hand, if ρ(A) < 1, then we
know from Theorem 4.24 that there is a matrix norm satisfying ||A|| < 1. Hence, it
follows from Theorem 4.26 that Ak →(0).
□
Theorem 4.28 shows that the spectral radius of A is the limit of a particular
sequence that can be computed from any matrix norm.

190
MATRIX FACTORIZATIONS AND MATRIX NORMS
Theorem 4.28
Let A be an m × m matrix. Then for any matrix norm ||A||
lim
k→∞||Ak||1/k = ρ(A).
Proof.
λ is an eigenvalue of A if and only if λk is an eigenvalue of Ak. Further,
|λ|k = |λk|, so ρ(A)k = ρ(Ak). This, along with Theorem 4.23, yields ρ(A)k ≤
||Ak||, or equivalently, ρ(A) ≤||Ak||1/k. Thus, the proof will be complete if we can
show that for arbitrary ϵ > 0, an integer Nϵ exists, such that ||Ak||1/k < ρ(A) + ϵ for
all k > Nϵ. This is the same as showing that an integer Nϵ exists, such that for all
k > Nϵ, ||Ak|| < {ρ(A) + ϵ}k, or, equivalently,
||Bk|| < 1,
(4.14)
where B = {ρ(A) + ϵ}−1A. However,
ρ(B) =
ρ(A)
ρ(A) + ϵ < 1,
and so (4.14) follows immediately from Theorem 4.27.
□
Our final result gives a bound that is sometimes useful.
Theorem 4.29
Consider a matrix norm || · || and an m × m matrix A for which
||Im|| = 1 and ||A|| < 1. Then Im −A has an inverse and
||(Im −A)−1|| ≤
1
1 −||A||.
Proof.
Since ||A|| < 1, the series ∞
i=0 Ai converges to some matrix B. Note that
(Im −A)
n

i=0
Ai =
n

i=0
Ai −
n+1

i=1
Ai = Im −An+1.
The limit as n →∞of the left-hand side is (Im −A)B, while the limit of the
right-hand side is Im due to Theorem 4.26. Thus, we have B = ∞
i=0 Ai =
(Im −A)−1, and so
||(Im −A)−1|| = ||Im +
∞

i=1
Ai|| ≤||Im|| +
∞

i=1
||Ai||
≤1 +
∞

i=1
||A||i =
1
1 −||A||,
where the final equality uses the well-known result for a geometric series.
□

PROBLEMS
191
Example 4.14
Let A be an m × m nonsingular matrix. In this example, we wish
to compare the inverse of A to the inverse of a perturbation A + B of A, where
B is another m × m matrix satisfying ||B|| < 1/||A−1|| for some norm for which
||Im|| = 1. Now
||A−1B|| ≤||A−1||||B|| < ||A−1||/||A−1|| = 1,
and consequently, according to Theorem 4.29, (Im + A−1B) is nonsingular. Note
that
A−1 −(A + B)−1 = {A−1(A + B) −Im}(A + B)−1
= (Im + A−1B −Im)(A + B)−1
= A−1B(A + B)−1 = A−1B{A(Im + A−1B)}−1
= A−1B(Im + A−1B)−1A−1.
Using this identity, the bound given in Theorem 4.29, and the fact that ||A−1B|| ≤
||A−1||||B|| < 1, we then find that a bound on the norm of the error associated with
using the inverse of the perturbed matrix instead of the inverse of A is given by
||A−1 −(A + B)−1|| ≤||A−1||2||B||||(Im + A−1B)−1||
≤||A−1||2||B||
1 −||A−1B|| ≤
||A−1||2||B||
1 −||A−1||||B||.
The corresponding bound on the relative error is
||A−1 −(A + B)−1||
||A−1||
≤
||A−1||||B||
1 −||A−1||||B|| =
κ(A)||B||/||A||
1 −κ(A)||B||/||A||,
where κ(A), known as the condition number for matrix inversion, is defined as
κ(A) = ||A−1||||A||. Since ||B|| < 1/||A−1||, it follows that κ(A) < ||A||/||B||, while
a lower bound for κ(A) is given by
κ(A) = ||A||||A−1|| ≥||AA−1|| = ||Im|| = 1.
Clearly, as κ(A) increases, the bound on the relative error increases.
PROBLEMS
4.1 Obtain a singular value decomposition for the matrix
A =
1
2
2
1
1
1
1
−1

.

192
MATRIX FACTORIZATIONS AND MATRIX NORMS
4.2 Let A be an m × n matrix.
(a) Show that the singular values of A are the same as those of A′.
(b) Show that the singular values of A are the same as those of FAG, if F and
G are orthogonal matrices.
(c) If α ̸= 0 is a scalar, how do the singular values of αA compare with those
of A?
4.3 Let A be an m × m matrix. Show that A has a zero eigenvalue if and only if it
has fewer than m singular values.
4.4 Let A be m × n and B be n × m. We will see in Chapter 7 that the nonzero
eigenvalues of AB are the same as those of BA. This is not necessarily true
for the singular values. Give an example of matrices A and B for which the
singular values of AB are not the same as those of BA.
4.5 Let A be an m × n matrix having rank r and singular values μ1, . . . , μr. Show
that the (m + n) × (m + n) matrix
B =

(0)
A
A′
(0)

has eigenvalues μ1, . . . , μr, −μ1, . . . , −μr, with the remaining eigenvalues
being zero.
4.6 Find a singular value decomposition for the vector x = (1, 5, 7, 5)′.
4.7 Let x be an m × 1 nonnull vector and y be an n × 1 nonnull vector. Obtain a
singular value decomposition of xy′ in terms of x and y.
4.8 Let A be m × n with m ≤n. The polar decomposition of A is
A = BR,
where B is an m × m nonnegative definite matrix satisfying rank(B) =
rank(A) and R is an m × n matrix satisfying RR′ = Im. Use the singular
value decomposition to establish the existence of the polar decomposition.
4.9 Let A be an m × n matrix, and let A = P1ΔQ′
1 be the decomposition given
in Corollary 4.1.1. Define the n × m matrix B as B = Q1Δ−1P ′
1. Simplify, as
much as possible, the expressions for ABA and BAB.
4.10 Let
A
and
B
be
m × n
matrices.
From
Theorem
1.10
we
know
that if B = CAD, where C
and D are nonsingular matrices, then
rank(B) = rank(A). Prove the converse; that is, if rank(B) = rank(A),
show that nonsingular matrices C and D exist, such that B = CAD.
4.11 If t is an estimator of θ, then the mean squared error (MSE) of t is defined by
MSE(t) = E[(t −θ)2] = var(t) + {E(t) −θ}2.
Consider the multicollinearity problem discussed in Example 4.4 in which r of
the singular values of Z1 are very small relative to the others. Suppose that we

PROBLEMS
193
want to estimate the response variable corresponding to an observation that has
the standardized explanatory variables at the values given in the k × 1 vector
z. Let ˆy = y + z′(Z′
1Z1)−1Z′
1y be the estimate obtained using ordinary least
squares regression, whereas ˜y = y + z′U1D−1
1 V ′
1y is the estimate obtained
using principal components regression, both being estimates of θ = δ0 + z′δ1.
Assume throughout that ϵ ∼NN(0, σ2IN).
(a) Show that if the vector v = (v1, . . . , vN)′ satisfies z′ = v′DU ′, then
MSE(ˆy) = σ2

N −1 +
k

i=1
v2
i

.
(b) Show that
MSE(˜y) = σ2

N −1 +
k−r

i=1
v2
i

+

k

i=k−r+1
diviαi
2
,
where di is the ith diagonal element of D.
(c) If r = 1, when will MSE(˜y) < MSE(ˆy)?
4.12 Suppose that ten observations are obtained in a process involving two explana-
tory variables and a response variable resulting in the following data:
x1
x2
y
−2.49
6.49
28.80
0.85
4.73
21.18
−0.78
4.24
24.73
−0.75
5.54
25.34
1.16
4.74
28.50
−1.52
5.86
27.19
−0.51
5.65
26.22
−0.05
4.50
20.71
−1.01
5.75
25.47
0.13
5.69
29.83
(a) Obtain the matrix of standardized explanatory variables Z1, use ordinary
least squares to estimate the parameters in the model y = δ01N + Z1δ1
+ ϵ, and obtain the fitted values ˆy = ˆδ01N + Z1ˆδ1.
(b) Compute the spectral decomposition of Z′
1Z1. Then use principal compo-
nents regression to obtain an alternative vector of fitted values.

194
MATRIX FACTORIZATIONS AND MATRIX NORMS
(c) Use both models of (a) and (b) to estimate the response variable for an
observation having x1 = −2 and x2 = 4.
4.13 Consider the 3 × 3 symmetric matrix given by
A =
⎡
⎣
3
1
−1
1
3
1
−1
1
3
⎤
⎦.
(a) Find the spectral decomposition of A.
(b) Find the symmetric square root matrix for A.
(c) Find a nonsymmetric square root matrix for A.
4.14 Use the spectral decomposition theorem to prove Theorem 4.5
4.15 Find a 3 × 2 matrix T, such that TT ′ = A, where
A =
⎡
⎣
5
4
0
4
5
3
0
3
5
⎤
⎦.
4.16 Suppose x ∼N3(0, Ω), where
Ω =
⎡
⎣
2
1
1
1
2
1
1
1
2
⎤
⎦.
Find a 3 × 3 matrix A, such that the components of z = Ax are independently
distributed.
4.17 Let the matrices A, B, and C be given by
A =
⎡
⎣
1
2
5
2
1
4
−1
1
1
⎤
⎦,
B =
⎡
⎣
0
1
0
0
1
0
−1
1
1
⎤
⎦,
C =
⎡
⎣
2
1
−1
2
5
3
−2
−1
1
⎤
⎦.
(a) Which of these matrices are diagonalizable?
(b) Which of these matrices have their rank equal to the number of nonzero
eigenvalues?
4.18 A 3 × 3 matrix A has eigenvalues λ1 = 1, λ2 = 2, λ3 = 3, and corresponding
eigenvectors x1 = (1, 1, 2)′, x2 = (−1, 1, −2)′ and x3 = (1, 1, 1)′. Find A.
4.19 Let A and B be m × m matrices and suppose that one of them is nonsingular.
Show that if AB is diagonalizable, then BA must also be diagonalizable. Show
by example that this is not necessarily true when both A and B are singular.
4.20 Let A be an m × m positive definite matrix and B be an m × m symmetric
matrix. Show that AB is a diagonalizable matrix and the number of its positive,
negative, and zero eigenvalues are the same as that of B.

PROBLEMS
195
4.21 Let A be an m × m matrix and B be an n × n matrix. Prove that the matrix
C =
 A
(0)
(0)
B

is diagonalizable if and only if the matrices A and B are diagonalizable. Using
induction, show that the square matrices A1, . . . , Ak are diagonalizable if and
only if diag(A1, . . . , Ak) is diagonalizable.
4.22 Let x and y be m × 1 vectors. Show that A = xy′ is diagonalizable if and only
if x′y ̸= 0.
4.23 Let A = k
i=1 μiPA(μi) be the spectral decomposition of the diagonalizable
matrix A. Show that
(a) PA(μi)PA(μj) = (0), if i ̸= j,
(b) k
i=1 PA(μi) = Im,
(c) PA(μi) is the projection matrix for the projection onto the null space of
(A −μiIm) along the column space of (A −μiIm).
4.24 Suppose A is an m × m diagonalizable matrix with linearly independent eigen-
vectors given by the columns of the m × m matrix X. Show that linearly inde-
pendent eigenvectors of A′ are given by the columns of Y = X−1′.
4.25 Find a 4 × 4 matrix A having eigenvalues 0 and 1 with multiplicities 3 and 1,
respectively, such that
(a) the rank of A is 1,
(b) the rank of A is 2,
(c) the rank of A is 3.
4.26 Repeat Example 4.10 for 5 × 5 matrices; that is, obtain a collection of 5 × 5
matrices in Jordan canonical form, such that every 5 × 5 matrix having the
eigenvalue λ with multiplicity 5 is similar to one of the matrices in this set.
4.27 Consider the 6 × 6 matrix
J =
⎡
⎢⎢⎢⎢⎢⎢⎣
2
1
0
0
0
0
0
2
0
0
0
0
0
0
2
1
0
0
0
0
0
2
0
0
0
0
0
0
3
1
0
0
0
0
0
3
⎤
⎥⎥⎥⎥⎥⎥⎦
,
which is in Jordan canonical form.
(a) Find the eigenvalues of J and their multiplicities.
(b) Find the eigenspaces of J.
4.28 An m × m matrix B is said to be nilpotent if Bk = (0) for some positive integer
k.
(a) Show that Jh(λ) = λIh + Bh, where Bh is nilpotent. In particular, show
that Bh
h = (0).

196
MATRIX FACTORIZATIONS AND MATRIX NORMS
(b) Let J = diag(Jh1(λ1), . . . , Jhr(λr)) be a Jordan canonical form. Show
that J can be written as J = D + B, where D is diagonal and B is nilpotent.
What is the smallest h such that Bh = (0)?
(c) Use part (b) to show that if A is similar to J, then A can be expressed as
A = F + G, where F is diagonalizable and G is nilpotent.
4.29 Suppose that A is 5 × 5 with the eigenvalue λ having multiplicity 5. If
(A −λI5)2 = (0), what are the possible Jordan canonical forms for A?
4.30 If Jh(λ) is an h × h Jordan block matrix, find the eigenvalues of {Jh(λ)}2.
If λ ̸= 0, how many linearly independent eigenvectors does {Jh(λ)}2 have?
Use this information to show that if an m × m nonsingular matrix A has
Jordan decomposition A = B−1JB, where J = diag(Jh1(λ1), . . . , Jhr(λr)),
then A2 has the Jordan decomposition given by A2 = B−1
∗J∗B∗, where
J∗= diag(Jh1(λ2
1), . . . , Jhr(λ2
r)).
4.31 Let A be an m × m nilpotent matrix. In Problem 3.39, it was shown that all of
the eigenvalues of A are 0. Use this and the Jordan canonical form of A to show
that there must be a positive integer h ≤m satisfying Ah = (0).
4.32 Let A be an m × m matrix. Show that the rank of A is equal to the number of
nonzero eigenvalues of A if and only if rank(A2) = rank(A).
4.33 Suppose that λ is an eigenvalue of A with multiplicity r. Show that there are
r linearly independent eigenvectors of A corresponding to λ if and only if
rank(A −λIm) = rank{(A −λIm)2}.
4.34 Let A and B be m × m matrices. Suppose that an m × m unitary matrix X
exists, such X∗AX and X∗BX are both upper triangular matrices. Show then
that the eigenvalues of AB −BA are all equal to 0.
4.35 Let T and U be m × m upper triangular matrices. In addition, suppose
that for some positive integer r < m, tij = 0 for 1 ≤i ≤r, 1 ≤j ≤r, and
ur+1,r+1 = 0. Show that the upper triangular matrix V = TU is such that
vij = 0 for 1 ≤i ≤r + 1, 1 ≤j ≤r + 1.
4.36 Use the Schur decomposition of a matrix A and the result of the previous
exercise to prove the Cayley–Hamilton theorem given as Theorem 3.8; that is,
if λ1, . . . , λm are the eigenvalues of A, show that
(A −λ1Im)(A −λ2Im) · · · (A −λmIm) = (0).
4.37 Obtain a Schur decomposition for the matrix C given in Problem 4.17.
4.38 Repeat Problem 4.37 by obtaining a different Schur decomposition of C.
4.39 Let A be an m × m matrix, and consider the Schur decomposition given in
Theorem 4.12. Show that although the matrix T is not uniquely defined, the
quantity 
i<j|tij|2 is uniquely defined.
4.40 Let A and B be m × m matrices and suppose that AB = BA. Show that there
exists an m × m unitary matrix X such that X∗AX and X∗BX are upper
triangular.

PROBLEMS
197
4.41 Suppose that A and B are m × m and diagonalizable. Show that A and B com-
mute, that is, AB = BA, if and only if they are simultaneously diagonalizable;
in other words, AB = BA, if and only if a nonsingular matrix X exists, such
that both X−1AX and X−1BX are diagonal matrices. This proves Theorem
4.20 when k = 2.
4.42 Let
A =
1
0
0
1

,
B =
0
1
0
0

.
(a) Show that AB = BA.
(b) Show that AB is not diagonalizable.
(c) Why does this not contradict the result of Problem 4.41?
4.43 Suppose that the m × m matrices A and B are diagonalizable and AB = BA.
Denote the eigenvalues of A by λ1, . . . , λm and those of B by μ1, . . . , μm. If
the eigenvalues of A + B are γ1, . . . , γm, show that for k = 1, . . . , m,
γk = λik + μjk,
where (i1, . . . , im) and (j1, . . . , jm) are permutations of (1, . . . , m).
4.44 Let A and B be m × m matrices, and suppose that A and B commute.
(a) If A and B are nonsingular, show that A−1 and B−1 commute.
(b) If i and j are positive integers, show that Ai and Bj commute.
4.45 Find 2 × 2 matrices A and B that do not satisfy the conditions of Theorem
4.15 and Theorem 4.16, yet a nonsingular matrix C exists for which CAC′ and
CBC′ are diagonal.
4.46 Suppose that A and B are m × m positive definite matrices. Show that A −B
is positive definite if and only if B−1 −A−1 is positive definite.
4.47 Let A and B be m × m symmetric matrices with B being positive definite.
Show that A + B is positive definite if and only if every eigenvalue of AB−1 is
greater than −1.
4.48 Let A and B be m × m matrices, and suppose A has m distinct eigenvalues.
Show that if AB = BA, then there exists a nonsingular matrix X such that both
X−1AX and X−1BX are diagonal.
4.49 Suppose A and B are m × m symmetric matrices.
(a) Show that if B is positive definite, then AB is diagonalizable.
(b) Show that if both A and B are nonnegative definite, then AB is diagonal-
izable.
4.50 Show that the functions ||A||E, ||A||1, ||A||∞, and ||A||2 given in Section 4.8 are,
in fact, matrix norms.
4.51 Let A be an m × m matrix, and consider the function
||A||∗= m( max
1≤i,j≤m |aij|).

198
MATRIX FACTORIZATIONS AND MATRIX NORMS
Show that ||A||∗is a matrix norm.
4.52 Prove Theorem 4.22.
4.53 For any matrix norm defined on m × m matrices, show that
(a) ||Im|| ≥1,
(b) ||A−1|| ≥||A||−1, if A is an m × m nonsingular matrix.
4.54 Show that
(a) the maximum column sum matrix norm, ||A||1, is induced by the sum vector
norm, ||x||1,
(b) the maximum row sum matrix norm, ||A||∞, is induced by the infinity vector
norm, ||x||∞.
4.55 Let A be an m × m real matrix with singular values δ1, . . . , δr. Show that
||A||E =
 r

i=1
δ2
i
1/2
.
4.56 Let A be an m × m real matrix with singular values δ1 ≥· · · ≥δr, and suppose
that B is another m × m real matrix with rank(B) = s < r. Show that
||B −A||2
E ≥
r

i=s+1
δ2
i .
4.57 Find examples of 2 × 2 matrices A and B to show that it is possible to have
(a) ρ(A + B) > ρ(A) + ρ(B),
(b) ρ(AB) > ρ(A)ρ(B).
4.58 Consider the 2 × 2 matrix of the form
A =

a
1
0
a

.
(a) Determine Ak for general positive integer k.
(b) Find ρ(A) and ρ(Ak).
(c) For which values of a does Ak converge to (0) as k →∞? In this case,
show how to construct a norm so that ||A|| < 1.
4.59 In this problem, we consider a factorization of an m × m matrix A of the form
A = LU, where L is an m × m lower triangular matrix and U is an m × m
upper triangular matrix.
(a) Let Aj be the j × j submatrix of A consisting of the first j rows and j
columns of A. Show that if r = rank(A) and |Aj| ̸= 0, j = 1, . . . , r, then
Ar can be factored as Ar = L∗U∗, where L∗is an r × r nonsingular lower
triangular matrix and U∗is an r × r nonsingular upper triangular matrix.
Apply this result to then show that A may be factored as A = LU, where L

PROBLEMS
199
is an m × m lower triangular matrix and U is an m × m upper triangular
matrix.
(b) Show that not every m × m matrix has an LU factorization by finding a
2 × 2 matrix that cannot be factored in this way.
(c) Show how the LU factorization of A can be used to simplify the computa-
tion of a solution x, to the system of equations Ax = c.
4.60 Suppose that A is an m × m matrix. Show that an m × m lower triangular
matrix L, an m × m upper triangular matrix U, and m × m permutation matri-
ces P and Q exist, such that A = PLUQ.
4.61 Suppose that A is an m × m matrix for which |Aj| ̸= 0, j = 1, . . . , m, where
Aj denotes the j × j submatrix of A consisting of the first j rows and j columns
of A.
(a) Show that there exist m × m lower triangular matrices L and M having all
diagonal elements equal to one and an m × m diagonal matrix D, such that
A = LDM ′.
(b) Show that if A is also symmetric, then M = L, so that A = LDL′.


5
GENERALIZED INVERSES
5.1
INTRODUCTION
The inverse of a matrix is defined for all square matrices that are nonsingular. There
are some situations in which we may have a rectangular matrix or a square singular
matrix, A, and still be in need of another matrix that in some ways behaves like the
inverse of A. One such situation, which is often encountered in the study of statistics
as well as in many other fields of application, involves finding solutions to a system
of linear equations. A system of linear equations can be written in matrix form as
Ax = c,
where A is an m × n matrix of constants, c is an m × 1 vector of constants, and x
is an n × 1 vector of variables for which we need to find solutions. If m = n and
A is nonsingular, then A−1 exists, and so by premultiplying our system of equations
by A−1, we see that the system is satisfied only if x = A−1c; that is, the system
has a solution, the solution is unique, and it is given by x = A−1c. When A−1 does
not exist, how do we determine whether the system has any solutions, and if solutions
exist, how many solutions are there, and how do we find them? We will see in Chapter
6 that the answers to all of these questions can be conveniently expressed in terms of
the generalized inverses discussed in this chapter.
A second application of generalized inverses in statistics involves quadratic forms
and chi-squared distributions. Suppose we have an m-dimensional random vector x
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

202
GENERALIZED INVERSES
that has a mean vector of zero and covariance matrix Ω. A useful transformation
in some situations is one that transforms x to another random vector, z, having the
identity matrix as its covariance matrix. For instance, in Chapter 11, we will see that
if z has a normal distribution, then the sum of squares of the components of z, that
is, z′z, has a chi-squared distribution. We saw in Example 4.5 that if T is any m × m
matrix satisfying Ω−1 = TT ′, then z = T ′x will have Im as its covariance matrix.
Then
z′z = x′(T ′)′T ′x = x′(TT ′)x = x′Ω−1x,
which of course, will be possible only if Ω is positive definite. If Ω is positive semidef-
inite with rank r, then it will be possible to find m × m matrices A and B, with
A = BB′, such that when z is defined by z = B′x,
var(z) =
 Ir
(0)
(0)
(0)

,
and z′z = x′Ax. We will see later that A is a generalized inverse of Ω and z′z still
has a chi-squared distribution if z has a normal distribution.
5.2
THE MOORE–PENROSE GENERALIZED INVERSE
A useful generalized inverse in statistical applications is one developed by Moore
(1920, 1935) and Penrose (1955). This inverse is defined so as to possess four prop-
erties that the inverse of a square nonsingular matrix has.
Definition 5.1
The Moore–Penrose inverse of the m × n matrix A is the n × m
matrix, denoted by A+, which satisfies the conditions
AA+A = A,
(5.1)
A+AA+ = A+,
(5.2)
(AA+)′ = AA+,
(5.3)
(A+A)′ = A+A.
(5.4)
One of the most important features of the Moore–Penrose inverse, one that dis-
tinguishes it from other generalized inverses that we will discuss in this chapter, is
that it is uniquely defined. This fact, along with the existence of the Moore–Penrose
inverse, is established in Theorem 5.1.
Theorem 5.1
Corresponding to each m × n matrix A, one and only one n × m
matrix A+ exists satisfying conditions (5.1)–(5.4).
Proof.
First we will prove the existence of A+. If A is the m × n null matrix,
then it is easily verified that the four conditions in Definition 5.1 are satisfied with

THE MOORE–PENROSE GENERALIZED INVERSE
203
A+ = (0), the n × m null matrix. If A ̸= (0), so that rank(A) = r > 0, then from
Corollary 4.1.1, we know m × r and n × r matrices P and Q exist, such that P ′P =
Q′Q = Ir and
A = PΔQ′,
where Δ is a diagonal matrix with positive diagonal elements. Note that if we define
A+ = QΔ−1P ′, then
AA+A = PΔQ′QΔ−1P ′PΔQ′ = PΔΔ−1ΔQ′
= PΔQ′ = A,
A+AA+ = QΔ−1P ′PΔQ′QΔ−1P ′ = QΔ−1ΔΔ−1P ′
= QΔ−1P ′ = A+,
AA+ = PΔQ′QΔ−1P ′ = PP ′
is symmetric,
A+A = QΔ−1P ′PΔQ′ = QQ′
is symmetric.
Thus, A+ = QΔ−1P ′ is a Moore–Penrose inverse of A, and so we have established
the existence of the Moore–Penrose inverse. Next, suppose that B and C are any two
matrices satisfying conditions (5.1)–(5.4) for A+. Then using these four conditions,
we find that
AB = (AB)′ = B′A′ = B′(ACA)′ = B′A′(AC)′
= (AB)′AC = ABAC = AC
and
BA = (BA)′ = A′B′ = (ACA)′B′ = (CA)′A′B′
= CA(BA)′ = CABA = CA.
Now using these two identities and (5.2), we see that
B = BAB = BAC = CAC = C.
Since B and C are identical, the Moore–Penrose inverse is unique.
□
We saw in the proof of Theorem 5.1 that the Moore–Penrose inverse of a matrix
A is explicitly related to the singular value decomposition of A; that is, this inverse
is nothing more than a simple function of the component matrices making up the
singular value decomposition of A.
Definition 5.1 is the definition of a generalized inverse given by Penrose (1955).
The following alternative definition, which we will find useful on some occasions,
utilizes properties of the Moore–Penrose inverse that were first illustrated by

204
GENERALIZED INVERSES
Moore (1935). This definition applies the concept of projection matrices that was
discussed in Chapter 2. Recall that if S is a vector subspace of Rmand PS is its
projection matrix, then for any x ∈Rm, PSx gives the orthogonal projection of x
onto S, whereas x −PSx is the component of x orthogonal to S; further, the unique
matrix PS is given by x1x′
1 + · · · + xrx′
r, where {x1,. . ., xr} is any orthonormal
basis for S.
Definition 5.2
Let A be an m × n matrix. Then the Moore–Penrose inverse of A is
the unique n × m matrix A+ satisfying
(a) AA+ = PR(A),
(b) A+A = PR(A+),
where PR(A) and PR(A+) are the projection matrices of the range spaces of A and A+,
respectively.
It should be noted that although A+AA′ = A′, condition (b) in Definition 5.2 can-
not simply be replaced by the condition A+A = PR(A′). As shown by Hu (2008), this
substitution would then require a third condition, rank(A+) = rank(A).
The equivalence of Definition 5.1 and Definition 5.2 is not immediately obvious.
Consequently, we will establish it in Theorem 5.2.
Theorem 5.2
Definition 5.2 is equivalent to Definition 5.1.
Proof.
We first show that a matrix A+ satisfying Definition 5.2 must also satisfy
Definition 5.1. Conditions (5.3) and (5.4) follow immediately because by definition,
a projection matrix is symmetric, whereas (5.1) and (5.2) follow because the columns
of A are in R(A) imply that
AA+A = PR(A)A = A,
and the columns of A+ are in R(A+) imply that
A+AA+ = PR(A+)A+ = A+.
Conversely, now suppose that A+ satisfies Definition 5.1. Premultiplying (5.2) by A
yields the identity
AA+AA+ = (AA+)2 = AA+,
which along with (5.3) shows that AA+ is idempotent and symmetric, and thus by
Theorem 2.22 is a projection matrix. To show that it is the projection matrix of the
range space of A, note that for any matrices B and C, for which BC is defined,
R(BC) ⊆R(B). Using this twice along with (5.1), we find that
R(A) = R(AA+A) ⊆R(AA+) ⊆R(A),

SOME BASIC PROPERTIES OF THE MOORE–PENROSE INVERSE
205
so that R(AA+) = R(A). This proves that PR(A) = AA+. A proof of PR(A+) =
A+A is obtained in a similar fashion using (5.1) and (5.4).
□
5.3
SOME BASIC PROPERTIES OF THE MOORE–PENROSE INVERSE
In this section, we will establish some of the basic properties of the Moore–Penrose
inverse, whereas in some of the subsequent sections we will look at some more spe-
cialized results. First, we have Theorem 5.3.
Theorem 5.3
Let A be an m × n matrix. Then
(a) (αA)+ = α−1A+, if α ̸= 0 is a scalar,
(b) (A′)+ = (A+)′,
(c) (A+)+ = A,
(d) A+ = A−1, if A is square and nonsingular,
(e) (A′A)+ = A+A+′ and (AA′)+ = A+′A+,
(f) (AA+)+ = AA+ and (A+A)+ = A+A,
(g) A+ = (A′A)+A′ = A′(AA′)+,
(h) A+ = (A′A)−1A′ and A+A = In, if rank(A) = n,
(i) A+ = A′(AA′)−1 and AA+ = Im, if rank(A) = m,
(j) A+ = A′, if the columns of A are orthogonal, that is, A′A = In.
Proof.
Each part is proven by simply verifying that the stated inverse satisfies con-
ditions (5.1)–(5.4). Here, we will only verify that (A′A)+ = A+A+′, given in (e), and
leave the remaining proofs to the reader. Since A+ satisfies the four conditions of a
Moore–Penrose inverse, we find that
A′A(A′A)+A′A = A′AA+A+′A′A = A′AA+(AA+)′A
= A′AA+AA+A = A′AA+A = A′A,
(A′A)+A′A(A′A)+ = A+A+′A′AA+A+′ = A+(AA+)′AA+A+′
= A+AA+AA+A+′ = A+AA+A+′
= A+A+′ = (A′A)+,
so that A+A+′ satisfies conditions (5.1) and (5.2) of the Moore–Penrose inverse
(A′A)+. In addition, note that
A′A(A′A)+ = A′AA+A+′ = A′(A+(AA+)′)′
= A′(A+AA+)′ = A′A+′
= (A+A)′,

206
GENERALIZED INVERSES
and A+A must be symmetric by definition, so it follows that condition (5.3) is satis-
fied for (A′A)+ = A+A+′. Likewise, condition (5.4) holds because
(A′A)+A′A = A+A+′A′A = A+(AA+)′A
= A+AA+A = A+A.
This then proves that (A′A)+ = A+A+′.
□
Example 5.1
Properties (h) and (i) of Theorem 5.3 give useful ways of computing
the Moore–Penrose inverse of matrices that have full column rank or full row rank.
We will demonstrate this by finding the Moore–Penrose inverses of
a =

1
1

and
A =

1
2
1
2
1
0

.
From property (h), for any vector a ̸= 0, a+ will be given by (a′a)−1a′, so here we
find that
a+ = [0.5
0.5].
For A, we can use property (i) because rank(A) = 2. Computing AA′ and (AA′)−1,
we get
AA′ =
6
4
4
5

,
(AA′)−1 = 1
14
 5
−4
−4
6

,
and so
A+ = A′(AA′)−1 = 1
14
⎡
⎣
1
2
2
1
1
0
⎤
⎦
 5
−4
−4
6

= 1
14
⎡
⎣
−3
8
6
−2
5
−4
⎤
⎦.
Theorem 5.4 establishes a relationship between the rank of a matrix and the rank
of its Moore–Penrose inverse.
Theorem 5.4
For any m × n matrix A,
rank(A) = rank(A+) = rank(AA+) = rank(A+A).
Proof.
Using condition (5.1) and the fact that the rank of a matrix product cannot
exceed the rank of any of the matrices in the product, we find that
rank(A) = rank(AA+A) ≤rank(AA+) ≤rank(A+).
(5.5)

SOME BASIC PROPERTIES OF THE MOORE–PENROSE INVERSE
207
In a similar fashion, using condition (5.2), we get
rank(A+) = rank(A+AA+) ≤rank(A+A) ≤rank(A).
(5.6)
The result follows immediately from (5.5) and (5.6).
□
We have seen through Definition 5.2 and Theorem 5.2 that A+A is the projec-
tion matrix of the range of A+. It also will be the projection matrix of the range of
any matrix B satisfying rank(B) = rank(A+) and A+AB = B. For instance, from
Theorem 5.4, we have rank(A′) = rank(A+) and
A+AA′ = (A+A)′A′ = A′A+′A′ = A′,
so A+A is also the projection matrix of the range of A′; that is, PR(A′) = A+A.
Theorem 5.5 summarizes some of the special properties possessed by the
Moore–Penrose inverse of a symmetric matrix.
Theorem 5.5
Let A be an m × m symmetric matrix. Then
(a) A+ is also symmetric,
(b) AA+ = A+A,
(c) A+ = A, if A is idempotent.
Proof.
Using Theorem 5.3(b) and the fact that A = A′, we have
A+ = (A′)+ = (A+)′,
which then proves (a). To prove (b), note that it follows from condition (5.3) of the
Moore–Penrose inverse of a matrix, along with the symmetry of both A and A+, that
AA+ = (AA+)′ = A+′A′ = A+A.
Finally, (c) is established by verifying the four conditions of the Moore–Penrose
inverse for A+ = A, when A2 = A. For instance, both conditions (5.1) and (5.2) hold
because
AAA = A2A = AA = A2 = A.
Note also that
(AA)′ = A′A′ = AA,
so that conditions (5.3) and (5.4) hold as well.
□
In the proof of Theorem 5.1, we saw that the Moore–Penrose inverse of any matrix
can be conveniently expressed in terms of the components involved in the singular

208
GENERALIZED INVERSES
value decomposition of that matrix. Likewise, in the special case of a symmetric
matrix, we will be able to write the Moore–Penrose inverse in terms of the com-
ponents of the spectral decomposition of that matrix, that is, in terms of its eigen-
values and eigenvectors. Before identifying this relationship, we first consider the
Moore–Penrose inverse of a diagonal matrix. The proof of this result, which simply
involves the verification of conditions (5.1)–(5.4), is left to the reader.
Theorem 5.6
Let Λ be the m × m diagonal matrix diag(λ1,. . ., λm). Then the
Moore–Penrose inverse Λ+ of Λ is the diagonal matrix diag(φ1,. . ., φm), where
φi =

λ−1
i ,
if λi ̸= 0,
0,
if λi = 0.
Theorem 5.7
Let x1,. . ., xm be a set of orthonormal eigenvectors correspond-
ing to the eigenvalues, λ1,. . ., λm, of the m × m symmetric matrix A. If we define
Λ = diag(λ1,. . ., λm) and X = (x1,. . ., xm), then
A+ = XΛ+X′.
Proof.
Let r = rank(A), and suppose that we have ordered the λi’s so that
λr+1 = · · · = λm = 0. Partition X as X = [X1
X2], where X1 is m × r, and par-
tition Λ in block diagonal form as Λ = diag(Λ1, (0)), where Λ1 = diag(λ1,. . ., λr).
Then, the spectral decomposition of A is given by
A = [X1
X2]
Λ1
(0)
(0)
(0)
 X′
1
X′
2

= X1Λ1X′
1,
and similarly the expression above for A+ reduces to A+ = X1Λ−1
1 X′
1. Thus, because
X′
1X1 = Ir, we have
AA+ = X1Λ1X′
1X1Λ−1
1 X′
1 = X1Λ1Λ−1
1 X′
1 = X1X′
1,
which is clearly symmetric, so condition (5.3) is satisfied. Similarly, A+A = X1X′
1,
and so (5.4) also holds. Conditions (5.1) and (5.2) hold because
AA+A = (AA+)A = X1X′
1X1Λ1X′
1
= X1Λ1X′
1 = A
and
A+AA+ = A+(AA+) = X1Λ−1
1 X′
1X1X′
1
= X1Λ−1
1 X′
1 = A+,
and so the proof is complete.
□

SOME BASIC PROPERTIES OF THE MOORE–PENROSE INVERSE
209
Example 5.2
Consider the symmetric matrix
A =
⎡
⎣
32
16
16
16
14
2
16
2
14
⎤
⎦.
It is easily verified that an eigenanalysis of A reveals that it can be expressed as
A =
⎡
⎣
2/
√
6
0
1/
√
6
−1/
√
2
1/
√
6
1/
√
2
⎤
⎦

48
0
0
12
 
2/
√
6
1/
√
6
1/
√
6
0
−1/
√
2
1/
√
2

.
Thus, using Theorem 5.7, we find that
A+ =
⎡
⎣
2/
√
6
0
1/
√
6
−1/
√
2
1/
√
6
1/
√
2
⎤
⎦
1/48
0
0
1/12
 2/
√
6
1/
√
6
1/
√
6
0
−1/
√
2
1/
√
2

=
1
288
⎡
⎣
4
2
2
2
13
−11
2
−11
13
⎤
⎦.
In Section 2.7, we saw that if the columns of an m × r matrix X form a basis for
a vector space S, then the projection matrix of S is given by X(X′X)−1X′; that is
PR(X) = X(X′X)−1X′.
Definition 5.2 indicates how this can be generalized to the situation in which X is
not full column rank. Thus, using Definition 5.2 and Theorem 5.3(g), we find that the
projection matrix of the space spanned by the columns of X is
PR(X) = XX+ = X(X′X)+X′.
(5.7)
Example 5.3
We will use (5.7) to obtain the projection matrix of the range of
X =
⎡
⎣
4
1
3
−4
−3
−1
0
−2
2
⎤
⎦.
The Moore–Penrose inverse of
X′X =
⎡
⎣
32
16
16
16
14
2
16
2
14
⎤
⎦

210
GENERALIZED INVERSES
was obtained in the previous example, which we use to find that
PR(X) = X(X′X)+X′
=
1
288
⎡
⎣
4
1
3
−4
−3
−1
0
−2
2
⎤
⎦
⎡
⎣
4
2
2
2
13
−11
2
−11
13
⎤
⎦
⎡
⎣
4
−4
0
1
−3
−2
3
−1
2
⎤
⎦
= 1
3
⎡
⎣
2
−1
1
−1
2
1
1
1
2
⎤
⎦.
This illustrates the use of (5.7). Actually, PR(X) can be computed without ever for-
mally computing any Moore–Penrose inverse because PR(X) is the total eigenprojec-
tion corresponding to the positive eigenvalues of XX′. Here we have
XX′ =
⎡
⎣
26
−22
4
−22
26
4
4
4
8
⎤
⎦,
which has z1 = (1/
√
2, −1/
√
2, 0)′ and z2 = (1/
√
6, 1/
√
6, 2/
√
6)′ as normal-
ized eigenvectors corresponding to its two positive eigenvalues. Thus, if we let
Z = (z1, z2), then
PR(X) = ZZ′ = 1
3
⎡
⎣
2
−1
1
−1
2
1
1
1
2
⎤
⎦.
Example 5.4
The Moore–Penrose inverse is useful in constructing quadratic forms
in normal random vectors so that they have chi-squared distributions. This is a topic
that we will investigate in more detail in Chapter 11; here we will look at a simple
illustration. A common situation encountered in inferential statistics is one in which
one has a sample statistic, t ∼Nm(θ, Ω), and it is desired to determine whether
the m × 1 parameter vector θ = 0; formally, we want to test the null hypothesis
H0: θ = 0 versus the alternative hypothesis H1: θ ̸= 0. One approach to this prob-
lem, if Ω is positive definite, is to base the decision between H0 and H1 on the
statistic
v1 = t′Ω−1t.
Now if T is any m × m matrix satisfying TT ′ = Ω, and we define u = T −1t, then
E(u) = T −1θ and
var(u) = T −1{var(t)}T −1′ = T −1(TT ′)T −1′ = Im,

THE MOORE–PENROSE INVERSE OF A MATRIX PRODUCT
211
so u ∼Nm(T −1θ, Im). Consequently, u1,. . ., um are independently distributed nor-
mal random variables, and so
v1 = t′Ω−1t = u′u =
m
	
i=1
u2
i
has a chi-squared distribution with m degrees of freedom. This chi-squared distri-
bution is central if θ = 0 and noncentral if θ ̸= 0, so we would choose H1 over H0
if v1 is sufficiently large. When Ω is positive semidefinite, the construction of v1
above can be generalized by using the Moore–Penrose inverse of Ω. In this case, if
rank(Ω) = r, and we write Ω = X1Λ1X′
1 and Ω+ = X1Λ−1
1 X′
1, where the m × r
matrix X1 and the r × r diagonal matrix Λ1 are defined as in the proof of Theorem
5.7, then w = Λ−1/2
1
X′
1t ∼Nr(Λ−1/2
1
X′
1θ, Ir), because
var(w) = Λ−1/2
1
X′
1{var(t)}X1Λ−1/2
1
= Λ−1/2
1
X′
1(X1Λ1X′
1)X1Λ−1/2
1
= Ir.
Thus, because the wi’s are independently distributed normal random variables,
v2 = t′Ω+t = w′w =
r
	
i=1
w2
i
has a chi-squared distribution, which is central if Λ−1/2
1
X′
1θ = 0, with r degrees of
freedom.
5.4
THE MOORE–PENROSE INVERSE OF A MATRIX PRODUCT
If A and B each is an m × m nonsingular matrix, then it follows that (AB)−1 =
B−1A−1. This property of the matrix inverse does not immediately generalize to the
Moore–Penrose inverse of a matrix; that is, if A is m × p and B is p × n, then we
cannot, in general, be assured that (AB)+ = B+A+. In this section, we look at some
results regarding this sort of factorization of the Moore–Penrose inverse of a product.
Example 5.5
Here we look at a very simple example, given by Greville (1966),
that illustrates a situation in which the factorization does not hold. Define the 2 × 1
vectors
a =

1
0

,
b =

1
1

,
so that
a+ = (a′a)−1a′ = [1
0],
b+ = (b′b)−1b′ = [0.5
0.5].

212
GENERALIZED INVERSES
Thus, we have
(a′b)+ = (1)+ = 1 ̸= b+a+′ = 0.5.
Actually, in the previous section, we have already given a few situations in which
the identity (AB)+ = B+A+ does hold. For example, in Theorem 5.3, we saw that
(A′A)+ = A+A+′ = A+A′+
and
(AA+)+ = AA+ = (A+)+A+.
Theorem 5.8 gives yet another situation.
Theorem 5.8
Let A be an m × n matrix, whereas P and Q are h × m and n × p
matrices satisfying P ′P = Im and QQ′ = In. Then
(PAQ)+ = Q+A+P + = Q′A+P ′.
The proof of Theorem 5.8, which we leave to the reader, simply involves
the verification of conditions (5.1)–(5.4). Note that Theorem 5.7, regarding the
Moore–Penrose inverse of a symmetric matrix, is a special case of Theorem 5.8.
Our next result gives a sufficient condition on the matrices A and B to guarantee
that (AB)+ = B+A+.
Theorem 5.9
Let A be an m × p matrix and B be a p × n matrix. If rank(A) =
rank(B) = p, then (AB)+ = B+A+.
Proof.
Since A is full column rank and B is full row rank, we know from
Theorem 5.3 that A+ = (A′A)−1A′ and B+ = B′(BB′)−1. Consequently, we find
that
ABB+A+AB = ABB′(BB′)−1(A′A)−1A′AB = AB,
B+A+ABB+A+ = B′(BB′)−1(A′A)−1A′ABB′(BB′)−1(A′A)−1A′
= B′(BB′)−1(A′A)−1A′ = B+A+,
so conditions (5.1) and (5.2) are satisfied. In addition,
ABB+A+ = ABB′(BB′)−1(A′A)−1A′
= A(A′A)−1A′,
B+A+AB = B′(BB′)−1(A′A)−1A′AB
= B′(BB′)−1B
are symmetric, so B+A+ is the Moore–Penrose inverse of AB.
□

THE MOORE–PENROSE INVERSE OF A MATRIX PRODUCT
213
Although Theorem 5.9 is useful, its major drawback is that it only gives a sufficient
condition for the factorization of (AB)+. The following result, due to Greville (1966),
gives several necessary and sufficient conditions for this factorization to hold.
Theorem 5.10
Let A be an m × p matrix and B be a p × n matrix. Then each of
the following conditions are necessary and sufficient for (AB)+ = B+A+:
(a) A+ABB′A′ = BB′A′ and BB+A′AB = A′AB.
(b) A+ABB′ and A′ABB+ are symmetric matrices.
(c) A+ABB′A′ABB+ = BB′A′A.
(d) A+AB = B(AB)+AB and BB+A′ = A′AB(AB)+.
Proof.
We will prove that the conditions given in (a) are necessary and sufficient;
the proofs for (b)–(d) will be left to the reader as an exercise. First assume that the
conditions of (a) hold. Premultiplying the first identity by B+ while postmultiplying
by (AB)′+ yields
B+A+AB(AB)′(AB)′+ = B+BB′A′(AB)′+.
(5.8)
Now for any matrix C,
C+CC′ = (C+C)′C′ = C′C+′C′
= C′C′+C′ = C′.
(5.9)
Using this identity, when C = B, on the right-hand side of (5.8) and its transpose on
the left-hand side, when C = AB, we obtain the equation
B+A+AB = (AB)′(AB)′+,
which, because of condition (5.4), is equivalent to
B+A+AB = (AB)+(AB) = PR((AB)+).
(5.10)
The final equality in (5.10) follows from the definition of the Moore–Penrose inverse
in term of projection matrices, as given in Definition 5.2. In a similar fashion, if we
take the transpose of the second identity in (a), which yields
B′A′ABB+ = B′A′A
and premultiply this by (AB)′+ and postmultiply this by A+, then, after simplifying
by using (5.9) on the left-hand side with C = (AB)′ and the transpose of (5.9) on the
right-hand side with C = A′, we obtain the equation
ABB+A+ = (AB)(AB)+ = PR(AB).
(5.11)

214
GENERALIZED INVERSES
However, from Definition 5.2, (AB)+ is the only matrix satisfying both (5.10) and
(5.11). Consequently, we must have (AB)+ = B+A+. Conversely, now suppose that
(AB)+ = B+A+. Applying this equation in (5.9), when C = AB, gives
(AB)′ = B+A+(AB)(AB)′.
Premultiplying this by ABB′B, we obtain
ABB′BB′A′ = ABB′BB+A+ABB′A′,
which, after using the transpose of (5.9) with C = B′ and then rearranging, simpli-
fies to
ABB′(Ip −A+A)BB′A′ = (0).
Note that because D = (Ip −A+A) is symmetric and idempotent, the equation above
is in the form E′D′DE = (0), where E = BB′A′. This then implies that ED = (0);
that is,
(Ip −A+A)BB′A′ = (0),
which is equivalent to the first identity in (a). In a similar fashion, using (AB)+ =
B+A+ in (5.9) with C = (AB)′ yields
AB = A+′B+′B′A′AB.
This, when premultiplied by B′A′AA′, can be simplified to an equation that is equiv-
alent to the second identity of (a).
□
Our next step is to find a general expression for (AB)+ that holds for all A and
B for which the product AB is defined. Our approach involves transforming A to a
matrix A1 and transforming B to B1, such that AB = A1B1 and (A1B1)+ = B+
1 A+
1 .
The result, due to Cline (1964a), is given in Theorem 5.11.
Theorem 5.11
Let A be an m × p matrix and B be a p × n matrix. If we define
B1 = A+AB and A1 = AB1B+
1 , then AB = A1B1 and (AB)+ = B+
1 A+
1 .
Proof.
Note that
AB = AA+AB = AB1 = AB1B+
1 B1 = A1B1,
so the first result holds. To verify the second statement, we will show that the two
conditions given in Theorem 5.10(a) are satisfied for A1 and B1. First note that
A+A1 = A+AB1B+
1 = A+A(A+AB)B+
1
= A+ABB+
1 = B1B+
1
(5.12)

THE MOORE–PENROSE INVERSE OF PARTITIONED MATRICES
215
and
A+
1 A1 = A+
1 AB1B+
1 = A+
1 A(B1B+
1 B1)B+
1
= A+
1 A1B1B+
1 .
(5.13)
Taking the transpose of (5.13) and using (5.12), along with conditions (5.3) and (5.4),
we get
A+
1 A1 = B1B+
1 A+
1 A1 = A+A1A+
1 A1 = A+A1 = B1B+
1 ,
and so
A+
1 A1B1B′
1A′
1 = B1B+
1 B1B′
1A′
1 = B1B′
1A′
1,
which is the first identity in Theorem 5.10(a). The second identity can be obtained by
noting that
A′
1 = (AB1B+
1 )′ = (AB1B+
1 B1B+
1 )′
= (A1B1B+
1 )′ = B1B+
1 A′
1,
and then postmultiplying this identity by A1B1.
□
Note that in Theorem 5.11, B was transformed to B1 by the projection matrix of
the range space of A+, whereas A was transformed to A1 by the projection matrix of
the range space of B1 and not that of B. Our next result indicates that the range space
of B can be used instead of that of B1, if we do not insist that AB = A1B1. A proof
of this result can be found in Campbell and Meyer (1979).
Theorem 5.12
Let A be an m × p matrix and B be a p × n matrix. If we define
B1 = A+AB and A1 = ABB+, then (AB)+ = B+
1 A+
1 .
5.5
THE MOORE–PENROSE INVERSE OF PARTITIONED MATRICES
Suppose that the m × n matrix A has been partitioned as A = [U
V ], where U is
m × n1 and V is m × n2. In some situations, it may be useful to have an expression
for A+ in terms of the submatrices, U and V . We begin with the general case, in
which no assumptions can be made regarding U and V .
Theorem 5.13
Let the m × n matrix A be partitioned as A = [U
V ], where U is
m × n1 , V is m × n2, and n = n1 + n2. Then
A+ =
U + −U +V (C+ + W)
C+ + W

,
where C = (Im −UU +)V , M = {In2 + (In2 −C+C)V ′U +′U +V (In2 −C+C)}−1,
and W = (In2 −C+C)MV ′U +′U +(Im −V C+).

216
GENERALIZED INVERSES
Proof.
Partition A+ as
A+ =

X
Y

,
so that
AA+ = UX + V Y
(5.14)
and
A+A =

XU
XV
Y U
Y V

.
(5.15)
Since AA+A = A, we have
AA+U = U,
(5.16)
AA+V = V.
(5.17)
Transposing (5.16) and then premultiplying by (U ′U)+, we get
U +AA+ = U +,
(5.18)
because (U ′U)+U ′ = U + by Theorem 5.3(g). Also from Theorem 5.3(e) and (g), we
have A+ = A′A+′A+, so that
X = U ′X′X + U ′Y ′Y ,
which leads to
U +UX = U +UU ′X′X + U +UU ′Y ′Y
= U ′X′X + U ′Y ′Y
= X.
Thus, premultiplying (5.14) by U + and using (5.18), we find that U + = X + U +V Y ,
which implies that
A+ =

U + −U +V Y
Y

.
(5.19)
Consequently, the proof will be complete if we can show that Y = C+ + W. Since
U +C = (0), it follows (Problem 5.14) that
C+U = (0),
(5.20)

THE MOORE–PENROSE INVERSE OF PARTITIONED MATRICES
217
and using (5.16) and (5.17), we get AA+C = C, or equivalently C′ = C′AA+. This
last identity, when premultiplied by (C′C)+, yields C+ = C+AA+. Thus, using
(5.19), we have
AA+ = UU + + (Im −UU +)V Y = UU + + CY ,
and when this identity is premultiplied by C+, it reduces to C+ = C+CY , so that
CY = CC+.
(5.21)
Also, C = CC+C = CC+(V −UU +V ) = CC+V , which implies
C+V = C+C.
(5.22)
Thus, CY V = CC+V = CC+C = C, and from (5.15) we know that Y V is sym-
metric, so Y V C′ = C′ or when postmultiplying this last identity by (CC′)+,
Y V C+ = C+.
(5.23)
Using the expression for A+ given in (5.19) and the identity A+AA+ = A+, we find
that
Y UU + + Y CY = Y
or
Y UU + + Y CC+ = Y
(5.24)
because of (5.21). The symmetry condition (A+A)′ = A+A yields the equations
U +V Y U = (U +V Y U)′,
(5.25)
(Y U)′ = U +V (In2 −Y V ).
(5.26)
Now from (5.26) and the definition of C,
(Y U)′ = U +V {In2 −Y (UU +V + C)}
= U +V −U +V Y UU +V −U +V Y C,
and because by (5.20) and (5.21)
(In2 −C+C)Y U = Y U −C+CY U = Y U −C+CC+U = Y U,
it follows that
(Y U)′ = {(In2 −C+C)Y U}′ = (Y U)′(In2 −C+C)
= (U +V −U +V Y UU +V −U +V Y C)(In2 −C+C)
= (U +V −U +V Y UU +V )(In2 −C+C).

218
GENERALIZED INVERSES
Transposing this last equation and using (5.25), we get
Y U = (In2 −C+C)V ′U +′ −(In2 −C+C)V ′U +′U +V Y U
= (In2 −C+C)V ′U +′ −(In2 −C+C)V ′U +′U +V (In2 −C+C)Y U,
which leads to
BY U = (In2 −C+C)V ′U +′,
(5.27)
where B = In2 + (In2 −C+C)V ′U +′U +V (In2 −C+C). Postmultiplying (5.27) by
U +(Im −V C+) and then using (5.22), (5.23) and (5.24), we have
B(Y −C+) = (In2 −C+C)V ′U +′U +(Im −V C+).
Since B is the sum of In2 and a nonnegative definite matrix, it must be positive definite
and, hence, nonsingular. Thus, our previous equation can be re-expressed as
Y = C+ + B−1(In2 −C+C)V ′U +′U +(Im −V C+)
= C+ + (In2 −C+C)B−1V ′U +′U +(Im −V C+)
= C+ + W,
where we have used the fact that B−1 and (In2 −C+C) commute because B and
(In2 −C+C) commute, and B−1 = M. This completes the proof.
□
The proofs of the following consequences of Theorem 5.13 can be found in Cline
(1964b), Boullion and Odell (1971), or Pringle and Rayner (1971).
Corollary 5.13.1
Let A and C be defined as in Theorem 5.13, and let
K = (In2 + V ′U +′U +V )−1. Then
(a) C+CV ′U +′U +V = (0) if and only if
A+ =

U + −U +V KV ′U +′U +
C+ + KV ′U +′U +

,
(b) C = (0) if and only if
A+ =

U + −U +V KV ′U +′U +
KV ′U +′U +

,

THE MOORE–PENROSE INVERSE OF A SUM
219
(c) C+CV ′U +′U +V = V ′U +′U +V if and only if
A+ =

U + −U +V C+
C+

,
(d) U ′V = (0) if and only if
A+ =

U +
V +

.
Our final theorem involves the Moore–Penrose inverse of a partitioned matrix that
has the block diagonal form. This result can be easily proven by simply verifying that
the conditions of the Moore–Penrose inverse are satisfied.
Theorem 5.14
Let the m × n matrix A be given by
A =
⎡
⎢⎢⎢⎣
A11
(0)
· · ·
(0)
(0)
A22
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Arr
⎤
⎥⎥⎥⎦,
where Aii is mi × ni, m1 + · · · + mr = m, and n1 + · · · + nr = n. Then
A+ =
⎡
⎢⎢⎢⎣
A+
11
(0)
· · ·
(0)
(0)
A+
22
· · ·
(0)
...
...
...
(0)
(0)
· · ·
A+
rr
⎤
⎥⎥⎥⎦.
Some additional results for the generalized inverse of a matrix A that is partitioned
into a 2 × 2 form will be given in Chapter 7.
5.6
THE MOORE–PENROSE INVERSE OF A SUM
Theorem 1.9 gave an expression for (A + CBD)−1, when the matrices A, B, and
A + CBD are all square and nonsingular. Although a generalization of this formula
to the case of a Moore–Penrose inverse is not available, there are some specialized
results for the Moore–Penrose inverse of a sum of matrices. Some of these results
are presented in this section. The proofs of our first two results use the results of the
previous section regarding partitioned matrices. These proofs can be found in Cline
(1965) or Boullion and Odell (1971).
Theorem 5.15
Let U be an m × n1 matrix and V be an m × n2 matrix. Then
(UU ′ + V V ′)+ = (Im −C+′V ′)U +′KU +(Im −V C+) + (CC′)+,

220
GENERALIZED INVERSES
where K = In1 −U +V (In2 −C+C)M(U +V )′ and C and M are defined as in
Theorem 5.13.
Theorem 5.16
Suppose U and V are both m × n matrices. If UV ′ = (0), then
(U + V )+ = U + + (In −U +V )(C+ + W),
where C and W are as given in Theorem 5.13.
Theorem 5.16 gives an expression for (U + V )+ that holds when the rows of U
are orthogonal to the rows of V . If, in addition, the columns of U are orthogonal to
the columns of V , this expression greatly simplifies. This special case is summarized
in Theorem 5.17.
Theorem 5.17
If U and V are m × n matrices satisfying UV ′ = (0) and U ′V =
(0), then
(U + V )+ = U + + V +.
Proof.
Using Theorem 5.3(g), we find that
U +V = (U ′U)+U ′V = (0)
and
V U + = V U ′(UU ′)+ = {(UU ′)+′UV ′}′ = (0).
Similarly, we have V +U = (0) and UV + = (0). As a result,
(U + V )(U + + V +) = UU + + V V +,
(5.28)
(U + + V +)(U + V ) = U +U + V +V,
(5.29)
which are both symmetric, so that conditions (5.3) and (5.4) are satisfied. Postmulti-
plying (5.28) by (U + V ) and (5.29) by (U + + V +) yields conditions (5.1) and (5.2),
so the result follows.
□
Theorem 5.17 can be easily generalized to more than two matrices.
Corollary 5.17.1
Let U1,. . ., Uk be m × n matrices satisfying UiU ′
j = (0) and
U ′
iUj = (0) for all i ̸= j. Then
(U1 + · · · + Uk)+ = U +
1 + · · · + U +
k .

THE MOORE–PENROSE INVERSE OF A SUM
221
We saw in Corollary 1.9.2 that if A and A + cd′ are nonsingular matrices, then
(A + cd′)−1 = A−1 −A−1cd′A−1
1 + d′A−1c.
Our final theorem gives a generalization of this result to the case in which A + cd′ is
singular and A is symmetric.
Theorem 5.18
Let A be an m × m nonsingular symmetric matrix and c and d
be m × 1 vectors. Then A + cd′ is singular if and only if 1 + d′A−1c = 0, and if
A + cd′ is singular,
(A + cd′)+ = (Im −yy+)A−1(Im −xx+),
where x = A−1d and y = A−1c.
Proof.
Our proof follows that of Trenkler (2000). From Theorem 3.6, |A + cd′| =
|A|(1 + d′A−1c), and so the stated necessary and sufficient condition for the sin-
gularity of A + cd′ follows. Now if 1 + d′A−1c = 0, it follows that (A + cd′)y =
c + c(d′A−1c) = c −c = 0, which implies (A + cd′)yy+ = (0), or equivalently
(A + cd′)(Im −yy+) = A + cd′.
(5.30)
In a similar fashion, because A is symmetric we can show that
(Im −xx+)(A + cd′) = A + cd′.
(5.31)
Thus, using (5.31), we get
(Im −yy+)A−1(Im −xx+)(A + cd′) = (Im −yy+)A−1(A + cd′)
= (Im −yy+)(Im + yd′)
= (Im −yy+),
(5.32)
whereas an application of (5.30) confirms that
(A + cd′)(Im −yy+)A−1(Im −xx+) = (A + cd′)A−1(Im −xx+)
= (Im + cx′)(Im −xx+)
= (Im −xx+).
(5.33)
This establishes conditions (5.3) and (5.4) of a Moore–Penrose inverse. Condition
(5.1) follows by premultiplying (5.32) by (A + cd′) and then applying (5.30),
whereas condition (5.2) is obtained by postmultiplying (5.33) by (A + cd′) and then
applying (5.31). This completes the proof.
□

222
GENERALIZED INVERSES
5.7
THE CONTINUITY OF THE MOORE–PENROSE INVERSE
It is very useful to establish the continuity of a function because continuous functions
enjoy many nice properties. In this section, we will give conditions under which the
elements of A+ are continuous functions of the elements of A. However, before doing
so, let us first consider the determinant of a square matrix A and the inverse of a
nonsingular matrix A. Recall that the determinant of an m × m matrix A can be
expressed as the sum of terms, where each term is +1 or −1 times the product of m
of the elements of A. Thus, because of the continuity of sums and the continuity of
scalar products, we immediately have the following.
Theorem 5.19
Let A be an m × m matrix. Then the determinant of A, |A|, is a
continuous function of the elements of A.
Suppose that A is an m × m nonsingular matrix so that |A| ̸= 0. Recall that the
inverse of A can be expressed as
A−1 = |A|−1A#,
(5.34)
where A# is the adjoint matrix of A. If A1, A2,. . . is a sequence of matrices such
that Ai →A as i →∞, then, because of the continuity of the determinant function,
|Ai| →|A|, and so an N must exist, such that |Ai| ̸= 0 for all i > N. Since each
element of an adjoint matrix is +1 or −1 times a determinant, it also follows from
the continuity of the determinant function that if Ai# is the adjoint matrix of Ai, then
Ai# →A# as i →∞. As a result, (5.34) has allowed us to establish the following.
Theorem 5.20
Let A be an m × m nonsingular matrix. Then the inverse of A, A−1,
is a continuous function of the elements of A.
An alternative way of establishing the continuity of A−1 is to show directly that
if Ai →A, then for some matrix norm, ||A−1 −A−1
i || →0. Without loss of gener-
ality, we assume our norm satisfies the identity ||Im|| = 1. Let Bi = Ai −A so that
Ai = A + Bi = A(Im + A−1Bi). Now since Bi →(0), there exists an integer N
such that ||Bi|| < 1/||A−1|| for all i > N. Thus, following the derivation given in
Example 4.14, we find that for i > N,
||A−1 −(A + Bi)−1|| ≤
||A−1||2||Bi||
1 −||A−1||||Bi||.
From this it immediately follows that ||A−1 −A−1
i || = ||A−1 −(A + Bi)−1|| →0
since ||Bi|| →0.
The continuity of the Moore–Penrose inverse is not as straightforward as the con-
tinuity of the inverse of a nonsingular matrix. If A is an m × n matrix and A1, A2,. . .
is an arbitrary sequence of m × n matrices satisfying Ai →A as i →∞, then we are
not assured that A+
i →A+. A simple example will illustrate the potential problem.

THE CONTINUITY OF THE MOORE–PENROSE INVERSE
223
Example 5.6
Consider the sequence of 2 × 2 matrices A1, A2,. . . , where
Ai =

1/i
0
0
1

.
Clearly, Ai →A, where
A =
0
0
0
1

.
However, note that rank(A) = 1, whereas rank(Ai) = 2 for all i. For this reason, we
do not have A+
i →A+. In fact,
A+
i =

i
0
0
1

does not converge to anything because its (1, 1)th element, i, goes to ∞. On the other
hand,
A+ =

0
0
0
1

.
If we have a sequence of matrices A1, A2,. . . for which rank(Ai) = rank(A)
for all i larger than some integer, say N, then we will not encounter the difficulty
observed in Example 5.6; that is, as Ai gets closer to A, A+
i will get closer to A+. This
continuity property of A+ is summarized in Theorem 5.21. A proof of this important
result can be found in Penrose (1955) or Campbell and Meyer (1979).
Theorem 5.21
Let A be an m × n matrix and A1, A2,. . . be a sequence of m × n
matrices, such that Ai →A as i →∞. Then
A+
i →A+
as i →∞
if and only if an integer N exists, such that
rank(Ai) = rank(A)
for all i > N.
Example 5.7
The conditions for the continuity of the Moore–Penrose inverse have
important implications in estimation and hypothesis testing problems. In particular,
in this example, we will discuss a property, referred to as consistency, that some esti-
mators possess. An estimator t, computed from a sample of size n, is said to be a
consistent estimator of a parameter θ if t converges in probability to θ, that is, if
lim
n→∞P(|t −θ| ≥ϵ) = 0,

224
GENERALIZED INVERSES
for any ϵ > 0. An important result associated with the property of consistency is that
continuous functions of consistent estimators are consistent; that is, if t is a consis-
tent estimator of θ, and g(t) is a continuous function of t, then g(t) is a consistent
estimator of g(θ). We will now apply some of these ideas to a situation involving the
estimation of the Moore–Penrose inverse of a matrix of parameters. For instance, let Ω
be an m × m positive semidefinite covariance matrix having rank r < m. Suppose
that the elements of the matrix Ω are unknown and are, therefore, to be estimated.
Suppose, in addition, that our sample estimate of Ω, which we will denote by ˆΩ, is
positive definite with probability one, so that rank(ˆΩ) = m with probability one, and
ˆΩ is a consistent estimator of Ω; that is, each element of ˆΩ is a consistent estimator of
the corresponding element of Ω. However, because rank(Ω) = r < m, ˆΩ+ is not a
consistent estimator Ω+. Intuitively, the problem here is obvious. If ˆΩ = XΛX′ is the
spectral decomposition of ˆΩ so that ˆΩ+ = ˆΩ−1 = XΛ−1X′, then the consistency of
ˆΩ is implying that as n increases, the m −r smallest diagonal elements of Λ are con-
verging to zero, whereas the m −r largest diagonal elements of Λ−1 are increasing
without bound. The difficulty here can be easily avoided if the value of r is known. In
this case, ˆΩ can be adjusted to yield an estimator of Ω having rank r. For example, if
ˆΩ has eigenvalues λ1 ≥λ2 ≥· · · ≥λm and corresponding normalized eigenvectors
x1,. . ., xm, and Pr is the eigenprojection
Pr =
r
	
i=1
xix′
i,
then
ˆΩ∗= Pr ˆΩPr =
r
	
i=1
λixix′
i
will be an estimator of Ω having rank of r. It can be shown then that, because of the
continuity of eigenprojections, ˆΩ∗is also a consistent estimator of Ω. More impor-
tantly, because rank(ˆΩ∗) = rank(Ω) = r, Theorem 5.21 guarantees that ˆΩ+
∗is a
consistent estimator of Ω+.
5.8
SOME OTHER GENERALIZED INVERSES
The Moore–Penrose inverse is just one of many generalized inverses that have been
developed in recent years. In this section, we will briefly discuss two other general-
ized inverses that have applications in statistics. Both of these inverses can be defined
by applying some of the four conditions, (5.1)–(5.4), or, for simplicity, 1–4, of the
Moore–Penrose inverse. In fact, we can define a different class of inverses corre-
sponding to each different subset of the conditions 1–4 that the inverse must satisfy.
Definition 5.3
For any m × n matrix A, let the n × m matrix denoted A(i1,...,ir)
be any matrix satisfying conditions i1,. . ., ir from among the four conditions 1–4;
A(i1,...,ir) will be called a {i1,. . ., ir}-inverse of A.

SOME OTHER GENERALIZED INVERSES
225
Thus, the Moore–Penrose inverse of A is the {1, 2, 3, 4}-inverse of A; that is,
A+ = A(1,2,3,4). Note that for any proper subset {i1,. . ., ir} of {1, 2, 3, 4}, A+ will
also be a {i1,. . ., ir}-inverse of A, but it may not be the only one. Since in many
cases, there are many different {i1,. . ., ir}-inverses of A, it may be easier to com-
pute a {i1,. . ., ir}-inverse of A than to compute the Moore–Penrose inverse. The
rest of this section will be devoted to the {1}-inverse of A and the {1, 3}-inverse of
A, which have special applications that will be discussed in Chapter 6. Discussion
of other useful {i1,. . ., ir}-inverses can be found in Ben-Israel and Greville (2003),
Campbell and Meyer (1979), and Rao and Mitra (1971).
In Chapter 6, we will see that in solving systems of linear equations, we will only
need an inverse matrix satisfying the first condition of the four Moore–Penrose con-
ditions. We will refer to any such {1}-inverse of A as simply a generalized inverse
of A, and we will write it using the fairly common notation, A−; that is, A(1) = A−.
One useful way of expressing a generalized inverse of a matrix A applies the singular
value decomposition of A. The following result, which is stated for a matrix A having
less than full rank, can easily be modified for matrices having full row rank or full
column rank.
Theorem 5.22
Suppose that the m × n matrix A has rank r > 0 and the singular
value decomposition given by
A = P

Δ
(0)
(0)
(0)

Q′,
where P and Q are m × m and n × n orthogonal matrices, respectively, and Δ is an
r × r nonsingular diagonal matrix. Let
B = Q

Δ−1
E
F
G

P ′,
where E is r × (m −r), F is (n −r) × r, and G is (n −r) × (m −r). Then for all
choices of E, F, and G, B is a generalized inverse of A, and any generalized inverse
of A can be expressed in the form of B for some E, F, and G.
Proof.
Note that
ABA = P

Δ
(0)
(0)
(0)

Q′Q

Δ−1
E
F
G

P ′P

Δ
(0)
(0)
(0)

Q′
= P

ΔΔ−1Δ
(0)
(0)
(0)

Q′
= P
 Δ
(0)
(0)
(0)

Q′ = A,
and so the matrix B is a generalized inverse of A regardless of the choice of E,
F, and G. On the other hand, if we write Q = [Q1
Q2], P = [P1
P2], where

226
GENERALIZED INVERSES
Q1 is n × r and P1 is m × r, then, because PP ′ = Im, QQ′ = In, any generalized
inverse B, of A, can be expressed as
B = QQ′BPP ′ = Q
Q′
1
Q′
2

B[P1
P2]P ′
= Q

Q′
1BP1
Q′
1BP2
Q′
2BP1
Q′
2BP2

P ′,
which is in the required form if we can show that Q′
1BP1 = Δ−1. Since B is a gen-
eralized inverse of A, ABA = A, or equivalently,
(P ′AQ)(Q′BP)(P ′AQ) = P ′AQ.
Writing this last identity in partitioned form and equating the (1, 1)th submatrices on
both sides, we find that
ΔQ′
1BP1Δ = Δ,
from which it immediately follows that Q′
1BP1 = Δ−1, and so the proof is
complete.
□
When A is an m × m nonsingular matrix, the matrix B in Theorem 5.22 simplifies
to B = QΔ−1P ′, where Δ, P, and Q are now all m × m matrices. In other words, an
immediate consequence of Theorem 5.22 is that A−1 is the only generalized inverse
of A when A is square and nonsingular.
Example 5.8
The 4 × 3 matrix
A =
⎡
⎢⎢⎣
1
0
0.5
1
0
0.5
0
−1
−0.5
0
−1
−0.5
⎤
⎥⎥⎦
has rank r = 2 and singular value decomposition with
P = 1
2
⎡
⎢⎢⎣
1
1
1
−1
1
1
−1
1
1
−1
1
1
1
−1
−1
−1
⎤
⎥⎥⎦,
Q′ =
⎡
⎣
1/
√
2
−1/
√
2
0
1/
√
3
1/
√
3
1/
√
3
1/
√
6
1/
√
6
−2/
√
6
⎤
⎦,
and
Δ =
√
2
0
0
√
3

.

SOME OTHER GENERALIZED INVERSES
227
If we take E, F, and G as null matrices and use the equation for B given in Theorem
5.22, we obtain as a generalized inverse of A the matrix
1
12
⎡
⎣
5
5
1
1
−1
−1
−5
−5
2
2
−2
−2
⎤
⎦.
Actually, from the proof of Theorem 5.1, we know that the matrix above is the
Moore–Penrose inverse. Different generalized inverses of A may be constructed
through different choices of E, F, and G; for example, if we again take E and F as
null matrices but now use
G = [1/
√
6
0],
then we obtain the generalized inverse
1
6
⎡
⎣
3
2
1
0
0
−1
−2
−3
0
2
−2
0
⎤
⎦.
Note that this matrix has rank 3, whereas the Moore–Penrose inverse has its rank
equal to that of A, which is 2.
Theorem 5.23 summarizes some of the basic properties of {1}-inverses.
Theorem 5.23
Let A be an m × n matrix, and let A−be a generalized inverse of
A. Then
(a) A−′ is a generalized inverse of A′,
(b) if α is a nonzero scalar, α−1A−is a generalized inverse of αA,
(c) if A is square and nonsingular, A−= A−1 uniquely,
(d) if B and C are nonsingular, C−1A−B−1 is a generalized inverse of BAC,
(e) rank(A) = rank(AA−) = rank(A−A) ≤rank(A−),
(f) rank(A) = m if and only if AA−= Im,
(g) rank(A) = n if and only if A−A = In,
(h) if m = n and A is symmetric, there exists a generalized inverse A−that is
symmetric,
(i) if m = n and A is nonnegative definite, there exists a generalized inverse
A−that is nonnegative definite.
Proof.
Properties (a)–(d) are easily proven by simply verifying that the one condi-
tion of a generalized inverse holds. To prove (e), note that because A = AA−A, we
can use Theorem 2.8 to get
rank(A) = rank(AA−A) ≤rank(AA−) ≤rank(A)

228
GENERALIZED INVERSES
and
rank(A) = rank(AA−A) ≤rank(A−A) ≤rank(A),
so that rank(A) = rank(AA−) = rank(A−A). In addition,
rank(A) = rank(AA−A) ≤rank(A−A) ≤rank(A−),
so the result follows. It follows from (e) that rank(A) = m if and only if AA−is
nonsingular. Premultiplying the equation
(AA−)2 = (AA−A)A−= AA−
by (AA−)−1 yields (f). Similarly, rank(A) = n if and only if A−A is nonsingular,
and so premultiplying
(A−A)2 = A−(AA−A) = A−A
by (A−A)−1 gives (g). Properties (h) and (i) follow immediately from the fact that
A+ is a generalized inverse of A.
□
Example 5.9
Some of the properties possessed by the Moore–Penrose inverse
do not carry over to the {1}-inverse. For instance, we have seen that A is the
Moore–Penrose inverse of A+; that is, (A+)+ = A. However, in general, we are not
guaranteed that A is a generalized inverse of A−, where A−is an arbitrary gener-
alized inverse of A. For example, consider the diagonal matrix A = diag(0, 2, 4).
One choice of a generalized inverse of A is A−= diag(1, 0.5, 0.25). Here A−is
nonsingular, so it has only one generalized inverse, namely, (A−)−1 = diag(1, 2, 4),
and, thus, A is not a generalized inverse of A−= diag(1, 0.5, 0.25).
All generalized inverses of a matrix A can be expressed in terms of any one par-
ticular generalized inverse. This relationship is given below.
Theorem 5.24
Let A−be any generalized inverse of the m × n matrix A. Then for
any n × m matrix C,
A−+ C −A−ACAA−
is a generalized inverse of A, and each generalized inverse of A can be expressed in
this form for some C.
Proof.
Since AA−A = A,
A(A−+ C −A−ACAA−)A = AA−A + ACA −AA−ACAA−A
= A + ACA −ACA = A,

SOME OTHER GENERALIZED INVERSES
229
so A−+ C −A−ACAA−is a generalized inverse of A regardless of the choice of
A−and C. Now let B be any generalized inverse of A and define C = B −A−,
where A−is some particular generalized inverse of A. Then, because ABA = A,
we have
A−+ C −A−ACAA−= A−+ (B −A−) −A−A(B −A−)AA−
= B −A−ABAA−+ A−AA−AA−
= B −A−AA−+ A−AA−= B,
and so the proof is complete.
□
From Definition 5.2, we know that AA+ is the matrix that projects vectors orthog-
onally onto R(A), whereas A+A projects vectors orthogonally onto R(A′). Theorem
5.25 looks at the matrices AA−and A−A.
Theorem 5.25
Let A be an m × n matrix, B be an m × p matrix, and C be a q × n
matrix. Then
(a) AA−B = B if and only if R(B) ⊂R(A),
(b) CA−A = C if and only if R(C′) ⊂R(A′).
Proof.
Clearly if AA−B = B, the columns of B are linear combinations of the
columns of A from which it follows that R(B) ⊂R(A). Conversely, if R(B) ⊂
R(A) then an n × p matrix D exists, such that B = AD. Using this and the identity
AA−A = A, we have
B = AD = AA−AD = AA−B,
and so we have proven (a). Part (b) is proven in a similar fashion.
□
Like AA+, the matrix AA−projects vectors onto the vector space R(A). For
instance, from Theorem 5.25, we see that x = AA−x for any x ∈R(A), and if
x /∈R(A), then y = AA−x ∈R(A) because y clearly is a linear combination of the
columns of A. However, although AA+ projects vectors orthogonally onto R(A), the
projections given by AA−are oblique projections unless AA−is symmetric. In par-
ticular, AA−projects onto R(A) along the vector space R(Im −AA−). Similarly,
(A−A)′ = A′A−′ is the projection matrix for R(A′) along R(In −A′A−′).
We will find the following result useful in a later chapter.
Theorem 5.26
Let A, B, and C be matrices of sizes p × m, m × n, and n × q,
respectively. If rank(ABC) = rank(B), then C(ABC)−A is a generalized inverse
of B.

230
GENERALIZED INVERSES
Proof.
Our proof follows that of Srivastava and Khatri (1979). Using Theorem 2.8,
we have
rank(B) = rank(ABC) ≤rank(AB) ≤rank(B)
and
rank(B) = rank(ABC) ≤rank(BC) ≤rank(B),
so that evidently
rank(AB) = rank(BC) = rank(B) = rank(ABC).
(5.35)
Using Theorem 2.10 along with the identity
A(BC){Iq −(ABC)−ABC} = (0),
we find that
rank(ABC) + rank(BC{Iq −(ABC)−ABC}) −rank(BC) ≤rank{(0)} = 0,
so that
rank(BC{Iq −(ABC)−ABC}) ≤rank(BC) −rank(ABC) = 0,
where the equality follows from (5.35). But this can be true only if
BC{Iq −(ABC)−ABC} = {Iq −BC(ABC)−A}B(C) = (0).
Again applying Theorem 2.10, this time on the middle expression above, we obtain
rank({Iq −BC(ABC)−A}B) + rank(BC) −rank(B) ≤rank{(0)} = 0,
or equivalently,
rank({Iq −BC(ABC)−A}B) ≤rank(B) −rank(BC) = 0,
where, again, the equality follows from (5.35). This implies that
{Iq −BC(ABC)−A}B = B −B{C(ABC)−A}B = (0),
and so the result follows.
□
Some properties of a generalized inverse of A′A are given in Theorem 5.27.

SOME OTHER GENERALIZED INVERSES
231
Theorem 5.27
Let (A′A)−be any generalized inverse of A′A, where A is an m × n
matrix. Then
(a) (A′A)−′ is a generalized inverse of A′A,
(b) the matrix A(A′A)−A′ does not depend on the choice of the generalized
inverse (A′A)−,
(c) A(A′A)−A′ is symmetric even if (A′A)−is not symmetric.
Proof.
Transposing the equation A′A(A′A)−A′A = A′A yields
A′A(A′A)−′A′A = A′A,
so (a) follows. To prove (b) and (c), first note that
A(A′A)−A′A = AA+A(A′A)−A′A = (AA+)′A(A′A)−A′A
= A+′A′A(A′A)−A′A = A+′A′A
= (AA+)′A = AA+A = A.
Then
A(A′A)−A′ = A(A′A)−A′A+′A′ = A(A′A)−A′(AA+)′
= A(A′A)−A′AA+ = AA+,
(5.36)
where the last equality applies the identity, A(A′A)−A′A = A, just proven; (b) fol-
lows from (5.36) because A+, and hence also AA+, is unique. The symmetry of
A(A′A)−A′ follows from the symmetry of AA+.
□
We have seen that if S is the vector space spanned by the columns of the m × r
matrix X1, then its projection matrix is given by
PS = X1(X′
1X1)+X′
1.
(5.37)
An immediate consequence of Theorem 5.27 is that the Moore–Penrose inverse in
(5.37) can be replaced by any generalized inverse of X′
1X1; that is, regardless of the
choice of (X′
1X1)−, we have
PS = X1(X′
1X1)−X′
1.
We will see in Chapter 6 that the {1, 3}-inverse is useful in finding least squares
solutions to an inconsistent system of linear equations. Consequently, this inverse is
commonly called the least squares inverse. We will denote a {1, 3}-inverse of A by
AL; that is, A(1,3) = AL. Since a least squares inverse of A is also a {1}-inverse of A,

232
GENERALIZED INVERSES
the properties given in Theorem 5.23 also apply to AL. Some additional properties
of least squares inverses are given below.
Theorem 5.28
Let A be an m × n matrix. Then
(a) for any least squares inverse, AL, of A, AAL = AA+,
(b) (A′A)−A′ is a least squares inverse of A for any generalized inverse, (A′A)−,
of A′A.
Proof.
Since AALA = A and (AAL)′ = AAL, we find that
AAL = AA+AAL = (AA+)′(AAL)′ = A+′A′AL′A′
= A+′(AALA)′ = A+′A′ = (AA+)′ = AA+,
and so (a) holds. Part (b) follows from the proof of Theorem 5.27 because it was
shown that
A(A′A)−A′A = A,
which yields the first condition of a least squares inverse, and it was also shown that
A(A′A)−A′ = AA+,
so that the symmetry of A(A′A)−A′ follows from the symmetry of AA+.
□
5.9
COMPUTING GENERALIZED INVERSES
In this section, we review some computational formulas for generalized inverses. The
emphasis here is not on the development of formulas best suited for the numerical
computation of generalized inverses on a computer. For instance, the most common
method of computing the Moore–Penrose inverse of a matrix is through the computa-
tion of its singular value decomposition; that is, if A = P1ΔQ′
1 is the singular value
decomposition of A as given in Corollary 4.1.1, then A+ can be easily computed
via the formula A+ = Q1Δ−1P ′
1. The formulas provided here and in the problems
are ones that, in some cases, may be useful for the computation of the generalized
inverse of matrices of small size but, in most cases, are primarily useful for theoretical
purposes.
Greville (1960) obtained an expression for the Moore–Penrose inverse of a matrix
partitioned in the form [B
c], where, of course, the matrix B and the vector c have
the same number of rows. This formula can be then used recursively to compute
the Moore–Penrose inverse of an m × n matrix A. To see this, let aj denote the jth

COMPUTING GENERALIZED INVERSES
233
column of A and define Aj = (a1,. . ., aj), so that Aj is the m × j matrix containing
the first j columns of A. Greville has shown that if we write Aj = [Aj−1
aj], then
A+
j =
A+
j−1 −djb′
j
b′
j

,
(5.38)
where dj = A+
j−1aj,
b′
j =

(c′
jcj)−1c′
j,
if cj ̸= 0,
(1 + d′
jdj)−1d′
jA+
j−1,
if cj = 0,
and cj = aj −Aj−1dj. Thus, A+ = A+
n can be computed by successively comput-
ing A+
2 , A+
3 ,. . ., A+
n.
Example 5.10
We will use the procedure above to compute the Moore–Penrose
inverse of the matrix
A =
⎡
⎣
1
1
2
3
1
−1
0
1
1
1
2
3
⎤
⎦.
We begin by computing the inverse of A2 = [a1
a2] = [A1
a2]. We find that
A+
1 = (a′
1a1)−1a′
1 = 1
3

1
1
1

,
d2 = A+
1 a2 = 1
3,
c2 = a2 −A1d2 = a2 −1
3a1 = 1
3
⎡
⎣
2
−4
2
⎤
⎦.
Since c2 ̸= 0, we get
b′
2 = c+
2 = (c′
2c2)−1c′
2 = 1
4[1
−2
1],
and thus,
A+
2 =

A+
1 −d2b′
2
b′
2

= 1
4

1
2
1
1
−2
1

.
The inverse of A3 = [A2
a3] now can be computed by using A+
2 and
d3 = A+
2 a3 =

1
1

,

234
GENERALIZED INVERSES
c3 = a3 −A2d3 =
⎡
⎣
2
0
2
⎤
⎦−
⎡
⎣
2
0
2
⎤
⎦= 0.
Since c3 = 0, we find that
b′
3 = (1 + d′
3d3)−1d′
3A+
2 = (1 + 2)−1[1
1]1
4

1
2
1
1
−2
1

= 1
6[1
0
1],
and so
A+
3 =

A+
2 −d3b′
3
b′
3

= 1
12
⎡
⎣
1
6
1
1
−6
1
2
0
2
⎤
⎦.
Finally, to obtain the Moore–Penrose inverse of A = A4, we compute
d4 = A+
3 a4 =
⎡
⎣
1
0
1
⎤
⎦,
c4 = a4 −A3d4 =
⎡
⎣
3
1
3
⎤
⎦−
⎡
⎣
3
1
3
⎤
⎦= 0,
b′
4 = (1 + d′
4d4)−1d′
4A+
3 = 1
12[1
2
1].
Consequently, the Moore–Penrose inverse of A is given by
A+
4 =
A+
3 −d4b′
4
b′
4

= 1
12
⎡
⎢⎢⎣
0
4
0
1
−6
1
1
−2
1
1
2
1
⎤
⎥⎥⎦.
A common method of computing a generalized inverse, that is, a {1}-inverse, of
a matrix is based on the row reduction of that matrix to Hermite form.
Definition 5.4
An m × m matrix H is said to be in Hermite form if the following
four conditions hold:
(a) H is an upper triangular matrix.
(b) hii equals 0 or 1 for each i.
(c) If hii = 0, then hij = 0 for all j.
(d) If hii = 1, then hji = 0 for all j ̸= i

COMPUTING GENERALIZED INVERSES
235
Before applying this concept of Hermite forms to find a generalized inverse of a
matrix, we will need a couple of results regarding matrices in Hermite form. The first
of these two results says that any square matrix can be transformed to a matrix in
Hermite form through its premultiplication by a nonsingular matrix. Details of the
proof are given in Rao (1973).
Theorem 5.29
Let A be an m × m matrix. Then a nonsingular m × m matrix C
exists, such that CA = H, where H is in Hermite form.
The proof of Theorem 5.30 will be left to the reader as an exercise.
Theorem 5.30
Suppose the m × m matrix H is in Hermite form. Then H is idem-
potent; that is, H2 = H.
The connection between a generalized inverse of a square matrix A and matrices in
Hermite form is established in the following theorem. This result says that any matrix
C satisfying the conditions of Theorem 5.29 will be a generalized inverse of A.
Theorem 5.31
Let A be an m × m matrix and C be an m × m nonsingular matrix
for which CA = H, where H is a matrix in Hermite form. Then the matrix C is a
generalized inverse of A.
Proof.
We need to show that ACA = A. Now from Theorem 5.30, we know that
H is idempotent, and so
CACA = H2 = H = CA.
The result then follows by premultiplying this equation by C−1.
□
The matrix C can be obtained by transforming A, through elementary row trans-
formations, to a matrix in Hermite form. This process is illustrated in the following
example.
Example 5.11
We will find a generalized inverse of the 3 × 3 matrix
A =
⎡
⎣
2
2
4
4
−2
2
2
−4
−2
⎤
⎦.
First, we perform row transformations on A so that the resulting matrix has its first
diagonal element equal to one, whereas the remaining elements in the first column
are all equal to zero. This can be achieved via the matrix equation C1A = A1, where
C1 =
⎡
⎣
1/2
0
0
−2
1
0
−1
0
1
⎤
⎦,
A1 =
⎡
⎣
1
1
2
0
−6
−6
0
−6
−6
⎤
⎦.

236
GENERALIZED INVERSES
Next, we use row transformations on A1 so that the resulting matrix has its second
diagonal element equal to one, whereas each of the remaining elements in the second
column is zero. This can be written as C2A1 = A2, where
C2 =
⎡
⎣
1
1/6
0
0
−1/6
0
0
−1
1
⎤
⎦,
A2 =
⎡
⎣
1
0
1
0
1
1
0
0
0
⎤
⎦.
The matrix A2 satisfies the conditions of Definition 5.4, and so it is in Hermite form.
Thus, we have C2A1 = C2C1A = A2, so by Theorem 5.31 a generalized inverse of
A is given by
C = C2C1 = 1
6
⎡
⎣
1
1
0
2
−1
0
6
−6
6
⎤
⎦.
Not only is a generalized inverse not necessarily unique, but this particular method
of producing a generalized inverse does not, in general, yield a unique matrix. For
instance, in the second transformation given above, C2A1 = A2, we could have
chosen
C2 =
⎡
⎣
1
0
1/6
0
−1/6
0
0
−2
2
⎤
⎦.
In this case, we would have obtained the generalized inverse
C = C2C1 = 1
6
⎡
⎣
2
0
1
2
−1
0
12
−12
12
⎤
⎦.
The method of finding a generalized inverse of a matrix by transforming it to a
matrix in Hermite form can be easily extended from square matrices to rectangular
matrices. Theorem 5.32 indicates how such an extension is possible.
Theorem 5.32
Let A be an m × n matrix, where m < n. Define the matrix A∗as
A∗=

A
(0)

,
so that A∗is n × n, and let C be any n × n nonsingular matrix for which CA∗is in
Hermite form. If we partition C as C = [C1
C2], where C1 is n × m, then C1 is a
generalized inverse of A.

COMPUTING GENERALIZED INVERSES
237
Proof.
We know from Theorem 5.31 that C is a generalized inverse of A∗. Hence,
A∗CA∗= A∗. Simplifying the left-hand side of this identity, we find that
A∗CA∗=

A
(0)

[C1
C2]

A
(0)

=

AC1
AC2
(0)
(0)
 
A
(0)

=
AC1A
(0)

.
Equating this identity to A∗, we get AC1A = A, and so the proof is complete.
□
Clearly, an analogous result holds for the case in which m > n.
Example 5.12
Suppose that we wish to find a generalized inverse of the matrix
A =
⎡
⎢⎢⎣
1
1
2
1
0
1
1
1
2
2
0
2
⎤
⎥⎥⎦.
Consequently, we consider the augmented matrix
A∗= [A
0] =
⎡
⎢⎢⎣
1
1
2
0
1
0
1
0
1
1
2
0
2
0
2
0
⎤
⎥⎥⎦.
Proceeding as in the previous example, we obtain a nonsingular matrix C so that CA∗
is in Hermite form. One such matrix is given by
C =
⎡
⎢⎢⎣
0
1
0
0
1
−1
0
0
−1
0
1
0
0
−2
0
1
⎤
⎥⎥⎦.
Thus, partitioning this matrix as
C =
C1
C2

,
we find that a generalized inverse of A is given by
C1 =
⎡
⎣
0
1
0
0
1
−1
0
0
−1
0
1
0
⎤
⎦.

238
GENERALIZED INVERSES
A least squares generalized inverse of a matrix A can be computed by
first computing a generalized inverse of A′A and then using the relationship,
AL = (A′A)−A′, established in Theorem 5.28(b).
Example 5.13
To find a least squares inverse of the matrix A from Example 5.12,
we first compute
A′A =
⎡
⎣
7
2
9
2
2
4
9
4
13
⎤
⎦.
By transforming this matrix to Hermite form, we find that a generalized inverse of
A′A is given by
(A′A)−= 1
10
⎡
⎣
2
−2
0
−2
7
0
−10
−10
10
⎤
⎦.
Hence, a least squares inverse of A is given by
AL = (A′A)−A′ = 1
10
⎡
⎣
0
2
0
4
5
−2
5
−4
0
0
0
0
⎤
⎦.
PROBLEMS
5.1 Prove results (a)–(d) of Theorem 5.3.
5.2 Use Theorem 5.3(h) to find the Moore–Penrose inverse of
A =
⎡
⎢⎢⎣
1
1
1
0
1
0
0
1
1
2
0
1
⎤
⎥⎥⎦.
5.3 Find the Moore–Penrose inverse of the vector
a =
⎡
⎢⎢⎣
2
1
3
2
⎤
⎥⎥⎦.
5.4 Provide the proofs for (f)–(j) of Theorem 5.3.
5.5 Prove Theorem 5.6.

PROBLEMS
239
5.6 Use the spectral decomposition of the matrix
A =
⎡
⎣
2
0
1
0
2
3
1
3
5
⎤
⎦
to find its Moore–Penrose inverse.
5.7 Consider the matrix
A =
⎡
⎣
0
−1
2
0
−1
2
3
2
−1
⎤
⎦.
(a) Find the Moore–Penrose inverse of AA′, and then use Theorem 5.3(g) to
find A+.
(b) Use A+ to find the projection matrix for the range of A and the projection
matrix for the row space of A.
5.8 Show that the converse of Theorem 5.5(c) does not hold. That is, give an
example of a symmetric matrix A for which A+ = A yet A is not idempotent.
5.9 Let A be an m × n matrix with rank(A) = 1. Show that A+ = c−1A′, where
c = tr(A′A).
5.10 Let x and y be m × 1 vectors, and let 1m be the m × 1 vector with each element
equal to one. Obtain expressions for the Moore–Penrose inverses of
(a) 1m1′
m,
(b) Im −m−11m1′
m,
(c) xx′,
(d) xy′.
5.11 Let A be an m × n matrix. Show that each of the matrices, AA+, A+A,
(Im −AA+), and (In −A+A) is idempotent.
5.12 Let A be an m × n matrix. Establish the following identities:
(a) A′AA+ = A+AA′ = A′.
(b) A′A+′A+ = A+A+′A′ = A+.
(c) A(A′A)+A′A = AA′(AA′)+A = A.
5.13 Let A be an m × n matrix and B be an n × n positive definite matrix. Show that
ABA′(ABA′)+A = A.
5.14 Let A be an m × n matrix. Show that
(a) AB = (0) if and only if B+A+ = (0), where B is an n × p matrix.
(b) A+B = (0) if and only if A′B = (0), where B is an m × p matrix.
5.15 Let A be an m × m symmetric matrix having rank r. Show that if A has one
nonzero eigenvalue λ of multiplicity r, then A+ = λ−2A .

240
GENERALIZED INVERSES
5.16 Let A be an m × n matrix and B be an n × p matrix. Show that if B has full
row rank, then
AB(AB)+ = AA+.
5.17 Let A be an m × m symmetric matrix. Show that
(a) if A is nonnegative definite, then so is A+,
(b) if Ax = 0 for some vector x, then A+x = 0 also.
5.18 Let A be an m × m symmetric matrix with rank(A) = r. Use the spectral
decomposition of A to show that if B is any m × m symmetric matrix with
rank(B) = m −r, such that AB = (0), then A+A + B+B = Im.
5.19 Let A and B be m × m nonnegative definite matrices, and suppose that A −B
is also nonnegative definite. Show that B+ −A+ is nonnegative definite if and
only if rank(A) = rank(B).
5.20 Let A be an m × n matrix and B be an n × m matrix. Suppose that rank(A) =
rank(B) and, further, that the space spanned by the eigenvectors corresponding
to the positive eigenvalues of A′A is the same as that spanned by the eigenvec-
tors corresponding to the positive eigenvalues of BB′. Show that (AB)+ =
B+A+.
5.21 Prove Theorem 5.8.
5.22 Prove (b)–(d) of Theorem 5.10.
5.23 For each case below use Theorem 5.10 to determine whether (AB)+ = B+A+.
(a) A =
⎡
⎣
0
0
0
1
0
0
0
1
0
⎤
⎦,
B =
⎡
⎣
1
0
0
0
0
0
0
0
2
⎤
⎦.
(b) A =
⎡
⎣
1
1
0
0
1
0
0
0
0
⎤
⎦,
B =
⎡
⎣
0
0
0
0
1
1
0
1
0
⎤
⎦.
5.24 Let A be an m × n matrix and B be an n × m matrix. Show that
(AB)+ = B+A+ if A′ABB′ = BB′A′A.
5.25 Prove Theorem 5.14.
5.26 Find the Moore–Penrose inverse of the matrix
A =
⎡
⎢⎢⎢⎢⎣
2
1
0
0
0
1
1
0
0
0
0
0
1
2
0
0
0
1
2
0
0
0
0
0
4
⎤
⎥⎥⎥⎥⎦
.
5.27 Use Corollary 5.13.1(d) to find the Moore–Penrose inverse of the matrix
A = [ U
V ], where
U =
⎡
⎣
1
1
1
1
1
1
1
1
1
⎤
⎦,
V =
⎡
⎣
1
−2
−1
1
0
1
⎤
⎦.

PROBLEMS
241
5.28 Use Corollary 5.13.1(c) to find the Moore–Penrose inverse of the matrix
A = [ U
V ], where
U =
⎡
⎢⎢⎢⎢⎣
1
1
1
−1
1
0
1
0
0
0
⎤
⎥⎥⎥⎥⎦
,
V =
⎡
⎢⎢⎢⎢⎣
2
2
2
0
−1
0
1
−2
0
1
⎤
⎥⎥⎥⎥⎦
.
5.29 Let the vectors w, x, y, and z be given by
w =
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦,
x =
⎡
⎢⎢⎣
1
1
−2
0
⎤
⎥⎥⎦,
y =
⎡
⎢⎢⎣
1
−1
0
0
⎤
⎥⎥⎦,
z =
⎡
⎢⎢⎣
1
1
1
−3
⎤
⎥⎥⎦.
Use Theorem 5.17 to find the Moore–Penrose inverse of the matrix
A = wx′ + yz′.
5.30 Suppose A is an m × n matrix while c and d are m × 1 and n × 1 vectors. Let
u = (Im −AA+)c and v = d′(In −A+A).
(a) Show that
(A + cd′)+ = A+ −A+cu+ −v+d′A+ + (1 + d′A+c)v+u+,
if u ̸= 0 and v ̸= 0.
(b) Show that
(A + cd′)+ = A+ −A+c(A+c)+A+ −v+d′A+,
if u = 0, v ̸= 0, and 1 + d′A+c = 0.
(c) Show that
(A + cd′)+ = A+ −(1 + d′A+c)−1A+cd′A+,
if u = 0, v = 0, and 1 + d′A+c ̸= 0.
5.31 If the m × 1 random vector x has a multinomial distribution (see, for example,
Johnson, et al. 1997), then var(xi) = npi(1 −pi) and cov(xi, xj) = −npipj
for i ̸= j, where n is a positive integer and 0 < pi < 1, such that p1 + · · · +
pm = 1. If Ω is the covariance matrix of x, use Theorem 5.18 to show that Ω is
singular and
Ω+ = n−1(Im −m−11m1′
m)D−1(Im −m−11m1′
m),
where D = diag(p1,. . ., pm).

242
GENERALIZED INVERSES
5.32 Let A be an m × m nonsingular symmetric matrix and c and d be m × 1
vectors. Show that if A + cd′ is singular, then it has A−1 as a generalized
inverse.
5.33 Let A be an m × m symmetric matrix, and suppose that c and d are m × 1
vectors that are in the column space of A. Show that if 1 + d′A+c ̸= 0
(A + cd′)+ = A+ −A+cd′A+
1 + d′A+c.
5.34 Find a generalized inverse, different than the Moore–Penrose inverse, of the
vector given in Problem 5.3.
5.35 Consider the diagonal matrix A = diag(0, 2, 3).
(a) Find a generalized inverse of A having rank of 2.
(b) Find a generalized inverse of A that has rank of 3 and is diagonal.
(c) Find a generalized inverse of A that is not diagonal.
5.36 Let A be an m × m matrix partitioned as
A =

A11
A12
A21
A22

,
where A11 is r × r. Show that if rank(A) = rank(A11) = r, then

A−1
11
(0)
(0)
(0)

is a generalized inverse of A.
5.37 Let A be an m × n matrix and B be an n × p matrix. Show that B−A−will be
a generalized inverse of AB for any choice of A−and B−if rank(B) = n.
5.38 Let A be an m × n matrix and B be an n × p matrix. Show that for any choice of
A−and B−, B−A−will be a generalized inverse of AB if and only if A−ABB−
is idempotent.
5.39 Show that a matrix B is a generalized inverse of A if and only if AB is
idempotent and rank(A) = rank(AB).
5.40 Let A, P, and Q be m × n, p × m, and n × q matrices, respectively. Show
that if P has full column rank and Q has full row rank, then Q−A−P −is a
generalized inverse of PAQ.
5.41 Let A be an m × n matrix, B be an m × m matrix, and C be an n × n matrix.
Show that if B and C are nonsingular, then an n × m matrix D is a generalized
inverse of BAC if and only if D = C−1A−B−1 for some generalized inverse
A−of A.
5.42 Show that the matrix AA−yields orthogonal projections if and only if it is
symmetric; that is, show that
(x −AA−x)′AA−x = 0
for all x if and only if AA−is symmetric.

PROBLEMS
243
5.43 A matrix B is called a reflexive generalized inverse of A if it satisfies the first
two conditions of a Moore–Penrose inverse; that is, B is a reflexive inverse of
A if ABA = A and BAB = B. Show the following.
(a) A generalized inverse B of a matrix A is reflexive if and only if rank(B) =
rank(A).
(b) For any two matrices E and F,
B = Q

Δ−1
E
F
FΔE

P ′
is a reflexive inverse of A, where A has the singular value decomposition
given by
A = P
 Δ
(0)
(0)
(0)

Q′.
5.44 Suppose that the m × n matrix A is partitioned as A = [A1
A2], where
A1 is m × r, and rank(A) = rank(A1) = r. Show that A(A′A)−A′ =
A1(A′
1A1)−A′
1.
5.45 Suppose that the m × n matrix A has been partitioned as A′ = [U ′
V ′], where
U is m1 × n, V is m2 × n and m1 + m2 = m. Show that a generalized inverse
of A is given by A−= [W
X], where
W = (In −(In −U −U){V (In −U −U)}−V )U −,
X = (In −U −U){V (In −U −U)}−.
5.46 Use the recursive procedure described in Section 9 to obtain the Moore–Penrose
inverse of the matrix.
A =
⎡
⎣
1
−1
−1
−1
1
1
2
−1
1
⎤
⎦.
5.47 Find a generalized inverse of the matrix A in the previous problem by finding
a nonsingular matrix that transforms it into a matrix having Hermite form.
5.48 Find a generalized inverse of the matrix
A =
⎡
⎣
1
−1
−2
1
−2
4
3
−2
1
1
−3
1
⎤
⎦.
5.49 Find a least squares inverse for the matrix A given in the previous problem.
5.50 Let A be an m × m matrix and C be an m × m nonsingular matrix such that
CA = H is in Hermite form. Show that A is idempotent if and only if H is a
generalized inverse of A.

244
GENERALIZED INVERSES
5.51 Let A be an m × n matrix and suppose that B is an n × n matrix that satisfies
conditions 1 and 4 of the Moore–Penrose inverse of A′A. Show that A+ = BA′.
5.52 Let A be an m × n matrix and B be an n × m matrix. Show that B is the
Moore–Penrose inverse of A if and only if B is a least squares inverse of A and
A is a least squares inverse of B.
5.53 Let A be an m × n matrix. Show that for any least squares generalized inverse,
(AA′)L, of AA′, A+ = A′(AA′)L.
5.54 Let A be an m × n matrix. Show that an n × m matrix B is a least squares
generalized inverse of A if and only if A′AB = A′.
5.55 Let A be an m × n matrix, and let (AA′)−and (A′A)−be any generalized
inverses of AA′ and A′A, respectively. Show that
A+ = A′(AA′)−A(A′A)−A′.
5.56 It was shown in Theorem 5.31 that a generalized inverse of an m × m matrix A
can be obtained by finding a nonsingular matrix that row reduces A to Hermite
form. Show that there is a similar result for column reduction to Hermite form;
that is, show that if C is a nonsingular matrix, such that AC = H, where H is
in Hermite form, then C is a generalized inverse of A.
5.57 Prove Theorem 5.30.
5.58 Let A be an m × n matrix. Show that
A+ = lim
δ→0 (A′A + δ2In)−1A′ = lim
δ→0 A′(AA′ + δ2Im)−1.
5.59 Penrose (1956) obtained the following recursive method for calculating the
Moore–Penrose generalized inverse of an m × n matrix A. Successively
calculate B2, B3,. . . , where
Bi+1 = i−1tr(BiA′A)In −BiA′A
and B1 is defined to be the n × n identity matrix. If rank(A) = r, then
Br+1A′A = (0) and
A+ = r{tr(BrA′A)}−1BrA′.
Use this method to compute the Moore–Penrose inverse of the matrix A of
Example 5.10.
5.60 Let λ be the largest eigenvalue of AA′, where A is an m × n matrix. Let α be
any constant satisfying 0 < α < 2/λ, and define X1 = αA′. Ben-Israel (1966)
has shown that if we define
Xi+1 = Xi(2Im −AXi)

PROBLEMS
245
for i = 1, 2,. . . , then Xi →A+ as i →∞. Use this iterative procedure to
compute the Moore–Penrose inverse of the matrix A of Example 5.10 on a
computer. Stop the iterative process when
tr{(Xi+1 −Xi)′(Xi+1 −Xi)}
gets small. Note that λ does not need to be computed because we must have
2
tr(AA′) < 2
λ .
5.61 Use the results of Section 5 to obtain the expression given in (5.38) for the
Moore–Penrose inverse of the matrix Aj = [Aj−1
aj].


6
SYSTEMS OF LINEAR EQUATIONS
6.1
INTRODUCTION
As mentioned at the beginning of Chapter 5, one of the applications of generalized
inverses is in finding solutions to a system of linear equations of the form
Ax = c,
(6.1)
where A is an m × n matrix of constants, c is an m × 1 vector of constants, and
x is an n × 1 vector of variables for which solutions are needed. There are three
possibilities regarding the solution x to (6.1): there is no solution, there is exactly
one solution, there is more than one solution. We will see in Section 6.3 that if there
is more than one solution, then there are actually infinitely many solutions. In this
chapter, we discuss such issues as the existence of solutions to (6.1), the form of
a general solution, and the number of linearly independent solutions. We also look
at the special application of finding least squares solutions to (6.1), when an exact
solution does not exist.
6.2
CONSISTENCY OF A SYSTEM OF EQUATIONS
In this section, we will obtain necessary and sufficient conditions for the existence
of a vector x satisfying (6.1). When one or more such vectors exist, the system of
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

248
SYSTEMS OF LINEAR EQUATIONS
equations is said to be consistent; otherwise, the system is referred to as an incon-
sistent system. Our first necessary and sufficient condition for consistency is that the
vector c is in the column space of A or, equivalently, that the rank of the augmented
matrix [A
c] is the same as the rank of A.
Theorem 6.1
The system of equations, Ax = c, is consistent if and only if
rank([A
c]) = rank(A).
Proof.
If a1, . . . , an are the columns of A, then the equation Ax = c can be written
as
Ax = [a1
· · ·
an]
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦=
n

i=1
xiai = c.
Clearly, this equation holds for some x if and only if c is a linear combination of the
columns of A, in which case rank[A
c] = rank(A).
□
Example 6.1
Consider the system of equations that has
A =
⎡
⎣
1
2
2
1
1
0
⎤
⎦,
c =
⎡
⎣
1
5
3
⎤
⎦.
Clearly, the rank of A is 2, whereas
		[A
c]
		 =
						
1
2
1
2
1
5
1
0
3
						
= 0,
so that the rank of [A
c] is also 2. Thus, we know from Theorem 6.1 that the system
of equations Ax = c is consistent.
Although Theorem 6.1 is useful in determining whether a given system of linear
equations is consistent, it does not tell us how to find a solution to the system when
it is consistent. Theorem 6.2 gives an alternative necessary and sufficient condition
for consistency applying a generalized inverse, A−, of A. An obvious consequence
of this result is that when the system Ax = c is consistent, then a solution will be
given by x = A−c.
Theorem 6.2
The system of equations Ax = c is consistent if and only if for some
generalized inverse, A−, of A, AA−c = c.
Proof.
First, suppose that the system is consistent and x∗is a solution, so that c =
Ax∗. Premultiplying this identity by AA−, where A−is any generalized inverse of
A, yields
AA−c = AA−Ax∗= Ax∗= c,

CONSISTENCY OF A SYSTEM OF EQUATIONS
249
as is required. Conversely, now suppose that there is a generalized inverse of A sat-
isfying AA−c = c. Define x∗= A−c, and note that
Ax∗= AA−c = c.
Thus, because x∗= A−c is a solution, the system is consistent, and so the proof is
complete.
□
Suppose that A1 and A2 are any two generalized inverses of A so that AA1A =
AA2A = A. In addition, suppose that A1 satisfies the condition of Theorem 6.2; that
is, AA1c = c. Then A2 satisfies the same condition because
AA2c = AA2(AA1c) = (AA2A)A1c = AA1c = c.
Thus, in applying Theorem 6.2, one will need to check the given condition for only
one generalized inverse of A, and it does not matter which generalized inverse is used.
In particular, we can use the Moore–Penrose inverse, A+, of A.
Corollary 6.2.1 and Corollary 6.2.2 involve some special cases regarding the
matrix A.
Corollary 6.2.1
If A is an m × m nonsingular matrix and c is an m × 1 vector of
constants, then the system Ax = c is consistent.
Corollary 6.2.2
If the m × n matrix A has rank equal to m, then the system Ax = c
is consistent.
Proof.
Since A has full row rank, it follows from Theorem 5.23(f) that AA−= Im.
As a result, AA−c = c, and so from Theorem 6.2, the system must be consistent. □
Example 6.2
Consider the system of equations Ax = c, where
A =
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦,
c =
⎡
⎣
3
2
5
⎤
⎦.
A generalized inverse of the transpose of A was given in Example 5.12. Using this
inverse, we find that
AA−c =
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦
⎡
⎢⎢⎣
0
1
−1
1
−1
0
0
0
1
0
0
0
⎤
⎥⎥⎦
⎡
⎣
3
2
5
⎤
⎦
=
⎡
⎣
1
0
0
0
1
0
1
1
0
⎤
⎦
⎡
⎣
3
2
5
⎤
⎦=
⎡
⎣
3
2
5
⎤
⎦.

250
SYSTEMS OF LINEAR EQUATIONS
Since this is c, the system of equations is consistent, and a solution is given by
A−c =
⎡
⎢⎢⎣
0
1
−1
1
−1
0
0
0
1
0
0
0
⎤
⎥⎥⎦
⎡
⎣
3
2
5
⎤
⎦=
⎡
⎢⎢⎣
−3
1
5
0
⎤
⎥⎥⎦.
The generalized inverse given in Example 5.12 is not the only generalized inverse for
A so we could have solved this problem using a different generalized inverse. For
instance, it is easily verified that
A−=
⎡
⎢⎢⎣
3
3
−1
1
−1
0
−4
−3
2
0
0
0
⎤
⎥⎥⎦
satisfies AA−A = A. Using this choice for A−, we again find that the consistency
condition holds because
AA−c =
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦
⎡
⎢⎢⎣
3
3
−1
1
−1
0
−4
−3
2
0
0
0
⎤
⎥⎥⎦
⎡
⎣
3
2
5
⎤
⎦
=
⎡
⎣
0
−1
1
−1
0
1
−1
−1
2
⎤
⎦
⎡
⎣
3
2
5
⎤
⎦=
⎡
⎣
3
2
5
⎤
⎦.
However, we get a different solution because
A−c =
⎡
⎢⎢⎣
3
3
−1
1
−1
0
−4
−3
2
0
0
0
⎤
⎥⎥⎦
⎡
⎣
3
2
5
⎤
⎦=
⎡
⎢⎢⎣
10
1
−8
0
⎤
⎥⎥⎦.
The system of linear equations Ax = c is a special case of the more general system
of linear equations given by AXB = C, where A is m × n, B is p × q, C is m × q,
and X is n × p. A necessary and sufficient condition for the existence of a solution
matrix X satisfying this system is given in Theorem 6.3.
Theorem 6.3
Let A, B, and C be matrices of constants, where A is m × n, B is
p × q, and C is m × q. Then the system of equations,
AXB = C,

SOLUTIONS TO A CONSISTENT SYSTEM OF EQUATIONS
251
is consistent if and only if for some generalized inverses A−and B−,
AA−CB−B = C.
(6.2)
Proof.
Suppose that the system is consistent and the matrix X∗is a solution, so that
C = AX∗B. Premultiplying by AA−and postmultiplying by B−B, where A−and
B−are any generalized inverses of A and B, we find that
AA−CB−B = AA−AX∗BB−B = AX∗B = C,
and so (6.2) holds. On the other hand, if A−and B−satisfy (6.2), define X∗=
A−CB−, and note that X∗is a solution because
AX∗B = AA−CB−B = C,
so the proof is complete.
□
Using an argument similar to that given after Theorem 6.2, we can verify that if
(6.2) is satisfied for any one particular choice of A−and B−, then it will hold for all
choices of A−and B−. Consequently, the application of Theorem 6.3 is not dependent
on the choices of generalized inverses for A and B.
6.3
SOLUTIONS TO A CONSISTENT SYSTEM OF EQUATIONS
We have seen that if the system of equations Ax = c is consistent, then x = A−c
is a solution regardless of the choice of the generalized inverse A−. Thus, if A−c is
not the same for all choices of A−, then our system of equations has more than one
solution. In fact, we will see that even when A−c does not depend on the choice of
A−, which is the case if c = 0, our system of equations may have many solutions.
Theorem 6.4 gives a general expression for all solutions to the system.
Theorem 6.4
Suppose that Ax = c is a consistent system of equations, and let A−
be any generalized inverse of the m × n matrix A. Then, for any n × 1 vector y,
xy = A−c + (In −A−A)y
(6.3)
is a solution, and for any solution, x∗, a vector y exists, such that x∗= xy.
Proof.
Since Ax = c is a consistent system of equations, we know from Theorem
6.2 that AA−c = c, and so
Axy = AA−c + A(In −A−A)y
= c + (A −AA−A)y = c,

252
SYSTEMS OF LINEAR EQUATIONS
because AA−A = A. Thus, xy is a solution regardless of the choice of y. On the other
hand, if x∗is an arbitrary solution, so that Ax∗= c, it follows that A−Ax∗= A−c.
Consequently,
A−c + (In −A−A)x∗= A−c + x∗−A−Ax∗= x∗,
so that x∗= xx∗. This completes the proof.
□
The set of solutions given in Theorem 6.4 is expressed in terms of a fixed general-
ized inverse A−and an arbitrary n × 1 vector y. Alternatively, this set of all solutions
can be expressed in terms of an arbitrary generalized inverse of A.
Corollary 6.4.1
Suppose that Ax = c is a consistent system of equations, where
c ̸= 0. If B is a generalized inverse of A, then x = Bc is a solution, and for any
solution x∗, a generalized inverse B exists, such that x∗= Bc.
Proof.
Theorem 6.4 was not dependent on the choice of the generalized inverse, so
by choosing A−= B and y = 0 in (6.3), we prove that x = Bc is a solution. All that
remains to be shown is that for any particular A−and y, we can find a generalized
inverse B, such that the expression in (6.3) equals Bc. Now because c ̸= 0, it has
at least one component, say ci, not equal to 0. Define the n × m matrix C as C =
c−1
i ye′
i, so that Cc = y. Since the system of equations Ax = c is consistent, we must
have AA−c = c, and so
xy = A−c + (In −A−A)y = A−c + (In −A−A)Cc
= A−c + Cc −A−ACc = A−c + Cc −A−ACAA−c
= (A−+ C −A−ACAA−)c.
However, it follows from Theorem 5.24 that A−+ C −A−ACAA−is a generalized
inverse of A for any choice of the n × m matrix C, and so the proof is complete. □
Example 6.3
For the consistent system of equations discussed in Example 6.2, we
have
A−A =
⎡
⎢⎢⎣
0
1
−1
1
−1
0
0
0
1
0
0
0
⎤
⎥⎥⎦
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦
=
⎡
⎢⎢⎣
−1
−1
−1
−2
0
1
0
2
2
1
2
2
0
0
0
0
⎤
⎥⎥⎦,

SOLUTIONS TO A CONSISTENT SYSTEM OF EQUATIONS
253
where we have used the first of the two generalized inverses given in that example.
Consequently, a general solution to this system of equations is given by
xy = A−c + (I4 −A−A)y
=
⎡
⎢⎢⎣
−3
1
5
0
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
2
1
1
2
0
0
0
−2
−2
−1
−1
−2
0
0
0
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
y1
y2
y3
y4
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
−3 + 2y1 + y2 + y3 + 2y4
1 −2y4
5 −2y1 −y2 −y3 −2y4
y4
⎤
⎥⎥⎦,
where y is an arbitrary 4 × 1 vector.
Our next theorem gives a result, analogous to Theorem 6.4, for the system of
equations AXB = C. The proof will be left to the reader as an exercise.
Theorem 6.5
Let AXB = C be a consistent system of equations, where A is m ×
n, B is p × q, and C is m × q. Then for any generalized inverses, A−and B−, and
any n × p matrix, Y ,
XY = A−CB−+ Y −A−AY BB−
is a solution, and for any solution, X∗, a matrix Y exists, such that X∗= XY .
Example 6.4
Suppose an m × m nonsingular matrix C is partitioned as
C = [A
B], where A is m × m1 and B is m × m2. We wish to find an expression
for C−1 in terms of the submatrices A and B. Partition C−1 as C−1 = [X′
Y ′]′,
where X is m1 × m and Y is m2 × m. The system of equations XA = Im1 is
consistent since A−A = Im1, and so it follows from Theorem 6.5 that
X = A−+ U(Im −AA−),
(6.4)
where U
is an arbitrary m1 × m matrix. Note that Theorem 5.23(e) and
(Im −AA−)A = (0) guarantee that rank(Im −AA−) = m2, and since the
columns of B are linearly independent of those in A, we must also have
rank{(Im −AA−)B} = m2.
Thus,
XB = A−B + U(Im −AA−)B = (0)
is
consistent, and another application of Theorem 6.5 yields
U = −A−B{(Im −AA−)B}−
+V (Im −{(Im −AA−)B}{(Im −AA−)B}−),
(6.5)

254
SYSTEMS OF LINEAR EQUATIONS
where V is an arbitrary m1 × m matrix. Since (Im −AA−)B and (Im −AA−) have
the same rank, they also have the same column space, so it follows from Theorem 5.25
that
{(Im −AA−)B}{(Im −AA−)B}−(Im −AA−) = (Im −AA−).
Using this while substituting (6.5) into (6.4), we get
X = A−−A−B{Im −AA−)B}−(Im −AA−).
In a similar fashion, using the equations Y B = Im2 and Y A = (0), we can show that
Y = B−−B−A{(Im −BB−)A}−(Im −BB−).
In some applications, it may be important to know whether a consistent system of
equations yields a unique solution; that is, under what conditions will (6.3) yield the
same solution for all choices of y?
Theorem 6.6
If Ax = c is a consistent system of equations, then the solution
x∗= A−c is a unique solution if and only if A−A = In, where A−is any generalized
inverse of the m × n matrix A.
Proof.
Note that x∗= A−c is a unique solution if and only if xy = x∗for all
choices of y, where xy is as defined in (6.3). In other words, the solution is unique
if and only if
(In −A−A)y = 0
for all y, and clearly this is equivalent to the condition (In −A−A) = (0) or
A−A = In.
□
We saw in Theorem 5.23(g) that rank(A) = n if and only if A−A = In. As a
result, we can restate the necessary and sufficient condition of Theorem 6.6 as follows
in Corollary 6.6.1.
Corollary 6.6.1
Suppose that Ax = c is a consistent system of equations. Then the
solution x∗= A−c is a unique solution if and only if rank(A) = n.
Example 6.5
We saw in Example 6.1 that the system of equations Ax = c, where
A =
⎡
⎣
1
2
2
1
1
0
⎤
⎦,
c =
⎡
⎣
1
5
3
⎤
⎦,

SOLUTIONS TO A CONSISTENT SYSTEM OF EQUATIONS
255
is consistent. The Moore–Penrose inverse of the transpose of A was obtained in
Example 5.1. Using this inverse, we find that
A+A = 1
14

−3
6
5
8
−2
−4
 ⎡
⎣
1
2
2
1
1
0
⎤
⎦
= 1
14

14
0
0
14

= I2.
Thus, the system of equations Ax = c has the unique solution given by
A+c = 1
14

−3
6
5
8
−2
−4
 ⎡
⎣
1
5
3
⎤
⎦
= 1
14

 42
−14

=

 3
−1

.
Suppose that a system of linear equations has more than one solution, and let x1
and x2 be two different solutions. Then, because Axi = c for i = 1 and 2, it follows
that for any scalar α
A{αx1 + (1 −α)x2} = αAx1 + (1 −α)Ax2 = αc + (1 −α)c = c.
Thus, x = {αx1 + (1 −α)x2} is also a solution. Since α was arbitrary, we see that
if a system has more than one solution, then it has infinitely many solutions. However,
the number of linearly independent solutions to a consistent system of equations hav-
ing c ̸= 0 must be between 1 and n; that is, a set of linearly independent solutions
{x1, . . . , xr} exists, such that every solution can be expressed as a linear combi-
nation of the solutions, x1, . . . , xr. In other words, any solution x can be written
as x = α1x1 + · · · + αrxr, for some coefficients α1, . . . , αr. Note that because
Axi = c for each i, we must have
Ax = A
 r

i=1
αixi

=
r

i=1
αiAxi =
r

i=1
αic =
 r

i=1
αi

c,
and so if x is a solution, the coefficients must satisfy the identity α1 + · · · + αr = 1.
Theorem 6.7 tells us exactly how to determine this number of linearly independent
solutions r when c ̸= 0. We will delay the discussion of the situation in which c = 0
until the next section.
Theorem 6.7
Suppose that the system Ax = c is consistent, where A is m × n
and c ̸= 0. Then each solution can be expressed as a linear combination of r linearly
independent solutions, where r = n −rank(A) + 1.

256
SYSTEMS OF LINEAR EQUATIONS
Proof.
Using (6.3) with the particular generalized inverse A+, we begin with
the
n + 1
solutions,
x0 = A+c, xe1 = A+c + (In −A+A)e1, . . . , xen =
A+c + (In −A+A)en, where, as usual, ei denotes the n × 1 vector whose only
nonzero element is 1 in the ith position. Now every solution can be expressed as a
linear combination of these solutions because for any y = (y1, . . . , yn)′,
xy = A+c + (In −A+A)y =

1 −
n

i=1
yi

x0 +
n

i=1
yixei.
Thus, if we define the n × (n + 1) matrix X = (x0, xe1, . . . , xen), the proof will be
complete if we can show that rank(X) = n −rank(A) + 1. Note that we can write
X as X = BC, where B and C are the n × (n + 1) and (n + 1) × (n + 1) matrices
given by B = (A+c, In −A+A) and
C =

1
1′
n
0
In

.
Clearly, C is nonsingular because it is upper triangular and the product of its diagonal
elements is 1. Consequently, from Theorem 1.10, we know that rank(X) = rank(B).
Note also that
(In −A+A)′A+c = (In −A+A)A+c = (A+ −A+AA+)c
= (A+ −A+)c = 0,
so that the first column of B is orthogonal to the remaining columns. This implies
that
rank(B) = rank(A+c) + rank(In −A+A) = 1 + rank(In −A+A),
because the consistency condition AA+c = c and c ̸= 0 guarantee that A+c ̸= 0. All
that remains is to show that rank(In −A+A) = n −rank(A). Now because A+A is
the projection matrix of R(A+) = R(A′), it follows that In −A+A is the projection
matrix of the orthogonal complement of R(A′) or, in other words, the null space
of A, N(A). Since dim{N(A)} = n −rank(A), we must have rank(In −A+A) =
n −rank(A).
□
Since x0 = A+c is orthogonal to the columns of (In −A+A), when constructing
a set of r linearly independent solutions, one of these solutions always will be x0,
with the remaining solutions given by xy for r −1 different choices of y ̸= 0. This
statement is not dependent on the choice of A+ as the generalized inverse in (6.3),
because A−c and (In −A−A)y are linearly independent regardless of the choice of
A−if c ̸= 0, y ̸= 0. The proof of this linear independence is left as an exercise.

SOLUTIONS TO A CONSISTENT SYSTEM OF EQUATIONS
257
Example 6.6
We saw that the system of equations Ax = c of Examples 6.2 and
6.3 has the set of solutions consisting of all vectors of the form
xy = A−c + (I4 −A−A)y =
⎡
⎢⎢⎣
−3 + 2y1 + y2 + y3 + 2y4
1 −2y4
5 −2y1 −y2 −y3 −2y4
y4
⎤
⎥⎥⎦.
Since the last row of the 3 × 4 matrix
A =
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦
is the sum of the first two rows, rank(A) = 2. Thus, the system of equations possesses
n −rank(A) + 1 = 4 −2 + 1 = 3
linearly independent solutions. Three linearly independent solutions can be obtained
through appropriate choices of the y vector. For instance, because A−c and (I4 −
A−A)y are linearly independent, the three solutions
A−c,
A−c + (I4 −A−A)·i,
A−c + (I4 −A−A)·j
will be linearly independent if the ith and jth columns of (I4 −A−A) are linearly
independent. Looking back at the matrix (I4 −A−A) given in Example 6.3, we see
that its first and fourth columns are linearly independent. Thus, three linearly inde-
pendent solutions of Ax = c are given by
A−c =
⎡
⎢⎢⎣
−3
1
5
0
⎤
⎥⎥⎦,
A−c + (I4 −A−A)·1 =
⎡
⎢⎢⎣
−3
1
5
0
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
2
0
−2
0
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1
1
3
0
⎤
⎥⎥⎦,
A−c + (I4 −A−A)·4 =
⎡
⎢⎢⎣
−3
1
5
0
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
2
−2
−2
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1
−1
3
1
⎤
⎥⎥⎦.

258
SYSTEMS OF LINEAR EQUATIONS
6.4
HOMOGENEOUS SYSTEMS OF EQUATIONS
The system of equations Ax = c is called a nonhomogeneous system of equations
when c ̸= 0, whereas Ax = 0 is referred to as a homogeneous system of equations. In
this section, we obtain some results regarding homogeneous systems of equations.
One obvious distinction between homogeneous and nonhomogeneous systems is that
a homogeneous system of equations must be consistent because it will always have
the trivial solution, x = 0. A homogeneous system will then have a unique solution
only when the trivial solution is the only solution. Conditions for the existence of
nontrivial solutions, which we state in the next theorem, follow directly from Theorem
6.6 and Corollary 6.6.1.
Theorem 6.8
Suppose that A is an m × n matrix. The system Ax = 0 has nontriv-
ial solutions if and only if A−A ̸= In, or equivalently if and only if rank(A) < n.
If the system Ax = 0 has more than one solution, and {x1, . . . , xr} is a set of r
solutions, then x = α1x1 + · · · + αrxr is also a solution regardless of the choice of
α1, . . . , αr, because
Ax = A
 r

i=1
αixi

=
r

i=1
αiAxi =
r

i=1
αi0 = 0.
In fact, we have the following.
Theorem 6.9
If A is an m × n matrix, then the set of all solutions to the system of
equations Ax = 0 forms a vector subspace of Rn having dimension n −rank(A).
Proof.
The result follows immediately from the fact that the set of all solutions of
Ax = 0 is the null space of A.
□
In contrast to Theorem 6.9, the set of all solutions to a nonhomogeneous system of
equations will not form a vector subspace. This is because, as we have seen in the pre-
vious section, a linear combination of solutions to a nonhomogeneous system yields
another solution only if the coefficients sum to one. Additionally, a nonhomogeneous
system cannot have 0 as a solution.
The general form of a solution given in Theorem 6.4 applies to both homogeneous
and nonhomogeneous systems. Thus, for any n × 1 vector y,
xy = (In −A−A)y
is a solution to the system Ax = 0, and for any solution, x∗, a vector y exists,
such that x∗= xy. Theorem 6.10 shows that the set of solutions of Ax = c can
be expressed in terms of the set of solutions to Ax = 0.

HOMOGENEOUS SYSTEMS OF EQUATIONS
259
Theorem 6.10
Let x∗be any solution to the system of equations Ax = c. Then
(a) if x# is a solution to the system Ax = 0, x = x∗+ x# is a solution of
Ax = c, and
(b) for any solution x to the equation Ax = c, a solution x# to the equation
Ax = 0 exists, such that x = x∗+ x#.
Proof.
Note that if x# is as defined in (a), then
A(x∗+ x#) = Ax∗+ Ax# = c + 0 = c,
and so x = x∗+ x# is a solution to Ax = c. To prove (b), define x# = x −x∗, so
that x = x∗+ x#. Then because Ax = c and Ax∗= c, it follows that
Ax# = A(x −x∗) = Ax −Ax∗= c −c = 0,
and so the proof is complete.
□
Our next result, regarding the number of linearly independent solutions possessed
by a homogeneous system of equations, follows immediately from Theorem 6.9.
Theorem 6.11
Each solution of the homogeneous system of equations Ax = 0
can be expressed as a linear combination of r linearly independent solutions, where
r = n −rank(A).
Example 6.7
Consider the system of equations Ax = 0, where
A =
⎡
⎣
1
2
2
1
1
0
⎤
⎦.
We saw in Example 6.5 that A+A = I2. Thus, the system only has the trivial solu-
tion 0.
Example 6.8
Since the matrix
A =
⎡
⎣
1
1
1
2
1
0
1
0
2
1
2
2
⎤
⎦
from Example 6.6 has rank of 2, the homogeneous system of equations Ax = 0
has r = n −rank(A) = 4 −2 = 2 linearly independent solutions. Any set of two

260
SYSTEMS OF LINEAR EQUATIONS
linearly independent columns of the matrix (I4 −A−A) will be a set of linearly inde-
pendent solutions; for example, the first and fourth columns,
⎡
⎢⎢⎢⎣
2
0
−2
0
⎤
⎥⎥⎥⎦,
⎡
⎢⎢⎢⎣
2
−2
−2
1
⎤
⎥⎥⎥⎦,
are linearly independent solutions.
6.5
LEAST SQUARES SOLUTIONS TO A SYSTEM OF LINEAR
EQUATIONS
In some situations in which we have an inconsistent system of equations Ax = c, it
may be desirable to find the vector or set of vectors that comes “closest” to satisfying
the equations. If x∗is one choice for x, then x∗will approximately satisfy our system
of equations if Ax∗−c is close to 0. One of the most common ways of measuring
the closeness of Ax∗−c to 0 is through the computation of the sum of squares of
the components of the vector Ax∗−c. Any vector minimizing this sum of squares
is referred to as a least squares solution.
Definition 6.1
The n × 1 vector x∗is said to be a least squares solution to the
system of equations Ax = c if the inequality
(Ax∗−c)′(Ax∗−c) ≤(Ax −c)′(Ax −c)
(6.6)
holds for every n × 1 vector x.
Of course, we have already used the concept of a least squares solution in many
of our examples on regression analysis. In particular, we have seen that if the matrix
X has full column rank, then the least squares solution for ˆβ in the fitted regression
equation, ˆy = X ˆβ, is given by ˆβ = (X′X)−1X′y. The generalized inverses that we
have discussed in Chapter 5 will enable us to obtain a unified treatment of this problem
including cases in which X is not of full rank.
In Section 5.8, we briefly discussed the {1, 3}-inverse of a matrix A, that is, any
matrix satisfying the first and third conditions of the Moore–Penrose inverse. We
referred to this type of inverse as a least squares inverse of A. Theorem 6.12 motivates
this description.
Theorem 6.12
Let AL be any {1, 3}-inverse of a matrix A. Then the vector
x∗= ALc is a least squares solution to the system of equations Ax = c.

LEAST SQUARES SOLUTIONS TO A SYSTEM OF LINEAR EQUATIONS
261
Proof.
We must show that (6.6) holds when x∗= ALc. The right-hand side of (6.6)
can be written as
(Ax −c)′(Ax −c) = {(Ax −AALc) + (AALc −c)}′
×{(Ax −AALc) + (AALc −c)}
= (Ax −AALc)′(Ax −AALc)
+ (AALc −c)′(AALc −c)
+ 2(Ax −AALc)′(AALc −c)
≥(AALc −c)′(AALc −c)
= (Ax∗−c)′(Ax∗−c),
where the inequality follows from the fact that
(Ax −AALc)′(Ax −AALc) ≥0
and
(Ax −AALc)′(AALc −c) = (x −ALc)′A′(AALc −c)
= (x −ALc)′A′((AAL)′c −c)
= (x −ALc)′(A′AL′A′c −A′c)
= (x −ALc)′(A′c −A′c) = 0.
(6.7)
This completes the proof.
□
Corollary 6.12.1
The vector x∗is a least squares solution to the system Ax = c if
and only if
(Ax∗−c)′(Ax∗−c) = c′(Im −AAL)c.
Proof.
From Theorem 6.12, ALc is a least squares solution for any choice of AL,
and its sum of squared errors is given by
(AALc −c)′(AALc −c) = c′(AAL −Im)′(AAL −Im)c
= c′(AAL −Im)2c
= c′(AALAAL −2AAL + Im)c
= c′(AAL −2AAL + Im)c
= c′(Im −AAL)c.

262
SYSTEMS OF LINEAR EQUATIONS
The result now follows because, by definition, a least squares solution minimizes the
sum of squared errors, and so any other vector x∗will be a least squares solution
if and only if its sum of squared errors is equal to this minimum sum of squares,
c′(Im −AAL)c.
□
Example 6.9
Let the system of equations Ax = c have A and c given by
A =
⎡
⎢⎢⎣
1
1
2
1
0
1
1
1
2
2
0
2
⎤
⎥⎥⎦,
c =
⎡
⎢⎢⎣
4
1
6
5
⎤
⎥⎥⎦.
In Example 5.13, we computed the least squares inverse,
AL = 1
10
⎡
⎣
0
2
0
4
5
−2
5
−4
0
0
0
0
⎤
⎦.
Since
AALc = 1
10
⎡
⎢⎢⎣
5
0
5
0
0
2
0
4
5
0
5
0
0
4
0
8
⎤
⎥⎥⎦
⎡
⎢⎢⎣
4
1
6
5
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
5
2.2
5
4.4
⎤
⎥⎥⎦̸= c,
it follows from Theorem 6.2 that the system of equations is inconsistent. A least
squares solution is then given by
ALc = 1
10
⎡
⎣
0
2
0
4
5
−2
5
−4
0
0
0
0
⎤
⎦
⎡
⎢⎢⎣
4
1
6
5
⎤
⎥⎥⎦=
⎡
⎣
2.2
2.8
0
⎤
⎦.
Since (AALc −c)′ = (5, 2.2, 5, 4.4)′ −(4, 1, 6, 5)′ = (1, 1.2, −1, −0.6)′, the sum
of squared errors for the least squares solution is
(AALc −c)′(AALc −c) = 3.8.
In general, a least squares solution is not unique. For instance, the reader can easily
verify that the matrix
B =
⎡
⎣
−2
−0.8
−2
−1.6
−1.5
−1.2
−1.5
−2.4
2
1
2
2
⎤
⎦

LEAST SQUARES SOLUTIONS TO A SYSTEM OF LINEAR EQUATIONS
263
is also a least squares inverse of A. Consequently,
Bc =
⎡
⎣
−2
−0.8
−2
−1.6
−1.5
−1.2
−1.5
−2.4
2
1
2
2
⎤
⎦
⎡
⎢⎢⎣
4
1
6
5
⎤
⎥⎥⎦=
⎡
⎣
−28.8
−28.2
31
⎤
⎦
is another least squares solution. However, because (ABc −c)′ = (5, 2.2, 5, 4.4)′ −
(4, 1, 6, 5)′ = (1, 1.2, −1, −0.6)′, the sum of squared errors for this least squares
solution is, as it must be, identical to that of the previous solution.
The following result will be useful in establishing the general form of a least
squares solution. It indicates that although a least squares solution x∗may not be
unique, the vector Ax∗will be unique.
Theorem 6.13
The vector x∗is a least squares solution to the system Ax = c if
and only if
Ax∗= AALc.
(6.8)
Proof.
Using Theorem 6.2, we see that the system of equations given in (6.8) is
consistent because
AAL(AALc) = (AALA)ALc = AALc.
The sum of squared errors for any vector x∗satisfying (6.8) is
(Ax∗−c)′(Ax∗−c) = (AALc −c)′(AALc −c)
= c′(AAL −Im)2c
= c′(Im −AAL)c,
so by Corollary 6.12.1, x∗is a least squares solution. Conversely, now suppose that
x∗is a least squares solution. Then from Corollary 6.12.1 we must have
(Ax∗−c)′(Ax∗−c) = c′(Im −AAL)c
= c′(Im −AAL)′(Im −AAL)c
= (AALc −c)′(AALc −c),
(6.9)
where we have used the fact that (Im −AAL) is symmetric and idempotent. How-
ever, we also have
(Ax∗−c)′(Ax∗−c) = {(Ax∗−AALc) + (AALc −c)}′
×{(Ax∗−AALc) + (AALc −c)}
= (Ax∗−AALc)′(Ax∗−AALc)
+ (AALc −c)′(AALc −c),
(6.10)

264
SYSTEMS OF LINEAR EQUATIONS
because (Ax∗−AALc)′(AALc −c) = 0, as shown in (6.7). Now (6.9) and (6.10)
imply that
(Ax∗−AALc)′(Ax∗−AALc) = 0,
which can be true only if
(Ax∗−AALc) = 0,
which establishes (6.8).
□
We now give an expression in Theorem 6.14 for a general least squares solution
to a system of equations.
Theorem 6.14
Let AL be any {1, 3}-inverse of the m × n matrix A. Define the
vector
xy = ALc + (In −ALA)y,
where y is an arbitrary n × 1 vector. Then, for each y, xy is a least squares solution
to the system of equations Ax = c, and for any least squares solution x∗a vector y
exists, such that x∗= xy.
Proof.
Since
A(In −ALA)y = (A −AALA)y = (A −A)y = 0,
we have Axy = AALc, and so by Theorem 6.13, xy is a least squares solution.
Conversely, if x∗is an arbitrary least squares solution, then by using Theorem 6.13
again, we must have
Ax∗= AALc,
which, when premultiplied by AL, implies that
0 = −ALA(x∗−ALc).
Adding x∗to both sides of this identity and then rearranging, we get
x∗= x∗−ALA(x∗−ALc)
= ALc + x∗−ALc −ALA(x∗−ALc)
= ALc + (In −ALA)(x∗−ALc).
This completes the proof because we have shown that x∗= xy, where y =
(x∗−ALc).
□
We saw in Example 6.9 that least squares solutions are not necessarily unique.
Theorem 6.14 can be used to obtain a necessary and sufficient condition for the solu-
tion to be unique.

LEAST SQUARES SOLUTIONS TO A SYSTEM OF LINEAR EQUATIONS
265
Theorem 6.15
If A is an m × n matrix, then the system of equations Ax = c has
a unique least squares solution if and only if rank(A) = n.
Proof.
It follows immediately from Theorem 6.14 that the least squares solution is
unique if and only if (In −ALA) = (0), or equivalently, ALA = In. The result now
follows from Theorem 5.23(g).
□
In some applications, when there is more than one least squares solution to the
system Ax = c, one maybe interested in finding the least squares solution that has
the smallest size. We will refer to such a solution as the minimal least squares solution.
Our next result indicates that this solution is given by x = A+c.
Theorem 6.16
Suppose x∗̸= A+c is a least squares solution to the system of
equations Ax = c. Then
x′
∗x∗> c′A+′A+c.
Proof.
It follows from Theorem 6.14 that x∗can be written as x∗= A+c + (In −
A+A)y for some n × 1 vector y. Since x∗̸= A+c, we must have (In −A+A)y ̸= 0,
or equivalently {(In −A+A)y}′(In −A+A)y = y′(In −A+A)y > 0. The result
now follows since
x′
∗x∗= c′A+′A+c + y′(In −A+A)y + 2y′(In −A+A)A+c
= c′A+′A+c + y′(In −A+A)y
> c′A+′A+c.
□
Even when the least squares solution to a system is not unique, certain linear com-
binations of the elements of least squares solutions may be unique. This is the subject
of our next theorem.
Theorem 6.17
Let x∗be a least squares solution to the system of equations Ax = c.
Then a′x∗is unique if and only if a is in the row space of A.
Proof.
Using Theorem 6.14, if a′x∗is unique regardless of the choice of the least
squares solution x∗, then
a′xy = a′ALc + a′(In −ALA)y
is the same for all choices of y. But this implies that
a′(In −ALA) = 0′.
(6.11)
Now if (6.11) holds, then
a′ = b′A,

266
SYSTEMS OF LINEAR EQUATIONS
where b′ = a′AL, and so a is in the row space of A. On the other hand, if a is in the
row space of A, then some vector b exists, such that a′ = b′A. This implies that
a′(In −ALA) = b′A(In −ALA) = b′(A −AALA) = b′(A −A) = 0′,
and so the least squares solution must be unique.
□
Example 6.10
We will obtain the general least squares solution to the system of
equations presented in Example 6.9. First, note that
ALA =
⎡
⎣
1
0
1
0
1
1
0
0
0
⎤
⎦,
so that
xy = ALc + (I3 −ALA)y
= 1
10
⎡
⎣
0
2
0
4
5
−2
5
−4
0
0
0
0
⎤
⎦
⎡
⎢⎢⎣
4
1
6
5
⎤
⎥⎥⎦+
⎡
⎣
0
0
−1
0
0
−1
0
0
1
⎤
⎦
⎡
⎣
y1
y2
y3
⎤
⎦
=
⎡
⎣
2.2 −y3
2.8 −y3
y3
⎤
⎦
is a least squares solution for any choice of y3. The quantity a′xy does not depend on
the choice of y3 as long as a is in the row space of A; in this case, that corresponds
to a being orthogonal to the vector (−1, −1, 1)′.
6.6
LEAST SQUARES ESTIMATION FOR LESS THAN FULL RANK
MODELS
In all of our previous examples of least squares estimation for a model of the form
y = Xβ + ϵ,
(6.12)
where y is N × 1, X is N × m, β is m × 1, and ϵ is N × 1, we have assumed that
rank(X) = m. In this case, the normal equations,
X′X ˆβ = X′y,
(6.13)
yield a unique solution, the unique least squares estimator of β, given by
ˆβ = (X′X)−1X′y.

LEAST SQUARES ESTIMATION FOR LESS THAN FULL RANK MODELS
267
However, in many applications, the matrix X has less than full rank.
Example 6.11
Consider the univariate one-way classification model, which was
written as
yij = μi + ϵij,
in Example 3.16, where i = 1, . . . , k and j = 1, . . . , ni. This model can be written
in the form of (6.12), where β = (μ1, . . . , μk)′ and
X =
⎡
⎢⎢⎢⎣
1n1
0
· · ·
0
0
1n2
· · ·
0
...
...
...
0
0
· · ·
1nk
⎤
⎥⎥⎥⎦.
In this case, X is of full rank, and so
ˆβ = (X′X)−1X′y = y =

y1j/n1, . . . ,

ykj/nk
′
.
An alternative way of writing this one-way classification model is
yij = μ + τi + ϵij,
which has k + 1 parameters instead of k. Here μ represents an overall effect, whereas
τi is an effect due to treatment i. In some respects, this form of the model is more
natural in that the reduced model, which has all treatment means identical, is simply
a submodel with some of the parameters equal to 0, that is, τ1 = · · · = τk = 0. If this
second form of the model is written as y = X∗β∗+ ϵ, then β∗= (μ, τ1, . . . , τk)′
and
X∗=
⎡
⎢⎢⎢⎢⎢⎣
1n1
1n1
0
· · ·
0
1n2
0
1n2
· · ·
0
1n3
0
0
· · ·
0
...
...
...
...
1nk
0
0
· · ·
1nk
⎤
⎥⎥⎥⎥⎥⎦
.
Thus, this second parameterization of the one-way classification model has the design
matrix X∗less than full rank because rank(X∗) = k.
In this section, we will apply some of the results of this chapter to the estimation
of parameters in the model given by (6.12) when X is less than full rank. First of
all, let us consider the task of solving the normal equations given by (6.13); that is,
using our usual notation for a system of equations, we want to solve Ax = c, where

268
SYSTEMS OF LINEAR EQUATIONS
A = X′X, x = ˆβ, and c = X′y. Now from Theorem 6.2, we see that (6.13) is a
consistent system of equations because
X′X(X′X)+X′y = X′XX+X+′X′y = X′XX+(XX+)′y
= X′XX+XX+y = X′XX+y
= X′(XX+)′y = X′X+′X′y = X′y.
Consequently, using Theorem 6.4, we find that the general solution ˆβ can be written
as
ˆβ = (X′X)−X′y + {Im −(X′X)−X′X}u,
(6.14)
or, if we use the Moore–Penrose generalized inverse, as
ˆβ = (X′X)+X′y + {Im −(X′X)+X′X}u
= X+y + (Im −X+X)u,
where u is an arbitrary m × 1 vector. The same general solution can be obtained by
applying the least squares results of Section 6.5 on the system of equations
y = X ˆβ.
Thus, using Theorem 6.14 with A = X, x = ˆβ, and c = y, the least squares solution
is given by
ˆβ = XLy + (Im −XLX)u,
which is, of course, equivalent to that given by (6.14).
One key difference between the full rank model and the less than full rank model
is that the least squares solution is unique only if X has full rank. When X is less
than full rank, the model y = Xβ + ϵ is overparameterized, and so not all of the
parameters or linear functions of the parameters are uniquely defined; this is what
leads to the infinitely many solutions for ˆβ. Thus, when estimating linear functions
of the parameters, we must make sure that we are trying to estimate a function of the
parameters that is uniquely defined. This leads to the following definition of what is
known as an estimable function.
Definition 6.2
The linear function a′β of the parameter vector β is estimable if and
only if some N × 1 vector b exists, such that
a′β = E(b′y) = b′E(y) = b′Xβ;
that is, if and only if there exists a linear function of the components of y, b′y, which
is an unbiased estimator of a′β.

LEAST SQUARES ESTIMATION FOR LESS THAN FULL RANK MODELS
269
The condition that a linear function a′β be estimable is equivalent to the condition
that the corresponding estimator a′ˆβ be unique. To see this, note that from Definition
6.2, the function a′β is estimable if and only if a is in the row space of X, whereas
it follows from Theorem 6.17 that a′ˆβ is unique if and only if a is in the row space
of X. In addition, because X′(XX′)+X is the projection matrix for the row space of
X, we get the more practical condition for estimability of a′β given by
X′(XX′)+Xa = a.
(6.15)
It follows from Theorems 5.3 and 5.28 that
X′(XX′)+X = X′X+′ = X′XL′ = X′(XX′)−X,
and so (6.15) is not dependent on the Moore–Penrose inverse as the choice of the
generalized inverse of XX′.
Finally, we will demonstrate the invariance of the vector of fitted values ˆy = X ˆβ
and its sum of squared errors (y −ˆy)′(y −ˆy) to the choice of the least squares solu-
tion ˆβ. Since XX+X = X,
ˆy = X ˆβ = X{X+y + (Im −X+X)u}
= XX+y + (X −XX+X)u = XX+y,
which does not depend on the vector u. Thus, ˆy is unique, whereas the uniqueness
of
(y −ˆy)′(y −ˆy) = y′(Im −XX+)y
follows immediately from the uniqueness of ˆy.
Example 6.12
Let us return to the one-way classification model
y = X∗β∗+ ϵ
of Example 6.11, where β∗= (μ, τ1, . . . , τk)′ and
X∗=
⎡
⎢⎢⎢⎢⎢⎣
1n1
1n1
0
· · ·
0
1n2
0
1n2
· · ·
0
1n3
0
0
· · ·
0
...
...
...
...
1nk
0
0
· · ·
1nk
⎤
⎥⎥⎥⎥⎥⎦
.
Since the rank of the n × (k + 1) matrix X∗, where n =  ni, is k, the least squares
solution for β∗is not unique. To find the form of the general solution, note that
X′
∗X∗=

n
n′
n
Dn

,

270
SYSTEMS OF LINEAR EQUATIONS
whereas a generalized inverse is given by
(X′
∗X∗)−=

n−1
0′
0
D−1
n −n−11k1′
k

,
where n = (n1, . . . , nk)′ and Dn = diag(n1, . . . , nk). Thus, using (6.14) we have
the general solution
ˆβ∗=

n−1
0′
0
D−1
n −n−11k1′
k
 
 ny
Dny

+

Ik+1 −

1
n−1n′
0
Ik −n−11kn′

u
=

y
y −y1k

+

0
−n−1n′
0
n−11kn′

u,
where y = (y1, . . . , yk)′ and y =  niyi/n. Choosing u = 0, we get the partic-
ular least squares solution that has ˆμ = y and ˆτi = yi −y for i = 1, . . . , k. Since
a′β∗is estimable only if a is in the row space of X, we find that the k quanti-
ties, μ + τi, i = 1, . . . , k, as well as any linear combinations of these quantities, are
estimable. In particular, because μ + τi = a′
iβ∗, where ai = (1, e′
i)′, its estimator is
given by
a′
iˆβ∗= [1
e′
i]

y
y −y1k

= yi.
The vector of fitted values is
ˆy = X∗ˆβ∗=
⎡
⎢⎢⎢⎢⎢⎣
1n1
1n1
0
· · ·
0
1n2
0
1n2
· · ·
0
1n3
0
0
· · ·
0
...
...
...
...
1nk
0
0
· · ·
1nk
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
y
y1 −y
y2 −y
...
yk −y
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎣
y11n1
y21n2
...
yk1nk
⎤
⎥⎥⎥⎦,
whereas the sum of squared errors is given by
(y −ˆy)′(y −ˆy) =
k

i=1
ni

j=1
(yij −yi)2.

SYSTEMS OF LINEAR EQUATIONS AND THE SINGULAR VALUE DECOMPOSITION
271
6.7
SYSTEMS OF LINEAR EQUATIONS AND THE SINGULAR VALUE
DECOMPOSITION
When A is square and nonsingular, then the solution to the system of equations Ax =
c can be conveniently expressed in terms of the inverse of A, as x = A−1c. For this
reason, it has seemed somewhat natural to deal with the solutions for the more general
case in terms of the generalization of A−1, A+. This is the approach that we have
taken throughout this chapter. Alternatively, we can attack this problem by directly
using the singular value decomposition, an approach that may offer more insight.
In this case, we will always be able to transform our system to a simpler system of
equations of the form
Dy = b,
(6.16)
where y is an n × 1 vector of variables, b is an m × 1 vector of constants, and D is
an m × n matrix such that dij = 0 if i ̸= j. In particular, D will have one of the four
forms, as given in Theorem 4.1,
(a)Δ,
(b)[Δ
(0)],
(c)

Δ
(0)

,
(d)

Δ
(0)
(0)
(0)

,
where Δ is an r × r nonsingular diagonal matrix and r = rank(A). Now if D has
the form given in (a), then the system (6.16) is consistent with the unique solution
given by y = Δ−1b. For (b), if we partition y as y = (y′
1, y′
2)′, where y1 is r × 1,
then (6.16) reduces to
Δy1 = b.
Thus, (6.16) is consistent and has solutions of the form
y =

Δ−1b
y2

,
where the (n −r) × 1 vector y2 is arbitrary. Since we then have n −r linearly inde-
pendent choices for y2, the number of linearly independent solutions is n −r if b = 0
and n −r + 1 if b ̸= 0. When D has the form given in (c), the system in (6.16) takes
the form

Δy
0

=

b1
b2

,
where b1 is r × 1 and b2 is (m −r) × 1, and so it is consistent only if b2 = 0. If this
is the case, the system then has a unique solution given by y = Δ−1b1. For the final
form given in (d), the system of equations in (6.16) appears as

Δy1
0

=

b1
b2

,
where y and b have been partitioned as before. As in the case of form (c), this system
is consistent only if b2 = 0, and as in the case of form (b), when consistent, it has

272
SYSTEMS OF LINEAR EQUATIONS
n −r linearly independent solutions if b = 0 and n −r + 1 linearly independent
solutions if b ̸= 0. The general solution is given by
y =

Δ−1b1
y2

,
where the (n −r) × 1 vector y2 is arbitrary.
All of the above can now be readily applied to the general system of equations,
Ax = c
(6.17)
by utilizing the singular value decomposition of A given by A = PDQ′ as in
Theorem 4.1. Premultiplication of this system of equations by P ′ produces the
system of equations in (6.16), where the vector of variables is given by y = Q′x and
the vector of constants is b = P ′c. Consequently, if y is a solution to (6.16), then
x = Qy will be a solution to (6.17). Thus, in the case of forms (a) and (b), (6.17) is
consistent with a unique solution given by
x = Qy = QΔ−1b = QΔ−1P ′c = A−1c,
when (a) is the form of D, whereas for form (b) the general solution is
x = Qy = [Q1
Q2]

Δ−1b
y2

= Q1Δ−1P ′c + Q2y2,
where Q1 is n × r and y2 is an arbitrary (n −r) × 1 vector. The term Q2y2 has no
effect on the value of Ax because the columns of the n × (n −r) matrix Q2 form
a basis for the null space of A. In the case of forms (c) and (d), the system (6.17) is
consistent only if c = P1b1, so that P2b2 = 0, where P = (P1, P2) and P1 is m × r;
that is, because the columns of P1 form a basis for the range of A, the system is
consistent if c is in the column space of A. Thus, if we partition c as c = (c′
1, c′
2)′,
where c1 is r × 1, then when form (c) holds, the unique solution will be given by
x = Qy = QΔ−1b1 = QΔ−1P ′
1c.
In the case of form (d), the general solution is
x = Qy = [Q1
Q2]

Δ−1b1
y2

= Q1Δ−1P ′
1c + Q2y2.

SPARSE LINEAR SYSTEMS OF EQUATIONS
273
6.8
SPARSE LINEAR SYSTEMS OF EQUATIONS
The typical approach to the numerical computation of solutions to a consistent sys-
tem of equations Ax = c, or least squares solutions when the system is inconsistent,
applies some factorization of A such as the QR factorization, the singular value
decomposition, or the LU decomposition, which factors A into the product of a lower
triangular matrix and upper triangular matrix. Any method of this type is referred to
as a direct method. One situation in which direct methods may not be appropriate is
when our system of equations is large and sparse; that is, m and n are large and a rel-
atively large number of the elements of the m × n matrix A are equal to zero. Thus,
although the size of A may be quite large, its storage will not require an enormous
amount of computer memory because we only need to store the nonzero values and
their locations. However, when A is sparse, the factors in its decompositions need not
be sparse, so if A is large enough, the computation of these factorizations may easily
require more memory than is available.
If there is some particular structure to the sparsity of A, then it may be possible
to implement a direct method that exploits this structure. A simple example of such
a situation is one in which A is m × m and tridiagonal; that is, A has the form
A =
⎡
⎢⎢⎢⎢⎢⎣
v1
w1
0
· · ·
0
0
0
u2
v2
w2
· · ·
0
0
0
...
...
...
...
...
...
0
0
0
· · ·
um−1
vm−1
wm−1
0
0
0
· · ·
0
um
vm
⎤
⎥⎥⎥⎥⎥⎦
.
In this case, if we define
L =
⎡
⎢⎢⎢⎢⎢⎣
r1
0
· · ·
0
0
u2
r2
· · ·
0
0
...
...
...
...
0
0
· · ·
rm−1
0
0
0
· · ·
um
rm
⎤
⎥⎥⎥⎥⎥⎦
,
U =
⎡
⎢⎢⎢⎢⎢⎣
1
s1
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
0
0
· · ·
1
sm−1
0
0
· · ·
0
1
⎤
⎥⎥⎥⎥⎥⎦
,
where r1 = v1, ri = vi −uiwi−1/ri−1, and si−1 = wi−1/ri−1, for i = 2, . . . , m,
then A can be factored as A = LU as long as each ri ̸= 0. Thus, the two factors, L
and U, are also sparse. The system Ax = c can easily be solved by first solving the
system Ly = c and then solving the system Ux = y. For more details on this and
adaptations of direct methods for other structured matrices, such as banded matrices
and block tridiagonal matrices, see Duff, et al. (1986) and Golub and Van Loan
(2013).
A second approach to the solution of sparse systems of equations uses iterative
methods. In this case, a sequence of vectors, x0, x1, . . . is generated with x0 being
some initial vector, whereas xj for j = 1, 2, . . . is a vector that is computed using
the previous vector xj−1, with the property that xj →x, as j →∞, where x is the

274
SYSTEMS OF LINEAR EQUATIONS
true solution to Ax = c. Typically, the computation in these methods only involves
A through its product with vectors, and this is an operation that will be easy to han-
dle if A is sparse. Two of the oldest and simplest iterative schemes are the Jacobi
and Gauss–Seidel methods. If A is m × m with nonzero diagonal elements, then the
system Ax = c can be written as
(A −DA)x + DAx = c,
which yields the identity
x = D−1
A {c −(A −DA)x}.
This identity is the motivation for the Jacobi method that computes xj as
xj = D−1
A {c −(A −DA)xj−1}.
On the other hand, the Gauss–Seidel method applies the splitting of A as A = A1 +
A2, where A1 is lower triangular and A2 is upper triangular with each of its diagonal
elements equal to zero. In this case, Ax = c can be rearranged as
A1x = c −A2x,
and this leads to the iterative scheme
A1xj = c −A2xj−1,
which is easily solved for xj because the system is triangular.
In recent years, some other more sophisticated iterative methods, requiring less
computation and having better convergence properties, have been developed. We will
briefly discuss a method for solving a system of equations which utilizes an algorithm
known as the Lanczos algorithm (Lanczos, 1950). For more information on this pro-
cedure, including convergence properties, generalizations to a general m × n matrix,
and to the problem of finding least squares solutions, as well as other iterative meth-
ods, the reader is referred to Young (1971), Hageman and Young (1981), and Golub
and Van Loan (2013).
Consider the function
f(x) = 1
2x′Ax −x′c,
where x is an m × 1 vector and A is an m × m positive definite matrix. The vector
of partial derivatives of f(x) given by
∇f(x) =
 ∂f
∂x1
, . . . , ∂f
∂xm
′
= Ax −c

SPARSE LINEAR SYSTEMS OF EQUATIONS
275
is sometimes referred to as the gradient of f(x). Setting this equation equal to the
zero vector, we find that the vector minimizing f, x = A−1c, is also the solution to
the system Ax = c. Thus, a vector that approximately minimizes f will also be an
approximate solution to Ax = c. One iterative method for finding the minimizer x
involves successively finding minimizers xj of f over a j-dimensional subspace of
Rm, starting with j = 1 and continually increasing j by 1. In particular, for some
set of orthonormal m × 1 vectors, q1, . . . , qm, we will define the jth subspace as
the space with the columns of the m × j matrix, Qj = (q1, . . . , qj), as its basis.
Consequently, for some j × 1 vector yj,
xj = Qjyj
(6.18)
and
f(xj) = min
y∈Rj f(Qjy) = min
y∈Rj g(y) = g(yj),
where
g(y) = 1
2y′(Q′
jAQj)y −y′Q′
jc.
Thus, the gradient of g(yj) must be equal to the null vector, and so
(Q′
jAQj)yj = Q′
jc.
(6.19)
To obtain xj, we can first use (6.19) to calculate yj and then use this in (6.18) to get
xj. The final xj, xm, will be the solution to Ax = c, but the goal here is to stop the
iterative process before j = m with a sufficiently accurate solution xj.
The iterative scheme described above will work with different sets of orthonormal
vectors q1, . . . , qm, but we will see that by a judicious choice of this set, we may
guarantee that the computation involved in computing the xj’s will be fairly straight-
forward even when A is large and sparse. These same vectors are also useful in an
iterative procedure for obtaining a few of the largest and smallest eigenvalues of A.
We will derive these vectors in the context of this eigenvalue problem and then later
return to our discussion of the system of equations Ax = c. Let λ1 and λmdenote
the largest and smallest eigenvalues of A, whereas λ1j and λjj denote the largest and
smallest eigenvalues of the j × j matrix Q′
jAQj. Now we have seen in Chapter 3
that λ1j ≤λ1, λjj ≥λm and that λ1 and λm are the maximum and minimum values
of the Rayleigh quotient,
R(x, A) = x′Ax
x′x .
Suppose that we have the j columns of Qj, and we wish to find an additional vector
qj+1 so as to form the matrix Qj+1 and have λ1,j+1 and λj+1,j+1 as close to λ1 and
and λm as possible. If uj is a vector in the space spanned by the columns of Qj and
satisfying R(uj, A) = λ1j, then because the gradient
∇R(uj, A) =
2
u′
juj
{Auj −R(uj, A)uj}

276
SYSTEMS OF LINEAR EQUATIONS
gives the direction in which R(uj, A) is increasing most rapidly, we would want to
choose qj+1 so that ∇R(uj, A) is in the space spanned by the columns of Qj+1.
On the other hand, if vj is a vector in the space spanned by Qj and satisfying
R(vj, A) = λjj, then because R(vj, A) is decreasing most rapidly in the direction
given by −∇R(vj, A), we would want to make sure that ∇R(vj, A) is also in the
space spanned by the columns of Qj+1. Both of these objectives can be satisfied if
the columns of Qj are spanned by the vectors q1, Aq1, . . . , Aj−1q1 and we select
qj+1 so that the columns of Qj+1 are spanned by the vectors q1, Aq1, . . . , Ajq1
because both ∇R(uj, A) and ∇R(vj, A) are of the form aAx + bx for some
vector x spanned by the columns of Qj. Thus, we start with an initial unit vector
q1, whereas for j ≥2, qj is selected as a unit vector orthogonal to q1, . . . , qj−1,
and such that the columns of Qj are spanned by the vectors q1, Aq1, . . . , Aj−1q1.
These particular qj vectors are known as the Lanczos vectors. The calculation of the
qj’s can be facilitated by the use of the tridiagonal factorization A = PTP ′, where
P is orthogonal and T has the tridiagonal form
T =
⎡
⎢⎢⎢⎢⎢⎣
α1
β1
0
· · ·
0
0
0
β1
α2
β2
· · ·
0
0
0
...
...
...
...
...
...
0
0
0
· · ·
βm−2
αm−1
βm−1
0
0
0
· · ·
0
βm−1
αm
⎤
⎥⎥⎥⎥⎥⎦
.
Using this factorization, we find that if we choose P and q1, so that Pe1 = q1, then
(q1, Aq1, . . . , Aj−1q1) = P(e1, Te1, . . . , T j−1e1).
Since (e1, Te1, . . . , T j−1e1) has upper triangular structure, the first j columns of P
span the column space of (q1, Aq1, . . . , Aj−1q1); that is, the qj’s can be obtained
by calculating the factorization A = PTP ′, or in other words, we can take Q =
(q1, . . . , qm) = P. Thus, because AQ = QT, we have
Aq1 = α1q1 + β1q2
(6.20)
and
Aqj = βj−1qj−1 + αjqj + βjqj+1,
(6.21)
for
j = 2, . . . , m −1.
Using
these
equations
and
the
orthonormality
of
the qj’s, it is easily shown that αj = q′
jAqj for all j, and as long as
pj = (A −αjIm)qj −βj−1qj−1 ̸= 0,
then
β2
j = p′
jpj,
and
qj+1 = pj/βj
for j = 1, . . . , m −1, if we define q0 = 0. Thus, we can continue calculating the
qj’s until we encounter a pj = 0. To see the significance of this event, let us suppose
that the iterative procedure has proceeded through the first j −1 steps with pi ̸= 0
for each i = 2, . . . , j −1, and so we have obtained the matrix Qj whose columns

SPARSE LINEAR SYSTEMS OF EQUATIONS
277
form a basis for the column space of (q1, Aq1, . . . , Aj−1q1). Note that it follows
immediately from the relationship AQ = QT that
AQj = QjTj + pje′
j,
where Tj is the j × j submatrix of T consisting of its first j rows and j columns. This
leads to the equation Q′
jAQj = Tj + Q′
jpje′
j. However, q′
iAqi = αi, whereas it fol-
lows from (6.20) and (6.21) that q′
i+1Aqi = βi and q′
kAqi = 0 if k > i + 1. Thus,
Q′
jAQj = Tj, and so we must have Q′
jpj = 0. Now if pj ̸= 0, then qj+1 = pj/βj
is orthogonal to the columns of Qj. Further, it follows from the fact that qj+1 is
a linear combination of Aqj, qj and qj−1 that the columns of Qj+1 = (Qj, qj+1)
form a basis for the column space of (q1, Aq1, . . . , Ajq1). If, on the other hand,
pj = 0, then AQj = QjTj. From this we see that the vectors Ajq1, . . . , Am−1q1
are in the space spanned by the columns of Qj, that is, the space spanned by the vec-
tors q1, Aq1, . . . , Aj−1q1. Consequently, the iterative procedure is complete because
there are only j qi’s.
In the iterative procedure described above, the largest and smallest eigenvalues of
Tj serve as approximations to the largest and smallest eigenvalues of A. In practice,
the termination of this iterative process is usually not because of the encounter of a
pj = 0, but because of sufficiently accurate approximations of the eigenvalues of A.
Now let us return to the problem of solving the system of equations Ax = c
through the iterative procedure based on the calculation of yj in (6.19) and then
xj in (6.18). We will see that the choice of the Lanczos vectors as the columns of
Qj will simplify the computations involved. For this choice of Qj, we have already
seen that Q′
jAQj = Tj, so that the system in (6.19) is a special case of the tridiag-
onal system of equations discussed at the beginning of this section, special in that
Tj is symmetric. As a result, the matrix Tj can be factored as Tj = LjDjL′
j, where
Dj = diag(d1, . . . , dj),
Lj =
⎡
⎢⎢⎢⎢⎢⎣
1
0
· · ·
0
0
l1
1
· · ·
0
0
...
...
...
...
0
0
· · ·
1
0
0
0
· · ·
lj−1
1
⎤
⎥⎥⎥⎥⎥⎦
,
d1 = α1, and for i = 2, . . . , j, li−1 = βi−1/di−1 and di = αi −βi−1li−1. Thus, the
solution for yj in (6.19) can be easily found by first solving Ljwj = Q′
jc, then
Djzj = wj, and finally L′
jyj = zj. Even as j increases, the computation required
is not extensive because Dj−1 and Lj−1 are submatrices of Dj and Lj, and so in the
jth iteration, we only need to calculate dj and lj−1 to obtain Dj and Lj from Dj−1
and Lj−1.
The next step is to compute xj from yj using (6.18). We will see that this also
may be done with a small amount of computation. Note that if we define the m × j

278
SYSTEMS OF LINEAR EQUATIONS
matrix Bj = (b1, . . . , bj) so that BjL′
j = Qj, then by premultiplying the equation
Tjyj = Q′
jc by QjT −1
j
and using (6.18), we get
xj = QjT −1
j Q′
jc = Qj(LjDjL′
j)−1Q′
jc = Bjzj,
(6.22)
where zj is as previously defined. It will be easier to compute xj from (6.22)
than from (6.18) because Bj and zj are simple to compute after Bj−1 and zj−1
have already been calculated. For instance, from the definition of Bj, we see that
b1 = q1 and bi = qi −li−1bi−1 for i > 1, and consequently, Bj = (Bj−1, bj).
Using the defining equations for wj and zj, we find that
LjDjzj = Q′
jc.
(6.23)
If we partition zj as zj = (γ′
j−1, γj)′, where γj−1 is a (j −1) × 1 vector, then by
using the fact that
Lj =

 Lj−1
0
lj−1e′
j−1
1

,
Dj =

Dj−1
0
0′
dj

,
we see that (6.23) implies that Lj−1Dj−1γj−1 = Q′
j−1c. As a result, γj−1 = zj−1,
and so to compute zj, we only need to compute γj, which is given by
γj = (q′
jc −lj−1dj−1γj−1)/dj,
where γj−1 is the last component of zj−1. Thus, (6.22) becomes
xj = Bjzj = [Bj−1
bj]

zj−1
γj

= Bj−1zj−1 + γjbj = xj−1 + γjbj,
and so we have a simple formula for computing the jth iterative solution from bj, γj,
and the (j −1)th iterative solution xj−1.
PROBLEMS
6.1 Consider the system of equations Ax = c, where A is the 4 × 3 matrix given
in Problem 5.2 and
c =
⎡
⎢⎢⎣
1
3
−1
0
⎤
⎥⎥⎦.
(a) Show that the system is consistent.

PROBLEMS
279
(b) Find a solution to this system of equations.
(c) How many linearly independent solutions are there?
6.2 The system of equations Ax = c has A equal to the 3 × 4 matrix given in Prob-
lem 5.48 and
c =
⎡
⎣
1
1
4
⎤
⎦.
(a) Show that the system of equations is consistent.
(b) Give the general solution.
(c) Find r, the number of linearly independent solutions.
(d) Give a set of r linearly independent solutions.
6.3 Suppose the system of equations Ax = c has
A =
⎡
⎢⎢⎣
5
2
1
3
1
1
2
1
0
1
2
−3
⎤
⎥⎥⎦.
For each c given below, determine whether the system of equations is consistent.
(a)
c =
⎡
⎢⎢⎣
1
1
1
1
⎤
⎥⎥⎦,
(b)
c =
⎡
⎢⎢⎣
3
2
1
−1
⎤
⎥⎥⎦,
(c)
c =
⎡
⎢⎢⎣
1
−1
1
−1
⎤
⎥⎥⎦.
6.4 Consider the system of equations Ax = c, where
A =

1
1
−1
0
2
2
1
1
1
1

,
c =

3
1

.
(a) Show that the system of equations is consistent.
(b) Give the general solution.
(c) Find r, the number of linearly independent solutions.
(d) Give a set of r linearly independent solutions.
6.5 Prove Theorem 6.5.
6.6 Consider the system of equations AXB = C, where X is a 3 × 3 matrix of
variables and
A =

1
3
1
3
2
1

,
B =
⎡
⎣
1
−1
1
0
0
1
⎤
⎦,
C =

4
2
2
1

.

280
SYSTEMS OF LINEAR EQUATIONS
(a) Show that the system of equations is consistent.
(b) Find the form of the general solution to this system.
6.7 The general solution of a consistent system of equations was given in Theorem
6.4 as A−c + (In −A−A)y. Show that the two vectors A−c and (In −A−A)y
are linearly independent if c ̸= 0 and (In −A−A)y ̸= 0.
6.8 Suppose we want to find solutions x to (6.1) under the restriction that the solu-
tions are in some vector space S. Let PS be the projection matrix for the orthog-
onal projection onto S. Show that a restricted solution exists if APS(APS)−c =
c and, in this case, the general solution is given by
xy = PS(APS)−c + PS{In −(APS)−APS}y,
where y is an arbitrary n × 1 vector.
6.9 Suppose the m × n matrix A and m × 1 vector c ̸= 0 are such that A−c is
the same for all choices of A−. Use Theorem 5.24 to show that if Ax = c is a
consistent system of equations, then it has a unique solution.
6.10 Let A be an m × n matrix, whereas c and d are m × 1 and n × 1 vectors,
respectively.
(a) Show that BAx = Bc has the same set of solutions for the n × 1 vector x
as Ax = c if the p × m matrix B has full column rank.
(b) Show that A′Ax = A′Ad has the same set of solutions for x as Ax = Ad.
6.11 For the homogeneous system of equations Ax = 0 in which
A =

−1
3
−2
1
2
−3
0
−2

,
determine r, the number of linearly independent solutions, and find a set of r
linearly independent solutions.
6.12 Suppose that x∗is a solution to the system of equations Ax = c and y∗is a
solution to the system of equations A′y = d. Show that d′x∗= c′y∗.
6.13 Suppose Ax = c is a consistent system of equations, and let G be a matrix that
satisfies conditions 1 and 4 of the Moore–Penrose generalized inverse of A.
Show that Gc is the minimal solution of the system Ax = c. That is, show that
for any other solution x∗̸= Gc, x′
∗x∗> c′G′Gc.
6.14 Show that if the system of equations AXB = C is consistent, then the solution
is unique if and only if A has full column rank and B has full row rank.
6.15 Suppose A is an m × m matrix satisfying AX1 = X1Λ1 for some m × r full
rank matrix X1 and diagonal matrix Λ1 = diag(λ1, . . . , λr). If X2 is an m ×
(m −r) matrix such that X = [X1
X2] is nonsingular, show that A can be
expressed as A = X1Λ1X−
1 −WX−
2 , where W is an m × (m −r) matrix,
and X−
1 and X−
2 are the generalized inverses of X1 and X2 satisfying X−1′ =
[X−′
1
X−′
2 ].

PROBLEMS
281
6.16 Let
A =

1
−1
1
1
2
3
1
−1

,
c =

1
2

,
B =

2
1
2
−1
0
1
1
1

,
d =

2
4

.
(a) Show that the system Ax = c is consistent and has three linearly indepen-
dent solutions.
(b) Show that the system Bx = d is consistent and has three linearly indepen-
dent solutions.
(c) Show that the systems Ax = c and Bx = d have a common solution and
that this common solution is unique.
6.17 Consider the systems of equations AX = C and XB = D, where A is m × n,
B is p × q, C is m × p, and D is n × q.
(a) Show that the two systems of equations have a common solution X if and
only if each system is consistent and AD = CB.
(b) Show that the general common solution is given by
X∗= A−C + (In −A−A)DB−+ (In −A−A)Y (Ip −BB−),
where Y is an arbitrary n × p matrix.
6.18 Suppose the r × m matrix A1 is a generalized inverse of the m × r
matrix X1, which has rank r. We wish to find m × (m −r) matrices X2
and A′
2 such that X = [X1
X2] is nonsingular and X−1 = [A′
1
A′
2]′.
Show that X2 and A2 can be expressed as X2 = (Im −X1A1)Y
and
A2 = {(Im −X1A1)Y }−(Im −X1A1), where Y is an arbitrary m × (m −r)
matrix for which rank{(Im −X1A1)Y } = m −r.
6.19 In Problem 5.49, a least squares inverse was found for the matrix
A =
⎡
⎣
1
−1
−2
1
−2
4
3
−2
1
1
−3
1
⎤
⎦.
(a) Use this least squares inverse to show that the system of equations Ax = c
is inconsistent, where c′ = (2, 1, 5).
(b) Find a least squares solution.
(c) Compute the sum of squared errors for a least squares solution to this system
of equations.

282
SYSTEMS OF LINEAR EQUATIONS
6.20 Consider the system of equations Ax = c, where
A =
⎡
⎢⎢⎣
1
0
2
2
−1
3
−1
2
0
−2
1
−3
⎤
⎥⎥⎦,
c =
⎡
⎢⎢⎣
2
2
5
0
⎤
⎥⎥⎦.
(a) Find a least squares inverse of A.
(b) Show that the system of equations is inconsistent.
(c) Find a least squares solution.
(d) Is this solution unique?
6.21 Show that x∗is a least squares solution to the system of equations Ax = c if
and only if
A′Ax∗= A′c.
6.22 Let A be an m × n matrix, and x∗, y∗, and c be n × 1, m × 1, and m × 1 vec-
tors, respectively. Suppose that x∗and y∗are such that the system of equations

Im
A
A′
(0)
 
y∗
x∗

=

c
0

holds. Show that x∗then must be a least squares solution to the system Ax = c.
6.23 The balanced two-way classification model with interaction is of the form
yijk = μ + τi + γj + ηij + ϵijk,
where i = 1, . . . , a, j = 1, . . . , b, and k = 1, . . . , n. The parameter μ repre-
sents an overall effect, τi is an effect due to the ith level of factor one, γj is an
effect due to the jth level of factor two, and ηij is an effect due to the interaction
of the ith and jth levels of factors one and two; as usual, the ϵijk’s represent
independent random errors, each distributed as N(0, σ2).
(a) Set up the vectors y, β, and ϵ and the matrix X so that the two-way model
above can be written in the matrix form y = Xβ + ϵ.
(b) Find the rank, r, of X. Determine a set of r linearly independent estimable
functions of the parameters, μ, τi, γj, and ηij.
(c) Find a least squares solution for the parameter vector β.
6.24 Consider the regression model
y = Xβ + ϵ,
where X is N × m, ϵ ∼NN(0, σ2C), and C is a known positive definite
matrix. In Example 4.6, for the case in which X is full column rank, we

PROBLEMS
283
obtained the generalized least squares estimator, ˆβ = (X′C−1X)−1X′C−1y,
that minimizes
(y −X ˆβ)′C−1(y −X ˆβ).
(6.24)
Show that if X is less than full column rank, then the generalized least squares
solution for β that minimizes (6.24) is given by
ˆβ = (X′C−1X)−X′C−1y + {Im −(X′C−1X)−X′C−1X}u,
where u is an arbitrary m × 1 vector.
6.25 Restricted least squares obtains the vectors ˆβ that minimize
(y −X ˆβ)′(y −X ˆβ),
subject to the restriction that ˆβ satisfies Bˆβ = b, where B is p × m and b is
p × 1, such that BB−b = b. Use Theorem 6.4 to find the general solution ˆβu to
the consistent system of equations Bˆβ = b, where ˆβu depends on an arbitrary
vector u. Substitute this expression for ˆβ into (y −X ˆβ)′(y −X ˆβ), and then
use Theorem 6.14 to obtain the general least squares solution uω, for u, where
uω depends on an arbitrary vector w. Substitute uω for u in ˆβu to show that
the general restricted least squares solution for β is given by
ˆβω = B−b + (Im −B−B){[X(Im −B−B)]L(y −XB−b)
+(Im −[X(Im −B−B)]LX(Im −B−B))w}.
6.26 In the previous problem, show that if we use the Moore–Penrose inverse as the
least squares inverse of [X(Im −B−B)] in the expression given for ˆβω, then
it simplifies to
ˆβω = B−b + [X(Im −B−B)]+(y −XB−b)
+(Im −B−B){Im −[X(Im −B−B)]+X(Im −B−B)}w.


7
PARTITIONED MATRICES
7.1
INTRODUCTION
The concept of partitioning matrices was first introduced in Chapter 1, and we have
subsequently used partitioned matrices throughout this text. Up to this point, most
of our applications involving partitioned matrices have utilized only the simple
operations of matrix addition and matrix multiplication. In this chapter, we will
obtain expressions for such things as the inverse, determinant, and rank of a matrix
in terms of its submatrices. We will restrict attention to an m × m matrix A that is
partitioned into the 2 × 2 form
A =

A11 A12
A21 A22

,
(7.1)
where A11 is m1 × m1, A12 is m1 × m2, A21 is m2 × m1, and A22 is m2 × m2.
Additional results, including ones for which A11 and A22 are not square matrices,
can be found in Harville (1997).
7.2
THE INVERSE
In this section, we obtain an expression for the inverse of A when it and at least one
of the submatrices down the diagonal are nonsingular.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

286
PARTITIONED MATRICES
Theorem 7.1
Let the m × m matrix A be partitioned as in (7.1), and suppose that
A is nonsingular. For notational convenience, write B = A−1 and partition B as
B =

B11 B12
B21 B22

,
where the submatrices of B are of the same sizes as the corresponding submatrices
of A. Then if A11 and A22 −A21A−1
11 A12 are nonsingular, we have
(a) B11 = A−1
11 + A−1
11 A12B22A21A−1
11 ,
(b) B22 = (A22 −A21A−1
11 A12)−1,
(c) B12 = −A−1
11 A12B22,
(d) B21 = −B22A21A−1
11 ,
whereas if A22 and A11 −A12A−1
22 A21 are nonsingular, we have
(e) B11 = (A11 −A12A−1
22 A21)−1,
(f) B22 = A−1
22 + A−1
22 A21B11A12A−1
22 ,
(g) B12 = −B11A12A−1
22 ,
(h) B21 = −A−1
22 A21B11.
Proof.
Suppose that A11 and A22 −A21A−1
11 A12 are nonsingular. The matrix
equation
AB =

A11 A12
A21 A22
 
B11 B12
B21 B22

=

Im1 (0)
(0) Im2

= Im
yields the four equations
A11B11 + A12B21 = Im1,
(7.2)
A21B12 + A22B22 = Im2,
(7.3)
A11B12 + A12B22 = (0),
(7.4)
A21B11 + A22B21 = (0).
(7.5)
Solving (7.4) for B12 immediately leads to the expression given in (c) for B12. Substi-
tuting this solution for B12 into (7.3) and solving for B22 yields the expression given
for B22 in (b). From (7.2) we get
B11 = A−1
11 −A−1
11 A12B21.
(7.6)
Substituting this result in (7.5) yields
A21A−1
11 −A21A−1
11 A12B21 + A22B21 = (0),

THE INVERSE
287
from which we get the expression for B21 given in (d). Finally, when this result
is substituted back in (7.6), we obtain the expression given for B11 in (a). The
expressions given in (e)–(h) are obtained in a similar fashion.
□
Note that, in general, A11 and A22 do not need to be nonsingular for A to be
nonsingular. For instance, if m1 = m2, A11 = A22 = (0), and A12 and A21 are
nonsingular, it is easily verified that
A−1 =

(0) A−1
21
A−1
12
(0)

.
Theorem 7.1 and additional results to be developed later in this chapter illustrate
the importance of the matrices A22 −A21A−1
11 A12 and A11 −A12A−1
22 A21 in the anal-
ysis of the partitioned matrix A given in (7.1). These matrices are commonly referred
to as Schur complements. In particular, A22 −A21A−1
11 A12 is called the Schur com-
plement of A11 in A, whereas A11 −A12A−1
22 A21 is called the Schur complement of
A22 in A. Some results involving Schur complements in addition to the ones given in
this chapter can be found in Ouellette (1981) and Zhang (2005).
Example 7.1
Consider the regression model
y = Xβ + ϵ,
where y is N × 1, X is N × (k + 1), β is (k + 1) × 1, and ϵ is N × 1. Suppose
that β and X are partitioned as β = (β′
1, β2)′ and X = (X1, X2) so that the product
X1β1 is defined, and we are interested in comparing this complete regression model
with the reduced regression model,
y = X1β1 + ϵ.
If X has full column rank, then the least squares estimators for the two models are
ˆβ = (X′X)−1X′y and ˆβ1 = (X′
1X1)−1X′
1y, respectively, and the difference in the
sums of squared errors for the two models,
(y −X1ˆβ1)′(y −X1ˆβ1) −(y −X ˆβ)′(y −X ˆβ)
= y′(IN −X1(X′
1X1)−1X′
1)y −y′(IN −X(X′X)−1X′)y
= y′X(X′X)−1X′y −y′X1(X′
1X1)−1X′
1y,
(7.7)
gives the reduction in the sum of squared errors attributable to the inclusion of the
term X2β2 in the complete model. By using the geometrical properties of least
squares regression in Example 2.11, we showed that this reduction in the sum of
squared errors simplifies to
y′X2∗(X′
2∗X2∗)−1X′
2∗y,

288
PARTITIONED MATRICES
where X2∗= (IN −X1(X′
1X1)−1X′
1)X2. An alternative way of showing this, which
we illustrate here, uses Theorem 7.1. Now X′X can be partitioned as
X′X =

X′
1X1 X′
1X2
X′
2X1 X′
2X2

,
and so if we let
C = (X′
2X2 −X′
2X1(X′
1X1)−1X′
1X2)−1 = (X′
2∗X2∗)−1,
we find from a direct application of Theorem 7.1 that
(X′X)−1 =

(X′
1X1)−1 + (X′
1X1)−1X′
1X2CX′
2X1(X′
1X1)−1 −(X′
1X1)−1X′
1X2C
−CX′
2X1(X′
1X1)−1
C

.
Substituting this into (7.7) and then simplifying, we get y′X2∗(X′
2∗X2∗)−1X′
2∗y, as
required.
7.3
THE DETERMINANT
In this section, we will begin by obtaining an expression for the determinant of A
when at least one of the submatrices A11 and A22 is nonsingular. Before doing this,
we will first consider some special cases.
Theorem 7.2
Let the m × m matrix A be partitioned as in (7.1). If A22 = Im2, and
A12 = (0) or A21 = (0), then |A| = |A11|.
Proof.
To find the determinant
|A| =

A11 (0)
A21 Im2
 ,
first apply the cofactor expansion formula for a determinant on the last column of A
to obtain
|A| =

A11
(0)
B
Im2−1
 ,
where B is the (m2 −1) × m1 matrix obtained by deleting the last row from A21.
Repeating this process another (m2 −1) times yields |A| = |A11|. In a similar
fashion, we obtain |A| = |A11| when A21 = (0), by repeatedly expanding along the
last row.
□
Clearly we have a result analogous to Theorem 7.2 when A11 = Im1 and A12 = (0)
or A21 = (0). Also, Theorem 7.2 can be generalized to Theorem 7.3.

THE DETERMINANT
289
Theorem 7.3
Let the m × m matrix A be partitioned as in (7.1). If A12 = (0) or
A21 = (0), then |A| = |A11||A22|.
Proof.
Observe that
|A| =

A11 (0)
A21 A22
 =

A11 (0)
A21 Im2


Im1 (0)
(0) A22
 = |A11||A22|,
where the last equality follows from Theorem 7.2. A similar proof yields
|A| = |A11||A22| when A21 = (0).
□
We are now ready to find an expression for the determinant of A when the only
thing we know is that A11 or A22 is nonsingular.
Theorem 7.4
Let the m × m matrix A be partitioned as in (7.1). Then
(a) |A| = |A22||A11 −A12A−1
22 A21|, if A22 is nonsingular,
(b) |A| = |A11||A22 −A21A−1
11 A12|, if A11 is nonsingular.
Proof.
Suppose that A22 is nonsingular. Note that, in this case, the identity

Im1 −A12A−1
22
(0)
Im2
 
A11 A12
A21 A22
 
Im1
(0)
−A−1
22 A21 Im2

=

A11 −A12A−1
22 A21 (0)
(0)
A22

holds. After taking the determinant of both sides of this identity and using the previous
theorem, we immediately get (a). The proof of (b) is obtained in a similar fashion by
using the identity

Im1
(0)
−A21A−1
11 Im2
 
A11 A12
A21 A22
 
Im1 −A−1
11 A12
(0)
Im2

=

A11
(0)
(0) A22 −A21A−1
11 A12

.
□
Example 7.2
We will find the determinant and inverse of the 2m × 2m matrix A
given by
A =

aIm
1m1′
m
1m1′
m
bIm

,

290
PARTITIONED MATRICES
where a and b are nonzero scalars. Using (a) of Theorem 7.4, we find that
|A| = |bIm| |aIm −1m1′
m(bIm)−11m1′
m|
= bm aIm −m
b 1m1′
m

= bmam−1

a −m2
b

,
where we have used the result of Problem 3.29(e) in the last step. The matrix A will
be nonsingular if |A| ̸= 0 or, equivalently, if
a ̸= m2
b .
In this case, using Theorem 7.1, we find that
B11 = (aIm −1m1′
m(bIm)−11m1′
m)−1
=

aIm −m
b 1m1′
m
−1
= a−1Im +
	
m
a(ab −m2)

1m1′
m,
where this last expression follows from Problem 3.29(d). In a similar fashion, we find
that
B22 = (bIm −1m1′
m(aIm)−11m1′
m)−1
=

bIm −m
a 1m1′
m
−1
= b−1Im +
	
m
b(ab −m2)

1m1′
m.
The remaining submatrices of B = A−1 are given by
B12 = −(aIm)−11m1′
m

b−1Im +
	
m
b(ab −m2)

1m1′
m

= −(ab −m2)−11m1′
m,
and because A is symmetric, B21 = B′
12 = B12. Putting this all together, we have
A−1 = B =

a−1(Im + mc1m1′
m)
−c1m1′
m
−c1m1′
m
b−1(Im + mc1m1′
m)

,
where c = (ab −m2)−1.
Theorem 7.4 can be used to prove the following useful inequality.

THE DETERMINANT
291
Theorem 7.5
Suppose the m × m matrix A is partitioned as in (7.1) and is a pos-
itive definite matrix. Then
|A| ≤|A11||A22|.
Proof.
Since A21A−1
11 A12 = A′
12A−1
11 A12 is nonnegative definite, it follows from
Theorem 3.28 that
λh(A22) = λh(A22 −A′
12A−1
11 A12 + A′
12A−1
11 A12)
≥λh(A22 −A′
12A−1
11 A12)
for h = 1,. . ., m2. Consequently, |A22| ≥|A22 −A′
12A−1
11 A12|, and so the result
follows from Theorem 7.4(b).
□
Theorem 7.1 and Theorem 7.4 are helpful in showing that conditional distribu-
tions formed from a random vector having a multivariate normal distribution are also
multivariate normal.
Example 7.3
Suppose that x ∼Nm(μ, Ω), where the covariance matrix Ω is
positive definite. Partition x as x = (x′
1, x′
2)′, where x1 is m1 × 1 and x2 is m2 × 1.
Let μ = (μ′
1, μ′
2)′ be partitioned similar to x, whereas
Ω =

Ω11 Ω12
Ω′
12 Ω22

,
where Ω11 is m1 × m1 and Ω22 is m2 × m2. We wish to determine the conditional
distribution of x1 given x2. We will do this by obtaining the conditional density of
x1 given x2, which is defined by
f1|2(x1|x2) = f(x)
f2(x2),
where f(x) and f2(x2) are the density functions of x and x2, respectively. Since
x ∼Nm(μ, Ω), which implies that x2 ∼Nm2(μ2, Ω22), it immediately follows that
f(x) =
1
(2π)m/2|Ω|1/2 exp
	
−1
2(x −μ)′Ω−1(x −μ)

and
f2(x2) =
1
(2π)m2/2|Ω22|1/2 exp
	
−1
2(x2 −μ2)′Ω−1
22 (x2 −μ2)

.
As a result, f1|2(x1|x2) has the form
f1|2(x1|x2) =
1
(2π)m1/2a e−b/2.

292
PARTITIONED MATRICES
Now using Theorem 7.4(a), we find that
a = |Ω|1/2|Ω22|−1/2
= |Ω22|1/2|B1|1/2|Ω22|−1/2 = |B1|1/2,
where B1 is the Schur complement Ω11 −Ω12Ω−1
22 Ω′
12. Also from Theorem
7.1(e)–(h),
Ω−1 =

B−1
1
−B−1
1 Ω12Ω−1
22
−Ω−1
22 Ω′
12B−1
1
Ω−1
22 + Ω−1
22 Ω′
12B−1
1 Ω12Ω−1
22

,
so we have
b = (x −μ)′Ω−1(x −μ) −(x2 −μ2)′Ω−1
22 (x2 −μ2)
= (x1 −μ1)′B−1
1 (x1 −μ1) + (x2 −μ2)′Ω−1
22 Ω′
12B−1
1 Ω12Ω−1
22 (x2 −μ2)
−(x1 −μ1)′B−1
1 Ω12Ω−1
22 (x2 −μ2)
−(x2 −μ2)′Ω−1
22 Ω′
12B−1
1 (x1 −μ1)
= {x1 −μ1 −Ω12Ω−1
22 (x2 −μ2)}′B−1
1 {x1 −μ1 −Ω12Ω−1
22 (x2 −μ2)}
= (x1 −c)′B−1
1 (x1 −c),
where c = μ1 + Ω12Ω−1
22 (x2 −μ2). Thus,
f1|2(x1|x2) =
1
(2π)m1/2|B1|1/2 exp
	
−1
2(x1 −c)′B−1
1 (x1 −c)

,
which is the Nm1(c, B1) density function; that is, we have shown that
x1|x2 ∼Nm1(μ1 + Ω12Ω−1
22 (x2 −μ2), Ω11 −Ω12Ω−1
22 Ω′
12).
Theorem 7.4 can be used to prove the following theorem which gives necessary
and sufficient conditions for an m × m symmetric matrix to be positive definite.
Theorem 7.6
Let A be an m × m symmetric matrix, and let Ak be its leading k × k
principal submatrix; that is, Ak is the matrix obtained by deleting the last m −k rows
and columns of A. Then A is positive definite if and only if all of its leading principal
minors, |A1|,. . ., |Am|, are positive.
Proof.
Suppose first that A is positive definite so that |Am| > 0. For an arbitrary k ×
1 vector x ̸= 0 where k < m, define the m × 1 vector y = (x′, 0′)′. Then because
y ̸= 0 and A is positive definite, we must have
y′Ay = x′Akx > 0.

THE DETERMINANT
293
Thus, Ak is positive definite, and so |Ak| > 0. Next assume that |Ak| > 0 for
k = 1,. . ., m. We will use induction to prove that A = Am is positive definite.
Trivially, A1 is positive definite because |A1| > 0 and A1 is 1 × 1. If Ak is positive
definite for some k < m, we will show that Ak+1 must also be positive definite. For
some k × 1 vector b and scalar c, Ak+1 can be partitioned into the form
Ak+1 =
Ak b
b′
c

.
Note that
B′Ak+1B =
Ak
0
0′
c −b′A−1
k b

,
where
B =

Ik −A−1
k b
0′
1

,
and Ak+1 is positive definite if and only if B′Ak+1B is positive definite. Since Ak
is positive definite, Ak+1 is positive definite if and only if c −b′A−1
k b is positive.
However, we know that |Ak| > 0, |Ak+1| > 0, and from Theorem 7.4
|Ak+1| = |Ak|(c −b′A−1
k b),
so the result follows.
□
We will also use Theorem 7.4 to establish Theorem 7.7.
Theorem 7.7
Let A and B be m × n and n × m matrices, respectively. Then
|Im + AB| = |In + BA|.
Proof.
Note that
 Im
A
−B In
 Im (0)
B
In

=
Im + AB A
(0)
In

,
so that by taking the determinant of both sides and using Theorem 7.4, we obtain the
identity

Im
A
−B In
 = |Im + AB|.
(7.8)
Similarly, observe that

Im (0)
B
In
 
Im
A
−B In

=

Im
A
(0) In + BA

,

294
PARTITIONED MATRICES
so that

Im
A
−B In
 = |In + BA|.
(7.9)
The result now follows by equating (7.8) and (7.9).
□
Corollary 7.7.1 follows directly from Theorem 7.7 if we replace A by −λ−1A.
Corollary 7.7.1
Let A and B be m × n and n × m matrices. Then the nonzero
eigenvalues of AB are the same as the nonzero eigenvalues of BA.
Suppose that both A11 and A22 are singular so that we cannot use Theorem 7.4
to compute the determinant of A. A natural question in this situation is whether the
formulas in Theorem 7.4 still hold if we replace the inverses by generalized inverses.
A simple example will illustrate that, in general, this is not the case. For instance, if
A =

0 1
1 0

,
then |A| = −1. However, the formulas |a11||a22 −a21a−
11a12| and |a22||a11 −
a12a−
22a21| both yield 0 regardless of the choice of the generalized inverses a−
11 and a−
22.
Conditions under which we can replace the inverses in Theorem 7.4 by
generalized inverses are given in our next theorem. The matrices A22 −A21A−
11A12
and A11 −A12A−
22A21 appearing in this theorem are commonly referred to as
generalized Schur complements.
Theorem 7.8
Let the m × m matrix A be partitioned as in (7.1), and suppose that
A−
11 and A−
22 are arbitrary generalized inverses of A11 and A22. Then
(a) if R(A21) ⊂R(A22) or R(A′
12) ⊂R(A′
22),
|A| = |A22||A11 −A12A−
22A21|,
(b) if R(A12) ⊂R(A11) or R(A′
21) ⊂R(A′
11),
|A| = |A11||A22 −A21A−
11A12|.
Proof.
It follows from Theorem 5.25 that A22A−
22A21 = A21 if R(A21) ⊂R(A22).
Consequently,
A =
A11 A12
A21 A22

=
Im1 A12
(0) A22
 A11 −A12A−
22A21 (0)
A−
22A21
Im2

.

THE DETERMINANT
295
Taking the determinant of both sides of this equation and then applying Theorem
7.3 yields the determinantal identity in (a). It also follows from Theorem 5.25 that
A12A−
22A22 = A12 if R(A′
12) ⊂R(A′
22). In this case,
A =

A11 A12
A21 A22

=

A11 −A12A−
22A21 A12A−
22
(0)
Im2
 
Im1 (0)
A21 A22

.
Taking the determinant of both sides of this equation and then applying Theorem 7.3,
we again get the determinantal identity in (a). This establishes (a). Part (b) is proven
in a similar fashion.
□
It is important to note that if A is a nonnegative definite matrix, then it satisfies
the conditions given in (a) and (b) of Theorem 7.8. To see this, recall that if A is
nonnegative definite, it can be written as A = TT ′, where T is m × m. Partitioning
T appropriately, we then get
A =

A11 A12
A21 A22

=

T1
T2
 
T ′
1 T ′
2

=

T1T ′
1 T1T ′
2
T2T ′
1 T2T ′
2

.
Since A22 = T2T ′
2, R(A22) = R(T2), and because A21 = T2T ′
1, R(A21) ⊂R(T2),
and so R(A21) ⊂R(A22). Clearly R(A′
22) = R(A22) and R(A′
12) = R(A21), so we
also have R(A′
12) ⊂R(A′
22); that is, the conditions of Theorem 7.8(a) hold. Similarly,
it can be shown that the conditions in (b) hold as well.
We will use the normal distribution as an illustration of an application in which a
generalized Schur complement arises naturally.
Example 7.4
Suppose that x ∼Nm(μ, Ω), where the covariance matrix Ω is
positive semidefinite. Since Ω is singular, x does not have a density function, and
so the derivation of the conditional distribution given in Example 7.3 does not apply
in this case. Let x, μ, and Ω be partitioned as in Example 7.3, and again we want
to find the conditional distribution of x1 given x2. For any m × m matrix A of
constants, we know that Ax ∼Nm(Aμ, AΩA′), and in particular, if
A =

Im1 −Ω12Ω−
22
(0)
Im2

,
we find that
Ax =
x1 −Ω12Ω−
22x2
x2


296
PARTITIONED MATRICES
has a multivariate normal distribution with mean vector
Aμ =
μ1 −Ω12Ω−
22μ2
μ2

.
The covariance matrix is
AΩA′ =

Im1 −Ω12Ω−
22
(0)
Im2
 
Ω11 Ω12
Ω′
12 Ω22
 
Im1
(0)
−Ω−′
22Ω′
12 Im2

=

Ω11 −Ω12Ω−
22Ω′
12 (0)
Ω′
12
Ω22
 
Im1
(0)
−Ω−′
22Ω′
12 Im2

=

Ω11 −Ω12Ω−
22Ω′
12 (0)
(0)
Ω22

.
(7.10)
In simplifying AΩA′, we have used the fact that Ω12Ω−
22Ω22 = Ω12 and the transpose
of this identity, Ω22Ω−′
22Ω′
12 = Ω′
12. As a result of the zero covariances in (7.10), it
follows that x1 −Ω12Ω−
22x2 is independently distributed of x2, and so its conditional
distribution given x2 is the same as its unconditional distribution. Consequently,
we have
x1|x2 ∼Nm1(μ1 + Ω12Ω−
22(x2 −μ2), Ω11 −Ω12Ω−
22Ω′
12).
7.4
RANK
In this section, we wish to find an expression for the rank of A in term of the subma-
trices given in (7.1). One special case was already given in Theorem 2.9; if
A =

A11 (0)
(0) A22

,
then rank(A) = rank(A11) + rank(A22). When A12 ̸= (0) or A21 ̸= (0), but A11 or
A22 is nonsingular, Theorem 7.9 can be used to determine the rank of A.
Theorem 7.9
Let A be defined as in (7.1). Then
(a) if A22 is nonsingular,
rank(A) = rank(A22) + rank(A11 −A12A−1
22 A21),
(b) if A11 is nonsingular,
rank(A) = rank(A11) + rank(A22 −A21A−1
11 A12).

RANK
297
Proof.
To prove part (a), note that because A22 is nonsingular, we can write A as
A =
A11 A12
A21 A22

=

Im1 A12A−1
22
(0)
Im2
 
A11 −A12A−1
22 A21 (0)
(0)
A22

×

Im1
(0)
A−1
22 A21 Im2

.
It follows from Theorem 7.4 that the determinant of the matrix

Im1 A12A−1
22
(0)
Im2

is 1, so this matrix is nonsingular. Likewise, the matrix

Im1
(0)
A−1
22 A21 Im2

is nonsingular, so an application of Theorem 1.10 yields
rank
	A11 A12
A21 A22

= rank
	
A11 −A12A−1
22 A21 (0)
(0)
A22

.
The result now follows from Theorem 2.9. The proof of part (b) is similar.
□
Under similar but slightly stronger conditions than those given in Theorem 7.8,
we are able to generalize the results of Theorem 7.9 to situations in which both A11
and A22 are singular matrices.
Theorem 7.10
Let the m × m matrix A be partitioned as in (7.1), and suppose that
A−
11 and A−
22 are any generalized inverses of A11 and A22.
(a) If R(A21) ⊂R(A22) and R(A′
12) ⊂R(A′
22), then
rank(A) = rank(A22) + rank(A11 −A12A−
22A21).
(b) If R(A12) ⊂R(A11) and R(A′
21) ⊂R(A′
11), then
rank(A) = rank(A11) + rank(A22 −A21A−
11A12).

298
PARTITIONED MATRICES
Proof.
We will only prove the result in (a) because the proof of (b) is very similar.
Since, by Theorem 5.25, R(A21) ⊂R(A22) and R(A′
12) ⊂R(A′
22) imply that
A22A−
22A21 = A21 and A12A−
22A22 = A12, it is easily verified that
C1AC2 = C3,
(7.11)
where
C1 =
Im1 −A12A−
22
(0)
Im2

,
C2 =

Im1
(0)
−A−
22A21 Im2

,
and
C3 =

A11 −A12A−
22A21 (0)
(0)
A22

.
Applying Theorem 7.4 to C1 and C2, we find that |C1| = 1 and |C2| = 1, and so it
follows Theorem 1.10 that
rank(A) = rank(C1AC2) = rank(C3).
The result now follows from Theorem 2.9.
□
7.5
GENERALIZED INVERSES
In this section, we will present some results for the one condition generalized inverse
A−and the Moore–Penrose inverse A+ of a matrix A partitioned in the form as
given in (7.1). We begin with the generalized inverse A−. First we will consider a
special case.
Theorem 7.11
Let the m × m matrix A be partitioned as in (7.1). Suppose that
A12 = (0) and A21 = (0) and that A−
11 and A−
22 are any generalized inverses of A11
and A22. Then

A−
11 (0)
(0) A−
22

is a generalized inverse of A.
Proof.
Denote the matrix given in the theorem by A−. The result is obtained by
verifying through the use of the identities, A11A−
11A11 = A11 and A22A−
22A22 = A22,
that AA−A = A.
□
Theorem 7.12 gives conditions under which the formulas given in Theorem 7.1
can be used to obtain a generalized inverse of A if we replace the inverses in those
formulas by generalized inverses.
Theorem 7.12
Let the m × m matrix A be partitioned as in (7.1), and suppose that
A−
11 and A−
22 are any generalized inverses of A11 and A22.

GENERALIZED INVERSES
299
(a) If R(A21) ⊂R(A22) and R(A′
12) ⊂R(A′
22), then
A−=

B−
1
−B−
1 A12A−
22
−A−
22A21B−
1
A−
22 + A−
22A21B−
1 A12A−
22

is a generalized inverse of A, where B1 = A11 −A12A−
22A21.
(b) If R(A12) ⊂R(A11) and R(A′
21) ⊂R(A′
11), then
A−=

A−
11 + A−
11A12B−
2 A21A−
11 −A−
11A12B−
2
−B−
2 A21A−
11
B−
2

is a generalized inverse of A, where B2 = A22 −A21A−
11A12.
Proof.
Suppose the conditions in (a) hold. Then equation (7.11) holds and this can
be written as
C1AC2 =

B1 (0)
(0) A22

,
or equivalently,
A = C−1
1
B1 (0)
(0) A22

C−1
2 ,
because C1 and C2 are nonsingular matrices. As a result of Theorem 5.23(d), a gen-
eralized inverse of A can be computed as
A−= C2
B1 (0)
(0) A22
−
C1.
Now using Theorem 7.11, we get
A−=

Im1
(0)
−A−
22A21 Im2
 B−
1
(0)
(0) A−
22
 Im1 −A12A−
22
(0)
Im2

=

B−
1
−B−
1 A12A−
22
−A−
22A21B−
1
A−
22 + A−
22A21B−
1 A12A−
22

,
thereby proving (a). The proof of (b) is similar.
□
Our next result, which is due to Gross (2000), gives an expression for the
Moore–Penrose generalized inverse A+ of the partitioned matrix A when it is
nonnegative definite.

300
PARTITIONED MATRICES
Theorem 7.13
Suppose the m × m nonnegative definite matrix A is partitioned as
in (7.1). Then
A+ =
A+
11 + A+
11A12B∼A′
12A+
11 −A+
11A12B∼
−B∼A′
12A+
11
B∼

+

−A+
11(A12Z + Z′A′
12)A+
11 A+
11Z′
ZA+
11
(0)

,
where
B∼= [Z Im2]
A+
11 + A+
11A12B+A′
12A+
11 −A+
11A12B+
−B+A′
12A+
11
B+
  Z′
Im2

,
Z = (Im2 −B+B)A′
12A+
11{Im1 + A+
11A12(Im2 −B+B)A′
12A+
11}−1,
B = A22 −A′
12A+
11A12.
Proof.
Since A is nonnegative definite, it can be written as
A =

A11 A12
A′
12 A22

=

U ′
V ′
 U V 
=

U ′U U ′V
V ′U V ′V

,
where U and V are r × m1 and r × m2 matrices and r = rank(A) ≤m. Denote the
Moore–Penrose inverse of A by G so that
G =

G11 G12
G21 G22

=

U V
+
U V
+′,
(7.12)
where we have used Theorem 5.3(e). Now it follows from Theorem 5.13 that
U V + =
U + −U +V (C+ + W)
C+ + W

,
(7.13)
where
C = (Ir −UU +)V,
W = ZU +(Ir −V C+)
and
Z = (Im2 −C+C){Im2
+ (Im2 −C+C)V ′U +′U +V (Im2 −C+C)}−1V ′U +′.
Since
U ′C = (U ′ −U ′UU +)V = (U ′ −U ′U +′U ′)V = (U ′ −U ′)V = (0),

GENERALIZED INVERSES
301
it follows (Problem 5.14) that C+U +′ = (0). Using this and (7.13) in (7.12), we find
that
G11 = (U ′U)+ + U +V G22V ′U +′
−U +V WU +′ −U +W ′V ′U +′,
(7.14)
G12 = −U +V G22 + U +W ′,
(7.15)
G22 = (C+ + W)(C+ + W)′.
(7.16)
Note that
C′C = V ′(Ir −UU +)′(Ir −UU +)V
= V ′(Ir −UU +)V
= V ′V −V ′U(U ′U)+U ′V
= A22 −A′
12A+
11A12 = B.
Thus, using Theorem 5.3(g), we have
C+C = (C′C)+C′C = B+B,
V ′U +′ = V ′U(U ′U)+ = A′
12A+
11,
U +V = (U ′U)+U ′V = A+
11A12,
so that
Z = (Im2 −B+B){Im2
+ (Im2 −B+B)A′
12A+
11A+
11A12(Im2 −B+B)}−1A′
12A+
11.
(7.17)
A simple application of Theorem 1.9 yields
{Im2 + (Im2 −B+B)A′
12A+
11A+
11A12(Im2 −B+B)}−1
= Im2 −(Im2 −B+B)A′
12A+
11{Im1 + A+
11A12(Im2 −B+B)A′
12A+
11}−1
× A+
11A12(Im2 −B+B).
(7.18)
Using (7.18) in (7.17) and simplifying, we get
Z = (Im2 −B+B)A′
12A+
11{Im1 + A+
11A12(Im2 −B+B)A′
12A+
11}−1,
which is the formula for Z given in the statement of the theorem. By again using
C+U +′ = (0), we also find that
WU +′ = ZU +U +′ = Z(U ′U)+ = ZA+
11,
(7.19)
WC+′ = −ZU +V (C′C)+ = −ZA+
11A12B+,
(7.20)

302
PARTITIONED MATRICES
and
WW ′ = Z(U +U +′ + U +V C+C+′V ′U +′)Z′
= Z(A+
11 + A+
11A12B+A′
12A+
11)Z′.
(7.21)
Substituting (7.19), (7.20), and (7.21) in (7.14), (7.15), and (7.16), we then get
G11 = A+
11 + A+
11A12G22A′
12A+
11 −A+
11A12ZA+
11 −A+
11Z′A′
12A+
11,
G12 = −A+
11A12G22 + A+
11Z′,
and
G22 = (C′C)+ + WC+′ + C+W ′ + WW ′
= B+ −ZA+
11A12B+ −B+A′
12A+
11Z′
+ Z(A+
11 + A+
11A12B+A′
12A+
11)Z′
= [Z Im2]
A+
11 + A+
11A12B+A′
12A+
11 −A+
11A12B+
−B+A′
12A+
11
B+
  Z′
Im2

= B∼,
and so the proof is complete.
□
7.6
EIGENVALUES
In this section, we explore relationships between the eigenvalues of A as given in
(7.1) with those of A11 and A22. As usual, we will denote the ordered eigenvalues
of a matrix such as A using λi(A); that is, if the eigenvalues of A are real, then
they will be identified as λ1(A) ≥· · · ≥λm(A). Our first result gives bounds on the
eigenvalues of A in terms of the eigenvalues of A11 and A22 when the matrix A is
nonnegative definite.
Theorem 7.14
Suppose the m × m nonnegative definite matrix A is partitioned as
in (7.1). Let h and i be integers between 1 and m inclusive. Then
(a) λh+i−1(A) ≤λh(A11) + λi(A22), if h + i ≤m + 1,
(b) λh+i−m(A) ≥λh(A11) + λi(A22), if h + i ≥m + 1,
where λh(A11) = 0 if h > m1 and λi(A22) = 0 if i > m2.
Proof.
Let A1/2 be the symmetric square root matrix of A, and partition it as
A1/2 = [F
G], where F is m × m1 and G is m × m2. Since
A =

A11 A12
A′
12 A22

= (A1/2)′A1/2 =

F ′F F ′G
G′F G′G

,

EIGENVALUES
303
we see that A11 = F ′F and A22 = G′G. However, we also have
A = A1/2(A1/2)′ = FF ′ + GG′.
(7.22)
Since the nonzero eigenvalues of A11 and FF ′ are the same and the nonzero
eigenvalues of A22 and GG′ are the same, the result follows by applying Theorem
3.23 to (7.22).
□
Theorem 7.14 can be used to obtain bounds for sums of eigenvalues of A. For
instance, from Theorem 7.14(a), it follows that
λh(A) ≤λh(A11) + λ1(A22)
for h = 1,. . ., m1, which immediately leads to
k

h=1
λh(A) ≤
k

h=1
λh(A11) + kλ1(A22),
for k = 1,. . ., m1. The following extension of Theorem 7.14 gives better bounds on
the sums of eigenvalues of A than we get from the repeated application of Theorem
7.14 described above.
Theorem 7.15
Suppose the m × m nonnegative definite matrix A is partitioned as
in (7.1), and let i1,. . ., ik be distinct integers with 1 ≤ij ≤m for j = 1,. . ., k. Then
for k = 1,. . ., m,
k

j=1
{λij(A11) + λm−k+j(A22)} ≤
k

j=1
λij(A)
≤
k

j=1
{λij(A11) + λj(A22)},
where λj(A11) = 0 if j > m1 and λj(A22) = 0 if j > m2.
Proof.
The result follows immediately by applying Theorem 3.24 to (7.22).
□
Theorem 7.14 can be used to obtain bounds on differences between eigenvalues
of a symmetric matrix A and corresponding eigenvalues of A11 or A22. These bounds
are useful in obtaining the asymptotic distribution of the eigenvalues of a random
symmetric matrix (Eaton and Tyler, 1991).
Theorem 7.16
Let A be an m × m symmetric matrix partitioned as in (7.1). If
λm1(A11) > λ1(A22), then
0 ≤λj(A) −λj(A11) ≤λ1(A12A′
12)/{λj(A11) −λ1(A22)},
(7.23)

304
PARTITIONED MATRICES
for j = 1,. . ., m1, and
0 ≤λm2−j+1(A22) −λm−j+1(A)
≤λ1(A12A′
12)/{λm1(A11) −λm2−j+1(A22)},
(7.24)
for j = 1,. . ., m2.
Proof.
Since A11 is the leading m1 × m1 principal submatrix of A, the lower bound
in (7.23) follows immediately from Theorem 3.20. Noting that the upper bound in
(7.23) holds for A if and only if the upper bound holds for the matrix A + αIm,
where α is an arbitrary constant, we may assume without loss of generality that
λ1(A22) = 0 because we can take α = −λ1(A22). In this case, the lower bound in
(7.23) implies that λj(A) ≥λj(A11) > 0, for j = 1,. . ., m1. Let ˆA be the matrix
obtained by replacing A22 in (7.1) by the null matrix so that
ˆA2 =
A2
11 + A12A′
12 A11A12
A′
12A11
A′
12A12

.
(7.25)
Since −A22 is nonnegative definite and ˆA = A −diag((0), A22), it follows from
Theorem 3.28 that λj( ˆA) ≥λj(A), for j = 1,. . ., m. Now the eigenvalues of
ˆA2 are the squares of the eigenvalues of ˆA, but we are not assured that the
ordered eigenvalues satisfy λj( ˆA2) = λ2
j( ˆA) for all j because ˆA is not necessarily
nonnegative definite and, hence, may have negative eigenvalues. However, we do
know that λj( ˆA) ≥λj(A) > 0 for j = 1,. . ., m1, and
λj( ˆA2) ≥λ2
j( ˆA) ≥λ2
j(A),
(7.26)
for j = 1,. . ., m1. Note also that
λj( ˆA2) ≤λj(A2
11 + A12A′
12) + λ1(A′
12A12)
≤{λj(A2
11) + λ1(A12A′
12)} + λ1(A′
12A12)
= λ2
j(A11) + 2λ1(A12A′
12),
(7.27)
where the first inequality applied Theorem 7.14(a), the second inequality applied
Theorem 3.23, whereas the equality follows from the fact that the positive
eigenvalues of A12A′
12 and A′
12A12 are the same, and λ2
j(A11) = λj(A2
11) because
λm1(A11) > 0. Combining (7.26) and (7.27), we find that for j = 1,. . ., m1,
λ2
j(A) ≤λ2
j(A11) + 2λ1(A12A′
12),
or equivalently,
{λj(A) −λj(A11)}{λj(A) + λj(A11)} = λ2
j(A) −λ2
j(A11)
≤2λ1(A12A′
12).

EIGENVALUES
305
Thus,
λj(A) −λj(A11) ≤2λ1(A12A′
12)/{λj(A) + λj(A11)}
≤λ1(A12A′
12)/λj(A11),
because λj(A) ≥λj(A11). This establishes the upper bound in (7.23) because
we are assuming that λ1(A22) = 0. The inequalities in (7.24) can be obtained by
applying those in (7.23) to −A.
□
The bounds given in Theorem 7.16 can be improved on; for instance, see Dümbgen
(1995).
In Theorem 7.17, we use Theorem 7.15 to obtain bounds on the difference between
a sum of eigenvalues of A and the corresponding sum of eigenvalues of A11.
Theorem 7.17
Let A be an m × m symmetric matrix partitioned as in (7.1). If
λm1(A11) > λ1(A22), then
0 ≤
k

j=1
{λj(A) −λj(A11)} ≤
k

j=1
λj(A12A′
12)/{λk(A11) −λ1(A22)},
(7.28)
for k = 1,. . ., m1.
Proof.
The lower bound follows from Theorem 3.20. As in the proof of Theorem
7.16, we may assume without loss of generality that λ1(A22) = 0. Applying Theorem
7.15 to ˆA2 defined in (7.25), we find that
k

j=1
λj( ˆA2) ≤
k

j=1
λj(A2
11 + A12A′
12) +
k

j=1
λj(A′
12A12).
(7.29)
An application of Theorem 3.24 yields
k

j=1
λj(A2
11 + A12A′
12) ≤
k

j=1
λj(A2
11) +
k

j=1
λj(A12A′
12),
and when combined with (7.29), we get
k

j=1
λj( ˆA2) ≤
k

j=1
λ2
j(A11) + 2
k

j=1
λj(A12A′
12),
because the eigenvalues of A11 are positive, and the positive eigenvalues of A12A′
12
and A′
12A12 are the same. Now using (7.26), we have
k

j=1
λ2
j(A) ≤
k

j=1
λ2
j(A11) + 2
k

j=1
λj(A12A′
12),

306
PARTITIONED MATRICES
or equivalently,
k

j=1
{λ2
j(A) −λ2
j(A11)} ≤2
k

j=1
λj(A12A′
12).
(7.30)
However,
k

j=1
{λ2
j(A) −λ2
j(A11)} =
k

j=1
{λj(A) + λj(A11)}{λj(A) −λj(A11)}
≥{λk(A) + λk(A11)}
k

j=1
{λj(A) −λj(A11)}
≥2λk(A11)
k

j=1
{λj(A) −λj(A11)}.
(7.31)
Combining (7.30) and (7.31) leads to
k

j=1
{λj(A) −λj(A11)} ≤
k

j=1
λj(A12A′
12)/λk(A11),
which establishes the upper bound in (7.28) because we have assumed that
λ1(A22) = 0.
□
Our final result compares the eigenvalues of A with those of the Schur complement
A11 −A12A−1
22 A′
12 when A is positive definite.
Theorem
7.18
Suppose
A
in
(7.1)
is
positive
definite,
and
let
B1 =
A11 −A12A−1
22 A′
12,
B2 = A22 −A′
12A−1
11 A12,
and
C = −B−1
1 A12A−1
22 .
Then
if λ1(B1) < λm2(B2),
0 ≤
k

j=1
{λm1−j+1(B1) −λm−j+1(A)}
≤
λ2
m1−k+1(B1)
{λ−1
m1−k+1(B1) −λ−1
m2(B2)}
k

j=1
λj(CC′),
for k = 1,. . ., m1.
Proof.
Using Theorem 7.1 and the fact that both A11 and A22 are nonsingular, the
inverse of A can be expressed as
A−1 =
B−1
1
C
C′
B−1
2

.

PROBLEMS
307
Applying Theorem 3.20 to A−1, we have for j = 1,. . ., m1, λj(B−1
1 ) ≤λj(A−1), so
that λ−1
m1−j+1(B1) ≤λ−1
m−j+1(A) or, equivalently, λm1−j+1(B1) ≥λm−j+1(A). This
proves the lower bound. Since λm1(B−1
1 ) = λ−1
1 (B1) > λ−1
m2(B2) = λ1(B−1
2 ), we
can apply Theorem 7.17 to A−1, which leads to
k

j=1
{λ−1
m−j+1(A) −λ−1
m1−j+1(B1)}
≤
k

j=1
λj(CC′)/{λ−1
m1−k+1(B1) −λ−1
m2(B2)}.
However,
k

j=1
{λ−1
m−j+1(A) −λ−1
m1−j+1(B1)}
=
k

j=1
λm1−j+1(B1) −λm−j+1(A)
λm−j+1(A)λm1−j+1(B1)
≥λ−1
m−k+1(A)λ−1
m1−k+1(B1)
k

j=1
{λm1−j+1(B1) −λm−j+1(A)},
so
k

j=1
{λm1−j+1(B1) −λm−j+1(A)}
≤
λm−k+1(A)λm1−k+1(B1)
{λ−1
m1−k+1(B1) −λ−1
m2(B2)}
k

j=1
λj(CC′)
≤
λ2
m1−k+1(B1)
{λ−1
m1−k+1(B1) −λ−1
m2(B2)}
k

j=1
λj(CC′),
thereby establishing the upper bound.
□
PROBLEMS
7.1 Consider the 2m × 2m matrix
A =
aIm bIm
cIm dIm

,
where a, b, c, and d are nonzero scalars.

308
PARTITIONED MATRICES
(a) Give an expression for the determinant of A.
(b) For what values of a, b, c, and d will A be nonsingular?
(c) Find an expression for A−1.
7.2 Let A be of the form
A =

A11 A12
A21 (0)

,
where each submatrix is m × m and the matrices A12 and A21 are nonsingular.
Find an expression for the inverse of A in terms of A11, A12, and A21 by applying
equations (7.2)–(7.5).
7.3 Generalize Example 7.2 by obtaining the determinant, conditions for
non-singularity, and the inverse of the 2m × 2m matrix
A =

aIm
c1m1′
m
d1m1′
m
bIm

,
where a, b, c, and d are nonzero scalars.
7.4 Let the matrix G be given by
G =
⎡
⎣
A
B
C
(0) D E
(0) (0) F
⎤
⎦,
where each of the matrices A, D, and F is square and nonsingular. Find the
inverse of G.
7.5 Use Theorems 7.1 and 7.4 to find the determinant and inverse of the matrix
A =
⎡
⎢⎢⎢⎢⎣
4 0 0 1 2
0 3 0 1 2
0 0 2 2 3
0 0 1 2 3
1 1 0 1 2
⎤
⎥⎥⎥⎥⎦
.
7.6 Suppose
V =
⎡
⎢⎢⎢⎣
A1 + Ak
Ak
· · ·
Ak
Ak
A2 + Ak · · ·
Ak
...
...
...
...
Ak
Ak
· · ·
Ak−1 + Ak
⎤
⎥⎥⎥⎦,
where A1,. . ., Ak are m × m positive definite matrices. Show that V −1 has the
form
⎡
⎢⎢⎢⎣
A−1
1
−A−1
1 B−1A−1
1
−A−1
1 B−1A−1
2
· · ·
−A−1
1 B−1A−1
k−1
−A−1
2 B−1A−1
1
A−1
2
−A−1
2 B−1A−1
2
· · ·
−A−1
2 B−1A−1
k−1
...
...
...
...
−A−1
k−1B−1A−1
1
−A−1
k−1B−1A−1
2
· · ·
A−1
k−1 −A−1
k−1B−1A−1
k−1
⎤
⎥⎥⎥⎦,
where B = k
i=1 A−1
i .

PROBLEMS
309
7.7 Let A be an m × m matrix partitioned as
A =
A11 A12
A21 A22

,
where A11 is m1 × m1 and rank(A) = rank(A11) = m1.
(a) Show that A22 = A21A−1
11 A12.
(b) Use the result of part (a) to show that
B =

A−1
11 (0)
(0) (0)

is a generalized inverse of A.
(c) Show that the Moore–Penrose inverse of A is given by
A+ =

A′
11
A′
12

C[A′
11 A′
21],
where C = (A11A′
11 + A12A′
12)−1A11(A′
11A11 + A′
21A21)−1.
7.8 Let A be a symmetric matrix partitioned as in (7.1). Show that A is positive
definite if and only if A11 and A22 −A21A−1
11 A12 are positive definite.
7.9 Let A be an m × m positive definite matrix, and let B be its inverse. Partition
A and B as
A =
A11 A12
A′
12 A22

,
B =
B11 B12
B′
12 B22

,
where A11 and B11 are m1 × m1 matrices. Show that the matrix

A11 −B−1
11
A12
A′
12
A22

is positive semidefinite with rank of m −m1.
7.10 Let A and B be defined as in Theorem 7.1. If A is positive definite, show that
B11 −A−1
11 is nonnegative definite.
7.11 Consider the m × m matrix
A =

A11
a
a′
amm

,
where the (m −1) × (m −1) matrix A11 is positive definite.
(a) Prove that |A| ≤amm|A11| with equality if and only if a = 0.
(b) Generalize the result of part (a) by proving that if a11,. . ., amm are the
diagonal elements of a positive definite matrix A, then |A| ≤a11 · · · amm
with equality if and only if A is a diagonal matrix.

310
PARTITIONED MATRICES
7.12 Let A and B be nonsingular matrices, with A being m × m and B being n × n.
If C is m × n, D is n × m, and A + CBD is nonsingular, then an expres-
sion for the inverse of A + CBD utilizing the inverse of B−1 + DA−1C was
given in Theorem 1.9. Show that A + CBD is nonsingular if and only if B−1 +
DA−1C is nonsingular by applying Theorem 7.4 to the matrix
E =
A
C
D −B−1

.
7.13 Let A be defined as in (7.1), and suppose that B is an m2 × m1 matrix.
Show that

A11
A12
A21 + BA11 A22 + BA12
 = |A|.
7.14 Suppose A is partitioned as in (7.1), m1 = m2, and A is positive definite. Use
Theorem 4.17 to show that |A12|2 < |A11||A22|.
7.15 Let A be defined as in (7.1), where m1 = m2 and A11A21 = A21A11. Show that
|A| = |A11A22 −A21A12|.
7.16 Let Γ = (Γ1, Γ2) be an m × m orthogonal matrix with Γ1 and Γ2 being m × m1
and m × m2, respectively. Show that if A is an m × m nonsingular matrix, then
|Γ′
1AΓ1| = |A||Γ′
2A−1Γ2|.
7.17 Let A be an m × m nonsingular matrix and B be an m × m matrix having
rank 1. By considering a matrix of the form
A d
c′ 1

,
show that
|A + B| = {1 + tr(A−1B)}|A|.
7.18 Let A be an m × m symmetric matrix, and let Ak be its leading k × k principal
submatrix.
(a) Show that if |A1| > 0,. . ., |Am−1| > 0 and |Am| ≥0, then A is nonnega-
tive definite.
(b) Give an example of a 2 × 2 symmetric matrix that has both of its lead-
ing principal minors being nonnegative, yet the matrix is not nonnegative
definite.
7.19 Provide the details of the proof of part (b) of Theorem 7.8.
7.20 Let A be an m × n matrix and B be an n × m matrix. Show that
rank(Im −AB) = rank(In −BA) + m −n.

PROBLEMS
311
7.21 Show that the conditions given in Theorem 7.8 are not necessary conditions.
For instance, find a matrix for which the conditions given in Theorem 7.8(a) do
not hold, yet the corresponding determinantal identity does hold.
7.22 Let u and v be m × 1 vectors and A be an m × m matrix. Show that if b ̸= 0,
then
rank(A −b−1uv′) < rank(A)
if and only if vectors x and y exist, such that u = Ay, v = A′x, and b = x′Ay.
7.23 Let A be defined as in (7.1), and suppose that the conditions R(A21) ⊂R(A22)
and R(A′
12) ⊂R(A′
22) both hold. Show that A11 −A12A−
22A21 does not
depend on the choice of the generalized inverse A−
22. Give an example of
a matrix A such that only one of these conditions holds and the Schur
complement A11 −A12A−
22A21 does depend on the choice of A−
22.
7.24 Let A be defined as in (7.1), and suppose that the conditions R(A21) ⊂R(A22)
and R(A′
12) ⊂R(A′
22) hold. Show that if A is idempotent, then the generalized
Schur complement A11 −A12A−
22A21 is also idempotent.
7.25 Suppose A is partitioned as in (7.1) and define B = A22 −A21A−
11A12,
C = (Im1 −A11A−
11)A12, D = A21(Im1 −A−
11A11), and E = (Im2 −C−C)
{(Im2 −DD−)B(Im2 −C−C)}−(Im2 −DD−). Show that a generalized
inverse of A is given by
A−=
A−
11 −A−
11A12C−F −GD−A21A−
11 −GD−BC−F GD−
C−F
(0)

+
GD−B + A−
11A12
−Im2

E[BC−F + A21A−
11
−Im2],
where F = Im1 −A11A−
11 and G = Im1 −A−
11A11.
7.26 Provide the details of the proof of part (b) of Theorem 7.12.
7.27 Let A be defined as in (7.1), and let B1 = A11 −A12A−
22A21. Show that the
matrix

B−
1
−B−
1 A12A−
22
−A−
22A21B−
1
A−
22 + A−
22A21B−
1 A12A−
22

is a generalized inverse of A if and only if
(a) (Im2 −A22A−
22)A21(Im1 −B−
1 B1) = (0),
(b) (Im1 −B1B−
1 )A12(Im2 −A−
22A22) = (0),
(c) (Im2 −A22A−
22)A21B−
1 A12(Im2 −A−
22A22) = (0).
7.28 Let A be defined as in (7.1), and let B2 be the Schur complement
B2 = A22 −A21A+
11A12. Consider the matrix
C =
A+
11 + A+
11A12B+
2 A21A+
11 −A+
11A12B+
2
−B+
2 A21A+
11
B+
2

.
Show that C is the Moore–Penrose inverse of A if the conditions

312
PARTITIONED MATRICES
(a) R(A12) ⊂R(A11) and R(A′
21) ⊂R(A′
11),
(b) R(A21) ⊂R(B2) and R(A′
12) ⊂R(B′
2)
both hold.
7.29 Consider the nonnegative definite matrix
A =
⎡
⎣
2 1 2
1 1 0
2 0 4
⎤
⎦.
Partition A so that A11 is 2 × 2. Use Theorem 7.13 to obtain the Moore–Penrose
inverse of A.
7.30 Consider the matrices B and B∼given in Theorem 7.13. Show that B∼is a
generalized inverse of B.
7.31 Consider the m × m matrix
A =

A11 A12
A′
12 (0)

,
where A11 is an m1 × m1 nonnegative definite matrix and A12 is m1 × m2.
Show that
A+ =

B+
A+′
12 −B+A11A+′
12
A+
12 −A+
12A11B+ A+
12A11B+A11A+′
12 −A+
12A11A+′
12

,
where B = CA11C and C = Im1 −A12A+
12 = Im1 −A+′
12A′
12.
7.32 Let A be an m × m symmetric matrix partitioned as in (7.1). Show that if
λm1(A11) > λ1(A22) > 0, then
0 ≤
k

j=1
{λ2
j(A) −λ2
j(A11)}
≤2
	
1 +
λ1(A22)
λk(A11) −λ1(A22)

k

j=1
λj(A12A′
12)
for k = 1,. . ., m1.
7.33 Show that, under the conditions of Theorem 7.18,
0 ≤
k

j=1
{λ2
m1−j+1(B1) −λ2
m−j+1(A)}
≤2λ4
m1−k+1(B1)

1 +
λ−1
m2(B2)
λ−1
m1−k+1(B1) −λ−1
m2(B2)

k

j=1
λj(CC′),
for k = 1,. . ., m1.

PROBLEMS
313
7.34 Suppose that A in (7.1) is nonnegative definite with A22 being positive definite.
Consider the Schur complement B1 = A11 −A12A−1
22 A′
12. Show that
λh+m2(A) ≤λh(B1) ≤λh(A),
for h = 1,. . ., m1.
7.35 Suppose that A in (7.1) is nonnegative definite with A22 being positive
definite, whereas rank(B1) = r, where B1 = A11 −A12A−1
22 A′
12. Let Q be any
m1 × r matrix satisfying Q′Q = Ir and B1 = QΔQ′, where Δ is a diagonal
matrix with the positive eigenvalues of B1 as its diagonal elements. Define
B2 = A22 −A′
12Q(Q′A11Q)−1Q′A12 and ˆC = −Δ−1Q′A12A−1
22 . Show that,
if λ1(B1) < λm2(B2), then
0 ≤
m1−r+k

j=1
{λm1−j+1(B1) −λm−j+1(A)}
≤
λ2
r−k+1(B1)
{λ−1
r−k+1(B1) −λ−1
m2(B2)}
k

j=1
λj( ˆC ˆC′),
for k = 1,. . ., r.


8
SPECIAL MATRICES AND MATRIX
OPERATIONS
8.1
INTRODUCTION
In this chapter, we will introduce and develop properties of some special matrix oper-
ators. In particular, we will look at two matrix products that differ from the ordinary
product of two matrices. One of these products, known as the Hadamard product,
simply involves the element-wise multiplication of the matrices. The other matrix
product, called the Kronecker product, produces a matrix that in partitioned form
has each of its submatrices being equal to an element from the first matrix times the
second matrix. Closely related to the Kronecker product is the vec operator, which
transforms a matrix to a vector by stacking its columns one underneath another. In
many situations, a seemingly complicated matrix expression can be written in a fairly
simple form by applying one or more of these matrix operators. In addition to these
operators, we will look at some special types of structured matrices that we have not
previously discussed and are important in some statistical applications.
8.2
THE KRONECKER PRODUCT
Some matrices possess a special type of structure that permits them to be expressed
as a product, commonly referred to as the Kronecker product, of two other matrices.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

316
SPECIAL MATRICES AND MATRIX OPERATIONS
If A is an m × n matrix and B is a p × q matrix, then the Kronecker product of A
and B, denoted by A ⊗B, is the mp × nq matrix
⎡
⎢⎢⎢⎣
a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
am1B
am2B
· · ·
amnB
⎤
⎥⎥⎥⎦.
(8.1)
This Kronecker product is more precisely known as the right Kronecker product, and
it is the most common definition of the Kronecker product appearing in the literature.
However, some authors (for example, Graybill, 1983) define the Kronecker product
as the left Kronecker product, which has B ⊗A as the matrix given in (8.1). Through-
out this book, any reference to the Kronecker product refers to the right Kronecker
product. The special structure of the matrix given in (8.1) leads to simplified formulas
for the computation of such things as its inverse, determinant, and eigenvalues. In this
section, we will develop some of these formulas as well as some of the more basic
properties of the Kronecker product.
Unlike ordinary matrix multiplication, the Kronecker product A ⊗B is defined
regardless of the sizes of A and B. However, as with ordinary matrix multiplica-
tion, the Kronecker product is not, in general, commutative as is demonstrated in the
following example.
Example 8.1
Let A and B be the 1 × 3 and 2 × 2 matrices given by
A = [0 1 2],
B =

1 2
3 4
	
.
Then we find that
A ⊗B = [0B 1B 2B] =
0 0 1 2 2 4
0 0 3 4 6 8
	
,
whereas
B ⊗A =
1A 2A
3A 4A
	
=
0 1 2 0 2 4
0 3 6 0 4 8
	
.
Some of the basic properties of the Kronecker product, which are easily proven
from its definition, are summarized in Theorem 8.1. The proofs are left to the reader
as an exercise.
Theorem 8.1
Let A, B, and C be any matrices and a and b be any two vectors.
Then
(a) α ⊗A = A ⊗α = αA, for any scalar α,
(b) (αA) ⊗(βB) = αβ(A ⊗B), for any scalars α and β,

THE KRONECKER PRODUCT
317
(c) (A ⊗B) ⊗C = A ⊗(B ⊗C),
(d) (A + B) ⊗C = (A ⊗C) + (B ⊗C), if A and B are of the same size,
(e) A ⊗(B + C) = (A ⊗B) + (A ⊗C), if B and C are of the same size,
(f) (A ⊗B)′ = A′ ⊗B′,
(g) ab′ = a ⊗b′ = b′ ⊗a.
Although as we have already pointed out, A ⊗B = B ⊗A does not hold in gen-
eral, we see from Theorem 8.1(g) that this commutative property does hold when A
and B′ are vectors.
We have a useful property involving the Kronecker product and ordinary matrix
multiplication in Theorem 8.2.
Theorem 8.2
Let A, B, C, and D be matrices of sizes m × h, p × k, h × n, and
k × q, respectively. Then
(A ⊗B)(C ⊗D) = AC ⊗BD.
(8.2)
Proof.
The left-hand side of (8.2) is
⎡
⎢⎣
a11B
· · ·
a1hB
...
...
am1B
· · ·
amhB
⎤
⎥⎦
⎡
⎢⎣
c11D
· · ·
c1nD
...
...
ch1D
· · ·
chnD
⎤
⎥⎦=
⎡
⎢⎣
F11
· · ·
F1n
...
...
Fm1
· · ·
Fmn
⎤
⎥⎦,
where
Fij =
h

l=1
ailcljBD = (A)i·(C)·jBD = (AC)ijBD.
The right-hand side of (8.2) is
AC ⊗BD =
⎡
⎢⎣
(AC)11BD
· · ·
(AC)1nBD
...
...
(AC)m1BD
· · ·
(AC)mnBD
⎤
⎥⎦,
and so the result follows.
□
Our next result demonstrates that the trace of the Kronecker product A ⊗B can
be expressed in terms of the trace of A and the trace of B when A and B are square
matrices.
Theorem 8.3
Let A be an m × m matrix and B be a p × p matrix. Then
tr(A ⊗B) = tr(A)tr(B).

318
SPECIAL MATRICES AND MATRIX OPERATIONS
Proof.
Using (8.1) when n = m, we see that
tr(A ⊗B) =
m

i=1
aiitr(B) =
 m

i=1
aii

tr(B) = tr(A)tr(B),
so that the result holds.
□
Theorem 8.3 gives a simplified expression for the trace of a Kronecker product.
There is an analogous result for the determinant of a Kronecker product. However,
let us first consider the inverse of A ⊗B and the eigenvalues of A ⊗B when A and
B are square matrices.
Theorem 8.4
Let A be an m × n matrix and B be a p × q matrix. Then
(a) (A ⊗B)−1 = A−1 ⊗B−1, if m = n, p = q, and A ⊗B is nonsingular,
(b) (A ⊗B)+ = A+ ⊗B+,
(c) (A ⊗B)−= A−⊗B−, for any generalized inverses, A−and B−, of A and B.
Proof.
Using Theorem 8.2, we find that
(A−1 ⊗B−1)(A ⊗B) = (A−1A ⊗B−1B) = Im ⊗Ip = Imp,
so (a) holds. We will leave the verification of (b) and (c) as an exercise for the reader.
□
Theorem 8.5
Let λ1, . . . , λm be the eigenvalues of the m × m matrix A, and let
θ1, . . . , θp be the eigenvalues of the p × p matrix B. Then the set of mp eigenvalues
of A ⊗B is given by {λiθj : i = 1, . . . , m; j = 1, . . . , p}.
Proof.
It follows from Theorem 4.12 that nonsingular matrices P and Q exist, such
that
P −1AP = T1,
Q−1BQ = T2,
where T1 and T2 are upper triangular matrices with the eigenvalues of A and B,
respectively, as diagonal elements. The eigenvalues of A ⊗B are the same as
those of
(P ⊗Q)−1(A ⊗B)(P ⊗Q) = (P −1 ⊗Q−1)(A ⊗B)(P ⊗Q)
= P −1AP ⊗Q−1BQ = T1 ⊗T2,
which must be upper triangular because T1 and T2 are upper triangular. The result
now follows because the eigenvalues of T1 ⊗T2 are its diagonal elements, which are
clearly given by {λiθj : i = 1, . . . , m; j = 1, . . . , p}.
□

THE KRONECKER PRODUCT
319
A simplified expression for the determinant of A ⊗B, when A and B are square
matrices, is most easily obtained by using the fact that the determinant of a matrix is
given by the product of its eigenvalues.
Theorem 8.6
Let A be an m × m matrix and B be a p × p matrix. Then
|A ⊗B| = |A|p|B|m.
Proof.
Let λ1, . . . , λm be the eigenvalues of A, and let θ1, . . . , θp be the eigenval-
ues of B. Then we have
|A| =
m

i=1
λi,
|B| =
p

j=1
θj,
and from Theorem 8.5
|A ⊗B| =
p

j=1
m

i=1
λiθj =
p

j=1
θm
j
 m

i=1
λi

=
p

j=1
θm
j |A|
= |A|p
⎛
⎝
p

j=1
θj
⎞
⎠
m
= |A|p|B|m,
and so the proof is complete.
□
Our final result on Kronecker products identifies a relationship between rank(A ⊗
B) and rank(A) and rank(B).
Theorem 8.7
Let A be an m × n matrix and B be a p × q matrix. Then
rank(A ⊗B) = rank(A) rank(B).
Proof.
Our proof uses Theorem 3.12, which states that the rank of a symmetric
matrix equals the number of its nonzero eigenvalues. Although A ⊗B as given is
not necessarily symmetric, the matrix (A ⊗B)(A ⊗B)′, as well as AA′ and BB′, is
symmetric. Now from Theorem 2.8, we have
rank(A ⊗B) = rank{(A ⊗B)(A ⊗B)′} = rank(AA′ ⊗BB′).
Since AA′ ⊗BB′ is symmetric, its rank is given by the number of its nonzero eigen-
values. Now if λ1, . . . , λm are the eigenvalues of AA′, and θ1, . . . , θp are the eigen-
values of BB′ then, by Theorem 8.5, the eigenvalues of AA′ ⊗BB′ are given by
{λiθj : i = 1, . . . , m; j = 1, . . . , p}. Clearly, the number of nonzero values in this
set is the number of nonzero λi’s times the number of nonzero θj’s. However, because

320
SPECIAL MATRICES AND MATRIX OPERATIONS
AA′ and BB′ are symmetric, the number of nonzero λi’s is given by rank(AA′) =
rank(A), and the number of nonzero θj’s is given by rank(BB′) = rank(B). The
proof is now complete.
□
Example 8.2
The computations involved in an analysis of variance are sometimes
particularly well suited for the use of the Kronecker product. For example, consider
the univariate one-way classification model,
yij = μ + τi + ϵij,
which was discussed in Example 3.16, Example 6.11, and Example 6.12. Suppose that
we have the same number of observations available from each of the k treatments, so
that j = 1, . . . , n for each i. In this case, the model may be written as
y = Xβ + ϵ,
where X = (1k ⊗1n, Ik ⊗1n), β = (μ, τ1, . . . , τk)′, y = (y′
1, . . . , y′
k)′, and
yi = (yi1, . . . , yin)′. Consequently, a least squares solution for β is easily computed
as
ˆβ = (X′X)−X′y =

1′
k ⊗1′
n
Ik ⊗1′
n

[1k ⊗1n
Ik ⊗1n]
−
×

1′
k ⊗1′
n
Ik ⊗1′
n

y
=

nk
n1′
k
n1k
nIk
−
1′
k ⊗1′
n
Ik ⊗1′
n

y
=

(nk)−1
0′
0
n−1(Ik −k−11k1′
k)
 
1′
k ⊗1′
n
Ik ⊗1′
n

y
=

(nk)−1(1′
k ⊗1′
n)
n−1(Ik ⊗1′
n) −(nk)−1(1k1′
k ⊗1′
n)

y,
This yields ˆμ = y and ˆτi = yi −y, where
y = 1
nk
k

i=1
n

j=1
yij,
yi = 1
n
n

j=1
yij.
Note that this solution is not unique because X is not full rank, and hence the solution
depends on the choice of the generalized inverse of X′X. However, for each i, μ + τi
is estimable and its estimate is given by ˆμ + ˆτi = yi. In addition, the sum of squared
errors for the model is always unique and is given by
(y −X ˆβ)′(y −X ˆβ) = y′(Ink −X(X′X)−X′)y

THE KRONECKER PRODUCT
321
= y′(Ink −n−1(Ik ⊗1n1′
n))y
=
k

i=1
y′
i(In −n−11n1′
n)yi
=
k

i=1
n

j=1
(yij −yi)2.
Since {(1k ⊗1n)′(1k ⊗1n)}−1(1k ⊗1n)′y = y, the reduced model
yij = μ + ϵij
has the least squares estimate ˆμ = y, whereas its sum of squared errors is
{y −y(1k ⊗1n)}′{y −y(1k ⊗1n)} =
k

i=1
n

j=1
(yij −y)2.
The difference in the sums of squared errors for these two models, the so-called sum
of squares for treatments (SST), is then
SST =
k

i=1
n

j=1
(yij −y)2 −
k

i=1
n

j=1
(yij −yi)2
=
k

i=1
n(yi −y)2.
Example 8.3
In this example, we will illustrate some of the computations involved
in the analysis of the two-way classification model with interaction, which is of the
form
yijk = μ + τi + γj + ηij + ϵijk,
where i = 1, . . . , a, j = 1, . . . , b, and k = 1, . . . , n (see Problem 6.23). Here μ
can be described as an overall effect, whereas τi is an effect due to the ith level
of factor A, γj is an effect due to the jth level of factor B, and ηij is an effect
due to the interaction of the ith and jth levels of factors A and B. If we define
the parameter vector β = (μ, τ1, . . . , τa, γ1, . . . , γb, η11, η12, . . . , ηab−1, ηab)′ and
the response vector y = (y111, . . . , y11n, y121, . . . , y1bn, y211, . . . , yabn)′, then the
model above can be written as
y = Xβ + ϵ,
where
X = (1a ⊗1b ⊗1n, Ia ⊗1b ⊗1n, 1a ⊗Ib ⊗1n, Ia ⊗Ib ⊗1n).

322
SPECIAL MATRICES AND MATRIX OPERATIONS
Now it is easily verified that the matrix
X′X =
⎡
⎢⎢⎢⎣
abn
bn1′
a
an1′
b
n1′
a ⊗1′
b
bn1a
bnIa
n1a ⊗1′
b
nIa ⊗1′
b
an1b
n1′
a ⊗1b
anIb
n1′
a ⊗Ib
n1a ⊗1b
nIa ⊗1b
n1a ⊗Ib
nIa ⊗Ib
⎤
⎥⎥⎥⎦
has as a generalized inverse the matrix
diag((abn)−1, (bn)−1(Ia −a−11a1′
a), (an)−1(Ib −b−11b1′
b), C),
where
C = n−1Ia ⊗Ib −(bn)−1Ia ⊗1b1′
b −(an)−11a1′
a ⊗Ib
+ (abn)−11a1′
a ⊗1b1′
b.
Using this generalized inverse, we find that a least squares solution for β is given by
ˆβ = (X′X)−X′y =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
y··
y1· −y··
...
ya· −y··
y·1 −y··
...
y·b −y··
y11 −y1· −y·1 + y··
...
yab −ya· −y·b + y··
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where
y·· = (abn)−1
a

i=1
b

j=1
n

k=1
yijk,
yi· = (bn)−1
b

j=1
n

k=1
yijk,
y·j = (an)−1
a

i=1
n

k=1
yijk,
yij = n−1
n

k=1
yijk.
Clearly, μ + τi + γj + ηij is estimable, and its estimate, which is the fitted value for
yijk, is ˆμ + ˆτi + ˆγj + ˆηij = yij. We will leave the computation of some of the sums
of squares associated with the analysis of this model for the reader as an exercise.

THE VEC OPERATOR
323
8.3
THE DIRECT SUM
The direct sum is a matrix operator that transforms several square matrices into one
block diagonal matrix with these matrices appearing as the submatrices along the
diagonal. Recall that a block diagonal matrix is of the form
diag(A1, . . . , Ar) =
⎡
⎢⎢⎢⎢⎣
A1
(0)
· · ·
(0)
(0)
A2
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Ar
⎤
⎥⎥⎥⎥⎦
,
where Ai is an mi × mi matrix. This block diagonal matrix is said to be the direct
sum of the matrices A1, . . . , Ar and is sometimes written as
diag(A1, . . . , Ar) = A1 ⊕· · · ⊕Ar.
Clearly, the commutative property does not hold for the direct sum because, for
instance,
A1 ⊕A2 =

A1 (0)
(0) A2

̸=

A2 (0)
(0) A1

= A2 ⊕A1,
unless A1 = A2. Direct sums of a matrix with itself can be expressed as Kronecker
products; that is, if A1 = · · · = Ar = A, then
A1 ⊕· · · ⊕Ar = A ⊕· · · ⊕A = Ir ⊗A.
Some of the basic properties of the direct sum are summarized in the following
theorem. The proofs, which are fairly straightforward, are left to the reader.
Theorem 8.8
Let A1, . . . , Ar be matrices, where Ai is mi × mi. Then
(a) tr(A1 ⊕· · · ⊕Ar) = tr(A1) + · · · + tr(Ar),
(b) |A1 ⊕· · · ⊕Ar| = |A1| · · · |Ar|,
(c) if each Ai is nonsingular, A = A1 ⊕· · · ⊕Ar is also nonsingular and A−1 =
A−1
1
⊕· · · ⊕A−1
r ,
(d) rank(A1 ⊕· · · ⊕Ar) = rank(A1) + · · · + rank(Ar),
(e) if the eigenvalues of Ai are denoted by λi,1, . . . , λi,mi, the eigenvalues of
A1 ⊕· · · ⊕Ar are given by {λi,j : i = 1, . . . , r; j = 1, . . . , mi}.
8.4
THE VEC OPERATOR
There are situations in which it is useful to transform a matrix to a vector that has
as its elements the elements of the matrix. One such situation in statistics involves

324
SPECIAL MATRICES AND MATRIX OPERATIONS
the study of the distribution of the sample covariance matrix S. It is usually more
convenient mathematically in distribution theory to express density functions and
moments of jointly distributed random variables in terms of the vector with these
random variables as its components. Thus, the distribution of the random matrix S is
usually given in terms of the vector formed by stacking columns of S, one underneath
the other.
The operator that transforms a matrix to a vector is known as the vec operator. If
the m × n matrix A has ai as its ith column, then vec(A) is the mn × 1 vector given
by
vec(A) =
⎡
⎢⎢⎢⎣
a1
a2
...
an
⎤
⎥⎥⎥⎦.
Example 8.4
If A is the 2 × 3 matrix given by
A =

2
0
5
8
1
3
	
,
then vec(A) is the 6 × 1 vector given by
vec(A) =
⎡
⎢⎢⎢⎢⎢⎢⎣
2
8
0
1
5
3
⎤
⎥⎥⎥⎥⎥⎥⎦
.
In this section, we develop some of the basic algebra associated with this operator.
For instance, if a is m × 1 and b is n × 1, then ab′ is m × n and
vec(ab′) = vec([b1a, b2a, . . . , bna]) =
⎡
⎢⎢⎢⎣
b1a
b2a
...
bna
⎤
⎥⎥⎥⎦= b ⊗a.
Theorem 8.9 gives this result and some others that follow directly from the definition
of the vec operator.
Theorem 8.9
Let a and b be any two vectors, whereas A and B are two matrices
of the same size. Then
(a) vec(a) = vec(a′) = a,
(b) vec(ab′) = b ⊗a,
(c) vec(αA + βB) = αvec(A) + β vec(B), where α and β are scalars.

THE VEC OPERATOR
325
The trace of a product of two matrices can be expressed in terms of the vecs of
those two matrices. This result is given in Theorem 8.10.
Theorem 8.10
Let A and B both be m × n matrices. Then
tr(A′B) = {vec(A)}′vec(B).
Proof.
As usual, let a1, . . . , an denote the columns of A and b1, . . . , bn denote
the columns of B. Then
tr(A′B) =
n

i=1
(A′B)ii =
n

i=1
a′
ibi = [a′
1, . . . , a′
n]
⎡
⎢⎣
b1
...
bn
⎤
⎥⎦
= {vec(A)}′vec(B),
as is required.
□
A generalization of Theorem 8.9(b) to the situation involving the vec of the product
of three matrices is given in Theorem 8.11.
Theorem 8.11
Let A, B, and C be matrices of sizes m × n, n × p, and p × q,
respectively. Then
vec(ABC) = (C′ ⊗A)vec(B).
Proof.
Note that if b1, . . . , bp are the columns of B, then B can be written as
B =
p

i=1
bie′
i,
where ei is the ith column of Ip. Thus,
vec(ABC) = vec

A
 p

i=1
bie′
i

C

=
p

i=1
vec(Abie′
iC)
=
p

i=1
vec{(Abi)(C′ei)′} =
p

i=1
C′ei ⊗Abi
= (C′ ⊗A)
p

i=1
(ei ⊗bi),
where the second to last equality follows from Theorem 8.9(b). By again using
Theorem 8.9(b), we find that

326
SPECIAL MATRICES AND MATRIX OPERATIONS
p

i=1
(ei ⊗bi) =
p

i=1
vec(bie′
i) = vec
 p

i=1
bie′
i

= vec(B),
and so the result follows.
□
Example 8.5
The growth curve model discussed in Example 2.18 models an m × n
response matrix Y as Y = XBZ + E, where the m × p and q × n matrices X and
Z are known, the p × q matrix B contains unknown parameters, and E is an m ×
n matrix of errors. In this example, we will illustrate an alternative way of finding
the least squares estimator, ˆB, of B by utilizing the vec operator and properties of
projections. Letting ˆY = X ˆBZ denote the matrix of fitted values, ˆB is chosen so that
tr{(Y −ˆY )(Y −ˆY )′} = {vec(Y ) −vec( ˆY )}′{vec(Y ) −vec( ˆY )}
is minimized. But since vec( ˆY ) = vec(X ˆBZ) = (Z′ ⊗X)vec( ˆB), we see that
vec( ˆY ) must be in the column space of (Z′ ⊗X), and so the required minimum is
obtained when vec( ˆY ) is the projection of vec(Y ) onto that space. Since
PR(Z′⊗X) = (Z′ ⊗X){(Z′ ⊗X)′(Z′ ⊗X)}+(Z′ ⊗X)′
= Z′(ZZ′)+Z ⊗X(X′X)+X′,
this leads to
vec( ˆY ) = PR(Z′⊗X)vec(Y )
= vec(X(X′X)+X′Y Z′(ZZ′)+Z),
or equivalently
ˆY = X ˆBZ = X(X′X)+X′Y Z′(ZZ′)+Z.
When rank(X) = p and rank(Z) = q, ˆB is the unique solution obtained by pre-
multiplying this equation by (X′X)−1X′ and postmultiplying by Z′(ZZ′)−1; that
is,
ˆB = (X′X)−1X′Y Z′(ZZ′)−1.
Example 8.6
We return to the multivariate multiple regression model,
Y = XB + E,
considered in Example 3.15. Here Y and E are N × m, X is N × k, and B is k × m.
In Example 3.15, it was established that the least squares estimator of B is ˆB =
(X′X)−1X′Y by showing that the sum of squared error corresponding to an arbitrary
estimator B0 is equal to the sum of squared errors corresponding to the estimator ˆB
plus a term that is nonnegative. Using the vec operator and the Kronecker product, we

THE VEC OPERATOR
327
can establish the same result by using the projection method employed in Example
2.11 to find the least squares estimator of β in the standard multiple regression model.
The least squares estimator ˆB by definition minimizes
{vec(Y ) −vec( ˆY )}′{vec(Y ) −vec( ˆY )},
(8.3)
where ˆY = X ˆB, and vec( ˆY ) = vec(X ˆB) = (Im ⊗X)vec( ˆB), so vec( ˆY ) is a point
in the subspace of RmN spanned by the columns of Im ⊗X. Since (8.3) is minimized
by selecting as this point the orthogonal projection of vec(Y ) onto this subspace, we
have
vec(X ˆB) = (Im ⊗X){(Im ⊗X)′(Im ⊗X)}−1(Im ⊗X)′ vec(Y )
= {Im ⊗X(X′X)−1X′}vec(Y )
= vec{X(X′X)−1X′Y },
that is, X ˆB = X(X′X)−1X′Y . Premultiplying this last equation by (X′X)−1X′,
we arrive at the desired result.
Theorem 8.10 also can be generalized to a result involving the product of more
than two matrices.
Theorem 8.12
Let A, B, C, and D be matrices of sizes m × n, n × p, p × q, and
q × m, respectively. Then
tr(ABCD) = {vec(A′)}′(D′ ⊗B)vec(C).
Proof.
Using Theorem 8.10, it follows that
tr(ABCD) = tr{A(BCD)} = {vec(A′)}′ vec(BCD).
From Theorem 8.11, however, we know that vec(BCD) = (D′ ⊗B)vec(C), and so
the proof is complete.
□
The proofs of the following consequences of Theorem 8.12 are left to the reader
as an exercise.
Corollary 8.12.1
Let A and C be matrices of sizes m × n and n × m, respectively,
whereas B and D are n × n. Then
(a) tr(ABC) = {vec(A′)}′(Im ⊗B)vec(C),
(b) tr(AD′BDC) = {vec(D)}′(A′C′ ⊗B)vec(D).

328
SPECIAL MATRICES AND MATRIX OPERATIONS
Other transformations of a matrix, A, to a vector may be useful when the matrix A
has some special structure. One such transformation for an m × m matrix, denoted by
v(A), is defined so as to produce the m(m + 1)/2 × 1 vector obtained from vec(A)
by deleting from it all of the elements that are above the diagonal of A. Thus, if A is
a lower triangular matrix, v(A) contains all of the elements of A except for the zeros
in the upper triangular portion of A. Yet another transformation of the m × m matrix
A to a vector is denoted by ˜v(A) and yields the m(m −1)/2 × 1 vector formed from
v(A) by deleting from it all of the diagonal elements of A; that is, ˜v(A) is the vector
obtained by stacking only the portion of the columns of A that are below its diagonal.
If A is a skew-symmetric matrix, then A can be reconstructed from ˜v(A) because the
diagonal elements of A must be zero, whereas aji = −aij if i ̸= j. The notation v(A)
and ˜v(A) we use here corresponds to the notation of Magnus (1988). Others (see, for
example, Henderson and Searle, 1979) use the notation vech(A) and veck(A). In
Section 7, we will discuss some transformations that relate the v and ˜v operators to
the vec operator.
Example 8.7
The v and ˜v operators are particularly useful when dealing with
covariance and correlation matrices. For instance, suppose that we are interested in
the distribution of the sample covariance matrix or the distribution of the sample
correlation matrix computed from a sample of observations on three different
variables. The resulting sample covariance and correlation matrices would be of the
form
S =
⎡
⎣
s11
s12
s13
s12
s22
s23
s13
s23
s33
⎤
⎦,
R =
⎡
⎣
1
r12
r13
r12
1
r23
r13
r23
1
⎤
⎦,
so that
vec(S) = (s11, s12, s13, s12, s22, s23, s13, s23, s33)′,
vec(R) = (1, r12, r13, r12, 1, r23, r13, r23, 1)′.
Since both S and R are symmetric, there are redundant elements in vec(S) and
vec(R). The elimination of these elements result in v(S) and v(R) given by
v(S) = (s11, s12, s13, s22, s23, s33)′,
v(R) = (1, r12, r13, 1, r23, 1)′.
Finally, by eliminating the nonrandom 1’s from v(R), we obtain
˜v(R) = (r12, r13, r23)′,
which contains all of the random variables in R.

THE HADAMARD PRODUCT
329
8.5
THE HADAMARD PRODUCT
A matrix operator that is a little more obscure than our other matrix operators, but
one that is finding increasing applications in statistics, is known as the Hadamard
product. This operator, which we will denote by the symbol ⊙, simply performs the
element-wise multiplication of two matrices; that is, if A and B are each m × n, then
A ⊙B =
⎡
⎢⎣
a11b11
· · ·
a1nb1n
...
...
am1bm1
· · ·
amnbmn
⎤
⎥⎦.
Clearly, this operation is only defined if the two matrices involved are of the same size.
Example 8.8
If A and B are the 2 × 3 matrices given by
A =
1
4
2
0
2
3
	
,
B =
3
1
3
6
5
1
	
,
then
A ⊙B =

3
4
6
0
10
3
	
.
One of the situations in which the Hadamard product finds application in statistics
is in expressions for the covariance structure of certain functions of the sample covari-
ance and sample correlation matrices. We will see examples of this later in Section
11.7. In this section, we will investigate some of the properties of this operator. For
a more complete treatment, along with some other examples of applications of the
operator in statistics, see Styan (1973) and Horn and Johnson (1991). We begin with
some elementary properties that follow directly from the definition of the Hadamard
product.
Theorem 8.13
Let A, B, and C be m × n matrices. Then
(a) A ⊙B = B ⊙A,
(b) (A ⊙B) ⊙C = A ⊙(B ⊙C),
(c) (A + B) ⊙C = A ⊙C + B ⊙C,
(d) (A ⊙B)′ = A′ ⊙B′,
(e) A ⊙(0) = (0),
(f) A ⊙1m1′
n = A,
(g) A ⊙Im = DA = diag(a11, . . . , amm) if m = n,
(h) D(A ⊙B) = (DA) ⊙B = A ⊙(DB) and (A ⊙B)E = (AE) ⊙B = A ⊙
(BE), if D is an m × m diagonal matrix and E is an n × n diagonal matrix,
(i) ab′ ⊙cd′ = (a ⊙c)(b ⊙d)′, where a and c are m × 1 vectors and b and d
are n × 1 vectors.

330
SPECIAL MATRICES AND MATRIX OPERATIONS
We will now show how A ⊙B is related to the Kronecker product A ⊗B; specif-
ically, A ⊙B is a submatrix of A ⊗B. To see this, define the m × m2 matrix Ψm
as
Ψm =
m

i=1
ei,m(ei,m ⊗ei,m)′,
where ei,m is the ith column of the identity matrix Im. Note that if A and B
are m × n, then Ψm(A ⊗B)Ψ′
n forms the m × n submatrix of the m2 × n2
matrix A ⊗B, which contains rows 1, m + 2, 2m + 3, . . . , m2 and columns
1, n + 2, 2n + 3, . . . , n2. Taking a closer look at this submatrix, we find that
Ψm(A ⊗B)Ψ′
n =
m

i=1
n

j=1
ei,m(ei,m ⊗ei,m)′(A ⊗B)(ej,n ⊗ej,n)e′
j,n
=
m

i=1
n

j=1
ei,m(e′
i,mAej,n ⊗e′
i,mBej,n)e′
j,n
=
m

i=1
n

j=1
aijbijei,me′
j,n = A ⊙B.
Although the rank of A ⊙B is not determined, in general, by the rank of A and
the rank of B, we do have the following bound.
Theorem 8.14
Let A and B be m × n matrices. Then
rank(A ⊙B) ≤rank(A) rank(B).
Proof.
Using the identity Ψm(A ⊗B)Ψ′
n = A ⊙B, we get
rank(A ⊙B) = rank(Ψm(A ⊗B)Ψ′
n) ≤rank(A ⊗B)
= rank(A)rank(B),
where we have used Theorem 2.8(a) and Theorem 8.7. This completes the proof. □
Example 8.9
Although Theorem 8.14 gives an upper bound for rank(A ⊙B) in
terms of rank(A) and rank(B), there is no corresponding lower bound. In other
words, it is possible that both A and B have full rank, whereas A ⊙B has rank equal
to 0. For instance, each of the matrices
A =
⎡
⎣
0
1
0
1
0
0
0
0
1
⎤
⎦,
B =
⎡
⎣
1
0
0
0
0
1
0
1
0
⎤
⎦
clearly has rank 3, and yet A ⊙B has rank 0 because A ⊙B = (0).

THE HADAMARD PRODUCT
331
Theorem 8.15 shows that a bilinear form in a Hadamard product of two matrices
may be written as a trace.
Theorem 8.15
Let A and B be m × n matrices, and let x and y be m × 1 and
n × 1 vectors, respectively. Then
(a) 1′
m(A ⊙B)1n = tr(AB′),
(b) x′(A ⊙B)y = tr(DxADyB′),
where Dx = diag(x1, . . . , xm) and Dy = diag(y1, . . . , yn).
Proof.
(a) follows because
1′
m(A ⊙B)1n =
m

i=1
n

j=1
(A ⊙B)ij =
m

i=1
n

j=1
aijbij
=
m

i=1
(A)i·(B′)·i =
m

i=1
(AB′)ii = tr(AB′).
Note also that x = Dx1m and y = Dy1n, so that by using (a) above and Theorem
8.13(h), we find that
x′(A ⊙B)y = 1′
mDx(A ⊙B)Dy1n = 1′
m(DxA ⊙BDy)1n
= tr(DxADyB′),
and this proves (b).
□
Example 8.10
A correlation matrix is a nonnegative definite matrix with each diag-
onal element equal to 1. In Example 3.10, we saw how this constraint on the diagonal
elements of a correlation matrix affects the possible choices for the eigenvalues and
eigenvectors of the correlation matrix. Let P be an m × m correlation matrix, and let
P = QDQ′ be its spectral decomposition; that is, Q is an m × m orthogonal matrix
with orthonormal eigenvectors of P as its columns, and D is a diagonal matrix with
the nonnegative eigenvalues of P, d1, . . . , dm, as its diagonal elements. Suppose we
have a particular orthogonal matrix Q, and we wish to determine the possible choices
for the diagonal elements of D so that QDQ′ has correlation-matrix structure. The
constraint on the diagonal elements of P can be expressed as
pii = (QDQ′)ii = (Q)i·D(Q′)·i =
m

j=1
djq2
ij = 1,
for i = 1, . . . , m. Note that these m equations can be written as the one matrix
equation
(Q ⊙Q)d = 1m,

332
SPECIAL MATRICES AND MATRIX OPERATIONS
where d = (d1, . . . , dm)′. Using the results from Chapter 6, we solve this matrix
equation to get
d = 1m + Ab,
(8.4)
where b is an arbitrary r × 1 vector, r is the dimension of the null space of Q ⊙Q,
and A is any m × r matrix whose columns form a basis for the null space of Q ⊙
Q. Any d obtained from (8.4) will produce a correlation matrix when used in P =
QDQ′ as long as each di ≥0, and any correlation matrix that has the columns of Q
as orthonormal eigenvectors will have its vector of eigenvalues as a solution to (8.4).
Note that 1m is a solution to (8.4) regardless of the choice of Q. This is not surprising
because d = 1m leads to P = Im and Im has the spectral decomposition Im = QQ′,
where Q can be any m × m orthogonal matrix. Also, d = 1m is the unique solution
to (Q ⊙Q)d = 1m only if Q ⊙Q is nonsingular. In other words, if the correlation
matrix P = QDQ′ ̸= Im, then Q ⊙Q must be singular.
The following result can be helpful in determining whether the Hadamard product
of two symmetric matrices is nonnegative definite or positive definite.
Theorem 8.16
Let A and B each be an m × m symmetric matrix. Then
(a) A ⊙B is nonnegative definite if A and B are nonnegative definite,
(b) A ⊙B is positive definite if A and B are positive definite.
Proof.
Clearly, if A and B are symmetric, then so is A ⊙B. Let B = XΛX′ be
the spectral decomposition of B so that bij =  λkxikxjk, where λk ≥0 for all k
because B is nonnegative definite. Then we find that for any m × 1 vector y,
y′(A ⊙B)y =
m

i=1
m

j=1
aijbijyiyj =
m

k=1
⎛
⎝
m

i=1
m

j=1
λk(yixik)aij(yjxjk)
⎞
⎠
=
m

k=1
λk(y ⊙xk)′A(y ⊙xk),
(8.5)
where xk represents the kth column of X. Since A is nonnegative definite, the sum in
(8.5) must be nonnegative, and so A ⊙B is also nonnegative definite. This proves (a).
Now if A is positive definite, then (8.5) will be positive for any y ̸= 0 that satisfies
y ⊙xk ̸= 0 for at least one k for which λk > 0. However, if B is also positive defi-
nite, then λk > 0 for all k, and if y has its hth component yh ̸= 0, then y ⊙xk = 0
for all k only if the hth row of X has all zeros, which is not possible because X is
nonsingular. Consequently, there is no y ̸= 0 for which (8.5) equals zero, and so (b)
follows.
□
Theorem 8.16(b) gives a sufficient condition for the matrix A ⊙B to be positive
definite. Example 8.11 demonstrates that this condition is not necessary.

THE HADAMARD PRODUCT
333
Example 8.11
Consider the 2 × 2 matrices
A =
1 1
1 1
	
,
B =
4 2
2 2
	
.
The matrix B is positive definite because, for instance, B = V V ′, where
V =

2 0
1 1
	
and rank(V ) = 2. Clearly, A ⊙B is also positive definite because A ⊙B = B.
However, A is not positive definite because rank(A) = 1.
A sufficient condition for the positive definiteness of A ⊙B, weaker than that
given in Theorem 8.16(b), is given in Theorem 8.17.
Theorem 8.17
Let A and B each be an m × m symmetric matrix. If B is positive
definite and A is nonnegative definite with positive diagonal elements, then A ⊙B
is positive definite.
Proof.
We need to show that for any x ̸= 0, x′(A ⊙B)x > 0. Since B is posi-
tive definite, a nonsingular matrix T exists, such that B = TT ′. It follows then from
Theorem 8.15(b) that
x′(A ⊙B)x = tr(DxADxB′) = tr(DxADxTT ′)
= tr(T ′DxADxT).
(8.6)
Since A is nonnegative definite, so is DxADx. In addition, if x ̸= 0 and A has no
diagonal elements equal to zero, then DxADx ̸= (0); that is, DxADx has rank of
at least one, and so it has at least one positive eigenvalue. Since T is nonsingular,
rank(DxADx) = rank(T ′DxADxT), and so T ′DxADxT is also nonnegative defi-
nite with at least one positive eigenvalue. The result now follows because (8.6) implies
that x′(A ⊙B)x is the sum of the eigenvalues of T ′DxADxT.
□
The following result, which gives a relationship between the determinant of a pos-
itive definite matrix and its diagonal elements, is commonly known as the Hadamard
inequality.
Theorem 8.18
If A is an m × m positive definite matrix, then
|A| ≤
m

i=1
aii,
with equality if and only if A is a diagonal matrix.

334
SPECIAL MATRICES AND MATRIX OPERATIONS
Proof.
Our proof is by induction. If m = 2, then
|A| = a11a22 −a2
12 ≤a11a22,
with equality if and only if a12 = 0, and so the result clearly holds when m = 2. For
general m, use the cofactor expansion formula for the determinant of A to obtain
|A| = a11

a22
a23
· · ·
a2m
a32
a33
· · ·
a3m
...
...
...
am2
am3
· · ·
amm

+

0
a12
· · ·
a1m
a21
a22
· · ·
a2m
...
...
...
am1
am2
· · ·
amm

= a11|A1| +

0
a′
a A1
 ,
(8.7)
where A1 is the (m −1) × (m −1) submatrix of A formed by deleting the first row
and column of A and a′ = (a12, . . . , a1m). Since A is positive definite, A1 also must
be positive definite. Consequently, we can use Theorem 7.4(a) to simplify the second
term in the right-hand side of (8.7), leading to the equation
|A| = a11|A1| −a′A−1
1 a|A1|.
Since A1 and A−1
1
are positive definite, it follows that
|A| ≤a11|A1|,
with equality if and only if a = 0. Thus, the result holds for the m × m matrix A if
the result holds for the (m −1) × (m −1) matrix A1, and so our induction proof is
complete.
□
Corollary 8.18.1
Let B be an m × m nonsingular matrix. Then
|B|2 ≤
m

i=1
⎛
⎝
m

j=1
b2
ij
⎞
⎠,
with equality if and only if the rows of B are orthogonal.
Proof.
Since B is nonsingular, the matrix A = BB′ is positive definite. Note that
|A| = |BB′| = |B||B′| = |B|2
and
aii = (BB′)ii = (B)i·(B′)·i = (B)i·(B)′
i· =
m

j=1
b2
ij,
and so the result follows immediately from Theorem 8.18.
□

THE HADAMARD PRODUCT
335
Theorem 8.18 also holds for positive semidefinite matrices except that, in this case,
A need not be diagonal for equality because one or more of its diagonal elements
may equal zero. Likewise, Corollary 8.18.1 holds for singular matrices except for the
statement concerning equality.
Hadamard’s inequality, as given in Theorem 8.18, can be expressed, with the
Hadamard product, as
|A|
 m

i=1
1

≤|A ⊙Im|,
(8.8)
where the term ( 1) corresponds to the product of the diagonal elements of Im.
Theorem 8.20 will show that the inequality (8.8) holds for other matrices besides the
identity. However, first we will need Theorem 8.19.
Theorem 8.19
Let A be an m × m positive definite matrix, and define
Aα = A −αe1e′
1,
where α = |A|/|A1| and A1 is the (m −1) × (m −1) submatrix of A formed by
deleting its first row and column. Then Aα is nonnegative definite.
Proof.
Let A be partitioned as
A =

a11 a′
a
A1

,
and note that because A is positive definite, so is A1. Thus, using Theorem 7.4, we
find that
|A| =

a11 a′
a
A1
 = |A1|(a11 −a′A−1
1 a),
and so α = |A|/|A1| = (a11 −a′A−1
1 a). Consequently, Aα may be written as
Aα =

a11 a′
a
A1

−

a11 −a′A−1
1 a 0′
0
(0)

=

a′A−1
1 a a′
a
A1

=

a′A−1
1
Im−1

A1[A−1
1 a Im−1].
Since A1 is positive definite, an (m −1) × (m −1) matrix T exists, such that A1 =
TT ′. If we let U ′ = T ′[A−1
1 a
Im−1], then Aα = UU ′, and so Aα is nonnegative
definite.
□

336
SPECIAL MATRICES AND MATRIX OPERATIONS
Theorem 8.20
Let A and B be m × m nonnegative definite matrices. Then
|A|
m

i=1
bii ≤|A ⊙B|.
Proof.
The result follows immediately if A is singular because |A| = 0, whereas
|A ⊙B| ≥0 is guaranteed by Theorem 8.16. For the case in which A is positive
definite, we will prove the result by induction. The result holds when m = 2, because
in this case
|A ⊙B| =

a11b11
a12b12
a12b12
a22b22
 = a11a22b11b22 −(a12b12)2
= (a11a22 −a2
12)b11b22 + a2
12(b11b22 −b2
12)
= |A|b11b22 + a2
12|B| ≥|A|b11b22.
To prove the result for general m, assume that it holds for m −1, so that
|A1|
m

i=2
bii ≤|A1 ⊙B1|,
(8.9)
where A1 and B1 are the submatrices of A and B formed by deleting their first row
and first column. From Theorem 8.19, we know that (A −αe1e′
1) is nonnegative
definite, where α = |A|/|A1|. Thus, by using Theorem 8.16(a), Theorem 8.13(c),
and the expansion formula for determinants, we find that
0 ≤|(A −αe1e′
1) ⊙B| = |A ⊙B −αe1e′
1 ⊙B|
= |A ⊙B −αb11e1e′
1|
= |A ⊙B| −αb11|(A ⊙B)1|,
where (A ⊙B)1 denotes the (m −1) × (m −1) submatrix of A ⊙B formed by
deleting its first row and column. However, (A ⊙B)1 = A1 ⊙B1 so that the inequal-
ity above, along with (8.9) and the identity α|A1| = |A|, implies that
|A ⊙B| ≥αb11|A1 ⊙B1| ≥αb11

|A1|
m

i=2
bii

= |A|
m

i=1
bii.
The proof is now complete.
□
Our final results on Hadamard products involve their eigenvalues. First we
obtain bounds for each eigenvalue of the matrix A ⊙B when A and B are
symmetric.

THE HADAMARD PRODUCT
337
Theorem 8.21
Let A and B be m × m symmetric matrices. If A and B are non-
negative definite, then the ith largest eigenvalue of A ⊙B satisfies
λm(A)

min
1≤i≤m bii

≤λi(A ⊙B) ≤λ1(A)

max
1≤i≤m bii

.
Proof.
Since B is nonnegative definite, an m × m matrix T exists, such that B =
TT ′. Let tj be the jth column of T, whereas tij denotes the (i, j)th element of T.
For any m × 1 vector, x ̸= 0, we find that
x′(A ⊙B)x =
m

i=1
m

j=1
aijbijxixj =
m

i=1
m

j=1
aij
 m

h=1
tihtjh

xixj
=
m

h=1
⎛
⎝
m

i=1
m

j=1
(xitih)aij(xjtjh)
⎞
⎠=
m

h=1
(x ⊙th)′A(x ⊙th)
≤λ1(A)
m

h=1
(x ⊙th)′(x ⊙th) = λ1(A)
m

h=1
m

j=1
x2
jt2
jh
= λ1(A)
m

j=1
x2
j
 m

h=1
t2
jh

= λ1(A)
m

j=1
x2
jbjj
≤λ1(A){ max
1≤i≤m bii}x′x,
(8.10)
where the first inequality arises from the relation
λ1(A) = max
y̸=0
y′Ay
y′y
given in Theorem 3.16, and the last inequality follows because λ1(A) is nonnegative.
Using this same result in Theorem 3.16 applied to A ⊙B, along with (8.10), we find
that for any i, 1 ≤i ≤m,
λi(A ⊙B) ≤λ1(A ⊙B) = max
x̸=0
x′(A ⊙B)x
x′x
≤λ1(A)

max
1≤i≤m bii

,
which is the required upper bound on λi(A ⊙B). By using the identity
λm(A) = min
y̸=0
y′Ay
y′y ,
the lower bound can be established in a similar fashion.
□
The bounds given in Theorem 8.21 can be improved; narrower bounds were
obtained by Im (1997). Our final result provides an alternative lower bound for the

338
SPECIAL MATRICES AND MATRIX OPERATIONS
smallest eigenvalue of (A ⊙B). The derivation of this bound will make use of the
following result.
Theorem 8.22
Let A be an m × m positive definite matrix. Then the matrix
(A ⊙A−1) −Im is nonnegative definite.
Proof.
Let
m

i=1
λixix′
i
be the spectral decomposition of A so that
A−1 =
m

i=1
λ−1
i xix′
i.
Then
(A ⊙A−1) −Im = (A ⊙A−1) −Im ⊙Im
=
⎛
⎝
m

i=1
λixix′
i ⊙
m

j=1
λ−1
j xjx′
j
⎞
⎠
−
⎛
⎝
m

i=1
xix′
i ⊙
m

j=1
xjx′
j
⎞
⎠
=
m

i=1
m

j=1
(λiλ−1
j
−1)(xix′
i ⊙xjx′
j)
=

i̸=j
(λiλ−1
j
−1)(xix′
i ⊙xjx′
j)
=

i<j
(λiλ−1
j
+ λjλ−1
i
−2)(xi ⊙xj)(xi ⊙xj)′
= XDX′,
where X is the m × m(m −1)/2 matrix having (xi ⊙xj), i < j, as its columns,
whereas D is the diagonal matrix with its corresponding diagonal elements given by
(λiλ−1
j
+ λjλ−1
i
−2), i < j. Since A is positive definite, λi > 0 for all i, and so
(λiλ−1
j
+ λjλ−1
i
−2) = λ−1
i λ−1
j (λi −λj)2 ≥0.
Thus, D is nonnegative definite, and consequently so is XDX′.
□
Theorem 8.23
Let A and B be m × m nonnegative definite matrices. Then
λm(A ⊙B) ≥λm(AB).

THE COMMUTATION MATRIX
339
Proof.
As a result of Theorem 8.16, A ⊙B is nonnegative definite, and so the
inequality is obvious if either A or B is singular because, in this case, AB will have a
zero eigenvalue. Suppose that A and B are positive definite, and let T be any matrix
such that TT ′ = B. Note that T ′AT −λm(AB)Im is nonnegative definite because
its ith largest eigenvalue is λi(T ′AT) −λm(AB), and λm(AB) = λm(T ′AT). As
a result,
T −1′(T ′AT −λm(AB)Im)T −1 = A −λm(AB)B−1
is also nonnegative definite. Thus, (A −λm(AB)B−1) ⊙B is nonnegative definite
because of Theorem 8.16, whereas λm(AB){(B−1 ⊙B) −Im} is nonnegative def-
inite because of Theorem 8.22, and so the sum of these two matrices, which is given
by
{(A −λm(AB)B−1) ⊙B} + λm(AB){(B−1 ⊙B) −Im}
= A ⊙B −λm(AB)(B−1 ⊙B) + λm(AB)(B−1 ⊙B) −λm(AB)Im
= A ⊙B −λm(AB)Im,
is also nonnegative definite. Consequently, for any x,
x′(A ⊙B)x ≥λm(AB)x′x,
and so the result follows from Theorem 3.16.
□
8.6
THE COMMUTATION MATRIX
An m × m permutation matrix was defined in Section 1.10 to be any matrix that can
be obtained from Im by permuting its columns. In this section, we discuss a special
class of permutation matrices, known as commutation matrices, which are very useful
when computing the moments of the multivariate normal and related distributions. We
will establish some of the basic properties of commutation matrices. A more complete
treatment of this subject can be found in Magnus and Neudecker (1979) and Magnus
(1988).
Definition 8.1
Let Hij be the m × n matrix that has its only nonzero element, a
one, in the (i, j)th position. Then the mn × mn commutation matrix, denoted by
Kmn, is given by
Kmn =
m

i=1
n

j=1
(Hij ⊗H′
ij).
(8.11)
The matrix Hij can be conveniently expressed in terms of columns from the iden-
tity matrices Im and In. If ei,m is the ith column of Im and ej,n is the jth column of
In, then Hij = ei,me′
j,n.

340
SPECIAL MATRICES AND MATRIX OPERATIONS
Note that, in general, there is more than one commutation matrix of order mn. For
example, for mn = 6, we have the four commutation matrices, K16, K23, K32, and
K61. Using (8.11), it is easy to verify that K16 = K61 = I6, whereas
K23 =
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎦
,
K32 =
⎡
⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
The fact that K32 = K′
23 is not a coincidence, because this is a general property that
follows from the definition of Kmn.
Theorem 8.24
The commutation matrix satisfies the properties
(a) Km1 = K1m = Im,
(b) K′
mn = Knm,
(c) K−1
mn = Knm.
Proof.
When Hij is m × 1, then Hij = ei,m, and so
Km1 =
m

i=1
(ei,m ⊗e′
i,m) = Im =
m

i=1
(e′
i,m ⊗ei,m) = K1m,
which proves (a). To prove (b), note that
K′
mn =
m

i=1
n

j=1
(Hij ⊗H′
ij)′ =
m

i=1
n

j=1
(H′
ij ⊗Hij) = Knm.
Finally, (c) follows because
HijH′
kl = ei,me′
j,nel,ne′
k,m =
ei,me′
k,m, if j = l,
(0),
if j ̸= l,
H′
ijHkl = ej,ne′
i,mek,me′
l,n =
ej,ne′
l,n, if i = k,
(0),
if i ̸= k,

THE COMMUTATION MATRIX
341
and so
KmnKnm = KmnK′
mn =
⎧
⎨
⎩
m

i=1
n

j=1
(Hij ⊗H′
ij)
⎫
⎬
⎭
 m

k=1
n

l=1
(Hkl ⊗H′
kl)′

=
m

i=1
n

j=1
m

k=1
n

l=1
(HijH′
kl ⊗H′
ijHkl)
=
m

i=1
n

j=1
(ei,me′
i,m ⊗ej,ne′
j,n)
=
 m

i=1
ei,me′
i,m

⊗
⎛
⎝
n

j=1
ej,ne′
j,n
⎞
⎠
= Im ⊗In = Imn.
□
Commutation matrices have important relationships with the vec operator and the
Kronecker product. For an m × n matrix A, vec(A) and vec(A′) are related because
they contain the same elements arranged in a different order; that is, an appropriate
reordering of the elements of vec(A) will produce vec(A′). The commutation matrix
Kmn is the matrix multiplier that transforms vec(A) to vec(A′).
Theorem 8.25
For any m × n matrix A,
Kmnvec(A) = vec(A′).
Proof.
Clearly, because aijH′
ij is the n × m matrix whose only nonzero element,
aij, is in the (j, i)th position, we have
A′ =
m

i=1
n

j=1
aijH′
ij =
m

i=1
n

j=1
(e′
i,mAej,n)ej,ne′
i,m
=
m

i=1
n

j=1
ej,n(e′
i,mAej,n)e′
i,m =
m

i=1
n

j=1
(ej,ne′
i,m)A(ej,ne′
i,m)
=
m

i=1
n

j=1
H′
ijAH′
ij.
Taking the vec of both sides and using Theorem 8.11, we get
vec(A′) = vec
⎛
⎝
m

i=1
n

j=1
H′
ijAH′
ij
⎞
⎠=
m

i=1
n

j=1
vec(H′
ijAH′
ij)

342
SPECIAL MATRICES AND MATRIX OPERATIONS
=
m

i=1
n

j=1
(Hij ⊗H′
ij)vec(A) = Kmnvec(A),
and so the result follows.
□
The term commutation arises from the fact that commutation matrices provide the
factors that allow a Kronecker product to commute. This property is summarized in
Theorem 8.26.
Theorem 8.26
Let A be an m × n matrix, B be a p × q matrix, x be an m × 1
vector, and y be a p × 1 vector. Then
(a) Kpm(A ⊗B) = (B ⊗A)Kqn,
(b) Kpm(A ⊗B)Knq = B ⊗A,
(c) Kpm(A ⊗y) = y ⊗A,
(d) Kmp(y ⊗A) = A ⊗y,
(e) Kpm(x ⊗y) = y ⊗x,
(f) tr{(B ⊗A)Kmn} = tr(BA), if p = n and q = m.
Proof.
If X is a q × n matrix, then by using Theorem 8.11 and Theorem 8.25, we
find that
Kpm(A ⊗B)vec(X) = Kpmvec(BXA′) = vec{(BXA′)′}
= vec(AX′B′) = (B ⊗A)vec(X′)
= (B ⊗A)Kqnvec(X).
Thus, if X is chosen so that vec(X) equals the ith column of Iqn, we observe that
the ith column of Kpm(A ⊗B) must be the same as the ith column of (B ⊗A)Kqn,
so (a) follows. Postmultiplying (a) by Knq and then applying Theorem 8.24(c) yields
(b). Properties (c)–(e) follow from (a) and Theorem 8.24(a) because
Kpm(A ⊗y) = (y ⊗A)K1n = y ⊗A,
Kmp(y ⊗A) = (A ⊗y)Kn1 = A ⊗y,
Kpm(x ⊗y) = (y ⊗x)K11 = y ⊗x.
Finally, using the definition of the commutation matrix, we get
tr{(B ⊗A)Kmn} =
m

i=1
n

j=1
tr{(B ⊗A)(Hij ⊗H′
ij)}
=
m

i=1
n

j=1
{tr(BHij)}{tr(AH′
ij)}

THE COMMUTATION MATRIX
343
=
m

i=1
n

j=1
(e′
j,nBei,m)(e′
i,mAej,n) =
m

i=1
n

j=1
bjiaij
=
n

j=1
(B)j·(A)·j =
n

j=1
(BA)jj = tr(BA),
which proves (f).
□
The commutation matrix also can be used to obtain a relationship between the vec
of a Kronecker product and the Kronecker product of the corresponding vecs.
Theorem 8.27
Let A be an m × n matrix and B be a p × q matrix. Then
vec(A ⊗B) = (In ⊗Kqm ⊗Ip){vec(A) ⊗vec(B)}.
Proof.
Our proof follows that given by Magnus (1988). Let a1, . . . , an be the
columns of A and b1, . . . , bq be the columns of B. Then, because A and B can
be written as
A =
n

i=1
aie′
i,n,
B =
q

j=1
bje′
j,q,
we have
vec(A ⊗B)
=
n

i=1
q

j=1
vec(aie′
i,n ⊗bje′
j,q)
=
n

i=1
q

j=1
vec{(ai ⊗bj)(e′
i,n ⊗e′
j,q)}
=
n

i=1
q

j=1
{(ei,n ⊗ej,q) ⊗(ai ⊗bj)}
=
n

i=1
q

j=1
{ei,n ⊗Kqm(ai ⊗ej,q) ⊗bj}
=
n

i=1
q

j=1
(In ⊗Kqm ⊗Ip)(ei,n ⊗ai ⊗ej,q ⊗bj)
= (In ⊗Kqm ⊗Ip)
⎧
⎨
⎩
n

i=1
(ei,n ⊗ai) ⊗
q

j=1
(ej,q ⊗bj)
⎫
⎬
⎭

344
SPECIAL MATRICES AND MATRIX OPERATIONS
= (In ⊗Kqm ⊗Ip)
⎧
⎨
⎩
n

i=1
vec(aie′
i,n) ⊗
q

j=1
vec(bje′
j,q)
⎫
⎬
⎭
= (In ⊗Kqm ⊗Ip){vec(A) ⊗vec(B)},
and so the proof is complete.
□
Our next theorem establishes some results for the special commutation matrix
Kmm. Corresponding results for the general commutation matrix Kmn can be found
in Magnus and Neudecker (1979) or Magnus (1988).
Theorem 8.28
The commutation matrix Kmm has the eigenvalue +1 repeated
1
2m(m + 1) times and the eigenvalue −1 repeated 1
2m(m −1) times. In addition,
tr(Kmm) = m
and
|Kmm| = (−1)m(m−1)/2.
Proof.
Since Kmm is real and symmetric, we know from Theorem 3.9 that its
eigenvalues are also real. Further, because Kmm is orthogonal, the square of
each eigenvalue must be 1, so it has eigenvalues +1 and −1 only. Let p be the
number of eigenvalues equal to −1, which implies that m2 −p is the number of
eigenvalues equal to +1. Since the trace equals the sum of the eigenvalues, we
must have tr(Kmm) = p(−1) + (m2 −p)(1) = m2 −2p. However, by using basic
properties of the trace, we also find that
tr(Kmm) = tr
⎧
⎨
⎩
m

i=1
m

j=1
(eie′
j ⊗eje′
i)
⎫
⎬
⎭=
m

i=1
m

j=1
tr(eie′
j ⊗eje′
i)
=
m

i=1
m

j=1
{tr(eie′
j)}{tr(eje′
i)} =
m

i=1
m

j=1
(e′
iej)2
=
m

i=1
1 = m.
Evidently, m2 −2p = m, so that p = 1
2m(m −1) as claimed. Finally, the formula
given for the determinant follows directly from the fact that the determinant equals
the product of the eigenvalues.
□
Commutation matrices can be used to permute the order in Kronecker products
of three or more matrices. For instance, suppose that A is m × n, B is p × q, and
C is r × s. Then if Kr,mp denotes the commutation matrix Krh, where h = mp, it
immediately follows from Theorem 8.26(b) that
Kr,mp(A ⊗B ⊗C)Knq,s = (C ⊗A ⊗B).

THE COMMUTATION MATRIX
345
Theorem 8.29 shows us how higher dimensional commutation matrices, such
as Kr,mp, are related to lower dimensional commutation matrices, such as Krm
and Krp.
Theorem 8.29
For any positive integers m, n, and p,
Knp,m = (In ⊗Kpm)(Knm ⊗Ip) = (Ip ⊗Knm)(Kpm ⊗In).
Proof.
Let a, b, and c be arbitrary m × 1, n × 1, and p × 1 vectors, respectively.
Then using Theorem 8.26(e), we find that
Knp,m(a ⊗b ⊗c) = b ⊗c ⊗a = b ⊗Kpm(a ⊗c)
= (In ⊗Kpm)(b ⊗a ⊗c)
= (In ⊗Kpm){Knm(a ⊗b) ⊗c}
= (In ⊗Kpm)(Knm ⊗Ip)(a ⊗b ⊗c).
This implies that Knp,m = (In ⊗Kpm)(Knm ⊗Ip) because a, b, and c are arbi-
trary. The second part is proven in a similar fashion because clearly Knp,m = Kpn,m
and
Kpn,m(a ⊗c ⊗b) = c ⊗b ⊗a = c ⊗Knm(a ⊗b)
= (Ip ⊗Knm)(c ⊗a ⊗b)
= (Ip ⊗Knm){Kpm(a ⊗c) ⊗b}
= (Ip ⊗Knm)(Kpm ⊗In)(a ⊗c ⊗b).
□
We will see later that the commutation matrix Kmm appears in some important
matrix moment formulas through the term Nm = 1
2(Im2 + Kmm). Consequently, we
will establish some basic properties of Nm.
Theorem 8.30
Let Nm = 1
2(Im2 + Kmm), and let A and B be m × m matrices.
Then
(a) Nm = N ′
m = N 2
m,
(b) NmKmm = Nm = KmmNm,
(c) Nmvec(A) = 1
2vec(A + A′),
(d) Nm(A ⊗B)Nm = Nm(B ⊗A)Nm.
Proof.
The symmetry of Nm follows from the symmetry of Im2 and Kmm, whereas
N 2
m = 1
4(Im2 + Kmm)2 = 1
4(Im2 + 2Kmm + K2
mm)
= 1
2(Im2 + Kmm) = Nm,

346
SPECIAL MATRICES AND MATRIX OPERATIONS
because K2
mm = Im2 follows from the fact that K−1
mm = Kmm. Similarly, (b) follows
from the fact that K2
mm = Im2. Part (c) is an immediate consequence of
Im2vec(A) = vec(A),
Kmmvec(A) = vec(A′).
Finally, note that by using part (b) and Theorem 8.26(b),
Nm(A ⊗B)Nm = NmKmm(B ⊗A)KmmNm = Nm(B ⊗A)Nm,
which proves (d).
□
The proof of our final result will be left to the reader as an exercise.
Theorem 8.31
Let A and B be m × m matrices, such that A = BB′. Then
(a) Nm(B ⊗B)Nm = (B ⊗B)Nm = Nm(B ⊗B),
(b) (B ⊗B)Nm(B′ ⊗B′) = Nm(A ⊗A).
8.7
SOME OTHER MATRICES ASSOCIATED WITH
THE VEC OPERATOR
In this section, we introduce several other matrices that, like the commutation matrix,
have important relationships with the vec operator. However, each of the matrices
we discuss here is useful in working with vec(A) when the matrix A is square and
has some particular structure. A more thorough discussion of this and other related
material can be found in Magnus (1988).
When the m × m matrix A is symmetric, then vec(A) contains redundant ele-
ments because aij = aji, for i ̸= j. For this reason, we previously defined v(A) to be
the m(m + 1)/2 × 1 vector formed by stacking the columns of the lower triangular
portion of A. The matrix that transforms v(A) into vec(A) is called the duplication
matrix; that is, if we denote this duplication matrix by Dm, then for any m × m
symmetric matrix A,
Dmv(A) = vec(A).
(8.12)
For instance, the duplication matrix D3 is given by
D3 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

SOME OTHER MATRICES ASSOCIATED WITH THE VEC OPERATOR
347
For an explicit expression of the m2 × m(m + 1)/2 duplication matrix Dm, refer to
Magnus (1988) or Problem 8.63.
Some properties of the duplication matrix and its Moore–Penrose inverse are sum-
marized in Theorem 8.32.
Theorem 8.32
Let Dm be the m2 × m(m + 1)/2 duplication matrix and D+
m be
its Moore–Penrose inverse. Then
(a) rank(Dm) = m(m + 1)/2,
(b) D+
m = (D′
mDm)−1D′
m,
(c) D+
mDm = Im(m+1)/2,
(d) D+
mvec(A) = v(A) for every m × m symmetric matrix A.
Proof.
Clearly, for every m(m + 1)/2 × 1 vector x, an m × m symmetric matrix A
exists, such that x = v(A). However, if for some symmetric A, Dmv(A) = 0, then
from the definition of Dm, vec(A) = 0, which then implies that v(A) = 0. Thus,
Dmx = 0 only if x = 0, and so Dm has full column rank. Parts (b) and (c) follow
immediately from (a) and Theorem 5.3, whereas (d) is obtained by premultiplying
(8.12) by D+
m and then applying (c).
□
The duplication matrix and its Moore–Penrose inverse have some important rela-
tionships with Kmm and Nm.
Theorem 8.33
Let Dm be the m2 × m(m + 1)/2 duplication matrix and D+
m be
its Moore–Penrose inverse. Then
(a) KmmDm = NmDm = Dm,
(b) D+
mKmm = D+
mNm = D+
m,
(c) DmD+
m = Nm.
Proof.
For any m × m symmetric matrix A, it follows that
KmmDmv(A) = Kmmvec(A) = vec(A′)
= vec(A) = Dmv(A).
(8.13)
Similarly, we have
NmDmv(A) = Nmvec(A) = 1
2vec(A + A′)
= vec(A) = Dmv(A).
(8.14)
Since {v(A) : A m × m and A′ = A} is all of m(m + 1)/2-dimensional space,
(8.13) and (8.14) establish (a). To prove (b), take the transpose of (a), premultiply

348
SPECIAL MATRICES AND MATRIX OPERATIONS
all three sides by (D′
mDm)−1, and then apply Theorem 8.32(b). We will prove (c) by
showing that for any m × m matrix A,
DmD+
mvec(A) = Nmvec(A).
If we define A∗= 1
2(A + A′), then A∗is symmetric and
Nmvec(A) = 1
2(Im2 + Kmm)vec(A) = 1
2{vec(A) + vec(A′)}
= vec(A∗).
Using this result and (b), we find that
DmD+
mvec(A) = DmD+
mNm vec(A) = DmD+
m vec(A∗)
= Dmv(A∗) = vec(A∗) = Nm vec(A),
and so the proof is complete.
□
We know from Theorem 8.32 that D+
m vec(A) = v(A) if A is an m × m symmet-
ric matrix. Suppose now that A is not symmetric. What will D+
m vec(A) produce? Let
A∗= 1
2(A + A′), and note that because A∗is symmetric, we must have
D+
m vec(A∗) = v(A∗) = 1
2v(A + A′).
However,
D+
m vec(A) −D+
m vec(A∗) = D+
m vec(A −A∗) = D+
m vec
1
2(A −A′)

= 1
2D+
m{vec(A) −vec(A′)}
= 1
2D+
m(Im2 −Kmm)vec(A)
= 1
2(D+
m −D+
m)vec(A) = 0,
where we have used Theorem 8.33(b) in the second to the last step. Thus, D+
m vec(A)
is the same as D+
mvec(A∗). This result and the analogous expression for Dmv(A),
the derivation of which we leave as an exercise, are summarized in Theorem 8.34.
Theorem 8.34
Let A be an m × m matrix. Then
(a) D+
mvec(A) = 1
2v(A + A′),
(b) Dmv(A) = vec(AL + A′
L −DA),

SOME OTHER MATRICES ASSOCIATED WITH THE VEC OPERATOR
349
where AL is the lower triangular matrix obtained from A by replacing aij by 0 if
i < j, and DA is the diagonal matrix having the same diagonal elements as A.
We will need Theorem 8.35 in Chapter 9.
Theorem 8.35
If A is an m × m nonsingular matrix, then D′
m(A ⊗A)Dm is non-
singular and its inverse is given by D+
m(A−1 ⊗A−1)D+′
m.
Proof.
To prove the result, we simply show that the product of the two matrices
given in Theorem 8.35 yields Im(m+1)/2. Using Theorem 8.31(a), Theorem 8.32(c),
and Theorem 8.33(a) and (c), we have
D′
m(A ⊗A)DmD+
m(A−1 ⊗A−1)D+′
m
= D′
m(A ⊗A)Nm(A−1 ⊗A−1)D+′
m
= D′
mNm(A ⊗A)(A−1 ⊗A−1)D+′
m = D′
mNmD+′
m
= (NmDm)′D+′
m = D′
mD+′
m = (D+
mDm)′ = Im(m+1)/2,
and so the result follows.
□
We next consider the situation in which the m × m matrix A is lower triangular.
In this case, the elements of vec(A) are identical to those of v(A), except that vec(A)
has some additional zeros. We will denote by L′
m the m2 × m(m + 1)/2 matrix that
transforms v(A) into vec(A); that is, L′
m satisfies
L′
mv(A) = vec(A).
(8.15)
Thus, for instance, for m = 3,
L′
3 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Note that L′
m can be obtained from Dm by replacing m(m −1)/2 of the rows of Dm
by rows of zeros. The properties of the matrix Lm in Theorem 8.36 can be proven
directly from its definition given in (8.15).
Theorem 8.36
The m(m + 1)/2 × m2 matrix Lm satisfies

350
SPECIAL MATRICES AND MATRIX OPERATIONS
(a) rank(Lm) = m(m + 1)/2,
(b) LmL′
m = Im(m+1)/2,
(c) L+
m = L′
m,
(d) Lm vec(A) = v(A), for every m × m matrix A.
Proof.
Note that if A is lower triangular, then vec(A)′ vec(A) = v(A)′v(A), and so
(8.15) implies
v(A)′LmL′
mv(A) −v(A)′v(A) = v(A)′(LmL′
m −Im(m+1)/2)v(A) = 0
for all lower triangular matrices A. But this can be true only if (b) holds because
{v(A) : A m × m and lower triangular} = Rm(m+1)/2. Part (a) follows immedi-
ately from (b), as does (c) because L+
m = L′
m(LmL′
m)−1. To prove (d), note that
every matrix A can be written A = AL + AU, where AL is lower triangular and AU
is upper triangular with each diagonal element equal to zero. Clearly,
0 = vec(AL)′ vec(AU) = v(AL)′Lm vec(AU),
and because, for fixed AU, this must hold for all choices of the lower triangular matrix
AL, it follows that
Lm vec(AU) = 0.
Thus, using this, along with (8.15), (b), and v(AL) = v(A), we have
Lm vec(A) = Lm vec(AL + AU) = Lm vec(AL)
= LmL′
mv(AL) = v(AL) = v(A),
and so the proof is complete.
□
We see from (d) in Theorem 8.36 that Lm is the matrix that eliminates the zeros
in vec(A) coming from the upper triangular portion of A so as to yield v(A). For this
reason, Lm is sometimes referred to as the elimination matrix. Theorem 8.37 gives
some relationships between Lm and the matrices Dm and Nm. We will leave the
proofs of these results as an exercise for the reader.
Theorem 8.37
The elimination matrix Lm satisfies
(a) LmDm = Im(m+1)/2,
(b) DmLmNm = Nm,
(c) D+
m = LmNm.

NONNEGATIVE MATRICES
351
The last matrix related to vec(A) that we will discuss is another sort of elimination
matrix. Suppose now that the m × m matrix A is a strictly lower triangular matrix;
that is, it is lower triangular and all of its diagonal elements are zero. In this case, ˜v(A)
contains all of the relevant elements of A. We denote by ˜L′
m the m2 × m(m −1)/2
matrix that transforms ˜v(A) into vec(A); that is,
˜L′
m˜v(A) = vec(A).
Thus, for m = 3, we have
˜L3 =
⎡
⎣
0 1
0
0
0
0
0 0
0
0 0
1
0
0
0
0 0
0
0 0
0
0
0
1
0 0
0
⎤
⎦.
Since ˜Lm is similar to Lm, some of its basic properties parallel those of Lm. For
instance, Theorem 8.38 is analogous to Theorem 8.36. The proof of this theorem,
which we omit, is similar to that of Theorem 8.36.
Theorem 8.38
The m(m −1)/2 × m2 matrix ˜Lm satisfies
(a) rank(˜Lm) = m(m −1)/2,
(b) ˜Lm ˜L′
m = Im(m−1)/2,
(c) ˜L+
m = ˜L′
m,
(d) ˜Lm vec(A) = ˜v(A), for every m × m matrix A.
Theorem 8.39 gives some relationships between ˜Lm, Lm, Dm, Kmm, and Nm.
The proof is left to the reader as an exercise.
Theorem 8.39
The m(m −1)/2 × m2 matrix ˜Lm satisfies
(a) ˜LmKmm ˜L′
m = (0),
(b) ˜LmKmmL′
m = (0),
(c) ˜LmDm = ˜LmL′
m,
(d) L′
mLm ˜L′
m = ˜L′
m,
(e) DmLm ˜L′
m = 2Nm ˜L′
m,
(f) ˜LmL′
mLm ˜L′
m = Im(m−1)/2.
8.8
NONNEGATIVE MATRICES
The topic of this section, nonnegative and positive matrices, should not be
confused with nonnegative definite and positive definite matrices, which we have
discussed earlier on several occasions. An m × n matrix A is a nonnegative matrix,

352
SPECIAL MATRICES AND MATRIX OPERATIONS
indicated by A ≥(0), if each element of A is nonnegative. Similarly, A is a positive
matrix, indicated by A > (0), if each element of A is positive. We will write A ≥B
and A > B to mean that A −B ≥(0) and A −B > (0), respectively. Any matrix
A can be transformed to a nonnegative matrix by replacing each of its elements
by its absolute value, which will be denoted by abs(A); that is, if A is an m × n
matrix, then abs(A) is also an m × n matrix with (i, j)th element given by |aij|.
We will investigate some of the properties of nonnegative square matrices as well as
indicate some of their applications in stochastic processes. For a more exhaustive
coverage of this topic, the reader is referred to the texts on nonnegative matrices
by Berman and Plemmons (1994), Minc (1988), and Seneta (2006), as well as the
books by Gantmacher (1959) and Horn and Johnson (2013). Most of the proofs that
we present here follow along the lines of the derivations, based on matrix norms,
given in Horn and Johnson (2013).
We begin with some results regarding the spectral radius of nonnegative and
positive matrices.
Theorem 8.40
Let A be an m × m matrix and x be an m × 1 vector. If A ≥(0)
and x > 0, then
min
1≤i≤m
m

j=1
aij ≤ρ(A) ≤max
1≤i≤m
m

j=1
aij,
(8.16)
min
1≤i≤m x−1
i
m

j=1
aijxj ≤ρ(A) ≤max
1≤i≤m x−1
i
m

j=1
aijxj,
(8.17)
with similar inequalities holding when minimizing and maximizing over columns
instead of rows.
Proof.
Let
α = min
1≤i≤m
m

j=1
aij,
and define the m × m matrix B to have (i, h)th element
bih = αaih
⎛
⎝
m

j=1
aij
⎞
⎠
−1
if α > 0 and bih = 0 if α = 0. Note that ||B||∞= α and bih ≤aih, so that
A ≥B. Clearly, it follows that for any positive integer k, Ak ≥Bk, which implies
that ||Ak||∞≥||Bk||∞or, equivalently,
{||Ak||∞}1/k ≥{||Bk||∞}1/k.

NONNEGATIVE MATRICES
353
Taking the limit as k →∞, it follows from Theorem 4.28 that ρ(A) ≥ρ(B). How-
ever, this proves the lower bound in (8.16) because ρ(B) = α follows from the fact
that ρ(B) ≥α because
B1m = α1m
and ρ(B) ≤||B||∞= α due to Theorem 4.23. The upper bound is proven in a similar
fashion using
α = max
1≤i≤m
m

j=1
aij.
The bounds in (8.17) follow directly from those in (8.16) because if we define the
matrix C = D−1
x ADx, then C ≥(0), ρ(C) = ρ(A), and cij = aijx−1
i xj.
□
Theorem 8.41
Let A be an m × m positive matrix. Then ρ(A) is positive and is
an eigenvalue of A. In addition, a positive eigenvector of A corresponding to the
eigenvalue ρ(A) exists.
Proof.
ρ(A) > 0 follows immediately from Theorem 8.40 because A is positive.
By the definition of ρ(A), there exists an eigenvalue of A, λ, such that |λ| = ρ(A).
Let x be an eigenvector of A corresponding to λ so that Ax = λx. Note that
ρ(A) abs(x) = |λ| abs(x) = abs(λx) = abs(Ax)
≤abs(A) abs(x) = A abs(x),
where the inequality clearly follows from the fact that

m

j=1
aijxj

≤
m

j=1
|aij||xj|
for each i. Thus, the vector y = A abs(x) −ρ(A)abs(x) is nonnegative. The vec-
tor z = A abs(x) is positive because A is positive and the eigenvector x must be a
nonnull vector. Now if we assume that y ̸= 0, then, again, because A is positive, we
have
0 < Ay = Az −ρ(A)z,
or simply Az > ρ(A)z. Premultiplying this inequality by D−1
z , we get
D−1
z Az > ρ(A)1m
or, in other words,
z−1
i
m

j=1
aijzj > ρ(A)

354
SPECIAL MATRICES AND MATRIX OPERATIONS
holds for each i. However, using Theorem 8.40 this implies that ρ(A) > ρ(A). Thus,
we must have y = 0. This yields A abs(x) = ρ(A) abs(x), so that abs(x) is an
eigenvector corresponding to ρ(A), and from this we get abs(x) = ρ(A)−1A abs(x),
which shows that abs(x) is positive because ρ(A) > 0 and A abs(x) > 0. This com-
pletes the proof.
□
An immediate consequence of the proof of Theorem 8.41 is Corollary 8.41.1.
Corollary 8.41.1
Let A be an m × m positive matrix, and suppose that λ is an
eigenvalue of A satisfying |λ| = ρ(A). If x is any eigenvector corresponding to λ,
then
A abs(x) = ρ(A) abs(x).
Before determining the dimensionality of the eigenspace associated with the
eigenvalue ρ(A), we need Theorem 8.42.
Theorem 8.42
Let x be an eigenvector corresponding to the eigenvalue λ of
the m × m positive matrix A. If |λ| = ρ(A), then some angle θ exists, such that
e−iθx > 0.
Proof.
Note that
abs(Ax) = abs(λx) = ρ(A) abs(x),
(8.18)
whereas it follows from Corollary 8.41.1 that
A abs(x) = ρ(A) abs(x).
(8.19)
Now by using (8.18) and (8.19), we find that
ρ(A)|xj| = |λ||xj| = |λxj| =

m

k=1
ajkxk
 ≤
m

k=1
|ajk||xk|
=
m

k=1
ajk|xk| = ρ(A)|xj|
holds for each j. Evidently

m

k=1
ajkxk
 =
m

k=1
ajk|xk|,
and this implies that the possibly complex numbers ajkxk = rkeiθk = rk(cos θk +
i sin θk), for k = 1, . . . , m, have identical angles; that is, some angle θ exists, such

NONNEGATIVE MATRICES
355
that each ajkxk, for k = 1, . . . , m can be written in the form ajkxk = rkeiθ =
rk(cos θ + i sin θ). In this case, e−iθajkxk = rk > 0, which implies that e−iθxk > 0
because ajk > 0.
□
Theorem 8.43 not only indicates that the eigenspace corresponding to ρ(A) has
dimension one, but also that ρ(A) is the only eigenvalue of A having modulus equal
to ρ(A).
Theorem 8.43
If A is an m × m positive matrix, then the dimension of the
eigenspace corresponding to the eigenvalue ρ(A) is one. Further, if λ is an
eigenvalue of A and λ ̸= ρ(A), then |λ| < ρ(A).
Proof.
The first statement will be proven by showing that if u and v are nonnull
vectors satisfying Au = ρ(A)u and Av = ρ(A)v, then some scalar c exists, such
that v = cu. Now from Theorem 8.42, we know angles θ1 and θ2 exist, such that
s = e−iθ1u > 0 and t = e−iθ2v > 0. Define w = t −ds, where
d = min
1≤j≤m s−1
j tj,
so that w is nonnegative with at least one component equal to 0. If w ̸= 0, then clearly
Aw > 0 because A is positive, which leads to a contradiction because
Aw = At −dAs = ρ(A)t −ρ(A)ds = ρ(A)w
then implies that w > 0. Thus, we must have w = 0, so t = ds and v = cu, where
c = dei(θ2−θ1). To prove the second statement of the theorem, first note that from the
definition of the spectral radius, |λ| ≤ρ(A) for any eigenvalue λ of A. Now if x is an
eigenvector corresponding to λ and |λ| = ρ(A), then it follows from Theorem 8.42
that an angle θ exists, such that u = e−iθx > 0. Clearly, Au = λu. Premultiplying
this identity by D−1
u , we get
D−1
u Au = λ1m,
so that
u−1
i
m

j=1
aijuj = λ
holds for each i. Now applying Theorem 8.40, we get λ = ρ(A).
□
We will see that the first statement in Theorem 8.43 actually can be replaced
by the stronger condition that ρ(A) must be a simple eigenvalue of A. However,
first we have the following results, the last of which is a useful limiting result
for A.

356
SPECIAL MATRICES AND MATRIX OPERATIONS
Theorem 8.44
Suppose that A is an m × m positive matrix, and x and y are posi-
tive vectors satisfying Ax = ρ(A)x, A′y = ρ(A)y, and x′y = 1. Then the following
hold:
(a) (A −ρ(A)xy′)k = Ak −ρ(A)kxy′, for k = 1, 2, . . . .
(b) Each nonzero eigenvalue of A −ρ(A)xy′ is an eigenvalue of A.
(c) ρ(A) is not an eigenvalue of A −ρ(A)xy′.
(d) ρ(A −ρ(A)xy′) < ρ(A).
(e) lim
k→∞{ρ(A)−1A}k = xy′.
Proof.
(a) is easily established by induction, because it clearly holds for k = 1, and
if it holds for k = j −1, then
(A −ρ(A)xy′)j = (A −ρ(A)xy′)j−1(A −ρ(A)xy′)
= (Aj−1 −ρ(A)j−1xy′)(A −ρ(A)xy′)
= Aj −ρ(A)Aj−1xy′ −ρ(A)j−1xy′A + ρ(A)jxy′xy′
= Aj −ρ(A)jxy′ −ρ(A)jxy′ + ρ(A)jxy′
= Aj −ρ(A)jxy′.
Next, suppose that λ ̸= 0 and u are an eigenvalue and eigenvector of (A −ρ(A)xy′),
so that
(A −ρ(A)xy′)u = λu.
Premultiplying this equation by xy′ and observing that xy′(A −ρ(A)xy′) = 0, we
see that we must have xy′u = 0. Consequently,
Au = (A −ρ(A)xy′)u = λu,
and so λ is also an eigenvalue of A, as is required for (b). To prove (c), suppose
that λ = ρ(A) is an eigenvalue of A −ρ(A)xy′ with u being a corresponding eigen-
vector. However, we have just seen that this supposition implies that u is also an
eigenvector of A corresponding to the eigenvalue ρ(A). Thus, from Theorem 8.43,
u = cx for some scalar c and
ρ(A)u = (A −ρ(A)xy′)u = (A −ρ(A)xy′)cx
= ρ(A)cx −ρ(A)cx = 0,
which is impossible because ρ(A) > 0 and u ̸= 0, and so (c) holds. Now (d) follows
directly from (b), (c), and Theorem 8.43. Finally, to prove (e), note that by dividing
both sides of the equation given in (a) by ρ(A)k and rearranging, we get
{ρ(A)−1A}k = xy′ + {ρ(A)−1A −xy′}k.

NONNEGATIVE MATRICES
357
Take the limit, as k →∞, of both sides of this equation and observe that from (d),
ρ{ρ(A)−1A −xy′} = ρ{A −ρ(A)xy′}
ρ(A)
< 1,
and so
lim
k→∞{ρ(A)−1A −xy′}k = (0)
follows from Theorem 4.26.
□
Theorem 8.45
Let A be an m × m positive matrix. Then the eigenvalue ρ(A) is a
simple eigenvalue of A.
Proof.
Let A = XTX∗be the Schur decomposition of A, so that X is a unitary
matrix and T is an upper triangular matrix with the eigenvalues of A as its diagonal
elements. Write T = T1 + T2, where T1 is diagonal and T2 is upper triangular with
each diagonal element equal to 0. Suppose that we have chosen X so that the diagonal
elements of T1 are ordered as T1 = diag(ρ(A), . . . , ρ(A), λr+1, . . . , λm), where r
is the multiplicity of the eigenvalue ρ(A) and |λj| < ρ(A) for j = r + 1, . . . , m,
because of Theorem 8.43. We need to show that r = 1. Note that, for any upper tri-
angular matrix U with ith diagonal element uii, U k is also upper triangular with its
ith diagonal element given by uk
ii. Using this, we find that
lim
k→∞{ρ(A)−1A}k = X

lim
k→∞{ρ(A)−1(T1 + T2)}k

X∗
= X

lim
k→∞diag

1, . . . , 1,
 λr+1
ρ(A)
k
, . . . ,
 λm
ρ(A)
k
+ T3

X∗
= X{diag(1, . . . , 1, 0, . . . , 0) + T3}X∗,
where this last diagonal matrix has r 1’s and T3 is an upper triangular matrix with
each diagonal element equal to 0. Clearly, this limiting matrix has rank at least r.
However, from Theorem 8.44(e) we see that the limiting matrix must have rank 1,
and this proves the result.
□
To this point, we have concentrated on positive matrices. Our next step is to extend
some of the previous results to nonnegative matrices. We will see that many of these
results generalize to the class of irreducible nonnegative matrices.
Definition 8.2
An m × m matrix A, with m ≥2, is called a reducible matrix if
some integer r, with 1 ≤r ≤m −1, and m × m permutation matrix P exist, such
that
PAP ′ =

B
C
(0) D
	
,

358
SPECIAL MATRICES AND MATRIX OPERATIONS
where B is r × r, C is r × (m −r), and D is (m −r) × (m −r). If A is not
reducible, then it is said to be irreducible.
We will need Theorem 8.46 regarding irreducible nonnegative matrices.
Theorem 8.46
An m × m nonnegative matrix A is irreducible if and only if
(Im + A)m−1 > (0).
Proof.
First suppose that A is irreducible. We will show that if x is an m × 1
nonnegative vector with r positive components, 1 ≤r ≤m −1, then (Im + A)x
has at least r + 1 positive components. Repeated use of this result verifies that
(Im + A)m−1 > (0) because each column of Im + A has at least two positive com-
ponents due to the fact that A is irreducible. Since A ≥(0), (Im + A)x = x + Ax
must have at least r positive components. If it has exactly r positive components,
then the jth component of Ax must be 0 for every j for which xj = 0. Equivalently,
for any permutation matrix P, the jth component of PAx must be 0 for every j for
which the jth component of Px is 0. If we choose a permutation matrix for which
y = Px has its m −r 0’s in the last m −r positions, then we find that the jth
component of PAx = PAP ′y must be 0 for j = r + 1, . . . , m. Since PAP ′ ≥(0)
and the first r components of y are positive, PAP ′ would have to be of the form
PAP ′ =
 B
C
(0) D
	
.
Since this result contradicts the fact that A is irreducible, the number of positive
components in the vector (Im + A)x must exceed r. Conversely, now suppose that
(Im + A)m−1 > (0), so that, clearly, (Im + A)m−1 is irreducible. Now A cannot be
reducible because, if for some permutation matrix P,
PAP ′ =

B
C
(0) D
	
,
then
P(Im + A)m−1P ′ = {P(Im + A)P ′}m−1 = (Im + PAP ′)m−1
=

Ir + B
C
(0)
Im−r + D
m−1
,
and the matrix on the right-hand side of this last equation has the upper triangular
form given in Definition 8.2.
□
We will generalize the result of Theorem 8.41 by showing that ρ(A) is positive, is
an eigenvalue of A, and has a positive eigenvector when A is an irreducible nonneg-
ative matrix. However, first we need Theorem 8.47.

NONNEGATIVE MATRICES
359
Theorem 8.47
Let A be an m × m irreducible nonnegative matrix, x be an m × 1
nonnegative vector, and define the function
f(x) = min
xi̸=0 x−1
i (A)i·x = min
xi̸=0 x−1
i
m

j=1
aijxj.
Then an m × 1 nonnegative vector b exists, such that b′1m = 1 and f(b) ≥f(x)
holds for any nonnegative x.
Proof.
Define the set
S = {y : y = (Im + A)m−1x∗, x∗∈Rm, x∗≥0, x′
∗1m = 1}.
Since S is a closed and bounded set, and f is a continuous function on S due to
the fact that y > 0 if y ∈S, there exists a c ∈S, such that f(c) ≥f(y) for all
y ∈S. Define b = c/(c′1m), and note that f is unaffected by scale changes, so
f(b) = f(c). Let x be an arbitrary nonnegative vector, and define x∗= x/(x′1m)
and y = (Im + A)m−1x∗. Now it follows from the definition of f that
Ax∗−f(x∗)x∗≥0.
Premultiply this equation by (Im + A)m−1 and use the fact that (Im + A)m−1A =
A(Im + A)m−1 to get
Ay −f(x∗)y ≥0.
However, α = f(y) is the largest value for which Ay −αy ≥0 because at least
one component of Ay −f(y)y is 0; that is, for some k, f(y) = y−1
k (A)k·y
and, consequently, the kth component of Ay −f(y)y will be 0. Thus, we have
shown that f(y) ≥f(x∗) = f(x). The result then follows from the fact that
f(y) ≤f(c) = f(b).
□
Theorem 8.48
Let A be an m × m irreducible nonnegative matrix. Then A has the
positive eigenvalue ρ(A) and associated with it a positive eigenvector x.
Proof.
We first show that f(b) is a positive eigenvalue of A, where f(b) is
defined as in Theorem 8.47, and b is a nonnegative vector satisfying b′1m = 1 and
maximizing f. Since b maximizes f(x) over all nonnegative x, we have
f(b) ≥f(m−11m) = min
1≤i≤m (1/m)−1(A)i·(m−11m)
= min
1≤i≤m
m

j=1
aij > 0,

360
SPECIAL MATRICES AND MATRIX OPERATIONS
because A is nonnegative and irreducible. To prove that f(b) is an eigenvalue of A,
recall that from the definition of f it follows that Ab −f(b)b ≥0. If Ab −f(b)b has
at least one positive component, then because (Im + A)m−1 > (0), we must have
(Im + A)m−1(Ab −f(b)b) = Ay −f(b)y > 0,
where y = (Im + A)m−1b. However, α = f(y) is the largest value for which Ay −
αy ≥0, so we would have f(y) > f(b), which cannot be true because b maximizes
f(y) over all y ≥0. Thus, Ab −f(b)b = 0, and so f(b) is an eigenvalue of A and b
is a corresponding eigenvector. Our next step is to show that f(b) = ρ(A) by showing
that f(b) ≥|λi|, where λi is an arbitrary eigenvalue of A. Now if u is an eigenvector
of A corresponding to λi, then Au = λiu or
λiuh =
m

j=1
ahjuj
for h = 1, . . . , m. Consequently,
|λi||uh| ≤
m

j=1
ahj|uj|,
for h = 1, . . . , m, or simply
A abs(u) −|λi| abs(u) ≥0,
which implies that |λi| ≤f(abs(u)) ≤f(b). Finally, we must find a pos-
itive
eigenvector
associated
with
the
eigenvalue
ρ(A) = f(b).
We
have
already found a nonnegative eigenvector, b. Note that Ab = f(b)b implies
that (Im + A)m−1b = {1 + f(b)}m−1b, and so
b = (Im + A)m−1b
{1 + f(b)}m−1 .
Thus, using Theorem 8.46, we find that b is actually positive.
□
The proof of Theorem 8.49 will be left to the reader as an exercise.
Theorem 8.49
If A is an m × m irreducible nonnegative matrix, then ρ(A) is a
simple eigenvalue of A.
Although ρ(A) is a simple eigenvalue of an irreducible nonnegative matrix A,
there may be other eigenvalues of A that have absolute value ρ(A). Consequently,
Theorem 8.44(e) does not immediately extend to irreducible nonnegative matrices.
This leads us to the following definition.

NONNEGATIVE MATRICES
361
Definition 8.3
An m × m nonnegative matrix A is said to be primitive if it is irre-
ducible and has only one eigenvalue satisfying |λi| = ρ(A).
Clearly, the result of Theorem 8.44(e) does extend to primitive matrices, and this
is summarized in Theorem 8.50.
Theorem 8.50
Let A be an m × m primitive nonnegative matrix, and suppose that
the m × 1 vectors x and y satisfy Ax = ρ(A)x, A′y = ρ(A)y, x > 0, y > 0, and
x′y = 1. Then
lim
k→∞{ρ(A)−1A}k = xy′.
Our final theorem of this section gives a general limit result that holds for all irre-
ducible nonnegative matrices. A proof of this result can be found in Horn and Johnson
(2013).
Theorem 8.51
Let A be an m × m irreducible nonnegative matrix, and suppose
that the m × 1 vectors x and y satisfy Ax = ρ(A)x, A′y = ρ(A)y, and x′y = 1.
Then
lim
N→∞

N −1
N

k=1
{ρ(A)−1A}k

= xy′.
Nonnegative matrices play an important role in the study of stochastic processes.
We will illustrate some of their applications to a particular type of stochastic process
known as a Markov chain. Additional information on Markov chains, and stochastic
processes in general, can be found in texts such as Bhattacharya and Waymire (2009),
Medhi (2009), and Pinsky and Karlin (2011).
Example 8.12
Suppose that we are observing some random phenomenon over time,
and at any one point in time our observation can take on any one of the m values,
sometimes referred to as states, 1, . . . , m. In other words, we have a sequence of
random variables Xt, for time periods t = 0, 1, . . . , where each random variable can
be equal to any one of the numbers, 1, . . . , m. If the probability that Xt is in state i
depends only on the state that Xt−1 is in and not on the states of prior time periods,
then this process is said to be a Markov chain. If this probability also does not depend
on the value of t, then the Markov chain is said to be homogeneous. In this case, the
state probabilities for any time period can be computed from the initial state proba-
bilities and what are known as the transition probabilities. We will write the initial
state probability vector p(0) = (p(0)
1 , . . . , p(0)
m )′, where p(0)
i
gives the probability that
the process starts out at time 0 in state i. The matrix of transition probabilities is the
m × m matrix P whose (i, j)th element, pij, gives the probability of Xt being in

362
SPECIAL MATRICES AND MATRIX OPERATIONS
state i given that Xt−1 is in state j. Thus, if p(t) = (p(t)
1 , . . . , p(t)
m )′ and p(t)
i
is the
probability that the system is in state i at time t, then, clearly,
p(1) = Pp(0),
p(2) = Pp(1) = PPp(0) = P 2p(0),
or for general t,
p(t) = P tp(0).
If we have a large population of individuals subject to this random process, then p(t)
i
could be described as the proportion of individuals in state i at time t, whereas p(0)
i
would be the proportion of individuals starting out in state i. A natural question then
is what is happening to these proportions as t increases? That is, can we determine
the limiting behavior of p(t)? Note that the answer depends on the limiting behavior
of P t, and P is a nonnegative matrix because each of its elements is a probability.
Thus, if P is a primitive matrix, we can apply Theorem 8.50. Now, because the jth
column of P gives the probabilities of the various states for time period t when we
are in state j at time period t −1, the column sum must be 1; that is, 1′
mP = 1′
m
or P ′1m = 1m, so P has an eigenvalue equal to 1. Further, a simple application of
Theorem 8.40 assures us that ρ(P) ≤1, so we must have ρ(P) = 1. Consequently, if
P is primitive and π is the m × 1 positive vector satisfying Pπ = π and π′1m = 1,
then
lim
t→∞{ρ(P)−1P}t = lim
t→∞P t = π1′
m.
Using this, we see that
lim
t→∞p(t) = lim
t→∞P tp(0) = π1′
mp(0) = π,
where the last step follows from the fact that 1′
mp(0) = 1. Thus, the system
approaches a point of equilibrium in which the proportions for the various states
are given by the components of π, and these proportions do not change from time
period to time period. Further, this limiting behavior is not dependent on the initial
proportions in p(0).
As a specific example, let us consider the problem of social mobility that involves
the transition between social classes over successive generations in a family. Suppose
that each individual is classified according to occupation, as being upper, middle, or
lower class, which have been labeled as states 1, 2, and 3, respectively. Suppose that
the transition matrix relating a son’s class to his father’s class is given by
P =
⎡
⎣
0.45 0.05 0.05
0.45 0.70 0.50
0.10 0.25 0.45
⎤
⎦,

CIRCULANT AND TOEPLITZ MATRICES
363
so that, for instance, the probabilities that a son will have an upper, middle, and
lower class occupation when his father has an upper class occupation are given by
the entries in the first column of P. Since P is positive, the limiting result previously
discussed applies. A simple eigenanalysis of the matrix P reveals that the
positive
vector
π,
which
satisfies
Pπ = π
and
π′1m = 1,
is
given
by
π = (0.083, 0.620, 0.297)′. Thus, if this random process satisfies the condi-
tions of a homogeneous Markov chain, then after many generations, the male
population would consist of 8.3% in the upper class, 62% in the middle class, and
29.7% in the lower class.
8.9
CIRCULANT AND TOEPLITZ MATRICES
In this section, we briefly discuss some structured matrices that have applications in
stochastic processes and time series analysis. For a more comprehensive treatment of
the first of these classes of matrices, see Davis (1994).
An m × m matrix A is said to be a circulant matrix if each row of A can be
obtained from the previous row by a circular rotation of elements; that is, if we shift
each element in the ith row over one column, with the element in the last column being
shifted back to the first column, we get the (i + 1)th row, unless i = m, in which case
we get the first row. Thus, if the elements of the first row of A are a1, a2, . . . , am,
then to be a circulant matrix, A must have the form
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a1
a2 a3 · · · am−1
am
am
a1 a2 · · · am−2 am−1
am−1 am a1 · · · am−3 am−2
...
...
...
...
...
a3
a4 a5 · · ·
a1
a2
a2
a3 a4 · · ·
am
a1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.20)
We will sometimes use the notation A = circ(a1, . . . , am) to refer to the circu-
lant matrix in (8.20). One special circulant matrix, which we will denote by Πm,
is circ(0, 1, 0, . . . , 0). This matrix, which also can be written as
Πm = (em, e1, . . . , em−1) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
e′
2
e′
3
...
e′
m
e′
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
is a permutation matrix, so Π−1
m = Π′
m. Note that if we use a1, . . . , am to denote the
columns of an arbitrary m × m matrix A and b′
1, . . . , b′
m to denote the rows, then
AΠm = (a1, a2, . . . , am)(em, e1, . . . , em−1)
= (am, a1, . . . , am−1),
(8.21)

364
SPECIAL MATRICES AND MATRIX OPERATIONS
ΠmA =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
e′
2
e′
3
...
e′
m
e′
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
b′
1
b′
2
...
b′
m−1
b′
m
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
b′
2
b′
3
...
b′
m
b′
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
(8.22)
and (8.21) equals (8.22) if and only if A is of the form given in (8.20). Thus, we have
Theorem 8.52.
Theorem 8.52
The m × m matrix A is a circulant matrix if and only if
A = ΠmAΠ′
m.
Theorem 8.53 gives an expression for an m × m circulant matrix in terms of a
sum of m matrices.
Theorem 8.53
The circulant matrix A = circ(a1, . . . , am) can be expressed as
A = a1Im + a2Πm + a3Π2
m + · · · + amΠm−1
m
.
Proof.
Using (8.20), we see that
A = a1Im + a2(em, e1, . . . , em−1) + a3(em−1, em, e1, . . . , em−2) + · · ·
+ am(e2, e3, . . . , em, e1).
Since the postmultiplication of any m × m matrix by Πm shifts the columns of that
matrix one place to the right, we find that
Π2
m = (em−1, em, . . . , em−2)
...
Πm−1
m
= (e2, e3, . . . , em, e1),
and so the result follows.
□
Certain operations on circulant matrices produce another circulant matrix. Some
of these are given in Theorem 8.54.
Theorem 8.54
Let A and B be m × m circulant matrices. Then
(a) A′ is circulant,

CIRCULANT AND TOEPLITZ MATRICES
365
(b) for any scalars α and β, αA + βB is circulant,
(c) for any positive integer r, Ar is circulant,
(d) A−1 is circulant, if A is nonsingular,
(e) AB is circulant.
Proof.
If A = circ(a1, . . . , am) and B = circ(b1, . . . , bm), it follows directly
from (8.20) that A′ = circ(a1, am, am−1, . . . , a2) and
αA + βB = circ(αa1 + βb1, . . . , αam + βbm).
Since A is circulant, we must have A = ΠmAΠ′
m. However, Πm is an orthogonal
matrix, so
Ar = (ΠmAΠ′
m)r = ΠmArΠ′
m,
and consequently by Theorem 8.52, Ar is also a circulant matrix. In a similar fashion,
we find that if A is nonsingular, then
A−1 = (ΠmAΠ′
m)−1 = Π′−1
m A−1Π−1
m = ΠmA−1Π′
m,
and so A−1 is circulant. Finally, to prove (e), note that we must have both A =
ΠmAΠ′
m and B = ΠmBΠ′
m, implying that
AB = (ΠmAΠ′
m)(ΠmBΠ′
m) = ΠmABΠ′
m,
and so the proof is complete.
□
The representation of a circulant matrix given in Theorem 8.53 provides a simple
way of proving Theorem 8.55.
Theorem 8.55
Suppose that A and B are m × m circulant matrices. Then their
product commutes; that is, AB = BA.
Proof.
If A = circ(a1, . . . , am) and B = circ(b1, . . . , bm), then it follows from
Theorem 8.53 that
A =
m

i=1
aiΠi−1
m ,
B =
m

j=1
bjΠj−1
m ,
where Π0
m = Im. Consequently,
AB =
 m

i=1
aiΠi−1
m
 ⎛
⎝
m

j=1
bjΠj−1
m
⎞
⎠=
m

i=1
m

j=1
(aiΠi−1
m )(bjΠj−1
m )
=
m

i=1
m

j=1
aibjΠi+j−2
m
=
m

i=1
m

j=1
(bjΠj−1
m )(aiΠi−1
m )

366
SPECIAL MATRICES AND MATRIX OPERATIONS
=
⎛
⎝
m

j=1
bjΠj−1
m
⎞
⎠
 m

i=1
aiΠi−1
m

= BA,
and so the result follows.
□
All circulant matrices are diagonalizable. We will show this by determining the
eigenvalues and eigenvectors of a circulant matrix. However, first let us find the eigen-
values and eigenvectors of the special circulant matrix Πm.
Theorem
8.56
Let
λ1, . . . , λm
be
the
m
solutions
to
the
polynomial
equation
λm −1 = 0;
that
is,
λj = θj−1,
where
θ = exp (2πi/m) =
cos(2π/m) + i sin(2π/m) and i = √−1. Define Λ to be the diagonal matrix
diag(1, θ, . . . , θm−1), and let
F =
1
√m
⎡
⎢⎢⎢⎢⎢⎣
1
1
1
· · ·
1
1
θ
θ2
· · ·
θm−1
1
θ2
θ4
· · ·
θ2(m−1)
...
...
...
...
1 θm−1 θ2(m−1) · · ·
θ(m−1)(m−1)
⎤
⎥⎥⎥⎥⎥⎦
.
Then the diagonalization of Πm is given by Πm = FΛF ∗, where F ∗is the conjugate
transpose of F; that is, the diagonal elements of Λ are the eigenvalues of Πm, whereas
the columns of F are corresponding eigenvectors.
Proof.
The eigenvalue-eigenvector equation, Πmx = λx, yields the equations
xj+1 = λxj,
for j = 1, . . . , m −1, and
x1 = λxm.
After repeated substitution, we obtain for any j, xj = λmxj. Thus, λm = 1, and so
the eigenvalues of Πm are 1, θ, . . . , θm−1. Substituting the eigenvalue θj−1 and x1 =
m−1/2 into the equations above, we find that an eigenvector corresponding to the
eigenvalue θj−1 is given by x = m−1/2(1, θj−1, . . . , θ(m−1)(j−1))′. Thus, we have
shown that the diagonal elements of Λ are the eigenvalues of Πm and the columns of
F are corresponding eigenvectors. The remainder of the proof, which simply involves
the verification that F −1 = F ∗, is left to the reader as an exercise.
□
The matrix F given in Theorem 8.56 is sometimes referred to as the Fourier
matrix of order m. The diagonalization of an arbitrary circulant matrix, which follows
directly from Theorem 8.53 and Theorem 8.56, is given in Theorem 8.57.

CIRCULANT AND TOEPLITZ MATRICES
367
Theorem 8.57
Let A be the m × m circulant matrix circ(a1, . . . , am). Then
A = FΔF ∗,
where Δ = diag(δ1, . . . , δm), δj = a1 + a2λ1
j + · · · + amλm−1
j
and λj and F are
defined as in Theorem 8.56.
Proof.
Since
Πm = FΛF ∗
and
FF ∗= Im,
we
have
Πj
m = FΛjF ∗,
for
j = 2, . . . , m −1. By using Theorem 8.53, we find that
A = a1Im + a2Πm + a3Π2
m + · · · + amΠm−1
m
= a1FF ∗+ a2FΛ1F ∗+ a3FΛ2F ∗+ · · · + amFΛm−1F ∗
= F(a1Im + a2Λ1 + a3Λ2 + · · · + amΛm−1)F ∗
= FΔF ∗,
and so the proof is complete.
□
The class of circulant matrices is a subclass of a larger class of matrices known as
Toeplitz matrices. The elements of an m × m Toeplitz matrix A satisfy aij = aj−i
for scalars a−m+1, a−m+2, . . . , am−1; that is, A has the form
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a0
a1
a2
· · ·
am−2 am−1
a−1
a0
a1
· · ·
am−3 am−2
a−2
a−1
a0
· · ·
am−4 am−3
...
...
...
...
...
a−m+2 a−m+3 a−m+4 · · ·
a0
a1
a−m+1 a−m+2 a−m+3 · · ·
a−1
a0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.23)
Two simple m × m Toeplitz matrices are
B =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 1 0 0 · · · 0
0 0 1 0 · · · 0
0 0 0 1 · · · 0
... ...
... ...
...
0 0 0 0 · · · 1
0 0 0 0 · · · 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
F =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 0 · · · 0 0 0
1 0 · · · 0 0 0
0 1 · · · 0 0 0
... ...
... ... ...
0 0 · · · 1 0 0
0 0 · · · 0 1 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
These are sometimes referred to as the backward shift and forward shift matrices
since for an m × m matrix C = [c1, . . . , cm], CB = [0, c1, . . . , cm−1] and CF =
[c2, . . . , cm, 0]. Any Toeplitz matrix can be expressed in terms of B and F since
a0Im + m−1
i=1 (a−iF i + aiBi) clearly yields the matrix given in (8.23).

368
SPECIAL MATRICES AND MATRIX OPERATIONS
If aj = a−j for j = 1, . . . , m −1, then the matrix A in (8.23) is a symmetric
Toeplitz matrix. One important and simple symmetric Toeplitz matrix is one that has
aj = a−j = 0 for j = 2, . . . , m −1, so that
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a0 a1
0 · · ·
0
0
a1 a0 a1 · · ·
0
0
0 a1 a0 · · ·
0
0
...
...
...
...
...
0
0
0 · · ·
a0 a1
0
0
0 · · ·
a1 a0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.24)
Toeplitz matrices are sometimes encountered in time series analysis.
Example 8.13
In this example, we consider a response variable y which is observed
over time. For instance, we may be recording the annual amount of rainfall at some
particular location and have the measurements for the last N years. These type of data
are called time series, and standard regression methods typically cannot be used since
the responses are usually correlated. One of the time series models sometimes used
for this type of data is a moving average model which models the response variable
for time period i as
yi =
∞

j=0
ρjϵi−j.
Here ρ is a constant satisfying |ρ| < 1, while the ϵi−j’s are uncorrelated random errors
each with mean 0 and variance σ2. We will determine the covariance matrix of the
vector, y = (y1, . . . , yN)′, of the N successive observations of the response. Clearly,
E(yi) = 0 for all i so for i = 1, . . . , N and h = 0, . . . , N −i, we have
cov(yi, yi+h) = E(yiyi+h) = E
⎛
⎝
∞

j=0
∞

l=0
ρjϵi−jρlϵi+h−l
⎞
⎠
= E
⎛
⎝
∞

j=0
∞

k=−h
ρj+k+hϵi−jϵi−k
⎞
⎠=
∞

j=0
ρ2j+hσ2
= ρhσ2
∞

j=0
ρ2j = ρhσ2
1 −ρ2 = ρhγ2,
where γ2 = var(yi) = σ2/(1 −ρ2). Thus, the covariance matrix of y is the symmet-
ric Toeplitz matrix
γ2
⎡
⎢⎢⎢⎢⎣
1
ρ
ρ2
· · ·
ρN−1
ρ
1
ρ
· · ·
ρN−2
...
...
...
...
ρN−1 ρN−2 ρN−3 · · ·
1
⎤
⎥⎥⎥⎥⎦
.

HADAMARD AND VANDERMONDE MATRICES
369
Some specialized results, such as formulas for eigenvalues and formulas for the
computation of the inverse of a Toeplitz matrix, can be found in Grenander and Szego
(1984) and Heinig and Rost (1984).
8.10
HADAMARD AND VANDERMONDE MATRICES
In this section, we discuss some matrices that have applications in the areas of design
of experiments and response surface methodology. We begin with a class of matrices
known as Hadamard matrices. An m × m matrix H is said to be a Hadamard matrix
if first, each element of H is either +1 or −1, and second, H satisfies
H′H = HH′ = mIm;
(8.25)
that is, the columns of H form an orthogonal set of vectors, and the rows form an
orthogonal set as well. For instance, a 2 × 2 Hadamard matrix is given by
H =

1
1
1 −1
	
,
whereas a 4 × 4 Hadamard matrix is given by
H =
⎡
⎢⎢⎣
1
1
1
1
−1 −1
1
1
1 −1
1 −1
1 −1 −1
1
⎤
⎥⎥⎦.
Some of the basic properties of Hadamard matrices are given in Theorem 8.58.
Theorem 8.58
Let Hm denote any m × m Hadamard matrix. Then
(a) m−1/2Hm is an m × m orthogonal matrix,
(b) |Hm| = ±mm/2,
(c) Hm ⊗Hn is an mn × mn Hadamard matrix.
Proof.
(a) follows directly from (8.25). Also using (8.25), we find that
|H′
mHm| = |mIm| = mm.
However,
|H′
mHm| = |H′
m||Hm| = |Hm|2,

370
SPECIAL MATRICES AND MATRIX OPERATIONS
and so (b) follows. Note that each element of Hm ⊗Hn is +1 or −1 because each
element is the product of an element from Hm and an element from Hn, and
(Hm ⊗Hn)′(Hm ⊗Hn) = H′
mHm ⊗H′
nHn
= mIm ⊗nIn = mnImn,
so (c) follows.
□
Hadamard matrices that have all of the elements of the first row equal to +1 are
called normalized Hadamard matrices. Our next result addresses the existence of nor-
malized Hadamard matrices.
Theorem 8.59
If an m × m Hadamard matrix exists, then an m × m normalized
Hadamard matrix exists.
Proof.
Suppose that H is an m × m Hadamard matrix. Let D be the diagonal
matrix with the elements of the first row of H as its diagonal elements; that is,
D = diag(h11, . . . , h1m). Note that D2 = Im because each diagonal element of
D is +1 or −1. Consider the m × m matrix H∗= HD. Each column of H∗is the
corresponding column of H multiplied by either +1 or −1, so clearly each element
of H∗is +1 or −1. The jth element in the first row of H∗is h2
1j = 1, so H∗has all
of its elements of the first row equal to +1. In addition,
H′
∗H∗= (HD)′HD = D′H′HD
= D(mIm)D = mD2 = mIm.
Thus, H∗is an m × m normalized Hadamard matrix, and so the proof is
complete.
□
Hadamard matrices of size m × m do not exist for every choice of m. We have
already given an example of a 2 × 2 Hadamard matrix, and this matrix can be used
repeatedly in Theorem 8.58(c) to obtain a 2n × 2n Hadamard matrix for any integer
n ≥2. However, m × m Hadamard matrices do exist for some values of m ̸= 2n.
Theorem 8.60 gives a necessary condition on the order m so that Hadamard matrices
of order m exist.
Theorem 8.60
If H is an m × m Hadamard matrix, where m > 2, then m is a
multiple of four.
Proof.
The result can be proven by using the fact that any three rows of H are
orthogonal to one another. Consequently, we will refer to the first three rows of H,
and, because of Theorem 8.59, we may assume that H is a normalized Hadamard
matrix, so that all of the elements in the first row are +1. Since the second and third

HADAMARD AND VANDERMONDE MATRICES
371
rows are orthogonal to the first row, they must each have r +1’s and r −1’s, where
r = m/2; thus, clearly,
m = 2r,
(8.26)
or, in other words, m is a multiple of 2. Let n+−be the number of columns in which
row 2 has a +1 and row 3 has a −1. Similarly, define n−+, n++, and n−−. Note
that the value of any one of these n’s determines the others because n++ + n+−= r,
n++ + n−+ = r, n−−+ n−+ = r, and n−−+ n+−= r. For instance, if n++ = s,
then n+−= (r −s), n−+ = (r −s), and n−−= s. However, the orthogonality of
rows 2 and 3 guarantee that n++ + n−−= n−+ + n+−, which yields the relationship
2s = 2(r −s).
Thus, r = 2s, and so using (8.26) we get m = 4s, which completes the proof.
□
Some additional results on Hadamard matrices can be found in Hedayat and Wallis
(1978), Agaian (1985), and Xian (2001).
An m × n matrix A is said to be a Vandermonde matrix if it has the form
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
· · ·
1
a1
a2
a3
· · ·
an
a2
1
a2
2
a2
3
· · ·
a2
n
...
...
...
...
am−1
1
am−1
2
am−1
3
· · ·
am−1
n
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.27)
For instance, if F is the m × m Fourier matrix discussed in Section 8.9, then A =
m1/2F is an m × m Vandermonde matrix with ai = θi−1, for i = 1, . . . , m. For a
statistical example, consider the polynomial regression model,
yi = β0 + β1xi + · · · + βkxk
i + ϵi,
in which the response variable y is regressed on one explanatory variable x through
a kth-degree polynomial. If we have N observations and the model is written in the
usual matrix form y = Xβ + ϵ, then X′ has the form of the Vandermonde matrix in
(8.27) with ai = xi, m = k + 1, and n = N.
Our final result of this chapter gives an expression for the determinant of a square
Vandermonde matrix.
Theorem 8.61
Let A be the m × m Vandermonde matrix given in (8.27). Then its
determinant is given by
|A| =

1≤i<j≤m
(aj −ai).
(8.28)

372
SPECIAL MATRICES AND MATRIX OPERATIONS
Proof.
Our proof is by induction. For m = 2, we find that
|A| =

1
1
a1 a2
 = a2 −a1,
and so (8.28) holds when A is 2 × 2. Next we assume that (8.28) holds for Vander-
monde matrices of order m −1 and show that it must also hold for order m. Thus, if
B is the (m −1) × (m −1) matrix obtained from A by deleting its last row and first
column, then, because B is a Vandermonde matrix of order m −1, we must have
|B| =

2≤i<j≤m
(aj −ai).
Define the m × m matrix
C =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0 · · ·
0
0
−a1
1
0 · · ·
0
0
0
−a1 1 · · ·
0
0
...
...
...
...
...
0
0
0 · · ·
1
0
0
0
0 · · · −a1 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
and note that by repeatedly using the cofactor expansion formula for a determinant
on the first row, we find that |C| = 1. Thus, |A| = |CA|. However, it is easily verified
that CA = E, where
E =

1 1′
m−1
0
BD
	
,
and D = diag((a2 −a1), (a3 −a1), . . . , (am −a1)). Consequently,
|A| = |CA| = |E| = |BD| = |B||D|
=
⎧
⎨
⎩

2≤i<j≤m
(aj −ai)
⎫
⎬
⎭
⎧
⎨
⎩

2≤j≤m
(aj −a1)
⎫
⎬
⎭
=

1≤i<j≤m
(aj −ai),
where the third equality was obtained by using Theorem 7.4. This completes the
proof.
□

PROBLEMS
373
PROBLEMS
8.1 Let the 2 × 2 matrices A and B be given by
A =

2 3
1 2

,
B =

5 3
3 2

.
(a) Compute A ⊗B and B ⊗A.
(b) Find tr(A ⊗B).
(c) Compute |A ⊗B|.
(d) Give the eigenvalues of A ⊗B.
(e) Find (A ⊗B)−1.
8.2 Give a simplified expression for Im ⊗In.
8.3 Prove the properties given in Theorem 8.1.
8.4 Suppose that A and B are m × n and p × q matrices, respectively, and c is
an r × 1 vector. Show that
(a) A(In ⊗c′) = A ⊗c′,
(b) (c ⊗Ip)B = c ⊗B.
8.5 Let a be an m × 1 vector and B be a p × q matrix. Suppose B is partitioned
as B = [B1 · · · Bk]. Show that
a ⊗B =
!a ⊗B1 · · ·
a ⊗Bk
"
.
8.6 Prove results (b) and (c) of Theorem 8.4.
8.7 Show that if A and B are symmetric matrices, then A ⊗B is also symmetric.
8.8 Show that A ⊗B is nonsingular if and only if A and B are nonsingular.
8.9 Let A be m × m and B be n × n. Show that A ⊗B is an orthogonal matrix
if and only if cA and c−1B are orthogonal matrices for some c > 0.
8.10 Find the rank of A ⊗B, where
A =
⎡
⎣
2 6
1 4
3 1
⎤
⎦,
B =
⎡
⎢⎣
5 2 4
2 1 1
1 0 2
⎤
⎥⎦.
8.11 For matrices A and B of any sizes, show that A ⊗B = (0) if and only if
A = (0) or B = (0).

374
SPECIAL MATRICES AND MATRIX OPERATIONS
8.12 Let xi be an eigenvector of the m × m matrix A corresponding to the eigen-
value λi. Let yj be an eigenvector of the p × p matrix B corresponding to the
eigenvalue θj.
(a) Show that xi ⊗yj is an eigenvector of A ⊗B.
(b) Give an example of matrices A and B, such that A ⊗B has an eigen-
vector that is not the Kronecker product of an eigenvector of A and an
eigenvector of B.
8.13 Suppose that A is an m × n matrix and B is an n × m matrix, where n > m.
Show that A ⊗B has a 0 eigenvalue with multiplicity at least (n −m)m.
8.14 Suppose x is an eigenvector of the m × m matrix A corresponding to the
eigenvalue λ, and y is an eigenvector of the n × n matrix B corresponding to
the eigenvalue μ. Show that y ⊗x is an eigenvector of (In ⊗A) + (B ⊗Im)
corresponding to the eigenvalue λ + μ.
8.15 Show that if A and B are positive definite matrices, then A ⊗B is also positive
definite.
8.16 It follows from Theorem 8.3 and Theorem 8.6 that if A and B are square
matrices, then tr(A ⊗B) = tr(B ⊗A) and |A ⊗B| = |B ⊗A|. Show that
when A and B are not square and A ⊗B is square, then the first of these two
identities need not hold whereas the second one does hold. That is, suppose
that A is m × n and B is n × m.
(a) Give an example for which tr(A ⊗B) ̸= tr(B ⊗A).
(b) Prove that |A ⊗B| = |B ⊗A|.
8.17 Let x be an m × 1 vector and y be an n × 1 vector. Verify that the three
matrices xy′, y′ ⊗x, and x ⊗y′ are identical.
8.18 Compute the sum of squared errors SSE = (y −ˆy)′(y −ˆy) for the two-way
classification model with interaction discussed in Example 8.3.
8.19 Consider the two-way classification model without interaction given by
yijk = μ + τi + γj + ϵijk,
where i = 1, . . . , a, j = 1, . . . , b, and k = 1, . . . , n.
(a) Find a least squares solution for β = (μ, τ1, . . . , τa, γ1, . . . , γb)′, and
use this to obtain the vector of fitted values and the sum of squared errors
for this model.
(b) Compute the sum of squared errors for the reduced model yijk = μ +
γj + ϵijk and use this along with the SSE computed in (a) to show that
the sum of squares for factor A is
SSA = nb
a

i=1
(yi· −y··)2.

PROBLEMS
375
(c) In a similar fashion, show that the sum of squares for factor B is
SSB = na
b

j=1
(y·j −y··)2.
(d) Find a set of as many linearly independent estimable functions of μ, τi,
and γj as possible.
(e) Use the sum of squared errors computed in (a) and the sum of squared
errors computed in Problem 8.18 to show that the sum of squares for
interaction in the model of Problem 8.18 is given by
SSAB = n
a

i=1
b

j=1
(yij −yi· −y·j + y··)2.
8.20 Prove Theorem 8.8.
8.21 Let A1, A2, A3, and A4 be square matrices. Show that, when the sizes of these
matrices are such that the appropriate operations are defined,
(a) (A1 ⊕A2) + (A3 ⊕A4) = (A1 + A3) ⊕(A2 + A4),
(b) (A1 ⊕A2)(A3 ⊕A4) = A1A3 ⊕A2A4,
(c) (A1 ⊕A2) ⊗A3 = (A1 ⊗A3) ⊕(A2 ⊗A3).
8.22 Give an example to show that, in general,
A1 ⊗(A2 ⊕A3) ̸= (A1 ⊗A2) ⊕(A1 ⊗A3).
8.23 Use Theorem 6.4 and Theorem 8.11 to prove Theorem 6.5.
8.24 Consider the system of equations AX −XB = C, where X is an m × n
matrix of variables and A, B, and C are matrices of constants. Show that
if the matrices A and B do not have any eigenvalues in common, then this
system has a unique solution for X.
8.25 Prove the results of Corollary 8.12.1.
8.26 Let A and B be m × n and n × p matrices, respectively, whereas c and d are
p × 1 and n × 1 vectors, respectively. Show that
(a) ABc = (c′ ⊗A)vec(B) = (A ⊗c′)vec(B′),
(b) d′Bc = (c′ ⊗d′)vec(B).
8.27 Let A, B, and C be m × m matrices. Show that if C is symmetric, then
{vec(C)}′(A ⊗B)vec(C) = {vec(C)}′(B ⊗A)vec(C).
8.28 For any matrix A and any vector b, show that
vec(A ⊗b) = vec(A) ⊗b.

376
SPECIAL MATRICES AND MATRIX OPERATIONS
8.29 Let A be an m × n matrix and B be an n × p matrix. Show that
vec(AB) = (Ip ⊗A)vec(B) = (B′ ⊗Im)vec(A)
= (B′ ⊗A)vec(In).
8.30 Let A be an m × m matrix, B be an n × n matrix, and C be an m × n matrix.
Prove that
vec(AC + CB) = {(In ⊗A) + (B′ ⊗Im)}vec(C).
8.31 Let A and B be m × n matrices. Show that
{tr(A′B)}2 ≤{tr(A′A)}{tr(B′B)}
with equality if and only if one of the matrices is a scalar multiple of the other.
8.32 Let A be an m × m symmetric matrix, and consider the function of A defined
by f(A) = tr(A2) −m−1{tr(A)}2. Show that f(A) can be expressed as
f(A) = {vec(A)}′{Im2 −m−1 vec(Im)vec(Im)′}vec(A).
8.33 If ei is the ith column of the identity matrix Im, verify that
vec(Im) =
m

i=1
(ei ⊗ei).
8.34 Prove property (h) of Theorem 8.13.
8.35 Let the 2 × 2 matrices A and B be given by
A =
1 2
2 4
	
,
B =
4 1
1 3
	
.
(a) Compute A ⊙B.
(b) Which of the matrices, A, B, and A ⊙B, are positive definite or positive
semidefinite? How does this relate to Theorem 8.17?
8.36 Give an example of matrices A and B such that neither is nonnegative definite,
yet A ⊙B is positive definite.
8.37 Let A, B, and C be m × n matrices. Show that
tr{(A′ ⊙B′)C} = tr{A′(B ⊙C)}.

PROBLEMS
377
8.38 Suppose that the m × m matrix A is diagonalizable; that is, a nonsingular
matrix X and a diagonal matrix Λ = diag(λ1, . . . , λm) exist, such that A =
XΛX−1. Show that if we define the vector of diagonal elements of A, a =
(a11, . . . , amm)′, and the vector of eigenvalues of A, λ = (λ1, . . . , λm)′,
then
(X ⊙X−1′)λ = a
and
(X ⊙X−1′)1m = (X ⊙X−1′)′1m = 1m.
8.39 Let A and B be m × m nonnegative definite matrices. Show that
(a) |A ⊙B| ≥|A||B|,
(b) |A ⊙A−1| ≥1, if A is positive definite.
8.40 Let A be an m × m matrix. Use Theorem 8.15 to show that A is nonnega-
tive definite if and only if tr(AB′) ≥0 for every m × m nonnegative definite
matrix B.
8.41 For each of the following pairs of 2 × 2 matrices, compute the smaller eigen-
value λ2(A ⊙B) and the lower bounds for this eigenvalue given by Theorem
8.21 and Theorem 8.23. Which bound is closer to the actual value?
(a)
A =
4 0
0 1
	
,
B =
2 0
0 3
	
.
(b)
A =
1 0
0 1
	
,
B =
 2
√
2
√
2
3
	
.
8.42 Let A be an m × m positive definite matrix. Use Theorem 8.19 to show that
if B = A−1, then a11b11 ≥1. Show how this result generalizes to aiibii ≥1
for i = 1, . . . , m.
8.43 Let A and B be m × m positive definite matrices, and consider the inequality
|A ⊙B| + |A||B| ≥|A|
m

i=1
bii + |B|
m

i=1
aii.
(a) Show that this inequality is equivalent to
|RA ⊙RB| + |RA||RB| ≥|RA| + |RB|,
where RA and RB represent the correlation matrices computed from A
and B.
(b) Use Theorem 8.20 on |RA ⊙C|, where C = RB −(e′
1R−1
B e1)−1e1e′
1,
to establish the inequality given in (a).

378
SPECIAL MATRICES AND MATRIX OPERATIONS
8.44 Suppose that A and B are m × m positive definite matrices. Show that
A ⊙B = AB if and only if both A and B are diagonal matrices.
8.45 Let A be an m × m positive definite matrix and B be an m × m positive
semidefinite matrix with exactly r positive diagonal elements. Show that
rank(A ⊙B) = r.
8.46 Show that if A and B are singular 2 × 2 matrices, then A ⊙B is also singular.
8.47 Let R be an m × m positive definite correlation matrix having λ as its smallest
eigenvalue. Show that if τ is the smallest eigenvalue of R ⊙R and R ̸= Im,
then τ > λ.
8.48 Consider the matrix
Ψm =
m

i=1
ei,m(ei,m ⊗ei,m)′,
which we have seen satisfies Ψm(A ⊗B)Ψ′
m = A ⊙B for any m × m
matrices A and B. Define w(A) to be the m × 1 vector containing the
diagonal elements of A; that is, w(A) = (a11, . . . , amm)′. Also let Λm be
the m2 × m2 matrix given by
Λm =
m

i=1
(Eii ⊗Eii) =
m

i=1
(ei,me′
i,m ⊗ei,me′
i,m).
Show that
(a) Ψ′
mw(A) = vec(A) for every diagonal matrix A,
(b) Ψmvec(A) = w(A) for every matrix A,
(c) ΨmΨ′
m = Im so that Ψ+
m = Ψ′
m,
(d) Ψ′
mΨm = Λm,
(e) ΛmNm = NmΛm = Λm,
(f) {vec(A)}′Λm(B ⊗B)Λm vec(A) = {w(A)}′(B ⊙B)w(A).
Additional properties of Ψm can be found in Magnus (1988).
8.49 Let A and B be m × m positive definite matrices. Since Ψm(A ⊗B)Ψ′
m =
A ⊙B and ΨmΨ′
m = Im, it follows that an m2 × m2 orthogonal matrix P
exists, such that P(A ⊗B)P ′ can be partitioned into the 2 × 2 form of (7.1)
with the (1, 1)th submatrix given by A ⊙B. Use this result and the result from
Problem 7.10 to show that
(a) A−1 ⊙B−1 −(A ⊙B)−1 is nonnegative definite,
(b) A−1 ⊙A−1 −(A ⊙A)−1 is nonnegative definite,
(c) A−1 ⊙A −(A−1 ⊙A)−1 is nonnegative definite.
8.50 Verify that the commutation matrix Kmn is a permutation matrix; that is, show
that each column of Kmn is a column of Imn and each column of Imn is a
column of Kmn.
8.51 Write out the commutation matrices K22 and K24.

PROBLEMS
379
8.52 The eigenvalues of Kmm were given in Theorem 8.28. Show that correspond-
ing eigenvectors are given by the vectors of the form el ⊗el, (el ⊗ek) +
(ek ⊗el), and (el ⊗ek) −(ek ⊗el).
8.53 Show that the commutation matrix Kmn can be expressed as
Kmn =
m

i=1
(ei ⊗In ⊗e′
i),
where ei is the ith column of Im. Use this to show that if A is n × m, x is
m × 1, and y is an arbitrary vector, then
K′
mn(x ⊗A ⊗y′) = A ⊗xy′.
8.54 Show that
(a) Knp,m = Kn,pmKp,nm = Kp,nmKn,pm,
(b) Knp,mKpm,nKmn,p = Imnp.
8.55 Let A be an m × n matrix with rank r, and let λ1, . . . , λr be the nonzero
eigenvalues of A′A. If we define
P = Kmn(A′ ⊗A),
show that
(a) P is symmetric,
(b) rank(P) = r2,
(c) tr(P) = tr(A′A),
(d) P 2 = (AA′) ⊗(A′A),
(e) the nonzero eigenvalues of P are λ1, . . . , λr and ±(λiλj)1/2 for all i < j.
8.56 Let A be an m × n matrix and B be a p × q matrix. Show that
(a) vec(A′ ⊗B) = (Kmq,n ⊗Ip){vec(A) ⊗vec(B)},
(b) vec(A ⊗B′) = (In ⊗Kp,mq){vec(A) ⊗vec(B)}.
8.57 Suppose that A is an m × n matrix and B is a p × q matrix with mp = nq.
Show that
tr(A ⊗B) = {vec(In) ⊗vec(Iq)}′{vec(A) ⊗vec(B′)}.
8.58 Show that
(a) Kmnp,q = (Imn ⊗Kpq)(Im ⊗Knq ⊗Ip)(Kmq ⊗Inp),
(b) Kmn,pq = (Im ⊗Knp ⊗Iq)(Kmp ⊗Knq)(Ip ⊗Kmq ⊗In).
8.59 Prove the results of Theorem 8.31.

380
SPECIAL MATRICES AND MATRIX OPERATIONS
8.60 Show that if A and B are m × m matrices, then
Nm(A ⊗B + B ⊗A)Nm = (A ⊗B + B ⊗A)Nm
= Nm(A ⊗B + B ⊗A)
= 2Nm(A ⊗B)Nm.
8.61 Consider the matrix Nm = 1
2(Im2 + Kmm).
(a) Show that Nm can be expressed as
Nm = 1
2
m

i=1
m

j=1
(eie′
i ⊗eje′
j + eie′
j ⊗eje′
i).
(b) Show that
Nm(a ⊗b) = 1
2(a ⊗b + b ⊗a)
for any m × 1 vectors a and b.
(c) Let Δ be the matrix that generalizes the property illustrated in (b) to the
Kronecker product of three m × 1 vectors, a ⊗b ⊗c; that is, suppose Δ
satisfies
Δ(a ⊗b ⊗c) = 1
6(a ⊗b ⊗c + a ⊗c ⊗b + b ⊗a ⊗c
+ b ⊗c ⊗a + c ⊗a ⊗b + c ⊗b ⊗a).
Show that Δ can be expressed as
Δ = 1
6
m

h=1
m

i=1
m

j=1
(ehe′
h ⊗eie′
i ⊗eje′
j + ehe′
h ⊗eie′
j ⊗eje′
i
+ ehe′
i ⊗eie′
h ⊗eje′
j + ehe′
j ⊗eie′
h ⊗eje′
i
+ ehe′
i ⊗eie′
j ⊗eje′
h + ehe′
j ⊗eie′
i ⊗eje′
h).
8.62 Write out the matrices N2 and N3.
8.63 For i = 1, . . . , m, j = 1, . . . , i, define the m(m + 1)/2 × 1 vector uij to be
the vector with one in its {(j −1)m + i −j(j −1)/2}th position and zeros
elsewhere. It can be easily verified that these vectors are the columns of the
identity matrix of order m(m + 1)/2; that is,
Im(m+1)/2 = (u11, u21, . . . , um1, u22, . . . , um2, u33, . . . , umm).

PROBLEMS
381
Let Eij be the m × m matrix whose only nonzero element is a one in the
(i, j)th position, and define
Tij =
Eij + Eji, if i ̸= j,
Eii,
if i = j.
Show that Dm = 
i≥j{vec(Tij)}u′
ij; that is, verify that

i≥j
{vec(Tij)}u′
ijv(A) = vec(A),
where A is an arbitrary m × m symmetric matrix.
8.64 Use the expression given for Dm in Problem 8.63 to prove Theorem 8.34(b).
8.65 Let uij, i = 1, . . . , m, j = 1, . . . , i be the m(m + 1)/2 × 1 vectors defined
in Problem 8.63. Show that
(a) D′
mDm = 2Im(m+1)/2 −m
i=1 uiiu′
ii,
(b) |D′
mDm| = 2m(m−1)/2.
8.66 Prove the results of Theorem 8.37.
8.67 If A is an m × m matrix, show that
(a) DmD+
m(A ⊗A)Dm = (A ⊗A)Dm,
(b) {D+
m(A ⊗A)Dm}i = D+
m(Ai ⊗Ai)Dm, where i is any positive inte-
ger.
8.68 Let A be an m × m nonsingular symmetric matrix and α be a scalar. Show
that
(D′
m{A ⊗A + αvec(A)vec(A)′}Dm)−1
= D+
m{A−1 ⊗A−1 −β vec(A−1)vec(A−1)′}D+′
m,
where β = α/(1 + mα).
8.69 If uij and Eij are defined as in Problem 8.63, show that
L′
m =

i≥j
{vec(Eij)}u′
ij;
that is, verify that

i≥j
{vec(Eij)}u′
ijv(A) = vec(A),
where A is an arbitrary m × m lower triangular matrix.
8.70 Prove the following results:
(a) L′
mLm = 
i≥j(Ejj ⊗Eii), where Eii was defined in Problem 8.63.

382
SPECIAL MATRICES AND MATRIX OPERATIONS
(b) If A and B are m × m lower triangular matrices, then
L′
mLm(A′ ⊗B)L′
m = (A′ ⊗B)L′
m.
(c) If A is an m × m nonsingular lower triangular matrix, α is a scalar, and
β = α/(1 + mα), then
(Lm{A′ ⊗A + αvec(A)vec(A′)′}L′
m)−1
= Lm{A−1′ ⊗A−1 −βvec(A−1)vec(A−1′)′}L′
m.
8.71 Prove Theorem 8.38.
8.72 For i = 2, . . . , m, j = 1, . . . , i −1, define the m(m −1)/2 × 1 vector ˜uij
to be the vector with one in its {(j −1)m + i −j(j + 1)/2}th position and
zeros elsewhere. It can be easily verified that these vectors are the columns of
the identity matrix of order m(m −1)/2; that is,
Im(m−1)/2 = (˜u21, . . . , ˜um1, ˜u32, . . . , ˜um2, ˜u43, . . . , ˜um,m−1).
Show that ˜L′
m = 
i>j{vec(Eij)}˜u′
ij; that is, verify that

i>j
{vec(Eij)}˜u′
ij˜v(A) = vec(A),
where A is an arbitrary m × m strictly lower triangular matrix.
8.73 Prove the results of Theorem 8.39.
8.74 Find a 2 × 2 nonnegative matrix A that has its spectral radius equal to 1, yet
Ak does not converge to anything as k →∞.
8.75 Show that the inverse of a nonsingular positive matrix cannot be nonnegative.
Show that the inverse of a nonsingular nonnegative matrix A can be nonneg-
ative only if A has exactly one nonzero element in each column.
8.76 Show that if A is a nonnegative matrix and, for some positive integer k, Ak is
a positive matrix, then ρ(A) > 0.
8.77 It can be shown (see, for example, Horn and Johnson, 2013) that if A is an
m × m nonnegative matrix, then ρ(A) is an eigenvalue of A and a nonneg-
ative eigenvector x corresponding to the eigenvalue ρ(A) exists. This result
is weaker than the result for irreducible nonnegative matrices. For each of
the following, find a 2 × 2 nonnull reducible matrix A, such that the stated
condition holds.
(a) ρ(A) = 0.
(b) x is not positive for any x satisfying Ax = ρ(A)x.
(c) ρ(A) is a multiple eigenvalue.

PROBLEMS
383
8.78 Verify that the absolute value of each of the eigenvalues of the 2 × 2 irre-
ducible matrix
A =
0 1
1 0
	
is equal to ρ(A).
8.79 We have seen in Theorem 8.41 that ρ(A) is an eigenvalue of A if A is positive.
In this exercise, we will use the extension of this result that says ρ(A) is an
eigenvalue of A if A is nonnegative (see Horn and Johnson, 2013). In the
following, assume that A is an m × m nonnegative matrix.
(a) Show that ρ(Im + A) = 1 + ρ(A).
(b) Show that if Ak > (0) for some positive integer k, then ρ(A) is a simple
eigenvalue of A.
(c) Apply part (b) on the matrix (Im + A) to prove Theorem 8.49; that is,
prove that for any irreducible nonnegative matrix A, ρ(A) must be a sim-
ple eigenvalue.
8.80 Consider the homogeneous Markov chain that has three states and the matrix
of transition probabilities given by
P =
⎡
⎣
0.50 0.25
0
0.50 0.50 0.25
0
0.25 0.75
⎤
⎦.
(a) Show that P is primitive.
(b) Determine the equilibrium distribution; that is, find the vector π such that
limt→∞p(t) = π.
8.81 An m × m matrix A is said to be completely positive (see, for example,
Berman and Shaked-Monderer, 2003) if it can be expressed as A = BB′
for some m × r nonnegative matrix B, where r ≤m. Clearly every com-
pletely positive matrix must be a nonnegative definite matrix and a nonnega-
tive matrix. Show that in the 2 × 2 case, these conditions are also sufficient;
that is, show that a 2 × 2 matrix A that is nonnegative definite and nonnegative
will also be completely positive.
8.82 Let A be the m × m circulant matrix circ(a1, . . . , am).
(a) Find the trace of A.
(b) Find the determinant of A.
8.83 Show that the conjugate transpose of the matrix F given in Theorem 8.56 is
F ∗=
1
√m
⎡
⎢⎢⎢⎢⎢⎣
1
1
1
· · ·
1
1
θ−1
θ−2
· · ·
θ−(m−1)
1
θ−2
θ−4
· · ·
θ−2(m−1)
...
...
...
...
1 θ−(m−1) θ−2(m−1) · · ·
θ−(m−1)(m−1)
⎤
⎥⎥⎥⎥⎥⎦
.

384
SPECIAL MATRICES AND MATRIX OPERATIONS
Then use the geometric series partial sum formula
n

j=0
rj = 1 −rn+1
1 −r
to prove that F −1 = F ∗.
8.84 Let F be defined as in Theorem 8.56, and let Γ = (e1, em, em−1, . . . , e2).
Show that
(a) F 2 = Γ,
(b) F 4 = Im,
(c) F 3 = F ∗.
8.85 Let Πm be the circulant matrix defined in Section 8.9. Show that
(a) Πm−1
m
= Π−1
m ,
(b) Πm
m = Im,
(c) Πmn+r
m
= Πr
m, for any integers n and r.
8.86 If A = circ(a1, . . . , am) and B = circ(b1, . . . , bm), find the eigenvalues of
A + B and AB.
8.87 Find the eigenvalues of the circulant matrix A = circ(1, . . . , 1) by using
Theorem 8.57.
8.88 Show that if A is a singular circulant matrix, then its Moore–Penrose inverse,
A+, is also a circulant matrix.
8.89 Find square matrices A and B of the same order, such that A and B are not
circulant matrices, yet their product AB is a circulant matrix.
8.90 Let B be the m × m Jordan block matrix Jm(0). Show that an m × m matrix
A is a Toeplitz matrix if and only if it can be written in the form
A = a0Im +
m−1

j=1
(ajBj + a−jBj′).
8.91 Consider the m × m Toeplitz matrix
A =
⎡
⎢⎢⎢⎢⎢⎣
1
b
b2
· · ·
bm−1
a
1
b
· · ·
bm−2
a2
a
1
· · ·
bm−3
...
...
...
...
am−1 am−2 am−3 · · ·
1
⎤
⎥⎥⎥⎥⎥⎦
,

PROBLEMS
385
where ab ̸= 1. Verify by multiplication that the inverse of A is given by
A−1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
c
−bc
0
· · ·
0
0
−ac (ab + 1)c
−bc
· · ·
0
0
0
−ac
(ab + 1)c · · ·
0
0
...
...
...
...
...
0
0
0
· · ·
(ab + 1)c −bc
0
0
0
· · ·
−ac
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
where c = (1 −ab)−1. Show that A is singular if ab = 1.
8.92 Suppose that z1, . . . , zm+1 are independent random variables each having
mean 0 and variance 1. Let x be the m × 1 random vector that has as its ith
component,
xi = zi+1 −ρzi,
where ρ is a constant. Show that the covariance matrix of x is a Toeplitz matrix
of the form given in (8.24), and find the values of a0 and a1.
8.93 Consider the m × m symmetric Toeplitz matrix given by
A =
⎡
⎢⎢⎢⎢⎢⎣
1
ρ
ρ2
· · ·
ρm−1
ρ
1
ρ
· · ·
ρm−2
ρ2
ρ
1
· · ·
ρm−3
...
...
...
...
ρm−1 ρm−2 ρm−3 · · ·
1
⎤
⎥⎥⎥⎥⎥⎦
,
where 0 < ρ < 1. Use Theorem 7.6 to show that A is positive definite.
8.94 Find a Hadamard matrix of order 8.
8.95 Give a Hadamard matrix of order 12, thereby illustrating the existence of a
Hadamard matrix of order m, where m ̸= 2n for any positive integer n.
8.96 Show that the determinant of a Hadamard matrix attains the upper bound of
the Hadamard inequality given in Corollary 8.18.1.
8.97 Let A, B, C, and D be m × m matrices with all of their elements equal to
+1 and −1, and define H as
H =
⎡
⎢⎢⎣
A
B
C
D
−B
A −D
C
−C
D
A
−B
−D −C
B
A
⎤
⎥⎥⎦.

386
SPECIAL MATRICES AND MATRIX OPERATIONS
Show that if
AA′ + BB′ + CC′ + DD′ = 4mIm
and
XY ′ = Y X′
for every pair of matrices X and Y chosen from A, B, C, and D, then H is a
Hadamard matrix of order 4m.
8.98 Let H and E be Hadamard matrices of order 4m and 4n, respectively. Parti-
tion these matrices as
H =

P Q
R S
	
,
E =

K
L
M N
	
,
where each submatrix of H is 2m × 2m and each submatrix of E is 2n × 2n.
Show that if F is defined as

(P + Q) ⊗K + (P −Q) ⊗M (P + Q) ⊗L + (P −Q) ⊗N
(R + S) ⊗K + (R −S) ⊗M
(R + S) ⊗L + (R −S) ⊗N
	
,
then 1
2F is a Hadamard matrix of order 8mn.
8.99 Show that when m = n, the Vandermonde matrix A given in (8.27) is non-
singular if and only if the m elements of the second row are distinct.
8.100 Let A be the Vandermonde matrix given in (8.27) with m = n. Prove that if
there are r distinct values in the set {a1, . . . , am}, then rank(A) = r.
8.101 Let P be the m × m orthogonal matrix (em, em−1, . . . , e1). Show that if A is
an m × m Vandermonde matrix, then PAA′ and AA′P are Toeplitz matrices.

9
MATRIX DERIVATIVES AND
RELATED TOPICS
9.1
INTRODUCTION
Differential calculus has widespread applications in statistics. For example, estima-
tion procedures such as the maximum likelihood method and the method of least
squares use the optimization properties of derivatives, whereas the so-called delta
method for obtaining the asymptotic distribution of a function of random variables
uses the first derivative to obtain a first-order Taylor series approximation. These and
other applications of differential calculus often involve vectors or matrices. In this
chapter, we obtain some of the most commonly encountered matrix derivatives.
9.2
MULTIVARIABLE DIFFERENTIAL CALCULUS
We will begin with a brief review of some of the basic notation, concepts, and results
of elementary and multivariable differential calculus. Throughout this section, we
will assume differentiability or multiple differentiability of the functions we discuss.
For more details on the conditions for differentiability, see Magnus and Neudecker
(1999). If f is a real-valued function of one variable, x, then its derivative at x, if it
exists, is given by
f (1)(x) = f ′(x) = d
dxf(x) = lim
u→0
f(x + u) −f(x)
u
.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

388
MATRIX DERIVATIVES AND RELATED TOPICS
Equivalently, f ′(x) is the quantity that gives the first-order Taylor formula for
f(x + u). In other words,
f(x + u) = f(x) + uf ′(x) + r1(u, x),
(9.1)
where the remainder r1(u, x) is a function of u and x satisfying
lim
u→0
r1(u, x)
u
= 0.
The quantity
duf(x) = uf ′(x)
(9.2)
appearing in (9.1) is called the first differential of f at x with increment u. This
increment u is the differential of x. Later we will use dx in place of u, that is, write
f(x + dx) instead of f(x + u), to emphasize the fact that u is the differential of
x. For notational convenience, we will often denote the differential given in (9.2)
simply by df. Generalizations of (9.1) can be obtained by taking higher ordered
derivatives; that is, with the ith derivative of f at x defined as
f (i)(x) = di
dxi f(x) = lim
u→0
f (i−1)(x + u) −f (i−1)(x)
u
,
we have the kth-order Taylor formula
f(x + u) = f(x) +
k

i=1
uif (i)(x)
i!
+ rk(u, x)
= f(x) +
k

i=1
di
uf(x)
i!
+ rk(u, x),
where rk(u, x) is a function of u and x satisfying
lim
u→0
rk(u, x)
uk
= 0,
and
di
uf(x) = uif (i)(x),
or simply dif, is the ith differential of f at x with increment u.
The chain rule is a useful formula for calculating the derivative of a composite
function. If y, g, and f are functions such that y(x) = g(f(x)), then
y′(x) = g′(f(x))f ′(x).
(9.3)

MULTIVARIABLE DIFFERENTIAL CALCULUS
389
If f is a real-valued function of the n × 1 vector x = (x1,. . ., xn)′, then its
derivative at x, if it exists, is given by the 1 × n row vector
∂
∂x′ f(x) =
 ∂
∂x1 f(x) · · ·
∂
∂xn f(x)
,
where
∂
∂xi
f(x) = lim
ui→0
f(x + uiei) −f(x)
ui
is the partial derivative of f with respect to xi, and ei is the ith column of In. The
first-order Taylor formula analogous to (9.1) is given by
f(x + u) = f(x) +
 ∂
∂x′ f(x)

u + r1(u, x),
(9.4)
where the remainder, r1(u, x), satisfies
lim
u→0
r1(u, x)
(u′u)1/2 = 0.
The second term on the right-hand side of (9.4) is the first differential of f at x with
incremental vector u; that is,
df = duf(x) =
 ∂
∂x′ f(x)

u =
n

i=1
ui
∂
∂xi
f(x).
It is important to note the relationship between the first differential and the first
derivative; the first differential of f at x in u is the first derivative of f at x times u.
The higher order differentials of f at x in the vector u are given by
dif = di
uf(x) =
n

j1=1
· · ·
n

ji=1
uj1 · · · uji
∂i
∂xj1 · · · ∂xji
f(x),
and these differentials appear in the kth-order Taylor formula,
f(x + u) = f(x) +
k

i=1
dif
i! + rk(u, x),
where the remainder rk(u, x) satisfies
lim
u→0
rk(u, x)
(u′u)k/2 = 0.
The second differential, d2f, can be written as a quadratic form in the vector u; that is,
d2f = u′Hfu,

390
MATRIX DERIVATIVES AND RELATED TOPICS
where Hf, called the Hessian, is the matrix of second-order partial derivatives given
by
Hf =
⎡
⎢⎢⎢⎢⎢⎣
∂2
∂x2
1 f(x)
∂2
∂x1∂x2 f(x) · · ·
∂2
∂x1∂xn f(x)
∂2
∂x2∂x1 f(x)
∂2
∂x2
2 f(x)
· · ·
∂2
∂x2∂xn f(x)
...
...
...
∂2
∂xn∂x1 f(x)
∂2
∂xn∂x2 f(x) · · ·
∂2
∂x2n f(x)
⎤
⎥⎥⎥⎥⎥⎦
.
9.3
VECTOR AND MATRIX FUNCTIONS
Suppose now that f1,. . ., fm each is a function of the same n × 1 vector
x = (x1,. . ., xn)′. These m functions can be conveniently expressed as components
of the vector function
f(x) =
⎡
⎢⎣
f1(x)
...
fm(x)
⎤
⎥⎦.
The function f is differentiable at x if and only if each component function fi is
differentiable at x. The Taylor formulas from the previous section can be applied
component-wise to f. For instance, the first-order Taylor formula is given by
f(x + u) = f(x) +
 ∂
∂x′ f(x)

u + r1(u, x)
= f(x) + d f(x) + r1(u, x),
where the vector remainder r1(u, x) satisfies
lim
u→0
r1(u, x)
(u′u)1/2 = 0
and the first derivative of f at x is given by the m × n matrix
∂
∂x′ f(x) =
⎡
⎢⎢⎢⎢⎢⎣
∂
∂x1 f1(x)
∂
∂x2 f1(x) · · ·
∂
∂xn f1(x)
∂
∂x1 f2(x)
∂
∂x2 f2(x) · · ·
∂
∂xn f2(x)
...
...
...
∂
∂x1 fm(x)
∂
∂x2 fm(x) · · ·
∂
∂xn fm(x)
⎤
⎥⎥⎥⎥⎥⎦
.
This matrix of partial derivatives is sometimes referred to as the Jacobian matrix of
f at x. Again, it is crucial to understand the relationship between the first differential
and the first derivative. If we obtain the first differential of f at x in u and write it in
the form
df = Bu,
then the m × n matrix B must be the derivative of f at x .

VECTOR AND MATRIX FUNCTIONS
391
If y and g are real-valued functions satisfying y(x) = g(f(x)), then the general-
ization of the chain rule given in (9.3) is
∂
∂xi
y(x) =
m

j=1
 ∂
∂fj
g( f)
  ∂
∂xi
fj(x)

=
 ∂
∂f ′ g( f)
  ∂
∂xi
f(x)

for i = 1,. . ., n, or simply
∂
∂x′ y(x) =
 ∂
∂f ′ g( f)
  ∂
∂x′ f(x)

.
In some applications, the fj’s or the xi’s are arranged in a matrix instead of a
vector. Thus, the most general case involves the p × q matrix function
F(X) =
⎡
⎢⎢⎢⎢⎢⎣
f11(X) f12(X) · · ·
f1q(X)
f21(X) f22(X) · · ·
f2q(X)
...
...
...
fp1(X) fp2(X) · · ·
fpq(X)
⎤
⎥⎥⎥⎥⎥⎦
of the m × n matrix X. Results for the vector function f(x) can be easily extended
to the matrix function F(X) by using the vec operator; that is, let f be the pq × 1
vector function such that f(vec(X)) = vec(F(X)). Then, for instance, the Jacobian
matrix of F at X is given by the pq × mn matrix
∂
∂vec(X)′ f(vec(X)) =
∂
∂vec(X)′ vec(F(X)),
which has as its (i, j)th element the partial derivative of the ith element of vec(F(X))
with respect to the jth element of vec(X). This could then be used to obtain the
first-order Taylor formula for vec(F(X + U)). The differentials of the matrix F(X)
are defined by the equations
vec(diF) = vec(di
UF(X)) = dif = di
vec(U) f(vec(X));
that is, diF, the ith-order differential of F at X in the incremental matrix U, is defined
as the p × q matrix obtained by unstacking the ith-order differential of f at vec(X)
in the incremental vector vec(U).
Basic properties of vector and matrix differentials follow in a fairly straightforward
fashion from the corresponding properties of scalar differentials. We will summarize
some of these properties here. If x and y are functions and α is a constant, then the
differential operator, d, satisfies

392
MATRIX DERIVATIVES AND RELATED TOPICS
(a) dα = 0,
(b) d(αx) = αdx,
(c) d(x + y) = dx + dy,
(d) d(xy) = (dx)y + x(dy),
(e) dxα = αxα−1dx,
(f) dex = exdx,
(g) d log(x) = x−1dx.
For instance, to illustrate property (d), note that
(x + dx)(y + dy) = xy + x(dy) + (dx)y + (dx)(dy),
and d(xy) will be given by the first-degree term in dx and dy, which is (dx)y + x(dy)
as required. Using these properties and the definition of a matrix differential, it is
easily shown that if X and Y are matrix functions and A is a matrix of constants,
then
(h) dA = (0),
(i) d(αX) = αdX,
(j) d(X′) = (dX)′,
(k) d(X + Y ) = dX + dY ,
(l) d(XY ) = (dX)Y + X(dY ),
(m) d tr(X) = tr(dX),
(n) d vec(X) = vec(dX),
(o) d(X ⊗Y ) = (dX) ⊗Y + X ⊗(dY ),
(p) d(X ⊙Y ) = (dX) ⊙Y + X ⊙(dY ).
We will verify property (l). Thus, we must show that the (i, j)th element of the
matrix on the left-hand side of the equation, (d(XY ))ij, is the same as the (i, j)th
element on the right-hand side, (dX)i·(Y )·j + (X)i·(dY )·j, where X is m × n and
Y is n × m. Using properties (c) and (d), we find that
(d(XY ))ij = d{(X)i·(Y )·j} = d
 n

k=1
xikykj

=
n

k=1
d(xikykj) =
n

k=1
{(dxik)ykj + xikdykj}
=
n

k=1
(dxik)ykj +
n

k=1
xikdykj
= (dX)i·(Y )·j + (X)i·(dY )·j,
and so (l) is proven.

VECTOR AND MATRIX FUNCTIONS
393
We illustrate the application of some of these properties first by finding the deriva-
tives of some simple scalar functions of a vector x, and then by finding the derivatives
of some simple matrix functions of a matrix X.
Example 9.1
Let x be an m × 1 vector of unrelated variables, and define the
functions
f(x) = a′x,
where a is an m × 1 vector of constants, and
g(x) = x′Ax,
where A is an m × m symmetric matrix of constants. The hth component of the
1 × m row vector ∂f/∂x′ is ∂f/∂xh and
∂
∂xh
f =
∂
∂xh
m

i=1
aixi =
m

i=1
ai
 ∂
∂xh
xi

= ah,
because
∂
∂xh
xi =

1,
if i = h,
0,
if i ̸= h.
This then implies that
∂
∂x′ f = a′.
In a similar fashion, we compute the hth component of the 1 × m row vector ∂g/∂x′
as
∂
∂xh
g =
∂
∂xh
m

i=1
m

j=1
aijxixj =
m

i=1
m

j=1
aij
 ∂
∂xh
xixj

=
m

i=1
m

j=1
aij
 ∂
∂xh
xi

xj + xi
 ∂
∂xh
xj

=
m

j=1
ahjxj +
m

i=1
aihxi
=
m

j=1
ajhxj +
m

i=1
aihxi = 2
m

i=1
aihxi,
because ajh = ahj. Note that this derivative can be written as 2x′(A)·h, so
∂
∂x′ g = 2x′A.

394
MATRIX DERIVATIVES AND RELATED TOPICS
An alternative approach to computing these derivatives, one that we will use in most
of our examples, involves direct calculation of the differential. As a by-product, we
obtain the derivative. For instance, the differential of the first function is
df = d(a′x) = a′dx.
Since this differential and the derivative are related through the equation
df =
 ∂
∂x′ f

dx,
we immediately observe that the derivative is given by
∂
∂x′ f = a′.
The differential of our second function is given by
dg = d(x′Ax) = d(x′)Ax + x′d(Ax) = (dx)′Ax + x′Adx
= {(dx)′Ax}′ + x′Adx = x′A′dx + x′Adx = 2x′Adx.
By again making use of the relationship between the differential and the derivative,
we observe that
∂
∂x′ g = 2x′A.
Example 9.2
Let X be an m × n matrix of unrelated variables, and define the func-
tions
F(X) = AX,
where A is a p × m matrix of constants, and
G(X) = (X −C)′B(X −C),
where B is an m × m symmetric matrix of constants and C is an m × n matrix of
constants. We will find the Jacobian matrices by first obtaining the differentials of
these functions. The derivatives can be obtained from these differentials because, for
instance, if we obtain
d vec(F) = Wd vec(X),
then the matrix W will be the derivative of vec(F(X)) with respect to vec(X). For
our first function, we find that
dF = d(AX) = AdX,

VECTOR AND MATRIX FUNCTIONS
395
so that
d vec(F) = vec(dF) = vec(AdX) = (In ⊗A) vec(dX)
= (In ⊗A)d vec(X),
where we have used Theorem 8.11. Thus, we must have
∂
∂vec(X)′ vec(F) = In ⊗A.
The differential of our second function is
dG = d{(X −C)′B(X −C)}
= {d(X′ −C′)}B(X −C) + (X −C)′B{d(X −C)}
= (dX)′B(X −C) + (X −C)′BdX,
from which we obtain
d vec(G) = {(X −C)′B ⊗In} vec(dX′) + {In ⊗(X −C)′B} vec(dX)
= {(X −C)′B ⊗In}Kmn vec(dX) + {In ⊗(X −C)′B} vec(dX)
= Knn{In ⊗(X −C)′B} vec(dX) + {In ⊗(X −C)′B} vec(dX)
= (In2 + Knn){In ⊗(X −C)′B} vec(dX)
= 2Nn{In ⊗(X −C)′B}d vec(X),
where we have used properties of the vec operator and the commutation matrix devel-
oped in Chapter 8. Consequently, we have
∂
∂vec(X)′ vec(G) = 2Nn{In ⊗(X −C)′B}.
In Example 9.3, we show how we can use the Jacobian matrix of the simple trans-
formation z = c + Ax to obtain the multivariate normal density function given in
(1.13).
Example 9.3
Suppose that z is an m × 1 random vector with density function f1(z)
that is positive for all z ∈S1 ⊆Rm. Let the m × 1 vector x = x(z) represent a
one-to-one transformation of S1 onto S2 ⊆Rm, so that the inverse transformation
z = z(x), x ∈S2 is unique. Denote the Jacobian matrix of z at x as
J =
∂
∂x′ z(x).

396
MATRIX DERIVATIVES AND RELATED TOPICS
If the partial derivatives in J exist and are continuous functions on the set S2, then
the density of x is given by
f2(x) = f1(z(x)) |J|.
We will use the formula above to obtain the multivariate normal density, given
in (1.13), from the standard normal density. Now recall that, by definition,
x ∼Nm(μ, Ω) if x can be expressed as x = μ + Tz, where TT ′ = Ω and the
components of z, z1,. . ., zm are independently distributed each as N(0, 1). Thus,
the density function of z is given by
f1(z) =
m

i=1
1
√
2π exp

−1
2z2
i

=
1
(2π)m/2 exp

−1
2z′z

.
The differential of the inverse transformation z = T −1(x −μ) is dz = T −1dx, and
so the necessary Jacobian matrix is J = T −1. Consequently, we find that the density
of x is given by
f2(x) =
1
(2π)m/2 exp

−1
2{T −1(x −μ)}′T −1(x −μ)

|T −1|
=
1
(2π)m/2|T| exp

−1
2(x −μ)′T −1′T −1(x −μ)

=
1
(2π)m/2|Ω|1/2 exp

−1
2(x −μ)′Ω−1(x −μ)

,
because
Ω−1 = (TT ′)−1 = T −1′T −1
and
|Ω|1/2 = |TT ′|1/2 = |T|1/2|T ′|1/2 = |T|1/2|T|1/2 = |T|.
9.4
SOME USEFUL MATRIX DERIVATIVES
In this section, we will obtain the differentials and the corresponding derivatives of
some important scalar functions and matrix functions of matrices. Throughout this
section, when dealing with functions of the form f(X) or F(X), we will assume that
the m × n matrix X is composed of mn unrelated variables; that is, X is assumed
not to have any particular structure such as symmetry, triangularity, and so on. We
begin with some scalar functions of X.
Theorem 9.1
Let X be an m × m matrix, and let X# denote the adjoint matrix of
X. Then
(a) d{tr(X)} = vec(Im)′d vec(X);
∂
∂vec(X)′ tr(X) = vec(Im)′,

SOME USEFUL MATRIX DERIVATIVES
397
(b) d|X| = tr(X#dX);
∂
∂vec(X)′ |X| = vec(X′
#)′,
(c) and if X is nonsingular,
d|X| = |X|tr(X−1dX);
∂
∂vec(X)′ |X| = |X| vec(X−1′)′.
Proof.
Part (a) follows directly from the fact that
d tr(X) = tr(dX) = tr(ImdX) = vec(Im)′ vec(dX)
= vec(Im)′d vec(X),
with the third equality following from Theorem 8.10. Since X# is the transpose of
the matrix of cofactors of X, to obtain the derivative in (b), we simply need to show
that
∂
∂xij
|X| = Xij,
where Xij is the cofactor of xij. By using the cofactor expansion formula on the ith
row of X, we can write the determinant of X as
|X| =
m

k=1
xikXik.
Note that for each k, Xik is a determinant computed after deleting the ith row so that
each Xik does not involve the element xij. Consequently, we have
∂
∂xij
|X| =
∂
∂xij
m

k=1
xikXik =
m

k=1
 ∂
∂xij
xik

Xik = Xij.
Using the relationship between the first differential and derivative, we then get
d|X| = {vec(X′
#)}′ vec(dX) = tr(X#dX)
as required. Part (c) follows directly from (b) because if X is nonsingular, then
X−1 = |X|−1X#.
□
An immediate consequence of Theorem 9.1(c) is Corollary 9.1.1.
Corollary 9.1.1
Let X be an m × m nonsingular matrix. Then
d{log(|X|)} = tr(X−1dX);
∂
∂vec(X)′ log(|X|) = vec(X−1′)′.

398
MATRIX DERIVATIVES AND RELATED TOPICS
Theorem 9.2 gives the differential and derivative of the inverse of a nonsingular
matrix.
Theorem 9.2
If X is a nonsingular m × m matrix, then
dX−1 = −X−1(dX)X−1;
∂
∂vec(X)′ vec(X−1) = −(X−1′ ⊗X−1).
Proof.
Computing the differential of both sides of the equation Im = XX−1, we
find that
(0) = dIm = d(XX−1) = (dX)X−1 + X(dX−1).
Premultiplying this equation by X−1 and then solving for dX−1 yields
dX−1 = −X−1(dX)X−1,
which leads to
d vec(X−1) = vec(dX−1) = −vec(X−1(dX)X−1)
= −(X−1′ ⊗X−1)vec(dX)
= −(X−1′ ⊗X−1)d vec(X),
and so the proof is complete.
□
A natural generalization of Theorem 9.2 is one that gives the differential and
derivative of the Moore–Penrose inverse of a matrix. Theorem 9.3 gives the form
of these when they exist at a matrix X. Conditions for the differentiability of X+ can
be found in Magnus and Neudecker (1999).
Theorem 9.3
If X is an m × n matrix and X+ is its Moore–Penrose inverse, then
dX+ = (In −X+X)(dX′)X+′X+ + X+X+′(dX′)(Im −XX+) −X+(dX)X+
and
∂
∂vec(X)′ vec(X+) = {X+′X+ ⊗(In −X+X) + (Im −XX+) ⊗X+X+′}
× Kmn −(X+′ ⊗X+).

SOME USEFUL MATRIX DERIVATIVES
399
Proof.
Note that
d(XX+) = (dX)X+ + XdX+,
from which we get
XdX+ = d(XX+) −(dX)X+.
(9.5)
Since X+ = X+XX+, we also have
dX+ = d(X+XX+) = d(X+X)X+ + X+XdX+
= d(X+X)X+ + X+d(XX+) −X+(dX)X+,
(9.6)
where we have used (9.5) in the last step. Thus, if we obtain expressions for d(X+X)
and d(XX+) in terms of dX, we can then find dX+. To find d(XX+), we use the
fact that XX+ is symmetric and idempotent to get
d(XX+) = d(XX+XX+) = d(XX+)XX+ + XX+d(XX+)
= d(XX+)XX+ + (d(XX+)XX+)′,
(9.7)
because d(XX+)′ = d((XX+)′) = d(XX+). However,
d(XX+)X = dX −XX+dX = (Im −XX+)dX,
(9.8)
because X = XX+X implies that
dX = d(XX+X) = d(XX+)X + XX+dX.
Now substituting (9.8) in (9.7), we find that
d(XX+) = (Im −XX+)(dX)X+ + {(Im −XX+)(dX)X+}′
= (Im −XX+)(dX)X+ + X+′(dX′)(Im −XX+).
(9.9)
By using the fact that X+X is symmetric and idempotent, we can show in a similar
fashion that
d(X+X) = X+(dX)(In −X+X) + (In −X+X)(dX′)X+′.
(9.10)
Substituting (9.9) and (9.10) into (9.6), and noting that (In −X+X)X+ = (0) and
X+(Im −XX+) = (0), we get
dX+ = (In −X+X)(dX′)X+′X+ + X+X+′(dX′)(Im −XX+) −X+(dX)X+,

400
MATRIX DERIVATIVES AND RELATED TOPICS
as is required. When we take the vec of both sides of the equation above, we get
d vec(X+) = {X+′X+ ⊗(In −X+X)} vec(dX′)
+ {(Im −XX+) ⊗X+X+′}vec(dX′)
−(X+′ ⊗X+)vec(dX)
= {X+′X+ ⊗(In −X+X)
+ (Im −XX+) ⊗X+X+′}Kmnd vec(X)
−(X+′ ⊗X+)d vec(X),
which yields the required expression for the differential.
□
9.5
DERIVATIVES OF FUNCTIONS OF PATTERNED MATRICES
In this section, we consider the computation of the derivative of a function of an
m × n matrix X when some of the variables of X are related to one another. In
particular, we will focus on the situation in which X is square and symmetric. For a
more general treatment of the topic of derivatives of functions of patterned matrices,
see Nel (1980).
If X is an m × m symmetric matrix of variables, then because of the symmetry it
only contains m(m + 1)/2 mathematically independent variables. These variables
are precisely the variables comprising the vector v(X). If f(X) is some vector
function of the matrix X, then the derivative of f will be given by the matrix
∂
∂v(X)′ f(X).
We can compute derivatives of this form by using the derivative
∂
∂vec(X)′ f(X)
for a general nonsymmetric matrix X, along with the chain rule. Specifically, from
the chain rule we have
∂
∂v(X)′ f(X) =

∂
∂vec(X)′ f(X)
 
∂
∂v(X)′ vec(X)

.
It must be emphasized here that the first of the two derivatives on the right-hand side
of this equation is computed ignoring the symmetry of X. The second of these two
derivatives can be conveniently expressed if we make use of the duplication matrix
Dm. Since Dmv(X) = vec(X), we immediately get Dmd v(X) = d vec(X), and so
∂
∂v(X)′ f(X) =

∂
∂vec(X)′ f(X)

Dm.

DERIVATIVES OF FUNCTIONS OF PATTERNED MATRICES
401
Consequently, the following results follow directly from Theorem 9.1, Theorem 9.2,
and Theorem 9.3.
Theorem 9.4
Let X be an m × m symmetric matrix of variables. Then
(a)
∂
∂v(X)′ |X| = vec(X′
#)′Dm,
(b)
∂
∂v(X)′ vec(X−1) = −(X−1 ⊗X−1)Dm if X is nonsingular,
(c)
∂
∂v(X)′ vec(X+) = ({X+X+ ⊗(Im −X+X)
+ (Im −XX+) ⊗X+X+}Kmm −(X+ ⊗X+))Dm.
The derivatives given in (b) and (c) of Theorem 9.4 still have some redundant
elements because of the symmetry of X−1 and X+. In general, if X is an m × m
symmetric matrix of variables and the m × m matrix function F(X) is also symmet-
ric, then all derivatives of elements of F(X) with respect to elements of X will be
contained in the matrix derivative
∂
∂v(X)′ v{F(X)}.
This matrix derivative can be easily computed from the derivative
A =
∂
∂v(X)′ vec{F(X)}
(9.11)
by again using the relationship vec(F) = Dmv(F). Thus, because (9.11) implies that
d vec(F) = A d v(X), we have
Dmd v(F) = A d v(X)
or
D+
mDmd v(F) = d v(F) = D+
mA d v(X),
because D+
mDm = Im(m+1)/2 by Theorem 8.32. Using this result, we obtain the
derivatives in Corollary 9.4.1.
Corollary 9.4.1
Let X be an m × m symmetric matrix of variables. Then
(a)
∂
∂v(X)′ v(X−1) = −D+
m(X−1 ⊗X−1)Dm if X is nonsingular,
(b)
∂
∂v(X)′ v(X+) = D+
m({X+X+ ⊗(Im −X+X)
+ (Im −XX+) ⊗X+X+}Kmm −(X+ ⊗X+))Dm.

402
MATRIX DERIVATIVES AND RELATED TOPICS
9.6
THE PERTURBATION METHOD
The perturbation method is a technique, closely related to the method using the
differential operator, for finding successive terms in a Taylor expansion formula. In
this section, we will use this method to obtain Taylor formulas for some important
matrix functions. A more rigorous treatment of this subject can be found in texts
such as Hinch (1991), Kato (1982), or Nayfeh (1981).
Suppose that the elements of dX are small, which we can emphasize by writing
dX = ϵY , where ϵ is a small scalar and Y is an m × n matrix. Then X + ϵY
represents a small perturbation of the m × n matrix X. The Taylor formula for the
vector function f of X would then be of the form
f(X + ϵY ) = f(X) +
∞

i=1
ϵigi(X, Y ),
where gi(X, Y ) represents some vector function of the two matrices, X and Y .
Similarly, if we have a matrix function F, then the expansion would be of the form
F(X + ϵY ) = F(X) +
∞

i=1
ϵiGi(X, Y ).
(9.12)
Our goal is to determine the first few terms in the summations given above. These then
can be applied in an approximation of f(X + ϵY ) or F(X + ϵY ) when ϵ is small.
For instance, suppose that m = n and our function is the matrix inverse function; that
is, F(X) = X−1. For notational simplicity, write Gi(X, Y ) = Gi, and suppose that
the m × m matrices X and (X + ϵY ) are nonsingular. Then (9.12) can be written
(X + ϵY )−1 = X−1 + ϵG1 + ϵ2G2 + ϵ3G3 + · · · .
However, we must have
Im = (X + ϵY )(X + ϵY )−1
= (X + ϵY )(X−1 + ϵG1 + ϵ2G2 + ϵ3G3 + · · · )
= Im + ϵ(Y X−1 + XG1) + ϵ2(Y G1 + XG2)
+ ϵ3(Y G2 + XG3) + · · · .
If this result is to hold for all ϵ, then we must have (Y X−1 + XG1) = (0) or,
equivalently,
G1 = −X−1Y X−1.
Similarly, we must have (Y G1 + XG2) = (0) so that
G2 = −X−1Y G1 = X−1Y X−1Y X−1,

THE PERTURBATION METHOD
403
and in fact, it should be apparent that we have the recursive relationship
Gh = −X−1Y Gh−1.
As a result, we have
(X + ϵY )−1 = X−1 −ϵX−1Y X−1 + ϵ2X−1Y X−1Y X−1
−ϵ3X−1Y X−1Y X−1Y X−1 + · · · ,
or, if we return to the notation dX = ϵY ,
(X + dX)−1 = X−1 −X−1(dX)X−1 + X−1(dX)X−1(dX)X−1
−X−1(dX)X−1(dX)X−1(dX)X−1 + · · · .
Next, we will use this perturbation method to determine the first few terms in the
Taylor series expansion for an eigenvalue of a symmetric matrix. Such an expansion
will be possible only if the corresponding eigenvalue of the unperturbed matrix X is
distinct. We will first consider the special case in which X is a diagonal matrix.
Theorem 9.5
Suppose X = diag(x1,. . ., xm), where x1 ≥· · · ≥xl−1 > xl >
xl+1 ≥· · · ≥xm, so that the lth diagonal element xl differs from the other diagonal
elements of X. Let U be an m × m symmetric matrix, and denote the lth largest
eigenvalue and corresponding normalized eigenvector of X + U by λl(X + U) and
γl(X + U), respectively. Then
λl(X + U) ≈xl + ull −

i̸=l
u2
il
(xi −xl) −

i̸=l
ullu2
il
(xi −xl)2
+

i̸=l

j̸=l
uilujluij
(xi −xl)(xj −xl),
γll(X + U) ≈1 −1
2

i̸=l
u2
il
(xi −xl)2 −

i̸=l
ullu2
il
(xi −xl)3
+

i̸=l

j̸=l
uilujluij
(xi −xl)2(xj −xl),
and for h ̸= l,
γhl(X + U) ≈−
uhl
(xh −xl) −
ulluhl
(xh −xl)2
+

i̸=l
uiluhi
(xh −xl)(xi −xl) −
u2
lluhl
(xh −xl)3

404
MATRIX DERIVATIVES AND RELATED TOPICS
+

i̸=l
ulluhiuil
(xh −xl)2(xi −xl) +

i̸=l
uhlu2
il
(xh −xl)2(xi −xl)
+

i̸=l
ulluhiuil
(xh −xl)(xi −xl)2
−

i̸=l

j̸=l
uhiuijujl
(xh −xl)(xi −xl)(xj −xl)
+ 1
2

i̸=l
uhlu2
il
(xh −xl)(xi −xl)2 ,
where γhl(X + U) denotes the hth element of γl(X + U), and these approximations
are accurate up through third-order terms in the u’s.
Proof.
Here U is the perturbation matrix, and we wish to write λl = λl(X + U)
and γl = γl(X + U) in the form
λl = xl + a1 + a2 + a3 + · · · ,
(9.13)
γl = el + b1 + b2 + b3 + · · · ,
(9.14)
where ai and bi only involve ith degree terms in the elements of U. Substituting
these expressions in the defining equation (X + U)γl = λlγl, and then equating ith
degree terms in the elements of U on the left-hand side of this equation to those on
the right-hand side, we obtain
Xel = xlel,
(9.15)
Xb1 + Uel = xlb1 + a1el,
(9.16)
Xb2 + Ub1 = xlb2 + a1b1 + a2el,
(9.17)
Xb3 + Ub2 = xlb3 + a1b2 + a2b1 + a3el.
(9.18)
In a similar fashion, the normalizing equation γ′
lγl = 1 yields the identities
e′
lel = 1,
(9.19)
e′
lb1 + b′
1el = 0,
(9.20)
e′
lb2 + b′
1b1 + b′
2el = 0,
(9.21)
e′
lb3 + b′
1b2 + b′
2b1 + b′
3el = 0.
(9.22)
Equations (9.15) and (9.19) are trivially true, whereas equations (9.16) and (9.20) can
be used to find a1 and b1. Premultiplying (9.16) by e′
l and then solving for a1, we find
that
a1 = e′
lUel = ull,
(9.23)

THE PERTURBATION METHOD
405
because e′
lXb1 = xle′
lb1 follows from (9.15). We can then rewrite (9.16) as the
system of linear equations
(X −xlIm)b1 = −(U −ullIm)el,
with the general solution for b1 given by
b1 = −(X −xlIm)+(U −ullIm)el + c1el,
where c1 is an arbitrary constant. Since (X −xlIm)+el = 0 and (9.20) implies that
e′
lb1 = 0, it follows that c1 = 0, and thus,
b1 = −(X −xlIm)+Uel.
(9.24)
Next, we will use (9.17) and (9.21) to find a2 and b2. Premultiplying (9.17) by e′
l and
then solving for a2, we find, after we again use the fact that e′
lb1 = 0, that
a2 = e′
lUb1 = −e′
lU(X −xlIm)+Uel.
(9.25)
Rewriting (9.17) as the system of equations in b2,
(X −xlIm)b2 = a2el −(U −a1Im)b1,
which for any scalar c2 has as a solution,
b2 = (X −xlIm)+{a2el −(U −a1Im)b1} + c2el.
Now because (X −xlIm)+el = 0 and (9.21) implies that e′
lb2 = −1
2b′
1b1, we find
that
c2 = −1
2b′
1b1 = −1
2e′
lU{(X −xlIm)+}2Uel = −1
2

i̸=l
u2
il
(xi −xl)2 ,
and so with this value for c2, the solution for b2 is given by
b2 = (X −xlIm)+(U −ullIm)(X −xlIm)+Uel + c2el.
(9.26)
To find a3, premultiply (9.18) by e′
l and solve for a3, after using e′
lb1 = 0, to get
a3 = e′
l(U −a1Im)b2
= e′
l(U −ullIm){(X −xlIm)+(U −ullIm)(X −xlIm)+Uel + c2el}
= e′
lU(X −xlIm)+(U −ullIm)(X −xlIm)+Uel.
(9.27)

406
MATRIX DERIVATIVES AND RELATED TOPICS
Equation (9.18) can be expressed as
(X −xlIm)b3 = a3el + a2b1 −(U −a1Im)b2,
so that the solution for b3 will be given by
b3 = (X −xlIm)+{a3el + a2b1 −(U −a1Im)b2} + c3el
= (X −xlIm)+{a2b1 −(U −a1Im)b2} + c3el,
(9.28)
where c3 is an arbitrary constant. By premultiplying this equation by e′
l and using
e′
lb3 = −b′
1b2 that follows from (9.22), we find that
c3 = −b′
1b2 = e′
lU{(X −xlIm)+}2(U −ullIm)(X −xlIm)+Uel
= −

i̸=l
ullu2
il
(xi −xl)3 +

i̸=l

j̸=l
uilujluij
(xi −xl)2(xj −xl).
The results now follow by substituting (9.23), (9.25) and (9.27) in (9.13), and (9.24),
(9.26) and (9.28) in (9.14).
□
We can use Theorem 9.5 to obtain expansion formulas for a general symmetric
matrix; that is, if Z is an m × m symmetric matrix and W is its associated sym-
metric perturbation matrix, then we can obtain expansion formulas for λl(Z + W)
and γl(Z + W). Let Z = QXQ′ be the spectral decomposition of Z, so that
X = diag(x1,. . ., xm), with xl being an eigenvalue of Z corresponding to the
eigenvector ql, which is the lth column of Q. As in Theorem 9.5, we assume that xl is
a distinct eigenvalue. If we let U = Q′WQ, then the eigenvalue-eigenvector equation
(Z + W)γl(Z + W) = λl(Z + W)γl(Z + W)
can be equivalently expressed as
(X + U)Q′γl(Z + W) = λl(Z + W)Q′γl(Z + W);
that is, U is the perturbation matrix of X and λl(Z + W) is an eigenvalue of
(X + U) corresponding to the eigenvector Q′γl(Z + W). Thus, if we use the
elements of U = QWQ′ in place of those of U in the formulas in Theorem 9.5, we
will obtain expansions for λl(Z + W) and Q′γl(Z + W). For instance, first-order
approximations of λl(Z + W) and γl(Z + W) are given by
λl(Z + W) ≈xl + q′
lWql,
γl(Z + W) ≈Q{el −(X −xlIm)+(Q′WQ)el}
= ql −(Z −xlIm)+Wql.

THE PERTURBATION METHOD
407
Theorem 9.6 is an immediate consequence of these first-order Taylor expansion
formulas.
Theorem 9.6
Let λl(Z) be the eigenvalue function defined on m × m symmetric
matrices Z, and let γl(Z) be a corresponding normalized eigenvector. If the matrix
Z is such that the eigenvalue λl(Z) is distinct, then differentials and derivatives at
that matrix Z are given by
dλl = γ′
l(dZ)γl,
∂
∂v(Z)′ λl(Z) = (γ′
l ⊗γ′
l)Dm,
dγl = −(Z −λlIm)+(dZ)γl,
∂
∂v(Z)′ γl(Z) = −{γ′
l ⊗(Z −λlIm)+}Dm.
The expansions given in and immediately after Theorem 9.5 do not hold when
the eigenvalue xl is not distinct. Suppose, for instance, that again x1 ≥· · · ≥xm,
but now xl = xl+1 = · · · = xl+r−1, so that the value xl is repeated as an eigenvalue
of Z = QXQ′, r times. In this case, we can get expansions for λl,l+r−1(Z + W),
the average of the perturbed eigenvalues λl(Z + W),. . ., λl+r−1(Z + W), and
the total eigenprojection Φl associated with this collection of eigenvalues; if
PZ+W {λl+i−1(Z + W)} represents the eigenprojection of Z + W associated with
the eigenvalue λl+i−1(Z + W), then this total eigenprojection is given by
Φl =
r

i=1
PZ+W {λl+i−1(Z + W)}
=
r

i=1
γl+i−1(Z + W)γ′
l+i−1(Z + W).
These expansions are summarized in Theorem 9.7. The proof, which is similar to that
of Theorem 9.5, is left to the reader.
Theorem
9.7
Let
Z
be
an
m × m
symmetric
matrix
with
eigenvalues
x1 ≥· · · ≥xl−1 > xl = xl+1 = · · · = xl+r−1 > xl+r ≥· · · ≥xm,
so
that
xl
is an eigenvalue with multiplicity r. Suppose that W is an m × m symmetric
matrix, and let λ1 ≥λ2 ≥· · · ≥λm be the eigenvalues of Z + W, whereas
λl,l+r−1 = r−1(λl + · · · + λl+r−1). Denote the eigenprojection of Z corresponding
to the repeated eigenvalue xl by Pl, and denote the total eigenprojection of
Z + W corresponding to the collection of eigenvalues λl,. . ., λl+r−1 by Φl. Define
Y = (Z −xlIm)+. Then the third-order Taylor approximations
λl,l+r−1 ≈xl + a1 + a2 + a3,
Φl ≈Pl + B1 + B2 + B3

408
MATRIX DERIVATIVES AND RELATED TOPICS
have
a1 = 1
r tr(WPl),
a2 = −1
r tr(WY WPl),
a3 = 1
r {tr(Y WY WPlW) −tr(Y 2WPlWPlW)},
B1 = −Y WPl −PlWY ,
B2 = Y WPlWY + Y WY WPl −Y 2WPlWPl + PlWY WY
−PlWPlWY 2 −PlWY 2WPl,
B3 = Y 2WPlWY WPl + PlWY WPlWY 2 + Y 2WPlWPlWY
+ Y WPlWPlWY 2 + Y 2WY WPlWPl + PlWPlWY WY 2
+ Y WY 2WPlWPl + PlWPlWY 2WY −Y 3WPlWPlWPl
−PlWPlWPlWY 3 −Y WY WPlWY −Y WPlWY WY
−Y WY WY WPl −PlWY WY WY + Y WPlWY 2WPl
+ PlWY 2WPlWY + PlWY 2WY WPl + PlWY WY 2WPl
−PlWY 3WPlWPl −PlWPlWY 3WPl.
Example 9.4
Suppose that Ω is an m × m covariance matrix and its smallest eigen-
value, λ, has multiplicity r. Let S be the sample covariance matrix defined in Section
1.13, and define A = S −Ω so that S = Ω + A. We can use Theorem 9.7 to obtain
approximations of functions of S in terms of A. For instance, as an illustration, we will
consider the first-order approximation of ˆP(S −ˆλIm) ˆP, where ˆλ is the average of
the r smallest eigenvalues of S and ˆP is the total eigenprojection of S corresponding
to these r eigenvalues. Now from Theorem 9.7, we have the approximations ˆλ ≈λ +
r−1 tr(AP) and ˆP ≈P + B1, where P is the eigenprojection of Ω corresponding to
its smallest eigenvalue and the formula for B1 can be obtained from Theorem 9.7.
Ignoring second-order terms in A, these approximations yield the approximation
ˆP(S −ˆλIm) ˆP ≈(P + B1)((Ω + A) −{λ + r−1 tr(AP)}Im)(P + B1)
= (P + B1)((Ω −λIm) + {A −r−1 tr(AP)Im})(P + B1)
= P(Ω −λIm)P + B1(Ω −λIm)P + P(Ω −λIm)B1
+ P{A −r−1 tr(AP)Im}P
= P{A −r−1 tr(AP)Im}P,
where the last equality has used the fact that P(Ω −λIm) = (Ω −λIm)P = (0).

MAXIMA AND MINIMA
409
9.7
MAXIMA AND MINIMA
One important application of derivatives involves finding the maxima or minima of
a function. A function f has a local maximum at an n × 1 point a if for some δ > 0,
f(a) ≥f(a + x) whenever x′x < δ. This function has an absolute maximum at a
if f(a) ≥f(x) for all x for which f is defined. Similar definitions hold for a local
minimum and an absolute minimum; in fact, if f has a local minimum at a point a,
then −f has a local maximum at a, and if f has an absolute minimum at a, then
−f has an absolute maximum at a. For this reason, we will at times confine our
discussion to only the case of a maximum. In this section and the next section, we
state some results that are helpful in finding local maxima and minima. For proofs
of these results, see Khuri (2003) or Magnus and Neudecker (1999). Our first result
gives a necessary condition for a function f to have a local maximum at a.
Theorem 9.8
Suppose the function f(x) is defined for all n × 1 vectors x ∈S,
where S is some subset of Rn. Let a be an interior point of S; that is, a δ > 0 exists,
such that a + u ∈S for all u′u < δ. If f has a local maximum at a and f is differ-
entiable at a, then
∂
∂a′ f(a) = 0′.
(9.29)
Any point a satisfying (9.29) is called a stationary point of f. Although
Theorem 9.8 indicates that any point at which a local maximum or local minimum
occurs must be a stationary point, the converse does not hold. A stationary point
that does not correspond to a local maximum or a local minimum is called a saddle
point. Theorem 9.9 is helpful in determining whether a particular stationary point
is a local maximum or minimum in those situations in which the function f is twice
differentiable.
Theorem 9.9
Suppose the function f(x) is defined for all n × 1 vectors x ∈S,
where S is some subset of Rn. Suppose also that f is twice differentiable at the
interior point a of S. If a is a stationary point of f and Hf is the Hessian matrix of
f at a, then
(a) f has a local minimum at a if Hf is positive definite,
(b) f has a local maximum at a if Hf is negative definite,
(c) f has a saddle point at a if Hf is nonsingular but not positive definite or negative
definite,
(d) f may have a local minimum, a local maximum, or a saddle point at a if Hf is
singular.
Example 9.5
On several occasions, we have discussed the problem of finding a
least squares solution ˆβ to the inconsistent system of equations
y = Xβ,

410
MATRIX DERIVATIVES AND RELATED TOPICS
where y is an N × 1 vector of constants, X is an N × (k + 1) matrix of constants,
and β is a (k + 1) × 1 vector of variables. A solution was obtained in Chapter 2 by
using the geometrical properties of least squares regression, whereas in Chapter 6 we
utilized the results developed on least squares generalized inverses. In this example,
we will show how the methods of this section may help obtain a solution. We will
assume that rank(X) = k + 1; that is, the matrix X has full column rank. Recall
that a least squares solution ˆβ is any vector that minimizes the sum of squared errors
given by
f(ˆβ) = (y −X ˆβ)′(y −X ˆβ).
The differential of f(ˆβ) is
df = {d(y −X ˆβ)′}(y −X ˆβ) + (y −X ˆβ)′d(y −X ˆβ)
= −(dˆβ)′X′(y −X ˆβ) −(y −X ˆβ)′Xdˆβ
= −2(y −X ˆβ)′Xdˆβ,
so that
∂
∂ˆβ
′ f(ˆβ) = −2(y −X ˆβ)′X.
Thus, on setting this first derivative equal to 0′ and rearranging, we find that the sta-
tionary values are given by the solutions ˆβ to the system of equations
X′X ˆβ = X′y.
(9.30)
Since X has full column rank, X′X is nonsingular, and so the unique solution to
(9.30) is
ˆβ = (X′X)−1X′y.
(9.31)
To verify that this solution minimizes the sum of squared errors, we need to obtain
the Hessian matrix Hf. The second differential of f(ˆβ) is given by
d2f = d(df) = −d{2(y −X ˆβ)′Xdˆβ}
= −2{d(y −X ˆβ)}′Xdˆβ
= 2(dˆβ)′X′Xdˆβ,
so that
Hf = 2X′X.
Since this matrix is positive definite, it follows from Theorem 9.9 that the solution
given in (9.31) minimizes f(ˆβ).

MAXIMA AND MINIMA
411
Example 9.6
One of the most popular ways of obtaining estimators of unknown
parameters is by a method known as maximum likelihood estimation. If we have
a random sample of vectors x1,. . ., xn from a population having density function
f(x; θ), where θ is a vector of parameters, then the likelihood function of θ is defined
to be the joint density function of x1,. . ., xn viewed as a function of θ; that is, this
likelihood function is given by
L(θ) =
n

i=1
f(xi; θ).
The method of maximum likelihood estimates θ by the vector ˆθ that maximizes L(θ).
In this example, we will use this method to obtain estimates of μ and Ω when our sam-
ple is coming from the normal distribution, Nm(μ, Ω). Thus, μ is an m × 1 vector,
Ω is an m × m positive definite matrix, and the required density function f(x; μ, Ω)
is given in (1.13). In deriving the estimates ˆμ and ˆΩ, we will find it a little bit easier
to maximize the function log(L(μ, Ω)), which is, of course, maximized at the same
solution as is L(μ, Ω). After omitting terms from log(L(μ, Ω)) that do not involve
μ or Ω, we find that we must maximize the function
g(μ, Ω) = −1
2n log |Ω| −1
2tr(Ω−1U),
where
U =
n

i=1
(xi −μ)(xi −μ)′.
The first differential of g is given by
dg = −1
2n d(log |Ω|) −1
2tr{(dΩ−1)U} −1
2tr(Ω−1dU)
= −1
2n tr(Ω−1dΩ) + 1
2tr{Ω−1(dΩ)Ω−1U}
+ 1
2tr

Ω−1

(dμ)
n

i=1
(xi −μ)′ +
n

i=1
(xi −μ)dμ′

= 1
2tr{(dΩ)Ω−1(U −nΩ)Ω−1}
+ 1
2tr(Ω−1{n(dμ)(x −μ)′ + n(x −μ)dμ′})
= 1
2tr{(dΩ)Ω−1(U −nΩ)Ω−1} + n(x −μ)′Ω−1dμ
= 1
2vec(dΩ)′(Ω−1 ⊗Ω−1) vec(U −nΩ) + n(x −μ)′Ω−1dμ,
where the second equality applied Corollary 9.1.1 and Theorem 9.2, and the fifth
equality applied Theorem 8.12. Since Ω is symmetric, vec(dΩ) = d vec(Ω) =

412
MATRIX DERIVATIVES AND RELATED TOPICS
Dmd v(Ω), and so the differential may be re-expressed as
dg = 1
2{d v(Ω)}′D′
m(Ω−1 ⊗Ω−1) vec(U −nΩ) + n(x −μ)′Ω−1dμ,
(9.32)
and thus,
∂
∂μ′ g = n(x −μ)′Ω−1,
∂
∂v(Ω)′ g = 1
2{vec(U −nΩ)}′(Ω−1 ⊗Ω−1)Dm.
On equating these first derivatives to null vectors, we obtain the equations
nΩ−1(x −μ) = 0,
D′
m(Ω−1 ⊗Ω−1) vec(U −nΩ) = 0.
From the first of these two equations, we obtain the solution for μ, ˆμ = x, whereas
the second can be rewritten as
D′
m(Ω−1 ⊗Ω−1)Dmv(U −nΩ) = 0,
because the symmetry of (U −nΩ) implies that vec(U −nΩ) = Dmv(U −nΩ).
Premultiplying this equation by D+
m(Ω ⊗Ω)D+′
m and using Theorem 8.35, we find
that
v(U −nΩ) = 0.
Since (U −nΩ) is symmetric, this implies that (U −nΩ) = (0), and so the solution
for Ω is ˆΩ = n−1U. All that remains is to show that the solution (ˆμ, ˆΩ) yields a
maximum. By differentiating (9.32) , we find that
d2g = 1
2{d v(Ω)}′D′
m{d(Ω−1 ⊗Ω−1)} vec(U −nΩ)
+ 1
2{d v(Ω)}′D′
m(Ω−1 ⊗Ω−1) vec(dU −ndΩ)
−n(dμ)′Ω−1dμ + n(x −μ)′(dΩ−1)dμ.
Evaluating this at μ = x and Ω = n−1U, we find that the first and the fourth terms
on the right-hand side of the equation above vanish. In addition, note that
dU = n(dμ)(x −μ)′ + n(x −μ)dμ′
also vanishes when evaluated at μ = x. Thus, at μ = x and Ω = n−1U,
d2g = −n
2 {d v(Ω)}′D′
m(Ω−1 ⊗Ω−1)Dmd v(Ω) −n(dμ)′Ω−1dμ
= [dμ′ {d v(Ω)}′]Hg
 dμ
d v(Ω)

,

CONVEX AND CONCAVE FUNCTIONS
413
where
Hg =

−nΩ−1
(0)
(0)
−n
2 D′
m(Ω−1 ⊗Ω−1)Dm

.
Clearly, Hg is negative definite because Ω−1 and D′
m(Ω−1 ⊗Ω−1)Dm are positive
definite matrices, which then establishes that the solution (ˆμ, ˆΩ) = (x, n−1U) yields
a maximum.
9.8
CONVEX AND CONCAVE FUNCTIONS
In Section 2.11, we discussed convex sets. Here we will extend the concept of convex-
ity to functions and obtain some special results that apply to this class of functions.
Definition 9.1
Let f(x) be a real-valued function defined for all x ∈S, where S is
a convex subset of Rm. Then f(x) is a convex function on S, if
f(cx1 + (1 −c)x2) ≤cf(x1) + (1 −c)f(x2)
(9.33)
for all x1 ̸= x2, x1 ∈S, x2 ∈S, and 0 < c < 1. The function f(x) is a strictly con-
vex function if inequality (9.33) is always a strict inequality. If −f(x) is a (strictly)
convex function, then f(x) is said to be a (strictly) concave function.
The implication of Definition 9.1 is that the line between any two points of a
convex function must be above the function. Figure 9.1 illustrates a convex function
when x is a scalar variable.
If f(x) is a convex function, then it is easily verified that the set defined by
T = {z = (x′, y)′ : x ∈S, y ≥f(x)}
..............................................................................................................................................................................................
.................................
.................................................................
.............................................
.........................................................
.............................................
cf(x1) + (1 −c)f(x2)
f(cx1 + (1 −c)x2)
x1
cx1 + (1 −c)x2
x2
f(x)
Figure 9.1
A convex function f(x) of a scalar variable x

414
MATRIX DERIVATIVES AND RELATED TOPICS
is a convex subset of Rm+1. For instance, if m = 1, then T will be a convex subset
of R2. In this case, for any a ∈S, the point (a, f(a)) will be a boundary point of the
set T. Now from the supporting hyperplane theorem, Theorem 2.32, we know that
there is a line passing through the point (a, f(a)), such that the function f(x) is never
below this line. Since this line passes through the point (a, f(a)), it can be written in
the form g(x) = f(a) + t(x −a), where t is the slope of the line, and thus, for all
x ∈S, we have
f(x) ≥f(a) + t(x −a).
(9.34)
The generalization of this result to arbitrary m is given below.
Theorem 9.10
Let f(x) be a real-valued convex function defined for all x ∈S,
where S is a convex subset of Rm. Then, corresponding to each interior point a ∈S,
an m × 1 vector t exists, such that
f(x) ≥f(a) + t′(x −a)
(9.35)
for all x ∈S.
Proof.
For any a ∈S, the point z∗= (a′, f(a))′ is a boundary point of the convex
set T defined above, and so it follows from Theorem 2.32 that an (m + 1) × 1
vector b = (b′
1, bm+1)′ ̸= 0 exists, for which b′z ≥b′z∗for all z ∈T. Clearly, for
any z = (x′, y)′ ∈T, we can arbitrarily increase the value of y and get another
point in T. For this reason, we see that bm+1 cannot be negative because if it was,
we could make b′z arbitrarily small and, in particular, less than b′z∗. Thus, bm+1 is
either positive or 0. Now for any x ∈S, (x′, f(x))′ ∈T, and so for this choice of
z in the inequality b′z ≥b′z∗, we get
b′
1x + bm+1f(x) ≥b′
1a + bm+1f(a).
If bm+1 is positive, then the inequality above may be rearranged to the form given
in (9.35) with t = −b−1
m+1b1. If, on the other hand, bm+1 = 0, then b′ z ≥b′ z∗
reduces to
b′
1x ≥b′
1a,
which implies that a is a boundary point of S. Thus, the proof is complete.
□
If f is a differentiable function, then the hyperplane given on the right-hand side
of (9.35) will be given by the tangent hyperplane to f(x) at x = a.
Theorem 9.11
Let f(x) be a real-valued convex function defined for all x ∈S,
where S is an open convex subset of Rm. If f(x) is differentiable and a ∈S, then
f(x) ≥f(a) +
 ∂
∂a′ f(a)

(x −a)
for all x ∈S.

CONVEX AND CONCAVE FUNCTIONS
415
Proof.
Suppose that x ∈S and a ∈S, and let y = x −a, so that x = a + y. Since
S is convex, the point
cx + (1 −c)a = c(a + y) + (1 −c)a = a + cy
is in S for 0 ≤c ≤1. Thus, because of the convexity of f, we have
f(a + cy) ≤cf(a + y) + (1 −c)f(a) = f(a) + c{f(a + y) −f(a)},
or equivalently,
f(a + y) ≥f(a) + c−1{f(a + cy) −f(a)}.
(9.36)
Now because f is differentiable, we also have the Taylor formula
f(a + cy) = f(a) +
 ∂
∂a′ f(a)

cy + r1(cy, a),
(9.37)
where the remainder satisfies lim c−1r1(cy, a) = 0 as c →0. Using (9.37) in (9.36),
we get
f(a + y) ≥f(a) +
 ∂
∂a′ f(a)

y + c−1r1(cy, a),
and so the result follows by letting c →0.
□
We can easily use Theorem 9.11 to show that a stationary point of a convex func-
tion will actually be an absolute minimum. Equivalently, a stationary point of a con-
cave function will be an absolute maximum of that function.
Theorem 9.12
Let f(x) be a real-valued convex function defined for all x ∈S,
where S is an open convex subset of Rm. If f(x) is differentiable and a ∈S is a
stationary point of f, then f has an absolute minimum at a.
Proof.
If a is a stationary point of f, then
∂
∂a′ f(a) = 0′.
Using this in the inequality of Theorem 9.11, we get f(x) ≥f(a) for all x ∈S, and
so the result follows.
□
We can use the inequality given in (9.35) to prove a useful inequality involving
the moments of a random vector y. This inequality is known as Jensens’s inequality.
However, before we can prove this result, we will need Theorem 9.13.

416
MATRIX DERIVATIVES AND RELATED TOPICS
Theorem 9.13
Suppose that S is a convex subset of Rm and y is an m × 1 random
vector with finite first moments. If P(y ∈S) = 1, then E(y) ∈S.
Proof.
We will prove the result by induction. Clearly, the result holds if m = 1,
because in this case, S is an interval, and it is easily shown that a random variable y
satisfying P(a ≤y ≤b) = 1 for some constants a and b will have a ≤E(y) ≤b.
Now assuming that the result holds for dimension m −1, we will show that it
must then hold for m. Define the convex set S∗= {x : x = u −E(y), u ∈S},
so that the proof will be complete if we show that 0 ∈S∗. Now if 0 /∈S∗,
it follows from Theorem 2.32 that an m × 1 vector a ̸= 0 exists, such that
a′x ≥0 for all x ∈S∗. Consequently, because P(y ∈S) = P(w ∈S∗) = 1,
where the random vector w = y −E(y), we have a′w ≥0 with probability
1; yet E(a′w) = 0, which is possible only if a′w = 0, in which case, w
is on the hyperplane defined by {x : a′x = 0}, with probability one. How-
ever, because P(w ∈S∗) = 1 as well, we must have P(w ∈S0) = 1, where
S0 = S∗∩{x : a′x = 0}. Now it follows from Theorem 2.28 that S0 is a convex
set, and it is contained within an (m −1)-dimensional vector space because
{x : a′x = 0} is an (m −1)-dimensional vector space. Thus, because our result
holds for (m −1)-dimensional spaces, we must have E(w) = 0 ∈S0. This leads to
the contradiction 0 ∈S∗, because S0 ⊆S∗, and so the proof is complete.
□
We now prove Jensen’s inequality in Theorem 9.14.
Theorem 9.14
Let f(x) be a real-valued convex function defined for all x ∈S,
where S is a convex subset of Rm. If y is an m × 1 random vector with finite first
moments and satisfying P(y ∈S) = 1, then
E(f(y)) ≥f(E(y)).
Proof.
Theorem 9.13 guarantees that E(y) ∈S. We first prove the result for m = 1.
If E(y) is an interior point of S, the result follows by taking the expected value of
both sides of (9.34) when x = y and a = E(y). Since S is an interval when m = 1,
E(y) can be a boundary point of S only if S is closed and P(y = c) = 1, where
c is an endpoint of the interval. In this case, the result is trivial because the terms
on the two sides of the inequality above are equal. We will complete the proof by
showing that if the result holds for m −1, then it must hold for m. If the m × 1
vector E(y) is an interior point of S, the result follows by taking the expected value
of both sides of (9.35) with x = y and a = E(y). If E(y) is a boundary point of S,
then we know from the supporting hyperplane theorem that an m × 1 unit vector b
exists, such that w = b′y ≥b′E(y) = μ with probability one. However, because we
also have E(w) = b′E(y) = μ, it follows that b′y = μ with probability one. Let P
be any m × m orthogonal matrix with its last column given by b, so that the vector
u = P ′y has the form u = (u′
1, μ)′, where u1 is an (m −1) × 1 vector. Define the
function g(u1) as
g(u1) = f

P

u1
μ

= f(y),

THE METHOD OF LAGRANGE MULTIPLIERS
417
for all u1 ∈S∗= {x : x = P ′
1y, y ∈S}, where P1 is the matrix obtained from P by
deleting its last column. The convexity of S∗and g follow from the convexity of S
and f, and so, because u1 is (m −1) × 1, our result applies to g(u1). Thus, we have
E(f(y)) = E(g(u1)) ≥g(E(u1))
= f

P

E(u1)
μ

= f(E(y)),
and so the proof is complete.
□
9.9
THE METHOD OF LAGRANGE MULTIPLIERS
In some situations, we may need to find a local maximum of a function f(x), where
f is defined for all x ∈S, whereas the desired maximum is over all x in T, a subset
of S. The method of Lagrange multipliers is useful in those situations in which the
set T can be expressed in terms of a number of equality constraints; that is, functions
g1,. . ., gm exist, such that
T = {x : x ∈Rn, g(x) = 0},
where g(x) is the m × 1 function given by (g1(x),. . ., gm(x))′.
The method of Lagrange multipliers involves the maximization of the Lagrange
function
L(x, λ) = f(x) −λ′g(x),
where the components of the m × 1 vector λ, λ1,. . ., λm, are called the Lagrange
multipliers. The stationary values of L(x, λ) are the solutions (x, λ) satisfying
∂
∂x′ L(x, λ) =
∂
∂x′ f(x) −λ′
 ∂
∂x′ g(x)

= 0′,
(9.38)
∂
∂λ′ L(x, λ) = −g(x)′ = 0′.
The second equation above is simply the equality constraints,
g(x) = 0
(9.39)
that determine the set T. Under certain conditions, the local maximum of the func-
tion f(x), subject to x ∈T, will be given by a vector x that, for some λ, satisfies
equations (9.38) and (9.39). We will present a procedure for determining whether
a particular solution vector x is a local maximum. This procedure is based on the
following result, a proof of which can be found in Magnus and Neudecker (1999).

418
MATRIX DERIVATIVES AND RELATED TOPICS
Theorem 9.15
Suppose the function f(x) is defined for all n × 1 vectors x ∈S,
where S is some subset of Rn, and g(x) is an m × 1 vector function defined for all
x ∈S, where m < n. Let a be an interior point of S, and suppose that the following
conditions hold:
(a) f and g are twice differentiable at a.
(b) The first derivative of g at a, (∂/∂a′)g(a), has full rank m.
(c) g(a) = 0.
(d) (∂/∂a′)L(a, λ) = 0′, where L(x, λ) = f(x) −λ′g(x) and λ is m × 1.
Let Hf and Hgi be the Hessian matrices of the functions f(x) and gi(x) evaluated
at x = a, and define
A = Hf −
m

i=1
λiHgi,
B =
∂
∂a′ g(a).
Then f(x) has a local maximum at x = a, subject to g(x) = 0, if
x′Ax < 0,
for all x ̸= 0 for which Bx = 0.
A similar result holds for a local minimum with the inequality x′Ax > 0
replacing x′Ax < 0. Theorem 9.16 provides a method for determining whether
x′Ax < 0 or x′Ax > 0 holds for all x ̸= 0 satisfying Bx = 0. Again, a proof can
be found in Magnus and Neudecker (1999).
Theorem 9.16
Let A be an n × n symmetric matrix and B be an m × n matrix.
For r = 1,. . ., n, let Arr be the r × r matrix obtained by deleting the last n −r rows
and columns of A, and let Br be the m × r matrix obtained by deleting the last n −r
columns of B. For r = 1,. . ., n, define the (m + r) × (m + r) matrix Δr as
Δr =

(0) Br
B′
r Arr

.
Then, if Bm is nonsingular, x′Ax > 0 holds for all x ̸= 0 satisfying Bx = 0 if and
only if
(−1)m|Δr| > 0,
for r = m + 1,. . ., n, and x′Ax < 0 holds for all x ̸= 0 satisfying Bx = 0 if and
only if
(−1)r|Δr| > 0,
for r = m + 1,. . ., n.

THE METHOD OF LAGRANGE MULTIPLIERS
419
Example 9.7
We will find solutions x = (x1, x2, x3)′, which maximize and mini-
mize the function
f(x) = x1 + x2 + x3,
subject to the constraints
x2
1 + x2
2 = 1,
(9.40)
x3 −x1 −x2 = 1.
(9.41)
Setting the first derivative of the Lagrange function
L(x, λ) = x1 + x2 + x3 −λ1(x2
1 + x2
2 −1) −λ2(x3 −x1 −x2 −1),
with respect to x, equal to 0′, we obtain the equations
1 −2λ1x1 + λ2 = 0,
1 −2λ1x2 + λ2 = 0,
1 −λ2 = 0.
The third equation gives λ2 = 1, and when this is substituted in the first two equations,
we find that we must have
x1 = x2 = 1
λ1
.
Using this equation in (9.40), we find that λ1 = ±
√
2, and so we have the stationary
points
(x1, x2, x3) =
 1
√
2, 1
√
2, 1 +
√
2

when λ1 =
√
2,
(x1, x2, x3) =

−1
√
2, −1
√
2, 1 −
√
2

when λ1 = −
√
2.
To determine whether either of these solutions yields a maximum or minimum, we
use Theorem 9.15 and Theorem 9.16. Thus, because m = 2 and n = 3, we only need
the determinant of the matrix
Δ3 =
⎡
⎢⎢⎢⎢⎣
0
0
2x1
2x2
0
0
0
−1
−1
1
2x1 −1 −2λ1
0
0
2x2 −1
0
−2λ1 0
0
1
0
0
0
⎤
⎥⎥⎥⎥⎦
.
By using the cofactor expansion formula for a determinant, it is fairly straightforward
to show that
|Δ3| = −8λ1(x2
1 + x2
2).

420
MATRIX DERIVATIVES AND RELATED TOPICS
Thus, when (x1, x2, x3, λ1, λ2) = (1/
√
2, 1/
√
2, 1 +
√
2,
√
2, 1), we have
(−1)r|Δr| = (−1)3|Δ3| = 8
√
2 > 0,
and so the solution (x1, x2, x3) = (1/
√
2, 1/
√
2, 1 +
√
2) yields a constrained
maximum. On the other hand, when (x1, x2, x3, λ1, λ2) = (−1/
√
2, −1/
√
2, 1 −
√
2, −
√
2, 1),
(−1)m|Δr| = (−1)2|Δ3| = 8
√
2 > 0,
so the solution (x1, x2, x3) = (−1/
√
2, −1/
√
2, 1 −
√
2) yields a constrained
minimum.
In some situations, in the process of obtaining the stationary values of L(x, λ),
it becomes apparent which solution yields a maximum and which solution yields a
minimum. Thus, in this case, there will be no need to compute the Δr matrices.
Example 9.8
Let A be an m × m symmetric matrix and x be an m × 1 nonnull
vector. We saw in Section 3.6 that
x′Ax
x′x
(9.42)
has a maximum value of λ1(A) and a minimum value of λm(A), where λ1(A) ≥
· · · ≥λm(A) are the eigenvalues of A. We will prove this result again, this time using
Lagrange’s method. Note that because z = (x′x)−1/2x is a unit vector, it follows
that maximizing or minimizing (9.42) over all x ̸= 0 is equivalent to maximizing or
minimizing the function
f(z) = z′Az,
subject to the constraint
z′z = 1.
(9.43)
Thus, the Lagrange function is
L(z, λ) = z′Az −λ(z′z −1).
Setting its first derivative, with respect to z′, equal to 0′, we obtain the equation
2Az −2λz = 0,
or, equivalently,
Az = λz,
(9.44)

THE METHOD OF LAGRANGE MULTIPLIERS
421
which is the eigenvalue-eigenvector equation for A. Thus, the Lagrange multiplier λ
is an eigenvalue of A. Further, premultiplying (9.44) by z′ and using (9.43), we find
that
λ = z′Az;
that is, if (z′, λ)′ is a stationary point of L(z, λ), then λ = z′Az must be an
eigenvalue of A. Consequently, the maximum value of z′Az, subject to z′z = 1, is
λ1(A), which is attained when z is equal to any unit eigenvector corresponding to
λ1(A). Similarly, the minimum value of z′Az, subject to z′z = 1, is λm(A), which
is attained at any unit eigenvector associated with λm(A).
Example 9.9
Let D = diag(d1,. . ., dm), where d1 > · · · > dm > 0 and let A be
an m × m positive definite matrix. Consider the function f(X) = tr(XAX′D),
where X is an m × m orthogonal matrix. We wish to maximize and minimize f(X)
over all choices for X. Since X must satisfy XX′ −Im = (0), we use the Lagrange
function
L(X, Λ) = tr(XAX′D) + tr{Λ(XX′ −Im)},
where Λ is a symmetric matrix of Lagrange multipliers. The differential of L(X, Λ)
is
dL = tr{(dX)AX′D} + tr{XA(dX)′D} + tr{d(Λ)(XX′ −Im)}
+ tr{Λ(dX)X′} + tr{ΛX(dX)′}
= 2 vec(DXA + ΛX)′d vec(X) + vec(XX′ −Im)′d vec(Λ),
and so the stationary values of L(X, Λ) occur at the solutions to
DXA + ΛX = (0),
XX′ = Im.
Thus, Λ = −DXAX′, and since Λ is symmetric, we have DXAX′ = XAX′D, or
DY = Y D,
(9.45)
where Y = XAX′. Examining the (i, j)th term on each side of (9.45), we see that
diyij = yijdj. Since di ̸= dj when i ̸= j, it follows that Y is a diagonal matrix. Con-
sequently, the stationary values of f(X) subject to XX′ = Im are given be the set of
values
m

i=1
djiλi(A),
where λ1(A) ≥· · · ≥λm(A) are the eigenvalues of A and (j1,. . ., jm) is a permu-
tation of (1,. . ., m), the set being formed over all such permutations. It readily then
follows that
max
XX′=Im tr(XAX′D) =
m

i=1
diλi(A),

422
MATRIX DERIVATIVES AND RELATED TOPICS
and
min
XX′=Im tr(XAX′D) =
m

i=1
dm−i+1λi(A).
In Example 9.10, we obtain the best quadratic unbiased estimator of σ2 in the
ordinary least squares regression model.
Example 9.10
Consider the multiple regression model y = Xβ + ϵ, where ϵ ∼
NN(0, σ2I). A quadratic estimator of σ2 is any estimator ˆσ2 that takes the form ˆσ2 =
y′Ay, where A is a symmetric matrix of constants. We wish to find the choice of A
that minimizes var(ˆσ2) over all choices of A for which ˆσ2 is unbiased. Now because
E(ϵ) = 0 and E(ϵϵ′) = σ2I, we have
E(y′Ay) = E{(Xβ + ϵ)′A(Xβ + ϵ)}
= E{β′X′AXβ + 2β′X′Aϵ + ϵ′Aϵ}
= β′X′AXβ + tr{AE(ϵϵ′)}
= β′X′AXβ + σ2tr(A),
and so ˆσ2 = y′Ay is unbiased regardless of the value of β only if
X′AX = (0)
(9.46)
and
tr(A) = 1.
(9.47)
Using the fact that the components of ϵ are independently distributed and the first
four moments of each component are 0, 1, 0, 3, it is easily verified that
var(y′Ay) = 2σ4 tr(A2) + 4σ2β′X′A2Xβ.
Thus, the required Lagrange function is
L(A, λ, Λ) = 2σ4 tr(A2) + 4σ2β′X′A2Xβ −tr(ΛX′AX) −λ{tr(A) −1},
where the Lagrange multipliers are given by λ and the components of the matrix Λ,
which is symmetric because X′AX is symmetric. Differentiation with respect to A
yields
dL = 2σ4tr{(dA)A + AdA} + 4σ2β′X′{(dA)A + AdA}Xβ
−tr{ΛX′(dA)X} −λtr(dA)
= tr({4σ4A + 4σ2(AXββ′X′ + Xββ′X′A) −XΛX′ −λIN}dA).
Thus, we must use
4σ4A + 4σ2(AXββ′X′ + Xββ′X′A) −XΛX′ −λIN = (0)
(9.48)

PROBLEMS
423
along with (9.46) and (9.47) to solve for A. Premultiplying and postmultiplying (9.48)
by XX+ and using (9.46) and the fact that X+ = (X′X)+X′, we find that
XΛX′ = −λXX+.
Substituting this result back into (9.48), we get
A = 1
4σ−4λ(IN −XX+) −σ−2H,
(9.49)
where H = Aγγ′ + γγ′A and γ = Xβ. Putting (9.49) back into (9.48) and
simplifying, we obtain
H = −σ−2(Hγγ′ + γγ′H).
(9.50)
By postmultiplying (9.50) by γ, we find that γ must be an eigenvector of H, which
in light of (9.50), can be true only if H is of the form H = cγγ′ for some scalar c.
Further, when we put H = cγγ′ in (9.50), we find that we must have c = 0; thus,
H = (0). In addition, if we take the trace of both sides of (9.49) and use (9.47), we
see that
λ =
4σ4
tr(IN −XX+) =
4σ4
N −r,
where r is the rank of X. Consequently, we have shown that (9.49) simplifies to
A = (N −r)−1(IN −XX+),
(9.51)
so that ˆσ2 = y′Ay = SSE/(N −r) is the familiar residual variance estimate. We
can easily show that (9.51) yields an absolute minimum by writing an arbitrary sym-
metric matrix satisfying (9.46) and (9.47), as A∗= A + B, where B must then satisfy
tr(B) = 0 and X′BX = (0). Then, because tr(AB) = 0 and AX = (0), we have
var(y′A∗y) = 2σ4tr(A2
∗) + 4σ2β′X′A2
∗Xβ
= 2σ4{tr(A2) + tr(B2) + 2tr(AB)} + 4σ2β′X′
× (A2 + B2 + AB + BA)Xβ
= 2σ4{tr(A2) + tr(B2)} + 4σ2β′X′B2Xβ
≥2σ4tr(A2) = var(y′Ay).
PROBLEMS
9.1 Consider the natural log function, f(x) = log(x).
(a) Obtain the kth-order Taylor formula for f(1 + u) in powers of u.
(b) Use the formula in part (a) with k = 5 to approximate log(1.1).

424
MATRIX DERIVATIVES AND RELATED TOPICS
9.2 Suppose the function f of the 2 × 1 vector x is given by
f(x) = (x2 −1)2
(x1 + 1)3 .
Give the second-order Taylor formula for f(0 + u) in powers of u1 and u2.
9.3 Suppose the 2 × 1 function f of the 3 × 1 vector x is given by
f(x) =
 x2
1 + x2
2 + x2
3
2x1 −x2 −x3

and the 2 × 1 function g of the 2 × 1 vector z is given by
g(z) =

z2/z1
z1z2

.
Use the chain rule to compute
∂
∂x′ y(x),
where y(x) is the composite function defined by y(x) = g( f(x)).
9.4 Let A be an m × m symmetric matrix of constants and x be an m × 1 vector
of variables. Find the differential and first derivative of f(x) = exp(−1
2x′Ax).
9.5 Let A and B be m × m symmetric matrices of constants and x be an m × 1
vector of variables. Find the differential and first derivative of the function
f(x) = x′Ax
x′Bx.
9.6 Let x be an m × 1 vector of variables. Find the differential and derivative
of xx′.
9.7 Let A and B be m × n matrices of constants and X be an n × m matrix of
variables. Find the differential and derivative of
(a) tr(AX),
(b) tr(AXBX).
9.8 Let X be an m × m nonsingular matrix, A be an m × m matrix of constants,
and a be an m × 1 vector of constants. Find the differential and derivative of
(a) |X2|,
(b) tr(AX−1),
(c) a′X−1a.
9.9 Let A be an m × m positive definite matrix of constants and X be an m × n
matrix of variables such that X′AX is nonsingular. Find the differential and
first derivative of log |X′AX|.

PROBLEMS
425
9.10 Let X be an m × n matrix with rank(X) = n. Show that
∂
∂vec(X)′ |X′X| = 2|X′X| (vec{X(X′X)−1})′.
9.11 Let X be an m × m matrix and n be a positive integer. Show that
∂
∂vec(X)′ vec(Xn) =
n

i=1
{(Xn−i)′ ⊗Xi−1}.
9.12 Let A and B be n × m and m × n matrices of constants, respectively. If X is
an m × m nonsingular matrix, find the derivatives of
(a) vec(AXB),
(b) vec(AX−1B).
9.13 Show that if X is an m × m nonsingular matrix and X# is its adjoint matrix,
then
∂
∂vec(X)′ vec(X#) = |X|{vec(X−1) vec(X−1′)′ −(X−1′ ⊗X−1)}.
9.14 Prove Corollary 9.1.1.
9.15 Let X be an m × m symmetric matrix of variables. For each of the following
functions, find the Jacobian matrix
∂
∂v(X)′ vec(F).
(a) F(X) = AXA′, where A is an m × m matrix of constants.
(b) F(X) = XBX, where B is an m × m symmetric matrix of constants.
9.16 Suppose X is an m × n matrix of rank n. Find the differential and first
derivative of Im −X(X′X)−1X′.
9.17 Let X be an m × n matrix. Show that
(a) if F(X) = X ⊗X,
∂
∂vec(X)′ vec(F) = (In ⊗Knm ⊗Im){Imn ⊗vec(X) + vec(X) ⊗Imn},
(b) if F(X) = X ⊙X,
∂
∂vec(X)′ vec(F) = 2Dvec(X).
9.18 Show that the Hessian matrix Hf is given by
(a) Hf = 2Imn if f(X) = tr(X′X), where X is an m × n matrix,
(b) Hf = 2Kmm if f(X) = tr(X2), where X is an m × m matrix,

426
MATRIX DERIVATIVES AND RELATED TOPICS
(c) Hf = −Kmm(X−1′ ⊗X−1) if f(X) = log |X|, where X is an m × m
nonsingular matrix.
9.19 Let X be an m × m nonsingular matrix. Show that
dnX−1 = (−1)nn!(X−1dX)nX−1.
9.20 Let X be an m × m matrix having correlation structure; that is, X is a sym-
metric matrix of variables, except that each of its diagonal elements is equal to
one. Show that if X is nonsingular, then
∂
∂˜v(X)′ ˜v(X−1) = −2˜LmNm(X−1 ⊗X−1)Nm ˜L′
m.
9.21 Suppose that Y is an m × m symmetric matrix and ϵ is a scalar, such that
(Im + ϵY )−1 exists. Let (Im + ϵY )−1/2 be the symmetric square root of
(Im + ϵY )−1, so that
(Im + ϵY )−1 = (Im + ϵY )−1/2(Im + ϵY )−1/2.
Using perturbation methods, show that
(Im + ϵY )−1/2 = Im +
∞

i=1
ϵiBi,
where
B1 = −1
2Y,
B2 = 3
8Y 2, B3 = −5
16Y 3, B4 = 35
128Y 4.
9.22 Let X be an m × n full column rank matrix, so that X+ = (X′X)−1X′. Let
Y be an m × n matrix and ϵ be a scalar, such that X + ϵY is also full column
rank. Show that
(X + ϵY )+ = X+ +
∞

i=1
ϵiBi,
where
B1 = (X′X)−1Y ′(Im −XX+) −X+Y X+.
9.23 Let S be an m × m sample covariance matrix, and suppose that Ω, the
corresponding population covariance matrix, has each of its diagonal elements
equal to one. Define A to be the difference between these two matrices; that is,
A = S −Ω, so that S = Ω + A. Note that the population correlation matrix is
also Ω, whereas the sample correlation matrix is given by R = D−1/2
S
SD−1/2
S
,
where
D−1/2
S
= diag(s−1/2
11
,. . ., s−1/2
mm ).
Show
that
the
approximation

PROBLEMS
427
R = Ω + C1 + C2 + C3, accurate up through third-order terms in the elements
of A, is given by
C1 = A −1
2(ΩDA + DAΩ),
C2 = 3
8(D2
AΩ + ΩD2
A) + 1
4DAΩDA −1
2(ADA + DAA),
C3 = 3
8(D2
AA + AD2
A) + 1
4DAADA −3
16(D2
AΩDA + DAΩD2
A)
−5
16(D3
AΩ + ΩD3
A),
where DA = diag(a11,. . ., amm).
9.24 Derive the results given in Theorem 9.7. First obtain expressions for B1, B2,
and B3 by using the equations (Z + W)Φl = Φl(Z + W), Φ2
l = Φl, and
Φ′
l = Φl. Then obtain expressions for a1, a2, and a3 by using the fact that
λl,l+r−1 = r−1tr{(Z + W)Φl}.
9.25 Let X = diag(x1,. . ., xm), where x1 ≥· · · ≥xm, and suppose that the lth
diagonal element is distinct, so that xl ̸= xi if i ̸= l. Let λ1 ≥· · · ≥λm
and γ1,. . ., γm be the eigenvalues and corresponding eigenvectors of
(Im + V )−1(X + U), where U and V are m × m symmetric matrices; that is,
for each i,
(X + U)γi = λi(Im + V )γi.
The purpose of this exercise is to obtain the first-order approximations
λl = xl + a1 and γl = cel + b1, where el is the lth column of Im. Higher
order approximations can be found in Sugiura (1976). These approximations
can be determined by using the eigenvalue-eigenvector equation just given
along with the appropriate scale constraint on γl.
(a) Show that a1 = ull −xlvll.
(b) Show that if c = 1 and γ′
lγl = 1, then
bl1 = 0,
bi1 = −uil −xlvil
xi −xl
for all i ̸= l,
where bi1 is the ith component of the vector b1.
(c) Show that if c = 1 and γ′
l(Im + V )γl = 1, then
bl1 = −1
2vll,
bi1 = −uil −xlvil
xi −xl
for all i ̸= l.
(d) Show that if c = x1/2
l
and γ′
lγl = λl, then
bl1 = ull −xlvll
2x1/2
l
,
bi1 = −x1/2
l
(uil −xlvil)
xi −xl
for all i ̸= l.

428
MATRIX DERIVATIVES AND RELATED TOPICS
9.26 Let Ω and S be as defined in Example 9.4 with the smallest eigenvalue of Ω
being λ = 1, and consider the quantity
U =
r−1 m
i=m−r+1 λ2
i(S)
{r−1 m
i=m−r+1 λi(S)}2 −1,
where λ1(S) ≥· · · ≥λm(S) are the eigenvalues of S. Note that
m

i=m−r+1
λ2
i(S) = tr(S2 ˆP),
where ˆP is the total eigenprojection of S corresponding to its r smallest
eigenvalues. Show that if we let A = S −Ω, so that S = Ω + A, then the
second-order approximation formula for U is given by
U ≈r−1(tr(APAP) −r−1{tr(AP)}2).
9.27 Consider the function f of the 2 × 1 vector x given by
f(x) = 2x3
1 + x3
2 −6x1 −27x2.
(a) Determine the stationary points of f.
(b) Identify each of the points in part (a) as a maximum, minimum, or saddle
point.
9.28 For each of the following functions, determine any local maxima or minima:
(a) x2
1 + 1
2x2
2 −2x1x2 + x1 −2x2 + 1.
(b) x3
1 + 3
2x2
1 + x2
2 −6x1 −2x2.
(c) x3
2 + 2x2
1 + x2
3 + 2x1x3 −3x2 −x3.
9.29 Let a be an m × 1 vector and B be an m × m symmetric matrix, each contain-
ing constants. Let x be an m × 1 vector of variables.
(a) Show that the function
f(x) = x′Bx + a′x
has stationary solutions given by
x = −1
2B+a + (Im −B+B)y,
where y is an arbitrary m × 1 vector.
(b) Show that if B is nonsingular, then only one stationary solution exists.
When will this solution yield a maximum or a minimum?

PROBLEMS
429
9.30 If the Hessian matrix Hf of a function f is singular at a stationary point x, then
we must take a closer look at the behavior of this function in the neighborhood
of the point x to determine whether the point is a maximum, minimum, or a
saddle point. For each of the functions below, show that 0 is a stationary point
and the Hessian matrix is singular at 0. In each case, determine whether 0 yields
a maximum, minimum, or a saddle point.
(a) x4
1 + x4
2.
(b) x2
1x2
2 −x4
1 −x4
2.
(c) x3
1 −x3
2.
9.31 Suppose that we have independent random samples from each of k multivariate
normal distributions with the ith distribution being Nm(μi, Ω). Thus, these dis-
tributions have possibly different mean vectors but identical covariance matri-
ces. If the ith sample is denoted by xi1,. . ., xini, show that the maximum
likelihood estimators of μi and Ω are given by
ˆμi = xi =
ni

j=1
xij
ni
,
ˆΩ =
k

i=1
ni

j=1
(xij −xi)(xij −xi)′
n
,
where n = n1 + · · · + nk.
9.32 Consider the multiple regression model,
y = Xβ + ϵ,
where y is N × 1, X is N × m, β is m × 1, and ϵ is N × 1. Suppose that
rank(X) = m and ϵ ∼NN(0, σ2IN), so that y ∼NN(Xβ, σ2IN). Find the
maximum likelihood estimates of β and σ2.
9.33 Let f(x) be a real-valued convex function defined for all x ∈S, where S is a
convex subset of Rm. Show that the set T = {z = (x′, y)′ : x ∈S, y ≥f(x)}
is convex.
9.34 Suppose that f(x) and g(x) are convex functions both defined on the convex
set S ⊆Rm. Show that the function af(x) + bg(x) is convex if a and b are
nonnegative scalars.
9.35 Prove the converse of Theorem 9.11; that is, show that if f(x) is defined and
differentiable on the open convex set S and
f(x) ≥f(a) +
 ∂
∂a′ f(a)

(x −a)
for all x ∈S and a ∈S, then f(x) is a convex function.
9.36 Let f(x) be a real-valued function defined for all x ∈S, where S is an open
convex subset of Rm, and suppose that f(x) is a twice differentiable function
on S. Show that f(x) is a convex function if and only if the Hessian matrix Hf
is nonnegative definite at each x ∈S.

430
MATRIX DERIVATIVES AND RELATED TOPICS
9.37 Let x be a 2 × 1 vector and consider the function f(x) = xc
1x1−c
2
for all x ∈S,
where 0 < c < 1 and S = {x : x1 > 0, x2 > 0}.
(a) Use the previous exercise to show that f(x) is a concave function.
(b) Show that if y is a 2 × 1 random vector with finite first moments and sat-
isfying P(y ∈S) = 1, then
E(yα
1 y1−α
2
) ≤{E(y1)}α{E(y2)}1−α
if 0 < α < 1.
9.38 Let x be a 3 × 1 vector, and define the function
f(x) = x1 + x2 −x3.
Find the maximum and minimum of f(x) subject to the constraint x′x = 1.
9.39 Find the shortest distance from the origin to a point on the surface given by
x2
1 + x2
2 + x2
3 + 4x1 −6x3 = 2.
9.40 Let A be an m × m positive definite matrix and x be an m × 1 vector. Find the
maximum and minimum of the function
f(x) = x′x,
subject to the constraint x′Ax = 1.
9.41 Find the maximum and minimum of the function
f(x) = x1(x2 + x3),
subject to the constraints x2
1 + x2
2 = 1 and x1x3 + x2 = 2.
9.42 For an m × 1 vector x with positive components, maximize the function
f(x) = x1x2 · · · xm,
subject to the constraint x1 + x2 + · · · + xm = a, where a is some positive
number. Use this to establish the inequality
(x1x2 · · · xm)1/m ≤1
m(x1 + · · · + xm)
for all positive real numbers x1,. . ., xm.
9.43 Let A and B be m × m matrices, with A being nonnegative definite and B being
positive definite. Following the approach of Example 9.8, apply the Lagrange
method to find the maximum and minimum values of
f(x) = x′Ax
x′Bx,
over all x ̸= 0.

PROBLEMS
431
9.44 Let a be an m × 1 nonnull vector and B be an m × m positive definite matrix.
Using the results of Problem 9.43, show that for x ̸= 0,
f(x) = (a′x)2
x′Bx
has a maximum value of
a′B−1a.
We can use this result to obtain the union-intersection test (see Example 3.16)
of the multivariate hypothesis H0 : μ = μ0 against H1 : μ ̸= μ0, where μ rep-
resents the m × 1 mean vector of a population and μ0 is an m × 1 vector of
constants. Let x and S denote the sample mean vector and sample covariance
matrix computed from a sample of size n from this population. Show that if we
base the union-intersection procedure on the univariate t statistic
t = (x −μ0)
s/√n
for testing H0 : μ = μ0, then the union-intersection test can be based on T 2 =
n(x −μ0)′S−1(x −μ0).
9.45 Suppose that x1,. . ., xn are independent and identically distributed random
variables with mean μ and variance σ2. Consider a linear estimator of μ, which
is any estimator of the form ˆμ =  aixi, where a1,. . ., an are constants.
(a) For what values of a1,. . ., an will ˆμ be an unbiased estimator of μ?
(b) Use the method of Lagrange multipliers to show that the sample mean x is
the best linear unbiased estimator of μ; that is, x has the smallest variance
among all unbiased linear estimators of μ.
9.46 A random process involves n independent trials, where each trial can result in
one of k distinct outcomes. Let pi denote the probability that a trial results in
outcome i and note that then p1 + · · · + pk = 1. Define the random variables,
x1,. . ., xk, where xi counts the number of times that outcome i occurs in the n
trials. Then the random vector x = (x1,. . ., xk)′ has the multinomial distribu-
tion with probability function given by
P(x1 = n1,. . ., xk = nk) =
n!
n1! · · · nk!pn1
1 · · · pnk
k ,
where n1,. . ., nk are nonnegative integers satisfying n1 + · · · + nk = n. Find
the maximum likelihood estimate of p = (p1,. . ., pk)′.
9.47 Suppose that the m × m positive definite covariance matrix Ω is partitioned in
the form
Ω =

Ω11 Ω12
Ω′
12 Ω22

,
where Ω11 is m1 × m1, Ω22 is m2 × m2, and m1 + m2 = m. Suppose also
that the m × 1 random vector x has covariance matrix Ω and is partitioned as

432
MATRIX DERIVATIVES AND RELATED TOPICS
x = (x′
1, x′
2)′, where x1 is m1 × 1 and x2 is m2 × 1. If the m1 × 1 vector a
and m2 × 1 vector b are vectors of constants, then the square of the correlation
between the random variables u = a′x1 and v = b′x2 is given by
f(a, b) =
(a′Ω12b)2
a′Ω11ab′Ω22b.
Show that the maximum value of f(a, b), that is, the maximum squared
correlation between u and v, subject to the constraints
a′Ω11a = 1,
b′Ω22b = 1,
is the largest eigenvalue of Ω−1
11 Ω12Ω−1
22 Ω′
12 or, equivalently, the largest eigen-
value of Ω−1
22 Ω′
12Ω−1
11 Ω12. What are the vectors a and b that yield this maximum?
9.48 Consider the multiple regression model y = Xβ + ϵ, where E(ϵ) = 0,
var(ϵ) = σ2IN, and the N × (k + 1) matrix X has rank k + 1. A linear
estimator of β has the form ˜β = Ly, where L is a (k + 1) × N matrix of
constants.
(a) What is the condition on L for ˜β to be an unbiased estimator of β?
(b) Show that the minimum variance unbiased linear estimator of β is the least
squares estimator ˆβ = (X′X)−1X′y. That is, show that tr{var(˜β)} is min-
imized by choosing L = (X′X)−1X′.

10
INEQUALITIES
10.1
INTRODUCTION
We have already encountered a number of inequalities in this text. For instance, we
saw the Cauchy-Schwarz inequality in Chapter 2, the Poincaré separation theorem in
Chapter 3, the Hadamard inequality in Chapter 8 and Jensen’s inequality in Chapter 9,
just to name a few of them. In this chapter, we present some additional important clas-
sical inequalities along with some extensions or generalizations of them. We begin
the chapter with an introduction to the theory of majorization, a concept which has
proven useful in developing a number of inequalities.
10.2
MAJORIZATION
Majorization is a preordering of the vectors in Rm. It can be viewed as an attempt
to make precise the notion that the components of one vector x are less spread out
than the components of another vector y. As an example, we may be comparing the
vector x = m−11m to the vector y = e1,m. In this section, we develop some of the
basic results regarding majorization and a few of its applications to the eigenvalues
of symmetric matrices. Additional results and applications can be found in Marshall
et al. (2011), Ando (1989, 1994), and Bhatia (1997).
In defining majorization, we will need to refer to the ordered components of
vectors. For a vector x = (x1, . . . , xm)′, denote its decreasing components by
x[1] ≥· · · ≥x[m] and its increasing components by x(1) ≤· · · ≤x(m).
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

434
INEQUALITIES
Definition 10.1
A vector x is said to be majorized by a vector y, indicated by
x ≺y, and y is said to majorize x, indicated by y ≻x, if
k

i=1
x[i] ≤
k

i=1
y[i],
k = 1, . . . , m −1,
m

i=1
xi =
m

i=1
yi,
or, equivalently,
k

i=1
x(i) ≥
k

i=1
y(i),
k = 1, . . . , m −1,
m

i=1
xi =
m

i=1
yi.
Clearly, for any x, x ≺x, and for any x, y, and z satisfying x ≺y and y ≺z,
then x ≺z. Also, notice that a majorization relationship between x and y is not
dependent on the ordering of the components of x or y. So, for instance, if P and Q
are m × m permutation matrices, then x ≺y if and only if Px ≺Qy. As a result, for
notational convenience, in some situations we will assume without loss of generality
that the components of x and y have the ordering x1 ≥· · · ≥xm and y1 ≥· · · ≥ym.
As an example, consider the m × 1 vectors xk = k−1 k
i=1 ei,m, k = 1, . . . , m.
Then m−11m = xm ≺xm−1 ≺· · · ≺x1 = e1,m, and for any m × 1 vector x hav-
ing nonnegative components and satisfying 1′
mx = 1, xm ≺x ≺x1. However, a
majorization relationship does not exist for every pair of vectors. For instance, if
x = 2
3e1,m + 1
6e2,m + 1
6e3,m, then x does not majorize x2 nor does x2 majorize x.
There is an important connection between majorization and doubly stochas-
ticmatrices. An m × m matrix P is doubly stochastic if it has nonnegative
components and P1m = P ′1m = 1m. We first establish the relation between
x = Py and y.
Theorem 10.1
If y is an m × 1 vector and P is an m × m doubly stochastic matrix,
then x = Py is majorized by y.
Proof.
We assume y1 ≥· · · ≥ym since, otherwise, we may replace y and P by Qy
and PQ′, where Q is a permutation matrix for which Qy has decreasing components.
Since 1′
mP = 1′
m, it immediately follows that
m

i=1
xi = 1′
mx = 1′
my =
m

i=1
yi.

MAJORIZATION
435
Note that x[i] = m
j=1 phijyj for some choice of hi, so k
i=1 x[i] = m
j=1 wjkyj,
where wjk = k
i=1 phij ≤m
i=1 pij = 1 and m
j=1 wjk = k. Consequently, we
have
k

i=1
y[i] −
k

i=1
x[i] =
k

i=1
yi −
m

i=1
wikyi
=
k

i=1
(1 −wik)yi +
k

i=1
wikyi −
m

i=1
wikyi
=
k

i=1
(1 −wik)yi −
m

i=k+1
wikyi
≥yk

k −
k

i=1
wik

−yk+1
m

i=k+1
wik
= (yk −yk+1)
m

i=k+1
wik ≥0,
and so the proof is complete.
□
Our next result will make use of a special linear transformation known as a
T-transform. It utilizes the matrix
T = λIm + (1 −λ)Q,
where 0 ≤λ ≤1 and Q is a permutation matrix that interchanges two components. It
is easily observed that T is doubly stochastic since Q is doubly stochastic. Theorem
10.1 indicates that the relationship x = Py, where P is doubly stochastic, is a suffi-
cient condition for x ≺y. Theorem 10.2 tells us it is also a necessary condition.
Theorem 10.2
If x is majorized by y, then there exists a doubly stochastic matrix
P such that x = Py.
Proof.
We assume without loss of generality that x1 ≥· · · ≥xm and y1 ≥· · · ≥
ym, and let r denote the number of nonzero differences yi −xi. If r = 0, the result
is trivial since x = Imy. We will prove the result by induction, assuming it holds
when r < h and proving that it holds when r = h. Thus, for the remainder of the
proof, we have x ≺y with exactly h > 0 nonzero differences yi −xi. We will find
a T-transform matrix T such that x ≺z = Ty and the number of nonzero differ-
ences zi −xi is less than h. Then the assumed result will yield x = P z = P T y for
some doubly stochastic matrix P and the proof will be complete since PT is also
doubly stochastic. Let j be the largest index for which xj < yj and k be the smallest

436
INEQUALITIES
index greater than j for which xk > yk. Such a j must exist since r = h > 0 and y
majorizes x, while a corresponding k is guaranteed by the fact that xi > yi for the
largest index i satisfying xi ̸= yi. As a result, we have
yj > xj ≥xk > yk,
(10.1)
and
xi = yi,
i = j + 1, . . . , k −1.
(10.2)
Let λ = 1 −δ/(yj −yk), where δ = min(yj −xj, xk −yk), so that 0 < λ < 1 fol-
lows from (10.1). Then using T = λIm + (1 −λ)Q, where Q is the permutation
matrix obtained from the identity matrix by interchanging the jth and kth rows, we
find that
z = Ty = (y1, . . . , yj−1, λyj + (1 −λ)yk, yj+1, . . . , yk−1,
λyk + (1 −λ)yj, yk+1, . . . , ym)′
= (y1, . . . , yj−1, yj −δ, yj+1, . . . , yk−1, yk + δ, yk+1, . . . , ym)′.
We will now show that x ≺z. First note that since xj ≤zj = yj −δ and xk ≥zk =
yk + δ, we have
zj−1 = yj−1 ≥yj > zj ≥xj ≥xj+1 = yj+1 = zj+1,
zk−1 = yk−1 = xk−1 ≥xk ≥zk > yk ≥yk+1 = zk+1;
that is, the components of z are arranged in order like those of x and y. Since y
majorizes x, we find that
l

i=1
zi =
l

i=1
yi ≥
l

i=1
xi,
(10.3)
for l = 1, . . . , j −1 and l = k, . . . , m, with equality when l = m. Using (10.3)
when l = j −1 and the fact that zj ≥xj, we have
l

i=1
zi ≥
l

i=1
xi
(10.4)
when l = j, while (10.2) then guarantees that (10.4) holds for l = j + 1, . . . , k −1,
and so we have established that x ≺z. All that remains is to show that the number of
nonzero differences zi −xi is less than h. Since zj = xj if δ = yj −xj and zk = xk
if δ = xk −yk, it follows that the number of nonzero differences zi −xi is h −1 or
h −2.
□

MAJORIZATION
437
The following is an immediate consequence of the proof of Theorem 10.2.
Corollary 10.2.1
If x ≺y, then there exist a finite number of T-transforms,
T1, . . . , Tn, such that x = Py, where P = T1 · · · Tn.
Our next result shows that there is a majorization relationship between the vector
of diagonal elements of a symmetric matrix and the vector of its eigenvalues.
Theorem 10.3
Let A be an m × m symmetric matrix with diagonal elements
a11, . . . , amm and eigenvalues λ1(A) ≥· · · ≥λm(A). If a = (a11, . . . , amm)′
and λ = (λ1(A), . . . , λm(A))′, then
a ≺λ.
Proof.
We may assume that a11 ≥· · · ≥amm since, if this is not the case, there is a
permutation matrix P for which PAP ′ has nonincreasing diagonal elements and the
same eigenvalues as A. For k = 1, . . . , m −1, let Ak be the leading k × k principal
submatrix of A. From Theorem 3.20, λi(Ak) ≤λi(A) for i = 1, . . . , k, so we have
k

i=1
aii = tr(Ak) =
k

i=1
λi(Ak) ≤
k

i=1
λi(A).
The equality part of Definition 10.1 follows immediately from the fact that tr(A) =
m
i=1 λi(A).
□
Our next result gives the converse of Theorem 10.3
Theorem 10.4
Suppose x and y are m × 1 vectors and x ≺y. Then there exists
an m × m symmetric matrix A with diagonal elements x1, . . . , xm and eigenvalues
y1, . . . , ym.
Proof.
Our proof follows that of Horn and Johnson (2013). Without loss of gener-
ality we assume that x1 ≥· · · ≥xm and y1 ≥· · · ≥ym. Note that since x ≺y, it
follows that y1 ≥x1 ≥xm ≥ym. Thus, the result immediately follows if y1 = ym,
since in this case x = y = y11m and the required matrix is given by A = y1Im. For
the remainder of the proof we assume that y1 > ym. We first consider the case in
which m = 2, so that y1 ≥x1 ≥x2 ≥y2 with y1 > y2. Now the matrix
P =
1
√y1 −y2
 √y1 −x2
√x2 −y2
−√x2 −y2
√y1 −x2

is orthogonal, and so if Λ = diag(y1, y2), then A = PΛP ′ is a symmetric matrix with
eigenvalues y1 and y2. The diagonal elements of A reduce to y1 + y2 −x2 and x2,
but since we must have x1 + x2 = y1 + y2, we see the diagonal elements are in fact
x1 and x2 as required. This proves the result for m = 2, and we will prove the result

438
INEQUALITIES
for larger m by induction. Suppose that m > 2 and assume that the result holds for
values less than or equal to m −1. Let k be the largest index for which yk ≥x1. Such
a k must exist since y1 ≥x1. Note that if k = m, then ym ≥x1 ≥xm ≥ym and so
x1 = · · · = xm = ym. But then yi −x1 ≥ym −x1 ≥0 and
m

i=1
(yi −x1) =
m

i=1
(yi −xi) =
m

i=1
yi −
m

i=1
xi = 0,
imply
each
yi = x1,
and
this
contradicts
the
assumption
that
y1 > ym.
Thus,
k ≤m −1
and
so
yk ≥x1 > yk+1 ≥ym.
This
and
the
identity
x1 + (yk + yk+1 −x1) = yk + yk+1
show
that
u = (x1, yk + yk+1 −x1)′ ≺
v = (yk, yk+1)′. Since yk > yk+1, we can use the construction for the case m = 2
to obtain an orthogonal matrix P1 such that P1DvP ′
1 has diagonal elements u1 = x1
and u2 = yk + yk+1 −x1. Letting z′ = (u2, z2)′, where z2 is the (m −2) × 1
vector obtained from y by removing yk and yk+1, we have

P1
(0)
(0) Im−2
 
Dv
(0)
(0) Dz2
 
P ′
1
(0)
(0) Im−2

=

x1
a′
a Dz

for some (m −1) × 1 vector a. The result will then follow if we can find an
(m −1) × (m −1) orthogonal matrix P2 such that P2DzP ′
2 has diagonal elements
x2, . . . , xm. Since we are assuming the result holds for m −1, the proof will be
complete if we can show w = (x2, . . . , xm)′ ≺z. Note that
yk = u2 + (x1 −yk+1) > u2 = yk+1 + (yk −x1) ≥yk+1.
(10.5)
First suppose that k = 1. It follows from (10.5) that the components of z, like those
of w, are nondecreasing. Then for h = 1, . . . , m −1,
h

i=1
zi = u2 +
h

i=2
yi+1 = y1 + y2 −x1 +
h+1

i=3
yi
=
h+1

i=1
yi −x1 ≥
h+1

i=1
xi −x1 =
h+1

i=2
xi =
h

i=1
wi,
with equality if h = m −1. Now suppose k > 1, in which case the ordered compo-
nents of z are y1 ≥· · · ≥yk−1 ≥u2 ≥yk+2 ≥· · · ≥ym. Then
h

i=1
z[i] =
h

i=1
yi ≥
h

i=1
xi ≥
h

i=1
xi+1 =
h

i=1
wi,

MAJORIZATION
439
for h = 1, . . . , k −1,
k

i=1
z[i] =
k−1

i=1
yi + u2 =
k−1

i=1
yi + yk + yk+1 −x1 =
k+1

i=1
yi −x1
≥
k+1

i=1
xi −x1 =
k+1

i=2
xi =
k

i=1
wi,
(10.6)
and
h

i=1
z[i] =
k−1

i=1
yi + u2 +
h

i=k+1
yi+1 =
k−1

i=1
yi + yk + yk+1 −x1 +
h+1

i=k+2
yi
=
h+1

i=1
yi −x1 ≥
h+1

i=1
xi −x1 =
h+1

i=2
xi =
h

i=1
wi,
(10.7)
for h = k + 1, . . . , m −1. Note that we have equality in (10.6) when k = m −1
and equality in (10.7) when h = m −1, so we have shown that w ≺z.
□
Our next result gives a majorization relationship between the sum of the two vec-
tors of eigenvalues of two symmetric matrices and the vector of eigenvalues of the
sum of the two matrices.
Theorem 10.5
Let A and B be m × m symmetric matrices. If the ith components
of a, b and c are λi(A), λi(B) and λi(A + B), respectively, then c ≺(a + b).
Proof.
The result can be proven using the extremal properties of eigenvalues. If P
is an m × k semiorthogonal matrix, then for k = 1, . . . , m, we have
k

i=1
ci =
k

i=1
λi(A + B) = max
P ′P =Ik tr{P ′(A + B)P}
= max
P ′P =Ik{tr(P ′AP) + tr(P ′BP)}
≤max
P ′P =Ik tr(P ′AP) + max
P ′P =Ik tr(P ′BP)
=
k

i=1
λi(A) +
k

i=1
λi(B) =
k

i=1
(ai + bi).
We have equality when k = m since tr(A + B) = tr(A) + tr(B).
□

440
INEQUALITIES
The result given in Theorem 10.5 holds when the components of a and b have
been ordered in the same manner. When one has nondecreasing components and the
other has nonincreasing components, we get the opposite majorization relationship.
Theorem 10.6
Let A and B be m × m symmetric matrices. If the ith components
of a and c are λi(A) and λi(A + B), respectively, and the ith component of b is
λm−i+1(B), then (a + b) ≺c.
Proof.
If P is an m × k semiorthogonal matrix, then for k = 1, . . . , m, we have
k

i=1
ci =
k

i=1
λi(A + B) = max
P ′P =Ik tr{P ′(A + B)P}
= max
P ′P =Ik{tr(P ′AP) + tr(P ′BP)}
≥max
P ′P =Ik{tr(P ′AP) + min
P ′P =Ik tr(P ′BP)}
= max
P ′P =Ik

tr(P ′AP) +
k

i=1
λm−i+1(B)
	
=
k

i=1
λi(A) +
k

i=1
λm−i+1(B) =
k

i=1
(ai + bi).
We have equality when k = m since tr(A + B) = tr(A) + tr(B).
□
Our next result gives upper and lower bounds for tr(AB) in terms of the eigen-
values of A and B.
Theorem 10.7
If A and B are m × m symmetric matrices, then
m

i=1
λi(A)λm−i+1(B) ≤tr(AB) ≤
m

i=1
λi(A)λi(B).
(10.8)
Proof.
Let A = PΛP ′ be the spectral decomposition of A, so that P is an m × m
orthogonal matrix and Λ = diag(λ1(A), . . . λm(A)). Then if C = P ′BP,
tr(AB) = tr(PΛP ′B) = tr(ΛP ′BP)
= tr(ΛC) =
m

i=1
λi(A)cii.
(10.9)

MAJORIZATION
441
Since the eigenvalues of C are the same as those of B, from Theorem 10.3, c ≺b,
where c = (c11, . . . , cmm)′ and b = (λ1(B), . . . , λm(B))′. Using this along with
the fact that λi(A) −λi+1(A) is nonnegative, we have
m

i=1
λi(A)cii = λ1(A)c11 +
m

i=2
λi(A)
⎛
⎝
i

j=1
cjj −
i−1

j=1
cjj
⎞
⎠
= λ1(A)c11 +
m

i=2
λi(A)
i

j=1
cjj −
m−1

i=1
λi+1(A)
i

j=1
cjj
=
m−1

i=1
{λi(A) −λi+1(A)}
i

j=1
cjj + λm(A)
m

j=1
cjj
≤
m−1

i=1
{λi(A) −λi+1(A)}
i

j=1
λj(B) + λm(A)
m

j=1
cjj
=
m−1

i=1
{λi(A) −λi+1(A)}
i

j=1
λj(B) + λm(A)
m

j=1
λj(B)
= λ1(A)λ1(B) +
m

i=2
λi(A)
i

j=1
λj(B)
−
m−1

i=1
λi+1(A)
i

j=1
λj(B)
= λ1(A)λ1(B) +
m

i=2
λi(A)
⎛
⎝
i

j=1
λj(B) −
i−1

j=1
λj(B)
⎞
⎠
=
m

i=1
λi(A)λi(B).
(10.10)
Combining (10.9) and (10.10) yields the upper bound in (10.8). Applying this upper
bound to A and −B, we get
−tr(AB) ≤
m

i=1
λi(A)λi(−B) = −
m

i=1
λi(A)λm−i+1(B),
which is equivalent to the lower bound in (10.8).
□
Theorem 10.7 can be used to obtain a more general result regarding a partial sum
of eigenvalues of a matrix product.

442
INEQUALITIES
Theorem 10.8
Let A and B be m × m symmetric matrices with A being nonneg-
ative definite. Then
k

i=1
λi(AB) ≥
k

i=1
λi(A)λm−i+1(B),
for k = 1, . . . , m.
Proof.
Let A = PDP ′ be the spectral decomposition of A, where the diagonal
matrix is given by D = diag(λ1(A), . . . , λm(A)), and let D∗be the m × m diago-
nal matrix D∗= diag(λ1(A), . . . , λk(A), 0, . . . , 0). It follows from Problem 3.12
that the eigenvalues of AB are the same as those of D1/2P ′BPD1/2. Using this and
Problem 3.57, we have
k

i=1
λi(AB) =
k

i=1
λi(D1/2P ′BPD1/2)
= max
C′C=Ik tr(C′D1/2P ′BPD1/2C)
≥tr

[Ik, (0)]D1/2P ′BPD1/2

Ik
(0)
	
= tr

D1/2

Ik (0)
(0) (0)

D1/2P ′BP

= tr(D∗P ′BP)
≥
m

i=1
λi(D∗)λm−i+1(P ′BP) =
k

i=1
λi(A)λm−i+1(B),
where the last inequality follows from Theorem 10.7.
□
Majorization can be combined with order-preserving functions to develop numer-
ous useful inequalities. A real-valued function φ(x) defined for all x ∈S, where S is
a subset of Rm, is said to be order-preserving if x ≺y implies that φ(x) ≤φ(y). We
will focus on functions φ(x) that have the form φ(x) = m
i=1 g(xi), where g(x) is a
real-valued scalar function. Our final result of this section gives a sufficient condition
for this type of function to be order-preserving.
Theorem 10.9
Let g be a real-valued convex function defined on an interval S in
R. If x ≺y, then
m

i=1
g(xi) ≤
m

i=1
g(yi).
(10.11)
In addition, if g is strictly convex on S, then we have equality in (10.11) if and only
if x[i] = y[i] for i = 1, . . . , m.

MAJORIZATION
443
Proof.
We assume without loss of generality that x1 ≥· · · ≥xm and y1 ≥· · · ≥
ym. It follows from Corollary 10.2.1 that there are m × 1 vectors x1, . . . , xn−1 such
that
x = x0 ≺x1 ≺· · · ≺xn−1 ≺xn = y
and xi−1 and xi have at most 2 components that differ. Consequently, it will suffice
to prove the result for m = 2. Since x ≺y, there exists a doubly stochastic matrix P
such that x = Py. Every 2 × 2 doubly stochastic matrix has the form
P =

α
1 −α
1 −α
α

for some α ∈[0, 1], so we have x1 = αy1 + (1 −α)y2 and x2 = (1 −α)y1 + αy2.
Then since g is convex
g(x1) + g(x2) = g(αy1 + (1 −α)y2) + g((1 −α)y1 + αy2)
≤{αg(y1) + (1 −α)g(y2)} + {(1 −α)g(y1) + αg(y2)}
= g(y1) + g(y2),
which establishes (10.11). Clearly, if x = y, we have equality in (10.11). Next
assume we have equality in (10.11) and note that this requires
g(αy1 + (1 −α)y2) = αg(y1) + (1 −α)g(y2),
and
g((1 −α)y1 + αy2) = (1 −α)g(y1) + αg(y2).
But if g is strictly convex these identities only hold if x1 = y1 and x2 = y2.
□
Example 10.1
As a simple example of Theorem 10.9, we consider the strictly con-
vex function g(x) = (x −a)2, where a is a real number. Then it follows that if x ≺y,
m

i=1
(xi −a)2 ≤
m

i=1
(yi −a)2,
with equality if and only if x[i] = y[i], for i = 1, . . . , m. In particular, if we choose
a = x = m−1 m
i=1 xi, so that a = y also, we have
s2
x =
1
m −1
m

i=1
(xi −x)2 ≤
1
m −1
m

i=1
(yi −y)2 = s2
y
(10.12)
if x ≺y. Since x ≺y means that the components of x are less spread out than those
of y in some sense, namely as indicated by Definition 10.1, the inequality in (10.12)
is not surprising.

444
INEQUALITIES
10.3
CAUCHY-SCHWARZ INEQUALITIES
We will start by restating the standard Cauchy-Schwarz inequality that was presented
in Section 2.2 and give an alternative proof.
Theorem 10.10
Suppose that x and y are m × 1 vectors. Then
(x′y)2 ≤(x′x)(y′y),
(10.13)
with equality if and only if one of the vectors is a scalar multiple of the other.
Proof.
If either vector is the null vector, then (10.13) holds with equality and that
null vector equals zero times the other vector. Otherwise, note that
(x′x)(y′y) −(x′y)2 = (x′x)y′Ay,
(10.14)
where A = Im −(x′x)−1xx′. Clearly, A has eigenvalues of 1 and 0 with multiplici-
ties of m −1 and 1. This guarantees that (10.14) is nonnegative and so (10.13) holds.
Eigenvectors of A corresponding to the 0 eigenvalue are of the form cx for c ̸= 0,
and so (10.14) equals 0, and hence (10.13) holds with equality, if and only if y has
this same form.
□
A simple extension of the standard Cauchy-Schwarz inequality is given next.
Theorem 10.11
If x and y are m × 1 vectors and A is an m × m positive definite
matrix, then
(x′y)2 ≤(x′Ax)(y′A−1y),
with equality if and only if one of the vectors, Ax and y, is a scalar multiple of the
other.
Proof.
Since A is positive definite, there exists a nonsingular matrix T such that
A = T ′T and A−1 = T −1T −1′. Defining u = Tx and v = T −1′y, we find that
(x′y)2 = (x′T ′T −1′y)2 = (u′v)2 ≤(u′u)(v′v)
= (x′T ′Tx)(y′T −1T −1′y) = (x′Ax)(y′A−1y),
where the inequality follows from Theorem 10.10. We have equality if and only if
one of the vectors, u = Tx and v = T −1′y, is a scalar multiple of the other. Premul-
tiplying by T ′, we see this is equivalent to saying that one of the vectors, Ax and y,
is a scalar multiple of the other.
□

CAUCHY-SCHWARZ INEQUALITIES
445
The rest of this section is devoted to matrix versions of the Cauchy-Schwarz
inequality. We first consider one involving the trace.
Theorem 10.12
If A and B are both m × n matrices, then
{tr(A′B)}2 ≤{tr(A′A)}{tr(B′B)},
with equality if and only if one of the matrices is a scalar multiple of the other.
Proof.
Using Theorem 8.10, we can write tr(A′B) = x′y, tr(A′A) = x′x and
tr(B′B) = y′y, where x = vec(A) and y = vec(B). The result then follows
immediately from Theorem 10.10.
□
Before giving a general Cauchy-Schwarz inequality involving determinants, we
first consider the following special case.
Theorem 10.13
Suppose that P and Q are both m × n semiorthogonal matrices
with n ≤m. Then
|P ′Q|2 ≤1,
with equality if and only if PP ′ = QQ′.
Proof.
We have
|P ′Q|2 = |Q′PP ′Q| ≤|Q′PP ′Q + Q′(Im −PP ′)Q| = |In| = 1,
(10.15)
where the inequality follows from Theorem 4.17 since Q′PP ′Q and Q′(Im −PP ′)Q
are both nonnegative definite. From that same theorem, we see that we have equality
in (10.15) if and only if Q′(Im −PP ′)Q = (0) which is equivalent to the condition
PP ′ = QQ′.
□
Next we have the generalization of Theorem 10.13 to arbitrary real matrices.
Theorem 10.14
Suppose that both A and B are m × n matrices. Then
|A′B|2 ≤|A′A| |B′B|,
(10.16)
with equality if and only if rank(A) < n or rank(B) < n, or B = AC for some
nonsingular matrix C.
Proof.
Clearly the inequality holds when |A′B| = 0 and, in this case, equality holds
if and only if rank(A) < n or rank(B) < n. For the remainder of the proof we

446
INEQUALITIES
assume |A′B| ̸= 0. Using the singular value decomposition we can write A and B
as A = P1D1Q1 and B = P2D2Q2, where the m × n matrix Pi and n × n matrix
Qi satisfy P ′
iPi = Q′
iQi = In, and Di is an n × n diagonal matrix with positive
diagonal elements. It then follows that
|A′B|2 = |Q′
1D1P ′
1P2D2Q2|2 = |D1|2|D2|2|P ′
1P2|2,
while |A′A| = |D1|2 and |B′B| = |D2|2. Thus, (10.16) follows directly from
Theorem 10.13. Also from Theorem 10.13, we have equality if and only if
P1P ′
1 = P2P ′
2, and since this is equivalent to A and B having the same column
space, the proof is complete.
□
10.4
HÖLDER’S INEQUALITY
We start with one version of an inequality known as Hölder’s inequality. If x and y are
m × 1 vectors with nonnegative components and α is a scalar satisfying 0 < α < 1,
then
m

i=1
xα
i y1−α
i
≤
 m

i=1
xi
α m

i=1
yi
1−α
,
(10.17)
with equality if and only if one of the vectors is a scalar multiple of the other. A proof
of this result, along with some extensions to more than two vectors, can be found in
Hardy et al. (1952). We will first use Hölder’s inequality to prove the following result.
Theorem 10.15
Suppose a is an m × 1 vector with nonnegative components and
1
p + 1
q = 1, where p > 1. Then
m

i=1
aibi ≤
 m

i=1
ap
i
1/p
(10.18)
for every m × 1 vector b having nonnegative components and satisfying
m
i=1 bq
i = 1. We have equality in (10.18) if and only if a = 0 or
bq
i =
ap
i
m

j=1
ap
j
,
for i = 1, . . . , m.
Proof.
Clearly both sides of (10.18) reduce to 0 when a = 0, so for the remainder
of the proof we assume a ̸= 0. Let xi = ap
i , yi = bq
i , and α = 1/p so that ai = xα
i
and bi = y1−α
i
. Then using (10.17), we have

HÖLDER’S INEQUALITY
447
m

i=1
aibi =
m

i=1
xα
i y1−α
i
≤
 m

i=1
xi
α m

i=1
yi
1−α
=
 m

i=1
ap
i
1/p m

i=1
bq
i
1/q
=
 m

i=1
ap
i
1/p
.
Further, we have equality if and only if x = (ap
1, . . . , ap
m)′ and y = (bq
1, . . . , bq
m)′
are scalar multiples of one another which, due to the constraint m
i=1 bq
i = 1, implies
that
bq
i =
ap
i
m

j=1
ap
j
.
□
In this section, we will look at some matrix versions of Hölder’s inequality. Our
first of these results involves determinants and is due to Fan (1950).
Theorem 10.16
Suppose that A and B are m × m nonnegative definite matrices
and α is a scalar satisfying 0 < α < 1. Then
|A|α|B|1−α ≤|αA + (1 −α)B|,
(10.19)
with equality if and only if A = B or αA + (1 −α)B is singular.
Proof.
Since αA + (1 −α)B is also nonnegative definite, (10.19) clearly holds
when A or B is singular, with equality if and only if αA + (1 −α)B is also singular.
For the remainder of the proof we assume that both A and B are positive definite.
Using Theorem 4.14, we can write A = TΛT ′ and B = TT ′, where T is a nonsin-
gular matrix, Λ = diag(λ1, . . . , λm), and λ1, . . . , λm are the eigenvalues of B−1A.
Thus, the proof will be complete if we can show that
|Λ|α =
m

i=1
λα
i ≤|αΛ + (1 −α)Im| =
m

i=1
(αλi + 1 −α),
with equality if and only if Λ = Im. This result is easily confirmed by showing the
function g(λ) = αλ + 1 −α −λα is minimized at λ = 1 when 0 < α < 1.
□
For nonnegative definite matrices A and B, we will next give an upper bound
for tr(AαB1−α), where 0 < α < 1. Here the αth power of the nonnegative
definite matrix A is defined to be Aα = XΛαX′, where A = XΛX′ represents
the spectral decomposition of A and Λα = diag(λα
1 , . . . , λα
m). Before establishing

448
INEQUALITIES
the bound, we will need a preliminary result. Both of these results are due to
Magnus (1987).
Theorem 10.17
Suppose A is an m × m nonnull nonnegative definite matrix and
1
p + 1
q = 1, where p > 1. Then
tr(AB) ≤{tr(Ap)}1/p
(10.20)
for every m × m nonnegative definite matrix B satisfying tr(Bq) = 1. We have
equality in (10.20) if and only if
Bq = {tr(Ap)}−1Ap.
Proof.
Let B = PΛP ′ be the spectral decomposition of B, and note that from the
conditions of the theorem, m
i=1 λq
i = 1, where λ1 ≥· · · ≥λm are the eigenvalues
of B. Put C = P ′AP so that
tr(AB) = tr(APΛP ′) = tr(CΛ) =
m

i=1
ciiλi,
where cii is the ith diagonal element of C. Applying Theorem 10.15, we have
tr(AB) ≤
 m

i=1
cp
ii
1/p
.
(10.21)
Since (c11, . . . , cmm)′ ≺(γ1, . . . , γm)′, where γ1 ≥· · · ≥γm are the eigenvalues
of A and C, and g(x) = xp is a strictly convex function on [0, ∞) when p > 1, an
application of Theorem 10.9 leads to
m

i=1
cp
ii ≤
m

i=1
γp
i = tr(Cp) = tr(Ap).
(10.22)
Combining (10.21) and (10.22) immediately yields (10.20). From Theorem 10.15,
equality occurs in (10.21) if and only if
λq
i =
cp
ii
m
j=1 cp
jj
,
for i = 1, . . . , m, and by Theorem 10.9, we have equality in (10.22) if and only
if c11, . . . , cmm are the eigenvalues of C, that is, C = Γ = diag(γ1, . . . , γm) and
A = P ΓP ′. Thus, we have equality in (10.20) if and only if

HÖLDER’S INEQUALITY
449
Bq = PΛqP ′ =
⎛
⎝
m

j=1
cp
jj
⎞
⎠
−1
P diag(cp
11, . . . , cp
mm)P ′
=
⎛
⎝
m

j=1
γp
j
⎞
⎠
−1
P ΓpP ′ = {tr(Ap)}−1Ap,
and so the proof is complete.
□
Now we can use Theorem 10.17 to establish a matrix version of Hölder’s inequality
involving traces.
Theorem 10.18
Suppose A and B are m × m nonnull nonnegative definite matri-
ces and α is a scalar satisfying 0 < α < 1. Then
tr(AαB1−α) ≤{tr(A)}α{tr(B)}1−α,
(10.23)
with equality if and only if B = cA for some positive scalar c.
Proof.
Let p = α−1 and q = (1 −α)−1, so that 1
p + 1
q = 1 and p > 1. Define
C =
B1/q
{tr(B)}1/q
so that tr(Cq) = 1, and then by Theorem 10.17,
tr(A1/pC) ≤[tr{(A1/p)p}]1/p = {tr(A)}1/p = {tr(A)}α.
This yields (10.23) since
tr(A1/pC) = tr(A1/pB1/q)
{tr(B)}1/q
= tr(AαB1−α)
{tr(B)}1−α .
According to Theorem 10.17, we have equality in (10.23) if and only if
Cq =
(A1/p)p
tr{(A1/p)p} =
A
tr(A),
or equivalently B = {tr(B)/tr(A)}A.
□

450
INEQUALITIES
10.5
MINKOWSKI’S INEQUALITY
Minkowski’s inequality is another well-known classical inequality. If x and y are
m × 1 vectors with nonnegative components and p > 1, then
 m

i=1
(xi + yi)p
	1/p
≤
 m

i=1
xp
i
1/p
+
 m

i=1
yp
i
1/p
,
(10.24)
with equality if and only if x and y are linearly dependent. A proof of this result, along
with some extensions can be found in Hardy et al. (1952). A Minkowski inequality
for products is given by
 m

i=1
(xi + yi)
	1/m
≥
 m

i=1
xi
1/m
+
 m

i=1
yi
1/m
,
(10.25)
where again we have equality if and only if x and y are linearly dependent. The
inequality (10.25) is actually a special case of a generalized Hölder inequality; see,
for instance, Magnus and Neudecker (1999).
In this section, we will obtain some matrix analogues of Minkowski’s inequalities.
Our first result, which involves determinants, can be viewed as a generalization of
(10.25).
Theorem 10.19
Let A and B be m × m nonnull nonnegative definite matrices.
Then
|A + B|1/m ≥|A|1/m + |B|1/m,
(10.26)
with equality if and only if A + B is singular or A = cB for some c > 0.
Proof.
Since A + B is nonnegative definite, the inequality clearly holds when
both A and B are singular with equality if and only if A + B is also singular. For
the remainder of the proof, we assume without loss of generality that B is positive
definite. Applying Theorem 4.14, there exists a nonsingular matrix T such that
A = TΛT ′ and B = TT ′, where Λ = diag(λ1, . . . , λm) has the eigenvalues of
T −1/2AT −1/2′ as its diagonal elements. Since T −1/2AT −1/2′ is nonnegative definite,

MINKOWSKI’S INEQUALITY
451
λi ≥0 for i = 1, . . . , m, and so using (10.25)
|A + B|1/m = |TΛT ′ + TT ′|1/m = |T|2/m|Λ + Im|1/m
= |T|2/m
 m

i=1
(λi + 1)
	1/m
≥|T|2/m
⎧
⎨
⎩
 m

i=1
λi
1/m
+
 m

i=1
1
1/m⎫
⎬
⎭
= |T|2/m(|Λ|1/m + |Im|1/m) = |TΛT ′|1/m + |TT ′|1/m
= |A|1/m + |B|1/m,
which establishes (10.26). We have equality if and only if (λ1, . . . , λm)′ and 1m are
linearly dependent; that is, λ1 = · · · = λm = c, so that Λ = cIm and A = TΛT ′ =
cTT ′ = cB.
□
Our next result is a matrix version of (10.24) involving traces. The proof of this
result, which utilizes Theorem 10.17, is due to Magnus (1987).
Theorem 10.20
Let A and B be m × m nonnull nonnegative definite matrices and
suppose p > 1. Then
[tr{(A + B)p}]1/p ≤{tr(Ap)}1/p + {tr(Bp)}1/p,
with equality if and only if B = cA for some c > 0.
Proof.
It follows from Theorem 10.17 that for any m × m nonnull nonnegative def-
inite matrix A
max
C∈S tr(AC) = {tr(Ap)}1/p,
where
S = {C : C is m × m nonnegative deﬁnite, tr(Cq) = 1},
and q is such that 1
p + 1
q = 1. Thus
[tr{(A + B)p}]1/p = max
C∈S tr{(A + B)C}
≤max
C∈S tr(AC) + max
C∈S tr(BC)
= {tr(Ap)}1/p + {tr(Bp)}1/p,
with equality if and only if tr{(A + B)C}, tr(AC), and tr(BC) are all maximized
by the same C. According to Theorem 10.17, this requires that (A + B)p, Ap, and
Bp be proportional to one another or, equivalently, B = cA, for some scalar c.
□

452
INEQUALITIES
10.6
THE ARITHMETIC-GEOMETRIC MEAN INEQUALITY
If x1 > 0, x2 > 0, and 0 ≤α ≤1, then, since g(x) = −log(x) is a convex function
on x ∈(0, ∞),
log{αx1 + (1 −α)x2} ≥α log(x1) + (1 −α) log(x2)
= log(xα
1 x1−α
2
),
or, equivalently,
αx1 + (1 −α)x2 ≥xα
1 x(1−α)
2
.
Clearly this inequality also holds if either x1 = 0 or x2 = 0. More generally, the
weighted arithmetic-geometric mean inequality is given by
m

i=1
αixi ≥
m

i=1
xαi
i ,
where xi ≥0, αi ≥0, and m
i=1 αi = 1. The special case in which αi =
1
m leads to
1
m
m

i=1
xi ≥
 m

i=1
xi
1/m
,
which is known as the arithmetic-geometric mean inequality. We have equality in any
of these inequalities if and only if x1 = · · · = xm.
The first of our results in this section uses the arithmetic-geometric mean inequal-
ity to establish a relationship between the trace and determinant of a nonnegative
definite matrix.
Theorem 10.21
Suppose A is an m × m nonnegative definite matrix. Then
1
mtr(A) ≥|A|1/m,
with equality if and only if A = cIm for some c > 0.
Proof.
Let λ1 ≥· · · ≥λm be the eigenvalues of A. The inequality in Theorem
10.21 is a direct consequence of the arithmetic-geometric mean inequality since
1
mtr(A) = 1
m
m

i=1
λi ≥
 m

i=1
λi
1/m
= |A|1/m.
We have equality if and only if λ1 = · · · = λm or, equivalently, A is proportional to
Im.
□

PROBLEMS
453
The arithmetic-geometric mean inequality immediately extends to diagonal matri-
ces. That is, if Λ1, . . . , Λn are m × m diagonal matrices with nonnegative diagonal
elements, then
1
n
n

i=1
Λi −
 n

i=1
Λi
1/n
is nonnegative definite and reduces to the null matrix if and only if Λ1 = · · · = Λn.
Our final result is a simple extension of this result to nonnegative definite matrices.
Theorem 10.22
Let A1, . . . , An be m × m nonnegative definite matrices. If
AiAj = AjAi for all i ̸= j, then the matrix
1
n
n

i=1
Ai −
 n

i=1
Ai
1/n
is nonnegative definite and reduces to the null matrix if and only if A1 = · · · = An
Proof.
By Theorem 4.19, there exists an orthogonal matrix P
such that
Ai = PΛiP ′, where Λi = diag(λi1, . . . , λim) and λi1, . . . , λim are the not
necessarily ordered eigenvalues of Ai. Consequently,
1
n
n

i=1
Ai −
 n

i=1
Ai
1/n
= P
⎧
⎨
⎩
1
n
n

i=1
Λi −
 n

i=1
Λi
1/n⎫
⎬
⎭P ′,
and so the result follows directly from the result for diagonal matrices.
□
PROBLEMS
10.1 If x ≺y and y ≺x, how are x and y related?
10.2 Suppose x, y, and z are all m × 1 vectors, x ≺y, and the components of z
are nonnegative.
(a) Show that
m

i=1
y[i]z[i] ≥
m

i=1
x[i]z[i].
(b) Show that
m

i=1
y[i]z[m+1−i] ≤
m

i=1
x[i]z[m+1−i].
10.3 Show that if x ≺z, y ≺z, and 0 ≤λ ≤1, then λx + (1 −λ)y ≺z.
10.4 Suppose x ≺y. Show that there exists an orthogonal matrix Q such that
x = Py, where P = Q ⊙Q.

454
INEQUALITIES
10.5 Show that x ≺y if and only if
m

i=1
|xi −a| ≤
m

i=1
|yi −a|
for all real scalars a.
10.6 Show that if for some m × m matrix P, Px ≺x for every m × 1 vector x,
then P is doubly stochastic.
10.7 If P is a nonsingular doubly stochastic matrix, show that P −11m = 1m and
1′
mP −1 = 1′
m. If, in addition, P −1 is doubly stochastic, that is, it has nonneg-
ative elements, show that P is a permutation matrix.
10.8 Give an alternative proof of Theorem 10.3 by using the spectral decomposition
of A.
10.9 Let A and B be m × m symmetric matrices having the partitioned forms
A =

A11 A12
A′
12 A22

,
B =

A11 (0)
(0) A22

,
where A11 is m1 × m1 A22 is m2 × m2 and m1 + m2 = m. If the components
of a are the eigenvalues of A and the components of b are the eigenvalues of
B, show that b ≺a.
10.10 Let x be an m × 1 vector with xi > 0 for all i and m
i=1 xi = 1. Use Theorem
10.9 to show that
m

i=1

xi + 1
xi
a
≥(m2 + 1)a
ma−1
for any a > 1.
10.11 Let x be an m × 1 vector with xi > 0 for all i and x = m−1 m
i=1 xi. Show
that
m

i=1
mx −xi
xi
≥m(m −1).
10.12 Let x and y be m × 1 vectors and A be an m × m nonnegative definite matrix.
Show that
(x′Ay)2 ≤(x′Ax)(y′Ay),
with equality if and only if one of the vectors Ax and Ay is a scalar multiple
of the other.
10.13 Let A be an m × m nonnegative definite matrix and x and y be m × 1 vectors
with y being in the column space of A. Show that
(x′y)2 ≤(x′Ax)(y′A−y),
with equality if and only if one of the vectors Ax and y is a scalar multiple of
the other.

PROBLEMS
455
10.14 Let A and B be m × m positive definite matrices. Use Theorem 10.11 to show
that
y′(A + B)−1y ≤(y′A−1y)(y′B−1y)
y′(A−1 + B−1)y
for any y ̸= 0.
10.15 Let x be an m × 1 vector and A be an m × m positive definite matrix with
eigenvalues λ1 ≥· · · ≥λm. Establish the Kantorovich inequality which is
given by
(x′Ax)(x′A−1x)
(x′x)2
≤(λ1 + λm)2
4λ1λm
.
10.16 Let x and y be m × 1 vectors. If x′1m = 1 and xi ≥0 for i = 1, . . . , m,
show that
(x′y)2 ≤
m

i=1
xiy2
i ,
with equality if and only if y1 = · · · = ym.
10.17 Let A be an m × m matrix with real eigenvalues. Show that
{tr(A)}2 ≤m tr(A2),
with equality if and only the eigenvalues are all equal.
10.18 If A and B are m × n matrices, show that
tr{(A′B)2} ≤tr{(A′A)(B′B)},
with equality if and only if AB′ is symmetric.
10.19 If A and B are m × n matrices, show that
tr{(A′B)2} ≤tr{(AA′)(BB′)},
with equality if and only if A′B is symmetric.
10.20 Let A be an m × m matrix. Show that
tr(A2) ≤tr(A′A),
with equality if and only if A is symmetric.
10.21 Let A1, . . . , An be m × m positive definite matrices, and let α1, . . . , αn be
nonnegative scalars satisfying n
i=1 αi = 1. Show that
n

i=1
|Ai|αi ≤

n

i=1
αiAi
 ,
with equality if and only if A1 = · · · = An.

456
INEQUALITIES
10.22 Let A and B be m × m nonnull nonnegative definite matrices and define
C = αA + (1 −α)B −AαB1−α,
where 0 < α < 1. Use Theorem 10.18 to show that tr(C) ≥0, with equality
if and only if A = B, thereby establishing the inequality
tr(AαB1−α) ≤tr{αA + (1 −α)B}.
10.23 If A and B are m × m positive definite matrices and 0 < α < 1, show that
tr[{αA + (1 −α)B}−1] ≤α tr(A−1) + (1 −α)tr(B−1).
10.24 Let A be an m × m nonnull nonnegative definite matrix. Show that
m−1 tr(AB) ≥|A|1/m
for every postive definite m × m matrix B satisfying |B| = 1, with equality
if and only if A is nonsingular and B = |A|1/mA−1.
10.25 Give an alternative proof of Theorem 10.19 by using the result from the pre-
vious problem.
10.26 Let A and B be m × m positive definite matrices and define
C =

A
B
(0) (0)

,
D =

(0)
A′B
B′A
(0)

.
Compare the eigenvalues of 1
2C′C and D by using Theorem 3.28 to show that
σi(AB) ≤σi(A2 + B2)/2 for i = 1, . . . , m, where σ1(A) ≥· · · ≥σm(A)
denote the singular values of A, thereby extending the arithmetic-geometric
mean inequality ab ≤(a2 + b2)/2.

11
SOME SPECIAL TOPICS RELATED TO
QUADRATIC FORMS
11.1
INTRODUCTION
We have seen that if A is an m × m symmetric matrix and x is an m × 1 vector, then
the function of x, x′Ax, is called a quadratic form in x. In many statistical applica-
tions, x is a random vector, whereas A is a matrix of constants. The most common
situation is one in which x has as its distribution, or as its asymptotic distribution,
the multivariate normal distribution. In this chapter, we investigate some of the dis-
tributional properties of x′Ax in this setting. In particular, we are most interested in
determining conditions under which x′Ax will have a chi-squared distribution.
11.2
SOME RESULTS ON IDEMPOTENT MATRICES
We have noted earlier that an m × m matrix A is said to be idempotent if A2 = A. We
will see in Section 11.3 that idempotent matrices play an essential role in the discus-
sion of conditions under which a quadratic form in normal variates has a chi-squared
distribution. Consequently, this section is devoted to establishing some of the basics
results regarding idempotent matrices.
Theorem 11.1
Let A be an m × m idempotent matrix. Then
(a) Im −A is also idempotent,
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

458
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
(b) each eigenvalue of A is 0 or 1,
(c) A is diagonalizable,
(d) rank(A) = tr(A).
Proof.
Since A2 = A, we have
(Im −A)2 = Im −2A + A2 = Im −A,
and so (a) holds. Let λ be an eigenvalue of A corresponding to the eigenvector x, so
that Ax = λx. Then because A2 = A, we find that
λx = Ax = A2x = A(Ax) = A(λx) = λAx = λ2x,
which implies that
λ(λ −1)x = 0.
Since eigenvectors are nonnull vectors, we must have λ(λ −1) = 0, and so (b)
follows. Let r be the number of eigenvalues of A equal to one, so that m −r is
the number of eigenvalues of A equal to zero. As a result, A −Im must have r
eigenvalues equal to zero and m −r eigenvalues equal to −1. By Theorem 4.8, (c)
will follow if we can show that
rank(A) = r,
rank(A −Im) = m −r.
(11.1)
Now from Theorem 4.10, we know that the rank of any square matrix is at least as
large as the number of its nonzero eigenvalues, so we must have
rank(A) ≥r,
rank(A −Im) ≥m −r.
(11.2)
However, Corollary 2.10.1 gives
rank(A) + rank(Im −A) ≤rank{A(Im −A)} + m
= rank{(0)} + m = m,
which with (11.2) implies (11.1), so (c) is proven. Finally, (d) is an immediate
consequence of (b) and (c).
□
Since any matrix with at least one 0 eigenvalue has to be a singular matrix, a
nonsingular idempotent matrix has all of its eigenvalues equal to 1. However, the only
diagonalizable matrix with all of its eigenvalues equal to 1 is the identity matrix; that
is, the only nonsingular m × m idempotent matrix is Im.
If A is a diagonal matrix, that is, A has the form diag(a1, . . . , am), then A2 =
diag(a2
1, . . . , a2
m). Equating A and A2, we find that a diagonal matrix is idempotent

SOME RESULTS ON IDEMPOTENT MATRICES
459
if and only if each diagonal element is 0 or 1, which is, of course, also an immediate
consequence of Theorem 11.1(b).
Example 11.1
Although an idempotent matrix has each of its eigenvalues equal to
1 or 0, the converse is not true; that is, a matrix having only eigenvalues of 1 and 0
need not be an idempotent matrix. For instance, it is easily verified that the matrix
A =
⎡
⎣
0
1
0
0
0
0
0
0
1
⎤
⎦
has eigenvalues 0 and 1 with multiplicities 2 and 1, respectively. However,
A2 =
⎡
⎣
0
0
0
0
0
0
0
0
1
⎤
⎦,
so that A is not idempotent.
The matrix A in Example 11.1 is not idempotent because it is not diagonalizable.
In other words, an m × m matrix A is idempotent if and only if each of its eigenvalues
is 0 or 1 and it is diagonalizable. In fact, we have a special case in Theorem 11.2.
Theorem 11.2
Let A be an m × m symmetric matrix. Then A is idempotent if and
only if each eigenvalue of A is 0 or 1.
Proof.
Let A = XΛX′ be the spectral decomposition of A, so that X is an orthog-
onal matrix and Λ is diagonal. Then
A2 = (XΛX′)2 = XΛX′XΛX′ = XΛ2X′.
Clearly, this equals A if and only if each diagonal element of Λ, that is, each eigen-
value of A, is 0 or 1.
□
Theorem 11.2 is generalized in Corollary 11.2.1.
Corollary 11.2.1
Let A be an m × m symmetric matrix and B be an m × m pos-
itive definite matrix. Then AB is idempotent if and only if each eigenvalue of AB is
0 or 1.
Proof.
Since B is positive definite, it can be expressed as B = TT ′, where T is an
m × m nonsingular matrix. Note that if the equation
ABAB = AB
(11.3)

460
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
is premultiplied by T ′ and postmultiplied by T −1′, it yields
T ′ATT ′AT = T ′AT.
(11.4)
Conversely, premultiplying (11.4) by T −1′ and postmultiplying by T ′, we get (11.3).
That is, AB is idempotent if and only if T ′AT is idempotent. Since AB and T ′AT
have the same eigenvalues by Theorem 3.2(d), the result follows immediately from
Theorem 11.2.
□
Theorem 11.3 gives some conditions for the sum of two idempotent matrices and
the product of two idempotent matrices to be idempotent.
Theorem 11.3
Let A and B be m × m idempotent matrices. Then
(a) A + B is idempotent if and only if AB = BA = (0),
(b) AB is idempotent if AB = BA.
Proof.
Since A and B are idempotent, we have
(A + B)2 = A2 + B2 + AB + BA = A + B + AB + BA,
so that A + B will be idempotent if and only if
AB = −BA.
(11.5)
Premultiplication of (11.5) by B and postmultiplication by A yields the identity
(BA)2 = −BA,
(11.6)
since A and B are idempotent. Similarly, premultiplying (11.5) by A and postmulti-
plying by B, we also find that
(AB)2 = −AB.
(11.7)
Thus, it follows from (11.6) and (11.7) that both −BA and −AB are idempotent
matrices, and because of (11.5), so then are AB and BA. Part (a) now follows because
the null matrix is the only idempotent matrix whose negative is also idempotent. To
prove (b), note that if A and B commute under multiplication, then
(AB)2 = ABAB = A(BA)B = A(AB)B = A2B2 = AB,
and so the result follows.
□
Example 11.2
The conditions given in Theorem 11.3 for (A + B) to be idempotent
are necessary and sufficient, whereas the condition given for AB to be idempotent is

SOME RESULTS ON IDEMPOTENT MATRICES
461
only sufficient. We can illustrate that this second condition is not necessary through
a simple example. Let A and B be defined as
A =

1
1
0
0

,
B =

0
0
1
1

,
and observe that A2 = A and B2 = B, so that both A and B are idempotent. In
addition, AB = A, so that AB is also idempotent. However, AB ̸= BA because
BA = B.
It is easily verified that a matrix having the form A = B(C′B)−1C′ is idempotent.
Our next result shows that every idempotent matrix can be expressed in this way.
Theorem 11.4
Suppose A is an m × m idempotent matrix of rank r. Then there
exist m × r matrices B and C such that
A = B(C′B)−1C′.
Proof.
It follows from the singular value decomposition that A can be expressed
as A = BDC′, where B and C are m × r full-rank matrices and D is a nonsingular
r × r matrix. Since A is idempotent, we have
BDC ′ = BDC ′BDC ′.
Premultiplying this by (B ′B)−1B ′ and postmultiplying by C(C ′C)−1 yields
D = DC ′BD,
from which we get D = (C ′B)−1 as required.
□
Most of the statistical applications involving idempotent matrices deal with sym-
metric idempotent matrices. For this reason, we end this section with some results for
this special class of matrices. Theorem 11.5 gives some restrictions on the elements
of a symmetric idempotent matrix.
Theorem 11.5
Suppose A is an m × m symmetric idempotent matrix. Then
(a) aii ≥0 for i = 1, . . . , m,
(b) aii ≤1 for i = 1, . . . , m,
(c) aij = aji = 0, for all j ̸= i, if aii = 0 or aii = 1.
Proof.
Since A is idempotent and symmetric, it follows that
aii = (A)ii = (A2)ii = (A′A)ii
= (A′)i·(A)·i =
m

j=1
a2
ji,
(11.8)

462
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
which clearly must be nonnegative. In addition, from (11.8), we have
aii = a2
ii +

j̸=i
a2
ji,
so that aii ≥a2
ii or aii(1 −aii) ≥0. However, because aii is nonnegative, this leads
to (1 −aii) ≥0, and thus (b) must hold. If aii = 0 or aii = 1, then aii = a2
ii, and so
again using (11.8), we must have

j̸=i
a2
ji = 0,
which, along with the symmetry of A, establishes (c).
□
Theorem 11.6 is useful in those situations in which it is easier to verify an identity
such as A3 = A2 than the identity A2 = A.
Theorem 11.6
Suppose that for some positive integer i, the m × m symmetric
matrix A satisfies Ai+1 = Ai. Then A is an idempotent matrix.
Proof.
If λ1, . . . , λm are the eigenvalues of A, then λi+1
1
, . . . , λi+1
m and λi
1, . . . λi
m
are the eigenvalues of Ai+1 and Ai, respectively. However, the identity Ai+1 = Ai
implies that λi+1
j
= λi
j, for j = 1, . . . , m, so each λj must be either 0 or 1. The
result now follows from Theorem 11.2.
□
11.3
COCHRAN’S THEOREM
Theorem 11.7, sometimes referred to as Cochran’s Theorem (Cochran, 1934), will
be useful in establishing the independence of several different quadratic forms in the
same normal variables.
Theorem 11.7
Let each of the m × m matrices A1, . . . , Ak be symmetric and
idempotent, and suppose that A1 + · · · + Ak = Im. Then AiAj = (0) whenever
i ̸= j.
Proof.
Select any one of the matrices, say Ah, and denote its rank by r. Since Ah
is symmetric and idempotent, an orthogonal matrix P exists, such that
P ′AhP = diag(Ir, (0)).
For j ̸= h, define Bj = P ′AjP, and note that
Im = P ′ImP = P ′
⎛
⎝
k

j=1
Aj
⎞
⎠P =
⎛
⎝
k

j=1
P ′AjP
⎞
⎠
= diag(Ir, (0)) +

j̸=h
Bj,

COCHRAN’S THEOREM
463
or equivalently,

j̸=h
Bj = diag((0), Im−r).
In particular, for l = 1, . . . , r,

j̸=h
(Bj)ll = 0.
However, clearly Bj is symmetric and idempotent because Aj is, and so, from
Theorem 11.5(a), its diagonal elements are nonnegative. Thus, we must have
(Bj)ll = 0 for each l = 1, . . . , r, which, along with Theorem 11.5(c), implies that
Bj must be of the form
Bj = diag((0), Cj),
where Cj is an (m −r) × (m −r) symmetric idempotent matrix. Now, for any
j ̸= h,
P ′AhAjP = (P ′AhP)(P ′AjP)
=

Ir
(0)
(0)
(0)
 (0)
(0)
(0)
Cj

= (0),
which can be true only if AhAj = (0), because P is nonsingular. Our proof is now
complete, because h was arbitrary.
□
Our next result is an extension of Cochran’s Theorem.
Theorem 11.8
Let A1, . . . , Ak be m × m symmetric matrices, and define
A = A1 + · · · + Ak. Consider the following statements:
(a) Ai is idempotent for i = 1, . . . , k.
(b) A is idempotent.
(c) AiAj = (0), for all i ̸= j.
Then if any two of these conditions hold, the third condition must also hold.
Proof.
First we show that (a) and (b) imply (c). Since A is symmetric and idempo-
tent, an orthogonal matrix P exists, such that
P ′AP = P ′(A1 + · · · + Ak)P = diag(Ir, (0)),
(11.9)
where r = rank(A). Let Bi = P ′AiP for i = 1, . . . , k, and note that Bi is symmet-
ric and idempotent. Thus, it follows from (11.9) and Theorem 11.5 that Bi must be

464
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
of the form diag(Ci, (0)), where the r × r matrix Ci also must be symmetric and
idempotent. However, (11.9) also implies that
C1 + · · · + Ck = Ir.
Consequently, C1, . . . , Ck satisfy the conditions of Theorem 11.7, and so
CiCj = (0) for every i ̸= j. From this result, we get BiBj = (0) and, hence,
AiAj = (0) for every i ̸= j, as is required. That (a) and (c) imply (b) follows
immediately, because
A2 =
 k

i=1
Ai
2
=
k

i=1
k

j=1
AiAj =
k

i=1
A2
i +
 
i̸=j
AiAj
=
k

i=1
Ai = A.
Finally, we must prove that (b) and (c) imply (a). If (c) holds, then AiAj = AjAi for
all i ̸= j, and so by Theorem 4.19, the matrices A1, . . . , Ak can be simultaneously
diagonalized; that is, an orthogonal matrix Q exists, such that
Q′AiQ = Di,
where each of the matrices D1, . . . , Dk is diagonal. Furthermore,
DiDj = Q′AiQQ′AjQ = Q′AiAjQ = Q′(0)Q = (0),
(11.10)
for every i ̸= j. Now because A is symmetric and idempotent, so also is the diagonal
matrix
Q′AQ = D1 + · · · + Dk.
As a result, each diagonal element of Q′AQ must be either 0 or 1, and because of
(11.10), the same can be said of the diagonal elements of D1, . . . , Dk. Thus, for each
i, Di is symmetric and idempotent and, hence, so is Ai = QDiQ′. This completes
the proof.
□
Suppose that the three conditions given in Theorem 11.8 hold. Then (a) implies
that tr(Ai) = rank(Ai) and (b) implies that
rank(A) = tr(A) = tr
 k

i=1
Ai

=
k

i=1
tr(Ai) =
k

i=1
rank(Ai).
Thus, we have shown that the conditions in Theorem 11.8 imply the fourth condition
(d) rank(A) = k
i=1 rank(Ai).

DISTRIBUTION OF QUADRATIC FORMS IN NORMAL VARIATES
465
Conversely, suppose that conditions (b) and (d) hold. We will show that these imply
(a) and (c). Let H = diag(A1, . . . , Ak) and F = 1m ⊗Im, so that A = F ′HF.
Then (d) can be written rank(F ′HF) = rank(H), and so it follows from Theorem
5.26 that F(F ′HF)−F ′ is a generalized inverse of H for any generalized inverse
(F ′HF)−of F ′HF. However, because A is idempotent, AImA = A, and hence,
Im is a generalized inverse of A = F ′HF. Thus, FF ′ is a generalized inverse of H,
which yields the equation
HFF ′H = H,
which in partitioned form is
⎡
⎢⎢⎢⎣
A2
1
A1A2
· · ·
A1Ak
A2A1
A2
2
· · ·
A2Ak
...
...
...
AkA1
AkA2
· · ·
A2
k
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
A1
(0)
· · ·
(0)
(0)
A2
· · ·
(0)
...
...
...
(0)
(0)
· · ·
Ak
⎤
⎥⎥⎥⎦.
This equation immediately gives conditions (a) and (c). Corollary 11.8.1 summarizes
the relationship among these four conditions.
Corollary 11.8.1
Let A1, . . . , Ak be m × m symmetric matrices, and define A =
A1 + · · · + Ak. Consider the following statements.
(a) Ai is idempotent for i = 1, . . . , k.
(b) A is idempotent.
(c) AiAj = (0), for all i ̸= j.
(d) rank(A) = k
i=1 rank(Ai).
All four of the conditions hold if any two of (a), (b), and (c) hold, or if (b) and (d) hold.
11.4
DISTRIBUTION OF QUADRATIC FORMS IN NORMAL VARIATES
The relationship between the normal and chi-squared distributions is fundamental in
obtaining the distribution of a quadratic form in normal random variables. Recall that
if z1, . . . , zr are independent random variables with zi ∼N(0, 1) for each i, then
r

i=1
z2
i ∼χ2
r.
This is used in Theorem 11.9 to determine when the quadratic form x′Ax has a
chi-squared distribution if the components of x are independently distributed, each
having the N(0, 1) distribution.

466
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Theorem 11.9
Let x ∼Nm(0, Im), and suppose that the m × m matrix A is sym-
metric, is idempotent, and has rank r. Then x′Ax ∼χ2
r.
Proof.
Since A is symmetric and idempotent, an orthogonal matrix P exists, such
that
A = PDP ′,
where D = diag(Ir, (0)). Let z = P ′x, and note that because x ∼Nm(0, Im),
E(z) = E(P ′x) = P ′E(x) = P ′0 = 0,
var(z) = var(P ′x) = P ′{var(x)}P = P ′ImP = P ′P = Im,
and so z ∼Nm(0, Im); that is, the components of z are, like the components of x,
independent standard normal random variables. Now because of the form of D, we
find that
x′Ax = x′PDP ′x = z′Dz =
r

i=1
z2
i ,
and so the result then follows.
□
Theorem 11.9 is a special case of Theorem 11.10 in which the multivariate normal
distribution has a general nonsingular covariance matrix.
Theorem 11.10
Let x ∼Nm(0, Ω), where Ω is a positive definite matrix, and let
A be an m × m symmetric matrix. If AΩ is idempotent and rank(AΩ) = r, then
x′Ax ∼χ2
r.
Proof.
Since Ω is positive definite, a nonsingular matrix T exists, which satisfies
Ω = TT ′. If we define z = T −1x, then E(z) = T −1E(x) = 0 and
var(z) = var(T −1x) = T −1{var(x)}T −1′
= T −1(TT ′)T −1′ = Im,
so that z ∼Nm(0, Im). The quadratic form x′Ax can be written in terms of z
because
x′Ax = x′T −1′T ′ATT −1x = z′T ′ATz.
All that remains is to show that T ′AT satisfies the conditions of Theorem 11.9.
Clearly, T ′AT is symmetric, because A is, and idempotent because
(T ′AT)2 = T ′ATT ′AT = T ′AΩAT = T ′AT,
where the last equality follows from the identity AΩA = A, which is a consequence
of the fact that AΩ is idempotent and Ω is nonsingular. Finally, because T ′AT and

DISTRIBUTION OF QUADRATIC FORMS IN NORMAL VARIATES
467
AΩ are idempotent, we have
rank(T ′AT) = tr(T ′AT) = tr(ATT ′)
= tr(AΩ) = rank(AΩ) = r,
and so the proof is complete.
□
It is not uncommon to have a quadratic form in a vector that has a singular multi-
variate normal distribution. Our next result generalizes Theorem 11.10 to this situa-
tion.
Theorem 11.11
Let x ∼Nm(0, Ω), where Ω is positive semidefinite, and suppose
that A is an m × m symmetric matrix. If ΩAΩAΩ = ΩAΩ and tr(AΩ) = r, then
x′Ax ∼χ2
r.
Proof.
Let n = rank(Ω), where n < m. Then an m × m orthogonal matrix
P = [P1
P2] exists, such that
Ω = [P1
P2]

Λ
(0)
(0)
(0)
 
P ′
1
P ′
2

= P1ΛP ′
1,
where P1 is m × n and Λ is an n × n nonsingular diagonal matrix. Define
z =
z1
z2

=
P ′
1x
P ′
2x

= P ′x,
and note that because P ′0 = 0 and P ′ΩP = diag(Λ, (0)), z ∼Nm(0, diag(Λ, (0)),
which means that z = (z′
1, 0′)′, where z1 has the nonsingular distribution Nn(0, Λ).
Now
x′Ax = x′PP ′APP ′x = z′P ′APz = z′
1P ′
1AP1z1,
and so the proof will be complete if we can show that the symmetric matrix P ′
1AP1
satisfies the conditions of the previous theorem, namely, that P ′
1AP1Λ is idempotent
and rank(P ′
1AP1Λ) = r. Since ΩAΩAΩ = ΩAΩ, we have
(Λ1/2P ′
1AP1Λ1/2)3 = Λ1/2P ′
1A(P1ΛP ′
1)A(P1ΛP ′
1)AP1Λ1/2
= Λ1/2P ′
1AΩAΩAP1Λ1/2 = Λ1/2P ′
1AΩAP1Λ1/2
= Λ1/2P ′
1A(P1ΛP ′
1)AP1Λ1/2 = (Λ1/2P ′
1AP1Λ1/2)2,
and so the idempotency of Λ1/2P ′
1AP1Λ1/2 follows from Theorem 11.6. However,
this also establishes the idempotency of P ′
1AP1Λ because Λ is nonsingular. Its rank
is r because
rank(P ′
1AP1Λ) = tr(P ′
1AP1Λ) = tr(AP1ΛP ′
1) = tr(AΩ) = r,
and so the result follows.
□

468
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Until now, our results have dealt with normal distributions having the zero mean
vector. In some applications, such as the determination of nonnull distributions in
hypothesis testing situations, we encounter quadratic forms in normal random vec-
tors having nonzero means. The next two theorems are helpful in determining whether
such a quadratic form has a chi-squared distribution. The proof of the first of these two
theorems, which is very similar to that of Theorem 11.10, is left to the reader. It applies
the relationship between the normal distribution and the noncentral chi-squared dis-
tribution; that is, if y1, . . . , yr are independently distributed with yi ∼N(μi, 1),
then
r

i=1
y2
i ∼χ2
r(λ),
where the noncentrality parameter of this noncentral chi-squared distribution is given
by
λ = 1
2
r

i=1
μ2
i.
Theorem 11.12
Let x ∼Nm(μ, Ω), where Ω is a positive definite matrix, and let
A be an m × m symmetric matrix. If AΩ is idempotent and rank(AΩ) = r, then
x′Ax ∼χ2
r(λ), where λ = 1
2μ′Aμ.
Theorem 11.13
Let x ∼Nm(μ, Ω), where Ω is positive semidefinite of rank n,
and suppose that A is an m × m symmetric matrix. Then x′Ax ∼χ2
r(λ), where λ =
1
2μ′Aμ if
(a) ΩAΩAΩ = ΩAΩ,
(b) μ′AΩAΩ = μ′AΩ,
(c) μ′AΩAμ = μ′Aμ,
(d) tr(AΩ) = r.
Proof.
Let P1, P2, and Λ be defined as in the proof of Theorem 11.11, so that
Ω = P1ΛP ′
1. Put C = [P1Λ−1/2
P2], and note that
z =
z1
z2

=

Λ−1/2P ′
1x
P ′
2x

= C ′x ∼Nm

Λ−1/2P ′
1μ
P ′
2μ

,

In
(0)
(0)
(0)

.
In other words,
z =

z1
P ′
2μ

,

DISTRIBUTION OF QUADRATIC FORMS IN NORMAL VARIATES
469
where z1 ∼Nn(Λ−1/2P ′
1μ, In). Now because C−1′ = [P1Λ1/2
P2], we find that
x′Ax = x′CC−1AC−1′C ′x = z′C−1AC−1′z
= [z′
1
μ′P2]

Λ1/2P ′
1AP1Λ1/2
Λ1/2P ′
1AP2
P ′
2AP1Λ1/2
P ′
2AP2
 
z1
P ′
2μ

= z′
1Λ1/2P ′
1AP1Λ1/2z1 + μ′P2P ′
2AP2P ′
2μ
+2μ′P2P ′
2AP1Λ1/2z1.
(11.11)
However, conditions (a)–(c) imply the identities
(i) P ′
1AΩAP1 = P ′
1AP1,
(ii) μ′P2P ′
2AΩAP1 = μ′P2P ′
2AP1,
(iii) μ′P2P ′
2AΩAΩAP2P ′
2μ = μ′P2P ′
2AΩAP2P ′
2μ = μ′P2P ′
2AP2P ′
2μ;
in particular, (a) implies (i), (b) and (i) imply (ii), whereas (iii) follows from (b), (c),
(i), and (ii). Using these identities in (11.11), we obtain
x′Ax = z′
1Λ1/2P ′
1AP1Λ1/2z1 + μ′P2P ′
2AΩAΩAP2P ′
2μ
+2μ′P2P ′
2AΩAP1Λ1/2z1
= (z1 + Λ1/2P ′
1AP2P ′
2μ)′Λ1/2P ′
1AP1Λ1/2(z1 + Λ1/2P ′
1AP2P ′
2μ)
= w′A∗w.
Now, w = (z1 + Λ1/2P ′
1AP2P ′
2μ) ∼Nn(θ, In), where
θ = Λ−1/2P ′
1μ + Λ1/2P ′
1AP2P ′
2μ,
and, because A∗= Λ1/2P ′
1AP1Λ1/2 is idempotent, a consequence of (i), we may
apply Theorem 11.12; that is, w′A∗w ∼χ2
r(λ), where
r = tr(A∗In) = tr(Λ1/2P ′
1AP1Λ1/2) = tr(AP1ΛP ′
1) = tr(AΩ)
and
λ = 1
2θ′A∗θ = 1
2(Λ−1/2P ′
1μ + Λ1/2P ′
1AP2P ′
2μ)′
×Λ1/2P ′
1AP1Λ1/2(Λ−1/2P ′
1μ + Λ1/2P ′
1AP2P ′
2μ)
= 1
2(μ′P1P ′
1AP1P ′
1μ + μ′P2P ′
2AΩAΩAP2P ′
2μ + 2μ′P1P ′
1AΩAP2P ′
2μ)
= 1
2(μ′P1P ′
1AP1P ′
1μ + μ′P2P ′
2AP2P ′
2μ + 2μ′P1P ′
1AP2P ′
2μ)
= 1
2μ′(P1P ′
1 + P2P ′
2)A(P1P ′
1 + P2P ′
2)μ = 1
2μ′Aμ.

470
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
This completes the proof.
□
A matrix A satisfying conditions (a), (b), and (c) of Theorem 11.13 is Ω+, the
Moore–Penrose inverse of Ω. That is, if x ∼Nm(μ, Ω), then x′Ω+x will have a
chi-squared distribution because the identity Ω+ΩΩ+ = Ω+ ensures that conditions
(a), (b), and (c) hold. The degrees of freedom r = rank(Ω) because rank(Ω+Ω) =
rank(Ω).
All of the theorems presented in this section give sufficient conditions for a
quadratic form to have a chi-squared distribution. Actually, in each case, the stated
conditions are necessary conditions as well, which is most easily proven using
moment generating functions. For more details, see Mathai and Provost (1992) or
Searle (1971).
Example 11.3
Let x1, . . . , xn be a random sample from a normal distribution with
mean μ and variance σ2; that is, the xi’s are independent random variables, each
having the distribution N(μ, σ2). The sample variance s2 is given by
s2 =
1
(n −1)
n

i=1
(xi −x)2.
We will use the results of this section to show that
t = (n −1)s2
σ2
=
n

i=1
(xi −x)2
σ2
∼χ2
n−1.
Define the n × 1 vector x = (x1, . . . , xn)′, so that x ∼Nn(μ1n, σ2In). Note that
if the n × n matrix A = (In −n−11n1′
n)/σ2, then
x′Ax = {x′x −n−1(1′
nx)2}
σ2
= σ−2
⎧
⎨
⎩
n

i=1
x2
i −n−1
 n

i=1
xi
2⎫
⎬
⎭
=
n

i=1
(xi −x)2
σ2
= (n −1)s2
σ2
= t,
and so t is a quadratic form in the random vector x. The matrix A(σ2In) = σ2A is
idempotent because
(σ2A)2 = (In −n−11n1′
n)2 = In −2n−11n1′
n + n−21n1′
n1n1′
n
= In −n−11n1′
n = σ2A,
and so, by Theorem 11.12, t has a chi-squared distribution. This chi-squared distri-
bution has n −1 degrees of freedom because
tr(σ2A) = tr(In −n−11n1′
n) = tr(In) −n−1tr(1n1′
n)
= n −n−11′
n1n = n −1,

INDEPENDENCE OF QUADRATIC FORMS
471
and the noncentrality parameter is given by
λ = 1
2μ′Aμ = 1
2
μ2
σ2 1′
n(In −n−11n1′
n)1n
= 1
2
μ2
σ2 (1′
n1n −n−11′
n1n1′
n1n)
= 1
2
μ2
σ2 (n −n) = 0.
Thus, we have shown that t ∼χ2
n−1.
11.5
INDEPENDENCE OF QUADRATIC FORMS
We now consider the situation in which we have several different quadratic forms,
each a function of the same multivariate normal vector. In some settings, it is impor-
tant to be able to determine whether these quadratic forms are distributed indepen-
dently of one another. For instance, this is useful in the partitioning of chi-squared
random variables as well as in the formation of ratios having an F distribution.
We begin with the following basic result regarding the statistical independence of
two quadratic forms in the same normal vector.
Theorem 11.14
Let x ∼Nm(μ, Ω), where Ω is positive definite, and suppose that
A and B are m × m symmetric matrices. If AΩB = (0), then x′Ax and x′Bx are
independently distributed.
Proof.
Since Ω is positive definite, a nonsingular matrix T exists, such that Ω =
TT ′. Define G = T ′AT and H = T ′BT, and note that if AΩB = (0), then
GH = (T ′AT)(T ′BT) = T ′AΩBT = T ′(0)T = (0).
(11.12)
Consequently, because of the symmetry of G and H, we also have
(0) = (0)′ = (GH)′ = H ′G′ = HG,
and so we have established that GH = HG. From Theorem 4.18, we know that an
orthogonal matrix P exists that simultaneously diagonalizes G and H; that is, for
some diagonal matrices C and D,
P ′GP = P ′T ′ATP = C,
P ′HP = P ′T ′BTP = D.
(11.13)
However, using (11.12) and (11.13), we find that
(0) = GH = PCP ′PDP ′ = PCDP ′,

472
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
which can be true only if CD = (0). Since C and D are diagonal matrices, this
means that if the ith diagonal element of one of these matrices is nonzero, the ith
diagonal element of the other must be zero. As a result, by choosing P appropri-
ately, we may obtain C and D in the form C = diag(c1, . . . , cm1, 0, . . . , 0) and
D = diag(0, . . . , 0, dm1+1, . . . , dm) for some integer m1. If we let y = P ′T −1x,
then our two quadratic forms simplify as
x′Ax = x′T −1′PP ′T ′ATPP ′T −1x = y′Cy =
m1

i=1
ciy2
i
and
x′Bx = x′T −1′PP ′T ′BTPP ′T −1x = y′Dy =
m

i=m1+1
diy2
i ;
that is, the first quadratic form is a function only of y1, . . . , ym1, whereas the second
quadratic form is a function of ym1+1, . . . , ym. Since
var(y) = var(P ′T −1x) = P ′T −1ΩT −1′P = Im,
the result then follows from the independence of y1, . . . , ym, which is a consequence
of the fact that y is normal.
□
Example 11.4
Suppose that x1, . . . , xk are independently distributed with
xi = (xi1, . . . , xin)′ ∼Nn(μ1n, σ2In) for each i. Let t1 and t2 be the random
quantities defined by
t1 = n
k

i=1
(xi −x)2,
t2 =
k

i=1
n

j=1
(xij −xi)2,
where
xi =
n

j=1
xij
n ,
x =
k

i=1
xi
k .
Note that t1 and t2 are the formulas for the sum of squares for treatments and the sum
of squares for error in a balanced one-way classification model (Example 8.2). Now
t1 can be expressed as
t1 = n
⎧
⎨
⎩
k

i=1
x2
i −k−1
 k

i=1
xi
2⎫
⎬
⎭= nx′(Ik −k−11k1′
k)x,
where
x = (x1, . . . , xk)′.
If
we
define
x
as
x = (x′
1, . . . , x′
k)′,
then
x ∼Nkn(μ, Ω)
with
μ = 1k ⊗μ1n = μ1kn
and
Ω = Ik ⊗σ2In = σ2Ikn,
and x = n−1(Ik ⊗1′
n)x, so

INDEPENDENCE OF QUADRATIC FORMS
473
t1 = n−1x′(Ik ⊗1n)(Ik −k−11k1′
k)(Ik ⊗1′
n)x
= n−1x′{(Ik −k−11k1′
k) ⊗1n1′
n}x = x′A1x,
where
A1 = n−1{(Ik −k−11k1′
k) ⊗1n1′
n}.
Since
(1n1′
n)2 = n1n1′
n
and
(Ik −k−11k1′
k)2 = (Ik −k−11k1′
k), we find that A1 is idempotent and, hence,
so is (A1/σ2)Ω. Thus, by Theorem 11.12, x′(A1/σ2)x = t1/σ2 has a chi-squared
distribution. This distribution is central because λ = 1
2μ′A1μ/σ2 = 0 , which
follows from the fact that
{(Ik −k−11k1′
k) ⊗1n1′
n}(1k ⊗μ1n) = nμ{(Ik −k−11k1′
k)1k ⊗1n}
= nμ{(1k −1k) ⊗1n} = 0,
whereas its degrees of freedom are given by
r1 = tr{(A1/σ2)Ω} = tr(A1) = n−1tr{(Ik −k−11k1′
k) ⊗1n1′
n}
= n−1tr(Ik −k−11k1′
k)tr(1n1′
n) = n−1(k −1)n = k −1.
Turning to t2, observe that it can be written as
t2 =
k

i=1
⎧
⎨
⎩
n

j=1
x2
ij −n−1
⎛
⎝
n

j=1
xij
⎞
⎠
2⎫
⎬
⎭=
k

i=1
x′
i(In −n−11n1′
n)xi
= x′{Ik ⊗(In −n−11n1′
n)}x = x′A2x,
where
A2 = Ik ⊗(In −n−11n1′
n).
Clearly,
A2
is
idempotent
because
(In −n−11n1′
n)
is
idempotent.
Thus,
(A2/σ2)Ω
is
idempotent,
and
so
x′(A2/σ2)x = t2/σ2
also
has
a
chi-squared
distribution.
In
particular,
t2/σ2 ∼χ2
k(n−1) because
tr{(A2/σ2)Ω} = tr(A2) = tr{Ik ⊗(In −n−11n1′
n)}
= tr(Ik)tr(In −n−11n1′
n) = k(n −1)
and
A2μ = {Ik ⊗(In −n−11n1′
n)}(1k ⊗μ1n)
= 1k ⊗μ(In −n−11n1′
n)1n = 1k ⊗μ(1n −1n) = 0,
thereby guaranteeing that
1
2μ′A2μ/σ2 = 0. Finally, we establish the indepen-
dence of t1 and t2 by using Theorem 11.14. This simply involves verifying that
(A1/σ2)Ω(A2/σ2) = A1A2/σ2 = (0), which is an immediate consequence of the
fact that
1n1′
n(In −n−11n1′
n) = (0).

474
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Example 11.5
Let us return to the general regression model
y = Xβ + ϵ,
where y and ϵ are N × 1, X is N × m, and β is m × 1. Suppose that β and X
are partitioned as β = (β′
1
β′
2)′ and X = (X1
X2), where β1 is m1 × 1, β2 is
m2 × 1, and we wish to test the hypothesis that β2 = 0. We will assume that each
component of β2 is estimable because this test would not be meaningful otherwise. It
is easily shown that this then implies that X2 has full column rank and rank(X1) =
r −m2, where r = rank(X). A test of β2 = 0 can be constructed by comparing the
sum of squared errors for the reduced model y = X1β1 + ϵ, which is
t1 = (y −X1ˆβ1)′(y −X1ˆβ1) = y′(IN −X1(X′
1X1)−X′
1)y,
with the sum of squared errors for the complete model, which is given by
t2 = (y −X ˆβ)′(y −X ˆβ) = y′(IN −X(X ′X)−X ′)y.
Now if ϵ ∼NN(0, σ2IN), then y ∼NN(Xβ, σ2IN). Thus, by applying Theorem
11.12 and using the fact that X(X ′X)−X ′X1 = X1, we find that (t1 −t2)/σ2 is
chi-squared because
X(X ′X)−X ′ −X1(X′
1X1)−X′
1
σ2

(σ2IN)
X(X ′X)−X ′ −X1(X′
1X1)−X′
1
σ2

=
X(X ′X)−X ′ −X1(X′
1X1)−X′
1
σ2

.
In particular, if β2 = 0, (t1 −t2)/σ2 ∼χ2
m2, because
tr{X(X ′X)−X ′ −X1(X′
1X1)−X′
1}
= tr{X(X ′X)−X ′} −tr{X1(X′
1X1)−X′
1}
= r −(r −m2) = m2
and
β′
1X′
1
X(X ′X)−X ′ −X1(X′
1X1)−X′
1
σ2

X1β1
= β′
1X′
1X1β1 −β′
1X′
1X1β1
σ2
= 0.
By a similar application of Theorem 11.12, we observe that t2/σ2 ∼χ2
N−r. In addi-
tion, it follows from Theorem 11.14 that (t1 −t2)/σ2 and t2/σ2 are independently
distributed because
X(X ′X)−X ′ −X1(X′
1X1)−X′
1
σ2

(σ2IN)
IN −X(X ′X)−X ′
σ2

= 0.

INDEPENDENCE OF QUADRATIC FORMS
475
This then permits the construction of an F statistic for testing that β2 = 0; that is, if
β2 = 0, then the statistic
F = (t1 −t2)/m2
t2/(N −r)
has the F distribution with m2 and N −r degrees of freedom.
The proof Theorem 11.15, which is similar to the proof of Theorem 11.14, is left
to the reader as an exercise.
Theorem 11.15
Let x ∼Nm(μ, Ω), where Ω is positive definite, and suppose that
A is an m × m symmetric matrix, whereas B is an n × m matrix. If BΩA = (0),
then x′Ax and Bx are independently distributed.
Example 11.6
Suppose that we have a random sample x1, . . . , xn from a normal
distribution with mean μ and variance σ2. In Example 11.3, it was shown that (n −
1)s2/σ2 ∼χ2
n−1, where s2, the sample variance, is given by
s2 =
1
(n −1)
n

i=1
(xi −x)2.
We will now use Theorem 11.15 to show that the sample mean,
x = 1
n
n

i=1
xi,
is independently distributed of s2. In Example 11.3, we saw that s2 is a scalar multiple
of the quadratic form
x′(In −n−11n1′
n)x,
where x = (x1, . . . , xn)′ ∼Nn(μ1n, σ2In). On the other hand, x can be expressed
as
x = n−11′
nx.
Consequently, the independence of x and s2 follows from the fact that
1′
n(σ2In)(In −n−11n1′
n) = σ2(1′
n −n−11′
n1n1′
n)
= σ2(1′
n −1′
n) = 0′.
When Ω is positive semidefinite, the condition AΩB = (0), given in Theorem
11.14, will still guarantee that the two quadratic forms x′Ax and x′Bx are indepen-
dently distributed. Likewise, when Ω is positive semidefinite, the condition BΩA =
(0), given in Theorem 11.15, will still guarantee that x′Ax and Bx are independently

476
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
distributed. However, in these situations, a weaker set of conditions will guarantee
independence. These conditions are given in Theorem 11.16 and Theorem 11.17.
The proofs are left as exercises.
Theorem 11.16
Let x ∼Nm(μ, Ω), where Ω is positive semidefinite, and suppose
that A and B are m × m symmetric matrices. Then x′Ax and x′Bx are indepen-
dently distributed if
(a) ΩAΩBΩ = (0),
(b) ΩAΩBμ = 0,
(c) ΩBΩAμ = 0,
(d) μ′AΩBμ = 0.
Theorem 11.17
Let x ∼Nm(μ, Ω), where Ω is positive semidefinite, and suppose
that A is an m × m symmetric matrix, whereas B is an n × m matrix. If BΩAΩ =
(0) and BΩAμ = 0, then x′Ax and Bx are independently distributed.
Our final result can be helpful in establishing that several quadratic forms in
the same normal random vector are independently distributed, with each having a
chi-squared distribution.
Theorem 11.18
Let x ∼Nm(μ, Ω), where Ω is positive definite. Suppose that Ai
is an m × m symmetric matrix of rank ri, for i = 1, . . . , k, and A = A1 + · · · + Ak
is of rank r. Consider the conditions
(a) AiΩ is idempotent for each i,
(b) AΩ is idempotent,
(c) AiΩAj = (0), for all i ̸= j,
(d) r = k
i=1 ri.
If any two of (a), (b), and (c) hold, or if (b) and (d) hold, then
(i) x′Aix ∼χ2
ri( 1
2μ′Aiμ),
(ii) x′Ax ∼χ2
r( 1
2μ′Aμ),
(iii) x′A1x, . . . , x′Akx are independently distributed.
Proof.
Since Ω is positive definite, a nonsingular matrix T satisfying Ω = TT ′
exists, and the conditions (a)–(d) can be equivalently expressed as
(a) T ′AiT is idempotent for each i,
(b) T ′AT is idempotent,

EXPECTED VALUES OF QUADRATIC FORMS
477
(c) (T ′AiT)(T ′AjT) = (0), for all i ̸= j ,
(d) rank(T ′AT) = k
i=1 rank(T ′AiT).
Since T ′A1T, . . . , T ′AkT and T ′AT satisfy the conditions of Corollary 11.8.1, we
are ensured that if any two of (a), (b), and (c) hold or if (b) and (d) hold, then all
four of the conditions (a)–(d) hold. Now using Theorem 11.12, (a) implies (i) and (b)
implies (ii), whereas Theorem 11.14, along with (c), guarantees that (iii) holds.
□
11.6
EXPECTED VALUES OF QUADRATIC FORMS
When a quadratic form satisfies the conditions given in the theorems of Section 11.4,
then its moments can be obtained directly from the appropriate chi-squared distri-
bution. In this section, we derive formulas for means, variances, and covariances
of quadratic forms that will be useful when this is not the case. We will start with
the most general case in which the random vector x has an arbitrary distribution.
The expressions we obtain involve the matrix of second moments of x, E(xx′) and
the matrix of fourth moments E(xx′ ⊗xx′).
Theorem 11.19
Let x be an m × 1 random vector having finite fourth moments, so
that both E(xx′) and E(xx′ ⊗xx′) exist. Denote the mean vector and covariance
matrix of x by μ and Ω. If A and B are m × m symmetric matrices, then
(a) E(x′Ax) = tr{AE(xx′)} = tr(AΩ) + μ′Aμ,
(b) var(x′Ax) = tr{(A ⊗A)E(xx′ ⊗xx′)} −{tr(AΩ) + μ′Aμ}2,
(c) cov(x′Ax, x′Bx) = tr{(A ⊗B)E(xx′ ⊗xx′)} −{tr(AΩ) + μ′Aμ}
× {tr(BΩ) + μ′Bμ}.
Proof.
The covariance matrix Ω is defined by
Ω = E{(x −μ)(x −μ′} = E(xx′) −μμ′,
so that E(xx′) = Ω + μμ′. Since x′Ax is a scalar, we have
E(x′Ax) = E{tr(x′Ax)} = E{tr(Axx′)} = tr{AE(xx′)}
= tr{A(Ω + μμ′)} = tr(AΩ) + tr(Aμμ′)
= tr(AΩ) + μ′Aμ,
and so (a) holds. Part (b) will follow from (c) by taking B = A. To prove (c), note
that

478
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
E(x′Axx′Bx) = E[tr{(x′ ⊗x′)(A ⊗B)(x ⊗x)}]
= E[tr{(A ⊗B)(x ⊗x)(x′ ⊗x′)}]
= tr{(A ⊗B)E(xx′ ⊗xx′)}.
Applying this result, along with part (a), in the equation
cov(x′Ax, x′Bx) = E(x′Axx′Bx) −E(x′Ax)E(x′Bx)
completes the proof.
□
When x has a multivariate normal distribution, the expressions for variances and
covariances, as well as for higher moments, simplify somewhat. This is a consequence
of the special structure of the moments of the multivariate normal distribution. The
commutation matrix Kmm, discussed in Chapter 8, plays a crucial role in obtaining
some of these matrix expressions. We will also use the m × m matrix Tij defined by
Tij = Eij + Eji = eie′
j + eje′
i ;
that is, all elements of Tij are equal to 0 except for the (i, j)th and (j, i)th elements,
which equal 1, unless i = j, in which case, the only nonzero element is a 2 in the
(i, i)th position. Before obtaining expressions for the variance and covariance of
quadratic forms in normal variates, we will need Theorem 11.20.
Theorem 11.20
If z ∼Nm(0, Im) and c is a vector of constants, then
(a) E(z ⊗z) = vec(Im),
(b) E(cz′ ⊗zz′) = (0),
E(zc′ ⊗zz′) = (0),
E(zz′ ⊗cz′) = (0),
and
E(zz′ ⊗zc′) = (0),
(c) E(zz′ ⊗zz′) = 2Nm + vec(Im){vec(Im)}′,
(d) var(z ⊗z) = 2Nm.
Proof.
Since E(z) = 0, Im = var(z) = E(zz′), and so
E(z ⊗z) = E{vec(zz′)} = vec{E(zz′)} = vec(Im).
It is easily verified using the standard normal moment generating function that
E(z3
i ) = 0,
E(z4
i ) = 3.
Each element of the matrices of expected values in (b) will be of the form
ciE(zjzkzl). Since the components of z are independent, we get
E(zjzkzl) = E(zj)E(zk)E(zl) = 0
when the three subscripts are distinct,

EXPECTED VALUES OF QUADRATIC FORMS
479
E(zjzkzl) = E(z2
j)E(zl) = (1)(0) = 0
when j = k ̸= l, and similarly for j = l ̸= k and l = k ̸= j, and
E(zjzkzl) = E(z3
j) = 0
when j = k = l. This proves (b). Next, we consider terms of the form E(zizjzkzl).
These terms equal 1 if i = j ̸= l = k, i = k ̸= j = l, or i = l ̸= j = k, equal 3 if
i = j = k = l, and equal zero otherwise. This leads to
E(zizjzz′) = Tij + δijIm,
where δij is the (i, j)th element of Im. Thus,
E(zz′ ⊗zz′) = E
⎧
⎨
⎩
⎛
⎝
m

i=1
m

j=1
Eijzizj
⎞
⎠⊗zz′
⎫
⎬
⎭
=
m

i=1
m

j=1
{Eij ⊗E(zizjzz′)}
=
m

i=1
m

j=1
{Eij ⊗(Tij + δijIm)}
=
m

i=1
m

j=1
(Eij ⊗Tij) +
m

i=1
m

j=1
(δijEij ⊗Im).
The third result now follows because
m

i=1
m

j=1
(Eij ⊗Tij) =
m

i=1
m

j=1
(Eij ⊗Eji) +
m

i=1
m

j=1
(Eij ⊗Eij)
= Kmm +
 m

i=1
(ei ⊗ei)
 ⎧
⎨
⎩
m

j=1
(e′
j ⊗e′
j)
⎫
⎬
⎭
= Kmm +
 m

i=1
vec(eie′
i)
 ⎧
⎨
⎩
m

j=1
{vec(eje′
j)}′
⎫
⎬
⎭
= Kmm + vec(Im){vec(Im)}′,
m

i=1
m

j=1
(δijEij ⊗Im) =
 m

i=1
Eii

⊗Im = Im ⊗Im = Im2,

480
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
and Im2 + Kmm = 2Nm. Finally, (d) is an immediate consequence of (a)
and (c).
□
Theorem 11.21 generalizes the results of Theorem 11.20 to a multivariate normal
distribution having a general positive definite covariance matrix.
Theorem 11.21
Let x ∼Nm(0, Ω), where Ω is positive definite, and let c be an
m × 1 vector of constants. Then
(a) E(x ⊗x) = vec(Ω),
(b) E(cx′ ⊗xx′) = (0),
E(xc′ ⊗xx′) = (0),
E(xx′ ⊗cx′) = (0),
and
E(xx′ ⊗xc′) = (0),
(c) E(xx′ ⊗xx′) = 2Nm(Ω ⊗Ω) + vec(Ω){vec(Ω)}′,
(d) var(x ⊗x) = 2Nm(Ω ⊗Ω).
Proof.
Let T be any nonsingular matrix satisfying Ω = TT ′, so that z = T −1x
and x = Tz, where z ∼Nm(0, Im). Then the results in Theorem 11.21 are con-
sequences of Theorem 11.20 because
E(x ⊗x) = (T ⊗T)E(z ⊗z) = (T ⊗T)vec(Im)
= vec(TT ′) = vec(Ω),
E(cx′ ⊗xx′) = (Im ⊗T)E(cz′ ⊗zz′)(T ′ ⊗T ′)
= (Im ⊗T)(0)(T ′ ⊗T ′) = (0)
and
E(xx′ ⊗xx′) = (T ⊗T)E(zz′ ⊗zz′)(T ′ ⊗T ′)
= (T ⊗T)(2Nm + vec(Im){vec(Im)}′)(T ′ ⊗T ′)
= 2(T ⊗T)Nm(T ′ ⊗T ′)
+(T ⊗T)vec(Im){vec(Im)}′(T ′ ⊗T ′)
= 2Nm(T ⊗T)(T ′ ⊗T ′) + vec(TT ′){vec(TT ′)}′
= 2Nm(Ω ⊗Ω) + vec(Ω){vec(Ω)}′.
□
We are now ready to obtain simplified expressions for the variance and covariance
of quadratic forms in normal variates.
Theorem 11.22
Let A and B be m × m symmetric matrices, and suppose that
x ∼Nm(0, Ω), where Ω is positive definite. Then

EXPECTED VALUES OF QUADRATIC FORMS
481
(a) E(x′Axx′Bx) = tr(AΩ)tr(BΩ) + 2tr(AΩBΩ),
(b) cov(x′Ax, x′Bx) = 2tr(AΩBΩ),
(c) var(x′Ax) = 2tr{(AΩ)2}.
Proof.
Since (c) is the special case of (b) in which B = A, we only need to prove
(a) and (b). Note that by making use of Theorem 11.21, we find that
E(x′Axx′Bx) = E{(x′ ⊗x′)(A ⊗B)(x ⊗x)}
= E[tr{(A ⊗B)(xx′ ⊗xx′)}]
= tr{(A ⊗B)E(xx′ ⊗xx′)}
= tr{(A ⊗B)(2Nm(Ω ⊗Ω) + vec(Ω){vec(Ω)}′)}
= tr{(A ⊗B)((Im2 + Kmm)(Ω ⊗Ω) + vec(Ω){vec(Ω)}′)}
= tr{(A ⊗B)(Ω ⊗Ω)} + tr{(A ⊗B)Kmm(Ω ⊗Ω)}
+tr((A ⊗B)vec(Ω){vec(Ω)}′).
Now
tr{(A ⊗B)(Ω ⊗Ω)} = tr(AΩ ⊗BΩ) = tr(AΩ)tr(BΩ)
follows directly from Theorem 8.3, whereas
tr{(A ⊗B)Kmm(Ω ⊗Ω)} = tr{(AΩ ⊗BΩ)Kmm} = tr(AΩBΩ)
follows from Theorem 8.26. Using the symmetry of A and Ω along with Theorem
8.10 and Theorem 8.11, the last term in E{x′Axx′Bx} simplifies as
tr((A ⊗B)vec(Ω){vec(Ω)}′) = {vec(Ω)}′(A ⊗B)vec(Ω)
= {vec(Ω)}′vec(BΩA) = tr(AΩBΩ).
This then proves (a). Using the definition of covariance and Theorem 11.19(a), we
also get
cov(x′Ax, x′Bx) = E(x′Axx′Bx) −E(x′Ax)E(x′Bx)
= 2tr(AΩBΩ),
which proves (b).
□
The formulas given in Theorem 11.22 become somewhat more complicated
when the normal distribution has a nonnull mean vector. These formulas are given
in Theorem 11.23.
Theorem 11.23
Let A and B be symmetric m × m matrices, and suppose that
x ∼Nm(μ, Ω), where Ω is positive definite. Then

482
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
(a) E(x′Axx′Bx) = tr(AΩ)tr(BΩ) + 2tr(AΩBΩ) + tr(AΩ)μ′Bμ +
4μ′AΩBμ + μ′Aμtr(BΩ) + μ′Aμμ′Bμ,
(b) cov(x′Ax, x′Bx) = 2tr(AΩBΩ) + 4μ′AΩBμ,
(c) var(x′Ax) = 2tr{(AΩ)2} + 4μ′AΩAμ.
Proof.
Again, (c) is a special case of (b), so we only need to prove (a) and (b). We
can write x = y + μ, where y ∼Nm(0, Ω) and, consequently,
E(x′Axx′Bx) = E{(y + μ)′A(y + μ)(y + μ)′B(y + μ)}
= E{(y′Ay + 2μ′Ay + μ′Aμ)(y′By + 2μ′By + μ′Bμ)}
= E(y′Ayy′By) + 2E(y′Ayμ′By) + E(y′Ay)μ′Bμ
+2E(μ′Ayy′By) + 4E(μ′Ayμ′By)
+2E(μ′Ay)μ′Bμ + μ′AμE(y′By)
+2μ′AμE(μ′By) + μ′Aμμ′Bμ.
The sixth and eighth terms in this last expression are zero because E(y) = 0, whereas
it follows from Theorem 11.21(b) that the second and fourth terms are zero. To sim-
plify the fifth term, note that
E(μ′Ayμ′By) = E{(μ′A ⊗μ′B)(y ⊗y)} = (Aμ ⊗Bμ)′E{(y ⊗y)}
= {vec(Bμμ′A)}′vec(Ω) = tr{(Bμμ′A)′Ω}
= tr(Aμμ′BΩ) = μ′AΩBμ.
Thus, using this result, Theorem 11.19(a), and Theorem 11.22(a), we find that
E(x′Axx′Bx) = tr(AΩ)tr(BΩ) + 2tr(AΩBΩ) + tr(AΩ)μ′Bμ
+4μ′AΩBμ + μ′Aμtr(BΩ) + μ′Aμμ′Bμ,
thereby proving (a); (b) then follows immediately from the definition of covariance
and Theorem 11.19(a).
□
Example 11.7
Let us return to the subject of Example 11.4, where we defined
A1 = n−1{(Ik −k−11k1′
k) ⊗1n1′
n}
and
A2 = Ik ⊗(In −n−11n1′
n).
It was shown that if x = (x′
1, . . . , x′
k)′ ∼Nkn(μ, Ω) with μ = 1k ⊗μ1n and Ω =
Ik ⊗σ2In, then t1/σ2 = x′(A1/σ2)x ∼χ2
k−1 and t2/σ2 = x′(A2/σ2)x ∼χ2
k(n−1),

EXPECTED VALUES OF QUADRATIC FORMS
483
independently. Since the mean of a chi-squared random variable equals its degrees
of freedom, whereas the variance is two times the degrees of freedom, we can easily
calculate the mean and variance of t1 and t2 without using the results of this section;
in particular, we have
E(t1) = σ2(k −1),
var(t1) = 2σ4(k −1),
E(t2) = σ2k(n −1),
var(t2) = 2σ4k(n −1).
Suppose now that xi ∼Nn(μ1n, σ2
i In), so that Ω = var(x) = D ⊗In, where D =
diag(σ2
1, . . . , σ2
k). It can be easily verified that, in this case, t1/σ2 and t2/σ2 no
longer satisfy the conditions of Theorem 11.12 for chi-squaredness, but they are still
independently distributed. The mean and variance of t1 and t2 can be computed by
using Theorem 11.19 and Theorem 11.23. For instance, the mean of t2 is given by
E(t2) = E(x′A2x) = tr(A2Ω) + μ′A2μ
= tr({Ik ⊗(In −n−11n1′
n)}(D ⊗In))
+μ2(1′
k ⊗1′
n){Ik ⊗(In −n−11n1′
n)}(1k ⊗1n)
= tr(D)tr(In −n−11n1′
n) + μ2(1′
k1k){1′
n(In −n−11n1′
n)1n}
= (n −1)
k

i=1
σ2
i ,
whereas its variance is
var(t2) = var(x′A2x) = 2tr{(A2Ω)2} + 4μ′A2ΩA2μ
= 2tr{D2 ⊗(In −n−11n1′
n)}
+4μ2(1′
k ⊗1′
n){D ⊗(In −n−11n1′
n)}(1k ⊗1n)
= 2tr(D2)tr{(In −n−11n1′
n)}
+4μ2(1′
kD1k){1′
n(In −n−11n1′
n)1n}
= 2(n −1)
k

i=1
σ4
i .
We will leave it to the reader to verify that
E(t1) = (1 −k−1)
k

i=1
σ2
i ,
var(t1) = 2
⎧
⎨
⎩(1 −2k−1)
k

i=1
σ4
i + k−2
 k

i=1
σ2
i
2⎫
⎬
⎭.

484
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
So far we have considered the expectation of a quadratic form as well as the
expectation of a product of two quadratic forms. A more general situation is one
in which we need the expected value of the product of n quadratic forms. This
expectation becomes more tedious to compute as n increases. For example, if A,
B, and C are m × m symmetric matrices and x ∼Nm(0, Ω), the expected value
E(x′Axx′Bxx′Cx) can be obtained by first computing E(xx′ ⊗xx′ ⊗xx′) and
then applying this result in the identity
E(x′Axx′Bxx′Cx) = tr{(A ⊗B ⊗C)E(xx′ ⊗xx′ ⊗xx′)}.
The details of this derivation are left as an exercise. Magnus (1978) used an alternative
method, using the cumulants of a distribution and their relationship to the moments
of a distribution, to obtain the expectation of the product of an arbitrary number of
quadratic forms. The results for a product of three and four quadratic forms are sum-
marized in Theorem 11.24.
Theorem 11.24
Let A, B, C, and D be symmetric m × m matrices, and suppose
that x ∼Nm(0, Im). Then
(a) E (x′Axx′Bxx′Cx) = tr(A)tr(B)tr(C) + 2{tr(A)tr(BC)
+ tr(B)tr(AC) + tr(C)tr(AB)}
+ 8tr(ABC),
(b) E (x′Axx′Bxx′Cxx′Dx)
= tr(A)tr(B)tr(C)tr(D) + 8{tr(A)tr(BCD)
+ tr(B)tr(ACD) + tr(C)tr(ABD)
+ tr(D)tr(ABC)} + 4{tr(AB)tr(CD)
+ tr(AC)tr(BD) + tr(AD)tr(BC)} + 2{tr(A)tr(B)tr(CD)
+ tr(A)tr(C)tr(BD) + tr(A)tr(D)tr(BC)
+ tr(B)tr(C)tr(AD) + tr(B)tr(D)tr(AC)
+ tr(C)tr(D)tr(AB)} + 16{tr(ABCD)
+ tr(ABDC) + tr(ACBD)}.
If x ∼Nm(0, Ω), where Ω is positive definite, then A, B, C, and D appearing in
the right-hand side of the equations in Theorem 11.24 are replaced by AΩ, BΩ, CΩ,
and DΩ.
An alternative approach to the calculation of moments of quadratic forms uses
tensor methods. This approach may be particularly appealing in those situations in

THE WISHART DISTRIBUTION
485
which higher ordered moments are needed or the random vector x does not have a
multivariate normal distribution. A detailed discussion of these tensor methods can
be found in McCullagh (1987).
11.7
THE WISHART DISTRIBUTION
When x1, . . . , xn are independently distributed, with xi ∼N(0, σ2) for every i, then
x′x =
n

i=1
x2
i ∼σ2χ2
n,
where x′ = (x1, . . . , xn); that is, x′x/σ2 has a chi-squared distribution with n
degrees of freedom. A natural matrix generalization of this situation, one that has
important applications in multivariate analysis, involves the distribution of
X ′X =
n

i=1
xix′
i,
where X ′ = (x1, . . . , xn) is an m × n matrix, such that x1, . . . , xn are indepen-
dent and xi ∼Nm(0, Ω) for each i. Thus, the components of the jth column of X
are independently distributed each as N(0, σjj), where σjj is the jth diagonal ele-
ment of Ω, so that the jth diagonal element of X ′X has the distribution σjjχ2
n. The
joint distribution of all elements of the m × m matrix X ′X is called the Wishart dis-
tribution with scale matrix Ω and degrees of freedom n, and it will be denoted by
Wm(Ω, n). This Wishart distribution, like the chi-squared distribution χ2
n, is said to
be central. More generally, if x1, . . . , xn are independent and xi ∼Nm(μi, Ω), then
X ′X has the noncentral Wishart distribution with noncentrality matrix Φ = 1
2M ′M,
where M ′ is the m × n matrix given by M ′ = (μ1, . . . , μn). We will denote this
noncentral Wishart distribution as Wm(Ω, n, Φ). Additional information regarding
the Wishart distribution,such as the form of its density function, can be found in texts
on multivariate analysis such as Srivastava and Khatri (1979) and Muirhead (1982).
If A is an n × n symmetric matrix and X ′ is an m × n matrix, then the matrix
X ′AX is sometimes called a generalized quadratic form. Theorem 11.25 gives some
generalizations of the results obtained in Section 11.4 and Section 11.5 regarding
quadratic forms to these generalized quadratic forms.
Theorem 11.25
Let X ′ be an m × n matrix whose columns are independently dis-
tributed, with the ith column having the Nm(μi, Ω) distribution, where Ω is positive
definite. Suppose that A and B are n × n symmetric matrices whereas C is k × n.
Let M ′ = (μ1, . . . , μn), Φ = 1
2M ′AM, and r = rank(A). Then
(a) X ′AX ∼Wm(Ω, r, Φ), if A is idempotent,
(b) X ′AX and X ′BX are independently distributed if AB = (0),
(c) X ′AX and CX are independently distributed if CA = (0).

486
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Proof.
The proof of (a) will be complete if we can show that an m × r matrix Y ′
exists, such that X ′AX = Y ′Y , where the columns of Y ′ are independently dis-
tributed, each having a normal distribution with the same covariance matrix Ω, and
1
2E(Y ′)E(Y ) = Φ. Since the columns of X ′ are independently distributed, it follows
that
vec(X ′) ∼Nnm(vec(M ′), In ⊗Ω).
Since A is symmetric, idempotent, and has rank r, an n × r matrix P must exist
which satisfies A = PP ′ and P ′P = Ir. Consequently, X ′AX = Y ′Y , where the
m × r matrix Y ′ = X ′P, so that
vec(Y ′) = vec(X ′P) = (P ′ ⊗Im)vec(X ′)
∼Nmr((P ′ ⊗Im)vec(M ′), (P ′ ⊗Im)(In ⊗Ω)(P ⊗Im))
∼Nmr(vec(M ′P), (Ir ⊗Ω)),
which means that the columns of Y ′ are independently and normally distributed, each
with covariance matrix Ω. Furthermore,
1
2E(Y ′)E(Y ) = 1
2M ′PP ′M = 1
2M ′AM = Φ,
and so (a) follows. To prove (b), note that because A and B are symmetric, AB = (0)
implies that AB = BA, so A and B are diagonalized by the same orthogonal matrix;
that is, there exist diagonal matrices C and D and an orthogonal matrix Q, such
that Q′AQ = C and Q′BQ = D. Furthermore, AB = (0) implies that CD = (0),
so that by appropriately choosing Q, we will have C = diag(c1, . . . , ch, 0, . . . , 0)
and D = diag(0, . . . , 0, dh+1, . . . , dn) for some h. Thus, if we let U = Q′X, we
find that
X ′AX = U ′CU =
h

i=1
ciuiu′
i,
X ′BX = U ′DU =
n

i=h+1
diuiu′
i,
where ui is the ith column of U ′. Since vec(U ′) ∼Nnm(vec(M ′Q), (In ⊗Ω)),
these columns are independently distributed, and so (b) follows. The proof of (c)
is similar to that of (b).
□
An application of our next result indicates that a principal submatrix of a Wishart
matrix also has a Wishart distribution.
Theorem 11.26
Suppose that V ∼Wm(Ω, n, Φ) and A is a p × m matrix of con-
stants with rank(A) = p. Then AV A′ ∼Wp(AΩA′, n, AΦA′).
Proof.
Since V ∼Wm(Ω, n, Φ), it can be written as V = X ′X, where the
columns of X ′ are independently distributed with the ith column xi ∼Nm(μi, Ω)

THE WISHART DISTRIBUTION
487
and the matrix M ′ having μi as its ith column satisfies
1
2M ′M = Φ. Let
Y ′ = AX ′, so that the columns of Y ′ are also independent with ith col-
umn
yi = Axi ∼Np(Aμi, AΩA′).
It
follows
from
Theorem
11.25
that
AV A′ = AX ′XA′ = Y ′Y has the distribution Wp(AΩA′, n, Φ∗). The matrix Φ∗
satisfies
Φ∗= 1
2E(Y ′)E(Y ) = 1
2AE(X ′)E(X)A′ = 1
2AM ′MA′ = AΦA′,
and so the proof is complete.
□
If a matrix V having a Wishart distribution is partitioned in the form
V =

V11
V12
V ′
12
V22

,
where V11 and V22 are square matrices, then it is an immediate consequence of
Theorem 11.26 that V11, as well as V22, has a Wishart distribution. Theorem 11.27
indicates that the Schur complement of V11 in V also has a Wishart distribution.
Theorem 11.27
Suppose that V ∼Wm(Ω, n), where Ω is positive definite. Parti-
tion V and Ω as
V =
V11
V12
V ′
12
V22

,
Ω =
Ω11
Ω12
Ω′
12
Ω22

,
where V11 and Ω11 are m1 × m1 and V22 and Ω22 are m2 × m2. Then
V11 −V12V −1
22 V ′
12 ∼Wm1(Ω11 −Ω12Ω−1
22 Ω′
12, n −m2).
Proof.
Since V ∼Wm(Ω, n), it can be expressed as V = X ′X, where the columns
of X ′ are independently distributed each having the distribution Nm(0, Ω). Parti-
tioning the n × m matrix X as X = (X1, X2), where X1 is n × m1, we find that
V11 = X′
1X1, V22 = X′
2X2, and V12 = X′
1X2. Thus,
V11 −V12V −1
22 V ′
12 = X′
1X1 −X′
1X2(X′
2X2)−1X′
2X1
= X′
1{In −X2(X′
2X2)−1X′
2}X1 = X′
1AX1,
where A = In −X2(X′
2X2)−1X2. Now, from Example 7.3, we know that, given
X2, the columns of X′
1 are independently and normally distributed with covariance
matrix Ω11 −Ω12Ω−1
22 Ω′
12, whereas E(X′
1|X2) = Ω12Ω−1
22 X′
2. Since A is a symmetric
idempotent matrix of rank n −m2, it follows from Theorem 11.25 that, given X2,
X′
1AX1 = V11 −V12V −1
22 V ′
12 ∼Wm1(Ω11 −Ω12Ω−1
22 Ω′
12, n −m2).
This
Wishart
distribution is central because
E(X′
1|X2)AE(X1|X2) = Ω12Ω−1
22 X′
2{In −X2(X′
2X2)−1X′
2}X2Ω−1
22 Ω′
12
= Ω12Ω−1
22 {X′
2X2 −X′
2X2}Ω−1
22 Ω′
12
= (0).

488
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
The result now follows because this conditional distribution of X′
1AX1 does not
depend on X2.
□
If the columns of the m × n matrix X ′ are independent and identically distributed
as Nm(0, Ω) and M ′ is an m × n matrix of constants, then V = (X + M)′(X + M)
has the Wishart distribution Wm(Ω, n, 1
2M ′M). A more general situation is one in
which the columns of X ′ are independent and identically distributed having zero
mean vector and some nonnormal multivariate distribution. In this case, the distri-
bution of V = (X + M)′(X + M), which may be complicated, will depend on the
specific nonnormal distribution. In particular, the moments of V are directly related
to the moments of the columns of X ′. Our next result gives expressions for the first
two moments of V when M = (0). Since V is a matrix and joint distributions are
more conveniently handled in the form of vectors, we will vectorize V ; that is, for
instance, variances and covariances of the elements of V can be obtained from the
matrix var{vec(V )}.
Theorem 11.28
Let the columns of the m × n matrix X ′ = (x1, . . . , xn) be
independently and identically distributed with E(xi) = 0, var(xi) = Ω, and
E(xix′
i ⊗xix′
i) = Ψ. If V = X ′X, then
(a) E(V ) = nΩ,
(b) var{vec(V )} = n{Ψ −vec(Ω)vec(Ω)′}.
Proof.
Since E(xi) = 0, we have Ω = E(xix′
i), and so
E(V ) = E(X ′X) =
n

i=1
E(xix′
i) =
n

i=1
Ω = nΩ.
In addition, because x1, . . . , xn are independent, we have
var{vec(V )} = var

vec
 n

i=1
xix′
i

= var
 n

i=1
vec(xix′
i)

=
n

i=1
var{vec(xix′
i)} =
n

i=1
var(xi ⊗xi)
=
n

i=1
{E(xix′
i ⊗xix′
i) −E(xi ⊗xi)E(x′
i ⊗x′
i)}
=
n

i=1
{Ψ −vec(Ω)vec(Ω)′} = n{Ψ −vec(Ω)vec(Ω)′},
and so the proof is complete.
□

THE WISHART DISTRIBUTION
489
The expression for var{vec(V )} simplifies when V has a Wishart distribution
because of the special structure of the fourth moments of the normal distribution. This
simplified expression is given in Theorem 11.29. Note that although this theorem is
stated for normally distributed columns, the first result given applies to the general
case as well.
Theorem 11.29
Let the columns of the m × n matrix X ′ be independently and
identically distributed as Nm(0, Ω). Define V = (X + M)′(X + M), where M ′ =
(μ1, . . . , μn) is an m × n matrix of constants, so that V ∼Wm(Ω, n, 1
2M ′M).
Then
(a) E(V ) = nΩ + M ′M,
(b) var{vec(V )} = 2Nm{n(Ω ⊗Ω) + Ω ⊗M ′M + M ′M ⊗Ω}.
Proof.
Since E(X) = (0) and E(X ′X) = nΩ from Theorem 11.28, it follows that
E(V ) = E(X ′X + X ′M + M ′X + M ′M)
= E(X ′X) + M ′M = nΩ + M ′M.
Proceeding as in the proof of Theorem 11.28, we obtain
var{vec(V )} =
n

i=1
var{(xi + μi) ⊗(xi + μi)}.
(11.14)
However,
(xi + μi) ⊗(xi + μi) = xi ⊗xi + xi ⊗μi + μi ⊗xi + μi ⊗μi
= xi ⊗xi + (Im2 + Kmm)(xi ⊗μi) + μi ⊗μi
= xi ⊗xi + 2Nm(Im ⊗μi)xi + μi ⊗μi.
Since all first- and third-order moments of xi are equal to 0, xi ⊗xi and xi are
uncorrelated, and so using Theorem 11.21 and Problem 8.60, we find that
var{(xi + μi) ⊗(xi + μi)} = var(xi ⊗xi) + var{2Nm(Im ⊗μi)xi}
= 2Nm(Ω ⊗Ω)
+4Nm(Im ⊗μi)Ω(Im ⊗μ′
i)Nm
= 2Nm(Ω ⊗Ω) + 4Nm(Ω ⊗μiμ′
i)Nm
= 2Nm(Ω ⊗Ω + Ω ⊗μiμ′
i
+μiμ′
i ⊗Ω).
(11.15)

490
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Now substituting (11.15) in (11.14) and simplifying, we obtain (b).
□
Example 11.8
In Example 11.3 and Example 11.6, it was shown that, when sam-
pling from a normal distribution, a constant multiple of the sample variance s2 has a
chi-squared distribution, and it is independently distributed of the sample mean x. In
this example, we consider the multivariate version of this problem involving x and S;
that is, suppose that x1, . . . , xn are independently distributed with xi ∼Nm(μ, Ω)
for each i, and define X ′ to be the m × n matrix (x1, . . . , xn). Then the sample
mean vector and sample covariance matrix can be expressed as
x = 1
n
n

i=1
xi = 1
nX ′1n
and
S =
1
n −1
n

i=1
(xi −x)(xi −x)′ =
1
n −1
 n

i=1
xix′
i −nx x′

=
1
n −1(X ′X −n−1X ′1n1′
nX) =
1
n −1X ′(In −n−11n1′
n)X.
Since A = (In −n−11n1′
n) is idempotent and rank(A) = tr(A) = n −1, it follows
from Theorem 11.25(a) that (n −1)S has a Wishart distribution. To determine its
noncentrality matrix, note that M ′ = (μ, . . . , μ) = μ1′
n, so that
M ′AM = μ1′
n(In −n−11n1′
n)1nμ′ = μ(n −n)μ′ = (0).
Thus, (n −1)S has the central Wishart distribution Wm(Ω, n −1). Furthermore,
using Theorem 11.25(c), we see that S and x are independently distributed because
1′
n(In −n−11n1′
n) = (1′
n −1′
n) = 0′.
In addition, it follows from Theorem 11.28 and Theorem 11.21 that
E(S) = Ω,
var{vec(S)} =
2
n −1Nm(Ω ⊗Ω) =
2
n −1Nm(Ω ⊗Ω)Nm.
The redundant elements in vec(S) can be eliminated by using v(S). Since v(S) =
D+
mvec(S), where Dm is the duplication matrix discussed in Section 8.7, we have
var{v(S)} =
2
n −1D+
mNm(Ω ⊗Ω)NmD+′
m.
In some situations, we may be interested only in the sample variances and not the
sample covariances; that is, the random vector of interest here is the m × 1 vector
s = (s11, . . . , smm)′. Expressions for the mean vector and covariance matrix of s

THE WISHART DISTRIBUTION
491
are easily obtained from the formulas above because s = w(S) = Ψmvec(S) as seen
in Problem 8.48, where
Ψm =
m

i=1
ei,m(ei,m ⊗ei,m)′.
Thus, using the properties of Ψm obtained in Problem 8.48, we find that
E(s) = Ψmvec{E(S)} = Ψmvec(Ω) = w(Ω),
var(s) = Ψmvar{vec(S)}Ψ′
m = Ψm

2
n −1Nm(Ω ⊗Ω)Nm

Ψ′
m
=
2
n −1Ψm(Ω ⊗Ω)Ψ′
m =
2
n −1(Ω ⊙Ω),
where ⊙is the Hadamard product.
Example 11.9
We can use the perturbation formulas for eigenvalues and eigenvec-
tors of a symmetric matrix obtained in Section 9.6 to approximate the distributions
of an eigenvalue or an eigenvector of a matrix having a Wishart distribution. One
important application in statistics that uses these asymptotic distributions is principal
components analysis, an analysis involving the eigenvalues and eigenvectors of the
m × m sample covariance matrix S. The exact distributions of an eigenvalue and an
eigenvector of S are rather complicated, whereas their asymptotic distributions follow
in a fairly straightforward manner from the asymptotic distribution of S. Now it can
be shown by using the central limit theorem (see Muirhead, 1982) that √n −1vec(S)
has an asymptotic normal distribution. In particular, using results from Example 11.8,
we have, asymptotically,
√
n −1{vec(S) −vec(Ω)} ∼Nm2(0, 2Nm(Ω ⊗Ω)),
where Ω is the population covariance matrix. Let W = S −Ω and W∗= √n −1W,
so that vec(W∗) has the asymptotic normal distribution indicated above. Suppose that
γi is a normalized eigenvector of S = Ω + W corresponding to the ith largest eigen-
value λi, whereas qi is a normalized eigenvector of Ω corresponding to its ith largest
eigenvalue xi. Now if xi is a distinct eigenvalue of Ω, then we have the first-order
approximations from Section 9.6
λi = xi + q′
iWqi = xi + (q′
i ⊗q′
i)vec(W),
γi = qi −(Ω −xiIm)+Wqi
= qi −{q′
i ⊗(Ω −xiIm)+}vec(W).
(11.16)

492
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Thus, the asymptotic normality of ai = √n −1(λi −xi) follows from the asymp-
totic normality of vec(W∗). Furthermore, we have, asymptotically,
E(ai) = (q′
i ⊗q′
i)E{vec(W∗)} = (q′
i ⊗q′
i)0 = 0,
var(ai) = (q′
i ⊗q′
i)(var{vec(W∗)})(qi ⊗qi)
= (q′
i ⊗q′
i)(2Nm(Ω ⊗Ω))(qi ⊗qi)
= 2(q′
iΩqi ⊗q′
iΩqi) = 2x2
i;
that
is,
for
large
n,
λi ∼N(xi, 2x2
i/(n −1)),
approximately.
Similarly,
bi = √n −1(γi −qi) is asymptotically normal with
E(bi) = −{q′
i ⊗(Ω −xiIm)+}E{vec(W∗)}
= −{q′
i ⊗(Ω −xiIm)+}0 = 0,
Ξ = var(bi) = {q′
i ⊗(Ω −xiIm)+}{var{vec(W∗)}}
×{q′
i ⊗(Ω −xiIm)+}′
= {q′
i ⊗(Ω −xiIm)+}{2Nm(Ω ⊗Ω)}{qi ⊗(Ω −xiIm)+}
= {(Ω −xiIm)+ ⊗q′
i + q′
i ⊗(Ω −xiIm)+}(Ω ⊗Ω)
×{qi ⊗(Ω −xiIm)+}
= q′
iΩqi ⊗(Ω −xiIm)+Ω(Ω −xiIm)+
= xi
⎧
⎨
⎩

j̸=i
xj
(xj −xi)2 qjq′
j
⎫
⎬
⎭,
and so for large n, γi ∼Nm(qi, (n −1)−1Ξ), approximately. While we can use
the first-order approximations in (11.16) to obtain the asymptotic distributions, we
can use higher order approximations, such as those given in Theorem 9.5, to fur-
ther improve the performance of these asymptotic distributions. The most common
application of this process involves asymptotic chi-squared distributions, so we will
illustrate the basic idea with the statistic
t = (n −1)(λi −xi)2
2x2
i
,
which, because of the asymptotic normality of λi, is asymptotically chi-squared with
one degree of freedom. The mean of this chi-squared distribution is 1, whereas the
exact mean of t is of the form
E(t) = 1 +
∞

j=1
cj
(n −1)(j+1)/2 ,

THE WISHART DISTRIBUTION
493
where the cj’s are constants. We can use the higher order approximations of λi to
determine the first constant c1, and then this may be used to compute an adjusted
statistic
t∗=

1 −
c1
(n −1)

t.
The mean of this adjusted statistic is
E(t∗) =

1 −
c1
(n −1)

E(t)
=

1 −
c1
(n −1)
 ⎛
⎝1 +
∞

j=1
cj
(n −1)(j+1)/2
⎞
⎠
= 1 +
∞

j=2
dj
(n −1)(j+1)/2 ,
where the dj’s are constants that are functions of the cj’s. Note that the mean of t∗
converges to 1 at a faster rate than does E(t). For this reason, the chi-squared distribu-
tion with one degree of freedom should approximate the distribution of this adjusted
statistic better than it would approximate the distribution of t. This type of adjustment
of asymptotically chi-squared statistics is commonly referred to as a Bartlett adjust-
ment (Bartlett, 1937, 1947). Some further discussion of Bartlett adjustments can be
found in Barndorff-Nielsen and Cox (1994).
Some of the inequalities for eigenvalues developed in Chapter 3 have important
applications regarding the distributions of eigenvalues of certain functions of Wishart
matrices. One such application is illustrated in Example 11.10.
Example 11.10
A multivariate analysis of variance, such as the multivariate
one-way classification model discussed in Example 3.16, uses the eigenvalues of
BW −1, where the m × m matrices B and W are independently distributed with
B ∼Wm(Im, b, Φ) and W ∼Wm(Im, w) (Problem 11.49). We will show that if
the rank of the noncentrality matrix Φ is r < m and V1 and V2 are independently
distributed with V1 ∼Wm−r(Im−r, b −r) and V2 ∼Wm−r(Im−r, w), then
P{λr+i(BW −1) > c} ≤P{λi(V1V −1
2
) > c},
for i = 1, . . . , m −r and any constant c. This result is useful in determining the
dimensionality in a canonical variate analysis (see Schott, 1984). Since rank(Φ) = r,
an r × m matrix T exists, such that 1
2T ′T = Φ. If we define the m × b matrix
M ′ = (T ′
(0)), then because
1
2M ′M = Φ and B ∼Wm(Im, b, Φ), it follows
that B can be expressed as B = X ′X, where X ′ is an m × b matrix for which

494
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
vec(X ′) ∼Nbm(vec(M ′), Ib ⊗Im). Partitioning X ′ as X ′ = (X′
1
X′
2), where
X′
1 is m × r, we find that
B = X′
1X1 + X′
2X2 = B1 + B2,
where B1 = X′
1X1 ∼Wm(Im, r, Φ) and B2 = X′
2X2 ∼Wm(Im, b −r) because
vec(X′
1) ∼Nrm(vec(T ′), Ir ⊗Im)
and
vec(X′
2) ∼N(b−r)m(vec{(0)}, Ib−r ⊗Im).
Now for fixed B1, let F be any m × (m −r) matrix satisfying F ′B1F = (0) and
F ′F = Im−r, and define the sets
S1(B1) = {B2, W : λr+i(BW −1) > c},
S2(B1) = {B2, W : λi{(F ′B2F)(F ′WF)−1} > c}.
It follows from Problem 3.47(a) that
λi{(F ′BF)(F ′WF)−1} = λi{(F ′B2F)(F ′WF)−1} ≥λr+i(BW −1),
so for each fixed B1, S1(B1) ⊆S2(B1), and it follows from Theorem 11.26 that
V1 = F ′B2F ∼Wm−r(Im−r, b −r) and V2 = F ′WF ∼Wm−r(Im−r, w). Conse-
quently, if g(W), f1(B1), and f2(B2) are the density functions for W, B1, and B2,
respectively, then

S1(B1)
g(W)f2(B2)dWdB2 ≤

S2(B1)
g(W)f2(B2)dWdB2
= P{λi(V1V −1
2
) > c}.
If we also define the sets
C1 = {B1, B2, W : λr+i(BW −1) > c},
C2 = {B1 : B1 positive deﬁnite },
then the desired result follows because
P{λr+i(BW −1) > c} =

C1
g(W)f1(B1)f2(B2)dWdB1dB2
=

C2

S1(B1)
g(W)f2(B2)dWdB2

f1(B1)dB1
≤

C2
P{λi(V1V −1
2
) > c}f1(B1)dB1
= P{λi(V1V −1
2
) > c}.

THE WISHART DISTRIBUTION
495
We can use the relationship between the sample correlation and sample covariance
matrices and the expression for var{vec(S)} given in Example 11.8 to obtain an
expression for the asymptotic covariance matrix of vec(R). This is the subject of our
final example.
Example 11.11
As in Example 11.8, let x1, . . . , xn be independently distributed
with xi ∼Nm(μ, Ω), for each i, and let S and R be the sample covariance and cor-
relation matrices computed from this sample. Thus, if we use the notation Da
X =
diag(xa
11, . . . , xa
mm), where X is an m × m matrix, then the sample correlation
matrix can be expressed as
R = D−1/2
S
SD−1/2
S
,
whereas the population correlation matrix is given by
P = D−1/2
Ω
ΩD−1/2
Ω
.
Note that if we define yi = D−1/2
Ω
xi, then y1, . . . , yn are independently distributed
with yi ∼Nm(D−1/2
Ω
μ, P). If S∗is the sample covariance matrix computed from
the yi’s, then S∗= D−1/2
Ω
SD−1/2
Ω
, D−1/2
S∗
= D−1/2
S
D1/2
Ω
= D1/2
Ω D−1/2
S
, and so
D−1/2
S∗
S∗D−1/2
S∗
= D−1/2
S
D1/2
Ω (D−1/2
Ω
SD−1/2
Ω
)D1/2
Ω D−1/2
S
= D−1/2
S
SD−1/2
S
= R;
that is, the sample correlation matrix computed from the yi’s is the same as that
computed from the xi’s. If A = S∗−P, then the first-order approximation for R is
given by (see Problem 9.23)
R = P + A −1
2(PDA + DAP),
and so
vec(R) = vec(P) + vec(A) −1
2{vec(PDA) + vec(DAP)}
= vec(P) + vec(A) −1
2{(Im ⊗P) + (P ⊗Im)}vec(DA)
= vec(P) +

Im2 −1
2 {(Im ⊗P)
+(P ⊗Im)}Λm

vec(A),
(11.17)
where
Λm =
m

i=1
(Eii ⊗Eii).

496
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
Thus, because
var{vec(A)} = var{vec(S∗)} =
2
n −1Nm(P ⊗P)Nm,
we get the first-order approximation
var{vec(R)} =
2
n −1HNm(P ⊗P)NmH ′,
where the matrix H is the premultiplier on vec(A) in the last expression given in
(11.17). Simplification (see Problem 11.53) leads to
var{vec(R)} =
2
n −1NmΘNm,
(11.18)
where
Θ = {Im2 −(Im ⊗P)Λm}(P ⊗P){Im2 −Λm(Im ⊗P)}.
Since R is symmetric and has each diagonal element equal to one, its redundant and
nonrandom elements can be eliminated by using ˜v(R). Since ˜v(R) = ˜Lmvec(R),
where ˜Lm is the matrix discussed in Section 8.7, we find that the asymptotic covari-
ance matrix of ˜v(R) is given by
var{˜v(R)} =
2
n −1
˜LmNmΘNm ˜L′
m.
Note that the Hadamard product and its associated properties can be useful in analyses
involving the manipulation of Θ because
Θ = P ⊗P −(Im ⊗P)Λm(P ⊗P) −(P ⊗P)Λm(Im ⊗P)
+(Im ⊗P)Λm(P ⊗P)Λm(Im ⊗P),
and the last term on the right-hand side of this equation can be expressed as
(Im ⊗P)Λm(P ⊗P)Λm(Im ⊗P) = (Im ⊗P)Ψ′
m(P ⊙P)Ψm(Im ⊗P).
PROBLEMS
11.1 We saw in the proof of Theorem 11.1 that if A is an m × m idempotent matrix,
then rank(A) + rank(Im −A) = m. Prove the converse; that is, show that if
A is an m × m matrix satisfying rank(A) + rank(Im −A) = m, then A is
idempotent.

PROBLEMS
497
11.2 Suppose that A is an m × m idempotent matrix. Show that each of the follow-
ing matrices is also idempotent:
(a) A′.
(b) BAB−1, where B is any m × m nonsingular matrix.
(c) An, where n is a positive integer.
11.3 Show that if A is an m × m symmetric idempotent matrix having rank r, then
A = PP ′ for some m × r matrix satisfying P ′P = Ir.
11.4 Let A be an m × n matrix. Show that each of the following matrices is idem-
potent:
(a) AA−.
(b) A−A.
(c) A(A′A)−A′.
11.5 Let A and B be m × m symmetric idempotent matrices. Show that if the col-
umn spaces of A and B are the same, then A = B.
11.6 Determine the class of m × 1 vectors {x}, for which xx′ is idempotent.
11.7 Determine the values of the scalars a, b, and c for which each of the following
is an idempotent matrix.
(a) a1m1′
m.
(b) bIm + c1m1′
m.
11.8 Let A be an m × n matrix with rank(A) = m. Show that A′(AA′)−1A is
symmetric, idempotent, and find its rank.
11.9 Let A and B be m × m matrices. Show that if B is nonsingular and AB is
idempotent, then BA is also idempotent.
11.10 Let A be an m × m symmetric idempotent matrix of rank r. Show that if B is
an m × r matrix of rank r satisfying AB = B, then A = B(B ′B)−1B ′.
11.11 Show that if A is an m × m matrix and A2 = cA for some scalar c, then
tr(A) = c rank(A).
11.12 Let A be an m × m symmetric idempotent matrix and B be an m × m non-
negative definite matrix. Show that if Im −A −B is nonnegative definite,
then AB = BA = (0).
11.13 Let A be an m × m symmetric idempotent matrix and B be an m × m matrix.
(a) Show that if AB = B, then A −BB+ is symmetric idempotent with rank
equal to rank(A) −rank(B).
(b) Show that if AB = (0) and rank(A) + rank(B) = m, then A = Im −
BB+.
11.14 Give an example of a collection of matrices A1, . . . , Ak that satisfies condi-
tions (a) and (d) of Corollary 11.8.1, but it does not satisfy conditions (b) and
(c). Similarly, find a collection of matrices that satisfies conditions (c) and (d),
but it does not satisfy conditions (a) and (b).

498
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
11.15 Prove Theorem 11.12.
11.16 Let x ∼Nm(μ, Ω), where Ω is a positive definite matrix, and let A be an
m × m symmetric matrix.
(a) Show that the moment generating function of y = x′Ax can be expressed
as
my(t) = |Im −2tAΩ|−1/2 exp

−1
2μ′[Im −(Im −2tAΩ)−1]Ω−1μ

.
(b) If w ∼χ2
r( 1
2μ′Aμ), then it can be shown that
mw(t) = (1 −2t)−r/2 exp

−1
2μ′Aμ[1 −(1 −2t)−1]

.
Use this result and the moment generating function from (a) to show that
the sufficient condition given in Theorem 11.12 is necessary as well. Do
this by equating the two moment generating functions at μ = 0 and using
the resulting equation to show that AΩ must be idempotent of rank r.
11.17 Let A be an m × m symmetric matrix with r = rank(A), and suppose that
x ∼Nm(0, Im). Show that the distribution of x′Ax can be expressed as a
linear combination of r independent chi-squared random variables, each with
one degree of freedom. What are the coefficients in this linear combination
when A is idempotent?
11.18 Extend the result of Problem 11.17 to the situation in which x ∼Nm(0, Ω),
where Ω is nonnegative definite; that is, show that if A is a symmetric
matrix, then x′Ax can be expressed as a linear combination of independent
chi-squared random variables each having one degree of freedom. How many
chi-squared random variables are in this linear combination?
11.19 Let x1, . . . , xn be a random sample from a normal distribution with mean μ
and variance σ2, and let x be the sample mean. Write
t = n(x −μ)2
σ2
as a quadratic form in the vector (x −μ1n), where x = (x1, . . . , xn)′. What
is the distribution of t?
11.20 Let x ∼Nm(μ, Ω), where Ω is positive definite, and suppose that A and B
are m × m symmetric matrices. Show that the sufficient condition given in
Theorem 11.14 is also necessary. That is, show that if x′Ax and x′Bx are
independently distributed, then AΩB = (0).
11.21 Suppose that x ∼Nn(μ, Ω), where Ω is positive definite. Partition x, μ, and
Ω as
x =

x1
x2

,
μ =

μ1
μ2

,
Ω =

Ω11
Ω12
Ω′
12
Ω22

,
where x1 is r × 1 and x2 is (n −r) × 1. Show that

PROBLEMS
499
(a) t1 = (x1 −μ1)′Ω−1
11 (x1 −μ1) ∼χ2
r,
(b) t2 = (x −μ)′Ω−1(x −μ) −(x1 −μ1)′Ω−1
11 (x1 −μ1) ∼χ2
n−r,
(c) t1 and t2 are independently distributed.
11.22 Suppose that x ∼Nm(μ, Ω) and the m × m matrices A1 and A2 are such
that t1 = x′A1x ∼χ2
d1 and t2 = x′A2x ∼χ2
d2, independently. Consequently,
t = (t1/d1)/(t2/d2) has the F distribution with d1 and d2 degrees of free-
dom. Show that if y has an elliptical distribution with mean vector μ and
covariance matrix Ω, then w = (w1/d1)/(w2/d2) has this same F distribution,
where w1 = y′A1y and w2 = y′A2y.
11.23 Prove Theorem 11.15.
11.24 Pearson’s chi-squared statistic is given by
t =
m

i=1
(nxi −nμi)2
nμi
,
where n is a positive integer, the xi’s are random variables, and the μi’s are
nonnegative constants satisfying μ1 + · · · + μm = 1. Let x = (x1, . . . , xm)′,
μ = (μ1, . . . , μm)′, and Ω = D −μμ′, where D = diag(μ1, . . . , μm).
(a) Show that Ω is a singular matrix.
(b) Show that if √n(x −μ) ∼Nm(0, Ω), then t ∼χ2
m−1.
11.25 Suppose that x ∼N4(0, I4), and consider the three functions of the compo-
nents of x given by
t1 = 1
4(x1 + x2 + x3 + x4)2 + 1
2(x1 −x2)2,
t2 = 1
12(x1 + x2 + x3 −3x4)2,
t3 = (x1 + x2 −2x3)2 + (x3 −x4)2.
(a) Write t1, t2, and t3 as quadratic forms in x.
(b) Which of these statistics have chi-squared distributions?
(c) Which of the pairs t1 and t2, t1 and t3, and t2 and t3 are independently
distributed?
11.26 Suppose that x ∼N4(μ, Ω), where μ = (1, −1, 1, −1)′ and Ω = I4 + 141′
4.
Define
t1 = 1
2(x1 −x2)2 + 1
2(x3 −x4)2,
t2 = 1
2(x1 + x2)2 + 1
2(x3 + x4)2.

500
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
(a) Does t1 or t2 have a chi-squared distribution? If so, identify the parameters
of the distribution.
(b) Are t1 and t2 independently distributed?
11.27 Prove Theorem 11.16.
11.28 Prove Theorem 11.17.
11.29 The purpose of this exercise is to generalize the results of Example 11.5 to
a test of the hypothesis that Hβ = c, where H is an m2 × m matrix having
rank m2 and c is an m2 × 1 vector; Example 11.5 dealt with the special case in
which H = ((0)
Im2) and c = 0. Let G be an (m −m2) × m matrix having
rank m −m2 and satisfying HG′ = (0). Show that the reduced model may
be written as
y∗= X∗β∗+ ϵ,
where y∗= y −XH ′(HH ′)−1c, X∗= XG′(GG′)−1, and β∗= Gβ. Use
the sum of squared errors for this reduced model and the sum of squared errors
for the complete model to construct the appropriate F statistic.
11.30 Suppose that x ∼Nm(0, Ω), where r = rank(Ω) < m. If T is any m × r
matrix satisfying TT ′ = Ω, and z ∼Nr(0, Ir), then x is distributed the same
as Tz. Use this to show that the formulas given in Theorem 11.22 for positive
definite Ω also hold when Ω is positive semidefinite.
11.31 Let z ∼Nm(0, Im). Use the fact that the first six moments of the standard
normal distribution are 0, 1, 0, 3, 0, and 15 to show that
E(zz′ ⊗zz′ ⊗zz′) = Im3 + 1
2
m

i=1
m

j=1
(Im ⊗Tij ⊗Tij
+Tij ⊗Im ⊗Tij + Tij ⊗Tij ⊗Im)
+
m

i=1
m

j=1
m

k=1
(Tij ⊗Tik ⊗Tjk),
where Tij = Eij + Eji.
11.32 Suppose that z ∼Nm(0, Im).
(a) Show that
E(zz′ ⊗zz′) = Nm{2Im2 + vec(Im)vec(Im)′}Nm.
(b) Let Δ be the matrix defined in Problem 8.61. Show that the sixth-order
moment matrix given in Problem 11.31 can be more compactly expressed
as
E(zz′ ⊗zz′ ⊗zz′) = Δ{6Im3 + 9Im ⊗vec(Im)vec(Im)′}Δ.
Expressions for higher order moment matrices of z, such as E(zz′ ⊗
zz′ ⊗zz′ ⊗zz′), can be found in Schott (2003).

PROBLEMS
501
11.33 Suppose that y has an elliptical distribution with mean vector 0, covariance
matrix Ω, and finite fourth moments.
(a) Show that for some constant c,
E(yy′ ⊗yy′) = c{2Nm(Ω ⊗Ω) + vec(Ω)vec(Ω)′}.
(b) Use the expression given in (a) to show that
c =
E(y4
i )
3{E(y2
i )}2 ,
regardless of the choice of i.
(c) Show that if S is the sample covariance matrix computed from a random
sample of size n from this elliptical distribution, then
var{vec(S)} ≈
1
(n −1){2cNm(Ω ⊗Ω) + (c −1)vec(Ω)vec(Ω)′},
for large n.
(d) If R is the sample correlation matrix, show that the first-order approxima-
tion
var{vec(R)} ≈
2c
n −1NmΘNm
holds, where Θ is as defined in Example 11.11.
11.34 Suppose that u is uniformly distributed on the m-dimensional unit sphere.
(a) Show that
E(u ⊗u) = m−1vec(Im).
(b) Show that
E(uu′ ⊗uu′) = {m(m + 2)}−1{2Nm + vec(Im)vec(Im)′}.
11.35 Let A, B, and C be m × m symmetric matrices, and suppose that
x ∼Nm(0, Im).
(a) Show that
E(x′Axx′Bxx′Cx) = tr{(A ⊗B ⊗C)E(xx′ ⊗xx′ ⊗xx′)}.
(b) Use part (a) and the result of Problem 11.31 to derive the formula given
in Theorem 11.24 for E(x′Axx′Bxx′Cx).
11.36 Let x ∼Nm(μ, Ω), where Ω is positive definite.

502
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
(a) Using Theorem 11.21, show that
var(x ⊗x) = 2Nm(Ω ⊗Ω + Ω ⊗μμ′ + μμ′ ⊗Ω).
(b) Show that the matrix (Ω ⊗Ω + Ω ⊗μμ′ + μμ′ ⊗Ω) is nonsingular.
(c) Determine the eigenvalues of Nm. Use these along with part (b) to show
that rank{var(x ⊗x)} = m(m + 1)/2.
11.37 Suppose that the m × 1 vector x and the n × 1 vector y are independently
distributed with E(x) = μ1, E(y) = μ2, E(xx′) = V1, and E(yy′) = V2.
Show that
(a) E(xy′ ⊗xy′) = vec(V1){vec(V2)}′,
(b) E(xy′ ⊗yx′) = (V1 ⊗V2)Kmn = Kmn(V2 ⊗V1),
(c) E(x ⊗x ⊗y ⊗y) = vec(V1) ⊗vec(V2),
(d) E(x ⊗y ⊗x ⊗y) = (Im ⊗Knm ⊗In){vec(V1) ⊗vec(V2)},
(e) var(x ⊗y) = V1 ⊗V2 −μ1μ′
1 ⊗μ2μ′
2.
11.38 Let A, B, and C be m × m symmetric matrices, and let a and b be m × 1
vectors of constants. If x ∼Nm(0, Ω), show that
(a) E(x′Aax′Bb) = a′AΩBb,
(b) E(x′Aax′Bbx′Cx) = a′AΩBbtr(ΩC) + 2a′AΩCΩBb.
11.39 Suppose that x ∼N4(μ, Ω), where μ = 14 and Ω = 4I4 + 141′
4. Let the ran-
dom variables t1 and t2 be defined by
t1 = (x1 + x2 −2x3)2 + (x3 −x4)2,
t2 = (x1 −x2 −x3)2 + (x1 + x2 −x4)2.
Use Theorem 11.23 to find
(a) var(t1),
(b) var(t2),
(c) cov(t1, t2).
11.40 Verify the formulas given at the end of Example 11.7 for E(t1) and var(t1).
11.41 Suppose that V1 ∼Wm(Ω, n1) and V2 ∼Wm(Ω, n2) are independently dis-
tributed. Show that V1 + V2 ∼Wm(Ω, n1 + n2).
11.42 Suppose that V ∼Wm(Ω, n, Φ) and a is a nonnull m × 1 vector of constants.
Show that a′V a/a′Ωa ∼χ2
n(λ), where λ = a′Φa/a′Ωa.
11.43 Consider the Wishart matrix V given in Theorem 11.27.
(a) Show that V11 −V12V −1
22 V ′
12 is independently distributed of V12 and V22.
(b) Show that the conditional distribution of V12 given V22 is multivariate
normal; in particular, show that given V22,
vec(V12) ∼Nm1m2(vec(Ω12Ω−1
22 V22), V22 ⊗(Ω11 −Ω12Ω−1
22 Ω′
12)).

PROBLEMS
503
11.44 Let V ∼Wm(Ω, n), where Ω is a positive definite matrix, and let Vk and Ωk be
the leading k × k principal submatrices of V and Ω; that is, Vk is the matrix
obtained by deleting the last m −k rows and columns of V , and similarly
for Ωk. Show that if we define |V0| = 1 and |Ω0| = 1, then
|Vk|
|Vk−1|
|Ωk−1|
|Ωk|
∼χ2
n−k+1
for k = 1, . . . , m.
11.45 Suppose
that
V ∼Wm(Ω, n),
where
Ω
is
positive
definite.
Use
Theorem 11.27 to show that if A is a p × m matrix of rank p, then
(AV −1A′)−1 ∼Wp((AΩ−1A′)−1, n −m + p).
11.46 Suppose that x ∼Nm(μ, Ω), where Ω is positive definite. Partition x as x =
(x′
1, x′
2)′, where x1 is m1 × 1 and x2 is m2 × 1. Similarly, μ and Ω are parti-
tioned as
μ =

μ1
μ2

,
Ω =

Ω11
Ω12
Ω′
12
Ω22

.
(a) Show that E(x1 ⊗x2) = vec(Ω′
12) + μ1 ⊗μ2.
(b) Show that
var(x1 ⊗x2) = Ω11 ⊗Ω22 + Ω11 ⊗μ2μ′
2 + μ1μ′
1 ⊗Ω22
+Km1m2(Ω′
12 ⊗Ω12 + Ω′
12 ⊗μ1μ′
2
+μ2μ′
1 ⊗Ω12).
11.47 Suppose that the columns of X ′ = (x1, . . . , xn) are independently dis-
tributed with xi ∼Nm(μi, Ω). Let A be an n × n symmetric matrix, and let
M ′ = (μ1, . . . , μn). Use the spectral decomposition of A to show that
(a) E(X ′AX) = tr(A)Ω + M ′AM,
(b) var{vec(X ′AX)} = 2Nm{tr(A2)(Ω ⊗Ω) + Ω ⊗M ′A2M +
M ′A2M ⊗Ω}.
11.48 Let A and B be m × n matrices of constants, whereas x ∼Nn(μ, Ω). Note
that (Ax ⊙Bx) = Ψm(Ax ⊗Bx), where Ψm is the matrix defined in
Section 8.5.
(a) Show that
E(Ax ⊙Bx) = DBΩA′1m + Aμ ⊙Bμ,
where DBΩA′ is the diagonal matrix with diagonal elements equal to those
of BΩA′.

504
SOME SPECIAL TOPICS RELATED TO QUADRATIC FORMS
(b) Show that
var(Ax ⊙Bx) = A(Ω + μμ′)A′ ⊙B(Ω + μμ′)B ′
+B(Ω + μμ′)A′ ⊙A(Ω + μμ′)B ′
−Aμμ′A′ ⊙Bμμ′B ′ −Bμμ′A′ ⊙Aμμ′B ′.
For some applications of these results as well as generalizations, see Hyn-
dman and Wand (1997), Neudecker and Liu (2001), Neudecker, et al.
(1995a), and Neudecker, et al. (1995b).
11.49 Suppose that the m × 1 vectors {yij, 1 ≤i ≤k, 1 ≤j ≤ni} are indepen-
dently distributed with yij ∼Nm(μi, Ω). A multivariate analysis of variance
uses the matrices (Example 3.16)
B =
k

i=1
ni(yi −y)(yi −y)′,
W =
k

i=1
ni

j=1
(yij −yi)(yij −yi)′,
where
yi =
ni

j=1
yij
ni
,
y =
k

i=1
niyi
n ,
n =
k

i=1
ni.
Use Theorem 11.25 to show that W and B are independently distributed,
W ∼Wm(Ω, w), and B ∼Wm(Ω, b, Φ), where w = n −k, b = k −1, and
Φ = 1
2
k

i=1
ni(μi −μ)(μi −μ)′,
μ =
k

i=1
niμi
n
.
11.50 Let X ′ = (x1, . . . , xn) be an m × n matrix, where x1, . . . , xn are indepen-
dent and xi ∼Nm(0, Ω) for each i. Show that
E(X ⊗X ⊗X ⊗X) = {vec(In) ⊗vec(In)}{vec(Ω) ⊗vec(Ω)}′
+vec(In ⊗In){vec(Ω ⊗Ω)}′ + vec(Knn)
×[vec{Kmm(Ω ⊗Ω)}]′.
11.51 Let the columns of the m × n matrix X ′ be independently and identically
distributed as Nm(0, Ω). Suppose the n × m matrix M and the n × n matrix
A contain constants, and define V = (X + M)′A(X + M). Show that
var{vec(V )} = {tr(A′A)}(Ω ⊗Ω) + {tr(A2)}Kmm(Ω ⊗Ω)
+M ′A′AM ⊗Ω + Ω ⊗M ′AA′M
+Kmm(M ′A2M ⊗Ω) + Kmm(Ω ⊗M ′A2M)′.

PROBLEMS
505
11.52 Suppose that the smallest eigenvalue of the m × m covariance matrix Ω has
multiplicity r, and let P denote the eigenprojection of Ω corresponding to this
smallest eigenvalue. Let S be the sample covariance matrix computed from a
random sample of size n, and define A = S −Ω. Then
U =
r−1 m
i=m−r+1 λ2
i(S)
{r−1 m
i=m−r+1 λi(S)}2 −1,
where λ1(S) ≥· · · ≥λm(S) are the eigenvalues of S, has the second-order
approximation formula in A (see Problem 9.26) given by
U ≈r−1(tr(APAP) −r−1{tr(AP)}2).
Use this approximation to show that, when sampling from a normal
population, nrU/2 can be approximated by the chi-squared distribution with
r(r + 1)/2 −1 degrees of freedom.
11.53 Use the results of Problem 8.48(e) and Problem 8.60 to show that

Im2 −1
2{(Im ⊗P) + (P ⊗Im)}Λm

Nm = Nm{Im2 −(Im ⊗P)Λm}
thereby verifying the simplified formula for var{vec(R)} given in (11.18).
11.54 Let S be the m × m sample covariance matrix computed from a sample
of size n from a normal population with covariance matrix Ω. Denote
the eigenvalues and normalized eigenvectors of Ω by x1 ≥· · · ≥xm
and q1, . . . , qm, and those of S by λ1 ≥· · · ≥λm and γ1, . . . , γm.
Suppose that xk > xk+1 = · · · = xm, and consider the eigenprojection,
P = m
i=k+1 qiq′
i, associated with the eigenvalues xk+1, . . . , xm. An
estimate of this eigenprojection is given by ˆP = m
i=k+1 γiγ′
i. Use Theorem
9.7 and the large sample distribution of S discussed in Example 11.9 to show
that for large n, vec( ˆP) ∼Nm2(vec(P), 2NmΨ/(n −1)), approximately,
where the matrix Ψ is given by
Ψ =
k

i=1
m

j=k+1
xixj
(xi −xj)2 (qiq′
i ⊗qjq′
j + qjq′
j ⊗qiq′
i).


REFERENCES
Agaian, S. S. (1985). Hadamard Matrices and Their Applications. Springer-Verlag, Berlin.
Anderson, T. W. (1955). The integral of a symmetric unimodal function over a symmetric
convex set and some probability inequalities. Proceedings of the American Mathematical
Society, 6, 170–176.
Anderson, T. W. (1996). Some inequalities for symmetric convex sets with applications. Annals
of Statistics, 24, 753–762.
Anderson, T. W. and Das Gupta, S. (1963). Some inequalities on characteristic roots of matri-
ces. Biometrika, 50, 522–524.
Ando, T. (1989). Majorization, doubly stochastic matrices, and comparison of eigenvalues,
Linear Algebra and Its Applications, 118, 163–248.
Ando, T. (1994). Majorizations and inequalities in matrix theory. Linear Algebra and Its Appli-
cations, 199, 17–67.
Andrilli, S. and Hecker, D. (2010). Elementary Linear Algebra, 4th ed., Academic Press,
New York.
Barndorff-Nielsen, O. E. and Cox, D. R. (1994). Inferences and Asymptotics. Chapman and
Hall, London.
Bartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal
Society of London, Ser. A, 160, 268–282.
Bartlett, M. S. (1947). Multivariate analysis. Journal of the Royal Statistical Society
Supplement, Ser. B, 9, 176–197.
Basilevsky, A. (1983). Applied Matrix Algebra in the Statistical Sciences. North-Holland,
New York.
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e

508
REFERENCES
Bellman, R. (1970). Introduction to Matrix Analysis. McGraw-Hill, New York.
Ben-Israel, A. (1966). A note on an iterative method for generalized inversion of matrices.
Mathematics of Computation, 20, 439–440.
Ben-Israel, A. and Greville, T. N. E. (2003). Generalized Inverses: Theory and Applications,
2nd ed. Springer-Verlag, New York.
Berkovitz, L. D. (2002). Convexity and Optimization in Rn. John Wiley, New York.
Berman, A. and Plemmons, R. J. (1994). Nonnegative Matrices in the Mathematical Sciences.
Society for Industrial and Applied Mathematics, Singapore.
Berman, A. and Shaked-Monderer, N. (2003). Completely Positive Matrices. World Scientific,
Singapore.
Bhatia, R. (1997). Matrix Analysis. Springer-Verlag, New York.
Bhattacharya, R. N. and Waymire, E. C. (2009). Stochastic Processes with Applications. Soci-
ety for Industrial and Applied Mathematics, Philadelphia.
Boullion, T. L. and Odell, P. L. (1971). Generalized Inverse Matrices. John Wiley, New York.
Campbell, S. L. and Meyer, C. D. (1979). Generalized Inverses of Linear Transformations.
Pitman, London.
Casella, G. and Berger, R. L. (2002). Statistical Inference. Duxbury, Pacific Grove, CA.
Cline, R. E. (1964a). Note on the generalized inverse of the product of matrices. SIAM Review,
6, 57–58.
Cline, R. E. (1964b). Representations for the generalized inverse of a partitioned matrix. SIAM
Journal of Applied Mathematics, 12, 588–600.
Cline, R. E. (1965). Representations for the generalized inverse of sums of matrices. SIAM
Journal of Numerical Analysis, 2, 99–114.
Cochran, W. G. (1934). The distribution of quadratic forms in a normal system with applica-
tions to the analysis of variance. Proceedings of the Cambridge Philosophical Society, 30,
178–191.
Davis, P. J. (1994). Circulant Matrices, 2nd ed. AMS Chelsea Publishing, Providence, RI.
Duff, I. S., Erisman, A. M., and Reid, J. K. (1986). Direct Methods for Sparse Matrices, Oxford
University Press, Oxford.
Dümbgen, L. (1995). A simple proof and refinement of Wielandt’s eigenvalue inequality.
Statistics & Probability Letters, 25, 113–115.
Eaton, M. L. and Tyler, D. E. (1991). On Wielandt’s inequality and its application to the asymp-
totic distribution of the eigenvalues of a random symmetric matrix. Annals of Statistics, 19,
260–271.
Elsner, L. (1982). On the variation of the spectra of matrices. Linear Algebra and Its Applica-
tions, 47, 127–138.
Eubank, R. L. and Webster, J. T. (1985). The singular-value decomposition as a tool for solving
estimability problems. American Statistician, 39, 64–66.
Fan, K. (1949). On a theorem of Weyl concerning eigenvalues of linear transformations, I.
Proceedings of the National Academy of Sciences of the USA, 35, 652–655.
Fan, K. (1950). On a theorem of Weyl concerning eigenvalues of linear transformations, II.
Proceedings of the National Academy of Sciences of the USA, 36, 31–35.
Fang, K. T., Kotz, S., and Ng, K. W. (1990). Symmetric Multivariate and Related Distributions.
Chapman and Hall, London.

REFERENCES
509
Ferguson, T. S. (1967). Mathematical Statistics: A Decision Theoretic Approach. Academic
Press, New York.
Gantmacher, F. R. (1959). The Theory of Matrices, Volumes I and II. Chelsea, New York.
Golub, G. H. and Van Loan, C. F. (2013). Matrix Computations, 4th ed. Johns Hopkins
University Press, Baltimore.
Graybill, F. A. (1983). Matrices With Applications in Statistics, 2nd ed. Wadsworth, Belmont,
CA.
Grenander, U. and Szego, G. (1984). Toeplitz Forms and Their Applications. Chelsea,
New York.
Greville, T. N. E. (1960). Some applications of the pseudoinverse of a matrix. SIAM Review,
2, 15–22.
Greville, T. N. E. (1966). Note on the generalized inverse of a matrix product. SIAM Review,
8, 518–521.
Gross, J. (2000). The Moore–Penrose inverse of a partitioned nonnegative definite matrix.
Linear Algebra and its Applications, 321, 113–121.
Gustafson, K. (1972). Antieigenvalue inequalities in operator theory. In Inequalities III
(O. Shisha, ed.), 115–119. Academic Press, New York.
Gustafson, K. (2006). The trigonometry of matrix statistics. International Statistical Review,
74, 187–202.
Hageman, L. A. and Young, D. M. (1981). Applied Iterative Methods. Academic Press,
New York.
Hardy, G. H., Littlewood, J. E., and Pólya, G. (1952). Inequalities, 2nd ed. Cambridge Univer-
sity Press, Cambridge.
Harville, D. A. (1997). Matrix Algebra From a Statistician’s Perspective. Springer, New York.
Healy, M. J. R. (1986). Matrices for Statistics. Clarendon Press, Oxford.
Hedayat, A. and Wallis, W. D. (1978). Hadamard matrices and their applications. Annals of
Statistics, 6, 1184–1238.
Heinig, G. and Rost, K. (1984). Algebraic Methods for Toeplitz-like Matrices and Operators.
Birkhäuser, Basel.
Henderson, H. V. and Searle, S. R. (1979). Vec and vech operators for matrices with some uses
in Jacobians and multivariate statistics. Canadian Journal of Statistics, 7, 65–81.
Hinch, E. J. (1991). Perturbation Methods. Cambridge University Press, Cambridge.
Horn, R. A. and Johnson, C. R. (1985). Matrix Analysis. Cambridge University Press,
Cambridge.
Horn, R. A. and Johnson, C. R. (1991). Topics in Matrix Analysis. Cambridge University Press,
Cambridge.
Horn, R. A. and Johnson, C. R. (2013). Matrix Analysis, 2nd ed. Cambridge University Press,
Cambridge.
Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components.
Journal of Educational Psychology, 24, 417–441, 498–520.
Hu, X. (2008). A three-condition characterization of the Moore–Penrose generalized inverse.
The American Statistician, 62, 216–218.
Huberty, C. J and Olejnik, S. (2006). Applied MANOVA and Discriminant Analysis, 2nd ed.
John Wiley, New York.

510
REFERENCES
Hyndman, R. J. and Wand, M. P. (1997). Nonparametric autocovariance function estimation.
Australian Journal of Statistics, 39, 313–324.
Im, E. I. (1997). Narrower eigenbounds for Hadamard products. Linear Algebra and Its Appli-
cations, 254, 141–144.
Jackson, J. E. (1991). A User’s Guide to Principal Components. John Wiley, New York.
Johnson, N. L., Kotz, S., and Balakrishnan, N. (1997). Discrete Multivariate Distributions.
John Wiley, New York.
Jolliffe, I. T. (2002). Principal Component Analysis, 2nd ed. Springer-Verlag, New York.
Kato, T. (1982). A Short Introduction to Perturbation Theory for Linear Operators.
Springer-Verlag, New York.
Kelly, P. J. and Weiss, M. L. (1979). Geometry and Convexity. John Wiley, New York.
Khattree, R. (2003). Antieigenvalues and antieigenvectors in statistics. Journal of Statistical
Planning and Inference, 114, 131–144.
Khuri, A. (2003). Advanced Calculus with Applications in Statistics, 2nd ed. John Wiley,
New York.
Krzanowski, W. J. (2000). Principles of Multivariate Analysis: A User’s Perspective, Revised
ed. Clarendon Press, Oxford.
Kutner, M., Nachtsheim, C., Neter, J, and Li, W. (2005). Applied Linear Statistical Models,
5th ed. McGraw-Hill, New York.
Lanczos, C. (1950). An iteration method for the solution of the eigenvalue problem of linear
differential and integral operators. Journal of Research of the National Bureau of Standards,
45, 255–282.
Lay, S. R. (1982). Convex Sets and Their Applications. John Wiley, New York.
Lidskiˇi, V. (1950). The proper values of the sum and product of symmetric matrices. Dokl.
Akad. Nauk. SSSR, 75, 769–772 (in Russian). (Translated by C. D. Benster, U. S. Depart-
ment of Commerce, National Bureau of Standards, Washington, D.C., N.B.S. Rep. 2248,
1953).
Lindgren, B. W. (1993). Statistical Theory, 4th ed. Chapman and Hall, New York.
Magnus, J. R. (1978). The moments of products of quadratic forms in normal variables.
Statistica Neerlandica, 32, 201–210.
Magnus, J. R. (1987). A representation theorem for (trAp)1/p. Linear Algebra and Its Appli-
cations, 95, 127–134.
Magnus, J. R. (1988). Linear Structures. Charles Griffin, London.
Magnus, J. R. and Neudecker, H. (1979). The commutation matrix: Some properties and appli-
cations. Annals of Statistics, 7, 381–394.
Magnus, J. R. and Neudecker, H. (1988). Matrix Differential Calculus with Applications in
Statistics and Econometrics. John Wiley, New York.
Magnus, J. R. and Neudecker, H. (1999). Matrix Differential Calculus with Applications in
Statistics and Econometrics, Revised ed. John Wiley, New York.
Mandel, J. (1982). Use of the singular value decomposition in regression analysis. American
Statistician, 36, 15–24.
Mardia, K. V., Kent, J. T., and Bibby, J. M. (1979). Multivariate Analysis. Academic Press,
New York.
Marshall, A. W., Olkin, I., and Arnold, B. C. (2011). Inequalities: Theory of Majorization and
Its Applications, 2nd ed. Springer, New York.

REFERENCES
511
Mathai, A. M. and Provost, S. B. (1992). Quadratic Forms in Random Variables. Marcel
Dekker, New York.
McCullagh, P. (1987). Tensor Methods in Statistics. Chapman and Hall, London.
McLachlan, G. J. (2005). Discriminant Analysis and Statistical Pattern Recognition. John
Wiley, New York.
Medhi, J. (2009). Stochastic Processes, 3rd ed. New Age Science, New Delhi.
Miller, R. G., Jr. (1981). Simultaneous Statistical Inference, 2nd ed. Springer-Verlag,
New York.
Minc, H. (1988). Nonnegative Matrices. John Wiley, New York.
Moore, E. H. (1920). On the reciprocal of the general algebraic matrix (Abstract). Bulletin of
the American Mathematical Society, 26, 394–395.
Moore, E. H. (1935). General analysis. Memoirs of the American Philosophical Society, 1,
147–209.
Morrison, D. F. (2005). Multivariate Statistical Methods, 4th ed. McGraw-Hill, New York.
Muirhead, R. J. (1982). Aspects of Multivariate Statistical Theory. John Wiley, New York.
Nayfeh, A. H. (1981). Introduction to Perturbation Techniques. John Wiley, New York.
Nel, D, G. (1980). On matrix differentiation in statistics. South African Statistical Journal, 14,
137–193.
Nelder, J. A. (1985). An alternative interpretation of the singular-value decomposition in
regression. American Statistician, 39, 63–64.
Neudecker, H. and Liu, S. (2001). Statistical properties of the Hadamard product of random
vectors. Statistical Papers, 42, 529–533.
Neudecker, H., Liu, S., and Polasek, W. (1995a). The Hadamard product and some of its appli-
cations in statistics. Statistics, 26, 365–373.
Neudecker, H., Polasek, W., and Liu, S. (1995b). The heteroskedastic linear regression model
and the Hadamard product: A note. Journal of Econometrics, 68, 361–366.
Olkin, I. and Tomsky, J. L. (1981). A new class of multivariate tests based on the
union-intersection principle. Annals of Statistics, 9, 792–802.
Ostrowski, A. M. (1973). Solutions of Equations in Euclidean and Banach Spaces. Academic
Press, New York.
Ouellette, D. V. (1981). Schur complements and statistics. Linear Algebra and its Applications,
36, 187–295.
Penrose, R. (1955). A generalized inverse for matrices. Proceedings of the Cambridge Philo-
sophical Society, 51, 406–413.
Penrose, R. (1956). On best approximate solutions of linear matrix equations. Proceedings of
the Cambridge Philosophical Society, 52, 17–19.
Perlman, M. D. (1990). T. W. Anderson’s Theorem on the integral of a symmetric unimodal
function over a symmetric convex set and its applications in probability and statistics. In The
Collected Papers of T. W. Anderson, 1943–1985 (George P. H. Styan, ed.), 2, 1627–1641.
John Wiley, New York.
Pinsky, M. A. and Karlin, S. (2011). An Introduction to Stochastic Modeling, 4th ed. Academic
Press, Burlington, MA.
Poincaré, H. (1890). Sur les équations aux dériées partielles de la physique mathématique.
American Journal of Mathematics, 12, 211–294.

512
REFERENCES
Poole, D. (2015). Linear Algebra: A Modern Introduction, 4th ed., Cengage Learning,
Stamford, CT.
Press, W. H., Teukolsky, S. A., Vetterline, W. T., and Flannery, B. P. (2007). Numerical Recipes:
The Art of Scientific Computing, 3rd ed. Cambridge University Press, Cambridge.
Pringle, R. M. and Rayner, A. A. (1971). Generalized Inverse Matrices with Applications to
Statistics. Charles Griffin, London.
Rao, C. R. (1973). Linear Statistical Inference and Its Applications, 2nd ed. John Wiley,
New York.
Rao, C. R. (2005). Antieigenvalues and antisingularvalues of a matrix and applications to prob-
lems in statistics. Research Letters in the Information and Mathematical Sciences, 8, 53–76.
Rao, C. R. and Mitra, S. K. (1971). Generalized Inverse of Matrices and Its Applications. John
Wiley, New York.
Rencher, A. C. and Schaalje, G. B. (2008). Linear Models in Statistics, 2nd ed. John Wiley,
New York.
Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press, Princeton.
Scheffé, H. (1953). A method for judging all contrasts in the analysis of variance. Biometrika,
40, 87–104.
Schott, J. R. (1984). Optimal bounds for the distribution of some test criteria for tests of dimen-
sionality. Biometrika, 71, 561–567.
Schott, J. R. (2003). Kronecker product permutation matrices and their application to moment
matrices of the normal distribution. Journal of Multivariate Analysis, 87, 177–190.
Searle, S. R. (1971). Linear Models. John Wiley, New York.
Searle, S. R. (1982). Matrix Algebra Useful for Statistics. John Wiley, New York.
Sen, A. K. and Srivastava, M. S. (1990). Regression Analysis: Theory, Methods, and Applica-
tions, Springer-Verlag, New York.
Seneta, E. (2006). Non-negative Matrices and Markov Chains, 2nd ed. Springer, New York.
Srivastava, M. S. and Khatri, C. G. (1979). An Introduction to Multivariate Statistics.
North-Holland, New York.
Stewart, G. W. (1998). Matrix Algorithms I: Basic Decompositions. SIAM, Philadelphia.
Stewart, G. W. (2001). Matrix Algorithms II: Eigensystems. SIAM, Philadelphia.
Styan, G. P. H. (1973). Hadamard products and multivariate statistical analysis. Linear Algebra
and Its Applications, 6, 217–240.
Sugiura, N. (1976). Asymptotic expansions of the distributions of the latent roots and latent
vector of the Wishart and multivariate F matrices. Journal of Multivariate Analysis, 6,
500–525.
Trenkler, G. (2000). On a generalization of the covariance matrix of the multinomial distribu-
tion. In Innovations in Multivariate Statistical Analysis, R.D.H. Heijmans, D.S.G. Pollock,
and A. Santorra, Eds., pp. 67–73. Kluwer, Boston.
Wielandt, H. (1955). An extremum property of sums of eigenvalues. Proceedings of the Amer-
ican Mathematical Society, 6, 106–110.
Xian, Y. Y. (2001). Theory and Applications of Higher-Dimensional Hadamard Matrices.
Kluwer, Boston.
Young, D. M. (1971). Iterative Solution of Large Linear Systems. Academic Press, New York.
Zhang, F. (2005). The Schur Complement and Its Applications. Springer, New York.
Zhang, F. (2011). Matrix Theory: Basic Results and Techniques, 2nd ed. Springer, New York.

INDEX
accumulation point, 81
adjoint, 9
analysis of variance, 138, 320
multivariate, 493
angle between vectors, 41, 142
antieigenvalue, 141–144
antieigenvector, 141–144
arithmetic-geometric mean inequality, 452–453
backward shift, 367
Bartlett adjustment, 493
basis, 49–53
orthonormal, 53–58
bilinear form, 16
block diagonal matrix, 13
boundary point, 82
canonical variate analysis, 119, 178–179, 493
Cauchy–Schwarz inequality, 38, 444–446
Cayley–Hamilton theorem, 105–106, 196
chain rule, 388, 391
characteristic equation, 96
characteristic root, 96
characteristic vector, 96
chi-squared distribution, 21–22
and Moore–Penrose inverse, 211
Matrix Analysis for Statistics, Third Edition. James R. Schott.
© 2017 John Wiley & Sons, Inc. Published 2017 by John Wiley & Sons, Inc.
Companion Website: www.wiley.com/go/Schott/MatrixAnalysis3e
and quadratic forms, 465–471
central, 21–22
noncentral, 22
Cholesky decomposition, 164
circulant matrix, 363–367
closed set, 81
closure, 81
Cochran’s theorem, 462–465
cofactor, 6
and determinant, 6
and inverse, 9
column space, 45
commutation matrix, 339–346
and Kronecker product, 342
and vec of a Kronecker product, 343
and vec operator, 341
eigenvalues, 344
eigenvectors, 379
completely positive matrix, 383
complex matrix, 18–19
complex number, 18
conjugate, 18
Euler’s formula, 18
modulus, 18
polar coordinates, 18
triangle inequality, 19

514
INDEX
condition number, 191
conjugate transpose, 19
consistent equations, 247–251
consistent estimator, 223–224
contaminated normal distribution, 27, 34
continuity
of a determinant, 222
of a Moore–Penrose inverse, 222–224
of an eigenprojection, 115
of an eigenvalue, 115
of an inverse matrix, 222
convergence with respect to a norm, 188
convex combination, 81
convex function, 413–416
absolute minimum of, 415
strictly, 413
convex hull, 81
convex set, 80–85
correlation, 24
maximum squared, 432
correlation matrix, 24
eigenvalues and eigenvectors, 113–114, 331
nonnegative definite, 25
sample, 25
Courant–Fischer min–max theorem, 120
covariance, 23
of quadratic forms, 477, 480–481
covariance matrix, 23
equal variances and equal correlations, 113
nonnegative definite, 24
sample, 25
decomposition
Cholesky, 164
Jordan, 173–174
LU, 198
polar, 192
QR, 60, 165
Schur, 175–178
singular value, 155–162
spectral, 108, 111, 162–168
density function, 20
multivariate, 22
derivative, 387–389
of a determinant, 396, 401
of a matrix function, 391
of a Moore–Penrose inverse, 398–400
of a patterned matrix, 400–401
of a trace, 396
of a vector function, 390
of an eigenvalue, 407
of an eigenvector, 407
of an inverse, 398, 401
partial, 389
second-order partial, 390
determinant, 5–9
and eigenvalues, 101
and trace, 452
continuity of, 222
derivative of, 396, 401
expansion by cofactors, 6–8
of a partitioned matrix, 288–296
of a product, 9, 445–446
of a sum, 102, 447, 450
determinantal inequality, 181, 291, 336, 445–447,
450–452, 455–456
diagonal matrix, 2
diagonalization, 103, 169, 171–173
simultaneous, 136, 178–184
differential, 388
of a determinant, 396
of a matrix function, 391
of a Moore–Penrose inverse, 398
of a trace, 396
of a vector function, 390
of an eigenvalue, 407
of an eigenvector, 407
of an inverse, 398
second, 389
dimension of a vector space, 49–53
direct sum of matrices, 323
discriminant analysis, 40
distance function, 39
Euclidean, 40, 55, 68, 166
Mahalanobis, 40, 68, 166
distance in the metric of, 40
doubly stochastic matrix, 434
duplication matrix, 346–349
eigenprojection, 110–112
continuity of, 115
eigenspace, 98, 172
dimension of, 100
eigenvalue, 95
and antieigenvalue, 142
and determinant, 101
and leading principal submatrix, 124
and majorization, 437–442
and rank, 104, 112, 171–173, 178
and trace, 101
asymptotic distribution of, 491
continuity of, 115
derivative of, 407
distinct, 98
extremal properties, 116–123
in the metric of, 136
monotonicity, 133
multiple, 98

INDEX
515
of a partitioned matrix, 302–307
of a positive definite matrix, 129
of a positive semidefinite matrix, 129
of a power, 100
of a product, 140–141
of a sum, 124–129, 133, 439–440
of a symmetric matrix, 106–114
of a transpose product, 131
of a triangular matrix, 99
of an idempotent matrix, 457
of an inverse matrix, 100
of an orthogonal matrix, 99
of the Schur complement, 306
perturbation of, 403–406, 427
relation to diagonal elements, 437
simple, 98
eigenvector, 95
and antieigenvector, 142
asymptotic distribution of, 491
common, 151, 184
derivative of, 407
left, 148
linear independence of, 103
of a symmetric matrix, 107–108
right, 148
elementary transformations, 14
elimination matrix, 349–351
elliptical distribution, 27
estimable function, 268
Euclidean distance function, 40, 55, 68,
166
Euclidean inner product, 39–40
Euclidean norm, 40, 42, 186
Euclidean space, 40
expected value, 20
of a quadratic form, 477–485
F distribution, 22
forward shift, 367
Fourier matrix, 366
Gauss–Seidel method, 274
generalized inverse, 225–230
and projection matrices, 231
computation of, 232–238
of a partitioned matrix, 298–299
reflexive, 243
generalized quadratic form, 485
gradient, 275
Gram–Schmidt orthonormalization, 53, 59–61
growth curve model, 72–73, 326
H¨older’s inequality, 446–449
Hadamard inequality, 333–335, 385
Hadamard matrix, 369–371
normalized, 370
Hadamard product, 329–339
as a Kronecker product, 330
eigenvalues of, 336–338
nonnegative definite, 332
positive definite, 332–333
rank of, 330
Helmert matrix, 16, 33
Hermite form, 234
Hermitian matrix, 19
Hessian, 390
homogeneous system of equations, 258–260
hyperplane, 81
idempotent matrix, 3, 64, 457–462
eigenvalues, 457
product of, 460
rank of, 457
sum of, 460
symmetric, 459, 461–462
trace of, 457
identity matrix, 2
indefinite matrix, 17
independence (linear), 42–45
independence (stochastic)
of quadratic forms, 471–477
of random variables, 23
inequality
arithmetic-geometric mean, 452–453
Cauchy–Schwarz, 38, 444–446
determinantal, 181, 291, 336, 445–447,
450–452, 455–456
H¨older’s, 446–449
Hadamard, 333–335, 385
Jensen’s, 415–417
Kantorovich, 455
Minkowski’s, 450, 451
trace, 376, 440, 445, 447, 449, 451–452,
455–456
triangle, 19, 40, 86
inner product, 38–39
Euclidean, 39–40
interior point, 82
intersection of vector spaces, 73
inverse matrix, 9–12
and cofactors, 9
continuity of, 222
derivative of, 398, 401
eigenvalues, 100
of a partitioned matrix, 285–288
of a product, 9
of a sum, 10–11
irreducible matrix, 357

516
INDEX
Jacobi method, 274
Jacobian matrix, 390–391
Jensen’s inequality, 415–417
Jordan block matrix, 173
Jordan decomposition, 173–174
Kantorovich inequality, 455
Kronecker product, 315–322
determinant of, 319
eigenvalues of, 318
eigenvectors of, 379
generalized inverse of, 318
inverse of, 318
Moore–Penrose inverse of, 318
rank of, 319
trace of, 317
Lagrange function, 417
Lagrange multipliers, 417
Lanczos
algorithm, 274–278
vectors, 276
latent roots, 96
latent vectors, 96
law of cosines, 41
least squares, 28
and best linear unbiased estimator, 130–131
and multicollinearity, 109–110, 161–162
and solutions to a system of equations, 260–266
generalized, 79, 166, 283
growth curve model, 72–73, 326
in less than full rank models, 64, 266–271
in multiple regression, 61–64, 410
in multivariate multiple regression, 132, 326
in one-way classification model, 90, 269–271,
320–321
in ridge regression, 145
in simple linear regression, 56–57
in two-way classification model, 282, 321–322
ordinary, 28–29
restricted, 91, 283
weighted, 70–71
with standardized explanatory variables, 69–70,
109
least squares inverse, 231
computation of, 238
least squares solution, 260
limit point, 81
linear combination, 36
linear dependence, 42–45
linear equations, 71–72
and singular value decomposition, 271–273
common solution, 281
consistency of, 247–251
homogeneous system of, 258–260
least squares solution of, 260–266
linearly independent solutions to, 255–257
minimal solution, 280
restricted solution, 280
solutions to, 251–258
sparse systems of, 273–278
direct methods, 273
iterative methods, 273
unique solution to, 254
linear independence, 42–45
linear model, 28
linear space, 36
linear transformation, 65–73
of matrices, 72
LU factorization, 198
Mahalanobis distance, 40, 68, 166
majorization, 433–443
and diagonal elements, 437
and doubly stochastic matrix, 434–436
and eigenvalues, 437–442
and order-preserving functions, 442
definition, 433–434
Markov chain, 361–363
matrix
addition, 2
backward shift, 367
block diagonal, 13
circulant, 363–367
commutation, 339–346
commuting, 182, 184
completely positive, 383
complex, 18–19
correlation, 24, 113–114
covariance, 23
definition, 1
diagonal, 2
diagonalizable, 103
doubly stochastic, 434
duplication, 346–349
eigenprojection, 110–112
elimination, 349–351
forward shift, 367
Fourier, 366
Hadamard, 369–371
Helmert, 16, 33
Hermitian, 19
Hessian, 390
idempotent, 3, 64, 457–461
identity, 2
indefinite, 17
inverse, 9–12
irreducible, 357

INDEX
517
Jacobian, 390–391
Jordan block, 173
multiplication by a matrix, 3
multiplication by a scalar, 3
negative definite, 17
negative semidefinite, 17
nilpotent, 149, 195
nonnegative, 351–363
nonnegative definite, 17
nonsingular, 9
null, 2
order of, 2
orthogonal, 15–16
partitioned, 12–14
permutation, 16
positive, 351–357
positive definite, 17
positive semidefinite, 17
primitive, 361
projection, 58–65, 77–78
rectangular, 2
reducible, 357
semiorthogonal, 16
similar, 169
singular, 9
skew-symmetric, 4
square, 2
square root, 17, 163
symmetric, 4
Toeplitz, 367–369
transpose, 3
triangular, 2
unitary, 19, 175
Vandermonde, 371–372
matrix function, 391
matrix norm, 184–191
Euclidean, 186
induced, 185
maximum column sum, 186
maximum row sum, 186
spectral, 186
maximum
absolute, 409
conditions for local maximum, 409
local, 409
of a convex function, 414
with equality constraints, 417–423
maximum likelihood estimation, 411–413, 429
mean, 20
sample, 25
mean squared error, 192
mean vector, 23
differences in, 118, 134, 178
sample, 25
Minkowski’s inequality, 450–451
minor, 6, 14
leading principal, 292, 310
moment generating function, 21
moments, 20
Moore–Penrose inverse, 202
and projection matrices, 204, 209
and quadratic forms in normal random vectors,
210
and the singular value decomposition, 203
and the spectral decomposition, 208
computation of, 206, 232–234
continuity of, 222–224
derivative of, 398–400
existence of, 202–203
of a block diagonal matrix, 219
of a diagonal matrix, 208
of a matrix product, 211–215
of a partitioned matrix, 215–219, 299–302
of a sum, 219–221
of a symmetric matrix, 207–208
properties, 205–211
rank, 206
uniqueness of, 202–203
multicollinearity, 109–110, 145, 161–162, 192
multinomial distribution, 241, 431
multiplicity of an eigenvalue, 98
algebraic, 98
geometric, 98
multivariate normal distribution, 26, 396, 411, 429
conditional distribution, 291–292, 295–296
density function, 26, 395
fourth-order moment matrix, 478, 481
maximum likelihood estimates, 411–413
moments, 478–480
singular, 26
sixth-order moment matrix, 500
standard, 26
multivariate t distribution, 27, 34
negative definite matrix, 17
negative semidefinite matrix, 17
nilpotent matrix, 150, 195
nonnegative definite matrix, 17
correlation matrix, 25
covariance matrix, 24
nonnegative matrix, 351–363
eigenvalues of, 359–361
eigenvectors of, 359–360
irreducible, 357
primitive, 361
reducible, 357
spectral radius of, 352
nonsingular matrix, 9

518
INDEX
norm
matrix, 184–191
vector, 39, 42
normal distribution, 21
standard, 21
normalized vector, 15
null matrix, 2
null space, 66–67
null vector, 2
oblique projection, 76–80
one-way classification model
multivariate, 137–139, 178, 493
univariate, 90, 267, 269–271, 320–321, 472,
482
order
of a minor, 14
of a square matrix, 2
order-preserving function, 442
orthogonal complement, 57–58
and null space, 66
dimension of, 58
orthogonal matrix, 15–16
orthogonal vectors, 15
orthonormal basis, 53–58
orthonormal vectors, 15
parallelogram identity, 86
partitioned matrix, 12–13
determinant of, 288–296
eigenvalues of, 302–307
generalized inverse of, 298–299
inverse of, 285–288
Moore–Penrose inverse of, 215–219,
299–302
product of, 12
rank of, 48, 296–298
Pearson’s chi-squared statistic, 499
permutation matrix, 16
perturbation method, 402
eigenprojection, 407–408
eigenvalue, 403–406, 427
matrix inverse, 402–403
Moore–Penrose inverse, 426
sample correlation matrix, 426
symmetric square root, 426
Poincar´e separation theorem, 123
polar decomposition, 192
positive definite matrix, 17
eigenvalues, 129
leading principal minors, 292
positive matrix, 351–357
eigenvalues, 353–357
eigenvectors, 353–356
spectral radius, 352
positive semidefinite matrix, 17
eigenvalues, 129
primitive matrix, 361
principal components analysis, 119–120, 491
principal submatrix, 124, 292, 310
leading, 124
probability function, 20
multivariate, 22
projection, 53–58
oblique, 76–80
relative to the A inner product, 79
projection matrix, 58–65, 209, 231
oblique, 77–78
QR factorization, 60, 165
quadratic form, 16–17
and Moore–Penrose inverse, 210
covariance of, 477, 480–481
distribution of, 465–471
expected value of, 477–485
generalized, 485
independence of, 471–477
matrix of, 17
moment generating function of, 498
variance of, 477, 480–481
random variable, 20
correlation, 24
covariance, 23
density function, 20
expected value, 20
independent, 23
mean, 20
moment generating function, 21
moments, 20
probability function, 20
variance, 20
random vector, 22
correlation matrix of, 24
covariance matrix of, 23
density function, 22
expected value, 23
mean, 23
probability function, 22
range, 45
rank, 14–15
and dimension of null space, 66
and eigenvalues, 104, 112, 171–173, 178
and linear independence, 45–49
full, 14
full column, 14
full row, 14
of a product, 14, 46, 48–49

INDEX
519
of a sum, 46
of partitioned matrix, 48, 296–298
Rayleigh quotient, 117, 120, 275
reducible matrix, 357
regression, 28–29
best linear unbiased estimator, 130–131
best quadratic unbiased estimator, 422–423
complete and reduced models, 62, 287, 474
F test, 474–475
generalized least squares, 79, 166–167, 283
minimum variance unbiased linear estimator,
432
multicollinearity, 109, 161, 192
multiple, 61–64
multivariate multiple, 132, 326
polynomial, 371
principal components, 109–110, 161–162
ridge, 145
simple linear, 56–57
weighted least squares, 70–71
with standardized explanatory variables, 69–70,
109–110
row space, 45
saddle point, 409
sample correlation matrix, 25
asymptotic covariance matrix of, 495–496
sample covariance matrix, 25
distribution of, 490
independent of the sample mean vector, 490
sample mean, 25
sample mean vector, 25
sample variance, 25
distribution of, 470–471
independent of the sample mean, 475
Schur complement, 287, 292, 294–295, 306
of a Wishart matrix, 487
Schur decomposition, 175–178
semiorthogonal matrix, 16
separating hyperplane theorem, 84
similar matrices, 169
simultaneous confidence intervals, 139
simultaneous diagonalization, 136, 178–184
singular matrix, 9
singular value decomposition, 155–162
and systems of equations, 271–273
of a vector, 159
singular values, 157
and eigenvalues, 160
skew-symmetric matrix, 4
spanning set, 37
spectral decomposition, 108, 111, 162–168
of a diagonalizable matrix, 170
spectral radius, 187
of a nonnegative matrix, 352
spectral set, 111
spherical distribution, 27
square root of a matrix, 17, 163
stationary point, 409
submatrix, 12–14
subspace, 36
sum of squares
for error, 29
for treatment, 91
sum of vector spaces, 74
supporting hyperplane theorem, 83
symmetric matrix, 4
T-transform, 435, 437
Taylor formula
first-order, 388–389
for a vector function, 390
kth-order, 388–389
time series, 368
Toeplitz matrix, 367–369
trace, 4
and determinant, 452
and eigenvalues, 101
derivative of, 396
of a product, 4, 440, 445, 448, 449
of a sum, 451
trace inequality, 376, 440, 445, 447, 449, 451,
452, 455, 456
transition probabilities, 361
transpose, 3–4
conjugate, 19
transpose product, 13, 167
eigenvalues, 131
triangle inequality, 19, 40, 86
triangular matrix
definition, 2
lower, 2
upper, 2
two-way classification model, 282, 321–322,
374
uniform distribution, 27
fourth-order moment matrix, 501
union-intersection procedure, 138–139, 431
unit vector, 15
unitary matrix, 19, 175
univariate normal distribution, 21
standard, 21
Vandermonde matrix, 371–372
variance, 20
of a quadratic form, 477, 480–481
sample, 25
vec operator, 324–328

520
INDEX
vector, 2
column, 2
definition, 2
normalized, 15
null, 2
orthogonal, 15
orthonormal, 15
row, 2
unit, 15
vector norm, 39
Euclidean, 40, 42
infinity norm, 42
max norm, 42
sum norm, 42
vector space, 36
basis of, 49–52
definition, 35
dimension of, 49
direct sum, 75
Euclidean, 40
intersection, 73
projection matrix of, 59–65, 77–78
spanning set, 37
sum, 74
vector subspace, 36
Weyl’s Theorem, 124
Wishart distribution, 485–490
and sample covariance matrix, 490–491
covariance matrix of, 488
mean of, 488

WILEY SERIES IN PROBABILITY AND STATISTICS
established by Walter A. Shewhart and Samuel S. Wilks
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice,
Geof H. Givens, Harvey Goldstein, Geert Molenberghs, David W. Scott,
Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg
Editors Emeriti: J. Stuart Hunter, Iain M. Johnstone, Joseph B. Kadane,
Jozef L. Teugels
The Wiley Series in Probability and Statistics is well established and authoritative. It covers many topics
of current research interest in both pure and applied statistics and probability theory. Written by leading
statisticians and institutions, the titles span both state-of-the-art developments in the field and classical
methods.
Reflecting the wide range of current research in statistics, the series encompasses applied, methodological
and theoretical statistics, ranging from applications and new techniques made possible by advances in
computerized practice to rigorous treatment of theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in aca­demia, industry,
government, or research.
† ABRAHAM and LEDOLTER · Statistical Methods for Forecasting
AGRESTI · Analysis of Ordinal Categorical Data, Second Edition
AGRESTI · An Introduction to Categorical Data Analysis, Second Edition
AGRESTI · Categorical Data Analysis, Third Edition
AGRESTI · Foundations of Linear and Generalized Linear Models
ALSTON, MENGERSEN and PETTITT (editors) · Case Studies in Bayesian Statistical Modelling and
Analysis
ALTMAN, GILL, and McDONALD · Numerical Issues in Statistical Computing for the Social Scientist
AMARATUNGA and CABRERA · Exploration and Analysis of DNA Microarray and Protein Array Data
AMARATUNGA, CABRERA, and SHKEDY · Exploration and Analysis of DNA Microarray and Other
High-Dimensional Data, Second Edition
ANDˇEL · Mathematics of Chance
ANDERSON · An Introduction to Multivariate Statistical Analysis, Third Edition
* ANDERSON · The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG · Statistical Methods for
Comparative Studies
ANDERSON and LOYNES · The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) · Advances in Biometry
ARNOLD, BALAKRISHNAN, and NAGARAJA · Records
* ARTHANARI and DODGE · Mathematical Programming in Statistics
AUGUSTIN, COOLEN, DE COOMAN and TROFFAES (editors) · Introduction to Imprecise Probabilities
* BAILEY · The Elements of Stochastic Processes with Applications to the Natural Sciences

BAJORSKI · Statistics for Imaging, Optics, and Photonics
BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications
BALAKRISHNAN and NG · Precedence-Type Tests and Applications
BALI, ENGLE, and MURRAY · Empirical Asset Pricing: The Cross-Section of Stock Returns
BARNETT · Comparative Statistical Inference, Third Edition
BARNETT · Environmental Statistics
BARNETT and LEWIS · Outliers in Statistical Data, Third Edition
Bartholomew, Knott, and Moustaki · Latent Variable Models and Factor Analysis: A Unified Approach,
Third Edition
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical Inference, Second
Edition
BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and Applications
BATES and WATTS · Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER, and GOLDSMAN · Design and Analysis of Experiments for Statistical Selec-
tion, Screening, and Multiple Comparisons
BEH and LOMBARDO · Correspondence Analysis: Theory, Practice and New Strategies
BEIRLANT, GOEGEBEUR, SEGERS, TEUGELS, and DE WAAL · Statistics of Extremes: Theory and
Applications
BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in Regression
† BELSLEY, KUH, and WELSCH · Regression Diagnostics: Identifying Influential Data and Sources of
Collinearity
BENDAT and PIERSOL · Random Data: Analysis and Measurement Procedures, Fourth Edition
BERNARDO and SMITH · Bayesian Theory
BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications
BIEMER, GROVES, LYBERG, MATHIOWETZ, and SUDMAN · Measurement Errors in Surveys
BILLINGSLEY · Convergence of Probability Measures, Second Edition
BILLINGSLEY · Probability and Measure, Anniversary Edition
BIRKES and DODGE · Alternative Methods of Regression
Bisgaard and Kulahci · Time Series Analysis and Forecasting by Example
Biswas, Datta, Fine, and Segal · Statistical Advances in the Biomedical Sciences: Clinical Trials, Epidemi-
ology, Survival Analysis, and Bioinformatics
BLISCHKE and MURTHY (editors) · Case Studies in Reliability and Maintenance
BLISCHKE and MURTHY · Reliability: Modeling, Prediction, and Optimization
BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN · Structural Equations with Latent Variables
BOLLEN and CURRAN · Latent Curve Models: A Structural Equation Perspective
BONNINI, CORAIN, MAROZZI and SALMASO · Nonparametric Hypothesis Testing: Rank and Permu-
tation Methods with Applications in R
BOROVKOV · Ergodicity and Stability of Stochastic Processes
BOSQ and BLANKE · Inference and Prediction in Large Dimensions
BOULEAU · Numerical Methods for Stochastic Processes
* BOX and TIAO · Bayesian Inference in Statistical Analysis

BOX · Improving Almost Anything, Revised Edition
* BOX and DRAPER · Evolutionary Operation: A Statistical Method for Process Improvement
BOX and DRAPER · Response Surfaces, Mixtures, and Ridge Analyses, Second Edition
BOX, HUNTER, and HUNTER · Statistics for Experimenters: Design, Innovation, and Discovery, Second
Editon
BOX, JENKINS, REINSEL, and LJUNG · Time Series Analysis: Forecasting and Control, Fifth Edition
BOX, LUCE ˜NO, and Paniagua-Qui ˜Nones · Statistical Control by Monitoring and Adjustment, Second
Edition
* BROWN and HOLLANDER · Statistics: A Biomedical Introduction
CAIROLI and DALANG · Sequential Stochastic Optimization
CASTILLO, HADI, BALAKRISHNAN, and SARABIA · Extreme Value and Related Models with Appli-
cations in Engineering and Science
CHAN · Time Series: Applications to Finance with R and S-Plus R⃝, Second Edition
CHARALAMBIDES · Combinatorial Methods in Discrete Distributions
CHATTERJEE and HADI · Regression Analysis by Example, Fourth Edition
CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression
Chen · The Fitness of Information: Quantitative Assessments of Critical Evidence
CHERNICK · Bootstrap Methods: A Guide for Practitioners and Researchers, Second Edition
CHERNICK and FRIIS · Introductory Biostatistics for the Health Sciences
CHIL`ES and DELFINER · Geostatistics: Modeling Spatial Uncertainty, Second Edition
CHIU, STOYAN, KENDALL and MECKE · Stochastic Geometry and Its Applications, Third Edition
CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and Methodologies, Third Edition
CLARKE · Linear Models: The Theory and Application of Analysis of Variance
CLARKE and DISNEY · Probability and Random Processes: A First Course with Applications, Second
Edition
* COCHRAN and COX · Experimental Designs, Second Edition
COLLINS and LANZA · Latent Class and Latent Transition Analysis: With Applications in the Social,
Behavioral, and Health Sciences
CONGDON · Applied Bayesian Modelling, Second Edition
CONGDON · Bayesian Models for Categorical Data
CONGDON · Bayesian Statistical Modelling, Second Edition
CONOVER · Practical Nonparametric Statistics, Third Edition
COOK · Regression Graphics
COOK and WEISBERG · An Introduction to Regression Graphics
COOK and WEISBERG · Applied Regression Including Computing and Graphics
CORNELL · A Primer on Experiments with Mixtures
CORNELL · Experiments with Mixtures, Designs, Models, and the Analysis of Mixture Data, Third
Edition
COX · A Handbook of Introductory Statistical Methods
CRESSIE · Statistics for Spatial Data, Revised Edition
CRESSIE and WIKLE · Statistics for Spatio-Temporal Data
CS ¨ORG ¨O and HORV ´ATH · Limit Theorems in Change Point Analysis
Dagpunar · Simulation and Monte Carlo: With Applications in Finance and MCMC

DANIEL · Applications of Statistics to Industrial Experimentation
DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences, Eighth Edition
* DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data, Second Edition
DASU and JOHNSON · Exploratory Data Mining and Data Cleaning
DAVID and NAGARAJA · Order Statistics, Third Edition
DAVINO, FURNO and VISTOCCO · Quantile Regression: Theory and Applications
* DEGROOT, FIENBERG, and KADANE · Statistics and the Law
DEL CASTILLO · Statistical Process Adjustment for Quality Control
DeMaris · Regression with Social Data: Modeling Continuous and Limited Response Variables
DEMIDENKO · Mixed Models: Theory and Applications with R, Second Edition
Denison, Holmes, Mallick, and Smith · Bayesian Methods for Nonlinear Classification and Regression
DETTE and STUDDEN · The Theory of Canonical Moments with Applications in Statistics, Probability,
and Analysis
DEY and MUKERJEE · Fractional Factorial Plans
DILLON and GOLDSTEIN · Multivariate Analysis: Methods and Applications
* DODGE and ROMIG · Sampling Inspection Tables, Second Edition
* DOOB · Stochastic Processes
DOWDY, WEARDEN, and CHILKO · Statistics for Research, Third Edition
DRAPER and SMITH · Applied Regression Analysis, Third Edition
DRYDEN and MARDIA · Statistical Shape Analysis
DUDEWICZ and MISHRA · Modern Mathematical Statistics
DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences, Fourth Edition
DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large Deviations
EDLER and KITSOS · Recent Advances in Quantitative Methods in Cancer and Human Health Risk
Assessment
* ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis
ENDERS · Applied Econometric Time Series, Third Edition
† ETHIER and KURTZ · Markov Processes: Characterization and Convergence
EVANS, HASTINGS, and PEACOCK · Statistical Distributions, Third Edition
EVERITT, LANDAU, LEESE, and STAHL · Cluster Analysis, Fifth Edition
FEDERER and KING · Variations on Split Plot and Split Block Experiment Designs
FELLER · An Introduction to Probability Theory and Its Applications, Volume I, Third Edition, Revised;
Volume II, Second Edition
FITZMAURICE, LAIRD, and WARE · Applied Longitudinal Analysis, Second Edition
* FLEISS · The Design and Analysis of Clinical Experiments
FLEISS · Statistical Methods for Rates and Proportions, Third Edition
† FLEMING and HARRINGTON · Counting Processes and Survival Analysis
FUJIKOSHI, ULYANOV, and SHIMIZU · Multivariate Statistics: High-Dimensional and Large-Sample
Approximations
FULLER · Introduction to Statistical Time Series, Second Edition
† FULLER · Measurement Error Models
GALLANT · Nonlinear Statistical Models
GEISSER · Modes of Parametric Statistical Inference

GELMAN and MENG · Applied Bayesian Modeling and Causal Inference from ncomplete-Data
Perspectives
GEWEKE · Contemporary Bayesian Econometrics and Statistics
GHOSH, MUKHOPADHYAY, and SEN · Sequential Estimation
GIESBRECHT and GUMPERTZ · Planning, Construction, and Statistical Analysis of Comparative
Experiments
GIFI · Nonlinear Multivariate Analysis
GIVENS and HOETING · Computational Statistics
GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems
GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate Observations, Second Edition
GOLDSTEIN · Multilevel Statistical Models, Fourth Edition
GOLDSTEIN and LEWIS · Assessment: Problems, Development, and Statistical Issues
Goldstein and Wooff · Bayes Linear Statistics
GRAHAM · Markov Chains: Analytic and Monte Carlo Computations
GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing
GROSS, SHORTLE, THOMPSON, and HARRIS · Fundamentals of Queueing Theory, Fourth Edition
GROSS, SHORTLE, THOMPSON, and HARRIS · Solutions Manual to Accompany Fundamentals of
Queueing Theory, Fourth Edition
* HAHN and SHAPIRO · Statistical Models in Engineering
HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners
HALD · A History of Probability and Statistics and their Applications Before 1750
† HAMPEL · Robust Statistics: The Approach Based on Influence Functions
Hartung, Knapp, and Sinha · Statistical Meta-Analysis with Applications
HEIBERGER · Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA · Design and Inference in Finite Population Sampling
HEDEKER and GIBBONS · Longitudinal Data Analysis
HELLER · MACSYMA for Statisticians
HERITIER, CANTONI, COPT, and VICTORIA-FESER · Robust Methods in Biostatistics
HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments, Volume 1: Introduction to
Experimental Design, Second Edition
HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments, Volume 2: Advanced Exper-
imental Design
HINKELMANN (editor) · Design and Analysis of Experiments, Volume 3: Special Designs and Applica-
tions
HOAGLIN, MOSTELLER, and TUKEY · Fundamentals of Exploratory Analysis of Variance
* HOAGLIN, MOSTELLER, and TUKEY · Exploring Data Tables, Trends and Shapes
* HOAGLIN, MOSTELLER, and TUKEY · Understanding Robust and Exploratory Data Analysis
HOCHBERG and TAMHANE · Multiple Comparison Procedures
HOCKING · Methods and Applications of Linear Models: Regression and the Analysis of Variance, Third
Edition
HOEL · Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN · Loss Distributions
HOLLANDER, WOLFE, and CHICKEN · Nonparametric Statistical Methods, Third Edition

HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition
HOSMER, LEMESHOW, and MAY · Applied Survival Analysis: Regression Modeling of Time-to-Event
Data, Second Edition
HUBER · Data Analysis: What Can Be Learned From the Past 50 Years
HUBER · Robust Statistics
† HUBER and Ronchetti · Robust Statistics, Second Edition
HUBERTY · Applied Discriminant Analysis, Second Edition
HUBERTY and OLEJNIK · Applied MANOVA and Discriminant Analysis, Second Edition
HUITEMA · The Analysis of Covariance and Alternatives: Statistical Methods for Experiments,
Quasi-Experiments, and Single-Case Studies, Second Edition
HUNT and KENNEDY · Financial Derivatives in Theory and Practice, Revised Edition
HURD and MIAMEE · Periodically Correlated Random Sequences: Spectral Theory and Practice
HUSKOVA, BERAN, and DUPAC · Collected Works of Jaroslav Hajek—with Commentary
HUZURBAZAR · Flowgraph Models for Multistate Time-to-Event Data
Jackman · Bayesian Analysis for the Social Sciences
† JACKSON · A User’s Guide to Principle Components
JOHN · Statistical Methods in Engineering and Quality Assurance
JOHNSON · Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of Statistics: A Volume in
Honor of Samuel Kotz
JOHNSON, KEMP, and KOTZ · Univariate Discrete Distributions, Third Edition
JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences: From the Seventeenth Cen-
tury to the Present
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions, Volume 1, Second
Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions, Volume 2, Second
Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Discrete Multivariate Distributions
JUDGE, GRIFFITHS, HILL, L ¨UTKEPOHL, and LEE · The Theory and Practice of Econometrics, Second
Edition
JUREK and MASON · Operator-Limit Distributions in Probability Theory
KADANE · Bayesian Methods and Ethics in a Clinical Trial Design
KADANE AND SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti Evidence
KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time Data, Second Edition
KARIYA and KURATA · Generalized Least Squares
KASS and VOS · Geometrical Foundations of Asymptotic Inference
† KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to Cluster Analysis
KEDEM and FOKIANOS · Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE, and LE · Shape and Shape Theory
KHURI · Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW, and SINHA · Statistical Tests for Mixed Linear Models
* KISH · Statistical Design for Research
KLEIBER and KOTZ · Statistical Size Distributions in Economics and Actuarial Sciences

Klemel¨a · Smoothing of Multivariate Data: Density Estimation and Visualization
KLUGMAN, PANJER, and WILLMOT · Loss Models: From Data to Decisions, Third Edition
KLUGMAN, PANJER, and WILLMOT · Loss Models: Further Topics
KLUGMAN, PANJER, and WILLMOT · Solutions Manual to Accompany Loss Models: From Data to
Decisions, Third Edition
KOSKI and NOBLE · Bayesian Networks: An Introduction
KOTZ, BALAKRISHNAN, and JOHNSON · Continuous Multivariate Distributions, Volume 1, Second
Edition
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes 1 to 9 with Index
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Supplement Volume
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume 1
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume 2
KOWALSKI and TU · Modern Applied U-Statistics
Krishnamoorthy and Mathew · Statistical Tolerance Regions: Theory, Applications, and Computation
Kroese, Taimre, and Botev · Handbook of Monte Carlo Methods
KROONENBERG · Applied Multiway Data Analysis
KULINSKAYA, MORGENTHALER, and STAUDTE · Meta Analysis: A Guide to Calibrating and
Combining Statistical Evidence
Kulkarni and Harman · An Elementary Introduction to Statistical Learning Theory
KUROWICKA and COOKE · Uncertainty Analysis with High Dimensional Dependence Modelling
KVAM and VIDAKOVIC · Nonparametric Statistics with Applications to Science and Engineering
LACHIN · Biostatistical Methods: The Assessment of Relative Risks, Second Edition
LAD · Operational Subjective Statistical Methods: A Mathematical, Philosophical, and Historical Intro-
duction
LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition
LAWLESS · Statistical Models and Methods for Lifetime Data, Second Edition
LAWSON · Statistical Methods in Spatial Epidemiology, Second Edition
LE · Applied Categorical Data Analysis, Second Edition
LE · Applied Survival Analysis
Lee · Structural Equation Modeling: A Bayesian Approach
LEE and WANG · Statistical Methods for Survival Data Analysis, Fourth Edition
LePAGE and BILLARD · Exploring the Limits of Bootstrap
LESSLER and KALSBEEK · Nonsampling Errors in Surveys
LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health Statistics
LIAO · Statistical Group Comparison
LIN · Introductory Stochastic Analysis for Finance and Insurance
LINDLEY · Understanding Uncertainty, Revised Edition
LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition
Lloyd · The Statistical Analysis of Categorical Data
LOWEN and TEICH · Fractal-Based Point Processes
MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications in Statistics and Econo-
metrics, Revised Edition
MALLER and ZHOU · Survival Analysis with Long Term Survivors

MARCHETTE · Random Graphs for Statistical Pattern Recognition
MARDIA and JUPP · Directional Statistics
MARKOVICH · Nonparametric Analysis of Univariate Heavy-Tailed Data: Research and Practice
MARONNA, MARTIN and YOHAI · Robust Statistics: Theory and Methods
MASON, GUNST, and HESS · Statistical Design and Analysis of Experiments with Applications to Engi-
neering and Science, Second Edition
McCULLOCH, SEARLE, and NEUHAUS · Generalized, Linear, and Mixed Models, Second Edition
McFADDEN · Management of Data in Clinical Trials, Second Edition
* McLACHLAN · Discriminant Analysis and Statistical Pattern Recognition
McLACHLAN, DO, and AMBROISE · Analyzing Microarray Gene Expression Data
McLACHLAN and KRISHNAN · The EM Algorithm and Extensions, Second Edition
McLACHLAN and PEEL · Finite Mixture Models
McNEIL · Epidemiological Research Methods
Meeker and Escobar · Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of Independent Random Vectors:
Heavy Tails in Theory and Practice
Mengersen, Robert, and Titterington · Mixtures: Estimation and Applications
MICKEY, DUNN, and CLARK · Applied Statistics: Analysis of Variance and Regression, Third Edition
* MILLER · Survival Analysis, Second Edition
MONTGOMERY, JENNINGS, and KULAHCI · Introduction to Time Series Analysis and Forecasting,
Second Edition
MONTGOMERY, PECK, and VINING · Introduction to Linear Regression Analysis, Fifth Edition
MORGENTHALER and TUKEY · ConfiguralPolysampling: A Route to Practical Robustness
MUIRHEAD · Aspects of Multivariate Statistical Theory
Muller and Stoyan · Comparison Methods for Stochastic Models and Risks
MURTHY, XIE, and JIANG · Weibull Models
MYERS, MONTGOMERY, and ANDERSON-COOK · Response Surface Methodology: Process and
Product Optimization Using Designed Experiments, Third Edition
MYERS, MONTGOMERY, VINING, and ROBINSON · Generalized Linear Models. With Applications
in Engineering and the Sciences, Second Edition
Natvig · Multistate Systems Reliability Theory With Applications
† NELSON · Accelerated Testing, Statistical Models, Test Plans, and Data Analyses
† NELSON · Applied Life Data Analysis
NEWMAN · Biostatistical Methods in Epidemiology
Ng, Tain, and Tang · Dirichlet Theory: Theory, Methods and Applications
OKABE, BOOTS, SUGIHARA, and CHIU · Spatial Tesselations: Concepts and Applications of Voronoi
Diagrams, Second Edition
OLIVER and SMITH · Influence Diagrams, Belief Nets and Decision Analysis
PALTA · Quantitative Methods in Population Health: Extensions of Ordinary Regressions
PANJER · Operational Risk: Modeling and Analytics
PANKRATZ · Forecasting with Dynamic Regression Models
PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and Cases
PARDOUX · Markov Processes and Applications: Algorithms, Networks, Genome and Finance

Parmigiani and Inoue · Decision Theory: Principles and Approaches
* PARZEN · Modern Probability Theory and Its Applications
PE ˜NA, TIAO, and TSAY · A Course in Time Series Analysis
Pesarin and Salmaso · Permutation Tests for Complex Data: Applications and Software
PIANTADOSI · Clinical Trials: A Methodologic Perspective, Second Edition
POURAHMADI · Foundations of Time Series Analysis and Prediction Theory
POURAHMADI · High-Dimensional Covariance Estimation
POWELL · Approximate Dynamic Programming: Solving the Curses of Dimensionality, Second Edition
POWELL and RYZHOV · Optimal Learning
PRESS · Subjective and Objective Bayesian Statistics, Second Edition
PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach
PURI, VILAPLANA, and WERTZ · New Perspectives in Theoretical and Applied Statistics
† PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic Programming
QIU · Image Processing and Jump Regression Analysis
* RAO · Linear Statistical Inference and Its Applications, Second Edition
RAO · Statistical Inference for Fractional Diffusion Processes
RAUSAND and HØYLAND · System Reliability Theory: Models, Statistical Methods, and Applications,
Second Edition
Rayner, THAS, and BEST · Smooth Tests of Goodnes of Fit: Using R, Second Edition
RENCHER and SCHAALJE · Linear Models in Statistics, Second Edition
RENCHER and CHRISTENSEN · Methods of Multivariate Analysis, Third Edition
RENCHER · Multivariate Statistical Inference with Applications
RIGDON and BASU · Statistical Methods for the Reliability of Repairable Systems
* RIPLEY · Spatial Statistics
* RIPLEY · Stochastic Simulation
ROHATGI and SALEH · An Introduction to Probability and Statistics, Third Edition
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS · Stochastic Processes for Insurance and Finance
ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and Practice
ROSSI, ALLENBY, and McCULLOCH · Bayesian Statistics and Marketing
† ROUSSEEUW and LEROY · Robust Regression and Outlier Detection
Royston and Sauerbrei · Multivariate Model Building: A Pragmatic Approach to Regression Analysis
Based on Fractional Polynomials for Modeling Continuous Variables
* RUBIN · Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN and KROESE · Simulation and the Monte Carlo Method, Second Edition
RUBINSTEIN and MELAMED · Modern Simulation and Modeling
RUBINSTEIN, RIDDER, and VAISMAN · Fast Sequential Monte Carlo Methods for Counting and
Optimization
RYAN · Modern Engineering Statistics
RYAN · Modern Experimental Design
RYAN · Modern Regression Methods, Second Edition
Ryan · Sample Size Determination and Power
RYAN · Statistical Methods for Quality Improvement, Third Edition

SALEH · Theory of Preliminary Test and Stein-Type Estimation with Applications
SALTELLI, CHAN, and SCOTT (editors) · Sensitivity Analysis
Scherer · Batch Effects and Noise in Microarray Experiments: Sources and Solutions
* SCHEFFE · The Analysis of Variance
SCHIMEK · Smoothing and Regression: Approaches, Computation, and Application
SCHOTT · Matrix Analysis for Statistics, Second Edition
SCHOTT ·Matrix Analysis for Statistics, Third Edition
Schoutens · Levy Processes in Finance: Pricing Financial Derivatives
SCOTT · Multivariate Density Estimation
SCOTT · Multivariate Density Estimation: Theory, Practice, and Visualization
* SEARLE · Linear Models
† SEARLE · Linear Models for Unbalanced Data
† SEARLE · Matrix Algebra Useful for Statistics
† SEARLE, CASELLA, and McCULLOCH · Variance Components
SEARLE and WILLETT · Matrix Algebra for Applied Economics
SEBER · A Matrix Handbook For Statisticians
† SEBER · Multivariate Observations
SEBER and LEE · Linear Regression Analysis, Second Edition
† SEBER and WILD · Nonlinear Regression
SENNOTT · Stochastic Dynamic Programming and the Control of Queueing Systems
* SERFLING · Approximation Theorems of Mathematical Statistics
SHAFER and VOVK · Probability and Finance: It’s Only a Game!
SHERMAN · Spatial Statistics and Spatio-Temporal Data: Covariance Functions and Directional
Properties
SILVAPULLE and SEN · Constrained Statistical Inference: Inequality, Order, and Shape Restrictions
SINGPURWALLA · Reliability and Risk: A Bayesian Perspective
SMALL and McLEISH · Hilbert Space Methods in Probability and Statistical Inference
SRIVASTAVA · Methods of Multivariate Statistics
STAPLETON · Linear Statistical Models, Second Edition
STAPLETON · Models for Probability and Statistical Inference: Theory and Applications
STAUDTE and SHEATHER · Robust Estimation and Testing
Stoyan · Counterexamples in Probability, Second Edition
STOYAN and STOYAN · Fractals, Random Shapes and Point Fields: Methods of Geometrical Statistics
STREET and BURGESS · The Construction of Optimal Stated Choice Experiments: Theory and Methods
STYAN · The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON, and SONG · Methods for Meta-Analysis in Medical Research
TAKEZAWA · Introduction to Nonparametric Regression
TAMHANE · Statistical Analysis of Designed Experiments: Theory and Applications
TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON · Empirical Model Building: Data, Models, and Reality, Second Edition
THOMPSON · Sampling, Third Edition
THOMPSON · Simulation: A Modeler’s Approach

THOMPSON and SEBER · Adaptive Sampling
THOMPSON, WILLIAMS, and FINDLAY · Models for Investors in Real World Markets
TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical Computing and Dynamic
Graphics
TROFFAES and DE COOMAN · Lower Previsions
TSAY · Analysis of Financial Time Series, Third Edition
TSAY · An Introduction to Analysis of Financial Data with R
TSAY · Multivariate Time Series Analysis: With R and Financial Applications
UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II: Categorical and Directional
Data
† VAN BELLE · Statistical Rules of Thumb, Second Edition
VAN BELLE, FISHER, HEAGERTY, and LUMLEY · Biostatistics: A Methodology for the Health
Sciences, Second Edition
VESTRUP · The Theory of Measures and Integration
VIDAKOVIC · Statistical Modeling by Wavelets
Viertl · Statistical Methods for Fuzzy Data
VINOD and REAGLE · Preparing for the Worst: Incorporating Downside Risk in Stock Market Invest-
ments
WALLER and GOTWAY · Applied Spatial Statistics for Public Health Data
WEISBERG · Applied Linear Regression, Fourth Edition
WEISBERG · Bias and Causation: Models and Judgment for Valid Comparisons
WELSH · Aspects of Statistical Inference
WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and Methods for p-Value
Adjustment
* WHITTAKER · Graphical Models in Applied Multivariate Statistics
WINKER · Optimization Heuristics in Economics: Applications of Threshold Accepting
WOODWORTH · Biostatistics: A Bayesian Introduction
WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical Data, Second Edition
WU and HAMADA · Experiments: Planning, Analysis, and Parameter Design Optimization, Second
Edition
WU and ZHANG · Nonparametric Regression Methods for Longitudinal Data Analysis
Yakir · Extremes in Random Fields
YIN · Clinical Trial Design: Bayesian and Frequentist Adaptive Methods
YOUNG, VALERO-MORA, and FRIENDLY · Visual Statistics: Seeing Data with Dynamic Interactive
Graphics
ZACKS · Examples and Problems in Mathematical Statistics
ZACKS · Stage-Wise Adaptive Designs
* ZELLNER · An Introduction to Bayesian Inference in Econometrics
ZELTERMAN · Discrete Distributions—Applications in the Health Sciences
ZHOU, OBUCHOWSKI, and McCLISH · Statistical Methods in Diagnostic Medicine, Second Edition

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.

